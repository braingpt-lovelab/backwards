# Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text

<div style="display: flex; justify-content: center; align-items: center;">
    <img src="https://github.com/user-attachments/assets/090584ca-12c4-44e4-aa48-d97b510e7fab" style="height: 300px; width: auto; margin-right: 10px;">
    <img src="https://github.com/user-attachments/assets/598edb45-8f9c-419d-95f3-4e2ade596299" style="height: 300px; width: auto;">
</div>



### Work with this repo locally:
```
git clone https://github.com/braingpt-lovelab/backwards --recursive
```

### Repo structure
* `model_training/`: training scripts for both forward and backward GPT-2 models.
* `analyses/`: post-training analyses scripts for producing results in the paper. 

### Attribution
```
@article{luo2024beyond,
  title={Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text},
  author={Luo, X. and Ramscar, M. and Love, B. C.},
  journal={arXiv preprint arXiv:2411.11061},
  year={2024}
}
```
