# Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text
![image](https://github.com/user-attachments/assets/090584ca-12c4-44e4-aa48-d97b510e7fab)

### Work with this repo locally:
```
git clone https://github.com/braingpt-lovelab/backwards --recursive
```

### Repo structure
* `model_training/`: training scripts for both forward and backward GPT-2 models.
* `analyses/`: post-training analyses scripts for producing results in the paper. 

### Attribution
```
@article{luo2024beyond,
  title={Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text},
  author={Luo, X. and Ramscar, M. and Love, B. C.},
  journal={arXiv preprint arXiv:2411.11061},
  year={2024}
}
```
