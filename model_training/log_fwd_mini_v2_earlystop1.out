nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_135713-y1iuvq6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-breeze-312
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/y1iuvq6b
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0,  8158,  1416,  ...,   413,    29,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5899e-04, -6.6102e-04, -7.2001e-05,  ...,  6.3213e-04,
         -2.1844e-04, -2.9654e-04],
        [-1.9372e-07, -1.3225e-07,  1.5087e-07,  ..., -7.1712e-08,
         -4.9546e-07, -4.7497e-07],
        [-5.4389e-07, -4.4517e-07,  3.5577e-07,  ..., -1.0990e-07,
         -1.4454e-06, -1.5125e-06],
        [-2.4773e-07, -1.8813e-07,  2.5332e-07,  ..., -2.4680e-08,
         -6.9663e-07, -6.4820e-07],
        [-2.4214e-07, -2.0955e-07,  2.7940e-08,  ..., -4.1211e-08,
         -6.3330e-07, -4.5821e-07]], device='cuda:0')
Loss: 1.381668210029602


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[   0,  266, 1422,  ...,  446, 1992,  586],
        [   0, 1607,   12,  ...,  895, 1503,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3242e-04, -1.1336e-03, -2.3221e-04,  ...,  3.8676e-04,
         -1.0851e-03, -2.9566e-04],
        [-4.7497e-08, -3.7253e-07,  4.4145e-07,  ...,  4.7497e-08,
         -1.3411e-06, -1.0226e-06],
        [-2.4773e-07, -1.0598e-06,  9.7044e-07,  ...,  3.3528e-07,
         -3.4720e-06, -2.8610e-06],
        [-2.0489e-08, -4.9174e-07,  6.6496e-07,  ...,  2.9942e-07,
         -1.8664e-06, -1.4044e-06],
        [-1.5926e-07, -4.6100e-07,  1.7229e-07,  ...,  1.3853e-07,
         -1.5087e-06, -8.8476e-07]], device='cuda:0')
Loss: 1.3763436079025269


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0,    14,   381,  ...,  2195,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5004e-04, -1.8562e-03, -1.4519e-04,  ...,  7.4696e-04,
         -2.1892e-03,  6.2129e-04],
        [ 6.3796e-07, -3.4890e-07,  1.1344e-06,  ...,  3.5483e-07,
         -2.3246e-06, -1.7565e-06],
        [ 1.6745e-06, -1.0903e-06,  2.6990e-06,  ...,  1.3262e-06,
         -6.1393e-06, -4.8727e-06],
        [ 1.0449e-06, -4.8085e-07,  1.5963e-06,  ...,  8.6939e-07,
         -3.1553e-06, -2.4401e-06],
        [ 4.2189e-07, -4.8790e-07,  6.5658e-07,  ...,  4.2538e-07,
         -2.3991e-06, -1.4286e-06]], device='cuda:0')
Loss: 1.37981116771698


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,   995,    13,  ...,  3494,   367,  6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0775e-05, -1.9968e-03,  7.1347e-04,  ...,  1.5155e-03,
         -3.4760e-03,  9.0713e-04],
        [ 5.7416e-07, -6.9721e-07,  1.1905e-06,  ...,  4.2096e-07,
         -2.7772e-06, -2.1476e-06],
        [ 1.4780e-06, -2.0813e-06,  2.7193e-06,  ...,  1.5851e-06,
         -7.2718e-06, -5.9605e-06],
        [ 9.5973e-07, -9.9867e-07,  1.6694e-06,  ...,  1.0920e-06,
         -3.7365e-06, -2.9579e-06],
        [ 3.2410e-07, -8.6601e-07,  6.0163e-07,  ...,  5.3435e-07,
         -2.8722e-06, -1.7490e-06]], device='cuda:0')
Loss: 1.375632643699646


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[    0,   287,  1477,  ...,   997,   292,  4471],
        [    0,  3227,   278,  ...,  2950,    14, 15544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9980e-04, -2.4794e-03,  1.1284e-03,  ...,  2.0875e-03,
         -4.4334e-03,  8.0653e-04],
        [ 8.2748e-07, -7.6753e-07,  1.6338e-06,  ...,  5.7556e-07,
         -3.6191e-06, -2.6114e-06],
        [ 2.1299e-06, -2.4408e-06,  3.6954e-06,  ...,  2.1067e-06,
         -9.4324e-06, -7.2494e-06],
        [ 1.2969e-06, -1.1579e-06,  2.2543e-06,  ...,  1.3397e-06,
         -4.7274e-06, -3.5539e-06],
        [ 5.1782e-07, -9.8243e-07,  8.9407e-07,  ...,  7.0198e-07,
         -3.7216e-06, -2.1365e-06]], device='cuda:0')
Loss: 1.377576470375061


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[    0, 29368,    13,  ...,   376,    88,  3333],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1308e-03, -2.4198e-03,  1.9124e-03,  ...,  2.4509e-03,
         -5.7382e-03,  1.3137e-03],
        [ 1.0566e-06, -7.0466e-07,  1.9989e-06,  ...,  5.8621e-07,
         -4.2748e-06, -3.2857e-06],
        [ 2.5956e-06, -2.4250e-06,  4.5187e-06,  ...,  2.2007e-06,
         -1.0990e-05, -8.9630e-06],
        [ 1.6005e-06, -1.1055e-06,  2.7684e-06,  ...,  1.4422e-06,
         -5.5172e-06, -4.4443e-06],
        [ 6.8638e-07, -9.5484e-07,  1.1194e-06,  ...,  7.0256e-07,
         -4.3325e-06, -2.6692e-06]], device='cuda:0')
Loss: 1.3740524053573608


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[   0,   12, 4957,  ...,  944,  278,  609],
        [   0, 5054, 3945,  ...,  272,  278,  516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9466e-03, -2.5292e-03,  2.1365e-03,  ...,  2.5738e-03,
         -5.9757e-03,  1.5254e-03],
        [ 1.1954e-06, -7.5449e-07,  2.2261e-06,  ...,  5.6083e-07,
         -4.7516e-06, -3.6545e-06],
        [ 2.8973e-06, -2.5750e-06,  4.9433e-06,  ...,  2.2176e-06,
         -1.2003e-05, -9.9391e-06],
        [ 1.7895e-06, -1.2108e-06,  3.1036e-06,  ...,  1.4785e-06,
         -6.1467e-06, -5.0254e-06],
        [ 7.7067e-07, -1.0363e-06,  1.2191e-06,  ...,  6.8510e-07,
         -4.8243e-06, -3.0044e-06]], device='cuda:0')
Loss: 1.379459261894226


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 3164,   12,  ...,  984,  344, 3993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3608e-03, -2.9404e-03,  1.5434e-03,  ...,  2.8837e-03,
         -6.7349e-03,  6.1856e-04],
        [ 1.4310e-06, -7.1793e-07,  2.6005e-06,  ...,  8.2346e-07,
         -5.8096e-06, -4.5970e-06],
        [ 3.5195e-06, -2.6067e-06,  5.7741e-06,  ...,  3.0223e-06,
         -1.4745e-05, -1.2428e-05],
        [ 2.1099e-06, -1.1372e-06,  3.6624e-06,  ...,  1.9125e-06,
         -7.5623e-06, -6.2995e-06],
        [ 9.5880e-07, -1.0687e-06,  1.4231e-06,  ...,  9.6822e-07,
         -5.8152e-06, -3.7830e-06]], device='cuda:0')
Loss: 1.3757104873657227
Graident accumulation at epoch 0, step 7, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3608e-04, -2.9404e-04,  1.5434e-04,  ...,  2.8837e-04,
         -6.7349e-04,  6.1856e-05],
        [ 1.4310e-07, -7.1793e-08,  2.6005e-07,  ...,  8.2346e-08,
         -5.8096e-07, -4.5970e-07],
        [ 3.5195e-07, -2.6067e-07,  5.7741e-07,  ...,  3.0223e-07,
         -1.4745e-06, -1.2428e-06],
        [ 2.1099e-07, -1.1372e-07,  3.6624e-07,  ...,  1.9125e-07,
         -7.5623e-07, -6.2995e-07],
        [ 9.5880e-08, -1.0687e-07,  1.4231e-07,  ...,  9.6822e-08,
         -5.8152e-07, -3.7830e-07]], device='cuda:0')
optimizer state dict: tensor([[5.5732e-09, 8.6460e-09, 2.3822e-09,  ..., 8.3158e-09, 4.5359e-08,
         3.8262e-10],
        [2.0477e-15, 5.1543e-16, 6.7625e-15,  ..., 6.7809e-16, 3.3751e-14,
         2.1132e-14],
        [1.2387e-14, 6.7947e-15, 3.3340e-14,  ..., 9.1340e-15, 2.1741e-13,
         1.5444e-13],
        [4.4517e-15, 1.2932e-15, 1.3413e-14,  ..., 3.6575e-15, 5.7189e-14,
         3.9683e-14],
        [9.1929e-16, 1.1421e-15, 2.0251e-15,  ..., 9.3745e-16, 3.3816e-14,
         1.4311e-14]], device='cuda:0')
optimizer state dict: 1.0
lr: [2.5445292620865143e-06, 2.5445292620865143e-06]
scheduler_last_epoch: 1


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0, 1594,  586,  ...,   13,  701,  308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1718e-04, -1.1122e-04,  6.3018e-04,  ..., -3.6876e-04,
          8.5216e-04, -1.0828e-03],
        [ 3.0175e-07, -5.1782e-07,  3.5390e-07,  ...,  3.3062e-08,
         -7.7486e-07, -3.5577e-07],
        [ 7.1153e-07, -1.4082e-06,  8.3074e-07,  ...,  1.9372e-07,
         -1.8030e-06, -8.6427e-07],
        [ 4.5449e-07, -7.8604e-07,  5.2899e-07,  ...,  1.5460e-07,
         -1.0803e-06, -5.4389e-07],
        [ 2.7381e-07, -5.7369e-07,  2.4028e-07,  ...,  9.5926e-08,
         -8.4192e-07, -3.4086e-07]], device='cuda:0')
Loss: 1.3777410984039307


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[    0,  3806,    13,  ..., 11786,  2254,   221],
        [    0,    18,   271,  ...,  4868,   963,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4393e-04, -9.4308e-04,  7.7401e-04,  ..., -7.8146e-04,
          5.1069e-04, -1.7971e-03],
        [ 6.8918e-07, -8.0094e-07,  8.2701e-07,  ..., -1.4110e-07,
         -1.4491e-06, -1.0375e-06],
        [ 1.6876e-06, -2.2277e-06,  1.8738e-06,  ..., -1.2666e-07,
         -3.4869e-06, -2.5555e-06],
        [ 1.0394e-06, -1.2163e-06,  1.2107e-06,  ...,  7.4506e-09,
         -2.0154e-06, -1.5348e-06],
        [ 5.8115e-07, -8.5123e-07,  5.5693e-07,  ..., -2.9802e-08,
         -1.4789e-06, -8.8103e-07]], device='cuda:0')
Loss: 1.3762363195419312


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[   0, 2383, 9843,  ...,  401, 3959,  300],
        [   0, 2816,  292,  ..., 3662,  461, 2723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2450e-04, -1.5055e-03,  1.5079e-03,  ..., -9.1928e-04,
          1.1402e-04, -1.9344e-03],
        [ 1.1493e-06, -5.6066e-07,  1.0580e-06,  ..., -1.1409e-07,
         -1.8999e-06, -1.6633e-06],
        [ 2.9020e-06, -1.7099e-06,  2.4140e-06,  ...,  7.6368e-08,
         -4.7684e-06, -4.2766e-06],
        [ 1.8217e-06, -8.8476e-07,  1.5609e-06,  ...,  1.8254e-07,
         -2.7157e-06, -2.5406e-06],
        [ 1.0282e-06, -6.5006e-07,  6.2771e-07,  ...,  4.6100e-08,
         -1.9819e-06, -1.3765e-06]], device='cuda:0')
Loss: 1.3759078979492188


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0, 32878,   593,  ...,   437,  1329,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0863e-04, -1.7732e-03,  1.5526e-03,  ..., -3.7463e-04,
         -1.2299e-03, -1.5848e-03],
        [ 1.1937e-06, -5.3586e-07,  1.2247e-06,  ...,  0.0000e+00,
         -2.6636e-06, -2.1961e-06],
        [ 3.1115e-06, -1.8091e-06,  2.9169e-06,  ...,  5.1782e-07,
         -7.0035e-06, -6.0201e-06],
        [ 1.9185e-06, -9.1293e-07,  1.8608e-06,  ...,  3.9395e-07,
         -3.6471e-06, -3.2969e-06],
        [ 1.0678e-06, -7.0198e-07,  6.9570e-07,  ...,  1.9418e-07,
         -2.7604e-06, -1.8086e-06]], device='cuda:0')
Loss: 1.3792593479156494


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,    5, 2122,  271],
        [   0,  381, 1659,  ..., 1403,  271, 6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5158e-03, -1.4043e-03,  1.3883e-03,  ...,  8.5730e-05,
         -1.6945e-03, -2.2113e-03],
        [ 1.2298e-06, -6.4762e-07,  1.7015e-06,  ..., -1.3225e-07,
         -3.2336e-06, -2.5686e-06],
        [ 3.1639e-06, -2.0410e-06,  3.8408e-06,  ...,  2.8312e-07,
         -8.1584e-06, -6.8881e-06],
        [ 1.9786e-06, -1.0759e-06,  2.5425e-06,  ...,  2.8498e-07,
         -4.3772e-06, -3.8184e-06],
        [ 1.0391e-06, -8.0490e-07,  1.0235e-06,  ...,  1.1176e-07,
         -3.3416e-06, -2.0992e-06]], device='cuda:0')
Loss: 1.3794163465499878


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[    0,   266,  2552,  ...,    13, 16179,   800],
        [    0, 17471,  7279,  ...,   328,  6179,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6272e-03, -1.6848e-03,  2.7662e-03,  ...,  3.0864e-04,
         -1.7143e-03, -1.8994e-03],
        [ 1.2647e-06, -6.8883e-07,  2.0014e-06,  ..., -2.1793e-07,
         -3.9451e-06, -3.4738e-06],
        [ 3.1700e-06, -2.2217e-06,  4.5560e-06,  ...,  1.4063e-07,
         -9.9912e-06, -9.3020e-06],
        [ 1.9962e-06, -1.0990e-06,  2.8983e-06,  ...,  2.2934e-07,
         -5.1521e-06, -4.8615e-06],
        [ 1.0170e-06, -8.5775e-07,  1.1837e-06,  ..., -1.8626e-09,
         -4.0419e-06, -2.8852e-06]], device='cuda:0')
Loss: 1.3737728595733643


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0, 9855,  278,  ...,  266, 3134,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8792e-03, -1.9441e-03,  3.3766e-03,  ...,  2.4101e-04,
         -1.8999e-03, -2.0666e-03],
        [ 1.4268e-06, -8.2946e-07,  2.3218e-06,  ..., -2.2852e-07,
         -4.9062e-06, -4.2859e-06],
        [ 3.3916e-06, -2.6948e-06,  5.4166e-06,  ...,  1.9209e-07,
         -1.2405e-05, -1.1507e-05],
        [ 2.2588e-06, -1.2740e-06,  3.3751e-06,  ...,  2.5670e-07,
         -6.4932e-06, -6.0461e-06],
        [ 1.1283e-06, -1.0161e-06,  1.3830e-06,  ...,  4.3947e-09,
         -4.9546e-06, -3.5856e-06]], device='cuda:0')
Loss: 1.376901626586914


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[    0,   199, 14973,  ...,   638,  1119,  1329],
        [    0,  2352,  4275,  ..., 10518,   342,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9947e-03, -2.7854e-03,  3.0757e-03,  ...,  5.7582e-04,
         -2.7795e-03, -2.5766e-03],
        [ 1.7732e-06, -8.6369e-07,  2.5518e-06,  ..., -3.1467e-07,
         -5.4389e-06, -4.9416e-06],
        [ 4.3527e-06, -2.8438e-06,  5.8897e-06,  ...,  5.3318e-08,
         -1.3791e-05, -1.3236e-05],
        [ 2.8288e-06, -1.3004e-06,  3.7737e-06,  ...,  2.0477e-07,
         -7.2084e-06, -7.0147e-06],
        [ 1.4487e-06, -1.0458e-06,  1.5018e-06,  ..., -3.7282e-08,
         -5.5172e-06, -4.1630e-06]], device='cuda:0')
Loss: 1.3801281452178955
Graident accumulation at epoch 0, step 15, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.1194e-04, -5.4317e-04,  4.4648e-04,  ...,  3.1712e-04,
         -8.8410e-04, -2.0199e-04],
        [ 3.0611e-07, -1.5098e-07,  4.8923e-07,  ...,  4.2645e-08,
         -1.0668e-06, -9.0789e-07],
        [ 7.5203e-07, -5.1898e-07,  1.1086e-06,  ...,  2.7734e-07,
         -2.7061e-06, -2.4421e-06],
        [ 4.7277e-07, -2.3238e-07,  7.0699e-07,  ...,  1.9260e-07,
         -1.4015e-06, -1.2684e-06],
        [ 2.3116e-07, -2.0076e-07,  2.7825e-07,  ...,  8.3412e-08,
         -1.0751e-06, -7.5677e-07]], device='cuda:0')
optimizer state dict: tensor([[2.1525e-08, 1.6396e-08, 1.1840e-08,  ..., 8.6391e-09, 5.3040e-08,
         7.0213e-09],
        [5.1900e-15, 1.2609e-15, 1.3268e-14,  ..., 7.7643e-16, 6.3299e-14,
         4.5531e-14],
        [3.1321e-14, 1.4875e-14, 6.7995e-14,  ..., 9.1278e-15, 4.0738e-13,
         3.2948e-13],
        [1.2449e-14, 2.9829e-15, 2.7641e-14,  ..., 3.6958e-15, 1.0909e-13,
         8.8850e-14],
        [3.0170e-15, 2.2346e-15, 4.2784e-15,  ..., 9.3790e-16, 6.4221e-14,
         3.1628e-14]], device='cuda:0')
optimizer state dict: 2.0
lr: [5.0890585241730285e-06, 5.0890585241730285e-06]
scheduler_last_epoch: 2


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8874e-04, -4.6690e-04,  5.7047e-04,  ...,  2.9755e-04,
         -5.0614e-05,  2.1809e-04],
        [-9.9652e-08, -6.5193e-07,  7.3761e-07,  ..., -1.4901e-07,
         -6.6310e-07, -6.0350e-07],
        [-2.3376e-07, -1.5423e-06,  1.5348e-06,  ..., -2.3562e-07,
         -1.4156e-06, -1.5050e-06],
        [-7.2177e-08, -7.3388e-07,  9.0897e-07,  ..., -5.3784e-08,
         -7.5623e-07, -7.2643e-07],
        [-1.1129e-07, -5.5134e-07,  5.1036e-07,  ..., -7.9628e-08,
         -5.7742e-07, -4.3027e-07]], device='cuda:0')
Loss: 1.3625731468200684


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[    0,  1581, 11884,  ...,  7031,   689,   527],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9855e-04, -1.6552e-03,  2.4885e-04,  ..., -1.2108e-04,
         -3.4685e-04,  3.6158e-05],
        [-3.2969e-07, -1.6056e-06,  1.1921e-06,  ..., -5.1223e-07,
         -1.7062e-06, -8.0280e-07],
        [-8.1118e-07, -4.0457e-06,  2.5928e-06,  ..., -1.0328e-06,
         -3.9786e-06, -2.0526e-06],
        [-3.5716e-07, -2.0154e-06,  1.5460e-06,  ..., -4.5798e-07,
         -2.0973e-06, -9.9465e-07],
        [-3.9814e-07, -1.6615e-06,  8.9221e-07,  ..., -4.3726e-07,
         -1.8068e-06, -5.4622e-07]], device='cuda:0')
Loss: 1.36259925365448


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  365, 1943,  259],
        [   0,  360,  259,  ...,   14,  381, 1371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9189e-04, -1.9352e-03,  1.8489e-03,  ..., -2.5120e-04,
         -6.4746e-04, -3.7440e-04],
        [-8.5868e-07, -2.2501e-06,  1.6913e-06,  ..., -5.7789e-07,
         -2.7344e-06, -1.4249e-06],
        [-2.0629e-06, -5.6177e-06,  3.6806e-06,  ..., -1.0999e-06,
         -6.4224e-06, -3.5800e-06],
        [-9.5321e-07, -2.8163e-06,  2.1718e-06,  ..., -4.7847e-07,
         -3.3192e-06, -1.7695e-06],
        [-9.7929e-07, -2.3432e-06,  1.2722e-06,  ..., -4.6578e-07,
         -2.9244e-06, -1.1013e-06]], device='cuda:0')
Loss: 1.3605363368988037


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[   0, 1304,  292,  ..., 2101,  292,  474],
        [   0, 2088, 5370,  ..., 1110, 3380,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1132e-03, -1.5650e-03,  1.9859e-03,  ...,  3.3433e-05,
         -1.8649e-03, -6.2931e-04],
        [-9.5600e-07, -2.9765e-06,  2.3469e-06,  ..., -9.5414e-07,
         -3.5055e-06, -2.5127e-06],
        [-2.1509e-06, -7.3314e-06,  5.2527e-06,  ..., -1.8971e-06,
         -8.3074e-06, -6.2473e-06],
        [-1.0217e-06, -3.6359e-06,  2.9206e-06,  ..., -8.1002e-07,
         -4.1425e-06, -2.9765e-06],
        [-1.1944e-06, -3.1106e-06,  1.7155e-06,  ..., -7.6566e-07,
         -3.7067e-06, -2.0773e-06]], device='cuda:0')
Loss: 1.3606832027435303


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 42329,   472,  ...,   292,    33,  3092],
        [    0,  3889,  4039,  ...,   616, 22910,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4719e-03, -2.3097e-03,  2.5237e-03,  ..., -2.2847e-05,
         -2.6834e-03, -9.5378e-04],
        [-1.1059e-06, -3.4980e-06,  2.6859e-06,  ..., -1.4049e-06,
         -4.1686e-06, -2.9001e-06],
        [-2.5048e-06, -8.7246e-06,  6.0499e-06,  ..., -2.9476e-06,
         -9.9987e-06, -7.2233e-06],
        [-1.1688e-06, -4.2729e-06,  3.3975e-06,  ..., -1.2552e-06,
         -4.8727e-06, -3.4571e-06],
        [-1.3919e-06, -3.7439e-06,  1.9744e-06,  ..., -1.2406e-06,
         -4.5113e-06, -2.3940e-06]], device='cuda:0')
Loss: 1.3635929822921753


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,   892,   271,  ...,   278,   266, 10237],
        [    0,    14,  8383,  ...,   266,  1717,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7586e-03, -2.8073e-03,  3.0003e-03,  ...,  2.9360e-04,
         -2.7228e-03, -1.6377e-03],
        [-1.5865e-06, -4.1574e-06,  3.3453e-06,  ..., -1.3884e-06,
         -5.0589e-06, -3.6117e-06],
        [-3.6596e-06, -1.0327e-05,  7.5325e-06,  ..., -2.7698e-06,
         -1.2144e-05, -9.0040e-06],
        [-1.6904e-06, -5.0701e-06,  4.2170e-06,  ..., -1.1681e-06,
         -5.9381e-06, -4.3511e-06],
        [-1.8668e-06, -4.3996e-06,  2.5108e-06,  ..., -1.2085e-06,
         -5.4203e-06, -3.0459e-06]], device='cuda:0')
Loss: 1.360700011253357


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[   0,  586, 1016,  ..., 7151, 8280,  300],
        [   0, 7070,   86,  ...,  298, 4930,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7672e-03, -3.4596e-03,  2.9256e-03,  ..., -1.2931e-04,
         -3.3297e-03, -1.4508e-03],
        [-1.5733e-06, -4.7944e-06,  3.8370e-06,  ..., -1.5150e-06,
         -6.0648e-06, -4.3977e-06],
        [-3.6725e-06, -1.1958e-05,  8.5309e-06,  ..., -2.9560e-06,
         -1.4424e-05, -1.0956e-05],
        [-1.6785e-06, -5.8487e-06,  4.8093e-06,  ..., -1.2245e-06,
         -7.0781e-06, -5.3346e-06],
        [-1.8997e-06, -5.0589e-06,  2.8536e-06,  ..., -1.3026e-06,
         -6.4261e-06, -3.7537e-06]], device='cuda:0')
Loss: 1.3630280494689941


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[   0, 3699, 3058,  ...,  820, 5327, 8055],
        [   0,  287,  271,  ..., 1039, 4186,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8030e-03, -3.2233e-03,  3.9658e-03,  ...,  6.1071e-04,
         -3.5021e-03, -5.6378e-04],
        [-1.7828e-06, -5.5656e-06,  4.5225e-06,  ..., -1.7078e-06,
         -6.8396e-06, -4.8783e-06],
        [-4.2052e-06, -1.3687e-05,  9.9838e-06,  ..., -3.2429e-06,
         -1.6116e-05, -1.2066e-05],
        [-1.8834e-06, -6.6683e-06,  5.6289e-06,  ..., -1.3567e-06,
         -7.9051e-06, -5.8822e-06],
        [-2.1278e-06, -5.7705e-06,  3.3379e-06,  ..., -1.4208e-06,
         -7.1749e-06, -4.1113e-06]], device='cuda:0')
Loss: 1.3629704713821411
Graident accumulation at epoch 0, step 23, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.3105e-04, -8.1118e-04,  7.9841e-04,  ...,  3.4648e-04,
         -1.1459e-03, -2.3817e-04],
        [ 9.7216e-08, -6.9244e-07,  8.9255e-07,  ..., -1.3240e-07,
         -1.6440e-06, -1.3049e-06],
        [ 2.5630e-07, -1.8358e-06,  1.9962e-06,  ..., -7.4685e-08,
         -4.0471e-06, -3.4045e-06],
        [ 2.3715e-07, -8.7597e-07,  1.1992e-06,  ...,  3.7669e-08,
         -2.0518e-06, -1.7298e-06],
        [-4.7409e-09, -7.5773e-07,  5.8421e-07,  ..., -6.7015e-08,
         -1.6851e-06, -1.0922e-06]], device='cuda:0')
optimizer state dict: tensor([[2.4755e-08, 2.6769e-08, 2.7555e-08,  ..., 9.0034e-09, 6.5251e-08,
         7.3322e-09],
        [8.3634e-15, 3.2235e-14, 3.3707e-14,  ..., 3.6923e-15, 1.1002e-13,
         6.9283e-14],
        [4.8973e-14, 2.0219e-13, 1.6760e-13,  ..., 1.9635e-14, 6.6669e-13,
         4.7474e-13],
        [1.5984e-14, 4.7446e-14, 5.9298e-14,  ..., 5.5328e-15, 1.7147e-13,
         1.2336e-13],
        [7.5417e-15, 3.5531e-14, 1.5415e-14,  ..., 2.9558e-15, 1.1564e-13,
         4.8499e-14]], device='cuda:0')
optimizer state dict: 3.0
lr: [7.633587786259543e-06, 7.633587786259543e-06]
scheduler_last_epoch: 3


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[    0,  2919,  1482,  ...,   587, 20186,   275],
        [    0,  1552,   271,  ...,    13,   287,   995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3917e-04,  6.5980e-04, -2.7737e-04,  ...,  5.4120e-04,
         -3.5274e-04,  3.5727e-04],
        [-9.7603e-07, -1.8924e-06,  1.1846e-06,  ..., -3.3341e-07,
         -9.8348e-07, -9.3132e-07],
        [-1.8701e-06, -3.7253e-06,  2.2203e-06,  ..., -4.6566e-07,
         -1.9222e-06, -1.8626e-06],
        [-8.4937e-07, -1.6317e-06,  1.0431e-06,  ..., -2.1141e-07,
         -8.3074e-07, -8.1584e-07],
        [-9.7603e-07, -1.8701e-06,  1.0356e-06,  ..., -2.7753e-07,
         -1.0282e-06, -8.6054e-07]], device='cuda:0')
Loss: 1.3362468481063843


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[   0, 6275,   12,  ..., 2027, 2887,  287],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1908e-05,  5.0246e-04, -4.9524e-04,  ...,  9.8193e-04,
         -1.0209e-03,  5.2860e-04],
        [-2.3022e-06, -3.7923e-06,  1.9856e-06,  ..., -8.2888e-07,
         -1.3728e-06, -1.8179e-06],
        [-4.3884e-06, -7.3016e-06,  3.5837e-06,  ..., -1.3076e-06,
         -2.6450e-06, -3.5688e-06],
        [-2.0489e-06, -3.3677e-06,  1.7956e-06,  ..., -5.8208e-07,
         -1.1548e-06, -1.6019e-06],
        [-2.4661e-06, -3.9563e-06,  1.8217e-06,  ..., -7.6555e-07,
         -1.5497e-06, -1.8105e-06]], device='cuda:0')
Loss: 1.3406950235366821


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,   706,  1005,  ...,   278,   266,  5590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5532e-04, -4.4468e-05, -1.2943e-04,  ...,  9.3684e-04,
         -1.9184e-03,  4.9841e-04],
        [-3.5912e-06, -5.9679e-06,  2.4047e-06,  ..., -1.5106e-06,
         -2.2631e-06, -2.5779e-06],
        [-6.7577e-06, -1.1414e-05,  4.2878e-06,  ..., -2.5295e-06,
         -4.2766e-06, -4.9844e-06],
        [-3.2187e-06, -5.4240e-06,  2.2370e-06,  ..., -1.1893e-06,
         -2.0154e-06, -2.2948e-06],
        [-3.7998e-06, -6.2510e-06,  2.1253e-06,  ..., -1.4845e-06,
         -2.5406e-06, -2.5444e-06]], device='cuda:0')
Loss: 1.3319610357284546


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[    0,  1619,   938,  ...,   292, 10026, 14367],
        [    0,   368,  2418,  ...,  3275,  1116,  5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7823e-04, -1.9681e-04,  1.5147e-04,  ...,  3.8823e-05,
         -2.2945e-03, -5.6369e-04],
        [-4.7982e-06, -8.2031e-06,  3.3882e-06,  ..., -2.3004e-06,
         -3.1497e-06, -3.8669e-06],
        [-9.0227e-06, -1.5855e-05,  6.2250e-06,  ..., -3.9153e-06,
         -5.9679e-06, -7.6368e-06],
        [-4.0568e-06, -7.0035e-06,  3.0044e-06,  ..., -1.7108e-06,
         -2.6003e-06, -3.1814e-06],
        [-5.0515e-06, -8.4564e-06,  2.9784e-06,  ..., -2.2519e-06,
         -3.5167e-06, -3.6173e-06]], device='cuda:0')
Loss: 1.3356143236160278


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   14,  560,  ...,   12, 8593,  266],
        [   0, 3544,  417,  ...,  380,  381, 3794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2372e-04, -6.3692e-04, -1.8280e-04,  ..., -1.7384e-04,
         -2.5462e-03, -5.9191e-04],
        [-5.9903e-06, -1.0408e-05,  3.8370e-06,  ..., -3.1982e-06,
         -3.7048e-06, -4.6901e-06],
        [-1.1452e-05, -2.0593e-05,  7.1190e-06,  ..., -5.6885e-06,
         -7.0855e-06, -9.4399e-06],
        [-5.1074e-06, -9.1046e-06,  3.4738e-06,  ..., -2.4596e-06,
         -3.1181e-06, -3.8929e-06],
        [-6.3628e-06, -1.0915e-05,  3.3788e-06,  ..., -3.2205e-06,
         -4.1872e-06, -4.3921e-06]], device='cuda:0')
Loss: 1.3416661024093628


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,  2587,    27,  ...,   259,  2462,  1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0264e-03, -2.3574e-04, -2.3098e-04,  ...,  2.0166e-04,
         -3.0190e-03, -4.9201e-04],
        [-7.2122e-06, -1.2189e-05,  4.2021e-06,  ..., -4.0028e-06,
         -4.0084e-06, -5.3532e-06],
        [-1.3761e-05, -2.3931e-05,  7.6927e-06,  ..., -7.1563e-06,
         -7.6033e-06, -1.0654e-05],
        [-6.2026e-06, -1.0692e-05,  3.8389e-06,  ..., -3.0855e-06,
         -3.3621e-06, -4.4815e-06],
        [-7.8827e-06, -1.3046e-05,  3.6824e-06,  ..., -4.1630e-06,
         -4.5747e-06, -5.0776e-06]], device='cuda:0')
Loss: 1.3349192142486572


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,  266, 1371, 4811],
        [   0,  278, 6046,  ..., 1671,  199,  395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8105e-03, -4.3805e-04, -6.1679e-04,  ...,  6.0762e-04,
         -3.7943e-03, -2.8954e-04],
        [-8.5756e-06, -1.4186e-05,  4.9882e-06,  ..., -4.5244e-06,
         -4.8839e-06, -6.5230e-06],
        [-1.6369e-05, -2.7895e-05,  9.2499e-06,  ..., -8.1249e-06,
         -9.3020e-06, -1.3128e-05],
        [-7.2084e-06, -1.2212e-05,  4.4685e-06,  ..., -3.4468e-06,
         -3.9991e-06, -5.3979e-06],
        [-9.2760e-06, -1.5058e-05,  4.3754e-06,  ..., -4.7106e-06,
         -5.4948e-06, -6.1803e-06]], device='cuda:0')
Loss: 1.3320298194885254


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[    0, 41010,  6737,  ...,   963,   409,   382],
        [    0,   380,   333,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9135e-03, -1.0262e-03, -1.2266e-03,  ...,  1.3180e-03,
         -4.3903e-03, -2.5079e-04],
        [-9.6187e-06, -1.5952e-05,  5.5432e-06,  ..., -5.0869e-06,
         -5.6326e-06, -7.2531e-06],
        [-1.8425e-05, -3.1412e-05,  1.0278e-05,  ..., -9.1381e-06,
         -1.0710e-05, -1.4633e-05],
        [-8.1360e-06, -1.3791e-05,  4.9938e-06,  ..., -3.8957e-06,
         -4.6175e-06, -6.0648e-06],
        [-1.0371e-05, -1.6809e-05,  4.8075e-06,  ..., -5.2359e-06,
         -6.2846e-06, -6.8508e-06]], device='cuda:0')
Loss: 1.3351662158966064
Graident accumulation at epoch 0, step 31, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4929e-04, -8.3268e-04,  5.9591e-04,  ...,  4.4362e-04,
         -1.4703e-03, -2.3943e-04],
        [-8.7438e-07, -2.2184e-06,  1.3576e-06,  ..., -6.2785e-07,
         -2.0429e-06, -1.8997e-06],
        [-1.6119e-06, -4.7933e-06,  2.8243e-06,  ..., -9.8103e-07,
         -4.7134e-06, -4.5273e-06],
        [-6.0017e-07, -2.1675e-06,  1.5786e-06,  ..., -3.5567e-07,
         -2.3084e-06, -2.1633e-06],
        [-1.0414e-06, -2.3628e-06,  1.0065e-06,  ..., -5.8390e-07,
         -2.1450e-06, -1.6681e-06]], device='cuda:0')
optimizer state dict: tensor([[2.8391e-08, 2.7795e-08, 2.9032e-08,  ..., 1.0731e-08, 8.4461e-08,
         7.3877e-09],
        [1.0087e-13, 2.8666e-13, 6.4401e-14,  ..., 2.9565e-14, 1.4163e-13,
         1.2182e-13],
        [3.8842e-13, 1.1887e-12, 2.7307e-13,  ..., 1.0312e-13, 7.8073e-13,
         6.8839e-13],
        [8.2163e-14, 2.3759e-13, 8.4176e-14,  ..., 2.0704e-14, 1.9262e-13,
         1.6002e-13],
        [1.1510e-13, 3.1802e-13, 3.8512e-14,  ..., 3.0367e-14, 1.5502e-13,
         9.5384e-14]], device='cuda:0')
optimizer state dict: 4.0
lr: [1.0178117048346057e-05, 1.0178117048346057e-05]
scheduler_last_epoch: 4


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,     9,   300,  ...,  6838,   328, 18619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7005e-04, -3.7711e-04,  1.7922e-04,  ...,  2.5884e-04,
          1.4648e-04,  1.6331e-05],
        [-2.3693e-06, -3.6061e-06,  5.8487e-07,  ..., -1.2740e-06,
         -5.2154e-07, -9.4250e-07],
        [-3.2634e-06, -5.0068e-06,  7.7114e-07,  ..., -1.7211e-06,
         -7.4133e-07, -1.3560e-06],
        [-1.4380e-06, -2.2203e-06,  3.6508e-07,  ..., -7.5623e-07,
         -3.1479e-07, -5.8860e-07],
        [-2.6971e-06, -4.0233e-06,  4.8056e-07,  ..., -1.4305e-06,
         -5.8115e-07, -9.5367e-07]], device='cuda:0')
Loss: 1.3013445138931274


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0,  7994,    12,  ..., 13800,   278,   795],
        [    0,    25,    26,  ...,     9,   287,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3308e-03, -9.2919e-05, -2.0045e-04,  ..., -4.2472e-04,
          1.8812e-04, -1.9125e-04],
        [-4.4554e-06, -6.8545e-06,  1.2256e-06,  ..., -2.8387e-06,
         -1.0356e-06, -1.7248e-06],
        [-6.0648e-06, -9.4473e-06,  1.5832e-06,  ..., -3.7625e-06,
         -1.4231e-06, -2.4363e-06],
        [-2.8163e-06, -4.4107e-06,  8.1584e-07,  ..., -1.7546e-06,
         -6.5565e-07, -1.1288e-06],
        [-5.1707e-06, -7.8976e-06,  1.1250e-06,  ..., -3.2559e-06,
         -1.2554e-06, -1.7770e-06]], device='cuda:0')
Loss: 1.3158239126205444


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0,  689, 2149,  ..., 4263,   14,  292],
        [   0, 8754,   14,  ..., 6125,  394,  927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4990e-03, -3.7944e-04, -3.2583e-04,  ..., -3.9769e-04,
          2.1652e-04, -6.2080e-04],
        [-7.0632e-06, -1.0476e-05,  1.8701e-06,  ..., -4.1053e-06,
         -1.1171e-06, -2.9989e-06],
        [-9.9689e-06, -1.4961e-05,  2.5071e-06,  ..., -5.6177e-06,
         -1.5241e-06, -4.3884e-06],
        [-4.6492e-06, -6.9439e-06,  1.3001e-06,  ..., -2.5965e-06,
         -6.9290e-07, -2.0452e-06],
        [-8.1509e-06, -1.2010e-05,  1.7248e-06,  ..., -4.6417e-06,
         -1.3895e-06, -3.1404e-06]], device='cuda:0')
Loss: 1.305778980255127


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,   266,   554,  ..., 10679,  3790,   857],
        [    0,   659,   278,  ...,   769,  1728,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0918e-03, -4.4932e-06, -3.2583e-04,  ..., -3.5086e-04,
         -2.8128e-04, -6.7311e-04],
        [-9.7007e-06, -1.4141e-05,  2.4661e-06,  ..., -5.2378e-06,
         -1.5828e-06, -4.0196e-06],
        [-1.3813e-05, -2.0385e-05,  3.3453e-06,  ..., -7.2569e-06,
         -2.1574e-06, -5.9456e-06],
        [-6.2957e-06, -9.2685e-06,  1.6857e-06,  ..., -3.2522e-06,
         -9.7789e-07, -2.6859e-06],
        [-1.1116e-05, -1.6093e-05,  2.2613e-06,  ..., -5.9009e-06,
         -1.9744e-06, -4.2208e-06]], device='cuda:0')
Loss: 1.3038296699523926


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[   0,   17,  292,  ..., 8055,  365, 3125],
        [   0,   12, 2735,  ...,   12,  344, 1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0918e-03,  1.5234e-04, -2.3118e-04,  ..., -3.6479e-04,
         -2.3106e-04, -1.3473e-03],
        [-1.2442e-05, -1.7762e-05,  3.2336e-06,  ..., -7.3239e-06,
         -2.3241e-06, -5.0999e-06],
        [-1.7554e-05, -2.5392e-05,  4.3437e-06,  ..., -1.0073e-05,
         -3.2005e-06, -7.4282e-06],
        [-8.0094e-06, -1.1578e-05,  2.2072e-06,  ..., -4.5635e-06,
         -1.4324e-06, -3.3453e-06],
        [-1.4246e-05, -2.0117e-05,  3.0063e-06,  ..., -8.2254e-06,
         -2.8275e-06, -5.3234e-06]], device='cuda:0')
Loss: 1.3153663873672485


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,   342,  4014,  ...,   368,   408,  2105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5666e-03, -1.5280e-04, -5.6434e-04,  ..., -6.3249e-05,
         -5.2631e-04, -1.8109e-03],
        [-1.4737e-05, -2.1547e-05,  3.9823e-06,  ..., -8.4266e-06,
         -2.4084e-06, -5.7854e-06],
        [-2.1040e-05, -3.1233e-05,  5.4464e-06,  ..., -1.1720e-05,
         -3.3188e-06, -8.5160e-06],
        [-9.6112e-06, -1.4231e-05,  2.7623e-06,  ..., -5.2787e-06,
         -1.4629e-06, -3.8091e-06],
        [-1.7002e-05, -2.4647e-05,  3.7625e-06,  ..., -9.5218e-06,
         -2.9719e-06, -6.0461e-06]], device='cuda:0')
Loss: 1.3098199367523193


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  5024,  3846,  ...,  5880,  1377,    12],
        [    0, 31571,    13,  ...,   367,  2177,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8650e-03,  1.6610e-04, -3.4712e-04,  ..., -1.3600e-04,
         -1.2174e-03, -1.4067e-03],
        [-1.7196e-05, -2.5570e-05,  4.9099e-06,  ..., -9.7677e-06,
         -2.7306e-06, -6.8359e-06],
        [-2.4512e-05, -3.6925e-05,  6.7055e-06,  ..., -1.3553e-05,
         -3.7807e-06, -1.0066e-05],
        [-1.1288e-05, -1.7032e-05,  3.4329e-06,  ..., -6.1579e-06,
         -1.6808e-06, -4.5244e-06],
        [-1.9699e-05, -2.9027e-05,  4.6529e-06,  ..., -1.0990e-05,
         -3.3760e-06, -7.0967e-06]], device='cuda:0')
Loss: 1.3075282573699951


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0, 29073,   916,  ...,    12,   287,   850],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0280e-03,  1.3750e-04, -9.5960e-04,  ..., -4.2230e-04,
         -2.0969e-03, -1.5911e-03],
        [-1.9908e-05, -2.9594e-05,  5.8934e-06,  ..., -1.1206e-05,
         -3.8110e-06, -7.4245e-06],
        [-2.8476e-05, -4.2915e-05,  8.0839e-06,  ..., -1.5609e-05,
         -5.4124e-06, -1.1001e-05],
        [-1.2957e-05, -1.9550e-05,  4.0662e-06,  ..., -7.0073e-06,
         -2.3551e-06, -4.8969e-06],
        [-2.2620e-05, -3.3468e-05,  5.5954e-06,  ..., -1.2532e-05,
         -4.6054e-06, -7.6480e-06]], device='cuda:0')
Loss: 1.3024098873138428
Graident accumulation at epoch 0, step 39, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0222,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0672e-03, -7.3566e-04,  4.4036e-04,  ...,  3.5703e-04,
         -1.5330e-03, -3.7460e-04],
        [-2.7777e-06, -4.9559e-06,  1.8112e-06,  ..., -1.6856e-06,
         -2.2197e-06, -2.4522e-06],
        [-4.2983e-06, -8.6055e-06,  3.3503e-06,  ..., -2.4438e-06,
         -4.7833e-06, -5.1747e-06],
        [-1.8358e-06, -3.9058e-06,  1.8274e-06,  ..., -1.0208e-06,
         -2.3131e-06, -2.4367e-06],
        [-3.1992e-06, -5.4733e-06,  1.4654e-06,  ..., -1.7787e-06,
         -2.3911e-06, -2.2661e-06]], device='cuda:0')
optimizer state dict: tensor([[3.7532e-08, 2.7786e-08, 2.9924e-08,  ..., 1.0899e-08, 8.8774e-08,
         9.9120e-09],
        [4.9710e-13, 1.1622e-12, 9.9069e-14,  ..., 1.5510e-13, 1.5602e-13,
         1.7682e-13],
        [1.1989e-12, 3.0292e-12, 3.3815e-13,  ..., 3.4666e-13, 8.0924e-13,
         8.0872e-13],
        [2.4995e-13, 6.1957e-13, 1.0063e-13,  ..., 6.9785e-14, 1.9798e-13,
         1.8384e-13],
        [6.2664e-13, 1.4378e-12, 6.9782e-14,  ..., 1.8738e-13, 1.7607e-13,
         1.5378e-13]], device='cuda:0')
optimizer state dict: 5.0
lr: [1.2722646310432571e-05, 1.2722646310432571e-05]
scheduler_last_epoch: 5


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0,  401, 3704,  ...,   14, 1062, 1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0413e-04,  5.7657e-04,  1.2707e-04,  ..., -1.9771e-04,
         -7.5889e-05, -2.2969e-04],
        [-3.9339e-06, -4.9472e-06,  4.1910e-07,  ..., -1.9521e-06,
         -1.2107e-07, -6.2957e-07],
        [-4.2617e-06, -5.3644e-06,  4.4890e-07,  ..., -2.0713e-06,
         -1.2759e-07, -6.7428e-07],
        [-2.2799e-06, -2.8610e-06,  2.6450e-07,  ..., -1.1027e-06,
         -6.5193e-08, -3.3714e-07],
        [-5.6922e-06, -7.0333e-06,  5.0664e-07,  ..., -2.8461e-06,
         -2.2165e-07, -7.5251e-07]], device='cuda:0')
Loss: 1.2790526151657104


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0, 38495, 36253,  ..., 11006,  5699,    19],
        [    0,     5,  7523,  ...,   199,  8871,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9696e-04,  5.5073e-04, -1.3862e-04,  ..., -1.9771e-04,
          5.8935e-05, -9.2613e-07],
        [-8.2552e-06, -1.0580e-05,  8.7172e-07,  ..., -4.2766e-06,
         -7.0222e-07, -1.7025e-06],
        [-8.8513e-06, -1.1355e-05,  8.8103e-07,  ..., -4.5300e-06,
         -7.1991e-07, -1.8068e-06],
        [-4.5449e-06, -5.7966e-06,  5.2527e-07,  ..., -2.2873e-06,
         -3.7067e-07, -8.9593e-07],
        [-1.1653e-05, -1.4782e-05,  9.7603e-07,  ..., -6.1095e-06,
         -1.0487e-06, -2.1309e-06]], device='cuda:0')
Loss: 1.2893766164779663


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  5353,  5234,  ...,  1458,    14,  7157],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9322e-04,  9.5483e-04,  1.7768e-04,  ...,  2.1327e-04,
         -1.9842e-04, -2.8742e-04],
        [-1.2159e-05, -1.5497e-05,  1.4342e-06,  ..., -6.5565e-06,
         -1.0934e-06, -2.7083e-06],
        [-1.3322e-05, -1.7017e-05,  1.5181e-06,  ..., -7.1377e-06,
         -1.1930e-06, -3.0063e-06],
        [-6.6310e-06, -8.4341e-06,  8.5309e-07,  ..., -3.5018e-06,
         -5.7649e-07, -1.4286e-06],
        [-1.6689e-05, -2.1130e-05,  1.6242e-06,  ..., -9.1195e-06,
         -1.6373e-06, -3.3602e-06]], device='cuda:0')
Loss: 1.2851920127868652


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,  1901, 11083,  ...,   360,  6055,  2374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6341e-04,  6.2147e-04, -2.1682e-04,  ...,  2.2378e-05,
         -1.3402e-04, -4.3165e-04],
        [-1.5974e-05, -2.0564e-05,  2.2538e-06,  ..., -8.5682e-06,
         -1.9129e-06, -3.4794e-06],
        [ 1.0259e-04,  1.0303e-04,  1.9333e-05,  ...,  7.5866e-05,
          7.6884e-06, -4.2619e-05],
        [-8.7321e-06, -1.1265e-05,  1.3001e-06,  ..., -4.5896e-06,
         -1.0030e-06, -1.8347e-06],
        [-2.2143e-05, -2.8312e-05,  2.6375e-06,  ..., -1.1936e-05,
         -2.8592e-06, -4.3437e-06]], device='cuda:0')
Loss: 1.277353048324585


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  1387,   369,  ..., 15722,    14,  8157],
        [    0,  4100,    12,  ...,    13,  4710,  1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3857e-03,  3.2882e-04, -6.0079e-04,  ..., -8.1072e-04,
          2.7744e-05, -1.1646e-03],
        [-1.9655e-05, -2.5392e-05,  2.8498e-06,  ..., -1.0595e-05,
         -2.6692e-06, -4.6492e-06],
        [ 9.8660e-05,  9.7813e-05,  1.9940e-05,  ...,  7.3720e-05,
          6.8801e-06, -4.3893e-05],
        [-1.0803e-05, -1.3992e-05,  1.6615e-06,  ..., -5.7146e-06,
         -1.4277e-06, -2.4717e-06],
        [-2.7120e-05, -3.4809e-05,  3.2969e-06,  ..., -1.4618e-05,
         -3.9022e-06, -5.8189e-06]], device='cuda:0')
Loss: 1.2921456098556519


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[   0,  515,  266,  ...,   18, 3770, 1345],
        [   0, 6945, 2360,  ...,   30,  413,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2822e-03,  3.9307e-04, -6.3346e-04,  ..., -5.3327e-04,
         -5.1168e-04, -1.2839e-03],
        [-2.3827e-05, -3.0607e-05,  3.3006e-06,  ..., -1.2815e-05,
         -3.4478e-06, -6.0201e-06],
        [ 9.4368e-05,  9.2419e-05,  2.0404e-05,  ...,  7.1515e-05,
          6.0679e-06, -4.5338e-05],
        [-1.2890e-05, -1.6615e-05,  1.9222e-06,  ..., -6.7800e-06,
         -1.8058e-06, -3.1646e-06],
        [-3.2961e-05, -4.2111e-05,  3.8333e-06,  ..., -1.7717e-05,
         -5.0571e-06, -7.6368e-06]], device='cuda:0')
Loss: 1.2784932851791382


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[   0,  292,  685,  ...,  278, 3281,  298],
        [   0, 2530,  634,  ...,   15, 8808,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8776e-03,  5.4310e-04, -1.0105e-03,  ...,  8.8677e-05,
         -9.5718e-04, -9.1999e-04],
        [-2.7820e-05, -3.5584e-05,  3.4962e-06,  ..., -1.4991e-05,
         -3.6918e-06, -6.7279e-06],
        [ 9.0047e-05,  8.7025e-05,  2.0606e-05,  ...,  6.9190e-05,
          5.7662e-06, -4.6113e-05],
        [-1.5005e-05, -1.9267e-05,  2.0424e-06,  ..., -7.9200e-06,
         -1.9241e-06, -3.5353e-06],
        [-3.8564e-05, -4.9084e-05,  3.9954e-06,  ..., -2.0772e-05,
         -5.4855e-06, -8.5272e-06]], device='cuda:0')
Loss: 1.2850874662399292


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[   0, 4834,  278,  ...,   13, 8382,  669],
        [   0,  298,  894,  ..., 7605, 3220,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8899e-03,  6.4339e-04, -1.7657e-03,  ...,  4.3832e-04,
         -9.1477e-04, -5.3093e-04],
        [-3.1963e-05, -4.0770e-05,  4.0103e-06,  ..., -1.7032e-05,
         -4.2580e-06, -7.4357e-06],
        [ 8.5815e-05,  8.1720e-05,  2.1091e-05,  ...,  6.7134e-05,
          5.2037e-06, -4.6877e-05],
        [-1.7241e-05, -2.2039e-05,  2.3236e-06,  ..., -8.9929e-06,
         -2.2240e-06, -3.8967e-06],
        [-4.4614e-05, -5.6565e-05,  4.6100e-06,  ..., -2.3723e-05,
         -6.3945e-06, -9.4883e-06]], device='cuda:0')
Loss: 1.2751591205596924
Graident accumulation at epoch 0, step 47, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1494e-03, -5.9776e-04,  2.1975e-04,  ...,  3.6516e-04,
         -1.4712e-03, -3.9023e-04],
        [-5.6963e-06, -8.5373e-06,  2.0311e-06,  ..., -3.2203e-06,
         -2.4235e-06, -2.9506e-06],
        [ 4.7131e-06,  4.2699e-07,  5.1243e-06,  ...,  4.5139e-06,
         -3.7846e-06, -9.3449e-06],
        [-3.3763e-06, -5.7191e-06,  1.8770e-06,  ..., -1.8180e-06,
         -2.3041e-06, -2.5827e-06],
        [-7.3407e-06, -1.0582e-05,  1.7799e-06,  ..., -3.9731e-06,
         -2.7914e-06, -2.9883e-06]], device='cuda:0')
optimizer state dict: tensor([[4.1066e-08, 2.8172e-08, 3.3012e-08,  ..., 1.1080e-08, 8.9522e-08,
         1.0184e-08],
        [1.5182e-12, 2.8232e-12, 1.1505e-13,  ..., 4.4504e-13, 1.7399e-13,
         2.3194e-13],
        [8.5619e-12, 9.7043e-12, 7.8263e-13,  ..., 4.8532e-12, 8.3551e-13,
         3.0054e-12],
        [5.4694e-13, 1.1047e-12, 1.0592e-13,  ..., 1.5059e-13, 2.0273e-13,
         1.9884e-13],
        [2.6164e-12, 4.6360e-12, 9.0964e-14,  ..., 7.4996e-13, 2.1678e-13,
         2.4366e-13]], device='cuda:0')
optimizer state dict: 6.0
lr: [1.5267175572519086e-05, 1.5267175572519086e-05]
scheduler_last_epoch: 6


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,   680,   401,  ...,  2872,   292, 23535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5653e-04, -2.5848e-04, -4.4173e-04,  ..., -2.3710e-04,
          2.2132e-04, -5.2105e-04],
        [-4.9174e-06, -5.4240e-06,  3.4645e-07,  ..., -2.8759e-06,
         -8.5309e-07, -9.5367e-07],
        [-4.3809e-06, -4.9174e-06,  2.9989e-07,  ..., -2.5779e-06,
         -7.6368e-07, -8.7544e-07],
        [-2.5630e-06, -2.8610e-06,  1.9558e-07,  ..., -1.4976e-06,
         -4.5449e-07, -4.8801e-07],
        [-7.8678e-06, -8.7023e-06,  4.6752e-07,  ..., -4.6194e-06,
         -1.4156e-06, -1.4603e-06]], device='cuda:0')
Loss: 1.2634332180023193


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[   0, 6132,  300,  ...,   37,  271,  259],
        [   0,   25,    5,  ..., 3935,   14,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1421e-04, -4.0013e-04, -5.6866e-04,  ..., -4.3177e-04,
          1.4547e-04, -9.3817e-04],
        [-1.0520e-05, -1.0997e-05,  3.4497e-07,  ..., -6.3330e-06,
         -1.4789e-06, -2.1309e-06],
        [-9.1493e-06, -9.6560e-06,  3.0384e-07,  ..., -5.5134e-06,
         -1.3150e-06, -1.8813e-06],
        [-5.4836e-06, -5.7966e-06,  2.1211e-07,  ..., -3.3006e-06,
         -7.7300e-07, -1.0990e-06],
        [-1.6630e-05, -1.7464e-05,  3.9674e-07,  ..., -1.0103e-05,
         -2.4140e-06, -3.2410e-06]], device='cuda:0')
Loss: 1.2700655460357666


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  413,   16,  ...,  493, 2104,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3055e-04, -6.1019e-04, -6.2826e-04,  ..., -3.5716e-04,
          3.6507e-04, -1.2088e-03],
        [-1.5587e-05, -1.6361e-05,  5.0422e-07,  ..., -9.5367e-06,
         -1.9372e-06, -3.1367e-06],
        [-1.3649e-05, -1.4454e-05,  4.3050e-07,  ..., -8.3447e-06,
         -1.7155e-06, -2.7940e-06],
        [-8.1956e-06, -8.6874e-06,  3.1781e-07,  ..., -5.0217e-06,
         -1.0105e-06, -1.6354e-06],
        [-2.5332e-05, -2.6703e-05,  5.7090e-07,  ..., -1.5557e-05,
         -3.2634e-06, -4.9174e-06]], device='cuda:0')
Loss: 1.2677818536758423


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0406e-05, -9.3668e-04, -1.1237e-03,  ..., -3.3358e-04,
          7.1667e-04, -1.5146e-03],
        [-2.0653e-05, -2.1845e-05,  7.3612e-07,  ..., -1.2532e-05,
         -2.2855e-06, -4.2990e-06],
        [-1.8030e-05, -1.9222e-05,  6.0652e-07,  ..., -1.0937e-05,
         -2.0191e-06, -3.8221e-06],
        [-1.0923e-05, -1.1653e-05,  4.5937e-07,  ..., -6.6385e-06,
         -1.2079e-06, -2.2613e-06],
        [-3.3319e-05, -3.5405e-05,  8.3540e-07,  ..., -2.0325e-05,
         -3.8520e-06, -6.6981e-06]], device='cuda:0')
Loss: 1.2679698467254639


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[   0, 1145,   35,  ...,  300, 5192,  518],
        [   0,   19,  669,  ...,   14, 4053,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6562e-05, -4.9409e-04, -1.1890e-03,  ..., -4.5521e-04,
          8.1123e-04, -1.6303e-03],
        [-2.5451e-05, -2.7031e-05,  6.6069e-07,  ..., -1.5497e-05,
         -2.6952e-06, -5.3495e-06],
        [-2.2143e-05, -2.3663e-05,  5.5321e-07,  ..., -1.3471e-05,
         -2.3805e-06, -4.7423e-06],
        [-1.3396e-05, -1.4320e-05,  4.4709e-07,  ..., -8.1584e-06,
         -1.4156e-06, -2.7940e-06],
        [-4.1544e-05, -4.4227e-05,  6.2399e-07,  ..., -2.5421e-05,
         -4.5858e-06, -8.4341e-06]], device='cuda:0')
Loss: 1.2737042903900146


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   437, 38603,  ..., 37253, 10432,   278],
        [    0,   266,  1194,  ...,  2267,    15,  1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5744e-04, -3.5795e-04, -1.2472e-03,  ..., -4.7537e-04,
          1.1529e-03, -1.2223e-03],
        [-3.0607e-05, -3.2693e-05,  5.6802e-07,  ..., -1.8373e-05,
         -2.9840e-06, -6.2399e-06],
        [-2.6762e-05, -2.8759e-05,  4.5542e-07,  ..., -1.6049e-05,
         -2.6338e-06, -5.5507e-06],
        [-1.6063e-05, -1.7270e-05,  4.0286e-07,  ..., -9.6336e-06,
         -1.5572e-06, -3.2410e-06],
        [-4.9651e-05, -5.3167e-05,  3.7253e-07,  ..., -2.9951e-05,
         -5.0850e-06, -9.7156e-06]], device='cuda:0')
Loss: 1.2723544836044312


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[    0,  6010,   829,  ...,   668,  1784,   587],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7173e-04, -2.7326e-04, -1.5252e-03,  ..., -2.7720e-04,
          1.0380e-03, -1.4117e-03],
        [-3.5912e-05, -3.8266e-05,  6.2949e-07,  ..., -2.1547e-05,
         -3.5763e-06, -7.2904e-06],
        [-3.1322e-05, -3.3557e-05,  4.9383e-07,  ..., -1.8746e-05,
         -3.1479e-06, -6.4597e-06],
        [-1.8775e-05, -2.0117e-05,  4.4593e-07,  ..., -1.1228e-05,
         -1.8384e-06, -3.7663e-06],
        [-5.7995e-05, -6.1929e-05,  4.1211e-07,  ..., -3.4928e-05,
         -6.0610e-06, -1.1303e-05]], device='cuda:0')
Loss: 1.270372986793518


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[   0,  328,  957,  ...,  298,  275, 8570],
        [   0,  741,  300,  ...,   83, 7111,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2740e-03, -1.8047e-04, -1.6725e-03,  ..., -1.3202e-04,
          1.0920e-03, -1.1901e-03],
        [-4.0919e-05, -4.4286e-05,  7.0446e-07,  ..., -2.4587e-05,
         -3.9563e-06, -8.2143e-06],
        [-3.5882e-05, -3.9041e-05,  5.4599e-07,  ..., -2.1502e-05,
         -3.4906e-06, -7.3016e-06],
        [-2.1324e-05, -2.3186e-05,  4.9156e-07,  ..., -1.2770e-05,
         -2.0210e-06, -4.2263e-06],
        [-6.5982e-05, -7.1466e-05,  4.4331e-07,  ..., -3.9786e-05,
         -6.7055e-06, -1.2688e-05]], device='cuda:0')
Loss: 1.2612338066101074
Graident accumulation at epoch 0, step 55, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1619e-03, -5.5603e-04,  3.0529e-05,  ...,  3.1544e-04,
         -1.2149e-03, -4.7022e-04],
        [-9.2185e-06, -1.2112e-05,  1.8984e-06,  ..., -5.3569e-06,
         -2.5768e-06, -3.4769e-06],
        [ 6.5355e-07, -3.5198e-06,  4.6665e-06,  ...,  1.9123e-06,
         -3.7552e-06, -9.1406e-06],
        [-5.1710e-06, -7.4658e-06,  1.7385e-06,  ..., -2.9133e-06,
         -2.2758e-06, -2.7470e-06],
        [-1.3205e-05, -1.6671e-05,  1.6462e-06,  ..., -7.5544e-06,
         -3.1828e-06, -3.9583e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2648e-08, 2.8177e-08, 3.5776e-08,  ..., 1.1087e-08, 9.0625e-08,
         1.1590e-08],
        [3.1910e-12, 4.7816e-12, 1.1543e-13,  ..., 1.0491e-12, 1.8947e-13,
         2.9918e-13],
        [9.8409e-12, 1.1219e-11, 7.8214e-13,  ..., 5.3107e-12, 8.4686e-13,
         3.0557e-12],
        [1.0011e-12, 1.6412e-12, 1.0606e-13,  ..., 3.1352e-13, 2.0661e-13,
         2.1650e-13],
        [6.9675e-12, 9.7387e-12, 9.1070e-14,  ..., 2.3321e-12, 2.6153e-13,
         4.0441e-13]], device='cuda:0')
optimizer state dict: 7.0
lr: [1.78117048346056e-05, 1.78117048346056e-05]
scheduler_last_epoch: 7


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,    14,    23,  ...,   278,   266,  1462],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4085e-04,  1.2752e-04, -5.4260e-07,  ...,  3.0539e-05,
          7.2305e-06,  2.9546e-04],
        [-5.9009e-06, -5.5730e-06, -2.7753e-07,  ..., -3.8445e-06,
         -9.6858e-07, -1.2368e-06],
        [-4.5598e-06, -4.3213e-06, -2.3190e-07,  ..., -2.9802e-06,
         -7.5251e-07, -9.6112e-07],
        [-3.1292e-06, -2.9653e-06, -1.4435e-07,  ..., -2.0415e-06,
         -5.1409e-07, -6.4448e-07],
        [-1.0192e-05, -9.6560e-06, -5.6252e-07,  ..., -6.6757e-06,
         -1.7062e-06, -2.1011e-06]], device='cuda:0')
Loss: 1.2467494010925293


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   271,  2862,  ...,   287,  5699,    18],
        [    0,  9458,   278,  ...,    15,  5251, 27858]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6754e-04,  1.7927e-04,  1.1910e-04,  ...,  1.5945e-04,
          1.3666e-04,  3.9993e-04],
        [-1.1623e-05, -1.0997e-05, -4.6846e-07,  ..., -7.4804e-06,
         -1.8403e-06, -2.3320e-06],
        [-9.1493e-06, -8.6725e-06, -3.9581e-07,  ..., -5.9009e-06,
         -1.4752e-06, -1.8552e-06],
        [-6.3777e-06, -6.0499e-06, -2.4727e-07,  ..., -4.0978e-06,
         -1.0170e-06, -1.2591e-06],
        [-2.0266e-05, -1.9193e-05, -9.7230e-07,  ..., -1.3113e-05,
         -3.3230e-06, -3.9563e-06]], device='cuda:0')
Loss: 1.2520456314086914


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[   0,   13, 5005,  ...,  654,  344,  259],
        [   0, 1238,   14,  ...,  368,  940,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4538e-04,  1.8089e-04, -5.8327e-06,  ...,  3.9326e-04,
          2.4775e-05,  2.1546e-04],
        [-1.7166e-05, -1.6451e-05, -8.4098e-07,  ..., -1.1086e-05,
         -2.6636e-06, -3.3677e-06],
        [-1.3679e-05, -1.3173e-05, -7.0129e-07,  ..., -8.8662e-06,
         -2.1532e-06, -2.7046e-06],
        [-9.4026e-06, -9.0450e-06, -4.3726e-07,  ..., -6.0648e-06,
         -1.4640e-06, -1.8179e-06],
        [-3.0398e-05, -2.9206e-05, -1.7323e-06,  ..., -1.9729e-05,
         -4.8429e-06, -5.7891e-06]], device='cuda:0')
Loss: 1.248404860496521


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0,  344, 3693,  ..., 1782, 3679,  292],
        [   0, 2027,  365,  ...,  368, 1782,  394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4702e-05,  2.2710e-04, -2.8984e-05,  ...,  7.8692e-04,
          1.9217e-04,  4.0183e-04],
        [-2.2560e-05, -2.1309e-05, -1.0766e-06,  ..., -1.4424e-05,
         -3.4757e-06, -4.4852e-06],
        [-1.8150e-05, -1.7196e-05, -9.0618e-07,  ..., -1.1623e-05,
         -2.8387e-06, -3.6247e-06],
        [-1.2562e-05, -1.1891e-05, -5.6857e-07,  ..., -8.0019e-06,
         -1.9521e-06, -2.4624e-06],
        [-4.0650e-05, -3.8445e-05, -2.2464e-06,  ..., -2.6077e-05,
         -6.5044e-06, -7.8157e-06]], device='cuda:0')
Loss: 1.2457482814788818


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0,   266,   283,  ...,   271, 48829,   580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8045e-04,  2.8562e-04,  7.1124e-05,  ...,  7.5604e-04,
          2.6225e-04,  3.3864e-04],
        [-2.8431e-05, -2.6494e-05, -1.3784e-06,  ..., -1.8075e-05,
         -4.3847e-06, -5.4613e-06],
        [-2.2769e-05, -2.1309e-05, -1.1446e-06,  ..., -1.4484e-05,
         -3.5614e-06, -4.3996e-06],
        [-1.5870e-05, -1.4812e-05, -7.2597e-07,  ..., -1.0043e-05,
         -2.4624e-06, -2.9989e-06],
        [-5.1022e-05, -4.7624e-05, -2.8275e-06,  ..., -3.2485e-05,
         -8.1360e-06, -9.4473e-06]], device='cuda:0')
Loss: 1.2538642883300781


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0, 21930,    12,  ...,  2849,   863,   578],
        [    0,   278, 14971,  ...,  2341,   266,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0759e-05,  3.1102e-04,  9.9973e-05,  ...,  5.8930e-04,
          1.7759e-04,  1.3759e-04],
        [-3.4064e-05, -3.1710e-05, -1.5628e-06,  ..., -2.1607e-05,
         -5.2638e-06, -6.3665e-06],
        [-2.7210e-05, -2.5421e-05, -1.2899e-06,  ..., -1.7285e-05,
         -4.2580e-06, -5.1185e-06],
        [-1.9103e-05, -1.7822e-05, -8.1630e-07,  ..., -1.2085e-05,
         -2.9616e-06, -3.5129e-06],
        [-6.1452e-05, -5.7280e-05, -3.2373e-06,  ..., -3.9071e-05,
         -9.8199e-06, -1.1057e-05]], device='cuda:0')
Loss: 1.2508753538131714


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[   0,  741, 2985,  ...,  199,  769,  278],
        [   0,  278, 4452,  ...,   14,   18, 3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9090e-04,  4.1728e-05,  3.5908e-04,  ...,  5.5293e-04,
          1.8698e-05,  7.2425e-05],
        [-3.9846e-05, -3.7044e-05, -1.9185e-06,  ..., -2.5198e-05,
         -5.9791e-06, -7.5810e-06],
        [-3.1918e-05, -2.9743e-05, -1.5823e-06,  ..., -2.0191e-05,
         -4.8466e-06, -6.1095e-06],
        [-2.2456e-05, -2.0906e-05, -1.0091e-06,  ..., -1.4156e-05,
         -3.3788e-06, -4.2096e-06],
        [-7.2181e-05, -6.7174e-05, -3.9823e-06,  ..., -4.5687e-05,
         -1.1168e-05, -1.3217e-05]], device='cuda:0')
Loss: 1.235849380493164


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0, 3119,  278,  ...,  352,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6431e-04, -8.3537e-05,  5.2022e-04,  ...,  4.0979e-04,
         -8.0363e-05,  1.9687e-04],
        [-4.5329e-05, -4.2260e-05, -2.1411e-06,  ..., -2.8804e-05,
         -6.7428e-06, -8.5793e-06],
        [-3.6269e-05, -3.3915e-05, -1.7677e-06,  ..., -2.3067e-05,
         -5.4576e-06, -6.9104e-06],
        [-2.5630e-05, -2.3931e-05, -1.1199e-06,  ..., -1.6242e-05,
         -3.8259e-06, -4.7833e-06],
        [-8.1956e-05, -7.6473e-05, -4.4592e-06,  ..., -5.2154e-05,
         -1.2577e-05, -1.4916e-05]], device='cuda:0')
Loss: 1.2525867223739624
Graident accumulation at epoch 0, step 63, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0821e-03, -5.0878e-04,  7.9498e-05,  ...,  3.2488e-04,
         -1.1014e-03, -4.0352e-04],
        [-1.2830e-05, -1.5127e-05,  1.4945e-06,  ..., -7.7016e-06,
         -2.9934e-06, -3.9872e-06],
        [-3.0387e-06, -6.5593e-06,  4.0231e-06,  ..., -5.8563e-07,
         -3.9254e-06, -8.9176e-06],
        [-7.2169e-06, -9.1123e-06,  1.4526e-06,  ..., -4.2462e-06,
         -2.4308e-06, -2.9507e-06],
        [-2.0080e-05, -2.2651e-05,  1.0357e-06,  ..., -1.2014e-05,
         -4.1222e-06, -5.0541e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2738e-08, 2.8155e-08, 3.6011e-08,  ..., 1.1243e-08, 9.0541e-08,
         1.1617e-08],
        [5.2426e-12, 6.5627e-12, 1.1990e-13,  ..., 1.8777e-12, 2.3474e-13,
         3.7248e-13],
        [1.1147e-11, 1.2358e-11, 7.8449e-13,  ..., 5.8375e-12, 8.7580e-13,
         3.1004e-12],
        [1.6570e-12, 2.2122e-12, 1.0721e-13,  ..., 5.7701e-13, 2.2104e-13,
         2.3917e-13],
        [1.3677e-11, 1.5577e-11, 1.1086e-13,  ..., 5.0499e-12, 4.1944e-13,
         6.2649e-13]], device='cuda:0')
optimizer state dict: 8.0
lr: [1.9999985024557586e-05, 1.9999985024557586e-05]
scheduler_last_epoch: 8


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,    13,    19,  ..., 22111,  2489,    14],
        [    0,  1854,   292,  ...,   328,  1360,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6356e-05,  2.7158e-04,  8.5619e-05,  ...,  8.6750e-05,
         -5.2903e-05, -1.0299e-04],
        [-5.4538e-06, -4.6492e-06, -3.4459e-07,  ..., -4.0829e-06,
         -8.7917e-07, -8.7917e-07],
        [-4.0531e-06, -3.4571e-06, -2.6450e-07,  ..., -3.0398e-06,
         -6.5565e-07, -6.5193e-07],
        [-3.2783e-06, -2.7865e-06, -1.9278e-07,  ..., -2.4438e-06,
         -5.2154e-07, -5.2154e-07],
        [-1.0371e-05, -8.7619e-06, -7.0781e-07,  ..., -7.6890e-06,
         -1.6764e-06, -1.6019e-06]], device='cuda:0')
Loss: 1.248663067817688


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  1253,   287,  ...,  2988,    14,   417],
        [    0,   461,  1169,  ..., 14135,  2771,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0331e-05,  1.0098e-04,  1.5885e-04,  ...,  9.2330e-05,
         -2.1633e-04, -2.6794e-04],
        [-1.1027e-05, -9.2983e-06, -6.6496e-07,  ..., -7.8976e-06,
         -2.0117e-06, -1.8477e-06],
        [-8.3148e-06, -7.0333e-06, -5.0850e-07,  ..., -5.9605e-06,
         -1.5199e-06, -1.4007e-06],
        [-6.9141e-06, -5.8264e-06, -3.8743e-07,  ..., -4.9323e-06,
         -1.2554e-06, -1.1474e-06],
        [-2.0981e-05, -1.7643e-05, -1.3597e-06,  ..., -1.4961e-05,
         -3.8669e-06, -3.4049e-06]], device='cuda:0')
Loss: 1.2286880016326904


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[   0, 1356,  634,  ..., 6604,  634,   14],
        [   0,  759, 1184,  ...,  472,  346,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0953e-04,  3.8932e-06,  1.6977e-04,  ...,  1.4342e-04,
         -4.0828e-04, -3.8058e-04],
        [-1.6332e-05, -1.3739e-05, -8.8010e-07,  ..., -1.1474e-05,
         -2.9355e-06, -2.7157e-06],
        [-1.2487e-05, -1.0520e-05, -6.8080e-07,  ..., -8.7768e-06,
         -2.2464e-06, -2.0824e-06],
        [-1.0476e-05, -8.8066e-06, -5.2340e-07,  ..., -7.3314e-06,
         -1.8738e-06, -1.7248e-06],
        [-3.1173e-05, -2.6166e-05, -1.8161e-06,  ..., -2.1845e-05,
         -5.6624e-06, -5.0217e-06]], device='cuda:0')
Loss: 1.2218878269195557


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,    14,  3741,  ...,   278, 12472, 10257],
        [    0,  1295,  1178,  ...,  4808,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9734e-05,  1.4760e-04,  2.4762e-05,  ...,  9.4009e-05,
         -4.4760e-04, -3.4675e-04],
        [-2.1666e-05, -1.8150e-05, -1.0887e-06,  ..., -1.5318e-05,
         -3.7141e-06, -3.5167e-06],
        [-1.6570e-05, -1.3888e-05, -8.4471e-07,  ..., -1.1712e-05,
         -2.8461e-06, -2.6897e-06],
        [-1.4022e-05, -1.1742e-05, -6.5472e-07,  ..., -9.8795e-06,
         -2.3916e-06, -2.2389e-06],
        [-4.1485e-05, -3.4690e-05, -2.2668e-06,  ..., -2.9266e-05,
         -7.2047e-06, -6.5044e-06]], device='cuda:0')
Loss: 1.242640733718872


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0,  493,  221,  ...,  259,  726, 2786],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5154e-04,  6.0933e-05,  4.6923e-05,  ...,  1.1278e-04,
         -3.2423e-04, -2.6252e-04],
        [-2.7090e-05, -2.2858e-05, -1.3756e-06,  ..., -1.9252e-05,
         -4.6119e-06, -4.6119e-06],
        [-2.0713e-05, -1.7479e-05, -1.0664e-06,  ..., -1.4722e-05,
         -3.5278e-06, -3.5353e-06],
        [-1.7479e-05, -1.4737e-05, -8.2795e-07,  ..., -1.2383e-05,
         -2.9579e-06, -2.9355e-06],
        [-5.1618e-05, -4.3452e-05, -2.8517e-06,  ..., -3.6627e-05,
         -8.8811e-06, -8.5011e-06]], device='cuda:0')
Loss: 1.245609998703003


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[   0, 9010,   17,  ..., 3813, 1147,  199],
        [   0, 2793,  271,  ...,  374,  298,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0193e-04,  2.5924e-04,  6.0095e-05,  ..., -6.1282e-05,
         -9.0143e-05, -1.6429e-04],
        [-3.2693e-05, -2.7657e-05, -1.5907e-06,  ..., -2.3305e-05,
         -5.4576e-06, -5.5730e-06],
        [-2.4825e-05, -2.0996e-05, -1.2275e-06,  ..., -1.7703e-05,
         -4.1537e-06, -4.2357e-06],
        [-2.1055e-05, -1.7792e-05, -9.5461e-07,  ..., -1.4976e-05,
         -3.4980e-06, -3.5353e-06],
        [-6.2108e-05, -5.2392e-05, -3.3043e-06,  ..., -4.4227e-05,
         -1.0483e-05, -1.0230e-05]], device='cuda:0')
Loss: 1.250514268875122


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[   0, 1171, 2926,  ...,  259, 4288,  654],
        [   0,  266, 1441,  ..., 1817, 1589,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4753e-04,  3.2157e-05, -2.5481e-05,  ...,  1.1583e-04,
         -2.3271e-04, -1.8311e-04],
        [-3.7998e-05, -3.2306e-05, -1.9297e-06,  ..., -2.7105e-05,
         -6.3963e-06, -6.5416e-06],
        [-2.8729e-05, -2.4438e-05, -1.4827e-06,  ..., -2.0504e-05,
         -4.8503e-06, -4.9509e-06],
        [-2.4498e-05, -2.0802e-05, -1.1697e-06,  ..., -1.7434e-05,
         -4.1015e-06, -4.1574e-06],
        [-7.2181e-05, -6.1214e-05, -3.9935e-06,  ..., -5.1439e-05,
         -1.2286e-05, -1.2025e-05]], device='cuda:0')
Loss: 1.2396451234817505


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[    0,   741,   266,  ...,   271,  5166,   596],
        [    0, 14349,   278,  ...,   365,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9584e-04, -4.5106e-05, -1.0049e-04,  ...,  1.3625e-04,
         -2.8137e-04, -6.2372e-05],
        [-4.3571e-05, -3.7104e-05, -2.2016e-06,  ..., -3.1069e-05,
         -7.1637e-06, -7.4953e-06],
        [-3.2872e-05, -2.8029e-05, -1.6866e-06,  ..., -2.3454e-05,
         -5.4277e-06, -5.6550e-06],
        [-2.8133e-05, -2.3931e-05, -1.3411e-06,  ..., -2.0012e-05,
         -4.6007e-06, -4.7684e-06],
        [-8.2433e-05, -7.0035e-05, -4.5523e-06,  ..., -5.8711e-05,
         -1.3724e-05, -1.3709e-05]], device='cuda:0')
Loss: 1.2226446866989136
Graident accumulation at epoch 0, step 71, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0235e-03, -4.6241e-04,  6.1500e-05,  ...,  3.0601e-04,
         -1.0194e-03, -3.6940e-04],
        [-1.5904e-05, -1.7325e-05,  1.1249e-06,  ..., -1.0038e-05,
         -3.4104e-06, -4.3380e-06],
        [-6.0221e-06, -8.7063e-06,  3.4521e-06,  ..., -2.8725e-06,
         -4.0757e-06, -8.5913e-06],
        [-9.3086e-06, -1.0594e-05,  1.1733e-06,  ..., -5.8228e-06,
         -2.6478e-06, -3.1324e-06],
        [-2.6315e-05, -2.7389e-05,  4.7689e-07,  ..., -1.6684e-05,
         -5.0824e-06, -5.9196e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2941e-08, 2.8129e-08, 3.5985e-08,  ..., 1.1251e-08, 9.0529e-08,
         1.1610e-08],
        [7.1358e-12, 7.9328e-12, 1.2463e-13,  ..., 2.8411e-12, 2.8583e-13,
         4.2829e-13],
        [1.2216e-11, 1.3131e-11, 7.8655e-13,  ..., 6.3818e-12, 9.0438e-13,
         3.1292e-12],
        [2.4468e-12, 2.7827e-12, 1.0890e-13,  ..., 9.7693e-13, 2.4198e-13,
         2.6166e-13],
        [2.0459e-11, 2.0466e-11, 1.3148e-13,  ..., 8.4917e-12, 6.0737e-13,
         8.1380e-13]], device='cuda:0')
optimizer state dict: 9.0
lr: [1.999900705266644e-05, 1.999900705266644e-05]
scheduler_last_epoch: 9


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0, 45050,   342,  ...,  3729,   287, 27888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7672e-05, -1.2870e-04, -2.2138e-04,  ...,  1.9908e-04,
          8.8755e-05,  1.6581e-05],
        [-4.7684e-06, -3.4720e-06,  5.2154e-08,  ..., -3.7849e-06,
         -5.6997e-07, -8.6799e-07],
        [-3.5465e-06, -2.5779e-06,  3.3295e-08,  ..., -2.8014e-06,
         -4.2841e-07, -6.3702e-07],
        [-3.7700e-06, -2.7269e-06,  4.5169e-08,  ..., -2.9802e-06,
         -4.5449e-07, -6.7055e-07],
        [-9.7156e-06, -7.0333e-06,  8.1956e-08,  ..., -7.6890e-06,
         -1.1921e-06, -1.7136e-06]], device='cuda:0')
Loss: 1.2366336584091187


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[   0, 6668,  565,  ...,  360,  259, 8166],
        [   0,   17,  292,  ..., 2269, 3887,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8935e-05, -3.8070e-05, -2.1354e-04,  ...,  2.6919e-04,
          1.5769e-04, -1.4675e-04],
        [-9.6858e-06, -6.9737e-06, -3.6322e-08,  ..., -7.5698e-06,
         -1.0915e-06, -1.5348e-06],
        [-7.2122e-06, -5.2005e-06, -3.3760e-08,  ..., -5.6177e-06,
         -8.1770e-07, -1.1325e-06],
        [-7.6145e-06, -5.4687e-06, -1.7229e-08,  ..., -5.9456e-06,
         -8.6240e-07, -1.1846e-06],
        [-1.9610e-05, -1.4067e-05, -1.2852e-07,  ..., -1.5289e-05,
         -2.2575e-06, -3.0026e-06]], device='cuda:0')
Loss: 1.2341158390045166


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0,   266,   824,  ...,  1799,   287,  6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7636e-04, -6.9506e-05, -2.2484e-04,  ...,  2.5093e-04,
          2.8939e-05, -1.1043e-04],
        [-1.4633e-05, -1.0490e-05, -8.1724e-08,  ..., -1.1221e-05,
         -1.8440e-06, -2.3358e-06],
        [-1.0908e-05, -7.8231e-06, -7.4040e-08,  ..., -8.3447e-06,
         -1.3877e-06, -1.7360e-06],
        [-1.1429e-05, -8.1807e-06, -4.6333e-08,  ..., -8.7619e-06,
         -1.4435e-06, -1.7993e-06],
        [-2.9445e-05, -2.1040e-05, -2.6450e-07,  ..., -2.2560e-05,
         -3.7700e-06, -4.5598e-06]], device='cuda:0')
Loss: 1.2281776666641235


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   409, 35049,  ...,    12,   699,   394],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0213e-04, -4.2872e-05, -2.5284e-04,  ...,  2.1058e-04,
         -2.8808e-05, -2.7660e-04],
        [-1.9550e-05, -1.4111e-05, -2.1770e-07,  ..., -1.5214e-05,
         -2.5295e-06, -3.2149e-06],
        [-1.4424e-05, -1.0416e-05, -1.7649e-07,  ..., -1.1206e-05,
         -1.8720e-06, -2.3618e-06],
        [-1.5140e-05, -1.0923e-05, -1.4459e-07,  ..., -1.1787e-05,
         -1.9539e-06, -2.4550e-06],
        [-3.9279e-05, -2.8282e-05, -5.7556e-07,  ..., -3.0547e-05,
         -5.1409e-06, -6.2883e-06]], device='cuda:0')
Loss: 1.2318204641342163


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,    13,  4467,  ...,  2390, 47857,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6891e-05,  2.8657e-05, -3.0233e-04,  ...,  4.4438e-05,
          9.6714e-05, -3.0814e-04],
        [-2.4647e-05, -1.7762e-05, -1.8824e-07,  ..., -1.8984e-05,
         -3.3826e-06, -4.1835e-06],
        [-1.8209e-05, -1.3128e-05, -1.5274e-07,  ..., -1.4007e-05,
         -2.5053e-06, -3.0883e-06],
        [-1.9044e-05, -1.3709e-05, -1.1222e-07,  ..., -1.4678e-05,
         -2.6096e-06, -3.1926e-06],
        [-4.9531e-05, -3.5584e-05, -5.3784e-07,  ..., -3.8087e-05,
         -6.8769e-06, -8.1956e-06]], device='cuda:0')
Loss: 1.2396296262741089


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[    0,  1497, 16170,  ...,  1888,  2350,   578],
        [    0,  2366,  5036,  ...,  1477,   352,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3089e-05, -5.2875e-05, -3.4770e-04,  ...,  2.2829e-04,
         -7.2182e-05, -3.5280e-04],
        [-2.9624e-05, -2.1249e-05, -2.4296e-07,  ..., -2.2799e-05,
         -4.1872e-06, -5.1260e-06],
        [-2.1920e-05, -1.5736e-05, -2.0396e-07,  ..., -1.6868e-05,
         -3.1088e-06, -3.7886e-06],
        [-2.3007e-05, -1.6481e-05, -1.5786e-07,  ..., -1.7717e-05,
         -3.2540e-06, -3.9376e-06],
        [-5.9605e-05, -4.2677e-05, -6.9989e-07,  ..., -4.5836e-05,
         -8.5384e-06, -1.0073e-05]], device='cuda:0')
Loss: 1.2327312231063843


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[   0, 4014,   88,  ..., 1103,   14, 1771],
        [   0, 1086,   14,  ...,  963,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5687e-05, -3.7996e-05, -3.6915e-04,  ...,  2.7318e-04,
         -5.4318e-05, -9.9269e-05],
        [-3.4660e-05, -2.5019e-05, -1.0233e-07,  ..., -2.6762e-05,
         -4.8801e-06, -5.9828e-06],
        [ 8.1697e-05,  5.2069e-05,  1.1001e-05,  ...,  1.0216e-04,
         -1.4078e-05, -4.4503e-07],
        [-2.6882e-05, -1.9386e-05, -4.3306e-08,  ..., -2.0757e-05,
         -3.7830e-06, -4.5933e-06],
        [-6.9499e-05, -5.0098e-05, -4.5775e-07,  ..., -5.3644e-05,
         -9.9242e-06, -1.1735e-05]], device='cuda:0')
Loss: 1.223325252532959


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   437,  1119,  ..., 32831,    83,   623],
        [    0, 13856,   278,  ...,    14,    69,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4427e-04, -1.3726e-04, -3.5012e-04,  ...,  3.3771e-04,
         -6.0221e-05, -1.0088e-04],
        [-3.9816e-05, -2.8789e-05, -1.1211e-07,  ..., -3.0696e-05,
         -5.5991e-06, -6.8881e-06],
        [ 7.7987e-05,  4.9357e-05,  1.0994e-05,  ...,  9.9311e-05,
         -1.4596e-05, -1.0970e-06],
        [-3.0726e-05, -2.2188e-05, -4.1990e-08,  ..., -2.3678e-05,
         -4.3083e-06, -5.2601e-06],
        [-7.9572e-05, -5.7429e-05, -4.9872e-07,  ..., -6.1333e-05,
         -1.1340e-05, -1.3463e-05]], device='cuda:0')
Loss: 1.230455994606018
Graident accumulation at epoch 0, step 79, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0035,  0.0220, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0338],
        [ 0.0334, -0.0096,  0.0407,  ...,  0.0223,  0.0063, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.0673e-04, -4.2990e-04,  2.0338e-05,  ...,  3.0918e-04,
         -9.2349e-04, -3.4255e-04],
        [-1.8295e-05, -1.8471e-05,  1.0012e-06,  ..., -1.2104e-05,
         -3.6293e-06, -4.5930e-06],
        [ 2.3788e-06, -2.9000e-06,  4.2063e-06,  ...,  7.3458e-06,
         -5.1277e-06, -7.8419e-06],
        [-1.1450e-05, -1.1754e-05,  1.0517e-06,  ..., -7.6083e-06,
         -2.8139e-06, -3.3452e-06],
        [-3.1641e-05, -3.0393e-05,  3.7933e-07,  ..., -2.1149e-05,
         -5.7081e-06, -6.6739e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2919e-08, 2.8120e-08, 3.6071e-08,  ..., 1.1354e-08, 9.0442e-08,
         1.1608e-08],
        [8.7140e-12, 8.7537e-12, 1.2452e-13,  ..., 3.7806e-12, 3.1689e-13,
         4.7531e-13],
        [1.8286e-11, 1.5554e-11, 9.0663e-13,  ..., 1.6238e-11, 1.1165e-12,
         3.1273e-12],
        [3.3885e-12, 3.2722e-12, 1.0879e-13,  ..., 1.5366e-12, 2.6030e-13,
         2.8907e-13],
        [2.6770e-11, 2.3744e-11, 1.3159e-13,  ..., 1.2245e-11, 7.3535e-13,
         9.9425e-13]], device='cuda:0')
optimizer state dict: 10.0
lr: [1.9996501145215088e-05, 1.9996501145215088e-05]
scheduler_last_epoch: 10


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  894,   16,  ...,  892,  300,  722]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4182e-05,  7.2969e-05,  4.8510e-05,  ..., -7.0122e-05,
          2.3306e-05,  5.6965e-05],
        [-4.7982e-06, -3.0845e-06,  3.5949e-07,  ..., -3.7849e-06,
         -6.1840e-07, -9.1642e-07],
        [-3.4273e-06, -2.2054e-06,  2.5518e-07,  ..., -2.7120e-06,
         -4.4331e-07, -6.5565e-07],
        [-4.1425e-06, -2.6673e-06,  3.1665e-07,  ..., -3.2634e-06,
         -5.3272e-07, -7.8976e-07],
        [-9.7752e-06, -6.2883e-06,  7.1153e-07,  ..., -7.6890e-06,
         -1.2666e-06, -1.8403e-06]], device='cuda:0')
Loss: 1.2216856479644775


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[    0,   360,  2374,  ...,   221,   474,   357],
        [    0,  3001,  3325,  ..., 16332,  2661,  1200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0527e-05,  9.7422e-05,  1.4640e-05,  ..., -1.5485e-04,
          9.6302e-05,  1.2612e-04],
        [-9.4771e-06, -6.0499e-06,  6.3516e-07,  ..., -7.5251e-06,
         -1.1809e-06, -1.7360e-06],
        [-6.9439e-06, -4.4256e-06,  4.6380e-07,  ..., -5.5134e-06,
         -8.6799e-07, -1.2703e-06],
        [-8.2850e-06, -5.2750e-06,  5.6624e-07,  ..., -6.5565e-06,
         -1.0319e-06, -1.5087e-06],
        [-1.9491e-05, -1.2457e-05,  1.2666e-06,  ..., -1.5438e-05,
         -2.4587e-06, -3.5241e-06]], device='cuda:0')
Loss: 1.2353297472000122


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1314e-04,  2.0729e-04, -1.4075e-06,  ..., -1.2578e-04,
          8.2362e-05,  1.7026e-04],
        [-1.4007e-05, -8.9258e-06,  9.7789e-07,  ..., -1.1161e-05,
         -1.7844e-06, -2.6338e-06],
        [-1.0282e-05, -6.5416e-06,  7.1712e-07,  ..., -8.1807e-06,
         -1.3094e-06, -1.9334e-06],
        [-1.2338e-05, -7.8529e-06,  8.8103e-07,  ..., -9.8050e-06,
         -1.5721e-06, -2.3022e-06],
        [-2.8729e-05, -1.8328e-05,  1.9483e-06,  ..., -2.2858e-05,
         -3.6955e-06, -5.3197e-06]], device='cuda:0')
Loss: 1.2365825176239014


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,    13,  3105,  ...,   496,    14,   879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2520e-04,  1.1123e-04, -1.2074e-05,  ..., -1.6140e-04,
          3.0964e-05,  2.0730e-04],
        [-1.8716e-05, -1.2010e-05,  1.2890e-06,  ..., -1.4976e-05,
         -2.3954e-06, -3.6396e-06],
        [-1.3843e-05, -8.8811e-06,  9.5181e-07,  ..., -1.1072e-05,
         -1.7714e-06, -2.6934e-06],
        [-1.6510e-05, -1.0565e-05,  1.1697e-06,  ..., -1.3173e-05,
         -2.1048e-06, -3.1814e-06],
        [-3.8445e-05, -2.4647e-05,  2.5705e-06,  ..., -3.0726e-05,
         -4.9695e-06, -7.3612e-06]], device='cuda:0')
Loss: 1.2262126207351685


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0, 31550,    14,  ...,   278,   266,  4901],
        [    0,   278,  1099,  ...,   496,    14,   879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8285e-04,  1.0651e-04, -1.2313e-04,  ..., -2.2568e-04,
          1.5386e-04,  1.7081e-04],
        [-2.3276e-05, -1.5020e-05,  1.5404e-06,  ..., -1.8716e-05,
         -2.9020e-06, -4.4852e-06],
        [-1.7211e-05, -1.1101e-05,  1.1362e-06,  ..., -1.3843e-05,
         -2.1514e-06, -3.3230e-06],
        [-2.0564e-05, -1.3232e-05,  1.3998e-06,  ..., -1.6496e-05,
         -2.5537e-06, -3.9302e-06],
        [-4.7922e-05, -3.0875e-05,  3.0696e-06,  ..., -3.8475e-05,
         -6.0350e-06, -9.0823e-06]], device='cuda:0')
Loss: 1.2265921831130981


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 1415,  300,  ..., 1497, 5715, 4555],
        [   0,  368,  266,  ...,  591,  767,  824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0078e-04,  2.3803e-04, -3.4522e-05,  ..., -2.4752e-04,
          4.6858e-05,  6.7764e-05],
        [-2.8074e-05, -1.8030e-05,  1.8682e-06,  ..., -2.2501e-05,
         -3.4720e-06, -5.4985e-06],
        [-2.0787e-05, -1.3337e-05,  1.3765e-06,  ..., -1.6659e-05,
         -2.5760e-06, -4.0717e-06],
        [-2.4676e-05, -1.5810e-05,  1.6885e-06,  ..., -1.9744e-05,
         -3.0380e-06, -4.7907e-06],
        [-5.7638e-05, -3.6925e-05,  3.7029e-06,  ..., -4.6104e-05,
         -7.1824e-06, -1.1079e-05]], device='cuda:0')
Loss: 1.218894362449646


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,   300, 13523,  ..., 42438,   786,  1416],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2746e-04,  3.2278e-04, -1.8153e-04,  ..., -3.4250e-04,
          2.6003e-04,  3.5529e-05],
        [-3.2693e-05, -2.0981e-05,  2.1141e-06,  ..., -2.6256e-05,
         -4.0494e-06, -6.4597e-06],
        [-2.4229e-05, -1.5527e-05,  1.5581e-06,  ..., -1.9446e-05,
         -3.0119e-06, -4.7870e-06],
        [-2.8759e-05, -1.8418e-05,  1.9101e-06,  ..., -2.3052e-05,
         -3.5558e-06, -5.6289e-06],
        [-6.7174e-05, -4.3005e-05,  4.1835e-06,  ..., -5.3853e-05,
         -8.3968e-06, -1.3016e-05]], device='cuda:0')
Loss: 1.230224847793579


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[   0, 3101,  275,  ..., 2345,  609,  287],
        [   0,  591, 1545,  ...,   71,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7423e-04,  3.4821e-04, -2.2002e-04,  ..., -3.5644e-04,
          2.7096e-04, -1.9521e-05],
        [-3.7402e-05, -2.3946e-05,  2.3320e-06,  ..., -2.9966e-05,
         -4.6678e-06, -7.3500e-06],
        [-2.7731e-05, -1.7732e-05,  1.7192e-06,  ..., -2.2218e-05,
         -3.4720e-06, -5.4501e-06],
        [-3.2872e-05, -2.1011e-05,  2.1104e-06,  ..., -2.6286e-05,
         -4.0960e-06, -6.4000e-06],
        [-7.6771e-05, -4.9084e-05,  4.6100e-06,  ..., -6.1452e-05,
         -9.6709e-06, -1.4812e-05]], device='cuda:0')
Loss: 1.2184165716171265
Graident accumulation at epoch 0, step 87, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0046, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0338],
        [ 0.0335, -0.0096,  0.0407,  ...,  0.0223,  0.0063, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4348e-04, -3.5209e-04, -3.6979e-06,  ...,  2.4262e-04,
         -8.0405e-04, -3.1025e-04],
        [-2.0206e-05, -1.9019e-05,  1.1343e-06,  ..., -1.3890e-05,
         -3.7332e-06, -4.8687e-06],
        [-6.3220e-07, -4.3832e-06,  3.9576e-06,  ...,  4.3895e-06,
         -4.9621e-06, -7.6027e-06],
        [-1.3592e-05, -1.2679e-05,  1.1576e-06,  ..., -9.4760e-06,
         -2.9421e-06, -3.6507e-06],
        [-3.6154e-05, -3.2263e-05,  8.0240e-07,  ..., -2.5179e-05,
         -6.1044e-06, -7.4877e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2951e-08, 2.8213e-08, 3.6084e-08,  ..., 1.1469e-08, 9.0425e-08,
         1.1597e-08],
        [1.0104e-11, 9.3184e-12, 1.2983e-13,  ..., 4.6747e-12, 3.3836e-13,
         5.2885e-13],
        [1.9036e-11, 1.5853e-11, 9.0868e-13,  ..., 1.6715e-11, 1.1275e-12,
         3.1539e-12],
        [4.4656e-12, 3.7104e-12, 1.1314e-13,  ..., 2.2260e-12, 2.7682e-13,
         3.2974e-13],
        [3.2637e-11, 2.6130e-11, 1.5271e-13,  ..., 1.6009e-11, 8.2814e-13,
         1.2126e-12]], device='cuda:0')
optimizer state dict: 11.0
lr: [1.999246768512805e-05, 1.999246768512805e-05]
scheduler_last_epoch: 11


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,  5625,  2558,  ...,   680,   292,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2323e-05,  2.5563e-05,  2.2798e-06,  ..., -9.1002e-05,
          2.3160e-05,  4.4088e-06],
        [-4.5598e-06, -2.5481e-06,  5.7369e-07,  ..., -3.6210e-06,
         -4.8429e-07, -1.0580e-06],
        [-3.2485e-06, -1.8179e-06,  4.0978e-07,  ..., -2.5779e-06,
         -3.4459e-07, -7.5623e-07],
        [-4.3511e-06, -2.4289e-06,  5.5879e-07,  ..., -3.4571e-06,
         -4.5635e-07, -1.0058e-06],
        [-9.0003e-06, -5.0068e-06,  1.1176e-06,  ..., -7.1526e-06,
         -9.6112e-07, -2.0713e-06]], device='cuda:0')
Loss: 1.2240058183670044


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,  1477,  5648,  ...,  4391,  1722,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8353e-05, -3.6532e-07,  1.4268e-05,  ..., -1.4103e-04,
          6.2668e-05, -3.5783e-05],
        [-9.0003e-06, -5.0813e-06,  1.1884e-06,  ..., -7.2420e-06,
         -9.8720e-07, -2.0079e-06],
        [-6.4522e-06, -3.6508e-06,  8.5495e-07,  ..., -5.2005e-06,
         -7.1153e-07, -1.4380e-06],
        [-8.6725e-06, -4.9025e-06,  1.1660e-06,  ..., -6.9886e-06,
         -9.4809e-07, -1.9260e-06],
        [-1.7881e-05, -1.0043e-05,  2.3320e-06,  ..., -1.4365e-05,
         -1.9744e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.2217415571212769


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,  3217, 16714,  ...,   462,   221,   474],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0156e-04,  3.1188e-05,  6.3935e-05,  ..., -1.2019e-04,
         -2.0335e-05, -2.1750e-04],
        [-1.3351e-05, -7.5549e-06,  1.7434e-06,  ..., -1.0729e-05,
         -1.4398e-06, -2.9989e-06],
        [-9.6112e-06, -5.4613e-06,  1.2610e-06,  ..., -7.7486e-06,
         -1.0412e-06, -2.1607e-06],
        [-1.2904e-05, -7.3165e-06,  1.7136e-06,  ..., -1.0371e-05,
         -1.3895e-06, -2.8796e-06],
        [-2.6643e-05, -1.5020e-05,  3.4422e-06,  ..., -2.1368e-05,
         -2.8871e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.2152314186096191


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   334,   287,  ...,  1348,  6139,   342],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6165e-04,  1.4612e-04,  5.3521e-05,  ..., -2.3181e-04,
          8.6631e-05, -1.4780e-04],
        [-1.7852e-05, -1.0177e-05,  2.3507e-06,  ..., -1.4499e-05,
         -1.9427e-06, -4.0196e-06],
        [-1.2755e-05, -7.3016e-06,  1.6857e-06,  ..., -1.0401e-05,
         -1.3933e-06, -2.8759e-06],
        [-1.7256e-05, -9.8646e-06,  2.3060e-06,  ..., -1.4022e-05,
         -1.8738e-06, -3.8631e-06],
        [-3.5524e-05, -2.0236e-05,  4.6343e-06,  ..., -2.8849e-05,
         -3.8855e-06, -7.9125e-06]], device='cuda:0')
Loss: 1.223623514175415


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,  221,  380,  560],
        [   0,  298,  374,  ...,  298,  413,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5115e-04,  3.5560e-04,  8.7286e-05,  ..., -2.6316e-04,
          1.6743e-04, -1.5255e-04],
        [-2.2382e-05, -1.2726e-05,  2.8834e-06,  ..., -1.8120e-05,
         -2.3823e-06, -4.8839e-06],
        [-1.5974e-05, -9.1046e-06,  2.0619e-06,  ..., -1.2964e-05,
         -1.7062e-06, -3.4869e-06],
        [-2.1547e-05, -1.2264e-05,  2.8163e-06,  ..., -1.7449e-05,
         -2.2836e-06, -4.6715e-06],
        [-4.4584e-05, -2.5302e-05,  5.6848e-06,  ..., -3.6091e-05,
         -4.7721e-06, -9.6187e-06]], device='cuda:0')
Loss: 1.210854172706604


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,   269,    12,  ..., 45645,    14,   298],
        [    0,  1526,   422,  ..., 22454,   409, 31482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8851e-05,  4.9827e-04,  8.6096e-06,  ..., -2.7654e-04,
          1.6743e-04, -1.1619e-04],
        [-2.6643e-05, -1.5169e-05,  3.4384e-06,  ..., -2.1785e-05,
         -2.9858e-06, -5.7891e-06],
        [-1.8969e-05, -1.0826e-05,  2.4531e-06,  ..., -1.5542e-05,
         -2.1290e-06, -4.1239e-06],
        [-2.5630e-05, -1.4603e-05,  3.3490e-06,  ..., -2.0951e-05,
         -2.8573e-06, -5.5321e-06],
        [-5.2989e-05, -3.0130e-05,  6.7577e-06,  ..., -4.3333e-05,
         -5.9716e-06, -1.1384e-05]], device='cuda:0')
Loss: 1.2201310396194458


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,    71,    14,  ...,  1770,   391, 39516],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0932e-05,  5.2692e-04,  3.8067e-05,  ..., -3.3550e-04,
          2.4691e-04, -4.1172e-05],
        [-3.1173e-05, -1.7732e-05,  4.0084e-06,  ..., -2.5451e-05,
         -3.4254e-06, -6.7651e-06],
        [-2.2188e-05, -1.2644e-05,  2.8536e-06,  ..., -1.8120e-05,
         -2.4401e-06, -4.8093e-06],
        [-2.9981e-05, -1.7047e-05,  3.9004e-06,  ..., -2.4438e-05,
         -3.2727e-06, -6.4522e-06],
        [-6.1989e-05, -3.5226e-05,  7.8753e-06,  ..., -5.0575e-05,
         -6.8471e-06, -1.3292e-05]], device='cuda:0')
Loss: 1.2225180864334106


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[    0,   275,  2101,  ...,  1145,   590,  1619],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6942e-04,  6.9129e-04,  7.3442e-05,  ..., -4.7762e-04,
          2.3235e-04, -1.0180e-04],
        [-3.5733e-05, -2.0266e-05,  4.6641e-06,  ..., -2.9162e-05,
         -3.9209e-06, -7.8604e-06],
        [ 1.0084e-04,  6.0757e-05, -3.9276e-07,  ...,  5.6471e-05,
          4.7423e-06,  1.9999e-05],
        [-3.4302e-05, -1.9461e-05,  4.5374e-06,  ..., -2.7969e-05,
         -3.7458e-06, -7.4953e-06],
        [-7.0810e-05, -4.0114e-05,  9.1419e-06,  ..., -5.7757e-05,
         -7.8157e-06, -1.5393e-05]], device='cuda:0')
Loss: 1.214664101600647
Graident accumulation at epoch 0, step 95, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0407,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.4219e-04, -2.4775e-04,  4.0161e-06,  ...,  1.7060e-04,
         -7.0041e-04, -2.8940e-04],
        [-2.1758e-05, -1.9143e-05,  1.4872e-06,  ..., -1.5417e-05,
         -3.7519e-06, -5.1679e-06],
        [ 9.5146e-06,  2.1308e-06,  3.5226e-06,  ...,  9.5976e-06,
         -3.9917e-06, -4.8425e-06],
        [-1.5663e-05, -1.3357e-05,  1.4956e-06,  ..., -1.1325e-05,
         -3.0224e-06, -4.0351e-06],
        [-3.9620e-05, -3.3048e-05,  1.6363e-06,  ..., -2.8437e-05,
         -6.2755e-06, -8.2782e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2937e-08, 2.8663e-08, 3.6053e-08,  ..., 1.1686e-08, 9.0389e-08,
         1.1596e-08],
        [1.1371e-11, 9.7198e-12, 1.5146e-13,  ..., 5.5205e-12, 3.5340e-13,
         5.9011e-13],
        [2.9185e-11, 1.9529e-11, 9.0793e-13,  ..., 1.9888e-11, 1.1488e-12,
         3.5507e-12],
        [5.6378e-12, 4.0854e-12, 1.3361e-13,  ..., 3.0061e-12, 2.9057e-13,
         3.8559e-13],
        [3.7619e-11, 2.7713e-11, 2.3613e-13,  ..., 1.9329e-11, 8.8840e-13,
         1.4484e-12]], device='cuda:0')
optimizer state dict: 12.0
lr: [1.9986907288753243e-05, 1.9986907288753243e-05]
scheduler_last_epoch: 12


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[    0,  1943,   300,  ..., 43803,   368,  2400],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5255e-05, -4.3250e-06,  2.4926e-05,  ..., -1.8580e-05,
         -2.3767e-05, -1.2665e-05],
        [-4.2319e-06, -2.3544e-06,  7.7486e-07,  ..., -3.5465e-06,
         -5.0291e-07, -1.1101e-06],
        [-2.9951e-06, -1.6689e-06,  5.4762e-07,  ..., -2.5034e-06,
         -3.5577e-07, -7.8604e-07],
        [-4.3511e-06, -2.4140e-06,  8.0094e-07,  ..., -3.6210e-06,
         -5.1409e-07, -1.1325e-06],
        [-8.2850e-06, -4.5896e-06,  1.5050e-06,  ..., -6.9141e-06,
         -9.8348e-07, -2.1458e-06]], device='cuda:0')
Loss: 1.2147639989852905


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[    0,  7185,   328,  ...,  1427,  1477,  1061],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4961e-05,  8.4826e-05, -2.4425e-05,  ..., -8.2647e-05,
         -5.3771e-05, -1.8591e-05],
        [-8.3447e-06, -4.7088e-06,  1.5311e-06,  ..., -7.0930e-06,
         -8.9779e-07, -2.1160e-06],
        [-5.8413e-06, -3.3006e-06,  1.0654e-06,  ..., -4.9472e-06,
         -6.2771e-07, -1.4827e-06],
        [-8.5831e-06, -4.8280e-06,  1.5795e-06,  ..., -7.2569e-06,
         -9.1828e-07, -2.1607e-06],
        [-1.6332e-05, -9.2089e-06,  2.9653e-06,  ..., -1.3828e-05,
         -1.7583e-06, -4.0978e-06]], device='cuda:0')
Loss: 1.2179569005966187


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[   0, 1034,  287,  ..., 9677,   13, 6687],
        [   0, 5603, 6598,  ..., 1692, 1713,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0664e-05,  6.7303e-05, -1.1942e-04,  ..., -1.1830e-04,
         -5.5844e-05, -6.8558e-05],
        [-1.2547e-05, -7.0035e-06,  2.3022e-06,  ..., -1.0625e-05,
         -1.3728e-06, -3.0920e-06],
        [-8.8215e-06, -4.9323e-06,  1.6131e-06,  ..., -7.4655e-06,
         -9.6858e-07, -2.1793e-06],
        [-1.2934e-05, -7.2122e-06,  2.3842e-06,  ..., -1.0923e-05,
         -1.4063e-06, -3.1665e-06],
        [-2.4617e-05, -1.3769e-05,  4.4852e-06,  ..., -2.0832e-05,
         -2.7046e-06, -6.0201e-06]], device='cuda:0')
Loss: 1.2120574712753296


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[   0,   17, 3978,  ..., 3988,  598,   12],
        [   0,  278, 7914,  ..., 1194,  300, 4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2482e-05,  1.2890e-04, -1.3926e-04,  ..., -1.1202e-04,
          5.8291e-06, -1.1273e-04],
        [-1.6809e-05, -9.3579e-06,  3.0771e-06,  ..., -1.4260e-05,
         -1.7751e-06, -4.1574e-06],
        [-1.1757e-05, -6.5491e-06,  2.1458e-06,  ..., -9.9689e-06,
         -1.2442e-06, -2.9132e-06],
        [-1.7285e-05, -9.6112e-06,  3.1814e-06,  ..., -1.4633e-05,
         -1.8161e-06, -4.2468e-06],
        [-3.3081e-05, -1.8418e-05,  6.0126e-06,  ..., -2.8044e-05,
         -3.5055e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.2258104085922241


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[    0,   380,   333,  ...,  8127,   504,   679],
        [    0, 28590,    12,  ...,   342, 29639,  1693]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6147e-04,  1.3033e-04, -2.1917e-04,  ..., -1.6868e-04,
          1.3428e-05, -4.9442e-05],
        [-2.1040e-05, -1.1712e-05,  3.8445e-06,  ..., -1.7881e-05,
         -2.3078e-06, -5.2378e-06],
        [-1.4722e-05, -8.1882e-06,  2.6822e-06,  ..., -1.2502e-05,
         -1.6149e-06, -3.6657e-06],
        [-2.1607e-05, -1.2010e-05,  3.9749e-06,  ..., -1.8328e-05,
         -2.3525e-06, -5.3495e-06],
        [-4.1306e-05, -2.2978e-05,  7.5027e-06,  ..., -3.5077e-05,
         -4.5411e-06, -1.0177e-05]], device='cuda:0')
Loss: 1.1993809938430786


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[    0,   300,   344,  ...,    14,  5077,  2715],
        [    0,    12,   287,  ...,    15, 35654,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5850e-04,  1.0050e-04, -1.9047e-04,  ..., -1.7648e-04,
          1.7797e-04, -6.8675e-05],
        [-2.5213e-05, -1.4052e-05,  4.6156e-06,  ..., -2.1338e-05,
         -2.7623e-06, -6.3479e-06],
        [-1.7658e-05, -9.8273e-06,  3.2261e-06,  ..., -1.4931e-05,
         -1.9297e-06, -4.4443e-06],
        [-2.5928e-05, -1.4439e-05,  4.7833e-06,  ..., -2.1920e-05,
         -2.8182e-06, -6.4969e-06],
        [-4.9412e-05, -2.7508e-05,  8.9929e-06,  ..., -4.1783e-05,
         -5.4277e-06, -1.2308e-05]], device='cuda:0')
Loss: 1.2073496580123901


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,    14, 49601,  ...,    12,   298,   374],
        [    0,  6408,   391,  ...,   870,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0302e-04,  1.1119e-04, -1.6754e-04,  ..., -2.2838e-04,
          2.9927e-04, -5.0883e-05],
        [-2.9564e-05, -1.6510e-05,  5.3234e-06,  ..., -2.4989e-05,
         -3.2876e-06, -7.3910e-06],
        [-2.0698e-05, -1.1541e-05,  3.7178e-06,  ..., -1.7479e-05,
         -2.2985e-06, -5.1782e-06],
        [-3.0279e-05, -1.6898e-05,  5.4948e-06,  ..., -2.5570e-05,
         -3.3397e-06, -7.5474e-06],
        [-5.7936e-05, -3.2306e-05,  1.0356e-05,  ..., -4.8935e-05,
         -6.4634e-06, -1.4335e-05]], device='cuda:0')
Loss: 1.2015633583068848


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[   0, 1611,  266,  ...,  266, 2673, 6277],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2170e-04,  1.4642e-04, -2.1260e-04,  ..., -2.0580e-04,
          2.2364e-04, -1.0632e-04],
        [-3.3677e-05, -1.8805e-05,  6.1430e-06,  ..., -2.8476e-05,
         -3.7439e-06, -8.5384e-06],
        [ 9.7952e-05,  4.7913e-05, -2.5135e-05,  ...,  8.5014e-05,
         -1.0274e-05,  1.2897e-05],
        [-3.4541e-05, -1.9267e-05,  6.3479e-06,  ..., -2.9162e-05,
         -3.8054e-06, -8.7246e-06],
        [-6.5923e-05, -3.6746e-05,  1.1936e-05,  ..., -5.5701e-05,
         -7.3500e-06, -1.6540e-05]], device='cuda:0')
Loss: 1.21623694896698
Graident accumulation at epoch 0, step 103, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0406,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9014e-04, -2.0833e-04, -1.7645e-05,  ...,  1.3296e-04,
         -6.0800e-04, -2.7109e-04],
        [-2.2950e-05, -1.9109e-05,  1.9528e-06,  ..., -1.6723e-05,
         -3.7511e-06, -5.5049e-06],
        [ 1.8358e-05,  6.7091e-06,  6.5678e-07,  ...,  1.7139e-05,
         -4.6199e-06, -3.0685e-06],
        [-1.7551e-05, -1.3948e-05,  1.9808e-06,  ..., -1.3109e-05,
         -3.1007e-06, -4.5041e-06],
        [-4.2250e-05, -3.3418e-05,  2.6663e-06,  ..., -3.1163e-05,
         -6.3830e-06, -9.1044e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2943e-08, 2.8656e-08, 3.6062e-08,  ..., 1.1717e-08, 9.0349e-08,
         1.1595e-08],
        [1.2494e-11, 1.0064e-11, 1.8904e-13,  ..., 6.3258e-12, 3.6706e-13,
         6.6242e-13],
        [3.8751e-11, 2.1805e-11, 1.5388e-12,  ..., 2.7095e-11, 1.2532e-12,
         3.7135e-12],
        [6.8253e-12, 4.4526e-12, 1.7377e-13,  ..., 3.8535e-12, 3.0476e-13,
         4.6133e-13],
        [4.1927e-11, 2.9035e-11, 3.7836e-13,  ..., 2.2412e-11, 9.4153e-13,
         1.7205e-12]], device='cuda:0')
optimizer state dict: 13.0
lr: [1.9979820805767768e-05, 1.9979820805767768e-05]
scheduler_last_epoch: 13


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,  3634,  3444,  ...,   642,  2156,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2414e-05,  2.6295e-05, -2.9867e-05,  ...,  2.5833e-05,
          2.2353e-06,  0.0000e+00],
        [-4.0233e-06, -2.2203e-06,  9.2760e-07,  ..., -3.5167e-06,
         -4.8056e-07, -1.1474e-06],
        [-2.7567e-06, -1.5125e-06,  6.3330e-07,  ..., -2.3991e-06,
         -3.2969e-07, -7.8231e-07],
        [-4.3511e-06, -2.3991e-06,  1.0058e-06,  ..., -3.7849e-06,
         -5.1782e-07, -1.2368e-06],
        [-7.8082e-06, -4.2915e-06,  1.7881e-06,  ..., -6.7949e-06,
         -9.3877e-07, -2.2054e-06]], device='cuda:0')
Loss: 1.225634217262268


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[   0, 7333,  342,  ...,   13, 1818, 6183],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4430e-05, -2.5576e-05,  1.3842e-05,  ..., -8.7279e-05,
          5.2773e-05,  3.8437e-07],
        [-8.1062e-06, -4.4405e-06,  1.8291e-06,  ..., -7.0035e-06,
         -8.8289e-07, -2.2054e-06],
        [-5.5283e-06, -3.0249e-06,  1.2442e-06,  ..., -4.7684e-06,
         -6.0350e-07, -1.4976e-06],
        [-8.6427e-06, -4.7386e-06,  1.9670e-06,  ..., -7.4655e-06,
         -9.3877e-07, -2.3469e-06],
        [-1.5616e-05, -8.5533e-06,  3.5018e-06,  ..., -1.3471e-05,
         -1.7136e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.203037977218628


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0, 2923,  391,  ...,   14, 5424,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1721e-05, -3.7625e-05, -1.4895e-05,  ..., -9.2946e-05,
          3.8441e-05,  2.1793e-05],
        [-1.2338e-05, -6.6757e-06,  2.7455e-06,  ..., -1.0550e-05,
         -1.3784e-06, -3.2857e-06],
        [-8.3745e-06, -4.5374e-06,  1.8626e-06,  ..., -7.1675e-06,
         -9.4064e-07, -2.2277e-06],
        [-1.3053e-05, -7.0781e-06,  2.9355e-06,  ..., -1.1176e-05,
         -1.4566e-06, -3.4720e-06],
        [-2.3484e-05, -1.2726e-05,  5.1931e-06,  ..., -2.0057e-05,
         -2.6412e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.2011780738830566


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[    0,    14,  3080,  ...,   910,   266,  5275],
        [    0, 11054,    12,  ...,   560,   199,   677]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7684e-05,  1.2908e-06,  1.8088e-05,  ..., -1.8905e-04,
          9.9911e-05, -2.2750e-05],
        [-1.6391e-05, -8.8513e-06,  3.6135e-06,  ..., -1.4022e-05,
         -1.8515e-06, -4.4629e-06],
        [-1.1176e-05, -6.0424e-06,  2.4624e-06,  ..., -9.5665e-06,
         -1.2685e-06, -3.0473e-06],
        [-1.7375e-05, -9.4026e-06,  3.8706e-06,  ..., -1.4886e-05,
         -1.9595e-06, -4.7311e-06],
        [-3.1471e-05, -1.7017e-05,  6.8992e-06,  ..., -2.6911e-05,
         -3.5763e-06, -8.5086e-06]], device='cuda:0')
Loss: 1.2019983530044556


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0, 10084,    12,  ..., 24717,   365,  1616],
        [    0,   367,  3675,  ...,    22,  3180,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3866e-05, -1.6172e-05,  1.5259e-05,  ..., -1.4771e-04,
          1.5737e-04,  3.2631e-05],
        [-2.0474e-05, -1.1101e-05,  4.5039e-06,  ..., -1.7539e-05,
         -2.3227e-06, -5.5730e-06],
        [-1.3977e-05, -7.5921e-06,  3.0734e-06,  ..., -1.1995e-05,
         -1.5926e-06, -3.8147e-06],
        [-2.1756e-05, -1.1817e-05,  4.8392e-06,  ..., -1.8671e-05,
         -2.4624e-06, -5.9232e-06],
        [-3.9339e-05, -2.1368e-05,  8.6129e-06,  ..., -3.3736e-05,
         -4.4964e-06, -1.0654e-05]], device='cuda:0')
Loss: 1.2306740283966064


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,   475,   668,  ..., 17680,   368,  1351],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2847e-05, -8.0109e-05,  2.4177e-06,  ..., -1.2949e-04,
          1.4793e-04, -1.5131e-05],
        [-2.4498e-05, -1.3277e-05,  5.4315e-06,  ..., -2.0996e-05,
         -2.8331e-06, -6.6310e-06],
        [-1.6734e-05, -9.0897e-06,  3.7104e-06,  ..., -1.4365e-05,
         -1.9409e-06, -4.5374e-06],
        [-2.6017e-05, -1.4141e-05,  5.8301e-06,  ..., -2.2352e-05,
         -3.0026e-06, -7.0408e-06],
        [-4.7088e-05, -2.5570e-05,  1.0394e-05,  ..., -4.0412e-05,
         -5.4874e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.2173869609832764


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[   0, 6909,  352,  ..., 1075,  706, 6909],
        [   0, 4672,  278,  ..., 7523, 2305,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2740e-04,  1.1971e-05, -6.6118e-05,  ..., -1.9240e-04,
          1.3452e-04,  4.1440e-05],
        [-2.8551e-05, -1.5408e-05,  6.3628e-06,  ..., -2.4572e-05,
         -3.2522e-06, -7.7784e-06],
        [-1.9521e-05, -1.0557e-05,  4.3511e-06,  ..., -1.6823e-05,
         -2.2277e-06, -5.3309e-06],
        [-3.0339e-05, -1.6421e-05,  6.8285e-06,  ..., -2.6166e-05,
         -3.4496e-06, -8.2627e-06],
        [ 1.3718e-04,  9.2933e-05, -5.0317e-05,  ...,  1.4135e-04,
          3.7764e-05,  4.7084e-05]], device='cuda:0')
Loss: 1.2203092575073242


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[   0,  600, 9092,  ...,  554, 1485,  328],
        [   0,  516,  596,  ..., 3109,  287,  394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0410e-04, -2.1426e-05, -1.0322e-04,  ..., -3.1826e-04,
          1.7255e-04,  3.2738e-05],
        [-3.2753e-05, -1.7717e-05,  7.3165e-06,  ..., -2.8133e-05,
         -3.6694e-06, -8.9034e-06],
        [-2.2337e-05, -1.2107e-05,  4.9956e-06,  ..., -1.9222e-05,
         -2.5071e-06, -6.0871e-06],
        [-3.4750e-05, -1.8850e-05,  7.8417e-06,  ..., -2.9922e-05,
         -3.8873e-06, -9.4399e-06],
        [ 1.2932e-04,  8.8611e-05, -4.8529e-05,  ...,  1.3465e-04,
          3.6982e-05,  4.4998e-05]], device='cuda:0')
Loss: 1.2190684080123901
Graident accumulation at epoch 0, step 111, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0406,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3154e-04, -1.8964e-04, -2.6203e-05,  ...,  8.7837e-05,
         -5.2995e-04, -2.4071e-04],
        [-2.3930e-05, -1.8970e-05,  2.4892e-06,  ..., -1.7864e-05,
         -3.7430e-06, -5.8448e-06],
        [ 1.4289e-05,  4.8274e-06,  1.0907e-06,  ...,  1.3503e-05,
         -4.4086e-06, -3.3704e-06],
        [-1.9271e-05, -1.4439e-05,  2.5669e-06,  ..., -1.4790e-05,
         -3.1794e-06, -4.9977e-06],
        [-2.5093e-05, -2.1215e-05, -2.4533e-06,  ..., -1.4582e-05,
         -2.0465e-06, -3.6942e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2911e-08, 2.8627e-08, 3.6037e-08,  ..., 1.1806e-08, 9.0288e-08,
         1.1585e-08],
        [1.3554e-11, 1.0368e-11, 2.4238e-13,  ..., 7.1110e-12, 3.8016e-13,
         7.4103e-13],
        [3.9211e-11, 2.1929e-11, 1.5622e-12,  ..., 2.7438e-11, 1.2583e-12,
         3.7468e-12],
        [8.0260e-12, 4.8034e-12, 2.3509e-13,  ..., 4.7449e-12, 3.1957e-13,
         5.4998e-13],
        [5.8608e-11, 3.6858e-11, 2.7331e-12,  ..., 4.0519e-11, 2.3083e-12,
         3.7436e-12]], device='cuda:0')
optimizer state dict: 14.0
lr: [1.997120931904809e-05, 1.997120931904809e-05]
scheduler_last_epoch: 14


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4464e-05,  2.5093e-05,  3.1658e-05,  ..., -4.2697e-05,
          0.0000e+00,  6.6364e-05],
        [-3.9637e-06, -2.1756e-06,  1.0729e-06,  ..., -3.5167e-06,
         -4.8429e-07, -1.2815e-06],
        [-2.6822e-06, -1.4678e-06,  7.2271e-07,  ..., -2.3693e-06,
         -3.2596e-07, -8.6427e-07],
        [-4.2915e-06, -2.3544e-06,  1.1697e-06,  ..., -3.7849e-06,
         -5.2527e-07, -1.3784e-06],
        [-7.6294e-06, -4.1723e-06,  2.0564e-06,  ..., -6.7353e-06,
         -9.3877e-07, -2.4438e-06]], device='cuda:0')
Loss: 1.2251358032226562


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[    0,   221,  6872,  ...,   806,   518,   266],
        [    0,  1985,   278,  ...,   677, 12292, 17956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0879e-05, -4.2036e-06,  3.7748e-05,  ...,  1.5084e-05,
          2.2272e-05, -1.8659e-05],
        [-7.8678e-06, -4.3511e-06,  2.1458e-06,  ..., -6.9886e-06,
         -9.1642e-07, -2.5257e-06],
        [-5.2899e-06, -2.9281e-06,  1.4417e-06,  ..., -4.6939e-06,
         -6.1654e-07, -1.6987e-06],
        [-8.5235e-06, -4.7088e-06,  2.3395e-06,  ..., -7.5400e-06,
         -9.9279e-07, -2.7269e-06],
        [-1.5050e-05, -8.3148e-06,  4.0829e-06,  ..., -1.3351e-05,
         -1.7658e-06, -4.7982e-06]], device='cuda:0')
Loss: 1.2102715969085693


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6726e-04,  1.0304e-05, -5.8059e-05,  ..., -9.4456e-05,
          2.5000e-05, -6.0347e-05],
        [ 8.1297e-05,  2.8647e-05, -4.1962e-05,  ...,  6.8483e-05,
          2.4342e-05,  3.5710e-05],
        [-8.0317e-06, -4.4033e-06,  2.1532e-06,  ..., -7.0632e-06,
         -9.9279e-07, -2.5481e-06],
        [-1.2934e-05, -7.0930e-06,  3.4943e-06,  ..., -1.1355e-05,
         -1.6000e-06, -4.0978e-06],
        [-2.2799e-05, -1.2487e-05,  6.0797e-06,  ..., -2.0057e-05,
         -2.8312e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.1964582204818726


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0900e-04, -1.9740e-05, -3.8183e-05,  ..., -5.6154e-05,
         -4.3104e-05, -3.0849e-05],
        [ 7.7303e-05,  2.6486e-05, -4.0911e-05,  ...,  6.5041e-05,
          2.3866e-05,  3.4503e-05],
        [-1.0729e-05, -5.8711e-06,  2.8647e-06,  ..., -9.3877e-06,
         -1.3150e-06, -3.3639e-06],
        [-1.7256e-05, -9.4473e-06,  4.6417e-06,  ..., -1.5080e-05,
         -2.1141e-06, -5.4091e-06],
        [-3.0488e-05, -1.6659e-05,  8.0913e-06,  ..., -2.6673e-05,
         -3.7551e-06, -9.5069e-06]], device='cuda:0')
Loss: 1.2125771045684814


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,   380,   981,  ...,   567,  5407,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7186e-04, -1.9740e-05, -2.0117e-05,  ..., -1.1686e-04,
         -5.0043e-05, -5.5316e-05],
        [ 7.3340e-05,  2.4311e-05, -3.9890e-05,  ...,  6.1613e-05,
          2.3378e-05,  3.3244e-05],
        [-1.3441e-05, -7.3612e-06,  3.5577e-06,  ..., -1.1727e-05,
         -1.6484e-06, -4.2245e-06],
        [-2.1607e-05, -1.1832e-05,  5.7667e-06,  ..., -1.8835e-05,
         -2.6468e-06, -6.7949e-06],
        [-3.8177e-05, -2.0891e-05,  1.0058e-05,  ..., -3.3319e-05,
         -4.7088e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1983031034469604


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,  4902,   518,  ...,  5493,  3227,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4955e-05, -2.0113e-05,  1.1977e-06,  ..., -2.1217e-04,
          1.7855e-05, -4.1591e-05],
        [ 6.9316e-05,  2.2195e-05, -3.8877e-05,  ...,  5.8097e-05,
          2.2882e-05,  3.1955e-05],
        [-1.6183e-05, -8.8066e-06,  4.2506e-06,  ..., -1.4126e-05,
         -1.9874e-06, -5.1036e-06],
        [-2.6047e-05, -1.4171e-05,  6.8918e-06,  ..., -2.2709e-05,
         -3.1870e-06, -8.2105e-06],
        [-4.5985e-05, -2.5004e-05,  1.2025e-05,  ..., -4.0144e-05,
         -5.6773e-06, -1.4439e-05]], device='cuda:0')
Loss: 1.2089574337005615


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0, 2228, 1416,  ..., 3766,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8558e-05,  1.1241e-05, -6.2618e-05,  ..., -1.5947e-04,
         -2.1616e-05,  6.9632e-06],
        [ 6.5293e-05,  2.0064e-05, -3.7849e-05,  ...,  5.4625e-05,
          2.2439e-05,  3.0711e-05],
        [-1.8895e-05, -1.0245e-05,  4.9472e-06,  ..., -1.6466e-05,
         -2.2873e-06, -5.9418e-06],
        [-3.0369e-05, -1.6466e-05,  8.0094e-06,  ..., -2.6464e-05,
         -3.6638e-06, -9.5516e-06],
        [-5.3734e-05, -2.9117e-05,  1.4007e-05,  ..., -4.6819e-05,
         -6.5416e-06, -1.6823e-05]], device='cuda:0')
Loss: 1.2021266222000122


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  1265,  1545,  ...,   292, 36667, 36197]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5590e-04,  6.0170e-06, -7.2928e-05,  ..., -1.9568e-04,
          1.6563e-05, -4.0528e-05],
        [ 6.1240e-05,  1.7844e-05, -3.6813e-05,  ...,  5.1049e-05,
          2.1955e-05,  2.9429e-05],
        [-2.1622e-05, -1.1750e-05,  5.6475e-06,  ..., -1.8880e-05,
         -2.6133e-06, -6.8061e-06],
        [-3.4660e-05, -1.8835e-05,  9.1195e-06,  ..., -3.0249e-05,
         -4.1705e-06, -1.0908e-05],
        [-6.1363e-05, -3.3319e-05,  1.5959e-05,  ..., -5.3525e-05,
         -7.4580e-06, -1.9222e-05]], device='cuda:0')
Loss: 1.2176851034164429
Graident accumulation at epoch 0, step 119, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0095,  0.0406,  ...,  0.0224,  0.0064, -0.0018],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.8397e-04, -1.7008e-04, -3.0875e-05,  ...,  5.9484e-05,
         -4.7530e-04, -2.2069e-04],
        [-1.5413e-05, -1.5289e-05, -1.4411e-06,  ..., -1.0973e-05,
         -1.1732e-06, -2.3174e-06],
        [ 1.0698e-05,  3.1697e-06,  1.5464e-06,  ...,  1.0265e-05,
         -4.2291e-06, -3.7140e-06],
        [-2.0810e-05, -1.4878e-05,  3.2222e-06,  ..., -1.6336e-05,
         -3.2785e-06, -5.5887e-06],
        [-2.8720e-05, -2.2425e-05, -6.1201e-07,  ..., -1.8477e-05,
         -2.5876e-06, -5.2470e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2893e-08, 2.8599e-08, 3.6006e-08,  ..., 1.1833e-08, 9.0198e-08,
         1.1575e-08],
        [1.7291e-11, 1.0676e-11, 1.5974e-12,  ..., 9.7098e-12, 8.6178e-13,
         1.6064e-12],
        [3.9639e-11, 2.2046e-11, 1.5926e-12,  ..., 2.7767e-11, 1.2638e-12,
         3.7894e-12],
        [9.2193e-12, 5.1534e-12, 3.1802e-13,  ..., 5.6552e-12, 3.3664e-13,
         6.6840e-13],
        [6.2315e-11, 3.7931e-11, 2.9850e-12,  ..., 4.3344e-11, 2.3616e-12,
         4.1094e-12]], device='cuda:0')
optimizer state dict: 15.0
lr: [1.996107414450454e-05, 1.996107414450454e-05]
scheduler_last_epoch: 15


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,    14,  1364, 19388],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3870e-05, -2.0028e-05, -5.2460e-07,  ...,  6.6685e-06,
         -3.7046e-05, -7.3824e-05],
        [-3.9935e-06, -2.2352e-06,  1.1772e-06,  ..., -3.5465e-06,
         -5.0291e-07, -1.3709e-06],
        [-2.6971e-06, -1.5050e-06,  7.8976e-07,  ..., -2.3842e-06,
         -3.3714e-07, -9.2015e-07],
        [-4.3809e-06, -2.4438e-06,  1.2890e-06,  ..., -3.8743e-06,
         -5.4389e-07, -1.4901e-06],
        [-7.7486e-06, -4.3213e-06,  2.2650e-06,  ..., -6.8247e-06,
         -9.6112e-07, -2.6226e-06]], device='cuda:0')
Loss: 1.221095323562622


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0, 1716, 1773,  ..., 5014,   12,  847],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3516e-05, -7.0016e-06,  2.7201e-05,  ..., -1.5155e-05,
         -1.7969e-05, -1.2875e-04],
        [-7.9572e-06, -4.5151e-06,  2.2724e-06,  ..., -7.0930e-06,
         -1.0096e-06, -2.6971e-06],
        [-5.3644e-06, -3.0324e-06,  1.5236e-06,  ..., -4.7684e-06,
         -6.7428e-07, -1.8105e-06],
        [-8.6427e-06, -4.8876e-06,  2.4661e-06,  ..., -7.6741e-06,
         -1.0841e-06, -2.9057e-06],
        [-1.5438e-05, -8.7023e-06,  4.3660e-06,  ..., -1.3649e-05,
         -1.9372e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.1975698471069336


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[    0,  2013,    13,  ...,   271,   266,   908],
        [    0, 12456,    14,  ...,  1822,  1016,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9425e-05,  3.1180e-05,  8.9120e-05,  ..., -1.0578e-05,
         -3.8533e-05, -7.2715e-05],
        [-1.1951e-05, -6.7651e-06,  3.4124e-06,  ..., -1.0669e-05,
         -1.4901e-06, -4.0308e-06],
        [-8.0317e-06, -4.5300e-06,  2.2799e-06,  ..., -7.1377e-06,
         -9.9093e-07, -2.6971e-06],
        [-1.2934e-05, -7.2867e-06,  3.6880e-06,  ..., -1.1474e-05,
         -1.5907e-06, -4.3213e-06],
        [-2.3067e-05, -1.2994e-05,  6.5267e-06,  ..., -2.0444e-05,
         -2.8536e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.2002543210983276


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1615,   287,  ...,   259,   623,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7832e-06,  7.8905e-05, -2.4697e-05,  ...,  6.0346e-06,
          2.0459e-06, -4.6783e-05],
        [-1.5914e-05, -9.0003e-06,  4.6268e-06,  ..., -1.4260e-05,
         -1.9781e-06, -5.3868e-06],
        [-1.0654e-05, -6.0126e-06,  3.0808e-06,  ..., -9.5218e-06,
         -1.3113e-06, -3.5949e-06],
        [-1.7256e-05, -9.7305e-06,  5.0142e-06,  ..., -1.5378e-05,
         -2.1197e-06, -5.7966e-06],
        [-3.0518e-05, -1.7196e-05,  8.7917e-06,  ..., -2.7210e-05,
         -3.7663e-06, -1.0237e-05]], device='cuda:0')
Loss: 1.204085111618042


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[    0,    29,   413,  ...,  2001,  1027,   292],
        [    0,  1526,  3502,  ..., 11727,  3736,  1661]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5132e-04,  1.3013e-04,  4.9000e-05,  ..., -1.3185e-05,
          5.9400e-05, -8.4576e-05],
        [-2.0057e-05, -1.1221e-05,  5.7146e-06,  ..., -1.7777e-05,
         -2.4922e-06, -6.7130e-06],
        [-1.3366e-05, -7.4729e-06,  3.7961e-06,  ..., -1.1832e-05,
         -1.6503e-06, -4.4666e-06],
        [-2.1726e-05, -1.2115e-05,  6.1914e-06,  ..., -1.9178e-05,
         -2.6710e-06, -7.2196e-06],
        [-3.8505e-05, -2.1458e-05,  1.0893e-05,  ..., -3.4004e-05,
         -4.7646e-06, -1.2785e-05]], device='cuda:0')
Loss: 1.1906014680862427


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 2336,   26,  ..., 2564,  271, 1422],
        [   0, 6538, 1805,  ...,  298,  271,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1293e-04,  1.4202e-04,  1.0004e-04,  ...,  2.1730e-05,
          1.0658e-04, -5.9946e-05],
        [-2.4050e-05, -1.3486e-05,  6.8396e-06,  ..., -2.1324e-05,
         -2.9840e-06, -8.1062e-06],
        [-1.6019e-05, -8.9779e-06,  4.5411e-06,  ..., -1.4186e-05,
         -1.9763e-06, -5.3942e-06],
        [-2.6047e-05, -1.4558e-05,  7.4133e-06,  ..., -2.3022e-05,
         -3.1963e-06, -8.7246e-06],
        [-4.6194e-05, -2.5809e-05,  1.3039e-05,  ..., -4.0829e-05,
         -5.7109e-06, -1.5467e-05]], device='cuda:0')
Loss: 1.2002562284469604


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[   0,  600,   14,  ...,  221, 8187, 1802],
        [   0, 3058,  292,  ..., 1387, 1236,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7362e-04,  1.6744e-04,  1.6215e-04,  ..., -1.4948e-05,
          6.4301e-05, -8.6663e-05],
        [-2.7984e-05, -1.5676e-05,  7.9870e-06,  ..., -2.4810e-05,
         -3.4589e-06, -9.4548e-06],
        [-1.8656e-05, -1.0438e-05,  5.3123e-06,  ..., -1.6525e-05,
         -2.2948e-06, -6.2995e-06],
        [-3.0339e-05, -1.6943e-05,  8.6725e-06,  ..., -2.6822e-05,
         -3.7141e-06, -1.0192e-05],
        [-5.3883e-05, -3.0071e-05,  1.5259e-05,  ..., -4.7624e-05,
         -6.6385e-06, -1.8075e-05]], device='cuda:0')
Loss: 1.2040667533874512


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[   0, 2377,  360,  ...,  266, 4745,  963],
        [   0, 1890,  278,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9822e-04,  2.2028e-04,  1.8769e-04,  ..., -1.3827e-05,
         -1.3787e-05, -2.1025e-04],
        [-3.1948e-05, -1.7866e-05,  9.1344e-06,  ..., -2.8238e-05,
         -3.8594e-06, -1.0811e-05],
        [-2.1338e-05, -1.1928e-05,  6.0908e-06,  ..., -1.8850e-05,
         -2.5686e-06, -7.2196e-06],
        [-3.4690e-05, -1.9357e-05,  9.9391e-06,  ..., -3.0592e-05,
         -4.1593e-06, -1.1683e-05],
        [-6.1631e-05, -3.4362e-05,  1.7494e-05,  ..., -5.4330e-05,
         -7.4282e-06, -2.0713e-05]], device='cuda:0')
Loss: 1.2111308574676514
Graident accumulation at epoch 0, step 127, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0034,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0224,  0.0064, -0.0018],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0575e-04, -1.3104e-04, -9.0185e-06,  ...,  5.2153e-05,
         -4.2915e-04, -2.1965e-04],
        [-1.7067e-05, -1.5547e-05, -3.8352e-07,  ..., -1.2700e-05,
         -1.4418e-06, -3.1667e-06],
        [ 7.4942e-06,  1.6599e-06,  2.0008e-06,  ...,  7.3533e-06,
         -4.0630e-06, -4.0645e-06],
        [-2.2198e-05, -1.5326e-05,  3.8939e-06,  ..., -1.7762e-05,
         -3.3666e-06, -6.1980e-06],
        [-3.2011e-05, -2.3619e-05,  1.1986e-06,  ..., -2.2062e-05,
         -3.0717e-06, -6.7936e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2889e-08, 2.8619e-08, 3.6005e-08,  ..., 1.1821e-08, 9.0108e-08,
         1.1608e-08],
        [1.8294e-11, 1.0984e-11, 1.6792e-12,  ..., 1.0498e-11, 8.7582e-13,
         1.7216e-12],
        [4.0055e-11, 2.2166e-11, 1.6281e-12,  ..., 2.8094e-11, 1.2692e-12,
         3.8377e-12],
        [1.0413e-11, 5.5229e-12, 4.1649e-13,  ..., 6.5854e-12, 3.5361e-13,
         8.0422e-13],
        [6.6051e-11, 3.9074e-11, 3.2881e-12,  ..., 4.6252e-11, 2.4144e-12,
         4.5343e-12]], device='cuda:0')
optimizer state dict: 16.0
lr: [1.9949416830880266e-05, 1.9949416830880266e-05]
scheduler_last_epoch: 16


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,  1478,    14,  ...,   266,  9417,  9105],
        [    0,   266,  1890,  ...,   287, 38242,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1566e-05,  2.4132e-05,  4.9323e-05,  ..., -5.6518e-05,
          1.7343e-05, -1.3965e-05],
        [-4.0531e-06, -2.3991e-06,  1.2219e-06,  ..., -3.5167e-06,
         -4.4517e-07, -1.4976e-06],
        [-2.6971e-06, -1.5944e-06,  8.1584e-07,  ..., -2.3544e-06,
         -2.9989e-07, -9.9838e-07],
        [-4.3809e-06, -2.5779e-06,  1.3262e-06,  ..., -3.7998e-06,
         -4.7684e-07, -1.6093e-06],
        [-7.8678e-06, -4.6194e-06,  2.3544e-06,  ..., -6.8247e-06,
         -8.6799e-07, -2.8908e-06]], device='cuda:0')
Loss: 1.2089474201202393


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3112e-05,  4.5850e-06,  4.4510e-05,  ..., -2.7706e-05,
          4.6852e-05, -2.0351e-05],
        [-8.0466e-06, -4.7982e-06,  2.4363e-06,  ..., -7.0930e-06,
         -8.7917e-07, -2.9355e-06],
        [-5.3346e-06, -3.1739e-06,  1.6168e-06,  ..., -4.7237e-06,
         -5.8673e-07, -1.9446e-06],
        [-8.6725e-06, -5.1409e-06,  2.6375e-06,  ..., -7.6443e-06,
         -9.4436e-07, -3.1516e-06],
        [-1.5438e-05, -9.1791e-06,  4.6492e-06,  ..., -1.3620e-05,
         -1.6987e-06, -5.6028e-06]], device='cuda:0')
Loss: 1.222679853439331


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0,   292,    33,  ...,   352,   266,  9129],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1893e-04,  4.5430e-05,  6.9701e-05,  ...,  5.9738e-07,
          4.7908e-06, -1.2896e-05],
        [-1.2249e-05, -7.1824e-06,  3.5986e-06,  ..., -1.0744e-05,
         -1.4231e-06, -4.4703e-06],
        [-8.0913e-06, -4.7386e-06,  2.3767e-06,  ..., -7.1079e-06,
         -9.4250e-07, -2.9504e-06],
        [-1.3143e-05, -7.6741e-06,  3.8743e-06,  ..., -1.1519e-05,
         -1.5218e-06, -4.7833e-06],
        [-2.3603e-05, -1.3798e-05,  6.8992e-06,  ..., -2.0713e-05,
         -2.7642e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.2007588148117065


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0,   14,  469,  ...,  367, 2564,  368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6664e-04,  2.9228e-05,  4.7258e-05,  ...,  3.4855e-05,
          1.4749e-05,  1.1359e-05],
        [-1.6332e-05, -9.5814e-06,  4.7907e-06,  ..., -1.4320e-05,
         -1.8757e-06, -5.9828e-06],
        [-1.0788e-05, -6.3255e-06,  3.1665e-06,  ..., -9.4771e-06,
         -1.2424e-06, -3.9563e-06],
        [-1.7524e-05, -1.0252e-05,  5.1633e-06,  ..., -1.5363e-05,
         -2.0061e-06, -6.4075e-06],
        [-3.1412e-05, -1.8388e-05,  9.1791e-06,  ..., -2.7567e-05,
         -3.6322e-06, -1.1474e-05]], device='cuda:0')
Loss: 1.207656741142273


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  995,   13, 6507],
        [   0,   43,  527,  ..., 4309,   14, 8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6356e-04, -3.4133e-05,  8.9529e-05,  ...,  1.0483e-05,
         -2.4772e-05,  2.0227e-05],
        [-2.0474e-05, -1.1936e-05,  6.0201e-06,  ..., -1.7941e-05,
         -2.3786e-06, -7.4431e-06],
        [-1.3545e-05, -7.8827e-06,  3.9823e-06,  ..., -1.1876e-05,
         -1.5795e-06, -4.9248e-06],
        [-2.1964e-05, -1.2770e-05,  6.4820e-06,  ..., -1.9237e-05,
         -2.5425e-06, -7.9647e-06],
        [-3.9399e-05, -2.2918e-05,  1.1548e-05,  ..., -3.4511e-05,
         -4.6082e-06, -1.4260e-05]], device='cuda:0')
Loss: 1.204783320426941


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,  1188,   278,  ...,   271,  8368,   292],
        [    0,   843, 17111,  ...,    12,   461,  6176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0240e-04, -3.9732e-05,  8.7657e-05,  ...,  2.8608e-05,
         -8.5357e-06,  4.8636e-05],
        [-2.4647e-05, -1.4305e-05,  7.2196e-06,  ..., -2.1562e-05,
         -2.8592e-06, -8.9332e-06],
        [-1.6287e-05, -9.4473e-06,  4.7721e-06,  ..., -1.4260e-05,
         -1.8962e-06, -5.9083e-06],
        [-2.6286e-05, -1.5244e-05,  7.7337e-06,  ..., -2.3007e-05,
         -3.0417e-06, -9.5144e-06],
        [-4.7386e-05, -2.7478e-05,  1.3858e-05,  ..., -4.1485e-05,
         -5.5321e-06, -1.7121e-05]], device='cuda:0')
Loss: 1.214857816696167


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0,    9,  391,  ...,  300, 2646, 1717],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1926e-04,  1.3207e-05,  4.4940e-05,  ..., -2.4426e-05,
          1.3936e-05,  8.3876e-05],
        [-2.8759e-05, -1.6689e-05,  8.4415e-06,  ..., -2.5213e-05,
         -3.3099e-06, -1.0408e-05],
        [-1.8984e-05, -1.1012e-05,  5.5768e-06,  ..., -1.6659e-05,
         -2.1942e-06, -6.8769e-06],
        [-3.0667e-05, -1.7777e-05,  9.0450e-06,  ..., -2.6882e-05,
         -3.5185e-06, -1.1079e-05],
        [-5.5254e-05, -3.2037e-05,  1.6198e-05,  ..., -4.8488e-05,
         -6.4000e-06, -1.9938e-05]], device='cuda:0')
Loss: 1.2219111919403076


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[   0, 1824,   13,  ...,  266, 5940,   19],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2582e-04, -9.6810e-08, -9.4132e-07,  ...,  6.7606e-08,
          2.7424e-05,  1.2127e-04],
        [-3.2783e-05, -1.9044e-05,  9.6634e-06,  ..., -2.8715e-05,
         -3.8054e-06, -1.1876e-05],
        [-2.1666e-05, -1.2591e-05,  6.3926e-06,  ..., -1.8999e-05,
         -2.5239e-06, -7.8604e-06],
        [-3.4988e-05, -2.0310e-05,  1.0364e-05,  ..., -3.0652e-05,
         -4.0475e-06, -1.2659e-05],
        [-6.3002e-05, -3.6597e-05,  1.8552e-05,  ..., -5.5283e-05,
         -7.3537e-06, -2.2769e-05]], device='cuda:0')
Loss: 1.196455717086792
Graident accumulation at epoch 0, step 135, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0047, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0225,  0.0064, -0.0018],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4260e-04, -1.1795e-04, -8.2108e-06,  ...,  4.6945e-05,
         -3.8349e-04, -1.8556e-04],
        [-1.8638e-05, -1.5896e-05,  6.2117e-07,  ..., -1.4301e-05,
         -1.6782e-06, -4.0377e-06],
        [ 4.5781e-06,  2.3478e-07,  2.4400e-06,  ...,  4.7181e-06,
         -3.9091e-06, -4.4441e-06],
        [-2.3477e-05, -1.5824e-05,  4.5408e-06,  ..., -1.9051e-05,
         -3.4347e-06, -6.8441e-06],
        [-3.5110e-05, -2.4917e-05,  2.9339e-06,  ..., -2.5384e-05,
         -3.4999e-06, -8.3911e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2862e-08, 2.8590e-08, 3.5969e-08,  ..., 1.1809e-08, 9.0019e-08,
         1.1611e-08],
        [1.9350e-11, 1.1336e-11, 1.7709e-12,  ..., 1.1312e-11, 8.8942e-13,
         1.8610e-12],
        [4.0484e-11, 2.2302e-11, 1.6673e-12,  ..., 2.8427e-11, 1.2743e-12,
         3.8957e-12],
        [1.1627e-11, 5.9299e-12, 5.2348e-13,  ..., 7.5183e-12, 3.6964e-13,
         9.6365e-13],
        [6.9954e-11, 4.0375e-11, 3.6290e-12,  ..., 4.9262e-11, 2.4660e-12,
         5.0482e-12]], device='cuda:0')
optimizer state dict: 17.0
lr: [1.993623915951455e-05, 1.993623915951455e-05]
scheduler_last_epoch: 17


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[    0, 17262,   342,  ...,   472,   346,   462],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2139e-05, -7.3832e-06,  3.3647e-05,  ...,  2.2109e-05,
         -1.2003e-05, -1.7065e-05],
        [-4.1723e-06, -2.5630e-06,  1.2070e-06,  ..., -3.6210e-06,
         -4.9546e-07, -1.6168e-06],
        [-2.7716e-06, -1.6913e-06,  8.0094e-07,  ..., -2.3991e-06,
         -3.2969e-07, -1.0654e-06],
        [-4.2915e-06, -2.6375e-06,  1.2517e-06,  ..., -3.7402e-06,
         -5.1036e-07, -1.6615e-06],
        [-8.0466e-06, -4.9472e-06,  2.3246e-06,  ..., -7.0035e-06,
         -9.6112e-07, -3.1143e-06]], device='cuda:0')
Loss: 1.2072744369506836


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,    18,    14,  ...,   446,   747,  1193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5207e-05,  1.8639e-05,  3.6626e-05,  ...,  2.2277e-05,
         -4.7963e-05,  3.0866e-05],
        [-8.2552e-06, -5.0515e-06,  2.3693e-06,  ..., -7.1824e-06,
         -1.0394e-06, -3.1665e-06],
        [-5.5432e-06, -3.3826e-06,  1.5907e-06,  ..., -4.8131e-06,
         -6.9849e-07, -2.1160e-06],
        [-8.5533e-06, -5.2452e-06,  2.4736e-06,  ..., -7.4655e-06,
         -1.0766e-06, -3.2783e-06],
        [-1.6034e-05, -9.8050e-06,  4.5896e-06,  ..., -1.3947e-05,
         -2.0340e-06, -6.1393e-06]], device='cuda:0')
Loss: 1.189347743988037


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2208,    12,  1901],
        [    0,  2950,    13,  ..., 16513,   300,  2205]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9944e-06, -3.3869e-06,  3.3300e-05,  ..., -2.0121e-05,
         -5.6294e-05,  1.0138e-05],
        [-1.2368e-05, -7.6145e-06,  3.5763e-06,  ..., -1.0759e-05,
         -1.5199e-06, -4.7609e-06],
        [-8.3297e-06, -5.1185e-06,  2.4103e-06,  ..., -7.2271e-06,
         -1.0245e-06, -3.1963e-06],
        [-1.2845e-05, -7.9125e-06,  3.7402e-06,  ..., -1.1191e-05,
         -1.5758e-06, -4.9397e-06],
        [-2.3961e-05, -1.4752e-05,  6.9141e-06,  ..., -2.0862e-05,
         -2.9653e-06, -9.2089e-06]], device='cuda:0')
Loss: 1.1952375173568726


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,   259,  2416,  ..., 14474,    12,   259],
        [    0,  6803,  6298,  ...,   490,  1781,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1633e-05,  5.0117e-06,  9.7677e-05,  ..., -2.0121e-05,
         -4.2916e-05, -3.0969e-05],
        [-1.6481e-05, -1.0118e-05,  4.7758e-06,  ..., -1.4320e-05,
         -1.9595e-06, -6.3404e-06],
        [-1.1057e-05, -6.7726e-06,  3.2075e-06,  ..., -9.5814e-06,
         -1.3169e-06, -4.2394e-06],
        [-1.7196e-05, -1.0550e-05,  5.0142e-06,  ..., -1.4946e-05,
         -2.0377e-06, -6.6012e-06],
        [-3.2008e-05, -1.9610e-05,  9.2536e-06,  ..., -2.7806e-05,
         -3.8259e-06, -1.2279e-05]], device='cuda:0')
Loss: 1.1961286067962646


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   271,   957,  ...,  1597,  1276,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0297e-05,  7.7037e-06,  1.1872e-04,  ..., -1.4537e-05,
         -5.5999e-05, -5.7700e-05],
        [-2.0593e-05, -1.2651e-05,  6.0722e-06,  ..., -1.7896e-05,
         -2.4699e-06, -7.9870e-06],
        [-1.3798e-05, -8.4564e-06,  4.0717e-06,  ..., -1.1966e-05,
         -1.6578e-06, -5.3346e-06],
        [-2.1517e-05, -1.3202e-05,  6.3851e-06,  ..., -1.8716e-05,
         -2.5742e-06, -8.3297e-06],
        [-3.9995e-05, -2.4498e-05,  1.1742e-05,  ..., -3.4720e-05,
         -4.8168e-06, -1.5453e-05]], device='cuda:0')
Loss: 1.197644591331482


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6726e-05,  2.2904e-06,  1.6061e-04,  ..., -2.6243e-05,
         -6.0543e-05,  4.2457e-06],
        [-2.4736e-05, -1.5169e-05,  7.2718e-06,  ..., -2.1473e-05,
         -2.9653e-06, -9.5591e-06],
        [-1.6570e-05, -1.0140e-05,  4.8764e-06,  ..., -1.4350e-05,
         -1.9893e-06, -6.3851e-06],
        [-2.5809e-05, -1.5810e-05,  7.6368e-06,  ..., -2.2411e-05,
         -3.0845e-06, -9.9614e-06],
        [-4.8101e-05, -2.9415e-05,  1.4082e-05,  ..., -4.1693e-05,
         -5.7854e-06, -1.8522e-05]], device='cuda:0')
Loss: 1.2023824453353882


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8002e-05,  3.0654e-06,  2.0237e-04,  ..., -4.7356e-05,
         -4.6205e-05,  2.9696e-05],
        [-2.8819e-05, -1.7643e-05,  8.5384e-06,  ..., -2.5019e-05,
         -3.5204e-06, -1.1168e-05],
        [-1.9327e-05, -1.1802e-05,  5.7295e-06,  ..., -1.6734e-05,
         -2.3637e-06, -7.4729e-06],
        [-3.0071e-05, -1.8388e-05,  8.9630e-06,  ..., -2.6122e-05,
         -3.6620e-06, -1.1645e-05],
        [-5.5969e-05, -3.4183e-05,  1.6510e-05,  ..., -4.8518e-05,
         -6.8583e-06, -2.1607e-05]], device='cuda:0')
Loss: 1.1882233619689941


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0,  397, 1267,  ..., 1276,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0063e-05,  1.3149e-05,  2.2310e-04,  ..., -7.7804e-05,
         -8.3082e-05, -2.1496e-05],
        [-3.2932e-05, -2.0161e-05,  9.7379e-06,  ..., -2.8580e-05,
         -4.0755e-06, -1.2740e-05],
        [-2.2098e-05, -1.3500e-05,  6.5379e-06,  ..., -1.9133e-05,
         -2.7362e-06, -8.5309e-06],
        [-3.4332e-05, -2.0996e-05,  1.0215e-05,  ..., -2.9817e-05,
         -4.2319e-06, -1.3269e-05],
        [-6.3956e-05, -3.9071e-05,  1.8835e-05,  ..., -5.5432e-05,
         -7.9311e-06, -2.4647e-05]], device='cuda:0')
Loss: 1.1984469890594482
Graident accumulation at epoch 0, step 143, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0225,  0.0065, -0.0018],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0434e-04, -1.0484e-04,  1.4921e-05,  ...,  3.4470e-05,
         -3.5345e-04, -1.6915e-04],
        [-2.0068e-05, -1.6323e-05,  1.5328e-06,  ..., -1.5729e-05,
         -1.9179e-06, -4.9080e-06],
        [ 1.9105e-06, -1.1387e-06,  2.8498e-06,  ...,  2.3330e-06,
         -3.7918e-06, -4.8528e-06],
        [-2.4562e-05, -1.6342e-05,  5.1082e-06,  ..., -2.0127e-05,
         -3.5144e-06, -7.4866e-06],
        [-3.7995e-05, -2.6332e-05,  4.5240e-06,  ..., -2.8389e-05,
         -3.9430e-06, -1.0017e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2823e-08, 2.8562e-08, 3.5983e-08,  ..., 1.1803e-08, 8.9936e-08,
         1.1600e-08],
        [2.0416e-11, 1.1731e-11, 1.8640e-12,  ..., 1.2117e-11, 9.0514e-13,
         2.0214e-12],
        [4.0932e-11, 2.2462e-11, 1.7084e-12,  ..., 2.8765e-11, 1.2805e-12,
         3.9646e-12],
        [1.2794e-11, 6.3648e-12, 6.2730e-13,  ..., 8.3999e-12, 3.8718e-13,
         1.1388e-12],
        [7.3974e-11, 4.1861e-11, 3.9801e-12,  ..., 5.2286e-11, 2.5265e-12,
         5.6506e-12]], device='cuda:0')
optimizer state dict: 18.0
lr: [1.99215431440706e-05, 1.99215431440706e-05]
scheduler_last_epoch: 18


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   281,   221,  ...,  2236, 15064,  1458],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9535e-06,  2.6266e-05,  2.2569e-06,  ..., -4.0173e-05,
         -1.3518e-05, -9.4243e-05],
        [-4.0829e-06, -2.5332e-06,  1.1548e-06,  ..., -3.5465e-06,
         -4.5635e-07, -1.6987e-06],
        [-2.7865e-06, -1.7211e-06,  7.8976e-07,  ..., -2.4140e-06,
         -3.1106e-07, -1.1548e-06],
        [-4.3213e-06, -2.6673e-06,  1.2293e-06,  ..., -3.7551e-06,
         -4.8056e-07, -1.7956e-06],
        [-8.1062e-06, -5.0068e-06,  2.2799e-06,  ..., -7.0035e-06,
         -8.9779e-07, -3.3379e-06]], device='cuda:0')
Loss: 1.2054047584533691


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0, 12472,  1059,  ...,   642,   365,  6517],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7080e-06,  6.1897e-06,  3.1102e-06,  ..., -1.9570e-05,
         -1.1080e-06, -1.3894e-04],
        [-8.2254e-06, -5.1707e-06,  2.3246e-06,  ..., -7.0930e-06,
         -8.9966e-07, -3.4049e-06],
        [-5.6475e-06, -3.5465e-06,  1.5981e-06,  ..., -4.8727e-06,
         -6.1840e-07, -2.3395e-06],
        [-8.6725e-06, -5.4389e-06,  2.4661e-06,  ..., -7.4953e-06,
         -9.4622e-07, -3.5912e-06],
        [-1.6272e-05, -1.0222e-05,  4.5747e-06,  ..., -1.4007e-05,
         -1.7770e-06, -6.6906e-06]], device='cuda:0')
Loss: 1.2096750736236572


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  5150,  1030,  ...,    14,   475,  1763],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2642e-04, -2.2768e-06,  6.9454e-06,  ...,  2.1595e-06,
          1.6140e-05, -6.1083e-05],
        [-1.2368e-05, -7.7188e-06,  3.4720e-06,  ..., -1.0654e-05,
         -1.3690e-06, -5.0664e-06],
        [-8.4937e-06, -5.2974e-06,  2.3879e-06,  ..., -7.3165e-06,
         -9.4064e-07, -3.4794e-06],
        [-1.3024e-05, -8.1211e-06,  3.6806e-06,  ..., -1.1235e-05,
         -1.4380e-06, -5.3346e-06],
        [-2.4557e-05, -1.5318e-05,  6.8694e-06,  ..., -2.1130e-05,
         -2.7157e-06, -9.9987e-06]], device='cuda:0')
Loss: 1.21113920211792


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0,   292,   960,  ...,   271,  1356,    14],
        [    0,   342,  8514,  ...,   266, 46850,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5304e-04,  5.3739e-06,  1.2721e-07,  ...,  2.1595e-06,
          5.9353e-06, -5.0862e-05],
        [-1.6481e-05, -1.0252e-05,  4.5970e-06,  ..., -1.4171e-05,
         -1.8198e-06, -6.7353e-06],
        [-1.1340e-05, -7.0482e-06,  3.1665e-06,  ..., -9.7454e-06,
         -1.2517e-06, -4.6343e-06],
        [-1.7345e-05, -1.0788e-05,  4.8727e-06,  ..., -1.4931e-05,
         -1.9092e-06, -7.0855e-06],
        [-3.2723e-05, -2.0325e-05,  9.1046e-06,  ..., -2.8074e-05,
         -3.6098e-06, -1.3292e-05]], device='cuda:0')
Loss: 1.1870664358139038


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  292,  334,  674],
        [   0,  300,  266,  ...,  271, 4111, 1188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0899e-04, -7.1924e-06,  1.5579e-05,  ...,  8.0115e-06,
         -1.6290e-05, -9.0019e-05],
        [-2.0623e-05, -1.2845e-05,  5.7146e-06,  ..., -1.7747e-05,
         -2.2650e-06, -8.3894e-06],
        [-1.4216e-05, -8.8364e-06,  3.9376e-06,  ..., -1.2204e-05,
         -1.5572e-06, -5.7742e-06],
        [-2.1666e-05, -1.3486e-05,  6.0350e-06,  ..., -1.8641e-05,
         -2.3711e-06, -8.8066e-06],
        [-4.1068e-05, -2.5541e-05,  1.1340e-05,  ..., -3.5256e-05,
         -4.5113e-06, -1.6615e-05]], device='cuda:0')
Loss: 1.2080308198928833


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0,  344, 8133,  ...,  368, 1119, 5539],
        [   0,  825, 1243,  ...,   15,   22,   42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5242e-04,  1.3247e-05,  2.0874e-05,  ..., -7.3142e-06,
         -3.4305e-05, -3.5983e-05],
        [-2.4706e-05, -1.5393e-05,  6.8471e-06,  ..., -2.1294e-05,
         -2.6952e-06, -1.0006e-05],
        [-1.7032e-05, -1.0595e-05,  4.7199e-06,  ..., -1.4648e-05,
         -1.8533e-06, -6.8918e-06],
        [-2.5958e-05, -1.6153e-05,  7.2271e-06,  ..., -2.2352e-05,
         -2.8200e-06, -1.0498e-05],
        [-4.9233e-05, -3.0607e-05,  1.3590e-05,  ..., -4.2319e-05,
         -5.3719e-06, -1.9833e-05]], device='cuda:0')
Loss: 1.1903893947601318


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0, 1167, 2667,  ..., 4769,   13, 5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8160e-04,  1.0071e-05,  6.0206e-05,  ..., -2.8680e-05,
         -1.9736e-05,  8.6964e-06],
        [-2.8819e-05, -1.7986e-05,  7.9796e-06,  ..., -2.4810e-05,
         -3.1684e-06, -1.1675e-05],
        [-1.9878e-05, -1.2383e-05,  5.4985e-06,  ..., -1.7077e-05,
         -2.1793e-06, -8.0392e-06],
        [-3.0249e-05, -1.8865e-05,  8.4117e-06,  ..., -2.6017e-05,
         -3.3118e-06, -1.2234e-05],
        [-5.7399e-05, -3.5733e-05,  1.5825e-05,  ..., -4.9263e-05,
         -6.3144e-06, -2.3112e-05]], device='cuda:0')
Loss: 1.1947449445724487


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,   275,   467,  ...,   298,   365,  2714],
        [    0,  3386, 43625,  ...,    19,  2125,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7209e-04,  8.2720e-05,  7.7449e-05,  ..., -2.9390e-05,
         -1.9736e-05,  1.9197e-05],
        [-3.2932e-05, -2.0519e-05,  9.1195e-06,  ..., -2.8357e-05,
         -3.5632e-06, -1.3366e-05],
        [-2.2724e-05, -1.4134e-05,  6.2883e-06,  ..., -1.9521e-05,
         -2.4494e-06, -9.2089e-06],
        [-3.4541e-05, -2.1502e-05,  9.6038e-06,  ..., -2.9713e-05,
         -3.7197e-06, -1.3992e-05],
        [-6.5625e-05, -4.0799e-05,  1.8090e-05,  ..., -5.6326e-05,
         -7.1004e-06, -2.6479e-05]], device='cuda:0')
Loss: 1.204606056213379
Graident accumulation at epoch 0, step 151, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0405,  ...,  0.0225,  0.0065, -0.0018],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.8112e-04, -8.6081e-05,  2.1174e-05,  ...,  2.8084e-05,
         -3.2008e-04, -1.5032e-04],
        [-2.1354e-05, -1.6742e-05,  2.2915e-06,  ..., -1.6992e-05,
         -2.0824e-06, -5.7538e-06],
        [-5.5299e-07, -2.4382e-06,  3.1936e-06,  ...,  1.4763e-07,
         -3.6576e-06, -5.2884e-06],
        [-2.5560e-05, -1.6858e-05,  5.5578e-06,  ..., -2.1086e-05,
         -3.5349e-06, -8.1372e-06],
        [-4.0758e-05, -2.7779e-05,  5.8806e-06,  ..., -3.1183e-05,
         -4.2588e-06, -1.1663e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2809e-08, 2.8540e-08, 3.5953e-08,  ..., 1.1792e-08, 8.9846e-08,
         1.1588e-08],
        [2.1480e-11, 1.2140e-11, 1.9453e-12,  ..., 1.2909e-11, 9.1693e-13,
         2.1981e-12],
        [4.1408e-11, 2.2639e-11, 1.7462e-12,  ..., 2.9117e-11, 1.2852e-12,
         4.0454e-12],
        [1.3975e-11, 6.8208e-12, 7.1891e-13,  ..., 9.2743e-12, 4.0062e-13,
         1.3334e-12],
        [7.8207e-11, 4.3483e-11, 4.3034e-12,  ..., 5.5406e-11, 2.5744e-12,
         6.3461e-12]], device='cuda:0')
optimizer state dict: 19.0
lr: [1.9905331030227867e-05, 1.9905331030227867e-05]
scheduler_last_epoch: 19


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,  1527,   292,  ...,  2122,   278,  1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8992e-05, -5.4470e-06,  6.0540e-05,  ...,  2.0776e-05,
          4.0542e-05,  5.8314e-06],
        [-4.1127e-06, -2.6375e-06,  1.0133e-06,  ..., -3.3975e-06,
         -3.6508e-07, -1.6838e-06],
        [-2.9355e-06, -1.8850e-06,  7.1898e-07,  ..., -2.4289e-06,
         -2.6077e-07, -1.1995e-06],
        [-4.2915e-06, -2.7567e-06,  1.0580e-06,  ..., -3.5465e-06,
         -3.7812e-07, -1.7583e-06],
        [-8.4639e-06, -5.4240e-06,  2.0713e-06,  ..., -6.9737e-06,
         -7.5251e-07, -3.4571e-06]], device='cuda:0')
Loss: 1.1819803714752197


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1862,  674,  ...,  391,  266, 7688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8791e-05, -4.4714e-05,  9.3111e-05,  ...,  2.2282e-06,
          7.0730e-05, -3.5857e-05],
        [-8.2254e-06, -5.2750e-06,  2.0564e-06,  ..., -6.8247e-06,
         -7.5810e-07, -3.4347e-06],
        [-5.8860e-06, -3.7774e-06,  1.4640e-06,  ..., -4.8876e-06,
         -5.4203e-07, -2.4512e-06],
        [-8.6129e-06, -5.5432e-06,  2.1607e-06,  ..., -7.1526e-06,
         -7.8976e-07, -3.5986e-06],
        [-1.6987e-05, -1.0878e-05,  4.2170e-06,  ..., -1.4067e-05,
         -1.5646e-06, -7.0632e-06]], device='cuda:0')
Loss: 1.1996870040893555


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0,  2496, 10545,  ...,   287, 13978,   408],
        [    0,    14,   747,  ...,  2039,   287,  8053]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2381e-05, -4.6742e-05,  1.2408e-04,  ..., -4.5514e-07,
          5.3233e-05, -6.1922e-05],
        [-1.2249e-05, -7.8827e-06,  3.0845e-06,  ..., -1.0267e-05,
         -1.1027e-06, -5.0813e-06],
        [-8.7321e-06, -5.6252e-06,  2.1942e-06,  ..., -7.3165e-06,
         -7.8604e-07, -3.6210e-06],
        [-1.2785e-05, -8.2403e-06,  3.2336e-06,  ..., -1.0714e-05,
         -1.1455e-06, -5.3048e-06],
        [-2.5392e-05, -1.6302e-05,  6.3628e-06,  ..., -2.1219e-05,
         -2.2873e-06, -1.0490e-05]], device='cuda:0')
Loss: 1.2045069932937622


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[   0,  668, 2474,  ...,  668, 4599,  360],
        [   0,   13, 1529,  ..., 8197, 2700, 9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1062e-05, -8.6505e-05,  1.1118e-04,  ..., -2.4516e-05,
          5.9260e-05, -3.9658e-05],
        [-1.6332e-05, -1.0520e-05,  4.1053e-06,  ..., -1.3709e-05,
         -1.4734e-06, -6.7279e-06],
        [-1.1653e-05, -7.5102e-06,  2.9206e-06,  ..., -9.7603e-06,
         -1.0505e-06, -4.7982e-06],
        [-1.7047e-05, -1.0982e-05,  4.2990e-06,  ..., -1.4290e-05,
         -1.5330e-06, -7.0184e-06],
        [-3.3855e-05, -2.1756e-05,  8.4639e-06,  ..., -2.8342e-05,
         -3.0585e-06, -1.3888e-05]], device='cuda:0')
Loss: 1.1897884607315063


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,  5143,  3877,  ...,   292, 44003,    12],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0258e-05, -9.0325e-05,  1.3322e-04,  ..., -2.4705e-05,
          4.8671e-05, -3.6301e-05],
        [-2.0474e-05, -1.3143e-05,  5.0589e-06,  ..., -1.7211e-05,
         -1.8533e-06, -8.3968e-06],
        [-1.4588e-05, -9.3654e-06,  3.5986e-06,  ..., -1.2249e-05,
         -1.3206e-06, -5.9828e-06],
        [-2.1338e-05, -1.3694e-05,  5.2974e-06,  ..., -1.7911e-05,
         -1.9278e-06, -8.7470e-06],
        [-4.2439e-05, -2.7180e-05,  1.0446e-05,  ..., -3.5614e-05,
         -3.8520e-06, -1.7345e-05]], device='cuda:0')
Loss: 1.1967936754226685


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0, 5221, 7166,  ..., 4309,  342,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6557e-05, -9.0483e-05,  1.5422e-04,  ..., -4.7596e-05,
          3.2570e-05, -5.7258e-05],
        [-2.4587e-05, -1.5765e-05,  6.0871e-06,  ..., -2.0698e-05,
         -2.2519e-06, -1.0118e-05],
        [-1.7479e-05, -1.1213e-05,  4.3213e-06,  ..., -1.4707e-05,
         -1.6000e-06, -7.1973e-06],
        [-2.5541e-05, -1.6376e-05,  6.3553e-06,  ..., -2.1502e-05,
         -2.3320e-06, -1.0513e-05],
        [-5.0843e-05, -3.2544e-05,  1.2547e-05,  ..., -4.2796e-05,
         -4.6678e-06, -2.0877e-05]], device='cuda:0')
Loss: 1.2013086080551147


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0,  1217,     9,  ...,  1821,     5,   278],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3908e-06, -8.4175e-05,  1.0603e-04,  ..., -4.5510e-05,
          4.0776e-05, -3.7722e-05],
        [-2.8670e-05, -1.8314e-05,  7.0632e-06,  ..., -2.4170e-05,
         -2.6245e-06, -1.1824e-05],
        [-2.0400e-05, -1.3039e-05,  5.0142e-06,  ..., -1.7181e-05,
         -1.8664e-06, -8.4117e-06],
        [-2.9802e-05, -1.9029e-05,  7.3686e-06,  ..., -2.5108e-05,
         -2.7176e-06, -1.2279e-05],
        [-5.9307e-05, -3.7819e-05,  1.4544e-05,  ..., -4.9949e-05,
         -5.4352e-06, -2.4378e-05]], device='cuda:0')
Loss: 1.1996577978134155


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ...,  271,   13,  704],
        [   0, 2159,  271,  ..., 1268,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1758e-05, -4.8882e-05,  1.2231e-04,  ..., -1.0378e-04,
          4.8703e-05, -2.5622e-05],
        [-3.2723e-05, -2.0862e-05,  8.0168e-06,  ..., -2.7657e-05,
         -3.0398e-06, -1.3433e-05],
        [-2.3276e-05, -1.4849e-05,  5.6922e-06,  ..., -1.9655e-05,
         -2.1607e-06, -9.5591e-06],
        [-3.4004e-05, -2.1681e-05,  8.3670e-06,  ..., -2.8729e-05,
         -3.1441e-06, -1.3955e-05],
        [-6.7711e-05, -4.3124e-05,  1.6525e-05,  ..., -5.7161e-05,
         -6.2995e-06, -2.7716e-05]], device='cuda:0')
Loss: 1.2128735780715942
Graident accumulation at epoch 0, step 159, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0225,  0.0065, -0.0017],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4418e-04, -8.2361e-05,  3.1288e-05,  ...,  1.4897e-05,
         -2.8320e-04, -1.3785e-04],
        [-2.2491e-05, -1.7154e-05,  2.8640e-06,  ..., -1.8058e-05,
         -2.1782e-06, -6.5218e-06],
        [-2.8253e-06, -3.6793e-06,  3.4435e-06,  ..., -1.8326e-06,
         -3.5079e-06, -5.7155e-06],
        [-2.6405e-05, -1.7340e-05,  5.8387e-06,  ..., -2.1850e-05,
         -3.4959e-06, -8.7190e-06],
        [-4.3453e-05, -2.9313e-05,  6.9451e-06,  ..., -3.3781e-05,
         -4.4628e-06, -1.3268e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2767e-08, 2.8514e-08, 3.5932e-08,  ..., 1.1791e-08, 8.9758e-08,
         1.1577e-08],
        [2.2529e-11, 1.2563e-11, 2.0076e-12,  ..., 1.3661e-11, 9.2526e-13,
         2.3763e-12],
        [4.1908e-11, 2.2837e-11, 1.7769e-12,  ..., 2.9474e-11, 1.2886e-12,
         4.1327e-12],
        [1.5117e-11, 7.2840e-12, 7.8819e-13,  ..., 1.0090e-11, 4.1011e-13,
         1.5268e-12],
        [8.2713e-11, 4.5300e-11, 4.5722e-12,  ..., 5.8618e-11, 2.6115e-12,
         7.1079e-12]], device='cuda:0')
optimizer state dict: 20.0
lr: [1.9887605295338853e-05, 1.9887605295338853e-05]
scheduler_last_epoch: 20
Epoch 0 | Batch 159/1048 | Training PPL: 16366.44734593623 | time 11.991100549697876
Saving checkpoint at epoch 0, step 159, batch 159
Epoch 0 | Validation PPL: 10.902067974460058 | Learning rate: 1.9887605295338853e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_159, AFTER epoch 0, step 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,    13,  6913,  ...,   278,  1317,  4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1532e-05,  5.7355e-05,  6.2920e-06,  ..., -2.3375e-05,
         -2.1437e-05, -5.4110e-05],
        [-4.0233e-06, -2.6226e-06,  9.0525e-07,  ..., -3.5018e-06,
         -3.3900e-07, -1.7956e-06],
        [-2.9057e-06, -1.8924e-06,  6.5193e-07,  ..., -2.5183e-06,
         -2.4214e-07, -1.2964e-06],
        [-4.1425e-06, -2.6971e-06,  9.3132e-07,  ..., -3.5763e-06,
         -3.4273e-07, -1.8403e-06],
        [-8.4043e-06, -5.4538e-06,  1.8775e-06,  ..., -7.2718e-06,
         -6.9663e-07, -3.7253e-06]], device='cuda:0')
Loss: 1.2193163633346558


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,   578,   221,  ...,   287,  1254,  4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5455e-05,  8.3513e-05, -5.6863e-05,  ...,  6.5240e-05,
         -3.3483e-05, -5.0211e-05],
        [-8.1062e-06, -5.2303e-06,  1.8664e-06,  ..., -6.8694e-06,
         -5.8860e-07, -3.5539e-06],
        [-5.9456e-06, -3.8296e-06,  1.3672e-06,  ..., -5.0217e-06,
         -4.2655e-07, -2.6003e-06],
        [-8.3745e-06, -5.3942e-06,  1.9297e-06,  ..., -7.0632e-06,
         -5.9791e-07, -3.6582e-06],
        [-1.7226e-05, -1.1057e-05,  3.9488e-06,  ..., -1.4514e-05,
         -1.2331e-06, -7.5102e-06]], device='cuda:0')
Loss: 1.19813072681427


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0,  504,  409,  ..., 5863, 2621,  824],
        [   0,  593, 1387,  ...,  508, 8222, 1415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8123e-05,  6.2965e-05, -5.0034e-05,  ...,  5.6003e-05,
         -2.4365e-06, -6.2535e-05],
        [-1.2189e-05, -7.8976e-06,  2.7344e-06,  ..., -1.0312e-05,
         -9.0152e-07, -5.3719e-06],
        [-8.9258e-06, -5.7817e-06,  2.0005e-06,  ..., -7.5400e-06,
         -6.5379e-07, -3.9339e-06],
        [-1.2606e-05, -8.1509e-06,  2.8312e-06,  ..., -1.0625e-05,
         -9.1828e-07, -5.5432e-06],
        [-2.5809e-05, -1.6689e-05,  5.7742e-06,  ..., -2.1756e-05,
         -1.8887e-06, -1.1325e-05]], device='cuda:0')
Loss: 1.1939586400985718


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,    13, 36961,  ...,  6671, 13711,  4568]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2214e-04,  6.0473e-05, -2.7550e-05,  ...,  4.0014e-05,
          2.2754e-05, -8.9303e-05],
        [-1.6242e-05, -1.0535e-05,  3.5688e-06,  ..., -1.3754e-05,
         -1.1772e-06, -7.1526e-06],
        [-1.1861e-05, -7.7039e-06,  2.6077e-06,  ..., -1.0043e-05,
         -8.5402e-07, -5.2303e-06],
        [-1.6779e-05, -1.0863e-05,  3.6918e-06,  ..., -1.4171e-05,
         -1.2014e-06, -7.3835e-06],
        [-3.4511e-05, -2.2352e-05,  7.5549e-06,  ..., -2.9147e-05,
         -2.4848e-06, -1.5140e-05]], device='cuda:0')
Loss: 1.207492470741272


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   292,   474,   221],
        [    0,  3761,   527,  ..., 24518,   391,   638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0983e-05,  5.2547e-05, -1.1455e-05,  ...,  4.5892e-05,
          5.7029e-05, -6.3293e-05],
        [-2.0325e-05, -1.3188e-05,  4.4294e-06,  ..., -1.7136e-05,
         -1.4678e-06, -8.8960e-06],
        [-1.4856e-05, -9.6560e-06,  3.2410e-06,  ..., -1.2532e-05,
         -1.0682e-06, -6.5118e-06],
        [-2.1040e-05, -1.3635e-05,  4.5970e-06,  ..., -1.7703e-05,
         -1.5032e-06, -9.2089e-06],
        [-4.3273e-05, -2.8074e-05,  9.4026e-06,  ..., -3.6418e-05,
         -3.1106e-06, -1.8895e-05]], device='cuda:0')
Loss: 1.1713643074035645


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   360,   259,  ...,  5710,   278,  2433],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7460e-05,  8.3348e-05,  3.6542e-05,  ...,  4.7939e-05,
          4.7717e-05, -8.4977e-05],
        [-2.4348e-05, -1.5795e-05,  5.3197e-06,  ..., -2.0564e-05,
         -1.7527e-06, -1.0692e-05],
        [-1.7792e-05, -1.1563e-05,  3.8967e-06,  ..., -1.5035e-05,
         -1.2796e-06, -7.8306e-06],
        [-2.5213e-05, -1.6347e-05,  5.5283e-06,  ..., -2.1264e-05,
         -1.7993e-06, -1.1079e-05],
        [-5.1856e-05, -3.3647e-05,  1.1303e-05,  ..., -4.3750e-05,
         -3.7253e-06, -2.2709e-05]], device='cuda:0')
Loss: 1.1960279941558838


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2063e-04,  6.2561e-05,  5.4117e-06,  ...,  2.8016e-05,
          3.1311e-05, -1.0541e-04],
        [-2.8402e-05, -1.8433e-05,  6.2175e-06,  ..., -2.4006e-05,
         -2.0619e-06, -1.2435e-05],
        [ 7.0459e-05,  6.2186e-05, -1.4532e-05,  ...,  6.0858e-05,
          9.7097e-06,  1.9223e-05],
        [-2.9445e-05, -1.9103e-05,  6.4671e-06,  ..., -2.4840e-05,
         -2.1216e-06, -1.2897e-05],
        [-6.0558e-05, -3.9309e-05,  1.3225e-05,  ..., -5.1141e-05,
         -4.3884e-06, -2.6450e-05]], device='cuda:0')
Loss: 1.2071577310562134


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 1083,  287,  ...,   12,  287, 2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3830e-04,  6.7717e-05,  1.4142e-05,  ..., -1.2302e-05,
          3.3677e-05, -6.6679e-05],
        [-3.2514e-05, -2.1130e-05,  7.1153e-06,  ..., -2.7448e-05,
         -2.3507e-06, -1.4238e-05],
        [ 6.7434e-05,  6.0219e-05, -1.3872e-05,  ...,  5.8339e-05,
          9.4983e-06,  1.7905e-05],
        [-3.3677e-05, -2.1875e-05,  7.3984e-06,  ..., -2.8387e-05,
         -2.4177e-06, -1.4752e-05],
        [-6.9320e-05, -4.5002e-05,  1.5117e-05,  ..., -5.8413e-05,
         -5.0068e-06, -3.0264e-05]], device='cuda:0')
Loss: 1.1818737983703613
Graident accumulation at epoch 0, step 167, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2359e-04, -6.7353e-05,  2.9573e-05,  ...,  1.2177e-05,
         -2.5151e-04, -1.3073e-04],
        [-2.3493e-05, -1.7552e-05,  3.2892e-06,  ..., -1.8997e-05,
         -2.1954e-06, -7.2934e-06],
        [ 4.2006e-06,  2.7105e-06,  1.7119e-06,  ...,  4.1846e-06,
         -2.2073e-06, -3.3535e-06],
        [-2.7132e-05, -1.7794e-05,  5.9947e-06,  ..., -2.2504e-05,
         -3.3880e-06, -9.3223e-06],
        [-4.6040e-05, -3.0882e-05,  7.7623e-06,  ..., -3.6244e-05,
         -4.5172e-06, -1.4968e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2743e-08, 2.8490e-08, 3.5896e-08,  ..., 1.1780e-08, 8.9670e-08,
         1.1570e-08],
        [2.3564e-11, 1.2997e-11, 2.0562e-12,  ..., 1.4401e-11, 9.2986e-13,
         2.5767e-12],
        [4.6413e-11, 2.6441e-11, 1.9675e-12,  ..., 3.2848e-11, 1.3775e-12,
         4.4492e-12],
        [1.6236e-11, 7.7553e-12, 8.4214e-13,  ..., 1.0886e-11, 4.1555e-13,
         1.7429e-12],
        [8.7436e-11, 4.7279e-11, 4.7961e-12,  ..., 6.1971e-11, 2.6339e-12,
         8.0167e-12]], device='cuda:0')
optimizer state dict: 21.0
lr: [1.9868368648050586e-05, 1.9868368648050586e-05]
scheduler_last_epoch: 21


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  1832,   292,  ...,  2176,  1345,    14],
        [    0,   767,  4478,  ...,   278,   266, 19201]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4191e-05, -1.6499e-05, -3.3142e-05,  ...,  3.3248e-06,
         -2.3714e-05, -2.0763e-06],
        [-4.0233e-06, -2.6673e-06,  7.1898e-07,  ..., -3.4124e-06,
         -2.1793e-07, -1.8477e-06],
        [-3.0398e-06, -2.0266e-06,  5.4389e-07,  ..., -2.5779e-06,
         -1.6298e-07, -1.3933e-06],
        [-4.1425e-06, -2.7418e-06,  7.4133e-07,  ..., -3.5018e-06,
         -2.2259e-07, -1.8999e-06],
        [-8.9407e-06, -5.9307e-06,  1.5870e-06,  ..., -7.5698e-06,
         -4.8429e-07, -4.0829e-06]], device='cuda:0')
Loss: 1.189488410949707


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 6112,  278,  ..., 4092,  490, 2774],
        [   0, 2319,   30,  ...,  508, 6703,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2221e-05, -3.0153e-06, -3.5707e-05,  ...,  1.7869e-05,
         -3.1714e-05,  6.2515e-05],
        [-8.0466e-06, -5.3793e-06,  1.5013e-06,  ..., -6.8545e-06,
         -4.3865e-07, -3.6731e-06],
        [ 8.8806e-05,  6.5684e-05, -1.9128e-05,  ...,  7.3533e-05,
          2.1321e-05,  6.3665e-05],
        [-8.2552e-06, -5.5283e-06,  1.5460e-06,  ..., -7.0333e-06,
         -4.4610e-07, -3.7625e-06],
        [-1.7762e-05, -1.1891e-05,  3.2932e-06,  ..., -1.5110e-05,
         -9.6858e-07, -8.0764e-06]], device='cuda:0')
Loss: 1.1957449913024902


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  2523,  4729, 13753],
        [    0,   259,  1380,  ...,   287, 10221,   280]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0698e-04, -2.2208e-05, -1.7630e-05,  ...,  1.9208e-05,
         -4.9940e-05,  4.4762e-05],
        [-1.2130e-05, -8.1360e-06,  2.2054e-06,  ..., -1.0237e-05,
         -6.4634e-07, -5.5060e-06],
        [ 8.5722e-05,  6.3598e-05, -1.8596e-05,  ...,  7.0970e-05,
          2.1164e-05,  6.2272e-05],
        [-1.2398e-05, -8.3297e-06,  2.2650e-06,  ..., -1.0476e-05,
         -6.5565e-07, -5.6252e-06],
        [-2.6703e-05, -1.7941e-05,  4.8354e-06,  ..., -2.2560e-05,
         -1.4249e-06, -1.2100e-05]], device='cuda:0')
Loss: 1.2016322612762451


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1016,   271,  ...,   461,   616,   993],
        [    0,   278,   264,  ..., 21836,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1097e-04, -6.3307e-05,  2.8008e-06,  ...,  1.9208e-05,
         -6.6510e-05,  4.5041e-05],
        [-1.6183e-05, -1.0848e-05,  2.9467e-06,  ..., -1.3590e-05,
         -9.0152e-07, -7.3761e-06],
        [ 8.2682e-05,  6.1572e-05, -1.8037e-05,  ...,  6.8452e-05,
          2.0972e-05,  6.0871e-05],
        [-1.6540e-05, -1.1101e-05,  3.0287e-06,  ..., -1.3903e-05,
         -9.1828e-07, -7.5325e-06],
        [-3.5584e-05, -2.3872e-05,  6.4597e-06,  ..., -2.9892e-05,
         -1.9874e-06, -1.6183e-05]], device='cuda:0')
Loss: 1.1901304721832275


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2336,   221,   334],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2126e-04, -1.1754e-04, -3.4805e-06,  ..., -1.5838e-05,
         -5.6518e-05,  5.1268e-05],
        [-2.0295e-05, -1.3590e-05,  3.6843e-06,  ..., -1.7017e-05,
         -1.1297e-06, -9.2387e-06],
        [ 7.9642e-05,  5.9530e-05, -1.7489e-05,  ...,  6.5918e-05,
          2.0801e-05,  5.9493e-05],
        [-2.0683e-05, -1.3873e-05,  3.7774e-06,  ..., -1.7360e-05,
         -1.1483e-06, -9.4175e-06],
        [-4.4584e-05, -2.9862e-05,  8.0690e-06,  ..., -3.7372e-05,
         -2.4904e-06, -2.0236e-05]], device='cuda:0')
Loss: 1.1952227354049683


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0,  278, 1059,  ...,  300, 1877,   13],
        [   0, 1125,  278,  ..., 6447,  609,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3068e-04, -1.3275e-04, -3.2699e-05,  ...,  6.3457e-06,
         -2.4337e-05,  7.3156e-05],
        [-2.4378e-05, -1.6227e-05,  4.4145e-06,  ..., -2.0385e-05,
         -1.3812e-06, -1.1109e-05],
        [ 7.6542e-05,  5.7518e-05, -1.6934e-05,  ...,  6.3370e-05,
          2.0610e-05,  5.8070e-05],
        [-2.4945e-05, -1.6615e-05,  4.5411e-06,  ..., -2.0847e-05,
         -1.4091e-06, -1.1355e-05],
        [-5.3585e-05, -3.5673e-05,  9.6634e-06,  ..., -4.4763e-05,
         -3.0492e-06, -2.4348e-05]], device='cuda:0')
Loss: 1.188739538192749


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[   0, 2314,  266,  ...,  342, 7299, 1099],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2225e-04, -1.6197e-04, -2.0901e-05,  ..., -2.0250e-05,
         -3.4285e-06,  8.3014e-05],
        [-2.8461e-05, -1.8924e-05,  5.1558e-06,  ..., -2.3767e-05,
         -1.6401e-06, -1.2971e-05],
        [ 2.8560e-04,  1.8062e-04, -1.3345e-05,  ...,  2.5634e-04,
          3.6302e-05,  1.3805e-04],
        [-2.9087e-05, -1.9372e-05,  5.3011e-06,  ..., -2.4289e-05,
         -1.6736e-06, -1.3247e-05],
        [-6.2466e-05, -4.1574e-05,  1.1280e-05,  ..., -5.2154e-05,
         -3.6154e-06, -2.8402e-05]], device='cuda:0')
Loss: 1.1941349506378174


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0, 19191,   266,  ...,   287,   843,  1528],
        [    0,   380,  6119,  ..., 11823,   287,  6797]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5563e-04, -2.1828e-04, -4.3589e-05,  ..., -2.6878e-05,
          2.3977e-06,  5.6729e-05],
        [-3.2455e-05, -2.1562e-05,  5.8860e-06,  ..., -2.7090e-05,
         -1.9027e-06, -1.4842e-05],
        [ 2.8257e-04,  1.7862e-04, -1.2794e-05,  ...,  2.5384e-04,
          3.6103e-05,  1.3663e-04],
        [-3.3230e-05, -2.2098e-05,  6.0573e-06,  ..., -2.7716e-05,
         -1.9418e-06, -1.5184e-05],
        [-7.1406e-05, -4.7445e-05,  1.2897e-05,  ..., -5.9545e-05,
         -4.2003e-06, -3.2574e-05]], device='cuda:0')
Loss: 1.2051953077316284
Graident accumulation at epoch 0, step 175, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0169,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0680e-04, -8.2446e-05,  2.2257e-05,  ...,  8.2717e-06,
         -2.2612e-04, -1.1198e-04],
        [-2.4389e-05, -1.7953e-05,  3.5488e-06,  ..., -1.9807e-05,
         -2.1662e-06, -8.0482e-06],
        [ 3.2038e-05,  2.0302e-05,  2.6135e-07,  ...,  2.9150e-05,
          1.6238e-06,  1.0645e-05],
        [-2.7742e-05, -1.8224e-05,  6.0009e-06,  ..., -2.3025e-05,
         -3.2434e-06, -9.9085e-06],
        [-4.8577e-05, -3.2538e-05,  8.2758e-06,  ..., -3.8574e-05,
         -4.4855e-06, -1.6728e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2725e-08, 2.8509e-08, 3.5862e-08,  ..., 1.1769e-08, 8.9580e-08,
         1.1562e-08],
        [2.4593e-11, 1.3449e-11, 2.0888e-12,  ..., 1.5120e-11, 9.3255e-13,
         2.7944e-12],
        [1.2621e-10, 5.8321e-11, 2.1292e-12,  ..., 9.7249e-11, 2.6796e-12,
         2.3114e-11],
        [1.7324e-11, 8.2358e-12, 8.7799e-13,  ..., 1.1643e-11, 4.1890e-13,
         1.9717e-12],
        [9.2447e-11, 4.9483e-11, 4.9576e-12,  ..., 6.5455e-11, 2.6489e-12,
         9.0698e-12]], device='cuda:0')
optimizer state dict: 22.0
lr: [1.9847624027890693e-05, 1.9847624027890693e-05]
scheduler_last_epoch: 22


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[    0, 18837,   394,  ...,   271,  1398,  1871],
        [    0,  3070,  9719,  ...,   600,  4207,  4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1399e-06,  7.5371e-06, -2.3635e-05,  ...,  7.7769e-05,
          1.3793e-05, -5.6754e-05],
        [-4.0531e-06, -2.6822e-06,  5.7369e-07,  ..., -3.2932e-06,
         -2.3376e-07, -1.9372e-06],
        [-3.1888e-06, -2.1160e-06,  4.5076e-07,  ..., -2.5928e-06,
         -1.8440e-07, -1.5274e-06],
        [-4.1425e-06, -2.7418e-06,  5.8860e-07,  ..., -3.3677e-06,
         -2.3749e-07, -1.9819e-06],
        [-9.1791e-06, -6.0797e-06,  1.2964e-06,  ..., -7.4804e-06,
         -5.2899e-07, -4.4107e-06]], device='cuda:0')
Loss: 1.1817307472229004


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[    0,   266, 11692,  ...,   278, 14620, 12718],
        [    0,  3646,  1340,  ...,    13,  7800,  2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9986e-05, -7.2459e-05,  2.4264e-06,  ...,  4.3053e-05,
          1.1465e-05, -7.0366e-05],
        [-8.0168e-06, -5.3495e-06,  1.0766e-06,  ..., -6.5565e-06,
         -4.8336e-07, -3.8296e-06],
        [-6.3181e-06, -4.2319e-06,  8.4750e-07,  ..., -5.1707e-06,
         -3.8184e-07, -3.0249e-06],
        [-8.1658e-06, -5.4538e-06,  1.0990e-06,  ..., -6.6757e-06,
         -4.8894e-07, -3.9041e-06],
        [-1.8179e-05, -1.2159e-05,  2.4363e-06,  ..., -1.4871e-05,
         -1.0952e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.190208911895752


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[   0,  898,  266,  ...,   12, 3222, 8095],
        [   0, 2296,  446,  ..., 2937,  287, 2795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2705e-05, -1.1177e-04,  1.1024e-05,  ...,  6.3952e-05,
          2.4599e-06, -5.0363e-05],
        [-1.1951e-05, -7.9572e-06,  1.6578e-06,  ..., -9.7603e-06,
         -6.8452e-07, -5.7369e-06],
        [-9.4324e-06, -6.2883e-06,  1.3076e-06,  ..., -7.7039e-06,
         -5.4203e-07, -4.5300e-06],
        [-1.2219e-05, -8.1360e-06,  1.6987e-06,  ..., -9.9689e-06,
         -6.9663e-07, -5.8711e-06],
        [-2.7299e-05, -1.8179e-05,  3.7700e-06,  ..., -2.2262e-05,
         -1.5665e-06, -1.3083e-05]], device='cuda:0')
Loss: 1.200032114982605


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,   638,  1276,  ...,  1589,  2432,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8126e-06, -1.1708e-04,  2.8261e-05,  ...,  7.3450e-05,
          1.3050e-05, -5.0363e-05],
        [-1.5944e-05, -1.0625e-05,  2.2352e-06,  ..., -1.3024e-05,
         -8.5495e-07, -7.6592e-06],
        [-1.2562e-05, -8.3745e-06,  1.7621e-06,  ..., -1.0252e-05,
         -6.7428e-07, -6.0275e-06],
        [-1.6302e-05, -1.0863e-05,  2.2948e-06,  ..., -1.3307e-05,
         -8.6892e-07, -7.8380e-06],
        [-3.6418e-05, -2.4259e-05,  5.0887e-06,  ..., -2.9683e-05,
         -1.9576e-06, -1.7434e-05]], device='cuda:0')
Loss: 1.1955487728118896


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0,   278,   490,  ...,   434,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1367e-05, -1.4487e-04,  6.6157e-09,  ...,  1.2241e-04,
         -1.9722e-05, -1.0315e-05],
        [-1.9878e-05, -1.3232e-05,  2.7865e-06,  ..., -1.6317e-05,
         -1.1344e-06, -9.5665e-06],
        [-1.5646e-05, -1.0416e-05,  2.1961e-06,  ..., -1.2830e-05,
         -8.9314e-07, -7.5251e-06],
        [-2.0355e-05, -1.3545e-05,  2.8610e-06,  ..., -1.6689e-05,
         -1.1539e-06, -9.7901e-06],
        [-4.5419e-05, -3.0220e-05,  6.3404e-06,  ..., -3.7193e-05,
         -2.5984e-06, -2.1785e-05]], device='cuda:0')
Loss: 1.2013083696365356


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,  5281,  4452,  ...,    14,  3391,    12],
        [    0,  2386,  4012,  ...,   300, 15480,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2759e-05, -1.3390e-04, -1.1830e-06,  ...,  1.0576e-04,
         -5.1887e-05,  3.3793e-05],
        [-2.3901e-05, -1.5929e-05,  3.3975e-06,  ..., -1.9595e-05,
         -1.3970e-06, -1.1489e-05],
        [-1.8775e-05, -1.2532e-05,  2.6766e-06,  ..., -1.5393e-05,
         -1.0980e-06, -9.0301e-06],
        [-2.4438e-05, -1.6302e-05,  3.4906e-06,  ..., -2.0042e-05,
         -1.4221e-06, -1.1757e-05],
        [-5.4538e-05, -3.6359e-05,  7.7263e-06,  ..., -4.4644e-05,
         -3.1982e-06, -2.6137e-05]], device='cuda:0')
Loss: 1.1843031644821167


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[   0, 9466,   36,  ..., 1795,  437,  874],
        [   0, 1875, 2117,  ..., 1422, 1059,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5914e-05, -9.0039e-05, -2.4693e-05,  ...,  9.1458e-05,
         -1.4421e-05, -2.4271e-06],
        [-2.7835e-05, -1.8537e-05,  4.0010e-06,  ..., -2.2843e-05,
         -1.6224e-06, -1.3411e-05],
        [-2.1905e-05, -1.4618e-05,  3.1572e-06,  ..., -1.7986e-05,
         -1.2768e-06, -1.0565e-05],
        [-2.8491e-05, -1.8999e-05,  4.1164e-06,  ..., -2.3395e-05,
         -1.6531e-06, -1.3739e-05],
        [-6.3598e-05, -4.2379e-05,  9.1195e-06,  ..., -5.2124e-05,
         -3.7160e-06, -3.0547e-05]], device='cuda:0')
Loss: 1.1905269622802734


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,  3059,  2013,  ...,   278,  1997,    14],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4504e-06, -9.2342e-05, -2.1052e-05,  ...,  9.7836e-05,
         -2.0556e-05, -5.5315e-05],
        [-3.1859e-05, -2.1189e-05,  4.5933e-06,  ..., -2.6152e-05,
         -1.8775e-06, -1.5363e-05],
        [-2.5064e-05, -1.6719e-05,  3.6266e-06,  ..., -2.0593e-05,
         -1.4789e-06, -1.2107e-05],
        [-3.2604e-05, -2.1711e-05,  4.7274e-06,  ..., -2.6777e-05,
         -1.9120e-06, -1.5736e-05],
        [-7.2718e-05, -4.8399e-05,  1.0461e-05,  ..., -5.9634e-05,
         -4.2971e-06, -3.4988e-05]], device='cuda:0')
Loss: 1.1843678951263428
Graident accumulation at epoch 0, step 183, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0169,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7706e-04, -8.3436e-05,  1.7926e-05,  ...,  1.7228e-05,
         -2.0556e-04, -1.0632e-04],
        [-2.5136e-05, -1.8277e-05,  3.6533e-06,  ..., -2.0441e-05,
         -2.1373e-06, -8.7797e-06],
        [ 2.6328e-05,  1.6600e-05,  5.9787e-07,  ...,  2.4176e-05,
          1.3135e-06,  8.3700e-06],
        [-2.8228e-05, -1.8573e-05,  5.8736e-06,  ..., -2.3400e-05,
         -3.1103e-06, -1.0491e-05],
        [-5.0991e-05, -3.4124e-05,  8.4943e-06,  ..., -4.0680e-05,
         -4.4667e-06, -1.8554e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2682e-08, 2.8489e-08, 3.5827e-08,  ..., 1.1767e-08, 8.9491e-08,
         1.1553e-08],
        [2.5584e-11, 1.3885e-11, 2.1078e-12,  ..., 1.5789e-11, 9.3514e-13,
         3.0276e-12],
        [1.2672e-10, 5.8542e-11, 2.1403e-12,  ..., 9.7576e-11, 2.6791e-12,
         2.3237e-11],
        [1.8370e-11, 8.6990e-12, 8.9946e-13,  ..., 1.2349e-11, 4.2214e-13,
         2.2174e-12],
        [9.7643e-11, 5.1776e-11, 5.0621e-12,  ..., 6.8946e-11, 2.6648e-12,
         1.0285e-11]], device='cuda:0')
optimizer state dict: 23.0
lr: [1.982537460481821e-05, 1.982537460481821e-05]
scheduler_last_epoch: 23


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[    0, 14652,    12,  ..., 17330,   996,  3294],
        [    0,   677,  8708,  ..., 19891,   267,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1030e-05,  7.9231e-06, -3.7509e-05,  ..., -3.9381e-06,
          3.0186e-06, -2.3857e-05],
        [-3.9339e-06, -2.6524e-06,  4.7125e-07,  ..., -3.2336e-06,
         -1.3970e-07, -1.9521e-06],
        [-3.1888e-06, -2.1458e-06,  3.8370e-07,  ..., -2.6226e-06,
         -1.1362e-07, -1.5870e-06],
        [-4.0531e-06, -2.7269e-06,  4.8801e-07,  ..., -3.3379e-06,
         -1.4249e-07, -2.0117e-06],
        [-9.1791e-06, -6.1691e-06,  1.0952e-06,  ..., -7.5698e-06,
         -3.2969e-07, -4.5598e-06]], device='cuda:0')
Loss: 1.1968201398849487


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1555,   12,  ...,  809,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8102e-05, -2.1987e-05, -8.1233e-05,  ...,  7.8537e-07,
          3.1475e-05, -1.8084e-05],
        [-7.8380e-06, -5.3495e-06,  9.3691e-07,  ..., -6.4969e-06,
         -3.3062e-07, -3.8743e-06],
        [-6.4075e-06, -4.3511e-06,  7.6555e-07,  ..., -5.3048e-06,
         -2.7101e-07, -3.1739e-06],
        [-8.0466e-06, -5.4687e-06,  9.6485e-07,  ..., -6.6608e-06,
         -3.3528e-07, -3.9786e-06],
        [-1.8358e-05, -1.2457e-05,  2.1830e-06,  ..., -1.5199e-05,
         -7.8045e-07, -9.0599e-06]], device='cuda:0')
Loss: 1.2194416522979736


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0, 43587,  1390,  ...,    12,   768,  1952],
        [    0,   266,  1634,  ...,   310,  1372,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7337e-05,  1.5651e-06, -7.5615e-05,  ...,  1.3384e-05,
          4.9433e-05, -3.9168e-05],
        [-1.1772e-05, -8.0019e-06,  1.3951e-06,  ..., -9.7454e-06,
         -5.2806e-07, -5.7966e-06],
        [-9.6262e-06, -6.5118e-06,  1.1399e-06,  ..., -7.9572e-06,
         -4.3306e-07, -4.7460e-06],
        [-1.2070e-05, -8.1807e-06,  1.4380e-06,  ..., -9.9838e-06,
         -5.3644e-07, -5.9605e-06],
        [-2.7537e-05, -1.8626e-05,  3.2485e-06,  ..., -2.2769e-05,
         -1.2405e-06, -1.3560e-05]], device='cuda:0')
Loss: 1.1863499879837036


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[  0,  12, 271,  ...,  12, 298, 273],
        [  0,  12, 287,  ...,  17, 271, 266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5785e-05,  8.0930e-06, -8.4281e-05,  ...,  2.8199e-05,
          3.0798e-05,  6.8264e-06],
        [-1.5706e-05, -1.0654e-05,  1.8571e-06,  ..., -1.2979e-05,
         -7.4413e-07, -7.7486e-06],
        [-1.2830e-05, -8.6725e-06,  1.5162e-06,  ..., -1.0580e-05,
         -6.0908e-07, -6.3330e-06],
        [-1.6093e-05, -1.0893e-05,  1.9129e-06,  ..., -1.3277e-05,
         -7.5437e-07, -7.9572e-06],
        [-3.6657e-05, -2.4766e-05,  4.3139e-06,  ..., -3.0220e-05,
         -1.7397e-06, -1.8060e-05]], device='cuda:0')
Loss: 1.1738216876983643


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9668e-05,  3.9203e-05, -8.9966e-05,  ...,  6.5489e-05,
          1.8456e-05, -2.3997e-05],
        [-1.9640e-05, -1.3307e-05,  2.3264e-06,  ..., -1.6198e-05,
         -9.2201e-07, -9.7305e-06],
        [-1.6034e-05, -1.0833e-05,  1.8980e-06,  ..., -1.3202e-05,
         -7.5437e-07, -7.9498e-06],
        [-2.0087e-05, -1.3590e-05,  2.3935e-06,  ..., -1.6555e-05,
         -9.3412e-07, -9.9838e-06],
        [-4.5776e-05, -3.0875e-05,  5.3942e-06,  ..., -3.7640e-05,
         -2.1551e-06, -2.2650e-05]], device='cuda:0')
Loss: 1.1948845386505127


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[   0,  635,   13,  ...,  292,   20,  445],
        [   0,  271, 8429,  ..., 9404,  963,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9440e-05,  2.2607e-05, -6.9873e-05,  ...,  9.3579e-05,
         -2.3874e-06, -4.1197e-05],
        [-2.3574e-05, -1.5944e-05,  2.7865e-06,  ..., -1.9431e-05,
         -1.0906e-06, -1.1697e-05],
        [-1.9222e-05, -1.2979e-05,  2.2724e-06,  ..., -1.5825e-05,
         -8.9128e-07, -9.5442e-06],
        [-2.4110e-05, -1.6287e-05,  2.8666e-06,  ..., -1.9848e-05,
         -1.1045e-06, -1.1995e-05],
        [-5.4955e-05, -3.7014e-05,  6.4597e-06,  ..., -4.5151e-05,
         -2.5518e-06, -2.7210e-05]], device='cuda:0')
Loss: 1.1847690343856812


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[   0, 1062,  648,  ...,  266, 4939,  278],
        [   0, 8840,   26,  ...,   28,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8245e-05,  3.8222e-05, -6.3637e-05,  ...,  1.2412e-04,
          1.2794e-05, -3.8544e-05],
        [-2.7448e-05, -1.8582e-05,  3.2354e-06,  ..., -2.2635e-05,
         -1.2908e-06, -1.3620e-05],
        [-2.2382e-05, -1.5140e-05,  2.6412e-06,  ..., -1.8448e-05,
         -1.0552e-06, -1.1124e-05],
        [-2.8074e-05, -1.8984e-05,  3.3285e-06,  ..., -2.3142e-05,
         -1.3085e-06, -1.3962e-05],
        [-6.4015e-05, -4.3213e-05,  7.5102e-06,  ..., -5.2691e-05,
         -3.0249e-06, -3.1739e-05]], device='cuda:0')
Loss: 1.1910382509231567


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,   266,  2511,  ...,  3220,  4164,  1173],
        [    0,    12,   496,  ..., 11354,  4856,  1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2685e-05,  5.2686e-05,  1.6256e-05,  ...,  1.0820e-04,
          5.8177e-06, -5.4097e-05],
        [-3.1352e-05, -2.1219e-05,  3.7197e-06,  ..., -2.5779e-05,
         -1.4724e-06, -1.5557e-05],
        [ 2.7138e-04,  2.1412e-04, -6.6107e-05,  ...,  2.5674e-04,
         -8.4032e-06,  1.3646e-04],
        [-3.2067e-05, -2.1696e-05,  3.8277e-06,  ..., -2.6360e-05,
         -1.4920e-06, -1.5959e-05],
        [-7.3195e-05, -4.9442e-05,  8.6427e-06,  ..., -6.0081e-05,
         -3.4533e-06, -3.6299e-05]], device='cuda:0')
Loss: 1.1866689920425415
Graident accumulation at epoch 0, step 191, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0404,  ...,  0.0226,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4209e-04, -6.9823e-05,  1.7759e-05,  ...,  2.6325e-05,
         -1.8443e-04, -1.0109e-04],
        [-2.5758e-05, -1.8571e-05,  3.6599e-06,  ..., -2.0975e-05,
         -2.0708e-06, -9.4574e-06],
        [ 5.0833e-05,  3.6352e-05, -6.0726e-06,  ...,  4.7432e-05,
          3.4184e-07,  2.1179e-05],
        [-2.8612e-05, -1.8885e-05,  5.6690e-06,  ..., -2.3696e-05,
         -2.9484e-06, -1.1038e-05],
        [-5.3211e-05, -3.5656e-05,  8.5091e-06,  ..., -4.2620e-05,
         -4.3654e-06, -2.0329e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2645e-08, 2.8463e-08, 3.5791e-08,  ..., 1.1767e-08, 8.9402e-08,
         1.1545e-08],
        [2.6541e-11, 1.4321e-11, 2.1195e-12,  ..., 1.6438e-11, 9.3637e-13,
         3.2666e-12],
        [2.0024e-10, 1.0433e-10, 6.5082e-12,  ..., 1.6340e-10, 2.7470e-12,
         4.1834e-11],
        [1.9379e-11, 9.1610e-12, 9.1321e-13,  ..., 1.3031e-11, 4.2394e-13,
         2.4698e-12],
        [1.0290e-10, 5.4169e-11, 5.1317e-12,  ..., 7.2487e-11, 2.6740e-12,
         1.1592e-11]], device='cuda:0')
optimizer state dict: 24.0
lr: [1.9801623778739208e-05, 1.9801623778739208e-05]
scheduler_last_epoch: 24


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  2346, 17886,  ...,   287,  6769,   806],
        [    0, 38717,  1679,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7704e-05, -6.6058e-05, -1.1911e-05,  ...,  3.0209e-05,
          2.4434e-05, -3.9143e-06],
        [-3.7998e-06, -2.5481e-06,  3.4459e-07,  ..., -3.1143e-06,
         -1.0990e-07, -1.9073e-06],
        [-3.2187e-06, -2.1607e-06,  2.9244e-07,  ..., -2.6375e-06,
         -9.1735e-08, -1.6168e-06],
        [-3.9637e-06, -2.6524e-06,  3.6135e-07,  ..., -3.2336e-06,
         -1.1083e-07, -1.9819e-06],
        [-9.0599e-06, -6.0499e-06,  8.1211e-07,  ..., -7.3910e-06,
         -2.5891e-07, -4.5300e-06]], device='cuda:0')
Loss: 1.202036738395691


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0, 1086,  292,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2437e-05, -7.3945e-05, -1.1627e-05,  ...,  4.8315e-05,
          8.7335e-06,  2.5960e-05],
        [-7.6741e-06, -5.1111e-06,  7.2084e-07,  ..., -6.2734e-06,
         -2.6636e-07, -3.8296e-06],
        [-6.5118e-06, -4.3362e-06,  6.1467e-07,  ..., -5.3346e-06,
         -2.2678e-07, -3.2559e-06],
        [-8.0168e-06, -5.3346e-06,  7.5810e-07,  ..., -6.5565e-06,
         -2.7567e-07, -4.0084e-06],
        [-1.8239e-05, -1.2130e-05,  1.7025e-06,  ..., -1.4901e-05,
         -6.3702e-07, -9.0897e-06]], device='cuda:0')
Loss: 1.1852984428405762


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  271,  669,   12],
        [   0, 2388, 6604,  ..., 5005, 1196,  717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3937e-05, -9.6195e-05, -9.0314e-06,  ...,  6.9516e-05,
          1.0340e-05,  4.1552e-05],
        [-1.1519e-05, -7.7039e-06,  1.0803e-06,  ..., -9.4175e-06,
         -4.0233e-07, -5.7518e-06],
        [-9.7603e-06, -6.5416e-06,  9.2015e-07,  ..., -8.0019e-06,
         -3.4366e-07, -4.8950e-06],
        [-1.1981e-05, -8.0317e-06,  1.1344e-06,  ..., -9.8199e-06,
         -4.1630e-07, -6.0052e-06],
        [-2.7299e-05, -1.8269e-05,  2.5481e-06,  ..., -2.2352e-05,
         -9.6112e-07, -1.3649e-05]], device='cuda:0')
Loss: 1.1857975721359253


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5259e-05, -1.1735e-04, -3.7697e-05,  ...,  8.7240e-05,
          2.1464e-05,  8.8932e-05],
        [-1.5363e-05, -1.0312e-05,  1.4678e-06,  ..., -1.2577e-05,
         -5.6438e-07, -7.6517e-06],
        [-1.3024e-05, -8.7619e-06,  1.2517e-06,  ..., -1.0699e-05,
         -4.8149e-07, -6.5193e-06],
        [-1.5974e-05, -1.0729e-05,  1.5404e-06,  ..., -1.3098e-05,
         -5.8115e-07, -7.9870e-06],
        [-3.6418e-05, -2.4468e-05,  3.4682e-06,  ..., -2.9862e-05,
         -1.3467e-06, -1.8179e-05]], device='cuda:0')
Loss: 1.2008891105651855


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  1550,   685,  ...,   943,  1239,   996],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9103e-05, -1.9714e-04, -5.7867e-05,  ...,  1.2122e-04,
          9.1686e-06,  1.3540e-04],
        [-1.9208e-05, -1.2904e-05,  1.8701e-06,  ..., -1.5706e-05,
         -7.1246e-07, -9.5293e-06],
        [-1.6332e-05, -1.0997e-05,  1.5963e-06,  ..., -1.3381e-05,
         -6.0908e-07, -8.1360e-06],
        [-1.9968e-05, -1.3426e-05,  1.9595e-06,  ..., -1.6332e-05,
         -7.3388e-07, -9.9391e-06],
        [-4.5478e-05, -3.0577e-05,  4.4107e-06,  ..., -3.7223e-05,
         -1.6987e-06, -2.2590e-05]], device='cuda:0')
Loss: 1.1852425336837769


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  696,  700,  328],
        [   0,  266, 1790,  ...,  292,   78,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0448e-04, -2.2653e-04, -4.1614e-05,  ...,  1.0743e-04,
          2.6558e-05,  1.5891e-04],
        [-2.3022e-05, -1.5542e-05,  2.2873e-06,  ..., -1.8865e-05,
         -8.6613e-07, -1.1481e-05],
        [-1.9580e-05, -1.3247e-05,  1.9539e-06,  ..., -1.6078e-05,
         -7.4226e-07, -9.7975e-06],
        [-2.3931e-05, -1.6168e-05,  2.3972e-06,  ..., -1.9625e-05,
         -8.9314e-07, -1.1966e-05],
        [-5.4419e-05, -3.6776e-05,  5.3868e-06,  ..., -4.4644e-05,
         -2.0638e-06, -2.7150e-05]], device='cuda:0')
Loss: 1.1976226568222046


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  1197, 10640,  ...,  2405,   437,  5880],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9289e-05, -2.5604e-04, -2.9416e-05,  ...,  1.2000e-04,
          1.4435e-05,  1.4699e-04],
        [-2.6822e-05, -1.8120e-05,  2.6822e-06,  ..., -2.1979e-05,
         -1.0263e-06, -1.3404e-05],
        [-2.2829e-05, -1.5453e-05,  2.2911e-06,  ..., -1.8731e-05,
         -8.7731e-07, -1.1437e-05],
        [-2.7925e-05, -1.8880e-05,  2.8126e-06,  ..., -2.2888e-05,
         -1.0589e-06, -1.3977e-05],
        [-6.3479e-05, -4.2915e-05,  6.3181e-06,  ..., -5.2035e-05,
         -2.4438e-06, -3.1710e-05]], device='cuda:0')
Loss: 1.178024411201477


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   706,  2680,   365],
        [    0,    33,    12,  ...,  1110,   467, 17467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8122e-05, -2.6668e-04, -5.1928e-05,  ...,  1.2738e-04,
          8.3208e-06,  1.8024e-04],
        [-3.0637e-05, -2.0683e-05,  3.0827e-06,  ..., -2.5123e-05,
         -1.2107e-06, -1.5341e-05],
        [-2.6077e-05, -1.7628e-05,  2.6338e-06,  ..., -2.1413e-05,
         -1.0338e-06, -1.3098e-05],
        [-3.1918e-05, -2.1547e-05,  3.2354e-06,  ..., -2.6181e-05,
         -1.2508e-06, -1.6004e-05],
        [-7.2479e-05, -4.8935e-05,  7.2606e-06,  ..., -5.9456e-05,
         -2.8796e-06, -3.6269e-05]], device='cuda:0')
Loss: 1.1606292724609375
Graident accumulation at epoch 0, step 199, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0226,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2769e-04, -8.9509e-05,  1.0790e-05,  ...,  3.6431e-05,
         -1.6515e-04, -7.2961e-05],
        [-2.6246e-05, -1.8782e-05,  3.6022e-06,  ..., -2.1390e-05,
         -1.9848e-06, -1.0046e-05],
        [ 4.3142e-05,  3.0954e-05, -5.2020e-06,  ...,  4.0548e-05,
          2.0428e-07,  1.7751e-05],
        [-2.8942e-05, -1.9151e-05,  5.4256e-06,  ..., -2.3945e-05,
         -2.7787e-06, -1.1535e-05],
        [-5.5138e-05, -3.6984e-05,  8.3843e-06,  ..., -4.4304e-05,
         -4.2168e-06, -2.1923e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2612e-08, 2.8506e-08, 3.5758e-08,  ..., 1.1771e-08, 8.9312e-08,
         1.1566e-08],
        [2.7453e-11, 1.4734e-11, 2.1269e-12,  ..., 1.7053e-11, 9.3690e-13,
         3.4986e-12],
        [2.0072e-10, 1.0454e-10, 6.5087e-12,  ..., 1.6369e-10, 2.7453e-12,
         4.1964e-11],
        [2.0379e-11, 9.6161e-12, 9.2277e-13,  ..., 1.3704e-11, 4.2508e-13,
         2.7235e-12],
        [1.0805e-10, 5.6509e-11, 5.1793e-12,  ..., 7.5949e-11, 2.6796e-12,
         1.2896e-11]], device='cuda:0')
optimizer state dict: 25.0
lr: [1.9776375178987234e-05, 1.9776375178987234e-05]
scheduler_last_epoch: 25


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,   12,  850, 4952],
        [   0, 4350,   14,  ...,  266, 9479,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2385e-06, -2.0429e-05, -3.8474e-05,  ...,  4.8768e-06,
          1.7557e-05,  1.5317e-05],
        [-3.7253e-06, -2.5183e-06,  3.3341e-07,  ..., -3.0845e-06,
         -1.4994e-07, -1.8552e-06],
        [-3.3081e-06, -2.2352e-06,  2.9616e-07,  ..., -2.7418e-06,
         -1.3318e-07, -1.6466e-06],
        [-3.9339e-06, -2.6524e-06,  3.5204e-07,  ..., -3.2485e-06,
         -1.5646e-07, -1.9521e-06],
        [-8.8811e-06, -6.0201e-06,  7.8976e-07,  ..., -7.3612e-06,
         -3.6322e-07, -4.4107e-06]], device='cuda:0')
Loss: 1.163453221321106


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[   0, 6067, 1188,  ..., 5282,  756,  342],
        [   0,  328,  266,  ...,   14, 3352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4235e-05, -5.0084e-05, -6.7153e-05,  ...,  2.0024e-05,
          2.7950e-05,  1.2952e-05],
        [-7.4804e-06, -5.0664e-06,  6.7614e-07,  ..., -6.1542e-06,
         -3.0827e-07, -3.6955e-06],
        [-6.6459e-06, -4.5002e-06,  6.0163e-07,  ..., -5.4538e-06,
         -2.7288e-07, -3.2783e-06],
        [-7.8678e-06, -5.3197e-06,  7.1339e-07,  ..., -6.4671e-06,
         -3.2224e-07, -3.8892e-06],
        [-1.7881e-05, -1.2100e-05,  1.6019e-06,  ..., -1.4663e-05,
         -7.4133e-07, -8.7917e-06]], device='cuda:0')
Loss: 1.184167742729187


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0, 39224,    34,  ...,   401,  1716,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2335e-05, -6.3071e-05, -3.6533e-05,  ...,  4.8004e-05,
         -1.4647e-06,  1.2391e-05],
        [-1.1265e-05, -7.6145e-06,  9.9279e-07,  ..., -9.2387e-06,
         -4.3958e-07, -5.5581e-06],
        [-9.9689e-06, -6.7502e-06,  8.7917e-07,  ..., -8.1509e-06,
         -3.8790e-07, -4.9174e-06],
        [-1.1832e-05, -7.9870e-06,  1.0468e-06,  ..., -9.6858e-06,
         -4.5728e-07, -5.8413e-06],
        [-2.6941e-05, -1.8239e-05,  2.3581e-06,  ..., -2.2054e-05,
         -1.0598e-06, -1.3262e-05]], device='cuda:0')
Loss: 1.1733125448226929


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   266, 27347,  ...,   368,  3367,    13],
        [    0, 11661,    12,  ...,  1707,   394,   264]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8685e-05, -9.6566e-05, -8.3764e-05,  ...,  5.9988e-05,
          3.9773e-06,  2.9780e-05],
        [ 8.0651e-05,  6.4099e-05, -5.4301e-06,  ...,  5.7929e-05,
          1.5663e-05,  2.4592e-05],
        [-1.3262e-05, -8.9854e-06,  1.1697e-06,  ..., -1.0863e-05,
         -5.1549e-07, -6.5789e-06],
        [-1.5765e-05, -1.0639e-05,  1.3951e-06,  ..., -1.2919e-05,
         -6.0815e-07, -7.8231e-06],
        [-3.5822e-05, -2.4229e-05,  3.1330e-06,  ..., -2.9325e-05,
         -1.4026e-06, -1.7703e-05]], device='cuda:0')
Loss: 1.176944375038147


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1976,  1329,  ...,   278,  9469,   292],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9801e-05, -1.0460e-04, -1.2527e-04,  ...,  6.8580e-05,
          2.3087e-05,  3.2160e-05],
        [ 7.6866e-05,  6.1580e-05, -5.0818e-06,  ...,  5.4874e-05,
          1.5527e-05,  2.2714e-05],
        [-1.6615e-05, -1.1221e-05,  1.4789e-06,  ..., -1.3575e-05,
         -6.3563e-07, -8.2478e-06],
        [-1.9729e-05, -1.3292e-05,  1.7639e-06,  ..., -1.6123e-05,
         -7.5065e-07, -9.7901e-06],
        [-4.4823e-05, -3.0249e-05,  3.9600e-06,  ..., -3.6597e-05,
         -1.7304e-06, -2.2173e-05]], device='cuda:0')
Loss: 1.1870214939117432


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0, 1358,  367,  ..., 1758, 2921,   12],
        [   0,  278,  554,  ...,  365, 3125,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5774e-05, -1.0649e-04, -1.1463e-04,  ...,  8.8877e-05,
          3.9539e-05,  4.1324e-05],
        [ 7.3096e-05,  5.9062e-05, -4.7726e-06,  ...,  5.1790e-05,
          1.5414e-05,  2.0844e-05],
        [-1.9923e-05, -1.3426e-05,  1.7509e-06,  ..., -1.6287e-05,
         -7.3574e-07, -9.8944e-06],
        [-2.3633e-05, -1.5914e-05,  2.0880e-06,  ..., -1.9342e-05,
         -8.6706e-07, -1.1742e-05],
        [-5.3823e-05, -3.6240e-05,  4.6976e-06,  ..., -4.3958e-05,
         -2.0005e-06, -2.6643e-05]], device='cuda:0')
Loss: 1.1999824047088623


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0, 4154,   12,  ...,   14,  560,  199],
        [   0,  554, 1034,  ..., 3313,  365,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8940e-05, -1.0296e-04, -1.0521e-04,  ...,  1.1740e-04,
          6.3249e-05,  1.8198e-05],
        [ 6.9326e-05,  5.6514e-05, -4.4262e-06,  ...,  4.8690e-05,
          1.5310e-05,  1.8989e-05],
        [-2.3216e-05, -1.5661e-05,  2.0545e-06,  ..., -1.8999e-05,
         -8.2608e-07, -1.1519e-05],
        [-2.7537e-05, -1.8567e-05,  2.4512e-06,  ..., -2.2560e-05,
         -9.7370e-07, -1.3664e-05],
        [-6.2823e-05, -4.2319e-05,  5.5209e-06,  ..., -5.1349e-05,
         -2.2501e-06, -3.1054e-05]], device='cuda:0')
Loss: 1.188599705696106


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[    0,  3408,   300,  ...,    14,  5870,    12],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1017e-05, -1.2674e-04, -1.4589e-04,  ...,  1.0669e-04,
          6.2859e-05,  7.0057e-06],
        [ 6.5571e-05,  5.3966e-05, -4.1095e-06,  ...,  4.5621e-05,
          1.5169e-05,  1.7119e-05],
        [-2.6524e-05, -1.7911e-05,  2.3358e-06,  ..., -2.1711e-05,
         -9.4902e-07, -1.3173e-05],
        [-3.1471e-05, -2.1249e-05,  2.7884e-06,  ..., -2.5794e-05,
         -1.1199e-06, -1.5631e-05],
        [-7.1764e-05, -4.8399e-05,  6.2771e-06,  ..., -5.8681e-05,
         -2.5835e-06, -3.5524e-05]], device='cuda:0')
Loss: 1.185383677482605
Graident accumulation at epoch 0, step 207, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1402e-04, -9.3232e-05, -4.8776e-06,  ...,  4.3457e-05,
         -1.4235e-04, -6.4965e-05],
        [-1.7064e-05, -1.1507e-05,  2.8310e-06,  ..., -1.4689e-05,
         -2.6941e-07, -7.3293e-06],
        [ 3.6176e-05,  2.6067e-05, -4.4482e-06,  ...,  3.4322e-05,
          8.8947e-08,  1.4659e-05],
        [-2.9195e-05, -1.9361e-05,  5.1619e-06,  ..., -2.4130e-05,
         -2.6128e-06, -1.1944e-05],
        [-5.6800e-05, -3.8126e-05,  8.1735e-06,  ..., -4.5741e-05,
         -4.0535e-06, -2.3283e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2577e-08, 2.8494e-08, 3.5744e-08,  ..., 1.1771e-08, 8.9227e-08,
         1.1554e-08],
        [3.1725e-11, 1.7632e-11, 2.1417e-12,  ..., 1.9117e-11, 1.1661e-12,
         3.7882e-12],
        [2.0122e-10, 1.0475e-10, 6.5076e-12,  ..., 1.6400e-10, 2.7435e-12,
         4.2095e-11],
        [2.1349e-11, 1.0058e-11, 9.2962e-13,  ..., 1.4355e-11, 4.2591e-13,
         2.9651e-12],
        [1.1310e-10, 5.8795e-11, 5.2136e-12,  ..., 7.9317e-11, 2.6836e-12,
         1.4145e-11]], device='cuda:0')
optimizer state dict: 26.0
lr: [1.974963266376872e-05, 1.974963266376872e-05]
scheduler_last_epoch: 26


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 33792,   352,  ...,   278,   546, 30495],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3176e-05, -3.4477e-05,  2.0018e-06,  ...,  3.3099e-06,
         -1.8806e-05, -3.3498e-05],
        [-3.6806e-06, -2.5034e-06,  3.1479e-07,  ..., -2.9951e-06,
         -9.4995e-08, -1.7956e-06],
        [-3.3081e-06, -2.2501e-06,  2.8312e-07,  ..., -2.6822e-06,
         -8.5216e-08, -1.6093e-06],
        [-3.9041e-06, -2.6524e-06,  3.3528e-07,  ..., -3.1590e-06,
         -9.9186e-08, -1.8999e-06],
        [-8.7619e-06, -5.9605e-06,  7.4878e-07,  ..., -7.1228e-06,
         -2.2817e-07, -4.2617e-06]], device='cuda:0')
Loss: 1.1751855611801147


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,    22,  2577,  ...,  4970,     9,  3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1131e-06, -3.7600e-05, -1.8727e-05,  ...,  2.0205e-05,
          8.6319e-06, -3.0733e-05],
        [-7.3612e-06, -5.0068e-06,  6.3702e-07,  ..., -6.0350e-06,
         -2.1793e-07, -3.6433e-06],
        [-6.6459e-06, -4.5300e-06,  5.7742e-07,  ..., -5.4538e-06,
         -1.9651e-07, -3.2857e-06],
        [-7.7784e-06, -5.2899e-06,  6.7800e-07,  ..., -6.3628e-06,
         -2.2771e-07, -3.8370e-06],
        [-1.7524e-05, -1.1891e-05,  1.5087e-06,  ..., -1.4335e-05,
         -5.2061e-07, -8.6427e-06]], device='cuda:0')
Loss: 1.188888430595398


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   409, 15720,  ...,    12,   287,  2350],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6167e-05, -5.9100e-05, -4.2560e-05,  ...,  5.0853e-06,
         -2.6821e-06, -2.1530e-05],
        [-1.1042e-05, -7.5251e-06,  9.7416e-07,  ..., -9.0748e-06,
         -3.9209e-07, -5.5134e-06],
        [-9.9987e-06, -6.8247e-06,  8.8476e-07,  ..., -8.2254e-06,
         -3.5577e-07, -4.9919e-06],
        [-1.1653e-05, -7.9572e-06,  1.0375e-06,  ..., -9.5665e-06,
         -4.1118e-07, -5.8040e-06],
        [-2.6345e-05, -1.7941e-05,  2.3171e-06,  ..., -2.1636e-05,
         -9.4343e-07, -1.3113e-05]], device='cuda:0')
Loss: 1.1995048522949219


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,   944,   278,  ..., 17330,  1683,   360],
        [    0,    13,  7805,  ...,  2733,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1959e-05, -5.0089e-05, -3.9173e-05,  ...,  2.0360e-05,
          1.2598e-05, -4.2802e-06],
        [-1.4663e-05, -1.0028e-05,  1.2778e-06,  ..., -1.2055e-05,
         -5.1502e-07, -7.3314e-06],
        [-1.3322e-05, -9.1344e-06,  1.1642e-06,  ..., -1.0952e-05,
         -4.6939e-07, -6.6608e-06],
        [-1.5497e-05, -1.0639e-05,  1.3635e-06,  ..., -1.2740e-05,
         -5.4343e-07, -7.7412e-06],
        [-3.5048e-05, -2.3991e-05,  3.0436e-06,  ..., -2.8789e-05,
         -1.2415e-06, -1.7494e-05]], device='cuda:0')
Loss: 1.1817182302474976


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[    0,   344, 14017,  ...,    65,   298,   634],
        [    0,  5699,    20,  ...,  3502,  2051,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2393e-07, -7.1213e-05, -5.1243e-05,  ..., -1.2700e-05,
          2.0652e-05, -3.1057e-05],
        [-1.8373e-05, -1.2532e-05,  1.5758e-06,  ..., -1.5095e-05,
         -6.3982e-07, -9.1419e-06],
        [-1.6659e-05, -1.1384e-05,  1.4324e-06,  ..., -1.3694e-05,
         -5.8208e-07, -8.2925e-06],
        [-1.9372e-05, -1.3262e-05,  1.6782e-06,  ..., -1.5929e-05,
         -6.7381e-07, -9.6411e-06],
        [-4.3809e-05, -2.9922e-05,  3.7439e-06,  ..., -3.6001e-05,
         -1.5395e-06, -2.1785e-05]], device='cuda:0')
Loss: 1.1958709955215454


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   685,  2461,  ...,   287,   298,  7943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5287e-05, -1.0474e-04, -7.4867e-05,  ..., -2.6377e-06,
          2.3574e-05, -4.0249e-05],
        [-2.2024e-05, -1.5020e-05,  1.8924e-06,  ..., -1.8135e-05,
         -7.8604e-07, -1.0945e-05],
        [-1.9968e-05, -1.3635e-05,  1.7211e-06,  ..., -1.6466e-05,
         -7.1526e-07, -9.9316e-06],
        [-2.3186e-05, -1.5870e-05,  2.0117e-06,  ..., -1.9118e-05,
         -8.2562e-07, -1.1526e-05],
        [-5.2512e-05, -3.5852e-05,  4.4964e-06,  ..., -4.3273e-05,
         -1.8897e-06, -2.6077e-05]], device='cuda:0')
Loss: 1.1927465200424194


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[   0,  270,  472,  ...,  292,   73,   14],
        [   0,  265, 1781,  ...,  334,  344,  984]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8710e-05, -6.3793e-05, -8.3378e-05,  ...,  2.8301e-05,
         -3.7609e-06, -5.3491e-05],
        [-2.5660e-05, -1.7524e-05,  2.2184e-06,  ..., -2.1204e-05,
         -9.5088e-07, -1.2778e-05],
        [-2.3246e-05, -1.5885e-05,  2.0154e-06,  ..., -1.9222e-05,
         -8.6427e-07, -1.1586e-05],
        [-2.6986e-05, -1.8477e-05,  2.3544e-06,  ..., -2.2307e-05,
         -9.9698e-07, -1.3433e-05],
        [-6.1274e-05, -4.1842e-05,  5.2787e-06,  ..., -5.0634e-05,
         -2.2883e-06, -3.0488e-05]], device='cuda:0')
Loss: 1.192609190940857


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,   12,  221,  ...,  593,  360,  726],
        [   0,   14,  333,  ...,  328, 5453, 4713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6799e-05, -6.8646e-05, -9.0644e-05,  ...,  3.8184e-05,
          7.2399e-06, -3.4314e-05],
        [-2.9311e-05, -2.0027e-05,  2.5202e-06,  ..., -2.4185e-05,
         -1.0980e-06, -1.4596e-05],
        [-2.6584e-05, -1.8165e-05,  2.2929e-06,  ..., -2.1949e-05,
         -9.9838e-07, -1.3247e-05],
        [-3.0860e-05, -2.1130e-05,  2.6785e-06,  ..., -2.5466e-05,
         -1.1516e-06, -1.5356e-05],
        [-7.0035e-05, -4.7833e-05,  6.0052e-06,  ..., -5.7787e-05,
         -2.6440e-06, -3.4839e-05]], device='cuda:0')
Loss: 1.1863031387329102
Graident accumulation at epoch 0, step 215, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.9730e-04, -9.0773e-05, -1.3454e-05,  ...,  4.2929e-05,
         -1.2739e-04, -6.1900e-05],
        [-1.8289e-05, -1.2359e-05,  2.7999e-06,  ..., -1.5638e-05,
         -3.5227e-07, -8.0559e-06],
        [ 2.9900e-05,  2.1644e-05, -3.7741e-06,  ...,  2.8695e-05,
         -1.9785e-08,  1.1868e-05],
        [-2.9362e-05, -1.9538e-05,  4.9136e-06,  ..., -2.4263e-05,
         -2.4667e-06, -1.2285e-05],
        [-5.8124e-05, -3.9096e-05,  7.9567e-06,  ..., -4.6946e-05,
         -3.9125e-06, -2.4439e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2537e-08, 2.8470e-08, 3.5716e-08,  ..., 1.1760e-08, 8.9138e-08,
         1.1544e-08],
        [3.2553e-11, 1.8015e-11, 2.1459e-12,  ..., 1.9683e-11, 1.1661e-12,
         3.9975e-12],
        [2.0173e-10, 1.0498e-10, 6.5064e-12,  ..., 1.6432e-10, 2.7418e-12,
         4.2229e-11],
        [2.2280e-11, 1.0494e-11, 9.3586e-13,  ..., 1.4990e-11, 4.2681e-13,
         3.1979e-12],
        [1.1789e-10, 6.1025e-11, 5.2444e-12,  ..., 8.2577e-11, 2.6879e-12,
         1.5345e-11]], device='cuda:0')
optimizer state dict: 27.0
lr: [1.972140031957344e-05, 1.972140031957344e-05]
scheduler_last_epoch: 27


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[    0,  1690,  2558,  ...,  2025,    12,   266],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4867e-05, -3.2973e-05, -2.0042e-05,  ..., -8.4288e-06,
          3.1569e-05, -3.9779e-06],
        [-3.6061e-06, -2.4587e-06,  3.0734e-07,  ..., -2.9653e-06,
         -1.5553e-07, -1.7956e-06],
        [-3.3528e-06, -2.2948e-06,  2.8498e-07,  ..., -2.7567e-06,
         -1.4529e-07, -1.6689e-06],
        [-3.7998e-06, -2.5928e-06,  3.2410e-07,  ..., -3.1143e-06,
         -1.6298e-07, -1.8850e-06],
        [-8.7023e-06, -5.9307e-06,  7.3388e-07,  ..., -7.1228e-06,
         -3.7812e-07, -4.3213e-06]], device='cuda:0')
Loss: 1.1919078826904297


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[    0,   709,   630,  ...,  6263,   409,   508],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4377e-05, -3.9679e-05,  3.2808e-06,  ..., -3.5445e-06,
          2.2875e-05, -4.6812e-07],
        [-7.1973e-06, -4.9323e-06,  6.4261e-07,  ..., -5.9158e-06,
         -3.1013e-07, -3.5539e-06],
        [-6.6906e-06, -4.6045e-06,  5.9605e-07,  ..., -5.4836e-06,
         -2.8871e-07, -3.3006e-06],
        [-7.5996e-06, -5.2154e-06,  6.8173e-07,  ..., -6.2287e-06,
         -3.2689e-07, -3.7476e-06],
        [-1.7226e-05, -1.1832e-05,  1.5236e-06,  ..., -1.4126e-05,
         -7.4692e-07, -8.4937e-06]], device='cuda:0')
Loss: 1.1728084087371826


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   292,    58,  ...,   319,   221,  1061],
        [    0,   344, 10706,  ...,  1184,   578,   825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2920e-05, -2.5643e-05,  1.0683e-05,  ...,  3.1693e-05,
          3.7728e-06,  4.1526e-06],
        [-1.0803e-05, -7.4059e-06,  1.0058e-06,  ..., -8.9109e-06,
         -4.8801e-07, -5.3272e-06],
        [-1.0028e-05, -6.8843e-06,  9.3319e-07,  ..., -8.2552e-06,
         -4.5449e-07, -4.9397e-06],
        [-1.1414e-05, -7.8231e-06,  1.0692e-06,  ..., -9.3877e-06,
         -5.1316e-07, -5.6177e-06],
        [-2.5749e-05, -1.7673e-05,  2.3805e-06,  ..., -2.1219e-05,
         -1.1660e-06, -1.2696e-05]], device='cuda:0')
Loss: 1.1713662147521973


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[   0, 6541,  287,  ..., 1061, 4786,  292],
        [   0,  285,  590,  ...,  199,  395, 3523]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2908e-06,  1.0605e-05,  1.9633e-05,  ...,  3.4473e-05,
          3.8620e-05,  4.1725e-05],
        [-1.4409e-05, -9.8944e-06,  1.3392e-06,  ..., -1.1906e-05,
         -6.9477e-07, -7.1079e-06],
        [-1.3351e-05, -9.1791e-06,  1.2424e-06,  ..., -1.1012e-05,
         -6.4448e-07, -6.5863e-06],
        [-1.5199e-05, -1.0431e-05,  1.4212e-06,  ..., -1.2532e-05,
         -7.2829e-07, -7.4878e-06],
        [-3.4213e-05, -2.3514e-05,  3.1590e-06,  ..., -2.8253e-05,
         -1.6503e-06, -1.6868e-05]], device='cuda:0')
Loss: 1.162642002105713


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2537e-05,  6.7227e-07,  4.0687e-06,  ...,  2.1371e-05,
          1.0812e-05,  3.2933e-05],
        [-1.8016e-05, -1.2368e-05,  1.6969e-06,  ..., -1.4901e-05,
         -8.6054e-07, -8.8960e-06],
        [-1.6659e-05, -1.1444e-05,  1.5702e-06,  ..., -1.3769e-05,
         -7.9628e-07, -8.2254e-06],
        [-1.8969e-05, -1.3009e-05,  1.7956e-06,  ..., -1.5676e-05,
         -8.9966e-07, -9.3505e-06],
        [-4.2737e-05, -2.9355e-05,  3.9972e-06,  ..., -3.5346e-05,
         -2.0415e-06, -2.1070e-05]], device='cuda:0')
Loss: 1.1739205121994019


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0,    83,    12,  ...,  3781,   292, 27247]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1382e-05, -2.4112e-06,  2.1762e-05,  ...,  2.5780e-05,
          2.5016e-05,  4.2275e-05],
        [-2.1696e-05, -1.4856e-05,  2.0266e-06,  ..., -1.7911e-05,
         -1.0710e-06, -1.0736e-05],
        [-2.0057e-05, -1.3754e-05,  1.8775e-06,  ..., -1.6570e-05,
         -9.9093e-07, -9.9242e-06],
        [-2.2843e-05, -1.5631e-05,  2.1458e-06,  ..., -1.8850e-05,
         -1.1204e-06, -1.1288e-05],
        [-5.1558e-05, -3.5316e-05,  4.7870e-06,  ..., -4.2588e-05,
         -2.5481e-06, -2.5481e-05]], device='cuda:0')
Loss: 1.1923210620880127


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[    0,   298,   369,  ...,  5936,   968,   259],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5031e-05, -8.6540e-07, -9.3530e-06,  ...,  4.1999e-05,
          3.5082e-05,  3.8496e-06],
        [-2.5272e-05, -1.7330e-05,  2.3432e-06,  ..., -2.0906e-05,
         -1.2349e-06, -1.2517e-05],
        [-2.3335e-05, -1.6019e-05,  2.1700e-06,  ..., -1.9312e-05,
         -1.1427e-06, -1.1563e-05],
        [-2.6613e-05, -1.8239e-05,  2.4829e-06,  ..., -2.2009e-05,
         -1.2927e-06, -1.3165e-05],
        [-6.0022e-05, -4.1157e-05,  5.5321e-06,  ..., -4.9651e-05,
         -2.9374e-06, -2.9683e-05]], device='cuda:0')
Loss: 1.1857447624206543


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 1276,  292,  ...,   83, 1837,   13],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7551e-05,  6.3443e-07, -3.9632e-05,  ...,  7.4279e-05,
          6.9536e-05,  8.7938e-06],
        [-2.8908e-05, -1.9774e-05,  2.6785e-06,  ..., -2.3842e-05,
         -1.3569e-06, -1.4335e-05],
        [-2.6673e-05, -1.8269e-05,  2.4773e-06,  ..., -2.2009e-05,
         -1.2536e-06, -1.3225e-05],
        [-3.0518e-05, -2.0847e-05,  2.8424e-06,  ..., -2.5153e-05,
         -1.4193e-06, -1.5102e-05],
        [-6.8724e-05, -4.6998e-05,  6.3255e-06,  ..., -5.6654e-05,
         -3.2261e-06, -3.4004e-05]], device='cuda:0')
Loss: 1.1804741621017456
Graident accumulation at epoch 0, step 223, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.7582e-04, -8.1632e-05, -1.6072e-05,  ...,  4.6064e-05,
         -1.0770e-04, -5.4830e-05],
        [-1.9351e-05, -1.3101e-05,  2.7878e-06,  ..., -1.6459e-05,
         -4.5274e-07, -8.6838e-06],
        [ 2.4242e-05,  1.7653e-05, -3.1489e-06,  ...,  2.3624e-05,
         -1.4316e-07,  9.3588e-06],
        [-2.9477e-05, -1.9669e-05,  4.7065e-06,  ..., -2.4352e-05,
         -2.3619e-06, -1.2567e-05],
        [-5.9184e-05, -3.9887e-05,  7.7936e-06,  ..., -4.7917e-05,
         -3.8439e-06, -2.5395e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2495e-08, 2.8441e-08, 3.5682e-08,  ..., 1.1754e-08, 8.9053e-08,
         1.1532e-08],
        [3.3356e-11, 1.8388e-11, 2.1509e-12,  ..., 2.0231e-11, 1.1668e-12,
         4.1989e-12],
        [2.0224e-10, 1.0521e-10, 6.5060e-12,  ..., 1.6464e-10, 2.7406e-12,
         4.2361e-11],
        [2.3189e-11, 1.0919e-11, 9.4301e-13,  ..., 1.5607e-11, 4.2840e-13,
         3.4228e-12],
        [1.2249e-10, 6.3172e-11, 5.2792e-12,  ..., 8.5704e-11, 2.6957e-12,
         1.6486e-11]], device='cuda:0')
optimizer state dict: 28.0
lr: [1.9691682460550022e-05, 1.9691682460550022e-05]
scheduler_last_epoch: 28


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0,   199,   769,  ..., 12038, 15317,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1211e-05,  9.9412e-06,  2.3962e-06,  ..., -4.6237e-05,
          2.2088e-05, -2.7460e-05],
        [-3.5465e-06, -2.4438e-06,  3.2224e-07,  ..., -2.9206e-06,
         -1.9465e-07, -1.7211e-06],
        [-3.3230e-06, -2.2948e-06,  3.0175e-07,  ..., -2.7418e-06,
         -1.8254e-07, -1.6168e-06],
        [-3.7402e-06, -2.5779e-06,  3.4086e-07,  ..., -3.0845e-06,
         -2.0396e-07, -1.8179e-06],
        [-8.4043e-06, -5.7817e-06,  7.5623e-07,  ..., -6.9141e-06,
         -4.6194e-07, -4.0829e-06]], device='cuda:0')
Loss: 1.1967830657958984


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[    0,  1128,  3231,  ...,  8375,   199,  2038],
        [    0, 27342,    17,  ...,  5125,  3244,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3523e-07,  3.3294e-05,  3.9901e-06,  ..., -4.3986e-05,
          1.2250e-05, -2.1646e-05],
        [-7.0781e-06, -4.8727e-06,  6.5006e-07,  ..., -5.8264e-06,
         -4.2003e-07, -3.4794e-06],
        [-6.6608e-06, -4.5747e-06,  6.1095e-07,  ..., -5.4836e-06,
         -3.9395e-07, -3.2708e-06],
        [-7.4506e-06, -5.1260e-06,  6.8918e-07,  ..., -6.1393e-06,
         -4.3865e-07, -3.6582e-06],
        [-1.6809e-05, -1.1533e-05,  1.5311e-06,  ..., -1.3828e-05,
         -9.9465e-07, -8.2552e-06]], device='cuda:0')
Loss: 1.1821467876434326


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[   0,  365, 5392,  ...,   14,  333,  199],
        [   0,  271,  266,  ...,  984,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2394e-05,  3.5160e-05,  3.5089e-05,  ..., -3.4414e-05,
         -6.1157e-06, -5.8847e-05],
        [-1.0639e-05, -7.3016e-06,  9.7975e-07,  ..., -8.7172e-06,
         -6.3051e-07, -5.2229e-06],
        [-1.0028e-05, -6.8694e-06,  9.2387e-07,  ..., -8.2105e-06,
         -5.9418e-07, -4.9174e-06],
        [-1.1235e-05, -7.7039e-06,  1.0412e-06,  ..., -9.2089e-06,
         -6.6031e-07, -5.5060e-06],
        [-2.5332e-05, -1.7315e-05,  2.3209e-06,  ..., -2.0713e-05,
         -1.4976e-06, -1.2398e-05]], device='cuda:0')
Loss: 1.1746701002120972


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[   0, 6294,  367,  ...,  496,   14,   18],
        [   0,  417,  199,  ...,   13,   20, 6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1440e-05,  3.5976e-05,  6.4697e-05,  ..., -5.5658e-05,
          1.5405e-05, -5.4207e-05],
        [-1.4260e-05, -9.7752e-06,  1.3039e-06,  ..., -1.1683e-05,
         -8.5589e-07, -6.9663e-06],
        [-1.3411e-05, -9.1791e-06,  1.2293e-06,  ..., -1.0982e-05,
         -8.0466e-07, -6.5565e-06],
        [-1.4991e-05, -1.0267e-05,  1.3802e-06,  ..., -1.2293e-05,
         -8.9221e-07, -7.3239e-06],
        [-3.3855e-05, -2.3127e-05,  3.0771e-06,  ..., -2.7686e-05,
         -2.0266e-06, -1.6481e-05]], device='cuda:0')
Loss: 1.1839121580123901


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  2958,   298,  ...,    12,   709,   616],
        [    0, 48705,   292,  ...,   266,  2548,  2697]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3331e-05,  4.1046e-05,  4.5671e-05,  ..., -6.2395e-05,
         -2.6471e-06, -2.8283e-05],
        [-1.7837e-05, -1.2204e-05,  1.6876e-06,  ..., -1.4603e-05,
         -1.0831e-06, -8.7693e-06],
        [-1.6794e-05, -1.1489e-05,  1.5944e-06,  ..., -1.3754e-05,
         -1.0198e-06, -8.2627e-06],
        [-1.8731e-05, -1.2800e-05,  1.7844e-06,  ..., -1.5348e-05,
         -1.1288e-06, -9.2089e-06],
        [-4.2319e-05, -2.8849e-05,  3.9749e-06,  ..., -3.4571e-05,
         -2.5630e-06, -2.0713e-05]], device='cuda:0')
Loss: 1.152995228767395


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,   381,  1795,  ...,    12,   344,   593]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0612e-05,  5.5934e-05,  4.3232e-05,  ..., -7.1995e-05,
         -4.2035e-06, -1.5261e-05],
        [-2.1428e-05, -1.4648e-05,  1.9968e-06,  ..., -1.7539e-05,
         -1.3141e-06, -1.0498e-05],
        [-2.0191e-05, -1.3798e-05,  1.8869e-06,  ..., -1.6540e-05,
         -1.2368e-06, -9.8944e-06],
        [-2.2501e-05, -1.5378e-05,  2.1122e-06,  ..., -1.8448e-05,
         -1.3690e-06, -1.1027e-05],
        [-5.0843e-05, -3.4660e-05,  4.7088e-06,  ..., -4.1574e-05,
         -3.1143e-06, -2.4825e-05]], device='cuda:0')
Loss: 1.1886335611343384


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,   471,   590,  ...,  5007,    13,  2920],
        [    0, 47684,   292,  ...,   287, 49958, 22022]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7080e-05,  5.5934e-05,  8.2224e-05,  ..., -5.1440e-05,
          2.2507e-05,  8.3935e-06],
        [-2.4989e-05, -1.7092e-05,  2.3469e-06,  ..., -2.0429e-05,
         -1.5749e-06, -1.2241e-05],
        [-2.3574e-05, -1.6123e-05,  2.2203e-06,  ..., -1.9282e-05,
         -1.4827e-06, -1.1548e-05],
        [-2.6256e-05, -1.7956e-05,  2.4848e-06,  ..., -2.1487e-05,
         -1.6429e-06, -1.2860e-05],
        [-5.9247e-05, -4.0442e-05,  5.5321e-06,  ..., -4.8399e-05,
         -3.7290e-06, -2.8938e-05]], device='cuda:0')
Loss: 1.1638119220733643


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[   0,   12,  630,  ..., 5049,   14, 2371],
        [   0,   18,   14,  ...,  300,  275, 1184]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0166e-05,  3.5163e-05,  1.0167e-04,  ..., -4.4165e-05,
          5.7614e-05,  4.1058e-05],
        [-2.8580e-05, -1.9595e-05,  2.7008e-06,  ..., -2.3350e-05,
         -1.8151e-06, -1.4007e-05],
        [ 4.5532e-05,  5.2749e-05,  5.9698e-06,  ...,  5.0755e-05,
          1.7130e-05,  4.4550e-05],
        [-3.0041e-05, -2.0593e-05,  2.8592e-06,  ..., -2.4572e-05,
         -1.8943e-06, -1.4722e-05],
        [-6.7830e-05, -4.6402e-05,  6.3702e-06,  ..., -5.5373e-05,
         -4.3064e-06, -3.3140e-05]], device='cuda:0')
Loss: 1.1863635778427124
Graident accumulation at epoch 0, step 231, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0205],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0403,  ...,  0.0227,  0.0066, -0.0015],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6325e-04, -6.9953e-05, -4.2978e-06,  ...,  3.7041e-05,
         -9.1167e-05, -4.5241e-05],
        [-2.0274e-05, -1.3750e-05,  2.7791e-06,  ..., -1.7148e-05,
         -5.8898e-07, -9.2161e-06],
        [ 2.6371e-05,  2.1162e-05, -2.2371e-06,  ...,  2.6338e-05,
          1.5842e-06,  1.2878e-05],
        [-2.9534e-05, -1.9761e-05,  4.5217e-06,  ..., -2.4374e-05,
         -2.3152e-06, -1.2783e-05],
        [-6.0049e-05, -4.0538e-05,  7.6513e-06,  ..., -4.8662e-05,
         -3.8901e-06, -2.6170e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2455e-08, 2.8414e-08, 3.5657e-08,  ..., 1.1744e-08, 8.8968e-08,
         1.1523e-08],
        [3.4139e-11, 1.8754e-11, 2.1561e-12,  ..., 2.0756e-11, 1.1689e-12,
         4.3909e-12],
        [2.0411e-10, 1.0789e-10, 6.5351e-12,  ..., 1.6705e-10, 3.0313e-12,
         4.4304e-11],
        [2.4068e-11, 1.1332e-11, 9.5024e-13,  ..., 1.6195e-11, 4.3156e-13,
         3.6362e-12],
        [1.2697e-10, 6.5262e-11, 5.3145e-12,  ..., 8.8684e-11, 2.7115e-12,
         1.7568e-11]], device='cuda:0')
optimizer state dict: 29.0
lr: [1.9660483627846746e-05, 1.9660483627846746e-05]
scheduler_last_epoch: 29


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[   0, 7094,  596,  ..., 4764, 9514,   14],
        [   0, 3377,   12,  ...,  333,  199,  769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4282e-06, -1.2499e-05,  9.8732e-06,  ...,  2.6272e-05,
         -1.1755e-08, -2.8948e-05],
        [-3.5167e-06, -2.4289e-06,  3.9302e-07,  ..., -2.8759e-06,
         -3.1106e-07, -1.6838e-06],
        [-3.4124e-06, -2.3693e-06,  3.8184e-07,  ..., -2.8014e-06,
         -3.0361e-07, -1.6391e-06],
        [-3.7104e-06, -2.5779e-06,  4.1910e-07,  ..., -3.0547e-06,
         -3.2783e-07, -1.7807e-06],
        [-8.3447e-06, -5.7817e-06,  9.2760e-07,  ..., -6.8247e-06,
         -7.3761e-07, -3.9935e-06]], device='cuda:0')
Loss: 1.1673489809036255


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,   266, 10726,  ..., 13973, 22191, 15913],
        [    0,   298,   301,  ...,    13, 10308,  2129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4605e-05,  1.1153e-05, -1.6312e-06,  ...,  3.5015e-05,
          5.9888e-06,  3.0599e-05],
        [-7.0184e-06, -4.8578e-06,  7.5251e-07,  ..., -5.7667e-06,
         -6.5006e-07, -3.3677e-06],
        [-6.7949e-06, -4.7088e-06,  7.2829e-07,  ..., -5.5879e-06,
         -6.2957e-07, -3.2634e-06],
        [-7.3612e-06, -5.0962e-06,  7.9535e-07,  ..., -6.0648e-06,
         -6.7987e-07, -3.5316e-06],
        [-1.6630e-05, -1.1504e-05,  1.7695e-06,  ..., -1.3649e-05,
         -1.5385e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.1758050918579102


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[    0,    20,  2637,  ..., 14044,     9,   292],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8017e-05, -4.6810e-06, -4.4078e-05,  ...,  6.3479e-06,
          5.7639e-05,  7.5368e-06],
        [-1.0505e-05, -7.3165e-06,  1.1679e-06,  ..., -8.6874e-06,
         -9.4436e-07, -5.0664e-06],
        [-1.0118e-05, -7.0482e-06,  1.1232e-06,  ..., -8.3596e-06,
         -9.0897e-07, -4.8801e-06],
        [-1.0997e-05, -7.6592e-06,  1.2312e-06,  ..., -9.1046e-06,
         -9.8534e-07, -5.2974e-06],
        [-2.4676e-05, -1.7196e-05,  2.7232e-06,  ..., -2.0385e-05,
         -2.2203e-06, -1.1861e-05]], device='cuda:0')
Loss: 1.1960432529449463


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,    67,   695,  ...,   437,   266, 44563],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6695e-05,  1.4220e-07, -3.2534e-05,  ...,  2.2785e-05,
          4.0539e-05,  1.5776e-05],
        [-1.3992e-05, -9.7156e-06,  1.5181e-06,  ..., -1.1489e-05,
         -1.2331e-06, -6.7204e-06],
        [-1.3515e-05, -9.3877e-06,  1.4640e-06,  ..., -1.1086e-05,
         -1.1884e-06, -6.4895e-06],
        [-1.4663e-05, -1.0177e-05,  1.6000e-06,  ..., -1.2055e-05,
         -1.2871e-06, -7.0333e-06],
        [-3.2961e-05, -2.2888e-05,  3.5502e-06,  ..., -2.7031e-05,
         -2.9020e-06, -1.5765e-05]], device='cuda:0')
Loss: 1.1741993427276611


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2132e-05,  1.9867e-05, -6.5692e-06,  ..., -1.1076e-05,
          5.2273e-05,  3.7848e-05],
        [-1.7554e-05, -1.2189e-05,  1.8757e-06,  ..., -1.4380e-05,
         -1.5087e-06, -8.4192e-06],
        [-1.6928e-05, -1.1757e-05,  1.8086e-06,  ..., -1.3858e-05,
         -1.4529e-06, -8.1211e-06],
        [-1.8373e-05, -1.2755e-05,  1.9744e-06,  ..., -1.5065e-05,
         -1.5739e-06, -8.8066e-06],
        [-4.1306e-05, -2.8700e-05,  4.3847e-06,  ..., -3.3796e-05,
         -3.5502e-06, -1.9759e-05]], device='cuda:0')
Loss: 1.1930264234542847


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[   0, 3665, 1419,  ...,  600,  847,  328],
        [   0,  352,  927,  ..., 1521, 3513,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0039e-05,  5.6080e-05, -3.0997e-05,  ..., -3.1982e-05,
          6.2387e-05,  6.2201e-05],
        [-2.1055e-05, -1.4588e-05,  2.2333e-06,  ..., -1.7256e-05,
         -1.7826e-06, -1.0088e-05],
        [-2.0266e-05, -1.4052e-05,  2.1495e-06,  ..., -1.6600e-05,
         -1.7136e-06, -9.7156e-06],
        [-2.2084e-05, -1.5303e-05,  2.3562e-06,  ..., -1.8105e-05,
         -1.8626e-06, -1.0572e-05],
        [-4.9472e-05, -3.4302e-05,  5.2154e-06,  ..., -4.0501e-05,
         -4.1872e-06, -2.3663e-05]], device='cuda:0')
Loss: 1.1773045063018799


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,  7555,  3908,  ...,   259,  8477,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0853e-05,  4.4881e-05, -1.1463e-05,  ..., -3.1104e-05,
          8.7153e-05,  7.4067e-05],
        [-2.4617e-05, -1.7047e-05,  2.6058e-06,  ..., -2.0146e-05,
         -2.0601e-06, -1.1779e-05],
        [-2.3693e-05, -1.6436e-05,  2.5090e-06,  ..., -1.9386e-05,
         -1.9819e-06, -1.1355e-05],
        [-2.5794e-05, -1.7881e-05,  2.7474e-06,  ..., -2.1115e-05,
         -2.1514e-06, -1.2338e-05],
        [-5.7817e-05, -4.0084e-05,  6.0834e-06,  ..., -4.7266e-05,
         -4.8392e-06, -2.7627e-05]], device='cuda:0')
Loss: 1.1660839319229126


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0, 1529, 5227,  ..., 1480,  367,  925],
        [   0, 3804,  300,  ..., 5062, 9848, 3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8615e-05,  4.4729e-05,  1.6133e-05,  ..., -1.3216e-05,
          1.1054e-04,  9.9554e-05],
        [-2.8118e-05, -1.9491e-05,  2.9616e-06,  ..., -2.3022e-05,
         -2.3469e-06, -1.3456e-05],
        [-2.7061e-05, -1.8790e-05,  2.8536e-06,  ..., -2.2158e-05,
         -2.2594e-06, -1.2971e-05],
        [-2.9460e-05, -2.0444e-05,  3.1237e-06,  ..., -2.4140e-05,
         -2.4512e-06, -1.4096e-05],
        [-6.6042e-05, -4.5836e-05,  6.9179e-06,  ..., -5.4061e-05,
         -5.5172e-06, -3.1590e-05]], device='cuda:0')
Loss: 1.1867514848709106
Graident accumulation at epoch 0, step 239, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0338],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0227,  0.0066, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5579e-04, -5.8485e-05, -2.2547e-06,  ...,  3.2016e-05,
         -7.0996e-05, -3.0762e-05],
        [-2.1058e-05, -1.4324e-05,  2.7974e-06,  ..., -1.7735e-05,
         -7.6477e-07, -9.6401e-06],
        [ 2.1028e-05,  1.7167e-05, -1.7280e-06,  ...,  2.1488e-05,
          1.1998e-06,  1.0293e-05],
        [-2.9526e-05, -1.9830e-05,  4.3819e-06,  ..., -2.4351e-05,
         -2.3288e-06, -1.2914e-05],
        [-6.0648e-05, -4.1068e-05,  7.5779e-06,  ..., -4.9202e-05,
         -4.0528e-06, -2.6712e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2420e-08, 2.8388e-08, 3.5621e-08,  ..., 1.1733e-08, 8.8891e-08,
         1.1521e-08],
        [3.4896e-11, 1.9115e-11, 2.1627e-12,  ..., 2.1266e-11, 1.1732e-12,
         4.5676e-12],
        [2.0463e-10, 1.0813e-10, 6.5367e-12,  ..., 1.6737e-10, 3.0334e-12,
         4.4428e-11],
        [2.4912e-11, 1.1738e-11, 9.5905e-13,  ..., 1.6762e-11, 4.3714e-13,
         3.8312e-12],
        [1.3121e-10, 6.7298e-11, 5.3570e-12,  ..., 9.1518e-11, 2.7392e-12,
         1.8548e-11]], device='cuda:0')
optimizer state dict: 30.0
lr: [1.9627808588917577e-05, 1.9627808588917577e-05]
scheduler_last_epoch: 30


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0, 18197,  1340,  ...,   360,   266,  1110],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6826e-05,  5.0652e-05, -2.3365e-05,  ...,  3.6158e-06,
          5.5605e-06,  1.8476e-05],
        [-3.4124e-06, -2.4140e-06,  4.2841e-07,  ..., -2.8461e-06,
         -3.6694e-07, -1.6242e-06],
        [-3.3230e-06, -2.3544e-06,  4.1910e-07,  ..., -2.7716e-06,
         -3.5577e-07, -1.5795e-06],
        [-3.5465e-06, -2.5034e-06,  4.4890e-07,  ..., -2.9504e-06,
         -3.7998e-07, -1.6838e-06],
        [-7.9274e-06, -5.6028e-06,  9.9093e-07,  ..., -6.5863e-06,
         -8.4937e-07, -3.7551e-06]], device='cuda:0')
Loss: 1.1946337223052979


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0, 37312,    12,  ...,   278,   795, 40854],
        [    0,   300, 11040,  ...,   266,  1736,  3487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4994e-05,  6.4629e-05, -3.6205e-05,  ..., -2.3510e-05,
         -3.8850e-06,  4.0358e-05],
        [-6.8694e-06, -4.8131e-06,  8.3260e-07,  ..., -5.6624e-06,
         -7.2829e-07, -3.2336e-06],
        [-6.6906e-06, -4.6939e-06,  8.1398e-07,  ..., -5.5134e-06,
         -7.0594e-07, -3.1441e-06],
        [-7.1377e-06, -5.0068e-06,  8.7172e-07,  ..., -5.8711e-06,
         -7.5251e-07, -3.3528e-06],
        [-1.5974e-05, -1.1206e-05,  1.9334e-06,  ..., -1.3143e-05,
         -1.6876e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1809028387069702


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,  1941,   437,  ..., 16539,  4129,  4156]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8612e-05,  6.5978e-05, -4.8063e-05,  ...,  1.2940e-05,
         -2.6608e-05,  4.1356e-05],
        [-1.0327e-05, -7.2420e-06,  1.2573e-06,  ..., -8.4788e-06,
         -1.0654e-06, -4.8801e-06],
        [-1.0073e-05, -7.0781e-06,  1.2312e-06,  ..., -8.2701e-06,
         -1.0356e-06, -4.7535e-06],
        [-1.0744e-05, -7.5549e-06,  1.3169e-06,  ..., -8.8066e-06,
         -1.1027e-06, -5.0664e-06],
        [-2.4080e-05, -1.6898e-05,  2.9244e-06,  ..., -1.9699e-05,
         -2.4736e-06, -1.1340e-05]], device='cuda:0')
Loss: 1.1755313873291016


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,   346,   462,  ..., 35247,  2547,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9564e-05,  2.7671e-05, -7.9427e-05,  ..., -1.1547e-05,
         -1.0692e-05,  1.3625e-05],
        [-1.3784e-05, -9.6709e-06,  1.6671e-06,  ..., -1.1310e-05,
         -1.4100e-06, -6.5267e-06],
        [-1.3456e-05, -9.4622e-06,  1.6317e-06,  ..., -1.1042e-05,
         -1.3728e-06, -6.3628e-06],
        [-1.4350e-05, -1.0088e-05,  1.7453e-06,  ..., -1.1757e-05,
         -1.4603e-06, -6.7800e-06],
        [-3.2127e-05, -2.2560e-05,  3.8780e-06,  ..., -2.6315e-05,
         -3.2783e-06, -1.5184e-05]], device='cuda:0')
Loss: 1.1693412065505981


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  923,   13,  ...,  300, 8262,   12],
        [   0, 3261, 1518,  ..., 5019,  287, 1906]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1254e-05,  5.7813e-05, -5.4704e-05,  ..., -2.9952e-05,
         -1.0692e-05,  7.7585e-05],
        [-1.7256e-05, -1.2100e-05,  2.0899e-06,  ..., -1.4141e-05,
         -1.7993e-06, -8.1658e-06],
        [-1.6868e-05, -1.1846e-05,  2.0470e-06,  ..., -1.3828e-05,
         -1.7546e-06, -7.9647e-06],
        [-1.7971e-05, -1.2621e-05,  2.1886e-06,  ..., -1.4707e-05,
         -1.8645e-06, -8.4862e-06],
        [-4.0233e-05, -2.8223e-05,  4.8541e-06,  ..., -3.2902e-05,
         -4.1872e-06, -1.8984e-05]], device='cuda:0')
Loss: 1.1811413764953613


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,   266,  1234,  ...,   908,   328, 26300],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7533e-05,  2.3624e-05, -6.0170e-05,  ..., -4.4520e-05,
         -2.9458e-05,  1.0327e-04],
        [-2.0683e-05, -1.4529e-05,  2.4978e-06,  ..., -1.6943e-05,
         -2.1476e-06, -9.7677e-06],
        [ 2.6489e-04,  1.8947e-04, -9.3455e-06,  ...,  2.7651e-04,
          1.6495e-05,  1.3754e-04],
        [-2.1547e-05, -1.5154e-05,  2.6152e-06,  ..., -1.7613e-05,
         -2.2259e-06, -1.0155e-05],
        [-4.8280e-05, -3.3915e-05,  5.8077e-06,  ..., -3.9458e-05,
         -5.0068e-06, -2.2739e-05]], device='cuda:0')
Loss: 1.166998267173767


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,  3441,   796,  ...,  7561,  1711,   857],
        [    0, 41855,     9,  ..., 33073,   401,  4528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9865e-05,  1.0332e-05, -7.3990e-05,  ..., -4.8074e-05,
         -1.7930e-05,  8.5237e-05],
        [-2.4155e-05, -1.6958e-05,  2.9150e-06,  ..., -1.9789e-05,
         -2.5108e-06, -1.1384e-05],
        [ 2.6152e-04,  1.8712e-04, -8.9395e-06,  ...,  2.7374e-04,
          1.6141e-05,  1.3597e-04],
        [-2.5153e-05, -1.7688e-05,  3.0529e-06,  ..., -2.0579e-05,
         -2.6021e-06, -1.1839e-05],
        [-5.6326e-05, -3.9577e-05,  6.7763e-06,  ..., -4.6074e-05,
         -5.8524e-06, -2.6494e-05]], device='cuda:0')
Loss: 1.1970351934432983


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0,   634, 10095,  ...,   367, 24607,   287],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3011e-05, -6.5902e-06, -7.3691e-05,  ..., -6.4691e-05,
         -3.1667e-05,  1.2093e-04],
        [-2.7627e-05, -1.9372e-05,  3.2987e-06,  ..., -2.2590e-05,
         -2.8759e-06, -1.3001e-05],
        [ 2.5809e-04,  1.8474e-04, -8.5595e-06,  ...,  2.7096e-04,
          1.5781e-05,  1.3437e-04],
        [-2.8759e-05, -2.0191e-05,  3.4533e-06,  ..., -2.3484e-05,
         -2.9802e-06, -1.3515e-05],
        [-6.4492e-05, -4.5270e-05,  7.6778e-06,  ..., -5.2661e-05,
         -6.7130e-06, -3.0294e-05]], device='cuda:0')
Loss: 1.1794111728668213
Graident accumulation at epoch 0, step 247, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0338],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4751e-04, -5.3295e-05, -9.3984e-06,  ...,  2.2345e-05,
         -6.7063e-05, -1.5593e-05],
        [-2.1715e-05, -1.4829e-05,  2.8475e-06,  ..., -1.8221e-05,
         -9.7589e-07, -9.9762e-06],
        [ 4.4735e-05,  3.3924e-05, -2.4112e-06,  ...,  4.6436e-05,
          2.6580e-06,  2.2701e-05],
        [-2.9450e-05, -1.9866e-05,  4.2891e-06,  ..., -2.4264e-05,
         -2.3939e-06, -1.2974e-05],
        [-6.1032e-05, -4.1488e-05,  7.5879e-06,  ..., -4.9548e-05,
         -4.3188e-06, -2.7070e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2383e-08, 2.8359e-08, 3.5591e-08,  ..., 1.1725e-08, 8.8803e-08,
         1.1524e-08],
        [3.5624e-11, 1.9471e-11, 2.1714e-12,  ..., 2.1755e-11, 1.1803e-12,
         4.7321e-12],
        [2.7104e-10, 1.4215e-10, 6.6035e-12,  ..., 2.4063e-10, 3.2794e-12,
         6.2440e-11],
        [2.5714e-11, 1.2134e-11, 9.7001e-13,  ..., 1.7297e-11, 4.4558e-13,
         4.0101e-12],
        [1.3523e-10, 6.9280e-11, 5.4106e-12,  ..., 9.4200e-11, 2.7816e-12,
         1.9447e-11]], device='cuda:0')
optimizer state dict: 31.0
lr: [1.95936623367937e-05, 1.95936623367937e-05]
scheduler_last_epoch: 31


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[    0,    14,  8047,  ...,  3813,     9,  8237],
        [    0,    26,   874,  ...,    12, 21591,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5730e-05,  6.6535e-06,  2.0194e-05,  ...,  4.6682e-06,
          5.6815e-06,  1.5355e-05],
        [-3.4124e-06, -2.4289e-06,  4.4703e-07,  ..., -2.8014e-06,
         -4.3213e-07, -1.5348e-06],
        [-3.3677e-06, -2.3991e-06,  4.4331e-07,  ..., -2.7716e-06,
         -4.2841e-07, -1.5199e-06],
        [-3.5465e-06, -2.5183e-06,  4.6752e-07,  ..., -2.9057e-06,
         -4.4703e-07, -1.5944e-06],
        [-7.9274e-06, -5.6326e-06,  1.0356e-06,  ..., -6.4969e-06,
         -1.0058e-06, -3.5614e-06]], device='cuda:0')
Loss: 1.1949021816253662


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[   0, 3141,  311,  ...,  328, 7818,  408],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5837e-06,  2.6536e-05,  1.6887e-05,  ...,  3.7526e-05,
         -1.4758e-05,  2.4819e-05],
        [-6.8396e-06, -4.8280e-06,  9.1828e-07,  ..., -5.5879e-06,
         -8.7731e-07, -3.0696e-06],
        [-6.7502e-06, -4.7684e-06,  9.0897e-07,  ..., -5.5283e-06,
         -8.6799e-07, -3.0324e-06],
        [-7.1079e-06, -5.0217e-06,  9.5926e-07,  ..., -5.8115e-06,
         -9.0897e-07, -3.1888e-06],
        [-1.5855e-05, -1.1206e-05,  2.1309e-06,  ..., -1.2964e-05,
         -2.0415e-06, -7.1079e-06]], device='cuda:0')
Loss: 1.1858172416687012


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   266,  2057,  ...,    88,  1801,    66],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2916e-05,  5.0262e-05, -3.2161e-06,  ...,  3.8344e-05,
          8.1885e-06,  9.4723e-05],
        [-1.0237e-05, -7.2420e-06,  1.3821e-06,  ..., -8.3596e-06,
         -1.3206e-06, -4.6045e-06],
        [-1.0148e-05, -7.1675e-06,  1.3709e-06,  ..., -8.2850e-06,
         -1.3094e-06, -4.5598e-06],
        [-1.0669e-05, -7.5549e-06,  1.4473e-06,  ..., -8.7023e-06,
         -1.3709e-06, -4.7982e-06],
        [-2.3842e-05, -1.6868e-05,  3.2187e-06,  ..., -1.9431e-05,
         -3.0771e-06, -1.0699e-05]], device='cuda:0')
Loss: 1.1755023002624512


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,  5136,   446,  ...,  1173,   300,   266],
        [    0,  9342,   600,  ...,   199, 12095,   291]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6906e-05,  9.2005e-06, -1.6102e-05,  ...,  3.0509e-05,
          1.0304e-05,  1.1233e-04],
        [-1.3679e-05, -9.6709e-06,  1.8496e-06,  ..., -1.1176e-05,
         -1.7677e-06, -6.1765e-06],
        [-1.3575e-05, -9.5814e-06,  1.8384e-06,  ..., -1.1086e-05,
         -1.7565e-06, -6.1318e-06],
        [-1.4246e-05, -1.0073e-05,  1.9353e-06,  ..., -1.1623e-05,
         -1.8347e-06, -6.4299e-06],
        [-3.1888e-05, -2.2501e-05,  4.3064e-06,  ..., -2.5988e-05,
         -4.1202e-06, -1.4365e-05]], device='cuda:0')
Loss: 1.1474974155426025


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[    0, 12182,  6294,  ...,  1042,  1070,  2228],
        [    0,  5646,    12,  ...,  1952,   287,  3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3348e-05,  1.8811e-06, -2.7128e-05,  ...,  5.2822e-05,
          1.5836e-05,  1.5020e-04],
        [-1.7121e-05, -1.2115e-05,  2.3376e-06,  ..., -1.3992e-05,
         -2.2333e-06, -7.7263e-06],
        [-1.7002e-05, -1.2025e-05,  2.3264e-06,  ..., -1.3888e-05,
         -2.2221e-06, -7.6741e-06],
        [-1.7807e-05, -1.2606e-05,  2.4457e-06,  ..., -1.4544e-05,
         -2.3190e-06, -8.0317e-06],
        [-3.9876e-05, -2.8163e-05,  5.4389e-06,  ..., -3.2514e-05,
         -5.2005e-06, -1.7956e-05]], device='cuda:0')
Loss: 1.1748642921447754


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,  3253,  1573,  ...,   298,   358,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4816e-05,  1.2932e-05,  2.8407e-06,  ...,  4.5894e-05,
         -3.1056e-07,  1.6217e-04],
        [-2.0564e-05, -1.4558e-05,  2.7884e-06,  ..., -1.6838e-05,
         -2.7325e-06, -9.2760e-06],
        [-2.0444e-05, -1.4469e-05,  2.7772e-06,  ..., -1.6734e-05,
         -2.7213e-06, -9.2238e-06],
        [-2.1324e-05, -1.5110e-05,  2.9095e-06,  ..., -1.7449e-05,
         -2.8294e-06, -9.6187e-06],
        [ 1.4123e-04,  1.0077e-04, -2.3698e-05,  ...,  1.1617e-04,
          4.6189e-05,  7.5047e-05]], device='cuda:0')
Loss: 1.197309970855713


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  257,  298,  ..., 3768,  271,  266],
        [   0,  300, 5631,  ..., 2278, 2669, 3011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3196e-05,  3.6479e-05,  1.9116e-05,  ...,  5.0280e-05,
         -3.5556e-05,  1.7564e-04],
        [-2.3976e-05, -1.7002e-05,  3.2261e-06,  ..., -1.9640e-05,
         -3.1758e-06, -1.0818e-05],
        [-2.3797e-05, -1.6883e-05,  3.2093e-06,  ..., -1.9491e-05,
         -3.1572e-06, -1.0736e-05],
        [-2.4855e-05, -1.7643e-05,  3.3658e-06,  ..., -2.0355e-05,
         -3.2876e-06, -1.1213e-05],
        [ 1.3336e-04,  9.5136e-05, -2.2692e-05,  ...,  1.0970e-04,
          4.5168e-05,  7.1500e-05]], device='cuda:0')
Loss: 1.16464364528656


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   609,    12,  ...,   409, 11041,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5191e-06,  3.3855e-05,  2.5733e-05,  ...,  4.6532e-05,
         -5.5130e-05,  1.6979e-04],
        [-2.7344e-05, -1.9386e-05,  3.6731e-06,  ..., -2.2411e-05,
         -3.6191e-06, -1.2316e-05],
        [ 5.5670e-05,  5.5598e-05, -4.3099e-06,  ...,  5.0964e-05,
          1.0756e-05,  3.3863e-05],
        [-2.8387e-05, -2.0146e-05,  3.8370e-06,  ..., -2.3261e-05,
         -3.7532e-06, -1.2785e-05],
        [ 1.2550e-04,  8.9563e-05, -2.1649e-05,  ...,  1.0320e-04,
          4.4125e-05,  6.7983e-05]], device='cuda:0')
Loss: 1.1685833930969238
Graident accumulation at epoch 0, step 255, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3291e-04, -4.4580e-05, -5.8852e-06,  ...,  2.4764e-05,
         -6.5870e-05,  2.9448e-06],
        [-2.2278e-05, -1.5285e-05,  2.9301e-06,  ..., -1.8640e-05,
         -1.2402e-06, -1.0210e-05],
        [ 4.5828e-05,  3.6091e-05, -2.6010e-06,  ...,  4.6888e-05,
          3.4677e-06,  2.3817e-05],
        [-2.9343e-05, -1.9894e-05,  4.2439e-06,  ..., -2.4164e-05,
         -2.5299e-06, -1.2955e-05],
        [-4.2380e-05, -2.8383e-05,  4.6642e-06,  ..., -3.4273e-05,
          5.2556e-07, -1.7565e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2341e-08, 2.8332e-08, 3.5556e-08,  ..., 1.1716e-08, 8.8717e-08,
         1.1541e-08],
        [3.6336e-11, 1.9828e-11, 2.1827e-12,  ..., 2.2235e-11, 1.1923e-12,
         4.8790e-12],
        [2.7387e-10, 1.4510e-10, 6.6154e-12,  ..., 2.4298e-10, 3.3918e-12,
         6.3524e-11],
        [2.6494e-11, 1.2528e-11, 9.8377e-13,  ..., 1.7820e-11, 4.5922e-13,
         4.1695e-12],
        [1.5085e-10, 7.7232e-11, 5.8739e-12,  ..., 1.0476e-10, 4.7258e-12,
         2.4049e-11]], device='cuda:0')
optimizer state dict: 32.0
lr: [1.9558050089320493e-05, 1.9558050089320493e-05]
scheduler_last_epoch: 32


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,  5775,    12,  ...,    12,  1034,  9257],
        [    0,  9058,  5481,  ...,   508, 15074,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2690e-05,  8.8906e-06, -1.4277e-05,  ...,  8.7392e-06,
          1.8941e-05, -1.5180e-06],
        [-3.3677e-06, -2.4438e-06,  5.2154e-07,  ..., -2.7865e-06,
         -5.3272e-07, -1.4603e-06],
        [-3.4124e-06, -2.4736e-06,  5.2899e-07,  ..., -2.8163e-06,
         -5.4017e-07, -1.4752e-06],
        [-3.5316e-06, -2.5630e-06,  5.5134e-07,  ..., -2.9206e-06,
         -5.5507e-07, -1.5274e-06],
        [-7.8678e-06, -5.7220e-06,  1.2219e-06,  ..., -6.4969e-06,
         -1.2442e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.1734819412231445


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[   0, 1921,  843,  ..., 9420,  352,  266],
        [   0,  795, 3185,  ...,   14, 1671,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8877e-06, -1.2328e-05,  7.2246e-06,  ...,  1.3104e-05,
          3.3386e-05,  1.4407e-06],
        [-6.7055e-06, -4.8578e-06,  1.0319e-06,  ..., -5.5581e-06,
         -1.0543e-06, -2.9206e-06],
        [-6.7800e-06, -4.9174e-06,  1.0468e-06,  ..., -5.6177e-06,
         -1.0654e-06, -2.9504e-06],
        [-6.9886e-06, -5.0813e-06,  1.0841e-06,  ..., -5.8115e-06,
         -1.0952e-06, -3.0473e-06],
        [-1.5616e-05, -1.1355e-05,  2.4065e-06,  ..., -1.2964e-05,
         -2.4587e-06, -6.8098e-06]], device='cuda:0')
Loss: 1.1589224338531494


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[    0,   422,    13,  ..., 14026,   368,  4999],
        [    0,   335,   446,  ...,  5795,    12, 12433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8514e-06, -2.0645e-05,  1.4040e-06,  ..., -2.1213e-05,
          2.8521e-05,  1.5215e-05],
        [-1.0043e-05, -7.2718e-06,  1.5423e-06,  ..., -8.3447e-06,
         -1.6019e-06, -4.3958e-06],
        [-1.0177e-05, -7.3761e-06,  1.5646e-06,  ..., -8.4490e-06,
         -1.6205e-06, -4.4480e-06],
        [-1.0490e-05, -7.6145e-06,  1.6205e-06,  ..., -8.7172e-06,
         -1.6652e-06, -4.5896e-06],
        [-2.3425e-05, -1.7017e-05,  3.5986e-06,  ..., -1.9491e-05,
         -3.7402e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.1649061441421509


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[   0, 2738,  278,  ...,  292,   35, 2147],
        [   0,  199, 2834,  ..., 3988, 1049,  935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2067e-05, -4.3606e-05,  5.4446e-06,  ..., -3.1649e-05,
          4.9299e-05,  1.0794e-06],
        [-1.3411e-05, -9.7007e-06,  2.0787e-06,  ..., -1.1086e-05,
         -2.1979e-06, -5.8785e-06],
        [ 2.0694e-04,  1.3238e-04, -3.4150e-05,  ...,  1.6607e-04,
          3.6972e-05,  7.9785e-05],
        [-1.3977e-05, -1.0148e-05,  2.1793e-06,  ..., -1.1563e-05,
         -2.2836e-06, -6.1318e-06],
        [-3.1233e-05, -2.2650e-05,  4.8354e-06,  ..., -2.5868e-05,
         -5.1260e-06, -1.3709e-05]], device='cuda:0')
Loss: 1.1627345085144043


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0,   879,    27,  ...,    13,  2764,  3860],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5416e-05, -2.9139e-05,  1.9188e-05,  ..., -3.5219e-05,
          6.2540e-05,  3.3554e-05],
        [-1.6794e-05, -1.2130e-05,  2.6114e-06,  ..., -1.3903e-05,
         -2.7344e-06, -7.3612e-06],
        [ 2.0353e-04,  1.2994e-04, -3.3614e-05,  ...,  1.6323e-04,
          3.6428e-05,  7.8287e-05],
        [-1.7494e-05, -1.2681e-05,  2.7344e-06,  ..., -1.4499e-05,
         -2.8424e-06, -7.6741e-06],
        [-3.9101e-05, -2.8282e-05,  6.0648e-06,  ..., -3.2425e-05,
         -6.3777e-06, -1.7151e-05]], device='cuda:0')
Loss: 1.1765708923339844


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[    0,   508,  3282,  ...,   334,   287, 31884],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5614e-05, -2.8314e-05,  2.1080e-05,  ..., -3.6613e-05,
          6.1538e-05,  4.7317e-05],
        [-2.0191e-05, -1.4573e-05,  3.1330e-06,  ..., -1.6719e-05,
         -3.2894e-06, -8.8513e-06],
        [ 2.0009e-04,  1.2747e-04, -3.3089e-05,  ...,  1.6037e-04,
          3.5865e-05,  7.6775e-05],
        [-2.0996e-05, -1.5199e-05,  3.2708e-06,  ..., -1.7405e-05,
         -3.4124e-06, -9.2089e-06],
        [-4.7028e-05, -3.3975e-05,  7.2718e-06,  ..., -3.9011e-05,
         -7.6741e-06, -2.0623e-05]], device='cuda:0')
Loss: 1.1726465225219727


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  432,  984,  ...,  287,  496,   14],
        [   0, 1270,  413,  ...,  413,  711,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7539e-05, -3.9013e-05,  3.9587e-05,  ..., -4.5543e-05,
          5.5177e-05,  4.9703e-05],
        [-2.3589e-05, -1.7017e-05,  3.6322e-06,  ..., -1.9580e-05,
         -3.8631e-06, -1.0356e-05],
        [ 1.9667e-04,  1.2501e-04, -3.2586e-05,  ...,  1.5749e-04,
          3.5288e-05,  7.5262e-05],
        [-2.4498e-05, -1.7717e-05,  3.7886e-06,  ..., -2.0355e-05,
         -4.0010e-06, -1.0759e-05],
        [-5.4896e-05, -3.9637e-05,  8.4341e-06,  ..., -4.5657e-05,
         -9.0003e-06, -2.4110e-05]], device='cuda:0')
Loss: 1.1954370737075806


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,    12,   287,  ...,   278,  4697,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3421e-05, -4.5046e-05,  2.2169e-05,  ..., -3.1681e-05,
          6.5677e-05,  5.0138e-05],
        [-2.6971e-05, -1.9446e-05,  4.1611e-06,  ..., -2.2411e-05,
         -4.4219e-06, -1.1832e-05],
        [ 1.9325e-04,  1.2255e-04, -3.2053e-05,  ...,  1.5463e-04,
          3.4722e-05,  7.3772e-05],
        [-2.8014e-05, -2.0236e-05,  4.3400e-06,  ..., -2.3291e-05,
         -4.5821e-06, -1.2286e-05],
        [-6.2704e-05, -4.5240e-05,  9.6560e-06,  ..., -5.2184e-05,
         -1.0289e-05, -2.7508e-05]], device='cuda:0')
Loss: 1.18583083152771
Graident accumulation at epoch 0, step 263, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2796e-04, -4.4627e-05, -3.0798e-06,  ...,  1.9119e-05,
         -5.2715e-05,  7.6641e-06],
        [-2.2747e-05, -1.5701e-05,  3.0532e-06,  ..., -1.9017e-05,
         -1.5584e-06, -1.0372e-05],
        [ 6.0570e-05,  4.4737e-05, -5.5462e-06,  ...,  5.7663e-05,
          6.5931e-06,  2.8813e-05],
        [-2.9210e-05, -1.9928e-05,  4.2535e-06,  ..., -2.4077e-05,
         -2.7351e-06, -1.2888e-05],
        [-4.4412e-05, -3.0069e-05,  5.1634e-06,  ..., -3.6064e-05,
         -5.5592e-07, -1.8559e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2305e-08, 2.8306e-08, 3.5521e-08,  ..., 1.1705e-08, 8.8633e-08,
         1.1532e-08],
        [3.7027e-11, 2.0186e-11, 2.1978e-12,  ..., 2.2715e-11, 1.2106e-12,
         5.0141e-12],
        [3.1094e-10, 1.5997e-10, 7.6362e-12,  ..., 2.6665e-10, 4.5940e-12,
         6.8903e-11],
        [2.7253e-11, 1.2925e-11, 1.0016e-12,  ..., 1.8345e-11, 4.7976e-13,
         4.3163e-12],
        [1.5463e-10, 7.9202e-11, 5.9612e-12,  ..., 1.0737e-10, 4.8270e-12,
         2.4782e-11]], device='cuda:0')
optimizer state dict: 33.0
lr: [1.9520977288360243e-05, 1.9520977288360243e-05]
scheduler_last_epoch: 33


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,   292,   380,  ...,   527, 37357,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9341e-06,  3.5924e-05,  1.8128e-05,  ...,  7.3625e-06,
          2.0941e-05, -1.1930e-05],
        [-3.2783e-06, -2.3991e-06,  5.5879e-07,  ..., -2.7716e-06,
         -6.7055e-07, -1.3933e-06],
        [-3.3826e-06, -2.4736e-06,  5.7742e-07,  ..., -2.8610e-06,
         -6.9290e-07, -1.4380e-06],
        [-3.4273e-06, -2.5034e-06,  5.8860e-07,  ..., -2.8908e-06,
         -6.9663e-07, -1.4529e-06],
        [-7.7486e-06, -5.6326e-06,  1.3188e-06,  ..., -6.5267e-06,
         -1.5795e-06, -3.2783e-06]], device='cuda:0')
Loss: 1.1613693237304688


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[    0, 33119,   391,  ...,   292,  4462,  2721],
        [    0,   527,   496,  ...,    12,   795,  8296]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7536e-07,  2.3189e-05,  3.4720e-05,  ..., -1.2870e-05,
          4.0863e-05, -4.4538e-07],
        [-6.6161e-06, -4.8578e-06,  1.1139e-06,  ..., -5.5283e-06,
         -1.3411e-06, -2.7791e-06],
        [-6.8247e-06, -5.0068e-06,  1.1511e-06,  ..., -5.7071e-06,
         -1.3821e-06, -2.8685e-06],
        [-6.8843e-06, -5.0515e-06,  1.1660e-06,  ..., -5.7518e-06,
         -1.3895e-06, -2.8908e-06],
        [-1.5497e-05, -1.1355e-05,  2.6077e-06,  ..., -1.2934e-05,
         -3.1367e-06, -6.4969e-06]], device='cuda:0')
Loss: 1.1678357124328613


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,  1295,   898,  ...,   298, 38754,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4005e-05,  4.8283e-05,  2.4234e-05,  ...,  1.8941e-05,
          8.3612e-06, -2.2022e-05],
        [-9.9093e-06, -7.2569e-06,  1.7136e-06,  ..., -8.3148e-06,
         -1.9930e-06, -4.1872e-06],
        [-1.0222e-05, -7.4804e-06,  1.7695e-06,  ..., -8.5831e-06,
         -2.0526e-06, -4.3213e-06],
        [-1.0327e-05, -7.5549e-06,  1.7956e-06,  ..., -8.6576e-06,
         -2.0675e-06, -4.3586e-06],
        [-2.3186e-05, -1.6958e-05,  4.0084e-06,  ..., -1.9431e-05,
         -4.6566e-06, -9.7901e-06]], device='cuda:0')
Loss: 1.1767774820327759


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[   0,  474,  513,  ...,  221, 2951, 7773],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3176e-05,  4.6164e-05, -2.4359e-06,  ...,  2.1110e-05,
          1.8622e-05, -3.6014e-05],
        [-1.3247e-05, -9.6858e-06,  2.2948e-06,  ..., -1.1101e-05,
         -2.6524e-06, -5.6028e-06],
        [-1.3649e-05, -9.9838e-06,  2.3693e-06,  ..., -1.1459e-05,
         -2.7306e-06, -5.7817e-06],
        [-1.3784e-05, -1.0073e-05,  2.3991e-06,  ..., -1.1548e-05,
         -2.7530e-06, -5.8264e-06],
        [-3.0935e-05, -2.2590e-05,  5.3495e-06,  ..., -2.5898e-05,
         -6.1840e-06, -1.3068e-05]], device='cuda:0')
Loss: 1.1684569120407104


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,    19,    14,  ...,   278,  2588,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3176e-05,  4.6875e-05,  9.4594e-07,  ...,  4.8948e-05,
          7.2729e-06, -3.2468e-06],
        [-1.6570e-05, -1.2130e-05,  2.8722e-06,  ..., -1.3888e-05,
         -3.3304e-06, -7.0259e-06],
        [-1.7077e-05, -1.2502e-05,  2.9653e-06,  ..., -1.4335e-05,
         -3.4273e-06, -7.2494e-06],
        [-1.7226e-05, -1.2591e-05,  2.9989e-06,  ..., -1.4439e-05,
         -3.4533e-06, -7.3016e-06],
        [-3.8683e-05, -2.8253e-05,  6.6906e-06,  ..., -3.2395e-05,
         -7.7561e-06, -1.6376e-05]], device='cuda:0')
Loss: 1.1692695617675781


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   266,  1658,  ...,   278,  1083,  5993],
        [    0, 18981,    13,  ...,   365,  2714,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2231e-05,  2.6261e-05, -2.9990e-05,  ...,  1.0528e-05,
         -6.7346e-06,  4.4070e-07],
        [-1.9893e-05, -1.4558e-05,  3.4198e-06,  ..., -1.6645e-05,
         -4.0010e-06, -8.4266e-06],
        [-2.0504e-05, -1.4991e-05,  3.5316e-06,  ..., -1.7181e-05,
         -4.1164e-06, -8.6948e-06],
        [-2.0683e-05, -1.5110e-05,  3.5688e-06,  ..., -1.7300e-05,
         -4.1462e-06, -8.7544e-06],
        [-4.6432e-05, -3.3885e-05,  7.9647e-06,  ..., -3.8832e-05,
         -9.3207e-06, -1.9640e-05]], device='cuda:0')
Loss: 1.1742260456085205


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0, 28011,    12,  ...,   346,   462,   221],
        [    0,  2663,    12,  ..., 24113,   497,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1446e-05,  1.8845e-05, -3.4725e-05,  ...,  1.6725e-05,
         -7.4672e-06, -1.2228e-05],
        [-2.3171e-05, -1.6943e-05,  3.9972e-06,  ..., -1.9401e-05,
         -4.6566e-06, -9.8422e-06],
        [-2.3901e-05, -1.7464e-05,  4.1313e-06,  ..., -2.0042e-05,
         -4.7944e-06, -1.0163e-05],
        [-2.4110e-05, -1.7613e-05,  4.1761e-06,  ..., -2.0191e-05,
         -4.8317e-06, -1.0237e-05],
        [-5.4121e-05, -3.9488e-05,  9.3207e-06,  ..., -4.5300e-05,
         -1.0863e-05, -2.2963e-05]], device='cuda:0')
Loss: 1.1760936975479126


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  445,   18,  ..., 1478,  578,  494],
        [   0, 8588, 3937,  ...,  516, 1128, 2341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4242e-05, -4.7345e-06, -3.1520e-05,  ...,  3.4095e-05,
          4.0029e-06,  6.1594e-06],
        [-2.6524e-05, -1.9386e-05,  4.5709e-06,  ..., -2.2188e-05,
         -5.3421e-06, -1.1265e-05],
        [-2.7373e-05, -1.9997e-05,  4.7237e-06,  ..., -2.2918e-05,
         -5.5023e-06, -1.1638e-05],
        [-2.7597e-05, -2.0161e-05,  4.7758e-06,  ..., -2.3097e-05,
         -5.5432e-06, -1.1720e-05],
        [-6.2048e-05, -4.5240e-05,  1.0669e-05,  ..., -5.1886e-05,
         -1.2480e-05, -2.6315e-05]], device='cuda:0')
Loss: 1.1775240898132324
Graident accumulation at epoch 0, step 271, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1659e-04, -4.0638e-05, -5.9238e-06,  ...,  2.0617e-05,
         -4.7043e-05,  7.5137e-06],
        [-2.3125e-05, -1.6069e-05,  3.2049e-06,  ..., -1.9334e-05,
         -1.9368e-06, -1.0462e-05],
        [ 5.1776e-05,  3.8264e-05, -4.5192e-06,  ...,  4.9604e-05,
          5.3836e-06,  2.4768e-05],
        [-2.9049e-05, -1.9951e-05,  4.3057e-06,  ..., -2.3979e-05,
         -3.0159e-06, -1.2771e-05],
        [-4.6176e-05, -3.1586e-05,  5.7140e-06,  ..., -3.7646e-05,
         -1.7483e-06, -1.9335e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2263e-08, 2.8278e-08, 3.5487e-08,  ..., 1.1694e-08, 8.8544e-08,
         1.1521e-08],
        [3.7694e-11, 2.0542e-11, 2.2165e-12,  ..., 2.3185e-11, 1.2380e-12,
         5.1360e-12],
        [3.1138e-10, 1.6021e-10, 7.6509e-12,  ..., 2.6691e-10, 4.6197e-12,
         6.8969e-11],
        [2.7987e-11, 1.3319e-11, 1.0234e-12,  ..., 1.8860e-11, 5.1001e-13,
         4.4493e-12],
        [1.5832e-10, 8.1169e-11, 6.0691e-12,  ..., 1.0996e-10, 4.9779e-12,
         2.5450e-11]], device='cuda:0')
optimizer state dict: 34.0
lr: [1.9482449598960544e-05, 1.9482449598960544e-05]
scheduler_last_epoch: 34


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0, 3968,  446,  ...,   22,  722,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1071e-05, -5.1240e-06, -1.5599e-05,  ...,  9.6637e-07,
         -2.7878e-05, -1.3945e-05],
        [-3.2634e-06, -2.4289e-06,  5.9977e-07,  ..., -2.7716e-06,
         -7.2271e-07, -1.3188e-06],
        [-3.4124e-06, -2.5481e-06,  6.2957e-07,  ..., -2.9057e-06,
         -7.5623e-07, -1.3784e-06],
        [-3.3826e-06, -2.5183e-06,  6.2212e-07,  ..., -2.8759e-06,
         -7.4506e-07, -1.3635e-06],
        [-7.6294e-06, -5.6624e-06,  1.4007e-06,  ..., -6.4671e-06,
         -1.6838e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.1678962707519531


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 1049,   12,  ...,  292, 3963,  755],
        [   0,  278, 5798,  ...,  266,  729, 1798]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3320e-05, -2.6356e-05, -3.4893e-05,  ..., -1.8165e-05,
         -3.7878e-05, -5.0133e-05],
        [-6.5416e-06, -4.8727e-06,  1.1958e-06,  ..., -5.5283e-06,
         -1.4491e-06, -2.6822e-06],
        [-6.8247e-06, -5.0813e-06,  1.2517e-06,  ..., -5.7817e-06,
         -1.5087e-06, -2.7940e-06],
        [-6.7800e-06, -5.0515e-06,  1.2442e-06,  ..., -5.7369e-06,
         -1.4938e-06, -2.7716e-06],
        [-1.5199e-05, -1.1295e-05,  2.7791e-06,  ..., -1.2845e-05,
         -3.3602e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.1546679735183716


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,  1098,   259,  ...,  6572,  1477,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0257e-05, -2.6841e-05, -4.8687e-05,  ..., -2.2993e-05,
         -3.4381e-05, -3.2898e-05],
        [-9.8199e-06, -7.3165e-06,  1.7807e-06,  ..., -8.2999e-06,
         -2.1793e-06, -4.0531e-06],
        [-1.0237e-05, -7.6145e-06,  1.8626e-06,  ..., -8.6576e-06,
         -2.2687e-06, -4.2170e-06],
        [-1.0207e-05, -7.5996e-06,  1.8589e-06,  ..., -8.6278e-06,
         -2.2538e-06, -4.2021e-06],
        [-2.2829e-05, -1.6958e-05,  4.1351e-06,  ..., -1.9282e-05,
         -5.0589e-06, -9.3728e-06]], device='cuda:0')
Loss: 1.1693952083587646


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   729,  3430,  ...,  9715,    13, 42383],
        [    0,  4599,  9005,  ...,   809,    13,  1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4263e-05, -2.4938e-05, -5.7138e-05,  ..., -6.4751e-06,
         -5.1615e-05, -3.1808e-05],
        [-1.3083e-05, -9.7156e-06,  2.3954e-06,  ..., -1.1042e-05,
         -2.9132e-06, -5.4166e-06],
        [-1.3664e-05, -1.0148e-05,  2.5108e-06,  ..., -1.1548e-05,
         -3.0436e-06, -5.6475e-06],
        [-1.3620e-05, -1.0118e-05,  2.5034e-06,  ..., -1.1489e-05,
         -3.0212e-06, -5.6252e-06],
        [-3.0458e-05, -2.2590e-05,  5.5730e-06,  ..., -2.5690e-05,
         -6.7800e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1550865173339844


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[   0,   17,   12,  ...,   12,  461,  806],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6666e-05, -1.1068e-05, -5.5658e-05,  ...,  8.4277e-06,
         -2.8090e-05, -4.4168e-05],
        [-1.6361e-05, -1.2115e-05,  3.0138e-06,  ..., -1.3813e-05,
         -3.6582e-06, -6.7949e-06],
        [-1.7077e-05, -1.2651e-05,  3.1553e-06,  ..., -1.4439e-05,
         -3.8221e-06, -7.0855e-06],
        [-1.7032e-05, -1.2621e-05,  3.1479e-06,  ..., -1.4380e-05,
         -3.7961e-06, -7.0557e-06],
        [-3.8058e-05, -2.8163e-05,  7.0035e-06,  ..., -3.2127e-05,
         -8.5160e-06, -1.5736e-05]], device='cuda:0')
Loss: 1.1801979541778564


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  4645,  7688,  ..., 26535,   471,   287],
        [    0,   221,   380,  ...,  3990,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7245e-05, -2.4979e-05, -5.5302e-05,  ...,  2.5474e-05,
         -5.7287e-05, -3.8850e-05],
        [-1.9640e-05, -1.4573e-05,  3.6210e-06,  ..., -1.6600e-05,
         -4.4070e-06, -8.1584e-06],
        [-2.0549e-05, -1.5244e-05,  3.7961e-06,  ..., -1.7375e-05,
         -4.6119e-06, -8.5309e-06],
        [-2.0444e-05, -1.5169e-05,  3.7812e-06,  ..., -1.7270e-05,
         -4.5747e-06, -8.4788e-06],
        [-4.5657e-05, -3.3855e-05,  8.4043e-06,  ..., -3.8564e-05,
         -1.0252e-05, -1.8895e-05]], device='cuda:0')
Loss: 1.159010410308838


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,  1234,   278,  ...,  8635,   271,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3915e-05, -4.2006e-05, -6.9905e-05,  ...,  1.9983e-05,
         -5.4987e-05, -6.4425e-05],
        [-2.2888e-05, -1.6972e-05,  4.2133e-06,  ..., -1.9372e-05,
         -5.1148e-06, -9.4995e-06],
        [-2.3961e-05, -1.7762e-05,  4.4145e-06,  ..., -2.0266e-05,
         -5.3495e-06, -9.9316e-06],
        [-2.3827e-05, -1.7673e-05,  4.3996e-06,  ..., -2.0146e-05,
         -5.3085e-06, -9.8720e-06],
        [-5.3257e-05, -3.9458e-05,  9.7826e-06,  ..., -4.5002e-05,
         -1.1899e-05, -2.2009e-05]], device='cuda:0')
Loss: 1.1687698364257812


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[    0,  3504,     9,  ...,  7166, 10945,  3119],
        [    0,   287,  2997,  ...,   437,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7389e-05, -6.2455e-05, -6.8545e-05,  ...,  2.5388e-06,
         -9.0117e-05, -6.7727e-05],
        [-2.6152e-05, -1.9386e-05,  4.8168e-06,  ..., -2.2143e-05,
         -5.8636e-06, -1.0841e-05],
        [-2.7373e-05, -2.0280e-05,  5.0440e-06,  ..., -2.3156e-05,
         -6.1318e-06, -1.1332e-05],
        [-2.7239e-05, -2.0191e-05,  5.0329e-06,  ..., -2.3037e-05,
         -6.0908e-06, -1.1273e-05],
        [-6.0827e-05, -4.5061e-05,  1.1183e-05,  ..., -5.1439e-05,
         -1.3642e-05, -2.5123e-05]], device='cuda:0')
Loss: 1.1700422763824463
Graident accumulation at epoch 0, step 279, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0403,  ...,  0.0229,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5192e-05, -4.2819e-05, -1.2186e-05,  ...,  1.8809e-05,
         -5.1351e-05, -1.0420e-08],
        [-2.3428e-05, -1.6401e-05,  3.3661e-06,  ..., -1.9615e-05,
         -2.3294e-06, -1.0499e-05],
        [ 4.3861e-05,  3.2409e-05, -3.5629e-06,  ...,  4.2328e-05,
          4.2320e-06,  2.1158e-05],
        [-2.8868e-05, -1.9975e-05,  4.3784e-06,  ..., -2.3884e-05,
         -3.3234e-06, -1.2622e-05],
        [-4.7641e-05, -3.2933e-05,  6.2609e-06,  ..., -3.9026e-05,
         -2.9377e-06, -1.9913e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2230e-08, 2.8253e-08, 3.5456e-08,  ..., 1.1683e-08, 8.8464e-08,
         1.1514e-08],
        [3.8340e-11, 2.0897e-11, 2.2375e-12,  ..., 2.3652e-11, 1.2711e-12,
         5.2484e-12],
        [3.1182e-10, 1.6046e-10, 7.6687e-12,  ..., 2.6718e-10, 4.6527e-12,
         6.9029e-11],
        [2.8701e-11, 1.3713e-11, 1.0477e-12,  ..., 1.9372e-11, 5.4659e-13,
         4.5720e-12],
        [1.6186e-10, 8.3119e-11, 6.1881e-12,  ..., 1.1250e-10, 5.1590e-12,
         2.6055e-11]], device='cuda:0')
optimizer state dict: 35.0
lr: [1.9442472908488653e-05, 1.9442472908488653e-05]
scheduler_last_epoch: 35


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[   0,  897,  328,  ...,  908,  696,  688],
        [   0, 3860,  694,  ..., 1027,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2151e-05,  1.6399e-05, -7.1936e-06,  ...,  2.8346e-05,
         -1.6705e-05, -3.4052e-05],
        [-3.2037e-06, -2.4140e-06,  6.4820e-07,  ..., -2.7716e-06,
         -8.0839e-07, -1.3113e-06],
        [-3.3677e-06, -2.5481e-06,  6.8545e-07,  ..., -2.9355e-06,
         -8.5309e-07, -1.3858e-06],
        [-3.3528e-06, -2.5332e-06,  6.8173e-07,  ..., -2.9057e-06,
         -8.4564e-07, -1.3709e-06],
        [-7.4506e-06, -5.6326e-06,  1.5050e-06,  ..., -6.4671e-06,
         -1.8850e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.1650183200836182


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[    0,   944,   278,  ...,  2374,   699,  8867],
        [    0, 25009,   407,  ..., 13076,    13,  5226]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2025e-05,  2.1501e-05, -5.0718e-06,  ...,  3.7508e-05,
         -1.1012e-05, -3.2125e-05],
        [-6.4224e-06, -4.8280e-06,  1.2666e-06,  ..., -5.5730e-06,
         -1.6205e-06, -2.6599e-06],
        [-6.7800e-06, -5.0962e-06,  1.3374e-06,  ..., -5.9009e-06,
         -1.7099e-06, -2.8089e-06],
        [-6.6906e-06, -5.0366e-06,  1.3225e-06,  ..., -5.8115e-06,
         -1.6838e-06, -2.7642e-06],
        [-1.4931e-05, -1.1235e-05,  2.9355e-06,  ..., -1.2964e-05,
         -3.7700e-06, -6.1691e-06]], device='cuda:0')
Loss: 1.1752650737762451


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  5159,   292,  ...,   772,   271,  3728],
        [    0,  3611, 10765,  ...,   271,  4317,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0449e-05,  1.9854e-05,  2.3763e-06,  ...,  4.7495e-05,
          1.8983e-05, -2.9387e-07],
        [-9.6411e-06, -7.2271e-06,  1.8813e-06,  ..., -8.3596e-06,
         -2.4065e-06, -3.9786e-06],
        [-1.0177e-05, -7.6294e-06,  1.9856e-06,  ..., -8.8364e-06,
         -2.5406e-06, -4.1947e-06],
        [-1.0043e-05, -7.5400e-06,  1.9632e-06,  ..., -8.7172e-06,
         -2.5034e-06, -4.1351e-06],
        [-2.2441e-05, -1.6838e-05,  4.3660e-06,  ..., -1.9461e-05,
         -5.6103e-06, -9.2387e-06]], device='cuda:0')
Loss: 1.1648050546646118


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0, 2286, 1085,  ..., 1387, 1184, 1802],
        [   0, 2348,  565,  ...,   12,  709,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8701e-05,  2.4147e-05, -4.7362e-06,  ...,  4.2120e-05,
          7.1061e-06, -3.4266e-05],
        [-1.2860e-05, -9.6262e-06,  2.5071e-06,  ..., -1.1161e-05,
         -3.1851e-06, -5.2601e-06],
        [-1.3545e-05, -1.0148e-05,  2.6412e-06,  ..., -1.1772e-05,
         -3.3565e-06, -5.5358e-06],
        [-1.3381e-05, -1.0028e-05,  2.6152e-06,  ..., -1.1623e-05,
         -3.3118e-06, -5.4613e-06],
        [-2.9862e-05, -2.2382e-05,  5.8115e-06,  ..., -2.5898e-05,
         -7.4133e-06, -1.2174e-05]], device='cuda:0')
Loss: 1.1605099439620972


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,  1796,   342,  ...,   668,  2903,   518],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8108e-05,  3.7641e-05, -3.2601e-05,  ...,  5.4484e-05,
         -1.3169e-05, -2.6431e-05],
        [-1.6063e-05, -1.2025e-05,  3.1404e-06,  ..., -1.3947e-05,
         -4.0233e-06, -6.5714e-06],
        [-1.6943e-05, -1.2681e-05,  3.3155e-06,  ..., -1.4722e-05,
         -4.2468e-06, -6.9290e-06],
        [-1.6734e-05, -1.2532e-05,  3.2820e-06,  ..., -1.4544e-05,
         -4.1872e-06, -6.8322e-06],
        [-3.7372e-05, -2.7955e-05,  7.2941e-06,  ..., -3.2395e-05,
         -9.3803e-06, -1.5244e-05]], device='cuda:0')
Loss: 1.1659196615219116


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[   0,  266, 2374,  ..., 1551,  518,  638],
        [   0, 6418,  446,  ...,  413,   29,  413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6220e-05,  5.8952e-05, -1.9834e-06,  ...,  2.3442e-05,
          7.7854e-06, -3.1911e-05],
        [-1.9386e-05, -1.4484e-05,  3.7551e-06,  ..., -1.6764e-05,
         -4.9062e-06, -7.8976e-06],
        [-2.0429e-05, -1.5259e-05,  3.9600e-06,  ..., -1.7688e-05,
         -5.1744e-06, -8.3223e-06],
        [-2.0146e-05, -1.5065e-05,  3.9153e-06,  ..., -1.7434e-05,
         -5.0925e-06, -8.1882e-06],
        [-4.5002e-05, -3.3617e-05,  8.7023e-06,  ..., -3.8862e-05,
         -1.1407e-05, -1.8284e-05]], device='cuda:0')
Loss: 1.1696796417236328


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[    0,   369, 19287,  ..., 12502,  6626,   292],
        [    0,  8822,  1486,  ...,    12,   287,  6903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5858e-05,  6.8835e-05, -1.7521e-06,  ...,  3.4228e-05,
         -3.3022e-06,  2.8166e-06],
        [-2.2635e-05, -1.6928e-05,  4.3735e-06,  ..., -1.9535e-05,
         -5.7705e-06, -9.2313e-06],
        [-2.3872e-05, -1.7837e-05,  4.6156e-06,  ..., -2.0623e-05,
         -6.0908e-06, -9.7379e-06],
        [-2.3514e-05, -1.7598e-05,  4.5598e-06,  ..., -2.0310e-05,
         -5.9903e-06, -9.5740e-06],
        [-5.2482e-05, -3.9220e-05,  1.0125e-05,  ..., -4.5240e-05,
         -1.3404e-05, -2.1353e-05]], device='cuda:0')
Loss: 1.1520721912384033


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9981e-06,  5.6261e-05, -4.5967e-06,  ...,  3.1610e-05,
          8.8020e-06, -1.3424e-06],
        [-2.5868e-05, -1.9327e-05,  5.0068e-06,  ..., -2.2307e-05,
         -6.6087e-06, -1.0572e-05],
        [-2.7284e-05, -2.0370e-05,  5.2825e-06,  ..., -2.3544e-05,
         -6.9775e-06, -1.1161e-05],
        [-2.6852e-05, -2.0087e-05,  5.2154e-06,  ..., -2.3171e-05,
         -6.8545e-06, -1.0967e-05],
        [-5.9903e-05, -4.4733e-05,  1.1578e-05,  ..., -5.1588e-05,
         -1.5326e-05, -2.4438e-05]], device='cuda:0')
Loss: 1.167914628982544
Graident accumulation at epoch 0, step 287, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0222, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5972e-05, -3.2911e-05, -1.1427e-05,  ...,  2.0089e-05,
         -4.5335e-05, -1.4362e-07],
        [-2.3672e-05, -1.6694e-05,  3.5302e-06,  ..., -1.9884e-05,
         -2.7574e-06, -1.0507e-05],
        [ 3.6746e-05,  2.7131e-05, -2.6784e-06,  ...,  3.5741e-05,
          3.1111e-06,  1.7926e-05],
        [-2.8666e-05, -1.9986e-05,  4.4621e-06,  ..., -2.3813e-05,
         -3.6765e-06, -1.2456e-05],
        [-4.8867e-05, -3.4113e-05,  6.7927e-06,  ..., -4.0282e-05,
         -4.1765e-06, -2.0366e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2188e-08, 2.8228e-08, 3.5420e-08,  ..., 1.1672e-08, 8.8375e-08,
         1.1502e-08],
        [3.8971e-11, 2.1250e-11, 2.2604e-12,  ..., 2.4126e-11, 1.3135e-12,
         5.3549e-12],
        [3.1225e-10, 1.6072e-10, 7.6889e-12,  ..., 2.6747e-10, 4.6967e-12,
         6.9084e-11],
        [2.9393e-11, 1.4103e-11, 1.0739e-12,  ..., 1.9890e-11, 5.9303e-13,
         4.6877e-12],
        [1.6529e-10, 8.5036e-11, 6.3160e-12,  ..., 1.1504e-10, 5.3887e-12,
         2.6627e-11]], device='cuda:0')
optimizer state dict: 36.0
lr: [1.9401053325731837e-05, 1.9401053325731837e-05]
scheduler_last_epoch: 36


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   221,   474,  ...,  1871,   271,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2059e-08,  1.7211e-05, -2.6279e-05,  ...,  3.4516e-05,
         -2.7267e-05,  2.5028e-05],
        [-3.1739e-06, -2.3991e-06,  6.5565e-07,  ..., -2.7865e-06,
         -8.5309e-07, -1.2890e-06],
        [-3.3379e-06, -2.5332e-06,  6.8918e-07,  ..., -2.9206e-06,
         -8.9779e-07, -1.3560e-06],
        [-3.3081e-06, -2.5034e-06,  6.8545e-07,  ..., -2.9057e-06,
         -8.8662e-07, -1.3486e-06],
        [-7.3314e-06, -5.5432e-06,  1.5125e-06,  ..., -6.4373e-06,
         -1.9670e-06, -2.9802e-06]], device='cuda:0')
Loss: 1.1643729209899902


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0917e-06,  4.1287e-05, -3.0223e-05,  ...,  3.9585e-05,
         -4.0521e-05,  5.2411e-05],
        [-6.3777e-06, -4.8280e-06,  1.2890e-06,  ..., -5.5581e-06,
         -1.7323e-06, -2.6003e-06],
        [-6.7204e-06, -5.0962e-06,  1.3597e-06,  ..., -5.8413e-06,
         -1.8254e-06, -2.7344e-06],
        [-6.6459e-06, -5.0217e-06,  1.3448e-06,  ..., -5.7817e-06,
         -1.7993e-06, -2.7046e-06],
        [-1.4693e-05, -1.1116e-05,  2.9653e-06,  ..., -1.2785e-05,
         -3.9935e-06, -5.9754e-06]], device='cuda:0')
Loss: 1.1567277908325195


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  7180,   266,  ...,  1805,    12,   221],
        [    0,    34,     9,  ...,    19,    14, 45576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8779e-06,  3.8401e-05, -2.1400e-05,  ...,  3.2410e-05,
         -8.2951e-05,  3.2787e-06],
        [-9.5665e-06, -7.2271e-06,  1.9036e-06,  ..., -8.3447e-06,
         -2.6077e-06, -3.9116e-06],
        [-1.0088e-05, -7.6294e-06,  2.0079e-06,  ..., -8.7917e-06,
         -2.7493e-06, -4.1202e-06],
        [-9.9540e-06, -7.5102e-06,  1.9856e-06,  ..., -8.6725e-06,
         -2.7046e-06, -4.0680e-06],
        [-2.1994e-05, -1.6600e-05,  4.3735e-06,  ..., -1.9193e-05,
         -5.9903e-06, -8.9854e-06]], device='cuda:0')
Loss: 1.1655282974243164


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[   0, 5522, 5662,  ...,  638, 1231, 1098],
        [   0,  747, 7890,  ...,  706, 8667,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1591e-05,  5.6298e-05, -2.4175e-05,  ...,  4.4047e-05,
         -8.2951e-05,  2.8066e-05],
        [-1.2800e-05, -9.6560e-06,  2.5630e-06,  ..., -1.1101e-05,
         -3.5055e-06, -5.2154e-06],
        [-1.3500e-05, -1.0192e-05,  2.7046e-06,  ..., -1.1712e-05,
         -3.6992e-06, -5.4985e-06],
        [-1.3322e-05, -1.0043e-05,  2.6748e-06,  ..., -1.1548e-05,
         -3.6396e-06, -5.4240e-06],
        [-2.9445e-05, -2.2203e-05,  5.8934e-06,  ..., -2.5541e-05,
         -8.0615e-06, -1.1995e-05]], device='cuda:0')
Loss: 1.1514304876327515


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,   292,   494,  ...,   259, 14134, 11544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9781e-05,  5.4116e-05, -3.8685e-05,  ...,  4.0495e-05,
         -7.6183e-05,  5.7226e-05],
        [-1.6063e-05, -1.2115e-05,  3.2224e-06,  ..., -1.3947e-05,
         -4.4219e-06, -6.5342e-06],
        [-1.6898e-05, -1.2755e-05,  3.3937e-06,  ..., -1.4663e-05,
         -4.6529e-06, -6.8694e-06],
        [-1.6674e-05, -1.2562e-05,  3.3528e-06,  ..., -1.4454e-05,
         -4.5747e-06, -6.7726e-06],
        [-3.6895e-05, -2.7806e-05,  7.3984e-06,  ..., -3.2008e-05,
         -1.0148e-05, -1.4991e-05]], device='cuda:0')
Loss: 1.1750774383544922


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[   0,  409,  394,  ...,  475, 5458,  328],
        [   0,  395, 4973,  ..., 5851,  409, 4370]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8676e-05,  8.3249e-05, -4.8796e-05,  ...,  7.0667e-05,
         -8.3389e-05,  2.9074e-05],
        [-1.9222e-05, -1.4514e-05,  3.8631e-06,  ..., -1.6689e-05,
         -5.2825e-06, -7.8157e-06],
        [-2.0236e-05, -1.5289e-05,  4.0717e-06,  ..., -1.7554e-05,
         -5.5581e-06, -8.2254e-06],
        [-1.9997e-05, -1.5080e-05,  4.0270e-06,  ..., -1.7330e-05,
         -5.4762e-06, -8.1137e-06],
        [-4.4227e-05, -3.3349e-05,  8.8811e-06,  ..., -3.8356e-05,
         -1.2130e-05, -1.7956e-05]], device='cuda:0')
Loss: 1.1756986379623413


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4215,  1478,  ...,   644,   409,  3803],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2274e-05,  9.4955e-05, -8.2181e-05,  ...,  6.3159e-05,
         -7.5340e-05,  5.4117e-05],
        [-2.2441e-05, -1.6928e-05,  4.4890e-06,  ..., -1.9476e-05,
         -6.1505e-06, -9.1046e-06],
        [-2.3633e-05, -1.7837e-05,  4.7348e-06,  ..., -2.0489e-05,
         -6.4708e-06, -9.5814e-06],
        [-2.3350e-05, -1.7583e-05,  4.6827e-06,  ..., -2.0221e-05,
         -6.3740e-06, -9.4548e-06],
        [-5.1647e-05, -3.8922e-05,  1.0327e-05,  ..., -4.4763e-05,
         -1.4126e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.1558184623718262


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[   0,  292,  474,  ..., 1085,  494, 2665],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5092e-05,  1.0443e-04, -1.2085e-04,  ...,  9.2191e-05,
         -9.8257e-05,  5.4117e-05],
        [-2.5645e-05, -1.9342e-05,  5.1036e-06,  ..., -2.2247e-05,
         -7.0408e-06, -1.0386e-05],
        [-2.7031e-05, -2.0385e-05,  5.3868e-06,  ..., -2.3425e-05,
         -7.4133e-06, -1.0945e-05],
        [-2.6673e-05, -2.0087e-05,  5.3234e-06,  ..., -2.3097e-05,
         -7.2978e-06, -1.0788e-05],
        [-5.9009e-05, -4.4435e-05,  1.1735e-05,  ..., -5.1111e-05,
         -1.6168e-05, -2.3872e-05]], device='cuda:0')
Loss: 1.155287265777588
Graident accumulation at epoch 0, step 295, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0222, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0159, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5884e-05, -1.9177e-05, -2.2370e-05,  ...,  2.7299e-05,
         -5.0628e-05,  5.2824e-06],
        [-2.3869e-05, -1.6958e-05,  3.6875e-06,  ..., -2.0120e-05,
         -3.1857e-06, -1.0495e-05],
        [ 3.0369e-05,  2.2380e-05, -1.8719e-06,  ...,  2.9825e-05,
          2.0587e-06,  1.5039e-05],
        [-2.8467e-05, -1.9996e-05,  4.5483e-06,  ..., -2.3741e-05,
         -4.0386e-06, -1.2289e-05],
        [-4.9881e-05, -3.5146e-05,  7.2869e-06,  ..., -4.1365e-05,
         -5.3756e-06, -2.0717e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2153e-08, 2.8211e-08, 3.5400e-08,  ..., 1.1669e-08, 8.8297e-08,
         1.1494e-08],
        [3.9589e-11, 2.1602e-11, 2.2841e-12,  ..., 2.4597e-11, 1.3618e-12,
         5.4574e-12],
        [3.1267e-10, 1.6097e-10, 7.7103e-12,  ..., 2.6775e-10, 4.7469e-12,
         6.9135e-11],
        [3.0075e-11, 1.4492e-11, 1.1011e-12,  ..., 2.0403e-11, 6.4570e-13,
         4.7994e-12],
        [1.6861e-10, 8.6926e-11, 6.4474e-12,  ..., 1.1754e-10, 5.6447e-12,
         2.7170e-11]], device='cuda:0')
optimizer state dict: 37.0
lr: [1.9358197179963892e-05, 1.9358197179963892e-05]
scheduler_last_epoch: 37


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0, 25938,   359,  ...,    36, 15859,   504],
        [    0,   382,    17,  ...,  8733,    13,  9306]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0621e-06,  7.7747e-07, -1.4076e-05,  ..., -2.3961e-06,
         -6.0715e-06, -5.6316e-05],
        [-3.1590e-06, -2.4140e-06,  6.8173e-07,  ..., -2.7716e-06,
         -8.9779e-07, -1.3188e-06],
        [-3.3677e-06, -2.5779e-06,  7.2643e-07,  ..., -2.9653e-06,
         -9.6112e-07, -1.4007e-06],
        [-3.3230e-06, -2.5332e-06,  7.1526e-07,  ..., -2.9206e-06,
         -9.3877e-07, -1.3784e-06],
        [-7.2420e-06, -5.5134e-06,  1.5572e-06,  ..., -6.3479e-06,
         -2.0564e-06, -3.0100e-06]], device='cuda:0')
Loss: 1.1524326801300049


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,   298, 49038,  ...,   288,  1690,  2736],
        [    0,   923,    13,  ...,   199,   677,  3826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3151e-05,  3.4591e-05,  2.9151e-05,  ..., -9.4002e-06,
          1.4517e-06, -4.2636e-06],
        [-6.3330e-06, -4.8280e-06,  1.3150e-06,  ..., -5.5879e-06,
         -1.8440e-06, -2.6226e-06],
        [-6.7204e-06, -5.1260e-06,  1.3970e-06,  ..., -5.9456e-06,
         -1.9595e-06, -2.7791e-06],
        [-6.6310e-06, -5.0366e-06,  1.3746e-06,  ..., -5.8562e-06,
         -1.9222e-06, -2.7344e-06],
        [-1.4544e-05, -1.1057e-05,  3.0100e-06,  ..., -1.2815e-05,
         -4.2170e-06, -5.9903e-06]], device='cuda:0')
Loss: 1.1693840026855469


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  1086,  5564,  ..., 29319, 32982,   344],
        [    0,   391,  1761,  ...,   346,    14,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7657e-05,  2.9823e-05,  4.0853e-05,  ..., -4.9692e-06,
         -4.3381e-05, -1.0106e-05],
        [-9.4771e-06, -7.2569e-06,  1.9856e-06,  ..., -8.3745e-06,
         -2.7530e-06, -3.8892e-06],
        [-1.0058e-05, -7.7039e-06,  2.1122e-06,  ..., -8.8960e-06,
         -2.9206e-06, -4.1276e-06],
        [-9.9242e-06, -7.5847e-06,  2.0824e-06,  ..., -8.7768e-06,
         -2.8722e-06, -4.0606e-06],
        [-2.1726e-05, -1.6600e-05,  4.5449e-06,  ..., -1.9163e-05,
         -6.2883e-06, -8.8811e-06]], device='cuda:0')
Loss: 1.1595276594161987


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0, 12449,    12,  ...,   292,  2178,   413],
        [    0,   287,  2269,  ..., 22413,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8232e-05,  4.0972e-05,  2.6556e-05,  ..., -1.0213e-05,
         -5.5243e-05,  3.6144e-06],
        [-1.2696e-05, -9.6709e-06,  2.6301e-06,  ..., -1.1206e-05,
         -3.6992e-06, -5.2154e-06],
        [-1.3456e-05, -1.0252e-05,  2.7940e-06,  ..., -1.1891e-05,
         -3.9265e-06, -5.5283e-06],
        [-1.3277e-05, -1.0103e-05,  2.7567e-06,  ..., -1.1727e-05,
         -3.8557e-06, -5.4389e-06],
        [-2.9027e-05, -2.2054e-05,  5.9977e-06,  ..., -2.5570e-05,
         -8.4341e-06, -1.1876e-05]], device='cuda:0')
Loss: 1.1388843059539795


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[    0,  2973, 20362,  ...,   271, 43821, 11776],
        [    0,    14,  4746,  ...,   266,  1119,  1705]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4401e-04,  6.6806e-05,  3.9541e-05,  ..., -2.1855e-05,
         -6.9521e-05,  1.5893e-05],
        [-1.5885e-05, -1.2070e-05,  3.2745e-06,  ..., -1.4007e-05,
         -4.6119e-06, -6.4895e-06],
        [ 7.1458e-05,  5.5075e-05, -2.2342e-05,  ...,  6.9147e-05,
          2.6389e-05,  4.8831e-05],
        [-1.6615e-05, -1.2621e-05,  3.4347e-06,  ..., -1.4663e-05,
         -4.8093e-06, -6.7726e-06],
        [-3.6329e-05, -2.7567e-05,  7.4729e-06,  ..., -3.2008e-05,
         -1.0520e-05, -1.4782e-05]], device='cuda:0')
Loss: 1.1711704730987549


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,  4889,  3593,  ..., 19787,   287, 22475],
        [    0,    24,    15,  ...,   221,   380,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4146e-04,  5.8619e-05, -1.4447e-05,  ..., -2.0717e-05,
         -1.0203e-04, -1.7996e-05],
        [-1.9059e-05, -1.4484e-05,  3.9265e-06,  ..., -1.6794e-05,
         -5.5172e-06, -7.8008e-06],
        [ 6.8106e-05,  5.2527e-05, -2.1652e-05,  ...,  6.6197e-05,
          2.5436e-05,  4.7453e-05],
        [-1.9938e-05, -1.5140e-05,  4.1164e-06,  ..., -1.7583e-05,
         -5.7556e-06, -8.1435e-06],
        [-4.3601e-05, -3.3110e-05,  8.9630e-06,  ..., -3.8415e-05,
         -1.2591e-05, -1.7777e-05]], device='cuda:0')
Loss: 1.1483112573623657


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[   0,  822, 5085,  ...,  293, 1608,  391],
        [   0,  677, 6499,  ..., 2738,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8911e-04,  5.8221e-05, -2.5827e-05,  ..., -5.4203e-05,
         -1.0089e-04, -3.0600e-05],
        [-2.2218e-05, -1.6883e-05,  4.5821e-06,  ..., -1.9595e-05,
         -6.4187e-06, -9.0823e-06],
        [ 6.4753e-05,  4.9978e-05, -2.0956e-05,  ...,  6.3232e-05,
          2.4482e-05,  4.6097e-05],
        [-2.3261e-05, -1.7673e-05,  4.8093e-06,  ..., -2.0534e-05,
         -6.7055e-06, -9.4846e-06],
        [-5.0873e-05, -3.8654e-05,  1.0468e-05,  ..., -4.4852e-05,
         -1.4678e-05, -2.0713e-05]], device='cuda:0')
Loss: 1.1694362163543701


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[    0,  1486,   292,  ...,  7484,    15,  5357],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1481e-04,  4.7598e-05, -3.9706e-05,  ..., -5.0964e-05,
         -9.5275e-05, -1.5254e-05],
        [-2.5421e-05, -1.9312e-05,  5.2303e-06,  ..., -2.2411e-05,
         -7.3500e-06, -1.0379e-05],
        [ 6.1370e-05,  4.7415e-05, -2.0267e-05,  ...,  6.0251e-05,
          2.3499e-05,  4.4726e-05],
        [-2.6569e-05, -2.0176e-05,  5.4836e-06,  ..., -2.3440e-05,
         -7.6666e-06, -1.0818e-05],
        [-5.8204e-05, -4.4197e-05,  1.1951e-05,  ..., -5.1290e-05,
         -1.6809e-05, -2.3678e-05]], device='cuda:0')
Loss: 1.1637306213378906
Graident accumulation at epoch 0, step 303, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0159, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5815e-05, -1.2499e-05, -2.4103e-05,  ...,  1.9473e-05,
         -5.5092e-05,  3.2287e-06],
        [-2.4024e-05, -1.7194e-05,  3.8418e-06,  ..., -2.0350e-05,
         -3.6021e-06, -1.0483e-05],
        [ 3.3469e-05,  2.4883e-05, -3.7113e-06,  ...,  3.2867e-05,
          4.2026e-06,  1.8007e-05],
        [-2.8277e-05, -2.0014e-05,  4.6418e-06,  ..., -2.3711e-05,
         -4.4014e-06, -1.2142e-05],
        [-5.0713e-05, -3.6051e-05,  7.7532e-06,  ..., -4.2357e-05,
         -6.5189e-06, -2.1013e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2157e-08, 2.8185e-08, 3.5366e-08,  ..., 1.1660e-08, 8.8218e-08,
         1.1483e-08],
        [4.0196e-11, 2.1954e-11, 2.3092e-12,  ..., 2.5074e-11, 1.4144e-12,
         5.5597e-12],
        [3.1612e-10, 1.6306e-10, 8.1133e-12,  ..., 2.7111e-10, 5.2944e-12,
         7.1066e-11],
        [3.0751e-11, 1.4885e-11, 1.1301e-12,  ..., 2.0932e-11, 7.0383e-13,
         4.9116e-12],
        [1.7183e-10, 8.8792e-11, 6.5837e-12,  ..., 1.2005e-10, 5.9216e-12,
         2.7703e-11]], device='cuda:0')
optimizer state dict: 38.0
lr: [1.9313911019977992e-05, 1.9313911019977992e-05]
scheduler_last_epoch: 38


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,   902, 11331,  ...,  1795,   365,   654],
        [    0,  1064,  1042,  ...,    12,   259,  4754]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8734e-05, -5.5520e-06, -7.4957e-06,  ...,  1.7913e-06,
          2.3596e-05,  1.0997e-05],
        [-3.1292e-06, -2.3991e-06,  6.8173e-07,  ..., -2.8014e-06,
         -9.2015e-07, -1.2591e-06],
        [-3.3528e-06, -2.5779e-06,  7.3388e-07,  ..., -3.0100e-06,
         -9.8348e-07, -1.3486e-06],
        [-3.2932e-06, -2.5332e-06,  7.1898e-07,  ..., -2.9504e-06,
         -9.6858e-07, -1.3262e-06],
        [-7.1824e-06, -5.5134e-06,  1.5721e-06,  ..., -6.4373e-06,
         -2.1160e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.1760932207107544


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[   0,   14,  357,  ...,   30,  287,  839],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5199e-06,  2.5700e-05, -4.3443e-06,  ..., -1.3027e-05,
          4.2725e-05,  6.5288e-05],
        [-6.2585e-06, -4.8280e-06,  1.3635e-06,  ..., -5.6177e-06,
         -1.8552e-06, -2.5481e-06],
        [-6.7055e-06, -5.1707e-06,  1.4640e-06,  ..., -6.0052e-06,
         -1.9819e-06, -2.7269e-06],
        [-6.5863e-06, -5.0962e-06,  1.4380e-06,  ..., -5.9009e-06,
         -1.9521e-06, -2.6822e-06],
        [-1.4395e-05, -1.1086e-05,  3.1367e-06,  ..., -1.2875e-05,
         -4.2617e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.1676498651504517


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,  2270,   278,  ..., 36325,  5892,  3558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0473e-05,  3.9423e-05, -6.8625e-06,  ..., -2.4657e-05,
          2.9227e-05,  8.6565e-05],
        [-9.3728e-06, -7.2420e-06,  2.0526e-06,  ..., -8.3894e-06,
         -2.7679e-06, -3.7923e-06],
        [ 1.4838e-04,  1.5099e-04, -5.5383e-05,  ...,  1.6200e-04,
          5.8088e-05,  6.2704e-05],
        [-9.8646e-06, -7.6443e-06,  2.1681e-06,  ..., -8.8066e-06,
         -2.9057e-06, -3.9861e-06],
        [-2.1547e-05, -1.6630e-05,  4.7237e-06,  ..., -1.9222e-05,
         -6.3479e-06, -8.6874e-06]], device='cuda:0')
Loss: 1.1539565324783325


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   688,  2353,  ..., 20538, 10393,    12],
        [    0, 10296,   809,  ..., 27683,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7484e-05,  3.8617e-05, -7.4136e-06,  ..., -1.3280e-05,
          6.2750e-05,  6.1112e-05],
        [-1.2487e-05, -9.6560e-06,  2.7269e-06,  ..., -1.1191e-05,
         -3.6806e-06, -5.0813e-06],
        [ 1.4507e-04,  1.4843e-04, -5.4668e-05,  ...,  1.5902e-04,
          5.7119e-05,  6.1333e-05],
        [-1.3158e-05, -1.0192e-05,  2.8796e-06,  ..., -1.1757e-05,
         -3.8669e-06, -5.3495e-06],
        [-2.8700e-05, -2.2173e-05,  6.2659e-06,  ..., -2.5660e-05,
         -8.4490e-06, -1.1638e-05]], device='cuda:0')
Loss: 1.1664867401123047


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0,   16,   14,  ...,  300, 9283,   14],
        [   0,  369,  726,  ...,   83,  409,  729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2391e-05,  4.8310e-05, -2.8313e-05,  ...,  3.1024e-06,
          6.8512e-05,  1.7366e-06],
        [-1.5602e-05, -1.2055e-05,  3.4161e-06,  ..., -1.3977e-05,
         -4.5970e-06, -6.3777e-06],
        [ 1.4177e-04,  1.4587e-04, -5.3934e-05,  ...,  1.5605e-04,
          5.6143e-05,  5.9955e-05],
        [-1.6451e-05, -1.2740e-05,  3.6098e-06,  ..., -1.4707e-05,
         -4.8354e-06, -6.7204e-06],
        [-3.5793e-05, -2.7627e-05,  7.8380e-06,  ..., -3.2008e-05,
         -1.0535e-05, -1.4588e-05]], device='cuda:0')
Loss: 1.166661024093628


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[    0,  4855, 15679,  ...,   278,   266,  1912],
        [    0,  4137,   300,  ...,  2579,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4284e-05,  6.6964e-05, -2.4392e-05,  ..., -6.3526e-06,
          3.4082e-05, -2.4518e-06],
        [-1.8701e-05, -1.4454e-05,  4.0792e-06,  ..., -1.6779e-05,
         -5.4836e-06, -7.6443e-06],
        [ 1.3844e-04,  1.4329e-04, -5.3226e-05,  ...,  1.5306e-04,
          5.5197e-05,  5.8599e-05],
        [-1.9729e-05, -1.5289e-05,  4.3102e-06,  ..., -1.7673e-05,
         -5.7705e-06, -8.0541e-06],
        [-4.2915e-05, -3.3140e-05,  9.3579e-06,  ..., -3.8445e-05,
         -1.2577e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.1639355421066284


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,  1041,    14,  ...,   360,   266, 14966],
        [    0, 20080, 11069,  ...,   300,  5768,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2669e-04,  6.5019e-05,  7.2951e-06,  ..., -4.4380e-07,
          5.2807e-06, -1.8392e-05],
        [-2.1815e-05, -1.6838e-05,  4.7982e-06,  ..., -1.9521e-05,
         -6.4000e-06, -8.9258e-06],
        [ 1.3510e-04,  1.4072e-04, -5.2451e-05,  ...,  1.5011e-04,
          5.4214e-05,  5.7220e-05],
        [-2.3022e-05, -1.7822e-05,  5.0738e-06,  ..., -2.0579e-05,
         -6.7391e-06, -9.4101e-06],
        [-5.0098e-05, -3.8654e-05,  1.1012e-05,  ..., -4.4763e-05,
         -1.4693e-05, -2.0444e-05]], device='cuda:0')
Loss: 1.1501781940460205


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[    0,  2241,  8274,  ...,   908,  1811,   278],
        [    0,  6660, 13165,  ...,   380,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2692e-04,  5.7709e-05,  2.3356e-05,  ...,  1.5071e-05,
          3.6684e-06, -3.2814e-05],
        [-2.4974e-05, -1.9252e-05,  5.5023e-06,  ..., -2.2322e-05,
         -7.3425e-06, -1.0207e-05],
        [ 1.3171e-04,  1.3812e-04, -5.1691e-05,  ...,  1.4710e-04,
          5.3200e-05,  5.5850e-05],
        [-2.6345e-05, -2.0370e-05,  5.8189e-06,  ..., -2.3514e-05,
         -7.7300e-06, -1.0751e-05],
        [-5.7340e-05, -4.4197e-05,  1.2621e-05,  ..., -5.1171e-05,
         -1.6853e-05, -2.3365e-05]], device='cuda:0')
Loss: 1.1706254482269287
Graident accumulation at epoch 0, step 311, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7542e-05, -5.4784e-06, -1.9357e-05,  ...,  1.9033e-05,
         -4.9216e-05, -3.7551e-07],
        [-2.4119e-05, -1.7400e-05,  4.0079e-06,  ..., -2.0547e-05,
         -3.9762e-06, -1.0456e-05],
        [ 4.3293e-05,  3.6207e-05, -8.5093e-06,  ...,  4.4290e-05,
          9.1024e-06,  2.1792e-05],
        [-2.8084e-05, -2.0050e-05,  4.7595e-06,  ..., -2.3692e-05,
         -4.7343e-06, -1.2003e-05],
        [-5.1376e-05, -3.6865e-05,  8.2400e-06,  ..., -4.3239e-05,
         -7.5523e-06, -2.1248e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2131e-08, 2.8160e-08, 3.5331e-08,  ..., 1.1648e-08, 8.8129e-08,
         1.1472e-08],
        [4.0780e-11, 2.2302e-11, 2.3372e-12,  ..., 2.5547e-11, 1.4669e-12,
         5.6583e-12],
        [3.3315e-10, 1.8197e-10, 1.0777e-11,  ..., 2.9248e-10, 8.1194e-12,
         7.4114e-11],
        [3.1414e-11, 1.5285e-11, 1.1628e-12,  ..., 2.1464e-11, 7.6288e-13,
         5.0223e-12],
        [1.7494e-10, 9.0657e-11, 6.7364e-12,  ..., 1.2255e-10, 6.1997e-12,
         2.8222e-11]], device='cuda:0')
optimizer state dict: 39.0
lr: [1.9268201613085963e-05, 1.9268201613085963e-05]
scheduler_last_epoch: 39


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   287,   358,  ...,   328,  1704,  3227],
        [    0,   474,   221,  ...,   287, 20640,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0829e-05,  1.5533e-05,  9.6961e-06,  ...,  5.5762e-06,
         -1.4627e-05,  1.9077e-05],
        [-3.0845e-06, -2.4140e-06,  6.9290e-07,  ..., -2.8163e-06,
         -8.9779e-07, -1.2666e-06],
        [-3.3230e-06, -2.6077e-06,  7.4878e-07,  ..., -3.0249e-06,
         -9.6858e-07, -1.3635e-06],
        [-3.2485e-06, -2.5481e-06,  7.3388e-07,  ..., -2.9653e-06,
         -9.4250e-07, -1.3262e-06],
        [-7.0632e-06, -5.5134e-06,  1.5870e-06,  ..., -6.4075e-06,
         -2.0564e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1679291725158691


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[   0,  365, 8790,  ..., 1172, 8806,  266],
        [   0,  278, 4191,  ...,  381, 3020,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0368e-05,  2.7379e-05, -2.2418e-05,  ...,  1.5417e-05,
         -9.0728e-06,  8.0694e-06],
        [-6.1542e-06, -4.7982e-06,  1.4119e-06,  ..., -5.6177e-06,
         -1.7881e-06, -2.5406e-06],
        [-6.6310e-06, -5.1856e-06,  1.5236e-06,  ..., -6.0499e-06,
         -1.9297e-06, -2.7344e-06],
        [-6.4969e-06, -5.0813e-06,  1.4976e-06,  ..., -5.9307e-06,
         -1.8850e-06, -2.6748e-06],
        [-1.4067e-05, -1.0967e-05,  3.2187e-06,  ..., -1.2785e-05,
         -4.0829e-06, -5.7667e-06]], device='cuda:0')
Loss: 1.1555147171020508


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[    0,  7030,   631,  ..., 34748,    12,   298],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4309e-05,  3.0979e-05, -2.1772e-05,  ...,  1.8483e-05,
         -2.8911e-05,  1.8882e-05],
        [-9.2238e-06, -7.2122e-06,  2.1309e-06,  ..., -8.3894e-06,
         -2.6971e-06, -3.8221e-06],
        [ 4.4890e-04,  3.2870e-04, -9.1787e-05,  ...,  4.3209e-04,
          1.5279e-04,  2.0754e-04],
        [-9.7603e-06, -7.6443e-06,  2.2613e-06,  ..., -8.8811e-06,
         -2.8461e-06, -4.0382e-06],
        [-2.1040e-05, -1.6451e-05,  4.8503e-06,  ..., -1.9103e-05,
         -6.1393e-06, -8.6725e-06]], device='cuda:0')
Loss: 1.1617859601974487


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[   0,  259, 6887,  ..., 1400,  292,  474],
        [   0,   12, 3518,  ..., 1580, 2573,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2955e-05,  3.7945e-05, -3.5148e-05,  ...,  3.1859e-05,
         -3.9704e-05,  1.4361e-05],
        [-1.2323e-05, -9.6262e-06,  2.8275e-06,  ..., -1.1176e-05,
         -3.6135e-06, -5.0962e-06],
        [ 4.4555e-04,  3.2609e-04, -9.1034e-05,  ...,  4.2908e-04,
          1.5180e-04,  2.0616e-04],
        [-1.3053e-05, -1.0207e-05,  3.0026e-06,  ..., -1.1832e-05,
         -3.8147e-06, -5.3942e-06],
        [-2.8163e-05, -2.2024e-05,  6.4522e-06,  ..., -2.5481e-05,
         -8.2403e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.1508692502975464


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[    0,   957,  1357,  ..., 26179,   287,  6458],
        [    0,   287,   552,  ...,  7407,  2401,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1512e-05,  2.3401e-05, -5.5567e-05,  ...,  3.1128e-05,
         -4.7534e-05,  1.4361e-05],
        [-1.5423e-05, -1.2025e-05,  3.5353e-06,  ..., -1.3947e-05,
         -4.5262e-06, -6.3702e-06],
        [ 4.4221e-04,  3.2350e-04, -9.0270e-05,  ...,  4.2610e-04,
          1.5082e-04,  2.0479e-04],
        [-1.6332e-05, -1.2755e-05,  3.7551e-06,  ..., -1.4767e-05,
         -4.7833e-06, -6.7428e-06],
        [-3.5256e-05, -2.7508e-05,  8.0764e-06,  ..., -3.1829e-05,
         -1.0341e-05, -1.4499e-05]], device='cuda:0')
Loss: 1.1641130447387695


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1848e-05,  1.6382e-05, -6.4296e-05,  ...,  2.2390e-05,
         -6.6468e-05,  1.4346e-05],
        [-1.8492e-05, -1.4424e-05,  4.2357e-06,  ..., -1.6689e-05,
         -5.4091e-06, -7.6294e-06],
        [ 4.3886e-04,  3.2087e-04, -8.9503e-05,  ...,  4.2310e-04,
          1.4985e-04,  2.0342e-04],
        [-1.9625e-05, -1.5333e-05,  4.5076e-06,  ..., -1.7717e-05,
         -5.7295e-06, -8.0913e-06],
        [-4.2349e-05, -3.3051e-05,  9.7007e-06,  ..., -3.8177e-05,
         -1.2383e-05, -1.7405e-05]], device='cuda:0')
Loss: 1.1446154117584229


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,   12,  287,  ...,   12, 5576,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2801e-05,  2.9711e-05, -7.9787e-05,  ...,  2.4427e-05,
         -5.2695e-05,  3.8658e-05],
        [-2.1592e-05, -1.6868e-05,  4.9546e-06,  ..., -1.9506e-05,
         -6.3106e-06, -8.8811e-06],
        [ 4.3555e-04,  3.1825e-04, -8.8732e-05,  ...,  4.2008e-04,
          1.4888e-04,  2.0207e-04],
        [-2.2888e-05, -1.7911e-05,  5.2676e-06,  ..., -2.0683e-05,
         -6.6794e-06, -9.4101e-06],
        [-4.9353e-05, -3.8564e-05,  1.1325e-05,  ..., -4.4525e-05,
         -1.4424e-05, -2.0221e-05]], device='cuda:0')
Loss: 1.166529893875122


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,    20,     9,  ...,    12,  2212, 24950],
        [    0,   380,  8157,  ...,   943,   352,  2278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0888e-05,  2.1690e-05, -8.5009e-05,  ...,  4.8831e-05,
         -6.0134e-05,  2.8797e-05],
        [-2.4647e-05, -1.9267e-05,  5.6773e-06,  ..., -2.2277e-05,
         -7.2084e-06, -1.0133e-05],
        [ 4.3224e-04,  3.1566e-04, -8.7953e-05,  ...,  4.1708e-04,
          1.4791e-04,  2.0072e-04],
        [-2.6152e-05, -2.0474e-05,  6.0387e-06,  ..., -2.3633e-05,
         -7.6331e-06, -1.0744e-05],
        [-5.6326e-05, -4.4048e-05,  1.2971e-05,  ..., -5.0843e-05,
         -1.6466e-05, -2.3082e-05]], device='cuda:0')
Loss: 1.160436987876892
Graident accumulation at epoch 0, step 319, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0077,  0.0032,  ..., -0.0095, -0.0021, -0.0339],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4876e-05, -2.7616e-06, -2.5923e-05,  ...,  2.2013e-05,
         -5.0308e-05,  2.5417e-06],
        [-2.4172e-05, -1.7586e-05,  4.1748e-06,  ..., -2.0720e-05,
         -4.2994e-06, -1.0423e-05],
        [ 8.2188e-05,  6.4152e-05, -1.6454e-05,  ...,  8.1570e-05,
          2.2983e-05,  3.9684e-05],
        [-2.7891e-05, -2.0092e-05,  4.8874e-06,  ..., -2.3686e-05,
         -5.0242e-06, -1.1877e-05],
        [-5.1871e-05, -3.7584e-05,  8.7132e-06,  ..., -4.3999e-05,
         -8.4437e-06, -2.1431e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2089e-08, 2.8132e-08, 3.5303e-08,  ..., 1.1639e-08, 8.8045e-08,
         1.1461e-08],
        [4.1346e-11, 2.2651e-11, 2.3671e-12,  ..., 2.6018e-11, 1.5174e-12,
         5.7554e-12],
        [5.1965e-10, 2.8143e-10, 1.8502e-11,  ..., 4.6614e-10, 2.9989e-11,
         1.1433e-10],
        [3.2067e-11, 1.5689e-11, 1.1982e-12,  ..., 2.2001e-11, 8.2038e-13,
         5.1327e-12],
        [1.7794e-10, 9.2507e-11, 6.8980e-12,  ..., 1.2502e-10, 6.4646e-12,
         2.8726e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
Epoch 0 | Batch 319/1048 | Training PPL: 10758.978594143282 | time 30.2725031375885
Saving checkpoint at epoch 0, step 319, batch 319
Epoch 0 | Validation PPL: 10.067967359066001 | Learning rate: 1.9221075944084176e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319, AFTER epoch 0, step 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  2372,  1319,  ...,  1253,   292, 34166],
        [    0,   494,   221,  ...,   298,  1062,  4923]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-3.4876e-05, -2.7616e-06, -2.5923e-05,  ...,  2.2013e-05,
         -5.0308e-05,  2.5417e-06],
        [-2.4172e-05, -1.7586e-05,  4.1748e-06,  ..., -2.0720e-05,
         -4.2994e-06, -1.0423e-05],
        [ 8.2188e-05,  6.4152e-05, -1.6454e-05,  ...,  8.1570e-05,
          2.2983e-05,  3.9684e-05],
        [-2.7891e-05, -2.0092e-05,  4.8874e-06,  ..., -2.3686e-05,
         -5.0242e-06, -1.1877e-05],
        [-5.1871e-05, -3.7584e-05,  8.7132e-06,  ..., -4.3999e-05,
         -8.4437e-06, -2.1431e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2089e-08, 2.8132e-08, 3.5303e-08,  ..., 1.1639e-08, 8.8045e-08,
         1.1461e-08],
        [4.1346e-11, 2.2651e-11, 2.3671e-12,  ..., 2.6018e-11, 1.5174e-12,
         5.7554e-12],
        [5.1965e-10, 2.8143e-10, 1.8502e-11,  ..., 4.6614e-10, 2.9989e-11,
         1.1433e-10],
        [3.2067e-11, 1.5689e-11, 1.1982e-12,  ..., 2.2001e-11, 8.2038e-13,
         5.1327e-12],
        [1.7794e-10, 9.2507e-11, 6.8980e-12,  ..., 1.2502e-10, 6.4646e-12,
         2.8726e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9426e-06,  4.0031e-05, -6.4102e-05,  ...,  1.8645e-05,
         -1.5636e-05, -3.2111e-05],
        [-3.0100e-06, -2.3991e-06,  7.3388e-07,  ..., -2.7567e-06,
         -8.9407e-07, -1.2890e-06],
        [-3.2783e-06, -2.6077e-06,  8.0094e-07,  ..., -3.0100e-06,
         -9.7603e-07, -1.4082e-06],
        [-3.2187e-06, -2.5630e-06,  7.8604e-07,  ..., -2.9504e-06,
         -9.5367e-07, -1.3784e-06],
        [-6.8545e-06, -5.4538e-06,  1.6689e-06,  ..., -6.2883e-06,
         -2.0415e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.150023341178894


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,  2734,  2703,  ...,  7851,   280,  1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0340e-05,  3.4363e-05, -4.0503e-05,  ..., -1.1180e-05,
         -2.2267e-05, -3.9125e-05],
        [-6.0797e-06, -4.7982e-06,  1.4193e-06,  ..., -5.4985e-06,
         -1.8030e-06, -2.5406e-06],
        [-6.6161e-06, -5.2303e-06,  1.5497e-06,  ..., -6.0052e-06,
         -1.9670e-06, -2.7716e-06],
        [-6.4671e-06, -5.1111e-06,  1.5162e-06,  ..., -5.8562e-06,
         -1.9148e-06, -2.7046e-06],
        [-1.3918e-05, -1.0997e-05,  3.2485e-06,  ..., -1.2606e-05,
         -4.1425e-06, -5.8264e-06]], device='cuda:0')
Loss: 1.165352702140808


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[    0,   271,  4787,  ...,   292,   494,   221],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0953e-06,  7.4649e-06, -4.8214e-05,  ..., -1.2652e-05,
         -2.0078e-05, -6.7745e-05],
        [-9.1493e-06, -7.2420e-06,  2.1234e-06,  ..., -8.2701e-06,
         -2.6673e-06, -3.7849e-06],
        [-9.9540e-06, -7.8827e-06,  2.3134e-06,  ..., -9.0152e-06,
         -2.9095e-06, -4.1276e-06],
        [-9.7156e-06, -7.7039e-06,  2.2613e-06,  ..., -8.7917e-06,
         -2.8312e-06, -4.0233e-06],
        [-2.0921e-05, -1.6570e-05,  4.8503e-06,  ..., -1.8924e-05,
         -6.1244e-06, -8.6576e-06]], device='cuda:0')
Loss: 1.1557785272598267


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,   287, 16974,  ...,   300,  2283,  4013]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6574e-07, -7.6290e-06, -4.7925e-05,  ..., -1.8508e-06,
         -2.3925e-05, -5.6646e-05],
        [-1.2204e-05, -9.6709e-06,  2.8424e-06,  ..., -1.1012e-05,
         -3.5465e-06, -5.0366e-06],
        [-1.3322e-05, -1.0550e-05,  3.1032e-06,  ..., -1.2025e-05,
         -3.8706e-06, -5.5060e-06],
        [-1.2979e-05, -1.0297e-05,  3.0287e-06,  ..., -1.1712e-05,
         -3.7663e-06, -5.3570e-06],
        [-2.7984e-05, -2.2173e-05,  6.5044e-06,  ..., -2.5243e-05,
         -8.1509e-06, -1.1548e-05]], device='cuda:0')
Loss: 1.1627811193466187


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[   0,   69,  462,  ...,  437,  266,  634],
        [   0, 1890,  278,  ...,  578,   72,  815]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8316e-06, -1.0821e-05, -1.2202e-04,  ..., -6.3317e-06,
         -2.6276e-05, -4.5142e-05],
        [-1.5244e-05, -1.2100e-05,  3.5726e-06,  ..., -1.3784e-05,
         -4.4331e-06, -6.3106e-06],
        [-1.6645e-05, -1.3217e-05,  3.9004e-06,  ..., -1.5050e-05,
         -4.8392e-06, -6.8918e-06],
        [-1.6227e-05, -1.2904e-05,  3.8110e-06,  ..., -1.4678e-05,
         -4.7125e-06, -6.7130e-06],
        [-3.4899e-05, -2.7686e-05,  8.1584e-06,  ..., -3.1531e-05,
         -1.0163e-05, -1.4424e-05]], device='cuda:0')
Loss: 1.1383451223373413


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[   0,  377,  472,  ..., 9256, 3807, 5499],
        [   0, 6203,  352,  ...,  266, 3437,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8316e-06, -9.9882e-06, -1.5343e-04,  ...,  6.4236e-06,
         -4.2174e-05, -6.2712e-05],
        [-1.8284e-05, -1.4514e-05,  4.2990e-06,  ..., -1.6555e-05,
         -5.3048e-06, -7.5772e-06],
        [-1.9968e-05, -1.5855e-05,  4.6939e-06,  ..., -1.8075e-05,
         -5.7891e-06, -8.2701e-06],
        [-1.9476e-05, -1.5482e-05,  4.5858e-06,  ..., -1.7643e-05,
         -5.6401e-06, -8.0615e-06],
        [-4.1842e-05, -3.3200e-05,  9.8124e-06,  ..., -3.7849e-05,
         -1.2159e-05, -1.7315e-05]], device='cuda:0')
Loss: 1.1594507694244385


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[    0, 47831,   266,  ...,    66,    17, 20005],
        [    0,  1016,  1387,  ..., 12156, 14838,  3550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0103e-05,  5.7777e-06, -1.4872e-04,  ..., -9.5980e-06,
         -3.8852e-05, -5.8078e-05],
        [-2.1324e-05, -1.6898e-05,  4.9993e-06,  ..., -1.9312e-05,
         -6.1616e-06, -8.8364e-06],
        [-2.3276e-05, -1.8463e-05,  5.4613e-06,  ..., -2.1085e-05,
         -6.7241e-06, -9.6485e-06],
        [-2.2724e-05, -1.8045e-05,  5.3383e-06,  ..., -2.0593e-05,
         -6.5528e-06, -9.4101e-06],
        [-4.8786e-05, -3.8683e-05,  1.1414e-05,  ..., -4.4167e-05,
         -1.4126e-05, -2.0206e-05]], device='cuda:0')
Loss: 1.1714296340942383


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  341,  298,  ...,  298, 1304,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8000e-05,  3.3076e-05, -1.2795e-04,  ..., -2.1903e-05,
         -3.6947e-05, -7.2954e-05],
        [-2.4348e-05, -1.9297e-05,  5.7332e-06,  ..., -2.2039e-05,
         -7.0408e-06, -1.0088e-05],
        [-2.6599e-05, -2.1100e-05,  6.2697e-06,  ..., -2.4095e-05,
         -7.6927e-06, -1.1019e-05],
        [-2.5943e-05, -2.0608e-05,  6.1244e-06,  ..., -2.3514e-05,
         -7.4878e-06, -1.0744e-05],
        [-5.5760e-05, -4.4227e-05,  1.3113e-05,  ..., -5.0485e-05,
         -1.6153e-05, -2.3097e-05]], device='cuda:0')
Loss: 1.1482067108154297
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0077,  0.0032,  ..., -0.0095, -0.0022, -0.0339],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0013],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4189e-05,  8.2211e-07, -3.6126e-05,  ...,  1.7621e-05,
         -4.8972e-05, -5.0079e-06],
        [-2.4190e-05, -1.7757e-05,  4.3306e-06,  ..., -2.0852e-05,
         -4.5735e-06, -1.0390e-05],
        [ 7.1309e-05,  5.5627e-05, -1.4181e-05,  ...,  7.1003e-05,
          1.9916e-05,  3.4614e-05],
        [-2.7696e-05, -2.0144e-05,  5.0111e-06,  ..., -2.3669e-05,
         -5.2705e-06, -1.1764e-05],
        [-5.2260e-05, -3.8248e-05,  9.1532e-06,  ..., -4.4648e-05,
         -9.2146e-06, -2.1598e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2048e-08, 2.8105e-08, 3.5284e-08,  ..., 1.1628e-08, 8.7958e-08,
         1.1455e-08],
        [4.1898e-11, 2.3001e-11, 2.3976e-12,  ..., 2.6478e-11, 1.5655e-12,
         5.8514e-12],
        [5.1984e-10, 2.8159e-10, 1.8523e-11,  ..., 4.6625e-10, 3.0019e-11,
         1.1434e-10],
        [3.2708e-11, 1.6098e-11, 1.2345e-12,  ..., 2.2532e-11, 8.7563e-13,
         5.2430e-12],
        [1.8087e-10, 9.4370e-11, 7.0630e-12,  ..., 1.2744e-10, 6.7191e-12,
         2.9231e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.9172541214186228e-05, 1.9172541214186228e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0,   401,   953,  ..., 10914,   554,  2360],
        [    0,  8353,  1842,  ...,    38,   643,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0236e-05,  6.4969e-06, -2.3425e-05,  ..., -2.1681e-05,
         -7.3310e-06, -2.0154e-06],
        [-2.9802e-06, -2.4289e-06,  7.1526e-07,  ..., -2.7418e-06,
         -8.3819e-07, -1.2740e-06],
        [-3.2932e-06, -2.6822e-06,  7.8604e-07,  ..., -3.0249e-06,
         -9.2387e-07, -1.4082e-06],
        [-3.1888e-06, -2.5928e-06,  7.6368e-07,  ..., -2.9206e-06,
         -8.9407e-07, -1.3635e-06],
        [-6.8247e-06, -5.5730e-06,  1.6317e-06,  ..., -6.2585e-06,
         -1.9222e-06, -2.9057e-06]], device='cuda:0')
Loss: 1.1427762508392334


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[   0,  843, 3365,  ..., 1136, 1615,  292],
        [   0, 1235,   14,  ..., 3301,  549,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8967e-05,  4.1453e-06,  5.6587e-06,  ..., -1.2405e-05,
          1.5528e-05,  1.6805e-05],
        [-6.0201e-06, -4.8578e-06,  1.4417e-06,  ..., -5.4687e-06,
         -1.6727e-06, -2.5257e-06],
        [-6.6161e-06, -5.3346e-06,  1.5795e-06,  ..., -6.0052e-06,
         -1.8366e-06, -2.7716e-06],
        [-6.4373e-06, -5.1856e-06,  1.5385e-06,  ..., -5.8413e-06,
         -1.7844e-06, -2.6971e-06],
        [-1.3769e-05, -1.1116e-05,  3.2857e-06,  ..., -1.2517e-05,
         -3.8296e-06, -5.7518e-06]], device='cuda:0')
Loss: 1.1633317470550537


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0,    12,  4567,  ...,  4154,  1799, 11883],
        [    0,  1099,   644,  ...,  5481,    14,  8782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9076e-05, -1.2212e-05,  5.5641e-06,  ..., -1.4440e-05,
          1.0618e-05,  1.7339e-05],
        [-9.0450e-06, -7.2718e-06,  2.1718e-06,  ..., -8.1807e-06,
         -2.4848e-06, -3.7625e-06],
        [-9.9093e-06, -7.9721e-06,  2.3767e-06,  ..., -8.9705e-06,
         -2.7195e-06, -4.1202e-06],
        [-9.6858e-06, -7.7784e-06,  2.3209e-06,  ..., -8.7470e-06,
         -2.6524e-06, -4.0233e-06],
        [-2.0683e-05, -1.6630e-05,  4.9546e-06,  ..., -1.8716e-05,
         -5.6848e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.1465392112731934


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   591,   688,  ...,   271,  3390,    12],
        [    0,   278, 11554,  ...,  4713,  1039, 17088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6204e-05, -2.8336e-05,  3.5506e-06,  ...,  3.0223e-05,
         -3.4897e-06,  1.2332e-05],
        [-1.2085e-05, -9.7156e-06,  2.8983e-06,  ..., -1.0908e-05,
         -3.3230e-06, -5.0142e-06],
        [-1.3247e-05, -1.0654e-05,  3.1739e-06,  ..., -1.1966e-05,
         -3.6396e-06, -5.4911e-06],
        [-1.2919e-05, -1.0371e-05,  3.0920e-06,  ..., -1.1638e-05,
         -3.5390e-06, -5.3495e-06],
        [-2.7597e-05, -2.2173e-05,  6.6012e-06,  ..., -2.4915e-05,
         -7.5847e-06, -1.1414e-05]], device='cuda:0')
Loss: 1.1538869142532349


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[   0, 1034, 5599,  ...,  259,  586, 1403],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0969e-05, -3.0623e-05, -1.7513e-05,  ...,  7.2525e-06,
         -6.2492e-07, -1.2815e-05],
        [-1.5080e-05, -1.2130e-05,  3.6210e-06,  ..., -1.3635e-05,
         -4.1239e-06, -6.2808e-06],
        [-1.6510e-05, -1.3277e-05,  3.9637e-06,  ..., -1.4946e-05,
         -4.5113e-06, -6.8694e-06],
        [-1.6153e-05, -1.2964e-05,  3.8743e-06,  ..., -1.4588e-05,
         -4.3996e-06, -6.7130e-06],
        [-3.4481e-05, -2.7686e-05,  8.2552e-06,  ..., -3.1173e-05,
         -9.4175e-06, -1.4305e-05]], device='cuda:0')
Loss: 1.160133719444275


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[   0, 1927,  287,  ..., 1027,  271,  266],
        [   0,  266, 3574,  ..., 7052, 3829,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4642e-06, -7.1099e-05, -7.2458e-06,  ..., -4.4788e-05,
         -5.4056e-05, -3.6022e-05],
        [-1.8105e-05, -1.4573e-05,  4.3139e-06,  ..., -1.6361e-05,
         -4.9211e-06, -7.5474e-06],
        [-1.9819e-05, -1.5944e-05,  4.7237e-06,  ..., -1.7926e-05,
         -5.3830e-06, -8.2552e-06],
        [-1.9372e-05, -1.5572e-05,  4.6156e-06,  ..., -1.7494e-05,
         -5.2452e-06, -8.0615e-06],
        [-4.1366e-05, -3.3259e-05,  9.8348e-06,  ..., -3.7402e-05,
         -1.1228e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.1437623500823975


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0, 1549,  824,  ..., 3609,  720,  417],
        [   0, 2700, 5221,  ...,  298,  259,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0356e-05, -6.9140e-05,  8.1686e-08,  ..., -5.0539e-05,
         -9.3217e-05, -2.0110e-05],
        [-2.1115e-05, -1.7017e-05,  5.0366e-06,  ..., -1.9088e-05,
         -5.7593e-06, -8.8289e-06],
        [-2.3097e-05, -1.8597e-05,  5.5060e-06,  ..., -2.0891e-05,
         -6.2920e-06, -9.6411e-06],
        [-2.2590e-05, -1.8179e-05,  5.3868e-06,  ..., -2.0415e-05,
         -6.1393e-06, -9.4250e-06],
        [-4.8190e-05, -3.8773e-05,  1.1466e-05,  ..., -4.3571e-05,
         -1.3128e-05, -2.0072e-05]], device='cuda:0')
Loss: 1.1526153087615967


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  2853, 21042,  ...,  4120,   607, 11176],
        [    0,    15,  2537,  ...,    14,  3544,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0356e-05, -5.5075e-05,  1.4536e-05,  ..., -5.3127e-05,
         -9.7677e-05, -2.9332e-05],
        [-2.4125e-05, -1.9431e-05,  5.7518e-06,  ..., -2.1815e-05,
         -6.5789e-06, -1.0110e-05],
        [-2.6390e-05, -2.1249e-05,  6.2920e-06,  ..., -2.3872e-05,
         -7.1898e-06, -1.1049e-05],
        [-2.5809e-05, -2.0772e-05,  6.1542e-06,  ..., -2.3320e-05,
         -7.0147e-06, -1.0796e-05],
        [-5.5075e-05, -4.4316e-05,  1.3106e-05,  ..., -4.9800e-05,
         -1.5005e-05, -2.3022e-05]], device='cuda:0')
Loss: 1.173558235168457
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9734e-05, -4.7676e-06, -3.1059e-05,  ...,  1.0546e-05,
         -5.3842e-05, -7.4403e-06],
        [-2.4183e-05, -1.7925e-05,  4.4728e-06,  ..., -2.0948e-05,
         -4.7741e-06, -1.0362e-05],
        [ 6.1539e-05,  4.7939e-05, -1.2134e-05,  ...,  6.1516e-05,
          1.7205e-05,  3.0048e-05],
        [-2.7507e-05, -2.0207e-05,  5.1254e-06,  ..., -2.3634e-05,
         -5.4450e-06, -1.1667e-05],
        [-5.2541e-05, -3.8855e-05,  9.5484e-06,  ..., -4.5163e-05,
         -9.7937e-06, -2.1740e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2006e-08, 2.8080e-08, 3.5249e-08,  ..., 1.1619e-08, 8.7880e-08,
         1.1445e-08],
        [4.2438e-11, 2.3356e-11, 2.4283e-12,  ..., 2.6927e-11, 1.6072e-12,
         5.9477e-12],
        [5.2002e-10, 2.8176e-10, 1.8544e-11,  ..., 4.6636e-10, 3.0040e-11,
         1.1434e-10],
        [3.3341e-11, 1.6513e-11, 1.2711e-12,  ..., 2.3053e-11, 9.2396e-13,
         5.3543e-12],
        [1.8372e-10, 9.6240e-11, 7.2277e-12,  ..., 1.2979e-10, 6.9375e-12,
         2.9732e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[    0,  3047,  4878,  ...,   352, 10854, 34025],
        [    0,   271,   266,  ...,  2805,   607, 10848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0763e-05, -1.4276e-05, -1.7540e-05,  ...,  1.2444e-06,
         -1.2755e-05, -2.0840e-05],
        [-2.9802e-06, -2.4140e-06,  7.0035e-07,  ..., -2.7269e-06,
         -7.3761e-07, -1.2591e-06],
        [-3.2634e-06, -2.6375e-06,  7.6368e-07,  ..., -2.9802e-06,
         -8.0466e-07, -1.3709e-06],
        [-3.2037e-06, -2.5928e-06,  7.4878e-07,  ..., -2.9206e-06,
         -7.8604e-07, -1.3486e-06],
        [-6.8545e-06, -5.5134e-06,  1.5944e-06,  ..., -6.2287e-06,
         -1.6838e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.136786699295044


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,  446, 1845,  ...,  422,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0045e-05, -2.3772e-05, -2.4643e-05,  ...,  3.8696e-06,
         -2.2888e-05, -1.0698e-05],
        [-6.0201e-06, -4.8727e-06,  1.4417e-06,  ..., -5.4836e-06,
         -1.5348e-06, -2.5332e-06],
        [-6.5565e-06, -5.3048e-06,  1.5683e-06,  ..., -5.9605e-06,
         -1.6689e-06, -2.7493e-06],
        [-6.4224e-06, -5.2005e-06,  1.5385e-06,  ..., -5.8413e-06,
         -1.6317e-06, -2.6971e-06],
        [-1.3709e-05, -1.1057e-05,  3.2634e-06,  ..., -1.2428e-05,
         -3.4869e-06, -5.7369e-06]], device='cuda:0')
Loss: 1.136494755744934


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[   0,  287, 3284,  ...,  221,  493,  221],
        [   0, 2426,  699,  ...,  221, 1551,  720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0496e-05, -3.4446e-05, -2.6795e-05,  ...,  3.5085e-05,
         -2.5596e-05,  6.2119e-06],
        [ 7.7375e-05,  5.7445e-05, -2.5385e-05,  ...,  7.6153e-05,
          2.2072e-05,  3.8478e-05],
        [-9.8795e-06, -7.9870e-06,  2.3507e-06,  ..., -9.0003e-06,
         -2.5220e-06, -4.1723e-06],
        [-9.6411e-06, -7.7933e-06,  2.2985e-06,  ..., -8.7768e-06,
         -2.4550e-06, -4.0755e-06],
        [-2.0653e-05, -1.6659e-05,  4.8950e-06,  ..., -1.8775e-05,
         -5.2676e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.158063530921936


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[    0,    14,  3609,  ...,   298,   413,    29],
        [    0,    14,   381,  ...,   278,   269, 10376]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7879e-05, -1.7735e-05,  2.3450e-07,  ...,  4.7362e-05,
         -5.1913e-05,  4.3024e-05],
        [ 7.4335e-05,  5.5001e-05, -2.4681e-05,  ...,  7.3441e-05,
          2.1293e-05,  3.7189e-05],
        [-1.3173e-05, -1.0639e-05,  3.1143e-06,  ..., -1.1951e-05,
         -3.3639e-06, -5.5656e-06],
        [-1.2860e-05, -1.0401e-05,  3.0473e-06,  ..., -1.1668e-05,
         -3.2820e-06, -5.4389e-06],
        [-2.7448e-05, -2.2143e-05,  6.4671e-06,  ..., -2.4855e-05,
         -7.0110e-06, -1.1578e-05]], device='cuda:0')
Loss: 1.140223503112793


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[   0, 7692,   12,  ...,  266, 2042,  278],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8209e-05, -8.6105e-06, -2.4185e-05,  ...,  4.5886e-05,
         -7.5426e-05,  1.8353e-05],
        [ 7.1295e-05,  5.2572e-05, -2.3962e-05,  ...,  7.0669e-05,
          2.0522e-05,  3.5907e-05],
        [-1.6481e-05, -1.3292e-05,  3.8967e-06,  ..., -1.4961e-05,
         -4.1984e-06, -6.9588e-06],
        [-1.6108e-05, -1.3009e-05,  3.8184e-06,  ..., -1.4633e-05,
         -4.1053e-06, -6.8098e-06],
        [-3.4273e-05, -2.7627e-05,  8.0839e-06,  ..., -3.1084e-05,
         -8.7395e-06, -1.4454e-05]], device='cuda:0')
Loss: 1.1479220390319824


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 3630, 2199,  ..., 4157,   27, 4765],
        [   0, 3352,  259,  ..., 3565,   12,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9463e-05, -3.0556e-05, -1.9705e-05,  ...,  6.6254e-05,
         -1.1102e-04,  1.9192e-05],
        [ 6.8270e-05,  5.0113e-05, -2.3247e-05,  ...,  6.7912e-05,
          1.9740e-05,  3.4656e-05],
        [-1.9759e-05, -1.5944e-05,  4.6752e-06,  ..., -1.7956e-05,
         -5.0440e-06, -8.3148e-06],
        [-1.9297e-05, -1.5602e-05,  4.5784e-06,  ..., -1.7539e-05,
         -4.9286e-06, -8.1286e-06],
        [-4.1097e-05, -3.3170e-05,  9.6932e-06,  ..., -3.7313e-05,
         -1.0505e-05, -1.7270e-05]], device='cuda:0')
Loss: 1.1535331010818481


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,  1266,  2257,  ..., 27146,  1141,  1196]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5000e-05, -2.8848e-05, -5.8260e-05,  ...,  4.2408e-05,
         -1.1076e-04,  7.6904e-07],
        [ 6.5215e-05,  4.7625e-05, -2.2543e-05,  ...,  6.5171e-05,
          1.8953e-05,  3.3411e-05],
        [-2.3067e-05, -1.8641e-05,  5.4389e-06,  ..., -2.0936e-05,
         -5.8971e-06, -9.6634e-06],
        [-2.2516e-05, -1.8224e-05,  5.3234e-06,  ..., -2.0444e-05,
         -5.7556e-06, -9.4399e-06],
        [-4.7922e-05, -3.8713e-05,  1.1265e-05,  ..., -4.3452e-05,
         -1.2264e-05, -2.0042e-05]], device='cuda:0')
Loss: 1.1427046060562134


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[    0, 11325,   278,  ...,   446,  1869,   642],
        [    0,  3306,  4057,  ...,   287,   266,  1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7405e-05, -5.2664e-05, -6.9374e-05,  ...,  6.3066e-05,
         -9.9826e-05,  5.6829e-05],
        [ 6.2190e-05,  4.5166e-05, -2.1820e-05,  ...,  6.2444e-05,
          1.8201e-05,  3.2182e-05],
        [-2.6360e-05, -2.1309e-05,  6.2250e-06,  ..., -2.3901e-05,
         -6.7167e-06, -1.0997e-05],
        [-2.5734e-05, -2.0832e-05,  6.0908e-06,  ..., -2.3335e-05,
         -6.5565e-06, -1.0744e-05],
        [-5.4747e-05, -4.4256e-05,  1.2890e-05,  ..., -4.9621e-05,
         -1.3970e-05, -2.2814e-05]], device='cuda:0')
Loss: 1.1566780805587769
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3501e-05, -9.5573e-06, -3.4891e-05,  ...,  1.5798e-05,
         -5.8441e-05, -1.0133e-06],
        [-1.5546e-05, -1.1616e-05,  1.8435e-06,  ..., -1.2609e-05,
         -2.4766e-06, -6.1074e-06],
        [ 5.2749e-05,  4.1014e-05, -1.0298e-05,  ...,  5.2974e-05,
          1.4813e-05,  2.5943e-05],
        [-2.7330e-05, -2.0269e-05,  5.2220e-06,  ..., -2.3604e-05,
         -5.5561e-06, -1.1575e-05],
        [-5.2762e-05, -3.9395e-05,  9.8825e-06,  ..., -4.5609e-05,
         -1.0211e-05, -2.1848e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1968e-08, 2.8055e-08, 3.5218e-08,  ..., 1.1611e-08, 8.7802e-08,
         1.1437e-08],
        [4.6263e-11, 2.5372e-11, 2.9020e-12,  ..., 3.0800e-11, 1.9369e-12,
         6.9775e-12],
        [5.2019e-10, 2.8194e-10, 1.8564e-11,  ..., 4.6646e-10, 3.0055e-11,
         1.1435e-10],
        [3.3970e-11, 1.6930e-11, 1.3069e-12,  ..., 2.3575e-11, 9.6602e-13,
         5.4644e-12],
        [1.8654e-10, 9.8102e-11, 7.3866e-12,  ..., 1.3212e-10, 7.1258e-12,
         3.0222e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.90712744520069e-05, 1.90712744520069e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   259,  2180,  ...,   638,  1615,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8010e-06, -4.5678e-06,  0.0000e+00,  ...,  2.0900e-06,
         -2.5197e-05,  1.1017e-05],
        [-3.0249e-06, -2.4587e-06,  6.9290e-07,  ..., -2.7418e-06,
         -7.1526e-07, -1.2815e-06],
        [-3.2187e-06, -2.6226e-06,  7.3761e-07,  ..., -2.9206e-06,
         -7.5996e-07, -1.3560e-06],
        [-3.1739e-06, -2.5779e-06,  7.3016e-07,  ..., -2.8759e-06,
         -7.4878e-07, -1.3337e-06],
        [-6.7949e-06, -5.5134e-06,  1.5572e-06,  ..., -6.1691e-06,
         -1.6019e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.1493808031082153


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0, 10215,   408,  ...,  6071,   360,  1317],
        [    0,  3125,   271,  ...,  1041,  1032,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6761e-05, -3.4822e-05, -2.2931e-05,  ..., -2.7492e-06,
         -3.1136e-05,  2.2907e-05],
        [-6.0797e-06, -4.9770e-06,  1.4380e-06,  ..., -5.5134e-06,
         -1.4529e-06, -2.6003e-06],
        [ 1.7033e-04,  1.4029e-04, -4.2143e-05,  ...,  1.6668e-04,
          2.5167e-05,  8.9599e-05],
        [-6.3628e-06, -5.2005e-06,  1.5087e-06,  ..., -5.7667e-06,
         -1.5162e-06, -2.7046e-06],
        [-1.3649e-05, -1.1146e-05,  3.2187e-06,  ..., -1.2368e-05,
         -3.2485e-06, -5.7966e-06]], device='cuda:0')
Loss: 1.1572803258895874


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0,  2380,  2667,  ...,    14,   381,  5621],
        [    0,    12, 32425,  ...,   389,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0742e-05, -4.5960e-05, -3.0924e-05,  ...,  1.2525e-05,
         -3.6478e-05,  8.0402e-06],
        [-9.1344e-06, -7.4506e-06,  2.1681e-06,  ..., -8.2552e-06,
         -2.1718e-06, -3.9116e-06],
        [ 1.6706e-04,  1.3765e-04, -4.1365e-05,  ...,  1.6374e-04,
          2.4395e-05,  8.8198e-05],
        [-9.5367e-06, -7.7635e-06,  2.2687e-06,  ..., -8.6129e-06,
         -2.2613e-06, -4.0606e-06],
        [-2.0474e-05, -1.6659e-05,  4.8503e-06,  ..., -1.8507e-05,
         -4.8578e-06, -8.7172e-06]], device='cuda:0')
Loss: 1.1692075729370117


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[   0,   12,  344,  ..., 2337, 1122,  408],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7314e-07, -6.1729e-05, -4.6321e-05,  ...,  3.2763e-05,
         -3.7847e-05,  2.7976e-05],
        [-1.2189e-05, -9.9093e-06,  2.9132e-06,  ..., -1.1027e-05,
         -2.9020e-06, -5.2229e-06],
        [ 1.6378e-04,  1.3501e-04, -4.0568e-05,  ...,  1.6077e-04,
          2.3617e-05,  8.6797e-05],
        [-1.2740e-05, -1.0341e-05,  3.0510e-06,  ..., -1.1504e-05,
         -3.0212e-06, -5.4240e-06],
        [-2.7329e-05, -2.2173e-05,  6.5118e-06,  ..., -2.4676e-05,
         -6.4820e-06, -1.1638e-05]], device='cuda:0')
Loss: 1.1590855121612549


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[    0,   456,    17,  ...,  1553, 29477,  2713],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8638e-06, -3.2559e-05, -1.1256e-05,  ...,  1.1801e-05,
         -3.0655e-05,  1.6026e-05],
        [-1.5274e-05, -1.2398e-05,  3.6247e-06,  ..., -1.3813e-05,
         -3.6471e-06, -6.5491e-06],
        [ 1.6051e-04,  1.3236e-04, -3.9811e-05,  ...,  1.5782e-04,
          2.2823e-05,  8.5389e-05],
        [-1.5914e-05, -1.2904e-05,  3.7849e-06,  ..., -1.4365e-05,
         -3.7886e-06, -6.7875e-06],
        [-3.4213e-05, -2.7716e-05,  8.0988e-06,  ..., -3.0875e-05,
         -8.1435e-06, -1.4588e-05]], device='cuda:0')
Loss: 1.1543818712234497


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  2310,   292,  ...,   462,   508,   586],
        [    0, 17900,   554,  ...,   266,  7912,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7735e-06, -2.3301e-05, -1.3911e-05,  ...,  6.2767e-06,
         -5.4896e-05,  2.2682e-05],
        [-1.8314e-05, -1.4871e-05,  4.3623e-06,  ..., -1.6555e-05,
         -4.3921e-06, -7.8604e-06],
        [ 1.5724e-04,  1.2971e-04, -3.9022e-05,  ...,  1.5487e-04,
          2.2026e-05,  8.3988e-05],
        [-1.9088e-05, -1.5482e-05,  4.5560e-06,  ..., -1.7226e-05,
         -4.5635e-06, -8.1509e-06],
        [-4.1038e-05, -3.3259e-05,  9.7528e-06,  ..., -3.7044e-05,
         -9.8199e-06, -1.7524e-05]], device='cuda:0')
Loss: 1.1376736164093018


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,    12,   287,  ...,  4626,    27,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9672e-05, -1.5131e-05, -3.3367e-05,  ..., -1.1384e-05,
         -5.3136e-05,  4.7893e-05],
        [-2.1383e-05, -1.7405e-05,  5.1074e-06,  ..., -1.9312e-05,
         -5.1409e-06, -9.1866e-06],
        [ 1.5398e-04,  1.2702e-04, -3.8232e-05,  ...,  1.5195e-04,
          2.1233e-05,  8.2580e-05],
        [-2.2262e-05, -1.8105e-05,  5.3272e-06,  ..., -2.0072e-05,
         -5.3346e-06, -9.5218e-06],
        [-4.7833e-05, -3.8862e-05,  1.1399e-05,  ..., -4.3154e-05,
         -1.1474e-05, -2.0459e-05]], device='cuda:0')
Loss: 1.1467034816741943


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[   0,  328, 6379,  ...,  287, 1342,    9],
        [   0,  401, 9370,  ...,    9,  287,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7306e-05, -3.9806e-05, -4.7026e-05,  ..., -2.0208e-05,
         -6.9002e-05,  6.9521e-05],
        [-2.4453e-05, -1.9878e-05,  5.8785e-06,  ..., -2.2069e-05,
         -5.8673e-06, -1.0513e-05],
        [ 1.5070e-04,  1.2439e-04, -3.7409e-05,  ...,  1.4902e-04,
          2.0458e-05,  8.1172e-05],
        [-2.5481e-05, -2.0698e-05,  6.1393e-06,  ..., -2.2948e-05,
         -6.0946e-06, -1.0908e-05],
        [-5.4687e-05, -4.4376e-05,  1.3120e-05,  ..., -4.9293e-05,
         -1.3098e-05, -2.3410e-05]], device='cuda:0')
Loss: 1.134730339050293
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0271,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4882e-05, -1.2582e-05, -3.6104e-05,  ...,  1.2197e-05,
         -5.9497e-05,  6.0401e-06],
        [-1.6437e-05, -1.2442e-05,  2.2470e-06,  ..., -1.3555e-05,
         -2.8156e-06, -6.5480e-06],
        [ 6.2544e-05,  4.9352e-05, -1.3009e-05,  ...,  6.2578e-05,
          1.5378e-05,  3.1466e-05],
        [-2.7145e-05, -2.0312e-05,  5.3137e-06,  ..., -2.3538e-05,
         -5.6100e-06, -1.1508e-05],
        [-5.2955e-05, -3.9893e-05,  1.0206e-05,  ..., -4.5977e-05,
         -1.0500e-05, -2.2004e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1929e-08, 2.8028e-08, 3.5185e-08,  ..., 1.1600e-08, 8.7719e-08,
         1.1430e-08],
        [4.6815e-11, 2.5742e-11, 2.9336e-12,  ..., 3.1256e-11, 1.9693e-12,
         7.0810e-12],
        [5.4238e-10, 2.9713e-10, 1.9945e-11,  ..., 4.8820e-10, 3.0444e-11,
         1.2082e-10],
        [3.4586e-11, 1.7342e-11, 1.3433e-12,  ..., 2.4078e-11, 1.0022e-12,
         5.5779e-12],
        [1.8934e-10, 9.9973e-11, 7.5514e-12,  ..., 1.3442e-10, 7.2902e-12,
         3.0740e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0,  1920,    19,  ...,  5232,   796,  1303],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8901e-05, -1.1550e-05, -1.7987e-05,  ...,  6.1143e-06,
          1.5805e-05,  7.6711e-06],
        [-3.0994e-06, -2.5332e-06,  7.3016e-07,  ..., -2.8014e-06,
         -7.1526e-07, -1.3635e-06],
        [-3.2485e-06, -2.6524e-06,  7.6368e-07,  ..., -2.9355e-06,
         -7.4878e-07, -1.4305e-06],
        [-3.1143e-06, -2.5481e-06,  7.3761e-07,  ..., -2.8163e-06,
         -7.1526e-07, -1.3784e-06],
        [-6.8545e-06, -5.6028e-06,  1.6168e-06,  ..., -6.1989e-06,
         -1.5795e-06, -3.0249e-06]], device='cuda:0')
Loss: 1.16524076461792


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,   600,   518,  ...,  3134,   278, 37342],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4091e-05, -3.7563e-05, -3.9500e-05,  ...,  1.1645e-05,
          2.7119e-05,  6.0831e-05],
        [-6.1244e-06, -4.9919e-06,  1.4715e-06,  ..., -5.5581e-06,
         -1.3895e-06, -2.7344e-06],
        [-6.4671e-06, -5.2601e-06,  1.5497e-06,  ..., -5.8562e-06,
         -1.4603e-06, -2.8908e-06],
        [-6.2436e-06, -5.0962e-06,  1.5050e-06,  ..., -5.6624e-06,
         -1.4082e-06, -2.7940e-06],
        [-1.3560e-05, -1.1057e-05,  3.2559e-06,  ..., -1.2308e-05,
         -3.0696e-06, -6.0648e-06]], device='cuda:0')
Loss: 1.1513824462890625


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[    0,   510,    13,  ...,  3454,   513,    13],
        [    0, 48007,   417,  ...,   944,   278,  2903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1014e-05,  4.5257e-06, -1.7630e-05,  ...,  2.2585e-05,
         -2.5130e-06,  5.7826e-05],
        [-9.2238e-06, -7.4953e-06,  2.1942e-06,  ..., -8.2701e-06,
         -2.0899e-06, -4.0978e-06],
        [-9.7454e-06, -7.9125e-06,  2.3134e-06,  ..., -8.7321e-06,
         -2.2016e-06, -4.3362e-06],
        [-9.4175e-06, -7.6592e-06,  2.2464e-06,  ..., -8.4341e-06,
         -2.1234e-06, -4.1947e-06],
        [-2.0474e-05, -1.6630e-05,  4.8652e-06,  ..., -1.8358e-05,
         -4.6268e-06, -9.1046e-06]], device='cuda:0')
Loss: 1.1356019973754883


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0,   14, 1266,  ..., 2288,  417,  199],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6932e-05, -2.8328e-05, -3.2167e-05,  ...,  2.2585e-05,
         -6.6178e-07,  5.2275e-05],
        [-1.2264e-05, -1.0014e-05,  2.9132e-06,  ..., -1.0997e-05,
         -2.7791e-06, -5.4613e-06],
        [-1.2994e-05, -1.0595e-05,  3.0808e-06,  ..., -1.1653e-05,
         -2.9355e-06, -5.7891e-06],
        [-1.2532e-05, -1.0237e-05,  2.9840e-06,  ..., -1.1221e-05,
         -2.8238e-06, -5.5879e-06],
        [-2.7269e-05, -2.2233e-05,  6.4671e-06,  ..., -2.4468e-05,
         -6.1691e-06, -1.2144e-05]], device='cuda:0')
Loss: 1.1586856842041016


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0, 14700,   717,  ..., 10570,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8847e-05,  1.2301e-05, -6.3562e-05,  ...,  1.4208e-05,
          7.8810e-06,  6.5230e-05],
        [-1.5333e-05, -1.2517e-05,  3.6471e-06,  ..., -1.3784e-05,
         -3.5055e-06, -6.8471e-06],
        [-1.6198e-05, -1.3217e-05,  3.8482e-06,  ..., -1.4558e-05,
         -3.6918e-06, -7.2345e-06],
        [-1.5646e-05, -1.2770e-05,  3.7290e-06,  ..., -1.4037e-05,
         -3.5577e-06, -6.9886e-06],
        [-3.4034e-05, -2.7746e-05,  8.0839e-06,  ..., -3.0577e-05,
         -7.7635e-06, -1.5184e-05]], device='cuda:0')
Loss: 1.1250823736190796


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0,   437,   266,  ...,   266, 16084,  1781]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6829e-05, -1.4657e-05, -7.3058e-05,  ...,  2.9526e-05,
          7.2643e-06,  6.5344e-05],
        [-1.8418e-05, -1.5020e-05,  4.3996e-06,  ..., -1.6510e-05,
         -4.1649e-06, -8.2105e-06],
        [-1.9446e-05, -1.5870e-05,  4.6454e-06,  ..., -1.7434e-05,
         -4.3884e-06, -8.6725e-06],
        [-1.8805e-05, -1.5348e-05,  4.5039e-06,  ..., -1.6838e-05,
         -4.2319e-06, -8.3894e-06],
        [-4.0859e-05, -3.3319e-05,  9.7528e-06,  ..., -3.6627e-05,
         -9.2238e-06, -1.8209e-05]], device='cuda:0')
Loss: 1.164446473121643


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,   508,   586,  ...,  6157,  3146,  7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4009e-05, -5.9680e-05, -9.0002e-05,  ...,  1.5708e-05,
          3.5879e-05,  2.7461e-05],
        [-2.1502e-05, -1.7524e-05,  5.1297e-06,  ..., -1.9252e-05,
         -4.8429e-06, -9.5516e-06],
        [-2.2709e-05, -1.8522e-05,  5.4203e-06,  ..., -2.0340e-05,
         -5.1036e-06, -1.0088e-05],
        [-2.1964e-05, -1.7926e-05,  5.2564e-06,  ..., -1.9655e-05,
         -4.9248e-06, -9.7603e-06],
        [-4.7743e-05, -3.8922e-05,  1.1384e-05,  ..., -4.2766e-05,
         -1.0736e-05, -2.1189e-05]], device='cuda:0')
Loss: 1.1473522186279297


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9373e-05, -6.5948e-05, -9.6609e-05,  ...,  2.9242e-05,
          2.0360e-05,  2.6038e-05],
        [-2.4557e-05, -2.0012e-05,  5.8822e-06,  ..., -2.2009e-05,
         -5.5283e-06, -1.0923e-05],
        [-2.5928e-05, -2.1145e-05,  6.2101e-06,  ..., -2.3246e-05,
         -5.8264e-06, -1.1533e-05],
        [-2.5094e-05, -2.0474e-05,  6.0275e-06,  ..., -2.2471e-05,
         -5.6252e-06, -1.1161e-05],
        [-5.4508e-05, -4.4405e-05,  1.3046e-05,  ..., -4.8876e-05,
         -1.2249e-05, -2.4214e-05]], device='cuda:0')
Loss: 1.1613881587982178
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4331e-05, -1.7919e-05, -4.2155e-05,  ...,  1.3902e-05,
         -5.1511e-05,  8.0399e-06],
        [-1.7249e-05, -1.3199e-05,  2.6105e-06,  ..., -1.4400e-05,
         -3.0869e-06, -6.9854e-06],
        [ 5.3697e-05,  4.2302e-05, -1.1087e-05,  ...,  5.3996e-05,
          1.3257e-05,  2.7166e-05],
        [-2.6940e-05, -2.0328e-05,  5.3851e-06,  ..., -2.3432e-05,
         -5.6115e-06, -1.1473e-05],
        [-5.3110e-05, -4.0344e-05,  1.0490e-05,  ..., -4.6267e-05,
         -1.0675e-05, -2.2225e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1888e-08, 2.8005e-08, 3.5160e-08,  ..., 1.1589e-08, 8.7631e-08,
         1.1419e-08],
        [4.7371e-11, 2.6117e-11, 2.9653e-12,  ..., 3.1709e-11, 1.9979e-12,
         7.1932e-12],
        [5.4251e-10, 2.9728e-10, 1.9964e-11,  ..., 4.8825e-10, 3.0447e-11,
         1.2084e-10],
        [3.5181e-11, 1.7744e-11, 1.3783e-12,  ..., 2.4559e-11, 1.0328e-12,
         5.6969e-12],
        [1.9212e-10, 1.0184e-10, 7.7140e-12,  ..., 1.3668e-10, 7.4329e-12,
         3.1296e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.896446322196428e-05, 1.896446322196428e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   271,   706,    13],
        [    0,    14, 15670,  ...,  2027,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0427e-05,  3.6178e-07, -1.9524e-05,  ..., -4.0187e-06,
         -8.3987e-06,  2.1196e-05],
        [-3.0547e-06, -2.5183e-06,  7.6368e-07,  ..., -2.7716e-06,
         -7.0035e-07, -1.4305e-06],
        [-3.2187e-06, -2.6524e-06,  8.0466e-07,  ..., -2.9057e-06,
         -7.3761e-07, -1.4976e-06],
        [-3.0845e-06, -2.5481e-06,  7.7486e-07,  ..., -2.8014e-06,
         -7.0408e-07, -1.4454e-06],
        [-6.7651e-06, -5.6028e-06,  1.6913e-06,  ..., -6.1393e-06,
         -1.5497e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.1291519403457642


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,  1234,   408,  ...,   292, 17323,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5114e-05,  2.8836e-06, -3.7887e-05,  ..., -2.7473e-06,
         -2.0127e-06,  5.0296e-05],
        [-6.1095e-06, -5.0217e-06,  1.5050e-06,  ..., -5.4985e-06,
         -1.3374e-06, -2.8163e-06],
        [-6.4075e-06, -5.2601e-06,  1.5795e-06,  ..., -5.7518e-06,
         -1.4044e-06, -2.9430e-06],
        [-6.2138e-06, -5.1111e-06,  1.5348e-06,  ..., -5.5879e-06,
         -1.3560e-06, -2.8610e-06],
        [-1.3590e-05, -1.1206e-05,  3.3528e-06,  ..., -1.2249e-05,
         -2.9802e-06, -6.2436e-06]], device='cuda:0')
Loss: 1.1365717649459839


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,     9,   298,  ...,    12, 24079,   287],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6923e-05, -6.5703e-07, -7.1274e-05,  ..., -2.1280e-05,
          2.5779e-06,  5.0296e-05],
        [-9.1791e-06, -7.5102e-06,  2.2501e-06,  ..., -8.2105e-06,
         -2.0228e-06, -4.2319e-06],
        [-9.6112e-06, -7.8678e-06,  2.3581e-06,  ..., -8.5831e-06,
         -2.1197e-06, -4.4182e-06],
        [-9.3281e-06, -7.6294e-06,  2.2911e-06,  ..., -8.3297e-06,
         -2.0489e-06, -4.2915e-06],
        [-2.0385e-05, -1.6719e-05,  4.9993e-06,  ..., -1.8239e-05,
         -4.5002e-06, -9.3579e-06]], device='cuda:0')
Loss: 1.1519843339920044


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[    0,    15,    83,  ...,  6030,    14, 14080],
        [    0, 29258,   765,  ...,  4196,    19,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9314e-05, -6.0743e-06, -6.2222e-05,  ..., -6.3192e-05,
         -1.5505e-05,  4.6396e-05],
        [-1.2249e-05, -1.0073e-05,  3.0398e-06,  ..., -1.0982e-05,
         -2.6822e-06, -5.6326e-06],
        [-1.2815e-05, -1.0550e-05,  3.1851e-06,  ..., -1.1474e-05,
         -2.8089e-06, -5.8860e-06],
        [-1.2413e-05, -1.0207e-05,  3.0883e-06,  ..., -1.1116e-05,
         -2.7120e-06, -5.7071e-06],
        [-2.7120e-05, -2.2322e-05,  6.7279e-06,  ..., -2.4319e-05,
         -5.9456e-06, -1.2428e-05]], device='cuda:0')
Loss: 1.1571472883224487


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[    0,  2670, 31283,  ...,    18,  9106,  1389],
        [    0,   287,   266,  ...,   333,   199,  3217]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3200e-05,  1.2895e-05, -4.5929e-05,  ..., -6.5874e-05,
         -6.3680e-06,  6.5725e-05],
        [-1.5318e-05, -1.2577e-05,  3.8110e-06,  ..., -1.3694e-05,
         -3.3826e-06, -7.0632e-06],
        [-1.6063e-05, -1.3188e-05,  4.0010e-06,  ..., -1.4350e-05,
         -3.5502e-06, -7.3984e-06],
        [-1.5512e-05, -1.2726e-05,  3.8669e-06,  ..., -1.3858e-05,
         -3.4198e-06, -7.1526e-06],
        [-3.3945e-05, -2.7835e-05,  8.4341e-06,  ..., -3.0339e-05,
         -7.5027e-06, -1.5587e-05]], device='cuda:0')
Loss: 1.1668074131011963


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  591,  953,  ..., 4118, 5750,  292],
        [   0, 4998, 1921,  ...,  968,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7800e-05,  1.7032e-05, -4.7901e-05,  ..., -5.7368e-05,
         -6.3680e-06,  6.8790e-05],
        [-1.8373e-05, -1.5080e-05,  4.5709e-06,  ..., -1.6436e-05,
         -4.0866e-06, -8.4937e-06],
        [-1.9267e-05, -1.5810e-05,  4.8019e-06,  ..., -1.7226e-05,
         -4.2878e-06, -8.9034e-06],
        [-1.8582e-05, -1.5244e-05,  4.6380e-06,  ..., -1.6615e-05,
         -4.1276e-06, -8.5905e-06],
        [-4.0650e-05, -3.3319e-05,  1.0103e-05,  ..., -3.6359e-05,
         -9.0450e-06, -1.8731e-05]], device='cuda:0')
Loss: 1.129915475845337


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,  2440,  1458,  ...,  7650,   328,  2297],
        [    0, 13576,   431,  ...,    14,   475,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1050e-05,  2.4784e-05, -5.0136e-05,  ..., -6.7093e-05,
          1.8235e-05,  8.2876e-05],
        [-2.1473e-05, -1.7613e-05,  5.3085e-06,  ..., -1.9193e-05,
         -4.7199e-06, -9.8497e-06],
        [-2.2501e-05, -1.8477e-05,  5.5768e-06,  ..., -2.0117e-05,
         -4.9509e-06, -1.0327e-05],
        [-2.1696e-05, -1.7807e-05,  5.3830e-06,  ..., -1.9386e-05,
         -4.7646e-06, -9.9614e-06],
        [-4.7535e-05, -3.8981e-05,  1.1742e-05,  ..., -4.2498e-05,
         -1.0461e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.163478970527649


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[   0,   13, 2497,  ...,  943,  259, 2646],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2344e-05,  2.3820e-05, -8.2197e-05,  ..., -7.0713e-05,
         -1.1575e-05,  9.4589e-05],
        [-2.4542e-05, -2.0146e-05,  6.0834e-06,  ..., -2.1905e-05,
         -5.4017e-06, -1.1258e-05],
        [-2.5749e-05, -2.1160e-05,  6.3963e-06,  ..., -2.2992e-05,
         -5.6699e-06, -1.1809e-05],
        [-2.4796e-05, -2.0370e-05,  6.1691e-06,  ..., -2.2128e-05,
         -5.4501e-06, -1.1377e-05],
        [-5.4359e-05, -4.4614e-05,  1.3456e-05,  ..., -4.8518e-05,
         -1.1973e-05, -2.4870e-05]], device='cuda:0')
Loss: 1.154394268989563
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1663e-05, -1.3745e-05, -4.6159e-05,  ...,  5.4404e-06,
         -4.7518e-05,  1.6695e-05],
        [-1.7978e-05, -1.3894e-05,  2.9578e-06,  ..., -1.5151e-05,
         -3.3184e-06, -7.4127e-06],
        [ 4.5752e-05,  3.5956e-05, -9.3389e-06,  ...,  4.6297e-05,
          1.1364e-05,  2.3269e-05],
        [-2.6726e-05, -2.0333e-05,  5.4635e-06,  ..., -2.3301e-05,
         -5.5953e-06, -1.1464e-05],
        [-5.3235e-05, -4.0771e-05,  1.0787e-05,  ..., -4.6492e-05,
         -1.0805e-05, -2.2489e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1854e-08, 2.7977e-08, 3.5131e-08,  ..., 1.1583e-08, 8.7544e-08,
         1.1417e-08],
        [4.7926e-11, 2.6497e-11, 2.9993e-12,  ..., 3.2157e-11, 2.0251e-12,
         7.3128e-12],
        [5.4263e-10, 2.9743e-10, 1.9985e-11,  ..., 4.8830e-10, 3.0449e-11,
         1.2086e-10],
        [3.5760e-11, 1.8141e-11, 1.4150e-12,  ..., 2.5024e-11, 1.0615e-12,
         5.8206e-12],
        [1.9489e-10, 1.0373e-10, 7.8874e-12,  ..., 1.3889e-10, 7.5689e-12,
         3.1883e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   685,   344,  ...,   680,   401,   616],
        [    0,    12,   401,  ...,  7665,  4101, 10193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5682e-05,  3.5978e-06, -8.6806e-06,  ..., -9.8161e-06,
         -2.4730e-05, -2.3323e-05],
        [-3.0547e-06, -2.5034e-06,  7.6741e-07,  ..., -2.7120e-06,
         -6.2212e-07, -1.4454e-06],
        [-3.1888e-06, -2.6226e-06,  8.0466e-07,  ..., -2.8312e-06,
         -6.4820e-07, -1.5125e-06],
        [-3.0845e-06, -2.5332e-06,  7.7859e-07,  ..., -2.7269e-06,
         -6.2585e-07, -1.4603e-06],
        [-6.7651e-06, -5.5432e-06,  1.6987e-06,  ..., -5.9903e-06,
         -1.3784e-06, -3.2037e-06]], device='cuda:0')
Loss: 1.1420022249221802


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  3773, 23452,  ..., 14393,  1121,   304],
        [    0,   221,   380,  ...,   630,  3765, 19107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2798e-05, -6.2078e-06,  4.7735e-06,  ..., -9.0435e-06,
         -2.4297e-05, -4.7847e-05],
        [-6.1095e-06, -5.0068e-06,  1.5348e-06,  ..., -5.4389e-06,
         -1.2629e-06, -2.8908e-06],
        [-6.4075e-06, -5.2601e-06,  1.6131e-06,  ..., -5.7071e-06,
         -1.3225e-06, -3.0324e-06],
        [-6.1542e-06, -5.0515e-06,  1.5497e-06,  ..., -5.4538e-06,
         -1.2666e-06, -2.9057e-06],
        [-1.3560e-05, -1.1116e-05,  3.4049e-06,  ..., -1.2040e-05,
         -2.8014e-06, -6.4075e-06]], device='cuda:0')
Loss: 1.1572355031967163


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   471,  6210,  ...,  4274,   344, 11451],
        [    0,   221,   709,  ...,  3365,  3504,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6226e-05, -3.0911e-05,  1.1310e-06,  ...,  2.2080e-05,
         -1.9081e-05, -6.5154e-05],
        [-9.1940e-06, -7.4953e-06,  2.2873e-06,  ..., -8.1360e-06,
         -1.9073e-06, -4.3437e-06],
        [-9.6411e-06, -7.8827e-06,  2.4065e-06,  ..., -8.5384e-06,
         -1.9968e-06, -4.5598e-06],
        [-9.2387e-06, -7.5549e-06,  2.3060e-06,  ..., -8.1509e-06,
         -1.9073e-06, -4.3586e-06],
        [-2.0385e-05, -1.6659e-05,  5.0738e-06,  ..., -1.8001e-05,
         -4.2319e-06, -9.6262e-06]], device='cuda:0')
Loss: 1.13746976852417


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,   287,  3121,   292],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5473e-05, -1.9186e-05, -1.4905e-05,  ..., -1.5487e-05,
         -1.3437e-05, -7.6975e-05],
        [-1.2264e-05, -1.0028e-05,  3.0659e-06,  ..., -1.0878e-05,
         -2.5779e-06, -5.8338e-06],
        [-1.2845e-05, -1.0520e-05,  3.2224e-06,  ..., -1.1399e-05,
         -2.6971e-06, -6.1169e-06],
        [-1.2293e-05, -1.0073e-05,  3.0845e-06,  ..., -1.0878e-05,
         -2.5705e-06, -5.8413e-06],
        [-2.7120e-05, -2.2203e-05,  6.7800e-06,  ..., -2.4021e-05,
         -5.6997e-06, -1.2890e-05]], device='cuda:0')
Loss: 1.1520131826400757


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   908,  1086,    12],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3206e-05, -8.8201e-06, -3.1319e-05,  ..., -1.4727e-05,
         -1.7278e-05, -5.6055e-05],
        [-1.5363e-05, -1.2547e-05,  3.8296e-06,  ..., -1.3605e-05,
         -3.2336e-06, -7.3016e-06],
        [-1.6093e-05, -1.3158e-05,  4.0196e-06,  ..., -1.4246e-05,
         -3.3826e-06, -7.6517e-06],
        [-1.5378e-05, -1.2577e-05,  3.8445e-06,  ..., -1.3590e-05,
         -3.2187e-06, -7.3016e-06],
        [-3.3945e-05, -2.7716e-05,  8.4564e-06,  ..., -3.0011e-05,
         -7.1377e-06, -1.6108e-05]], device='cuda:0')
Loss: 1.137607216835022


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    45,  6556,  ...,  1477,   352,  1611],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0145e-04,  1.1714e-05, -2.3135e-05,  ..., -3.2979e-05,
         -2.6563e-06, -5.5315e-05],
        [-1.8463e-05, -1.5095e-05,  4.5933e-06,  ..., -1.6376e-05,
         -3.8855e-06, -8.8140e-06],
        [-1.9312e-05, -1.5810e-05,  4.8131e-06,  ..., -1.7121e-05,
         -4.0568e-06, -9.2238e-06],
        [-1.8477e-05, -1.5125e-05,  4.6082e-06,  ..., -1.6347e-05,
         -3.8669e-06, -8.8140e-06],
        [-4.0770e-05, -3.3319e-05,  1.0133e-05,  ..., -3.6091e-05,
         -8.5682e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1410932540893555


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[   0,  421, 6007,  ...,  408, 2105,  843],
        [   0,  471,   12,  ...,   13, 9909, 2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0618e-04,  2.3876e-06, -3.1810e-05,  ..., -1.8644e-05,
          2.4348e-05, -4.2382e-05],
        [-2.1532e-05, -1.7628e-05,  5.3681e-06,  ..., -1.9088e-05,
         -4.5076e-06, -1.0282e-05],
        [-2.2531e-05, -1.8477e-05,  5.6326e-06,  ..., -1.9968e-05,
         -4.7125e-06, -1.0766e-05],
        [-2.1547e-05, -1.7658e-05,  5.3868e-06,  ..., -1.9059e-05,
         -4.4852e-06, -1.0274e-05],
        [-4.7505e-05, -3.8892e-05,  1.1839e-05,  ..., -4.2051e-05,
         -9.9391e-06, -2.2650e-05]], device='cuda:0')
Loss: 1.1483745574951172


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   271,   266,  ...,  8122,  1387,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0073e-04,  7.8527e-06, -7.1373e-05,  ..., -7.8012e-06,
         -6.9372e-07, -5.2596e-05],
        [-2.4572e-05, -2.0117e-05,  6.1356e-06,  ..., -2.1771e-05,
         -5.1074e-06, -1.1720e-05],
        [-2.5734e-05, -2.1100e-05,  6.4410e-06,  ..., -2.2799e-05,
         -5.3421e-06, -1.2279e-05],
        [-2.4617e-05, -2.0176e-05,  6.1616e-06,  ..., -2.1771e-05,
         -5.0887e-06, -1.1727e-05],
        [-5.4210e-05, -4.4405e-05,  1.3530e-05,  ..., -4.7982e-05,
         -1.1258e-05, -2.5824e-05]], device='cuda:0')
Loss: 1.155415415763855
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0012],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4244e-06, -1.1585e-05, -4.8680e-05,  ...,  4.1162e-06,
         -4.2835e-05,  9.7657e-06],
        [-1.8637e-05, -1.4516e-05,  3.2756e-06,  ..., -1.5813e-05,
         -3.4973e-06, -7.8434e-06],
        [ 3.8604e-05,  3.0250e-05, -7.7609e-06,  ...,  3.9387e-05,
          9.6938e-06,  1.9714e-05],
        [-2.6515e-05, -2.0317e-05,  5.5333e-06,  ..., -2.3148e-05,
         -5.5447e-06, -1.1490e-05],
        [-5.3332e-05, -4.1135e-05,  1.1061e-05,  ..., -4.6641e-05,
         -1.0850e-05, -2.2823e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1823e-08, 2.7949e-08, 3.5101e-08,  ..., 1.1571e-08, 8.7456e-08,
         1.1408e-08],
        [4.8482e-11, 2.6875e-11, 3.0340e-12,  ..., 3.2599e-11, 2.0492e-12,
         7.4428e-12],
        [5.4275e-10, 2.9758e-10, 2.0006e-11,  ..., 4.8833e-10, 3.0447e-11,
         1.2088e-10],
        [3.6331e-11, 1.8530e-11, 1.4515e-12,  ..., 2.5473e-11, 1.0863e-12,
         5.9523e-12],
        [1.9763e-10, 1.0560e-10, 8.0625e-12,  ..., 1.4106e-10, 7.6880e-12,
         3.2518e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.885217280831754e-05, 1.885217280831754e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  631, 3370,  940],
        [   0,  328, 1410,  ..., 7344,   12, 5067]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0329e-05,  9.0032e-06, -2.4776e-05,  ...,  0.0000e+00,
         -1.0549e-06,  3.5731e-05],
        [-3.0845e-06, -2.5034e-06,  7.7859e-07,  ..., -2.7120e-06,
         -6.1840e-07, -1.5125e-06],
        [-3.2037e-06, -2.6226e-06,  8.1211e-07,  ..., -2.8312e-06,
         -6.4448e-07, -1.5721e-06],
        [-3.0398e-06, -2.4885e-06,  7.6741e-07,  ..., -2.6822e-06,
         -6.0722e-07, -1.4901e-06],
        [-6.7353e-06, -5.4836e-06,  1.6987e-06,  ..., -5.9307e-06,
         -1.3486e-06, -3.2932e-06]], device='cuda:0')
Loss: 1.1242687702178955


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,  6408,   278,  1119],
        [    0, 24063,   717,  ...,  2228,  1416,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8981e-05, -2.8376e-07, -6.0777e-05,  ...,  2.2015e-05,
         -2.7247e-06,  6.4837e-05],
        [-6.1393e-06, -4.9919e-06,  1.5795e-06,  ..., -5.3942e-06,
         -1.2219e-06, -3.0324e-06],
        [-6.4224e-06, -5.2303e-06,  1.6503e-06,  ..., -5.6475e-06,
         -1.2778e-06, -3.1590e-06],
        [-6.0946e-06, -4.9770e-06,  1.5683e-06,  ..., -5.3644e-06,
         -1.2070e-06, -3.0026e-06],
        [-1.3471e-05, -1.0967e-05,  3.4571e-06,  ..., -1.1802e-05,
         -2.6748e-06, -6.6161e-06]], device='cuda:0')
Loss: 1.1363412141799927


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,   298,  2230,  ...,  2300,  3698,  4764],
        [    0,  3908,  4274,  ...,   298,  7998, 11109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0109e-04,  1.9359e-05, -6.1100e-05,  ...,  1.8608e-05,
         -6.9258e-06,  1.1296e-04],
        [-9.2238e-06, -7.5102e-06,  2.3656e-06,  ..., -8.0764e-06,
         -1.8924e-06, -4.5821e-06],
        [-9.6560e-06, -7.8529e-06,  2.4736e-06,  ..., -8.4490e-06,
         -1.9781e-06, -4.7833e-06],
        [-9.1344e-06, -7.4506e-06,  2.3432e-06,  ..., -8.0019e-06,
         -1.8664e-06, -4.5300e-06],
        [-2.0236e-05, -1.6451e-05,  5.1782e-06,  ..., -1.7643e-05,
         -4.1425e-06, -9.9987e-06]], device='cuda:0')
Loss: 1.1284170150756836


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   199,  7513,  ...,   271,   259,   957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1523e-05, -1.4284e-05, -9.0422e-05,  ...,  2.5992e-05,
          2.1388e-06,  1.2110e-04],
        [-1.2308e-05, -9.9838e-06,  3.1628e-06,  ..., -1.0759e-05,
         -2.5332e-06, -6.1244e-06],
        [-1.2875e-05, -1.0446e-05,  3.3118e-06,  ..., -1.1265e-05,
         -2.6487e-06, -6.3926e-06],
        [-1.2174e-05, -9.8944e-06,  3.1330e-06,  ..., -1.0654e-05,
         -2.4997e-06, -6.0499e-06],
        [-2.6911e-05, -2.1815e-05,  6.9067e-06,  ..., -2.3484e-05,
         -5.5358e-06, -1.3337e-05]], device='cuda:0')
Loss: 1.1422611474990845


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[    0, 23749, 27341,  ..., 34110,   342,  9672],
        [    0,  1172,   365,  ...,  1119, 15573,  3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5274e-05, -3.1736e-05, -1.0424e-04,  ...,  3.7383e-05,
          2.0471e-06,  1.1667e-04],
        [-1.5363e-05, -1.2487e-05,  3.9488e-06,  ..., -1.3471e-05,
         -3.1255e-06, -7.6219e-06],
        [-1.6063e-05, -1.3053e-05,  4.1313e-06,  ..., -1.4096e-05,
         -3.2671e-06, -7.9572e-06],
        [-1.5214e-05, -1.2383e-05,  3.9116e-06,  ..., -1.3337e-05,
         -3.0845e-06, -7.5325e-06],
        [-3.3617e-05, -2.7299e-05,  8.6278e-06,  ..., -2.9415e-05,
         -6.8322e-06, -1.6615e-05]], device='cuda:0')
Loss: 1.1555427312850952


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  259, 1329,  ...,  266,  706, 1663],
        [   0,   12, 1250,  ...,  381, 1524, 2204]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6086e-05, -3.6319e-05, -9.8262e-05,  ...,  1.8050e-05,
          2.4773e-05,  1.4687e-04],
        [-1.8492e-05, -1.5035e-05,  4.7274e-06,  ..., -1.6198e-05,
         -3.7551e-06, -9.1493e-06],
        [-1.9312e-05, -1.5706e-05,  4.9435e-06,  ..., -1.6928e-05,
         -3.9190e-06, -9.5516e-06],
        [-1.8284e-05, -1.4886e-05,  4.6752e-06,  ..., -1.6004e-05,
         -3.6992e-06, -9.0301e-06],
        [-4.0472e-05, -3.2872e-05,  1.0327e-05,  ..., -3.5375e-05,
         -8.2105e-06, -1.9968e-05]], device='cuda:0')
Loss: 1.1553860902786255


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2222e-05,  2.6390e-06, -1.1578e-04,  ...,  2.5312e-05,
          9.9312e-06,  1.4171e-04],
        [-2.1577e-05, -1.7539e-05,  5.5097e-06,  ..., -1.8880e-05,
         -4.3772e-06, -1.0677e-05],
        [-2.2545e-05, -1.8328e-05,  5.7630e-06,  ..., -1.9744e-05,
         -4.5709e-06, -1.1154e-05],
        [-2.1309e-05, -1.7345e-05,  5.4464e-06,  ..., -1.8641e-05,
         -4.3064e-06, -1.0535e-05],
        [-4.7207e-05, -3.8356e-05,  1.2040e-05,  ..., -4.1246e-05,
         -9.5665e-06, -2.3305e-05]], device='cuda:0')
Loss: 1.1285558938980103


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[   0, 3978, 2697,  ...,  461, 5955, 3792],
        [   0, 1732,  699,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3610e-05,  3.5669e-05, -1.3475e-04,  ...,  5.1406e-05,
          2.8408e-05,  1.0846e-04],
        [-2.4632e-05, -1.9997e-05,  6.2883e-06,  ..., -2.1592e-05,
         -4.9919e-06, -1.2212e-05],
        [-2.5749e-05, -2.0906e-05,  6.5826e-06,  ..., -2.2590e-05,
         -5.2117e-06, -1.2755e-05],
        [-2.4334e-05, -1.9789e-05,  6.2212e-06,  ..., -2.1338e-05,
         -4.9099e-06, -1.2055e-05],
        [-5.3942e-05, -4.3780e-05,  1.3754e-05,  ..., -4.7237e-05,
         -1.0915e-05, -2.6673e-05]], device='cuda:0')
Loss: 1.1447765827178955
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5843e-05, -6.8597e-06, -5.7288e-05,  ...,  8.8452e-06,
         -3.5711e-05,  1.9635e-05],
        [-1.9237e-05, -1.5064e-05,  3.5768e-06,  ..., -1.6391e-05,
         -3.6467e-06, -8.2802e-06],
        [ 3.2168e-05,  2.5135e-05, -6.3266e-06,  ...,  3.3190e-05,
          8.2032e-06,  1.6467e-05],
        [-2.6297e-05, -2.0264e-05,  5.6021e-06,  ..., -2.2967e-05,
         -5.4812e-06, -1.1547e-05],
        [-5.3393e-05, -4.1399e-05,  1.1330e-05,  ..., -4.6701e-05,
         -1.0857e-05, -2.3208e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1786e-08, 2.7923e-08, 3.5084e-08,  ..., 1.1562e-08, 8.7370e-08,
         1.1408e-08],
        [4.9040e-11, 2.7248e-11, 3.0705e-12,  ..., 3.3032e-11, 2.0720e-12,
         7.5845e-12],
        [5.4287e-10, 2.9771e-10, 2.0029e-11,  ..., 4.8835e-10, 3.0444e-11,
         1.2093e-10],
        [3.6886e-11, 1.8903e-11, 1.4888e-12,  ..., 2.5903e-11, 1.1094e-12,
         6.0917e-12],
        [2.0034e-10, 1.0741e-10, 8.2437e-12,  ..., 1.4315e-10, 7.7995e-12,
         3.3197e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[    0,   199,  5990,  ...,   278,   638,  5513],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4269e-06,  9.1007e-06, -1.6309e-05,  ..., -4.7186e-05,
          2.3015e-06,  4.6941e-06],
        [-3.1292e-06, -2.5332e-06,  8.4192e-07,  ..., -2.7269e-06,
         -6.0350e-07, -1.6168e-06],
        [-3.2187e-06, -2.6077e-06,  8.6799e-07,  ..., -2.8163e-06,
         -6.1840e-07, -1.6689e-06],
        [-3.0547e-06, -2.4736e-06,  8.2701e-07,  ..., -2.6673e-06,
         -5.8860e-07, -1.5795e-06],
        [-6.7353e-06, -5.4538e-06,  1.8179e-06,  ..., -5.9009e-06,
         -1.3039e-06, -3.4869e-06]], device='cuda:0')
Loss: 1.1505606174468994


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1482e-06,  1.9801e-05, -1.5759e-05,  ..., -5.4042e-05,
          1.4711e-05,  1.1935e-05],
        [-6.1989e-06, -5.0217e-06,  1.6466e-06,  ..., -5.4091e-06,
         -1.1995e-06, -3.2037e-06],
        [-6.4075e-06, -5.1856e-06,  1.7025e-06,  ..., -5.5879e-06,
         -1.2368e-06, -3.3081e-06],
        [-6.0797e-06, -4.9174e-06,  1.6205e-06,  ..., -5.3048e-06,
         -1.1735e-06, -3.1367e-06],
        [-1.3411e-05, -1.0878e-05,  3.5614e-06,  ..., -1.1712e-05,
         -2.6003e-06, -6.9290e-06]], device='cuda:0')
Loss: 1.142570972442627


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5390e-05,  2.5127e-05,  2.2106e-05,  ..., -7.7926e-05,
          1.0723e-05,  2.0380e-05],
        [-9.2536e-06, -7.4953e-06,  2.4512e-06,  ..., -8.0466e-06,
         -1.7919e-06, -4.7907e-06],
        [ 9.6300e-05,  7.6130e-05, -1.7831e-05,  ...,  5.7460e-05,
          6.2004e-06,  3.5563e-05],
        [-9.1046e-06, -7.3761e-06,  2.4177e-06,  ..., -7.9274e-06,
         -1.7583e-06, -4.7088e-06],
        [-2.0087e-05, -1.6302e-05,  5.3272e-06,  ..., -1.7494e-05,
         -3.8967e-06, -1.0401e-05]], device='cuda:0')
Loss: 1.1231986284255981


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,   342,   970,  ...,   401,  2907,  1657],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0108e-05,  1.7401e-06, -6.2542e-06,  ..., -5.8466e-05,
         -2.0524e-06,  6.5172e-05],
        [-1.2323e-05, -9.9987e-06,  3.2894e-06,  ..., -1.0714e-05,
         -2.4177e-06, -6.3628e-06],
        [ 9.3052e-05,  7.3492e-05, -1.6948e-05,  ...,  5.4644e-05,
          5.5447e-06,  3.3909e-05],
        [ 5.4908e-04,  4.4693e-04, -1.4750e-04,  ...,  4.1227e-04,
          1.4056e-04,  2.5616e-04],
        [-2.6792e-05, -2.1726e-05,  7.1451e-06,  ..., -2.3305e-05,
         -5.2601e-06, -1.3813e-05]], device='cuda:0')
Loss: 1.1314899921417236


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   870,   278,  ...,  1274, 10112,  3269],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6592e-05,  1.7732e-05, -3.5630e-05,  ..., -3.8331e-05,
          3.2485e-06,  6.6660e-05],
        [-1.5378e-05, -1.2502e-05,  4.1239e-06,  ..., -1.3441e-05,
         -3.0287e-06, -7.9349e-06],
        [ 8.9863e-05,  7.0870e-05, -1.6076e-05,  ...,  5.1813e-05,
          4.9114e-06,  3.2278e-05],
        [ 5.4609e-04,  4.4448e-04, -1.4669e-04,  ...,  4.0961e-04,
          1.3997e-04,  2.5463e-04],
        [-3.3468e-05, -2.7180e-05,  8.9630e-06,  ..., -2.9236e-05,
         -6.5863e-06, -1.7226e-05]], device='cuda:0')
Loss: 1.1545886993408203


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,  2278,   292,  ..., 12060,  1319,   292],
        [    0,   300,  3808,  ...,   496,    14,  1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1335e-05, -2.3323e-06, -3.5630e-05,  ..., -8.2023e-05,
          3.2794e-05,  6.3420e-05],
        [-1.8433e-05, -1.5020e-05,  4.9546e-06,  ..., -1.6123e-05,
         -3.6098e-06, -9.5069e-06],
        [ 8.6674e-05,  6.8247e-05, -1.5212e-05,  ...,  4.9026e-05,
          4.3079e-06,  3.0646e-05],
        [ 5.4307e-04,  4.4200e-04, -1.4586e-04,  ...,  4.0698e-04,
          1.3940e-04,  2.5308e-04],
        [-4.0144e-05, -3.2663e-05,  1.0774e-05,  ..., -3.5077e-05,
         -7.8529e-06, -2.0653e-05]], device='cuda:0')
Loss: 1.1311054229736328


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,   259,  6022,  ...,  1871,  1209,  1241]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7710e-06, -1.1685e-05, -5.6726e-05,  ..., -7.5228e-05,
         -9.8747e-07,  6.7983e-05],
        [-2.1487e-05, -1.7509e-05,  5.7667e-06,  ..., -1.8775e-05,
         -4.2059e-06, -1.1101e-05],
        [ 1.7659e-04,  1.1860e-04, -3.6896e-05,  ...,  1.3373e-04,
          1.6836e-05,  8.4725e-05],
        [ 5.4005e-04,  4.3954e-04, -1.4506e-04,  ...,  4.0434e-04,
          1.3881e-04,  2.5150e-04],
        [-4.6819e-05, -3.8087e-05,  1.2539e-05,  ..., -4.0859e-05,
         -9.1493e-06, -2.4125e-05]], device='cuda:0')
Loss: 1.1335415840148926


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[    0,   508,  2322,  ...,   968,   266, 15123],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8373e-06, -5.3407e-07, -7.7739e-05,  ..., -6.2643e-05,
         -1.2596e-05,  7.9838e-05],
        [-2.4587e-05, -1.9982e-05,  6.5789e-06,  ..., -2.1443e-05,
         -4.8019e-06, -1.2666e-05],
        [ 1.7335e-04,  1.1602e-04, -3.6046e-05,  ...,  1.3096e-04,
          1.6218e-05,  8.3101e-05],
        [ 5.3699e-04,  4.3709e-04, -1.4425e-04,  ...,  4.0172e-04,
          1.3822e-04,  2.4997e-04],
        [-5.3585e-05, -4.3511e-05,  1.4313e-05,  ..., -4.6670e-05,
         -1.0453e-05, -2.7537e-05]], device='cuda:0')
Loss: 1.1498327255249023
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4742e-05, -6.2271e-06, -5.9333e-05,  ...,  1.6964e-06,
         -3.3399e-05,  2.5655e-05],
        [-1.9772e-05, -1.5556e-05,  3.8770e-06,  ..., -1.6896e-05,
         -3.7623e-06, -8.7188e-06],
        [ 4.6287e-05,  3.4223e-05, -9.2985e-06,  ...,  4.2966e-05,
          9.0047e-06,  2.3130e-05],
        [ 3.0032e-05,  2.5472e-05, -9.3836e-06,  ...,  1.9501e-05,
          8.8893e-06,  1.4605e-05],
        [-5.3413e-05, -4.1610e-05,  1.1629e-05,  ..., -4.6698e-05,
         -1.0816e-05, -2.3641e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1744e-08, 2.7895e-08, 3.5055e-08,  ..., 1.1555e-08, 8.7283e-08,
         1.1403e-08],
        [4.9596e-11, 2.7620e-11, 3.1107e-12,  ..., 3.3459e-11, 2.0930e-12,
         7.7373e-12],
        [5.7238e-10, 3.1088e-10, 2.1309e-11,  ..., 5.0501e-10, 3.0676e-11,
         1.2771e-10],
        [3.2521e-10, 2.0993e-10, 2.2297e-11,  ..., 1.8725e-10, 2.0214e-11,
         6.8568e-11],
        [2.0301e-10, 1.0920e-10, 8.4403e-12,  ..., 1.4518e-10, 7.9009e-12,
         3.3922e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.8734471844266252e-05, 1.8734471844266252e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,    29,   413,  ...,  1527,  1503,   369],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5419e-06,  1.6590e-05,  9.7735e-06,  ..., -1.7086e-06,
          1.2764e-06, -4.8947e-06],
        [-3.1143e-06, -2.4587e-06,  8.6054e-07,  ..., -2.6524e-06,
         -6.2585e-07, -1.6689e-06],
        [-3.2485e-06, -2.5630e-06,  8.9779e-07,  ..., -2.7567e-06,
         -6.5193e-07, -1.7434e-06],
        [-3.0994e-06, -2.4438e-06,  8.6054e-07,  ..., -2.6375e-06,
         -6.1840e-07, -1.6615e-06],
        [-6.7055e-06, -5.2750e-06,  1.8552e-06,  ..., -5.7220e-06,
         -1.3486e-06, -3.6061e-06]], device='cuda:0')
Loss: 1.1306838989257812


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0, 1049,  292,  ...,  221,  380,  341],
        [   0,   13,  786,  ...,  275, 2623,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6629e-05,  1.3616e-05, -1.1337e-05,  ..., -8.5043e-06,
          3.8340e-07, -1.1953e-05],
        [-6.1691e-06, -4.9770e-06,  1.7360e-06,  ..., -5.3048e-06,
         -1.2368e-06, -3.3081e-06],
        [ 8.4705e-05,  6.1107e-05, -1.4862e-05,  ...,  8.0484e-05,
          1.6377e-05,  4.8687e-05],
        [-6.1542e-06, -4.9621e-06,  1.7360e-06,  ..., -5.2899e-06,
         -1.2256e-06, -3.2932e-06],
        [-1.3292e-05, -1.0699e-05,  3.7402e-06,  ..., -1.1444e-05,
         -2.6599e-06, -7.1377e-06]], device='cuda:0')
Loss: 1.150447130203247


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[   0, 4823,   12,  ..., 1756, 3406,  300],
        [   0,  494,  221,  ...,  437,  266, 2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0263e-04, -6.1120e-06, -4.2798e-05,  ..., -1.7549e-05,
          3.8340e-07, -2.9121e-06],
        [-9.1791e-06, -7.4208e-06,  2.5965e-06,  ..., -7.9721e-06,
         -1.8887e-06, -4.9695e-06],
        [ 3.4461e-04,  2.9194e-04, -9.1815e-05,  ...,  3.0596e-04,
          7.8909e-05,  2.2441e-04],
        [-9.1493e-06, -7.4059e-06,  2.5928e-06,  ..., -7.9423e-06,
         -1.8701e-06, -4.9472e-06],
        [-1.9819e-05, -1.6004e-05,  5.6028e-06,  ..., -1.7226e-05,
         -4.0680e-06, -1.0744e-05]], device='cuda:0')
Loss: 1.1579972505569458


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[   0,  391, 9095,  ...,  417,  199, 2038],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8780e-05, -4.9485e-05, -5.6592e-05,  ..., -6.3738e-06,
         -1.8782e-05,  7.9413e-07],
        [-1.2264e-05, -9.8795e-06,  3.4533e-06,  ..., -1.0669e-05,
         -2.4624e-06, -6.6161e-06],
        [ 3.4137e-04,  2.8936e-04, -9.0917e-05,  ...,  3.0314e-04,
          7.8309e-05,  2.2269e-04],
        [ 7.4410e-05,  6.3120e-05, -3.5402e-05,  ...,  6.4664e-05,
          1.7484e-05,  5.1117e-05],
        [-2.6494e-05, -2.1309e-05,  7.4580e-06,  ..., -2.3037e-05,
         -5.3123e-06, -1.4305e-05]], device='cuda:0')
Loss: 1.135195255279541


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[   0, 2579,  278,  ...,   56,    9,  271],
        [   0,  995,   13,  ..., 2192, 2534,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2038e-04, -8.2176e-05, -7.0418e-05,  ...,  6.7431e-07,
         -1.1703e-05, -1.2096e-05],
        [-1.5303e-05, -1.2323e-05,  4.3251e-06,  ..., -1.3337e-05,
         -3.0734e-06, -8.2403e-06],
        [ 3.3819e-04,  2.8680e-04, -9.0004e-05,  ...,  3.0036e-04,
          7.7669e-05,  2.2099e-04],
        [ 7.1370e-05,  6.0691e-05, -3.4534e-05,  ...,  6.2012e-05,
          1.6877e-05,  4.9500e-05],
        [-3.3081e-05, -2.6584e-05,  9.3356e-06,  ..., -2.8789e-05,
         -6.6310e-06, -1.7822e-05]], device='cuda:0')
Loss: 1.1441148519515991


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0, 18322,   287,  ...,   953,   271,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7288e-05, -9.2324e-05, -1.0286e-04,  ...,  1.5419e-05,
         -6.7844e-05, -3.5982e-05],
        [-1.8373e-05, -1.4767e-05,  5.1968e-06,  ..., -1.5989e-05,
         -3.6582e-06, -9.9167e-06],
        [ 3.3495e-04,  2.8422e-04, -8.9084e-05,  ...,  2.9757e-04,
          7.7050e-05,  2.1923e-04],
        [ 6.8271e-05,  5.8217e-05, -3.3651e-05,  ...,  5.9330e-05,
          1.6288e-05,  4.7801e-05],
        [-3.9697e-05, -3.1859e-05,  1.1213e-05,  ..., -3.4511e-05,
         -7.8976e-06, -2.1428e-05]], device='cuda:0')
Loss: 1.10907781124115


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,  3543,   391,  ...,  3370,  2926,  8090],
        [    0,   221,   264,  ...,  3613,  3222, 14000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7830e-05, -6.7675e-05, -1.2125e-04,  ...,  1.0706e-05,
         -8.5987e-05, -2.3168e-05],
        [-2.1443e-05, -1.7226e-05,  6.0424e-06,  ..., -1.8641e-05,
         -4.2841e-06, -1.1563e-05],
        [ 3.3175e-04,  2.8166e-04, -8.8205e-05,  ...,  2.9481e-04,
          7.6402e-05,  2.1751e-04],
        [ 6.5231e-05,  5.5773e-05, -3.2813e-05,  ...,  5.6707e-05,
          1.5674e-05,  4.6170e-05],
        [-4.6313e-05, -3.7163e-05,  1.3031e-05,  ..., -4.0233e-05,
         -9.2387e-06, -2.4974e-05]], device='cuda:0')
Loss: 1.1317639350891113


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2356,   292,  ...,    12,   287,   300],
        [    0,  1456, 32380,  ...,    12,  1172, 12557]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4511e-04, -8.1109e-05, -1.5706e-04,  ...,  1.2786e-06,
         -7.9371e-05, -6.5377e-05],
        [-2.4498e-05, -1.9670e-05,  6.8992e-06,  ..., -2.1324e-05,
         -4.9211e-06, -1.3225e-05],
        [ 3.2859e-04,  2.7912e-04, -8.7318e-05,  ...,  2.9203e-04,
          7.5746e-05,  2.1579e-04],
        [ 6.2191e-05,  5.3344e-05, -3.1959e-05,  ...,  5.4025e-05,
          1.5044e-05,  4.4523e-05],
        [-5.2899e-05, -4.2439e-05,  1.4871e-05,  ..., -4.6015e-05,
         -1.0602e-05, -2.8551e-05]], device='cuda:0')
Loss: 1.1447316408157349
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0147, -0.0272,  ...,  0.0282, -0.0157, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7779e-05, -1.3715e-05, -6.9105e-05,  ...,  1.6546e-06,
         -3.7997e-05,  1.6552e-05],
        [-2.0244e-05, -1.5967e-05,  4.1793e-06,  ..., -1.7339e-05,
         -3.8781e-06, -9.1694e-06],
        [ 7.4517e-05,  5.8713e-05, -1.7100e-05,  ...,  6.7872e-05,
          1.5679e-05,  4.2397e-05],
        [ 3.3248e-05,  2.8259e-05, -1.1641e-05,  ...,  2.2953e-05,
          9.5048e-06,  1.7596e-05],
        [-5.3361e-05, -4.1693e-05,  1.1953e-05,  ..., -4.6629e-05,
         -1.0795e-05, -2.4132e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1724e-08, 2.7873e-08, 3.5045e-08,  ..., 1.1543e-08, 8.7202e-08,
         1.1396e-08],
        [5.0146e-11, 2.7979e-11, 3.1552e-12,  ..., 3.3880e-11, 2.1152e-12,
         7.9045e-12],
        [6.7978e-10, 3.8848e-10, 2.8912e-11,  ..., 5.8978e-10, 3.6383e-11,
         1.7415e-10],
        [3.2875e-10, 2.1257e-10, 2.3296e-11,  ..., 1.8998e-10, 2.0420e-11,
         7.0482e-11],
        [2.0561e-10, 1.1089e-10, 8.6530e-12,  ..., 1.4715e-10, 8.0055e-12,
         3.4703e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 50
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  Learning Rate ‚ñà‚ñà‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÅ
wandb: Validation PPL ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 319
wandb:          Epoch 0
wandb:  Learning Rate 2e-05
wandb:   Training PPL 10758.97859
wandb: Validation PPL 10.06797
wandb: 
wandb: üöÄ View run upbeat-breeze-312 at: https://wandb.ai/kenotron/brainlessgpt/runs/y1iuvq6b
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_135713-y1iuvq6b/logs
