nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_132855-ah77duc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-forest-310
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/ah77duc8
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,   334,   287,  ...,  1348,  6139,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2704e-04,  5.9281e-04, -1.6846e-04,  ...,  2.3848e-04,
          4.8112e-04, -5.9746e-04],
        [ 3.9302e-07, -1.9645e-09,  2.4773e-07,  ..., -1.3225e-07,
         -5.5507e-07, -5.6997e-07],
        [ 1.0133e-06, -8.1025e-08,  5.6624e-07,  ..., -1.8999e-07,
         -1.4305e-06, -1.4380e-06],
        [ 6.2212e-07, -3.1432e-08,  3.7067e-07,  ..., -7.9628e-08,
         -8.3074e-07, -8.4192e-07],
        [ 3.5577e-07, -4.9127e-08,  1.5646e-07,  ..., -6.8918e-08,
         -6.0722e-07, -4.9174e-07]], device='cuda:0')
Loss: 1.3798766136169434


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,  1412, 11275,  ...,   668, 14849,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2516e-03,  4.9267e-04, -3.0263e-04,  ...,  4.9037e-04,
          2.9440e-04, -6.7642e-04],
        [ 4.9127e-07, -2.7019e-07,  5.5134e-07,  ..., -1.5169e-07,
         -1.1101e-06, -1.2144e-06],
        [ 1.2089e-06, -8.0746e-07,  1.1623e-06,  ..., -1.7218e-07,
         -2.6897e-06, -2.9728e-06],
        [ 7.3295e-07, -4.0210e-07,  8.0839e-07,  ..., -3.8883e-08,
         -1.5423e-06, -1.6615e-06],
        [ 3.8533e-07, -3.3784e-07,  3.1199e-07,  ..., -5.0175e-08,
         -1.1586e-06, -1.0356e-06]], device='cuda:0')
Loss: 1.3779733180999756


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7629e-03,  4.1507e-04, -1.6026e-04,  ...,  3.5682e-04,
         -2.1228e-04,  2.5696e-04],
        [ 8.7498e-07, -1.4166e-07,  5.5827e-07,  ...,  3.0850e-08,
         -2.1383e-06, -1.6689e-06],
        [ 2.3711e-06, -3.9581e-07,  1.1192e-06,  ...,  4.0897e-07,
         -5.6103e-06, -4.3660e-06],
        [ 1.3663e-06, -1.7951e-07,  8.6357e-07,  ...,  3.0384e-07,
         -3.0771e-06, -2.4326e-06],
        [ 7.5600e-07, -2.2375e-07,  1.7788e-07,  ...,  1.7800e-07,
         -2.2762e-06, -1.4417e-06]], device='cuda:0')
Loss: 1.3773101568222046


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  7555,  3908,  ...,   259,  8477,   278],
        [    0,  2895,    26,  ..., 11645,  1535,  1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4040e-03, -8.6908e-04,  5.9628e-05,  ...,  4.3353e-04,
         -9.7745e-04,  1.4020e-04],
        [ 9.7090e-07, -1.3977e-07,  6.9238e-07,  ...,  2.2270e-07,
         -2.7642e-06, -2.1420e-06],
        [ 2.6375e-06, -5.2061e-07,  1.3968e-06,  ...,  1.0795e-06,
         -7.3835e-06, -5.7891e-06],
        [ 1.4938e-06, -2.0291e-07,  1.0387e-06,  ...,  6.5961e-07,
         -3.8855e-06, -3.1143e-06],
        [ 7.6194e-07, -2.5635e-07,  1.9441e-07,  ...,  4.0897e-07,
         -2.9393e-06, -1.8366e-06]], device='cuda:0')
Loss: 1.3781037330627441


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[  0, 368, 266,  ..., 591, 767, 824],
        [  0,  25,  26,  ...,   9, 287, 298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0139e-03, -8.3908e-04,  7.8833e-05,  ...,  8.6843e-04,
         -1.3492e-03,  1.0800e-04],
        [ 1.0435e-06,  5.5152e-09,  9.5688e-07,  ...,  9.0455e-08,
         -3.3192e-06, -2.6021e-06],
        [ 2.7847e-06, -2.7288e-07,  1.9444e-06,  ...,  8.9139e-07,
         -8.6874e-06, -7.0110e-06],
        [ 1.6168e-06, -3.6205e-08,  1.4298e-06,  ...,  5.4738e-07,
         -4.5821e-06, -3.8333e-06],
        [ 8.2387e-07, -1.6787e-07,  3.7323e-07,  ...,  2.9721e-07,
         -3.5465e-06, -2.2389e-06]], device='cuda:0')
Loss: 1.3758723735809326


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0,   29,  413,  ..., 1527, 1503,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5182e-03, -8.5118e-04,  2.6907e-04,  ...,  1.3948e-03,
         -4.1878e-04, -8.5133e-04],
        [ 7.5111e-07,  3.7879e-08,  1.0412e-06,  ..., -1.1630e-07,
         -4.0270e-06, -2.9095e-06],
        [ 2.1439e-06, -1.9837e-07,  2.0501e-06,  ...,  4.3504e-07,
         -1.0513e-05, -7.6964e-06],
        [ 1.3020e-06, -4.7730e-09,  1.5816e-06,  ...,  3.4156e-07,
         -5.4911e-06, -4.1779e-06],
        [ 4.8673e-07, -1.3737e-07,  3.4086e-07,  ...,  7.8348e-08,
         -4.4331e-06, -2.4671e-06]], device='cuda:0')
Loss: 1.3811368942260742


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[    0,   221,   825,  ...,   616,  3661,  8052],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3373e-03, -1.0179e-03,  2.7495e-04,  ...,  1.6629e-03,
         -2.8341e-04, -1.0099e-03],
        [ 7.8115e-07, -4.0357e-07,  1.5031e-06,  ..., -2.0757e-07,
         -4.7535e-06, -3.4496e-06],
        [ 2.1923e-06, -1.3160e-06,  3.0186e-06,  ...,  3.5216e-07,
         -1.2197e-05, -9.0376e-06],
        [ 1.3472e-06, -6.3435e-07,  2.2820e-06,  ...,  3.1723e-07,
         -6.4969e-06, -4.9379e-06],
        [ 4.9477e-07, -5.3598e-07,  6.3889e-07,  ...,  3.7835e-08,
         -5.1260e-06, -2.9216e-06]], device='cuda:0')
Loss: 1.37653386592865


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2159e-03, -1.1212e-03,  5.5159e-04,  ...,  9.4682e-04,
         -5.9043e-04, -6.5467e-04],
        [ 1.0214e-06, -3.6515e-07,  2.0209e-06,  ..., -1.4098e-07,
         -5.5693e-06, -4.0978e-06],
        [ 2.8852e-06, -1.3034e-06,  4.3076e-06,  ...,  6.0920e-07,
         -1.4536e-05, -1.0878e-05],
        [ 1.7439e-06, -5.2864e-07,  3.0531e-06,  ...,  4.6904e-07,
         -7.7114e-06, -5.8506e-06],
        [ 7.2480e-07, -5.1595e-07,  1.0990e-06,  ...,  1.2491e-07,
         -6.0871e-06, -3.5623e-06]], device='cuda:0')
Loss: 1.376060128211975
Graident accumulation at epoch 0, step 7, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2159e-04, -1.1212e-04,  5.5159e-05,  ...,  9.4682e-05,
         -5.9043e-05, -6.5467e-05],
        [ 1.0214e-07, -3.6515e-08,  2.0209e-07,  ..., -1.4098e-08,
         -5.5693e-07, -4.0978e-07],
        [ 2.8852e-07, -1.3034e-07,  4.3076e-07,  ...,  6.0920e-08,
         -1.4536e-06, -1.0878e-06],
        [ 1.7439e-07, -5.2864e-08,  3.0531e-07,  ...,  4.6904e-08,
         -7.7114e-07, -5.8506e-07],
        [ 7.2480e-08, -5.1595e-08,  1.0990e-07,  ...,  1.2491e-08,
         -6.0871e-07, -3.5623e-07]], device='cuda:0')
optimizer state dict: tensor([[2.7206e-08, 1.2571e-09, 3.0425e-10,  ..., 8.9647e-10, 3.4860e-10,
         4.2859e-10],
        [1.0433e-15, 1.3334e-16, 4.0841e-15,  ..., 1.9875e-17, 3.1017e-14,
         1.6792e-14],
        [8.3246e-15, 1.6988e-15, 1.8555e-14,  ..., 3.7113e-16, 2.1130e-13,
         1.1833e-13],
        [3.0412e-15, 2.7946e-16, 9.3215e-15,  ..., 2.2000e-16, 5.9465e-14,
         3.4229e-14],
        [5.2534e-16, 2.6621e-16, 1.2077e-15,  ..., 1.5603e-17, 3.7053e-14,
         1.2690e-14]], device='cuda:0')
optimizer state dict: 1.0
lr: [2.5445292620865143e-06, 2.5445292620865143e-06]
scheduler_last_epoch: 1


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[    0,  9657,   300,  ...,    12,   271,   266],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6306e-04, -5.3534e-04,  7.0934e-04,  ...,  6.1408e-04,
         -1.2313e-03, -6.9040e-04],
        [ 2.1304e-08, -1.9791e-08,  3.4645e-07,  ...,  2.8312e-07,
         -1.0431e-06, -4.1537e-07],
        [ 3.9116e-08, -1.9278e-07,  8.0466e-07,  ...,  7.6741e-07,
         -2.6077e-06, -1.1399e-06],
        [ 3.8883e-08, -5.9139e-08,  5.1409e-07,  ...,  4.7125e-07,
         -1.3486e-06, -5.9605e-07],
        [-1.0768e-08, -6.7521e-08,  2.5332e-07,  ...,  3.1665e-07,
         -1.0878e-06, -3.5390e-07]], device='cuda:0')
Loss: 1.3802980184555054


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6015e-04, -1.8815e-03,  5.2908e-04,  ...,  1.2968e-03,
         -1.8072e-03, -7.4086e-04],
        [ 2.5413e-07, -4.9477e-07,  6.9663e-07,  ...,  2.0768e-07,
         -1.8440e-06, -7.9162e-07],
        [ 5.3085e-07, -1.4221e-06,  1.4305e-06,  ...,  6.2492e-07,
         -4.5300e-06, -2.0787e-06],
        [ 3.7230e-07, -8.1537e-07,  9.9093e-07,  ...,  4.2445e-07,
         -2.4438e-06, -1.1213e-06],
        [ 1.4383e-07, -4.9593e-07,  4.4238e-07,  ...,  2.7032e-07,
         -1.8217e-06, -6.1840e-07]], device='cuda:0')
Loss: 1.3787602186203003


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[    0,    73,    30,  ...,  4112,    12,  9416],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5080e-04, -1.9903e-03,  7.3113e-04,  ...,  1.8867e-03,
         -1.9853e-03, -7.9921e-04],
        [ 3.6124e-07, -5.7067e-07,  1.1027e-06,  ...,  6.4261e-08,
         -2.4512e-06, -1.4734e-06],
        [-2.2072e-05, -1.4559e-05, -3.9971e-05,  ...,  1.1649e-04,
         -3.8392e-05, -1.5753e-05],
        [ 5.4925e-07, -9.0478e-07,  1.6093e-06,  ...,  3.2806e-07,
         -3.2485e-06, -2.1644e-06],
        [ 1.8853e-07, -6.4960e-07,  6.8080e-07,  ...,  1.3155e-07,
         -2.4661e-06, -1.2070e-06]], device='cuda:0')
Loss: 1.3779780864715576


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[   0, 8416,  669,  ...,  298,  894,  496],
        [   0, 4890, 1528,  ...,  847,  328, 1703]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6752e-04, -2.1329e-03,  1.6270e-03,  ...,  2.4249e-03,
         -1.4135e-03, -4.3775e-04],
        [ 5.5961e-07, -5.9546e-07,  1.4082e-06,  ..., -1.8533e-07,
         -3.3081e-06, -2.0284e-06],
        [-2.1540e-05, -1.4756e-05, -3.9230e-05,  ...,  1.1591e-04,
         -4.0777e-05, -1.7273e-05],
        [ 9.0688e-07, -1.0096e-06,  2.1197e-06,  ...,  2.2585e-08,
         -4.5300e-06, -2.9877e-06],
        [ 3.3661e-07, -7.2271e-07,  8.8383e-07,  ..., -1.0128e-07,
         -3.4273e-06, -1.6671e-06]], device='cuda:0')
Loss: 1.378662347793579


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4792e-03, -1.8712e-03,  1.6975e-03,  ...,  3.0601e-03,
         -2.3207e-03,  2.6368e-04],
        [ 1.0197e-06, -7.5845e-07,  2.1309e-06,  ..., -3.8557e-07,
         -4.1947e-06, -2.6282e-06],
        [-2.0407e-05, -1.5300e-05, -3.7509e-05,  ...,  1.1553e-04,
         -4.3027e-05, -1.8815e-05],
        [ 1.6445e-06, -1.3746e-06,  3.2745e-06,  ..., -1.6461e-07,
         -5.7667e-06, -3.8818e-06],
        [ 6.7003e-07, -8.6986e-07,  1.3905e-06,  ..., -2.5216e-07,
         -4.2096e-06, -2.1197e-06]], device='cuda:0')
Loss: 1.3743202686309814


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 1871,  401,  ...,   14, 4797,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5415e-03, -1.7045e-03,  1.3520e-03,  ...,  3.3409e-03,
         -2.1817e-03, -1.2775e-03],
        [ 1.2581e-06, -8.5623e-07,  2.5090e-06,  ..., -6.4820e-07,
         -4.9509e-06, -3.0883e-06],
        [-1.9867e-05, -1.5591e-05, -3.6589e-05,  ...,  1.1500e-04,
         -4.4882e-05, -2.0037e-05],
        [ 2.0654e-06, -1.5181e-06,  3.9861e-06,  ..., -4.4401e-07,
         -6.9737e-06, -4.6454e-06],
        [ 9.3080e-07, -1.0123e-06,  1.7332e-06,  ..., -4.9989e-07,
         -5.1036e-06, -2.5760e-06]], device='cuda:0')
Loss: 1.378920078277588


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[    0, 21413,  1735,  ..., 10789, 12523,    12],
        [    0,  2771,    13,  ...,  1412,    35,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5733e-03, -2.7006e-03,  1.8157e-03,  ...,  3.4644e-03,
         -2.6538e-03, -1.9089e-03],
        [ 1.3128e-06, -1.2884e-06,  3.0715e-06,  ..., -3.3155e-07,
         -5.8301e-06, -3.8035e-06],
        [-2.0023e-05, -1.6731e-05, -3.5262e-05,  ...,  1.1589e-04,
         -4.6953e-05, -2.1870e-05],
        [ 2.1846e-06, -2.0992e-06,  4.7386e-06,  ...,  3.2829e-08,
         -8.0913e-06, -5.5879e-06],
        [ 9.1229e-07, -1.4035e-06,  2.1299e-06,  ..., -2.2235e-07,
         -5.8785e-06, -3.1423e-06]], device='cuda:0')
Loss: 1.3786959648132324


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7581e-03, -3.1079e-03,  2.5922e-03,  ...,  3.1623e-03,
         -2.9356e-03, -1.4865e-03],
        [ 1.6351e-06, -1.0984e-06,  3.4645e-06,  ..., -4.6380e-07,
         -6.9775e-06, -4.4033e-06],
        [-1.9315e-05, -1.6427e-05, -3.4491e-05,  ...,  1.1567e-04,
         -4.9635e-05, -2.3345e-05],
        [ 2.6131e-06, -1.8887e-06,  5.2862e-06,  ..., -4.0745e-08,
         -9.5963e-06, -6.4336e-06],
        [ 1.1898e-06, -1.2629e-06,  2.3497e-06,  ..., -2.7474e-07,
         -6.9961e-06, -3.6415e-06]], device='cuda:0')
Loss: 1.374688982963562
Graident accumulation at epoch 0, step 15, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4524e-04, -4.1169e-04,  3.0886e-04,  ...,  4.0145e-04,
         -3.4670e-04, -2.0757e-04],
        [ 2.5543e-07, -1.4270e-07,  5.2833e-07,  ..., -5.9068e-08,
         -1.1990e-06, -8.0913e-07],
        [-1.6718e-06, -1.7600e-06, -3.0614e-06,  ...,  1.1622e-05,
         -6.2718e-06, -3.3135e-06],
        [ 4.1826e-07, -2.3645e-07,  8.0340e-07,  ...,  3.8139e-08,
         -1.6537e-06, -1.1699e-06],
        [ 1.8421e-07, -1.7272e-07,  3.3388e-07,  ..., -1.6232e-08,
         -1.2475e-06, -6.8475e-07]], device='cuda:0')
optimizer state dict: tensor([[3.0270e-08, 1.0915e-08, 7.0232e-09,  ..., 1.0896e-08, 8.9659e-09,
         2.6377e-09],
        [3.7157e-15, 1.3396e-15, 1.6083e-14,  ..., 2.3496e-16, 7.9671e-14,
         3.6164e-14],
        [3.8138e-13, 2.7155e-13, 1.2082e-12,  ..., 1.3379e-11, 2.6747e-12,
         6.6320e-13],
        [9.8662e-15, 3.8465e-15, 3.7256e-14,  ..., 2.2144e-16, 1.5150e-13,
         7.5586e-14],
        [1.9405e-15, 1.8608e-15, 6.7277e-15,  ..., 9.1070e-17, 8.5961e-14,
         2.5938e-14]], device='cuda:0')
optimizer state dict: 2.0
lr: [5.0890585241730285e-06, 5.0890585241730285e-06]
scheduler_last_epoch: 2


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    27,  3961,  ...,   462,   221,   474],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7847e-04, -1.4383e-04,  1.0530e-03,  ...,  2.7368e-04,
         -4.0078e-04, -4.8506e-04],
        [ 1.3318e-07, -6.5565e-07,  6.2212e-07,  ..., -1.7928e-08,
         -4.9546e-07, -6.1095e-07],
        [ 2.2352e-07, -1.5944e-06,  1.3560e-06,  ...,  6.8394e-09,
         -1.1101e-06, -1.4827e-06],
        [ 2.1700e-07, -9.3132e-07,  8.5309e-07,  ...,  5.3085e-08,
         -7.2643e-07, -8.8662e-07],
        [ 1.0943e-07, -6.8545e-07,  5.0291e-07,  ..., -9.4878e-09,
         -5.5879e-07, -5.5879e-07]], device='cuda:0')
Loss: 1.3627934455871582


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,   13, 2920,  609],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7369e-04, -1.0385e-04,  1.6264e-03,  ...,  3.5411e-05,
         -7.8724e-04,  6.7062e-05],
        [ 1.6554e-07, -1.1045e-06,  1.0543e-06,  ..., -3.5879e-07,
         -1.2070e-06, -9.7416e-07],
        [ 3.4133e-07, -2.7567e-06,  2.3246e-06,  ..., -7.7175e-07,
         -2.9206e-06, -2.4252e-06],
        [ 2.8405e-07, -1.4119e-06,  1.4156e-06,  ..., -3.0268e-07,
         -1.5646e-06, -1.3337e-06],
        [ 1.0831e-07, -1.2256e-06,  7.9349e-07,  ..., -3.2986e-07,
         -1.3448e-06, -7.9069e-07]], device='cuda:0')
Loss: 1.363017201423645


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0, 3408,  300,  ...,   14, 5870,   12],
        [   0, 2314,  266,  ...,  342, 7299, 1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4261e-03, -7.8187e-04,  2.1259e-03,  ..., -4.1283e-04,
         -1.3909e-03,  7.0244e-04],
        [ 4.3539e-08, -1.6447e-06,  1.2945e-06,  ..., -4.7893e-07,
         -2.1607e-06, -1.4920e-06],
        [-4.9404e-05,  1.0386e-04,  1.3722e-05,  ...,  4.3271e-05,
          1.6698e-05,  2.2356e-05],
        [ 1.0617e-07, -2.1458e-06,  1.7490e-06,  ..., -3.7113e-07,
         -2.6748e-06, -2.0713e-06],
        [-8.6337e-08, -1.7881e-06,  8.8103e-07,  ..., -4.0204e-07,
         -2.3358e-06, -1.2266e-06]], device='cuda:0')
Loss: 1.3624402284622192


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[    0,   365,  5911,  ...,   925,   408,   266],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8372e-03, -1.0852e-03,  2.5811e-03,  ...,  3.5042e-04,
         -1.5740e-03,  6.9739e-04],
        [ 5.1921e-08, -2.3451e-06,  1.8235e-06,  ..., -5.0152e-07,
         -2.9802e-06, -2.2445e-06],
        [-4.9352e-05,  1.0189e-04,  1.4944e-05,  ...,  4.3338e-05,
          1.4672e-05,  2.0359e-05],
        [ 1.2154e-07, -2.9393e-06,  2.3227e-06,  ..., -3.2247e-07,
         -3.4831e-06, -2.8647e-06],
        [-1.0683e-07, -2.5369e-06,  1.2834e-06,  ..., -3.8935e-07,
         -3.1777e-06, -1.9008e-06]], device='cuda:0')
Loss: 1.3635389804840088


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 10251,   278,  ...,   278,   319,    13],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1393e-03, -2.2230e-03,  2.7716e-03,  ...,  1.2660e-03,
         -2.0754e-03,  7.3207e-04],
        [-1.3807e-07, -3.0790e-06,  2.2501e-06,  ..., -6.9337e-07,
         -3.5353e-06, -3.0715e-06],
        [-4.9877e-05,  9.9996e-05,  1.5935e-05,  ...,  4.3009e-05,
          1.3308e-05,  1.8228e-05],
        [-1.0571e-07, -3.9376e-06,  2.9635e-06,  ..., -4.8080e-07,
         -4.1500e-06, -3.9674e-06],
        [-3.4711e-07, -3.3081e-06,  1.6112e-06,  ..., -5.4861e-07,
         -3.7700e-06, -2.6906e-06]], device='cuda:0')
Loss: 1.3633633852005005


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3459e-03, -2.1515e-03,  3.0214e-03,  ...,  1.6878e-03,
         -2.4141e-03,  4.2771e-04],
        [-2.3586e-07, -3.8911e-06,  2.9653e-06,  ..., -6.0350e-07,
         -4.7497e-06, -3.9693e-06],
        [-5.0097e-05,  9.7835e-05,  1.7634e-05,  ...,  4.3593e-05,
          1.0462e-05,  1.6053e-05],
        [-2.5006e-07, -4.9733e-06,  3.9134e-06,  ..., -2.3306e-07,
         -5.6624e-06, -5.0925e-06],
        [-5.3617e-07, -4.1202e-06,  2.1923e-06,  ..., -4.1729e-07,
         -5.0366e-06, -3.5213e-06]], device='cuda:0')
Loss: 1.3647500276565552


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[    0,  1635,   266,  ...,   437,  3302,   287],
        [    0,  2278,   292,  ..., 12060,  1319,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5444e-03, -3.1637e-03,  3.1121e-03,  ...,  2.1456e-03,
         -4.0442e-03,  4.0059e-04],
        [-2.2037e-07, -4.4759e-06,  3.4329e-06,  ..., -6.7055e-07,
         -5.5321e-06, -4.6995e-06],
        [-5.0091e-05,  9.6390e-05,  1.8654e-05,  ...,  4.3581e-05,
          8.6295e-06,  1.4354e-05],
        [-1.7462e-07, -5.6848e-06,  4.5020e-06,  ..., -2.0477e-07,
         -6.5342e-06, -5.9530e-06],
        [-5.3699e-07, -4.8168e-06,  2.5909e-06,  ..., -4.1934e-07,
         -5.9642e-06, -4.1844e-06]], device='cuda:0')
Loss: 1.359602451324463


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   352,   644,  ...,  2928,   590,  3040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8436e-03, -2.2367e-03,  3.3572e-03,  ...,  2.5300e-03,
         -4.3870e-03,  5.8270e-04],
        [-4.1502e-07, -4.9118e-06,  4.3754e-06,  ..., -9.3319e-07,
         -6.4261e-06, -5.2955e-06],
        [-5.0616e-05,  9.5384e-05,  2.0480e-05,  ...,  4.3067e-05,
          6.7221e-06,  1.3043e-05],
        [-4.0652e-07, -6.2324e-06,  5.7612e-06,  ..., -4.4878e-07,
         -7.6219e-06, -6.7428e-06],
        [-8.0148e-07, -5.3197e-06,  3.4329e-06,  ..., -6.5217e-07,
         -6.9253e-06, -4.7060e-06]], device='cuda:0')
Loss: 1.361112117767334
Graident accumulation at epoch 0, step 23, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6508e-04, -5.9419e-04,  6.1369e-04,  ...,  6.1431e-04,
         -7.5073e-04, -1.2854e-04],
        [ 1.8839e-07, -6.1961e-07,  9.1304e-07,  ..., -1.4648e-07,
         -1.7217e-06, -1.2578e-06],
        [-6.5663e-06,  7.9544e-06, -7.0732e-07,  ...,  1.4766e-05,
         -4.9724e-06, -1.6779e-06],
        [ 3.3578e-07, -8.3605e-07,  1.2992e-06,  ..., -1.0553e-08,
         -2.2505e-06, -1.7272e-06],
        [ 8.5645e-08, -6.8742e-07,  6.4378e-07,  ..., -7.9826e-08,
         -1.8152e-06, -1.0869e-06]], device='cuda:0')
optimizer state dict: tensor([[3.8325e-08, 1.5906e-08, 1.8287e-08,  ..., 1.7286e-08, 2.8203e-08,
         2.9746e-09],
        [3.8842e-15, 2.5464e-14, 3.5211e-14,  ..., 1.1056e-15, 1.2089e-13,
         6.4170e-14],
        [2.9430e-12, 9.3693e-12, 1.6264e-12,  ..., 1.5221e-11, 2.7173e-12,
         8.3265e-13],
        [1.0022e-14, 4.2686e-14, 7.0410e-14,  ..., 4.2262e-16, 2.0944e-13,
         1.2098e-13],
        [2.5809e-15, 3.0158e-14, 1.8505e-14,  ..., 5.1631e-16, 1.3384e-13,
         4.8058e-14]], device='cuda:0')
optimizer state dict: 3.0
lr: [7.633587786259543e-06, 7.633587786259543e-06]
scheduler_last_epoch: 3


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[   0,   14,  496,  ...,  368,  259,  490],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0572e-04,  9.0876e-04,  1.2372e-03,  ...,  1.0457e-03,
          2.2787e-04,  3.8898e-04],
        [-9.6858e-07, -1.9222e-06,  1.4808e-07,  ..., -7.4878e-07,
         -6.8173e-07, -8.4192e-07],
        [-1.9670e-06, -4.1425e-06,  2.8312e-07,  ..., -1.4305e-06,
         -1.3113e-06, -1.7434e-06],
        [-9.1270e-07, -1.8924e-06,  1.6205e-07,  ..., -6.6683e-07,
         -6.3330e-07, -7.8976e-07],
        [-1.3411e-06, -2.6077e-06,  6.1467e-08,  ..., -9.7603e-07,
         -1.0207e-06, -1.0505e-06]], device='cuda:0')
Loss: 1.3372037410736084


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[    0,  3037,  4511,  ...,  1711,    12,  2655],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0622e-04,  7.1593e-04,  1.2924e-03,  ...,  1.2289e-03,
         -1.2785e-03,  1.3596e-03],
        [-1.6913e-06, -4.0680e-06,  2.9150e-07,  ..., -1.4678e-06,
         -1.0375e-06, -2.1458e-06],
        [-3.3304e-06, -8.2850e-06,  4.4703e-07,  ..., -2.6971e-06,
         -1.8626e-06, -4.2915e-06],
        [-1.4938e-06, -3.7998e-06,  3.1572e-07,  ..., -1.2442e-06,
         -9.3132e-07, -1.9446e-06],
        [-2.0787e-06, -4.7237e-06,  6.7608e-08,  ..., -1.6503e-06,
         -1.4212e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.3387364149093628


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   369, 17432,  ...,   874,  2577,    14],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1426e-04,  3.5022e-04,  1.4170e-03,  ...,  1.5502e-03,
         -1.7005e-03,  1.7104e-03],
        [-2.1532e-06, -5.6848e-06,  7.0315e-07,  ..., -1.9968e-06,
         -1.5013e-06, -2.8461e-06],
        [-4.3735e-06, -1.1832e-05,  1.2405e-06,  ..., -3.7104e-06,
         -2.8126e-06, -5.8040e-06],
        [-1.9036e-06, -5.3942e-06,  7.3481e-07,  ..., -1.6782e-06,
         -1.4082e-06, -2.6040e-06],
        [-2.6263e-06, -6.5342e-06,  3.8612e-07,  ..., -2.1420e-06,
         -2.0023e-06, -2.9132e-06]], device='cuda:0')
Loss: 1.3371920585632324


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8887e-04,  4.5345e-04,  1.4027e-03,  ...,  1.8035e-03,
         -2.3203e-03,  1.6463e-03],
        [-2.9281e-06, -7.5325e-06,  1.2545e-06,  ..., -2.5220e-06,
         -1.7751e-06, -3.5651e-06],
        [-5.8562e-06, -1.5333e-05,  2.2314e-06,  ..., -4.5672e-06,
         -3.3714e-06, -7.1824e-06],
        [-2.5518e-06, -6.9141e-06,  1.2452e-06,  ..., -2.0396e-06,
         -1.6261e-06, -3.1888e-06],
        [-3.5241e-06, -8.5607e-06,  8.7041e-07,  ..., -2.6971e-06,
         -2.4121e-06, -3.5688e-06]], device='cuda:0')
Loss: 1.337646722793579


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   20,   13,  ...,  496,   14, 1032],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6490e-03,  3.9083e-04,  1.6351e-03,  ...,  1.9984e-03,
         -2.4496e-03,  1.5739e-03],
        [-3.5018e-06, -9.4697e-06,  1.7257e-06,  ..., -3.3975e-06,
         -2.7139e-06, -4.0308e-06],
        [-7.0855e-06, -1.9416e-05,  3.0957e-06,  ..., -6.2659e-06,
         -5.2787e-06, -8.1360e-06],
        [-3.1330e-06, -9.0152e-06,  1.7704e-06,  ..., -2.8703e-06,
         -2.5947e-06, -3.6433e-06],
        [-4.2319e-06, -1.0855e-05,  1.3156e-06,  ..., -3.6955e-06,
         -3.5148e-06, -4.0010e-06]], device='cuda:0')
Loss: 1.341617465019226


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[   0,  721, 1119,  ...,  600,  328, 3363],
        [   0, 1478,   14,  ...,  266, 9417, 9105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8649e-03,  1.6582e-04,  1.2002e-03,  ...,  2.4142e-03,
         -2.6865e-03,  1.7154e-03],
        [-4.0978e-06, -1.1355e-05,  2.7763e-06,  ..., -3.8445e-06,
         -3.5074e-06, -4.5486e-06],
        [-8.2552e-06, -2.3171e-05,  5.0776e-06,  ..., -7.0594e-06,
         -6.7912e-06, -9.2387e-06],
        [-3.7178e-06, -1.0967e-05,  2.9253e-06,  ..., -3.2056e-06,
         -3.3695e-06, -4.1761e-06],
        [-4.9174e-06, -1.3016e-05,  2.3959e-06,  ..., -4.0866e-06,
         -4.4610e-06, -4.4815e-06]], device='cuda:0')
Loss: 1.338265061378479


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,   12, 2418,  ...,  446,  381, 2204],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0371e-03, -4.2079e-04,  1.1789e-03,  ...,  2.3975e-03,
         -2.8924e-03,  2.0665e-03],
        [-5.1707e-06, -1.3515e-05,  3.5660e-06,  ..., -4.5784e-06,
         -3.9563e-06, -5.3048e-06],
        [-1.0416e-05, -2.7493e-05,  6.5677e-06,  ..., -8.4080e-06,
         -7.6741e-06, -1.0803e-05],
        [-4.6678e-06, -1.2890e-05,  3.6852e-06,  ..., -3.7681e-06,
         -3.7458e-06, -4.8317e-06],
        [-6.1020e-06, -1.5281e-05,  3.0963e-06,  ..., -4.8168e-06,
         -4.9639e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.3309565782546997


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[   0, 2706,  292,  ...,   13, 8954,   13],
        [   0, 1596, 2700,  ...,  943,  266, 4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7192e-03,  5.2828e-06,  1.6244e-03,  ...,  1.6946e-03,
         -3.6974e-03,  1.2074e-03],
        [-6.2510e-06, -1.5542e-05,  3.9814e-06,  ..., -5.5693e-06,
         -5.1484e-06, -6.1579e-06],
        [-1.2636e-05, -3.1725e-05,  7.2978e-06,  ..., -1.0375e-05,
         -1.0118e-05, -1.2584e-05],
        [-1.3407e-04,  6.6401e-05, -5.0483e-05,  ...,  1.4784e-04,
         -3.2978e-05,  1.0621e-04],
        [-7.4431e-06, -1.7785e-05,  3.4222e-06,  ..., -6.0089e-06,
         -6.5360e-06, -6.1244e-06]], device='cuda:0')
Loss: 1.3389981985092163
Graident accumulation at epoch 0, step 31, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1505e-03, -5.3424e-04,  7.1476e-04,  ...,  7.2233e-04,
         -1.0454e-03,  5.0575e-06],
        [-4.5555e-07, -2.1118e-06,  1.2199e-06,  ..., -6.8876e-07,
         -2.0644e-06, -1.7478e-06],
        [-7.1733e-06,  3.9865e-06,  9.3192e-08,  ...,  1.2252e-05,
         -5.4869e-06, -2.7685e-06],
        [-1.3105e-05,  5.8876e-06, -3.8790e-06,  ...,  1.4775e-05,
         -5.3233e-06,  9.0667e-06],
        [-6.6723e-07, -2.3971e-06,  9.2162e-07,  ..., -6.7273e-07,
         -2.2873e-06, -1.5906e-06]], device='cuda:0')
optimizer state dict: tensor([[5.2119e-08, 1.5890e-08, 2.0907e-08,  ..., 2.0140e-08, 4.1846e-08,
         4.4295e-09],
        [4.2956e-14, 2.6699e-13, 5.1027e-14,  ..., 3.2122e-14, 1.4727e-13,
         1.0203e-13],
        [3.0997e-12, 1.0366e-11, 1.6780e-12,  ..., 1.5313e-11, 2.8169e-12,
         9.9018e-13],
        [1.7984e-11, 4.4517e-12, 2.6189e-12,  ..., 2.1857e-11, 1.2968e-12,
         1.1402e-11],
        [5.7979e-14, 3.4642e-13, 3.0199e-14,  ..., 3.6623e-14, 1.7642e-13,
         8.5518e-14]], device='cuda:0')
optimizer state dict: 4.0
lr: [1.0178117048346057e-05, 1.0178117048346057e-05]
scheduler_last_epoch: 4


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 6088, 1172,  ...,  546,  401,  925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5133e-04,  1.7206e-04,  2.0590e-05,  ...,  4.3410e-05,
         -2.2934e-04,  1.3083e-04],
        [-2.1309e-06, -3.5763e-06,  1.6391e-07,  ..., -1.6466e-06,
         -8.4192e-07, -8.3819e-07],
        [-3.2187e-06, -5.5134e-06,  1.3970e-07,  ..., -2.4885e-06,
         -1.2890e-06, -1.3337e-06],
        [-1.3933e-06, -2.3842e-06,  1.1269e-07,  ..., -1.0356e-06,
         -5.2899e-07, -5.8487e-07],
        [-2.4289e-06, -4.0829e-06,  2.6543e-08,  ..., -1.8701e-06,
         -1.0282e-06, -9.1642e-07]], device='cuda:0')
Loss: 1.3064333200454712


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0533e-04,  3.6500e-04, -2.0863e-04,  ...,  3.9312e-04,
         -7.9417e-04, -2.0443e-04],
        [-4.4703e-06, -7.0930e-06,  7.6741e-07,  ..., -3.3006e-06,
         -1.7509e-06, -1.9111e-06],
        [-6.5863e-06, -1.0639e-05,  9.5554e-07,  ..., -4.7833e-06,
         -2.6077e-06, -2.8834e-06],
        [-3.0845e-06, -4.9919e-06,  5.6345e-07,  ..., -2.2054e-06,
         -1.1884e-06, -1.3523e-06],
        [-5.1856e-06, -8.1956e-06,  5.8161e-07,  ..., -3.7774e-06,
         -2.1681e-06, -2.0638e-06]], device='cuda:0')
Loss: 1.3034213781356812


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0, 2652,  271,  ...,  634, 1921,  266],
        [   0,  266, 7407,  ...,  287,  365, 4371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1597e-03,  2.7042e-04,  3.6227e-04,  ...,  5.0820e-04,
         -5.3893e-04,  3.4255e-05],
        [-6.5863e-06, -1.0908e-05,  1.3337e-06,  ..., -4.6641e-06,
         -2.2948e-06, -3.3118e-06],
        [-9.6411e-06, -1.6272e-05,  1.7453e-06,  ..., -6.7502e-06,
         -3.4049e-06, -4.9397e-06],
        [ 9.8724e-05,  4.1541e-04,  2.7625e-05,  ...,  7.6231e-05,
          1.2159e-04,  2.1024e-04],
        [-7.7039e-06, -1.2755e-05,  1.1441e-06,  ..., -5.3346e-06,
         -2.8834e-06, -3.5763e-06]], device='cuda:0')
Loss: 1.3092361688613892


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,  4263,  4865,  ...,  1878,   278,  4450],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5597e-03,  7.5703e-04,  3.6227e-04,  ...,  1.0099e-03,
         -6.6050e-04, -8.8594e-04],
        [-9.0599e-06, -1.4603e-05,  1.3707e-06,  ..., -6.2063e-06,
         -2.3093e-06, -3.9600e-06],
        [-1.3366e-05, -2.1875e-05,  1.7219e-06,  ..., -9.0152e-06,
         -3.4417e-06, -5.8934e-06],
        [ 9.7003e-05,  4.1283e-04,  2.7677e-05,  ...,  7.5173e-05,
          1.2158e-04,  2.0983e-04],
        [-1.1355e-05, -1.8120e-05,  9.7929e-07,  ..., -7.5549e-06,
         -2.9635e-06, -4.4107e-06]], device='cuda:0')
Loss: 1.3086187839508057


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,   510,    13,  ...,  3454,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5597e-03,  7.9210e-04,  8.9638e-04,  ...,  1.2019e-03,
         -4.1731e-04, -7.7846e-04],
        [-1.1265e-05, -1.8343e-05,  1.4012e-06,  ..., -7.5921e-06,
         -2.3740e-06, -4.8168e-06],
        [-1.6749e-05, -2.7537e-05,  1.6744e-06,  ..., -1.1072e-05,
         -3.5851e-06, -7.1600e-06],
        [ 9.5386e-05,  4.1007e-04,  2.7737e-05,  ...,  7.4204e-05,
          1.2153e-04,  2.0921e-04],
        [-1.4514e-05, -2.3186e-05,  7.9582e-07,  ..., -9.4771e-06,
         -3.1432e-06, -5.3495e-06]], device='cuda:0')
Loss: 1.29960036277771


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7588e-03,  4.9380e-04,  4.6593e-04,  ...,  1.3226e-03,
         -1.2569e-03, -6.4024e-04],
        [-1.3217e-05, -2.1920e-05,  1.7625e-06,  ..., -8.7619e-06,
         -2.6646e-06, -5.3719e-06],
        [-1.9893e-05, -3.3319e-05,  2.1773e-06,  ..., -1.2875e-05,
         -4.0527e-06, -8.0317e-06],
        [ 9.3978e-05,  4.0747e-04,  2.8029e-05,  ...,  7.3399e-05,
          1.2136e-04,  2.0879e-04],
        [-1.7151e-05, -2.7955e-05,  1.1292e-06,  ..., -1.0997e-05,
         -3.6201e-06, -5.9828e-06]], device='cuda:0')
Loss: 1.3088510036468506


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  4323,  8213,  ...,  1153,   278,  4258],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0983e-03,  1.2047e-04,  2.7087e-04,  ...,  1.1348e-03,
         -2.0648e-03, -5.0002e-04],
        [-1.5378e-05, -2.5794e-05,  2.2058e-06,  ..., -1.0118e-05,
         -2.9831e-06, -6.7502e-06],
        [-2.2903e-05, -3.8803e-05,  2.7324e-06,  ..., -1.4730e-05,
         -4.5053e-06, -9.9987e-06],
        [ 9.2518e-05,  4.0484e-04,  2.8357e-05,  ...,  7.2516e-05,
          1.2116e-04,  2.0789e-04],
        [-1.9684e-05, -3.2395e-05,  1.4869e-06,  ..., -1.2584e-05,
         -4.0280e-06, -7.4282e-06]], device='cuda:0')
Loss: 1.3047385215759277


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6894e-03, -2.0183e-04,  1.2624e-03,  ...,  1.2101e-03,
         -2.2732e-03, -4.9851e-04],
        [-1.8016e-05, -2.9549e-05,  2.3921e-06,  ..., -1.1466e-05,
         -3.7691e-06, -7.3463e-06],
        [-2.6584e-05, -4.4107e-05,  2.8320e-06,  ..., -1.6540e-05,
         -5.6154e-06, -1.0852e-05],
        [ 9.0700e-05,  4.0224e-04,  2.8494e-05,  ...,  7.1626e-05,
          1.2062e-04,  2.0749e-04],
        [-2.3156e-05, -3.7372e-05,  1.4765e-06,  ..., -1.4313e-05,
         -5.1083e-06, -8.1100e-06]], device='cuda:0')
Loss: 1.303229570388794
Graident accumulation at epoch 0, step 39, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2044e-03, -5.0100e-04,  7.6953e-04,  ...,  7.7111e-04,
         -1.1682e-03, -4.5300e-05],
        [-2.2115e-06, -4.8556e-06,  1.3371e-06,  ..., -1.7665e-06,
         -2.2348e-06, -2.3076e-06],
        [-9.1143e-06, -8.2293e-07,  3.6708e-07,  ...,  9.3727e-06,
         -5.4998e-06, -3.5768e-06],
        [-2.7242e-06,  4.5523e-05, -6.4177e-07,  ...,  2.0460e-05,
          7.2709e-06,  2.8909e-05],
        [-2.9161e-06, -5.8946e-06,  9.7711e-07,  ..., -2.0367e-06,
         -2.5694e-06, -2.2426e-06]], device='cuda:0')
optimizer state dict: tensor([[5.4921e-08, 1.5915e-08, 2.2480e-08,  ..., 2.1585e-08, 4.6971e-08,
         4.6736e-09],
        [3.6747e-13, 1.1399e-12, 5.6698e-14,  ..., 1.6357e-13, 1.6133e-13,
         1.5589e-13],
        [3.8033e-12, 1.2302e-11, 1.6844e-12,  ..., 1.5571e-11, 2.8456e-12,
         1.1069e-12],
        [2.6193e-11, 1.6624e-10, 3.4282e-12,  ..., 2.6966e-11, 1.5844e-11,
         5.4441e-11],
        [5.9414e-13, 1.7427e-12, 3.2348e-14,  ..., 2.4144e-13, 2.0234e-13,
         1.5120e-13]], device='cuda:0')
optimizer state dict: 5.0
lr: [1.2722646310432571e-05, 1.2722646310432571e-05]
scheduler_last_epoch: 5


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1760,     9,  ...,  5996,    71,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5223e-04, -3.6605e-04, -7.0380e-04,  ..., -2.3266e-04,
         -3.1141e-04, -1.3403e-04],
        [-3.8147e-06, -5.5134e-06, -2.9616e-07,  ..., -2.0862e-06,
         -4.2655e-07, -9.3877e-07],
        [-4.0233e-06, -5.8115e-06, -3.1851e-07,  ..., -2.1607e-06,
         -4.5076e-07, -1.0058e-06],
        [-2.2352e-06, -3.2634e-06, -1.5274e-07,  ..., -1.2070e-06,
         -2.3656e-07, -5.3272e-07],
        [-5.5432e-06, -7.9870e-06, -5.4017e-07,  ..., -2.9951e-06,
         -6.8918e-07, -1.2591e-06]], device='cuda:0')
Loss: 1.2807292938232422


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0,   266,  7264,  ...,  3211,   328,   275],
        [    0,  2496, 10545,  ...,   287, 13978,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6255e-04, -8.2329e-04, -7.7831e-04,  ..., -2.3266e-04,
         -6.7052e-04, -4.4032e-04],
        [-7.2271e-06, -1.0401e-05, -3.5577e-07,  ..., -3.7625e-06,
         -8.6986e-07, -1.8142e-06],
        [-7.9572e-06, -1.1504e-05, -4.1258e-07,  ..., -4.0829e-06,
         -9.4995e-07, -2.0191e-06],
        [-4.3362e-06, -6.3181e-06, -1.6973e-07,  ..., -2.2277e-06,
         -4.9733e-07, -1.0692e-06],
        [-1.0639e-05, -1.5378e-05, -7.4040e-07,  ..., -5.5134e-06,
         -1.4268e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.2779666185379028


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  6847,   437,  ...,    17,    14,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5153e-04, -2.0851e-04, -6.7402e-04,  ...,  1.2310e-04,
         -6.5639e-04, -7.4775e-04],
        [-1.0863e-05, -1.5557e-05, -4.0955e-07,  ..., -6.2957e-06,
         -1.2461e-06, -2.9169e-06],
        [-1.2010e-05, -1.7256e-05, -4.9174e-07,  ..., -6.8396e-06,
         -1.3746e-06, -3.2932e-06],
        [-6.4373e-06, -9.2983e-06, -1.8568e-07,  ..., -3.6582e-06,
         -6.9197e-07, -1.7099e-06],
        [-1.6063e-05, -2.3007e-05, -9.0711e-07,  ..., -9.2536e-06,
         -2.0489e-06, -4.0382e-06]], device='cuda:0')
Loss: 1.2928253412246704


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 37683,    14,  1500],
        [    0,    12,  2085,  ...,   287,   593,  4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8301e-04, -6.2076e-04, -6.0044e-04,  ...,  2.4969e-04,
         -1.3684e-03, -9.1270e-04],
        [-1.4201e-05, -2.0534e-05, -7.0012e-07,  ..., -8.4564e-06,
         -1.3416e-06, -3.9600e-06],
        [-1.6004e-05, -2.3305e-05, -8.8476e-07,  ..., -9.4026e-06,
         -1.4864e-06, -4.5747e-06],
        [-8.4639e-06, -1.2338e-05, -3.4494e-07,  ..., -4.9397e-06,
         -7.4250e-07, -2.3320e-06],
        [-2.0832e-05, -3.0130e-05, -1.4286e-06,  ..., -1.2353e-05,
         -2.2613e-06, -5.4389e-06]], device='cuda:0')
Loss: 1.2732077836990356


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9576e-04, -8.5634e-04, -8.8697e-04,  ...,  2.5096e-04,
         -1.9168e-03, -1.1971e-03],
        [-1.7732e-05, -2.5451e-05, -1.0708e-06,  ..., -1.0706e-05,
         -2.0755e-06, -4.9360e-06],
        [-2.0325e-05, -2.9296e-05, -1.3765e-06,  ..., -1.2100e-05,
         -2.3954e-06, -5.7667e-06],
        [-1.0714e-05, -1.5467e-05, -5.6101e-07,  ..., -6.3255e-06,
         -1.2119e-06, -2.9467e-06],
        [-2.6286e-05, -3.7670e-05, -2.1402e-06,  ..., -1.5795e-05,
         -3.4161e-06, -6.8396e-06]], device='cuda:0')
Loss: 1.280368685722351


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   271,  4787,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2509e-04, -1.1215e-03, -1.4245e-03,  ...,  3.9600e-04,
         -1.9726e-03, -1.4848e-03],
        [-2.1219e-05, -3.0667e-05, -1.2813e-06,  ..., -1.2703e-05,
         -2.4759e-06, -5.7146e-06],
        [-2.4125e-05, -3.5048e-05, -1.6503e-06,  ..., -1.4246e-05,
         -2.8256e-06, -6.6310e-06],
        [-1.2755e-05, -1.8522e-05, -6.7370e-07,  ..., -7.4729e-06,
         -1.4494e-06, -3.4031e-06],
        [-3.1471e-05, -4.5419e-05, -2.5593e-06,  ..., -1.8746e-05,
         -4.0829e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.27906334400177


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[    0,  2923,   391,  ...,    14,  5424,   298],
        [    0,  4988, 36842,  ...,  7630, 18362,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0126e-03, -1.6418e-03, -1.5193e-03,  ...,  3.5925e-04,
         -2.1066e-03, -1.6067e-03],
        [-2.4945e-05, -3.6180e-05, -1.3856e-06,  ..., -1.4938e-05,
         -3.0645e-06, -6.5081e-06],
        [-2.8118e-05, -4.1008e-05, -1.7956e-06,  ..., -1.6600e-05,
         -3.4813e-06, -7.4990e-06],
        [-1.4737e-05, -2.1473e-05, -7.1025e-07,  ..., -8.6352e-06,
         -1.7623e-06, -3.8221e-06],
        [-3.6657e-05, -5.3108e-05, -2.8126e-06,  ..., -2.1815e-05,
         -4.9509e-06, -8.8736e-06]], device='cuda:0')
Loss: 1.2883261442184448


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[    0,   908,    14,  ...,    19,    27,   287],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3732e-04, -1.9069e-03, -1.5829e-03,  ...,  2.7357e-04,
         -2.2439e-03, -2.1920e-03],
        [-2.8729e-05, -4.1038e-05, -1.7823e-06,  ..., -1.7039e-05,
         -3.1192e-06, -7.3314e-06],
        [-3.2350e-05, -4.6432e-05, -2.2799e-06,  ..., -1.8895e-05,
         -3.5823e-06, -8.4601e-06],
        [-1.6987e-05, -2.4363e-05, -9.2073e-07,  ..., -9.8571e-06,
         -1.7965e-06, -4.3064e-06],
        [-4.2230e-05, -6.0290e-05, -3.5055e-06,  ..., -2.4930e-05,
         -5.1036e-06, -9.9987e-06]], device='cuda:0')
Loss: 1.282493233680725
Graident accumulation at epoch 0, step 47, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1777e-03, -6.4159e-04,  5.3429e-04,  ...,  7.2135e-04,
         -1.2758e-03, -2.5997e-04],
        [-4.8633e-06, -8.4738e-06,  1.0252e-06,  ..., -3.2938e-06,
         -2.3233e-06, -2.8100e-06],
        [-1.1438e-05, -5.3838e-06,  1.0238e-07,  ...,  6.5460e-06,
         -5.3080e-06, -4.0651e-06],
        [-4.1505e-06,  3.8534e-05, -6.6966e-07,  ...,  1.7428e-05,
          6.3642e-06,  2.5587e-05],
        [-6.8475e-06, -1.1334e-05,  5.2885e-07,  ..., -4.3260e-06,
         -2.8228e-06, -3.0182e-06]], device='cuda:0')
optimizer state dict: tensor([[5.5745e-08, 1.9536e-08, 2.4963e-08,  ..., 2.1638e-08, 5.1960e-08,
         9.4736e-09],
        [1.1925e-12, 2.8228e-12, 5.9818e-14,  ..., 4.5375e-13, 1.7090e-13,
         2.0948e-13],
        [4.8461e-12, 1.4445e-11, 1.6879e-12,  ..., 1.5913e-11, 2.8556e-12,
         1.1774e-12],
        [2.6455e-11, 1.6667e-10, 3.4256e-12,  ..., 2.7036e-11, 1.5832e-11,
         5.4405e-11],
        [2.3769e-12, 5.3759e-12, 4.4605e-14,  ..., 8.6268e-13, 2.2818e-13,
         2.5103e-13]], device='cuda:0')
optimizer state dict: 6.0
lr: [1.5267175572519086e-05, 1.5267175572519086e-05]
scheduler_last_epoch: 6


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8090e-04,  2.9831e-04, -8.8301e-04,  ...,  3.4794e-05,
         -9.4462e-05,  1.9278e-04],
        [-4.5896e-06, -5.6922e-06, -3.5763e-07,  ..., -2.7418e-06,
          2.1886e-07, -5.4762e-07],
        [-4.4107e-06, -5.5134e-06, -3.7253e-07,  ..., -2.6375e-06,
          2.2259e-07, -5.3644e-07],
        [-2.6822e-06, -3.3528e-06, -2.0489e-07,  ..., -1.5944e-06,
          1.4249e-07, -3.1665e-07],
        [-7.7486e-06, -9.6560e-06, -6.8918e-07,  ..., -4.6492e-06,
          3.6508e-07, -8.5309e-07]], device='cuda:0')
Loss: 1.2664928436279297


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4713e-04,  2.1994e-04, -7.7900e-04,  ..., -3.3527e-05,
         -6.9873e-04, -8.7961e-05],
        [-9.2387e-06, -1.1355e-05, -7.7114e-07,  ..., -5.4091e-06,
         -1.6484e-07, -1.2144e-06],
        [-8.7023e-06, -1.0818e-05, -7.6741e-07,  ..., -5.0962e-06,
         -1.2200e-07, -1.1697e-06],
        [-5.3495e-06, -6.6608e-06, -4.3120e-07,  ..., -3.1367e-06,
         -7.9162e-08, -6.8545e-07],
        [-1.5229e-05, -1.8775e-05, -1.4491e-06,  ..., -9.0003e-06,
         -2.7940e-07, -1.8440e-06]], device='cuda:0')
Loss: 1.2703611850738525


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,    13,  1529,  ...,  8197,  2700,  9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8204e-04,  2.6672e-04, -1.1716e-03,  ...,  2.9148e-05,
         -4.1663e-04, -4.4591e-05],
        [-1.3769e-05, -1.6928e-05, -1.4156e-06,  ..., -8.3596e-06,
         -8.2422e-07, -1.6503e-06],
        [-1.2696e-05, -1.5765e-05, -1.3486e-06,  ..., -7.6890e-06,
         -6.9570e-07, -1.5721e-06],
        [-8.0019e-06, -9.9391e-06, -7.8883e-07,  ..., -4.8429e-06,
         -4.5728e-07, -9.2201e-07],
        [-2.2978e-05, -2.8372e-05, -2.6412e-06,  ..., -1.4037e-05,
         -1.4491e-06, -2.5295e-06]], device='cuda:0')
Loss: 1.2486217021942139


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[   0, 8353, 1842,  ...,   38,  643,  472],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5327e-03,  8.8804e-05, -1.6067e-03,  ..., -1.5046e-04,
         -5.1242e-04, -3.0940e-04],
        [-1.8150e-05, -2.2888e-05, -1.9521e-06,  ..., -1.1176e-05,
         -1.2582e-06, -2.1942e-06],
        [-1.6898e-05, -2.1577e-05, -1.8924e-06,  ..., -1.0416e-05,
         -1.1204e-06, -2.1234e-06],
        [-1.0505e-05, -1.3366e-05, -1.0775e-06,  ..., -6.4597e-06,
         -6.9756e-07, -1.2349e-06],
        [-3.0369e-05, -3.8505e-05, -3.6247e-06,  ..., -1.8835e-05,
         -2.2128e-06, -3.3714e-06]], device='cuda:0')
Loss: 1.2760602235794067


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7394e-03, -3.1901e-04, -1.6376e-03,  ..., -2.7845e-04,
         -8.2999e-04, -2.9370e-04],
        [-2.2650e-05, -2.8759e-05, -2.4028e-06,  ..., -1.4037e-05,
         -1.4082e-06, -2.7008e-06],
        [-2.1100e-05, -2.7031e-05, -2.3395e-06,  ..., -1.3039e-05,
         -1.2694e-06, -2.5798e-06],
        [-1.3083e-05, -1.6734e-05, -1.3253e-06,  ..., -8.0839e-06,
         -7.7579e-07, -1.5106e-06],
        [-3.7879e-05, -4.8220e-05, -4.4890e-06,  ..., -2.3603e-05,
         -2.5276e-06, -4.1015e-06]], device='cuda:0')
Loss: 1.267844557762146


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0,   271,   266,  ...,   275,  2576,  3588]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9750e-03, -3.2450e-04, -1.8395e-03,  ..., -5.0138e-04,
         -7.3410e-04, -6.5305e-05],
        [-2.6971e-05, -3.4332e-05, -2.9765e-06,  ..., -1.6928e-05,
         -1.4603e-06, -3.1386e-06],
        [-2.5272e-05, -3.2455e-05, -2.9095e-06,  ..., -1.5840e-05,
         -1.3222e-06, -3.0082e-06],
        [-1.5646e-05, -2.0057e-05, -1.6512e-06,  ..., -9.7826e-06,
         -8.1048e-07, -1.7583e-06],
        [-4.5389e-05, -5.7936e-05, -5.5842e-06,  ..., -2.8670e-05,
         -2.6701e-06, -4.7833e-06]], device='cuda:0')
Loss: 1.2739779949188232


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[   0, 3968,  446,  ...,   22,  722,  342],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8783e-03, -4.4471e-04, -2.0248e-03,  ..., -4.1764e-04,
         -1.0282e-03, -2.3843e-04],
        [-3.1561e-05, -3.9816e-05, -3.7104e-06,  ..., -1.9684e-05,
         -2.0042e-06, -3.5930e-06],
        [-2.9832e-05, -3.7879e-05, -3.6545e-06,  ..., -1.8552e-05,
         -1.8401e-06, -3.4850e-06],
        [-1.8314e-05, -2.3246e-05, -2.0554e-06,  ..., -1.1370e-05,
         -1.1085e-06, -2.0172e-06],
        [-5.3197e-05, -6.7234e-05, -6.8955e-06,  ..., -3.3349e-05,
         -3.6238e-06, -5.4836e-06]], device='cuda:0')
Loss: 1.2664296627044678


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[    0,  4538,   271,  ...,  1603,   591,   688],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9656e-03, -5.8730e-04, -2.3444e-03,  ..., -5.2603e-04,
         -1.1071e-03, -3.0002e-04],
        [-3.6329e-05, -4.5776e-05, -3.8920e-06,  ..., -2.2605e-05,
         -2.5257e-06, -4.0587e-06],
        [-3.4213e-05, -4.3362e-05, -3.8221e-06,  ..., -2.1249e-05,
         -2.3206e-06, -3.9339e-06],
        [-2.0966e-05, -2.6569e-05, -2.1439e-06,  ..., -1.2986e-05,
         -1.3898e-06, -2.2687e-06],
        [-6.0886e-05, -7.6830e-05, -7.2699e-06,  ..., -3.8117e-05,
         -4.5178e-06, -6.1318e-06]], device='cuda:0')
Loss: 1.2546656131744385
Graident accumulation at epoch 0, step 55, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2565e-03, -6.3617e-04,  2.4642e-04,  ...,  5.9661e-04,
         -1.2589e-03, -2.6397e-04],
        [-8.0099e-06, -1.2204e-05,  5.3344e-07,  ..., -5.2249e-06,
         -2.3435e-06, -2.9349e-06],
        [-1.3715e-05, -9.1817e-06, -2.9007e-07,  ...,  3.7665e-06,
         -5.0093e-06, -4.0520e-06],
        [-5.8320e-06,  3.2024e-05, -8.1709e-07,  ...,  1.4387e-05,
          5.5888e-06,  2.2802e-05],
        [-1.2251e-05, -1.7884e-05, -2.5103e-07,  ..., -7.7051e-06,
         -2.9923e-06, -3.3295e-06]], device='cuda:0')
optimizer state dict: tensor([[5.9553e-08, 1.9861e-08, 3.0434e-08,  ..., 2.1893e-08, 5.3133e-08,
         9.5542e-09],
        [2.5111e-12, 4.9155e-12, 7.4906e-14,  ..., 9.6428e-13, 1.7711e-13,
         2.2575e-13],
        [6.0118e-12, 1.6311e-11, 1.7008e-12,  ..., 1.6348e-11, 2.8581e-12,
         1.1917e-12],
        [2.6868e-11, 1.6721e-10, 3.4267e-12,  ..., 2.7178e-11, 1.5818e-11,
         5.4356e-11],
        [6.0817e-12, 1.1273e-11, 9.7412e-14,  ..., 2.3147e-12, 2.4837e-13,
         2.8837e-13]], device='cuda:0')
optimizer state dict: 7.0
lr: [1.78117048346056e-05, 1.78117048346056e-05]
scheduler_last_epoch: 7


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,   396,   298,  ...,    52,  5065,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6247e-04, -1.3013e-05, -6.4574e-05,  ..., -1.9461e-04,
         -1.0618e-04, -8.6244e-05],
        [-5.0664e-06, -5.3644e-06, -3.0361e-07,  ..., -3.3826e-06,
         -7.1526e-07, -4.0978e-07],
        [-4.1723e-06, -4.4405e-06, -2.5705e-07,  ..., -2.7865e-06,
         -5.8115e-07, -3.2596e-07],
        [-3.0696e-06, -3.2485e-06, -1.7416e-07,  ..., -2.0564e-06,
         -4.2655e-07, -2.2165e-07],
        [-9.1195e-06, -9.6560e-06, -6.2585e-07,  ..., -6.1095e-06,
         -1.2964e-06, -6.3330e-07]], device='cuda:0')
Loss: 1.2280025482177734


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   344, 10706,  ...,  1184,   578,   825],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7415e-05,  3.9292e-05, -2.4032e-04,  ..., -4.5710e-04,
         -2.7192e-04, -3.4051e-04],
        [-1.0282e-05, -1.0669e-05, -6.4075e-07,  ..., -6.8098e-06,
         -1.2927e-06, -5.4110e-07],
        [-8.2850e-06, -8.6427e-06, -5.3644e-07,  ..., -5.4985e-06,
         -1.0431e-06, -4.5262e-07],
        [-6.3330e-06, -6.5714e-06, -3.7719e-07,  ..., -4.2021e-06,
         -7.8417e-07, -3.0501e-07],
        [-1.8299e-05, -1.8954e-05, -1.3076e-06,  ..., -1.2159e-05,
         -2.3544e-06, -8.2981e-07]], device='cuda:0')
Loss: 1.25034499168396


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5914e-05,  3.2185e-04,  3.0830e-04,  ..., -3.8760e-04,
         -7.5037e-04, -2.5414e-04],
        [-1.5557e-05, -1.6183e-05, -1.2256e-06,  ..., -1.0505e-05,
         -1.9260e-06, -1.1185e-06],
        [-1.2457e-05, -1.2994e-05, -9.9465e-07,  ..., -8.4043e-06,
         -1.5460e-06, -9.1456e-07],
        [-9.4920e-06, -9.8944e-06, -7.0687e-07,  ..., -6.4075e-06,
         -1.1604e-06, -6.5519e-07],
        [-2.7716e-05, -2.8789e-05, -2.4177e-06,  ..., -1.8746e-05,
         -3.5018e-06, -1.8207e-06]], device='cuda:0')
Loss: 1.2635445594787598


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 2728, 3139,  ..., 2254,  221,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0560e-04,  4.2754e-04,  3.5385e-04,  ..., -4.2835e-04,
         -9.0131e-04, -5.3406e-04],
        [-2.0862e-05, -2.1905e-05, -1.5777e-06,  ..., -1.4201e-05,
         -2.5965e-06, -1.5879e-06],
        [-1.6719e-05, -1.7583e-05, -1.2834e-06,  ..., -1.1370e-05,
         -2.0862e-06, -1.2927e-06],
        [-1.2740e-05, -1.3381e-05, -9.0525e-07,  ..., -8.6576e-06,
         -1.5739e-06, -9.2527e-07],
        [-3.7372e-05, -3.9101e-05, -3.1292e-06,  ..., -2.5451e-05,
         -4.7684e-06, -2.5881e-06]], device='cuda:0')
Loss: 1.2412476539611816


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,   437,   638,  ...,  4514,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1484e-04,  5.3472e-04,  2.2783e-04,  ..., -5.0446e-04,
         -8.0101e-04, -5.4094e-04],
        [-2.6077e-05, -2.7329e-05, -2.0135e-06,  ..., -1.7643e-05,
         -3.1963e-06, -1.7770e-06],
        [-2.1040e-05, -2.2084e-05, -1.6559e-06,  ..., -1.4201e-05,
         -2.5779e-06, -1.4352e-06],
        [-1.5974e-05, -1.6719e-05, -1.1530e-06,  ..., -1.0774e-05,
         -1.9409e-06, -1.0291e-06],
        [-4.6670e-05, -4.8697e-05, -3.9563e-06,  ..., -3.1531e-05,
         -5.8711e-06, -2.8415e-06]], device='cuda:0')
Loss: 1.2411726713180542


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0,   638,  2708,  ..., 28492,  1814,    12],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9307e-04,  3.4827e-04,  3.7140e-05,  ..., -6.8349e-04,
         -7.7789e-04, -4.4663e-04],
        [-3.0875e-05, -3.2723e-05, -2.3283e-06,  ..., -2.0966e-05,
         -3.8967e-06, -2.0899e-06],
        [-2.5123e-05, -2.6643e-05, -1.9390e-06,  ..., -1.7017e-05,
         -3.1777e-06, -1.7015e-06],
        [-1.9014e-05, -2.0131e-05, -1.3355e-06,  ..., -1.2860e-05,
         -2.3767e-06, -1.2107e-06],
        [-5.5373e-05, -5.8472e-05, -4.6082e-06,  ..., -3.7551e-05,
         -7.2047e-06, -3.3481e-06]], device='cuda:0')
Loss: 1.2514125108718872


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8342e-04,  3.4529e-04,  1.0302e-04,  ..., -6.3950e-04,
         -9.9906e-04, -5.2601e-04],
        [-3.5733e-05, -3.7938e-05, -2.7586e-06,  ..., -2.4304e-05,
         -4.3046e-06, -2.4792e-06],
        [-2.9325e-05, -3.1143e-05, -2.3134e-06,  ..., -1.9878e-05,
         -3.5483e-06, -2.0480e-06],
        [-2.2173e-05, -2.3499e-05, -1.6056e-06,  ..., -1.5005e-05,
         -2.6468e-06, -1.4529e-06],
        [-6.4671e-05, -6.8426e-05, -5.4874e-06,  ..., -4.3929e-05,
         -8.0206e-06, -4.0447e-06]], device='cuda:0')
Loss: 1.2478585243225098


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,  3990,   717,    12],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2993e-04,  4.6901e-04, -1.7861e-04,  ..., -8.1602e-04,
         -8.7348e-04, -5.3317e-04],
        [-4.0621e-05, -4.3482e-05, -3.1143e-06,  ..., -2.7895e-05,
         -4.9230e-06, -2.8629e-06],
        [-3.3349e-05, -3.5733e-05, -2.6207e-06,  ..., -2.2843e-05,
         -4.0587e-06, -2.3609e-06],
        [-2.5257e-05, -2.7016e-05, -1.8254e-06,  ..., -1.7256e-05,
         -3.0398e-06, -1.6838e-06],
        [-7.3314e-05, -7.8261e-05, -6.1952e-06,  ..., -5.0306e-05,
         -9.1232e-06, -4.6371e-06]], device='cuda:0')
Loss: 1.250149130821228
Graident accumulation at epoch 0, step 63, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2238e-03, -5.2565e-04,  2.0392e-04,  ...,  4.5535e-04,
         -1.2203e-03, -2.9089e-04],
        [-1.1271e-05, -1.5332e-05,  1.6866e-07,  ..., -7.4920e-06,
         -2.6015e-06, -2.9277e-06],
        [-1.5679e-05, -1.1837e-05, -5.2314e-07,  ...,  1.1055e-06,
         -4.9142e-06, -3.8829e-06],
        [-7.7746e-06,  2.6120e-05, -9.1792e-07,  ...,  1.1222e-05,
          4.7259e-06,  2.0353e-05],
        [-1.8358e-05, -2.3922e-05, -8.4544e-07,  ..., -1.1965e-05,
         -3.6054e-06, -3.4603e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0358e-08, 2.0061e-08, 3.0436e-08,  ..., 2.2537e-08, 5.3843e-08,
         9.8289e-09],
        [4.1586e-12, 6.8012e-12, 8.4530e-14,  ..., 1.7414e-12, 2.0117e-13,
         2.3372e-13],
        [7.1179e-12, 1.7572e-11, 1.7060e-12,  ..., 1.6854e-11, 2.8718e-12,
         1.1961e-12],
        [2.7479e-11, 1.6777e-10, 3.4267e-12,  ..., 2.7448e-11, 1.5811e-11,
         5.4304e-11],
        [1.1450e-11, 1.7387e-11, 1.3569e-13,  ..., 4.8431e-12, 3.3135e-13,
         3.0959e-13]], device='cuda:0')
optimizer state dict: 8.0
lr: [1.9999985024557586e-05, 1.9999985024557586e-05]
scheduler_last_epoch: 8


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,   271,  3616,  ...,    12,  1348,  5037],
        [    0,   380,  2114,  ...,   456, 28979,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9161e-04,  2.0178e-05,  6.4299e-05,  ..., -5.7948e-05,
          2.9256e-05, -2.6747e-04],
        [-4.8280e-06, -4.5598e-06,  3.1199e-08,  ..., -3.6210e-06,
         -5.8860e-07, -4.3400e-07],
        [-3.7700e-06, -3.5614e-06,  1.5134e-08,  ..., -2.8461e-06,
         -4.6939e-07, -3.3714e-07],
        [-3.7849e-06, -3.5763e-06,  3.7951e-08,  ..., -2.8461e-06,
         -4.6566e-07, -3.3341e-07],
        [-9.1791e-06, -8.6427e-06,  1.3388e-08,  ..., -6.9141e-06,
         -1.1548e-06, -7.7859e-07]], device='cuda:0')
Loss: 1.2463165521621704


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  5583,   598,  ...,   199,   395,  6551],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1531e-04,  3.6838e-05,  1.6889e-04,  ..., -7.0557e-05,
          2.0681e-04, -4.3288e-04],
        [-9.9540e-06, -9.3579e-06,  7.3342e-08,  ..., -7.4953e-06,
         -1.2368e-06, -8.3260e-07],
        [-7.7635e-06, -7.3165e-06,  4.0745e-08,  ..., -5.8711e-06,
         -9.6485e-07, -6.4634e-07],
        [-7.5996e-06, -7.1526e-06,  7.7300e-08,  ..., -5.7220e-06,
         -9.4622e-07, -6.1281e-07],
        [-1.8716e-05, -1.7583e-05,  3.6322e-08,  ..., -1.4126e-05,
         -2.3842e-06, -1.4603e-06]], device='cuda:0')
Loss: 1.2516616582870483


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,  1099,  2851,  ...,   518,   496,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7801e-04,  5.7463e-06,  1.0454e-04,  ..., -1.1495e-04,
          1.3740e-04, -4.6470e-04],
        [-1.4871e-05, -1.3947e-05,  1.0827e-07,  ..., -1.1146e-05,
         -2.0899e-06, -1.3094e-06],
        [-1.1638e-05, -1.0937e-05,  6.4960e-08,  ..., -8.7321e-06,
         -1.6429e-06, -1.0282e-06],
        [-1.1355e-05, -1.0684e-05,  1.2526e-07,  ..., -8.5086e-06,
         -1.5981e-06, -9.8534e-07],
        [-2.7955e-05, -2.6166e-05,  6.1351e-08,  ..., -2.0981e-05,
         -4.0308e-06, -2.3060e-06]], device='cuda:0')
Loss: 1.2301126718521118


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,   685,  3482,  ..., 23113,    12,  6481],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3769e-04,  1.7352e-05, -9.1961e-05,  ..., -2.7069e-06,
         -8.0458e-05, -7.0178e-04],
        [-1.9878e-05, -1.8388e-05,  1.0916e-07,  ..., -1.5020e-05,
         -2.7716e-06, -1.6857e-06],
        [-1.5453e-05, -1.4335e-05,  6.5993e-08,  ..., -1.1683e-05,
         -2.1681e-06, -1.3169e-06],
        [-1.5229e-05, -1.4126e-05,  1.3917e-07,  ..., -1.1489e-05,
         -2.1197e-06, -1.2666e-06],
        [-3.7491e-05, -3.4571e-05,  7.7998e-09,  ..., -2.8312e-05,
         -5.3495e-06, -2.9765e-06]], device='cuda:0')
Loss: 1.2374067306518555


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0926e-04,  5.2824e-05, -7.1393e-05,  ..., -1.6587e-04,
          7.6999e-05, -6.5984e-04],
        [-2.4885e-05, -2.3037e-05,  1.1159e-07,  ..., -1.8731e-05,
         -3.4906e-06, -2.1812e-06],
        [-1.9476e-05, -1.8075e-05,  7.0417e-08,  ..., -1.4678e-05,
         -2.7493e-06, -1.7285e-06],
        [-1.9029e-05, -1.7658e-05,  1.5524e-07,  ..., -1.4320e-05,
         -2.6673e-06, -1.6466e-06],
        [-4.6611e-05, -4.3094e-05, -3.0152e-08,  ..., -3.5137e-05,
         -6.6832e-06, -3.8594e-06]], device='cuda:0')
Loss: 1.2444071769714355


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,  3393,  3380,  ...,   292,  6502,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2797e-04,  1.1867e-04, -2.7483e-05,  ..., -2.6705e-04,
          6.9403e-05, -6.8343e-04],
        [-2.9892e-05, -2.7746e-05,  1.8423e-07,  ..., -2.2411e-05,
         -4.0047e-06, -2.6580e-06],
        [-2.3440e-05, -2.1785e-05,  1.2257e-07,  ..., -1.7583e-05,
         -3.1628e-06, -2.1067e-06],
        [-2.2843e-05, -2.1234e-05,  2.2136e-07,  ..., -1.7107e-05,
         -3.0678e-06, -2.0042e-06],
        [-5.5969e-05, -5.1856e-05,  6.7172e-08,  ..., -4.1991e-05,
         -7.6666e-06, -4.7125e-06]], device='cuda:0')
Loss: 1.2313002347946167


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1398e-04,  2.1743e-04,  1.0937e-05,  ..., -2.9420e-04,
         -5.5336e-06, -9.2088e-04],
        [-3.4899e-05, -3.2485e-05,  2.3685e-07,  ..., -2.6375e-05,
         -4.7125e-06, -3.0007e-06],
        [-2.7239e-05, -2.5377e-05,  1.5377e-07,  ..., -2.0593e-05,
         -3.7029e-06, -2.3693e-06],
        [-2.6539e-05, -2.4721e-05,  2.6979e-07,  ..., -2.0042e-05,
         -3.5930e-06, -2.2575e-06],
        [-6.5386e-05, -6.0737e-05,  1.2910e-07,  ..., -4.9472e-05,
         -9.0227e-06, -5.3197e-06]], device='cuda:0')
Loss: 1.2374823093414307


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[   0,  278,  668,  ..., 2743,  638,  609],
        [   0, 2836, 3084,  ..., 3634, 6464,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2589e-04,  2.5031e-04,  2.0178e-04,  ..., -1.0783e-04,
         -1.2489e-04, -9.5080e-04],
        [-3.9995e-05, -3.7253e-05,  4.3057e-07,  ..., -3.0130e-05,
         -5.2080e-06, -3.3751e-06],
        [-3.1233e-05, -2.9132e-05,  3.0837e-07,  ..., -2.3544e-05,
         -4.0941e-06, -2.6580e-06],
        [-3.0354e-05, -2.8327e-05,  4.3371e-07,  ..., -2.2873e-05,
         -3.9693e-06, -2.5202e-06],
        [-7.4387e-05, -6.9201e-05,  4.3272e-07,  ..., -5.6118e-05,
         -9.9204e-06, -5.9120e-06]], device='cuda:0')
Loss: 1.2194772958755493
Graident accumulation at epoch 0, step 71, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1640e-03, -4.4805e-04,  2.0371e-04,  ...,  3.9903e-04,
         -1.1108e-03, -3.5688e-04],
        [-1.4143e-05, -1.7524e-05,  1.9485e-07,  ..., -9.7558e-06,
         -2.8621e-06, -2.9724e-06],
        [-1.7234e-05, -1.3566e-05, -4.3999e-07,  ..., -1.3595e-06,
         -4.8322e-06, -3.7604e-06],
        [-1.0032e-05,  2.0675e-05, -7.8275e-07,  ...,  7.8128e-06,
          3.8564e-06,  1.8066e-05],
        [-2.3961e-05, -2.8449e-05, -7.1762e-07,  ..., -1.6380e-05,
         -4.2369e-06, -3.7055e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0690e-08, 2.0104e-08, 3.0446e-08,  ..., 2.2526e-08, 5.3805e-08,
         1.0723e-08],
        [5.7540e-12, 8.1822e-12, 8.4631e-14,  ..., 2.6475e-12, 2.2809e-13,
         2.4488e-13],
        [8.0863e-12, 1.8403e-11, 1.7044e-12,  ..., 1.7391e-11, 2.8857e-12,
         1.2020e-12],
        [2.8373e-11, 1.6841e-10, 3.4234e-12,  ..., 2.7944e-11, 1.5811e-11,
         5.4256e-11],
        [1.6972e-11, 2.2158e-11, 1.3575e-13,  ..., 7.9875e-12, 4.2944e-13,
         3.4423e-13]], device='cuda:0')
optimizer state dict: 9.0
lr: [1.999900705266644e-05, 1.999900705266644e-05]
scheduler_last_epoch: 9


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6117e-05, -3.7590e-05, -2.4893e-05,  ...,  1.0190e-04,
         -6.7935e-05,  6.1392e-05],
        [-4.4703e-06, -3.5912e-06,  5.6997e-07,  ..., -3.7104e-06,
         -4.9919e-07, -3.8929e-07],
        [-3.3826e-06, -2.7418e-06,  4.3213e-07,  ..., -2.8312e-06,
         -3.7998e-07, -2.9244e-07],
        [-4.0829e-06, -3.2783e-06,  5.3644e-07,  ..., -3.3975e-06,
         -4.6007e-07, -3.4645e-07],
        [-8.7023e-06, -7.0333e-06,  1.0952e-06,  ..., -7.2718e-06,
         -1.0058e-06, -7.2271e-07]], device='cuda:0')
Loss: 1.2296308279037476


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[    0,  3164,    12,  ...,   984,   344,  3993],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2761e-04,  2.1337e-05, -2.0866e-04,  ...,  9.7685e-05,
         -6.2306e-05,  1.0650e-04],
        [-8.9407e-06, -7.2271e-06,  1.1735e-06,  ..., -7.3463e-06,
         -1.1139e-06, -1.1530e-06],
        [-6.8098e-06, -5.5283e-06,  8.9593e-07,  ..., -5.6028e-06,
         -8.5123e-07, -8.7731e-07],
        [-8.4043e-06, -6.7949e-06,  1.1325e-06,  ..., -6.8992e-06,
         -1.0561e-06, -1.0766e-06],
        [-1.7405e-05, -1.4096e-05,  2.2426e-06,  ..., -1.4335e-05,
         -2.2203e-06, -2.1830e-06]], device='cuda:0')
Loss: 1.2300288677215576


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   822,  5085,  ...,   293,  1608,   391],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4103e-04,  3.9136e-05, -1.8688e-04,  ..., -7.2890e-05,
          3.8528e-07,  1.5357e-04],
        [-1.3441e-05, -1.0759e-05,  1.6112e-06,  ..., -1.0982e-05,
         -1.7248e-06, -1.6410e-06],
        [-1.0282e-05, -8.2552e-06,  1.2256e-06,  ..., -8.4043e-06,
         -1.3262e-06, -1.2517e-06],
        [-1.2636e-05, -1.0118e-05,  1.5479e-06,  ..., -1.0312e-05,
         -1.6298e-06, -1.5255e-06],
        [-2.6166e-05, -2.0951e-05,  3.0510e-06,  ..., -2.1398e-05,
         -3.4422e-06, -3.0883e-06]], device='cuda:0')
Loss: 1.2319172620773315


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8118e-05,  6.5152e-05, -2.0110e-04,  ...,  4.0669e-05,
          1.6684e-04,  3.2847e-04],
        [-1.8060e-05, -1.4544e-05,  2.0992e-06,  ..., -1.4946e-05,
         -2.3916e-06, -2.2296e-06],
        [-1.3635e-05, -1.1027e-05,  1.5777e-06,  ..., -1.1295e-05,
         -1.8105e-06, -1.6838e-06],
        [-1.6719e-05, -1.3486e-05,  1.9874e-06,  ..., -1.3843e-05,
         -2.2221e-06, -2.0433e-06],
        [-3.5048e-05, -2.8253e-05,  3.9637e-06,  ..., -2.9087e-05,
         -4.7460e-06, -4.1984e-06]], device='cuda:0')
Loss: 1.2399674654006958


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0,  7111,   409,  ...,  1908,  1260,   883],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7360e-05,  2.8853e-05, -1.4861e-04,  ..., -5.6931e-06,
          2.0705e-04,  4.2245e-04],
        [-2.2590e-05, -1.8120e-05,  2.4959e-06,  ..., -1.8686e-05,
         -2.9020e-06, -2.6301e-06],
        [-1.7121e-05, -1.3784e-05,  1.8869e-06,  ..., -1.4171e-05,
         -2.2016e-06, -1.9986e-06],
        [-2.0832e-05, -1.6734e-05,  2.3656e-06,  ..., -1.7241e-05,
         -2.6859e-06, -2.4009e-06],
        [ 3.5720e-05,  8.1699e-06, -5.0795e-05,  ...,  6.2858e-05,
          7.6741e-07,  2.9154e-05]], device='cuda:0')
Loss: 1.2471424341201782


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[   0,   12,  358,  ...,  352,  266,  319],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2685e-05,  1.0962e-04, -2.5258e-04,  ...,  1.2243e-04,
          1.8341e-04,  4.6414e-04],
        [-2.7120e-05, -2.1815e-05,  3.0436e-06,  ..., -2.2560e-05,
         -3.4794e-06, -3.1441e-06],
        [-2.0549e-05, -1.6600e-05,  2.3004e-06,  ..., -1.7107e-05,
         -2.6375e-06, -2.3842e-06],
        [-2.4885e-05, -2.0057e-05,  2.8647e-06,  ..., -2.0728e-05,
         -3.2000e-06, -2.8517e-06],
        [ 2.7018e-05,  1.0471e-06, -4.9775e-05,  ...,  5.5378e-05,
         -3.6508e-07,  2.8219e-05]], device='cuda:0')
Loss: 1.2368066310882568


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[    0,  1549,   824,  ...,  3609,   720,   417],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0342e-04,  1.7037e-04, -4.2167e-04,  ...,  1.4237e-04,
          1.1683e-04,  6.3347e-04],
        [-3.1471e-05, -2.5392e-05,  3.5763e-06,  ..., -2.6152e-05,
         -4.0792e-06, -3.7588e-06],
        [-2.3961e-05, -1.9401e-05,  2.7176e-06,  ..., -1.9923e-05,
         -3.1013e-06, -2.8610e-06],
        [-2.8998e-05, -2.3425e-05,  3.3826e-06,  ..., -2.4125e-05,
         -3.7588e-06, -3.4180e-06],
        [ 1.8673e-05, -5.7478e-06, -4.8784e-05,  ...,  4.8494e-05,
         -1.5199e-06,  2.7086e-05]], device='cuda:0')
Loss: 1.232405662536621


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   292, 21215,  ...,   266,   818,  1527],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2881e-04,  1.2920e-04, -4.0070e-04,  ...,  3.0237e-04,
          3.1968e-05,  7.8672e-04],
        [-3.6091e-05, -2.9087e-05,  3.9823e-06,  ..., -2.9862e-05,
         -4.6641e-06, -4.4405e-06],
        [-2.7448e-05, -2.2188e-05,  3.0249e-06,  ..., -2.2709e-05,
         -3.5465e-06, -3.3751e-06],
        [-3.3200e-05, -2.6762e-05,  3.7644e-06,  ..., -2.7463e-05,
         -4.2841e-06, -4.0252e-06],
        [ 9.7322e-06, -1.2871e-05, -4.8016e-05,  ...,  4.1341e-05,
         -2.6748e-06,  2.5797e-05]], device='cuda:0')
Loss: 1.2275738716125488
Graident accumulation at epoch 0, step 79, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0035,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0705e-03, -3.9033e-04,  1.4327e-04,  ...,  3.8937e-04,
         -9.9653e-04, -2.4252e-04],
        [-1.6338e-05, -1.8680e-05,  5.7360e-07,  ..., -1.1766e-05,
         -3.0423e-06, -3.1192e-06],
        [-1.8256e-05, -1.4428e-05, -9.3496e-08,  ..., -3.4944e-06,
         -4.7036e-06, -3.7219e-06],
        [-1.2349e-05,  1.5931e-05, -3.2804e-07,  ...,  4.2853e-06,
          3.0424e-06,  1.5857e-05],
        [-2.0591e-05, -2.6892e-05, -5.4475e-06,  ..., -1.0608e-05,
         -4.0807e-06, -7.5521e-07]], device='cuda:0')
optimizer state dict: tensor([[6.0681e-08, 2.0100e-08, 3.0576e-08,  ..., 2.2595e-08, 5.3752e-08,
         1.1331e-08],
        [7.0508e-12, 9.0201e-12, 1.0041e-13,  ..., 3.5366e-12, 2.4961e-13,
         2.6435e-13],
        [8.8316e-12, 1.8877e-11, 1.7118e-12,  ..., 1.7890e-11, 2.8953e-12,
         1.2122e-12],
        [2.9447e-11, 1.6895e-10, 3.4342e-12,  ..., 2.8670e-11, 1.5814e-11,
         5.4218e-11],
        [1.7050e-11, 2.2302e-11, 2.4412e-12,  ..., 9.6886e-12, 4.3616e-13,
         1.0094e-12]], device='cuda:0')
optimizer state dict: 10.0
lr: [1.9996501145215088e-05, 1.9996501145215088e-05]
scheduler_last_epoch: 10


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0, 5007, 7551,  ...,    9, 2095,  300],
        [   0,  271,  259,  ..., 1345,  352,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0991e-05,  1.3721e-04,  2.9445e-05,  ...,  4.4881e-05,
          1.2975e-05, -1.3047e-05],
        [-4.2915e-06, -3.0845e-06,  9.2387e-07,  ..., -3.7849e-06,
         -5.2899e-07, -5.6997e-07],
        [-3.0994e-06, -2.2203e-06,  6.6310e-07,  ..., -2.7269e-06,
         -3.8370e-07, -4.1164e-07],
        [-4.3213e-06, -3.0845e-06,  9.3505e-07,  ..., -3.7849e-06,
         -5.3272e-07, -5.6997e-07],
        [-8.2850e-06, -5.9605e-06,  1.7509e-06,  ..., -7.2718e-06,
         -1.0356e-06, -1.0878e-06]], device='cuda:0')
Loss: 1.2149485349655151


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[   0, 4337, 2057,  ..., 3020, 1722,  369],
        [   0,   15,   19,  ...,   12,  287, 7897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2552e-04,  1.7122e-04, -1.8440e-04,  ..., -4.8729e-05,
          1.5482e-04,  6.8525e-05],
        [-8.3447e-06, -5.9903e-06,  1.7844e-06,  ..., -7.4059e-06,
         -1.1176e-06, -1.1623e-06],
        [-6.0946e-06, -4.3660e-06,  1.3001e-06,  ..., -5.4091e-06,
         -8.2143e-07, -8.4937e-07],
        [-8.6129e-06, -6.1542e-06,  1.8626e-06,  ..., -7.6294e-06,
         -1.1548e-06, -1.1921e-06],
        [-1.6272e-05, -1.1653e-05,  3.4273e-06,  ..., -1.4395e-05,
         -2.1979e-06, -2.2203e-06]], device='cuda:0')
Loss: 1.2311123609542847


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[   0,   12, 3518,  ..., 1580, 2573,  409],
        [   0,  266, 1784,  ..., 1119, 1276,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3311e-04,  2.7469e-04, -1.7324e-04,  ..., -2.9170e-05,
          3.0360e-05,  9.9300e-05],
        [-1.2517e-05, -8.9109e-06,  2.6524e-06,  ..., -1.1072e-05,
         -1.7174e-06, -1.8813e-06],
        [-9.1344e-06, -6.4969e-06,  1.9334e-06,  ..., -8.0764e-06,
         -1.2610e-06, -1.3746e-06],
        [-1.2934e-05, -9.1791e-06,  2.7716e-06,  ..., -1.1414e-05,
         -1.7770e-06, -1.9334e-06],
        [-2.4557e-05, -1.7434e-05,  5.1260e-06,  ..., -2.1636e-05,
         -3.4124e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.2061128616333008


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1559e-04,  2.8494e-04, -9.7894e-05,  ..., -5.6998e-05,
          5.4956e-06,  2.1901e-04],
        [-1.6719e-05, -1.1876e-05,  3.4720e-06,  ..., -1.4722e-05,
         -2.1812e-06, -2.5406e-06],
        [-1.2264e-05, -8.7172e-06,  2.5406e-06,  ..., -1.0788e-05,
         -1.6075e-06, -1.8626e-06],
        [-1.7464e-05, -1.2383e-05,  3.6657e-06,  ..., -1.5348e-05,
         -2.2799e-06, -2.6375e-06],
        [-3.2842e-05, -2.3276e-05,  6.7130e-06,  ..., -2.8819e-05,
         -4.3474e-06, -4.8950e-06]], device='cuda:0')
Loss: 1.2170326709747314


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6959e-04,  4.2552e-04, -2.3004e-05,  ..., -7.3493e-05,
          9.2051e-05,  2.3181e-04],
        [-2.0981e-05, -1.4931e-05,  4.2915e-06,  ..., -1.8567e-05,
         -2.7586e-06, -3.1181e-06],
        [-1.5348e-05, -1.0923e-05,  3.1255e-06,  ..., -1.3575e-05,
         -2.0210e-06, -2.2799e-06],
        [-2.1845e-05, -1.5527e-05,  4.5113e-06,  ..., -1.9312e-05,
         -2.8685e-06, -3.2261e-06],
        [-4.1306e-05, -2.9325e-05,  8.2999e-06,  ..., -3.6418e-05,
         -5.4948e-06, -6.0126e-06]], device='cuda:0')
Loss: 1.2358052730560303


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4122e-04,  4.1718e-04, -5.9546e-05,  ..., -1.3155e-04,
          2.1623e-04,  4.5277e-05],
        [-2.5213e-05, -1.7807e-05,  5.2042e-06,  ..., -2.2173e-05,
         -3.2242e-06, -3.8333e-06],
        [-1.8507e-05, -1.3068e-05,  3.8035e-06,  ..., -1.6257e-05,
         -2.3674e-06, -2.8089e-06],
        [-2.6375e-05, -1.8597e-05,  5.5023e-06,  ..., -2.3156e-05,
         -3.3639e-06, -3.9861e-06],
        [-4.9651e-05, -3.4988e-05,  1.0073e-05,  ..., -4.3511e-05,
         -6.4187e-06, -7.3910e-06]], device='cuda:0')
Loss: 1.2109326124191284


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9565e-05,  4.1335e-04,  1.3646e-05,  ..., -1.9131e-04,
          4.3373e-04, -3.6984e-05],
        [-2.9415e-05, -2.0817e-05,  6.0610e-06,  ..., -2.5883e-05,
         -3.6098e-06, -4.4741e-06],
        [-2.1577e-05, -1.5259e-05,  4.4294e-06,  ..., -1.8969e-05,
         -2.6468e-06, -3.2745e-06],
        [-3.0786e-05, -2.1741e-05,  6.4075e-06,  ..., -2.7031e-05,
         -3.7663e-06, -4.6454e-06],
        [-5.7876e-05, -4.0859e-05,  1.1735e-05,  ..., -5.0783e-05,
         -7.1786e-06, -8.6129e-06]], device='cuda:0')
Loss: 1.2260634899139404


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0, 16064, 10937,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5037e-05,  3.0704e-04, -6.3987e-05,  ..., -1.2853e-04,
          4.7346e-04,  7.1979e-06],
        [-3.3617e-05, -2.3827e-05,  6.9067e-06,  ..., -2.9519e-05,
         -4.1388e-06, -5.0776e-06],
        [-2.4691e-05, -1.7494e-05,  5.0552e-06,  ..., -2.1651e-05,
         -3.0398e-06, -3.7234e-06],
        [-3.5197e-05, -2.4900e-05,  7.3090e-06,  ..., -3.0831e-05,
         -4.3176e-06, -5.2750e-06],
        [-6.6102e-05, -4.6760e-05,  1.3374e-05,  ..., -5.7906e-05,
         -8.2292e-06, -9.7752e-06]], device='cuda:0')
Loss: 1.2296346426010132
Graident accumulation at epoch 0, step 87, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.6996e-04, -3.2059e-04,  1.2254e-04,  ...,  3.3758e-04,
         -8.4953e-04, -2.1755e-04],
        [-1.8066e-05, -1.9195e-05,  1.2069e-06,  ..., -1.3542e-05,
         -3.1520e-06, -3.3151e-06],
        [-1.8899e-05, -1.4735e-05,  4.2138e-07,  ..., -5.3101e-06,
         -4.5373e-06, -3.7220e-06],
        [-1.4634e-05,  1.1848e-05,  4.3567e-07,  ...,  7.7369e-07,
          2.3064e-06,  1.3743e-05],
        [-2.5142e-05, -2.8878e-05, -3.5654e-06,  ..., -1.5338e-05,
         -4.4956e-06, -1.6572e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0625e-08, 2.0175e-08, 3.0550e-08,  ..., 2.2589e-08, 5.3922e-08,
         1.1320e-08],
        [8.1739e-12, 9.5788e-12, 1.4801e-13,  ..., 4.4045e-12, 2.6649e-13,
         2.8987e-13],
        [9.4324e-12, 1.9164e-11, 1.7356e-12,  ..., 1.8340e-11, 2.9017e-12,
         1.2248e-12],
        [3.0656e-11, 1.6940e-10, 3.4841e-12,  ..., 2.9592e-11, 1.5817e-11,
         5.4192e-11],
        [2.1402e-11, 2.4466e-11, 2.6176e-12,  ..., 1.3032e-11, 5.0344e-13,
         1.1039e-12]], device='cuda:0')
optimizer state dict: 11.0
lr: [1.999246768512805e-05, 1.999246768512805e-05]
scheduler_last_epoch: 11


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[   0, 2555,  984,  ..., 5900, 1576,  271],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0504e-05,  7.6433e-05,  1.2652e-04,  ..., -1.5957e-05,
          6.1041e-05,  5.5992e-05],
        [-4.2617e-06, -2.8461e-06,  1.1027e-06,  ..., -3.7998e-06,
         -4.0419e-07, -8.5309e-07],
        [-2.9355e-06, -1.9670e-06,  7.5996e-07,  ..., -2.6226e-06,
         -2.7940e-07, -5.8860e-07],
        [-4.7684e-06, -3.1888e-06,  1.2442e-06,  ..., -4.2617e-06,
         -4.5449e-07, -9.4995e-07],
        [-7.9870e-06, -5.3644e-06,  2.0564e-06,  ..., -7.1526e-06,
         -7.6741e-07, -1.5870e-06]], device='cuda:0')
Loss: 1.2167268991470337


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0, 18905,  2311,  ..., 10213,   908,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6077e-05,  1.1445e-04,  1.7600e-04,  ..., -6.6117e-05,
          3.0379e-05,  8.9978e-05],
        [-8.4043e-06, -5.6475e-06,  2.1532e-06,  ..., -7.6443e-06,
         -7.2829e-07, -1.6503e-06],
        [-5.9009e-06, -3.9637e-06,  1.5125e-06,  ..., -5.3793e-06,
         -5.1130e-07, -1.1586e-06],
        [-9.4175e-06, -6.3181e-06,  2.4363e-06,  ..., -8.5831e-06,
         -8.1770e-07, -1.8403e-06],
        [-1.6153e-05, -1.0848e-05,  4.0978e-06,  ..., -1.4693e-05,
         -1.4156e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.226354718208313


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7069e-05,  1.5045e-04,  2.0223e-04,  ..., -1.2813e-04,
          1.1212e-05,  5.0438e-05],
        [-1.2606e-05, -8.4192e-06,  3.2485e-06,  ..., -1.1548e-05,
         -1.0412e-06, -2.5742e-06],
        [-8.7768e-06, -5.8711e-06,  2.2613e-06,  ..., -8.0615e-06,
         -7.2364e-07, -1.7919e-06],
        [-1.4096e-05, -9.4175e-06,  3.6731e-06,  ..., -1.2964e-05,
         -1.1660e-06, -2.8685e-06],
        [-2.4199e-05, -1.6153e-05,  6.1691e-06,  ..., -2.2173e-05,
         -2.0117e-06, -4.8876e-06]], device='cuda:0')
Loss: 1.2160032987594604


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,  1083,   287,  ...,    12,   287,  2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3167e-05,  1.6782e-04,  2.0794e-04,  ..., -2.3858e-04,
          6.0568e-05,  3.0134e-04],
        [-1.6868e-05, -1.1355e-05,  4.3288e-06,  ..., -1.5482e-05,
         -1.4696e-06, -3.4571e-06],
        [-1.1727e-05, -7.8976e-06,  3.0026e-06,  ..., -1.0774e-05,
         -1.0198e-06, -2.4028e-06],
        [-1.8895e-05, -1.2711e-05,  4.8876e-06,  ..., -1.7375e-05,
         -1.6466e-06, -3.8594e-06],
        [-3.2365e-05, -2.1726e-05,  8.1956e-06,  ..., -2.9653e-05,
         -2.8312e-06, -6.5565e-06]], device='cuda:0')
Loss: 1.2103368043899536


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[    0,  1265,  1545,  ...,   292, 36667, 36197],
        [    0,  6538,  1805,  ...,   298,   271,   721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2436e-05,  2.9354e-04,  1.9181e-04,  ..., -2.8503e-04,
          6.9344e-05,  3.0262e-04],
        [-2.1100e-05, -1.4171e-05,  5.2974e-06,  ..., -1.9446e-05,
         -1.8626e-06, -4.3474e-06],
        [-1.4678e-05, -9.8646e-06,  3.6731e-06,  ..., -1.3530e-05,
         -1.2917e-06, -3.0212e-06],
        [-2.3514e-05, -1.5780e-05,  5.9456e-06,  ..., -2.1696e-05,
         -2.0638e-06, -4.8205e-06],
        [-4.0472e-05, -2.7090e-05,  1.0021e-05,  ..., -3.7223e-05,
         -3.5875e-06, -8.2329e-06]], device='cuda:0')
Loss: 1.2379521131515503


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0,   634,  1621,  ...,   688,   586,  8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2220e-04,  3.2216e-04,  1.9966e-04,  ..., -2.5689e-04,
          6.9344e-05,  3.6063e-04],
        [-2.5243e-05, -1.6943e-05,  6.3628e-06,  ..., -2.3246e-05,
         -2.2873e-06, -5.2229e-06],
        [-1.7583e-05, -1.1802e-05,  4.4219e-06,  ..., -1.6198e-05,
         -1.5898e-06, -3.6322e-06],
        [-2.8104e-05, -1.8850e-05,  7.1377e-06,  ..., -2.5898e-05,
         -2.5313e-06, -5.7817e-06],
        [-4.8280e-05, -3.2276e-05,  1.2018e-05,  ..., -4.4376e-05,
         -4.3884e-06, -9.8497e-06]], device='cuda:0')
Loss: 1.2336032390594482


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,   259,  1380,  ...,   287, 10221,   280],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4412e-04,  3.0419e-04,  1.8963e-04,  ..., -3.6219e-04,
          6.2697e-05,  2.9948e-04],
        [-2.9355e-05, -1.9789e-05,  7.3314e-06,  ..., -2.7090e-05,
         -2.6431e-06, -6.0312e-06],
        [-2.0489e-05, -1.3813e-05,  5.1036e-06,  ..., -1.8910e-05,
         -1.8412e-06, -4.1984e-06],
        [-3.2693e-05, -2.2024e-05,  8.2329e-06,  ..., -3.0190e-05,
         -2.9318e-06, -6.6757e-06],
        [-5.6148e-05, -3.7700e-05,  1.3858e-05,  ..., -5.1707e-05,
         -5.0813e-06, -1.1370e-05]], device='cuda:0')
Loss: 1.2182998657226562


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[   0,  759, 1184,  ...,  472,  346,   14],
        [   0,  560,  199,  ..., 6408,  278, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7216e-04,  3.5789e-04,  2.2703e-04,  ..., -3.2077e-04,
         -6.2228e-05,  2.3638e-04],
        [-3.3498e-05, -2.2456e-05,  8.4862e-06,  ..., -3.0860e-05,
         -3.0510e-06, -6.9775e-06],
        [-2.3425e-05, -1.5698e-05,  5.9232e-06,  ..., -2.1592e-05,
         -2.1318e-06, -4.8652e-06],
        [-3.7462e-05, -2.5094e-05,  9.5740e-06,  ..., -3.4541e-05,
         -3.4031e-06, -7.7561e-06],
        [-6.4135e-05, -4.2826e-05,  1.6078e-05,  ..., -5.8979e-05,
         -5.8822e-06, -1.3158e-05]], device='cuda:0')
Loss: 1.2090948820114136
Graident accumulation at epoch 0, step 95, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.9018e-04, -2.5274e-04,  1.3299e-04,  ...,  2.7174e-04,
         -7.7080e-04, -1.7216e-04],
        [-1.9609e-05, -1.9521e-05,  1.9348e-06,  ..., -1.5274e-05,
         -3.1419e-06, -3.6813e-06],
        [-1.9352e-05, -1.4831e-05,  9.7156e-07,  ..., -6.9383e-06,
         -4.2967e-06, -3.8364e-06],
        [-1.6917e-05,  8.1540e-06,  1.3495e-06,  ..., -2.7578e-06,
          1.7354e-06,  1.1593e-05],
        [-2.9042e-05, -3.0273e-05, -1.6010e-06,  ..., -1.9702e-05,
         -4.6342e-06, -2.8073e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0594e-08, 2.0283e-08, 3.0571e-08,  ..., 2.2669e-08, 5.3872e-08,
         1.1365e-08],
        [9.2878e-12, 1.0073e-11, 2.1987e-13,  ..., 5.3524e-12, 2.7553e-13,
         3.3826e-13],
        [9.9717e-12, 1.9391e-11, 1.7690e-12,  ..., 1.8788e-11, 2.9033e-12,
         1.2473e-12],
        [3.2029e-11, 1.6986e-10, 3.5723e-12,  ..., 3.0756e-11, 1.5812e-11,
         5.4198e-11],
        [2.5494e-11, 2.6276e-11, 2.8735e-12,  ..., 1.6497e-11, 5.3754e-13,
         1.2759e-12]], device='cuda:0')
optimizer state dict: 12.0
lr: [1.9986907288753243e-05, 1.9986907288753243e-05]
scheduler_last_epoch: 12


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[   0, 3119,  278,  ...,  352,  674,  369],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1433e-04, -2.6302e-05,  1.1482e-04,  ..., -9.9002e-05,
         -3.2116e-05, -1.5612e-05],
        [-4.1127e-06, -2.7418e-06,  1.2890e-06,  ..., -3.9637e-06,
         -2.4587e-07, -1.1995e-06],
        [ 1.4350e-04,  1.4550e-04, -3.3605e-05,  ...,  2.1572e-04,
         -2.2020e-05,  9.9067e-05],
        [-4.7982e-06, -3.1888e-06,  1.5050e-06,  ..., -4.5896e-06,
         -2.8498e-07, -1.3858e-06],
        [-7.6890e-06, -5.1260e-06,  2.3842e-06,  ..., -7.3910e-06,
         -4.6566e-07, -2.2203e-06]], device='cuda:0')
Loss: 1.2381075620651245


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5596e-05,  1.2157e-05,  9.7360e-05,  ..., -9.8575e-05,
         -8.0121e-05, -4.8877e-05],
        [-8.1658e-06, -5.3644e-06,  2.5332e-06,  ..., -7.6890e-06,
         -4.6380e-07, -2.4065e-06],
        [ 1.4072e-04,  1.4369e-04, -3.2744e-05,  ...,  2.1314e-04,
         -2.2169e-05,  9.8233e-05],
        [-9.5665e-06, -6.2734e-06,  2.9802e-06,  ..., -8.9705e-06,
         -5.4017e-07, -2.7940e-06],
        [-1.5497e-05, -1.0192e-05,  4.7833e-06,  ..., -1.4603e-05,
         -8.9407e-07, -4.5449e-06]], device='cuda:0')
Loss: 1.217403531074524


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0335e-04, -7.0297e-06,  2.1606e-05,  ...,  1.9815e-06,
         -6.3687e-05,  3.7228e-05],
        [-1.2487e-05, -8.1062e-06,  3.7700e-06,  ..., -1.1683e-05,
         -8.1584e-07, -3.5986e-06],
        [ 1.3783e-04,  1.4184e-04, -3.1910e-05,  ...,  2.1044e-04,
         -2.2409e-05,  9.7428e-05],
        [-1.4365e-05, -9.3281e-06,  4.3735e-06,  ..., -1.3441e-05,
         -9.2946e-07, -4.1202e-06],
        [-2.3603e-05, -1.5378e-05,  7.1079e-06,  ..., -2.2143e-05,
         -1.5609e-06, -6.7800e-06]], device='cuda:0')
Loss: 1.2312082052230835


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,   590,    16,  ...,    13,    35,  1151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3474e-04, -4.5507e-05,  4.4893e-05,  ...,  8.1774e-06,
         -6.1448e-05,  3.2314e-05],
        [-1.6779e-05, -1.0803e-05,  5.0440e-06,  ..., -1.5706e-05,
         -1.0785e-06, -4.8876e-06],
        [ 1.3501e-04,  1.4006e-04, -3.1068e-05,  ...,  2.0779e-04,
         -2.2580e-05,  9.6579e-05],
        [-1.9282e-05, -1.2428e-05,  5.8413e-06,  ..., -1.8060e-05,
         -1.2238e-06, -5.5879e-06],
        [-3.1590e-05, -2.0385e-05,  9.4622e-06,  ..., -2.9624e-05,
         -2.0489e-06, -9.1493e-06]], device='cuda:0')
Loss: 1.2311487197875977


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   17,  271,  266],
        [   0, 6112,  278,  ..., 4092,  490, 2774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9547e-04, -6.6749e-05,  2.0886e-05,  ...,  4.9524e-05,
         -1.3613e-04,  9.3947e-05],
        [-2.0951e-05, -1.3486e-05,  6.3702e-06,  ..., -1.9610e-05,
         -1.3355e-06, -6.0126e-06],
        [ 1.9786e-04,  1.9873e-04, -6.4065e-05,  ...,  3.1118e-04,
          2.1287e-06,  1.4962e-04],
        [-2.4170e-05, -1.5572e-05,  7.3984e-06,  ..., -2.2620e-05,
         -1.5181e-06, -6.9067e-06],
        [-3.9518e-05, -2.5481e-05,  1.1966e-05,  ..., -3.7014e-05,
         -2.5332e-06, -1.1265e-05]], device='cuda:0')
Loss: 1.2035574913024902


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[   0, 1064, 1042,  ...,   12,  259, 4754],
        [   0,  278,  565,  ..., 1125, 5222,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6065e-04, -6.3583e-05,  2.7727e-05,  ...,  7.8624e-05,
         -8.5556e-05,  1.9496e-04],
        [-2.5243e-05, -1.6183e-05,  7.6070e-06,  ..., -2.3454e-05,
         -1.6093e-06, -7.1898e-06],
        [ 1.9492e-04,  1.9689e-04, -6.3220e-05,  ...,  3.0855e-04,
          1.9425e-06,  1.4882e-04],
        [-2.9057e-05, -1.8641e-05,  8.8140e-06,  ..., -2.7001e-05,
         -1.8291e-06, -8.2478e-06],
        [-4.7684e-05, -3.0607e-05,  1.4320e-05,  ..., -4.4346e-05,
         -3.0696e-06, -1.3500e-05]], device='cuda:0')
Loss: 1.2202738523483276


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,     9,   300,  ...,  6838,   328, 18619],
        [    0,  1934,  2413,  ..., 19697,    13, 16325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4149e-04, -5.4743e-05, -2.1779e-05,  ...,  1.4970e-04,
         -2.0296e-05,  2.3075e-04],
        [-2.9474e-05, -1.8850e-05,  8.8215e-06,  ..., -2.7418e-05,
         -1.8999e-06, -8.3894e-06],
        [ 1.9210e-04,  1.9511e-04, -6.2415e-05,  ...,  3.0590e-04,
          1.7478e-06,  1.4802e-04],
        [-3.3885e-05, -2.1696e-05,  1.0207e-05,  ..., -3.1531e-05,
         -2.1607e-06, -9.6187e-06],
        [-5.5552e-05, -3.5584e-05,  1.6570e-05,  ..., -5.1767e-05,
         -3.6210e-06, -1.5721e-05]], device='cuda:0')
Loss: 1.2057510614395142


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0,  4485,   741,  ...,   292,   221,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9668e-04, -1.2772e-04,  1.6008e-05,  ...,  5.3150e-05,
         -1.6815e-05,  2.9376e-04],
        [-3.3706e-05, -2.1607e-05,  1.0066e-05,  ..., -3.1412e-05,
         -2.1458e-06, -9.6038e-06],
        [ 1.8926e-04,  1.9326e-04, -6.1577e-05,  ...,  3.0322e-04,
          1.5830e-06,  1.4720e-04],
        [-3.8743e-05, -2.4855e-05,  1.1653e-05,  ..., -3.6120e-05,
         -2.4382e-06, -1.1005e-05],
        [-6.3479e-05, -4.0740e-05,  1.8895e-05,  ..., -5.9247e-05,
         -4.0885e-06, -1.7986e-05]], device='cuda:0')
Loss: 1.2044321298599243
Graident accumulation at epoch 0, step 103, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.3083e-04, -2.4024e-04,  1.2129e-04,  ...,  2.4988e-04,
         -6.9540e-04, -1.2557e-04],
        [-2.1019e-05, -1.9730e-05,  2.7479e-06,  ..., -1.6887e-05,
         -3.0423e-06, -4.2736e-06],
        [ 1.5093e-06,  5.9783e-06, -5.2833e-06,  ...,  2.4077e-05,
         -3.7087e-06,  1.1268e-05],
        [-1.9099e-05,  4.8531e-06,  2.3798e-06,  ..., -6.0940e-06,
          1.3181e-06,  9.3337e-06],
        [-3.2485e-05, -3.1320e-05,  4.4858e-07,  ..., -2.3657e-05,
         -4.5797e-06, -4.3251e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0621e-08, 2.0279e-08, 3.0540e-08,  ..., 2.2649e-08, 5.3819e-08,
         1.1439e-08],
        [1.0415e-11, 1.0530e-11, 3.2097e-13,  ..., 6.3338e-12, 2.7986e-13,
         4.3016e-13],
        [4.5780e-11, 5.6723e-11, 5.5590e-12,  ..., 1.1071e-10, 2.9029e-12,
         2.2915e-11],
        [3.3498e-11, 1.7031e-10, 3.7045e-12,  ..., 3.2029e-11, 1.5803e-11,
         5.4265e-11],
        [2.9498e-11, 2.7909e-11, 3.2276e-12,  ..., 1.9991e-11, 5.5372e-13,
         1.5982e-12]], device='cuda:0')
optimizer state dict: 13.0
lr: [1.9979820805767768e-05, 1.9979820805767768e-05]
scheduler_last_epoch: 13


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3964e-05,  7.0505e-05,  1.9763e-05,  ..., -4.5684e-05,
         -5.0492e-05,  0.0000e+00],
        [-4.2617e-06, -2.6226e-06,  1.3709e-06,  ..., -3.9935e-06,
         -1.8720e-07, -1.4454e-06],
        [-2.7865e-06, -1.7211e-06,  8.9407e-07,  ..., -2.6077e-06,
         -1.2387e-07, -9.4622e-07],
        [-5.0068e-06, -3.0845e-06,  1.6168e-06,  ..., -4.6790e-06,
         -2.1514e-07, -1.6913e-06],
        [-7.9274e-06, -4.8876e-06,  2.5332e-06,  ..., -7.4208e-06,
         -3.5577e-07, -2.6822e-06]], device='cuda:0')
Loss: 1.2200907468795776


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2856e-05,  9.7881e-05,  3.8873e-05,  ..., -6.6345e-05,
          2.4654e-05,  2.3560e-05],
        [-8.6427e-06, -5.4389e-06,  2.7865e-06,  ..., -8.1062e-06,
         -4.0978e-07, -2.9281e-06],
        [-5.6177e-06, -3.5465e-06,  1.8068e-06,  ..., -5.2750e-06,
         -2.6450e-07, -1.8999e-06],
        [-1.0073e-05, -6.3479e-06,  3.2559e-06,  ..., -9.4473e-06,
         -4.6659e-07, -3.4049e-06],
        [-1.6153e-05, -1.0192e-05,  5.1707e-06,  ..., -1.5169e-05,
         -7.6927e-07, -5.4389e-06]], device='cuda:0')
Loss: 1.2166001796722412


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0,  527, 2811,  ...,  287, 1288,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5729e-05,  8.5532e-05,  1.1143e-04,  ..., -2.3690e-04,
          1.1191e-05,  1.5516e-06],
        [-1.2904e-05, -8.0913e-06,  4.0904e-06,  ..., -1.2100e-05,
         -5.9325e-07, -4.3958e-06],
        [-8.4043e-06, -5.2899e-06,  2.6599e-06,  ..., -7.8827e-06,
         -3.8557e-07, -2.8610e-06],
        [-1.5080e-05, -9.4622e-06,  4.7833e-06,  ..., -1.4126e-05,
         -6.7893e-07, -5.1185e-06],
        [-2.4140e-05, -1.5169e-05,  7.5996e-06,  ..., -2.2680e-05,
         -1.1232e-06, -8.1807e-06]], device='cuda:0')
Loss: 1.2107356786727905


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[   0,  300, 1064,  ..., 6953,  944,  278],
        [   0,   35, 3815,  ...,  278, 7097, 4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2246e-04,  6.5928e-05,  1.1582e-04,  ..., -1.7931e-04,
          9.1146e-06, -1.5104e-05],
        [-1.7226e-05, -1.0699e-05,  5.4911e-06,  ..., -1.6123e-05,
         -7.8697e-07, -5.8785e-06],
        [-1.1250e-05, -7.0035e-06,  3.5800e-06,  ..., -1.0520e-05,
         -5.1409e-07, -3.8370e-06],
        [-2.0027e-05, -1.2428e-05,  6.3926e-06,  ..., -1.8716e-05,
         -8.9221e-07, -6.8098e-06],
        [-3.2187e-05, -1.9997e-05,  1.0192e-05,  ..., -3.0130e-05,
         -1.4864e-06, -1.0923e-05]], device='cuda:0')
Loss: 1.2053838968276978


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1732e-05,  8.7674e-05,  1.7844e-04,  ..., -2.8903e-04,
          8.6592e-05,  3.6959e-06],
        [-2.1398e-05, -1.3337e-05,  6.8843e-06,  ..., -2.0146e-05,
         -8.7544e-07, -7.2792e-06],
        [-1.3992e-05, -8.7321e-06,  4.4927e-06,  ..., -1.3173e-05,
         -5.7206e-07, -4.7572e-06],
        [-2.4945e-05, -1.5527e-05,  8.0317e-06,  ..., -2.3454e-05,
         -9.9139e-07, -8.4490e-06],
        [-3.9876e-05, -2.4855e-05,  1.2755e-05,  ..., -3.7581e-05,
         -1.6531e-06, -1.3500e-05]], device='cuda:0')
Loss: 1.22590172290802


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,  9017,   600,  ...,  6133,  1098,   352],
        [    0,   287,  6761,  ...,  1918, 33351,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2702e-05,  1.2021e-04,  2.4543e-04,  ..., -3.4497e-04,
          8.1441e-05,  6.5944e-05],
        [-2.5749e-05, -1.6063e-05,  8.2478e-06,  ..., -2.4229e-05,
         -1.0524e-06, -8.8066e-06],
        [-1.6823e-05, -1.0513e-05,  5.3868e-06,  ..., -1.5840e-05,
         -6.8708e-07, -5.7556e-06],
        [-3.0041e-05, -1.8731e-05,  9.6485e-06,  ..., -2.8253e-05,
         -1.1954e-06, -1.0237e-05],
        [-4.7982e-05, -2.9951e-05,  1.5303e-05,  ..., -4.5210e-05,
         -1.9828e-06, -1.6332e-05]], device='cuda:0')
Loss: 1.2179982662200928


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[    0,   445,     8,  ...,    13, 25386,    17],
        [    0,  1756,   271,  ...,   259, 48595, 19882]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8504e-05,  9.3329e-05,  2.4140e-04,  ..., -2.7232e-04,
          7.5327e-05,  2.8506e-05],
        [-3.0011e-05, -1.8656e-05,  9.5963e-06,  ..., -2.8193e-05,
         -1.1395e-06, -1.0304e-05],
        [-1.9640e-05, -1.2226e-05,  6.2808e-06,  ..., -1.8463e-05,
         -7.4576e-07, -6.7465e-06],
        [-3.5048e-05, -2.1771e-05,  1.1243e-05,  ..., -3.2902e-05,
         -1.3006e-06, -1.1995e-05],
        [-5.5969e-05, -3.4809e-05,  1.7822e-05,  ..., -5.2601e-05,
         -2.1579e-06, -1.9118e-05]], device='cuda:0')
Loss: 1.2060751914978027


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,  3380,  1197,  ...,   631,   369,  3123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3367e-04,  8.1288e-05,  2.5614e-04,  ..., -1.8992e-04,
          7.2114e-05,  5.5530e-06],
        [-3.4392e-05, -2.1398e-05,  1.0952e-05,  ..., -3.2187e-05,
         -1.2754e-06, -1.1750e-05],
        [-2.2516e-05, -1.4029e-05,  7.1749e-06,  ..., -2.1085e-05,
         -8.3749e-07, -7.6964e-06],
        [-4.0054e-05, -2.4915e-05,  1.2808e-05,  ..., -3.7462e-05,
         -1.4598e-06, -1.3649e-05],
        [-6.4135e-05, -3.9905e-05,  2.0340e-05,  ..., -6.0022e-05,
         -2.4205e-06, -2.1800e-05]], device='cuda:0')
Loss: 1.212812066078186
Graident accumulation at epoch 0, step 111, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.6111e-04, -2.0809e-04,  1.3478e-04,  ...,  2.0590e-04,
         -6.1865e-04, -1.1245e-04],
        [-2.2356e-05, -1.9896e-05,  3.5684e-06,  ..., -1.8417e-05,
         -2.8656e-06, -5.0212e-06],
        [-8.9315e-07,  3.9775e-06, -4.0375e-06,  ...,  1.9561e-05,
         -3.4216e-06,  9.3711e-06],
        [-2.1195e-05,  1.8763e-06,  3.4226e-06,  ..., -9.2308e-06,
          1.0403e-06,  7.0354e-06],
        [-3.5650e-05, -3.2178e-05,  2.4377e-06,  ..., -2.7293e-05,
         -4.3637e-06, -6.0726e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0579e-08, 2.0265e-08, 3.0576e-08,  ..., 2.2663e-08, 5.3770e-08,
         1.1428e-08],
        [1.1587e-11, 1.0978e-11, 4.4061e-13,  ..., 7.3634e-12, 2.8121e-13,
         5.6778e-13],
        [4.6242e-11, 5.6863e-11, 5.6049e-12,  ..., 1.1104e-10, 2.9007e-12,
         2.2951e-11],
        [3.5069e-11, 1.7076e-10, 3.8649e-12,  ..., 3.3401e-11, 1.5789e-11,
         5.4397e-11],
        [3.3582e-11, 2.9474e-11, 3.6381e-12,  ..., 2.3574e-11, 5.5902e-13,
         2.0718e-12]], device='cuda:0')
optimizer state dict: 14.0
lr: [1.997120931904809e-05, 1.997120931904809e-05]
scheduler_last_epoch: 14


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5285e-05,  2.5428e-05,  4.5488e-05,  ..., -5.7467e-05,
          0.0000e+00, -1.4887e-04],
        [-4.2915e-06, -2.5779e-06,  1.2517e-06,  ..., -3.9041e-06,
         -4.4471e-08, -1.6391e-06],
        [-2.7418e-06, -1.6466e-06,  8.0094e-07,  ..., -2.4885e-06,
         -2.7125e-08, -1.0505e-06],
        [-5.0664e-06, -3.0398e-06,  1.4901e-06,  ..., -4.6194e-06,
         -5.0291e-08, -1.9372e-06],
        [-7.9870e-06, -4.7982e-06,  2.3246e-06,  ..., -7.2420e-06,
         -8.5216e-08, -3.0398e-06]], device='cuda:0')
Loss: 1.1980466842651367


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[   0,  360,  259,  ...,   14,  381, 1371],
        [   0,  995,   13,  ..., 3494,  367, 6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8895e-05,  7.1864e-05,  2.0188e-04,  ..., -1.5265e-04,
         -1.5346e-05, -1.4455e-04],
        [-8.7023e-06, -5.1856e-06,  2.4959e-06,  ..., -7.8082e-06,
         -8.2422e-08, -3.2112e-06],
        [-5.5730e-06, -3.3230e-06,  1.6056e-06,  ..., -4.9919e-06,
         -5.1106e-08, -2.0638e-06],
        [-1.0252e-05, -6.1095e-06,  2.9728e-06,  ..., -9.2089e-06,
         -9.2899e-08, -3.7923e-06],
        [-1.6272e-05, -9.7156e-06,  4.6641e-06,  ..., -1.4603e-05,
         -1.6112e-07, -5.9903e-06]], device='cuda:0')
Loss: 1.2234630584716797


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6697e-05,  7.7317e-05,  2.6264e-04,  ..., -1.6321e-04,
         -2.4299e-05, -9.9097e-05],
        [-1.2994e-05, -7.7933e-06,  3.7327e-06,  ..., -1.1742e-05,
         -2.0023e-07, -4.8354e-06],
        [-8.3596e-06, -5.0068e-06,  2.4065e-06,  ..., -7.5251e-06,
         -1.2608e-07, -3.1143e-06],
        [-1.5259e-05, -9.1344e-06,  4.4107e-06,  ..., -1.3769e-05,
         -2.2608e-07, -5.6699e-06],
        [-2.4498e-05, -1.4693e-05,  7.0184e-06,  ..., -2.2113e-05,
         -3.9116e-07, -9.0748e-06]], device='cuda:0')
Loss: 1.2005606889724731


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0, 1732,  292,  ..., 3440, 4010, 1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7068e-05,  1.0219e-04,  2.2026e-04,  ..., -1.0730e-04,
         -1.9806e-05, -1.0049e-04],
        [-1.7345e-05, -1.0461e-05,  5.0440e-06,  ..., -1.5706e-05,
         -3.1851e-07, -6.4597e-06],
        [-1.1146e-05, -6.7130e-06,  3.2485e-06,  ..., -1.0058e-05,
         -2.0058e-07, -4.1574e-06],
        [-2.0295e-05, -1.2219e-05,  5.9456e-06,  ..., -1.8388e-05,
         -3.6019e-07, -7.5549e-06],
        [-3.2663e-05, -1.9699e-05,  9.4771e-06,  ..., -2.9594e-05,
         -6.1840e-07, -1.2115e-05]], device='cuda:0')
Loss: 1.2155998945236206


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  1874,   300,  ...,    14,  5372,    12],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8980e-05,  1.0219e-04,  2.3654e-04,  ..., -1.2019e-04,
         -3.0275e-05, -9.0995e-05],
        [-2.1696e-05, -1.3098e-05,  6.3702e-06,  ..., -1.9670e-05,
         -4.3306e-07, -8.1062e-06],
        [-1.3947e-05, -8.4043e-06,  4.0978e-06,  ..., -1.2606e-05,
         -2.7416e-07, -5.2154e-06],
        [-2.5362e-05, -1.5274e-05,  7.4878e-06,  ..., -2.3007e-05,
         -4.9151e-07, -9.4622e-06],
        [-4.0770e-05, -2.4587e-05,  1.1936e-05,  ..., -3.6985e-05,
         -8.3819e-07, -1.5169e-05]], device='cuda:0')
Loss: 1.1947963237762451


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,  1103,   271,  ...,   957,   756,   368],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9298e-05,  5.9629e-05,  2.3920e-04,  ..., -1.4318e-04,
         -9.2345e-05, -8.9089e-05],
        [-2.6017e-05, -1.5691e-05,  7.6368e-06,  ..., -2.3633e-05,
         -5.0571e-07, -9.7677e-06],
        [-1.6734e-05, -1.0073e-05,  4.9137e-06,  ..., -1.5154e-05,
         -3.2212e-07, -6.2883e-06],
        [-3.0488e-05, -1.8343e-05,  9.0003e-06,  ..., -2.7716e-05,
         -5.7532e-07, -1.1429e-05],
        [-4.8876e-05, -2.9445e-05,  1.4305e-05,  ..., -4.4405e-05,
         -9.7975e-07, -1.8269e-05]], device='cuda:0')
Loss: 1.214760661125183


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1597e-04,  5.5686e-05,  3.0826e-04,  ..., -7.0970e-05,
         -9.3767e-05, -2.4123e-05],
        [-3.0339e-05, -1.8254e-05,  8.8885e-06,  ..., -2.7567e-05,
         -5.8394e-07, -1.1407e-05],
        [-1.9535e-05, -1.1735e-05,  5.7258e-06,  ..., -1.7703e-05,
         -3.7241e-07, -7.3463e-06],
        [-3.5614e-05, -2.1368e-05,  1.0490e-05,  ..., -3.2395e-05,
         -6.6846e-07, -1.3351e-05],
        [-5.6982e-05, -3.4243e-05,  1.6630e-05,  ..., -5.1767e-05,
         -1.1316e-06, -2.1294e-05]], device='cuda:0')
Loss: 1.2064613103866577


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8832e-04,  9.8069e-05,  3.1038e-04,  ..., -1.0344e-04,
         -3.3312e-06, -5.7010e-05],
        [-3.4690e-05, -2.0966e-05,  1.0200e-05,  ..., -3.1561e-05,
         -6.4727e-07, -1.2994e-05],
        [-2.2292e-05, -1.3456e-05,  6.5602e-06,  ..., -2.0221e-05,
         -4.1339e-07, -8.3521e-06],
        [-4.0680e-05, -2.4527e-05,  1.2025e-05,  ..., -3.7044e-05,
         -7.3598e-07, -1.5199e-05],
        [-6.4969e-05, -3.9250e-05,  1.9044e-05,  ..., -5.9098e-05,
         -1.2517e-06, -2.4214e-05]], device='cuda:0')
Loss: 1.2055790424346924
Graident accumulation at epoch 0, step 119, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0034,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0383e-04, -1.7747e-04,  1.5234e-04,  ...,  1.7497e-04,
         -5.5712e-04, -1.0691e-04],
        [-2.3590e-05, -2.0003e-05,  4.2315e-06,  ..., -1.9732e-05,
         -2.6437e-06, -5.8184e-06],
        [-3.0331e-06,  2.2342e-06, -2.9777e-06,  ...,  1.5583e-05,
         -3.1208e-06,  7.5988e-06],
        [-2.3143e-05, -7.6407e-07,  4.2829e-06,  ..., -1.2012e-05,
          8.6264e-07,  4.8119e-06],
        [-3.8582e-05, -3.2885e-05,  4.0983e-06,  ..., -3.0474e-05,
         -4.0525e-06, -7.8868e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0553e-08, 2.0254e-08, 3.0641e-08,  ..., 2.2651e-08, 5.3716e-08,
         1.1420e-08],
        [1.2779e-11, 1.1406e-11, 5.4420e-13,  ..., 8.3521e-12, 2.8135e-13,
         7.3605e-13],
        [4.6692e-11, 5.6987e-11, 5.6423e-12,  ..., 1.1134e-10, 2.8980e-12,
         2.2998e-11],
        [3.6689e-11, 1.7119e-10, 4.0056e-12,  ..., 3.4740e-11, 1.5774e-11,
         5.4573e-11],
        [3.7770e-11, 3.0985e-11, 3.9971e-12,  ..., 2.7043e-11, 5.6003e-13,
         2.6561e-12]], device='cuda:0')
optimizer state dict: 15.0
lr: [1.996107414450454e-05, 1.996107414450454e-05]
scheduler_last_epoch: 15


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3745e-05,  2.7827e-05, -3.5145e-05,  ...,  2.8055e-05,
          4.1965e-06, -5.9378e-05],
        [-4.3213e-06, -2.6375e-06,  1.1846e-06,  ..., -3.8445e-06,
         -9.6858e-08, -1.8105e-06],
        [-2.7716e-06, -1.6987e-06,  7.5996e-07,  ..., -2.4736e-06,
         -6.2399e-08, -1.1623e-06],
        [-5.1260e-06, -3.1292e-06,  1.4156e-06,  ..., -4.5598e-06,
         -1.1409e-07, -2.1458e-06],
        [-8.2850e-06, -5.0664e-06,  2.2650e-06,  ..., -7.3612e-06,
         -1.8626e-07, -3.4720e-06]], device='cuda:0')
Loss: 1.2038288116455078


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5145e-05, -2.7777e-05, -9.7223e-07,  ..., -1.5899e-05,
          2.3535e-05, -1.3267e-04],
        [-8.6725e-06, -5.3495e-06,  2.4289e-06,  ..., -7.7188e-06,
         -1.9697e-07, -3.6359e-06],
        [-5.5730e-06, -3.4496e-06,  1.5609e-06,  ..., -4.9770e-06,
         -1.2573e-07, -2.3395e-06],
        [-1.0192e-05, -6.2883e-06,  2.8759e-06,  ..., -9.0897e-06,
         -2.2771e-07, -4.2617e-06],
        [-1.6570e-05, -1.0222e-05,  4.6343e-06,  ..., -1.4782e-05,
         -3.7719e-07, -6.9290e-06]], device='cuda:0')
Loss: 1.2050583362579346


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[   0,  462, 9202,  ...,   15, 3256,  271],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6003e-05, -1.0312e-04, -2.7224e-05,  ...,  4.2647e-06,
         -2.3528e-05, -2.5512e-05],
        [-1.3024e-05, -8.0466e-06,  3.6508e-06,  ..., -1.1593e-05,
         -2.1735e-07, -5.3942e-06],
        [-8.3894e-06, -5.1856e-06,  2.3507e-06,  ..., -7.4804e-06,
         -1.3900e-07, -3.4720e-06],
        [-1.5259e-05, -9.4175e-06,  4.3064e-06,  ..., -1.3590e-05,
         -2.4808e-07, -6.3032e-06],
        [-2.4974e-05, -1.5408e-05,  6.9886e-06,  ..., -2.2262e-05,
         -4.2305e-07, -1.0327e-05]], device='cuda:0')
Loss: 1.2051247358322144


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   266,  8802,  ...,  8401,     9,   287],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9574e-05, -1.5490e-04, -5.8815e-05,  ..., -2.8645e-05,
         -1.3795e-05,  1.8178e-05],
        [-1.7405e-05, -1.0744e-05,  4.8578e-06,  ..., -1.5527e-05,
         -2.7462e-07, -7.2345e-06],
        [-1.1176e-05, -6.9067e-06,  3.1218e-06,  ..., -9.9838e-06,
         -1.7299e-07, -4.6492e-06],
        [-2.0325e-05, -1.2532e-05,  5.7071e-06,  ..., -1.8150e-05,
         -3.0908e-07, -8.4341e-06],
        [-3.3379e-05, -2.0593e-05,  9.2983e-06,  ..., -2.9832e-05,
         -5.3295e-07, -1.3858e-05]], device='cuda:0')
Loss: 1.2117854356765747


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8941e-05, -1.7831e-04, -2.7824e-05,  ..., -2.9194e-05,
          5.5108e-07,  2.8985e-05],
        [-2.1815e-05, -1.3366e-05,  6.0648e-06,  ..., -1.9401e-05,
         -3.3120e-07, -9.0301e-06],
        [-1.3977e-05, -8.5756e-06,  3.8892e-06,  ..., -1.2442e-05,
         -2.0908e-07, -5.7891e-06],
        [-2.5481e-05, -1.5616e-05,  7.1377e-06,  ..., -2.2709e-05,
         -3.7288e-07, -1.0550e-05],
        [-4.1783e-05, -2.5600e-05,  1.1608e-05,  ..., -3.7223e-05,
         -6.4191e-07, -1.7270e-05]], device='cuda:0')
Loss: 1.2010778188705444


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 1268,  278,  ...,  461,  925,  630],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3917e-05, -1.6145e-04, -1.4695e-05,  ..., -1.8885e-05,
          2.9985e-06, -6.2368e-06],
        [-2.6166e-05, -1.6049e-05,  7.1973e-06,  ..., -2.3305e-05,
         -4.1595e-07, -1.0788e-05],
        [-1.6734e-05, -1.0282e-05,  4.6045e-06,  ..., -1.4931e-05,
         -2.6356e-07, -6.9067e-06],
        [-3.0577e-05, -1.8761e-05,  8.4639e-06,  ..., -2.7269e-05,
         -4.6787e-07, -1.2606e-05],
        [-5.0128e-05, -3.0756e-05,  1.3769e-05,  ..., -4.4733e-05,
         -8.0676e-07, -2.0638e-05]], device='cuda:0')
Loss: 1.1960797309875488


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    14,   333,   199],
        [    0,   650,    14,  ...,  3687,   278, 26952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3999e-04, -1.1822e-04, -1.9143e-05,  ..., -2.3665e-05,
         -4.4778e-05, -4.8417e-05],
        [-3.0518e-05, -1.8671e-05,  8.3894e-06,  ..., -2.7180e-05,
         -5.4541e-07, -1.2591e-05],
        [-1.9535e-05, -1.1973e-05,  5.3719e-06,  ..., -1.7419e-05,
         -3.4599e-07, -8.0615e-06],
        [-3.5644e-05, -2.1830e-05,  9.8571e-06,  ..., -3.1769e-05,
         -6.1409e-07, -1.4707e-05],
        [-5.8472e-05, -3.5793e-05,  1.6049e-05,  ..., -5.2154e-05,
         -1.0545e-06, -2.4080e-05]], device='cuda:0')
Loss: 1.1917500495910645


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  4665,   909,  ...,  3607,   259,  1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0038e-04, -1.3359e-04,  1.9399e-05,  ...,  2.7772e-06,
         -2.1837e-05, -6.3379e-05],
        [-3.4958e-05, -2.1309e-05,  9.6560e-06,  ..., -3.1054e-05,
         -6.2177e-07, -1.4417e-05],
        [-2.2352e-05, -1.3649e-05,  6.1728e-06,  ..., -1.9878e-05,
         -3.9442e-07, -9.2164e-06],
        [-4.0829e-05, -2.4900e-05,  1.1332e-05,  ..., -3.6269e-05,
         -7.0257e-07, -1.6823e-05],
        [-6.6996e-05, -4.0859e-05,  1.8463e-05,  ..., -5.9575e-05,
         -1.2063e-06, -2.7567e-05]], device='cuda:0')
Loss: 1.2020074129104614
Graident accumulation at epoch 0, step 127, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0047, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5349e-04, -1.7308e-04,  1.3904e-04,  ...,  1.5775e-04,
         -5.0359e-04, -1.0256e-04],
        [-2.4726e-05, -2.0134e-05,  4.7740e-06,  ..., -2.0864e-05,
         -2.4415e-06, -6.6783e-06],
        [-4.9649e-06,  6.4582e-07, -2.0627e-06,  ...,  1.2037e-05,
         -2.8482e-06,  5.9173e-06],
        [-2.4912e-05, -3.1776e-06,  4.9878e-06,  ..., -1.4438e-05,
          7.0612e-07,  2.6484e-06],
        [-4.1423e-05, -3.3683e-05,  5.5348e-06,  ..., -3.3384e-05,
         -3.7679e-06, -9.8548e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0533e-08, 2.0252e-08, 3.0611e-08,  ..., 2.2628e-08, 5.3663e-08,
         1.1412e-08],
        [1.3988e-11, 1.1849e-11, 6.3690e-13,  ..., 9.3081e-12, 2.8145e-13,
         9.4316e-13],
        [4.7145e-11, 5.7117e-11, 5.6748e-12,  ..., 1.1163e-10, 2.8953e-12,
         2.3060e-11],
        [3.8319e-11, 1.7164e-10, 4.1300e-12,  ..., 3.6020e-11, 1.5758e-11,
         5.4802e-11],
        [4.2220e-11, 3.2623e-11, 4.3340e-12,  ..., 3.0565e-11, 5.6093e-13,
         3.4134e-12]], device='cuda:0')
optimizer state dict: 16.0
lr: [1.9949416830880266e-05, 1.9949416830880266e-05]
scheduler_last_epoch: 16


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6020e-05, -1.8335e-08, -3.2768e-05,  ...,  4.1930e-05,
         -2.5881e-05,  2.4462e-05],
        [-4.4405e-06, -2.7567e-06,  1.1176e-06,  ..., -3.8147e-06,
         -9.1735e-08, -2.0266e-06],
        [-2.8610e-06, -1.7807e-06,  7.1898e-07,  ..., -2.4587e-06,
         -5.8673e-08, -1.3039e-06],
        [-5.1856e-06, -3.2187e-06,  1.3113e-06,  ..., -4.4703e-06,
         -1.0291e-07, -2.3544e-06],
        [-8.7023e-06, -5.4240e-06,  2.1905e-06,  ..., -7.5102e-06,
         -1.8533e-07, -3.9637e-06]], device='cuda:0')
Loss: 1.2105021476745605


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1682,  271,  ...,  367, 3210,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9043e-05,  1.7106e-05, -1.0102e-04,  ...,  1.1564e-05,
         -1.4436e-05, -2.2950e-05],
        [-8.7619e-06, -5.4389e-06,  2.2277e-06,  ..., -7.6145e-06,
         -1.9791e-07, -3.9190e-06],
        [-5.6028e-06, -3.4943e-06,  1.4231e-06,  ..., -4.8727e-06,
         -1.2666e-07, -2.5108e-06],
        [-1.0192e-05, -6.3330e-06,  2.6003e-06,  ..., -8.8811e-06,
         -2.2305e-07, -4.5598e-06],
        [-1.7166e-05, -1.0699e-05,  4.3511e-06,  ..., -1.4961e-05,
         -3.9767e-07, -7.6741e-06]], device='cuda:0')
Loss: 1.2233384847640991


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0,   471,    12,  ...,    13,  9909,  2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4850e-05,  6.8920e-05, -1.0197e-04,  ..., -2.6911e-06,
         -4.7331e-05, -3.7832e-05],
        [-1.3173e-05, -8.1509e-06,  3.2634e-06,  ..., -1.1414e-05,
         -2.7660e-07, -5.8860e-06],
        [-8.4192e-06, -5.2303e-06,  2.0862e-06,  ..., -7.3016e-06,
         -1.7602e-07, -3.7625e-06],
        [-1.5318e-05, -9.4771e-06,  3.8147e-06,  ..., -1.3292e-05,
         -3.1199e-07, -6.8396e-06],
        [-2.5749e-05, -1.6004e-05,  6.3628e-06,  ..., -2.2352e-05,
         -5.5041e-07, -1.1489e-05]], device='cuda:0')
Loss: 1.2209683656692505


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  221,  334,  ...,  706, 2680,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0144e-06,  5.5956e-05, -9.4195e-05,  ...,  4.7005e-05,
         -4.8753e-05, -7.8604e-05],
        [-1.7405e-05, -1.0774e-05,  4.3884e-06,  ..., -1.5125e-05,
         -3.6275e-07, -7.9125e-06],
        [-1.1176e-05, -6.9365e-06,  2.8200e-06,  ..., -9.7156e-06,
         -2.3306e-07, -5.0738e-06],
        [-2.0355e-05, -1.2577e-05,  5.1558e-06,  ..., -1.7673e-05,
         -4.1211e-07, -9.2238e-06],
        [-3.4153e-05, -2.1219e-05,  8.5980e-06,  ..., -2.9713e-05,
         -7.2736e-07, -1.5482e-05]], device='cuda:0')
Loss: 1.1920833587646484


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[    0,  2159,   271,  ...,  1268,   344,   259],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0964e-06,  7.6049e-05, -9.0061e-05,  ..., -2.7147e-06,
         -6.0606e-05, -8.9496e-05],
        [-2.1726e-05, -1.3441e-05,  5.4389e-06,  ..., -1.8895e-05,
         -4.5914e-07, -9.8199e-06],
        [-1.3962e-05, -8.6501e-06,  3.4943e-06,  ..., -1.2144e-05,
         -2.9593e-07, -6.3032e-06],
        [-2.5362e-05, -1.5661e-05,  6.3777e-06,  ..., -2.2054e-05,
         -5.2014e-07, -1.1429e-05],
        [-4.2677e-05, -2.6494e-05,  1.0654e-05,  ..., -3.7163e-05,
         -9.1828e-07, -1.9237e-05]], device='cuda:0')
Loss: 1.2123727798461914


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9580e-05,  1.4141e-04, -9.3923e-05,  ..., -2.1861e-06,
         -6.4534e-05, -1.0795e-04],
        [-2.6107e-05, -1.6138e-05,  6.5491e-06,  ..., -2.2635e-05,
         -5.1502e-07, -1.1757e-05],
        [-1.6809e-05, -1.0408e-05,  4.2133e-06,  ..., -1.4573e-05,
         -3.3178e-07, -7.5623e-06],
        [-3.0488e-05, -1.8805e-05,  7.6741e-06,  ..., -2.6405e-05,
         -5.8347e-07, -1.3694e-05],
        [-5.1320e-05, -3.1829e-05,  1.2830e-05,  ..., -4.4525e-05,
         -1.0328e-06, -2.3052e-05]], device='cuda:0')
Loss: 1.195442795753479


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0, 1979,  352,  ...,  292, 1591,  446],
        [   0,  944,  278,  ..., 2374,  699, 8867]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9389e-05,  1.9161e-04, -1.4028e-04,  ..., -1.2706e-04,
         -4.2677e-05, -1.3102e-04],
        [-3.0458e-05, -1.8835e-05,  7.6890e-06,  ..., -2.6509e-05,
         -6.7521e-07, -1.3784e-05],
        [-1.9595e-05, -1.2144e-05,  4.9435e-06,  ..., -1.7047e-05,
         -4.3469e-07, -8.8662e-06],
        [-3.5495e-05, -2.1935e-05,  8.9929e-06,  ..., -3.0875e-05,
         -7.6508e-07, -1.6034e-05],
        [-5.9724e-05, -3.7074e-05,  1.5035e-05,  ..., -5.2035e-05,
         -1.3513e-06, -2.6986e-05]], device='cuda:0')
Loss: 1.221433401107788


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[    0,  6673,   298,  ...,  4391,   292,   221],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7010e-05,  1.4953e-04, -1.4827e-04,  ..., -1.7790e-04,
         -5.1346e-05, -1.5742e-04],
        [-3.4839e-05, -2.1607e-05,  8.8215e-06,  ..., -3.0294e-05,
         -8.1956e-07, -1.5780e-05],
        [-2.2396e-05, -1.3918e-05,  5.6624e-06,  ..., -1.9461e-05,
         -5.2736e-07, -1.0148e-05],
        [-4.0621e-05, -2.5168e-05,  1.0312e-05,  ..., -3.5286e-05,
         -9.3086e-07, -1.8373e-05],
        [-6.8247e-05, -4.2439e-05,  1.7211e-05,  ..., -5.9336e-05,
         -1.6345e-06, -3.0860e-05]], device='cuda:0')
Loss: 1.1952818632125854
Graident accumulation at epoch 0, step 135, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.9684e-04, -1.4082e-04,  1.1031e-04,  ...,  1.2418e-04,
         -4.5836e-04, -1.0804e-04],
        [-2.5738e-05, -2.0281e-05,  5.1787e-06,  ..., -2.1807e-05,
         -2.2793e-06, -7.5885e-06],
        [-6.7081e-06, -8.1053e-07, -1.2901e-06,  ...,  8.8870e-06,
         -2.6161e-06,  4.3108e-06],
        [-2.6483e-05, -5.3767e-06,  5.5202e-06,  ..., -1.6523e-05,
          5.4242e-07,  5.4623e-07],
        [-4.4106e-05, -3.4558e-05,  6.7024e-06,  ..., -3.5979e-05,
         -3.5546e-06, -1.1955e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0480e-08, 2.0254e-08, 3.0602e-08,  ..., 2.2637e-08, 5.3612e-08,
         1.1426e-08],
        [1.5188e-11, 1.2304e-11, 7.1408e-13,  ..., 1.0217e-11, 2.8184e-13,
         1.1912e-12],
        [4.7600e-11, 5.7253e-11, 5.7012e-12,  ..., 1.1189e-10, 2.8926e-12,
         2.3140e-11],
        [3.9931e-11, 1.7210e-10, 4.2322e-12,  ..., 3.7230e-11, 1.5743e-11,
         5.5085e-11],
        [4.6836e-11, 3.4392e-11, 4.6259e-12,  ..., 3.4055e-11, 5.6304e-13,
         4.3623e-12]], device='cuda:0')
optimizer state dict: 17.0
lr: [1.993623915951455e-05, 1.993623915951455e-05]
scheduler_last_epoch: 17


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[   0,  471,   14,  ..., 1260, 2129,  367],
        [   0,  278, 1099,  ...,  496,   14,  879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7856e-05, -4.1883e-05, -3.1847e-05,  ...,  3.6095e-05,
          4.9850e-05, -3.9008e-05],
        [-4.4107e-06, -2.7418e-06,  9.4995e-07,  ..., -3.6210e-06,
         -1.0245e-07, -2.0415e-06],
        [-2.8312e-06, -1.7658e-06,  6.1095e-07,  ..., -2.3246e-06,
         -6.6590e-08, -1.3113e-06],
        [-5.0962e-06, -3.1739e-06,  1.1027e-06,  ..., -4.1723e-06,
         -1.2014e-07, -2.3544e-06],
        [-8.6427e-06, -5.3942e-06,  1.8626e-06,  ..., -7.1228e-06,
         -2.0862e-07, -3.9935e-06]], device='cuda:0')
Loss: 1.1931864023208618


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0,  9430,   287,  ...,  1141,  2280,   408],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4466e-05, -5.7915e-05, -5.2183e-05,  ...,  1.8549e-05,
          7.8863e-05, -8.0000e-05],
        [-8.7321e-06, -5.4538e-06,  1.9036e-06,  ..., -7.2718e-06,
         -2.0303e-07, -4.0978e-06],
        [-5.6326e-06, -3.5241e-06,  1.2293e-06,  ..., -4.6939e-06,
         -1.3225e-07, -2.6450e-06],
        [-1.0103e-05, -6.3181e-06,  2.2128e-06,  ..., -8.4043e-06,
         -2.3562e-07, -4.7386e-06],
        [-1.7166e-05, -1.0729e-05,  3.7327e-06,  ..., -1.4305e-05,
         -4.1071e-07, -8.0466e-06]], device='cuda:0')
Loss: 1.193744421005249


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,    12,   630,  ...,  5049,    14,  2371],
        [    0,  2973, 20362,  ...,   271, 43821, 11776]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3584e-04, -6.1151e-05, -2.5019e-05,  ...,  6.9980e-06,
          8.3166e-05, -7.0501e-05],
        [-1.3143e-05, -8.1211e-06,  2.8871e-06,  ..., -1.0878e-05,
         -3.8743e-07, -6.1542e-06],
        [ 1.8483e-04,  1.3852e-04, -6.5720e-05,  ...,  1.5302e-04,
          2.2151e-05,  1.0619e-04],
        [-1.5229e-05, -9.4324e-06,  3.3602e-06,  ..., -1.2636e-05,
         -4.4703e-07, -7.1526e-06],
        [-2.5928e-05, -1.6034e-05,  5.6699e-06,  ..., -2.1487e-05,
         -7.8138e-07, -1.2130e-05]], device='cuda:0')
Loss: 1.206203579902649


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  3543,   391,  ...,  3370,  2926,  8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1522e-04, -8.0427e-05,  2.8995e-05,  ...,  6.9980e-06,
          3.7190e-05, -1.2213e-04],
        [-1.7583e-05, -1.0878e-05,  3.8296e-06,  ..., -1.4514e-05,
         -5.0850e-07, -8.2105e-06],
        [ 1.8200e-04,  1.3675e-04, -6.5116e-05,  ...,  1.5070e-04,
          2.2074e-05,  1.0488e-04],
        [-2.0295e-05, -1.2591e-05,  4.4480e-06,  ..., -1.6809e-05,
         -5.8301e-07, -9.5069e-06],
        [-3.4809e-05, -2.1547e-05,  7.5474e-06,  ..., -2.8729e-05,
         -1.0272e-06, -1.6212e-05]], device='cuda:0')
Loss: 1.1961668729782104


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,    13, 26335,  ...,     5,  2570, 34403],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1320e-04, -5.1892e-05,  6.9529e-05,  ..., -2.5188e-05,
          4.5392e-05, -1.2193e-04],
        [-2.1905e-05, -1.3605e-05,  4.7982e-06,  ..., -1.8150e-05,
         -6.8173e-07, -1.0252e-05],
        [ 1.7922e-04,  1.3499e-04, -6.4490e-05,  ...,  1.4836e-04,
          2.1961e-05,  1.0356e-04],
        [-2.5332e-05, -1.5751e-05,  5.5805e-06,  ..., -2.1040e-05,
         -7.8324e-07, -1.1876e-05],
        [-4.3392e-05, -2.6971e-05,  9.4548e-06,  ..., -3.5942e-05,
         -1.3793e-06, -2.0266e-05]], device='cuda:0')
Loss: 1.2151843309402466


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,  1529,  5227,  ...,  1480,   367,   925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5026e-04, -9.6512e-05,  3.2390e-05,  ..., -3.5145e-05,
          5.4044e-05, -7.8665e-05],
        [-2.6256e-05, -1.6347e-05,  5.7593e-06,  ..., -2.1785e-05,
         -8.8383e-07, -1.2264e-05],
        [ 1.7642e-04,  1.3323e-04, -6.3876e-05,  ...,  1.4602e-04,
          2.1831e-05,  1.0227e-04],
        [-3.0309e-05, -1.8895e-05,  6.6832e-06,  ..., -2.5213e-05,
         -1.0123e-06, -1.4186e-05],
        [-5.2035e-05, -3.2425e-05,  1.1355e-05,  ..., -4.3154e-05,
         -1.7853e-06, -2.4259e-05]], device='cuda:0')
Loss: 1.219317078590393


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,    14,  8058,  ..., 10316,   352,   266],
        [    0,   677,  9606,  ...,  9468,  9268,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9051e-05, -1.1530e-04, -4.5161e-05,  ..., -3.7235e-06,
          9.2655e-05, -8.4727e-05],
        [-3.0577e-05, -1.9029e-05,  6.7577e-06,  ..., -2.5317e-05,
         -9.8906e-07, -1.4290e-05],
        [ 1.7361e-04,  1.3151e-04, -6.3231e-05,  ...,  1.4374e-04,
          2.1762e-05,  1.0096e-04],
        [-3.5375e-05, -2.2024e-05,  7.8529e-06,  ..., -2.9325e-05,
         -1.1353e-06, -1.6555e-05],
        [-6.0558e-05, -3.7700e-05,  1.3322e-05,  ..., -5.0098e-05,
         -1.9986e-06, -2.8253e-05]], device='cuda:0')
Loss: 1.1817563772201538


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[    0,  4108,    85,  ...,    40,    12,  1530],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1653e-04, -1.5646e-04, -6.3528e-05,  ..., -1.3865e-05,
          1.3557e-04, -9.0081e-05],
        [-3.4958e-05, -2.1771e-05,  7.7002e-06,  ..., -2.8998e-05,
         -1.1371e-06, -1.6332e-05],
        [ 1.7077e-04,  1.2972e-04, -6.2620e-05,  ...,  1.4135e-04,
          2.1666e-05,  9.9637e-05],
        [-4.0442e-05, -2.5198e-05,  8.9407e-06,  ..., -3.3587e-05,
         -1.3039e-06, -1.8910e-05],
        [-6.9320e-05, -4.3184e-05,  1.5192e-05,  ..., -5.7459e-05,
         -2.3004e-06, -3.2336e-05]], device='cuda:0')
Loss: 1.2080302238464355
Graident accumulation at epoch 0, step 143, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2550e-04, -1.4239e-04,  9.2929e-05,  ...,  1.1038e-04,
         -3.9897e-04, -1.0625e-04],
        [-2.6660e-05, -2.0430e-05,  5.4309e-06,  ..., -2.2526e-05,
         -2.1651e-06, -8.4628e-06],
        [ 1.1040e-05,  1.2243e-05, -7.4231e-06,  ...,  2.2134e-05,
         -1.8785e-07,  1.3843e-05],
        [-2.7879e-05, -7.3588e-06,  5.8622e-06,  ..., -1.8229e-05,
          3.5780e-07, -1.3993e-06],
        [-4.6627e-05, -3.5421e-05,  7.5513e-06,  ..., -3.8127e-05,
         -3.4291e-06, -1.3993e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0433e-08, 2.0258e-08, 3.0576e-08,  ..., 2.2615e-08, 5.3577e-08,
         1.1423e-08],
        [1.6395e-11, 1.2765e-11, 7.7266e-13,  ..., 1.1047e-11, 2.8286e-13,
         1.4568e-12],
        [7.6714e-11, 7.4024e-11, 9.6167e-12,  ..., 1.3176e-10, 3.3592e-12,
         3.3044e-11],
        [4.1526e-11, 1.7257e-10, 4.3079e-12,  ..., 3.8320e-11, 1.5729e-11,
         5.5387e-11],
        [5.1594e-11, 3.6222e-11, 4.8520e-12,  ..., 3.7323e-11, 5.6777e-13,
         5.4035e-12]], device='cuda:0')
optimizer state dict: 18.0
lr: [1.99215431440706e-05, 1.99215431440706e-05]
scheduler_last_epoch: 18


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   452,   298,  ...,   287,  1575,  7856],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4593e-06, -5.4383e-05, -3.0385e-06,  ...,  9.1895e-06,
         -5.2399e-06,  5.0471e-05],
        [-4.2915e-06, -2.8163e-06,  7.9349e-07,  ..., -3.5167e-06,
         -2.3469e-07, -2.1160e-06],
        [-2.7865e-06, -1.8328e-06,  5.1782e-07,  ..., -2.2799e-06,
         -1.5181e-07, -1.3784e-06],
        [-5.0068e-06, -3.2932e-06,  9.3132e-07,  ..., -4.1127e-06,
         -2.7381e-07, -2.4736e-06],
        [-8.6427e-06, -5.6624e-06,  1.5870e-06,  ..., -7.0632e-06,
         -4.7125e-07, -4.2617e-06]], device='cuda:0')
Loss: 1.1957000494003296


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   685,   344,  ...,   680,   401,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6313e-05, -6.6169e-05,  4.0660e-05,  ..., -5.8446e-05,
         -5.7215e-05,  1.2799e-05],
        [-8.5831e-06, -5.5730e-06,  1.5683e-06,  ..., -6.9886e-06,
         -4.1444e-07, -4.2468e-06],
        [-5.5879e-06, -3.6210e-06,  1.0207e-06,  ..., -4.5300e-06,
         -2.6915e-07, -2.7642e-06],
        [-1.0014e-05, -6.4969e-06,  1.8366e-06,  ..., -8.1360e-06,
         -4.7963e-07, -4.9472e-06],
        [-1.7345e-05, -1.1235e-05,  3.1516e-06,  ..., -1.4096e-05,
         -8.4005e-07, -8.5831e-06]], device='cuda:0')
Loss: 1.203396201133728


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  3001,  3325,  ..., 16332,  2661,  1200],
        [    0,    14,  6436,  ...,   271,  1211,  8917]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3272e-05, -6.4090e-05,  2.7405e-05,  ..., -6.4207e-05,
         -5.3401e-05,  3.2384e-05],
        [-1.2875e-05, -8.2999e-06,  2.3358e-06,  ..., -1.0476e-05,
         -5.9977e-07, -6.3032e-06],
        [-8.3745e-06, -5.3942e-06,  1.5199e-06,  ..., -6.7949e-06,
         -3.9116e-07, -4.0978e-06],
        [-1.4961e-05, -9.6411e-06,  2.7232e-06,  ..., -1.2159e-05,
         -6.9290e-07, -7.3165e-06],
        [-2.6047e-05, -1.6749e-05,  4.7013e-06,  ..., -2.1160e-05,
         -1.2219e-06, -1.2726e-05]], device='cuda:0')
Loss: 1.2153596878051758


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6251e-05, -5.0691e-05,  4.1194e-06,  ..., -6.4207e-05,
         -2.7634e-05,  7.4600e-05],
        [-1.7166e-05, -1.1072e-05,  3.1143e-06,  ..., -1.4007e-05,
         -7.7207e-07, -8.3596e-06],
        [-1.1146e-05, -7.1973e-06,  2.0266e-06,  ..., -9.0897e-06,
         -5.0431e-07, -5.4389e-06],
        [-1.9878e-05, -1.2830e-05,  3.6210e-06,  ..., -1.6212e-05,
         -8.9034e-07, -9.6858e-06],
        [-3.4630e-05, -2.2322e-05,  6.2585e-06,  ..., -2.8253e-05,
         -1.5739e-06, -1.6868e-05]], device='cuda:0')
Loss: 1.2028933763504028


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  266,  997,  ..., 2670,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4373e-05, -4.7293e-05, -9.4662e-05,  ..., -9.3589e-05,
         -2.3761e-05,  4.7772e-05],
        [-2.1398e-05, -1.3798e-05,  3.9004e-06,  ..., -1.7479e-05,
         -1.0217e-06, -1.0416e-05],
        [-1.3918e-05, -8.9854e-06,  2.5444e-06,  ..., -1.1355e-05,
         -6.6543e-07, -6.7875e-06],
        [-2.4855e-05, -1.6019e-05,  4.5449e-06,  ..., -2.0266e-05,
         -1.1809e-06, -1.2100e-05],
        [-4.3213e-05, -2.7865e-05,  7.8455e-06,  ..., -3.5286e-05,
         -2.0806e-06, -2.1040e-05]], device='cuda:0')
Loss: 1.1749876737594604


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0, 1412,   35,  ..., 6077,  298, 1826],
        [   0,  278, 5210,  ..., 1968, 2002,  923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3813e-05, -5.2585e-05, -9.8234e-05,  ..., -1.2426e-04,
          3.1733e-06,  5.6649e-05],
        [-2.5719e-05, -1.6555e-05,  4.6641e-06,  ..., -2.0996e-05,
         -1.2377e-06, -1.2502e-05],
        [-1.6749e-05, -1.0796e-05,  3.0436e-06,  ..., -1.3649e-05,
         -8.0606e-07, -8.1509e-06],
        [-2.9862e-05, -1.9222e-05,  5.4352e-06,  ..., -2.4348e-05,
         -1.4286e-06, -1.4514e-05],
        [-5.1975e-05, -3.3468e-05,  9.3803e-06,  ..., -4.2409e-05,
         -2.5164e-06, -2.5243e-05]], device='cuda:0')
Loss: 1.19451105594635


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3338e-05, -9.0348e-05, -4.7762e-05,  ..., -1.5694e-04,
          3.8384e-05,  4.2045e-05],
        [-3.0011e-05, -1.9327e-05,  5.4613e-06,  ..., -2.4393e-05,
         -1.4734e-06, -1.4588e-05],
        [-1.9580e-05, -1.2621e-05,  3.5651e-06,  ..., -1.5885e-05,
         -9.6252e-07, -9.5218e-06],
        [-3.4839e-05, -2.2441e-05,  6.3628e-06,  ..., -2.8282e-05,
         -1.7025e-06, -1.6928e-05],
        [-6.0618e-05, -3.9041e-05,  1.0975e-05,  ..., -4.9233e-05,
         -3.0007e-06, -2.9415e-05]], device='cuda:0')
Loss: 1.1938050985336304


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0708e-05, -6.4459e-05, -1.7223e-05,  ..., -1.4532e-04,
          3.8384e-05,  2.2666e-05],
        [-3.4332e-05, -2.2098e-05,  6.2138e-06,  ..., -2.7880e-05,
         -1.7378e-06, -1.6645e-05],
        [-2.2426e-05, -1.4439e-05,  4.0568e-06,  ..., -1.8179e-05,
         -1.1339e-06, -1.0878e-05],
        [-3.9876e-05, -2.5660e-05,  7.2382e-06,  ..., -3.2336e-05,
         -2.0061e-06, -1.9312e-05],
        [-6.9559e-05, -4.4763e-05,  1.2517e-05,  ..., -5.6416e-05,
         -3.5446e-06, -3.3647e-05]], device='cuda:0')
Loss: 1.2012262344360352
Graident accumulation at epoch 0, step 151, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0037,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0278, -0.0159, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6388e-04, -1.3459e-04,  8.1913e-05,  ...,  8.4809e-05,
         -3.5523e-04, -9.3356e-05],
        [-2.7427e-05, -2.0597e-05,  5.5092e-06,  ..., -2.3061e-05,
         -2.1224e-06, -9.2810e-06],
        [ 7.6930e-06,  9.5747e-06, -6.2751e-06,  ...,  1.8102e-05,
         -2.8245e-07,  1.1371e-05],
        [-2.9078e-05, -9.1889e-06,  5.9998e-06,  ..., -1.9640e-05,
          1.2141e-07, -3.1906e-06],
        [-4.8920e-05, -3.6355e-05,  8.0479e-06,  ..., -3.9956e-05,
         -3.4407e-06, -1.5959e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0381e-08, 2.0242e-08, 3.0546e-08,  ..., 2.2613e-08, 5.3525e-08,
         1.1412e-08],
        [1.7557e-11, 1.3241e-11, 8.1050e-13,  ..., 1.1813e-11, 2.8559e-13,
         1.7324e-12],
        [7.7140e-11, 7.4159e-11, 9.6236e-12,  ..., 1.3196e-10, 3.3571e-12,
         3.3129e-11],
        [4.3075e-11, 1.7305e-10, 4.3560e-12,  ..., 3.9328e-11, 1.5718e-11,
         5.5705e-11],
        [5.6381e-11, 3.8189e-11, 5.0039e-12,  ..., 4.0468e-11, 5.7976e-13,
         6.5302e-12]], device='cuda:0')
optimizer state dict: 19.0
lr: [1.9905331030227867e-05, 1.9905331030227867e-05]
scheduler_last_epoch: 19


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1455e-05,  2.0221e-06,  1.4126e-05,  ..., -6.2307e-05,
          1.7852e-05,  1.6732e-05],
        [-4.2021e-06, -2.7865e-06,  6.5938e-07,  ..., -3.3528e-06,
         -3.1479e-07, -2.0862e-06],
        [ 1.1256e-04,  7.7636e-05, -8.5504e-06,  ...,  6.3365e-05,
          1.4080e-05,  3.3855e-05],
        [-4.7684e-06, -3.1739e-06,  7.5251e-07,  ..., -3.7998e-06,
         -3.5763e-07, -2.3842e-06],
        [-8.5235e-06, -5.6922e-06,  1.3337e-06,  ..., -6.7949e-06,
         -6.4448e-07, -4.2617e-06]], device='cuda:0')
Loss: 1.1940371990203857


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[    0, 39224,    34,  ...,   401,  1716,   271],
        [    0,   741,  4933,  ...,   932,   365,   838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5575e-05,  1.2047e-05,  2.8765e-05,  ..., -7.9113e-05,
         -1.0810e-05,  3.4304e-05],
        [-8.3447e-06, -5.5581e-06,  1.2927e-06,  ..., -6.6608e-06,
         -5.7928e-07, -4.1276e-06],
        [ 1.0976e-04,  7.5765e-05, -8.1257e-06,  ...,  6.1130e-05,
          1.3901e-05,  3.2477e-05],
        [-9.5367e-06, -6.3777e-06,  1.4827e-06,  ..., -7.6145e-06,
         -6.6310e-07, -4.7386e-06],
        [-1.7285e-05, -1.1563e-05,  2.6599e-06,  ..., -1.3798e-05,
         -1.2070e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.203771710395813


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0, 28590,    12,  ...,   342, 29639,  1693],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8966e-05, -1.8264e-05,  2.1340e-05,  ..., -1.1535e-04,
         -4.6949e-05,  4.8429e-05],
        [-1.2547e-05, -8.3745e-06,  1.9148e-06,  ..., -1.0028e-05,
         -9.3691e-07, -6.1691e-06],
        [ 1.0699e-04,  7.3895e-05, -7.7140e-06,  ...,  5.8894e-05,
          1.3663e-05,  3.1121e-05],
        [-1.4305e-05, -9.5814e-06,  2.1942e-06,  ..., -1.1459e-05,
         -1.0692e-06, -7.0781e-06],
        [-2.5928e-05, -1.7375e-05,  3.9339e-06,  ..., -2.0742e-05,
         -1.9483e-06, -1.2785e-05]], device='cuda:0')
Loss: 1.1972758769989014


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5512e-05, -6.4238e-05, -1.6519e-05,  ..., -1.1845e-04,
         -2.2323e-05,  1.5284e-05],
        [-1.6719e-05, -1.1221e-05,  2.5630e-06,  ..., -1.3396e-05,
         -1.2014e-06, -8.2403e-06],
        [ 1.0417e-04,  7.1973e-05, -7.2763e-06,  ...,  5.6629e-05,
          1.3484e-05,  2.9728e-05],
        [-1.9163e-05, -1.2890e-05,  2.9542e-06,  ..., -1.5363e-05,
         -1.3765e-06, -9.4920e-06],
        [-3.4571e-05, -2.3246e-05,  5.2676e-06,  ..., -2.7686e-05,
         -2.4997e-06, -1.7047e-05]], device='cuda:0')
Loss: 1.1986366510391235


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1478e-05, -8.0437e-05, -4.6666e-05,  ..., -1.3249e-04,
         -2.6737e-05,  1.8176e-05],
        [-2.0891e-05, -1.4022e-05,  3.2373e-06,  ..., -1.6689e-05,
         -1.5590e-06, -1.0267e-05],
        [ 1.0136e-04,  7.0088e-05, -6.8218e-06,  ...,  5.4409e-05,
          1.3244e-05,  2.8357e-05],
        [-2.4021e-05, -1.6153e-05,  3.7439e-06,  ..., -1.9208e-05,
         -1.7919e-06, -1.1861e-05],
        [-4.3273e-05, -2.9087e-05,  6.6683e-06,  ..., -3.4571e-05,
         -3.2485e-06, -2.1279e-05]], device='cuda:0')
Loss: 1.1942152976989746


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  4103,   278,  4190],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4620e-05, -7.9137e-05, -6.8366e-05,  ..., -9.2935e-05,
         -3.7454e-06,  1.3532e-05],
        [-2.5034e-05, -1.6779e-05,  3.9190e-06,  ..., -1.9997e-05,
         -1.8887e-06, -1.2308e-05],
        [ 9.8557e-05,  6.8225e-05, -6.3617e-06,  ...,  5.2189e-05,
          1.3021e-05,  2.6979e-05],
        [-2.8819e-05, -1.9342e-05,  4.5374e-06,  ..., -2.3022e-05,
         -2.1718e-06, -1.4231e-05],
        [-5.1916e-05, -3.4809e-05,  8.0764e-06,  ..., -4.1425e-05,
         -3.9376e-06, -2.5541e-05]], device='cuda:0')
Loss: 1.2004778385162354


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,   669,   292,  ...,  4032,   271,  4442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5112e-06, -5.1521e-05, -1.3484e-04,  ..., -5.5616e-05,
          2.7665e-05, -2.0964e-05],
        [-2.9147e-05, -1.9550e-05,  4.5523e-06,  ..., -2.3335e-05,
         -2.1737e-06, -1.4335e-05],
        [ 9.5800e-05,  6.6363e-05, -5.9352e-06,  ...,  4.9954e-05,
          1.2828e-05,  2.5623e-05],
        [-3.3587e-05, -2.2575e-05,  5.2825e-06,  ..., -2.6897e-05,
         -2.5053e-06, -1.6585e-05],
        [-6.0499e-05, -4.0591e-05,  9.3952e-06,  ..., -4.8369e-05,
         -4.5337e-06, -2.9743e-05]], device='cuda:0')
Loss: 1.1978869438171387


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,  361, 1224,  ..., 4401, 4261, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8243e-06, -4.9993e-05, -5.1993e-05,  ..., -9.7747e-05,
          5.7708e-05, -1.9100e-05],
        [-3.3289e-05, -2.2352e-05,  5.1335e-06,  ..., -2.6658e-05,
         -2.4773e-06, -1.6332e-05],
        [ 9.3043e-05,  6.4500e-05, -5.5478e-06,  ...,  4.7733e-05,
          1.2626e-05,  2.4289e-05],
        [ 2.8275e-04,  1.8006e-04,  8.7072e-08,  ...,  2.5082e-04,
          3.1348e-05,  1.2092e-04],
        [-6.9201e-05, -4.6462e-05,  1.0617e-05,  ..., -5.5343e-05,
         -5.1744e-06, -3.3915e-05]], device='cuda:0')
Loss: 1.2157676219940186
Graident accumulation at epoch 0, step 159, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0169,  0.0142, -0.0268,  ...,  0.0278, -0.0159, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1848e-04, -1.2613e-04,  6.8523e-05,  ...,  6.6554e-05,
         -3.1394e-04, -8.5930e-05],
        [-2.8013e-05, -2.0772e-05,  5.4716e-06,  ..., -2.3421e-05,
         -2.1579e-06, -9.9860e-06],
        [ 1.6228e-05,  1.5067e-05, -6.2024e-06,  ...,  2.1065e-05,
          1.0084e-06,  1.2663e-05],
        [ 2.1042e-06,  9.7357e-06,  5.4086e-06,  ...,  7.4059e-06,
          3.2441e-06,  9.2201e-06],
        [-5.0948e-05, -3.7366e-05,  8.3048e-06,  ..., -4.1495e-05,
         -3.6141e-06, -1.7754e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0321e-08, 2.0224e-08, 3.0518e-08,  ..., 2.2600e-08, 5.3475e-08,
         1.1401e-08],
        [1.8648e-11, 1.3727e-11, 8.3604e-13,  ..., 1.2512e-11, 2.9144e-13,
         1.9973e-12],
        [8.5720e-11, 7.8245e-11, 9.6447e-12,  ..., 1.3411e-10, 3.5132e-12,
         3.3686e-11],
        [1.2298e-10, 2.0530e-10, 4.3517e-12,  ..., 1.0220e-10, 1.6685e-11,
         7.0270e-11],
        [6.1113e-11, 4.0310e-11, 5.1116e-12,  ..., 4.3490e-11, 6.0596e-13,
         7.6739e-12]], device='cuda:0')
optimizer state dict: 20.0
lr: [1.9887605295338853e-05, 1.9887605295338853e-05]
scheduler_last_epoch: 20
Epoch 0 | Batch 159/1048 | Training PPL: 16749.78959952863 | time 11.974066257476807
Saving checkpoint at epoch 0, step 159, batch 159
Epoch 0 | Validation PPL: 10.918424350874787 | Learning rate: 1.9887605295338853e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_159, AFTER epoch 0, step 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[   0, 1487,  409,  ..., 6979, 1273,  496],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5573e-05, -1.9300e-05, -2.6009e-05,  ...,  3.6614e-05,
          3.6709e-05,  1.3497e-05],
        [-4.0829e-06, -2.8610e-06,  5.0291e-07,  ..., -3.2187e-06,
         -3.9488e-07, -1.9670e-06],
        [-2.8312e-06, -1.9819e-06,  3.5018e-07,  ..., -2.2352e-06,
         -2.7381e-07, -1.3635e-06],
        [-4.7684e-06, -3.3230e-06,  5.8860e-07,  ..., -3.7402e-06,
         -4.5449e-07, -2.2799e-06],
        [-8.7023e-06, -6.0797e-06,  1.0654e-06,  ..., -6.8247e-06,
         -8.3447e-07, -4.1723e-06]], device='cuda:0')
Loss: 1.201482892036438


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   298,   527,   496],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0339e-05, -2.1595e-05, -2.4840e-05,  ..., -1.8337e-06,
          4.5634e-05, -6.8460e-05],
        [-8.1658e-06, -5.6624e-06,  1.0692e-06,  ..., -6.4224e-06,
         -7.8417e-07, -4.0084e-06],
        [-5.6773e-06, -3.9488e-06,  7.4320e-07,  ..., -4.4703e-06,
         -5.4576e-07, -2.7865e-06],
        [-9.5367e-06, -6.6161e-06,  1.2517e-06,  ..., -7.4953e-06,
         -9.0711e-07, -4.6641e-06],
        [-1.7345e-05, -1.2040e-05,  2.2575e-06,  ..., -1.3620e-05,
         -1.6540e-06, -8.4937e-06]], device='cuda:0')
Loss: 1.199562907218933


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4840e-04, -5.0532e-06, -1.0924e-04,  ..., -2.1836e-05,
          7.0179e-05, -1.0404e-04],
        [-1.2189e-05, -8.4937e-06,  1.6019e-06,  ..., -9.6262e-06,
         -1.1791e-06, -6.0052e-06],
        [-8.4639e-06, -5.9158e-06,  1.1139e-06,  ..., -6.6906e-06,
         -8.1956e-07, -4.1723e-06],
        [-1.4216e-05, -9.8944e-06,  1.8738e-06,  ..., -1.1206e-05,
         -1.3635e-06, -6.9737e-06],
        [-2.5868e-05, -1.8030e-05,  3.3826e-06,  ..., -2.0385e-05,
         -2.4922e-06, -1.2696e-05]], device='cuda:0')
Loss: 1.2028224468231201


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[   0, 4215, 1478,  ...,  644,  409, 3803],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1993e-04,  4.3911e-05, -1.3906e-04,  ..., -1.4087e-05,
          1.2588e-04, -1.2538e-04],
        [-1.6183e-05, -1.1295e-05,  2.0936e-06,  ..., -1.2830e-05,
         -1.4771e-06, -7.9870e-06],
        [-1.1250e-05, -7.8678e-06,  1.4585e-06,  ..., -8.9407e-06,
         -1.0272e-06, -5.5581e-06],
        [-1.8924e-05, -1.3202e-05,  2.4587e-06,  ..., -1.4991e-05,
         -1.7136e-06, -9.3132e-06],
        [-3.4511e-05, -2.4080e-05,  4.4405e-06,  ..., -2.7329e-05,
         -3.1404e-06, -1.6987e-05]], device='cuda:0')
Loss: 1.2100294828414917


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,    14,  4494,  ...,  4830,   368,   266],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9040e-04,  5.2880e-05, -1.1989e-04,  ..., -2.3281e-05,
          1.8494e-04, -7.1973e-05],
        [-2.0295e-05, -1.4171e-05,  2.6040e-06,  ..., -1.6049e-05,
         -1.8906e-06, -9.9689e-06],
        [-1.4126e-05, -9.8795e-06,  1.8161e-06,  ..., -1.1191e-05,
         -1.3178e-06, -6.9439e-06],
        [-2.3693e-05, -1.6510e-05,  3.0510e-06,  ..., -1.8701e-05,
         -2.1905e-06, -1.1608e-05],
        [-4.3273e-05, -3.0190e-05,  5.5209e-06,  ..., -3.4183e-05,
         -4.0308e-06, -2.1219e-05]], device='cuda:0')
Loss: 1.203911542892456


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   593,   300,  ...,   278,  4694,    12],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6190e-04, -1.7452e-05, -1.1058e-04,  ..., -2.2079e-05,
          1.6212e-04, -6.7351e-05],
        [-2.4319e-05, -1.6958e-05,  3.1590e-06,  ..., -1.9237e-05,
         -2.2594e-06, -1.1966e-05],
        [-1.6928e-05, -1.1802e-05,  2.1998e-06,  ..., -1.3396e-05,
         -1.5749e-06, -8.3297e-06],
        [-2.8461e-05, -1.9789e-05,  3.7067e-06,  ..., -2.2471e-05,
         -2.6263e-06, -1.3977e-05],
        [-5.1856e-05, -3.6091e-05,  6.6906e-06,  ..., -4.0948e-05,
         -4.8131e-06, -2.5451e-05]], device='cuda:0')
Loss: 1.1919013261795044


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,    15, 35654,     9],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2157e-04, -6.3916e-05, -1.1065e-04,  ..., -8.3121e-05,
          2.2394e-04, -5.1920e-05],
        [-2.8372e-05, -1.9804e-05,  3.6955e-06,  ..., -2.2411e-05,
         -2.6710e-06, -1.3977e-05],
        [-1.9729e-05, -1.3769e-05,  2.5686e-06,  ..., -1.5572e-05,
         -1.8580e-06, -9.7081e-06],
        [-3.3200e-05, -2.3112e-05,  4.3362e-06,  ..., -2.6166e-05,
         -3.1032e-06, -1.6317e-05],
        [-6.0439e-05, -4.2111e-05,  7.8157e-06,  ..., -4.7654e-05,
         -5.6811e-06, -2.9683e-05]], device='cuda:0')
Loss: 1.1838569641113281


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1529,  354,  ...,  709,  271,  266],
        [   0,  409,  699,  ...,   12,  546,  696]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0533e-04, -7.6680e-05, -1.1387e-04,  ..., -1.4034e-04,
          2.3271e-04, -4.2477e-05],
        [-3.2455e-05, -2.2575e-05,  4.1947e-06,  ..., -2.5585e-05,
         -2.9802e-06, -1.5989e-05],
        [-2.2575e-05, -1.5706e-05,  2.9150e-06,  ..., -1.7777e-05,
         -2.0731e-06, -1.1101e-05],
        [-3.7938e-05, -2.6345e-05,  4.9174e-06,  ..., -2.9862e-05,
         -3.4627e-06, -1.8641e-05],
        [-6.9201e-05, -4.8041e-05,  8.8736e-06,  ..., -5.4449e-05,
         -6.3442e-06, -3.3975e-05]], device='cuda:0')
Loss: 1.2060719728469849
Graident accumulation at epoch 0, step 167, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9716e-04, -1.2119e-04,  5.0283e-05,  ...,  4.5864e-05,
         -2.5928e-04, -8.1585e-05],
        [-2.8457e-05, -2.0953e-05,  5.3439e-06,  ..., -2.3637e-05,
         -2.2401e-06, -1.0586e-05],
        [ 1.2348e-05,  1.1990e-05, -5.2907e-06,  ...,  1.7181e-05,
          7.0022e-07,  1.0287e-05],
        [-1.9000e-06,  6.1276e-06,  5.3594e-06,  ...,  3.6791e-06,
          2.5734e-06,  6.4340e-06],
        [-5.2774e-05, -3.8433e-05,  8.3617e-06,  ..., -4.2790e-05,
         -3.8871e-06, -1.9376e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0302e-08, 2.0210e-08, 3.0500e-08,  ..., 2.2597e-08, 5.3475e-08,
         1.1391e-08],
        [1.9682e-11, 1.4223e-11, 8.5280e-13,  ..., 1.3154e-11, 3.0003e-13,
         2.2510e-12],
        [8.6144e-11, 7.8413e-11, 9.6436e-12,  ..., 1.3429e-10, 3.5139e-12,
         3.3776e-11],
        [1.2429e-10, 2.0579e-10, 4.3715e-12,  ..., 1.0299e-10, 1.6680e-11,
         7.0547e-11],
        [6.5841e-11, 4.2578e-11, 5.1852e-12,  ..., 4.6412e-11, 6.4560e-13,
         8.8205e-12]], device='cuda:0')
optimizer state dict: 21.0
lr: [1.9868368648050586e-05, 1.9868368648050586e-05]
scheduler_last_epoch: 21


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,  3860,   694,  ...,  1027,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2047e-07,  7.2153e-06,  2.0217e-06,  ...,  2.9109e-05,
          4.3769e-05, -4.5807e-05],
        [-3.9041e-06, -2.7716e-06,  4.2468e-07,  ..., -3.0845e-06,
         -4.6007e-07, -1.8477e-06],
        [-2.8163e-06, -1.9968e-06,  3.0547e-07,  ..., -2.2203e-06,
         -3.2969e-07, -1.3337e-06],
        [-4.6790e-06, -3.3230e-06,  5.1409e-07,  ..., -3.6806e-06,
         -5.4762e-07, -2.2054e-06],
        [-8.5831e-06, -6.1095e-06,  9.3132e-07,  ..., -6.7651e-06,
         -1.0133e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.189774990081787


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 6132,  300,  ...,   37,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1525e-05,  2.4310e-05, -2.7681e-06,  ...,  2.5439e-05,
          3.6639e-05, -5.1090e-05],
        [-7.8678e-06, -5.6028e-06,  8.2329e-07,  ..., -6.1691e-06,
         -9.2946e-07, -3.7104e-06],
        [-5.7071e-06, -4.0680e-06,  5.9605e-07,  ..., -4.4703e-06,
         -6.7241e-07, -2.6897e-06],
        [-9.3877e-06, -6.6757e-06,  9.9093e-07,  ..., -7.3314e-06,
         -1.0990e-06, -4.3958e-06],
        [-1.7285e-05, -1.2308e-05,  1.7956e-06,  ..., -1.3530e-05,
         -2.0415e-06, -8.1360e-06]], device='cuda:0')
Loss: 1.190589427947998


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0,    26,   874,  ...,    12, 21591,    12],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9808e-05,  1.1526e-06,  4.7930e-06,  ...,  1.1980e-05,
          5.1801e-05, -2.0446e-05],
        [-1.1832e-05, -8.3894e-06,  1.2144e-06,  ..., -9.2238e-06,
         -1.3597e-06, -5.5656e-06],
        [-8.5533e-06, -6.0797e-06,  8.7731e-07,  ..., -6.6757e-06,
         -9.8161e-07, -4.0233e-06],
        [-1.4096e-05, -9.9987e-06,  1.4585e-06,  ..., -1.0967e-05,
         -1.6093e-06, -6.6012e-06],
        [-2.5928e-05, -1.8418e-05,  2.6487e-06,  ..., -2.0236e-05,
         -2.9877e-06, -1.2189e-05]], device='cuda:0')
Loss: 1.2101457118988037


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0828e-05,  2.0218e-05, -1.6214e-05,  ...,  1.1980e-05,
          5.6485e-05, -6.9592e-05],
        [-1.5736e-05, -1.1221e-05,  1.6075e-06,  ..., -1.2264e-05,
         -1.8012e-06, -7.3761e-06],
        [-1.1370e-05, -8.1360e-06,  1.1623e-06,  ..., -8.8811e-06,
         -1.3020e-06, -5.3346e-06],
        [-1.8775e-05, -1.3396e-05,  1.9316e-06,  ..., -1.4618e-05,
         -2.1383e-06, -8.7768e-06],
        [-3.4511e-05, -2.4647e-05,  3.5092e-06,  ..., -2.6941e-05,
         -3.9637e-06, -1.6183e-05]], device='cuda:0')
Loss: 1.1819267272949219


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[   0,  287, 1070,  ...,  292,  221,  374],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9099e-05,  3.7274e-05, -3.5981e-05,  ...,  2.2863e-05,
          3.0499e-05, -5.6310e-05],
        [-1.9670e-05, -1.3977e-05,  2.0228e-06,  ..., -1.5333e-05,
         -2.2575e-06, -9.2313e-06],
        [-1.4171e-05, -1.0103e-05,  1.4603e-06,  ..., -1.1072e-05,
         -1.6280e-06, -6.6608e-06],
        [-2.3454e-05, -1.6659e-05,  2.4308e-06,  ..., -1.8254e-05,
         -2.6785e-06, -1.0967e-05],
        [-4.3094e-05, -3.0637e-05,  4.4070e-06,  ..., -3.3617e-05,
         -4.9546e-06, -2.0206e-05]], device='cuda:0')
Loss: 1.1927295923233032


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0, 4100,   12,  ...,   13, 4710, 1558],
        [   0,  278,  266,  ...,   13, 2853,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7061e-05,  2.3770e-05, -7.2317e-05,  ...,  2.0590e-05,
          6.1360e-05, -1.0559e-04],
        [-2.3574e-05, -1.6749e-05,  2.4512e-06,  ..., -1.8403e-05,
         -2.6692e-06, -1.1086e-05],
        [-1.6972e-05, -1.2100e-05,  1.7677e-06,  ..., -1.3277e-05,
         -1.9241e-06, -7.9945e-06],
        [-2.8163e-05, -1.9997e-05,  2.9486e-06,  ..., -2.1935e-05,
         -3.1702e-06, -1.3202e-05],
        [-5.1618e-05, -3.6687e-05,  5.3309e-06,  ..., -4.0293e-05,
         -5.8524e-06, -2.4229e-05]], device='cuda:0')
Loss: 1.2055459022521973


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   344,  8133,  ...,   368,  1119,  5539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9090e-05,  4.1444e-05, -8.2433e-05,  ...,  3.1183e-05,
          8.8240e-05, -1.0160e-04],
        [-2.7478e-05, -1.9565e-05,  2.8238e-06,  ..., -2.1458e-05,
         -3.0976e-06, -1.2919e-05],
        [-1.9819e-05, -1.4141e-05,  2.0377e-06,  ..., -1.5497e-05,
         -2.2352e-06, -9.3281e-06],
        [-3.2812e-05, -2.3350e-05,  3.3937e-06,  ..., -2.5585e-05,
         -3.6769e-06, -1.5393e-05],
        [-6.0320e-05, -4.2915e-05,  6.1467e-06,  ..., -4.7088e-05,
         -6.8024e-06, -2.8282e-05]], device='cuda:0')
Loss: 1.1995817422866821


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0,   342,  4781,  ...,   630,   940,   271],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8238e-05,  8.0413e-05, -1.2167e-04,  ...,  7.7058e-05,
          1.2874e-04, -1.1482e-04],
        [-3.1471e-05, -2.2367e-05,  3.2298e-06,  ..., -2.4542e-05,
         -3.5278e-06, -1.4752e-05],
        [-2.2680e-05, -1.6153e-05,  2.3283e-06,  ..., -1.7703e-05,
         -2.5425e-06, -1.0647e-05],
        [-3.7551e-05, -2.6673e-05,  3.8743e-06,  ..., -2.9236e-05,
         -4.1835e-06, -1.7568e-05],
        [-6.9082e-05, -4.9055e-05,  7.0259e-06,  ..., -5.3823e-05,
         -7.7449e-06, -3.2306e-05]], device='cuda:0')
Loss: 1.1957652568817139
Graident accumulation at epoch 0, step 175, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6527e-04, -1.0103e-04,  3.3088e-05,  ...,  4.8983e-05,
         -2.2047e-04, -8.4908e-05],
        [-2.8759e-05, -2.1094e-05,  5.1325e-06,  ..., -2.3728e-05,
         -2.3689e-06, -1.1003e-05],
        [ 8.8449e-06,  9.1757e-06, -4.5288e-06,  ...,  1.3693e-05,
          3.7594e-07,  8.1932e-06],
        [-5.4651e-06,  2.8476e-06,  5.2109e-06,  ...,  3.8758e-07,
          1.8977e-06,  4.0337e-06],
        [-5.4405e-05, -3.9495e-05,  8.2281e-06,  ..., -4.3893e-05,
         -4.2729e-06, -2.0669e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0248e-08, 2.0196e-08, 3.0484e-08,  ..., 2.2581e-08, 5.3438e-08,
         1.1393e-08],
        [2.0653e-11, 1.4709e-11, 8.6238e-13,  ..., 1.3744e-11, 3.1218e-13,
         2.4664e-12],
        [8.6572e-11, 7.8596e-11, 9.6394e-12,  ..., 1.3447e-10, 3.5169e-12,
         3.3855e-11],
        [1.2558e-10, 2.0630e-10, 4.3821e-12,  ..., 1.0374e-10, 1.6681e-11,
         7.0785e-11],
        [7.0547e-11, 4.4941e-11, 5.2294e-12,  ..., 4.9262e-11, 7.0494e-13,
         9.8554e-12]], device='cuda:0')
optimizer state dict: 22.0
lr: [1.9847624027890693e-05, 1.9847624027890693e-05]
scheduler_last_epoch: 22


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[   0,  275, 2351,  ...,   14, 4520,   12],
        [   0,  292,  685,  ...,  278, 3281,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9601e-06,  6.3604e-05, -1.4123e-05,  ...,  3.9754e-05,
          6.1477e-06,  6.8482e-06],
        [-3.8743e-06, -2.7716e-06,  3.6508e-07,  ..., -2.9951e-06,
         -5.2527e-07, -1.7285e-06],
        [-2.8759e-06, -2.0564e-06,  2.7195e-07,  ..., -2.2054e-06,
         -3.8743e-07, -1.2740e-06],
        [-4.7684e-06, -3.3975e-06,  4.5262e-07,  ..., -3.6657e-06,
         -6.4075e-07, -2.1160e-06],
        [-8.7619e-06, -6.2585e-06,  8.1956e-07,  ..., -6.7055e-06,
         -1.1772e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.1884300708770752


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[   0,  638, 1276,  ..., 1589, 2432,  292],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1637e-05,  6.3942e-05,  5.7246e-06,  ...,  6.4844e-05,
          2.1248e-06,  2.4297e-05],
        [-7.6890e-06, -5.5581e-06,  7.1898e-07,  ..., -5.9903e-06,
         -1.0096e-06, -3.4720e-06],
        [-5.6922e-06, -4.1127e-06,  5.3458e-07,  ..., -4.4107e-06,
         -7.4506e-07, -2.5630e-06],
        [-9.4175e-06, -6.7651e-06,  8.8662e-07,  ..., -7.2867e-06,
         -1.2256e-06, -4.2319e-06],
        [-1.7285e-05, -1.2457e-05,  1.6093e-06,  ..., -1.3381e-05,
         -2.2575e-06, -7.7486e-06]], device='cuda:0')
Loss: 1.1992326974868774


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,   598,   696,  ...,  4048,  1795,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1926e-06,  2.1350e-05,  1.0777e-05,  ...,  1.0949e-04,
          9.7425e-06,  2.3524e-05],
        [-1.1533e-05, -8.3148e-06,  1.1157e-06,  ..., -8.9258e-06,
         -1.5534e-06, -5.2378e-06],
        [-8.5384e-06, -6.1542e-06,  8.2888e-07,  ..., -6.5863e-06,
         -1.1493e-06, -3.8743e-06],
        [-1.4096e-05, -1.0133e-05,  1.3746e-06,  ..., -1.0878e-05,
         -1.8924e-06, -6.3926e-06],
        [-2.5868e-05, -1.8597e-05,  2.4922e-06,  ..., -1.9908e-05,
         -3.4794e-06, -1.1683e-05]], device='cuda:0')
Loss: 1.18036949634552


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6282e-06,  1.3959e-05,  4.3308e-05,  ...,  1.1328e-04,
          7.4951e-06,  2.3524e-05],
        [-1.5438e-05, -1.1116e-05,  1.4976e-06,  ..., -1.1921e-05,
         -2.1160e-06, -6.9737e-06],
        [-1.1399e-05, -8.2105e-06,  1.1101e-06,  ..., -8.7917e-06,
         -1.5628e-06, -5.1558e-06],
        [-1.8775e-05, -1.3500e-05,  1.8384e-06,  ..., -1.4469e-05,
         -2.5667e-06, -8.4788e-06],
        [-3.4630e-05, -2.4885e-05,  3.3453e-06,  ..., -2.6613e-05,
         -4.7460e-06, -1.5587e-05]], device='cuda:0')
Loss: 1.1741465330123901


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  607,  443,  ...,  259, 2646, 1597]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8628e-05,  4.6046e-05,  4.6800e-05,  ...,  1.3859e-04,
         -1.4112e-05,  6.1100e-05],
        [-1.9237e-05, -1.3873e-05,  1.8757e-06,  ..., -1.4901e-05,
         -2.6263e-06, -8.6948e-06],
        [-1.4216e-05, -1.0267e-05,  1.3914e-06,  ..., -1.1012e-05,
         -1.9409e-06, -6.4373e-06],
        [-2.3395e-05, -1.6853e-05,  2.3004e-06,  ..., -1.8090e-05,
         -3.1814e-06, -1.0565e-05],
        [-4.3154e-05, -3.1084e-05,  4.1910e-06,  ..., -3.3289e-05,
         -5.8934e-06, -1.9461e-05]], device='cuda:0')
Loss: 1.2066510915756226


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 46357, 11101, 10621],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0154e-04,  2.9216e-05,  9.2955e-05,  ...,  1.6733e-04,
         -5.9356e-05,  4.0609e-05],
        [-2.3007e-05, -1.6585e-05,  2.2277e-06,  ..., -1.7822e-05,
         -3.1404e-06, -1.0408e-05],
        [-1.7017e-05, -1.2279e-05,  1.6540e-06,  ..., -1.3188e-05,
         -2.3209e-06, -7.7114e-06],
        [-2.7984e-05, -2.0146e-05,  2.7344e-06,  ..., -2.1651e-05,
         -3.8035e-06, -1.2651e-05],
        [-5.1737e-05, -3.7253e-05,  4.9919e-06,  ..., -3.9935e-05,
         -7.0632e-06, -2.3365e-05]], device='cuda:0')
Loss: 1.1835803985595703


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1624e-04,  1.8909e-05,  7.4668e-05,  ...,  1.7308e-04,
         -6.2990e-05,  8.0675e-05],
        [-2.6852e-05, -1.9312e-05,  2.6040e-06,  ..., -2.0802e-05,
         -3.6135e-06, -1.2167e-05],
        [-1.9848e-05, -1.4290e-05,  1.9316e-06,  ..., -1.5378e-05,
         -2.6692e-06, -9.0078e-06],
        [-3.2663e-05, -2.3469e-05,  3.1963e-06,  ..., -2.5272e-05,
         -4.3772e-06, -1.4782e-05],
        [-6.0380e-05, -4.3392e-05,  5.8375e-06,  ..., -4.6641e-05,
         -8.1286e-06, -2.7299e-05]], device='cuda:0')
Loss: 1.1981918811798096


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4548e-04, -4.0420e-05,  1.0406e-04,  ...,  1.9386e-04,
         -1.1073e-04,  9.7632e-05],
        [-3.0667e-05, -2.2069e-05,  2.9635e-06,  ..., -2.3767e-05,
         -4.1388e-06, -1.3910e-05],
        [-2.2665e-05, -1.6332e-05,  2.1979e-06,  ..., -1.7568e-05,
         -3.0566e-06, -1.0297e-05],
        [-3.7313e-05, -2.6822e-05,  3.6359e-06,  ..., -2.8878e-05,
         -5.0142e-06, -1.6913e-05],
        [-6.8963e-05, -4.9591e-05,  6.6422e-06,  ..., -5.3316e-05,
         -9.3132e-06, -3.1233e-05]], device='cuda:0')
Loss: 1.1957186460494995
Graident accumulation at epoch 0, step 183, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1419e-04, -9.4967e-05,  4.0185e-05,  ...,  6.3471e-05,
         -2.0950e-04, -6.6654e-05],
        [-2.8950e-05, -2.1192e-05,  4.9156e-06,  ..., -2.3732e-05,
         -2.5459e-06, -1.1294e-05],
        [ 5.6940e-06,  6.6249e-06, -3.8561e-06,  ...,  1.0567e-05,
          3.2689e-08,  6.3442e-06],
        [-8.6499e-06, -1.1941e-07,  5.0534e-06,  ..., -2.5390e-06,
          1.2065e-06,  1.9391e-06],
        [-5.5860e-05, -4.0505e-05,  8.0695e-06,  ..., -4.4836e-05,
         -4.7769e-06, -2.1726e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0209e-08, 2.0178e-08, 3.0465e-08,  ..., 2.2596e-08, 5.3397e-08,
         1.1391e-08],
        [2.1573e-11, 1.5182e-11, 8.7030e-13,  ..., 1.4295e-11, 3.2900e-13,
         2.6574e-12],
        [8.6999e-11, 7.8784e-11, 9.6346e-12,  ..., 1.3464e-10, 3.5227e-12,
         3.3927e-11],
        [1.2685e-10, 2.0681e-10, 4.3910e-12,  ..., 1.0447e-10, 1.6689e-11,
         7.1000e-11],
        [7.5233e-11, 4.7356e-11, 5.2683e-12,  ..., 5.2056e-11, 7.9097e-13,
         1.0821e-11]], device='cuda:0')
optimizer state dict: 23.0
lr: [1.982537460481821e-05, 1.982537460481821e-05]
scheduler_last_epoch: 23


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4263e-06,  1.7479e-05, -4.5693e-05,  ...,  7.3315e-06,
          1.5207e-05,  6.8208e-05],
        [-3.7551e-06, -2.7269e-06,  3.1665e-07,  ..., -2.9206e-06,
         -4.8429e-07, -1.6317e-06],
        [-2.8610e-06, -2.0862e-06,  2.4028e-07,  ..., -2.2203e-06,
         -3.6880e-07, -1.2442e-06],
        [-4.6790e-06, -3.4124e-06,  3.9488e-07,  ..., -3.6508e-06,
         -6.0350e-07, -2.0266e-06],
        [-8.5235e-06, -6.1691e-06,  7.0781e-07,  ..., -6.6161e-06,
         -1.0952e-06, -3.6806e-06]], device='cuda:0')
Loss: 1.1991146802902222


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  285,  590,  ...,  199,  395, 3523],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8067e-05,  4.3257e-05, -5.8229e-05,  ...,  2.5945e-05,
          7.3107e-05,  9.3112e-05],
        [-7.4059e-06, -5.3942e-06,  6.3144e-07,  ..., -5.8413e-06,
         -1.0543e-06, -3.2857e-06],
        [-5.6773e-06, -4.1574e-06,  4.8243e-07,  ..., -4.4703e-06,
         -8.1025e-07, -2.5183e-06],
        [-9.2387e-06, -6.7353e-06,  7.8976e-07,  ..., -7.2867e-06,
         -1.3113e-06, -4.0829e-06],
        [-1.6868e-05, -1.2279e-05,  1.4193e-06,  ..., -1.3292e-05,
         -2.4065e-06, -7.4506e-06]], device='cuda:0')
Loss: 1.1718671321868896


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0,    12, 30621,  ...,   578,  3126,    14],
        [    0,    13,  5005,  ...,   654,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3624e-05,  4.7780e-05, -4.1524e-05,  ...,  3.2493e-06,
          5.2570e-05,  7.1671e-05],
        [-1.1072e-05, -8.0764e-06,  9.8906e-07,  ..., -8.6576e-06,
         -1.5832e-06, -4.9397e-06],
        [ 8.0630e-05,  3.9773e-05, -2.1807e-05,  ...,  7.4195e-05,
         -4.2195e-07,  3.2062e-05],
        [-1.3888e-05, -1.0148e-05,  1.2498e-06,  ..., -1.0863e-05,
         -1.9819e-06, -6.1840e-06],
        [-2.5332e-05, -1.8477e-05,  2.2426e-06,  ..., -1.9819e-05,
         -3.6359e-06, -1.1265e-05]], device='cuda:0')
Loss: 1.189130425453186


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[    0,  6762,   689,  ...,  7061,    14,   381],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6302e-05,  4.7483e-05, -1.8393e-05,  ...,  1.5670e-05,
          2.5427e-05,  7.1902e-05],
        [-1.4737e-05, -1.0774e-05,  1.3094e-06,  ..., -1.1578e-05,
         -2.1122e-06, -6.6012e-06],
        [ 7.7829e-05,  3.7702e-05, -2.1563e-05,  ...,  7.1960e-05,
         -8.2801e-07,  3.0788e-05],
        [-1.8477e-05, -1.3530e-05,  1.6540e-06,  ..., -1.4514e-05,
         -2.6450e-06, -8.2701e-06],
        [-3.3677e-05, -2.4647e-05,  2.9653e-06,  ..., -2.6494e-05,
         -4.8429e-06, -1.5050e-05]], device='cuda:0')
Loss: 1.1835694313049316


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0, 13642, 14635,  ...,   367,  1040,  8580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0149e-05,  1.9285e-05,  2.1918e-05,  ...,  1.8286e-05,
         -6.7338e-06,  7.1144e-05],
        [-1.8463e-05, -1.3515e-05,  1.6373e-06,  ..., -1.4469e-05,
         -2.6077e-06, -8.2403e-06],
        [ 7.4968e-05,  3.5601e-05, -2.1312e-05,  ...,  6.9740e-05,
         -1.2080e-06,  2.9529e-05],
        [-2.3097e-05, -1.6943e-05,  2.0638e-06,  ..., -1.8105e-05,
         -3.2596e-06, -1.0312e-05],
        [-4.2260e-05, -3.0965e-05,  3.7178e-06,  ..., -3.3170e-05,
         -5.9903e-06, -1.8835e-05]], device='cuda:0')
Loss: 1.1875944137573242


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3128e-06,  3.3374e-05,  1.4652e-05,  ...,  2.5859e-05,
          2.5603e-05,  8.0691e-05],
        [-2.2143e-05, -1.6212e-05,  2.0061e-06,  ..., -1.7360e-05,
         -3.1851e-06, -9.9167e-06],
        [ 7.2107e-05,  3.3514e-05, -2.1023e-05,  ...,  6.7505e-05,
         -1.6569e-06,  2.8233e-05],
        [-2.7716e-05, -2.0310e-05,  2.5313e-06,  ..., -2.1726e-05,
         -3.9861e-06, -1.2413e-05],
        [-5.0783e-05, -3.7193e-05,  4.5672e-06,  ..., -3.9846e-05,
         -7.3314e-06, -2.2709e-05]], device='cuda:0')
Loss: 1.1951848268508911


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[    0, 21178,  1952,  ..., 14930,     9,   689],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3851e-05,  1.9175e-05,  1.6588e-05,  ...,  1.0155e-05,
         -3.0291e-05,  9.5898e-05],
        [-2.5794e-05, -1.8910e-05,  2.3562e-06,  ..., -2.0251e-05,
         -3.6806e-06, -1.1563e-05],
        [ 6.9261e-05,  3.1428e-05, -2.0749e-05,  ...,  6.5255e-05,
         -2.0425e-06,  2.6959e-05],
        [-3.2276e-05, -2.3678e-05,  2.9709e-06,  ..., -2.5332e-05,
         -4.6045e-06, -1.4469e-05],
        [-5.9187e-05, -4.3362e-05,  5.3681e-06,  ..., -4.6462e-05,
         -8.4713e-06, -2.6479e-05]], device='cuda:0')
Loss: 1.1871591806411743


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,     9,   391,  ...,   300,  2646,  1717],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6134e-05,  1.3717e-05, -1.8341e-05,  ...,  1.8898e-05,
         -2.8519e-05,  6.6306e-05],
        [-2.9519e-05, -2.1607e-05,  2.6841e-06,  ..., -2.3171e-05,
         -4.2133e-06, -1.3202e-05],
        [ 6.6385e-05,  2.9357e-05, -2.0496e-05,  ...,  6.3004e-05,
         -2.4541e-06,  2.5700e-05],
        [-3.6925e-05, -2.7031e-05,  3.3844e-06,  ..., -2.8968e-05,
         -5.2676e-06, -1.6510e-05],
        [-6.7711e-05, -4.9531e-05,  6.1132e-06,  ..., -5.3138e-05,
         -9.6932e-06, -3.0220e-05]], device='cuda:0')
Loss: 1.192455530166626
Graident accumulation at epoch 0, step 191, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0205],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0168,  0.0143, -0.0269,  ...,  0.0279, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8016e-04, -8.4098e-05,  3.4332e-05,  ...,  5.9014e-05,
         -1.9140e-04, -5.3358e-05],
        [-2.9006e-05, -2.1233e-05,  4.6924e-06,  ..., -2.3676e-05,
         -2.7126e-06, -1.1485e-05],
        [ 1.1763e-05,  8.8982e-06, -5.5201e-06,  ...,  1.5810e-05,
         -2.1599e-07,  8.2798e-06],
        [-1.1477e-05, -2.8105e-06,  4.8865e-06,  ..., -5.1819e-06,
          5.5913e-07,  9.4134e-08],
        [-5.7045e-05, -4.1408e-05,  7.8739e-06,  ..., -4.5666e-05,
         -5.2685e-06, -2.2575e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0150e-08, 2.0158e-08, 3.0435e-08,  ..., 2.2573e-08, 5.3345e-08,
         1.1384e-08],
        [2.2423e-11, 1.5633e-11, 8.7663e-13,  ..., 1.4817e-11, 3.4642e-13,
         2.8290e-12],
        [9.1319e-11, 7.9567e-11, 1.0045e-11,  ..., 1.3848e-10, 3.5252e-12,
         3.4554e-11],
        [1.2808e-10, 2.0733e-10, 4.3980e-12,  ..., 1.0520e-10, 1.6700e-11,
         7.1202e-11],
        [7.9742e-11, 4.9762e-11, 5.3004e-12,  ..., 5.4827e-11, 8.8414e-13,
         1.1723e-11]], device='cuda:0')
optimizer state dict: 24.0
lr: [1.9801623778739208e-05, 1.9801623778739208e-05]
scheduler_last_epoch: 24


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  6541,   287,  ...,  1061,  4786,   292],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7582e-05, -2.4179e-05, -1.2886e-05,  ..., -2.2282e-05,
         -5.3163e-06, -1.0306e-05],
        [-3.5465e-06, -2.6375e-06,  3.1851e-07,  ..., -2.8163e-06,
         -5.3272e-07, -1.5572e-06],
        [-2.8610e-06, -2.1309e-06,  2.5705e-07,  ..., -2.2799e-06,
         -4.3027e-07, -1.2517e-06],
        [-4.5896e-06, -3.4124e-06,  4.1351e-07,  ..., -3.6359e-06,
         -6.8545e-07, -1.9968e-06],
        [-8.2254e-06, -6.1393e-06,  7.3388e-07,  ..., -6.5565e-06,
         -1.2368e-06, -3.5912e-06]], device='cuda:0')
Loss: 1.1765599250793457


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[    0,   199,   769,  ..., 12038, 15317,   342],
        [    0,  1832,   292,  ...,  2176,  1345,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8070e-05, -3.1622e-05, -3.0125e-05,  ..., -8.6590e-05,
          1.8252e-05, -3.4174e-05],
        [-7.1228e-06, -5.2452e-06,  5.8487e-07,  ..., -5.7220e-06,
         -1.0002e-06, -3.1069e-06],
        [-5.7071e-06, -4.2021e-06,  4.6939e-07,  ..., -4.5896e-06,
         -8.0280e-07, -2.4885e-06],
        [-9.2089e-06, -6.7651e-06,  7.5996e-07,  ..., -7.3761e-06,
         -1.2890e-06, -3.9935e-06],
        [-1.6510e-05, -1.2189e-05,  1.3448e-06,  ..., -1.3292e-05,
         -2.3246e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.2034821510314941


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[    0, 14949,    12,  ...,   669, 10168,  7166],
        [    0,   401,   266,  ...,   266,  2236,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2689e-05, -9.8574e-05, -3.8692e-05,  ..., -4.1424e-05,
          2.1583e-05,  1.5650e-05],
        [-1.0699e-05, -7.9125e-06,  8.8476e-07,  ..., -8.5384e-06,
         -1.5181e-06, -4.6119e-06],
        [-8.5980e-06, -6.3628e-06,  7.0967e-07,  ..., -6.8694e-06,
         -1.2219e-06, -3.7104e-06],
        [-1.3798e-05, -1.0177e-05,  1.1455e-06,  ..., -1.0982e-05,
         -1.9521e-06, -5.9307e-06],
        [-2.4855e-05, -1.8418e-05,  2.0415e-06,  ..., -1.9878e-05,
         -3.5390e-06, -1.0699e-05]], device='cuda:0')
Loss: 1.1886848211288452


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,   546,   360,  ...,  9107,  2772,  4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2924e-05, -1.2368e-04, -1.7376e-05,  ..., -4.8003e-05,
          4.7987e-05,  3.9477e-05],
        [-1.4290e-05, -1.0550e-05,  1.1772e-06,  ..., -1.1444e-05,
         -2.0172e-06, -6.1840e-06],
        [-1.1474e-05, -8.4788e-06,  9.4436e-07,  ..., -9.1940e-06,
         -1.6224e-06, -4.9770e-06],
        [-1.8388e-05, -1.3545e-05,  1.5218e-06,  ..., -1.4678e-05,
         -2.5891e-06, -7.9423e-06],
        [-3.3200e-05, -2.4557e-05,  2.7157e-06,  ..., -2.6613e-05,
         -4.7013e-06, -1.4350e-05]], device='cuda:0')
Loss: 1.1893318891525269


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  7094,   596,  ...,  4764,  9514,    14],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0524e-05, -1.4750e-04,  1.2689e-05,  ...,  1.0313e-05,
          3.0592e-05,  4.4382e-05],
        [-1.7852e-05, -1.3188e-05,  1.4920e-06,  ..., -1.4260e-05,
         -2.5164e-06, -7.7561e-06],
        [-1.4365e-05, -1.0625e-05,  1.1995e-06,  ..., -1.1474e-05,
         -2.0284e-06, -6.2510e-06],
        [-2.3007e-05, -1.6972e-05,  1.9316e-06,  ..., -1.8328e-05,
         -3.2373e-06, -9.9838e-06],
        [-4.1544e-05, -3.0726e-05,  3.4459e-06,  ..., -3.3200e-05,
         -5.8711e-06, -1.8030e-05]], device='cuda:0')
Loss: 1.1995103359222412


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,  360, 2374,  ...,  221,  474,  357],
        [   0, 4882,   12,  ...,   12, 9575,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8491e-06, -1.5929e-04,  5.8362e-05,  ...,  3.9103e-05,
          1.7481e-07,  2.0755e-05],
        [-2.1428e-05, -1.5765e-05,  1.8086e-06,  ..., -1.7107e-05,
         -2.9970e-06, -9.3058e-06],
        [-1.7241e-05, -1.2696e-05,  1.4547e-06,  ..., -1.3769e-05,
         -2.4140e-06, -7.5027e-06],
        [-2.7627e-05, -2.0310e-05,  2.3432e-06,  ..., -2.2009e-05,
         -3.8557e-06, -1.1995e-05],
        [-4.9889e-05, -3.6746e-05,  4.1798e-06,  ..., -3.9846e-05,
         -6.9961e-06, -2.1651e-05]], device='cuda:0')
Loss: 1.204007625579834


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  2261,     9,  ..., 15008,    14,   333],
        [    0,  2973,    30,  ...,   408,   259,  1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4469e-06, -2.3913e-04,  3.0951e-05,  ...,  6.2422e-05,
         -1.4930e-05,  1.6308e-05],
        [-2.4945e-05, -1.8373e-05,  2.0992e-06,  ..., -1.9923e-05,
         -3.4999e-06, -1.0833e-05],
        [-2.0087e-05, -1.4812e-05,  1.6904e-06,  ..., -1.6049e-05,
         -2.8219e-06, -8.7395e-06],
        [-3.2187e-05, -2.3693e-05,  2.7213e-06,  ..., -2.5645e-05,
         -4.5076e-06, -1.3962e-05],
        [-5.8115e-05, -4.2856e-05,  4.8541e-06,  ..., -4.6402e-05,
         -8.1733e-06, -2.5213e-05]], device='cuda:0')
Loss: 1.1853313446044922


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[   0, 5281, 4452,  ...,   14, 3391,   12],
        [   0, 2220, 1110,  ...,  382,   18,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3801e-05, -2.3463e-04,  7.1053e-05,  ...,  6.8886e-05,
         -1.8590e-05,  7.0344e-05],
        [-2.8461e-05, -2.0996e-05,  2.4103e-06,  ..., -2.2724e-05,
         -3.9916e-06, -1.2368e-05],
        [-2.2948e-05, -1.6943e-05,  1.9455e-06,  ..., -1.8343e-05,
         -3.2224e-06, -9.9912e-06],
        [-3.6746e-05, -2.7090e-05,  3.1311e-06,  ..., -2.9296e-05,
         -5.1484e-06, -1.5959e-05],
        [-6.6400e-05, -4.9025e-05,  5.5879e-06,  ..., -5.3018e-05,
         -9.3356e-06, -2.8834e-05]], device='cuda:0')
Loss: 1.1803280115127563
Graident accumulation at epoch 0, step 199, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0036,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0168,  0.0143, -0.0269,  ...,  0.0279, -0.0158, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4577e-04, -9.9151e-05,  3.8004e-05,  ...,  6.0001e-05,
         -1.7412e-04, -4.0988e-05],
        [-2.8952e-05, -2.1209e-05,  4.4642e-06,  ..., -2.3581e-05,
         -2.8405e-06, -1.1573e-05],
        [ 8.2920e-06,  6.3141e-06, -4.7735e-06,  ...,  1.2395e-05,
         -5.1663e-07,  6.4527e-06],
        [-1.4004e-05, -5.2385e-06,  4.7110e-06,  ..., -7.5933e-06,
         -1.1614e-08, -1.5112e-06],
        [-5.7981e-05, -4.2169e-05,  7.6453e-06,  ..., -4.6401e-05,
         -5.6752e-06, -2.3201e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0094e-08, 2.0193e-08, 3.0409e-08,  ..., 2.2555e-08, 5.3292e-08,
         1.1377e-08],
        [2.3210e-11, 1.6059e-11, 8.8156e-13,  ..., 1.5319e-11, 3.6201e-13,
         2.9792e-12],
        [9.1754e-11, 7.9774e-11, 1.0039e-11,  ..., 1.3868e-10, 3.5321e-12,
         3.4619e-11],
        [1.2931e-10, 2.0786e-10, 4.4034e-12,  ..., 1.0596e-10, 1.6710e-11,
         7.1386e-11],
        [8.4071e-11, 5.2115e-11, 5.3263e-12,  ..., 5.7583e-11, 9.7041e-13,
         1.2543e-11]], device='cuda:0')
optimizer state dict: 25.0
lr: [1.9776375178987234e-05, 1.9776375178987234e-05]
scheduler_last_epoch: 25


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8990e-05, -1.4258e-05, -2.4791e-05,  ..., -3.8020e-07,
         -9.3892e-06,  1.5971e-05],
        [-3.4571e-06, -2.5332e-06,  2.8312e-07,  ..., -2.7865e-06,
         -4.5076e-07, -1.4603e-06],
        [-2.9057e-06, -2.1309e-06,  2.3749e-07,  ..., -2.3395e-06,
         -3.7998e-07, -1.2293e-06],
        [-4.5598e-06, -3.3379e-06,  3.7625e-07,  ..., -3.6806e-06,
         -5.9605e-07, -1.9222e-06],
        [-8.1658e-06, -5.9605e-06,  6.6310e-07,  ..., -6.5565e-06,
         -1.0654e-06, -3.4273e-06]], device='cuda:0')
Loss: 1.1803641319274902


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,    43,   503,    67],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1687e-05, -3.4688e-05, -6.4191e-06,  ...,  2.1912e-05,
         -1.6840e-06,  3.5532e-06],
        [-6.8843e-06, -5.0664e-06,  5.5134e-07,  ..., -5.5730e-06,
         -8.8476e-07, -2.8983e-06],
        [-5.7966e-06, -4.2617e-06,  4.6473e-07,  ..., -4.6790e-06,
         -7.4506e-07, -2.4438e-06],
        [-9.1195e-06, -6.7055e-06,  7.3574e-07,  ..., -7.3761e-06,
         -1.1735e-06, -3.8296e-06],
        [-1.6272e-05, -1.1951e-05,  1.2964e-06,  ..., -1.3143e-05,
         -2.0936e-06, -6.8247e-06]], device='cuda:0')
Loss: 1.185127854347229


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  2377,   360,  ...,   266,  4745,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0497e-04, -4.1368e-05,  2.3495e-05,  ...,  5.0410e-05,
         -5.8724e-05, -3.5173e-05],
        [-1.0312e-05, -7.5847e-06,  8.4750e-07,  ..., -8.3148e-06,
         -1.3113e-06, -4.3362e-06],
        [-8.6576e-06, -6.3628e-06,  7.1246e-07,  ..., -6.9737e-06,
         -1.1027e-06, -3.6508e-06],
        [-1.3739e-05, -1.0088e-05,  1.1381e-06,  ..., -1.1072e-05,
         -1.7472e-06, -5.7667e-06],
        [-2.4438e-05, -1.7941e-05,  1.9968e-06,  ..., -1.9670e-05,
         -3.1143e-06, -1.0252e-05]], device='cuda:0')
Loss: 1.2021859884262085


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3440e-05, -3.2462e-05, -3.1412e-06,  ...,  7.1024e-05,
         -5.2629e-05, -3.5205e-05],
        [-1.3709e-05, -1.0118e-05,  1.1306e-06,  ..., -1.1086e-05,
         -1.7919e-06, -5.7667e-06],
        [-1.1533e-05, -8.4937e-06,  9.5274e-07,  ..., -9.3281e-06,
         -1.5087e-06, -4.8578e-06],
        [-1.8239e-05, -1.3441e-05,  1.5162e-06,  ..., -1.4752e-05,
         -2.3805e-06, -7.6592e-06],
        [-3.2485e-05, -2.3901e-05,  2.6636e-06,  ..., -2.6226e-05,
         -4.2543e-06, -1.3635e-05]], device='cuda:0')
Loss: 1.1892452239990234


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0295e-05, -1.2206e-05,  7.0541e-06,  ...,  3.9237e-05,
         -4.0853e-05, -1.9317e-05],
        [-1.7181e-05, -1.2651e-05,  1.3802e-06,  ..., -1.3873e-05,
         -2.2501e-06, -7.1973e-06],
        [-1.4439e-05, -1.0610e-05,  1.1614e-06,  ..., -1.1653e-05,
         -1.8924e-06, -6.0573e-06],
        [-2.2858e-05, -1.6809e-05,  1.8515e-06,  ..., -1.8448e-05,
         -2.9877e-06, -9.5591e-06],
        [-4.0710e-05, -2.9892e-05,  3.2522e-06,  ..., -3.2812e-05,
         -5.3421e-06, -1.7017e-05]], device='cuda:0')
Loss: 1.1831638813018799


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0,  300,  369,  ...,   12,  970,   12],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7993e-05,  2.2754e-05,  1.2022e-05,  ...,  1.2533e-05,
         -4.4401e-05, -1.7906e-06],
        [-2.0623e-05, -1.5125e-05,  1.6242e-06,  ..., -1.6659e-05,
         -2.7046e-06, -8.6352e-06],
        [-1.7315e-05, -1.2681e-05,  1.3644e-06,  ..., -1.3977e-05,
         -2.2724e-06, -7.2569e-06],
        [-2.7388e-05, -2.0072e-05,  2.1737e-06,  ..., -2.2128e-05,
         -3.5875e-06, -1.1452e-05],
        [-4.8757e-05, -3.5703e-05,  3.8184e-06,  ..., -3.9339e-05,
         -6.4075e-06, -2.0385e-05]], device='cuda:0')
Loss: 1.1878135204315186


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0,  271, 3421,  ...,  306,  472,  346],
        [   0,  221, 4070,  ..., 1061, 3189,   26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9017e-05,  4.0301e-05,  1.6116e-05,  ...,  2.0886e-05,
         -3.6534e-05, -1.2148e-05],
        [-2.4065e-05, -1.7658e-05,  1.9167e-06,  ..., -1.9416e-05,
         -3.1535e-06, -1.0081e-05],
        [-2.0206e-05, -1.4797e-05,  1.6084e-06,  ..., -1.6287e-05,
         -2.6487e-06, -8.4713e-06],
        [-3.1978e-05, -2.3425e-05,  2.5649e-06,  ..., -2.5794e-05,
         -4.1835e-06, -1.3374e-05],
        [-5.6863e-05, -4.1664e-05,  4.5039e-06,  ..., -4.5836e-05,
         -7.4655e-06, -2.3782e-05]], device='cuda:0')
Loss: 1.186242699623108


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3438e-04,  2.5015e-05,  1.0403e-05,  ...,  3.7462e-05,
         -6.7567e-05, -2.4572e-05],
        [-2.7448e-05, -2.0161e-05,  2.2240e-06,  ..., -2.2143e-05,
         -3.6638e-06, -1.1504e-05],
        [-2.3082e-05, -1.6928e-05,  1.8710e-06,  ..., -1.8612e-05,
         -3.0827e-06, -9.6858e-06],
        [-3.6478e-05, -2.6762e-05,  2.9802e-06,  ..., -2.9445e-05,
         -4.8615e-06, -1.5274e-05],
        [-6.4850e-05, -4.7565e-05,  5.2303e-06,  ..., -5.2273e-05,
         -8.6650e-06, -2.7135e-05]], device='cuda:0')
Loss: 1.1795977354049683
Graident accumulation at epoch 0, step 207, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0036,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0168,  0.0143, -0.0269,  ...,  0.0279, -0.0158, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0775e-04, -8.6735e-05,  3.5244e-05,  ...,  5.7747e-05,
         -1.6346e-04, -3.9346e-05],
        [-2.8802e-05, -2.1105e-05,  4.2402e-06,  ..., -2.3437e-05,
         -2.9229e-06, -1.1566e-05],
        [ 5.1546e-06,  3.9899e-06, -4.1091e-06,  ...,  9.2944e-06,
         -7.7323e-07,  4.8388e-06],
        [-1.6252e-05, -7.3909e-06,  4.5379e-06,  ..., -9.7784e-06,
         -4.9660e-07, -2.8874e-06],
        [-5.8668e-05, -4.2709e-05,  7.4038e-06,  ..., -4.6988e-05,
         -5.9742e-06, -2.3594e-05]], device='cuda:0')
optimizer state dict: tensor([[6.0052e-08, 2.0173e-08, 3.0379e-08,  ..., 2.2534e-08, 5.3243e-08,
         1.1367e-08],
        [2.3940e-11, 1.6449e-11, 8.8563e-13,  ..., 1.5794e-11, 3.7507e-13,
         3.1085e-12],
        [9.2195e-11, 7.9981e-11, 1.0032e-11,  ..., 1.3888e-10, 3.5380e-12,
         3.4678e-11],
        [1.3051e-10, 2.0837e-10, 4.4079e-12,  ..., 1.0672e-10, 1.6717e-11,
         7.1547e-11],
        [8.8193e-11, 5.4326e-11, 5.3483e-12,  ..., 6.0258e-11, 1.0445e-12,
         1.3267e-11]], device='cuda:0')
optimizer state dict: 26.0
lr: [1.974963266376872e-05, 1.974963266376872e-05]
scheduler_last_epoch: 26


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1951e-05,  9.5462e-06, -8.2174e-06,  ...,  3.5929e-05,
          7.1764e-06,  6.7668e-05],
        [-3.3677e-06, -2.4289e-06,  2.6822e-07,  ..., -2.6673e-06,
         -3.8370e-07, -1.3635e-06],
        [-2.9504e-06, -2.1309e-06,  2.3469e-07,  ..., -2.3395e-06,
         -3.3714e-07, -1.1995e-06],
        [-4.6492e-06, -3.3677e-06,  3.7253e-07,  ..., -3.6806e-06,
         -5.2899e-07, -1.8850e-06],
        [-8.1062e-06, -5.8413e-06,  6.3702e-07,  ..., -6.4075e-06,
         -9.2387e-07, -3.2634e-06]], device='cuda:0')
Loss: 1.165980577468872


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5432e-05,  2.3787e-05, -3.4421e-06,  ...,  2.8597e-05,
          8.5593e-07,  6.9674e-05],
        [-6.7055e-06, -4.7982e-06,  5.4576e-07,  ..., -5.3644e-06,
         -7.4692e-07, -2.7195e-06],
        [-5.9009e-06, -4.2319e-06,  4.7870e-07,  ..., -4.7237e-06,
         -6.5751e-07, -2.3991e-06],
        [-9.2685e-06, -6.6459e-06,  7.5810e-07,  ..., -7.4059e-06,
         -1.0319e-06, -3.7551e-06],
        [-1.6153e-05, -1.1563e-05,  1.3001e-06,  ..., -1.2904e-05,
         -1.8030e-06, -6.5267e-06]], device='cuda:0')
Loss: 1.1893116235733032


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0,     9,   870,  ...,  2671,   965,  3229]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2311e-05,  6.1303e-06,  2.3896e-05,  ...,  7.1661e-05,
         -2.1727e-05,  8.2146e-05],
        [-1.0043e-05, -7.1973e-06,  8.2143e-07,  ..., -8.0466e-06,
         -1.1344e-06, -4.0680e-06],
        [-8.8364e-06, -6.3479e-06,  7.2084e-07,  ..., -7.0930e-06,
         -1.0002e-06, -3.5912e-06],
        [-1.3947e-05, -1.0014e-05,  1.1474e-06,  ..., -1.1161e-05,
         -1.5721e-06, -5.6475e-06],
        [-2.4259e-05, -1.7375e-05,  1.9670e-06,  ..., -1.9401e-05,
         -2.7493e-06, -9.7901e-06]], device='cuda:0')
Loss: 1.1774578094482422


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  2314,   516,  ...,  1871,    13,  1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5954e-05, -1.2675e-05, -1.6616e-05,  ...,  1.2354e-04,
         -2.1324e-05,  9.0995e-05],
        [-1.3351e-05, -9.5814e-06,  1.1045e-06,  ..., -1.0759e-05,
         -1.4864e-06, -5.4017e-06],
        [-1.1742e-05, -8.4490e-06,  9.7230e-07,  ..., -9.4771e-06,
         -1.3094e-06, -4.7684e-06],
        [-1.8537e-05, -1.3322e-05,  1.5460e-06,  ..., -1.4931e-05,
         -2.0601e-06, -7.5027e-06],
        [-3.2127e-05, -2.3037e-05,  2.6375e-06,  ..., -2.5839e-05,
         -3.5875e-06, -1.2949e-05]], device='cuda:0')
Loss: 1.178513526916504


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0, 1358,  367,  ..., 1758, 2921,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8813e-05, -3.1740e-05, -1.2040e-05,  ...,  1.2916e-04,
         -1.9381e-05,  1.1358e-04],
        [-1.6704e-05, -1.1995e-05,  1.3597e-06,  ..., -1.3456e-05,
         -1.8366e-06, -6.7502e-06],
        [-1.4648e-05, -1.0535e-05,  1.1930e-06,  ..., -1.1817e-05,
         -1.6131e-06, -5.9381e-06],
        [-2.3127e-05, -1.6630e-05,  1.8999e-06,  ..., -1.8626e-05,
         -2.5369e-06, -9.3505e-06],
        [-4.0114e-05, -2.8789e-05,  3.2447e-06,  ..., -3.2276e-05,
         -4.4256e-06, -1.6153e-05]], device='cuda:0')
Loss: 1.1875216960906982


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0,  1746,    14,  ...,  3134,  5968,     9],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9940e-05, -2.5492e-05, -3.1064e-05,  ...,  1.3528e-04,
         -7.0868e-06,  1.1854e-04],
        [-2.0057e-05, -1.4439e-05,  1.6578e-06,  ..., -1.6168e-05,
         -2.2240e-06, -8.1286e-06],
        [ 6.7856e-05,  5.4012e-05,  5.6630e-06,  ...,  7.1257e-05,
          5.9761e-06,  2.8183e-05],
        [-2.7716e-05, -1.9968e-05,  2.3097e-06,  ..., -2.2322e-05,
         -3.0659e-06, -1.1235e-05],
        [-4.8101e-05, -3.4630e-05,  3.9525e-06,  ..., -3.8713e-05,
         -5.3495e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1812536716461182


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0, 16803,   965,  ..., 36064,    12, 13769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4863e-05, -6.0386e-06,  2.6686e-06,  ...,  1.5683e-04,
         -1.9214e-05,  1.3332e-04],
        [-2.3425e-05, -1.6883e-05,  1.9353e-06,  ..., -1.8865e-05,
         -2.5984e-06, -9.4920e-06],
        [ 6.4906e-05,  5.1882e-05,  5.9051e-06,  ...,  6.8903e-05,
          5.6483e-06,  2.6984e-05],
        [-3.2365e-05, -2.3350e-05,  2.6952e-06,  ..., -2.6047e-05,
         -3.5837e-06, -1.3120e-05],
        [-5.6207e-05, -4.0501e-05,  4.6156e-06,  ..., -4.5180e-05,
         -6.2510e-06, -2.2709e-05]], device='cuda:0')
Loss: 1.1807862520217896


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8595e-05, -9.0231e-06, -2.0095e-05,  ...,  1.3989e-04,
         -3.2404e-05,  1.2492e-04],
        [-2.6807e-05, -1.9297e-05,  2.1942e-06,  ..., -2.1547e-05,
         -2.9858e-06, -1.0870e-05],
        [ 6.1911e-05,  4.9751e-05,  6.1352e-06,  ...,  6.6533e-05,
          5.3055e-06,  2.5769e-05],
        [-3.7014e-05, -2.6673e-05,  3.0529e-06,  ..., -2.9728e-05,
         -4.1164e-06, -1.5005e-05],
        [-6.4313e-05, -4.6283e-05,  5.2340e-06,  ..., -5.1588e-05,
         -7.1824e-06, -2.6003e-05]], device='cuda:0')
Loss: 1.1686567068099976
Graident accumulation at epoch 0, step 215, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0152,  0.0036,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0407,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0158, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8212e-04, -7.8963e-05,  2.9710e-05,  ...,  6.5962e-05,
         -1.5036e-04, -2.2920e-05],
        [-2.8602e-05, -2.0924e-05,  4.0356e-06,  ..., -2.3248e-05,
         -2.9292e-06, -1.1496e-05],
        [ 1.0830e-05,  8.5660e-06, -3.0846e-06,  ...,  1.5018e-05,
         -1.6536e-07,  6.9319e-06],
        [-1.8328e-05, -9.3191e-06,  4.3894e-06,  ..., -1.1773e-05,
         -8.5859e-07, -4.0992e-06],
        [-5.9232e-05, -4.3066e-05,  7.1868e-06,  ..., -4.7448e-05,
         -6.0950e-06, -2.3835e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9994e-08, 2.0153e-08, 3.0349e-08,  ..., 2.2531e-08, 5.3191e-08,
         1.1371e-08],
        [2.4635e-11, 1.6805e-11, 8.8956e-13,  ..., 1.6242e-11, 3.8361e-13,
         3.2236e-12],
        [9.5936e-11, 8.2376e-11, 1.0060e-11,  ..., 1.4317e-10, 3.5627e-12,
         3.5308e-11],
        [1.3175e-10, 2.0887e-10, 4.4128e-12,  ..., 1.0749e-10, 1.6717e-11,
         7.1701e-11],
        [9.2241e-11, 5.6413e-11, 5.3704e-12,  ..., 6.2859e-11, 1.0951e-12,
         1.3930e-11]], device='cuda:0')
optimizer state dict: 27.0
lr: [1.972140031957344e-05, 1.972140031957344e-05]
scheduler_last_epoch: 27


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[   0,  275, 2101,  ..., 1145,  590, 1619],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0486e-05,  4.2203e-06, -4.9044e-05,  ...,  2.5464e-05,
          2.8239e-06,  3.7100e-06],
        [-3.2634e-06, -2.2948e-06,  2.4959e-07,  ..., -2.6524e-06,
         -2.7940e-07, -1.3337e-06],
        [-2.9504e-06, -2.0862e-06,  2.2631e-07,  ..., -2.3991e-06,
         -2.5518e-07, -1.2144e-06],
        [-4.6492e-06, -3.2783e-06,  3.5949e-07,  ..., -3.7849e-06,
         -4.0047e-07, -1.9073e-06],
        [-7.8082e-06, -5.5134e-06,  5.9977e-07,  ..., -6.3777e-06,
         -6.7428e-07, -3.2037e-06]], device='cuda:0')
Loss: 1.1750853061676025


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[   0, 1862,  674,  ...,  391,  266, 7688],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3486e-05, -1.0093e-05, -3.0853e-05,  ...,  1.3454e-05,
          1.3822e-05, -2.1193e-05],
        [-6.5118e-06, -4.6492e-06,  5.3272e-07,  ..., -5.3048e-06,
         -5.8860e-07, -2.6077e-06],
        [-5.9158e-06, -4.2319e-06,  4.8522e-07,  ..., -4.8131e-06,
         -5.3644e-07, -2.3767e-06],
        [-9.2983e-06, -6.6608e-06,  7.7114e-07,  ..., -7.5847e-06,
         -8.4192e-07, -3.7327e-06],
        [-1.5616e-05, -1.1176e-05,  1.2778e-06,  ..., -1.2726e-05,
         -1.4119e-06, -6.2436e-06]], device='cuda:0')
Loss: 1.1897848844528198


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5731e-05, -4.1590e-05, -3.1873e-05,  ...,  1.3622e-05,
          1.4365e-05,  1.9776e-07],
        [-9.7603e-06, -7.0035e-06,  8.3074e-07,  ..., -7.9572e-06,
         -8.7917e-07, -3.9339e-06],
        [ 1.1365e-04,  5.4027e-05, -9.3749e-06,  ...,  7.9723e-05,
          4.9525e-06,  2.8416e-05],
        [-1.3918e-05, -1.0028e-05,  1.2014e-06,  ..., -1.1355e-05,
         -1.2536e-06, -5.6177e-06],
        [-2.3425e-05, -1.6838e-05,  1.9930e-06,  ..., -1.9073e-05,
         -2.1085e-06, -9.4175e-06]], device='cuda:0')
Loss: 1.1799733638763428


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,  6491,  3667,  ...,  5042,    14,  2152]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3724e-05, -4.3828e-05,  1.8376e-05,  ...,  1.8013e-05,
          3.2186e-06, -5.3555e-06],
        [-1.3053e-05, -9.3281e-06,  1.0859e-06,  ..., -1.0610e-05,
         -1.2014e-06, -5.2303e-06],
        [ 1.1064e-04,  5.1897e-05, -9.1412e-06,  ...,  7.7295e-05,
          4.6582e-06,  2.7224e-05],
        [-1.8626e-05, -1.3366e-05,  1.5683e-06,  ..., -1.5169e-05,
         -1.7136e-06, -7.4804e-06],
        [-3.1352e-05, -2.2441e-05,  2.6003e-06,  ..., -2.5481e-05,
         -2.8834e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.2003172636032104


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3499e-05, -6.5696e-05,  5.0617e-05,  ...,  2.4401e-05,
          1.9482e-06, -3.4122e-05],
        [-1.6302e-05, -1.1668e-05,  1.3579e-06,  ..., -1.3262e-05,
         -1.4696e-06, -6.5044e-06],
        [ 1.0770e-04,  4.9781e-05, -8.8934e-06,  ...,  7.4881e-05,
          4.4142e-06,  2.6069e-05],
        [-2.3276e-05, -1.6719e-05,  1.9595e-06,  ..., -1.8969e-05,
         -2.0973e-06, -9.2983e-06],
        [-3.9160e-05, -2.8074e-05,  3.2522e-06,  ..., -3.1859e-05,
         -3.5316e-06, -1.5602e-05]], device='cuda:0')
Loss: 1.183322787284851


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7108e-05, -5.7726e-05,  2.4083e-05,  ...,  3.4862e-05,
          4.1474e-06,  2.3748e-05],
        [-1.9625e-05, -1.4022e-05,  1.6466e-06,  ..., -1.5959e-05,
         -1.7788e-06, -7.8380e-06],
        [ 1.0469e-04,  4.7635e-05, -8.6308e-06,  ...,  7.2437e-05,
          4.1330e-06,  2.4862e-05],
        [-2.8014e-05, -2.0072e-05,  2.3730e-06,  ..., -2.2814e-05,
         -2.5369e-06, -1.1198e-05],
        [-4.7147e-05, -3.3736e-05,  3.9414e-06,  ..., -3.8326e-05,
         -4.2766e-06, -1.8790e-05]], device='cuda:0')
Loss: 1.175032138824463


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0, 5982, 9385,  ...,   26,  469,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8717e-05, -3.5509e-05,  4.0829e-05,  ...,  4.2212e-05,
         -1.1511e-05,  4.3036e-05],
        [-2.2888e-05, -1.6361e-05,  1.8999e-06,  ..., -1.8641e-05,
         -2.0880e-06, -9.1344e-06],
        [ 1.0174e-04,  4.5519e-05, -8.4008e-06,  ...,  7.0008e-05,
          3.8517e-06,  2.3685e-05],
        [-3.2604e-05, -2.3365e-05,  2.7325e-06,  ..., -2.6599e-05,
         -2.9728e-06, -1.3024e-05],
        [-5.4955e-05, -3.9339e-05,  4.5486e-06,  ..., -4.4763e-05,
         -5.0254e-06, -2.1905e-05]], device='cuda:0')
Loss: 1.1736980676651


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0,  199, 2834,  ...,  287, 3121,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8989e-05, -2.7502e-05, -5.8729e-06,  ...,  8.2348e-05,
         -1.5293e-05, -1.7176e-05],
        [-2.6092e-05, -1.8671e-05,  2.1979e-06,  ..., -2.1279e-05,
         -2.3879e-06, -1.0453e-05],
        [ 9.8837e-05,  4.3418e-05, -8.1307e-06,  ...,  6.7624e-05,
          3.5798e-06,  2.2493e-05],
        [-3.7193e-05, -2.6658e-05,  3.1609e-06,  ..., -3.0354e-05,
         -3.3993e-06, -1.4909e-05],
        [-6.2585e-05, -4.4823e-05,  5.2527e-06,  ..., -5.1022e-05,
         -5.7407e-06, -2.5034e-05]], device='cuda:0')
Loss: 1.1842851638793945
Graident accumulation at epoch 0, step 223, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0158, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6101e-04, -7.3817e-05,  2.6152e-05,  ...,  6.7601e-05,
         -1.3685e-04, -2.2345e-05],
        [-2.8351e-05, -2.0699e-05,  3.8518e-06,  ..., -2.3051e-05,
         -2.8750e-06, -1.1392e-05],
        [ 1.9631e-05,  1.2051e-05, -3.5892e-06,  ...,  2.0279e-05,
          2.0916e-07,  8.4880e-06],
        [-2.0214e-05, -1.1053e-05,  4.2666e-06,  ..., -1.3631e-05,
         -1.1127e-06, -5.1802e-06],
        [-5.9568e-05, -4.3242e-05,  6.9934e-06,  ..., -4.7806e-05,
         -6.0596e-06, -2.3955e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9935e-08, 2.0134e-08, 3.0319e-08,  ..., 2.2516e-08, 5.3138e-08,
         1.1360e-08],
        [2.5291e-11, 1.7137e-11, 8.9350e-13,  ..., 1.6679e-11, 3.8893e-13,
         3.3296e-12],
        [1.0561e-10, 8.4179e-11, 1.0116e-11,  ..., 1.4760e-10, 3.5719e-12,
         3.5778e-11],
        [1.3300e-10, 2.0937e-10, 4.4184e-12,  ..., 1.0831e-10, 1.6712e-11,
         7.1852e-11],
        [9.6066e-11, 5.8366e-11, 5.3926e-12,  ..., 6.5399e-11, 1.1269e-12,
         1.4542e-11]], device='cuda:0')
optimizer state dict: 28.0
lr: [1.9691682460550022e-05, 1.9691682460550022e-05]
scheduler_last_epoch: 28


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,  3235,   471,  ...,  1967,  4273,  2738],
        [    0, 21410, 13160,  ...,   292,    69,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3043e-06, -1.5850e-06, -2.6232e-06,  ...,  2.0593e-05,
         -1.7002e-05,  2.9660e-05],
        [-3.2037e-06, -2.2501e-06,  2.9802e-07,  ..., -2.6077e-06,
         -2.3190e-07, -1.2517e-06],
        [-3.0249e-06, -2.1160e-06,  2.7940e-07,  ..., -2.4587e-06,
         -2.1886e-07, -1.1772e-06],
        [-4.6492e-06, -3.2634e-06,  4.3213e-07,  ..., -3.7849e-06,
         -3.3528e-07, -1.8179e-06],
        [-7.7486e-06, -5.4240e-06,  7.1153e-07,  ..., -6.2883e-06,
         -5.5879e-07, -3.0100e-06]], device='cuda:0')
Loss: 1.1953791379928589


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[   0,  278, 5717,  ..., 5342, 5147,   14],
        [   0, 6518,  681,  ...,  401, 9748,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0602e-05, -1.6764e-05,  2.0608e-05,  ...,  4.1255e-05,
         -4.1863e-06,  4.1271e-05],
        [-6.3330e-06, -4.5002e-06,  5.7928e-07,  ..., -5.1558e-06,
         -4.4145e-07, -2.4512e-06],
        [-6.0052e-06, -4.2617e-06,  5.4948e-07,  ..., -4.8876e-06,
         -4.1816e-07, -2.3171e-06],
        [-9.2685e-06, -6.5863e-06,  8.5123e-07,  ..., -7.5400e-06,
         -6.4261e-07, -3.5837e-06],
        [-1.5378e-05, -1.0908e-05,  1.3933e-06,  ..., -1.2487e-05,
         -1.0692e-06, -5.9307e-06]], device='cuda:0')
Loss: 1.1756656169891357


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[    0,   292,    48,  ...,   199, 19047,   292],
        [    0, 23809, 27646,  ...,   266,  3373,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8245e-06, -2.1235e-05,  3.2169e-05,  ...,  7.1456e-05,
         -1.8677e-05,  5.3866e-05],
        [-9.5069e-06, -6.7651e-06,  8.6240e-07,  ..., -7.7635e-06,
         -6.6590e-07, -3.6806e-06],
        [-9.0003e-06, -6.4075e-06,  8.1770e-07,  ..., -7.3463e-06,
         -6.3051e-07, -3.4794e-06],
        [-1.3918e-05, -9.8944e-06,  1.2685e-06,  ..., -1.1355e-05,
         -9.7044e-07, -5.3868e-06],
        [-2.3007e-05, -1.6361e-05,  2.0750e-06,  ..., -1.8746e-05,
         -1.6093e-06, -8.8811e-06]], device='cuda:0')
Loss: 1.1693633794784546


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2915,   275,  2565],
        [    0, 12472,  1059,  ...,   642,   365,  6517]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2855e-05, -3.6701e-05,  2.7980e-05,  ...,  9.4154e-05,
         -3.1115e-06,  4.4116e-05],
        [-1.2651e-05, -9.0003e-06,  1.1418e-06,  ..., -1.0356e-05,
         -9.1735e-07, -4.9099e-06],
        [-1.1966e-05, -8.5235e-06,  1.0822e-06,  ..., -9.7901e-06,
         -8.6892e-07, -4.6417e-06],
        [-1.8537e-05, -1.3188e-05,  1.6801e-06,  ..., -1.5154e-05,
         -1.3374e-06, -7.1898e-06],
        [-3.0547e-05, -2.1756e-05,  2.7418e-06,  ..., -2.4974e-05,
         -2.2165e-06, -1.1832e-05]], device='cuda:0')
Loss: 1.192940592765808


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  1034,  5599,  ...,   259,   586,  1403],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3576e-05, -4.2342e-05,  3.2726e-05,  ...,  8.4314e-05,
          3.4443e-06,  5.1181e-05],
        [-1.5795e-05, -1.1235e-05,  1.4193e-06,  ..., -1.2919e-05,
         -1.1390e-06, -6.1318e-06],
        [-1.4946e-05, -1.0639e-05,  1.3448e-06,  ..., -1.2219e-05,
         -1.0785e-06, -5.7966e-06],
        [-2.3156e-05, -1.6466e-05,  2.0899e-06,  ..., -1.8924e-05,
         -1.6615e-06, -8.9779e-06],
        [-3.8236e-05, -2.7210e-05,  3.4161e-06,  ..., -3.1233e-05,
         -2.7567e-06, -1.4812e-05]], device='cuda:0')
Loss: 1.182521104812622


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[   0, 4995,  287,  ...,  300, 4531, 4729],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1375e-05, -3.9320e-05,  4.5077e-05,  ...,  1.0030e-04,
         -6.7446e-07,  6.7490e-05],
        [-1.8984e-05, -1.3471e-05,  1.7062e-06,  ..., -1.5497e-05,
         -1.3812e-06, -7.3612e-06],
        [-1.7941e-05, -1.2740e-05,  1.6149e-06,  ..., -1.4648e-05,
         -1.3057e-06, -6.9514e-06],
        [-2.7835e-05, -1.9759e-05,  2.5146e-06,  ..., -2.2709e-05,
         -2.0135e-06, -1.0788e-05],
        [-4.5866e-05, -3.2574e-05,  4.1015e-06,  ..., -3.7402e-05,
         -3.3379e-06, -1.7747e-05]], device='cuda:0')
Loss: 1.1898490190505981


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,    12,  2735,  ...,    12,   344,  1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8992e-05, -3.9320e-05,  5.4670e-05,  ...,  1.0258e-04,
          1.9184e-05,  4.9461e-05],
        [-2.2143e-05, -1.5736e-05,  1.9949e-06,  ..., -1.8090e-05,
         -1.6177e-06, -8.5980e-06],
        [-2.0936e-05, -1.4886e-05,  1.8869e-06,  ..., -1.7107e-05,
         -1.5302e-06, -8.1211e-06],
        [-3.2485e-05, -2.3097e-05,  2.9393e-06,  ..., -2.6524e-05,
         -2.3600e-06, -1.2606e-05],
        [-5.3555e-05, -3.8087e-05,  4.7982e-06,  ..., -4.3690e-05,
         -3.9153e-06, -2.0742e-05]], device='cuda:0')
Loss: 1.1965460777282715


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[    0,   287,  9430,  ...,  3121,   352,   360],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8712e-05,  6.0366e-06,  7.1861e-05,  ...,  8.0605e-05,
         -3.2108e-07,  7.5467e-05],
        [-2.5287e-05, -1.8001e-05,  2.2780e-06,  ..., -2.0668e-05,
         -1.8384e-06, -9.8273e-06],
        [-2.3901e-05, -1.7032e-05,  2.1551e-06,  ..., -1.9535e-05,
         -1.7378e-06, -9.2760e-06],
        [-3.7134e-05, -2.6435e-05,  3.3602e-06,  ..., -3.0324e-05,
         -2.6822e-06, -1.4417e-05],
        [-6.1184e-05, -4.3601e-05,  5.4836e-06,  ..., -4.9949e-05,
         -4.4517e-06, -2.3723e-05]], device='cuda:0')
Loss: 1.181644082069397
Graident accumulation at epoch 0, step 231, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0158, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4303e-04, -6.5832e-05,  3.0723e-05,  ...,  6.8901e-05,
         -1.2320e-04, -1.2564e-05],
        [-2.8045e-05, -2.0429e-05,  3.6944e-06,  ..., -2.2813e-05,
         -2.7714e-06, -1.1236e-05],
        [ 1.5278e-05,  9.1428e-06, -3.0148e-06,  ...,  1.6297e-05,
          1.4457e-08,  6.7116e-06],
        [-2.1906e-05, -1.2591e-05,  4.1759e-06,  ..., -1.5301e-05,
         -1.2696e-06, -6.1039e-06],
        [-5.9729e-05, -4.3278e-05,  6.8424e-06,  ..., -4.8020e-05,
         -5.8988e-06, -2.3932e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9875e-08, 2.0113e-08, 3.0294e-08,  ..., 2.2500e-08, 5.3085e-08,
         1.1354e-08],
        [2.5905e-11, 1.7444e-11, 8.9779e-13,  ..., 1.7089e-11, 3.9192e-13,
         3.4229e-12],
        [1.0607e-10, 8.4385e-11, 1.0110e-11,  ..., 1.4784e-10, 3.5714e-12,
         3.5829e-11],
        [1.3424e-10, 2.0986e-10, 4.4253e-12,  ..., 1.0912e-10, 1.6703e-11,
         7.1988e-11],
        [9.9713e-11, 6.0209e-11, 5.4173e-12,  ..., 6.7829e-11, 1.1456e-12,
         1.5091e-11]], device='cuda:0')
optimizer state dict: 29.0
lr: [1.9660483627846746e-05, 1.9660483627846746e-05]
scheduler_last_epoch: 29


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,  6909,   352,  ...,  1075,   706,  6909]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4838e-05,  1.6716e-05, -1.3509e-05,  ...,  4.3131e-07,
         -9.6272e-07,  5.6818e-07],
        [-3.0994e-06, -2.1458e-06,  3.2037e-07,  ..., -2.5481e-06,
         -2.0023e-07, -1.1921e-06],
        [-3.0249e-06, -2.1011e-06,  3.1292e-07,  ..., -2.5034e-06,
         -1.9651e-07, -1.1623e-06],
        [-4.6194e-06, -3.2037e-06,  4.8056e-07,  ..., -3.8147e-06,
         -2.9802e-07, -1.7732e-06],
        [-7.5102e-06, -5.1856e-06,  7.7114e-07,  ..., -6.1691e-06,
         -4.8801e-07, -2.8759e-06]], device='cuda:0')
Loss: 1.1765140295028687


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,    12,   546,  ..., 24994, 31107,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9831e-06,  1.3337e-05, -1.3106e-05,  ...,  2.7773e-05,
         -9.5509e-06, -5.0958e-07],
        [-6.1989e-06, -4.3213e-06,  6.5565e-07,  ..., -5.0664e-06,
         -4.0792e-07, -2.3767e-06],
        [-6.0350e-06, -4.2170e-06,  6.3889e-07,  ..., -4.9472e-06,
         -3.9954e-07, -2.3171e-06],
        [-9.2685e-06, -6.4671e-06,  9.8348e-07,  ..., -7.5847e-06,
         -6.1095e-07, -3.5539e-06],
        [-1.5050e-05, -1.0461e-05,  1.5832e-06,  ..., -1.2279e-05,
         -9.9465e-07, -5.7518e-06]], device='cuda:0')
Loss: 1.1764215230941772


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0,  642,  287,  ...,  800,   12, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1078e-05, -9.3406e-06,  3.1966e-05,  ...,  2.6395e-05,
         -3.5257e-05,  2.5393e-05],
        [-9.3281e-06, -6.5118e-06,  9.8161e-07,  ..., -7.6145e-06,
         -6.4075e-07, -3.5763e-06],
        [-9.0599e-06, -6.3330e-06,  9.5367e-07,  ..., -7.4059e-06,
         -6.2305e-07, -3.4720e-06],
        [-1.3947e-05, -9.7454e-06,  1.4715e-06,  ..., -1.1384e-05,
         -9.5740e-07, -5.3421e-06],
        [-2.2620e-05, -1.5765e-05,  2.3693e-06,  ..., -1.8448e-05,
         -1.5572e-06, -8.6427e-06]], device='cuda:0')
Loss: 1.170745611190796


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,   266,  2604,  ...,   278,  4035,  4165],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6007e-05, -2.1046e-05,  6.9499e-05,  ..., -1.8809e-05,
         -2.6155e-05,  2.9035e-05],
        [-1.2442e-05, -8.6874e-06,  1.2778e-06,  ..., -1.0148e-05,
         -8.6986e-07, -4.7758e-06],
        [-1.2100e-05, -8.4490e-06,  1.2405e-06,  ..., -9.8795e-06,
         -8.4564e-07, -4.6417e-06],
        [-1.8567e-05, -1.2964e-05,  1.9111e-06,  ..., -1.5140e-05,
         -1.2945e-06, -7.1228e-06],
        [-3.0190e-05, -2.1011e-05,  3.0808e-06,  ..., -2.4587e-05,
         -2.1085e-06, -1.1548e-05]], device='cuda:0')
Loss: 1.1728992462158203


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,  1875,  2117,  ...,  1422,  1059,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5628e-05,  2.4607e-07,  3.2185e-05,  ..., -2.7047e-05,
          2.6162e-05,  8.1408e-06],
        [-1.5542e-05, -1.0878e-05,  1.6447e-06,  ..., -1.2711e-05,
         -1.1157e-06, -5.9977e-06],
        [-1.5095e-05, -1.0565e-05,  1.5963e-06,  ..., -1.2368e-05,
         -1.0859e-06, -5.8264e-06],
        [-2.3156e-05, -1.6212e-05,  2.4587e-06,  ..., -1.8954e-05,
         -1.6615e-06, -8.9332e-06],
        [-3.7611e-05, -2.6256e-05,  3.9600e-06,  ..., -3.0726e-05,
         -2.7046e-06, -1.4469e-05]], device='cuda:0')
Loss: 1.1801869869232178


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2550e-05, -7.0939e-05,  5.9304e-05,  ..., -3.1512e-05,
          5.7063e-05, -2.7505e-05],
        [-1.8626e-05, -1.2979e-05,  1.9521e-06,  ..., -1.5229e-05,
         -1.3001e-06, -7.1973e-06],
        [-1.8105e-05, -1.2621e-05,  1.8962e-06,  ..., -1.4827e-05,
         -1.2657e-06, -6.9961e-06],
        [-2.7776e-05, -1.9357e-05,  2.9188e-06,  ..., -2.2709e-05,
         -1.9353e-06, -1.0721e-05],
        [-4.5180e-05, -3.1412e-05,  4.7050e-06,  ..., -3.6895e-05,
         -3.1553e-06, -1.7405e-05]], device='cuda:0')
Loss: 1.1821269989013672


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8462e-05, -7.7618e-05,  6.2786e-05,  ..., -6.7029e-05,
          3.9112e-05, -8.0401e-05],
        [-2.1771e-05, -1.5184e-05,  2.2557e-06,  ..., -1.7792e-05,
         -1.5199e-06, -8.4043e-06],
        [-2.1160e-05, -1.4752e-05,  2.1905e-06,  ..., -1.7315e-05,
         -1.4789e-06, -8.1658e-06],
        [-3.2455e-05, -2.2635e-05,  3.3733e-06,  ..., -2.6524e-05,
         -2.2613e-06, -1.2517e-05],
        [-5.2810e-05, -3.6746e-05,  5.4352e-06,  ..., -4.3124e-05,
         -3.6918e-06, -2.0325e-05]], device='cuda:0')
Loss: 1.166598916053772


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0,   11,  360,  ..., 4524, 1553,  401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9539e-05, -6.7113e-05,  8.7450e-05,  ..., -5.9414e-05,
          2.0984e-05, -5.1504e-05],
        [-2.4885e-05, -1.7375e-05,  2.5816e-06,  ..., -2.0355e-05,
         -1.7295e-06, -9.5963e-06],
        [-2.4170e-05, -1.6883e-05,  2.5071e-06,  ..., -1.9804e-05,
         -1.6829e-06, -9.3281e-06],
        [-3.7074e-05, -2.5883e-05,  3.8613e-06,  ..., -3.0324e-05,
         -2.5723e-06, -1.4290e-05],
        [-6.0260e-05, -4.1991e-05,  6.2138e-06,  ..., -4.9263e-05,
         -4.1984e-06, -2.3186e-05]], device='cuda:0')
Loss: 1.1816800832748413
Graident accumulation at epoch 0, step 239, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0335, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0157, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3268e-04, -6.5960e-05,  3.6395e-05,  ...,  5.6070e-05,
         -1.0878e-04, -1.6458e-05],
        [-2.7729e-05, -2.0123e-05,  3.5832e-06,  ..., -2.2567e-05,
         -2.6672e-06, -1.1072e-05],
        [ 1.1333e-05,  6.5403e-06, -2.4626e-06,  ...,  1.2687e-05,
         -1.5528e-07,  5.1076e-06],
        [-2.3423e-05, -1.3920e-05,  4.1445e-06,  ..., -1.6803e-05,
         -1.3999e-06, -6.9225e-06],
        [-5.9782e-05, -4.3149e-05,  6.7796e-06,  ..., -4.8144e-05,
         -5.7288e-06, -2.3857e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9817e-08, 2.0098e-08, 3.0271e-08,  ..., 2.2481e-08, 5.3032e-08,
         1.1346e-08],
        [2.6499e-11, 1.7728e-11, 9.0356e-13,  ..., 1.7487e-11, 3.9452e-13,
         3.5116e-12],
        [1.0655e-10, 8.4586e-11, 1.0107e-11,  ..., 1.4808e-10, 3.5706e-12,
         3.5880e-11],
        [1.3548e-10, 2.1032e-10, 4.4358e-12,  ..., 1.0993e-10, 1.6693e-11,
         7.2120e-11],
        [1.0324e-10, 6.1912e-11, 5.4505e-12,  ..., 7.0188e-11, 1.1621e-12,
         1.5613e-11]], device='cuda:0')
optimizer state dict: 30.0
lr: [1.9627808588917577e-05, 1.9627808588917577e-05]
scheduler_last_epoch: 30


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9346e-05, -1.5724e-05, -1.2359e-05,  ..., -8.5445e-06,
          1.2737e-05,  1.5765e-05],
        [-3.0249e-06, -2.1011e-06,  3.5577e-07,  ..., -2.5183e-06,
         -2.0396e-07, -1.1921e-06],
        [-3.0249e-06, -2.1011e-06,  3.5577e-07,  ..., -2.5183e-06,
         -2.0489e-07, -1.1921e-06],
        [-4.5598e-06, -3.1441e-06,  5.3644e-07,  ..., -3.7700e-06,
         -3.0734e-07, -1.7881e-06],
        [-7.3910e-06, -5.1260e-06,  8.6427e-07,  ..., -6.1095e-06,
         -4.9919e-07, -2.9057e-06]], device='cuda:0')
Loss: 1.1874990463256836


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0,   278,   554,  ...,   365,  3125,   271],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6413e-05, -3.9249e-06, -1.1139e-05,  ..., -1.9910e-05,
          3.3076e-05,  5.2036e-06],
        [-6.0797e-06, -4.2021e-06,  6.9104e-07,  ..., -5.0366e-06,
         -4.0326e-07, -2.3767e-06],
        [-6.0499e-06, -4.1872e-06,  6.8918e-07,  ..., -5.0217e-06,
         -4.0326e-07, -2.3693e-06],
        [-9.1195e-06, -6.2734e-06,  1.0431e-06,  ..., -7.5400e-06,
         -6.0350e-07, -3.5539e-06],
        [-1.4812e-05, -1.0222e-05,  1.6801e-06,  ..., -1.2219e-05,
         -9.8720e-07, -5.7817e-06]], device='cuda:0')
Loss: 1.2096071243286133


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  1067,   271,  ...,   266,   940,   271],
        [    0,   365,  1941,  ..., 38029,  1790, 44066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8181e-06, -1.7202e-06, -2.4625e-05,  ..., -2.5747e-05,
          5.5780e-05,  1.9832e-05],
        [-9.1642e-06, -6.3181e-06,  1.0375e-06,  ..., -7.5698e-06,
         -6.0350e-07, -3.5688e-06],
        [-9.1195e-06, -6.3032e-06,  1.0356e-06,  ..., -7.5549e-06,
         -6.0257e-07, -3.5539e-06],
        [-1.3709e-05, -9.4473e-06,  1.5646e-06,  ..., -1.1325e-05,
         -9.0152e-07, -5.3272e-06],
        [-2.2262e-05, -1.5348e-05,  2.5183e-06,  ..., -1.8328e-05,
         -1.4715e-06, -8.6427e-06]], device='cuda:0')
Loss: 1.1880518198013306


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0,  1688,   790,  ...,   546,   696,    12],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4362e-05,  2.9541e-06, -5.9024e-05,  ..., -1.1924e-05,
          8.5150e-05,  6.7108e-05],
        [-1.2279e-05, -8.4937e-06,  1.4063e-06,  ..., -1.0118e-05,
         -8.0746e-07, -4.7535e-06],
        [ 3.6190e-04,  2.9845e-04, -3.2950e-05,  ...,  3.2100e-04,
          1.7190e-05,  1.3982e-04],
        [-1.8358e-05, -1.2681e-05,  2.1160e-06,  ..., -1.5125e-05,
         -1.2051e-06, -7.0930e-06],
        [-2.9802e-05, -2.0593e-05,  3.4086e-06,  ..., -2.4498e-05,
         -1.9707e-06, -1.1504e-05]], device='cuda:0')
Loss: 1.181771159172058


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  996, 2226,  ...,  516, 3470,   14],
        [   0, 1561,   14,  ..., 4433,  352, 1561]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7376e-05,  2.0749e-05, -6.4076e-05,  ...,  3.3538e-05,
          8.5150e-05,  1.0572e-04],
        [-1.5348e-05, -1.0580e-05,  1.7732e-06,  ..., -1.2621e-05,
         -1.0263e-06, -5.9530e-06],
        [ 3.5885e-04,  2.9636e-04, -3.2583e-05,  ...,  3.1850e-04,
          1.6972e-05,  1.3862e-04],
        [-2.2948e-05, -1.5810e-05,  2.6710e-06,  ..., -1.8895e-05,
         -1.5330e-06, -8.8960e-06],
        [-3.7223e-05, -2.5630e-05,  4.2953e-06,  ..., -3.0577e-05,
         -2.5034e-06, -1.4409e-05]], device='cuda:0')
Loss: 1.168616533279419


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    14,  7870,  ...,   284,   830,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7381e-05,  2.1903e-05, -6.5766e-05,  ...,  4.1560e-05,
          8.1850e-05,  1.3703e-04],
        [-1.8388e-05, -1.2681e-05,  2.1458e-06,  ..., -1.5095e-05,
         -1.2396e-06, -7.1377e-06],
        [ 4.6362e-04,  3.6795e-04, -4.7429e-05,  ...,  3.9314e-04,
          2.7693e-05,  1.6245e-04],
        [-2.7537e-05, -1.8984e-05,  3.2336e-06,  ..., -2.2620e-05,
         -1.8552e-06, -1.0677e-05],
        [-4.4674e-05, -3.0756e-05,  5.2005e-06,  ..., -3.6627e-05,
         -3.0249e-06, -1.7285e-05]], device='cuda:0')
Loss: 1.1601378917694092


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,  9582,  3645,  ...,  1027,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8669e-05,  3.9761e-06, -8.2501e-05,  ...,  4.2239e-05,
          9.6898e-05,  1.1547e-04],
        [-2.1473e-05, -1.4812e-05,  2.4978e-06,  ..., -1.7643e-05,
         -1.4575e-06, -8.2925e-06],
        [ 4.6056e-04,  3.6583e-04, -4.7081e-05,  ...,  3.9060e-04,
          2.7476e-05,  1.6129e-04],
        [-3.2127e-05, -2.2158e-05,  3.7588e-06,  ..., -2.6435e-05,
         -2.1793e-06, -1.2405e-05],
        [-5.2184e-05, -3.5942e-05,  6.0536e-06,  ..., -4.2856e-05,
         -3.5577e-06, -2.0117e-05]], device='cuda:0')
Loss: 1.1892638206481934


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0, 24063,   717,  ...,  2228,  1416,     9],
        [    0,   741,   300,  ...,    83,  7111,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9796e-05, -1.1248e-05, -6.2669e-05,  ...,  7.6087e-05,
          8.0298e-05,  1.2566e-04],
        [-2.4527e-05, -1.6943e-05,  2.8443e-06,  ..., -2.0146e-05,
         -1.6699e-06, -9.4622e-06],
        [ 4.5748e-04,  3.6369e-04, -4.6730e-05,  ...,  3.8807e-04,
          2.7262e-05,  1.6011e-04],
        [-3.6716e-05, -2.5362e-05,  4.2804e-06,  ..., -3.0190e-05,
         -2.4978e-06, -1.4156e-05],
        [-5.9605e-05, -4.1127e-05,  6.8918e-06,  ..., -4.8935e-05,
         -4.0717e-06, -2.2948e-05]], device='cuda:0')
Loss: 1.1639000177383423
Graident accumulation at epoch 0, step 247, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0224, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0157, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2740e-04, -6.0489e-05,  2.6489e-05,  ...,  5.8071e-05,
         -8.9873e-05, -2.2463e-06],
        [-2.7409e-05, -1.9805e-05,  3.5093e-06,  ..., -2.2325e-05,
         -2.5674e-06, -1.0911e-05],
        [ 5.5947e-05,  4.2255e-05, -6.8894e-06,  ...,  5.0226e-05,
          2.5865e-06,  2.0608e-05],
        [-2.4752e-05, -1.5065e-05,  4.1580e-06,  ..., -1.8142e-05,
         -1.5097e-06, -7.6458e-06],
        [-5.9765e-05, -4.2947e-05,  6.7908e-06,  ..., -4.8223e-05,
         -5.5631e-06, -2.3766e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9763e-08, 2.0078e-08, 3.0245e-08,  ..., 2.2464e-08, 5.2985e-08,
         1.1350e-08],
        [2.7074e-11, 1.7997e-11, 9.1075e-13,  ..., 1.7875e-11, 3.9691e-13,
         3.5976e-12],
        [3.1573e-10, 2.1677e-10, 1.2280e-11,  ..., 2.9853e-10, 4.3103e-12,
         6.1479e-11],
        [1.3670e-10, 2.1075e-10, 4.4496e-12,  ..., 1.1073e-10, 1.6682e-11,
         7.2248e-11],
        [1.0669e-10, 6.3541e-11, 5.4925e-12,  ..., 7.2512e-11, 1.1775e-12,
         1.6124e-11]], device='cuda:0')
optimizer state dict: 31.0
lr: [1.95936623367937e-05, 1.95936623367937e-05]
scheduler_last_epoch: 31


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[   0,  266, 3536,  ...,  266, 1883,  266],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1739e-05,  6.0739e-06,  2.9893e-05,  ...,  1.5357e-06,
         -3.7946e-05, -1.3215e-05],
        [-2.9951e-06, -2.0564e-06,  3.6694e-07,  ..., -2.4736e-06,
         -1.8068e-07, -1.1697e-06],
        [-3.0845e-06, -2.1160e-06,  3.7812e-07,  ..., -2.5481e-06,
         -1.8720e-07, -1.1995e-06],
        [-4.5002e-06, -3.0994e-06,  5.5507e-07,  ..., -3.7253e-06,
         -2.7008e-07, -1.7583e-06],
        [-7.3016e-06, -5.0068e-06,  8.8662e-07,  ..., -6.0201e-06,
         -4.4145e-07, -2.8312e-06]], device='cuda:0')
Loss: 1.1670382022857666


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4246e-05, -6.0493e-06,  2.8660e-05,  ..., -1.6136e-05,
         -1.6751e-05, -2.1248e-05],
        [-6.0350e-06, -4.1574e-06,  7.2643e-07,  ..., -4.9919e-06,
         -3.9767e-07, -2.3618e-06],
        [-6.1691e-06, -4.2468e-06,  7.4506e-07,  ..., -5.1111e-06,
         -4.0885e-07, -2.4065e-06],
        [-9.0003e-06, -6.2287e-06,  1.0952e-06,  ..., -7.4804e-06,
         -5.9232e-07, -3.5316e-06],
        [-1.4603e-05, -1.0073e-05,  1.7509e-06,  ..., -1.2070e-05,
         -9.6671e-07, -5.6922e-06]], device='cuda:0')
Loss: 1.1651506423950195


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   600,  9092,  ...,   554,  1485,   328],
        [    0, 42306,   278,  ...,  1110,  3427,  4224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2889e-05, -1.9169e-05,  2.4669e-05,  ..., -3.9233e-06,
         -5.8961e-07, -2.6375e-06],
        [-9.0748e-06, -6.2287e-06,  1.0766e-06,  ..., -7.5251e-06,
         -5.9325e-07, -3.5241e-06],
        [-9.2685e-06, -6.3628e-06,  1.1027e-06,  ..., -7.7039e-06,
         -6.0908e-07, -3.5986e-06],
        [-1.3530e-05, -9.3132e-06,  1.6205e-06,  ..., -1.1250e-05,
         -8.8103e-07, -5.2676e-06],
        [-2.1964e-05, -1.5080e-05,  2.5928e-06,  ..., -1.8179e-05,
         -1.4398e-06, -8.5086e-06]], device='cuda:0')
Loss: 1.1750013828277588


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,   278,  7524,  ...,  1288,   669,   352],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4180e-05, -1.8644e-05,  3.4706e-06,  ...,  3.1972e-05,
         -9.8020e-06,  5.0673e-06],
        [-1.2085e-05, -8.2850e-06,  1.4398e-06,  ..., -1.0028e-05,
         -7.8976e-07, -4.6939e-06],
        [-1.2323e-05, -8.4639e-06,  1.4734e-06,  ..., -1.0252e-05,
         -8.0746e-07, -4.7907e-06],
        [-1.8030e-05, -1.2398e-05,  2.1681e-06,  ..., -1.5005e-05,
         -1.1735e-06, -7.0259e-06],
        [-2.9206e-05, -2.0027e-05,  3.4682e-06,  ..., -2.4229e-05,
         -1.9129e-06, -1.1325e-05]], device='cuda:0')
Loss: 1.1665208339691162


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,   12,  616,  ...,  278,  266, 2907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5765e-05, -1.4785e-05, -3.2409e-05,  ...,  3.5008e-05,
         -1.4928e-05, -3.9024e-05],
        [-1.5110e-05, -1.0341e-05,  1.8161e-06,  ..., -1.2532e-05,
         -1.0058e-06, -5.8562e-06],
        [-1.5393e-05, -1.0565e-05,  1.8552e-06,  ..., -1.2800e-05,
         -1.0272e-06, -5.9679e-06],
        [-2.2590e-05, -1.5512e-05,  2.7381e-06,  ..., -1.8775e-05,
         -1.4976e-06, -8.7693e-06],
        [-3.6478e-05, -2.5004e-05,  4.3735e-06,  ..., -3.0249e-05,
         -2.4345e-06, -1.4111e-05]], device='cuda:0')
Loss: 1.1719352006912231


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,    14,   469,  ...,   367,  2564,   368],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6528e-07, -5.6123e-06, -7.6387e-05,  ...,  3.8707e-05,
          8.4962e-06, -3.1859e-05],
        [-1.8120e-05, -1.2442e-05,  2.2072e-06,  ..., -1.5050e-05,
         -1.2266e-06, -7.0259e-06],
        [-1.8463e-05, -1.2696e-05,  2.2538e-06,  ..., -1.5348e-05,
         -1.2517e-06, -7.1526e-06],
        [-2.7090e-05, -1.8656e-05,  3.3267e-06,  ..., -2.2531e-05,
         -1.8273e-06, -1.0520e-05],
        [-4.3690e-05, -3.0041e-05,  5.3085e-06,  ..., -3.6269e-05,
         -2.9635e-06, -1.6913e-05]], device='cuda:0')
Loss: 1.1798056364059448


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  342,  516,  ...,   12,  729, 3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3120e-06, -1.9077e-06, -5.8074e-05,  ...,  5.3975e-05,
          5.2565e-05, -1.5616e-05],
        [-2.1175e-05, -1.4558e-05,  2.5537e-06,  ..., -1.7554e-05,
         -1.4491e-06, -8.1882e-06],
        [-2.1562e-05, -1.4842e-05,  2.6058e-06,  ..., -1.7896e-05,
         -1.4789e-06, -8.3297e-06],
        [-3.1680e-05, -2.1815e-05,  3.8482e-06,  ..., -2.6271e-05,
         -2.1588e-06, -1.2256e-05],
        [-5.1081e-05, -3.5137e-05,  6.1430e-06,  ..., -4.2319e-05,
         -3.5036e-06, -1.9714e-05]], device='cuda:0')
Loss: 1.1752982139587402


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,  1911,   679,  ...,    19,  3737,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9276e-06,  1.3758e-05, -7.2492e-05,  ...,  6.1665e-05,
          5.4102e-05, -1.0222e-05],
        [-2.4185e-05, -1.6600e-05,  2.8796e-06,  ..., -2.0057e-05,
         -1.6307e-06, -9.3207e-06],
        [-2.4647e-05, -1.6928e-05,  2.9411e-06,  ..., -2.0474e-05,
         -1.6661e-06, -9.4920e-06],
        [-3.6210e-05, -2.4885e-05,  4.3400e-06,  ..., -3.0041e-05,
         -2.4308e-06, -1.3955e-05],
        [-5.8383e-05, -4.0084e-05,  6.9328e-06,  ..., -4.8429e-05,
         -3.9488e-06, -2.2471e-05]], device='cuda:0')
Loss: 1.178918719291687
Graident accumulation at epoch 0, step 255, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0151,  0.0035,  ..., -0.0031,  0.0224, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0157, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1406e-04, -5.3064e-05,  1.6591e-05,  ...,  5.8431e-05,
         -7.5475e-05, -3.0438e-06],
        [-2.7086e-05, -1.9485e-05,  3.4463e-06,  ..., -2.2098e-05,
         -2.4738e-06, -1.0752e-05],
        [ 4.7888e-05,  3.6337e-05, -5.9063e-06,  ...,  4.3156e-05,
          2.1612e-06,  1.7598e-05],
        [-2.5898e-05, -1.6047e-05,  4.1762e-06,  ..., -1.9332e-05,
         -1.6018e-06, -8.2768e-06],
        [-5.9626e-05, -4.2661e-05,  6.8050e-06,  ..., -4.8244e-05,
         -5.4016e-06, -2.3637e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9704e-08, 2.0058e-08, 3.0220e-08,  ..., 2.2445e-08, 5.2935e-08,
         1.1339e-08],
        [2.7632e-11, 1.8255e-11, 9.1813e-13,  ..., 1.8259e-11, 3.9917e-13,
         3.6809e-12],
        [3.1602e-10, 2.1684e-10, 1.2277e-11,  ..., 2.9865e-10, 4.3087e-12,
         6.1508e-11],
        [1.3787e-10, 2.1116e-10, 4.4640e-12,  ..., 1.1152e-10, 1.6671e-11,
         7.2371e-11],
        [1.1000e-10, 6.5085e-11, 5.5351e-12,  ..., 7.4785e-11, 1.1919e-12,
         1.6613e-11]], device='cuda:0')
optimizer state dict: 32.0
lr: [1.9558050089320493e-05, 1.9558050089320493e-05]
scheduler_last_epoch: 32


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2348e-07,  5.1641e-06, -8.2058e-06,  ...,  1.6015e-05,
         -6.9500e-07,  9.8308e-06],
        [-2.9951e-06, -2.0415e-06,  3.8929e-07,  ..., -2.5034e-06,
         -2.1700e-07, -1.1399e-06],
        [ 9.1303e-05,  5.9656e-05, -8.4442e-06,  ...,  6.1098e-05,
         -3.4842e-07,  1.9572e-05],
        [-4.5300e-06, -3.0845e-06,  5.8860e-07,  ..., -3.7849e-06,
         -3.2596e-07, -1.7211e-06],
        [-7.1824e-06, -4.8876e-06,  9.3132e-07,  ..., -5.9903e-06,
         -5.2154e-07, -2.7418e-06]], device='cuda:0')
Loss: 1.1754875183105469


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[    0,  4902,   518,  ...,  5493,  3227,   278],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2337e-05, -5.2259e-05,  3.7972e-07,  ...,  1.3200e-05,
          1.4842e-05,  2.8016e-05],
        [-6.0201e-06, -4.0829e-06,  7.8417e-07,  ..., -5.0068e-06,
         -4.8894e-07, -2.2948e-06],
        [ 8.8159e-05,  5.7525e-05, -8.0325e-06,  ...,  5.8490e-05,
         -6.2968e-07,  1.8365e-05],
        [-9.1195e-06, -6.1840e-06,  1.1921e-06,  ..., -7.5847e-06,
         -7.3761e-07, -3.4794e-06],
        [-1.4484e-05, -9.8348e-06,  1.8850e-06,  ..., -1.2040e-05,
         -1.1772e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.1816836595535278


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[   0, 3159,  278,  ...,  266, 2545,  863],
        [   0,  266, 2057,  ...,   88, 1801,   66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1675e-05, -4.7897e-05, -1.2116e-05,  ...,  1.5250e-05,
          3.9032e-05,  5.1562e-05],
        [-9.0152e-06, -6.1095e-06,  1.1735e-06,  ..., -7.4655e-06,
         -6.8825e-07, -3.4347e-06],
        [ 8.5044e-05,  5.5424e-05, -7.6265e-06,  ...,  5.5942e-05,
         -8.3643e-07,  1.7181e-05],
        [-1.3679e-05, -9.2685e-06,  1.7881e-06,  ..., -1.1325e-05,
         -1.0394e-06, -5.2154e-06],
        [-2.1726e-05, -1.4722e-05,  2.8275e-06,  ..., -1.7971e-05,
         -1.6615e-06, -8.2850e-06]], device='cuda:0')
Loss: 1.1783908605575562


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0327e-05, -3.7015e-05, -5.1076e-05,  ...,  2.2559e-06,
          8.7642e-05,  5.8162e-05],
        [-1.1995e-05, -8.1509e-06,  1.5683e-06,  ..., -9.9689e-06,
         -9.4157e-07, -4.5821e-06],
        [ 8.1990e-05,  5.3338e-05, -7.2223e-06,  ...,  5.3379e-05,
         -1.0953e-06,  1.6004e-05],
        [-1.8179e-05, -1.2338e-05,  2.3842e-06,  ..., -1.5080e-05,
         -1.4193e-06, -6.9439e-06],
        [-2.8819e-05, -1.9550e-05,  3.7625e-06,  ..., -2.3901e-05,
         -2.2650e-06, -1.1012e-05]], device='cuda:0')
Loss: 1.1803206205368042


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3685e-05, -2.6568e-05, -5.4362e-05,  ..., -1.1020e-06,
          1.1122e-04,  9.8877e-05],
        [-1.5005e-05, -1.0207e-05,  1.9576e-06,  ..., -1.2502e-05,
         -1.1595e-06, -5.7146e-06],
        [ 7.8875e-05,  5.1222e-05, -6.8200e-06,  ...,  5.0756e-05,
         -1.3217e-06,  1.4834e-05],
        [-2.2709e-05, -1.5423e-05,  2.9728e-06,  ..., -1.8880e-05,
         -1.7472e-06, -8.6501e-06],
        [-3.6061e-05, -2.4468e-05,  4.6939e-06,  ..., -2.9981e-05,
         -2.7902e-06, -1.3739e-05]], device='cuda:0')
Loss: 1.1940433979034424


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[   0,  895, 4110,  ..., 1578, 1245,   13],
        [   0,   22, 2577,  ..., 4970,    9, 3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6198e-05, -1.6945e-05, -7.4462e-05,  ..., -1.9160e-05,
          1.3368e-04,  1.1009e-04],
        [-1.8030e-05, -1.2249e-05,  2.3283e-06,  ..., -1.4991e-05,
         -1.3765e-06, -6.8694e-06],
        [ 7.5746e-05,  4.9106e-05, -6.4362e-06,  ...,  4.8178e-05,
         -1.5470e-06,  1.3642e-05],
        [-2.7269e-05, -1.8507e-05,  3.5353e-06,  ..., -2.2635e-05,
         -2.0731e-06, -1.0386e-05],
        [-4.3303e-05, -2.9355e-05,  5.5805e-06,  ..., -3.5942e-05,
         -3.3155e-06, -1.6496e-05]], device='cuda:0')
Loss: 1.1894810199737549


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0, 3825, 1626,  ..., 5096, 3775,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0794e-05, -3.6420e-05, -9.9475e-05,  ..., -1.6861e-05,
          1.1062e-04,  1.3957e-04],
        [-2.1040e-05, -1.4290e-05,  2.6915e-06,  ..., -1.7479e-05,
         -1.5842e-06, -8.0243e-06],
        [ 7.2632e-05,  4.6990e-05, -6.0600e-06,  ...,  4.5601e-05,
         -1.7622e-06,  1.2442e-05],
        [-3.1829e-05, -2.1607e-05,  4.0904e-06,  ..., -2.6405e-05,
         -2.3860e-06, -1.2137e-05],
        [-5.0545e-05, -3.4273e-05,  6.4485e-06,  ..., -4.1902e-05,
         -3.8147e-06, -1.9267e-05]], device='cuda:0')
Loss: 1.168965458869934


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,    13, 41550,  ...,    12,   546,  1996],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4896e-05, -3.2554e-05, -1.0160e-04,  ..., -3.1081e-05,
          1.1939e-04,  1.5610e-04],
        [-2.4021e-05, -1.6347e-05,  3.0659e-06,  ..., -1.9968e-05,
         -1.8142e-06, -9.1717e-06],
        [ 6.9532e-05,  4.4859e-05, -5.6707e-06,  ...,  4.3008e-05,
         -2.0006e-06,  1.1250e-05],
        [-3.6299e-05, -2.4691e-05,  4.6529e-06,  ..., -3.0145e-05,
         -2.7306e-06, -1.3858e-05],
        [-5.7667e-05, -3.9160e-05,  7.3388e-06,  ..., -4.7863e-05,
         -4.3623e-06, -2.1994e-05]], device='cuda:0')
Loss: 1.1685104370117188
Graident accumulation at epoch 0, step 263, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0151,  0.0035,  ..., -0.0031,  0.0224, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0098,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0157, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1215e-04, -5.1013e-05,  4.7714e-06,  ...,  4.9480e-05,
         -5.5988e-05,  1.2871e-05],
        [-2.6780e-05, -1.9171e-05,  3.4083e-06,  ..., -2.1885e-05,
         -2.4078e-06, -1.0594e-05],
        [ 5.0052e-05,  3.7189e-05, -5.8828e-06,  ...,  4.3141e-05,
          1.7450e-06,  1.6963e-05],
        [-2.6938e-05, -1.6911e-05,  4.2239e-06,  ..., -2.0413e-05,
         -1.7147e-06, -8.8349e-06],
        [-5.9430e-05, -4.2311e-05,  6.8584e-06,  ..., -4.8206e-05,
         -5.2977e-06, -2.3472e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9653e-08, 2.0039e-08, 3.0200e-08,  ..., 2.2424e-08, 5.2897e-08,
         1.1352e-08],
        [2.8181e-11, 1.8504e-11, 9.2661e-13,  ..., 1.8640e-11, 4.0207e-13,
         3.7613e-12],
        [3.2054e-10, 2.1863e-10, 1.2296e-11,  ..., 3.0020e-10, 4.3084e-12,
         6.1573e-11],
        [1.3905e-10, 2.1156e-10, 4.4812e-12,  ..., 1.1232e-10, 1.6662e-11,
         7.2490e-11],
        [1.1321e-10, 6.6553e-11, 5.5834e-12,  ..., 7.7001e-11, 1.2098e-12,
         1.7080e-11]], device='cuda:0')
optimizer state dict: 33.0
lr: [1.9520977288360243e-05, 1.9520977288360243e-05]
scheduler_last_epoch: 33


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,  1106,   259,  ...,   271,   679,   382]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3076e-05,  1.2001e-05, -2.2436e-05,  ...,  4.1047e-06,
         -1.6542e-06, -3.3916e-05],
        [-2.9802e-06, -2.0266e-06,  3.8929e-07,  ..., -2.5034e-06,
         -2.4401e-07, -1.1325e-06],
        [-3.1292e-06, -2.1309e-06,  4.0978e-07,  ..., -2.6375e-06,
         -2.5705e-07, -1.1921e-06],
        [-4.4703e-06, -3.0249e-06,  5.8487e-07,  ..., -3.7551e-06,
         -3.6322e-07, -1.6913e-06],
        [-7.1228e-06, -4.8280e-06,  9.2760e-07,  ..., -5.9903e-06,
         -5.8487e-07, -2.6971e-06]], device='cuda:0')
Loss: 1.1847444772720337


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5667e-05,  1.4741e-06,  9.8427e-06,  ...,  2.1171e-05,
          1.4058e-05, -5.0273e-05],
        [-6.0052e-06, -4.0680e-06,  8.0280e-07,  ..., -5.0217e-06,
         -4.8801e-07, -2.2873e-06],
        [-6.3479e-06, -4.2915e-06,  8.4750e-07,  ..., -5.3048e-06,
         -5.1595e-07, -2.4140e-06],
        [-9.0003e-06, -6.0797e-06,  1.2070e-06,  ..., -7.5400e-06,
         -7.2643e-07, -3.4198e-06],
        [-1.4365e-05, -9.7156e-06,  1.9185e-06,  ..., -1.2040e-05,
         -1.1697e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.1792473793029785


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,   266,  2374,  ...,  1551,   518,   638],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0317e-06, -1.0364e-06,  1.1520e-05,  ...,  2.1165e-05,
          2.7534e-05, -1.0362e-05],
        [-9.0450e-06, -6.1095e-06,  1.2238e-06,  ..., -7.5549e-06,
         -7.9162e-07, -3.4198e-06],
        [-9.5218e-06, -6.4373e-06,  1.2871e-06,  ..., -7.9572e-06,
         -8.3260e-07, -3.5986e-06],
        [-1.3530e-05, -9.1344e-06,  1.8366e-06,  ..., -1.1325e-05,
         -1.1772e-06, -5.1111e-06],
        [-2.1487e-05, -1.4514e-05,  2.9020e-06,  ..., -1.7971e-05,
         -1.8813e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1505825519561768


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[    0,   300,  5201,  ...,  1997,  7423,   417],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4473e-06, -1.5195e-06,  4.1054e-06,  ...,  4.3463e-05,
          2.2354e-05,  6.3428e-06],
        [-1.2040e-05, -8.1211e-06,  1.6633e-06,  ..., -1.0058e-05,
         -1.0245e-06, -4.5672e-06],
        [-1.2666e-05, -8.5384e-06,  1.7490e-06,  ..., -1.0595e-05,
         -1.0766e-06, -4.8056e-06],
        [-1.8030e-05, -1.2144e-05,  2.4959e-06,  ..., -1.5080e-05,
         -1.5255e-06, -6.8322e-06],
        [-2.8580e-05, -1.9252e-05,  3.9376e-06,  ..., -2.3901e-05,
         -2.4326e-06, -1.0818e-05]], device='cuda:0')
Loss: 1.1846184730529785


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4473e-06, -4.0820e-06,  2.8313e-06,  ...,  4.4929e-05,
          2.9108e-05, -1.5593e-07],
        [-1.5050e-05, -1.0163e-05,  2.0787e-06,  ..., -1.2577e-05,
         -1.2796e-06, -5.7071e-06],
        [-1.5795e-05, -1.0654e-05,  2.1812e-06,  ..., -1.3217e-05,
         -1.3411e-06, -5.9903e-06],
        [-2.2560e-05, -1.5214e-05,  3.1255e-06,  ..., -1.8895e-05,
         -1.9073e-06, -8.5533e-06],
        [-3.5733e-05, -2.4080e-05,  4.9211e-06,  ..., -2.9892e-05,
         -3.0361e-06, -1.3530e-05]], device='cuda:0')
Loss: 1.1749932765960693


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   518,  9048,  ...,  1354,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3827e-05, -1.5786e-05,  1.3717e-05,  ...,  3.2382e-05,
          3.8682e-05,  3.0328e-05],
        [-1.8016e-05, -1.2189e-05,  2.5090e-06,  ..., -1.5050e-05,
         -1.5199e-06, -6.8620e-06],
        [-1.8895e-05, -1.2770e-05,  2.6319e-06,  ..., -1.5810e-05,
         -1.5944e-06, -7.1973e-06],
        [-2.7031e-05, -1.8269e-05,  3.7774e-06,  ..., -2.2620e-05,
         -2.2687e-06, -1.0297e-05],
        [-4.2766e-05, -2.8878e-05,  5.9418e-06,  ..., -3.5763e-05,
         -3.6098e-06, -1.6257e-05]], device='cuda:0')
Loss: 1.168382167816162


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0,  1485,   271,  ...,  6359,  1799,  5442],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9444e-05, -1.4918e-05,  1.6500e-06,  ...,  1.4599e-05,
          5.4774e-05,  2.2861e-05],
        [-2.1011e-05, -1.4260e-05,  2.9206e-06,  ..., -1.7583e-05,
         -1.7770e-06, -8.0243e-06],
        [-2.2024e-05, -1.4931e-05,  3.0641e-06,  ..., -1.8448e-05,
         -1.8626e-06, -8.4117e-06],
        [-3.1501e-05, -2.1353e-05,  4.3958e-06,  ..., -2.6405e-05,
         -2.6505e-06, -1.2025e-05],
        [-4.9829e-05, -3.3766e-05,  6.9104e-06,  ..., -4.1723e-05,
         -4.2170e-06, -1.8984e-05]], device='cuda:0')
Loss: 1.173697590827942


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  328, 6875,  ...,  369,  654,  300],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5422e-05, -1.3373e-05, -8.7538e-06,  ...,  2.2664e-05,
          6.7673e-05,  5.9512e-05],
        [-2.4006e-05, -1.6287e-05,  3.3379e-06,  ..., -2.0087e-05,
         -2.0247e-06, -9.1568e-06],
        [-2.5183e-05, -1.7062e-05,  3.5036e-06,  ..., -2.1085e-05,
         -2.1216e-06, -9.6038e-06],
        [-3.6031e-05, -2.4408e-05,  5.0291e-06,  ..., -3.0175e-05,
         -3.0212e-06, -1.3731e-05],
        [-5.6982e-05, -3.8594e-05,  7.9013e-06,  ..., -4.7684e-05,
         -4.8093e-06, -2.1681e-05]], device='cuda:0')
Loss: 1.1763041019439697
Graident accumulation at epoch 0, step 271, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0035,  ..., -0.0031,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0166,  0.0145, -0.0270,  ...,  0.0281, -0.0157, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0447e-04, -4.7249e-05,  3.4188e-06,  ...,  4.6798e-05,
         -4.3622e-05,  1.7535e-05],
        [-2.6502e-05, -1.8883e-05,  3.4012e-06,  ..., -2.1705e-05,
         -2.3695e-06, -1.0450e-05],
        [ 4.2529e-05,  3.1764e-05, -4.9441e-06,  ...,  3.6718e-05,
          1.3584e-06,  1.4306e-05],
        [-2.7848e-05, -1.7661e-05,  4.3044e-06,  ..., -2.1389e-05,
         -1.8453e-06, -9.3245e-06],
        [-5.9186e-05, -4.1939e-05,  6.9627e-06,  ..., -4.8154e-05,
         -5.2489e-06, -2.3293e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9595e-08, 2.0019e-08, 3.0170e-08,  ..., 2.2402e-08, 5.2848e-08,
         1.1344e-08],
        [2.8729e-11, 1.8751e-11, 9.3682e-13,  ..., 1.9025e-11, 4.0576e-13,
         3.8414e-12],
        [3.2086e-10, 2.1871e-10, 1.2296e-11,  ..., 3.0035e-10, 4.3086e-12,
         6.1604e-11],
        [1.4021e-10, 2.1195e-10, 4.5020e-12,  ..., 1.1312e-10, 1.6655e-11,
         7.2606e-11],
        [1.1635e-10, 6.7976e-11, 5.6403e-12,  ..., 7.9198e-11, 1.2317e-12,
         1.7533e-11]], device='cuda:0')
optimizer state dict: 34.0
lr: [1.9482449598960544e-05, 1.9482449598960544e-05]
scheduler_last_epoch: 34


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4565e-05,  1.9635e-05,  1.2016e-05,  ..., -1.1922e-05,
         -7.6895e-06,  3.1925e-05],
        [-2.9802e-06, -2.0266e-06,  4.3400e-07,  ..., -2.4587e-06,
         -3.0361e-07, -1.1027e-06],
        [-3.1590e-06, -2.1458e-06,  4.5821e-07,  ..., -2.6077e-06,
         -3.2037e-07, -1.1697e-06],
        [-4.4703e-06, -3.0398e-06,  6.4820e-07,  ..., -3.6806e-06,
         -4.5262e-07, -1.6540e-06],
        [-7.0333e-06, -4.7684e-06,  1.0133e-06,  ..., -5.7817e-06,
         -7.1526e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1441093683242798


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 7185,  328,  ..., 1427, 1477, 1061],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4693e-06,  1.0579e-05,  4.2891e-05,  ...,  1.0964e-05,
         -9.4799e-06,  3.3769e-05],
        [-5.9456e-06, -4.0382e-06,  8.6240e-07,  ..., -4.9323e-06,
         -5.7742e-07, -2.1905e-06],
        [-6.3032e-06, -4.2766e-06,  9.1270e-07,  ..., -5.2303e-06,
         -6.1095e-07, -2.3246e-06],
        [-8.9109e-06, -6.0648e-06,  1.2927e-06,  ..., -7.4059e-06,
         -8.6427e-07, -3.2932e-06],
        [-1.4037e-05, -9.5367e-06,  2.0266e-06,  ..., -1.1623e-05,
         -1.3672e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.1708481311798096


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1097e-05,  7.0076e-06,  6.1646e-05,  ...,  3.5951e-05,
          7.4463e-06,  6.7908e-05],
        [-8.9109e-06, -6.0648e-06,  1.3132e-06,  ..., -7.4208e-06,
         -8.6799e-07, -3.3304e-06],
        [-9.4324e-06, -6.4224e-06,  1.3895e-06,  ..., -7.8529e-06,
         -9.1828e-07, -3.5316e-06],
        [-1.3381e-05, -9.1344e-06,  1.9781e-06,  ..., -1.1161e-05,
         -1.3020e-06, -5.0142e-06],
        [-2.1011e-05, -1.4305e-05,  3.0845e-06,  ..., -1.7464e-05,
         -2.0526e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.1712086200714111


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0, 26473,  2117,  ...,    13,  3292,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9169e-05, -4.8140e-06,  3.7657e-05,  ...,  4.3129e-05,
         -2.6554e-05,  7.9199e-05],
        [-1.1891e-05, -8.0615e-06,  1.7695e-06,  ..., -9.8944e-06,
         -1.1623e-06, -4.4554e-06],
        [-1.2591e-05, -8.5384e-06,  1.8738e-06,  ..., -1.0461e-05,
         -1.2293e-06, -4.7237e-06],
        [-1.7881e-05, -1.2159e-05,  2.6710e-06,  ..., -1.4886e-05,
         -1.7434e-06, -6.7130e-06],
        [-2.8044e-05, -1.9014e-05,  4.1574e-06,  ..., -2.3276e-05,
         -2.7493e-06, -1.0520e-05]], device='cuda:0')
Loss: 1.1608459949493408


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[    0,   685,  2461,  ...,   287,   298,  7943],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3315e-05, -3.4056e-05,  3.7001e-05,  ...,  4.6822e-05,
         -3.9028e-05,  8.7318e-05],
        [-1.4856e-05, -1.0058e-05,  2.1867e-06,  ..., -1.2368e-05,
         -1.4696e-06, -5.5805e-06],
        [-1.5736e-05, -1.0654e-05,  2.3171e-06,  ..., -1.3083e-05,
         -1.5553e-06, -5.9158e-06],
        [-2.2352e-05, -1.5169e-05,  3.3043e-06,  ..., -1.8612e-05,
         -2.2054e-06, -8.4117e-06],
        [-3.5048e-05, -2.3723e-05,  5.1409e-06,  ..., -2.9117e-05,
         -3.4757e-06, -1.3173e-05]], device='cuda:0')
Loss: 1.169668197631836


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1573e-06, -6.4141e-05,  5.3774e-05,  ...,  4.2190e-05,
         -5.4355e-05,  8.7861e-05],
        [-1.7837e-05, -1.2085e-05,  2.6301e-06,  ..., -1.4871e-05,
         -1.7863e-06, -6.7130e-06],
        [-1.8895e-05, -1.2815e-05,  2.7884e-06,  ..., -1.5736e-05,
         -1.8924e-06, -7.1228e-06],
        [-2.6822e-05, -1.8224e-05,  3.9712e-06,  ..., -2.2382e-05,
         -2.6822e-06, -1.0110e-05],
        [-4.2021e-05, -2.8491e-05,  6.1765e-06,  ..., -3.4988e-05,
         -4.2208e-06, -1.5825e-05]], device='cuda:0')
Loss: 1.1932542324066162


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0, 2310,  292,  ...,  462,  508,  586]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8552e-06, -4.8314e-05,  3.5548e-05,  ...,  4.8165e-05,
         -4.2758e-05,  7.6957e-05],
        [-2.0772e-05, -1.4096e-05,  3.0678e-06,  ..., -1.7345e-05,
         -2.0843e-06, -7.8306e-06],
        [-2.2024e-05, -1.4946e-05,  3.2540e-06,  ..., -1.8358e-05,
         -2.2091e-06, -8.3148e-06],
        [-3.1263e-05, -2.1249e-05,  4.6343e-06,  ..., -2.6107e-05,
         -3.1311e-06, -1.1802e-05],
        [-4.8965e-05, -3.3230e-05,  7.2047e-06,  ..., -4.0799e-05,
         -4.9248e-06, -1.8463e-05]], device='cuda:0')
Loss: 1.1658101081848145


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  292,   33,  ...,  352,  266, 9129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9305e-05, -7.2321e-06,  3.4492e-05,  ...,  3.7028e-05,
         -4.0677e-05,  1.0886e-04],
        [-2.3767e-05, -1.6123e-05,  3.4906e-06,  ..., -1.9833e-05,
         -2.3805e-06, -8.9556e-06],
        [-2.5168e-05, -1.7077e-05,  3.6992e-06,  ..., -2.0981e-05,
         -2.5202e-06, -9.4995e-06],
        [-3.5763e-05, -2.4304e-05,  5.2750e-06,  ..., -2.9862e-05,
         -3.5763e-06, -1.3500e-05],
        [-5.5999e-05, -3.7998e-05,  8.1956e-06,  ..., -4.6670e-05,
         -5.6215e-06, -2.1100e-05]], device='cuda:0')
Loss: 1.1702834367752075
Graident accumulation at epoch 0, step 279, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0166,  0.0145, -0.0270,  ...,  0.0281, -0.0157, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5957e-05, -4.3247e-05,  6.5262e-06,  ...,  4.5821e-05,
         -4.3328e-05,  2.6667e-05],
        [-2.6229e-05, -1.8607e-05,  3.4102e-06,  ..., -2.1518e-05,
         -2.3706e-06, -1.0301e-05],
        [ 3.5759e-05,  2.6880e-05, -4.0798e-06,  ...,  3.0948e-05,
          9.7052e-07,  1.1926e-05],
        [-2.8639e-05, -1.8325e-05,  4.4015e-06,  ..., -2.2236e-05,
         -2.0184e-06, -9.7421e-06],
        [-5.8867e-05, -4.1545e-05,  7.0860e-06,  ..., -4.8005e-05,
         -5.2861e-06, -2.3074e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9535e-08, 1.9999e-08, 3.0141e-08,  ..., 2.2381e-08, 5.2797e-08,
         1.1344e-08],
        [2.9265e-11, 1.8992e-11, 9.4807e-13,  ..., 1.9399e-11, 4.1102e-13,
         3.9177e-12],
        [3.2117e-10, 2.1878e-10, 1.2298e-11,  ..., 3.0049e-10, 4.3107e-12,
         6.1632e-11],
        [1.4135e-10, 2.1232e-10, 4.5254e-12,  ..., 1.1390e-10, 1.6651e-11,
         7.2716e-11],
        [1.1936e-10, 6.9352e-11, 5.7018e-12,  ..., 8.1297e-11, 1.2621e-12,
         1.7961e-11]], device='cuda:0')
optimizer state dict: 35.0
lr: [1.9442472908488653e-05, 1.9442472908488653e-05]
scheduler_last_epoch: 35


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4228e-06, -1.1388e-05, -1.3831e-05,  ..., -7.7276e-07,
         -1.7389e-06,  3.0943e-05],
        [-2.9802e-06, -2.0415e-06,  5.0291e-07,  ..., -2.5034e-06,
         -3.6694e-07, -1.1176e-06],
        [-3.1441e-06, -2.1607e-06,  5.3272e-07,  ..., -2.6375e-06,
         -3.8743e-07, -1.1846e-06],
        [-4.4405e-06, -3.0547e-06,  7.5623e-07,  ..., -3.7402e-06,
         -5.4762e-07, -1.6764e-06],
        [-6.9141e-06, -4.7386e-06,  1.1697e-06,  ..., -5.8115e-06,
         -8.5309e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1709996461868286


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[   0, 1850,  311,  ..., 3655, 3133, 9000],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7608e-06, -4.7237e-05, -4.4373e-05,  ..., -3.3170e-07,
          2.5044e-06,  5.3319e-05],
        [-5.9754e-06, -4.0978e-06,  9.9093e-07,  ..., -5.0068e-06,
         -7.3947e-07, -2.2426e-06],
        [ 3.6547e-04,  2.6432e-04, -5.8848e-05,  ...,  2.9377e-04,
          3.9504e-05,  1.0909e-04],
        [-8.8811e-06, -6.1244e-06,  1.4827e-06,  ..., -7.4506e-06,
         -1.0990e-06, -3.3453e-06],
        [-1.3858e-05, -9.5367e-06,  2.3022e-06,  ..., -1.1623e-05,
         -1.7174e-06, -5.2154e-06]], device='cuda:0')
Loss: 1.1694492101669312


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4019e-05, -4.0050e-05, -3.6962e-05,  ...,  5.8116e-06,
         -4.4685e-05,  4.7222e-05],
        [-8.9258e-06, -6.1095e-06,  1.4659e-06,  ..., -7.4953e-06,
         -1.0859e-06, -3.3677e-06],
        [ 3.6235e-04,  2.6221e-04, -5.8345e-05,  ...,  2.9113e-04,
          3.9139e-05,  1.0790e-04],
        [-1.3292e-05, -9.1344e-06,  2.1942e-06,  ..., -1.1176e-05,
         -1.6168e-06, -5.0291e-06],
        [-2.0713e-05, -1.4216e-05,  3.4049e-06,  ..., -1.7405e-05,
         -2.5220e-06, -7.8231e-06]], device='cuda:0')
Loss: 1.1647324562072754


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0,  461,  654,  ..., 6548, 7171,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1697e-05, -7.2542e-05, -4.2565e-05,  ..., -1.9259e-05,
         -6.2399e-05,  6.4276e-05],
        [-1.1921e-05, -8.1509e-06,  1.9297e-06,  ..., -9.9689e-06,
         -1.4696e-06, -4.4703e-06],
        [ 3.5918e-04,  2.6005e-04, -5.7853e-05,  ...,  2.8851e-04,
          3.8733e-05,  1.0673e-04],
        [-1.7762e-05, -1.2189e-05,  2.8871e-06,  ..., -1.4856e-05,
         -2.1867e-06, -6.6757e-06],
        [-2.7657e-05, -1.8954e-05,  4.4778e-06,  ..., -2.3127e-05,
         -3.4124e-06, -1.0386e-05]], device='cuda:0')
Loss: 1.1603610515594482


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1322e-05, -1.0641e-04, -4.4431e-05,  ..., -7.5996e-06,
         -4.1549e-05,  9.9166e-05],
        [ 7.2850e-05,  4.6858e-05, -1.6445e-05,  ...,  6.2705e-05,
          2.0780e-05,  3.3030e-05],
        [ 5.5667e-04,  3.9205e-04, -8.3088e-05,  ...,  4.2552e-04,
          5.9880e-05,  1.7002e-04],
        [-2.2233e-05, -1.5244e-05,  3.6210e-06,  ..., -1.8612e-05,
         -2.7642e-06, -8.3596e-06],
        [-3.4660e-05, -2.3723e-05,  5.6177e-06,  ..., -2.8998e-05,
         -4.3176e-06, -1.3024e-05]], device='cuda:0')
Loss: 1.1849548816680908


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[    0,    14, 49045,  ...,    12,   706,   409],
        [    0,   806,   300,  ...,   360,  4918,  1106]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2359e-06, -9.7166e-05, -5.1627e-05,  ..., -2.1069e-05,
         -3.4092e-05,  1.0433e-04],
        [ 6.9825e-05,  4.4786e-05, -1.5965e-05,  ...,  6.0186e-05,
          2.0389e-05,  3.1913e-05],
        [ 5.5351e-04,  3.8988e-04, -8.2585e-05,  ...,  4.2288e-04,
          5.9471e-05,  1.6885e-04],
        [-2.6703e-05, -1.8299e-05,  4.3325e-06,  ..., -2.2322e-05,
         -3.3416e-06, -1.0014e-05],
        [-4.1634e-05, -2.8491e-05,  6.7279e-06,  ..., -3.4809e-05,
         -5.2191e-06, -1.5602e-05]], device='cuda:0')
Loss: 1.170272707939148


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[  0, 669,  14,  ..., 596, 292, 494],
        [  0, 432, 984,  ..., 287, 496,  14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7209e-05, -6.5387e-05, -7.3867e-05,  ..., -8.0036e-06,
         -2.9421e-05,  1.1482e-04],
        [ 6.6830e-05,  4.2760e-05, -1.5477e-05,  ...,  5.7653e-05,
          2.0029e-05,  3.0780e-05],
        [ 5.5034e-04,  3.8774e-04, -8.2071e-05,  ...,  4.2020e-04,
          5.9091e-05,  1.6765e-04],
        [-3.1143e-05, -2.1309e-05,  5.0552e-06,  ..., -2.6092e-05,
         -3.8743e-06, -1.1690e-05],
        [-4.8608e-05, -3.3200e-05,  7.8604e-06,  ..., -4.0710e-05,
         -6.0536e-06, -1.8239e-05]], device='cuda:0')
Loss: 1.1780985593795776


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  266,  720,  342],
        [   0, 2992,  352,  ...,  259, 2063, 6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2672e-05, -8.5171e-05, -8.6716e-05,  ...,  1.1936e-06,
         -3.0669e-05,  9.8160e-05],
        [ 6.3835e-05,  4.0733e-05, -1.4985e-05,  ...,  5.5165e-05,
          1.9645e-05,  2.9648e-05],
        [ 5.4720e-04,  3.8559e-04, -8.1553e-05,  ...,  4.1758e-04,
          5.8686e-05,  1.6645e-04],
        [-3.5584e-05, -2.4334e-05,  5.7891e-06,  ..., -2.9802e-05,
         -4.4443e-06, -1.3374e-05],
        [-5.5492e-05, -3.7879e-05,  8.9929e-06,  ..., -4.6462e-05,
         -6.9402e-06, -2.0847e-05]], device='cuda:0')
Loss: 1.1713929176330566
Graident accumulation at epoch 0, step 287, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0166,  0.0145, -0.0270,  ...,  0.0281, -0.0156, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.8628e-05, -4.7440e-05, -2.7980e-06,  ...,  4.1358e-05,
         -4.2062e-05,  3.3816e-05],
        [-1.7222e-05, -1.2673e-05,  1.5707e-06,  ..., -1.3850e-05,
         -1.6900e-07, -6.3058e-06],
        [ 8.6903e-05,  6.2751e-05, -1.1827e-05,  ...,  6.9611e-05,
          6.7421e-06,  2.7379e-05],
        [-2.9334e-05, -1.8926e-05,  4.5402e-06,  ..., -2.2993e-05,
         -2.2610e-06, -1.0105e-05],
        [-5.8529e-05, -4.1178e-05,  7.2766e-06,  ..., -4.7851e-05,
         -5.4515e-06, -2.2851e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9476e-08, 1.9986e-08, 3.0118e-08,  ..., 2.2358e-08, 5.2745e-08,
         1.1343e-08],
        [3.3311e-11, 2.0632e-11, 1.1717e-12,  ..., 2.2423e-11, 7.9656e-13,
         4.7928e-12],
        [6.2027e-10, 3.6724e-10, 1.8936e-11,  ..., 4.7456e-10, 7.7505e-12,
         8.9278e-11],
        [1.4247e-10, 2.1270e-10, 4.5543e-12,  ..., 1.1467e-10, 1.6654e-11,
         7.2822e-11],
        [1.2232e-10, 7.0717e-11, 5.7770e-12,  ..., 8.3374e-11, 1.3090e-12,
         1.8377e-11]], device='cuda:0')
optimizer state dict: 36.0
lr: [1.9401053325731837e-05, 1.9401053325731837e-05]
scheduler_last_epoch: 36


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,  1890,   278,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3291e-05, -4.1931e-06, -1.2068e-05,  ..., -3.8243e-06,
          2.1013e-05, -7.2885e-06],
        [-2.9951e-06, -2.0415e-06,  5.2527e-07,  ..., -2.5183e-06,
         -4.3586e-07, -1.1176e-06],
        [-3.1590e-06, -2.1458e-06,  5.5507e-07,  ..., -2.6524e-06,
         -4.6007e-07, -1.1772e-06],
        [-4.4107e-06, -2.9951e-06,  7.7486e-07,  ..., -3.7104e-06,
         -6.4075e-07, -1.6466e-06],
        [-6.8545e-06, -4.6790e-06,  1.1995e-06,  ..., -5.7817e-06,
         -9.9838e-07, -2.5630e-06]], device='cuda:0')
Loss: 1.1483234167099


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,  7849,   278,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3516e-05, -2.2896e-05, -2.3149e-05,  ..., -3.7220e-05,
          3.3107e-05,  1.1835e-06],
        [-5.9605e-06, -4.0829e-06,  1.0952e-06,  ..., -5.0068e-06,
         -8.9779e-07, -2.2501e-06],
        [-6.3181e-06, -4.3064e-06,  1.1623e-06,  ..., -5.3048e-06,
         -9.4809e-07, -2.3842e-06],
        [-8.7619e-06, -5.9903e-06,  1.6168e-06,  ..., -7.3910e-06,
         -1.3188e-06, -3.3155e-06],
        [-1.3649e-05, -9.3281e-06,  2.5034e-06,  ..., -1.1474e-05,
         -2.0564e-06, -5.1558e-06]], device='cuda:0')
Loss: 1.173591136932373


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  3658,   271,  ...,   278,   970,    12],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6870e-05, -1.5482e-05, -1.1333e-05,  ..., -6.0753e-05,
          3.9092e-05,  1.5577e-05],
        [-8.9854e-06, -6.1393e-06,  1.5870e-06,  ..., -7.5251e-06,
         -1.3504e-06, -3.3751e-06],
        [-9.5069e-06, -6.4820e-06,  1.6838e-06,  ..., -7.9572e-06,
         -1.4249e-06, -3.5688e-06],
        [-1.3202e-05, -9.0003e-06,  2.3395e-06,  ..., -1.1086e-05,
         -1.9781e-06, -4.9621e-06],
        [-2.0593e-05, -1.4037e-05,  3.6284e-06,  ..., -1.7256e-05,
         -3.0920e-06, -7.7337e-06]], device='cuda:0')
Loss: 1.158612847328186


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   298,   335,   298],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0164e-05, -7.6376e-06,  5.9009e-06,  ..., -5.1529e-05,
          3.9092e-05, -3.0530e-05],
        [-1.1981e-05, -8.1807e-06,  2.1197e-06,  ..., -1.0028e-05,
         -1.8310e-06, -4.4703e-06],
        [-1.2666e-05, -8.6427e-06,  2.2464e-06,  ..., -1.0595e-05,
         -1.9316e-06, -4.7237e-06],
        [-1.7583e-05, -1.1995e-05,  3.1218e-06,  ..., -1.4737e-05,
         -2.6785e-06, -6.5640e-06],
        [-2.7508e-05, -1.8746e-05,  4.8578e-06,  ..., -2.3007e-05,
         -4.2021e-06, -1.0252e-05]], device='cuda:0')
Loss: 1.1597236394882202


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[   0,  397, 1267,  ..., 1276,  292,  221],
        [   0,   14,  475,  ..., 6895, 5842, 2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7351e-05,  6.1465e-06, -9.9491e-06,  ..., -7.6931e-05,
          4.0170e-05, -5.7415e-05],
        [-1.4976e-05, -1.0252e-05,  2.6450e-06,  ..., -1.2517e-05,
         -2.2575e-06, -5.5730e-06],
        [-1.5810e-05, -1.0818e-05,  2.7977e-06,  ..., -1.3217e-05,
         -2.3786e-06, -5.8860e-06],
        [-2.1964e-05, -1.5020e-05,  3.8967e-06,  ..., -1.8403e-05,
         -3.3006e-06, -8.1882e-06],
        [-3.4332e-05, -2.3454e-05,  6.0573e-06,  ..., -2.8700e-05,
         -5.1707e-06, -1.2770e-05]], device='cuda:0')
Loss: 1.1687629222869873


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,   741,   266,  ...,   271,  5166,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8474e-05, -1.9181e-05,  1.2936e-05,  ..., -8.2525e-05,
          1.3361e-05, -3.6454e-05],
        [-1.7986e-05, -1.2338e-05,  3.1739e-06,  ..., -1.5020e-05,
         -2.7046e-06, -6.6683e-06],
        [-1.8984e-05, -1.3024e-05,  3.3565e-06,  ..., -1.5870e-05,
         -2.8517e-06, -7.0408e-06],
        [-2.6435e-05, -1.8120e-05,  4.6827e-06,  ..., -2.2113e-05,
         -3.9637e-06, -9.8124e-06],
        [-4.1187e-05, -2.8223e-05,  7.2643e-06,  ..., -3.4422e-05,
         -6.1914e-06, -1.5259e-05]], device='cuda:0')
Loss: 1.1633639335632324


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0,  1737,   278,  ...,  2604,   367,  2002]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2974e-05, -4.5186e-05,  8.9794e-06,  ..., -6.7574e-05,
          2.7698e-05, -8.9225e-06],
        [-2.1011e-05, -1.4424e-05,  3.7253e-06,  ..., -1.7524e-05,
         -3.1684e-06, -7.7859e-06],
        [-2.2173e-05, -1.5229e-05,  3.9376e-06,  ..., -1.8507e-05,
         -3.3397e-06, -8.2180e-06],
        [-3.0905e-05, -2.1204e-05,  5.4985e-06,  ..., -2.5809e-05,
         -4.6492e-06, -1.1466e-05],
        [-4.8101e-05, -3.2991e-05,  8.5160e-06,  ..., -4.0114e-05,
         -7.2494e-06, -1.7792e-05]], device='cuda:0')
Loss: 1.1724368333816528


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9693e-05, -6.4473e-05,  4.6309e-05,  ..., -7.4216e-05,
          1.4871e-05, -8.9225e-06],
        [-2.4006e-05, -1.6481e-05,  4.2617e-06,  ..., -1.9997e-05,
         -3.6079e-06, -8.9109e-06],
        [-2.5317e-05, -1.7375e-05,  4.5002e-06,  ..., -2.1100e-05,
         -3.8017e-06, -9.3952e-06],
        [-3.5346e-05, -2.4259e-05,  6.2995e-06,  ..., -2.9489e-05,
         -5.3048e-06, -1.3143e-05],
        [-5.4955e-05, -3.7670e-05,  9.7454e-06,  ..., -4.5776e-05,
         -8.2552e-06, -2.0370e-05]], device='cuda:0')
Loss: 1.1633334159851074
Graident accumulation at epoch 0, step 295, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0156, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4735e-05, -4.9143e-05,  2.1126e-06,  ...,  2.9801e-05,
         -3.6369e-05,  2.9542e-05],
        [-1.7901e-05, -1.3053e-05,  1.8398e-06,  ..., -1.4465e-05,
         -5.1289e-07, -6.5663e-06],
        [ 7.5681e-05,  5.4739e-05, -1.0194e-05,  ...,  6.0540e-05,
          5.6877e-06,  2.3701e-05],
        [-2.9935e-05, -1.9459e-05,  4.7162e-06,  ..., -2.3643e-05,
         -2.5654e-06, -1.0409e-05],
        [-5.8172e-05, -4.0827e-05,  7.5235e-06,  ..., -4.7643e-05,
         -5.7319e-06, -2.2603e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9419e-08, 1.9971e-08, 3.0090e-08,  ..., 2.2342e-08, 5.2693e-08,
         1.1331e-08],
        [3.3854e-11, 2.0883e-11, 1.1887e-12,  ..., 2.2800e-11, 8.0878e-13,
         4.8674e-12],
        [6.2029e-10, 3.6718e-10, 1.8938e-11,  ..., 4.7453e-10, 7.7572e-12,
         8.9277e-11],
        [1.4358e-10, 2.1308e-10, 4.5895e-12,  ..., 1.1543e-10, 1.6665e-11,
         7.2922e-11],
        [1.2522e-10, 7.2066e-11, 5.8662e-12,  ..., 8.5387e-11, 1.3758e-12,
         1.8774e-11]], device='cuda:0')
optimizer state dict: 37.0
lr: [1.9358197179963892e-05, 1.9358197179963892e-05]
scheduler_last_epoch: 37


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,   292,    36,     9],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6678e-05,  1.5897e-05, -8.4855e-06,  ...,  2.2950e-06,
          6.5282e-06, -7.0764e-06],
        [-3.0249e-06, -2.0564e-06,  5.9605e-07,  ..., -2.5183e-06,
         -5.2527e-07, -1.1474e-06],
        [-3.2037e-06, -2.1756e-06,  6.2957e-07,  ..., -2.6524e-06,
         -5.5879e-07, -1.2144e-06],
        [-4.4405e-06, -3.0100e-06,  8.7172e-07,  ..., -3.6657e-06,
         -7.6741e-07, -1.6764e-06],
        [-6.8247e-06, -4.6194e-06,  1.3411e-06,  ..., -5.6624e-06,
         -1.1846e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.146870732307434


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,  9466,    36,  ...,  1795,   437,   874],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7076e-06,  3.4046e-05, -1.8612e-05,  ...,  9.4826e-06,
         -2.2960e-06,  8.8062e-06],
        [-6.0648e-06, -4.0978e-06,  1.2182e-06,  ..., -5.0366e-06,
         -1.0617e-06, -2.2873e-06],
        [-6.4075e-06, -4.3213e-06,  1.2852e-06,  ..., -5.3197e-06,
         -1.1250e-06, -2.4140e-06],
        [-8.8513e-06, -5.9754e-06,  1.7807e-06,  ..., -7.3463e-06,
         -1.5497e-06, -3.3304e-06],
        [-1.3649e-05, -9.2089e-06,  2.7418e-06,  ..., -1.1325e-05,
         -2.3991e-06, -5.1558e-06]], device='cuda:0')
Loss: 1.1629917621612549


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2948e-06,  5.2549e-05, -5.3263e-05,  ...,  7.9249e-06,
         -7.8196e-08, -8.2557e-06],
        [-9.0897e-06, -6.1542e-06,  1.8105e-06,  ..., -7.5698e-06,
         -1.5534e-06, -3.3900e-06],
        [-9.5516e-06, -6.4671e-06,  1.9036e-06,  ..., -7.9721e-06,
         -1.6391e-06, -3.5688e-06],
        [-1.3232e-05, -8.9556e-06,  2.6412e-06,  ..., -1.1042e-05,
         -2.2650e-06, -4.9323e-06],
        [-2.0415e-05, -1.3798e-05,  4.0680e-06,  ..., -1.7017e-05,
         -3.5092e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.1685168743133545


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0,  2344,   271,  ...,  5415,    14,  1075],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5359e-05,  3.2812e-05, -7.2774e-05,  ..., -1.1329e-05,
         -2.5388e-06,  5.6310e-06],
        [-1.2115e-05, -8.2105e-06,  2.4103e-06,  ..., -1.0118e-05,
         -2.0973e-06, -4.5300e-06],
        [-1.2740e-05, -8.6278e-06,  2.5406e-06,  ..., -1.0654e-05,
         -2.2165e-06, -4.7758e-06],
        [-1.7643e-05, -1.1936e-05,  3.5204e-06,  ..., -1.4737e-05,
         -3.0585e-06, -6.5938e-06],
        [-2.7239e-05, -1.8418e-05,  5.4166e-06,  ..., -2.2739e-05,
         -4.7386e-06, -1.0192e-05]], device='cuda:0')
Loss: 1.1756902933120728


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0,  578,  221,  ...,  287, 1254, 4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3810e-05,  6.7613e-05, -1.4777e-04,  ...,  2.6682e-05,
          4.3051e-06,  3.1787e-06],
        [-1.5140e-05, -1.0282e-05,  3.0361e-06,  ..., -1.2636e-05,
         -2.5965e-06, -5.6326e-06],
        [-1.5944e-05, -1.0803e-05,  3.2000e-06,  ..., -1.3307e-05,
         -2.7455e-06, -5.9381e-06],
        [-2.2054e-05, -1.4931e-05,  4.4294e-06,  ..., -1.8388e-05,
         -3.7849e-06, -8.1882e-06],
        [-3.4034e-05, -2.3037e-05,  6.8173e-06,  ..., -2.8372e-05,
         -5.8562e-06, -1.2651e-05]], device='cuda:0')
Loss: 1.1656335592269897


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  9472, 15004,   511],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8563e-05,  7.9446e-05, -1.7743e-04,  ...,  4.9106e-05,
          1.1856e-05, -6.9109e-05],
        [-1.8179e-05, -1.2338e-05,  3.6322e-06,  ..., -1.5140e-05,
         -3.1218e-06, -6.7800e-06],
        [-1.9148e-05, -1.2979e-05,  3.8259e-06,  ..., -1.5944e-05,
         -3.3006e-06, -7.1451e-06],
        [-2.6494e-05, -1.7941e-05,  5.3011e-06,  ..., -2.2054e-05,
         -4.5523e-06, -9.8646e-06],
        [-4.0889e-05, -2.7657e-05,  8.1509e-06,  ..., -3.4004e-05,
         -7.0408e-06, -1.5229e-05]], device='cuda:0')
Loss: 1.1501247882843018


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   401,  3740,  ...,  5980,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8260e-05,  7.8282e-05, -1.4476e-04,  ...,  3.3716e-05,
          1.9957e-05, -5.4619e-05],
        [-2.1219e-05, -1.4424e-05,  4.2282e-06,  ..., -1.7658e-05,
         -3.6359e-06, -7.9051e-06],
        [-2.2337e-05, -1.5169e-05,  4.4517e-06,  ..., -1.8597e-05,
         -3.8408e-06, -8.3223e-06],
        [-3.0935e-05, -2.0981e-05,  6.1728e-06,  ..., -2.5734e-05,
         -5.3048e-06, -1.1504e-05],
        [-4.7743e-05, -3.2336e-05,  9.4995e-06,  ..., -3.9697e-05,
         -8.2105e-06, -1.7762e-05]], device='cuda:0')
Loss: 1.1574565172195435


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  365, 2849,  ...,    9, 3365, 5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2867e-05,  7.4293e-05, -1.1911e-04,  ...,  1.3509e-05,
          4.1945e-05, -7.0537e-05],
        [-2.4259e-05, -1.6481e-05,  4.8503e-06,  ..., -2.0191e-05,
         -4.1462e-06, -9.0450e-06],
        [-2.5526e-05, -1.7330e-05,  5.1074e-06,  ..., -2.1249e-05,
         -4.3735e-06, -9.5144e-06],
        [-3.5346e-05, -2.3976e-05,  7.0818e-06,  ..., -2.9415e-05,
         -6.0424e-06, -1.3158e-05],
        [-5.4598e-05, -3.6955e-05,  1.0900e-05,  ..., -4.5389e-05,
         -9.3579e-06, -2.0310e-05]], device='cuda:0')
Loss: 1.1747674942016602
Graident accumulation at epoch 0, step 303, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0156, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.7548e-05, -3.6800e-05, -1.0009e-05,  ...,  2.8172e-05,
         -2.8537e-05,  1.9535e-05],
        [-1.8537e-05, -1.3396e-05,  2.1408e-06,  ..., -1.5037e-05,
         -8.7623e-07, -6.8141e-06],
        [ 6.5560e-05,  4.7532e-05, -8.6642e-06,  ...,  5.2361e-05,
          4.6816e-06,  2.0380e-05],
        [-3.0476e-05, -1.9911e-05,  4.9527e-06,  ..., -2.4220e-05,
         -2.9131e-06, -1.0684e-05],
        [-5.7815e-05, -4.0440e-05,  7.8612e-06,  ..., -4.7418e-05,
         -6.0945e-06, -2.2374e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9360e-08, 1.9956e-08, 3.0074e-08,  ..., 2.2319e-08, 5.2642e-08,
         1.1325e-08],
        [3.4409e-11, 2.1134e-11, 1.2110e-12,  ..., 2.3185e-11, 8.2516e-13,
         4.9444e-12],
        [6.2032e-10, 3.6711e-10, 1.8945e-11,  ..., 4.7450e-10, 7.7685e-12,
         8.9278e-11],
        [1.4469e-10, 2.1344e-10, 4.6350e-12,  ..., 1.1618e-10, 1.6685e-11,
         7.3022e-11],
        [1.2808e-10, 7.3359e-11, 5.9791e-12,  ..., 8.7361e-11, 1.4620e-12,
         1.9168e-11]], device='cuda:0')
optimizer state dict: 38.0
lr: [1.9313911019977992e-05, 1.9313911019977992e-05]
scheduler_last_epoch: 38


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,   377,   472,  ...,  9256,  3807,  5499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9289e-05, -8.6347e-06, -1.2991e-05,  ...,  4.0011e-06,
         -1.0541e-05, -2.9027e-05],
        [-3.0249e-06, -2.0713e-06,  6.6310e-07,  ..., -2.5630e-06,
         -5.7369e-07, -1.1474e-06],
        [-3.1590e-06, -2.1607e-06,  6.9290e-07,  ..., -2.6673e-06,
         -5.9977e-07, -1.1995e-06],
        [-4.3511e-06, -2.9802e-06,  9.5367e-07,  ..., -3.6806e-06,
         -8.2329e-07, -1.6466e-06],
        [-6.6757e-06, -4.5598e-06,  1.4603e-06,  ..., -5.6326e-06,
         -1.2666e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.1695140600204468


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[    0, 10565,  2677,  ...,   298,   292, 11188],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3540e-05, -1.7306e-06, -2.5440e-05,  ...,  8.4439e-06,
         -2.9763e-05, -3.0568e-05],
        [-6.0499e-06, -4.1425e-06,  1.3076e-06,  ..., -5.0962e-06,
         -1.1623e-06, -2.2873e-06],
        [-6.3181e-06, -4.3213e-06,  1.3709e-06,  ..., -5.3197e-06,
         -1.2182e-06, -2.3916e-06],
        [-8.7321e-06, -5.9754e-06,  1.8924e-06,  ..., -7.3463e-06,
         -1.6764e-06, -3.3006e-06],
        [-1.3411e-05, -9.1791e-06,  2.8983e-06,  ..., -1.1265e-05,
         -2.5779e-06, -5.0515e-06]], device='cuda:0')
Loss: 1.156489372253418


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4362e-05,  2.8820e-05, -2.7948e-05,  ..., -1.4046e-05,
         -4.5127e-06, -6.4902e-05],
        [-9.0450e-06, -6.2138e-06,  1.9819e-06,  ..., -7.6443e-06,
         -1.7732e-06, -3.4198e-06],
        [-9.4622e-06, -6.4820e-06,  2.0750e-06,  ..., -7.9870e-06,
         -1.8589e-06, -3.5763e-06],
        [-1.3113e-05, -8.9854e-06,  2.8759e-06,  ..., -1.1057e-05,
         -2.5667e-06, -4.9472e-06],
        [-2.0087e-05, -1.3769e-05,  4.3884e-06,  ..., -1.6928e-05,
         -3.9339e-06, -7.5698e-06]], device='cuda:0')
Loss: 1.1584901809692383


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   292,    65,  ...,    12,   857,   344],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1883e-05,  3.2434e-05, -3.3532e-05,  ..., -2.7423e-06,
         -6.6558e-06, -4.9702e-05],
        [-1.2115e-05, -8.2999e-06,  2.6487e-06,  ..., -1.0192e-05,
         -2.3693e-06, -4.5672e-06],
        [-1.2666e-05, -8.6725e-06,  2.7716e-06,  ..., -1.0654e-05,
         -2.4810e-06, -4.7684e-06],
        [-1.7554e-05, -1.2010e-05,  3.8445e-06,  ..., -1.4752e-05,
         -3.4310e-06, -6.6087e-06],
        [-2.6911e-05, -1.8418e-05,  5.8636e-06,  ..., -2.2590e-05,
         -5.2676e-06, -1.0118e-05]], device='cuda:0')
Loss: 1.1674062013626099


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0, 1159,  278,  ...,    9,  271,  266],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4681e-05,  1.9279e-06, -3.6822e-05,  ...,  2.4350e-05,
         -4.8552e-05, -2.7499e-05],
        [-1.5125e-05, -1.0371e-05,  3.3155e-06,  ..., -1.2696e-05,
         -2.9467e-06, -5.6997e-06],
        [-1.5840e-05, -1.0848e-05,  3.4720e-06,  ..., -1.3292e-05,
         -3.0920e-06, -5.9605e-06],
        [-2.1935e-05, -1.5005e-05,  4.8131e-06,  ..., -1.8388e-05,
         -4.2729e-06, -8.2552e-06],
        [-3.3587e-05, -2.3007e-05,  7.3463e-06,  ..., -2.8163e-05,
         -6.5565e-06, -1.2636e-05]], device='cuda:0')
Loss: 1.1573790311813354


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[  0, 342, 726,  ...,  12, 895, 367],
        [  0, 278, 266,  ..., 292, 474, 221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5642e-05, -2.2981e-05, -5.2385e-05,  ...,  2.3878e-05,
         -5.5644e-05, -4.1601e-05],
        [-1.8120e-05, -1.2442e-05,  3.9823e-06,  ..., -1.5214e-05,
         -3.5204e-06, -6.8247e-06],
        [-1.8984e-05, -1.3024e-05,  4.1723e-06,  ..., -1.5929e-05,
         -3.6918e-06, -7.1377e-06],
        [-2.6315e-05, -1.8030e-05,  5.7891e-06,  ..., -2.2054e-05,
         -5.1074e-06, -9.8944e-06],
        [-4.0263e-05, -2.7627e-05,  8.8215e-06,  ..., -3.3766e-05,
         -7.8306e-06, -1.5125e-05]], device='cuda:0')
Loss: 1.1572387218475342


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,   586,   940,  ...,  1471,  2612,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5103e-05, -3.5620e-05, -8.3677e-05,  ...,  2.6112e-05,
         -7.2203e-05, -1.7475e-05],
        [-2.1160e-05, -1.4499e-05,  4.6343e-06,  ..., -1.7747e-05,
         -4.1500e-06, -7.9572e-06],
        [-2.2203e-05, -1.5199e-05,  4.8615e-06,  ..., -1.8612e-05,
         -4.3549e-06, -8.3297e-06],
        [-3.0696e-05, -2.0996e-05,  6.7279e-06,  ..., -2.5705e-05,
         -6.0126e-06, -1.1519e-05],
        [-4.7028e-05, -3.2187e-05,  1.0259e-05,  ..., -3.9399e-05,
         -9.2238e-06, -1.7628e-05]], device='cuda:0')
Loss: 1.1659929752349854


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[   0,  401, 9370,  ...,    9,  287,  518],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0548e-06, -6.6756e-05, -7.0232e-05,  ..., -1.3376e-06,
         -9.4241e-05, -7.7262e-06],
        [-2.4199e-05, -1.6585e-05,  5.3234e-06,  ..., -2.0266e-05,
         -4.7460e-06, -9.1046e-06],
        [-2.5406e-05, -1.7390e-05,  5.5842e-06,  ..., -2.1264e-05,
         -4.9807e-06, -9.5367e-06],
        [-3.5107e-05, -2.4006e-05,  7.7263e-06,  ..., -2.9355e-05,
         -6.8769e-06, -1.3180e-05],
        [-5.3793e-05, -3.6806e-05,  1.1787e-05,  ..., -4.5002e-05,
         -1.0550e-05, -2.0176e-05]], device='cuda:0')
Loss: 1.1469255685806274
Graident accumulation at epoch 0, step 311, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0223,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0156, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9488e-05, -3.9795e-05, -1.6032e-05,  ...,  2.5221e-05,
         -3.5108e-05,  1.6808e-05],
        [-1.9103e-05, -1.3715e-05,  2.4591e-06,  ..., -1.5560e-05,
         -1.2632e-06, -7.0432e-06],
        [ 5.6464e-05,  4.1040e-05, -7.2394e-06,  ...,  4.4999e-05,
          3.7154e-06,  1.7388e-05],
        [-3.0939e-05, -2.0320e-05,  5.2301e-06,  ..., -2.4733e-05,
         -3.3095e-06, -1.0934e-05],
        [-5.7412e-05, -4.0077e-05,  8.2537e-06,  ..., -4.7176e-05,
         -6.5401e-06, -2.2154e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9301e-08, 1.9941e-08, 3.0049e-08,  ..., 2.2297e-08, 5.2598e-08,
         1.1314e-08],
        [3.4960e-11, 2.1388e-11, 1.2381e-12,  ..., 2.3573e-11, 8.4686e-13,
         5.0223e-12],
        [6.2035e-10, 3.6705e-10, 1.8957e-11,  ..., 4.7448e-10, 7.7856e-12,
         8.9280e-11],
        [1.4577e-10, 2.1380e-10, 4.6901e-12,  ..., 1.1692e-10, 1.6716e-11,
         7.3123e-11],
        [1.3084e-10, 7.4641e-11, 6.1121e-12,  ..., 8.9299e-11, 1.5718e-12,
         1.9556e-11]], device='cuda:0')
optimizer state dict: 39.0
lr: [1.9268201613085963e-05, 1.9268201613085963e-05]
scheduler_last_epoch: 39


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9798e-06, -1.7099e-05, -1.2247e-05,  ...,  2.3563e-05,
          2.1093e-05,  4.8483e-06],
        [-3.0547e-06, -2.0862e-06,  7.0781e-07,  ..., -2.5481e-06,
         -6.0722e-07, -1.1548e-06],
        [-3.2336e-06, -2.2054e-06,  7.4506e-07,  ..., -2.6822e-06,
         -6.4075e-07, -1.2144e-06],
        [-4.4405e-06, -3.0249e-06,  1.0282e-06,  ..., -3.6955e-06,
         -8.8289e-07, -1.6764e-06],
        [-6.7353e-06, -4.5896e-06,  1.5572e-06,  ..., -5.6028e-06,
         -1.3411e-06, -2.5332e-06]], device='cuda:0')
Loss: 1.1643816232681274


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[    0,  1471,   266,  ...,   525,  5202,   292],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2454e-05, -3.3738e-05, -1.5230e-05,  ...,  2.3395e-05,
         -8.1962e-06, -1.5331e-05],
        [-6.0797e-06, -4.1425e-06,  1.4380e-06,  ..., -5.0664e-06,
         -1.2219e-06, -2.2799e-06],
        [-6.4224e-06, -4.3809e-06,  1.5162e-06,  ..., -5.3495e-06,
         -1.2890e-06, -2.3991e-06],
        [-8.8513e-06, -6.0350e-06,  2.1011e-06,  ..., -7.3910e-06,
         -1.7807e-06, -3.3230e-06],
        [-1.3411e-05, -9.1493e-06,  3.1665e-06,  ..., -1.1176e-05,
         -2.6971e-06, -5.0068e-06]], device='cuda:0')
Loss: 1.1502338647842407


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0, 3978, 2697,  ...,  461, 5955, 3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3139e-06, -2.5626e-05, -3.9885e-05,  ...,  2.5561e-05,
          2.8575e-05, -8.0886e-05],
        [-9.1046e-06, -6.1989e-06,  2.1420e-06,  ..., -7.6443e-06,
         -1.8813e-06, -3.4347e-06],
        [-9.5814e-06, -6.5267e-06,  2.2501e-06,  ..., -8.0466e-06,
         -1.9781e-06, -3.6061e-06],
        [-1.3202e-05, -9.0152e-06,  3.1218e-06,  ..., -1.1116e-05,
         -2.7306e-06, -4.9844e-06],
        [-2.0027e-05, -1.3679e-05,  4.7088e-06,  ..., -1.6838e-05,
         -4.1425e-06, -7.5400e-06]], device='cuda:0')
Loss: 1.181830883026123


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4699e-05,  1.7532e-05, -8.1764e-05,  ...,  2.0974e-05,
          3.8465e-05, -6.8207e-05],
        [-1.2144e-05, -8.2701e-06,  2.8424e-06,  ..., -1.0177e-05,
         -2.4885e-06, -4.5598e-06],
        [ 6.0019e-05,  5.2118e-05, -2.0375e-05,  ...,  7.7784e-05,
          3.2103e-05,  3.8743e-05],
        [-1.7643e-05, -1.2055e-05,  4.1500e-06,  ..., -1.4812e-05,
         -3.6210e-06, -6.6310e-06],
        [-2.6733e-05, -1.8269e-05,  6.2585e-06,  ..., -2.2441e-05,
         -5.4911e-06, -1.0028e-05]], device='cuda:0')
Loss: 1.1510621309280396


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0, 2663,  328,  ...,  342,  266, 1163]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8307e-05, -2.7633e-06, -5.8652e-05,  ...,  1.7297e-05,
          3.5169e-05, -6.8207e-05],
        [-1.5169e-05, -1.0356e-05,  3.5539e-06,  ..., -1.2711e-05,
         -3.0883e-06, -5.6848e-06],
        [ 5.6830e-05,  4.9913e-05, -1.9623e-05,  ...,  7.5117e-05,
          3.1470e-05,  3.7550e-05],
        [-2.2084e-05, -1.5110e-05,  5.1931e-06,  ..., -1.8507e-05,
         -4.5002e-06, -8.2776e-06],
        [-3.3438e-05, -2.2888e-05,  7.8306e-06,  ..., -2.8014e-05,
         -6.8247e-06, -1.2517e-05]], device='cuda:0')
Loss: 1.1547975540161133


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6041e-05,  8.7332e-06, -5.9895e-05,  ...,  1.6480e-05,
          2.6870e-05, -1.8250e-05],
        [-1.8179e-05, -1.2442e-05,  4.2804e-06,  ..., -1.5229e-05,
         -3.7141e-06, -6.8024e-06],
        [ 1.5276e-04,  1.0656e-04, -5.7531e-05,  ...,  1.4395e-04,
          6.7001e-05,  8.1266e-05],
        [-2.6494e-05, -1.8179e-05,  6.2659e-06,  ..., -2.2203e-05,
         -5.4166e-06, -9.9242e-06],
        [-4.0054e-05, -2.7478e-05,  9.4250e-06,  ..., -3.3557e-05,
         -8.2031e-06, -1.4976e-05]], device='cuda:0')
Loss: 1.155562162399292


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4601e-05,  1.1803e-05, -5.2934e-05,  ...,  3.8892e-05,
          3.4520e-05,  1.3723e-05],
        [-2.1234e-05, -1.4558e-05,  5.0105e-06,  ..., -1.7762e-05,
         -4.3735e-06, -7.9274e-06],
        [ 1.4951e-04,  1.0431e-04, -5.6753e-05,  ...,  1.4125e-04,
          6.6301e-05,  8.0074e-05],
        [-3.0935e-05, -2.1234e-05,  7.3314e-06,  ..., -2.5883e-05,
         -6.3702e-06, -1.1556e-05],
        [-4.6760e-05, -3.2097e-05,  1.1027e-05,  ..., -3.9130e-05,
         -9.6485e-06, -1.7434e-05]], device='cuda:0')
Loss: 1.1513175964355469


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4947e-05,  3.1595e-05, -6.1654e-05,  ...,  1.4909e-05,
          5.6828e-06,  2.7023e-05],
        [-2.4244e-05, -1.6600e-05,  5.7071e-06,  ..., -2.0266e-05,
         -5.0031e-06, -9.0748e-06],
        [ 1.4633e-04,  1.0215e-04, -5.6019e-05,  ...,  1.3860e-04,
          6.5638e-05,  7.8860e-05],
        [-3.5346e-05, -2.4244e-05,  8.3596e-06,  ..., -2.9579e-05,
         -7.2978e-06, -1.3247e-05],
        [-5.3465e-05, -3.6657e-05,  1.2577e-05,  ..., -4.4733e-05,
         -1.1057e-05, -1.9997e-05]], device='cuda:0')
Loss: 1.1469842195510864
Graident accumulation at epoch 0, step 319, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0406,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0156, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.6034e-05, -3.2656e-05, -2.0594e-05,  ...,  2.4189e-05,
         -3.1029e-05,  1.7830e-05],
        [-1.9617e-05, -1.4003e-05,  2.7839e-06,  ..., -1.6031e-05,
         -1.6372e-06, -7.2464e-06],
        [ 6.5451e-05,  4.7150e-05, -1.2117e-05,  ...,  5.4358e-05,
          9.9076e-06,  2.3535e-05],
        [-3.1380e-05, -2.0713e-05,  5.5430e-06,  ..., -2.5218e-05,
         -3.7083e-06, -1.1165e-05],
        [-5.7018e-05, -3.9735e-05,  8.6860e-06,  ..., -4.6932e-05,
         -6.9917e-06, -2.1938e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9243e-08, 1.9922e-08, 3.0023e-08,  ..., 2.2275e-08, 5.2546e-08,
         1.1303e-08],
        [3.5513e-11, 2.1642e-11, 1.2695e-12,  ..., 2.3960e-11, 8.7104e-13,
         5.0996e-12],
        [6.4114e-10, 3.7711e-10, 2.2076e-11,  ..., 4.9322e-10, 1.2086e-11,
         9.5409e-11],
        [1.4688e-10, 2.1418e-10, 4.7553e-12,  ..., 1.1768e-10, 1.6752e-11,
         7.3225e-11],
        [1.3357e-10, 7.5910e-11, 6.2641e-12,  ..., 9.1211e-11, 1.6925e-12,
         1.9936e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
Epoch 0 | Batch 319/1048 | Training PPL: 9661.205925358337 | time 30.28303360939026
Saving checkpoint at epoch 0, step 319, batch 319
Epoch 0 | Validation PPL: 10.077746495291187 | Learning rate: 1.9221075944084176e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319, AFTER epoch 0, step 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  3036,   471,  ...,   287,  1906,    12],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-6.6034e-05, -3.2656e-05, -2.0594e-05,  ...,  2.4189e-05,
         -3.1029e-05,  1.7830e-05],
        [-1.9617e-05, -1.4003e-05,  2.7839e-06,  ..., -1.6031e-05,
         -1.6372e-06, -7.2464e-06],
        [ 6.5451e-05,  4.7150e-05, -1.2117e-05,  ...,  5.4358e-05,
          9.9076e-06,  2.3535e-05],
        [-3.1380e-05, -2.0713e-05,  5.5430e-06,  ..., -2.5218e-05,
         -3.7083e-06, -1.1165e-05],
        [-5.7018e-05, -3.9735e-05,  8.6860e-06,  ..., -4.6932e-05,
         -6.9917e-06, -2.1938e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9243e-08, 1.9922e-08, 3.0023e-08,  ..., 2.2275e-08, 5.2546e-08,
         1.1303e-08],
        [3.5513e-11, 2.1642e-11, 1.2695e-12,  ..., 2.3960e-11, 8.7104e-13,
         5.0996e-12],
        [6.4114e-10, 3.7711e-10, 2.2076e-11,  ..., 4.9322e-10, 1.2086e-11,
         9.5409e-11],
        [1.4688e-10, 2.1418e-10, 4.7553e-12,  ..., 1.1768e-10, 1.6752e-11,
         7.3225e-11],
        [1.3357e-10, 7.5910e-11, 6.2641e-12,  ..., 9.1211e-11, 1.6925e-12,
         1.9936e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1740e-06, -6.8921e-06, -2.1118e-05,  ..., -1.6407e-05,
          2.3121e-05,  1.0554e-05],
        [-2.9951e-06, -2.0713e-06,  7.4133e-07,  ..., -2.5332e-06,
         -7.0408e-07, -1.1474e-06],
        [-3.1888e-06, -2.2054e-06,  7.8976e-07,  ..., -2.6971e-06,
         -7.4878e-07, -1.2219e-06],
        [-4.3809e-06, -3.0398e-06,  1.0878e-06,  ..., -3.7104e-06,
         -1.0282e-06, -1.6838e-06],
        [-6.5863e-06, -4.5598e-06,  1.6317e-06,  ..., -5.5730e-06,
         -1.5497e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.1565660238265991


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[   0,  266, 2653,  ...,   29,   16,   14],
        [   0,  591,  953,  ..., 4118, 5750,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2432e-06, -1.6836e-05, -3.5724e-05,  ..., -1.8520e-05,
         -2.1239e-05,  5.9948e-06],
        [-5.9754e-06, -4.1127e-06,  1.4901e-06,  ..., -5.0366e-06,
         -1.3523e-06, -2.2873e-06],
        [-6.3628e-06, -4.3809e-06,  1.5907e-06,  ..., -5.3793e-06,
         -1.4417e-06, -2.4363e-06],
        [-8.7917e-06, -6.0648e-06,  2.1979e-06,  ..., -7.4208e-06,
         -1.9893e-06, -3.3677e-06],
        [-1.3113e-05, -9.0301e-06,  3.2783e-06,  ..., -1.1086e-05,
         -2.9728e-06, -5.0217e-06]], device='cuda:0')
Loss: 1.1449904441833496


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  897,  328,  ...,  908,  696,  688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8185e-05, -3.1638e-05, -6.2136e-05,  ..., -6.8809e-06,
         -5.6388e-05,  2.1366e-05],
        [-9.0003e-06, -6.1989e-06,  2.2314e-06,  ..., -7.5847e-06,
         -2.0079e-06, -3.4124e-06],
        [-9.5516e-06, -6.5863e-06,  2.3767e-06,  ..., -8.0764e-06,
         -2.1346e-06, -3.6284e-06],
        [-1.3232e-05, -9.1344e-06,  3.2932e-06,  ..., -1.1161e-05,
         -2.9504e-06, -5.0217e-06],
        [-1.9759e-05, -1.3620e-05,  4.9099e-06,  ..., -1.6689e-05,
         -4.4107e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1571062803268433


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5235e-05, -1.3642e-05, -7.8817e-05,  ..., -1.5784e-07,
         -5.7069e-05, -1.9948e-05],
        [-1.1981e-05, -8.2552e-06,  2.9914e-06,  ..., -1.0088e-05,
         -2.6524e-06, -4.5300e-06],
        [ 6.9772e-05,  5.3267e-05, -2.4274e-05,  ...,  6.7980e-05,
          2.6218e-05,  2.8134e-05],
        [-1.7643e-05, -1.2189e-05,  4.4182e-06,  ..., -1.4871e-05,
         -3.9041e-06, -6.6757e-06],
        [-2.6256e-05, -1.8120e-05,  6.5640e-06,  ..., -2.2173e-05,
         -5.8189e-06, -9.9391e-06]], device='cuda:0')
Loss: 1.1563949584960938


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,  6067,  1188,  ...,  5282,   756,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4143e-05, -2.0004e-05, -8.6145e-05,  ..., -4.4385e-05,
         -6.1843e-05, -8.5935e-06],
        [-1.4961e-05, -1.0312e-05,  3.7253e-06,  ..., -1.2621e-05,
         -3.3155e-06, -5.6475e-06],
        [ 6.6598e-05,  5.1092e-05, -2.3495e-05,  ...,  6.5283e-05,
          2.5514e-05,  2.6942e-05],
        [-2.2054e-05, -1.5214e-05,  5.4985e-06,  ..., -1.8612e-05,
         -4.8801e-06, -8.3223e-06],
        [-3.2842e-05, -2.2620e-05,  8.1733e-06,  ..., -2.7746e-05,
         -7.2792e-06, -1.2398e-05]], device='cuda:0')
Loss: 1.1582653522491455


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4143e-05,  1.2763e-05, -3.3527e-05,  ..., -3.7067e-05,
         -6.2493e-05,  1.8547e-05],
        [-1.7986e-05, -1.2383e-05,  4.4629e-06,  ..., -1.5140e-05,
         -3.9749e-06, -6.7875e-06],
        [ 6.3365e-05,  4.8871e-05, -2.2709e-05,  ...,  6.2586e-05,
          2.4806e-05,  2.5720e-05],
        [-2.6524e-05, -1.8269e-05,  6.5863e-06,  ..., -2.2337e-05,
         -5.8562e-06, -1.0014e-05],
        [-3.9488e-05, -2.7180e-05,  9.7901e-06,  ..., -3.3289e-05,
         -8.7321e-06, -1.4901e-05]], device='cuda:0')
Loss: 1.1546097993850708


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[   0,  462,  221,  ...,   29,  413, 1801],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4480e-05,  2.3672e-06, -4.5450e-05,  ..., -2.7602e-05,
         -8.3725e-05,  1.4760e-05],
        [-2.0981e-05, -1.4439e-05,  5.2229e-06,  ..., -1.7658e-05,
         -4.6343e-06, -7.9349e-06],
        [ 6.0146e-05,  4.6666e-05, -2.1893e-05,  ...,  5.9889e-05,
          2.4098e-05,  2.4491e-05],
        [-3.0965e-05, -2.1324e-05,  7.7188e-06,  ..., -2.6062e-05,
         -6.8322e-06, -1.1712e-05],
        [-4.6074e-05, -3.1710e-05,  1.1459e-05,  ..., -3.8832e-05,
         -1.0185e-05, -1.7419e-05]], device='cuda:0')
Loss: 1.1484636068344116


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0, 2018, 4798,  ...,  292, 1919,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2951e-05,  8.8218e-07, -4.3526e-05,  ..., -4.4030e-05,
         -1.0836e-04, -1.4958e-05],
        [-2.3931e-05, -1.6481e-05,  5.9865e-06,  ..., -2.0161e-05,
         -5.2825e-06, -9.0748e-06],
        [ 5.6987e-05,  4.4475e-05, -2.1074e-05,  ...,  5.7206e-05,
          2.3402e-05,  2.3269e-05],
        [-3.5375e-05, -2.4378e-05,  8.8662e-06,  ..., -2.9802e-05,
         -7.8008e-06, -1.3418e-05],
        [-5.2571e-05, -3.6210e-05,  1.3135e-05,  ..., -4.4316e-05,
         -1.1608e-05, -1.9923e-05]], device='cuda:0')
Loss: 1.1499038934707642
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2725e-05, -2.9302e-05, -2.2887e-05,  ...,  1.7368e-05,
         -3.8761e-05,  1.4551e-05],
        [-2.0048e-05, -1.4251e-05,  3.1042e-06,  ..., -1.6444e-05,
         -2.0017e-06, -7.4292e-06],
        [ 6.4604e-05,  4.6883e-05, -1.3013e-05,  ...,  5.4643e-05,
          1.1257e-05,  2.3509e-05],
        [-3.1779e-05, -2.1079e-05,  5.8753e-06,  ..., -2.5676e-05,
         -4.1176e-06, -1.1390e-05],
        [-5.6573e-05, -3.9382e-05,  9.1310e-06,  ..., -4.6670e-05,
         -7.4533e-06, -2.1737e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9184e-08, 1.9902e-08, 2.9995e-08,  ..., 2.2255e-08, 5.2505e-08,
         1.1292e-08],
        [3.6050e-11, 2.1892e-11, 1.3040e-12,  ..., 2.4342e-11, 8.9808e-13,
         5.1769e-12],
        [6.4375e-10, 3.7871e-10, 2.2498e-11,  ..., 4.9599e-10, 1.2622e-11,
         9.5855e-11],
        [1.4798e-10, 2.1456e-10, 4.8291e-12,  ..., 1.1845e-10, 1.6796e-11,
         7.3332e-11],
        [1.3620e-10, 7.7145e-11, 6.4304e-12,  ..., 9.3084e-11, 1.8256e-12,
         2.0313e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.9172541214186228e-05, 1.9172541214186228e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0, 12456,    14,  ...,  1822,  1016,   365],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8856e-05,  3.9725e-05, -1.3340e-06,  ...,  1.2861e-05,
          1.8204e-05,  1.3412e-05],
        [-2.9951e-06, -2.0862e-06,  7.4133e-07,  ..., -2.5183e-06,
         -7.0408e-07, -1.1101e-06],
        [-3.2187e-06, -2.2352e-06,  7.9349e-07,  ..., -2.7120e-06,
         -7.5251e-07, -1.1921e-06],
        [-4.4107e-06, -3.0696e-06,  1.0952e-06,  ..., -3.7253e-06,
         -1.0356e-06, -1.6391e-06],
        [-6.5863e-06, -4.5598e-06,  1.6242e-06,  ..., -5.5432e-06,
         -1.5423e-06, -2.4289e-06]], device='cuda:0')
Loss: 1.1527483463287354


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  1853,    12,   709],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5053e-05,  3.8517e-05, -1.2086e-05,  ..., -1.1055e-05,
          7.7430e-06, -2.4851e-05],
        [-5.9307e-06, -4.1276e-06,  1.5013e-06,  ..., -5.0515e-06,
         -1.3821e-06, -2.2128e-06],
        [-6.4075e-06, -4.4554e-06,  1.6168e-06,  ..., -5.4538e-06,
         -1.4864e-06, -2.3916e-06],
        [-8.8215e-06, -6.1393e-06,  2.2352e-06,  ..., -7.5102e-06,
         -2.0489e-06, -3.2932e-06],
        [-1.3083e-05, -9.0897e-06,  3.3006e-06,  ..., -1.1116e-05,
         -3.0398e-06, -4.8727e-06]], device='cuda:0')
Loss: 1.1714617013931274


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7624e-05,  1.4769e-05, -4.8441e-05,  ..., -2.1421e-06,
          2.4882e-05, -2.1306e-05],
        [-8.8960e-06, -6.1840e-06,  2.2613e-06,  ..., -7.5400e-06,
         -2.0675e-06, -3.3155e-06],
        [-9.6411e-06, -6.7055e-06,  2.4475e-06,  ..., -8.1807e-06,
         -2.2352e-06, -3.5986e-06],
        [-1.3262e-05, -9.2238e-06,  3.3751e-06,  ..., -1.1235e-05,
         -3.0696e-06, -4.9472e-06],
        [-1.9640e-05, -1.3649e-05,  4.9770e-06,  ..., -1.6630e-05,
         -4.5523e-06, -7.3165e-06]], device='cuda:0')
Loss: 1.1366559267044067


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,     9,   287,  ...,   259,  8244,  1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1960e-05, -4.3951e-06, -7.0975e-05,  ..., -1.9574e-05,
          2.6155e-05, -3.3017e-06],
        [-1.1846e-05, -8.2552e-06,  3.0510e-06,  ..., -1.0058e-05,
         -2.7493e-06, -4.4107e-06],
        [-1.2830e-05, -8.9407e-06,  3.3006e-06,  ..., -1.0893e-05,
         -2.9691e-06, -4.7833e-06],
        [-1.7703e-05, -1.2338e-05,  4.5672e-06,  ..., -1.5020e-05,
         -4.0904e-06, -6.5938e-06],
        [-2.6107e-05, -1.8209e-05,  6.7055e-06,  ..., -2.2143e-05,
         -6.0499e-06, -9.7156e-06]], device='cuda:0')
Loss: 1.1493918895721436


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9003e-05,  8.8235e-06, -6.1041e-05,  ..., -4.7907e-06,
          1.4810e-05,  1.6764e-05],
        [-1.4812e-05, -1.0327e-05,  3.8408e-06,  ..., -1.2577e-05,
         -3.4571e-06, -5.5060e-06],
        [-1.6034e-05, -1.1176e-05,  4.1574e-06,  ..., -1.3620e-05,
         -3.7327e-06, -5.9754e-06],
        [-2.2143e-05, -1.5453e-05,  5.7593e-06,  ..., -1.8790e-05,
         -5.1484e-06, -8.2403e-06],
        [-3.2663e-05, -2.2799e-05,  8.4564e-06,  ..., -2.7716e-05,
         -7.6145e-06, -1.2144e-05]], device='cuda:0')
Loss: 1.1504765748977661


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[    0,    34,  3881,  ...,  1027,   271,   266],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3819e-05, -2.7488e-05, -3.7450e-05,  ..., -1.1475e-05,
         -3.8647e-05, -4.8941e-06],
        [-1.7732e-05, -1.2383e-05,  4.6007e-06,  ..., -1.5065e-05,
         -4.0978e-06, -6.5863e-06],
        [-1.9222e-05, -1.3411e-05,  4.9844e-06,  ..., -1.6347e-05,
         -4.4331e-06, -7.1526e-06],
        [-2.6584e-05, -1.8567e-05,  6.9141e-06,  ..., -2.2575e-05,
         -6.1169e-06, -9.8795e-06],
        [-3.9101e-05, -2.7329e-05,  1.0133e-05,  ..., -3.3230e-05,
         -9.0301e-06, -1.4529e-05]], device='cuda:0')
Loss: 1.1536362171173096


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 3795,  908,  587],
        [   0,  368, 2418,  ..., 3275, 1116, 5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8580e-05,  1.0993e-05, -9.3765e-05,  ...,  2.1073e-05,
         -3.9785e-05, -4.2623e-06],
        [-2.0683e-05, -1.4424e-05,  5.3830e-06,  ..., -1.7568e-05,
         -4.7795e-06, -7.7114e-06],
        [-2.2441e-05, -1.5631e-05,  5.8375e-06,  ..., -1.9073e-05,
         -5.1782e-06, -8.3819e-06],
        [-3.0994e-05, -2.1622e-05,  8.0839e-06,  ..., -2.6315e-05,
         -7.1377e-06, -1.1563e-05],
        [-4.5598e-05, -3.1829e-05,  1.1854e-05,  ..., -3.8743e-05,
         -1.0535e-05, -1.7002e-05]], device='cuda:0')
Loss: 1.146456003189087


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  3605,  2572,  ...,   300,   259,  1513],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8988e-05,  1.0590e-05, -7.2088e-05,  ..., -1.3793e-05,
         -3.9654e-05, -2.5880e-05],
        [-2.3633e-05, -1.6496e-05,  6.1467e-06,  ..., -2.0102e-05,
         -5.4464e-06, -8.7991e-06],
        [-2.5630e-05, -1.7866e-05,  6.6571e-06,  ..., -2.1800e-05,
         -5.8971e-06, -9.5442e-06],
        [-3.5435e-05, -2.4736e-05,  9.2313e-06,  ..., -3.0100e-05,
         -8.1360e-06, -1.3188e-05],
        [-5.2154e-05, -3.6418e-05,  1.3538e-05,  ..., -4.4346e-05,
         -1.2018e-05, -1.9401e-05]], device='cuda:0')
Loss: 1.1641075611114502
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0271,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.9554e-05, -2.5313e-05, -2.7807e-05,  ...,  1.4252e-05,
         -3.8851e-05,  1.0508e-05],
        [-2.0407e-05, -1.4476e-05,  3.4084e-06,  ..., -1.6809e-05,
         -2.3462e-06, -7.5662e-06],
        [ 5.5581e-05,  4.0408e-05, -1.1046e-05,  ...,  4.6999e-05,
          9.5416e-06,  2.0203e-05],
        [-3.2145e-05, -2.1445e-05,  6.2109e-06,  ..., -2.6119e-05,
         -4.5194e-06, -1.1570e-05],
        [-5.6131e-05, -3.9086e-05,  9.5716e-06,  ..., -4.6438e-05,
         -7.9098e-06, -2.1503e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9130e-08, 1.9882e-08, 2.9970e-08,  ..., 2.2233e-08, 5.2454e-08,
         1.1282e-08],
        [3.6572e-11, 2.2142e-11, 1.3405e-12,  ..., 2.4722e-11, 9.2684e-13,
         5.2491e-12],
        [6.4376e-10, 3.7865e-10, 2.2520e-11,  ..., 4.9597e-10, 1.2644e-11,
         9.5850e-11],
        [1.4909e-10, 2.1496e-10, 4.9095e-12,  ..., 1.1924e-10, 1.6846e-11,
         7.3433e-11],
        [1.3879e-10, 7.8394e-11, 6.6072e-12,  ..., 9.4957e-11, 1.9682e-12,
         2.0669e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[  0, 792,  83,  ..., 300, 768, 932],
        [  0,  76,  15,  ...,  14, 333, 199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2201e-06, -1.4786e-06,  1.6626e-05,  ...,  5.6678e-06,
         -6.4427e-06,  1.6409e-05],
        [-2.8908e-06, -2.0266e-06,  7.5996e-07,  ..., -2.4736e-06,
         -6.9290e-07, -1.0878e-06],
        [-3.1888e-06, -2.2352e-06,  8.4192e-07,  ..., -2.7269e-06,
         -7.6368e-07, -1.1995e-06],
        [-4.4405e-06, -3.0994e-06,  1.1697e-06,  ..., -3.7849e-06,
         -1.0580e-06, -1.6615e-06],
        [-6.4075e-06, -4.4703e-06,  1.6913e-06,  ..., -5.4836e-06,
         -1.5348e-06, -2.4140e-06]], device='cuda:0')
Loss: 1.1636883020401


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1042, 5738,  ...,   12,  287, 3643],
        [   0,  775,  266,  ...,  409,  328, 5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8916e-05,  1.9557e-05,  2.8498e-05,  ..., -8.5058e-06,
          2.1852e-05,  2.8641e-05],
        [-5.7966e-06, -4.0531e-06,  1.5236e-06,  ..., -5.0068e-06,
         -1.4007e-06, -2.1979e-06],
        [-6.3777e-06, -4.4703e-06,  1.6764e-06,  ..., -5.4985e-06,
         -1.5423e-06, -2.4214e-06],
        [-8.8513e-06, -6.1989e-06,  2.3320e-06,  ..., -7.6294e-06,
         -2.1383e-06, -3.3528e-06],
        [-1.2755e-05, -8.9109e-06,  3.3602e-06,  ..., -1.1027e-05,
         -3.0845e-06, -4.8429e-06]], device='cuda:0')
Loss: 1.1381300687789917


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,  1235,    14,  ...,  3301,   549,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4162e-06,  4.3048e-05,  1.7015e-05,  ..., -2.6197e-05,
          4.6975e-05,  3.8164e-05],
        [-8.6874e-06, -6.0797e-06,  2.2762e-06,  ..., -7.5102e-06,
         -2.0899e-06, -3.2857e-06],
        [-9.5665e-06, -6.7055e-06,  2.5071e-06,  ..., -8.2552e-06,
         -2.2985e-06, -3.6210e-06],
        [-1.3292e-05, -9.3132e-06,  3.4943e-06,  ..., -1.1474e-05,
         -3.1963e-06, -5.0291e-06],
        [-1.9133e-05, -1.3411e-05,  5.0217e-06,  ..., -1.6570e-05,
         -4.6045e-06, -7.2569e-06]], device='cuda:0')
Loss: 1.1515727043151855


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1938e-05,  4.5277e-05,  1.8886e-05,  ..., -3.7330e-05,
          5.6110e-05,  4.8822e-05],
        [-1.1623e-05, -8.1211e-06,  3.0287e-06,  ..., -1.0058e-05,
         -2.7753e-06, -4.3958e-06],
        [-1.2770e-05, -8.9407e-06,  3.3304e-06,  ..., -1.1027e-05,
         -3.0473e-06, -4.8280e-06],
        [-1.7762e-05, -1.2428e-05,  4.6417e-06,  ..., -1.5348e-05,
         -4.2394e-06, -6.7130e-06],
        [-2.5600e-05, -1.7911e-05,  6.6757e-06,  ..., -2.2143e-05,
         -6.1095e-06, -9.6858e-06]], device='cuda:0')
Loss: 1.153490424156189


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,   300,   344,  ...,    14,  5077,  2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6012e-05,  6.5692e-05,  5.6253e-06,  ..., -5.8176e-05,
          9.0172e-05,  8.3765e-05],
        [-1.4544e-05, -1.0177e-05,  3.7849e-06,  ..., -1.2547e-05,
         -3.4943e-06, -5.5283e-06],
        [-1.6004e-05, -1.1206e-05,  4.1686e-06,  ..., -1.3784e-05,
         -3.8408e-06, -6.0797e-06],
        [-2.2233e-05, -1.5572e-05,  5.8040e-06,  ..., -1.9163e-05,
         -5.3346e-06, -8.4490e-06],
        [-3.2067e-05, -2.2471e-05,  8.3596e-06,  ..., -2.7686e-05,
         -7.7039e-06, -1.2204e-05]], device='cuda:0')
Loss: 1.133782148361206


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 4852,  266,  ..., 2523, 2080, 2632],
        [   0, 1234,  278,  ..., 8635,  271,  546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5934e-06,  3.8341e-05,  7.6326e-06,  ..., -5.3497e-05,
          6.7916e-05,  8.1420e-05],
        [-1.7434e-05, -1.2219e-05,  4.5486e-06,  ..., -1.5050e-05,
         -4.1649e-06, -6.6236e-06],
        [-1.9193e-05, -1.3456e-05,  5.0105e-06,  ..., -1.6555e-05,
         -4.5821e-06, -7.2867e-06],
        [-2.6643e-05, -1.8686e-05,  6.9737e-06,  ..., -2.3007e-05,
         -6.3628e-06, -1.0118e-05],
        [-3.8475e-05, -2.6971e-05,  1.0043e-05,  ..., -3.3230e-05,
         -9.1866e-06, -1.4618e-05]], device='cuda:0')
Loss: 1.1444469690322876


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,  4868,  1027,  ...,   409,  3047,  2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6058e-06,  6.1602e-05, -6.7943e-06,  ..., -4.3205e-05,
          6.0685e-05,  6.9524e-05],
        [-2.0370e-05, -1.4290e-05,  5.3011e-06,  ..., -1.7568e-05,
         -4.8615e-06, -7.7337e-06],
        [-2.2411e-05, -1.5721e-05,  5.8338e-06,  ..., -1.9297e-05,
         -5.3421e-06, -8.5011e-06],
        [-3.1084e-05, -2.1815e-05,  8.1137e-06,  ..., -2.6807e-05,
         -7.4133e-06, -1.1794e-05],
        [-4.4912e-05, -3.1501e-05,  1.1690e-05,  ..., -3.8743e-05,
         -1.0714e-05, -1.7047e-05]], device='cuda:0')
Loss: 1.1479434967041016


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[   0,   12, 1250,  ...,  381, 1524, 2204],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6621e-05,  6.0937e-05, -8.9633e-06,  ..., -3.5738e-05,
          5.8254e-05,  1.2594e-04],
        [-2.3305e-05, -1.6332e-05,  6.0722e-06,  ..., -2.0072e-05,
         -5.5321e-06, -8.8289e-06],
        [-2.5615e-05, -1.7956e-05,  6.6794e-06,  ..., -2.2039e-05,
         -6.0759e-06, -9.7007e-06],
        [-3.5554e-05, -2.4915e-05,  9.2909e-06,  ..., -3.0622e-05,
         -8.4266e-06, -1.3456e-05],
        [-5.1409e-05, -3.6031e-05,  1.3396e-05,  ..., -4.4286e-05,
         -1.2197e-05, -1.9461e-05]], device='cuda:0')
Loss: 1.1637566089630127
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0021],
        [-0.0165,  0.0147, -0.0271,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1937e-05, -1.6688e-05, -2.5923e-05,  ...,  9.2525e-06,
         -2.9140e-05,  2.2051e-05],
        [-2.0697e-05, -1.4661e-05,  3.6748e-06,  ..., -1.7136e-05,
         -2.6648e-06, -7.6925e-06],
        [ 4.7461e-05,  3.4571e-05, -9.2734e-06,  ...,  4.0095e-05,
          7.9798e-06,  1.7213e-05],
        [-3.2486e-05, -2.1792e-05,  6.5189e-06,  ..., -2.6569e-05,
         -4.9101e-06, -1.1759e-05],
        [-5.5659e-05, -3.8780e-05,  9.9541e-06,  ..., -4.6223e-05,
         -8.3385e-06, -2.1299e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9072e-08, 1.9866e-08, 2.9940e-08,  ..., 2.2212e-08, 5.2405e-08,
         1.1286e-08],
        [3.7079e-11, 2.2387e-11, 1.3760e-12,  ..., 2.5100e-11, 9.5652e-13,
         5.3218e-12],
        [6.4377e-10, 3.7860e-10, 2.2542e-11,  ..., 4.9596e-10, 1.2668e-11,
         9.5849e-11],
        [1.5021e-10, 2.1536e-10, 4.9909e-12,  ..., 1.2006e-10, 1.6900e-11,
         7.3540e-11],
        [1.4129e-10, 7.9614e-11, 6.7801e-12,  ..., 9.6823e-11, 2.1150e-12,
         2.1027e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.90712744520069e-05, 1.90712744520069e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[   0,   14,  560,  ..., 1248, 1398, 1268],
        [   0,  474,  221,  ..., 2945,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4125e-05,  2.6063e-05,  0.0000e+00,  ..., -8.8539e-06,
         -3.5586e-05, -9.3466e-06],
        [-2.8312e-06, -2.0117e-06,  7.8231e-07,  ..., -2.4736e-06,
         -7.1526e-07, -1.1101e-06],
        [-3.1739e-06, -2.2650e-06,  8.7917e-07,  ..., -2.7865e-06,
         -8.0466e-07, -1.2517e-06],
        [-4.4405e-06, -3.1739e-06,  1.2368e-06,  ..., -3.9041e-06,
         -1.1250e-06, -1.7434e-06],
        [-6.2883e-06, -4.4703e-06,  1.7360e-06,  ..., -5.5134e-06,
         -1.5870e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.140528678894043


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0,   616,  4935,  ...,    89,  4448,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6542e-05,  6.1951e-05,  1.7438e-05,  ...,  5.7684e-07,
         -1.5118e-05, -2.7703e-05],
        [-5.7220e-06, -4.0382e-06,  1.5460e-06,  ..., -4.9770e-06,
         -1.4678e-06, -2.2650e-06],
        [-6.3926e-06, -4.5300e-06,  1.7323e-06,  ..., -5.5879e-06,
         -1.6466e-06, -2.5406e-06],
        [-8.8513e-06, -6.2734e-06,  2.4065e-06,  ..., -7.7486e-06,
         -2.2724e-06, -3.5092e-06],
        [-1.2726e-05, -9.0003e-06,  3.4347e-06,  ..., -1.1086e-05,
         -3.2634e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.166933298110962


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4562e-05,  6.9001e-05, -1.9659e-05,  ..., -3.9684e-06,
         -4.0378e-06, -4.8594e-05],
        [-8.5533e-06, -6.0648e-06,  2.3432e-06,  ..., -7.4804e-06,
         -2.1867e-06, -3.4198e-06],
        [-9.5218e-06, -6.7651e-06,  2.6152e-06,  ..., -8.3447e-06,
         -2.4438e-06, -3.8147e-06],
        [-1.3292e-05, -9.4324e-06,  3.6582e-06,  ..., -1.1653e-05,
         -3.3975e-06, -5.3123e-06],
        [-1.8954e-05, -1.3441e-05,  5.1856e-06,  ..., -1.6570e-05,
         -4.8503e-06, -7.5698e-06]], device='cuda:0')
Loss: 1.1502561569213867


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  2192,  2534,   287],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8636e-05,  6.9802e-05, -4.3237e-05,  ..., -2.4866e-05,
         -1.7412e-06, -5.5802e-05],
        [-1.1399e-05, -8.1062e-06,  3.1032e-06,  ..., -9.9838e-06,
         -2.8685e-06, -4.5300e-06],
        [-1.2696e-05, -9.0301e-06,  3.4608e-06,  ..., -1.1131e-05,
         -3.2037e-06, -5.0515e-06],
        [-1.7703e-05, -1.2577e-05,  4.8354e-06,  ..., -1.5527e-05,
         -4.4554e-06, -7.0333e-06],
        [-2.5272e-05, -1.7941e-05,  6.8620e-06,  ..., -2.2113e-05,
         -6.3628e-06, -1.0028e-05]], device='cuda:0')
Loss: 1.167754054069519


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[   0,  504,  546,  ...,  634,  328,  630],
        [   0, 1730, 2068,  ...,  445, 2704,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1181e-05,  1.1335e-04, -1.5305e-05,  ...,  2.3227e-05,
         -4.0270e-05, -3.7181e-05],
        [-1.4275e-05, -1.0163e-05,  3.8929e-06,  ..., -1.2502e-05,
         -3.5726e-06, -5.6624e-06],
        [-1.5929e-05, -1.1355e-05,  4.3511e-06,  ..., -1.3962e-05,
         -3.9972e-06, -6.3330e-06],
        [-2.2173e-05, -1.5765e-05,  6.0648e-06,  ..., -1.9431e-05,
         -5.5432e-06, -8.7917e-06],
        [-3.1650e-05, -2.2501e-05,  8.6129e-06,  ..., -2.7686e-05,
         -7.9274e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1552550792694092


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  5885,   271,  ...,   278,  1049,    12],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6525e-05,  1.0687e-04, -1.7038e-05,  ...,  3.1090e-05,
         -3.8530e-05, -5.1879e-05],
        [-1.7136e-05, -1.2204e-05,  4.6343e-06,  ..., -1.5005e-05,
         -4.2580e-06, -6.7726e-06],
        [-1.9118e-05, -1.3620e-05,  5.1782e-06,  ..., -1.6749e-05,
         -4.7609e-06, -7.5698e-06],
        [-2.6643e-05, -1.8939e-05,  7.2271e-06,  ..., -2.3335e-05,
         -6.6087e-06, -1.0528e-05],
        [-3.7998e-05, -2.7001e-05,  1.0259e-05,  ..., -3.3230e-05,
         -9.4473e-06, -1.5005e-05]], device='cuda:0')
Loss: 1.1634266376495361


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0,  360,  259,  ...,   12,  358,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3278e-06,  9.4014e-05,  4.5198e-06,  ...,  3.4562e-05,
         -5.4656e-05, -5.0769e-05],
        [-1.9997e-05, -1.4231e-05,  5.3756e-06,  ..., -1.7494e-05,
         -4.9546e-06, -7.8902e-06],
        [-2.2307e-05, -1.5885e-05,  6.0052e-06,  ..., -1.9535e-05,
         -5.5395e-06, -8.8215e-06],
        [-3.1084e-05, -2.2084e-05,  8.3819e-06,  ..., -2.7210e-05,
         -7.6890e-06, -1.2264e-05],
        [-4.4376e-05, -3.1501e-05,  1.1913e-05,  ..., -3.8803e-05,
         -1.1005e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.1490858793258667


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0312e-05,  1.0119e-04, -5.4618e-06,  ...,  3.7678e-05,
         -6.4024e-05, -5.7150e-05],
        [-2.2858e-05, -1.6257e-05,  6.1318e-06,  ..., -1.9968e-05,
         -5.6662e-06, -9.0227e-06],
        [ 7.2563e-05,  4.8574e-05, -1.6935e-05,  ...,  5.6446e-05,
          1.1214e-05,  2.5097e-05],
        [-3.5554e-05, -2.5243e-05,  9.5591e-06,  ..., -3.1054e-05,
         -8.7917e-06, -1.4022e-05],
        [-5.0753e-05, -3.6001e-05,  1.3590e-05,  ..., -4.4316e-05,
         -1.2584e-05, -1.9997e-05]], device='cuda:0')
Loss: 1.1450364589691162
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5712e-05, -4.9002e-06, -2.3877e-05,  ...,  1.2095e-05,
         -3.2628e-05,  1.4131e-05],
        [-2.0913e-05, -1.4821e-05,  3.9205e-06,  ..., -1.7419e-05,
         -2.9649e-06, -7.8255e-06],
        [ 4.9971e-05,  3.5972e-05, -1.0040e-05,  ...,  4.1730e-05,
          8.3032e-06,  1.8001e-05],
        [-3.2793e-05, -2.2137e-05,  6.8229e-06,  ..., -2.7018e-05,
         -5.2983e-06, -1.1985e-05],
        [-5.5168e-05, -3.8502e-05,  1.0318e-05,  ..., -4.6032e-05,
         -8.7630e-06, -2.1169e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9013e-08, 1.9856e-08, 2.9910e-08,  ..., 2.2191e-08, 5.2356e-08,
         1.1278e-08],
        [3.7564e-11, 2.2629e-11, 1.4123e-12,  ..., 2.5474e-11, 9.8767e-13,
         5.3979e-12],
        [6.4839e-10, 3.8058e-10, 2.2806e-11,  ..., 4.9865e-10, 1.2781e-11,
         9.6383e-11],
        [1.5132e-10, 2.1578e-10, 5.0773e-12,  ..., 1.2090e-10, 1.6960e-11,
         7.3663e-11],
        [1.4372e-10, 8.0830e-11, 6.9580e-12,  ..., 9.8690e-11, 2.2712e-12,
         2.1406e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8757e-05, -1.6804e-05,  2.0378e-05,  ..., -2.1504e-05,
          4.3384e-06,  1.8479e-05],
        [-2.8610e-06, -2.0415e-06,  7.7486e-07,  ..., -2.4885e-06,
         -7.2643e-07, -1.1325e-06],
        [-3.2634e-06, -2.3246e-06,  8.8289e-07,  ..., -2.8312e-06,
         -8.2701e-07, -1.2964e-06],
        [ 1.9606e-04,  1.1899e-04, -3.7819e-05,  ...,  1.3669e-04,
          4.2047e-05,  6.7152e-05],
        [-6.4373e-06, -4.5598e-06,  1.7360e-06,  ..., -5.5730e-06,
         -1.6317e-06, -2.5481e-06]], device='cuda:0')
Loss: 1.1520500183105469


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,  3351,   352,  ...,    17,   287,   357],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9844e-06, -2.7803e-05,  1.0720e-05,  ..., -1.3516e-05,
         -2.3968e-05,  2.9295e-06],
        [-5.6922e-06, -4.0531e-06,  1.5721e-06,  ..., -4.9919e-06,
         -1.4491e-06, -2.2948e-06],
        [-6.4373e-06, -4.5896e-06,  1.7770e-06,  ..., -5.6475e-06,
         -1.6354e-06, -2.6003e-06],
        [ 1.9162e-04,  1.1583e-04, -3.6560e-05,  ...,  1.3276e-04,
          4.0915e-05,  6.5327e-05],
        [-1.2696e-05, -9.0003e-06,  3.4943e-06,  ..., -1.1086e-05,
         -3.2261e-06, -5.1111e-06]], device='cuda:0')
Loss: 1.150604486465454


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  565,   27,  ...,   88, 4451,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1469e-05, -1.2335e-05,  2.1325e-05,  ..., -1.8798e-05,
         -2.7941e-05, -3.4473e-05],
        [-8.5235e-06, -6.0797e-06,  2.3395e-06,  ..., -7.4655e-06,
         -2.1644e-06, -3.4496e-06],
        [-9.6411e-06, -6.8843e-06,  2.6450e-06,  ..., -8.4341e-06,
         -2.4438e-06, -3.9041e-06],
        [ 1.8718e-04,  1.1264e-04, -3.5353e-05,  ...,  1.2888e-04,
          3.9797e-05,  6.3523e-05],
        [-1.9014e-05, -1.3530e-05,  5.2080e-06,  ..., -1.6630e-05,
         -4.8280e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.1334205865859985


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0, 1445, 3597,  ...,  281,   78,    9],
        [   0, 4448,   12,  ..., 3183,  328, 9559]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1286e-05,  1.0905e-05,  3.2559e-05,  ..., -1.8798e-05,
         -1.5511e-05, -3.5966e-06],
        [-1.1355e-05, -8.1062e-06,  3.1330e-06,  ..., -9.9540e-06,
         -2.8983e-06, -4.6119e-06],
        [-1.2860e-05, -9.1791e-06,  3.5465e-06,  ..., -1.1250e-05,
         -3.2745e-06, -5.2229e-06],
        [ 1.8274e-04,  1.0947e-04, -3.4109e-05,  ...,  1.2501e-04,
          3.8650e-05,  6.1706e-05],
        [-2.5332e-05, -1.8030e-05,  6.9737e-06,  ..., -2.2143e-05,
         -6.4597e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.1536585092544556


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,   292, 15087,  ...,  2675,  1663,    12],
        [    0,  5862,    13,  ..., 12497,   287,  3570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0356e-04,  4.0042e-05,  2.8691e-05,  ..., -3.6511e-05,
         -9.9329e-06, -2.4192e-05],
        [-1.4156e-05, -1.0133e-05,  3.9153e-06,  ..., -1.2428e-05,
         -3.6210e-06, -5.7742e-06],
        [-1.6049e-05, -1.1489e-05,  4.4368e-06,  ..., -1.4067e-05,
         -4.0978e-06, -6.5416e-06],
        [ 1.7833e-04,  1.0628e-04, -3.2872e-05,  ...,  1.2110e-04,
          3.7510e-05,  5.9880e-05],
        [-3.1620e-05, -2.2560e-05,  8.7246e-06,  ..., -2.7657e-05,
         -8.0764e-06, -1.2860e-05]], device='cuda:0')
Loss: 1.1441570520401


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[   0, 5489,   80,  ...,  221,  380,  333],
        [   0,  898,  266,  ...,   12, 3222, 8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0622e-04,  4.3998e-05, -2.2937e-05,  ..., -2.9478e-05,
         -2.9994e-05, -6.0530e-05],
        [-1.6972e-05, -1.2144e-05,  4.7013e-06,  ..., -1.4886e-05,
         -4.3288e-06, -6.9290e-06],
        [-1.9252e-05, -1.3769e-05,  5.3309e-06,  ..., -1.6868e-05,
         -4.9025e-06, -7.8529e-06],
        [ 1.7389e-04,  1.0312e-04, -3.1628e-05,  ...,  1.1723e-04,
          3.6400e-05,  5.8062e-05],
        [-3.7879e-05, -2.7031e-05,  1.0468e-05,  ..., -3.3110e-05,
         -9.6485e-06, -1.5423e-05]], device='cuda:0')
Loss: 1.1600276231765747


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0,   381, 13565,  ...,     9,   847,   300],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0657e-04,  8.1118e-05, -3.3429e-05,  ..., -4.1861e-05,
         -1.0869e-05, -4.4373e-05],
        [-1.9848e-05, -1.4201e-05,  5.4687e-06,  ..., -1.7390e-05,
         -5.0664e-06, -8.0913e-06],
        [-2.2486e-05, -1.6078e-05,  6.1952e-06,  ..., -1.9699e-05,
         -5.7332e-06, -9.1642e-06],
        [ 1.6942e-04,  9.9932e-05, -3.0436e-05,  ...,  1.1333e-04,
          3.5260e-05,  5.6259e-05],
        [-4.4316e-05, -3.1620e-05,  1.2182e-05,  ..., -3.8713e-05,
         -1.1295e-05, -1.8016e-05]], device='cuda:0')
Loss: 1.1493490934371948


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[   0, 5896,  352,  ..., 1168,  767, 1390],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3194e-04,  7.3896e-05, -5.7492e-05,  ..., -5.2571e-05,
         -6.9458e-06, -3.9191e-05],
        [-2.2665e-05, -1.6212e-05,  6.2436e-06,  ..., -1.9878e-05,
         -5.7966e-06, -9.2387e-06],
        [-2.5690e-05, -1.8373e-05,  7.0781e-06,  ..., -2.2531e-05,
         -6.5640e-06, -1.0476e-05],
        [ 1.6498e-04,  9.6773e-05, -2.9214e-05,  ...,  1.0942e-04,
          3.4112e-05,  5.4456e-05],
        [-5.0575e-05, -3.6091e-05,  1.3903e-05,  ..., -4.4227e-05,
         -1.2919e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.165358066558838
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5334e-05,  2.9794e-06, -2.7238e-05,  ...,  5.6285e-06,
         -3.0060e-05,  8.7985e-06],
        [-2.1088e-05, -1.4960e-05,  4.1528e-06,  ..., -1.7665e-05,
         -3.2481e-06, -7.9668e-06],
        [ 4.2405e-05,  3.0537e-05, -8.3278e-06,  ...,  3.5304e-05,
          6.8165e-06,  1.5154e-05],
        [-1.3016e-05, -1.0246e-05,  3.2192e-06,  ..., -1.3374e-05,
         -1.3572e-06, -5.3408e-06],
        [-5.4709e-05, -3.8261e-05,  1.0676e-05,  ..., -4.5852e-05,
         -9.1787e-06, -2.1108e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8971e-08, 1.9842e-08, 2.9883e-08,  ..., 2.2171e-08, 5.2304e-08,
         1.1268e-08],
        [3.8040e-11, 2.2869e-11, 1.4498e-12,  ..., 2.5843e-11, 1.0203e-12,
         5.4779e-12],
        [6.4841e-10, 3.8054e-10, 2.2834e-11,  ..., 4.9866e-10, 1.2811e-11,
         9.6396e-11],
        [1.7838e-10, 2.2493e-10, 5.9257e-12,  ..., 1.3275e-10, 1.8107e-11,
         7.6555e-11],
        [1.4614e-10, 8.2052e-11, 7.1443e-12,  ..., 1.0055e-10, 2.4358e-12,
         2.1807e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.896446322196428e-05, 1.896446322196428e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1266e-05, -5.3110e-07, -3.1088e-06,  ..., -2.0089e-06,
          1.3012e-05,  1.4503e-05],
        [-2.8163e-06, -2.0117e-06,  8.0466e-07,  ..., -2.4587e-06,
         -7.6368e-07, -1.1995e-06],
        [-3.2485e-06, -2.3097e-06,  9.2387e-07,  ..., -2.8312e-06,
         -8.7917e-07, -1.3784e-06],
        [-4.5002e-06, -3.2037e-06,  1.2815e-06,  ..., -3.9339e-06,
         -1.2144e-06, -1.9073e-06],
        [-6.3479e-06, -4.5300e-06,  1.8030e-06,  ..., -5.5432e-06,
         -1.7285e-06, -2.6971e-06]], device='cuda:0')
Loss: 1.1524543762207031


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    13,   786,  ...,   275,  2623,    13],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0739e-05,  4.2424e-05, -4.7350e-05,  ..., -1.1070e-05,
         -9.5753e-06,  3.0664e-05],
        [-5.6177e-06, -4.0233e-06,  1.6019e-06,  ..., -4.9174e-06,
         -1.5050e-06, -2.3767e-06],
        [ 9.0212e-05,  6.6648e-05, -1.3386e-05,  ...,  8.5288e-05,
          1.8807e-05,  3.6530e-05],
        [-8.9705e-06, -6.4224e-06,  2.5555e-06,  ..., -7.8380e-06,
         -2.3916e-06, -3.7849e-06],
        [-1.2636e-05, -9.0599e-06,  3.5912e-06,  ..., -1.1057e-05,
         -3.3975e-06, -5.3346e-06]], device='cuda:0')
Loss: 1.1650769710540771


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,  1304,   292,  ...,  2101,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6758e-06,  2.8907e-05, -5.6048e-05,  ...,  6.3279e-07,
         -8.8981e-06,  3.0664e-05],
        [-8.4490e-06, -6.0201e-06,  2.3842e-06,  ..., -7.3761e-06,
         -2.2613e-06, -3.5614e-06],
        [ 8.6948e-05,  6.4338e-05, -1.2481e-05,  ...,  8.2441e-05,
          1.7932e-05,  3.5167e-05],
        [-1.3530e-05, -9.6411e-06,  3.8147e-06,  ..., -1.1802e-05,
         -3.6061e-06, -5.6848e-06],
        [-1.8954e-05, -1.3530e-05,  5.3346e-06,  ..., -1.6540e-05,
         -5.0887e-06, -7.9721e-06]], device='cuda:0')
Loss: 1.1377182006835938


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[   0,   14,   23,  ...,  278,  266, 1462],
        [   0,   14,  381,  ..., 7106,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3359e-05,  4.3551e-05, -6.9536e-05,  ..., -1.6818e-05,
         -1.6033e-05,  6.2341e-05],
        [-1.1250e-05, -8.0168e-06,  3.1665e-06,  ..., -9.8348e-06,
         -2.9989e-06, -4.7162e-06],
        [ 8.3715e-05,  6.2029e-05, -1.1575e-05,  ...,  7.9595e-05,
          1.7082e-05,  3.3833e-05],
        [-1.8030e-05, -1.2860e-05,  5.0738e-06,  ..., -1.5765e-05,
         -4.7907e-06, -7.5400e-06],
        [-2.5243e-05, -1.8001e-05,  7.0855e-06,  ..., -2.2054e-05,
         -6.7428e-06, -1.0565e-05]], device='cuda:0')
Loss: 1.146453857421875


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[   0, 1101,  300,  ..., 6104,  367,  993],
        [   0, 3592,  417,  ..., 4893,  328,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0129e-05,  5.2857e-05, -8.7389e-05,  ..., -1.3718e-05,
         -5.4460e-05,  7.1773e-05],
        [-1.4067e-05, -9.9987e-06,  3.9674e-06,  ..., -1.2264e-05,
         -3.7439e-06, -5.8934e-06],
        [ 8.0451e-05,  5.9734e-05, -1.0644e-05,  ...,  7.6779e-05,
          1.6218e-05,  3.2470e-05],
        [-2.2560e-05, -1.6049e-05,  6.3628e-06,  ..., -1.9670e-05,
         -5.9903e-06, -9.4324e-06],
        [-3.1531e-05, -2.2411e-05,  8.8736e-06,  ..., -2.7478e-05,
         -8.4043e-06, -1.3188e-05]], device='cuda:0')
Loss: 1.1469249725341797


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  300, 2607,  ..., 1279,  368,  266],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7802e-05,  8.5999e-05, -1.2728e-04,  ..., -5.5687e-06,
         -5.4460e-05,  8.5672e-05],
        [-1.6898e-05, -1.2055e-05,  4.7721e-06,  ..., -1.4737e-05,
         -4.5225e-06, -7.1079e-06],
        [ 7.7203e-05,  5.7350e-05, -9.7166e-06,  ...,  7.3933e-05,
          1.5320e-05,  3.1069e-05],
        [-2.7061e-05, -1.9327e-05,  7.6443e-06,  ..., -2.3603e-05,
         -7.2271e-06, -1.1370e-05],
        [-3.7789e-05, -2.7001e-05,  1.0654e-05,  ..., -3.2961e-05,
         -1.0133e-05, -1.5885e-05]], device='cuda:0')
Loss: 1.1559287309646606


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1809e-05,  9.6880e-05, -1.0633e-04,  ..., -7.8662e-06,
         -6.6170e-05,  1.0185e-04],
        [-1.9759e-05, -1.4111e-05,  5.5544e-06,  ..., -1.7211e-05,
         -5.2899e-06, -8.2999e-06],
        [ 7.3925e-05,  5.4980e-05, -8.8188e-06,  ...,  7.1087e-05,
          1.4437e-05,  2.9705e-05],
        [-3.1590e-05, -2.2575e-05,  8.8811e-06,  ..., -2.7508e-05,
         -8.4415e-06, -1.3247e-05],
        [-4.4197e-05, -3.1620e-05,  1.2405e-05,  ..., -3.8505e-05,
         -1.1861e-05, -1.8552e-05]], device='cuda:0')
Loss: 1.1371839046478271


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0,  1716,   271,  ...,   292,    78,  1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8280e-06,  8.4150e-05, -1.0977e-04,  ...,  1.3587e-05,
         -7.4818e-05,  8.5462e-05],
        [-2.2590e-05, -1.6153e-05,  6.3442e-06,  ..., -1.9670e-05,
         -6.0573e-06, -9.4846e-06],
        [ 7.0646e-05,  5.2626e-05, -7.9061e-06,  ...,  6.8241e-05,
          1.3547e-05,  2.8335e-05],
        [-3.6091e-05, -2.5809e-05,  1.0133e-05,  ..., -3.1412e-05,
         -9.6634e-06, -1.5132e-05],
        [-5.0575e-05, -3.6180e-05,  1.4171e-05,  ..., -4.4018e-05,
         -1.3590e-05, -2.1219e-05]], device='cuda:0')
Loss: 1.151222825050354
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1484e-05,  1.1096e-05, -3.5492e-05,  ...,  6.4243e-06,
         -3.4536e-05,  1.6465e-05],
        [-2.1238e-05, -1.5079e-05,  4.3719e-06,  ..., -1.7865e-05,
         -3.5290e-06, -8.1186e-06],
        [ 4.5229e-05,  3.2746e-05, -8.2857e-06,  ...,  3.8598e-05,
          7.4895e-06,  1.6472e-05],
        [-1.5323e-05, -1.1802e-05,  3.9106e-06,  ..., -1.5177e-05,
         -2.1878e-06, -6.3199e-06],
        [-5.4296e-05, -3.8053e-05,  1.1026e-05,  ..., -4.5668e-05,
         -9.6198e-06, -2.1119e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8912e-08, 1.9829e-08, 2.9866e-08,  ..., 2.2149e-08, 5.2257e-08,
         1.1264e-08],
        [3.8513e-11, 2.3107e-11, 1.4886e-12,  ..., 2.6204e-11, 1.0560e-12,
         5.5624e-12],
        [6.5275e-10, 3.8293e-10, 2.2873e-11,  ..., 5.0282e-10, 1.2982e-11,
         9.7102e-11],
        [1.7951e-10, 2.2537e-10, 6.0225e-12,  ..., 1.3361e-10, 1.8182e-11,
         7.6708e-11],
        [1.4855e-10, 8.3279e-11, 7.3380e-12,  ..., 1.0238e-10, 2.6181e-12,
         2.2236e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   199,  7513,  ...,   271,   259,   957],
        [    0,   266,   858,  ..., 11265,   607,  7455]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4983e-05, -2.8393e-05, -3.3114e-06,  ..., -1.8677e-05,
          1.2530e-06,  2.7248e-05],
        [-2.7865e-06, -1.9968e-06,  7.9721e-07,  ..., -2.4587e-06,
         -7.9349e-07, -1.2442e-06],
        [-3.2783e-06, -2.3544e-06,  9.3505e-07,  ..., -2.8759e-06,
         -9.3132e-07, -1.4603e-06],
        [-4.4703e-06, -3.2187e-06,  1.2815e-06,  ..., -3.9339e-06,
         -1.2666e-06, -1.9968e-06],
        [-6.2287e-06, -4.4703e-06,  1.7807e-06,  ..., -5.4836e-06,
         -1.7732e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.14253568649292


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2028e-05, -4.2852e-05, -1.9686e-05,  ..., -3.7222e-05,
         -2.2435e-05,  1.9261e-05],
        [-5.5432e-06, -3.9786e-06,  1.5944e-06,  ..., -4.9025e-06,
         -1.5460e-06, -2.4661e-06],
        [-6.4969e-06, -4.6641e-06,  1.8664e-06,  ..., -5.7369e-06,
         -1.8105e-06, -2.8908e-06],
        [-8.9407e-06, -6.4373e-06,  2.5779e-06,  ..., -7.8976e-06,
         -2.4885e-06, -3.9786e-06],
        [-1.2457e-05, -8.9407e-06,  3.5763e-06,  ..., -1.0997e-05,
         -3.4720e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.1591715812683105


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   344,  8260,  ..., 16020, 18216, 11348],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6533e-05, -6.3346e-05, -2.7108e-05,  ..., -1.6000e-05,
         -2.2567e-05,  4.1247e-05],
        [-8.3297e-06, -5.9903e-06,  2.4065e-06,  ..., -7.3314e-06,
         -2.3060e-06, -3.6806e-06],
        [ 8.6820e-05,  5.9459e-05, -2.6159e-05,  ...,  7.4505e-05,
          2.5910e-05,  3.7086e-05],
        [-1.3471e-05, -9.7156e-06,  3.9041e-06,  ..., -1.1832e-05,
         -3.7178e-06, -5.9456e-06],
        [ 7.1943e-05,  4.8666e-05, -2.4174e-05,  ...,  6.3691e-05,
          2.2091e-05,  2.6556e-05]], device='cuda:0')
Loss: 1.1502400636672974


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   271,   266,  7246],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8044e-05, -8.2780e-05, -6.3459e-05,  ..., -8.5831e-06,
         -3.7176e-05,  3.2458e-05],
        [-1.1101e-05, -7.9870e-06,  3.2149e-06,  ..., -9.7752e-06,
         -3.0771e-06, -4.9174e-06],
        [ 8.3556e-05,  5.7105e-05, -2.5209e-05,  ...,  7.1629e-05,
          2.5005e-05,  3.5626e-05],
        [-1.7941e-05, -1.2949e-05,  5.2154e-06,  ..., -1.5795e-05,
         -4.9621e-06, -7.9423e-06],
        [ 6.5714e-05,  4.4165e-05, -2.2356e-05,  ...,  5.8208e-05,
          2.0355e-05,  2.3769e-05]], device='cuda:0')
Loss: 1.1453486680984497


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0, 10205,   342,  ...,  2523,  4729, 13753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2287e-05, -1.2315e-04, -5.1783e-05,  ...,  6.9916e-06,
         -5.1300e-05,  3.6295e-05],
        [-1.3888e-05, -1.0014e-05,  4.0270e-06,  ..., -1.2219e-05,
         -3.8408e-06, -6.1542e-06],
        [ 8.0263e-05,  5.4706e-05, -2.4255e-05,  ...,  6.8739e-05,
          2.4107e-05,  3.4173e-05],
        [-2.2441e-05, -1.6227e-05,  6.5267e-06,  ..., -1.9759e-05,
         -6.1914e-06, -9.9391e-06],
        [ 5.9426e-05,  3.9606e-05, -2.0530e-05,  ...,  5.2694e-05,
          1.8634e-05,  2.0998e-05]], device='cuda:0')
Loss: 1.1586556434631348


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9917e-05, -1.3250e-04, -3.4388e-05,  ..., -2.1043e-05,
         -3.5538e-05,  5.5702e-05],
        [-1.6645e-05, -1.2010e-05,  4.8243e-06,  ..., -1.4663e-05,
         -4.6007e-06, -7.3686e-06],
        [ 7.7030e-05,  5.2351e-05, -2.3320e-05,  ...,  6.5878e-05,
          2.3217e-05,  3.2758e-05],
        [-2.6911e-05, -1.9461e-05,  7.8231e-06,  ..., -2.3723e-05,
         -7.4208e-06, -1.1891e-05],
        [ 5.3227e-05,  3.5105e-05, -1.8742e-05,  ...,  4.7211e-05,
          1.6927e-05,  1.8286e-05]], device='cuda:0')
Loss: 1.130213975906372


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,    14,   747,  ...,   367,   300,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7091e-05, -1.2695e-04, -1.6417e-05,  ..., -8.1857e-06,
         -3.5244e-05,  5.9136e-05],
        [-1.9372e-05, -1.4007e-05,  5.6252e-06,  ..., -1.7092e-05,
         -5.3607e-06, -8.5831e-06],
        [ 7.3811e-05,  4.9997e-05, -2.2374e-05,  ...,  6.3016e-05,
          2.2323e-05,  3.1327e-05],
        [-3.1352e-05, -2.2694e-05,  9.1270e-06,  ..., -2.7657e-05,
         -8.6501e-06, -1.3858e-05],
        [ 4.7058e-05,  3.0635e-05, -1.6939e-05,  ...,  4.1727e-05,
          1.5214e-05,  1.5559e-05]], device='cuda:0')
Loss: 1.1507552862167358


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8593e-05, -1.2956e-04, -2.3691e-05,  ..., -8.2297e-06,
         -2.4912e-05,  4.3337e-05],
        [-2.2113e-05, -1.6019e-05,  6.4112e-06,  ..., -1.9521e-05,
         -6.1318e-06, -9.8348e-06],
        [ 7.0592e-05,  4.7642e-05, -2.1450e-05,  ...,  6.0155e-05,
          2.1417e-05,  2.9852e-05],
        [-3.5793e-05, -2.5943e-05,  1.0401e-05,  ..., -3.1590e-05,
         -9.8944e-06, -1.5885e-05],
        [ 4.0889e-05,  2.6105e-05, -1.5173e-05,  ...,  3.6243e-05,
          1.3485e-05,  1.2743e-05]], device='cuda:0')
Loss: 1.1362183094024658
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5476e-05, -2.9690e-06, -3.4312e-05,  ...,  4.9589e-06,
         -3.3574e-05,  1.9152e-05],
        [-2.1326e-05, -1.5173e-05,  4.5759e-06,  ..., -1.8031e-05,
         -3.7893e-06, -8.2902e-06],
        [ 4.7766e-05,  3.4236e-05, -9.6021e-06,  ...,  4.0753e-05,
          8.8823e-06,  1.7810e-05],
        [-1.7370e-05, -1.3216e-05,  4.5596e-06,  ..., -1.6819e-05,
         -2.9585e-06, -7.2764e-06],
        [-4.4777e-05, -3.1637e-05,  8.4058e-06,  ..., -3.7477e-05,
         -7.3093e-06, -1.7733e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8854e-08, 1.9826e-08, 2.9836e-08,  ..., 2.2127e-08, 5.2206e-08,
         1.1255e-08],
        [3.8963e-11, 2.3340e-11, 1.5282e-12,  ..., 2.6559e-11, 1.0925e-12,
         5.6535e-12],
        [6.5708e-10, 3.8481e-10, 2.3311e-11,  ..., 5.0594e-10, 1.3428e-11,
         9.7897e-11],
        [1.8061e-10, 2.2582e-10, 6.1246e-12,  ..., 1.3447e-10, 1.8262e-11,
         7.6883e-11],
        [1.5007e-10, 8.3877e-11, 7.5609e-12,  ..., 1.0360e-10, 2.7973e-12,
         2.2376e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.885217280831754e-05, 1.885217280831754e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  747, 7890,  ...,  706, 8667,  271],
        [   0,  328,  490,  ..., 6280, 4283, 4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5441e-05, -7.7798e-06, -1.0510e-05,  ...,  0.0000e+00,
         -1.8788e-05,  2.0652e-05],
        [-2.7418e-06, -1.9968e-06,  8.2701e-07,  ..., -2.4140e-06,
         -7.7114e-07, -1.2740e-06],
        [-3.2932e-06, -2.3991e-06,  9.9093e-07,  ..., -2.8908e-06,
         -9.2387e-07, -1.5199e-06],
        [-4.5002e-06, -3.2783e-06,  1.3560e-06,  ..., -3.9637e-06,
         -1.2666e-06, -2.0862e-06],
        [-6.2287e-06, -4.5300e-06,  1.8701e-06,  ..., -5.4538e-06,
         -1.7509e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1288381814956665


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,  9058,  5481,  ...,   508, 15074,   300],
        [    0,  2790,   266,  ...,   401,  1496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3838e-08,  3.1896e-05, -1.1535e-05,  ..., -2.3383e-05,
         -6.0061e-06,  1.9049e-05],
        [-5.4985e-06, -4.0084e-06,  1.6317e-06,  ..., -4.8280e-06,
         -1.5385e-06, -2.5481e-06],
        [-6.5416e-06, -4.7684e-06,  1.9409e-06,  ..., -5.7518e-06,
         -1.8291e-06, -3.0249e-06],
        [-9.0003e-06, -6.5714e-06,  2.6748e-06,  ..., -7.9274e-06,
         -2.5183e-06, -4.1723e-06],
        [-1.2487e-05, -9.0897e-06,  3.6955e-06,  ..., -1.0967e-05,
         -3.4943e-06, -5.7667e-06]], device='cuda:0')
Loss: 1.1436798572540283


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,  2834,   266,  ..., 39474,    12, 15441],
        [    0,   292,    46,  ...,  1217,    17,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2302e-06,  8.0718e-05,  9.5339e-06,  ..., -2.4840e-05,
         -4.5343e-06,  9.6306e-06],
        [-8.2552e-06, -5.9903e-06,  2.4252e-06,  ..., -7.2420e-06,
         -2.2911e-06, -3.8072e-06],
        [-9.8050e-06, -7.1228e-06,  2.8796e-06,  ..., -8.6129e-06,
         -2.7195e-06, -4.5225e-06],
        [-1.3530e-05, -9.8348e-06,  3.9786e-06,  ..., -1.1891e-05,
         -3.7551e-06, -6.2436e-06],
        [-1.8746e-05, -1.3590e-05,  5.4985e-06,  ..., -1.6451e-05,
         -5.2005e-06, -8.6278e-06]], device='cuda:0')
Loss: 1.1406916379928589


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0, 23842,   342,  ...,   365,  4011, 10151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0106e-05,  7.4663e-05, -1.6390e-06,  ..., -3.8765e-05,
          6.2228e-06,  2.5782e-05],
        [-1.1012e-05, -7.9870e-06,  3.2336e-06,  ..., -9.6858e-06,
         -3.0622e-06, -5.0738e-06],
        [-1.3068e-05, -9.4920e-06,  3.8408e-06,  ..., -1.1519e-05,
         -3.6359e-06, -6.0275e-06],
        [-1.8001e-05, -1.3083e-05,  5.2974e-06,  ..., -1.5855e-05,
         -5.0068e-06, -8.2999e-06],
        [-2.4974e-05, -1.8090e-05,  7.3165e-06,  ..., -2.1964e-05,
         -6.9439e-06, -1.1489e-05]], device='cuda:0')
Loss: 1.1594295501708984


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[   0, 4209,  278,  ...,  287, 9971,  717],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4278e-06,  4.8726e-05, -2.1548e-05,  ..., -3.9317e-05,
          1.5611e-05,  1.3367e-05],
        [-1.3739e-05, -9.9838e-06,  4.0419e-06,  ..., -1.2085e-05,
         -3.8408e-06, -6.3628e-06],
        [-1.6317e-05, -1.1861e-05,  4.8019e-06,  ..., -1.4380e-05,
         -4.5635e-06, -7.5549e-06],
        [-2.2471e-05, -1.6361e-05,  6.6310e-06,  ..., -1.9789e-05,
         -6.2808e-06, -1.0401e-05],
        [-3.1173e-05, -2.2620e-05,  9.1568e-06,  ..., -2.7418e-05,
         -8.7172e-06, -1.4409e-05]], device='cuda:0')
Loss: 1.1220781803131104


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  292,   41,  ...,  271, 9536,  287],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0487e-05,  1.2882e-04, -2.1817e-05,  ..., -3.4242e-05,
          5.0483e-05,  3.6040e-05],
        [-1.6525e-05, -1.2010e-05,  4.8503e-06,  ..., -1.4514e-05,
         -4.6678e-06, -7.6368e-06],
        [-1.9625e-05, -1.4260e-05,  5.7630e-06,  ..., -1.7270e-05,
         -5.5395e-06, -9.0674e-06],
        [-2.6971e-05, -1.9625e-05,  7.9423e-06,  ..., -2.3723e-05,
         -7.6145e-06, -1.2457e-05],
        [-3.7491e-05, -2.7210e-05,  1.0990e-05,  ..., -3.2932e-05,
         -1.0587e-05, -1.7300e-05]], device='cuda:0')
Loss: 1.1347291469573975


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1824,    13,  ...,   266,  5940,    19],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5158e-05,  1.3231e-04, -4.6035e-05,  ..., -2.7183e-05,
          3.3647e-05,  3.5612e-05],
        [-1.9267e-05, -1.3992e-05,  5.6736e-06,  ..., -1.6928e-05,
         -5.4128e-06, -8.8811e-06],
        [-2.2873e-05, -1.6615e-05,  6.7391e-06,  ..., -2.0131e-05,
         -6.4187e-06, -1.0543e-05],
        [-3.1471e-05, -2.2873e-05,  9.2983e-06,  ..., -2.7686e-05,
         -8.8289e-06, -1.4499e-05],
        [-4.3690e-05, -3.1680e-05,  1.2852e-05,  ..., -3.8385e-05,
         -1.2264e-05, -2.0117e-05]], device='cuda:0')
Loss: 1.1447120904922485


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2542e-05,  1.3847e-04, -6.3841e-05,  ..., -2.5199e-05,
          5.7649e-05,  5.5549e-05],
        [-2.1994e-05, -1.5989e-05,  6.4932e-06,  ..., -1.9372e-05,
         -6.1877e-06, -1.0155e-05],
        [-2.6122e-05, -1.8999e-05,  7.7151e-06,  ..., -2.3037e-05,
         -7.3425e-06, -1.2055e-05],
        [-3.5912e-05, -2.6137e-05,  1.0639e-05,  ..., -3.1650e-05,
         -1.0088e-05, -1.6570e-05],
        [-4.9800e-05, -3.6150e-05,  1.4693e-05,  ..., -4.3869e-05,
         -1.4000e-05, -2.2963e-05]], device='cuda:0')
Loss: 1.1275378465652466
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8674e-05,  1.1175e-05, -3.7264e-05,  ...,  1.9431e-06,
         -2.4451e-05,  2.2792e-05],
        [-2.1393e-05, -1.5255e-05,  4.7676e-06,  ..., -1.8165e-05,
         -4.0291e-06, -8.4767e-06],
        [ 4.0377e-05,  2.8912e-05, -7.8704e-06,  ...,  3.4374e-05,
          7.2598e-06,  1.4823e-05],
        [-1.9224e-05, -1.4508e-05,  5.1676e-06,  ..., -1.8302e-05,
         -3.6714e-06, -8.2058e-06],
        [-4.5279e-05, -3.2089e-05,  9.0344e-06,  ..., -3.8116e-05,
         -7.9783e-06, -1.8256e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8796e-08, 1.9825e-08, 2.9810e-08,  ..., 2.2106e-08, 5.2157e-08,
         1.1247e-08],
        [3.9408e-11, 2.3573e-11, 1.5689e-12,  ..., 2.6908e-11, 1.1297e-12,
         5.7510e-12],
        [6.5710e-10, 3.8479e-10, 2.3347e-11,  ..., 5.0596e-10, 1.3468e-11,
         9.7944e-11],
        [1.8172e-10, 2.2628e-10, 6.2317e-12,  ..., 1.3534e-10, 1.8346e-11,
         7.7081e-11],
        [1.5240e-10, 8.5100e-11, 7.7692e-12,  ..., 1.0542e-10, 2.9905e-12,
         2.2881e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0,   12, 4957,  ...,  944,  278,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8173e-05, -1.8002e-06, -2.1337e-05,  ..., -9.7243e-06,
          1.2627e-05, -1.7107e-05],
        [-2.7120e-06, -1.9968e-06,  8.3819e-07,  ..., -2.4140e-06,
         -7.9721e-07, -1.3337e-06],
        [-3.2634e-06, -2.3991e-06,  1.0133e-06,  ..., -2.9057e-06,
         -9.6112e-07, -1.6019e-06],
        [-4.4703e-06, -3.2932e-06,  1.3858e-06,  ..., -3.9935e-06,
         -1.3113e-06, -2.1905e-06],
        [-6.1393e-06, -4.5300e-06,  1.8999e-06,  ..., -5.4836e-06,
         -1.8030e-06, -3.0100e-06]], device='cuda:0')
Loss: 1.1161174774169922


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,   285,    53,  ...,   259,  5012,  3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7619e-05, -1.5191e-05, -2.7849e-06,  ..., -6.7954e-07,
          3.9751e-05,  2.5429e-06],
        [-5.4538e-06, -3.9786e-06,  1.6615e-06,  ..., -4.8429e-06,
         -1.5870e-06, -2.6673e-06],
        [-6.5565e-06, -4.7833e-06,  2.0042e-06,  ..., -5.8264e-06,
         -1.9073e-06, -3.1963e-06],
        [-8.9705e-06, -6.5565e-06,  2.7418e-06,  ..., -7.9870e-06,
         -2.6077e-06, -4.3809e-06],
        [-1.2308e-05, -9.0003e-06,  3.7551e-06,  ..., -1.0967e-05,
         -3.5837e-06, -6.0052e-06]], device='cuda:0')
Loss: 1.1353075504302979


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  292,  263,  ...,  342, 4575,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6854e-05,  2.9685e-05, -4.1426e-06,  ..., -1.1275e-05,
          2.0845e-05, -1.4767e-05],
        [-8.1658e-06, -5.9456e-06,  2.4959e-06,  ..., -7.2569e-06,
         -2.3656e-06, -3.9861e-06],
        [-9.8050e-06, -7.1377e-06,  3.0100e-06,  ..., -8.7321e-06,
         -2.8424e-06, -4.7833e-06],
        [-1.3471e-05, -9.8050e-06,  4.1276e-06,  ..., -1.1981e-05,
         -3.8967e-06, -6.5714e-06],
        [-1.8477e-05, -1.3441e-05,  5.6550e-06,  ..., -1.6451e-05,
         -5.3570e-06, -9.0003e-06]], device='cuda:0')
Loss: 1.1647365093231201


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6360e-05,  3.1600e-05,  1.8502e-05,  ..., -9.9007e-06,
          2.6927e-05,  2.9968e-05],
        [-1.0893e-05, -7.9423e-06,  3.3267e-06,  ..., -9.6709e-06,
         -3.1218e-06, -5.2899e-06],
        [-1.3083e-05, -9.5516e-06,  4.0084e-06,  ..., -1.1638e-05,
         -3.7551e-06, -6.3553e-06],
        [-1.7971e-05, -1.3098e-05,  5.4985e-06,  ..., -1.5974e-05,
         -5.1484e-06, -8.7321e-06],
        [-2.4617e-05, -1.7941e-05,  7.5251e-06,  ..., -2.1875e-05,
         -7.0632e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1504147052764893


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,   287,  6932,  ...,  1549,  1480,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7306e-05,  1.6443e-05, -7.9402e-06,  ..., -2.9899e-05,
          2.5955e-05,  2.7679e-05],
        [-1.3590e-05, -9.9391e-06,  4.1500e-06,  ..., -1.2085e-05,
         -3.8892e-06, -6.6087e-06],
        [-1.6347e-05, -1.1966e-05,  5.0068e-06,  ..., -1.4558e-05,
         -4.6827e-06, -7.9498e-06],
        [-2.2411e-05, -1.6391e-05,  6.8620e-06,  ..., -1.9938e-05,
         -6.4075e-06, -1.0908e-05],
        [-3.0726e-05, -2.2471e-05,  9.3952e-06,  ..., -2.7329e-05,
         -8.7991e-06, -1.4931e-05]], device='cuda:0')
Loss: 1.1406365633010864


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,    12,  1471,  ...,  1356,   600,    12],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5781e-05,  5.0009e-06, -7.9402e-06,  ..., -2.1204e-05,
          5.4479e-06,  3.4400e-05],
        [-1.6317e-05, -1.1936e-05,  4.9658e-06,  ..., -1.4529e-05,
         -4.6641e-06, -7.9200e-06],
        [-1.9625e-05, -1.4380e-05,  5.9903e-06,  ..., -1.7494e-05,
         -5.6177e-06, -9.5293e-06],
        [-2.6882e-05, -1.9670e-05,  8.2031e-06,  ..., -2.3931e-05,
         -7.6741e-06, -1.3053e-05],
        [-3.6865e-05, -2.6971e-05,  1.1235e-05,  ..., -3.2812e-05,
         -1.0550e-05, -1.7881e-05]], device='cuda:0')
Loss: 1.129813551902771


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2337e-04,  3.0810e-05, -2.4080e-05,  ..., -2.2711e-05,
          2.9458e-05,  3.5937e-05],
        [-1.9029e-05, -1.3947e-05,  5.7593e-06,  ..., -1.6958e-05,
         -5.4464e-06, -9.2462e-06],
        [-2.2888e-05, -1.6794e-05,  6.9402e-06,  ..., -2.0415e-05,
         -6.5528e-06, -1.1116e-05],
        [-3.1322e-05, -2.2963e-05,  9.4995e-06,  ..., -2.7895e-05,
         -8.9481e-06, -1.5214e-05],
        [-4.3035e-05, -3.1561e-05,  1.3031e-05,  ..., -3.8326e-05,
         -1.2323e-05, -2.0877e-05]], device='cuda:0')
Loss: 1.1560540199279785


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3903e-04, -1.0571e-05, -3.6725e-05,  ..., -2.4155e-05,
          3.2070e-05,  3.7173e-05],
        [-2.1726e-05, -1.5914e-05,  6.5640e-06,  ..., -1.9357e-05,
         -6.1952e-06, -1.0565e-05],
        [-2.6152e-05, -1.9178e-05,  7.9162e-06,  ..., -2.3320e-05,
         -7.4580e-06, -1.2711e-05],
        [-3.5793e-05, -2.6226e-05,  1.0833e-05,  ..., -3.1859e-05,
         -1.0185e-05, -1.7390e-05],
        [-4.9144e-05, -3.6031e-05,  1.4849e-05,  ..., -4.3750e-05,
         -1.4022e-05, -2.3857e-05]], device='cuda:0')
Loss: 1.1391383409500122
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0033,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0148, -0.0272,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9710e-05,  9.0004e-06, -3.7210e-05,  ..., -6.6669e-07,
         -1.8799e-05,  2.4230e-05],
        [-2.1426e-05, -1.5321e-05,  4.9472e-06,  ..., -1.8284e-05,
         -4.2457e-06, -8.6855e-06],
        [ 3.3724e-05,  2.4103e-05, -6.2917e-06,  ...,  2.8605e-05,
          5.7881e-06,  1.2070e-05],
        [-2.0881e-05, -1.5680e-05,  5.7342e-06,  ..., -1.9658e-05,
         -4.3228e-06, -9.1242e-06],
        [-4.5666e-05, -3.2483e-05,  9.6159e-06,  ..., -3.8680e-05,
         -8.5827e-06, -1.8816e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8757e-08, 1.9806e-08, 2.9782e-08,  ..., 2.2084e-08, 5.2106e-08,
         1.1237e-08],
        [3.9841e-11, 2.3802e-11, 1.6104e-12,  ..., 2.7256e-11, 1.1669e-12,
         5.8569e-12],
        [6.5713e-10, 3.8477e-10, 2.3386e-11,  ..., 5.0600e-10, 1.3511e-11,
         9.8008e-11],
        [1.8282e-10, 2.2674e-10, 6.3428e-12,  ..., 1.3622e-10, 1.8431e-11,
         7.7306e-11],
        [1.5467e-10, 8.6313e-11, 7.9819e-12,  ..., 1.0723e-10, 3.1841e-12,
         2.3427e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.8734471844266252e-05, 1.8734471844266252e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   298, 11712,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2413e-05,  6.9118e-05, -2.2104e-05,  ...,  1.6443e-05,
         -1.4955e-05,  4.0348e-05],
        [-2.6673e-06, -1.9670e-06,  8.1956e-07,  ..., -2.3991e-06,
         -8.1584e-07, -1.3784e-06],
        [-3.2485e-06, -2.3842e-06,  9.9838e-07,  ..., -2.9057e-06,
         -9.9093e-07, -1.6764e-06],
        [-4.4107e-06, -3.2485e-06,  1.3560e-06,  ..., -3.9339e-06,
         -1.3411e-06, -2.2799e-06],
        [-6.0797e-06, -4.4703e-06,  1.8626e-06,  ..., -5.4538e-06,
         -1.8552e-06, -3.1441e-06]], device='cuda:0')
Loss: 1.1238845586776733


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0,   13, 4363,  ...,  271, 2462,  709],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3918e-05,  8.7267e-05, -5.6223e-05,  ...,  1.4521e-05,
         -1.6662e-05,  7.3768e-05],
        [-5.3942e-06, -3.9786e-06,  1.6205e-06,  ..., -4.8429e-06,
         -1.5795e-06, -2.7567e-06],
        [-6.5267e-06, -4.7833e-06,  1.9595e-06,  ..., -5.8413e-06,
         -1.9036e-06, -3.3304e-06],
        [-8.9407e-06, -6.5714e-06,  2.6897e-06,  ..., -7.9870e-06,
         -2.6077e-06, -4.5747e-06],
        [-1.2279e-05, -9.0003e-06,  3.6806e-06,  ..., -1.0997e-05,
         -3.5837e-06, -6.2734e-06]], device='cuda:0')
Loss: 1.1412582397460938


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[    0,  7203,   271,  ...,    12,   275,  3338],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7664e-05,  9.4665e-05, -8.6347e-05,  ...,  1.5091e-05,
         -1.6662e-05,  6.8483e-05],
        [-8.0615e-06, -5.9605e-06,  2.4028e-06,  ..., -7.2569e-06,
         -2.3358e-06, -4.1127e-06],
        [-9.7454e-06, -7.1675e-06,  2.9020e-06,  ..., -8.7619e-06,
         -2.8126e-06, -4.9621e-06],
        [-1.3381e-05, -9.8795e-06,  3.9935e-06,  ..., -1.2010e-05,
         -3.8594e-06, -6.8247e-06],
        [-1.8388e-05, -1.3530e-05,  5.4687e-06,  ..., -1.6540e-05,
         -5.3123e-06, -9.3728e-06]], device='cuda:0')
Loss: 1.135072946548462


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7683e-05,  1.1357e-04, -7.9357e-05,  ..., -2.5902e-05,
         -2.0371e-05,  4.8116e-05],
        [-1.0759e-05, -7.9274e-06,  3.1963e-06,  ..., -9.6858e-06,
         -3.0808e-06, -5.4687e-06],
        [-1.2994e-05, -9.5367e-06,  3.8557e-06,  ..., -1.1683e-05,
         -3.7141e-06, -6.5938e-06],
        [-1.7881e-05, -1.3173e-05,  5.3197e-06,  ..., -1.6063e-05,
         -5.1036e-06, -9.0897e-06],
        [-2.4498e-05, -1.8001e-05,  7.2643e-06,  ..., -2.2054e-05,
         -7.0035e-06, -1.2442e-05]], device='cuda:0')
Loss: 1.1446863412857056


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,   278,  6046,  ...,  1671,   199,   395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6534e-05,  1.1574e-04, -1.0060e-04,  ...,  2.9076e-05,
         -5.2137e-05,  1.8286e-05],
        [-1.3441e-05, -9.8795e-06,  4.0159e-06,  ..., -1.2085e-05,
         -3.8520e-06, -6.8098e-06],
        [-1.6257e-05, -1.1921e-05,  4.8615e-06,  ..., -1.4603e-05,
         -4.6529e-06, -8.2329e-06],
        [-2.2322e-05, -1.6421e-05,  6.6906e-06,  ..., -2.0057e-05,
         -6.3777e-06, -1.1325e-05],
        [-3.0607e-05, -2.2441e-05,  9.1344e-06,  ..., -2.7508e-05,
         -8.7619e-06, -1.5497e-05]], device='cuda:0')
Loss: 1.1195210218429565


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   278,  6318,  ...,   458,    17,     9],
        [    0,  7377, 30662,  ...,   287,   694, 13403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5433e-05,  1.4940e-04, -1.1399e-04,  ...,  8.1834e-05,
         -7.8328e-05,  2.9307e-05],
        [-1.6168e-05, -1.1876e-05,  4.8168e-06,  ..., -1.4514e-05,
         -4.6156e-06, -8.1658e-06],
        [-1.9550e-05, -1.4335e-05,  5.8301e-06,  ..., -1.7524e-05,
         -5.5730e-06, -9.8720e-06],
        [-2.6852e-05, -1.9729e-05,  8.0168e-06,  ..., -2.4080e-05,
         -7.6368e-06, -1.3575e-05],
        [-3.6836e-05, -2.7001e-05,  1.0960e-05,  ..., -3.3021e-05,
         -1.0505e-05, -1.8582e-05]], device='cuda:0')
Loss: 1.1238675117492676


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,   759,  1128,  ...,   221,   474,   221],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5323e-05,  1.1061e-04, -1.1402e-04,  ...,  8.9075e-05,
         -9.4830e-05,  1.6837e-05],
        [-1.8865e-05, -1.3873e-05,  5.6289e-06,  ..., -1.6898e-05,
         -5.3793e-06, -9.5293e-06],
        [-2.2829e-05, -1.6749e-05,  6.8136e-06,  ..., -2.0415e-05,
         -6.5006e-06, -1.1533e-05],
        [-3.1352e-05, -2.3052e-05,  9.3803e-06,  ..., -2.8074e-05,
         -8.9109e-06, -1.5855e-05],
        [-4.2975e-05, -3.1531e-05,  1.2808e-05,  ..., -3.8445e-05,
         -1.2241e-05, -2.1681e-05]], device='cuda:0')
Loss: 1.1397958993911743


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0,    12,   287,  ...,   199,   769, 18432]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2535e-05,  1.1473e-04, -1.3302e-04,  ...,  7.7394e-05,
         -9.6796e-05,  3.4523e-05],
        [-2.1607e-05, -1.5870e-05,  6.4187e-06,  ..., -1.9327e-05,
         -6.1467e-06, -1.0885e-05],
        [-2.6122e-05, -1.9133e-05,  7.7598e-06,  ..., -2.3320e-05,
         -7.4171e-06, -1.3158e-05],
        [-3.5882e-05, -2.6345e-05,  1.0692e-05,  ..., -3.2097e-05,
         -1.0177e-05, -1.8105e-05],
        [-4.9174e-05, -3.6031e-05,  1.4588e-05,  ..., -4.3929e-05,
         -1.3970e-05, -2.4736e-05]], device='cuda:0')
Loss: 1.1494847536087036
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4992e-05,  1.9573e-05, -4.6791e-05,  ...,  7.1394e-06,
         -2.6599e-05,  2.5259e-05],
        [-2.1444e-05, -1.5376e-05,  5.0944e-06,  ..., -1.8388e-05,
         -4.4358e-06, -8.9055e-06],
        [ 2.7740e-05,  1.9780e-05, -4.8866e-06,  ...,  2.3412e-05,
          4.4675e-06,  9.5471e-06],
        [-2.2381e-05, -1.6747e-05,  6.2299e-06,  ..., -2.0901e-05,
         -4.9083e-06, -1.0022e-05],
        [-4.6017e-05, -3.2838e-05,  1.0113e-05,  ..., -3.9205e-05,
         -9.1214e-06, -1.9408e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8706e-08, 1.9799e-08, 2.9770e-08,  ..., 2.2068e-08, 5.2063e-08,
         1.1227e-08],
        [4.0268e-11, 2.4030e-11, 1.6500e-12,  ..., 2.7602e-11, 1.2036e-12,
         5.9695e-12],
        [6.5716e-10, 3.8475e-10, 2.3423e-11,  ..., 5.0604e-10, 1.3552e-11,
         9.8083e-11],
        [1.8392e-10, 2.2721e-10, 6.4508e-12,  ..., 1.3711e-10, 1.8516e-11,
         7.7557e-11],
        [1.5693e-10, 8.7525e-11, 8.1867e-12,  ..., 1.0905e-10, 3.3761e-12,
         2.4016e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 50
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  Learning Rate ‚ñà‚ñà‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÅ
wandb: Validation PPL ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 319
wandb:          Epoch 0
wandb:  Learning Rate 2e-05
wandb:   Training PPL 9661.20593
wandb: Validation PPL 10.07775
wandb: 
wandb: üöÄ View run dulcet-forest-310 at: https://wandb.ai/kenotron/brainlessgpt/runs/ah77duc8
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_132855-ah77duc8/logs
