nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_103944-cnbw8ie3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-tree-308
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/cnbw8ie3
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,   334,   287,  ...,  1348,  6139,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2704e-04,  5.9281e-04, -1.6846e-04,  ...,  2.3848e-04,
          4.8112e-04, -5.9746e-04],
        [ 3.9302e-07, -1.9645e-09,  2.4773e-07,  ..., -1.3225e-07,
         -5.5507e-07, -5.6997e-07],
        [ 1.0133e-06, -8.1025e-08,  5.6624e-07,  ..., -1.8999e-07,
         -1.4305e-06, -1.4380e-06],
        [ 6.2212e-07, -3.1432e-08,  3.7067e-07,  ..., -7.9628e-08,
         -8.3074e-07, -8.4192e-07],
        [ 3.5577e-07, -4.9127e-08,  1.5646e-07,  ..., -6.8918e-08,
         -6.0722e-07, -4.9174e-07]], device='cuda:0')
Loss: 1.3798766136169434


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,  1412, 11275,  ...,   668, 14849,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2516e-03,  4.9267e-04, -3.0263e-04,  ...,  4.9037e-04,
          2.9440e-04, -6.7642e-04],
        [ 4.9127e-07, -2.7019e-07,  5.5134e-07,  ..., -1.5169e-07,
         -1.1101e-06, -1.2144e-06],
        [ 1.2089e-06, -8.0746e-07,  1.1623e-06,  ..., -1.7218e-07,
         -2.6897e-06, -2.9728e-06],
        [ 7.3295e-07, -4.0210e-07,  8.0839e-07,  ..., -3.8883e-08,
         -1.5423e-06, -1.6615e-06],
        [ 3.8533e-07, -3.3784e-07,  3.1199e-07,  ..., -5.0175e-08,
         -1.1586e-06, -1.0356e-06]], device='cuda:0')
Loss: 1.3779733180999756


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7629e-03,  4.1507e-04, -1.6026e-04,  ...,  3.5682e-04,
         -2.1228e-04,  2.5696e-04],
        [ 8.7498e-07, -1.4166e-07,  5.5827e-07,  ...,  3.0850e-08,
         -2.1383e-06, -1.6689e-06],
        [ 2.3711e-06, -3.9581e-07,  1.1192e-06,  ...,  4.0897e-07,
         -5.6103e-06, -4.3660e-06],
        [ 1.3663e-06, -1.7951e-07,  8.6357e-07,  ...,  3.0384e-07,
         -3.0771e-06, -2.4326e-06],
        [ 7.5600e-07, -2.2375e-07,  1.7788e-07,  ...,  1.7800e-07,
         -2.2762e-06, -1.4417e-06]], device='cuda:0')
Loss: 1.3773101568222046


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  7555,  3908,  ...,   259,  8477,   278],
        [    0,  2895,    26,  ..., 11645,  1535,  1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4040e-03, -8.6908e-04,  5.9628e-05,  ...,  4.3353e-04,
         -9.7745e-04,  1.4020e-04],
        [ 9.7090e-07, -1.3977e-07,  6.9238e-07,  ...,  2.2270e-07,
         -2.7642e-06, -2.1420e-06],
        [ 2.6375e-06, -5.2061e-07,  1.3968e-06,  ...,  1.0795e-06,
         -7.3835e-06, -5.7891e-06],
        [ 1.4938e-06, -2.0291e-07,  1.0387e-06,  ...,  6.5961e-07,
         -3.8855e-06, -3.1143e-06],
        [ 7.6194e-07, -2.5635e-07,  1.9441e-07,  ...,  4.0897e-07,
         -2.9393e-06, -1.8366e-06]], device='cuda:0')
Loss: 1.3781037330627441


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[  0, 368, 266,  ..., 591, 767, 824],
        [  0,  25,  26,  ...,   9, 287, 298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0139e-03, -8.3908e-04,  7.8833e-05,  ...,  8.6843e-04,
         -1.3492e-03,  1.0800e-04],
        [ 1.0435e-06,  5.5152e-09,  9.5688e-07,  ...,  9.0455e-08,
         -3.3192e-06, -2.6021e-06],
        [ 2.7847e-06, -2.7288e-07,  1.9444e-06,  ...,  8.9139e-07,
         -8.6874e-06, -7.0110e-06],
        [ 1.6168e-06, -3.6205e-08,  1.4298e-06,  ...,  5.4738e-07,
         -4.5821e-06, -3.8333e-06],
        [ 8.2387e-07, -1.6787e-07,  3.7323e-07,  ...,  2.9721e-07,
         -3.5465e-06, -2.2389e-06]], device='cuda:0')
Loss: 1.3758723735809326


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0,   29,  413,  ..., 1527, 1503,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5182e-03, -8.5118e-04,  2.6907e-04,  ...,  1.3948e-03,
         -4.1878e-04, -8.5133e-04],
        [ 7.5111e-07,  3.7879e-08,  1.0412e-06,  ..., -1.1630e-07,
         -4.0270e-06, -2.9095e-06],
        [ 2.1439e-06, -1.9837e-07,  2.0501e-06,  ...,  4.3504e-07,
         -1.0513e-05, -7.6964e-06],
        [ 1.3020e-06, -4.7730e-09,  1.5816e-06,  ...,  3.4156e-07,
         -5.4911e-06, -4.1779e-06],
        [ 4.8673e-07, -1.3737e-07,  3.4086e-07,  ...,  7.8348e-08,
         -4.4331e-06, -2.4671e-06]], device='cuda:0')
Loss: 1.3811368942260742


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[    0,   221,   825,  ...,   616,  3661,  8052],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3373e-03, -1.0179e-03,  2.7495e-04,  ...,  1.6629e-03,
         -2.8341e-04, -1.0099e-03],
        [ 7.8115e-07, -4.0357e-07,  1.5031e-06,  ..., -2.0757e-07,
         -4.7535e-06, -3.4496e-06],
        [ 2.1923e-06, -1.3160e-06,  3.0186e-06,  ...,  3.5216e-07,
         -1.2197e-05, -9.0376e-06],
        [ 1.3472e-06, -6.3435e-07,  2.2820e-06,  ...,  3.1723e-07,
         -6.4969e-06, -4.9379e-06],
        [ 4.9477e-07, -5.3598e-07,  6.3889e-07,  ...,  3.7835e-08,
         -5.1260e-06, -2.9216e-06]], device='cuda:0')
Loss: 1.37653386592865


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2159e-03, -1.1212e-03,  5.5159e-04,  ...,  9.4682e-04,
         -5.9043e-04, -6.5467e-04],
        [ 1.0214e-06, -3.6515e-07,  2.0209e-06,  ..., -1.4098e-07,
         -5.5693e-06, -4.0978e-06],
        [ 2.8852e-06, -1.3034e-06,  4.3076e-06,  ...,  6.0920e-07,
         -1.4536e-05, -1.0878e-05],
        [ 1.7439e-06, -5.2864e-07,  3.0531e-06,  ...,  4.6904e-07,
         -7.7114e-06, -5.8506e-06],
        [ 7.2480e-07, -5.1595e-07,  1.0990e-06,  ...,  1.2491e-07,
         -6.0871e-06, -3.5623e-06]], device='cuda:0')
Loss: 1.376060128211975
Graident accumulation at epoch 0, step 7, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2159e-04, -1.1212e-04,  5.5159e-05,  ...,  9.4682e-05,
         -5.9043e-05, -6.5467e-05],
        [ 1.0214e-07, -3.6515e-08,  2.0209e-07,  ..., -1.4098e-08,
         -5.5693e-07, -4.0978e-07],
        [ 2.8852e-07, -1.3034e-07,  4.3076e-07,  ...,  6.0920e-08,
         -1.4536e-06, -1.0878e-06],
        [ 1.7439e-07, -5.2864e-08,  3.0531e-07,  ...,  4.6904e-08,
         -7.7114e-07, -5.8506e-07],
        [ 7.2480e-08, -5.1595e-08,  1.0990e-07,  ...,  1.2491e-08,
         -6.0871e-07, -3.5623e-07]], device='cuda:0')
optimizer state dict: tensor([[2.7206e-08, 1.2571e-09, 3.0425e-10,  ..., 8.9647e-10, 3.4860e-10,
         4.2859e-10],
        [1.0433e-15, 1.3334e-16, 4.0841e-15,  ..., 1.9875e-17, 3.1017e-14,
         1.6792e-14],
        [8.3246e-15, 1.6988e-15, 1.8555e-14,  ..., 3.7113e-16, 2.1130e-13,
         1.1833e-13],
        [3.0412e-15, 2.7946e-16, 9.3215e-15,  ..., 2.2000e-16, 5.9465e-14,
         3.4229e-14],
        [5.2534e-16, 2.6621e-16, 1.2077e-15,  ..., 1.5603e-17, 3.7053e-14,
         1.2690e-14]], device='cuda:0')
optimizer state dict: 1.0
lr: [5.0890585241730285e-06, 5.0890585241730285e-06]
scheduler_last_epoch: 1


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[    0,  9657,   300,  ...,    12,   271,   266],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6306e-04, -5.3534e-04,  7.0934e-04,  ...,  6.1408e-04,
         -1.2313e-03, -6.9040e-04],
        [ 2.1304e-08, -1.9791e-08,  3.4645e-07,  ...,  2.8312e-07,
         -1.0431e-06, -4.1537e-07],
        [ 3.9116e-08, -1.9278e-07,  8.0466e-07,  ...,  7.6741e-07,
         -2.6077e-06, -1.1399e-06],
        [ 3.8883e-08, -5.9139e-08,  5.1409e-07,  ...,  4.7125e-07,
         -1.3486e-06, -5.9605e-07],
        [-1.0768e-08, -6.7521e-08,  2.5332e-07,  ...,  3.1665e-07,
         -1.0878e-06, -3.5390e-07]], device='cuda:0')
Loss: 1.3802980184555054


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6015e-04, -1.8815e-03,  5.2908e-04,  ...,  1.2968e-03,
         -1.8072e-03, -7.4086e-04],
        [ 2.5413e-07, -4.9477e-07,  6.9663e-07,  ...,  2.0768e-07,
         -1.8440e-06, -7.9162e-07],
        [ 5.3085e-07, -1.4221e-06,  1.4305e-06,  ...,  6.2492e-07,
         -4.5300e-06, -2.0787e-06],
        [ 3.7230e-07, -8.1537e-07,  9.9093e-07,  ...,  4.2445e-07,
         -2.4438e-06, -1.1213e-06],
        [ 1.4383e-07, -4.9593e-07,  4.4238e-07,  ...,  2.7032e-07,
         -1.8217e-06, -6.1840e-07]], device='cuda:0')
Loss: 1.3787602186203003


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[    0,    73,    30,  ...,  4112,    12,  9416],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5080e-04, -1.9903e-03,  7.3113e-04,  ...,  1.8867e-03,
         -1.9853e-03, -7.9921e-04],
        [ 3.6124e-07, -5.7067e-07,  1.1027e-06,  ...,  6.4261e-08,
         -2.4512e-06, -1.4734e-06],
        [-2.2072e-05, -1.4559e-05, -3.9971e-05,  ...,  1.1649e-04,
         -3.8392e-05, -1.5753e-05],
        [ 5.4925e-07, -9.0478e-07,  1.6093e-06,  ...,  3.2806e-07,
         -3.2485e-06, -2.1644e-06],
        [ 1.8853e-07, -6.4960e-07,  6.8080e-07,  ...,  1.3155e-07,
         -2.4661e-06, -1.2070e-06]], device='cuda:0')
Loss: 1.3779780864715576


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[   0, 8416,  669,  ...,  298,  894,  496],
        [   0, 4890, 1528,  ...,  847,  328, 1703]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6752e-04, -2.1329e-03,  1.6270e-03,  ...,  2.4249e-03,
         -1.4135e-03, -4.3775e-04],
        [ 5.5961e-07, -5.9546e-07,  1.4082e-06,  ..., -1.8533e-07,
         -3.3081e-06, -2.0284e-06],
        [-2.1540e-05, -1.4756e-05, -3.9230e-05,  ...,  1.1591e-04,
         -4.0777e-05, -1.7273e-05],
        [ 9.0688e-07, -1.0096e-06,  2.1197e-06,  ...,  2.2585e-08,
         -4.5300e-06, -2.9877e-06],
        [ 3.3661e-07, -7.2271e-07,  8.8383e-07,  ..., -1.0128e-07,
         -3.4273e-06, -1.6671e-06]], device='cuda:0')
Loss: 1.378662347793579


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4792e-03, -1.8712e-03,  1.6975e-03,  ...,  3.0601e-03,
         -2.3207e-03,  2.6368e-04],
        [ 1.0197e-06, -7.5845e-07,  2.1309e-06,  ..., -3.8557e-07,
         -4.1947e-06, -2.6282e-06],
        [-2.0407e-05, -1.5300e-05, -3.7509e-05,  ...,  1.1553e-04,
         -4.3027e-05, -1.8815e-05],
        [ 1.6445e-06, -1.3746e-06,  3.2745e-06,  ..., -1.6461e-07,
         -5.7667e-06, -3.8818e-06],
        [ 6.7003e-07, -8.6986e-07,  1.3905e-06,  ..., -2.5216e-07,
         -4.2096e-06, -2.1197e-06]], device='cuda:0')
Loss: 1.3743202686309814


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 1871,  401,  ...,   14, 4797,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5415e-03, -1.7045e-03,  1.3520e-03,  ...,  3.3409e-03,
         -2.1817e-03, -1.2775e-03],
        [ 1.2581e-06, -8.5623e-07,  2.5090e-06,  ..., -6.4820e-07,
         -4.9509e-06, -3.0883e-06],
        [-1.9867e-05, -1.5591e-05, -3.6589e-05,  ...,  1.1500e-04,
         -4.4882e-05, -2.0037e-05],
        [ 2.0654e-06, -1.5181e-06,  3.9861e-06,  ..., -4.4401e-07,
         -6.9737e-06, -4.6454e-06],
        [ 9.3080e-07, -1.0123e-06,  1.7332e-06,  ..., -4.9989e-07,
         -5.1036e-06, -2.5760e-06]], device='cuda:0')
Loss: 1.378920078277588


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[    0, 21413,  1735,  ..., 10789, 12523,    12],
        [    0,  2771,    13,  ...,  1412,    35,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5733e-03, -2.7006e-03,  1.8157e-03,  ...,  3.4644e-03,
         -2.6538e-03, -1.9089e-03],
        [ 1.3128e-06, -1.2884e-06,  3.0715e-06,  ..., -3.3155e-07,
         -5.8301e-06, -3.8035e-06],
        [-2.0023e-05, -1.6731e-05, -3.5262e-05,  ...,  1.1589e-04,
         -4.6953e-05, -2.1870e-05],
        [ 2.1846e-06, -2.0992e-06,  4.7386e-06,  ...,  3.2829e-08,
         -8.0913e-06, -5.5879e-06],
        [ 9.1229e-07, -1.4035e-06,  2.1299e-06,  ..., -2.2235e-07,
         -5.8785e-06, -3.1423e-06]], device='cuda:0')
Loss: 1.3786959648132324


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7581e-03, -3.1079e-03,  2.5922e-03,  ...,  3.1623e-03,
         -2.9356e-03, -1.4865e-03],
        [ 1.6351e-06, -1.0984e-06,  3.4645e-06,  ..., -4.6380e-07,
         -6.9775e-06, -4.4033e-06],
        [-1.9315e-05, -1.6427e-05, -3.4491e-05,  ...,  1.1567e-04,
         -4.9635e-05, -2.3345e-05],
        [ 2.6131e-06, -1.8887e-06,  5.2862e-06,  ..., -4.0745e-08,
         -9.5963e-06, -6.4336e-06],
        [ 1.1898e-06, -1.2629e-06,  2.3497e-06,  ..., -2.7474e-07,
         -6.9961e-06, -3.6415e-06]], device='cuda:0')
Loss: 1.374688982963562
Graident accumulation at epoch 0, step 15, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4524e-04, -4.1169e-04,  3.0886e-04,  ...,  4.0145e-04,
         -3.4670e-04, -2.0757e-04],
        [ 2.5543e-07, -1.4270e-07,  5.2833e-07,  ..., -5.9068e-08,
         -1.1990e-06, -8.0913e-07],
        [-1.6718e-06, -1.7600e-06, -3.0614e-06,  ...,  1.1622e-05,
         -6.2718e-06, -3.3135e-06],
        [ 4.1826e-07, -2.3645e-07,  8.0340e-07,  ...,  3.8139e-08,
         -1.6537e-06, -1.1699e-06],
        [ 1.8421e-07, -1.7272e-07,  3.3388e-07,  ..., -1.6232e-08,
         -1.2475e-06, -6.8475e-07]], device='cuda:0')
optimizer state dict: tensor([[3.0270e-08, 1.0915e-08, 7.0232e-09,  ..., 1.0896e-08, 8.9659e-09,
         2.6377e-09],
        [3.7157e-15, 1.3396e-15, 1.6083e-14,  ..., 2.3496e-16, 7.9671e-14,
         3.6164e-14],
        [3.8138e-13, 2.7155e-13, 1.2082e-12,  ..., 1.3379e-11, 2.6747e-12,
         6.6320e-13],
        [9.8662e-15, 3.8465e-15, 3.7256e-14,  ..., 2.2144e-16, 1.5150e-13,
         7.5586e-14],
        [1.9405e-15, 1.8608e-15, 6.7277e-15,  ..., 9.1070e-17, 8.5961e-14,
         2.5938e-14]], device='cuda:0')
optimizer state dict: 2.0
lr: [1.0178117048346057e-05, 1.0178117048346057e-05]
scheduler_last_epoch: 2


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    27,  3961,  ...,   462,   221,   474],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4483e-04, -1.7383e-05,  9.3854e-04,  ...,  3.2340e-04,
         -3.6396e-04, -2.9746e-04],
        [-1.8254e-07, -1.2666e-06,  7.2271e-07,  ..., -2.2817e-07,
         -4.4331e-07, -7.4878e-07],
        [-4.6380e-07, -2.7716e-06,  1.4678e-06,  ..., -4.3772e-07,
         -9.0897e-07, -1.6689e-06],
        [-1.9651e-07, -1.5572e-06,  8.7172e-07,  ..., -1.9837e-07,
         -5.8115e-07, -9.3877e-07],
        [-2.0303e-07, -1.3262e-06,  6.2212e-07,  ..., -2.2072e-07,
         -5.2899e-07, -7.1898e-07]], device='cuda:0')
Loss: 1.3484306335449219


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,   13, 2920,  609],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4122e-04,  8.7827e-05,  1.5309e-03,  ...,  1.3538e-04,
         -7.4824e-04,  2.6012e-04],
        [-4.7497e-07, -2.3246e-06,  1.2517e-06,  ..., -8.2795e-07,
         -1.1027e-06, -1.2107e-06],
        [-1.1045e-06, -5.2601e-06,  2.5779e-06,  ..., -1.7639e-06,
         -2.4587e-06, -2.7716e-06],
        [-4.7591e-07, -2.6152e-06,  1.4715e-06,  ..., -7.7952e-07,
         -1.2629e-06, -1.4268e-06],
        [-5.6438e-07, -2.5630e-06,  1.0226e-06,  ..., -8.4285e-07,
         -1.2927e-06, -1.0598e-06]], device='cuda:0')
Loss: 1.3509353399276733


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0, 3408,  300,  ...,   14, 5870,   12],
        [   0, 2314,  266,  ...,  342, 7299, 1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5777e-03, -5.4679e-04,  1.9527e-03,  ..., -3.0752e-04,
         -1.3388e-03,  9.3224e-04],
        [-9.2946e-07, -3.4869e-06,  1.5739e-06,  ..., -1.1818e-06,
         -2.0564e-06, -1.8366e-06],
        [-2.9867e-05,  1.3636e-04,  5.8682e-06,  ...,  6.5392e-05,
          2.1039e-05,  1.6581e-05],
        [-1.0012e-06, -3.9488e-06,  1.8589e-06,  ..., -1.0906e-06,
         -2.2314e-06, -2.1867e-06],
        [-1.1083e-06, -3.7774e-06,  1.1930e-06,  ..., -1.1558e-06,
         -2.3283e-06, -1.6149e-06]], device='cuda:0')
Loss: 1.3480736017227173


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[    0,   365,  5911,  ...,   925,   408,   266],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9934e-03, -7.1216e-04,  2.3076e-03,  ...,  5.2333e-04,
         -1.5931e-03,  8.9054e-04],
        [-1.2964e-06, -4.8578e-06,  2.2147e-06,  ..., -1.4594e-06,
         -2.8312e-06, -2.7195e-06],
        [-3.0698e-05,  1.3297e-04,  7.2391e-06,  ...,  6.4860e-05,
          1.9280e-05,  1.4435e-05],
        [-1.3402e-06, -5.2825e-06,  2.4661e-06,  ..., -1.2871e-06,
         -2.9020e-06, -2.9989e-06],
        [-1.5143e-06, -5.2452e-06,  1.7220e-06,  ..., -1.4054e-06,
         -3.1628e-06, -2.4457e-06]], device='cuda:0')
Loss: 1.3493497371673584


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 10251,   278,  ...,   278,   319,    13],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1656e-03, -1.6622e-03,  2.4576e-03,  ...,  1.5040e-03,
         -2.0245e-03,  9.2902e-04],
        [-1.8403e-06, -6.2361e-06,  2.7400e-06,  ..., -1.8971e-06,
         -3.3155e-06, -3.6806e-06],
        [-3.1972e-05,  1.2980e-04,  8.3716e-06,  ...,  6.4007e-05,
          1.8192e-05,  1.2185e-05],
        [-1.9474e-06, -6.8918e-06,  3.1441e-06,  ..., -1.7062e-06,
         -3.4049e-06, -4.1015e-06],
        [-2.1439e-06, -6.7353e-06,  2.1635e-06,  ..., -1.8356e-06,
         -3.7067e-06, -3.4068e-06]], device='cuda:0')
Loss: 1.3491437435150146


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3156e-03, -1.6141e-03,  2.5600e-03,  ...,  1.9328e-03,
         -2.1995e-03,  5.7814e-04],
        [-2.2911e-06, -7.7337e-06,  3.5670e-06,  ..., -2.0703e-06,
         -4.5002e-06, -4.7162e-06],
        [-3.2963e-05,  1.2628e-04,  1.0182e-05,  ...,  6.3978e-05,
          1.5629e-05,  9.8754e-06],
        [-2.4540e-06, -8.5309e-06,  4.0941e-06,  ..., -1.7690e-06,
         -4.6864e-06, -5.2117e-06],
        [-2.7064e-06, -8.2776e-06,  2.8824e-06,  ..., -1.9697e-06,
         -5.0031e-06, -4.4052e-06]], device='cuda:0')
Loss: 1.3514447212219238


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[    0,  1635,   266,  ...,   437,  3302,   287],
        [    0,  2278,   292,  ..., 12060,  1319,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4881e-03, -2.4970e-03,  2.5842e-03,  ...,  2.3524e-03,
         -3.7189e-03,  6.1590e-04],
        [-2.5909e-06, -8.9481e-06,  4.1332e-06,  ..., -2.3665e-06,
         -5.2303e-06, -5.5842e-06],
        [-3.3648e-05,  1.2359e-04,  1.1329e-05,  ...,  6.3464e-05,
          1.4072e-05,  8.0128e-06],
        [-2.7185e-06, -9.8124e-06,  4.7199e-06,  ..., -1.9786e-06,
         -5.3942e-06, -6.1058e-06],
        [-3.0603e-06, -9.7007e-06,  3.4040e-06,  ..., -2.2268e-06,
         -5.9083e-06, -5.2433e-06]], device='cuda:0')
Loss: 1.3446325063705444


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   352,   644,  ...,  2928,   590,  3040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8383e-03, -1.4850e-03,  2.7779e-03,  ...,  2.7235e-03,
         -4.0871e-03,  8.9337e-04],
        [-3.1348e-06, -1.0028e-05,  5.2135e-06,  ..., -2.8806e-06,
         -6.0759e-06, -6.2995e-06],
        [-3.4825e-05,  1.2138e-04,  1.3282e-05,  ...,  6.2495e-05,
          1.2411e-05,  6.5748e-06],
        [-3.2922e-06, -1.0997e-05,  5.9865e-06,  ..., -2.4666e-06,
         -6.2883e-06, -6.9216e-06],
        [-3.7011e-06, -1.0908e-05,  4.4173e-06,  ..., -2.7334e-06,
         -6.8508e-06, -5.9027e-06]], device='cuda:0')
Loss: 1.347732424736023
Graident accumulation at epoch 0, step 23, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6455e-04, -5.1903e-04,  5.5577e-04,  ...,  6.3365e-04,
         -7.2074e-04, -9.7473e-05],
        [-8.3593e-08, -1.1313e-06,  9.9685e-07,  ..., -3.4122e-07,
         -1.6867e-06, -1.3582e-06],
        [-4.9871e-06,  1.0554e-05, -1.4271e-06,  ...,  1.6709e-05,
         -4.4035e-06, -2.3247e-06],
        [ 4.7209e-08, -1.3125e-06,  1.3217e-06,  ..., -2.1234e-07,
         -2.1171e-06, -1.7451e-06],
        [-2.0431e-07, -1.2462e-06,  7.4222e-07,  ..., -2.8795e-07,
         -1.8078e-06, -1.2066e-06]], device='cuda:0')
optimizer state dict: tensor([[3.8295e-08, 1.3109e-08, 1.4733e-08,  ..., 1.8302e-08, 2.5661e-08,
         3.4332e-09],
        [1.3539e-14, 1.0191e-13, 4.3248e-14,  ..., 8.5325e-15, 1.1651e-13,
         7.5811e-14],
        [1.5938e-12, 1.5005e-11, 1.3834e-12,  ..., 1.7272e-11, 2.8261e-12,
         7.0576e-13],
        [2.0695e-14, 1.2478e-13, 7.3057e-14,  ..., 6.3054e-15, 1.9089e-13,
         1.2342e-13],
        [1.5637e-14, 1.2084e-13, 2.6233e-14,  ..., 7.5626e-15, 1.3281e-13,
         6.0754e-14]], device='cuda:0')
optimizer state dict: 3.0
lr: [1.5267175572519086e-05, 1.5267175572519086e-05]
scheduler_last_epoch: 3


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[   0,   14,  496,  ...,  368,  259,  490],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3868e-04,  7.3968e-04,  6.8484e-04,  ...,  5.9814e-04,
          3.9275e-04,  3.2364e-04],
        [-2.1458e-06, -3.5465e-06,  6.2399e-08,  ..., -1.3486e-06,
         -3.7625e-07, -9.6858e-07],
        [-3.2783e-06, -5.5730e-06,  7.5903e-08,  ..., -1.9968e-06,
         -5.2899e-07, -1.4976e-06],
        [-1.5423e-06, -2.5928e-06,  6.1002e-08,  ..., -9.3877e-07,
         -2.6636e-07, -6.8545e-07],
        [-3.2485e-06, -5.3346e-06, -3.6554e-08,  ..., -1.9968e-06,
         -6.5565e-07, -1.3560e-06]], device='cuda:0')
Loss: 1.3075193166732788


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[    0,  3037,  4511,  ...,  1711,    12,  2655],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9847e-04,  6.1397e-04,  3.4986e-04,  ...,  7.5629e-04,
         -6.8904e-04,  9.0618e-04],
        [-4.0978e-06, -7.3910e-06,  1.0245e-07,  ..., -2.6897e-06,
         -4.0524e-07, -2.4140e-06],
        [-6.1542e-06, -1.1325e-05,  5.5530e-08,  ..., -3.9190e-06,
         -5.0117e-07, -3.6731e-06],
        [-2.8536e-06, -5.2601e-06,  1.1339e-07,  ..., -1.8291e-06,
         -2.8004e-07, -1.6764e-06],
        [-5.4985e-06, -9.7454e-06, -1.1944e-07,  ..., -3.5167e-06,
         -7.4180e-07, -2.9057e-06]], device='cuda:0')
Loss: 1.3114036321640015


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   369, 17432,  ...,   874,  2577,    14],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9166e-04,  4.3836e-04,  2.9790e-04,  ...,  1.0272e-03,
         -1.0044e-03,  1.1130e-03],
        [-5.6475e-06, -1.0714e-05,  3.9488e-07,  ..., -3.7998e-06,
         -5.2538e-07, -3.2783e-06],
        [-8.6874e-06, -1.6749e-05,  4.6717e-07,  ..., -5.6401e-06,
         -6.7719e-07, -5.0813e-06],
        [-3.9786e-06, -7.7337e-06,  3.4901e-07,  ..., -2.5965e-06,
         -3.9459e-07, -2.2911e-06],
        [-7.4804e-06, -1.3977e-05,  1.2270e-07,  ..., -4.8429e-06,
         -9.6066e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.3113595247268677


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0387e-04,  5.3102e-04, -8.0512e-05,  ...,  1.2560e-03,
         -1.3830e-03,  9.0085e-04],
        [-7.5996e-06, -1.4350e-05,  8.3074e-07,  ..., -4.9546e-06,
         -4.3411e-07, -4.1761e-06],
        [-1.1504e-05, -2.1964e-05,  1.0669e-06,  ..., -7.2271e-06,
         -5.8126e-07, -6.3926e-06],
        [-5.2527e-06, -1.0088e-05,  6.7311e-07,  ..., -3.3006e-06,
         -3.2800e-07, -2.8536e-06],
        [-9.9391e-06, -1.8477e-05,  5.5856e-07,  ..., -6.2734e-06,
         -9.5965e-07, -4.8652e-06]], device='cuda:0')
Loss: 1.308286190032959


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   20,   13,  ...,  496,   14, 1032],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3749e-03,  5.4972e-04, -2.6179e-04,  ...,  1.3544e-03,
         -1.4965e-03,  7.7450e-04],
        [-9.3579e-06, -1.8060e-05,  1.2033e-06,  ..., -6.4746e-06,
         -1.1047e-06, -4.8168e-06],
        [-1.4216e-05, -2.7657e-05,  1.5699e-06,  ..., -9.4771e-06,
         -1.5871e-06, -7.3537e-06],
        [-6.6012e-06, -1.3009e-05,  9.9163e-07,  ..., -4.4182e-06,
         -8.3464e-07, -3.3155e-06],
        [-1.2234e-05, -2.3276e-05,  9.3109e-07,  ..., -8.2105e-06,
         -1.8276e-06, -5.5581e-06]], device='cuda:0')
Loss: 1.3143407106399536


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[   0,  721, 1119,  ...,  600,  328, 3363],
        [   0, 1478,   14,  ...,  266, 9417, 9105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1204e-03,  4.5568e-04, -7.5678e-04,  ...,  1.6251e-03,
         -1.4566e-03,  7.5711e-04],
        [-1.1064e-05, -2.1726e-05,  2.1383e-06,  ..., -7.4953e-06,
         -1.5163e-06, -5.5432e-06],
        [-1.6764e-05, -3.3140e-05,  2.9110e-06,  ..., -1.0945e-05,
         -2.1831e-06, -8.5011e-06],
        [-7.9125e-06, -1.5855e-05,  1.7702e-06,  ..., -5.1335e-06,
         -1.1271e-06, -3.8594e-06],
        [-1.4439e-05, -2.8044e-05,  2.0338e-06,  ..., -9.4473e-06,
         -2.4200e-06, -6.3665e-06]], device='cuda:0')
Loss: 1.309226393699646


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,   12, 2418,  ...,  446,  381, 2204],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0864e-03,  1.2036e-04, -8.8027e-04,  ...,  1.4413e-03,
         -1.4971e-03,  9.0656e-04],
        [-1.3314e-05, -2.5660e-05,  2.8014e-06,  ..., -8.8662e-06,
         -1.6015e-06, -6.4820e-06],
        [-2.0161e-05, -3.9071e-05,  3.8795e-06,  ..., -1.2942e-05,
         -2.3154e-06, -9.9838e-06],
        [-9.4250e-06, -1.8522e-05,  2.2694e-06,  ..., -6.0052e-06,
         -1.1743e-06, -4.4852e-06],
        [-1.7211e-05, -3.2842e-05,  2.7118e-06,  ..., -1.1101e-05,
         -2.5652e-06, -7.4022e-06]], device='cuda:0')
Loss: 1.3009275197982788


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[   0, 2706,  292,  ...,   13, 8954,   13],
        [   0, 1596, 2700,  ...,  943,  266, 4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8310e-03,  5.0697e-04, -8.3467e-04,  ...,  8.5976e-04,
         -2.1446e-03,  2.6743e-04],
        [-1.5520e-05, -2.9415e-05,  3.0808e-06,  ..., -1.0483e-05,
         -2.4919e-06, -7.5176e-06],
        [-2.3574e-05, -4.4912e-05,  4.2334e-06,  ..., -1.5400e-05,
         -3.6937e-06, -1.1608e-05],
        [-6.7135e-05,  1.3835e-04, -2.7031e-05,  ...,  1.5749e-04,
         -4.5617e-05,  9.8809e-05],
        [-2.0251e-05, -3.7998e-05,  2.9232e-06,  ..., -1.3322e-05,
         -3.8989e-06, -8.7284e-06]], device='cuda:0')
Loss: 1.314985990524292
Graident accumulation at epoch 0, step 31, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0612e-03, -4.1643e-04,  4.1672e-04,  ...,  6.5626e-04,
         -8.6312e-04, -6.0983e-05],
        [-1.6272e-06, -3.9596e-06,  1.2053e-06,  ..., -1.3554e-06,
         -1.7672e-06, -1.9741e-06],
        [-6.8458e-06,  5.0075e-06, -8.6108e-07,  ...,  1.3498e-05,
         -4.3325e-06, -3.2530e-06],
        [-6.6710e-06,  1.2653e-05, -1.5135e-06,  ...,  1.5558e-05,
         -6.4671e-06,  8.3103e-06],
        [-2.2090e-06, -4.9214e-06,  9.6031e-07,  ..., -1.5913e-06,
         -2.0169e-06, -1.9587e-06]], device='cuda:0')
optimizer state dict: tensor([[4.6271e-08, 1.3353e-08, 1.5415e-08,  ..., 1.9023e-08, 3.0235e-08,
         3.5013e-09],
        [2.5438e-13, 9.6704e-13, 5.2696e-14,  ..., 1.1842e-13, 1.2260e-13,
         1.3225e-13],
        [2.1479e-12, 1.7007e-11, 1.3999e-12,  ..., 1.7491e-11, 2.8369e-12,
         8.3980e-13],
        [4.5278e-12, 1.9264e-11, 8.0364e-13,  ..., 2.4810e-11, 2.2716e-12,
         9.8865e-12],
        [4.2571e-13, 1.5646e-12, 3.4752e-14,  ..., 1.8502e-13, 1.4788e-13,
         1.3688e-13]], device='cuda:0')
optimizer state dict: 4.0
lr: [1.9999985024557586e-05, 1.9999985024557586e-05]
scheduler_last_epoch: 4


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 6088, 1172,  ...,  546,  401,  925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2200e-04,  5.8591e-05, -9.8375e-05,  ...,  9.5853e-05,
         -3.5894e-05, -7.3428e-05],
        [-4.1425e-06, -5.4240e-06, -5.1409e-07,  ..., -2.6673e-06,
         -4.3213e-07, -6.4075e-07],
        [-4.2915e-06, -5.6326e-06, -5.6997e-07,  ..., -2.7567e-06,
         -4.5262e-07, -6.8545e-07],
        [-2.2948e-06, -3.0398e-06, -2.7940e-07,  ..., -1.4529e-06,
         -2.1979e-07, -3.6508e-07],
        [-5.9903e-06, -7.8678e-06, -8.6799e-07,  ..., -3.8445e-06,
         -6.8545e-07, -8.7544e-07]], device='cuda:0')
Loss: 1.2769209146499634


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9758e-04,  2.2580e-04, -3.8744e-04,  ...,  3.2686e-04,
         -4.1517e-04, -3.0354e-04],
        [-8.3148e-06, -1.0729e-05, -6.0769e-07,  ..., -5.2303e-06,
         -9.0897e-07, -1.4342e-06],
        [-8.4937e-06, -1.0997e-05, -6.8359e-07,  ..., -5.3048e-06,
         -9.4064e-07, -1.4827e-06],
        [-4.7982e-06, -6.2436e-06, -3.2689e-07,  ..., -2.9728e-06,
         -5.0291e-07, -8.2701e-07],
        [-1.2189e-05, -1.5736e-05, -1.1418e-06,  ..., -7.6443e-06,
         -1.4380e-06, -1.9483e-06]], device='cuda:0')
Loss: 1.2707602977752686


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0, 2652,  271,  ...,  634, 1921,  266],
        [   0,  266, 7407,  ...,  287,  365, 4371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8757e-04,  8.2649e-05, -2.3527e-04,  ...,  4.7161e-04,
         -2.2145e-04, -1.1024e-04],
        [-1.2279e-05, -1.6361e-05, -7.1758e-07,  ..., -7.5847e-06,
         -1.1073e-06, -2.5444e-06],
        [-1.2487e-05, -1.6719e-05, -8.0932e-07,  ..., -7.6890e-06,
         -1.1455e-06, -2.6152e-06],
        [ 2.5186e-04,  4.5374e-04,  5.2902e-05,  ...,  1.6775e-04,
          5.8116e-05,  1.3643e-04],
        [-1.8090e-05, -2.4140e-05, -1.3858e-06,  ..., -1.1116e-05,
         -1.7956e-06, -3.4906e-06]], device='cuda:0')
Loss: 1.2812362909317017


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,  4263,  4865,  ...,  1878,   278,  4450],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6966e-04,  3.9345e-04, -2.3527e-04,  ...,  7.4857e-04,
         -1.7311e-04, -7.0592e-04],
        [-1.6391e-05, -2.1845e-05, -1.1981e-06,  ..., -1.0073e-05,
         -8.5961e-07, -3.0845e-06],
        [-1.6659e-05, -2.2292e-05, -1.3197e-06,  ..., -1.0192e-05,
         -9.0152e-07, -3.1590e-06],
        [ 2.4949e-04,  4.5058e-04,  5.2641e-05,  ...,  1.6634e-04,
          5.8262e-05,  1.3615e-04],
        [-2.5064e-05, -3.3379e-05, -2.3469e-06,  ..., -1.5318e-05,
         -1.4193e-06, -4.3139e-06]], device='cuda:0')
Loss: 1.2804598808288574


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,   510,    13,  ...,  3454,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6966e-04,  3.0180e-04, -9.9705e-05,  ...,  8.6434e-04,
          1.5984e-04, -7.7850e-04],
        [-2.0444e-05, -2.7329e-05, -1.7271e-06,  ..., -1.2428e-05,
         -6.2771e-07, -3.7774e-06],
        [-2.0832e-05, -2.7895e-05, -1.8859e-06,  ..., -1.2577e-05,
         -6.9197e-07, -3.8482e-06],
        [ 2.4707e-04,  4.4730e-04,  5.2351e-05,  ...,  1.6496e-04,
          5.8401e-05,  1.3575e-04],
        [-3.1739e-05, -4.2260e-05, -3.3602e-06,  ..., -1.9163e-05,
         -1.1306e-06, -5.2415e-06]], device='cuda:0')
Loss: 1.2660402059555054


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0758e-03,  1.2743e-04, -4.5279e-04,  ...,  9.6178e-04,
         -2.1169e-04, -7.4493e-04],
        [-2.4229e-05, -3.2693e-05, -1.9516e-06,  ..., -1.4618e-05,
         -6.6799e-07, -4.2189e-06],
        [-2.4945e-05, -3.3736e-05, -2.1728e-06,  ..., -1.4916e-05,
         -7.5018e-07, -4.3157e-06],
        [ 2.4485e-04,  4.4414e-04,  5.2230e-05,  ...,  1.6369e-04,
          5.8394e-05,  1.3548e-04],
        [-3.7849e-05, -5.0843e-05, -3.8445e-06,  ..., -2.2665e-05,
         -1.2750e-06, -5.8599e-06]], device='cuda:0')
Loss: 1.2804354429244995


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  4323,  8213,  ...,  1153,   278,  4258],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1932e-03, -2.2136e-04, -7.9140e-04,  ...,  8.0508e-04,
         -6.2864e-04, -6.6964e-04],
        [-2.8402e-05, -3.8385e-05, -2.2422e-06,  ..., -1.7002e-05,
         -6.4494e-07, -5.2694e-06],
        [-2.8998e-05, -3.9369e-05, -2.4876e-06,  ..., -1.7241e-05,
         -7.3569e-07, -5.3588e-06],
        [ 2.4250e-04,  4.4091e-04,  5.2089e-05,  ...,  1.6236e-04,
          5.8419e-05,  1.3491e-04],
        [-4.3929e-05, -5.9128e-05, -4.3921e-06,  ..., -2.6181e-05,
         -1.2676e-06, -7.2829e-06]], device='cuda:0')
Loss: 1.272847056388855


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6897e-04, -4.5701e-04, -4.3708e-04,  ...,  8.3616e-04,
         -7.6366e-04, -7.4282e-04],
        [-3.2872e-05, -4.3958e-05, -2.6911e-06,  ..., -1.9431e-05,
         -1.0324e-06, -5.7537e-06],
        [-3.3379e-05, -4.4852e-05, -2.9979e-06,  ..., -1.9595e-05,
         -1.1268e-06, -5.8357e-06],
        [ 2.3996e-04,  4.3773e-04,  5.1841e-05,  ...,  1.6100e-04,
          5.8202e-05,  1.3464e-04],
        [-5.1022e-05, -6.8009e-05, -5.2936e-06,  ..., -3.0026e-05,
         -1.9195e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.2704589366912842
Graident accumulation at epoch 0, step 39, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0420e-03, -4.2048e-04,  3.3134e-04,  ...,  6.7425e-04,
         -8.5318e-04, -1.2917e-04],
        [-4.7517e-06, -7.9595e-06,  8.1562e-07,  ..., -3.1630e-06,
         -1.6937e-06, -2.3521e-06],
        [-9.4991e-06,  2.1519e-08, -1.0748e-06,  ...,  1.0189e-05,
         -4.0120e-06, -3.5113e-06],
        [ 1.7993e-05,  5.5161e-05,  3.8219e-06,  ...,  3.0102e-05,
         -1.6635e-10,  2.0944e-05],
        [-7.0902e-06, -1.1230e-05,  3.3492e-07,  ..., -4.4348e-06,
         -2.0072e-06, -2.5586e-06]], device='cuda:0')
optimizer state dict: tensor([[4.6980e-08, 1.3548e-08, 1.5591e-08,  ..., 1.9703e-08, 3.0788e-08,
         4.0496e-09],
        [1.3347e-12, 2.8984e-12, 5.9885e-14,  ..., 4.9587e-13, 1.2354e-13,
         1.6522e-13],
        [3.2599e-12, 1.9002e-11, 1.4075e-12,  ..., 1.7858e-11, 2.8353e-12,
         8.7302e-13],
        [6.2106e-11, 2.1086e-10, 3.4903e-12,  ..., 5.0705e-11, 5.6569e-12,
         2.8005e-11],
        [3.0285e-12, 6.1882e-12, 6.2740e-14,  ..., 1.0864e-12, 1.5141e-13,
         2.0006e-13]], device='cuda:0')
optimizer state dict: 5.0
lr: [1.9996501145215088e-05, 1.9996501145215088e-05]
scheduler_last_epoch: 5


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1760,     9,  ...,  5996,    71,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8324e-04, -1.3944e-04, -4.6265e-04,  ..., -1.5185e-04,
         -1.1103e-04, -1.0212e-04],
        [-5.0962e-06, -5.9903e-06, -9.9093e-07,  ..., -3.2634e-06,
         -7.0035e-07, -3.2410e-07],
        [-4.0531e-06, -4.7684e-06, -7.9721e-07,  ..., -2.5928e-06,
         -5.5879e-07, -2.5891e-07],
        [-3.0696e-06, -3.6210e-06, -5.8487e-07,  ..., -1.9670e-06,
         -4.1351e-07, -1.7881e-07],
        [-8.7619e-06, -1.0252e-05, -1.7658e-06,  ..., -5.5730e-06,
         -1.2293e-06, -4.7311e-07]], device='cuda:0')
Loss: 1.2504453659057617


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0,   266,  7264,  ...,  3211,   328,   275],
        [    0,  2496, 10545,  ...,   287, 13978,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1326e-04, -3.6440e-04, -5.2307e-04,  ..., -1.5185e-04,
         -2.7393e-04, -2.8936e-04],
        [-9.9242e-06, -1.1563e-05, -1.7919e-06,  ..., -6.2287e-06,
         -1.3895e-06, -6.0163e-07],
        [-8.1360e-06, -9.5069e-06, -1.4827e-06,  ..., -5.0962e-06,
         -1.1325e-06, -4.8894e-07],
        [-6.0350e-06, -7.0482e-06, -1.0617e-06,  ..., -3.7849e-06,
         -8.2888e-07, -3.4552e-07],
        [-1.7226e-05, -2.0027e-05, -3.2261e-06,  ..., -1.0788e-05,
         -2.4661e-06, -8.9221e-07]], device='cuda:0')
Loss: 1.2479232549667358


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  6847,   437,  ...,    17,    14,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9359e-04,  1.4099e-05, -4.8148e-04,  ..., -3.5566e-05,
         -2.4287e-04, -5.0802e-04],
        [-1.4782e-05, -1.7226e-05, -2.6710e-06,  ..., -9.7007e-06,
         -2.0489e-06, -1.0375e-06],
        [-1.2189e-05, -1.4216e-05, -2.2203e-06,  ..., -7.9572e-06,
         -1.6838e-06, -8.6706e-07],
        [-8.9407e-06, -1.0416e-05, -1.5758e-06,  ..., -5.8413e-06,
         -1.2089e-06, -6.1374e-07],
        [-2.5749e-05, -2.9922e-05, -4.8205e-06,  ..., -1.6868e-05,
         -3.6433e-06, -1.6149e-06]], device='cuda:0')
Loss: 1.2657170295715332


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 37683,    14,  1500],
        [    0,    12,  2085,  ...,   287,   593,  4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9028e-04, -2.6192e-04, -5.0572e-04,  ...,  5.9450e-06,
         -5.5744e-04, -6.0217e-04],
        [-1.9431e-05, -2.2799e-05, -3.6545e-06,  ..., -1.2949e-05,
         -2.5555e-06, -1.4640e-06],
        [-1.6272e-05, -1.9133e-05, -3.0957e-06,  ..., -1.0803e-05,
         -2.1327e-06, -1.2489e-06],
        [-1.1802e-05, -1.3858e-05, -2.1718e-06,  ..., -7.8231e-06,
         -1.5143e-06, -8.6520e-07],
        [-3.3796e-05, -3.9518e-05, -6.5863e-06,  ..., -2.2501e-05,
         -4.5747e-06, -2.2892e-06]], device='cuda:0')
Loss: 1.2417411804199219


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2457e-04, -3.7367e-04, -6.1023e-04,  ...,  1.4279e-05,
         -8.3512e-04, -7.7055e-04],
        [-2.4259e-05, -2.8402e-05, -4.7348e-06,  ..., -1.6287e-05,
         -3.5055e-06, -1.8869e-06],
        [-2.0504e-05, -2.4021e-05, -4.0494e-06,  ..., -1.3709e-05,
         -2.9635e-06, -1.6214e-06],
        [-1.4827e-05, -1.7345e-05, -2.8349e-06,  ..., -9.8944e-06,
         -2.1029e-06, -1.1241e-06],
        [-4.2439e-05, -4.9472e-05, -8.6129e-06,  ..., -2.8431e-05,
         -6.2808e-06, -2.9709e-06]], device='cuda:0')
Loss: 1.2511290311813354


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   271,  4787,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0771e-04, -5.2427e-04, -9.5647e-04,  ...,  8.8511e-05,
         -8.0655e-04, -9.9176e-04],
        [-2.9027e-05, -3.4094e-05, -5.6960e-06,  ..., -1.9476e-05,
         -4.2170e-06, -2.0992e-06],
        [-2.4438e-05, -2.8700e-05, -4.8541e-06,  ..., -1.6317e-05,
         -3.5483e-06, -1.8012e-06],
        [-1.7703e-05, -2.0772e-05, -3.4049e-06,  ..., -1.1802e-05,
         -2.5332e-06, -1.2536e-06],
        [-5.0843e-05, -5.9485e-05, -1.0356e-05,  ..., -3.4004e-05,
         -7.5772e-06, -3.2559e-06]], device='cuda:0')
Loss: 1.2511932849884033


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[    0,  2923,   391,  ...,    14,  5424,   298],
        [    0,  4988, 36842,  ...,  7630, 18362,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4815e-04, -7.6668e-04, -1.0490e-03,  ...,  6.5570e-05,
         -8.3754e-04, -1.0927e-03],
        [-3.4094e-05, -4.0054e-05, -6.6385e-06,  ..., -2.2843e-05,
         -5.0552e-06, -2.3162e-06],
        [-2.8521e-05, -3.3528e-05, -5.6252e-06,  ..., -1.9029e-05,
         -4.2375e-06, -1.9763e-06],
        [-2.0579e-05, -2.4155e-05, -3.9265e-06,  ..., -1.3694e-05,
         -3.0063e-06, -1.3709e-06],
        [-5.9366e-05, -6.9559e-05, -1.2003e-05,  ..., -3.9697e-05,
         -9.0152e-06, -3.5670e-06]], device='cuda:0')
Loss: 1.2629730701446533


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[    0,   908,    14,  ...,    19,    27,   287],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0664e-04, -8.3014e-04, -1.0588e-03,  ..., -4.6006e-05,
         -8.9854e-04, -1.3462e-03],
        [-3.9130e-05, -4.5598e-05, -7.6890e-06,  ..., -2.6032e-05,
         -5.5358e-06, -2.6012e-06],
        [-3.2663e-05, -3.8058e-05, -6.5006e-06,  ..., -2.1636e-05,
         -4.6454e-06, -2.2277e-06],
        [-2.3574e-05, -2.7433e-05, -4.5374e-06,  ..., -1.5579e-05,
         -3.2894e-06, -1.5367e-06],
        [-6.7949e-05, -7.9036e-05, -1.3873e-05,  ..., -4.5180e-05,
         -9.8720e-06, -4.0010e-06]], device='cuda:0')
Loss: 1.255104899406433
Graident accumulation at epoch 0, step 47, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.7844e-04, -4.6145e-04,  1.9232e-04,  ...,  6.0223e-04,
         -8.5771e-04, -2.5087e-04],
        [-8.1895e-06, -1.1723e-05, -3.4842e-08,  ..., -5.4499e-06,
         -2.0779e-06, -2.3770e-06],
        [-1.1816e-05, -3.7864e-06, -1.6174e-06,  ...,  7.0062e-06,
         -4.0753e-06, -3.3829e-06],
        [ 1.3836e-05,  4.6902e-05,  2.9860e-06,  ...,  2.5534e-05,
         -3.2909e-07,  1.8696e-05],
        [-1.3176e-05, -1.8011e-05, -1.0859e-06,  ..., -8.5093e-06,
         -2.7936e-06, -2.7028e-06]], device='cuda:0')
optimizer state dict: tensor([[4.7099e-08, 1.4224e-08, 1.6696e-08,  ..., 1.9686e-08, 3.1565e-08,
         5.8579e-09],
        [2.8646e-12, 4.9747e-12, 1.1895e-13,  ..., 1.1731e-12, 1.5407e-13,
         1.7182e-13],
        [4.3235e-12, 2.0431e-11, 1.4484e-12,  ..., 1.8308e-11, 2.8541e-12,
         8.7711e-13],
        [6.2600e-11, 2.1140e-10, 3.5074e-12,  ..., 5.0897e-11, 5.6620e-12,
         2.7980e-11],
        [7.6426e-12, 1.2429e-11, 2.5514e-13,  ..., 3.1266e-12, 2.4872e-13,
         2.1587e-13]], device='cuda:0')
optimizer state dict: 6.0
lr: [1.9986907288753243e-05, 1.9986907288753243e-05]
scheduler_last_epoch: 6


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3132e-04,  2.0187e-04, -4.2670e-04,  ..., -1.8124e-05,
          2.6413e-06,  1.2655e-04],
        [-5.0366e-06, -5.1856e-06, -5.8860e-07,  ..., -3.6508e-06,
         -7.3388e-07, -1.3504e-07],
        [-3.9637e-06, -4.0829e-06, -4.7311e-07,  ..., -2.8759e-06,
         -5.7369e-07, -1.1083e-07],
        [-3.5018e-06, -3.6061e-06, -4.0606e-07,  ..., -2.5332e-06,
         -5.0291e-07, -9.2201e-08],
        [-9.2387e-06, -9.5367e-06, -1.1176e-06,  ..., -6.6757e-06,
         -1.3486e-06, -2.1886e-07]], device='cuda:0')
Loss: 1.2450376749038696


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2043e-04,  1.8190e-04, -3.6031e-04,  ..., -1.6336e-05,
         -2.9695e-04, -2.5912e-05],
        [-1.0103e-05, -1.0282e-05, -1.2629e-06,  ..., -7.2867e-06,
         -1.8142e-06, -3.5018e-07],
        [-7.8678e-06, -8.0466e-06, -9.9465e-07,  ..., -5.6922e-06,
         -1.4119e-06, -2.8405e-07],
        [-7.0035e-06, -7.1526e-06, -8.6240e-07,  ..., -5.0515e-06,
         -1.2517e-06, -2.3004e-07],
        [-1.8418e-05, -1.8775e-05, -2.3842e-06,  ..., -1.3292e-05,
         -3.3304e-06, -5.5786e-07]], device='cuda:0')
Loss: 1.2470332384109497


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,    13,  1529,  ...,  8197,  2700,  9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4916e-04,  2.5707e-04, -5.0079e-04,  ...,  7.9674e-06,
         -1.6779e-04,  1.4481e-06],
        [-1.5020e-05, -1.5318e-05, -2.0303e-06,  ..., -1.1012e-05,
         -3.0883e-06, -3.8138e-07],
        [-1.1593e-05, -1.1861e-05, -1.5758e-06,  ..., -8.5086e-06,
         -2.3730e-06, -3.1432e-07],
        [-1.0461e-05, -1.0699e-05, -1.3877e-06,  ..., -7.6592e-06,
         -2.1458e-06, -2.3935e-07],
        [-2.7716e-05, -2.8312e-05, -3.8743e-06,  ..., -2.0295e-05,
         -5.7444e-06, -5.8115e-07]], device='cuda:0')
Loss: 1.2197754383087158


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[   0, 8353, 1842,  ...,   38,  643,  472],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2654e-04,  2.1054e-04, -6.9969e-04,  ..., -1.2329e-04,
         -1.8164e-04, -1.3105e-04],
        [-1.9848e-05, -2.0593e-05, -2.7567e-06,  ..., -1.4782e-05,
         -4.2282e-06, -5.1083e-07],
        [-1.5438e-05, -1.6093e-05, -2.1644e-06,  ..., -1.1519e-05,
         -3.2857e-06, -4.2515e-07],
        [-1.3784e-05, -1.4350e-05, -1.8757e-06,  ..., -1.0267e-05,
         -2.9355e-06, -3.2596e-07],
        [-3.6776e-05, -3.8266e-05, -5.2825e-06,  ..., -2.7418e-05,
         -7.9051e-06, -7.7300e-07]], device='cuda:0')
Loss: 1.2554720640182495


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3943e-04,  4.1280e-06, -6.8473e-04,  ..., -2.3200e-04,
         -3.1862e-04, -8.0796e-05],
        [-2.4855e-05, -2.5839e-05, -3.4757e-06,  ..., -1.8537e-05,
         -5.1968e-06, -6.0862e-07],
        [-1.9282e-05, -2.0117e-05, -2.7269e-06,  ..., -1.4409e-05,
         -4.0345e-06, -4.9686e-07],
        [-1.7226e-05, -1.7956e-05, -2.3600e-06,  ..., -1.2845e-05,
         -3.5949e-06, -3.8254e-07],
        [-4.6015e-05, -4.7863e-05, -6.6534e-06,  ..., -3.4332e-05,
         -9.7081e-06, -8.9873e-07]], device='cuda:0')
Loss: 1.2449110746383667


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0,   271,   266,  ...,   275,  2576,  3588]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0630e-03,  3.6481e-05, -7.4398e-04,  ..., -3.3324e-04,
         -2.7712e-04,  3.8669e-05],
        [-2.9683e-05, -3.0875e-05, -4.2394e-06,  ..., -2.2262e-05,
         -6.1058e-06, -6.6916e-07],
        [-2.3097e-05, -2.4110e-05, -3.3341e-06,  ..., -1.7360e-05,
         -4.7535e-06, -5.4459e-07],
        [-2.0608e-05, -2.1502e-05, -2.8890e-06,  ..., -1.5453e-05,
         -4.2357e-06, -4.1770e-07],
        [-5.5194e-05, -5.7459e-05, -8.1584e-06,  ..., -4.1425e-05,
         -1.1459e-05, -9.6671e-07]], device='cuda:0')
Loss: 1.2522222995758057


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[   0, 3968,  446,  ...,   22,  722,  342],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8408e-04,  6.4926e-06, -7.7799e-04,  ..., -3.2558e-04,
         -4.4705e-04, -6.7284e-05],
        [-3.4720e-05, -3.5912e-05, -5.0962e-06,  ..., -2.5958e-05,
         -7.2755e-06, -7.6788e-07],
        [-2.7150e-05, -2.8163e-05, -4.0345e-06,  ..., -2.0355e-05,
         -5.6922e-06, -6.3074e-07],
        [-2.4080e-05, -2.4989e-05, -3.4701e-06,  ..., -1.8016e-05,
         -5.0366e-06, -4.7870e-07],
        [-6.4552e-05, -6.6876e-05, -9.7975e-06,  ..., -4.8339e-05,
         -1.3664e-05, -1.1083e-06]], device='cuda:0')
Loss: 1.2428922653198242


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[    0,  4538,   271,  ...,  1603,   591,   688],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0398e-03, -3.9963e-05, -8.8372e-04,  ..., -3.7270e-04,
         -5.0812e-04, -8.0689e-05],
        [-3.9876e-05, -4.1127e-05, -5.6103e-06,  ..., -2.9713e-05,
         -8.4899e-06, -8.2189e-07],
        [-3.1114e-05, -3.2187e-05, -4.4294e-06,  ..., -2.3261e-05,
         -6.6310e-06, -6.7800e-07],
        [-2.7582e-05, -2.8521e-05, -3.8110e-06,  ..., -2.0564e-05,
         -5.8562e-06, -5.0990e-07],
        [-7.3850e-05, -7.6294e-05, -1.0766e-05,  ..., -5.5164e-05,
         -1.5900e-05, -1.1493e-06]], device='cuda:0')
Loss: 1.2292808294296265
Graident accumulation at epoch 0, step 55, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.8457e-04, -4.1930e-04,  8.4720e-05,  ...,  5.0473e-04,
         -8.2276e-04, -2.3386e-04],
        [-1.1358e-05, -1.4664e-05, -5.9239e-07,  ..., -7.8762e-06,
         -2.7191e-06, -2.2215e-06],
        [-1.3745e-05, -6.6264e-06, -1.8986e-06,  ...,  3.9795e-06,
         -4.3309e-06, -3.1124e-06],
        [ 9.6941e-06,  3.9360e-05,  2.3063e-06,  ...,  2.0924e-05,
         -8.8180e-07,  1.6775e-05],
        [-1.9244e-05, -2.3839e-05, -2.0539e-06,  ..., -1.3175e-05,
         -4.1042e-06, -2.5475e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8133e-08, 1.4211e-08, 1.7460e-08,  ..., 1.9805e-08, 3.1791e-08,
         5.8585e-09],
        [4.4517e-12, 6.6611e-12, 1.5030e-13,  ..., 2.0547e-12, 2.2599e-13,
         1.7233e-13],
        [5.2873e-12, 2.1447e-11, 1.4665e-12,  ..., 1.8831e-11, 2.8952e-12,
         8.7669e-13],
        [6.3298e-11, 2.1200e-10, 3.5185e-12,  ..., 5.1269e-11, 5.6907e-12,
         2.7952e-11],
        [1.3089e-11, 1.8237e-11, 3.7079e-13,  ..., 6.1665e-12, 5.0127e-13,
         2.1697e-13]], device='cuda:0')
optimizer state dict: 7.0
lr: [1.997120931904809e-05, 1.997120931904809e-05]
scheduler_last_epoch: 7


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,   396,   298,  ...,    52,  5065,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1608e-05,  4.1091e-05,  6.7640e-06,  ..., -1.3784e-04,
         -5.2658e-05, -2.9070e-05],
        [-4.9174e-06, -4.3511e-06, -1.3039e-07,  ..., -3.9339e-06,
         -1.3411e-06, -3.2783e-07],
        [-3.6210e-06, -3.2037e-06, -9.8720e-08,  ..., -2.8908e-06,
         -9.8348e-07, -2.3562e-07],
        [-3.8743e-06, -3.4124e-06, -9.4995e-08,  ..., -3.0845e-06,
         -1.0505e-06, -2.4214e-07],
        [-9.2983e-06, -8.2254e-06, -2.8312e-07,  ..., -7.3910e-06,
         -2.5332e-06, -5.6624e-07]], device='cuda:0')
Loss: 1.208550214767456


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   344, 10706,  ...,  1184,   578,   825],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8988e-05,  1.6821e-04, -7.4609e-05,  ..., -2.9434e-04,
         -1.7589e-04, -1.6593e-04],
        [-9.9540e-06, -8.6725e-06, -3.2131e-07,  ..., -7.9274e-06,
         -2.6003e-06, -4.8801e-07],
        [-7.1973e-06, -6.2883e-06, -2.3935e-07,  ..., -5.7369e-06,
         -1.8887e-06, -3.5763e-07],
        [-7.8976e-06, -6.8694e-06, -2.4214e-07,  ..., -6.2585e-06,
         -2.0564e-06, -3.6974e-07],
        [-1.8597e-05, -1.6212e-05, -6.7800e-07,  ..., -1.4752e-05,
         -4.8876e-06, -8.3819e-07]], device='cuda:0')
Loss: 1.2321109771728516


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0360e-05,  3.3040e-04,  2.6487e-04,  ..., -2.9506e-04,
         -4.8720e-04, -1.3946e-04],
        [-1.5020e-05, -1.3083e-05, -6.1188e-07,  ..., -1.2070e-05,
         -3.9488e-06, -9.3319e-07],
        [-1.0788e-05, -9.4324e-06, -4.4331e-07,  ..., -8.6725e-06,
         -2.8498e-06, -6.7614e-07],
        [-1.1832e-05, -1.0312e-05, -4.5355e-07,  ..., -9.4771e-06,
         -3.1069e-06, -7.1991e-07],
        [-2.8074e-05, -2.4498e-05, -1.2517e-06,  ..., -2.2441e-05,
         -7.4208e-06, -1.6540e-06]], device='cuda:0')
Loss: 1.2457176446914673


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 2728, 3139,  ..., 2254,  221,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5269e-05,  4.7883e-04,  3.1277e-04,  ..., -3.5367e-04,
         -5.5209e-04, -2.8147e-04],
        [-2.0117e-05, -1.7643e-05, -7.6555e-07,  ..., -1.6212e-05,
         -5.3048e-06, -1.3150e-06],
        [-1.4499e-05, -1.2755e-05, -5.5647e-07,  ..., -1.1683e-05,
         -3.8408e-06, -9.5367e-07],
        [-1.5855e-05, -1.3903e-05, -5.6438e-07,  ..., -1.2726e-05,
         -4.1798e-06, -1.0123e-06],
        [-3.7730e-05, -3.3081e-05, -1.5795e-06,  ..., -3.0249e-05,
         -1.0014e-05, -2.3320e-06]], device='cuda:0')
Loss: 1.221134901046753


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,   437,   638,  ...,  4514,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0515e-05,  5.8208e-04,  2.8091e-04,  ..., -4.0529e-04,
         -5.1002e-04, -2.8253e-04],
        [-2.5153e-05, -2.2054e-05, -1.0077e-06,  ..., -2.0176e-05,
         -6.6012e-06, -1.5069e-06],
        [-1.8209e-05, -1.6019e-05, -7.3807e-07,  ..., -1.4618e-05,
         -4.7944e-06, -1.0896e-06],
        [-1.9819e-05, -1.7375e-05, -7.3947e-07,  ..., -1.5855e-05,
         -5.2005e-06, -1.1530e-06],
        [-4.7147e-05, -4.1306e-05, -2.0564e-06,  ..., -3.7640e-05,
         -1.2457e-05, -2.6431e-06]], device='cuda:0')
Loss: 1.2231658697128296


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0,   638,  2708,  ..., 28492,  1814,    12],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1146e-04,  5.3040e-04,  2.1266e-04,  ..., -5.1782e-04,
         -4.8785e-04, -1.7013e-04],
        [-2.9922e-05, -2.6435e-05, -1.1614e-06,  ..., -2.4080e-05,
         -7.9423e-06, -1.8012e-06],
        [-2.1771e-05, -1.9312e-05, -8.5915e-07,  ..., -1.7524e-05,
         -5.8003e-06, -1.3113e-06],
        [-2.3633e-05, -2.0891e-05, -8.4983e-07,  ..., -1.8969e-05,
         -6.2734e-06, -1.3784e-06],
        [-5.6148e-05, -4.9591e-05, -2.3860e-06,  ..., -4.4972e-05,
         -1.5020e-05, -3.1646e-06]], device='cuda:0')
Loss: 1.2336807250976562


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5774e-04,  5.5806e-04,  3.3463e-04,  ..., -4.9532e-04,
         -6.2549e-04, -1.9550e-04],
        [-3.4750e-05, -3.0726e-05, -1.3821e-06,  ..., -2.7984e-05,
         -9.1195e-06, -2.1327e-06],
        [-2.5421e-05, -2.2575e-05, -1.0268e-06,  ..., -2.0474e-05,
         -6.7018e-06, -1.5665e-06],
        [-2.7597e-05, -2.4408e-05, -1.0240e-06,  ..., -2.2158e-05,
         -7.2420e-06, -1.6447e-06],
        [-6.5565e-05, -5.7995e-05, -2.8498e-06,  ..., -5.2571e-05,
         -1.7345e-05, -3.7905e-06]], device='cuda:0')
Loss: 1.2292001247406006


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,  3990,   717,    12],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0196e-04,  6.7984e-04,  1.6746e-04,  ..., -5.7024e-04,
         -5.6356e-04, -1.9565e-04],
        [-3.9607e-05, -3.5226e-05, -1.5553e-06,  ..., -3.2067e-05,
         -1.0438e-05, -2.4568e-06],
        [-2.8983e-05, -2.5898e-05, -1.1609e-06,  ..., -2.3469e-05,
         -7.6704e-06, -1.8030e-06],
        [-3.1501e-05, -2.8029e-05, -1.1590e-06,  ..., -2.5421e-05,
         -8.2925e-06, -1.8999e-06],
        [-7.4506e-05, -6.6340e-05, -3.2075e-06,  ..., -6.0111e-05,
         -1.9774e-05, -4.3418e-06]], device='cuda:0')
Loss: 1.2329025268554688
Graident accumulation at epoch 0, step 63, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0035,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2631e-04, -3.0939e-04,  9.2994e-05,  ...,  3.9724e-04,
         -7.9684e-04, -2.3004e-04],
        [-1.4183e-05, -1.6720e-05, -6.8868e-07,  ..., -1.0295e-05,
         -3.4910e-06, -2.2450e-06],
        [-1.5269e-05, -8.5536e-06, -1.8248e-06,  ...,  1.2346e-06,
         -4.6648e-06, -2.9815e-06],
        [ 5.5746e-06,  3.2621e-05,  1.9598e-06,  ...,  1.6290e-05,
         -1.6229e-06,  1.4908e-05],
        [-2.4770e-05, -2.8089e-05, -2.1693e-06,  ..., -1.7868e-05,
         -5.6712e-06, -2.7269e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8246e-08, 1.4659e-08, 1.7471e-08,  ..., 2.0110e-08, 3.2077e-08,
         5.8910e-09],
        [6.0160e-12, 7.8954e-12, 1.5257e-13,  ..., 3.0810e-12, 3.3472e-13,
         1.7819e-13],
        [6.1220e-12, 2.2096e-11, 1.4664e-12,  ..., 1.9363e-11, 2.9511e-12,
         8.7907e-13],
        [6.4227e-11, 2.1257e-10, 3.5163e-12,  ..., 5.1864e-11, 5.7537e-12,
         2.7928e-11],
        [1.8627e-11, 2.2620e-11, 3.8071e-13,  ..., 9.7737e-12, 8.9177e-13,
         2.3560e-13]], device='cuda:0')
optimizer state dict: 8.0
lr: [1.9949416830880266e-05, 1.9949416830880266e-05]
scheduler_last_epoch: 8


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,   271,  3616,  ...,    12,  1348,  5037],
        [    0,   380,  2114,  ...,   456, 28979,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1908e-04,  2.7941e-05,  5.0084e-05,  ..., -4.8171e-05,
          2.9993e-05, -1.3908e-04],
        [-4.6194e-06, -3.7551e-06,  3.2224e-07,  ..., -3.9935e-06,
         -1.1027e-06, -6.5938e-07],
        [-3.2783e-06, -2.6673e-06,  2.2445e-07,  ..., -2.8461e-06,
         -7.8604e-07, -4.6566e-07],
        [-4.4405e-06, -3.6210e-06,  3.2224e-07,  ..., -3.8743e-06,
         -1.0654e-06, -6.2957e-07],
        [-8.6427e-06, -7.0035e-06,  5.7742e-07,  ..., -7.4804e-06,
         -2.0713e-06, -1.1995e-06]], device='cuda:0')
Loss: 1.2351793050765991


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  5583,   598,  ...,   199,   395,  6551],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0436e-05,  5.8392e-05,  1.2990e-04,  ..., -7.8077e-05,
          1.5104e-04, -2.2613e-04],
        [-9.4473e-06, -7.6592e-06,  6.6124e-07,  ..., -8.1360e-06,
         -2.2352e-06, -1.3150e-06],
        [-6.7055e-06, -5.4389e-06,  4.6194e-07,  ..., -5.7966e-06,
         -1.5870e-06, -9.3132e-07],
        [-8.9407e-06, -7.2420e-06,  6.4448e-07,  ..., -7.7486e-06,
         -2.1234e-06, -1.2293e-06],
        [-1.7583e-05, -1.4186e-05,  1.1735e-06,  ..., -1.5169e-05,
         -4.1723e-06, -2.3767e-06]], device='cuda:0')
Loss: 1.238576889038086


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,  1099,  2851,  ...,   518,   496,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2131e-05,  6.0777e-05,  1.3164e-04,  ..., -1.0398e-04,
          1.0091e-04, -2.4113e-04],
        [-1.4126e-05, -1.1384e-05,  1.0002e-06,  ..., -1.2130e-05,
         -3.4720e-06, -2.0154e-06],
        [-1.0073e-05, -8.1211e-06,  7.0222e-07,  ..., -8.6576e-06,
         -2.4773e-06, -1.4380e-06],
        [-1.3411e-05, -1.0818e-05,  9.8348e-07,  ..., -1.1563e-05,
         -3.3006e-06, -1.8999e-06],
        [-2.6286e-05, -2.1130e-05,  1.7844e-06,  ..., -2.2620e-05,
         -6.4969e-06, -3.6582e-06]], device='cuda:0')
Loss: 1.215898871421814


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,   685,  3482,  ..., 23113,    12,  6481],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2212e-04,  8.7614e-05,  9.1127e-06,  ..., -3.1149e-05,
         -4.4206e-05, -3.7729e-04],
        [-1.8895e-05, -1.5065e-05,  1.3188e-06,  ..., -1.6272e-05,
         -4.6194e-06, -2.6450e-06],
        [-1.3411e-05, -1.0699e-05,  9.2573e-07,  ..., -1.1563e-05,
         -3.2820e-06, -1.8794e-06],
        [-1.7941e-05, -1.4320e-05,  1.2964e-06,  ..., -1.5527e-05,
         -4.3884e-06, -2.4922e-06],
        [-3.5167e-05, -2.7955e-05,  2.3469e-06,  ..., -3.0369e-05,
         -8.6427e-06, -4.8056e-06]], device='cuda:0')
Loss: 1.2236557006835938


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7726e-04,  1.1176e-04,  4.8333e-05,  ..., -1.5633e-04,
          4.5779e-05, -3.6324e-04],
        [-2.3633e-05, -1.8850e-05,  1.6429e-06,  ..., -2.0325e-05,
         -5.7817e-06, -3.3714e-06],
        [-1.6868e-05, -1.3471e-05,  1.1632e-06,  ..., -1.4514e-05,
         -4.1313e-06, -2.4159e-06],
        [-2.2441e-05, -1.7911e-05,  1.6149e-06,  ..., -1.9372e-05,
         -5.4911e-06, -3.1851e-06],
        [-4.3809e-05, -3.4899e-05,  2.9132e-06,  ..., -3.7760e-05,
         -1.0774e-05, -6.1244e-06]], device='cuda:0')
Loss: 1.2306616306304932


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,  3393,  3380,  ...,   292,  6502,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8359e-04,  1.6428e-04,  9.9213e-05,  ..., -2.3046e-04,
          4.5426e-05, -3.6370e-04],
        [-2.8372e-05, -2.2665e-05,  2.0172e-06,  ..., -2.4319e-05,
         -6.8173e-06, -4.0755e-06],
        [-2.0280e-05, -1.6212e-05,  1.4314e-06,  ..., -1.7390e-05,
         -4.8801e-06, -2.9225e-06],
        [-2.6941e-05, -2.1547e-05,  1.9800e-06,  ..., -2.3156e-05,
         -6.4820e-06, -3.8520e-06],
        [-5.2571e-05, -4.1962e-05,  3.5875e-06,  ..., -4.5151e-05,
         -1.2711e-05, -7.4059e-06]], device='cuda:0')
Loss: 1.21747624874115


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4011e-04,  2.2652e-04,  1.5147e-04,  ..., -2.6260e-04,
          6.8293e-07, -5.1729e-04],
        [-3.3110e-05, -2.6539e-05,  2.3730e-06,  ..., -2.8551e-05,
         -7.9945e-06, -4.6715e-06],
        [-2.3589e-05, -1.8895e-05,  1.6754e-06,  ..., -2.0325e-05,
         -5.7034e-06, -3.3360e-06],
        [-3.1352e-05, -2.5123e-05,  2.3190e-06,  ..., -2.7061e-05,
         -7.5772e-06, -4.4033e-06],
        [-6.1393e-05, -4.9144e-05,  4.2319e-06,  ..., -5.3018e-05,
         -1.4916e-05, -8.4862e-06]], device='cuda:0')
Loss: 1.2240327596664429


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[   0,  278,  668,  ..., 2743,  638,  609],
        [   0, 2836, 3084,  ..., 3634, 6464,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1019e-04,  2.5802e-04,  2.8455e-04,  ..., -1.2371e-04,
         -9.9871e-05, -5.2126e-04],
        [-3.7909e-05, -3.0383e-05,  2.8200e-06,  ..., -3.2604e-05,
         -9.0227e-06, -5.3085e-06],
        [-2.7031e-05, -2.1666e-05,  1.9977e-06,  ..., -2.3246e-05,
         -6.4448e-06, -3.7905e-06],
        [-3.5882e-05, -2.8774e-05,  2.7567e-06,  ..., -3.0905e-05,
         -8.5533e-06, -4.9919e-06],
        [-6.9976e-05, -5.6028e-05,  5.0105e-06,  ..., -6.0290e-05,
         -1.6779e-05, -9.5889e-06]], device='cuda:0')
Loss: 1.2039330005645752
Graident accumulation at epoch 0, step 71, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6470e-04, -2.5265e-04,  1.1215e-04,  ...,  3.4514e-04,
         -7.2714e-04, -2.5916e-04],
        [-1.6556e-05, -1.8086e-05, -3.3781e-07,  ..., -1.2526e-05,
         -4.0442e-06, -2.5514e-06],
        [-1.6445e-05, -9.8649e-06, -1.4425e-06,  ..., -1.2134e-06,
         -4.8428e-06, -3.0624e-06],
        [ 1.4290e-06,  2.6481e-05,  2.0395e-06,  ...,  1.1570e-05,
         -2.3159e-06,  1.2918e-05],
        [-2.9290e-05, -3.0883e-05, -1.4513e-06,  ..., -2.2111e-05,
         -6.7819e-06, -3.4131e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8294e-08, 1.4711e-08, 1.7534e-08,  ..., 2.0106e-08, 3.2055e-08,
         6.1568e-09],
        [7.4471e-12, 8.8106e-12, 1.6037e-13,  ..., 4.1409e-12, 4.1580e-13,
         2.0619e-13],
        [6.8465e-12, 2.2543e-11, 1.4689e-12,  ..., 1.9884e-11, 2.9897e-12,
         8.9256e-13],
        [6.5450e-11, 2.1319e-10, 3.5204e-12,  ..., 5.2767e-11, 5.8211e-12,
         2.7925e-11],
        [2.3505e-11, 2.5736e-11, 4.0543e-13,  ..., 1.3399e-11, 1.1724e-12,
         3.2732e-13]], device='cuda:0')
optimizer state dict: 9.0
lr: [1.99215431440706e-05, 1.99215431440706e-05]
scheduler_last_epoch: 9


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4482e-05, -3.1337e-05, -1.9321e-05,  ...,  7.1632e-05,
         -6.8256e-05,  4.2653e-05],
        [-4.3213e-06, -3.0845e-06,  8.3447e-07,  ..., -3.9339e-06,
         -7.6741e-07, -8.1956e-07],
        [-2.9951e-06, -2.1458e-06,  5.8115e-07,  ..., -2.7418e-06,
         -5.3644e-07, -5.6997e-07],
        [-4.6492e-06, -3.3230e-06,  9.1270e-07,  ..., -4.2319e-06,
         -8.3074e-07, -8.8289e-07],
        [-8.1062e-06, -5.7817e-06,  1.5497e-06,  ..., -7.3612e-06,
         -1.4603e-06, -1.5199e-06]], device='cuda:0')
Loss: 1.2189403772354126


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[    0,  3164,    12,  ...,   984,   344,  3993],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0520e-04,  2.1538e-05, -1.4913e-04,  ...,  7.8515e-05,
         -6.7484e-05,  7.0918e-05],
        [-8.6129e-06, -6.1989e-06,  1.6764e-06,  ..., -7.7784e-06,
         -1.5907e-06, -1.8924e-06],
        [-6.0201e-06, -4.3362e-06,  1.1735e-06,  ..., -5.4389e-06,
         -1.1139e-06, -1.3262e-06],
        [-9.5367e-06, -6.8545e-06,  1.8813e-06,  ..., -8.6129e-06,
         -1.7658e-06, -2.0973e-06],
        [-1.6153e-05, -1.1593e-05,  3.1143e-06,  ..., -1.4573e-05,
         -3.0100e-06, -3.5167e-06]], device='cuda:0')
Loss: 1.2203258275985718


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   822,  5085,  ...,   293,  1608,   391],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3143e-04,  4.9917e-05, -1.3167e-04,  ..., -3.8641e-05,
         -2.7434e-05,  1.0823e-04],
        [-1.2964e-05, -9.2238e-06,  2.3954e-06,  ..., -1.1623e-05,
         -2.4103e-06, -2.7716e-06],
        [-9.0748e-06, -6.4820e-06,  1.6764e-06,  ..., -8.1658e-06,
         -1.6950e-06, -1.9483e-06],
        [-1.4335e-05, -1.0192e-05,  2.6785e-06,  ..., -1.2875e-05,
         -2.6710e-06, -3.0659e-06],
        [-2.4259e-05, -1.7256e-05,  4.4331e-06,  ..., -2.1785e-05,
         -4.5598e-06, -5.1335e-06]], device='cuda:0')
Loss: 1.2218562364578247


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4732e-05,  6.8496e-05, -1.4204e-04,  ...,  3.2402e-05,
          7.8660e-05,  2.4640e-04],
        [-1.7405e-05, -1.2442e-05,  3.1553e-06,  ..., -1.5765e-05,
         -3.3081e-06, -3.7476e-06],
        [-1.2055e-05, -8.6576e-06,  2.1867e-06,  ..., -1.0952e-05,
         -2.2948e-06, -2.6040e-06],
        [-1.8984e-05, -1.3575e-05,  3.4831e-06,  ..., -1.7226e-05,
         -3.6135e-06, -4.0866e-06],
        [-3.2425e-05, -2.3216e-05,  5.8189e-06,  ..., -2.9415e-05,
         -6.2212e-06, -6.9216e-06]], device='cuda:0')
Loss: 1.2296196222305298


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0,  7111,   409,  ...,  1908,  1260,   883],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1538e-04,  4.1390e-05, -9.8598e-05,  ..., -1.1657e-05,
          9.2689e-05,  3.0100e-04],
        [-2.1756e-05, -1.5512e-05,  3.8445e-06,  ..., -1.9729e-05,
         -4.0680e-06, -4.5672e-06],
        [-1.5110e-05, -1.0818e-05,  2.6748e-06,  ..., -1.3724e-05,
         -2.8275e-06, -3.1814e-06],
        [-2.3663e-05, -1.6868e-05,  4.2357e-06,  ..., -2.1458e-05,
         -4.4294e-06, -4.9584e-06],
        [ 4.4098e-05,  1.5302e-05, -4.4298e-05,  ...,  6.8559e-05,
          7.1302e-06,  2.9379e-05]], device='cuda:0')
Loss: 1.2374745607376099


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[   0,   12,  358,  ...,  352,  266,  319],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9908e-05,  9.7085e-05, -1.5056e-04,  ...,  5.3934e-05,
          5.5374e-05,  3.2771e-04],
        [-2.6107e-05, -1.8686e-05,  4.6454e-06,  ..., -2.3782e-05,
         -4.8876e-06, -5.4762e-06],
        [-1.8135e-05, -1.3024e-05,  3.2298e-06,  ..., -1.6540e-05,
         -3.3975e-06, -3.8110e-06],
        [-2.8282e-05, -2.0236e-05,  5.0925e-06,  ..., -2.5779e-05,
         -5.2936e-06, -5.9120e-06],
        [ 3.6052e-05,  9.4313e-06, -4.2837e-05,  ...,  6.1079e-05,
          5.6028e-06,  2.7732e-05]], device='cuda:0')
Loss: 1.2275359630584717


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[    0,  1549,   824,  ...,  3609,   720,   417],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8699e-05,  1.4412e-04, -2.6222e-04,  ...,  5.3745e-05,
          1.1501e-05,  4.3445e-04],
        [-3.0339e-05, -2.1771e-05,  5.4352e-06,  ..., -2.7627e-05,
         -5.7109e-06, -6.4671e-06],
        [-2.1145e-05, -1.5214e-05,  3.7923e-06,  ..., -1.9252e-05,
         -3.9786e-06, -4.5113e-06],
        [-3.2961e-05, -2.3663e-05,  5.9791e-06,  ..., -3.0011e-05,
         -6.2026e-06, -6.9998e-06],
        [ 2.8244e-05,  3.7688e-06, -4.1399e-05,  ...,  5.4045e-05,
          4.0829e-06,  2.5944e-05]], device='cuda:0')
Loss: 1.2217402458190918


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   292, 21215,  ...,   266,   818,  1527],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1100e-05,  1.2767e-04, -2.2508e-04,  ...,  1.5985e-04,
         -5.9499e-05,  5.3718e-04],
        [-3.4750e-05, -2.4915e-05,  6.1207e-06,  ..., -3.1561e-05,
         -6.5304e-06, -7.5027e-06],
        [-2.4199e-05, -1.7390e-05,  4.2673e-06,  ..., -2.1964e-05,
         -4.5486e-06, -5.2266e-06],
        [-3.7730e-05, -2.7046e-05,  6.7316e-06,  ..., -3.4213e-05,
         -7.0818e-06, -8.1100e-06],
        [ 2.0018e-05, -2.0724e-06, -4.0125e-05,  ...,  4.6744e-05,
          2.5481e-06,  2.4037e-05]], device='cuda:0')
Loss: 1.2174878120422363
Graident accumulation at epoch 0, step 79, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.8634e-04, -2.1462e-04,  7.8427e-05,  ...,  3.2661e-04,
         -6.6038e-04, -1.7952e-04],
        [-1.8375e-05, -1.8769e-05,  3.0804e-07,  ..., -1.4430e-05,
         -4.2928e-06, -3.0465e-06],
        [-1.7221e-05, -1.0617e-05, -8.7156e-07,  ..., -3.2885e-06,
         -4.8134e-06, -3.2788e-06],
        [-2.4869e-06,  2.1129e-05,  2.5087e-06,  ...,  6.9918e-06,
         -2.7925e-06,  1.0815e-05],
        [-2.4360e-05, -2.8002e-05, -5.3187e-06,  ..., -1.5225e-05,
         -5.8489e-06, -6.6813e-07]], device='cuda:0')
optimizer state dict: tensor([[4.8252e-08, 1.4713e-08, 1.7568e-08,  ..., 2.0111e-08, 3.2026e-08,
         6.4392e-09],
        [8.6472e-12, 9.4226e-12, 1.9767e-13,  ..., 5.1329e-12, 4.5803e-13,
         2.6228e-13],
        [7.4253e-12, 2.2823e-11, 1.4857e-12,  ..., 2.0346e-11, 3.0074e-12,
         9.1898e-13],
        [6.6808e-11, 2.1371e-10, 3.5622e-12,  ..., 5.3885e-11, 5.8655e-12,
         2.7962e-11],
        [2.3882e-11, 2.5715e-11, 2.0151e-12,  ..., 1.5570e-11, 1.1777e-12,
         9.0475e-13]], device='cuda:0')
optimizer state dict: 10.0
lr: [1.9887605295338853e-05, 1.9887605295338853e-05]
scheduler_last_epoch: 10


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0, 5007, 7551,  ...,    9, 2095,  300],
        [   0,  271,  259,  ..., 1345,  352,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0288e-05,  9.3025e-05,  8.7428e-06,  ...,  2.1622e-05,
          1.6810e-05, -1.9567e-05],
        [-4.1425e-06, -2.7120e-06,  1.1101e-06,  ..., -3.8743e-06,
         -5.7742e-07, -1.0729e-06],
        [-2.7716e-06, -1.8179e-06,  7.4133e-07,  ..., -2.5928e-06,
         -3.8929e-07, -7.1526e-07],
        [-4.7684e-06, -3.1143e-06,  1.2815e-06,  ..., -4.4405e-06,
         -6.6683e-07, -1.2293e-06],
        [-7.6890e-06, -5.0068e-06,  2.0266e-06,  ..., -7.1526e-06,
         -1.0729e-06, -1.9670e-06]], device='cuda:0')
Loss: 1.2057009935379028


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[   0, 4337, 2057,  ..., 3020, 1722,  369],
        [   0,   15,   19,  ...,   12,  287, 7897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6287e-04,  1.0981e-04, -1.5116e-04,  ..., -4.3564e-05,
          1.1489e-04,  4.6850e-05],
        [-8.1062e-06, -5.3048e-06,  2.1532e-06,  ..., -7.6294e-06,
         -1.1809e-06, -2.1383e-06],
        [-5.4836e-06, -3.5986e-06,  1.4566e-06,  ..., -5.1558e-06,
         -8.0466e-07, -1.4454e-06],
        [-9.5069e-06, -6.2138e-06,  2.5481e-06,  ..., -8.9407e-06,
         -1.3895e-06, -2.5034e-06],
        [-1.5110e-05, -9.8646e-06,  3.9786e-06,  ..., -1.4156e-05,
         -2.2054e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.2232969999313354


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[   0,   12, 3518,  ..., 1580, 2573,  409],
        [   0,  266, 1784,  ..., 1119, 1276,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1281e-05,  1.8867e-04, -1.3537e-04,  ..., -4.4593e-05,
          2.0050e-05,  6.8181e-05],
        [-1.2159e-05, -7.8976e-06,  3.2187e-06,  ..., -1.1399e-05,
         -1.7993e-06, -3.3006e-06],
        [-8.2254e-06, -5.3570e-06,  2.1756e-06,  ..., -7.7039e-06,
         -1.2219e-06, -2.2314e-06],
        [-1.4305e-05, -9.2834e-06,  3.8147e-06,  ..., -1.3381e-05,
         -2.1197e-06, -3.8743e-06],
        [-2.2739e-05, -1.4752e-05,  5.9754e-06,  ..., -2.1249e-05,
         -3.3826e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.197189450263977


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3865e-04,  1.9317e-04, -8.0002e-05,  ..., -7.6833e-05,
         -1.8289e-05,  1.6337e-04],
        [-1.6242e-05, -1.0535e-05,  4.2394e-06,  ..., -1.5169e-05,
         -2.3097e-06, -4.4182e-06],
        [-1.1027e-05, -7.1749e-06,  2.8759e-06,  ..., -1.0297e-05,
         -1.5739e-06, -2.9951e-06],
        [-1.9282e-05, -1.2517e-05,  5.0738e-06,  ..., -1.7971e-05,
         -2.7418e-06, -5.2303e-06],
        [-3.0369e-05, -1.9699e-05,  7.8827e-06,  ..., -2.8312e-05,
         -4.3511e-06, -8.1807e-06]], device='cuda:0')
Loss: 1.2087726593017578


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8081e-04,  2.8784e-04, -2.3229e-05,  ..., -1.0320e-04,
          5.3230e-05,  1.7533e-04],
        [-2.0355e-05, -1.3247e-05,  5.2527e-06,  ..., -1.9103e-05,
         -2.9318e-06, -5.4762e-06],
        [-1.3798e-05, -9.0003e-06,  3.5539e-06,  ..., -1.2949e-05,
         -1.9912e-06, -3.7067e-06],
        [-2.4110e-05, -1.5706e-05,  6.2659e-06,  ..., -2.2590e-05,
         -3.4645e-06, -6.4671e-06],
        [-3.8117e-05, -2.4825e-05,  9.7677e-06,  ..., -3.5733e-05,
         -5.5283e-06, -1.0163e-05]], device='cuda:0')
Loss: 1.228296160697937


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2623e-05,  2.6804e-04, -6.0730e-05,  ..., -1.5294e-04,
          1.5450e-04,  3.9654e-05],
        [-2.4438e-05, -1.5810e-05,  6.3479e-06,  ..., -2.2843e-05,
         -3.4310e-06, -6.6310e-06],
        [-1.6615e-05, -1.0766e-05,  4.3064e-06,  ..., -1.5512e-05,
         -2.3339e-06, -4.5002e-06],
        [-2.9057e-05, -1.8805e-05,  7.6070e-06,  ..., -2.7120e-05,
         -4.0643e-06, -7.8678e-06],
        [-4.5806e-05, -2.9624e-05,  1.1809e-05,  ..., -4.2737e-05,
         -6.4671e-06, -1.2308e-05]], device='cuda:0')
Loss: 1.2027270793914795


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7134e-05,  2.4907e-04,  1.2151e-05,  ..., -2.2545e-04,
          3.2390e-04, -1.9075e-05],
        [-2.8521e-05, -1.8477e-05,  7.4059e-06,  ..., -2.6658e-05,
         -3.8929e-06, -7.7337e-06],
        [-1.9372e-05, -1.2577e-05,  5.0254e-06,  ..., -1.8090e-05,
         -2.6468e-06, -5.2489e-06],
        [-3.3915e-05, -2.1994e-05,  8.8736e-06,  ..., -3.1650e-05,
         -4.6156e-06, -9.1791e-06],
        [-5.3406e-05, -3.4600e-05,  1.3776e-05,  ..., -4.9829e-05,
         -7.3351e-06, -1.4350e-05]], device='cuda:0')
Loss: 1.2174900770187378


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0, 16064, 10937,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9555e-06,  1.5614e-04, -5.0950e-05,  ..., -1.7573e-04,
          3.4803e-04,  1.5500e-05],
        [-3.2574e-05, -2.1145e-05,  8.4490e-06,  ..., -3.0398e-05,
         -4.4554e-06, -8.8066e-06],
        [-2.2158e-05, -1.4409e-05,  5.7369e-06,  ..., -2.0653e-05,
         -3.0324e-06, -5.9865e-06],
        [-3.8773e-05, -2.5183e-05,  1.0133e-05,  ..., -3.6120e-05,
         -5.2862e-06, -1.0461e-05],
        [-6.1035e-05, -3.9607e-05,  1.5713e-05,  ..., -5.6833e-05,
         -8.3931e-06, -1.6347e-05]], device='cuda:0')
Loss: 1.2220278978347778
Graident accumulation at epoch 0, step 87, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0860e-04, -1.7754e-04,  6.5489e-05,  ...,  2.7638e-04,
         -5.5953e-04, -1.6002e-04],
        [-1.9795e-05, -1.9007e-05,  1.1221e-06,  ..., -1.6026e-05,
         -4.3091e-06, -3.6225e-06],
        [-1.7714e-05, -1.0997e-05, -2.1071e-07,  ..., -5.0250e-06,
         -4.6353e-06, -3.5496e-06],
        [-6.1155e-06,  1.6497e-05,  3.2711e-06,  ...,  2.6806e-06,
         -3.0419e-06,  8.6873e-06],
        [-2.8027e-05, -2.9163e-05, -3.2155e-06,  ..., -1.9386e-05,
         -6.1034e-06, -2.2360e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8204e-08, 1.4722e-08, 1.7553e-08,  ..., 2.0122e-08, 3.2115e-08,
         6.4330e-09],
        [9.6996e-12, 9.8602e-12, 2.6886e-13,  ..., 6.0518e-12, 4.7742e-13,
         3.3957e-13],
        [7.9088e-12, 2.3008e-11, 1.5171e-12,  ..., 2.0753e-11, 3.0136e-12,
         9.5390e-13],
        [6.8245e-11, 2.1413e-10, 3.6613e-12,  ..., 5.5136e-11, 5.8876e-12,
         2.8044e-11],
        [2.7583e-11, 2.7258e-11, 2.2600e-12,  ..., 1.8785e-11, 1.2470e-12,
         1.1711e-12]], device='cuda:0')
optimizer state dict: 11.0
lr: [1.9847624027890693e-05, 1.9847624027890693e-05]
scheduler_last_epoch: 11


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[   0, 2555,  984,  ..., 5900, 1576,  271],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6389e-09,  4.4544e-05,  9.7376e-05,  ..., -4.3853e-06,
          4.2800e-05,  5.0492e-05],
        [-4.1127e-06, -2.5630e-06,  1.2368e-06,  ..., -3.8147e-06,
         -3.0920e-07, -1.3486e-06],
        [-2.6822e-06, -1.6764e-06,  8.0839e-07,  ..., -2.4885e-06,
         -2.0117e-07, -8.7917e-07],
        [-5.0962e-06, -3.1739e-06,  1.5423e-06,  ..., -4.7088e-06,
         -3.8370e-07, -1.6689e-06],
        [-7.4804e-06, -4.6492e-06,  2.2352e-06,  ..., -6.9141e-06,
         -5.6252e-07, -2.4438e-06]], device='cuda:0')
Loss: 1.2103558778762817


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0, 18905,  2311,  ..., 10213,   908,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3190e-05,  4.7199e-05,  1.3566e-04,  ..., -4.7735e-05,
          1.8514e-05,  8.2314e-05],
        [-8.1360e-06, -5.0813e-06,  2.4289e-06,  ..., -7.6592e-06,
         -5.5134e-07, -2.6450e-06],
        [-5.3793e-06, -3.3677e-06,  1.6093e-06,  ..., -5.0813e-06,
         -3.6322e-07, -1.7472e-06],
        [-1.0073e-05, -6.2883e-06,  3.0324e-06,  ..., -9.4771e-06,
         -6.8173e-07, -3.2634e-06],
        [-1.5080e-05, -9.3877e-06,  4.4703e-06,  ..., -1.4156e-05,
         -1.0245e-06, -4.8578e-06]], device='cuda:0')
Loss: 1.2200783491134644


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2738e-05,  5.4579e-05,  1.4733e-04,  ..., -9.8632e-05,
          3.8827e-07,  5.1357e-05],
        [-1.2219e-05, -7.5996e-06,  3.6582e-06,  ..., -1.1563e-05,
         -7.8045e-07, -4.0531e-06],
        [-8.0168e-06, -4.9993e-06,  2.4028e-06,  ..., -7.6145e-06,
         -5.1036e-07, -2.6561e-06],
        [-1.5110e-05, -9.3877e-06,  4.5598e-06,  ..., -1.4305e-05,
         -9.6299e-07, -4.9919e-06],
        [-2.2560e-05, -1.4007e-05,  6.7204e-06,  ..., -2.1338e-05,
         -1.4454e-06, -7.4357e-06]], device='cuda:0')
Loss: 1.2094837427139282


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,  1083,   287,  ...,    12,   287,  2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8187e-05,  4.4432e-05,  1.4242e-04,  ..., -1.9899e-04,
          5.2004e-05,  2.4667e-04],
        [-1.6361e-05, -1.0237e-05,  4.8727e-06,  ..., -1.5497e-05,
         -1.1176e-06, -5.4315e-06],
        [-1.0714e-05, -6.7204e-06,  3.1926e-06,  ..., -1.0177e-05,
         -7.3109e-07, -3.5539e-06],
        [-2.0236e-05, -1.2651e-05,  6.0722e-06,  ..., -1.9163e-05,
         -1.3784e-06, -6.6981e-06],
        [-3.0160e-05, -1.8835e-05,  8.9407e-06,  ..., -2.8551e-05,
         -2.0675e-06, -9.9540e-06]], device='cuda:0')
Loss: 1.2040921449661255


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[    0,  1265,  1545,  ...,   292, 36667, 36197],
        [    0,  6538,  1805,  ...,   298,   271,   721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9583e-05,  1.3290e-04,  1.2166e-04,  ..., -2.3680e-04,
          5.4962e-05,  2.5279e-04],
        [-2.0474e-05, -1.2800e-05,  5.9977e-06,  ..., -1.9461e-05,
         -1.4212e-06, -6.8173e-06],
        [-1.3411e-05, -8.3968e-06,  3.9265e-06,  ..., -1.2770e-05,
         -9.2853e-07, -4.4592e-06],
        [-2.5213e-05, -1.5736e-05,  7.4357e-06,  ..., -2.3931e-05,
         -1.7378e-06, -8.3521e-06],
        [-3.7730e-05, -2.3514e-05,  1.0997e-05,  ..., -3.5822e-05,
         -2.6301e-06, -1.2472e-05]], device='cuda:0')
Loss: 1.2317346334457397


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0,   634,  1621,  ...,   688,   586,  8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4713e-05,  1.4476e-04,  1.2788e-04,  ..., -2.0881e-04,
          5.4962e-05,  3.0821e-04],
        [-2.4498e-05, -1.5303e-05,  7.2122e-06,  ..., -2.3276e-05,
         -1.7360e-06, -8.1733e-06],
        [-1.6063e-05, -1.0043e-05,  4.7274e-06,  ..., -1.5289e-05,
         -1.1353e-06, -5.3532e-06],
        [-3.0130e-05, -1.8790e-05,  8.9258e-06,  ..., -2.8610e-05,
         -2.1197e-06, -1.0006e-05],
        [-4.5061e-05, -2.8044e-05,  1.3188e-05,  ..., -4.2737e-05,
         -3.2037e-06, -1.4916e-05]], device='cuda:0')
Loss: 1.227596640586853


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,   259,  1380,  ...,   287, 10221,   280],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8763e-05,  1.1199e-04,  1.2357e-04,  ..., -2.9735e-04,
          4.7779e-05,  2.6915e-04],
        [-2.8521e-05, -1.7881e-05,  8.3372e-06,  ..., -2.7150e-05,
         -2.0042e-06, -9.4771e-06],
        [-1.8716e-05, -1.1750e-05,  5.4725e-06,  ..., -1.7837e-05,
         -1.3132e-06, -6.2175e-06],
        [-3.5077e-05, -2.1949e-05,  1.0327e-05,  ..., -3.3379e-05,
         -2.4512e-06, -1.1608e-05],
        [-5.2422e-05, -3.2753e-05,  1.5244e-05,  ..., -4.9800e-05,
         -3.7029e-06, -1.7300e-05]], device='cuda:0')
Loss: 1.2121012210845947


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[   0,  759, 1184,  ...,  472,  346,   14],
        [   0,  560,  199,  ..., 6408,  278, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2764e-04,  1.3377e-04,  1.4458e-04,  ..., -2.6120e-04,
         -4.7879e-05,  2.2297e-04],
        [-3.2544e-05, -2.0310e-05,  9.6187e-06,  ..., -3.0950e-05,
         -2.3115e-06, -1.0900e-05],
        [-2.1398e-05, -1.3366e-05,  6.3255e-06,  ..., -2.0370e-05,
         -1.5190e-06, -7.1600e-06],
        [-4.0203e-05, -2.5019e-05,  1.1966e-05,  ..., -3.8207e-05,
         -2.8405e-06, -1.3396e-05],
        [-5.9903e-05, -3.7223e-05,  1.7613e-05,  ..., -5.6833e-05,
         -4.2841e-06, -1.9908e-05]], device='cuda:0')
Loss: 1.20297372341156
Graident accumulation at epoch 0, step 95, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5050e-04, -1.4641e-04,  7.3399e-05,  ...,  2.2262e-04,
         -5.0837e-04, -1.2172e-04],
        [-2.1070e-05, -1.9137e-05,  1.9718e-06,  ..., -1.7519e-05,
         -4.1093e-06, -4.3503e-06],
        [-1.8083e-05, -1.1234e-05,  4.4292e-07,  ..., -6.5595e-06,
         -4.3237e-06, -3.9106e-06],
        [-9.5243e-06,  1.2346e-05,  4.1405e-06,  ..., -1.4081e-06,
         -3.0217e-06,  6.4789e-06],
        [-3.1215e-05, -2.9969e-05, -1.1326e-06,  ..., -2.3131e-05,
         -5.9214e-06, -4.0032e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8172e-08, 1.4726e-08, 1.7556e-08,  ..., 2.0170e-08, 3.2086e-08,
         6.4763e-09],
        [1.0749e-11, 1.0263e-11, 3.6111e-13,  ..., 7.0036e-12, 4.8229e-13,
         4.5805e-13],
        [8.3588e-12, 2.3163e-11, 1.5556e-12,  ..., 2.1147e-11, 3.0129e-12,
         1.0042e-12],
        [6.9793e-11, 2.1454e-10, 3.8008e-12,  ..., 5.6541e-11, 5.8897e-12,
         2.8195e-11],
        [3.1144e-11, 2.8616e-11, 2.5679e-12,  ..., 2.1996e-11, 1.2641e-12,
         1.5662e-12]], device='cuda:0')
optimizer state dict: 12.0
lr: [1.9801623778739208e-05, 1.9801623778739208e-05]
scheduler_last_epoch: 12


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[   0, 3119,  278,  ...,  352,  674,  369],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8224e-05, -4.3196e-05,  8.9159e-05,  ..., -8.9203e-05,
         -2.8848e-05,  4.9396e-06],
        [-4.0531e-06, -2.5183e-06,  1.3709e-06,  ..., -3.9041e-06,
         -5.5414e-08, -1.6615e-06],
        [ 1.4773e-04,  1.3076e-04, -3.8979e-05,  ...,  2.1269e-04,
         -2.5851e-05,  1.1925e-04],
        [-5.0664e-06, -3.1441e-06,  1.7211e-06,  ..., -4.8578e-06,
         -6.7055e-08, -2.0713e-06],
        [-7.3612e-06, -4.5598e-06,  2.4736e-06,  ..., -7.0632e-06,
         -1.0384e-07, -2.9951e-06]], device='cuda:0')
Loss: 1.2331515550613403


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1128e-05, -3.2176e-05,  7.4175e-05,  ..., -8.1211e-05,
         -6.1812e-05, -2.2960e-05],
        [-8.0168e-06, -4.9323e-06,  2.7120e-06,  ..., -7.6145e-06,
         -8.7544e-08, -3.3230e-06],
        [ 1.4513e-04,  1.2917e-04, -3.8100e-05,  ...,  2.1026e-04,
         -2.5871e-05,  1.1816e-04],
        [-1.0103e-05, -6.1989e-06,  3.4273e-06,  ..., -9.5367e-06,
         -1.0710e-07, -4.1574e-06],
        [-1.4812e-05, -9.0599e-06,  4.9621e-06,  ..., -1.3977e-05,
         -1.6810e-07, -6.0946e-06]], device='cuda:0')
Loss: 1.212284803390503


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5689e-04, -7.1925e-05,  6.0942e-06,  ..., -3.7749e-06,
         -4.2536e-05,  5.6469e-05],
        [-1.2219e-05, -7.4506e-06,  4.0457e-06,  ..., -1.1548e-05,
         -2.3842e-07, -4.9919e-06],
        [ 1.4243e-04,  1.2755e-04, -3.7243e-05,  ...,  2.0772e-04,
         -2.5969e-05,  1.1708e-04],
        [-1.5169e-05, -9.2387e-06,  5.0440e-06,  ..., -1.4275e-05,
         -2.8498e-07, -6.1691e-06],
        [-2.2501e-05, -1.3649e-05,  7.3910e-06,  ..., -2.1160e-05,
         -4.4564e-07, -9.1344e-06]], device='cuda:0')
Loss: 1.226502776145935


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,   590,    16,  ...,    13,    35,  1151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9635e-05, -1.1614e-04,  2.9537e-05,  ..., -1.0924e-06,
         -4.0349e-05,  5.8634e-05],
        [-1.6421e-05, -9.9242e-06,  5.4091e-06,  ..., -1.5512e-05,
         -3.1339e-07, -6.7353e-06],
        [ 1.3979e-04,  1.2599e-04, -3.6383e-05,  ...,  2.0522e-04,
         -2.6015e-05,  1.1598e-04],
        [-2.0355e-05, -1.2308e-05,  6.7353e-06,  ..., -1.9163e-05,
         -3.7206e-07, -8.3148e-06],
        [-3.0071e-05, -1.8120e-05,  9.8497e-06,  ..., -2.8312e-05,
         -5.8068e-07, -1.2264e-05]], device='cuda:0')
Loss: 1.2263405323028564


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   17,  271,  266],
        [   0, 6112,  278,  ..., 4092,  490, 2774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4585e-04, -1.5542e-04,  9.0014e-06,  ...,  3.5034e-05,
         -9.9486e-05,  1.1134e-04],
        [-2.0534e-05, -1.2398e-05,  6.8098e-06,  ..., -1.9357e-05,
         -3.7812e-07, -8.3372e-06],
        [ 2.0551e-04,  1.8139e-04, -7.2079e-05,  ...,  3.0631e-04,
         -8.5386e-06,  1.7788e-04],
        [-2.5511e-05, -1.5423e-05,  8.5011e-06,  ..., -2.3991e-05,
         -4.4797e-07, -1.0327e-05],
        [-3.7611e-05, -2.2650e-05,  1.2413e-05,  ..., -3.5375e-05,
         -6.9616e-07, -1.5199e-05]], device='cuda:0')
Loss: 1.1983712911605835


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[   0, 1064, 1042,  ...,   12,  259, 4754],
        [   0,  278,  565,  ..., 1125, 5222,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0977e-04, -1.7700e-04,  2.4574e-06,  ...,  5.9165e-05,
         -4.9890e-05,  1.9309e-04],
        [-2.4736e-05, -1.4871e-05,  8.1360e-06,  ..., -2.3156e-05,
         -4.5961e-07, -9.9838e-06],
        [ 2.0279e-04,  1.7978e-04, -7.1215e-05,  ...,  3.0383e-04,
         -8.5912e-06,  1.7681e-04],
        [-3.0667e-05, -1.8463e-05,  1.0140e-05,  ..., -2.8670e-05,
         -5.4715e-07, -1.2353e-05],
        [-4.5359e-05, -2.7210e-05,  1.4856e-05,  ..., -4.2379e-05,
         -8.5821e-07, -1.8224e-05]], device='cuda:0')
Loss: 1.2152860164642334


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,     9,   300,  ...,  6838,   328, 18619],
        [    0,  1934,  2413,  ..., 19697,    13, 16325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8351e-04, -1.7669e-04, -3.7789e-05,  ...,  1.1373e-04,
          6.6684e-06,  2.2431e-04],
        [-2.8878e-05, -1.7330e-05,  9.4399e-06,  ..., -2.7061e-05,
         -5.5647e-07, -1.1653e-05],
        [ 2.0015e-04,  1.7821e-04, -7.0388e-05,  ...,  3.0134e-04,
         -8.6541e-06,  1.7574e-04],
        [-3.5763e-05, -2.1487e-05,  1.1757e-05,  ..., -3.3498e-05,
         -6.6636e-07, -1.4409e-05],
        [-5.2869e-05, -3.1650e-05,  1.7211e-05,  ..., -4.9472e-05,
         -1.0426e-06, -2.1249e-05]], device='cuda:0')
Loss: 1.199967622756958


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0,  4485,   741,  ...,   292,   221,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3121e-04, -2.5398e-04, -8.5733e-06,  ...,  2.8562e-05,
          2.4776e-05,  2.7812e-04],
        [-3.3021e-05, -1.9848e-05,  1.0781e-05,  ..., -3.0994e-05,
         -6.1351e-07, -1.3344e-05],
        [ 1.9750e-04,  1.7660e-04, -6.9523e-05,  ...,  2.9883e-04,
         -8.6897e-06,  1.7466e-04],
        [-4.0889e-05, -2.4617e-05,  1.3433e-05,  ..., -3.8356e-05,
         -7.3249e-07, -1.6496e-05],
        [-6.0409e-05, -3.6240e-05,  1.9640e-05,  ..., -5.6595e-05,
         -1.1493e-06, -2.4304e-05]], device='cuda:0')
Loss: 1.1989336013793945
Graident accumulation at epoch 0, step 103, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0034,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0141, -0.0267,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.0857e-04, -1.5717e-04,  6.5202e-05,  ...,  2.0321e-04,
         -4.5505e-04, -8.1737e-05],
        [-2.2265e-05, -1.9208e-05,  2.8527e-06,  ..., -1.8866e-05,
         -3.7597e-06, -5.2496e-06],
        [ 3.4753e-06,  7.5496e-06, -6.5537e-06,  ...,  2.3979e-05,
         -4.7603e-06,  1.3947e-05],
        [-1.2661e-05,  8.6496e-06,  5.0698e-06,  ..., -5.1029e-06,
         -2.7928e-06,  4.1815e-06],
        [-3.4134e-05, -3.0596e-05,  9.4463e-07,  ..., -2.6477e-05,
         -5.4442e-06, -6.0332e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8178e-08, 1.4775e-08, 1.7539e-08,  ..., 2.0151e-08, 3.2054e-08,
         6.5472e-09],
        [1.1829e-11, 1.0647e-11, 4.7698e-13,  ..., 7.9573e-12, 4.8218e-13,
         6.3565e-13],
        [4.7356e-11, 5.4327e-11, 6.3875e-12,  ..., 1.1042e-10, 3.0854e-12,
         3.1511e-11],
        [7.1395e-11, 2.1493e-10, 3.9774e-12,  ..., 5.7955e-11, 5.8844e-12,
         2.8439e-11],
        [3.4762e-11, 2.9901e-11, 2.9511e-12,  ..., 2.5177e-11, 1.2642e-12,
         2.1553e-12]], device='cuda:0')
optimizer state dict: 13.0
lr: [1.974963266376872e-05, 1.974963266376872e-05]
scheduler_last_epoch: 13


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3011e-05,  4.5296e-05,  9.4027e-06,  ..., -3.6708e-05,
         -4.2519e-05,  0.0000e+00],
        [-4.1425e-06, -2.4289e-06,  1.3709e-06,  ..., -3.8445e-06,
          6.6590e-08, -1.8477e-06],
        [-2.6822e-06, -1.5646e-06,  8.8289e-07,  ..., -2.4736e-06,
          4.1677e-08, -1.1921e-06],
        [-5.2154e-06, -3.0398e-06,  1.7285e-06,  ..., -4.8280e-06,
          8.7079e-08, -2.3246e-06],
        [-7.6890e-06, -4.4703e-06,  2.5183e-06,  ..., -7.0930e-06,
          1.1548e-07, -3.4124e-06]], device='cuda:0')
Loss: 1.2154868841171265


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1973e-05,  5.7657e-05,  2.1188e-05,  ..., -5.9500e-05,
          2.6106e-05,  2.3044e-05],
        [-8.4043e-06, -5.0068e-06,  2.7791e-06,  ..., -7.8082e-06,
          1.0524e-07, -3.7327e-06],
        [-5.3942e-06, -3.2112e-06,  1.7807e-06,  ..., -4.9919e-06,
          6.9384e-08, -2.3916e-06],
        [-1.0490e-05, -6.2436e-06,  3.4794e-06,  ..., -9.7156e-06,
          1.4179e-07, -4.6641e-06],
        [-1.5616e-05, -9.2685e-06,  5.1409e-06,  ..., -1.4454e-05,
          1.9278e-07, -6.8992e-06]], device='cuda:0')
Loss: 1.2115004062652588


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0,  527, 2811,  ...,  287, 1288,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9166e-05,  4.1751e-05,  7.7109e-05,  ..., -2.0006e-04,
          1.7233e-05,  1.0482e-05],
        [-1.2547e-05, -7.4506e-06,  4.0978e-06,  ..., -1.1653e-05,
          1.6950e-07, -5.5879e-06],
        [-8.0764e-06, -4.7833e-06,  2.6301e-06,  ..., -7.4655e-06,
          1.0990e-07, -3.5912e-06],
        [-1.5706e-05, -9.3132e-06,  5.1409e-06,  ..., -1.4514e-05,
          2.2515e-07, -7.0035e-06],
        [-2.3365e-05, -1.3828e-05,  7.5847e-06,  ..., -2.1577e-05,
          3.0641e-07, -1.0356e-05]], device='cuda:0')
Loss: 1.2058374881744385


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[   0,  300, 1064,  ..., 6953,  944,  278],
        [   0,   35, 3815,  ...,  278, 7097, 4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0914e-04, -2.5729e-06,  7.3865e-05,  ..., -1.5006e-04,
          2.2013e-05,  4.6512e-06],
        [-1.6779e-05, -9.8497e-06,  5.4985e-06,  ..., -1.5527e-05,
          2.3004e-07, -7.4655e-06],
        [-1.0818e-05, -6.3330e-06,  3.5390e-06,  ..., -9.9689e-06,
          1.4855e-07, -4.8056e-06],
        [-2.0862e-05, -1.2249e-05,  6.8694e-06,  ..., -1.9252e-05,
          3.0710e-07, -9.2983e-06],
        [-3.1173e-05, -1.8239e-05,  1.0163e-05,  ..., -2.8700e-05,
          4.1584e-07, -1.3798e-05]], device='cuda:0')
Loss: 1.2011170387268066


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4908e-05, -2.5440e-06,  1.1875e-04,  ..., -2.4247e-04,
          9.3075e-05,  1.7444e-05],
        [-2.0862e-05, -1.2279e-05,  6.8918e-06,  ..., -1.9401e-05,
          3.8836e-07, -9.2685e-06],
        [-1.3456e-05, -7.9051e-06,  4.4368e-06,  ..., -1.2472e-05,
          2.5099e-07, -5.9679e-06],
        [-2.5988e-05, -1.5303e-05,  8.6278e-06,  ..., -2.4110e-05,
          5.1013e-07, -1.1563e-05],
        [-3.8654e-05, -2.2680e-05,  1.2711e-05,  ..., -3.5793e-05,
          7.0268e-07, -1.7092e-05]], device='cuda:0')
Loss: 1.2209093570709229


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,  9017,   600,  ...,  6133,  1098,   352],
        [    0,   287,  6761,  ...,  1918, 33351,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7843e-05,  2.4681e-06,  1.7749e-04,  ..., -2.9667e-04,
          9.2690e-05,  7.5446e-05],
        [-2.5094e-05, -1.4782e-05,  8.2552e-06,  ..., -2.3305e-05,
          4.6054e-07, -1.1176e-05],
        [-1.6183e-05, -9.5218e-06,  5.3197e-06,  ..., -1.4991e-05,
          2.9826e-07, -7.2047e-06],
        [-3.1292e-05, -1.8463e-05,  1.0356e-05,  ..., -2.9027e-05,
          6.0513e-07, -1.3962e-05],
        [-4.6521e-05, -2.7329e-05,  1.5244e-05,  ..., -4.3064e-05,
          8.3959e-07, -2.0638e-05]], device='cuda:0')
Loss: 1.2136069536209106


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[    0,   445,     8,  ...,    13, 25386,    17],
        [    0,  1756,   271,  ...,   259, 48595, 19882]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0477e-05, -3.6386e-05,  1.6661e-04,  ..., -2.3264e-04,
          9.5346e-05,  4.3967e-05],
        [-2.9236e-05, -1.7166e-05,  9.6187e-06,  ..., -2.7120e-05,
          6.0862e-07, -1.3061e-05],
        [-1.8880e-05, -1.1079e-05,  6.2063e-06,  ..., -1.7479e-05,
          3.9418e-07, -8.4341e-06],
        [-3.6508e-05, -2.1473e-05,  1.2077e-05,  ..., -3.3826e-05,
          7.9046e-07, -1.6332e-05],
        [-5.4210e-05, -3.1769e-05,  1.7762e-05,  ..., -5.0128e-05,
          1.1059e-06, -2.4125e-05]], device='cuda:0')
Loss: 1.201424479484558


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,  3380,  1197,  ...,   631,   369,  3123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1005e-04, -6.8045e-05,  1.7635e-04,  ..., -1.5201e-04,
          9.6117e-05,  2.4331e-05],
        [-3.3498e-05, -1.9684e-05,  1.0982e-05,  ..., -3.0965e-05,
          7.2177e-07, -1.4909e-05],
        [-2.1636e-05, -1.2711e-05,  7.0930e-06,  ..., -1.9953e-05,
          4.6543e-07, -9.6262e-06],
        [-4.1753e-05, -2.4572e-05,  1.3769e-05,  ..., -3.8564e-05,
          9.2736e-07, -1.8597e-05],
        [-6.2078e-05, -3.6418e-05,  2.0280e-05,  ..., -5.7220e-05,
          1.3090e-06, -2.7522e-05]], device='cuda:0')
Loss: 1.2080137729644775
Graident accumulation at epoch 0, step 111, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5872e-04, -1.4825e-04,  7.6316e-05,  ...,  1.6769e-04,
         -3.9994e-04, -7.1131e-05],
        [-2.3388e-05, -1.9256e-05,  3.6657e-06,  ..., -2.0076e-05,
         -3.3116e-06, -6.2155e-06],
        [ 9.6408e-07,  5.5235e-06, -5.1890e-06,  ...,  1.9586e-05,
         -4.2377e-06,  1.1590e-05],
        [-1.5570e-05,  5.3274e-06,  5.9397e-06,  ..., -8.4490e-06,
         -2.4208e-06,  1.9037e-06],
        [-3.6929e-05, -3.1178e-05,  2.8782e-06,  ..., -2.9551e-05,
         -4.7689e-06, -8.1822e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8141e-08, 1.4765e-08, 1.7552e-08,  ..., 2.0154e-08, 3.2031e-08,
         6.5412e-09],
        [1.2939e-11, 1.1023e-11, 5.9711e-13,  ..., 8.9081e-12, 4.8222e-13,
         8.5728e-13],
        [4.7776e-11, 5.4434e-11, 6.4315e-12,  ..., 1.1071e-10, 3.0825e-12,
         3.1572e-11],
        [7.3067e-11, 2.1532e-10, 4.1630e-12,  ..., 5.9385e-11, 5.8794e-12,
         2.8757e-11],
        [3.8581e-11, 3.1197e-11, 3.3594e-12,  ..., 2.8426e-11, 1.2646e-12,
         2.9106e-12]], device='cuda:0')
optimizer state dict: 14.0
lr: [1.9691682460550022e-05, 1.9691682460550022e-05]
scheduler_last_epoch: 14


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9940e-05,  9.7691e-06,  3.8137e-05,  ..., -5.0547e-05,
          0.0000e+00, -1.2541e-04],
        [-4.1723e-06, -2.3842e-06,  1.2219e-06,  ..., -3.6806e-06,
          2.2352e-07, -1.9372e-06],
        [-2.7120e-06, -1.5497e-06,  7.9349e-07,  ..., -2.3842e-06,
          1.4622e-07, -1.2591e-06],
        [-5.2750e-06, -3.0100e-06,  1.5497e-06,  ..., -4.6492e-06,
          2.8498e-07, -2.4438e-06],
        [-7.8678e-06, -4.4703e-06,  2.2948e-06,  ..., -6.8843e-06,
          4.1723e-07, -3.6359e-06]], device='cuda:0')
Loss: 1.1935677528381348


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[   0,  360,  259,  ...,   14,  381, 1371],
        [   0,  995,   13,  ..., 3494,  367, 6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9340e-05,  4.4106e-05,  1.6853e-04,  ..., -1.3488e-04,
         -6.9995e-06, -1.1296e-04],
        [-8.4639e-06, -4.7982e-06,  2.4363e-06,  ..., -7.3761e-06,
          4.5169e-07, -3.8147e-06],
        [-5.4985e-06, -3.1218e-06,  1.5870e-06,  ..., -4.7833e-06,
          2.9523e-07, -2.4810e-06],
        [-1.0639e-05, -6.0499e-06,  3.0845e-06,  ..., -9.2685e-06,
          5.7369e-07, -4.7982e-06],
        [-1.5974e-05, -9.0599e-06,  4.5896e-06,  ..., -1.3888e-05,
          8.4564e-07, -7.1824e-06]], device='cuda:0')
Loss: 1.2194212675094604


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7129e-05,  3.0555e-05,  2.1951e-04,  ..., -1.5253e-04,
         -5.2743e-06, -6.8827e-05],
        [-1.2666e-05, -7.2122e-06,  3.6433e-06,  ..., -1.1086e-05,
          6.0163e-07, -5.7369e-06],
        [-8.2552e-06, -4.7088e-06,  2.3805e-06,  ..., -7.2122e-06,
          3.9442e-07, -3.7402e-06],
        [-1.5825e-05, -9.0450e-06,  4.5821e-06,  ..., -1.3858e-05,
          7.6182e-07, -7.1675e-06],
        [-2.4080e-05, -1.3709e-05,  6.8992e-06,  ..., -2.1011e-05,
          1.1306e-06, -1.0863e-05]], device='cuda:0')
Loss: 1.1960779428482056


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0, 1732,  292,  ..., 3440, 4010, 1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1440e-05,  3.6137e-05,  1.8196e-04,  ..., -1.0307e-04,
          5.4148e-06, -6.1275e-05],
        [-1.6898e-05, -9.6709e-06,  4.9248e-06,  ..., -1.4842e-05,
          7.6368e-07, -7.6592e-06],
        [-1.0997e-05, -6.3032e-06,  3.2075e-06,  ..., -9.6411e-06,
          5.0012e-07, -4.9844e-06],
        [-2.1070e-05, -1.2100e-05,  6.1765e-06,  ..., -1.8507e-05,
          9.6578e-07, -9.5516e-06],
        [-3.2127e-05, -1.8358e-05,  9.3132e-06,  ..., -2.8104e-05,
          1.4342e-06, -1.4499e-05]], device='cuda:0')
Loss: 1.2113531827926636


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  1874,   300,  ...,    14,  5372,    12],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4555e-05,  3.6137e-05,  1.9356e-04,  ..., -1.1468e-04,
          4.6677e-06, -4.8570e-05],
        [-2.1160e-05, -1.2115e-05,  6.2138e-06,  ..., -1.8597e-05,
          9.2201e-07, -9.6112e-06],
        [-1.3754e-05, -7.8827e-06,  4.0419e-06,  ..., -1.2070e-05,
          6.0257e-07, -6.2510e-06],
        [-2.6345e-05, -1.5110e-05,  7.7784e-06,  ..., -2.3156e-05,
          1.1632e-06, -1.1966e-05],
        [-4.0114e-05, -2.2918e-05,  1.1712e-05,  ..., -3.5137e-05,
          1.7248e-06, -1.8150e-05]], device='cuda:0')
Loss: 1.1905664205551147


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,  1103,   271,  ...,   957,   756,   368],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9758e-05, -1.6967e-05,  1.9724e-04,  ..., -1.3082e-04,
         -4.8690e-05, -3.8977e-05],
        [-2.5362e-05, -1.4514e-05,  7.4506e-06,  ..., -2.2337e-05,
          1.1176e-06, -1.1578e-05],
        [-1.6496e-05, -9.4473e-06,  4.8466e-06,  ..., -1.4514e-05,
          7.2923e-07, -7.5325e-06],
        [-3.1650e-05, -1.8150e-05,  9.3505e-06,  ..., -2.7865e-05,
          1.4147e-06, -1.4439e-05],
        [-4.8041e-05, -2.7448e-05,  1.4037e-05,  ..., -4.2200e-05,
          2.0918e-06, -2.1845e-05]], device='cuda:0')
Loss: 1.2105180025100708


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5898e-05, -3.3250e-05,  2.5482e-04,  ..., -6.2195e-05,
         -4.5100e-05,  2.1886e-05],
        [-2.9594e-05, -1.6898e-05,  8.6799e-06,  ..., -2.6062e-05,
          1.3141e-06, -1.3515e-05],
        [-1.9252e-05, -1.1005e-05,  5.6475e-06,  ..., -1.6943e-05,
          8.5775e-07, -8.7991e-06],
        [-3.6955e-05, -2.1130e-05,  1.0900e-05,  ..., -3.2544e-05,
          1.6605e-06, -1.6868e-05],
        [-5.5969e-05, -3.1918e-05,  1.6332e-05,  ..., -4.9204e-05,
          2.4550e-06, -2.5466e-05]], device='cuda:0')
Loss: 1.2021347284317017


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5931e-04, -9.3289e-06,  2.5226e-04,  ..., -8.7534e-05,
          3.4037e-05, -2.4320e-06],
        [-3.3826e-05, -1.9401e-05,  9.9540e-06,  ..., -2.9832e-05,
          1.5208e-06, -1.5408e-05],
        [-2.1964e-05, -1.2614e-05,  6.4634e-06,  ..., -1.9357e-05,
          9.9000e-07, -1.0014e-05],
        [-4.2230e-05, -2.4244e-05,  1.2487e-05,  ..., -3.7223e-05,
          1.9232e-06, -1.9222e-05],
        [-6.3837e-05, -3.6597e-05,  1.8701e-05,  ..., -5.6207e-05,
          2.8387e-06, -2.8983e-05]], device='cuda:0')
Loss: 1.201183557510376
Graident accumulation at epoch 0, step 119, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0153,  0.0039,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1878e-04, -1.3436e-04,  9.3910e-05,  ...,  1.4217e-04,
         -3.5654e-04, -6.4261e-05],
        [-2.4432e-05, -1.9270e-05,  4.2945e-06,  ..., -2.1052e-05,
         -2.8284e-06, -7.1348e-06],
        [-1.3288e-06,  3.7098e-06, -4.0238e-06,  ...,  1.5692e-05,
         -3.7149e-06,  9.4292e-06],
        [-1.8236e-05,  2.3703e-06,  6.5945e-06,  ..., -1.1326e-05,
         -1.9864e-06, -2.0895e-07],
        [-3.9619e-05, -3.1720e-05,  4.4605e-06,  ..., -3.2217e-05,
         -4.0081e-06, -1.0262e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8119e-08, 1.4751e-08, 1.7598e-08,  ..., 2.0141e-08, 3.2000e-08,
         6.5347e-09],
        [1.4070e-11, 1.1389e-11, 6.9559e-13,  ..., 9.7892e-12, 4.8405e-13,
         1.0938e-12],
        [4.8211e-11, 5.4539e-11, 6.4668e-12,  ..., 1.1097e-10, 3.0804e-12,
         3.1641e-11],
        [7.4777e-11, 2.1569e-10, 4.3148e-12,  ..., 6.0711e-11, 5.8772e-12,
         2.9097e-11],
        [4.2618e-11, 3.2505e-11, 3.7058e-12,  ..., 3.1557e-11, 1.2714e-12,
         3.7477e-12]], device='cuda:0')
optimizer state dict: 15.0
lr: [1.9627808588917577e-05, 1.9627808588917577e-05]
scheduler_last_epoch: 15


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4195e-05,  1.2402e-05, -3.6536e-05,  ...,  1.8488e-05,
          1.4020e-05, -5.0351e-05],
        [-4.2319e-06, -2.4736e-06,  1.1176e-06,  ..., -3.6061e-06,
          1.6391e-07, -2.0117e-06],
        [-2.8014e-06, -1.6391e-06,  7.4133e-07,  ..., -2.3842e-06,
          1.0803e-07, -1.3337e-06],
        [-5.3048e-06, -3.0994e-06,  1.4082e-06,  ..., -4.5300e-06,
          2.0582e-07, -2.5332e-06],
        [-8.2254e-06, -4.8280e-06,  2.1756e-06,  ..., -7.0333e-06,
          3.2037e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.19968843460083


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2448e-05, -5.1071e-05, -9.0656e-06,  ..., -1.7896e-05,
          3.9673e-05, -1.1362e-04],
        [-8.4937e-06, -5.0068e-06,  2.2873e-06,  ..., -7.2420e-06,
          3.3341e-07, -4.0382e-06],
        [-5.6177e-06, -3.3230e-06,  1.5199e-06,  ..., -4.7982e-06,
          2.2165e-07, -2.6822e-06],
        [-1.0550e-05, -6.2287e-06,  2.8610e-06,  ..., -9.0301e-06,
          4.1816e-07, -5.0366e-06],
        [-1.6451e-05, -9.7454e-06,  4.4405e-06,  ..., -1.4096e-05,
          6.5006e-07, -7.8380e-06]], device='cuda:0')
Loss: 1.2007876634597778


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[   0,  462, 9202,  ...,   15, 3256,  271],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7016e-05, -1.3168e-04, -3.3302e-05,  ...,  1.6162e-06,
          5.1652e-06, -1.6250e-05],
        [-1.2755e-05, -7.5251e-06,  3.4422e-06,  ..., -1.0878e-05,
          5.7928e-07, -6.0052e-06],
        [-8.4490e-06, -4.9993e-06,  2.2873e-06,  ..., -7.2122e-06,
          3.8370e-07, -3.9861e-06],
        [-1.5795e-05, -9.3281e-06,  4.2915e-06,  ..., -1.3500e-05,
          7.2177e-07, -7.4506e-06],
        [-2.4796e-05, -1.4693e-05,  6.7055e-06,  ..., -2.1219e-05,
          1.1232e-06, -1.1683e-05]], device='cuda:0')
Loss: 1.200986623764038


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   266,  8802,  ...,  8401,     9,   287],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4371e-05, -1.9337e-04, -5.9015e-05,  ..., -2.5627e-05,
          2.1211e-05,  2.6412e-05],
        [-1.7047e-05, -1.0043e-05,  4.5821e-06,  ..., -1.4573e-05,
          7.8604e-07, -8.0466e-06],
        [-1.1265e-05, -6.6534e-06,  3.0324e-06,  ..., -9.6262e-06,
          5.2154e-07, -5.3272e-06],
        [-2.1040e-05, -1.2413e-05,  5.6848e-06,  ..., -1.8001e-05,
          9.7882e-07, -9.9540e-06],
        [-3.3200e-05, -1.9610e-05,  8.9109e-06,  ..., -2.8402e-05,
          1.5274e-06, -1.5676e-05]], device='cuda:0')
Loss: 1.208747148513794


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8567e-05, -2.2656e-04, -3.4575e-05,  ..., -2.4624e-05,
          3.9524e-05,  3.6609e-05],
        [-2.1368e-05, -1.2502e-05,  5.7295e-06,  ..., -1.8209e-05,
          9.9279e-07, -1.0058e-05],
        [-1.4082e-05, -8.2627e-06,  3.7812e-06,  ..., -1.2010e-05,
          6.5658e-07, -6.6385e-06],
        [-2.6375e-05, -1.5453e-05,  7.1153e-06,  ..., -2.2501e-05,
          1.2377e-06, -1.2442e-05],
        [-4.1544e-05, -2.4378e-05,  1.1131e-05,  ..., -3.5435e-05,
          1.9278e-06, -1.9550e-05]], device='cuda:0')
Loss: 1.1971620321273804


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 1268,  278,  ...,  461,  925,  630],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7857e-05, -2.2510e-04, -2.5954e-05,  ..., -1.2868e-05,
          4.6824e-05,  4.8973e-06],
        [-2.5630e-05, -1.5020e-05,  6.8024e-06,  ..., -2.1860e-05,
          1.1753e-06, -1.2025e-05],
        [-1.6868e-05, -9.9093e-06,  4.4815e-06,  ..., -1.4409e-05,
          7.7579e-07, -7.9274e-06],
        [-3.1650e-05, -1.8552e-05,  8.4415e-06,  ..., -2.7031e-05,
          1.4687e-06, -1.4871e-05],
        [-4.9829e-05, -2.9296e-05,  1.3202e-05,  ..., -4.2558e-05,
          2.2836e-06, -2.3395e-05]], device='cuda:0')
Loss: 1.1919198036193848


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    14,   333,   199],
        [    0,   650,    14,  ...,  3687,   278, 26952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2651e-04, -1.9989e-04, -3.4091e-05,  ..., -1.6761e-05,
          7.8894e-06, -2.6334e-05],
        [-2.9892e-05, -1.7494e-05,  7.9274e-06,  ..., -2.5481e-05,
          1.3132e-06, -1.4037e-05],
        [-1.9699e-05, -1.1541e-05,  5.2266e-06,  ..., -1.6809e-05,
          8.6846e-07, -9.2611e-06],
        [-3.6895e-05, -2.1592e-05,  9.8348e-06,  ..., -3.1501e-05,
          1.6429e-06, -1.7345e-05],
        [-5.8115e-05, -3.4094e-05,  1.5393e-05,  ..., -4.9621e-05,
          2.5518e-06, -2.7299e-05]], device='cuda:0')
Loss: 1.1875700950622559


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  4665,   909,  ...,  3607,   259,  1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8308e-04, -2.2300e-04,  3.4553e-06,  ...,  8.9508e-06,
          3.7284e-05, -3.0034e-05],
        [-3.4243e-05, -1.9968e-05,  9.1195e-06,  ..., -2.9102e-05,
          1.4994e-06, -1.6078e-05],
        [-2.2545e-05, -1.3158e-05,  6.0052e-06,  ..., -1.9178e-05,
          9.8953e-07, -1.0587e-05],
        [-4.2230e-05, -2.4632e-05,  1.1295e-05,  ..., -3.5942e-05,
          1.8720e-06, -1.9848e-05],
        [-6.6578e-05, -3.8892e-05,  1.7703e-05,  ..., -5.6684e-05,
          2.9095e-06, -3.1263e-05]], device='cuda:0')
Loss: 1.19795560836792
Graident accumulation at epoch 0, step 127, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8521e-04, -1.4323e-04,  8.4865e-05,  ...,  1.2885e-04,
         -3.1716e-04, -6.0838e-05],
        [-2.5413e-05, -1.9340e-05,  4.7770e-06,  ..., -2.1857e-05,
         -2.3956e-06, -8.0291e-06],
        [-3.4504e-06,  2.0230e-06, -3.0209e-06,  ...,  1.2205e-05,
         -3.2445e-06,  7.4276e-06],
        [-2.0635e-05, -3.2994e-07,  7.0645e-06,  ..., -1.3788e-05,
         -1.6006e-06, -2.1729e-06],
        [-4.2315e-05, -3.2437e-05,  5.7847e-06,  ..., -3.4664e-05,
         -3.3164e-06, -1.2362e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8104e-08, 1.4786e-08, 1.7581e-08,  ..., 2.0121e-08, 3.1970e-08,
         6.5290e-09],
        [1.5229e-11, 1.1776e-11, 7.7806e-13,  ..., 1.0626e-11, 4.8581e-13,
         1.3512e-12],
        [4.8671e-11, 5.4657e-11, 6.4964e-12,  ..., 1.1123e-10, 3.0783e-12,
         3.1721e-11],
        [7.6486e-11, 2.1608e-10, 4.4381e-12,  ..., 6.1942e-11, 5.8748e-12,
         2.9462e-11],
        [4.7008e-11, 3.3986e-11, 4.0155e-12,  ..., 3.4738e-11, 1.2786e-12,
         4.7213e-12]], device='cuda:0')
optimizer state dict: 16.0
lr: [1.9558050089320493e-05, 1.9558050089320493e-05]
scheduler_last_epoch: 16


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9715e-05, -6.0941e-06, -2.8053e-05,  ...,  3.9965e-05,
         -1.6696e-05,  2.7596e-05],
        [-4.3511e-06, -2.6077e-06,  1.0207e-06,  ..., -3.5614e-06,
          1.5181e-07, -2.1309e-06],
        [-2.9355e-06, -1.7583e-06,  6.8918e-07,  ..., -2.4140e-06,
          1.0291e-07, -1.4380e-06],
        [-5.3346e-06, -3.1888e-06,  1.2517e-06,  ..., -4.3809e-06,
          1.8999e-07, -2.6077e-06],
        [-8.7619e-06, -5.2452e-06,  2.0415e-06,  ..., -7.1526e-06,
          3.0175e-07, -4.2915e-06]], device='cuda:0')
Loss: 1.2069752216339111


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1682,  271,  ...,  367, 3210,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0205e-05,  4.6686e-06, -9.1438e-05,  ...,  2.0387e-05,
         -5.2345e-06, -1.3473e-05],
        [-8.5831e-06, -5.1409e-06,  2.0266e-06,  ..., -7.0930e-06,
          2.9430e-07, -4.1425e-06],
        [-5.7667e-06, -3.4571e-06,  1.3597e-06,  ..., -4.7833e-06,
          1.9791e-07, -2.7940e-06],
        [-1.0490e-05, -6.2883e-06,  2.4810e-06,  ..., -8.7023e-06,
          3.6694e-07, -5.0664e-06],
        [-1.7285e-05, -1.0341e-05,  4.0531e-06,  ..., -1.4246e-05,
          5.8487e-07, -8.3447e-06]], device='cuda:0')
Loss: 1.2197201251983643


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0,   471,    12,  ...,    13,  9909,  2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1334e-06,  4.6299e-05, -9.4969e-05,  ...,  1.0296e-05,
         -2.9535e-05, -2.3242e-05],
        [-1.2904e-05, -7.7039e-06,  2.9691e-06,  ..., -1.0625e-05,
          4.6287e-07, -6.2287e-06],
        [-8.6576e-06, -5.1782e-06,  1.9968e-06,  ..., -7.1526e-06,
          3.1153e-07, -4.1872e-06],
        [-1.5765e-05, -9.4175e-06,  3.6433e-06,  ..., -1.3024e-05,
          5.7463e-07, -7.5996e-06],
        [-2.5928e-05, -1.5467e-05,  5.9381e-06,  ..., -2.1309e-05,
          9.2201e-07, -1.2487e-05]], device='cuda:0')
Loss: 1.2179452180862427


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  221,  334,  ...,  706, 2680,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6642e-05,  2.5758e-05, -9.2170e-05,  ...,  5.3197e-05,
         -2.6803e-05, -5.3706e-05],
        [-1.7077e-05, -1.0192e-05,  3.9972e-06,  ..., -1.4082e-05,
          6.1840e-07, -8.3596e-06],
        [-1.1504e-05, -6.8769e-06,  2.6971e-06,  ..., -9.5069e-06,
          4.1677e-07, -5.6401e-06],
        [-2.0951e-05, -1.2502e-05,  4.9248e-06,  ..., -1.7315e-05,
          7.7020e-07, -1.0252e-05],
        [-3.4392e-05, -2.0504e-05,  8.0243e-06,  ..., -2.8342e-05,
          1.2331e-06, -1.6809e-05]], device='cuda:0')
Loss: 1.1880078315734863


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[    0,  2159,   271,  ...,  1268,   344,   259],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0097e-05,  3.5578e-05, -8.8080e-05,  ...,  7.0171e-06,
         -3.2921e-05, -5.9782e-05],
        [-2.1309e-05, -1.2711e-05,  4.9472e-06,  ..., -1.7583e-05,
          7.6368e-07, -1.0386e-05],
        [-1.4365e-05, -8.5831e-06,  3.3416e-06,  ..., -1.1876e-05,
          5.1409e-07, -7.0110e-06],
        [-2.6137e-05, -1.5572e-05,  6.0946e-06,  ..., -2.1607e-05,
          9.5181e-07, -1.2726e-05],
        [-4.2975e-05, -2.5600e-05,  9.9465e-06,  ..., -3.5435e-05,
          1.5274e-06, -2.0891e-05]], device='cuda:0')
Loss: 1.2085061073303223


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0257e-05,  8.8952e-05, -9.7298e-05,  ...,  1.0600e-05,
         -3.3757e-05, -7.2356e-05],
        [-2.5600e-05, -1.5259e-05,  5.9530e-06,  ..., -2.1055e-05,
          9.5181e-07, -1.2442e-05],
        [-1.7285e-05, -1.0319e-05,  4.0308e-06,  ..., -1.4246e-05,
          6.4261e-07, -8.4117e-06],
        [-3.1412e-05, -1.8701e-05,  7.3388e-06,  ..., -2.5868e-05,
          1.1846e-06, -1.5244e-05],
        [-5.1677e-05, -3.0756e-05,  1.1973e-05,  ..., -4.2468e-05,
          1.9036e-06, -2.5034e-05]], device='cuda:0')
Loss: 1.191427230834961


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0, 1979,  352,  ...,  292, 1591,  446],
        [   0,  944,  278,  ..., 2374,  699, 8867]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1618e-05,  1.3188e-04, -1.3897e-04,  ..., -9.4192e-05,
         -1.5753e-05, -8.7292e-05],
        [-2.9862e-05, -1.7807e-05,  6.9812e-06,  ..., -2.4647e-05,
          1.0435e-06, -1.4573e-05],
        [-2.0161e-05, -1.2040e-05,  4.7274e-06,  ..., -1.6659e-05,
          7.0455e-07, -9.8571e-06],
        [-3.6597e-05, -2.1800e-05,  8.5980e-06,  ..., -3.0249e-05,
          1.2992e-06, -1.7837e-05],
        [-6.0141e-05, -3.5822e-05,  1.4015e-05,  ..., -4.9621e-05,
          2.0806e-06, -2.9296e-05]], device='cuda:0')
Loss: 1.2179079055786133


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[    0,  6673,   298,  ...,  4391,   292,   221],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4840e-05,  8.8154e-05, -1.4764e-04,  ..., -1.3417e-04,
         -2.0621e-05, -1.0476e-04],
        [-3.4153e-05, -2.0415e-05,  8.0094e-06,  ..., -2.8163e-05,
          1.1516e-06, -1.6689e-05],
        [-2.3052e-05, -1.3791e-05,  5.4166e-06,  ..., -1.9014e-05,
          7.7672e-07, -1.1280e-05],
        [-4.1872e-05, -2.5004e-05,  9.8646e-06,  ..., -3.4571e-05,
          1.4342e-06, -2.0429e-05],
        [-6.8665e-05, -4.1008e-05,  1.6041e-05,  ..., -5.6595e-05,
          2.2920e-06, -3.3498e-05]], device='cuda:0')
Loss: 1.1913948059082031
Graident accumulation at epoch 0, step 135, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4517e-04, -1.2009e-04,  6.1614e-05,  ...,  1.0255e-04,
         -2.8750e-04, -6.5230e-05],
        [-2.6287e-05, -1.9448e-05,  5.1002e-06,  ..., -2.2487e-05,
         -2.0409e-06, -8.8951e-06],
        [-5.4106e-06,  4.4164e-07, -2.1772e-06,  ...,  9.0829e-06,
         -2.8424e-06,  5.5568e-06],
        [-2.2759e-05, -2.7974e-06,  7.3445e-06,  ..., -1.5866e-05,
         -1.2971e-06, -3.9986e-06],
        [-4.4950e-05, -3.3294e-05,  6.8103e-06,  ..., -3.6857e-05,
         -2.7555e-06, -1.4476e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8063e-08, 1.4779e-08, 1.7585e-08,  ..., 2.0119e-08, 3.1938e-08,
         6.5335e-09],
        [1.6380e-11, 1.2181e-11, 8.4144e-13,  ..., 1.1409e-11, 4.8665e-13,
         1.6284e-12],
        [4.9154e-11, 5.4793e-11, 6.5192e-12,  ..., 1.1148e-10, 3.0759e-12,
         3.1817e-11],
        [7.8163e-11, 2.1649e-10, 4.5309e-12,  ..., 6.3075e-11, 5.8710e-12,
         2.9850e-11],
        [5.1676e-11, 3.5633e-11, 4.2688e-12,  ..., 3.7907e-11, 1.2826e-12,
         5.8387e-12]], device='cuda:0')
optimizer state dict: 17.0
lr: [1.9482449598960544e-05, 1.9482449598960544e-05]
scheduler_last_epoch: 17


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[   0,  471,   14,  ..., 1260, 2129,  367],
        [   0,  278, 1099,  ...,  496,   14,  879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1226e-05, -4.3117e-05, -2.6697e-05,  ...,  3.3773e-05,
          4.8569e-05, -3.3507e-05],
        [-4.2915e-06, -2.6077e-06,  8.3447e-07,  ..., -3.3379e-06,
          1.0990e-07, -2.0713e-06],
        [-2.9504e-06, -1.7881e-06,  5.7369e-07,  ..., -2.2948e-06,
          7.4971e-08, -1.4156e-06],
        [-5.2452e-06, -3.1739e-06,  1.0207e-06,  ..., -4.0531e-06,
          1.3132e-07, -2.5034e-06],
        [-8.7619e-06, -5.3048e-06,  1.6987e-06,  ..., -6.8247e-06,
          2.1793e-07, -4.2021e-06]], device='cuda:0')
Loss: 1.189488172531128


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0,  9430,   287,  ...,  1141,  2280,   408],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6970e-05, -6.3467e-05, -4.6966e-05,  ...,  2.2123e-05,
          7.6233e-05, -6.5677e-05],
        [-8.5235e-06, -5.1856e-06,  1.6727e-06,  ..., -6.7055e-06,
          2.2771e-07, -4.1574e-06],
        [-5.8860e-06, -3.5688e-06,  1.1548e-06,  ..., -4.6194e-06,
          1.5600e-07, -2.8536e-06],
        [-1.0401e-05, -6.3181e-06,  2.0489e-06,  ..., -8.1658e-06,
          2.7660e-07, -5.0366e-06],
        [-1.7405e-05, -1.0550e-05,  3.4049e-06,  ..., -1.3709e-05,
          4.5635e-07, -8.4341e-06]], device='cuda:0')
Loss: 1.1903458833694458


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,    12,   630,  ...,  5049,    14,  2371],
        [    0,  2973, 20362,  ...,   271, 43821, 11776]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2248e-04, -6.7573e-05, -2.1190e-05,  ...,  1.5183e-05,
          8.1651e-05, -5.2274e-05],
        [-1.2815e-05, -7.7188e-06,  2.5369e-06,  ..., -1.0043e-05,
          2.6310e-07, -6.2287e-06],
        [ 1.8931e-04,  1.3580e-04, -5.9449e-05,  ...,  1.4759e-04,
          1.1498e-05,  1.0943e-04],
        [-1.5706e-05, -9.4324e-06,  3.1143e-06,  ..., -1.2279e-05,
          3.2433e-07, -7.5996e-06],
        [-2.6286e-05, -1.5765e-05,  5.1782e-06,  ..., -2.0593e-05,
          5.2620e-07, -1.2726e-05]], device='cuda:0')
Loss: 1.2029625177383423


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  3543,   391,  ...,  3370,  2926,  8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0382e-04, -8.7491e-05,  2.9970e-05,  ...,  1.5183e-05,
          4.3060e-05, -9.2181e-05],
        [-1.7136e-05, -1.0341e-05,  3.3602e-06,  ..., -1.3396e-05,
          3.4925e-07, -8.2999e-06],
        [ 1.8635e-04,  1.3401e-04, -5.8883e-05,  ...,  1.4529e-04,
          1.1558e-05,  1.0801e-04],
        [-2.0921e-05, -1.2591e-05,  4.1127e-06,  ..., -1.6332e-05,
          4.3190e-07, -1.0103e-05],
        [-3.5286e-05, -2.1189e-05,  6.8769e-06,  ..., -2.7537e-05,
          7.0222e-07, -1.7017e-05]], device='cuda:0')
Loss: 1.1924355030059814


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,    13, 26335,  ...,     5,  2570, 34403],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8561e-05, -6.5228e-05,  6.6618e-05,  ..., -1.1378e-05,
          5.4321e-05, -9.1489e-05],
        [-2.1368e-05, -1.2919e-05,  4.2133e-06,  ..., -1.6749e-05,
          3.9116e-07, -1.0371e-05],
        [ 1.8342e-04,  1.3223e-04, -5.8294e-05,  ...,  1.4298e-04,
          1.1586e-05,  1.0658e-04],
        [-2.6107e-05, -1.5751e-05,  5.1633e-06,  ..., -2.0444e-05,
          4.8382e-07, -1.2636e-05],
        [-4.3988e-05, -2.6524e-05,  8.6278e-06,  ..., -3.4451e-05,
          7.8138e-07, -2.1279e-05]], device='cuda:0')
Loss: 1.2121610641479492


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,  1529,  5227,  ...,  1480,   367,   925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2709e-04, -1.0859e-04,  3.2555e-05,  ..., -1.6936e-05,
          6.5155e-05, -4.8507e-05],
        [-2.5630e-05, -1.5512e-05,  5.0552e-06,  ..., -2.0102e-05,
          4.0873e-07, -1.2398e-05],
        [ 1.8050e-04,  1.3044e-04, -5.7717e-05,  ...,  1.4067e-04,
          1.1598e-05,  1.0518e-04],
        [-3.1263e-05, -1.8895e-05,  6.1840e-06,  ..., -2.4498e-05,
          5.0839e-07, -1.5095e-05],
        [-5.2750e-05, -3.1888e-05,  1.0364e-05,  ..., -4.1366e-05,
          8.1351e-07, -2.5451e-05]], device='cuda:0')
Loss: 1.2159769535064697


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,    14,  8058,  ..., 10316,   352,   266],
        [    0,   677,  9606,  ...,  9468,  9268,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0392e-05, -1.3470e-04, -4.2909e-05,  ...,  1.6662e-05,
          1.0275e-04, -5.6978e-05],
        [-2.9862e-05, -1.8045e-05,  5.9307e-06,  ..., -2.3365e-05,
          5.1537e-07, -1.4454e-05],
        [ 1.7758e-04,  1.2868e-04, -5.7110e-05,  ...,  1.3842e-04,
          1.1670e-05,  1.0376e-04],
        [-3.6478e-05, -2.2009e-05,  7.2718e-06,  ..., -2.8491e-05,
          6.3877e-07, -1.7613e-05],
        [-6.1452e-05, -3.7074e-05,  1.2152e-05,  ..., -4.8041e-05,
          1.0259e-06, -2.9624e-05]], device='cuda:0')
Loss: 1.1783533096313477


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[    0,  4108,    85,  ...,    40,    12,  1530],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2719e-05, -1.7918e-04, -5.9880e-05,  ...,  7.9301e-06,
          1.4484e-04, -5.7831e-05],
        [-3.4153e-05, -2.0653e-05,  6.7614e-06,  ..., -2.6762e-05,
          5.8429e-07, -1.6525e-05],
        [ 1.7462e-04,  1.2688e-04, -5.6536e-05,  ...,  1.3607e-04,
          1.1718e-05,  1.0233e-04],
        [-4.1693e-05, -2.5168e-05,  8.2850e-06,  ..., -3.2634e-05,
          7.2445e-07, -2.0117e-05],
        [-7.0333e-05, -4.2468e-05,  1.3858e-05,  ..., -5.5075e-05,
          1.1637e-06, -3.3915e-05]], device='cuda:0')
Loss: 1.204912543296814
Graident accumulation at epoch 0, step 143, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9138e-04, -1.2600e-04,  4.9465e-05,  ...,  9.3085e-05,
         -2.4427e-04, -6.4490e-05],
        [-2.7074e-05, -1.9568e-05,  5.2663e-06,  ..., -2.2915e-05,
         -1.7783e-06, -9.6582e-06],
        [ 1.2592e-05,  1.3085e-05, -7.6130e-06,  ...,  2.1781e-05,
         -1.3863e-06,  1.5234e-05],
        [-2.4652e-05, -5.0344e-06,  7.4386e-06,  ..., -1.7543e-05,
         -1.0949e-06, -5.6104e-06],
        [-4.7488e-05, -3.4212e-05,  7.5151e-06,  ..., -3.8679e-05,
         -2.3636e-06, -1.6420e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8024e-08, 1.4796e-08, 1.7571e-08,  ..., 2.0099e-08, 3.1927e-08,
         6.5303e-09],
        [1.7530e-11, 1.2595e-11, 8.8631e-13,  ..., 1.2114e-11, 4.8651e-13,
         1.8999e-12],
        [7.9596e-11, 7.0836e-11, 9.7090e-12,  ..., 1.2988e-10, 3.2101e-12,
         4.2257e-11],
        [7.9823e-11, 2.1691e-10, 4.5950e-12,  ..., 6.4077e-11, 5.8656e-12,
         3.0225e-11],
        [5.6571e-11, 3.7401e-11, 4.4565e-12,  ..., 4.0902e-11, 1.2826e-12,
         6.9831e-12]], device='cuda:0')
optimizer state dict: 18.0
lr: [1.9401053325731837e-05, 1.9401053325731837e-05]
scheduler_last_epoch: 18


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   452,   298,  ...,   287,  1575,  7856],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8715e-06, -5.1487e-05, -3.9611e-06,  ...,  1.1439e-05,
         -2.2709e-06,  5.0882e-05],
        [-4.1723e-06, -2.6822e-06,  6.7800e-07,  ..., -3.2187e-06,
         -5.3085e-08, -2.0564e-06],
        [-2.9504e-06, -1.8850e-06,  4.7684e-07,  ..., -2.2799e-06,
         -3.6787e-08, -1.4529e-06],
        [-5.1558e-06, -3.2932e-06,  8.3819e-07,  ..., -3.9637e-06,
         -6.4261e-08, -2.5332e-06],
        [-8.8215e-06, -5.6326e-06,  1.4231e-06,  ..., -6.7949e-06,
         -1.1036e-07, -4.3213e-06]], device='cuda:0')
Loss: 1.1922242641448975


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   685,   344,  ...,   680,   401,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4759e-05, -6.5686e-05,  3.4247e-05,  ..., -4.7377e-05,
         -4.8191e-05,  1.5968e-05],
        [-8.3745e-06, -5.3048e-06,  1.3411e-06,  ..., -6.4075e-06,
         -5.4242e-08, -4.1276e-06],
        [-5.9158e-06, -3.7327e-06,  9.4250e-07,  ..., -4.5300e-06,
         -3.8235e-08, -2.9132e-06],
        [-1.0312e-05, -6.4969e-06,  1.6503e-06,  ..., -7.8678e-06,
         -6.3308e-08, -5.0664e-06],
        [-1.7762e-05, -1.1176e-05,  2.8163e-06,  ..., -1.3530e-05,
         -1.1694e-07, -8.7023e-06]], device='cuda:0')
Loss: 1.2001198530197144


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  3001,  3325,  ..., 16332,  2661,  1200],
        [    0,    14,  6436,  ...,   271,  1211,  8917]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7931e-05, -6.6109e-05,  2.1075e-05,  ..., -4.9940e-05,
         -4.2342e-05,  3.4788e-05],
        [-1.2577e-05, -7.8976e-06,  1.9930e-06,  ..., -9.6112e-06,
         -6.3439e-08, -6.1244e-06],
        [-8.8811e-06, -5.5656e-06,  1.4026e-06,  ..., -6.7949e-06,
         -4.5802e-08, -4.3213e-06],
        [-1.5408e-05, -9.6411e-06,  2.4438e-06,  ..., -1.1742e-05,
         -7.3902e-08, -7.4804e-06],
        [-2.6643e-05, -1.6689e-05,  4.1947e-06,  ..., -2.0295e-05,
         -1.4092e-07, -1.2934e-05]], device='cuda:0')
Loss: 1.2117741107940674


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1964e-05, -5.7306e-05,  7.8584e-07,  ..., -4.9940e-05,
         -1.4909e-05,  7.4807e-05],
        [-1.6779e-05, -1.0535e-05,  2.6524e-06,  ..., -1.2860e-05,
         -6.0441e-08, -8.1360e-06],
        [-1.1832e-05, -7.4282e-06,  1.8682e-06,  ..., -9.0748e-06,
         -4.4609e-08, -5.7369e-06],
        [-2.0474e-05, -1.2830e-05,  3.2485e-06,  ..., -1.5676e-05,
         -6.9129e-08, -9.9093e-06],
        [-3.5465e-05, -2.2233e-05,  5.5805e-06,  ..., -2.7120e-05,
         -1.3891e-07, -1.7136e-05]], device='cuda:0')
Loss: 1.1990841627120972


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  266,  997,  ..., 2670,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9162e-05, -5.9756e-05, -8.9747e-05,  ..., -7.8008e-05,
         -5.5978e-06,  5.4858e-05],
        [-2.0921e-05, -1.3128e-05,  3.3230e-06,  ..., -1.6049e-05,
         -1.2796e-07, -1.0148e-05],
        [-1.4782e-05, -9.2760e-06,  2.3451e-06,  ..., -1.1340e-05,
         -9.0244e-08, -7.1600e-06],
        [-2.5600e-05, -1.6034e-05,  4.0792e-06,  ..., -1.9610e-05,
         -1.5015e-07, -1.2383e-05],
        [-4.4286e-05, -2.7746e-05,  6.9961e-06,  ..., -3.3885e-05,
         -2.8140e-07, -2.1398e-05]], device='cuda:0')
Loss: 1.1716240644454956


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0, 1412,   35,  ..., 6077,  298, 1826],
        [   0,  278, 5210,  ..., 1968, 2002,  923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6737e-05, -6.7411e-05, -9.5481e-05,  ..., -1.0460e-04,
          2.1769e-05,  6.4345e-05],
        [-2.5153e-05, -1.5751e-05,  3.9712e-06,  ..., -1.9282e-05,
         -1.6708e-07, -1.2174e-05],
        [-1.7777e-05, -1.1139e-05,  2.8033e-06,  ..., -1.3635e-05,
         -1.1748e-07, -8.5980e-06],
        [-3.0756e-05, -1.9237e-05,  4.8727e-06,  ..., -2.3544e-05,
         -1.9416e-07, -1.4856e-05],
        [-5.3227e-05, -3.3319e-05,  8.3596e-06,  ..., -4.0740e-05,
         -3.6057e-07, -2.5690e-05]], device='cuda:0')
Loss: 1.1908491849899292


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7527e-05, -1.0472e-04, -4.9197e-05,  ..., -1.2895e-04,
          5.5938e-05,  5.2932e-05],
        [-2.9355e-05, -1.8388e-05,  4.6529e-06,  ..., -2.2411e-05,
         -2.2109e-07, -1.4201e-05],
        [-2.0772e-05, -1.3016e-05,  3.2876e-06,  ..., -1.5855e-05,
         -1.5800e-07, -1.0043e-05],
        [-3.5882e-05, -2.2456e-05,  5.7071e-06,  ..., -2.7359e-05,
         -2.6121e-07, -1.7330e-05],
        [-6.2108e-05, -3.8862e-05,  9.7826e-06,  ..., -4.7326e-05,
         -4.8257e-07, -2.9951e-05]], device='cuda:0')
Loss: 1.1904783248901367


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8089e-05, -8.2811e-05, -2.0041e-05,  ..., -1.2096e-04,
          5.5938e-05,  3.5838e-05],
        [-3.3587e-05, -2.1026e-05,  5.2899e-06,  ..., -2.5615e-05,
         -3.0584e-07, -1.6212e-05],
        [-2.3782e-05, -1.4894e-05,  3.7421e-06,  ..., -1.8150e-05,
         -2.1690e-07, -1.1474e-05],
        [-4.1038e-05, -2.5675e-05,  6.4857e-06,  ..., -3.1263e-05,
         -3.6273e-07, -1.9774e-05],
        [-7.1228e-05, -4.4554e-05,  1.1146e-05,  ..., -5.4240e-05,
         -6.6604e-07, -3.4273e-05]], device='cuda:0')
Loss: 1.1980515718460083
Graident accumulation at epoch 0, step 151, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4444e-04, -1.2168e-04,  4.2514e-05,  ...,  7.1680e-05,
         -2.1425e-04, -5.4457e-05],
        [-2.7725e-05, -1.9714e-05,  5.2687e-06,  ..., -2.3185e-05,
         -1.6311e-06, -1.0314e-05],
        [ 8.9548e-06,  1.0287e-05, -6.4775e-06,  ...,  1.7788e-05,
         -1.2694e-06,  1.2564e-05],
        [-2.6291e-05, -7.0985e-06,  7.3433e-06,  ..., -1.8915e-05,
         -1.0217e-06, -7.0267e-06],
        [-4.9862e-05, -3.5246e-05,  7.8782e-06,  ..., -4.0235e-05,
         -2.1939e-06, -1.8205e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7982e-08, 1.4788e-08, 1.7554e-08,  ..., 2.0093e-08, 3.1899e-08,
         6.5250e-09],
        [1.8641e-11, 1.3025e-11, 9.1341e-13,  ..., 1.2758e-11, 4.8612e-13,
         2.1608e-12],
        [8.0082e-11, 7.0987e-11, 9.7133e-12,  ..., 1.3008e-10, 3.2069e-12,
         4.2346e-11],
        [8.1427e-11, 2.1735e-10, 4.6325e-12,  ..., 6.4990e-11, 5.8599e-12,
         3.0586e-11],
        [6.1588e-11, 3.9349e-11, 4.5763e-12,  ..., 4.3803e-11, 1.2818e-12,
         8.1507e-12]], device='cuda:0')
optimizer state dict: 19.0
lr: [1.9313911019977992e-05, 1.9313911019977992e-05]
scheduler_last_epoch: 19


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7031e-05,  1.9619e-06,  1.3439e-05,  ..., -5.2543e-05,
          1.8138e-05,  1.9473e-05],
        [-4.0829e-06, -2.6673e-06,  5.4389e-07,  ..., -3.0547e-06,
         -1.7323e-07, -1.9819e-06],
        [ 1.1383e-04,  7.6507e-05, -7.2773e-06,  ...,  5.9789e-05,
          9.7243e-06,  3.4094e-05],
        [-4.8876e-06, -3.1888e-06,  6.5565e-07,  ..., -3.6657e-06,
         -2.0675e-07, -2.3693e-06],
        [-8.7619e-06, -5.6922e-06,  1.1548e-06,  ..., -6.5565e-06,
         -3.7625e-07, -4.2319e-06]], device='cuda:0')
Loss: 1.1906957626342773


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[    0, 39224,    34,  ...,   401,  1716,   271],
        [    0,   741,  4933,  ...,   932,   365,   838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9650e-05,  9.5958e-06,  2.8189e-05,  ..., -6.6303e-05,
         -5.3159e-06,  3.5900e-05],
        [-8.1360e-06, -5.3197e-06,  1.0617e-06,  ..., -6.0946e-06,
         -3.0641e-07, -3.9041e-06],
        [ 1.1084e-04,  7.4555e-05, -6.8955e-06,  ...,  5.7539e-05,
          9.6252e-06,  3.2671e-05],
        [-9.8050e-06, -6.4075e-06,  1.2852e-06,  ..., -7.3463e-06,
         -3.6694e-07, -4.7088e-06],
        [-1.7762e-05, -1.1563e-05,  2.2948e-06,  ..., -1.3292e-05,
         -6.7428e-07, -8.4937e-06]], device='cuda:0')
Loss: 1.2006070613861084


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0, 28590,    12,  ...,   342, 29639,  1693],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5741e-05, -1.6253e-05,  1.9232e-05,  ..., -9.4438e-05,
         -4.0814e-05,  5.0065e-05],
        [-1.2219e-05, -8.0019e-06,  1.5683e-06,  ..., -9.1791e-06,
         -5.1875e-07, -5.8413e-06],
        [ 1.0787e-04,  7.2603e-05, -6.5248e-06,  ...,  5.5289e-05,
          9.4706e-06,  3.1263e-05],
        [-1.4722e-05, -9.6262e-06,  1.8999e-06,  ..., -1.1057e-05,
         -6.2026e-07, -7.0333e-06],
        [-2.6584e-05, -1.7375e-05,  3.3900e-06,  ..., -1.9968e-05,
         -1.1381e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.193820595741272


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0024e-05, -5.9432e-05, -1.6449e-05,  ..., -9.5309e-05,
         -1.6778e-05,  2.1060e-05],
        [-1.6302e-05, -1.0714e-05,  2.1048e-06,  ..., -1.2249e-05,
         -6.4727e-07, -7.7933e-06],
        [ 1.0488e-04,  7.0606e-05, -6.1299e-06,  ...,  5.3024e-05,
          9.3751e-06,  2.9817e-05],
        [-1.9699e-05, -1.2949e-05,  2.5630e-06,  ..., -1.4827e-05,
         -7.7859e-07, -9.4324e-06],
        [-3.5465e-05, -2.3246e-05,  4.5523e-06,  ..., -2.6643e-05,
         -1.4212e-06, -1.6928e-05]], device='cuda:0')
Loss: 1.195592999458313


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3770e-05, -7.6147e-05, -4.7960e-05,  ..., -1.0661e-04,
         -1.9028e-05,  2.3275e-05],
        [-2.0355e-05, -1.3381e-05,  2.6599e-06,  ..., -1.5274e-05,
         -8.6240e-07, -9.7156e-06],
        [ 1.0187e-04,  6.8639e-05, -5.7183e-06,  ...,  5.0789e-05,
          9.2168e-06,  2.8394e-05],
        [-2.4676e-05, -1.6227e-05,  3.2485e-06,  ..., -1.8537e-05,
         -1.0412e-06, -1.1787e-05],
        [-4.4405e-05, -2.9087e-05,  5.7667e-06,  ..., -3.3259e-05,
         -1.8943e-06, -2.1130e-05]], device='cuda:0')
Loss: 1.191038727760315


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  4103,   278,  4190],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9947e-06, -7.3477e-05, -6.6384e-05,  ..., -6.4554e-05,
          3.1762e-06,  2.0589e-05],
        [-2.4408e-05, -1.6004e-05,  3.2261e-06,  ..., -1.8299e-05,
         -1.0505e-06, -1.1653e-05],
        [ 9.8871e-05,  6.6702e-05, -5.2992e-06,  ...,  4.8554e-05,
          9.0771e-06,  2.6964e-05],
        [-2.9624e-05, -1.9431e-05,  3.9414e-06,  ..., -2.2233e-05,
         -1.2694e-06, -1.4156e-05],
        [-5.3287e-05, -3.4839e-05,  6.9961e-06,  ..., -3.9876e-05,
         -2.3060e-06, -2.5362e-05]], device='cuda:0')
Loss: 1.197033405303955


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,   669,   292,  ...,  4032,   271,  4442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8157e-06, -4.6268e-05, -1.2880e-04,  ..., -2.7855e-05,
          3.4116e-05, -7.9180e-06],
        [-2.8402e-05, -1.8656e-05,  3.7476e-06,  ..., -2.1353e-05,
         -1.1986e-06, -1.3575e-05],
        [ 9.5920e-05,  6.4750e-05, -4.9155e-06,  ...,  4.6304e-05,
          8.9672e-06,  2.5555e-05],
        [-3.4541e-05, -2.2680e-05,  4.5858e-06,  ..., -2.5988e-05,
         -1.4519e-06, -1.6510e-05],
        [-6.2048e-05, -4.0650e-05,  8.1286e-06,  ..., -4.6551e-05,
         -2.6319e-06, -2.9564e-05]], device='cuda:0')
Loss: 1.1944910287857056


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,  361, 1224,  ..., 4401, 4261, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9300e-05, -4.4533e-05, -5.2682e-05,  ..., -6.4846e-05,
          6.1279e-05, -5.7931e-06],
        [-3.2425e-05, -2.1324e-05,  4.2189e-06,  ..., -2.4393e-05,
         -1.3635e-06, -1.5467e-05],
        [ 9.2970e-05,  6.2798e-05, -4.5709e-06,  ...,  4.4083e-05,
          8.8480e-06,  2.4177e-05],
        [ 2.8516e-04,  1.7758e-04,  3.3801e-06,  ...,  2.3975e-04,
          2.3966e-05,  1.1826e-04],
        [-7.0930e-05, -4.6521e-05,  9.1717e-06,  ..., -5.3257e-05,
         -2.9970e-06, -3.3706e-05]], device='cuda:0')
Loss: 1.2124555110931396
Graident accumulation at epoch 0, step 159, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1192e-04, -1.1396e-04,  3.2995e-05,  ...,  5.8028e-05,
         -1.8670e-04, -4.9591e-05],
        [-2.8195e-05, -1.9875e-05,  5.1637e-06,  ..., -2.3306e-05,
         -1.6043e-06, -1.0829e-05],
        [ 1.7356e-05,  1.5538e-05, -6.2869e-06,  ...,  2.0418e-05,
         -2.5765e-07,  1.3725e-05],
        [ 4.8537e-06,  1.1370e-05,  6.9470e-06,  ...,  6.9514e-06,
          1.4771e-06,  5.5021e-06],
        [-5.1969e-05, -3.6373e-05,  8.0076e-06,  ..., -4.1537e-05,
         -2.2742e-06, -1.9755e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7934e-08, 1.4775e-08, 1.7539e-08,  ..., 2.0077e-08, 3.1870e-08,
         6.5185e-09],
        [1.9673e-11, 1.3467e-11, 9.3029e-13,  ..., 1.3340e-11, 4.8749e-13,
         2.3979e-12],
        [8.8645e-11, 7.4859e-11, 9.7245e-12,  ..., 1.3190e-10, 3.2820e-12,
         4.2888e-11],
        [1.6266e-10, 2.4867e-10, 4.6393e-12,  ..., 1.2240e-10, 6.4284e-12,
         4.4541e-11],
        [6.6557e-11, 4.1474e-11, 4.6559e-12,  ..., 4.6595e-11, 1.2895e-12,
         9.2787e-12]], device='cuda:0')
optimizer state dict: 20.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 20
Epoch 0 | Batch 159/1048 | Training PPL: 16311.800611336972 | time 12.030632734298706
Saving checkpoint at epoch 0, step 159, batch 159
Epoch 0 | Validation PPL: 10.850048809548472 | Learning rate: 1.9221075944084176e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_159, AFTER epoch 0, step 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[   0, 1487,  409,  ..., 6979, 1273,  496],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4791e-05, -1.4885e-05, -2.4363e-05,  ...,  3.7473e-05,
          3.4583e-05,  1.5396e-05],
        [-3.9935e-06, -2.7269e-06,  3.9674e-07,  ..., -2.9653e-06,
         -2.9244e-07, -1.8254e-06],
        [-3.0398e-06, -2.0862e-06,  3.0361e-07,  ..., -2.2650e-06,
         -2.2259e-07, -1.3858e-06],
        [-4.8578e-06, -3.3379e-06,  4.8801e-07,  ..., -3.6210e-06,
         -3.5390e-07, -2.2203e-06],
        [-8.9407e-06, -6.1095e-06,  8.8289e-07,  ..., -6.6161e-06,
         -6.5193e-07, -4.0531e-06]], device='cuda:0')
Loss: 1.198467493057251


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   298,   527,   496],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7464e-05, -1.5582e-05, -2.1929e-05,  ...,  8.1988e-06,
          4.1920e-05, -6.1099e-05],
        [-7.9572e-06, -5.4091e-06,  8.5123e-07,  ..., -5.9158e-06,
         -5.7556e-07, -3.7104e-06],
        [-6.0797e-06, -4.1425e-06,  6.5006e-07,  ..., -4.5300e-06,
         -4.3958e-07, -2.8312e-06],
        [-9.7454e-06, -6.6459e-06,  1.0468e-06,  ..., -7.2569e-06,
         -7.0035e-07, -4.5449e-06],
        [-1.7822e-05, -1.2100e-05,  1.8887e-06,  ..., -1.3202e-05,
         -1.2815e-06, -8.2552e-06]], device='cuda:0')
Loss: 1.1964521408081055


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4315e-04,  4.0942e-06, -1.0296e-04,  ..., -4.0207e-06,
          6.3366e-05, -9.0164e-05],
        [-1.1891e-05, -8.1211e-06,  1.2778e-06,  ..., -8.8662e-06,
         -8.6613e-07, -5.5581e-06],
        [-9.0748e-06, -6.1989e-06,  9.7416e-07,  ..., -6.7651e-06,
         -6.6031e-07, -4.2394e-06],
        [-1.4544e-05, -9.9391e-06,  1.5683e-06,  ..., -1.0833e-05,
         -1.0524e-06, -6.7949e-06],
        [-2.6584e-05, -1.8120e-05,  2.8349e-06,  ..., -1.9759e-05,
         -1.9297e-06, -1.2368e-05]], device='cuda:0')
Loss: 1.1993893384933472


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[   0, 4215, 1478,  ...,  644,  409, 3803],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1587e-04,  5.1298e-05, -1.3086e-04,  ...,  4.2953e-06,
          1.1603e-04, -1.0951e-04],
        [-1.5795e-05, -1.0803e-05,  1.6633e-06,  ..., -1.1817e-05,
         -1.0729e-06, -7.3910e-06],
        [-1.2070e-05, -8.2552e-06,  1.2703e-06,  ..., -9.0301e-06,
         -8.1863e-07, -5.6475e-06],
        [-1.9372e-05, -1.3262e-05,  2.0489e-06,  ..., -1.4484e-05,
         -1.3076e-06, -9.0748e-06],
        [-3.5465e-05, -2.4199e-05,  3.7067e-06,  ..., -2.6464e-05,
         -2.4028e-06, -1.6540e-05]], device='cuda:0')
Loss: 1.206943154335022


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,    14,  4494,  ...,  4830,   368,   266],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9180e-04,  6.3050e-05, -1.0894e-04,  ..., -7.4927e-06,
          1.7340e-04, -5.4249e-05],
        [-1.9819e-05, -1.3545e-05,  2.0657e-06,  ..., -1.4782e-05,
         -1.3877e-06, -9.2313e-06],
        [-1.5154e-05, -1.0356e-05,  1.5795e-06,  ..., -1.1310e-05,
         -1.0626e-06, -7.0557e-06],
        [-2.4229e-05, -1.6585e-05,  2.5406e-06,  ..., -1.8075e-05,
         -1.6894e-06, -1.1310e-05],
        [-4.4465e-05, -3.0339e-05,  4.6007e-06,  ..., -3.3110e-05,
         -3.1143e-06, -2.0653e-05]], device='cuda:0')
Loss: 1.2009730339050293


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   593,   300,  ...,   278,  4694,    12],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6252e-04, -1.5974e-06, -9.8920e-05,  ..., -3.2523e-06,
          1.5070e-04, -4.6652e-05],
        [-2.3752e-05, -1.6212e-05,  2.5127e-06,  ..., -1.7717e-05,
         -1.6559e-06, -1.1086e-05],
        [-1.8150e-05, -1.2383e-05,  1.9185e-06,  ..., -1.3545e-05,
         -1.2666e-06, -8.4639e-06],
        [-2.9117e-05, -1.9878e-05,  3.0957e-06,  ..., -2.1711e-05,
         -2.0210e-06, -1.3605e-05],
        [-5.3287e-05, -3.6269e-05,  5.5842e-06,  ..., -3.9667e-05,
         -3.7104e-06, -2.4796e-05]], device='cuda:0')
Loss: 1.1884984970092773


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,    15, 35654,     9],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1631e-04, -4.2706e-05, -9.9773e-05,  ..., -5.8464e-05,
          2.0779e-04, -2.8258e-05],
        [-2.7716e-05, -1.8939e-05,  2.9411e-06,  ..., -2.0638e-05,
         -1.9632e-06, -1.2949e-05],
        [-2.1145e-05, -1.4439e-05,  2.2426e-06,  ..., -1.5751e-05,
         -1.4985e-06, -9.8720e-06],
        [-3.3975e-05, -2.3216e-05,  3.6247e-06,  ..., -2.5287e-05,
         -2.3954e-06, -1.5885e-05],
        [-6.2108e-05, -4.2319e-05,  6.5342e-06,  ..., -4.6164e-05,
         -4.3921e-06, -2.8908e-05]], device='cuda:0')
Loss: 1.1804661750793457


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1529,  354,  ...,  709,  271,  266],
        [   0,  409,  699,  ...,   12,  546,  696]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0439e-04, -5.4602e-05, -1.0455e-04,  ..., -1.0856e-04,
          2.1367e-04, -1.7495e-05],
        [-3.1710e-05, -2.1607e-05,  3.3323e-06,  ..., -2.3559e-05,
         -2.1765e-06, -1.4804e-05],
        [-2.4185e-05, -1.6466e-05,  2.5406e-06,  ..., -1.7986e-05,
         -1.6615e-06, -1.1288e-05],
        [-3.8832e-05, -2.6464e-05,  4.1053e-06,  ..., -2.8849e-05,
         -2.6561e-06, -1.8150e-05],
        [-7.1049e-05, -4.8280e-05,  7.4059e-06,  ..., -5.2720e-05,
         -4.8727e-06, -3.3081e-05]], device='cuda:0')
Loss: 1.2027634382247925
Graident accumulation at epoch 0, step 167, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0117e-04, -1.0803e-04,  1.9240e-05,  ...,  4.1369e-05,
         -1.4666e-04, -4.6381e-05],
        [-2.8547e-05, -2.0048e-05,  4.9806e-06,  ..., -2.3331e-05,
         -1.6615e-06, -1.1227e-05],
        [ 1.3202e-05,  1.2338e-05, -5.4041e-06,  ...,  1.6577e-05,
         -3.9803e-07,  1.1224e-05],
        [ 4.8512e-07,  7.5863e-06,  6.6628e-06,  ...,  3.3714e-06,
          1.0637e-06,  3.1370e-06],
        [-5.3877e-05, -3.7564e-05,  7.9474e-06,  ..., -4.2655e-05,
         -2.5340e-06, -2.1088e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7928e-08, 1.4763e-08, 1.7532e-08,  ..., 2.0069e-08, 3.1884e-08,
         6.5123e-09],
        [2.0659e-11, 1.3920e-11, 9.4047e-13,  ..., 1.3882e-11, 4.9174e-13,
         2.6147e-12],
        [8.9142e-11, 7.5056e-11, 9.7212e-12,  ..., 1.3209e-10, 3.2815e-12,
         4.2973e-11],
        [1.6401e-10, 2.4912e-10, 4.6515e-12,  ..., 1.2311e-10, 6.4290e-12,
         4.4826e-11],
        [7.1538e-11, 4.3763e-11, 4.7060e-12,  ..., 4.9328e-11, 1.3120e-12,
         1.0364e-11]], device='cuda:0')
optimizer state dict: 21.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 21


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,  3860,   694,  ...,  1027,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0254e-06,  8.6121e-06,  2.6851e-06,  ...,  2.9462e-05,
          4.0971e-05, -4.3931e-05],
        [-3.8445e-06, -2.6822e-06,  3.2783e-07,  ..., -2.8610e-06,
         -3.9302e-07, -1.6987e-06],
        [-3.0100e-06, -2.1160e-06,  2.5705e-07,  ..., -2.2501e-06,
         -3.0920e-07, -1.3411e-06],
        [-4.7684e-06, -3.3379e-06,  4.1351e-07,  ..., -3.5763e-06,
         -4.8801e-07, -2.1160e-06],
        [-8.8215e-06, -6.1691e-06,  7.4878e-07,  ..., -6.5863e-06,
         -9.0525e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.1868326663970947


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 6132,  300,  ...,   37,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5626e-06,  2.7106e-05, -6.1008e-07,  ...,  2.8337e-05,
          3.2325e-05, -4.7025e-05],
        [-7.7188e-06, -5.4091e-06,  6.3330e-07,  ..., -5.7220e-06,
         -7.9721e-07, -3.4049e-06],
        [-6.1095e-06, -4.2915e-06,  5.0105e-07,  ..., -4.5449e-06,
         -6.3144e-07, -2.7046e-06],
        [-9.5665e-06, -6.7055e-06,  7.9535e-07,  ..., -7.1079e-06,
         -9.8348e-07, -4.2170e-06],
        [-1.7762e-05, -1.2428e-05,  1.4417e-06,  ..., -1.3173e-05,
         -1.8328e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.1874667406082153


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0,    26,   874,  ...,    12, 21591,    12],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6206e-05,  8.3870e-06,  7.1614e-06,  ...,  1.8497e-05,
          4.6144e-05, -1.7047e-05],
        [-1.1593e-05, -8.0913e-06,  9.2946e-07,  ..., -8.5682e-06,
         -1.1642e-06, -5.1036e-06],
        [-9.1493e-06, -6.4075e-06,  7.3481e-07,  ..., -6.7800e-06,
         -9.2015e-07, -4.0382e-06],
        [-1.4365e-05, -1.0043e-05,  1.1660e-06,  ..., -1.0625e-05,
         -1.4361e-06, -6.3181e-06],
        [-2.6643e-05, -1.8597e-05,  2.1197e-06,  ..., -1.9699e-05,
         -2.6785e-06, -1.1712e-05]], device='cuda:0')
Loss: 1.2072488069534302


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2870e-05,  2.9191e-05, -1.3887e-05,  ...,  1.8497e-05,
          4.8343e-05, -6.2025e-05],
        [-1.5408e-05, -1.0818e-05,  1.2256e-06,  ..., -1.1399e-05,
         -1.5441e-06, -6.7726e-06],
        [-1.2174e-05, -8.5682e-06,  9.7137e-07,  ..., -9.0301e-06,
         -1.2238e-06, -5.3570e-06],
        [-1.9133e-05, -1.3456e-05,  1.5404e-06,  ..., -1.4156e-05,
         -1.9111e-06, -8.3894e-06],
        [-3.5465e-05, -2.4885e-05,  2.7977e-06,  ..., -2.6226e-05,
         -3.5577e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.178568959236145


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[   0,  287, 1070,  ...,  292,  221,  374],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4291e-05,  4.8527e-05, -3.3942e-05,  ...,  3.1267e-05,
          2.5727e-05, -4.9248e-05],
        [-1.9282e-05, -1.3471e-05,  1.5441e-06,  ..., -1.4246e-05,
         -1.9334e-06, -8.4713e-06],
        [-1.5184e-05, -1.0639e-05,  1.2210e-06,  ..., -1.1265e-05,
         -1.5292e-06, -6.6832e-06],
        [-2.3901e-05, -1.6749e-05,  1.9390e-06,  ..., -1.7688e-05,
         -2.3916e-06, -1.0490e-05],
        [-4.4286e-05, -3.0965e-05,  3.5167e-06,  ..., -3.2753e-05,
         -4.4480e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1895503997802734


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0, 4100,   12,  ...,   13, 4710, 1558],
        [   0,  278,  266,  ...,   13, 2853,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2974e-05,  3.5321e-05, -7.0073e-05,  ...,  3.1783e-05,
          5.1399e-05, -9.3465e-05],
        [-2.3127e-05, -1.6153e-05,  1.8738e-06,  ..., -1.7107e-05,
         -2.2799e-06, -1.0177e-05],
        [-1.8194e-05, -1.2740e-05,  1.4799e-06,  ..., -1.3500e-05,
         -1.8012e-06, -8.0243e-06],
        [-2.8700e-05, -2.0102e-05,  2.3544e-06,  ..., -2.1264e-05,
         -2.8238e-06, -1.2621e-05],
        [-5.3048e-05, -3.7074e-05,  4.2617e-06,  ..., -3.9250e-05,
         -5.2378e-06, -2.3305e-05]], device='cuda:0')
Loss: 1.2024768590927124


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   344,  8133,  ...,   368,  1119,  5539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6451e-05,  5.4338e-05, -8.0456e-05,  ...,  4.1369e-05,
          7.5704e-05, -8.9045e-05],
        [-2.6971e-05, -1.8865e-05,  2.1514e-06,  ..., -1.9953e-05,
         -2.6505e-06, -1.1861e-05],
        [-2.1249e-05, -1.4901e-05,  1.6997e-06,  ..., -1.5765e-05,
         -2.0955e-06, -9.3654e-06],
        [-3.3468e-05, -2.3469e-05,  2.7008e-06,  ..., -2.4796e-05,
         -3.2801e-06, -1.4707e-05],
        [-6.1929e-05, -4.3362e-05,  4.8988e-06,  ..., -4.5866e-05,
         -6.0946e-06, -2.7210e-05]], device='cuda:0')
Loss: 1.1963508129119873


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0,   342,  4781,  ...,   630,   940,   271],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2460e-05,  9.1193e-05, -1.2124e-04,  ...,  8.4629e-05,
          1.1605e-04, -9.9893e-05],
        [-3.0875e-05, -2.1577e-05,  2.4568e-06,  ..., -2.2814e-05,
         -3.0212e-06, -1.3545e-05],
        [-2.4319e-05, -1.7032e-05,  1.9399e-06,  ..., -1.8016e-05,
         -2.3860e-06, -1.0684e-05],
        [-3.8296e-05, -2.6807e-05,  3.0808e-06,  ..., -2.8327e-05,
         -3.7346e-06, -1.6779e-05],
        [-7.0870e-05, -4.9561e-05,  5.5917e-06,  ..., -5.2422e-05,
         -6.9402e-06, -3.1054e-05]], device='cuda:0')
Loss: 1.192864179611206
Graident accumulation at epoch 0, step 175, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8030e-04, -8.8105e-05,  5.1918e-06,  ...,  4.5695e-05,
         -1.2039e-04, -5.1733e-05],
        [-2.8779e-05, -2.0201e-05,  4.7282e-06,  ..., -2.3279e-05,
         -1.7975e-06, -1.1458e-05],
        [ 9.4501e-06,  9.4009e-06, -4.6697e-06,  ...,  1.3118e-05,
         -5.9683e-07,  9.0329e-06],
        [-3.3930e-06,  4.1470e-06,  6.3046e-06,  ...,  2.0155e-07,
          5.8390e-07,  1.1454e-06],
        [-5.5576e-05, -3.8764e-05,  7.7118e-06,  ..., -4.3632e-05,
         -2.9746e-06, -2.2084e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7889e-08, 1.4757e-08, 1.7529e-08,  ..., 2.0056e-08, 3.1866e-08,
         6.5158e-09],
        [2.1592e-11, 1.4372e-11, 9.4556e-13,  ..., 1.4388e-11, 5.0037e-13,
         2.7955e-12],
        [8.9644e-11, 7.5271e-11, 9.7153e-12,  ..., 1.3228e-10, 3.2839e-12,
         4.3044e-11],
        [1.6531e-10, 2.4959e-10, 4.6564e-12,  ..., 1.2379e-10, 6.4366e-12,
         4.5063e-11],
        [7.6489e-11, 4.6176e-11, 4.7326e-12,  ..., 5.2027e-11, 1.3588e-12,
         1.1318e-11]], device='cuda:0')
optimizer state dict: 22.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 22


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[   0,  275, 2351,  ...,   14, 4520,   12],
        [   0,  292,  685,  ...,  278, 3281,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8906e-06,  6.1820e-05, -1.5528e-05,  ...,  4.0015e-05,
          5.0260e-06,  6.3885e-06],
        [-3.8147e-06, -2.6822e-06,  2.7567e-07,  ..., -2.8014e-06,
         -4.9174e-07, -1.5721e-06],
        [-3.0696e-06, -2.1607e-06,  2.2352e-07,  ..., -2.2501e-06,
         -3.9488e-07, -1.2666e-06],
        [-4.8578e-06, -3.4273e-06,  3.5390e-07,  ..., -3.5614e-06,
         -6.2212e-07, -1.9968e-06],
        [-8.9407e-06, -6.2883e-06,  6.4075e-07,  ..., -6.5565e-06,
         -1.1474e-06, -3.6806e-06]], device='cuda:0')
Loss: 1.185326337814331


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[   0,  638, 1276,  ..., 1589, 2432,  292],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1501e-05,  6.5380e-05,  5.8137e-06,  ...,  6.7892e-05,
         -3.5623e-07,  2.3930e-05],
        [-7.5698e-06, -5.3793e-06,  5.4017e-07,  ..., -5.6028e-06,
         -9.4622e-07, -3.1590e-06],
        [-6.0946e-06, -4.3213e-06,  4.3586e-07,  ..., -4.5002e-06,
         -7.5996e-07, -2.5406e-06],
        [-9.5963e-06, -6.8247e-06,  6.9104e-07,  ..., -7.0930e-06,
         -1.1921e-06, -3.9935e-06],
        [-1.7703e-05, -1.2547e-05,  1.2517e-06,  ..., -1.3053e-05,
         -2.2054e-06, -7.3612e-06]], device='cuda:0')
Loss: 1.196162223815918


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,   598,   696,  ...,  4048,  1795,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6302e-06,  2.8173e-05,  1.1984e-05,  ...,  1.1120e-04,
          4.7684e-06,  2.3656e-05],
        [-1.1340e-05, -8.0466e-06,  8.4378e-07,  ..., -8.3447e-06,
         -1.4566e-06, -4.7609e-06],
        [-9.1344e-06, -6.4671e-06,  6.7987e-07,  ..., -6.7204e-06,
         -1.1735e-06, -3.8370e-06],
        [-1.4395e-05, -1.0207e-05,  1.0785e-06,  ..., -1.0580e-05,
         -1.8440e-06, -6.0350e-06],
        [-2.6464e-05, -1.8746e-05,  1.9521e-06,  ..., -1.9461e-05,
         -3.3975e-06, -1.1101e-05]], device='cuda:0')
Loss: 1.1771272420883179


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9973e-06,  2.3428e-05,  4.4472e-05,  ...,  1.1456e-04,
          1.1517e-06,  2.3656e-05],
        [-1.5154e-05, -1.0759e-05,  1.1325e-06,  ..., -1.1146e-05,
         -1.9856e-06, -6.3404e-06],
        [-1.2204e-05, -8.6427e-06,  9.1176e-07,  ..., -8.9705e-06,
         -1.5963e-06, -5.1036e-06],
        [-1.9163e-05, -1.3590e-05,  1.4435e-06,  ..., -1.4082e-05,
         -2.5034e-06, -8.0168e-06],
        [-3.5405e-05, -2.5094e-05,  2.6263e-06,  ..., -2.6017e-05,
         -4.6417e-06, -1.4797e-05]], device='cuda:0')
Loss: 1.1710186004638672


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  607,  443,  ...,  259, 2646, 1597]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5888e-05,  5.7092e-05,  4.4717e-05,  ...,  1.4040e-04,
         -2.2960e-05,  5.8514e-05],
        [-1.8880e-05, -1.3426e-05,  1.4175e-06,  ..., -1.3933e-05,
         -2.4661e-06, -7.9051e-06],
        [-1.5214e-05, -1.0803e-05,  1.1437e-06,  ..., -1.1221e-05,
         -1.9837e-06, -6.3702e-06],
        [-2.3872e-05, -1.6958e-05,  1.8068e-06,  ..., -1.7613e-05,
         -3.1069e-06, -9.9987e-06],
        [-4.4167e-05, -3.1352e-05,  3.2894e-06,  ..., -3.2544e-05,
         -5.7667e-06, -1.8463e-05]], device='cuda:0')
Loss: 1.2035144567489624


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 46357, 11101, 10621],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4870e-05,  4.3152e-05,  8.8942e-05,  ...,  1.6923e-04,
         -6.6740e-05,  3.9535e-05],
        [-2.2590e-05, -1.6049e-05,  1.6801e-06,  ..., -1.6674e-05,
         -2.9467e-06, -9.4622e-06],
        [-1.8224e-05, -1.2934e-05,  1.3579e-06,  ..., -1.3441e-05,
         -2.3730e-06, -7.6294e-06],
        [-2.8551e-05, -2.0280e-05,  2.1458e-06,  ..., -2.1085e-05,
         -3.7141e-06, -1.1966e-05],
        [-5.2989e-05, -3.7581e-05,  3.9116e-06,  ..., -3.9041e-05,
         -6.9067e-06, -2.2143e-05]], device='cuda:0')
Loss: 1.1805311441421509


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0842e-04,  3.6956e-05,  7.2812e-05,  ...,  1.7661e-04,
         -7.1566e-05,  7.8525e-05],
        [-2.6360e-05, -1.8686e-05,  1.9651e-06,  ..., -1.9461e-05,
         -3.3937e-06, -1.1057e-05],
        [-2.1264e-05, -1.5065e-05,  1.5870e-06,  ..., -1.5676e-05,
         -2.7306e-06, -8.9109e-06],
        [-3.3319e-05, -2.3618e-05,  2.5090e-06,  ..., -2.4602e-05,
         -4.2766e-06, -1.3977e-05],
        [-6.1870e-05, -4.3780e-05,  4.5784e-06,  ..., -4.5598e-05,
         -7.9572e-06, -2.5883e-05]], device='cuda:0')
Loss: 1.1953017711639404


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3975e-04, -2.0959e-05,  1.0005e-04,  ...,  2.0191e-04,
         -1.2046e-04,  9.5911e-05],
        [-3.0115e-05, -2.1353e-05,  2.2370e-06,  ..., -2.2233e-05,
         -3.8855e-06, -1.2644e-05],
        [-2.4289e-05, -1.7211e-05,  1.8058e-06,  ..., -1.7911e-05,
         -3.1255e-06, -1.0185e-05],
        [-3.8087e-05, -2.7001e-05,  2.8536e-06,  ..., -2.8118e-05,
         -4.8950e-06, -1.5989e-05],
        [-7.0691e-05, -5.0038e-05,  5.2080e-06,  ..., -5.2094e-05,
         -9.1121e-06, -2.9609e-05]], device='cuda:0')
Loss: 1.1927088499069214
Graident accumulation at epoch 0, step 183, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3829e-04, -8.1391e-05,  1.4677e-05,  ...,  6.1316e-05,
         -1.2039e-04, -3.6968e-05],
        [-2.8913e-05, -2.0316e-05,  4.4791e-06,  ..., -2.3175e-05,
         -2.0063e-06, -1.1577e-05],
        [ 6.0762e-06,  6.7398e-06, -4.0222e-06,  ...,  1.0015e-05,
         -8.4970e-07,  7.1112e-06],
        [-6.8624e-06,  1.0322e-06,  5.9595e-06,  ..., -2.6305e-06,
          3.6009e-08, -5.6803e-07],
        [-5.7088e-05, -3.9891e-05,  7.4614e-06,  ..., -4.4478e-05,
         -3.5884e-06, -2.2837e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7860e-08, 1.4743e-08, 1.7522e-08,  ..., 2.0077e-08, 3.1848e-08,
         6.5185e-09],
        [2.2477e-11, 1.4813e-11, 9.4962e-13,  ..., 1.4868e-11, 5.1497e-13,
         2.9526e-12],
        [9.0144e-11, 7.5492e-11, 9.7088e-12,  ..., 1.3247e-10, 3.2904e-12,
         4.3105e-11],
        [1.6659e-10, 2.5007e-10, 4.6599e-12,  ..., 1.2446e-10, 6.4541e-12,
         4.5273e-11],
        [8.1410e-11, 4.8633e-11, 4.7550e-12,  ..., 5.4689e-11, 1.4405e-12,
         1.2183e-11]], device='cuda:0')
optimizer state dict: 23.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 23


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5897e-06,  2.0432e-05, -4.4808e-05,  ...,  8.5494e-06,
          1.1981e-05,  6.4582e-05],
        [-3.6955e-06, -2.6524e-06,  2.3376e-07,  ..., -2.7567e-06,
         -4.8429e-07, -1.4752e-06],
        [-3.0398e-06, -2.1905e-06,  1.9185e-07,  ..., -2.2799e-06,
         -4.0047e-07, -1.2144e-06],
        [-4.7684e-06, -3.4273e-06,  3.0175e-07,  ..., -3.5763e-06,
         -6.2585e-07, -1.8999e-06],
        [-8.6427e-06, -6.2287e-06,  5.4389e-07,  ..., -6.4671e-06,
         -1.1399e-06, -3.4571e-06]], device='cuda:0')
Loss: 1.1961733102798462


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  285,  590,  ...,  199,  395, 3523],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8127e-05,  4.6731e-05, -6.1589e-05,  ...,  2.5359e-05,
          6.7051e-05,  8.5494e-05],
        [-7.2867e-06, -5.2601e-06,  4.6380e-07,  ..., -5.5134e-06,
         -1.0543e-06, -2.9728e-06],
        [-6.0499e-06, -4.3660e-06,  3.8370e-07,  ..., -4.5747e-06,
         -8.7544e-07, -2.4661e-06],
        [-9.4175e-06, -6.7800e-06,  6.0350e-07,  ..., -7.1228e-06,
         -1.3560e-06, -3.8221e-06],
        [-1.7166e-05, -1.2428e-05,  1.0878e-06,  ..., -1.3024e-05,
         -2.4959e-06, -7.0035e-06]], device='cuda:0')
Loss: 1.1688002347946167


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0,    12, 30621,  ...,   578,  3126,    14],
        [    0,    13,  5005,  ...,   654,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0650e-05,  5.4490e-05, -4.5134e-05,  ...,  7.5088e-06,
          4.4478e-05,  6.7829e-05],
        [-1.0893e-05, -7.8678e-06,  7.3761e-07,  ..., -8.1807e-06,
         -1.5832e-06, -4.4703e-06],
        [ 8.1688e-05,  4.1083e-05, -1.9835e-05,  ...,  7.1015e-05,
          1.2371e-06,  2.9261e-05],
        [-1.4156e-05, -1.0222e-05,  9.7044e-07,  ..., -1.0625e-05,
         -2.0489e-06, -5.7891e-06],
        [-2.5809e-05, -1.8716e-05,  1.7434e-06,  ..., -1.9431e-05,
         -3.7700e-06, -1.0595e-05]], device='cuda:0')
Loss: 1.1861881017684937


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[    0,  6762,   689,  ...,  7061,    14,   381],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3102e-05,  5.4881e-05, -2.5451e-05,  ...,  2.0332e-05,
          1.6543e-05,  6.8258e-05],
        [-1.4499e-05, -1.0505e-05,  9.7603e-07,  ..., -1.0937e-05,
         -2.1085e-06, -5.9754e-06],
        [ 7.8708e-05,  3.8907e-05, -1.9638e-05,  ...,  6.8735e-05,
          7.9937e-07,  2.8017e-05],
        [-1.8835e-05, -1.3649e-05,  1.2834e-06,  ..., -1.4201e-05,
         -2.7344e-06, -7.7412e-06],
        [-3.4392e-05, -2.4945e-05,  2.3022e-06,  ..., -2.5958e-05,
         -5.0217e-06, -1.4156e-05]], device='cuda:0')
Loss: 1.1804877519607544


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0, 13642, 14635,  ...,   367,  1040,  8580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5161e-06,  3.0876e-05,  1.2735e-05,  ...,  2.7476e-05,
         -1.7839e-05,  6.9329e-05],
        [-1.8165e-05, -1.3173e-05,  1.2219e-06,  ..., -1.3679e-05,
         -2.6040e-06, -7.4580e-06],
        [ 7.5668e-05,  3.6687e-05, -1.9434e-05,  ...,  6.6456e-05,
          3.8773e-07,  2.6780e-05],
        [-2.3544e-05, -1.7092e-05,  1.6037e-06,  ..., -1.7717e-05,
         -3.3714e-06, -9.6485e-06],
        [-4.3154e-05, -3.1322e-05,  2.8871e-06,  ..., -3.2514e-05,
         -6.2138e-06, -1.7703e-05]], device='cuda:0')
Loss: 1.1847681999206543


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8981e-06,  4.4135e-05,  3.2822e-06,  ...,  3.2232e-05,
          1.3334e-05,  7.8131e-05],
        [-2.1785e-05, -1.5795e-05,  1.5050e-06,  ..., -1.6406e-05,
         -3.1814e-06, -8.9779e-06],
        [ 7.2628e-05,  3.4482e-05, -1.9193e-05,  ...,  6.4161e-05,
         -1.0028e-07,  2.5506e-05],
        [-2.8253e-05, -2.0489e-05,  1.9763e-06,  ..., -2.1264e-05,
         -4.1202e-06, -1.1615e-05],
        [-5.1856e-05, -3.7611e-05,  3.5651e-06,  ..., -3.9071e-05,
         -7.6070e-06, -2.1338e-05]], device='cuda:0')
Loss: 1.192232608795166


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[    0, 21178,  1952,  ..., 14930,     9,   689],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4440e-05,  3.0932e-05,  2.3387e-06,  ...,  1.8868e-05,
         -4.3210e-05,  9.1885e-05],
        [-2.5392e-05, -1.8418e-05,  1.7714e-06,  ..., -1.9133e-05,
         -3.6769e-06, -1.0461e-05],
        [ 6.9603e-05,  3.2276e-05, -1.8967e-05,  ...,  6.1866e-05,
         -5.1938e-07,  2.4254e-05],
        [-3.2932e-05, -2.3887e-05,  2.3246e-06,  ..., -2.4796e-05,
         -4.7646e-06, -1.3538e-05],
        [-6.0439e-05, -4.3869e-05,  4.1984e-06,  ..., -4.5568e-05,
         -8.7991e-06, -2.4870e-05]], device='cuda:0')
Loss: 1.1841775178909302


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,     9,   391,  ...,   300,  2646,  1717],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7947e-05,  2.8131e-05, -3.0413e-05,  ...,  3.0388e-05,
         -4.2604e-05,  6.5186e-05],
        [-2.9057e-05, -2.1040e-05,  2.0191e-06,  ..., -2.1890e-05,
         -4.2096e-06, -1.1943e-05],
        [ 6.6549e-05,  3.0086e-05, -1.8760e-05,  ...,  5.9571e-05,
         -9.6269e-07,  2.3017e-05],
        [-3.7670e-05, -2.7269e-05,  2.6468e-06,  ..., -2.8357e-05,
         -5.4501e-06, -1.5460e-05],
        [-6.9141e-05, -5.0098e-05,  4.7795e-06,  ..., -5.2124e-05,
         -1.0066e-05, -2.8372e-05]], device='cuda:0')
Loss: 1.1895171403884888
Graident accumulation at epoch 0, step 191, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1167e-04, -7.0438e-05,  1.0168e-05,  ...,  5.8223e-05,
         -1.1262e-04, -2.6753e-05],
        [-2.8927e-05, -2.0389e-05,  4.2331e-06,  ..., -2.3046e-05,
         -2.2266e-06, -1.1614e-05],
        [ 1.2123e-05,  9.0744e-06, -5.4960e-06,  ...,  1.4971e-05,
         -8.6100e-07,  8.7018e-06],
        [-9.9432e-06, -1.7980e-06,  5.6282e-06,  ..., -5.2031e-06,
         -5.1260e-07, -2.0572e-06],
        [-5.8293e-05, -4.0912e-05,  7.1932e-06,  ..., -4.5243e-05,
         -4.2361e-06, -2.3390e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7813e-08, 1.4729e-08, 1.7505e-08,  ..., 2.0058e-08, 3.1818e-08,
         6.5162e-09],
        [2.3299e-11, 1.5241e-11, 9.5275e-13,  ..., 1.5332e-11, 5.3218e-13,
         3.0923e-12],
        [9.4483e-11, 7.6321e-11, 1.0051e-11,  ..., 1.3589e-10, 3.2880e-12,
         4.3592e-11],
        [1.6785e-10, 2.5056e-10, 4.6622e-12,  ..., 1.2514e-10, 6.4773e-12,
         4.5467e-11],
        [8.6109e-11, 5.1095e-11, 4.7731e-12,  ..., 5.7351e-11, 1.5404e-12,
         1.2976e-11]], device='cuda:0')
optimizer state dict: 24.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 24


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  6541,   287,  ...,  1061,  4786,   292],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9612e-05, -2.1825e-05, -1.3687e-05,  ..., -1.9814e-05,
         -7.9619e-06, -7.1455e-06],
        [-3.5018e-06, -2.5779e-06,  2.4214e-07,  ..., -2.6822e-06,
         -5.6252e-07, -1.4007e-06],
        [-3.0249e-06, -2.2352e-06,  2.0955e-07,  ..., -2.3246e-06,
         -4.8801e-07, -1.2144e-06],
        [-4.6790e-06, -3.4422e-06,  3.2596e-07,  ..., -3.5763e-06,
         -7.4878e-07, -1.8701e-06],
        [-8.4043e-06, -6.1989e-06,  5.7742e-07,  ..., -6.4671e-06,
         -1.3560e-06, -3.3528e-06]], device='cuda:0')
Loss: 1.1734110116958618


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[    0,   199,   769,  ..., 12038, 15317,   342],
        [    0,  1832,   292,  ...,  2176,  1345,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6031e-05, -2.7517e-05, -3.3611e-05,  ..., -8.0495e-05,
          1.5058e-05, -3.0440e-05],
        [-7.0333e-06, -5.1260e-06,  4.3493e-07,  ..., -5.4538e-06,
         -1.0617e-06, -2.7940e-06],
        [-6.0350e-06, -4.4107e-06,  3.7439e-07,  ..., -4.6939e-06,
         -9.1456e-07, -2.4065e-06],
        [-9.3877e-06, -6.8396e-06,  5.8673e-07,  ..., -7.2718e-06,
         -1.4156e-06, -3.7327e-06],
        [-1.6868e-05, -1.2308e-05,  1.0356e-06,  ..., -1.3113e-05,
         -2.5630e-06, -6.7055e-06]], device='cuda:0')
Loss: 1.2009117603302002


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[    0, 14949,    12,  ...,   669, 10168,  7166],
        [    0,   401,   266,  ...,   266,  2236,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2968e-05, -8.7797e-05, -4.3691e-05,  ..., -3.0296e-05,
          1.4921e-05,  1.9802e-05],
        [-1.0565e-05, -7.7337e-06,  6.6031e-07,  ..., -8.1360e-06,
         -1.6093e-06, -4.1574e-06],
        [-9.0897e-06, -6.6757e-06,  5.6904e-07,  ..., -7.0184e-06,
         -1.3895e-06, -3.5837e-06],
        [-1.4067e-05, -1.0282e-05,  8.8662e-07,  ..., -1.0833e-05,
         -2.1383e-06, -5.5358e-06],
        [-2.5392e-05, -1.8597e-05,  1.5758e-06,  ..., -1.9610e-05,
         -3.8818e-06, -9.9838e-06]], device='cuda:0')
Loss: 1.1856335401535034


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,   546,   360,  ...,  9107,  2772,  4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1596e-05, -1.0826e-04, -2.1091e-05,  ..., -3.6093e-05,
          4.0086e-05,  4.4239e-05],
        [-1.4096e-05, -1.0312e-05,  8.7824e-07,  ..., -1.0908e-05,
         -2.1420e-06, -5.5805e-06],
        [-1.2130e-05, -8.8960e-06,  7.5623e-07,  ..., -9.4026e-06,
         -1.8459e-06, -4.8056e-06],
        [-1.8716e-05, -1.3679e-05,  1.1791e-06,  ..., -1.4484e-05,
         -2.8349e-06, -7.4059e-06],
        [-3.3855e-05, -2.4796e-05,  2.0973e-06,  ..., -2.6256e-05,
         -5.1558e-06, -1.3396e-05]], device='cuda:0')
Loss: 1.186348557472229


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  7094,   596,  ...,  4764,  9514,    14],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7323e-06, -1.2795e-04,  8.4216e-06,  ...,  2.4690e-05,
          1.9435e-05,  5.0526e-05],
        [-1.7613e-05, -1.2890e-05,  1.1185e-06,  ..., -1.3590e-05,
         -2.6748e-06, -7.0035e-06],
        [-1.5184e-05, -1.1131e-05,  9.6485e-07,  ..., -1.1742e-05,
         -2.3078e-06, -6.0424e-06],
        [-2.3425e-05, -1.7136e-05,  1.5032e-06,  ..., -1.8090e-05,
         -3.5465e-06, -9.3132e-06],
        [-4.2379e-05, -3.1024e-05,  2.6748e-06,  ..., -3.2753e-05,
         -6.4448e-06, -1.6838e-05]], device='cuda:0')
Loss: 1.1965008974075317


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,  360, 2374,  ...,  221,  474,  357],
        [   0, 4882,   12,  ...,   12, 9575,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9083e-06, -1.3649e-04,  5.0368e-05,  ...,  5.5248e-05,
         -1.2919e-05,  2.8875e-05],
        [-2.1145e-05, -1.5408e-05,  1.3607e-06,  ..., -1.6302e-05,
         -3.1851e-06, -8.4043e-06],
        [-1.8224e-05, -1.3307e-05,  1.1753e-06,  ..., -1.4082e-05,
         -2.7474e-06, -7.2494e-06],
        [-2.8133e-05, -2.0504e-05,  1.8273e-06,  ..., -2.1711e-05,
         -4.2282e-06, -1.1183e-05],
        [-5.0902e-05, -3.7104e-05,  3.2559e-06,  ..., -3.9309e-05,
         -7.6815e-06, -2.0221e-05]], device='cuda:0')
Loss: 1.201375961303711


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  2261,     9,  ..., 15008,    14,   333],
        [    0,  2973,    30,  ...,   408,   259,  1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1621e-06, -2.1133e-04,  2.4885e-05,  ...,  8.0611e-05,
         -2.9067e-05,  2.6069e-05],
        [-2.4617e-05, -1.7971e-05,  1.5786e-06,  ..., -1.8984e-05,
         -3.7178e-06, -9.7826e-06],
        [-2.1234e-05, -1.5527e-05,  1.3644e-06,  ..., -1.6406e-05,
         -3.2112e-06, -8.4415e-06],
        [-3.2783e-05, -2.3931e-05,  2.1216e-06,  ..., -2.5302e-05,
         -4.9435e-06, -1.3024e-05],
        [-5.9307e-05, -4.3273e-05,  3.7774e-06,  ..., -4.5776e-05,
         -8.9705e-06, -2.3529e-05]], device='cuda:0')
Loss: 1.1822444200515747


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[   0, 5281, 4452,  ...,   14, 3391,   12],
        [   0, 2220, 1110,  ...,  382,   18,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6807e-05, -2.0492e-04,  6.0657e-05,  ...,  8.8239e-05,
         -3.6593e-05,  7.9814e-05],
        [-2.8074e-05, -2.0534e-05,  1.8170e-06,  ..., -2.1666e-05,
         -4.2394e-06, -1.1168e-05],
        [-2.4259e-05, -1.7762e-05,  1.5730e-06,  ..., -1.8746e-05,
         -3.6675e-06, -9.6560e-06],
        [-3.7432e-05, -2.7373e-05,  2.4457e-06,  ..., -2.8893e-05,
         -5.6438e-06, -1.4894e-05],
        [-6.7711e-05, -4.9502e-05,  4.3549e-06,  ..., -5.2303e-05,
         -1.0245e-05, -2.6911e-05]], device='cuda:0')
Loss: 1.1772549152374268
Graident accumulation at epoch 0, step 199, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8382e-04, -8.3887e-05,  1.5217e-05,  ...,  6.1225e-05,
         -1.0501e-04, -1.6096e-05],
        [-2.8842e-05, -2.0403e-05,  3.9915e-06,  ..., -2.2908e-05,
         -2.4279e-06, -1.1569e-05],
        [ 8.4852e-06,  6.3907e-06, -4.7891e-06,  ...,  1.1599e-05,
         -1.1417e-06,  6.8660e-06],
        [-1.2692e-05, -4.3555e-06,  5.3100e-06,  ..., -7.5721e-06,
         -1.0257e-06, -3.3409e-06],
        [-5.9235e-05, -4.1771e-05,  6.9094e-06,  ..., -4.5949e-05,
         -4.8370e-06, -2.3742e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7770e-08, 1.4756e-08, 1.7491e-08,  ..., 2.0046e-08, 3.1788e-08,
         6.5161e-09],
        [2.4064e-11, 1.5648e-11, 9.5510e-13,  ..., 1.5786e-11, 5.4962e-13,
         3.2139e-12],
        [9.4977e-11, 7.6561e-11, 1.0043e-11,  ..., 1.3610e-10, 3.2982e-12,
         4.3641e-11],
        [1.6908e-10, 2.5106e-10, 4.6635e-12,  ..., 1.2585e-10, 6.5027e-12,
         4.5643e-11],
        [9.0608e-11, 5.3494e-11, 4.7873e-12,  ..., 6.0029e-11, 1.6438e-12,
         1.3687e-11]], device='cuda:0')
optimizer state dict: 25.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 25


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7458e-05, -1.3071e-05, -2.7398e-05,  ..., -2.6791e-06,
         -1.2295e-05,  1.5044e-05],
        [-3.3975e-06, -2.4736e-06,  2.1607e-07,  ..., -2.6673e-06,
         -5.0664e-07, -1.3188e-06],
        [-3.0398e-06, -2.2203e-06,  1.9185e-07,  ..., -2.3842e-06,
         -4.5449e-07, -1.1772e-06],
        [-4.6492e-06, -3.3826e-06,  2.9616e-07,  ..., -3.6508e-06,
         -6.9290e-07, -1.7956e-06],
        [-8.2850e-06, -6.0201e-06,  5.1782e-07,  ..., -6.4969e-06,
         -1.2368e-06, -3.1888e-06]], device='cuda:0')
Loss: 1.1775978803634644


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,    43,   503,    67],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1416e-05, -3.1567e-05, -1.2852e-05,  ...,  2.1968e-05,
         -8.5058e-06,  2.6324e-06],
        [-6.7800e-06, -4.9621e-06,  4.2003e-07,  ..., -5.3346e-06,
         -9.9465e-07, -2.6152e-06],
        [-6.0648e-06, -4.4405e-06,  3.7439e-07,  ..., -4.7684e-06,
         -8.9221e-07, -2.3395e-06],
        [-9.2983e-06, -6.7949e-06,  5.7742e-07,  ..., -7.3165e-06,
         -1.3672e-06, -3.5763e-06],
        [-1.6510e-05, -1.2070e-05,  1.0096e-06,  ..., -1.2994e-05,
         -2.4363e-06, -6.3479e-06]], device='cuda:0')
Loss: 1.182523488998413


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  2377,   360,  ...,   266,  4745,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0679e-04, -3.6875e-05,  1.4766e-05,  ...,  5.3404e-05,
         -6.6824e-05, -3.4175e-05],
        [-1.0163e-05, -7.4208e-06,  6.5099e-07,  ..., -7.9721e-06,
         -1.4789e-06, -3.9116e-06],
        [-9.0599e-06, -6.6310e-06,  5.7928e-07,  ..., -7.1079e-06,
         -1.3225e-06, -3.4943e-06],
        [-1.4007e-05, -1.0222e-05,  8.9966e-07,  ..., -1.0982e-05,
         -2.0377e-06, -5.3868e-06],
        [-2.4796e-05, -1.8120e-05,  1.5721e-06,  ..., -1.9461e-05,
         -3.6284e-06, -9.5218e-06]], device='cuda:0')
Loss: 1.1994370222091675


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3905e-05, -2.5417e-05, -1.4667e-05,  ...,  7.0150e-05,
         -5.8909e-05, -3.5685e-05],
        [-1.3515e-05, -9.9093e-06,  8.6613e-07,  ..., -1.0639e-05,
         -2.0154e-06, -5.2005e-06],
        [-1.2070e-05, -8.8662e-06,  7.7300e-07,  ..., -9.5069e-06,
         -1.8030e-06, -4.6566e-06],
        [-1.8597e-05, -1.3620e-05,  1.1977e-06,  ..., -1.4618e-05,
         -2.7679e-06, -7.1526e-06],
        [-3.2961e-05, -2.4170e-05,  2.0936e-06,  ..., -2.5958e-05,
         -4.9323e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.1866326332092285


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8877e-05, -3.8395e-06, -6.3195e-06,  ...,  3.9970e-05,
         -5.0959e-05, -2.0407e-05],
        [-1.6928e-05, -1.2398e-05,  1.0487e-06,  ..., -1.3307e-05,
         -2.5295e-06, -6.4895e-06],
        [-1.5110e-05, -1.1072e-05,  9.3505e-07,  ..., -1.1876e-05,
         -2.2613e-06, -5.8040e-06],
        [-2.3276e-05, -1.7032e-05,  1.4529e-06,  ..., -1.8284e-05,
         -3.4757e-06, -8.9258e-06],
        [-4.1306e-05, -3.0220e-05,  2.5369e-06,  ..., -3.2485e-05,
         -6.1914e-06, -1.5810e-05]], device='cuda:0')
Loss: 1.1804741621017456


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0,  300,  369,  ...,   12,  970,   12],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4561e-05,  3.4482e-05, -3.2190e-06,  ...,  1.2186e-05,
         -5.7242e-05, -4.0245e-06],
        [-2.0310e-05, -1.4827e-05,  1.2275e-06,  ..., -1.5974e-05,
         -3.0398e-06, -7.7859e-06],
        [-1.8120e-05, -1.3232e-05,  1.0934e-06,  ..., -1.4246e-05,
         -2.7157e-06, -6.9514e-06],
        [-2.7895e-05, -2.0340e-05,  1.6969e-06,  ..., -2.1935e-05,
         -4.1723e-06, -1.0692e-05],
        [-4.9472e-05, -3.6091e-05,  2.9635e-06,  ..., -3.8952e-05,
         -7.4282e-06, -1.8939e-05]], device='cuda:0')
Loss: 1.1847960948944092


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0,  271, 3421,  ...,  306,  472,  346],
        [   0,  221, 4070,  ..., 1061, 3189,   26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2800e-05,  5.3747e-05, -9.2270e-07,  ...,  1.9212e-05,
         -5.0331e-05, -1.4134e-05],
        [-2.3708e-05, -1.7300e-05,  1.4529e-06,  ..., -1.8626e-05,
         -3.5465e-06, -9.0897e-06],
        [-2.1145e-05, -1.5438e-05,  1.2945e-06,  ..., -1.6600e-05,
         -3.1665e-06, -8.1137e-06],
        [-3.2544e-05, -2.3738e-05,  2.0098e-06,  ..., -2.5570e-05,
         -4.8652e-06, -1.2480e-05],
        [-5.7697e-05, -4.2111e-05,  3.5074e-06,  ..., -4.5359e-05,
         -8.6576e-06, -2.2098e-05]], device='cuda:0')
Loss: 1.1835911273956299


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2951e-04,  4.0086e-05, -9.6558e-06,  ...,  3.9362e-05,
         -8.3695e-05, -2.6008e-05],
        [-2.7031e-05, -1.9759e-05,  1.6931e-06,  ..., -2.1249e-05,
         -4.1090e-06, -1.0379e-05],
        [-2.4155e-05, -1.7658e-05,  1.5125e-06,  ..., -1.8969e-05,
         -3.6769e-06, -9.2760e-06],
        [-3.7134e-05, -2.7135e-05,  2.3451e-06,  ..., -2.9191e-05,
         -5.6401e-06, -1.4253e-05],
        [-6.5804e-05, -4.8071e-05,  4.0885e-06,  ..., -5.1707e-05,
         -1.0028e-05, -2.5213e-05]], device='cuda:0')
Loss: 1.176927924156189
Graident accumulation at epoch 0, step 207, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0152,  0.0037,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5249e-04, -7.1490e-05,  1.2730e-05,  ...,  5.9038e-05,
         -1.0288e-04, -1.7087e-05],
        [-2.8661e-05, -2.0339e-05,  3.7616e-06,  ..., -2.2742e-05,
         -2.5960e-06, -1.1450e-05],
        [ 5.2212e-06,  3.9858e-06, -4.1589e-06,  ...,  8.5423e-06,
         -1.3952e-06,  5.2518e-06],
        [-1.5136e-05, -6.6335e-06,  5.0135e-06,  ..., -9.7340e-06,
         -1.4872e-06, -4.4321e-06],
        [-5.9892e-05, -4.2401e-05,  6.6273e-06,  ..., -4.6525e-05,
         -5.3561e-06, -2.3889e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7739e-08, 1.4743e-08, 1.7474e-08,  ..., 2.0027e-08, 3.1763e-08,
         6.5102e-09],
        [2.4770e-11, 1.6022e-11, 9.5701e-13,  ..., 1.6222e-11, 5.6595e-13,
         3.3184e-12],
        [9.5465e-11, 7.6796e-11, 1.0036e-11,  ..., 1.3633e-10, 3.3084e-12,
         4.3684e-11],
        [1.7029e-10, 2.5155e-10, 4.6644e-12,  ..., 1.2658e-10, 6.5280e-12,
         4.5801e-11],
        [9.4847e-11, 5.5751e-11, 4.7992e-12,  ..., 6.2643e-11, 1.7427e-12,
         1.4309e-11]], device='cuda:0')
optimizer state dict: 26.0
lr: [1.8547933878823103e-05, 1.8547933878823103e-05]
scheduler_last_epoch: 26


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2394e-05,  1.4589e-05, -9.8311e-06,  ...,  3.7872e-05,
          3.5281e-06,  6.5826e-05],
        [-3.3081e-06, -2.3991e-06,  2.0955e-07,  ..., -2.5779e-06,
         -4.6380e-07, -1.2368e-06],
        [-3.0547e-06, -2.2054e-06,  1.9278e-07,  ..., -2.3842e-06,
         -4.2841e-07, -1.1399e-06],
        [-4.7088e-06, -3.4124e-06,  2.9989e-07,  ..., -3.6806e-06,
         -6.5938e-07, -1.7658e-06],
        [-8.1658e-06, -5.9009e-06,  5.1036e-07,  ..., -6.3479e-06,
         -1.1474e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.1630827188491821


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4745e-05,  3.1384e-05, -5.0185e-06,  ...,  3.3302e-05,
         -6.1851e-06,  6.8851e-05],
        [-6.6012e-06, -4.7386e-06,  4.2934e-07,  ..., -5.1856e-06,
         -9.0525e-07, -2.4661e-06],
        [-6.1095e-06, -4.3809e-06,  3.9674e-07,  ..., -4.8131e-06,
         -8.3819e-07, -2.2799e-06],
        [-9.3877e-06, -6.7353e-06,  6.1654e-07,  ..., -7.4059e-06,
         -1.2890e-06, -3.5167e-06],
        [-1.6332e-05, -1.1683e-05,  1.0505e-06,  ..., -1.2815e-05,
         -2.2426e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.1868629455566406


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0,     9,   870,  ...,  2671,   965,  3229]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0785e-05,  1.6362e-05,  2.2444e-05,  ...,  7.5872e-05,
         -3.1241e-05,  8.1206e-05],
        [-9.8944e-06, -7.1079e-06,  6.4634e-07,  ..., -7.7784e-06,
         -1.3709e-06, -3.6955e-06],
        [-9.1493e-06, -6.5714e-06,  5.9698e-07,  ..., -7.2122e-06,
         -1.2685e-06, -3.4124e-06],
        [-1.4126e-05, -1.0148e-05,  9.3132e-07,  ..., -1.1146e-05,
         -1.9595e-06, -5.2825e-06],
        [-2.4498e-05, -1.7583e-05,  1.5870e-06,  ..., -1.9282e-05,
         -3.4049e-06, -9.1195e-06]], device='cuda:0')
Loss: 1.1745918989181519


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  2314,   516,  ...,  1871,    13,  1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5551e-05,  7.6321e-07, -2.0028e-05,  ...,  1.3064e-04,
         -3.4294e-05,  8.9869e-05],
        [-1.3158e-05, -9.4622e-06,  8.7265e-07,  ..., -1.0401e-05,
         -1.8012e-06, -4.9099e-06],
        [-1.2159e-05, -8.7470e-06,  8.0653e-07,  ..., -9.6411e-06,
         -1.6652e-06, -4.5300e-06],
        [-1.8805e-05, -1.3515e-05,  1.2591e-06,  ..., -1.4901e-05,
         -2.5742e-06, -7.0184e-06],
        [-3.2425e-05, -2.3305e-05,  2.1346e-06,  ..., -2.5660e-05,
         -4.4554e-06, -1.2070e-05]], device='cuda:0')
Loss: 1.1755683422088623


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0, 1358,  367,  ..., 1758, 2921,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7875e-05, -1.4144e-05, -1.6598e-05,  ...,  1.3763e-04,
         -3.4811e-05,  1.1233e-04],
        [-1.6466e-05, -1.1832e-05,  1.0692e-06,  ..., -1.3009e-05,
         -2.2314e-06, -6.1318e-06],
        [-1.5169e-05, -1.0908e-05,  9.8534e-07,  ..., -1.2025e-05,
         -2.0582e-06, -5.6475e-06],
        [-2.3484e-05, -1.6868e-05,  1.5404e-06,  ..., -1.8582e-05,
         -3.1814e-06, -8.7470e-06],
        [-4.0531e-05, -2.9117e-05,  2.6152e-06,  ..., -3.2037e-05,
         -5.5134e-06, -1.5050e-05]], device='cuda:0')
Loss: 1.1848788261413574


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0,  1746,    14,  ...,  3134,  5968,     9],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9464e-05, -4.5877e-06, -3.5278e-05,  ...,  1.4557e-04,
         -2.5280e-05,  1.1690e-04],
        [-1.9774e-05, -1.4246e-05,  1.3076e-06,  ..., -1.5631e-05,
         -2.6971e-06, -7.3835e-06],
        [ 6.7528e-05,  5.4791e-05,  6.2583e-06,  ...,  6.9752e-05,
          8.7394e-06,  2.5928e-05],
        [-2.8133e-05, -2.0266e-05,  1.8775e-06,  ..., -2.2277e-05,
         -3.8370e-06, -1.0513e-05],
        [-4.8637e-05, -3.5018e-05,  3.1963e-06,  ..., -3.8445e-05,
         -6.6534e-06, -1.8105e-05]], device='cuda:0')
Loss: 1.1785171031951904


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0, 16803,   965,  ..., 36064,    12, 13769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4706e-05,  1.6157e-05, -2.4909e-06,  ...,  1.6792e-04,
         -3.9202e-05,  1.3072e-04],
        [-2.3082e-05, -1.6645e-05,  1.5264e-06,  ..., -1.8239e-05,
         -3.1516e-06, -8.6278e-06],
        [ 6.4488e-05,  5.2586e-05,  6.4604e-06,  ...,  6.7353e-05,
          8.3203e-06,  2.4788e-05],
        [-3.2842e-05, -2.3693e-05,  2.1923e-06,  ..., -2.5988e-05,
         -4.4852e-06, -1.2279e-05],
        [-5.6803e-05, -4.0948e-05,  3.7327e-06,  ..., -4.4882e-05,
         -7.7784e-06, -2.1160e-05]], device='cuda:0')
Loss: 1.178139090538025


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8428e-05,  1.6448e-05, -2.6131e-05,  ...,  1.5375e-04,
         -5.5764e-05,  1.2388e-04],
        [-2.6420e-05, -1.9029e-05,  1.7276e-06,  ..., -2.0832e-05,
         -3.6173e-06, -9.8795e-06],
        [ 6.1404e-05,  5.0381e-05,  6.6485e-06,  ...,  6.4954e-05,
          7.8882e-06,  2.3625e-05],
        [-3.7551e-05, -2.7061e-05,  2.4792e-06,  ..., -2.9653e-05,
         -5.1446e-06, -1.4044e-05],
        [-6.5029e-05, -4.6819e-05,  4.2282e-06,  ..., -5.1260e-05,
         -8.9332e-06, -2.4229e-05]], device='cuda:0')
Loss: 1.166017770767212
Graident accumulation at epoch 0, step 215, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3240e-04, -6.2696e-05,  8.8439e-06,  ...,  6.8510e-05,
         -9.8170e-05, -2.9906e-06],
        [-2.8437e-05, -2.0208e-05,  3.5582e-06,  ..., -2.2551e-05,
         -2.6981e-06, -1.1293e-05],
        [ 1.0839e-05,  8.6253e-06, -3.0782e-06,  ...,  1.4183e-05,
         -4.6684e-07,  7.0892e-06],
        [-1.7378e-05, -8.6762e-06,  4.7601e-06,  ..., -1.1726e-05,
         -1.8529e-06, -5.3933e-06],
        [-6.0405e-05, -4.2843e-05,  6.3874e-06,  ..., -4.6998e-05,
         -5.7138e-06, -2.3923e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7693e-08, 1.4728e-08, 1.7457e-08,  ..., 2.0031e-08, 3.1734e-08,
         6.5191e-09],
        [2.5444e-11, 1.6368e-11, 9.5904e-13,  ..., 1.6640e-11, 5.7847e-13,
         3.4127e-12],
        [9.9140e-11, 7.9257e-11, 1.0070e-11,  ..., 1.4041e-10, 3.3673e-12,
         4.4198e-11],
        [1.7153e-10, 2.5203e-10, 4.6658e-12,  ..., 1.2733e-10, 6.5480e-12,
         4.5952e-11],
        [9.8981e-11, 5.7888e-11, 4.8123e-12,  ..., 6.5208e-11, 1.8208e-12,
         1.4882e-11]], device='cuda:0')
optimizer state dict: 27.0
lr: [1.8417028402436446e-05, 1.8417028402436446e-05]
scheduler_last_epoch: 27


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[   0,  275, 2101,  ..., 1145,  590, 1619],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2542e-06,  6.2477e-06, -4.9526e-05,  ...,  2.5470e-05,
          2.4260e-06,  4.8321e-06],
        [-3.2037e-06, -2.2650e-06,  2.0117e-07,  ..., -2.5779e-06,
         -3.7439e-07, -1.2219e-06],
        [-3.0249e-06, -2.1458e-06,  1.8906e-07,  ..., -2.4438e-06,
         -3.5390e-07, -1.1548e-06],
        [-4.7088e-06, -3.3379e-06,  2.9616e-07,  ..., -3.7849e-06,
         -5.4762e-07, -1.7956e-06],
        [-7.8678e-06, -5.5730e-06,  4.9174e-07,  ..., -6.3479e-06,
         -9.2387e-07, -2.9951e-06]], device='cuda:0')
Loss: 1.1725831031799316


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[   0, 1862,  674,  ...,  391,  266, 7688],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4439e-05, -7.3664e-06, -3.5240e-05,  ...,  1.5146e-05,
          9.2880e-06, -2.0322e-05],
        [-6.4075e-06, -4.5896e-06,  4.3306e-07,  ..., -5.1558e-06,
         -7.7672e-07, -2.3842e-06],
        [-6.0648e-06, -4.3511e-06,  4.1071e-07,  ..., -4.9025e-06,
         -7.3574e-07, -2.2575e-06],
        [-9.4175e-06, -6.7651e-06,  6.4261e-07,  ..., -7.5996e-06,
         -1.1399e-06, -3.5018e-06],
        [-1.5736e-05, -1.1295e-05,  1.0617e-06,  ..., -1.2696e-05,
         -1.9073e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.187293529510498


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8564e-05, -3.5610e-05, -3.6359e-05,  ...,  1.7593e-05,
          6.2695e-06,  1.2379e-06],
        [-9.6112e-06, -6.9141e-06,  6.8080e-07,  ..., -7.7337e-06,
         -1.1623e-06, -3.5986e-06],
        [ 1.1409e-04,  5.4733e-05, -8.2959e-06,  ...,  7.9366e-05,
          7.9505e-06,  2.6104e-05],
        [-1.4096e-05, -1.0177e-05,  1.0096e-06,  ..., -1.1370e-05,
         -1.7025e-06, -5.2750e-06],
        [-2.3603e-05, -1.7017e-05,  1.6689e-06,  ..., -1.9014e-05,
         -2.8536e-06, -8.8066e-06]], device='cuda:0')
Loss: 1.1773284673690796


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,  6491,  3667,  ...,  5042,    14,  2152]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8372e-05, -3.3896e-05,  1.4849e-05,  ...,  2.0869e-05,
         -5.9620e-06, -3.1323e-06],
        [-1.2845e-05, -9.2089e-06,  8.8476e-07,  ..., -1.0327e-05,
         -1.5795e-06, -4.7833e-06],
        [ 1.1101e-04,  5.2543e-05, -8.1004e-06,  ...,  7.6907e-05,
          7.5519e-06,  2.4971e-05],
        [-1.8865e-05, -1.3560e-05,  1.3132e-06,  ..., -1.5184e-05,
         -2.3171e-06, -7.0259e-06],
        [-3.1590e-05, -2.2680e-05,  2.1681e-06,  ..., -2.5392e-05,
         -3.8818e-06, -1.1727e-05]], device='cuda:0')
Loss: 1.1980828046798706


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9401e-05, -5.1654e-05,  4.6230e-05,  ...,  2.8179e-05,
         -1.0000e-05, -3.0310e-05],
        [-1.6049e-05, -1.1519e-05,  1.1064e-06,  ..., -1.2919e-05,
         -1.9465e-06, -5.9456e-06],
        [ 1.0800e-04,  5.0367e-05, -7.8908e-06,  ...,  7.4464e-05,
          7.2073e-06,  2.3876e-05],
        [-2.3574e-05, -1.6958e-05,  1.6410e-06,  ..., -1.8999e-05,
         -2.8536e-06, -8.7321e-06],
        [-3.9458e-05, -2.8372e-05,  2.7083e-06,  ..., -3.1769e-05,
         -4.7833e-06, -1.4573e-05]], device='cuda:0')
Loss: 1.1806581020355225


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0886e-05, -4.0924e-05,  1.9937e-05,  ...,  3.6617e-05,
         -9.4429e-06,  2.7076e-05],
        [-1.9312e-05, -1.3843e-05,  1.3430e-06,  ..., -1.5542e-05,
         -2.3525e-06, -7.1675e-06],
        [ 1.0491e-04,  4.8162e-05, -7.6664e-06,  ...,  7.1990e-05,
          6.8236e-06,  2.2728e-05],
        [-2.8342e-05, -2.0370e-05,  1.9893e-06,  ..., -2.2843e-05,
         -3.4459e-06, -1.0513e-05],
        [-4.7505e-05, -3.4094e-05,  3.2857e-06,  ..., -3.8207e-05,
         -5.7817e-06, -1.7554e-05]], device='cuda:0')
Loss: 1.1725680828094482


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0, 5982, 9385,  ...,   26,  469,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2041e-05, -1.7287e-05,  3.5558e-05,  ...,  4.4578e-05,
         -2.6075e-05,  4.5859e-05],
        [-2.2516e-05, -1.6153e-05,  1.5460e-06,  ..., -1.8150e-05,
         -2.7586e-06, -8.3521e-06],
        [ 1.0189e-04,  4.5986e-05, -7.4755e-06,  ...,  6.9531e-05,
          6.4399e-06,  2.1611e-05],
        [-3.2991e-05, -2.3708e-05,  2.2855e-06,  ..., -2.6628e-05,
         -4.0345e-06, -1.2226e-05],
        [-5.5373e-05, -3.9756e-05,  3.7812e-06,  ..., -4.4644e-05,
         -6.7875e-06, -2.0459e-05]], device='cuda:0')
Loss: 1.171024203300476


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0,  199, 2834,  ...,  287, 3121,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1843e-05, -8.1005e-06, -1.3614e-05,  ...,  8.5364e-05,
         -3.1832e-05, -1.3685e-05],
        [-2.5675e-05, -1.8433e-05,  1.7919e-06,  ..., -2.0713e-05,
         -3.1516e-06, -9.5591e-06],
        [ 9.8909e-05,  4.3825e-05, -7.2436e-06,  ...,  6.7117e-05,
          6.0692e-06,  2.0471e-05],
        [-3.7640e-05, -2.7061e-05,  2.6505e-06,  ..., -3.0398e-05,
         -4.6119e-06, -1.4000e-05],
        [-6.3062e-05, -4.5329e-05,  4.3772e-06,  ..., -5.0902e-05,
         -7.7486e-06, -2.3395e-05]], device='cuda:0')
Loss: 1.1820173263549805
Graident accumulation at epoch 0, step 223, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0280, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1597e-04, -5.7236e-05,  6.5981e-06,  ...,  7.0195e-05,
         -9.1536e-05, -4.0601e-06],
        [-2.8161e-05, -2.0030e-05,  3.3816e-06,  ..., -2.2367e-05,
         -2.7435e-06, -1.1120e-05],
        [ 1.9646e-05,  1.2145e-05, -3.4947e-06,  ...,  1.9477e-05,
          1.8677e-07,  8.4273e-06],
        [-1.9404e-05, -1.0515e-05,  4.5491e-06,  ..., -1.3593e-05,
         -2.1288e-06, -6.2539e-06],
        [-6.0671e-05, -4.3091e-05,  6.1864e-06,  ..., -4.7389e-05,
         -5.9173e-06, -2.3871e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7647e-08, 1.4714e-08, 1.7440e-08,  ..., 2.0018e-08, 3.1704e-08,
         6.5127e-09],
        [2.6077e-11, 1.6692e-11, 9.6129e-13,  ..., 1.7052e-11, 5.8782e-13,
         3.5007e-12],
        [1.0882e-10, 8.1099e-11, 1.0112e-11,  ..., 1.4477e-10, 3.4008e-12,
         4.4573e-11],
        [1.7277e-10, 2.5251e-10, 4.6682e-12,  ..., 1.2813e-10, 6.5627e-12,
         4.6102e-11],
        [1.0286e-10, 5.9884e-11, 4.8266e-12,  ..., 6.7734e-11, 1.8790e-12,
         1.5414e-11]], device='cuda:0')
optimizer state dict: 28.0
lr: [1.828097834093899e-05, 1.828097834093899e-05]
scheduler_last_epoch: 28


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,  3235,   471,  ...,  1967,  4273,  2738],
        [    0, 21410, 13160,  ...,   292,    69,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9651e-06,  3.7123e-06, -1.6854e-06,  ...,  2.0559e-05,
         -1.7630e-05,  2.9840e-05],
        [-3.1590e-06, -2.2203e-06,  2.5146e-07,  ..., -2.5630e-06,
         -3.3341e-07, -1.1548e-06],
        [-3.0696e-06, -2.1607e-06,  2.4401e-07,  ..., -2.4885e-06,
         -3.2410e-07, -1.1250e-06],
        [-4.7088e-06, -3.3230e-06,  3.7439e-07,  ..., -3.8147e-06,
         -4.9919e-07, -1.7211e-06],
        [-7.7486e-06, -5.4538e-06,  6.1095e-07,  ..., -6.2883e-06,
         -8.2329e-07, -2.8312e-06]], device='cuda:0')
Loss: 1.193020224571228


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[   0,  278, 5717,  ..., 5342, 5147,   14],
        [   0, 6518,  681,  ...,  401, 9748,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8557e-06, -1.0725e-05,  1.9161e-05,  ...,  4.1469e-05,
         -6.3522e-06,  4.0403e-05],
        [-6.2436e-06, -4.4554e-06,  4.8615e-07,  ..., -5.0664e-06,
         -6.4448e-07, -2.2575e-06],
        [-6.0946e-06, -4.3511e-06,  4.7591e-07,  ..., -4.9472e-06,
         -6.2957e-07, -2.2054e-06],
        [-9.3877e-06, -6.7055e-06,  7.3388e-07,  ..., -7.5996e-06,
         -9.6858e-07, -3.3900e-06],
        [-1.5438e-05, -1.0997e-05,  1.1884e-06,  ..., -1.2487e-05,
         -1.5944e-06, -5.5730e-06]], device='cuda:0')
Loss: 1.1732507944107056


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[    0,   292,    48,  ...,   199, 19047,   292],
        [    0, 23809, 27646,  ...,   266,  3373,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5038e-06, -1.2945e-05,  2.7939e-05,  ...,  7.2069e-05,
         -2.2142e-05,  5.2990e-05],
        [-9.3728e-06, -6.7055e-06,  7.2364e-07,  ..., -7.6145e-06,
         -9.7044e-07, -3.3900e-06],
        [-9.1344e-06, -6.5267e-06,  7.0781e-07,  ..., -7.4357e-06,
         -9.4622e-07, -3.3081e-06],
        [-1.4096e-05, -1.0073e-05,  1.0934e-06,  ..., -1.1444e-05,
         -1.4566e-06, -5.0962e-06],
        [-2.3127e-05, -1.6510e-05,  1.7695e-06,  ..., -1.8746e-05,
         -2.3954e-06, -8.3447e-06]], device='cuda:0')
Loss: 1.166809320449829


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2915,   275,  2565],
        [    0, 12472,  1059,  ...,   642,   365,  6517]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3311e-05, -2.6832e-05,  2.2461e-05,  ...,  9.4801e-05,
         -7.2308e-06,  4.4028e-05],
        [-1.2472e-05, -8.9258e-06,  9.5740e-07,  ..., -1.0148e-05,
         -1.3225e-06, -4.5300e-06],
        [-1.2144e-05, -8.6874e-06,  9.3505e-07,  ..., -9.9093e-06,
         -1.2871e-06, -4.4107e-06],
        [-1.8746e-05, -1.3411e-05,  1.4473e-06,  ..., -1.5259e-05,
         -1.9819e-06, -6.8024e-06],
        [-3.0696e-05, -2.1935e-05,  2.3395e-06,  ..., -2.4945e-05,
         -3.2559e-06, -1.1116e-05]], device='cuda:0')
Loss: 1.1905125379562378


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  1034,  5599,  ...,   259,   586,  1403],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4208e-05, -3.0228e-05,  2.7466e-05,  ...,  8.6050e-05,
         -4.3609e-06,  5.2696e-05],
        [-1.5572e-05, -1.1131e-05,  1.1893e-06,  ..., -1.2666e-05,
         -1.6447e-06, -5.6550e-06],
        [-1.5169e-05, -1.0833e-05,  1.1623e-06,  ..., -1.2368e-05,
         -1.6000e-06, -5.5060e-06],
        [-2.3425e-05, -1.6734e-05,  1.7993e-06,  ..., -1.9044e-05,
         -2.4661e-06, -8.5011e-06],
        [-3.8385e-05, -2.7418e-05,  2.9132e-06,  ..., -3.1203e-05,
         -4.0568e-06, -1.3918e-05]], device='cuda:0')
Loss: 1.1801385879516602


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[   0, 4995,  287,  ...,  300, 4531, 4729],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1335e-05, -2.6265e-05,  3.8536e-05,  ...,  1.0146e-04,
         -1.1632e-05,  6.8564e-05],
        [-1.8701e-05, -1.3351e-05,  1.4314e-06,  ..., -1.5199e-05,
         -1.9874e-06, -6.7949e-06],
        [-1.8209e-05, -1.2979e-05,  1.3979e-06,  ..., -1.4812e-05,
         -1.9316e-06, -6.6087e-06],
        [-2.8163e-05, -2.0087e-05,  2.1681e-06,  ..., -2.2858e-05,
         -2.9802e-06, -1.0215e-05],
        [-4.6015e-05, -3.2842e-05,  3.5055e-06,  ..., -3.7372e-05,
         -4.8950e-06, -1.6674e-05]], device='cuda:0')
Loss: 1.1873372793197632


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,    12,  2735,  ...,    12,   344,  1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6737e-05, -2.6265e-05,  4.7709e-05,  ...,  1.0110e-04,
          5.6845e-06,  5.0998e-05],
        [-2.1815e-05, -1.5602e-05,  1.6736e-06,  ..., -1.7747e-05,
         -2.3264e-06, -7.9423e-06],
        [-2.1249e-05, -1.5169e-05,  1.6345e-06,  ..., -1.7285e-05,
         -2.2631e-06, -7.7188e-06],
        [-3.2872e-05, -2.3484e-05,  2.5351e-06,  ..., -2.6703e-05,
         -3.4906e-06, -1.1936e-05],
        [-5.3763e-05, -3.8415e-05,  4.1015e-06,  ..., -4.3660e-05,
         -5.7369e-06, -1.9506e-05]], device='cuda:0')
Loss: 1.1941328048706055


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[    0,   287,  9430,  ...,  3121,   352,   360],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6230e-05,  2.2881e-05,  6.2849e-05,  ...,  7.9347e-05,
         -1.9076e-05,  7.7263e-05],
        [-2.4915e-05, -1.7852e-05,  1.9111e-06,  ..., -2.0266e-05,
         -2.6468e-06, -9.0748e-06],
        [-2.4259e-05, -1.7345e-05,  1.8664e-06,  ..., -1.9744e-05,
         -2.5742e-06, -8.8215e-06],
        [-3.7551e-05, -2.6882e-05,  2.8983e-06,  ..., -3.0518e-05,
         -3.9749e-06, -1.3649e-05],
        [-6.1452e-05, -4.3988e-05,  4.6864e-06,  ..., -4.9919e-05,
         -6.5342e-06, -2.2307e-05]], device='cuda:0')
Loss: 1.1793773174285889
Graident accumulation at epoch 0, step 231, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0275e-04, -4.9225e-05,  1.2223e-05,  ...,  7.1111e-05,
         -8.4290e-05,  4.0722e-06],
        [-2.7836e-05, -1.9812e-05,  3.2345e-06,  ..., -2.2157e-05,
         -2.7338e-06, -1.0915e-05],
        [ 1.5256e-05,  9.1963e-06, -2.9586e-06,  ...,  1.5555e-05,
         -8.9326e-08,  6.7025e-06],
        [-2.1219e-05, -1.2151e-05,  4.3840e-06,  ..., -1.5286e-05,
         -2.3134e-06, -6.9935e-06],
        [-6.0749e-05, -4.3181e-05,  6.0364e-06,  ..., -4.7642e-05,
         -5.9790e-06, -2.3714e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7599e-08, 1.4699e-08, 1.7427e-08,  ..., 2.0004e-08, 3.1672e-08,
         6.5122e-09],
        [2.6672e-11, 1.6994e-11, 9.6398e-13,  ..., 1.7446e-11, 5.9424e-13,
         3.5795e-12],
        [1.0930e-10, 8.1318e-11, 1.0106e-11,  ..., 1.4502e-10, 3.4040e-12,
         4.4606e-11],
        [1.7401e-10, 2.5298e-10, 4.6719e-12,  ..., 1.2893e-10, 6.5719e-12,
         4.6242e-11],
        [1.0653e-10, 6.1759e-11, 4.8438e-12,  ..., 7.0158e-11, 1.9198e-12,
         1.5897e-11]], device='cuda:0')
optimizer state dict: 29.0
lr: [1.8139866849701876e-05, 1.8139866849701876e-05]
scheduler_last_epoch: 29


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,  6909,   352,  ...,  1075,   706,  6909]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3353e-05,  1.8355e-05, -1.4502e-05,  ...,  4.7900e-07,
         -2.6173e-06,  9.7100e-07],
        [-3.0547e-06, -2.1309e-06,  2.7940e-07,  ..., -2.5183e-06,
         -2.9616e-07, -1.1101e-06],
        [-3.0398e-06, -2.1309e-06,  2.7753e-07,  ..., -2.5183e-06,
         -2.9616e-07, -1.1101e-06],
        [-4.6790e-06, -3.2634e-06,  4.3027e-07,  ..., -3.8445e-06,
         -4.5076e-07, -1.6987e-06],
        [-7.4804e-06, -5.2154e-06,  6.8173e-07,  ..., -6.1691e-06,
         -7.3016e-07, -2.7269e-06]], device='cuda:0')
Loss: 1.1743862628936768


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,    12,   546,  ..., 24994, 31107,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8543e-06,  1.4201e-05, -1.5330e-05,  ...,  2.8447e-05,
         -1.2501e-05,  2.3229e-07],
        [-6.0946e-06, -4.2915e-06,  5.7369e-07,  ..., -5.0068e-06,
         -5.9977e-07, -2.2203e-06],
        [-6.0648e-06, -4.2766e-06,  5.6997e-07,  ..., -4.9770e-06,
         -5.9791e-07, -2.2128e-06],
        [-9.3579e-06, -6.5863e-06,  8.8476e-07,  ..., -7.6592e-06,
         -9.1828e-07, -3.4049e-06],
        [-1.4991e-05, -1.0550e-05,  1.4044e-06,  ..., -1.2279e-05,
         -1.4789e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.174289345741272


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0,  642,  287,  ...,  800,   12, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7121e-05, -8.2246e-06,  2.9167e-05,  ...,  2.9225e-05,
         -4.3959e-05,  2.5976e-05],
        [-9.1791e-06, -6.4820e-06,  8.5868e-07,  ..., -7.5251e-06,
         -9.3132e-07, -3.3379e-06],
        [-9.1046e-06, -6.4224e-06,  8.4937e-07,  ..., -7.4506e-06,
         -9.2387e-07, -3.3155e-06],
        [-1.4096e-05, -9.9391e-06,  1.3225e-06,  ..., -1.1504e-05,
         -1.4249e-06, -5.1185e-06],
        [-2.2560e-05, -1.5914e-05,  2.0973e-06,  ..., -1.8448e-05,
         -2.2948e-06, -8.1956e-06]], device='cuda:0')
Loss: 1.168483018875122


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,   266,  2604,  ...,   278,  4035,  4165],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2447e-05, -1.7067e-05,  6.9184e-05,  ..., -1.7465e-05,
         -3.8539e-05,  2.9086e-05],
        [-1.2249e-05, -8.6427e-06,  1.1139e-06,  ..., -1.0028e-05,
         -1.2610e-06, -4.4554e-06],
        [-1.2159e-05, -8.5682e-06,  1.1008e-06,  ..., -9.9391e-06,
         -1.2498e-06, -4.4331e-06],
        [-1.8775e-05, -1.3217e-05,  1.7099e-06,  ..., -1.5303e-05,
         -1.9204e-06, -6.8173e-06],
        [-3.0100e-05, -2.1219e-05,  2.7157e-06,  ..., -2.4587e-05,
         -3.0994e-06, -1.0937e-05]], device='cuda:0')
Loss: 1.1708097457885742


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,  1875,  2117,  ...,  1422,  1059,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0764e-05,  5.0625e-06,  2.7805e-05,  ..., -2.4632e-05,
          1.2427e-05,  8.2107e-06],
        [-1.5303e-05, -1.0818e-05,  1.4380e-06,  ..., -1.2562e-05,
         -1.6056e-06, -5.5954e-06],
        [-1.5169e-05, -1.0714e-05,  1.4212e-06,  ..., -1.2442e-05,
         -1.5907e-06, -5.5581e-06],
        [-2.3425e-05, -1.6540e-05,  2.2054e-06,  ..., -1.9148e-05,
         -2.4419e-06, -8.5533e-06],
        [-3.7521e-05, -2.6524e-05,  3.5018e-06,  ..., -3.0726e-05,
         -3.9376e-06, -1.3709e-05]], device='cuda:0')
Loss: 1.177987813949585


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2301e-05, -6.4042e-05,  5.0557e-05,  ..., -3.0357e-05,
          4.3385e-05, -2.7710e-05],
        [-1.8358e-05, -1.2919e-05,  1.7025e-06,  ..., -1.5050e-05,
         -1.8869e-06, -6.7204e-06],
        [-1.8194e-05, -1.2800e-05,  1.6838e-06,  ..., -1.4916e-05,
         -1.8701e-06, -6.6683e-06],
        [-2.8074e-05, -1.9744e-05,  2.6096e-06,  ..., -2.2948e-05,
         -2.8703e-06, -1.0259e-05],
        [-4.5061e-05, -3.1710e-05,  4.1500e-06,  ..., -3.6895e-05,
         -4.6343e-06, -1.6481e-05]], device='cuda:0')
Loss: 1.179657220840454


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5680e-05, -7.0141e-05,  5.3335e-05,  ..., -6.5356e-05,
          2.3181e-05, -8.1337e-05],
        [-2.1458e-05, -1.5110e-05,  1.9632e-06,  ..., -1.7583e-05,
         -2.2054e-06, -7.8455e-06],
        [-2.1264e-05, -1.4961e-05,  1.9427e-06,  ..., -1.7419e-05,
         -2.1849e-06, -7.7859e-06],
        [-3.2812e-05, -2.3082e-05,  3.0119e-06,  ..., -2.6822e-05,
         -3.3546e-06, -1.1981e-05],
        [-5.2691e-05, -3.7104e-05,  4.7870e-06,  ..., -4.3124e-05,
         -5.4166e-06, -1.9252e-05]], device='cuda:0')
Loss: 1.164473533630371


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0,   11,  360,  ..., 4524, 1553,  401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6968e-05, -5.4229e-05,  7.5628e-05,  ..., -5.7022e-05,
          1.8624e-06, -5.2717e-05],
        [-2.4527e-05, -1.7285e-05,  2.2482e-06,  ..., -2.0117e-05,
         -2.5146e-06, -8.9630e-06],
        [-2.4289e-05, -1.7121e-05,  2.2240e-06,  ..., -1.9923e-05,
         -2.4904e-06, -8.8885e-06],
        [-3.7462e-05, -2.6390e-05,  3.4496e-06,  ..., -3.0667e-05,
         -3.8221e-06, -1.3672e-05],
        [-6.0141e-05, -4.2409e-05,  5.4762e-06,  ..., -4.9263e-05,
         -6.1691e-06, -2.1949e-05]], device='cuda:0')
Loss: 1.1793793439865112
Graident accumulation at epoch 0, step 239, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.6174e-05, -4.9725e-05,  1.8564e-05,  ...,  5.8297e-05,
         -7.5675e-05, -1.6067e-06],
        [-2.7505e-05, -1.9560e-05,  3.1359e-06,  ..., -2.1953e-05,
         -2.7119e-06, -1.0720e-05],
        [ 1.1301e-05,  6.5645e-06, -2.4403e-06,  ...,  1.2007e-05,
         -3.2943e-07,  5.1434e-06],
        [-2.2843e-05, -1.3575e-05,  4.2906e-06,  ..., -1.6824e-05,
         -2.4643e-06, -7.6613e-06],
        [-6.0688e-05, -4.3104e-05,  5.9804e-06,  ..., -4.7804e-05,
         -5.9980e-06, -2.3538e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7553e-08, 1.4688e-08, 1.7415e-08,  ..., 1.9987e-08, 3.1641e-08,
         6.5085e-09],
        [2.7247e-11, 1.7276e-11, 9.6807e-13,  ..., 1.7833e-11, 5.9997e-13,
         3.6563e-12],
        [1.0978e-10, 8.1530e-11, 1.0101e-11,  ..., 1.4527e-10, 3.4068e-12,
         4.4641e-11],
        [1.7524e-10, 2.5342e-10, 4.6792e-12,  ..., 1.2974e-10, 6.5799e-12,
         4.6383e-11],
        [1.1004e-10, 6.3496e-11, 4.8689e-12,  ..., 7.2515e-11, 1.9559e-12,
         1.6362e-11]], device='cuda:0')
optimizer state dict: 30.0
lr: [1.799378017770064e-05, 1.799378017770064e-05]
scheduler_last_epoch: 30


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8313e-05, -1.4344e-05, -1.4070e-05,  ..., -1.0478e-05,
          1.0856e-05,  1.5770e-05],
        [-2.9802e-06, -2.0862e-06,  3.1479e-07,  ..., -2.5034e-06,
         -2.9616e-07, -1.1250e-06],
        [-3.0100e-06, -2.1160e-06,  3.1851e-07,  ..., -2.5183e-06,
         -2.9989e-07, -1.1399e-06],
        [-4.5896e-06, -3.2187e-06,  4.8801e-07,  ..., -3.8445e-06,
         -4.5635e-07, -1.7360e-06],
        [-7.3612e-06, -5.1558e-06,  7.7486e-07,  ..., -6.1691e-06,
         -7.3761e-07, -2.7716e-06]], device='cuda:0')
Loss: 1.1853222846984863


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0,   278,   554,  ...,   365,  3125,   271],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4424e-05, -3.2237e-07, -1.3136e-05,  ..., -2.1820e-05,
          2.7917e-05,  4.3959e-06],
        [-5.9903e-06, -4.1723e-06,  6.1095e-07,  ..., -5.0068e-06,
         -5.8673e-07, -2.2426e-06],
        [-6.0201e-06, -4.2021e-06,  6.1467e-07,  ..., -5.0217e-06,
         -5.9046e-07, -2.2650e-06],
        [-9.1791e-06, -6.4224e-06,  9.4436e-07,  ..., -7.6890e-06,
         -8.9779e-07, -3.4496e-06],
        [-1.4752e-05, -1.0282e-05,  1.5013e-06,  ..., -1.2338e-05,
         -1.4529e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.207598328590393


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  1067,   271,  ...,   266,   940,   271],
        [    0,   365,  1941,  ..., 38029,  1790, 44066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0251e-07,  4.0934e-06, -2.6906e-05,  ..., -2.9042e-05,
          4.9028e-05,  2.1077e-05],
        [-9.0301e-06, -6.2883e-06,  9.1828e-07,  ..., -7.5251e-06,
         -8.7917e-07, -3.3677e-06],
        [-9.0748e-06, -6.3330e-06,  9.2573e-07,  ..., -7.5549e-06,
         -8.8476e-07, -3.3975e-06],
        [-1.3828e-05, -9.6560e-06,  1.4175e-06,  ..., -1.1533e-05,
         -1.3430e-06, -5.1707e-06],
        [-2.2173e-05, -1.5438e-05,  2.2501e-06,  ..., -1.8477e-05,
         -2.1681e-06, -8.2701e-06]], device='cuda:0')
Loss: 1.1859854459762573


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0,  1688,   790,  ...,   546,   696,    12],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7495e-05,  1.1450e-05, -6.1342e-05,  ..., -1.5436e-05,
          7.7047e-05,  6.9650e-05],
        [-1.2100e-05, -8.4490e-06,  1.2480e-06,  ..., -1.0058e-05,
         -1.1753e-06, -4.4852e-06],
        [ 3.6194e-04,  3.0028e-04, -2.7116e-05,  ...,  3.2054e-04,
          2.9010e-05,  1.3384e-04],
        [-1.8507e-05, -1.2964e-05,  1.9204e-06,  ..., -1.5408e-05,
         -1.7937e-06, -6.8843e-06],
        [-2.9683e-05, -2.0742e-05,  3.0547e-06,  ..., -2.4676e-05,
         -2.8983e-06, -1.1012e-05]], device='cuda:0')
Loss: 1.1796813011169434


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  996, 2226,  ...,  516, 3470,   14],
        [   0, 1561,   14,  ..., 4433,  352, 1561]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0796e-05,  3.2120e-05, -6.7764e-05,  ...,  3.1140e-05,
          7.7047e-05,  1.0930e-04],
        [-1.5110e-05, -1.0535e-05,  1.5758e-06,  ..., -1.2547e-05,
         -1.4845e-06, -5.6252e-06],
        [ 3.5890e-04,  2.9818e-04, -2.6785e-05,  ...,  3.1804e-04,
          2.8699e-05,  1.3270e-04],
        [-2.3156e-05, -1.6168e-05,  2.4270e-06,  ..., -1.9222e-05,
         -2.2687e-06, -8.6352e-06],
        [-3.7074e-05, -2.5839e-05,  3.8557e-06,  ..., -3.0786e-05,
         -3.6582e-06, -1.3798e-05]], device='cuda:0')
Loss: 1.1663614511489868


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    14,  7870,  ...,   284,   830,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9733e-05,  3.6020e-05, -6.9692e-05,  ...,  4.0760e-05,
          7.1051e-05,  1.4083e-04],
        [-1.8105e-05, -1.2636e-05,  1.9092e-06,  ..., -1.5005e-05,
         -1.7881e-06, -6.7428e-06],
        [ 4.6370e-04,  3.7023e-04, -4.0380e-05,  ...,  3.9249e-04,
          4.1602e-05,  1.5555e-04],
        [-2.7776e-05, -1.9401e-05,  2.9448e-06,  ..., -2.3022e-05,
         -2.7362e-06, -1.0364e-05],
        [-4.4495e-05, -3.1024e-05,  4.6752e-06,  ..., -3.6865e-05,
         -4.4070e-06, -1.6555e-05]], device='cuda:0')
Loss: 1.158100962638855


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,  9582,  3645,  ...,  1027,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2014e-05,  2.0618e-05, -8.5224e-05,  ...,  4.0697e-05,
          8.4480e-05,  1.1909e-04],
        [-2.1145e-05, -1.4752e-05,  2.2221e-06,  ..., -1.7539e-05,
         -2.0973e-06, -7.8380e-06],
        [ 4.6066e-04,  3.6811e-04, -4.0067e-05,  ...,  3.8996e-04,
          4.1291e-05,  1.5446e-04],
        [-3.2425e-05, -2.2635e-05,  3.4217e-06,  ..., -2.6897e-05,
         -3.2075e-06, -1.2033e-05],
        [-5.2005e-05, -3.6269e-05,  5.4426e-06,  ..., -4.3124e-05,
         -5.1707e-06, -1.9252e-05]], device='cuda:0')
Loss: 1.1872515678405762


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0, 24063,   717,  ...,  2228,  1416,     9],
        [    0,   741,   300,  ...,    83,  7111,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3337e-05,  7.0467e-06, -6.6041e-05,  ...,  7.4528e-05,
          6.5471e-05,  1.2941e-04],
        [-2.4155e-05, -1.6883e-05,  2.5276e-06,  ..., -2.0027e-05,
         -2.3991e-06, -8.9407e-06],
        [ 4.5759e-04,  3.6595e-04, -3.9754e-05,  ...,  3.8743e-04,
          4.0984e-05,  1.5333e-04],
        [-3.7044e-05, -2.5898e-05,  3.8929e-06,  ..., -3.0711e-05,
         -3.6713e-06, -1.3731e-05],
        [-5.9396e-05, -4.1485e-05,  6.1952e-06,  ..., -4.9233e-05,
         -5.9120e-06, -2.1964e-05]], device='cuda:0')
Loss: 1.1617683172225952
Graident accumulation at epoch 0, step 247, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4890e-05, -4.4048e-05,  1.0103e-05,  ...,  5.9920e-05,
         -6.1560e-05,  1.1495e-05],
        [-2.7170e-05, -1.9292e-05,  3.0751e-06,  ..., -2.1761e-05,
         -2.6806e-06, -1.0542e-05],
        [ 5.5930e-05,  4.2503e-05, -6.1717e-06,  ...,  4.9549e-05,
          3.8019e-06,  1.9962e-05],
        [-2.4263e-05, -1.4807e-05,  4.2508e-06,  ..., -1.8212e-05,
         -2.5850e-06, -8.2683e-06],
        [-6.0559e-05, -4.2942e-05,  6.0018e-06,  ..., -4.7947e-05,
         -5.9894e-06, -2.3380e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7513e-08, 1.4673e-08, 1.7402e-08,  ..., 1.9973e-08, 3.1613e-08,
         6.5187e-09],
        [2.7803e-11, 1.7543e-11, 9.7349e-13,  ..., 1.8216e-11, 6.0513e-13,
         3.7326e-12],
        [3.1906e-10, 2.1537e-10, 1.1671e-11,  ..., 2.9522e-10, 5.0831e-12,
         6.8107e-11],
        [1.7644e-10, 2.5384e-10, 4.6896e-12,  ..., 1.3055e-10, 6.5868e-12,
         4.6525e-11],
        [1.1346e-10, 6.5154e-11, 4.9024e-12,  ..., 7.4866e-11, 1.9889e-12,
         1.6828e-11]], device='cuda:0')
optimizer state dict: 31.0
lr: [1.7842807614798848e-05, 1.7842807614798848e-05]
scheduler_last_epoch: 31


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[   0,  266, 3536,  ...,  266, 1883,  266],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4217e-05,  6.7660e-06,  3.1634e-05,  ...,  2.0766e-06,
         -3.8548e-05, -1.1802e-05],
        [-2.9504e-06, -2.0564e-06,  3.2969e-07,  ..., -2.4736e-06,
         -2.6077e-07, -1.1176e-06],
        [-3.0398e-06, -2.1160e-06,  3.4086e-07,  ..., -2.5481e-06,
         -2.7008e-07, -1.1548e-06],
        [-4.5598e-06, -3.1739e-06,  5.1409e-07,  ..., -3.8147e-06,
         -4.0047e-07, -1.7285e-06],
        [-7.2420e-06, -5.0366e-06,  8.0839e-07,  ..., -6.0499e-06,
         -6.4448e-07, -2.7418e-06]], device='cuda:0')
Loss: 1.16513991355896


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3714e-05, -4.4381e-06,  2.8676e-05,  ..., -1.8380e-05,
         -1.7195e-05, -1.9492e-05],
        [-5.9456e-06, -4.1574e-06,  6.5379e-07,  ..., -4.9919e-06,
         -5.6066e-07, -2.2575e-06],
        [-6.0797e-06, -4.2468e-06,  6.7055e-07,  ..., -5.0962e-06,
         -5.7556e-07, -2.3171e-06],
        [-9.1195e-06, -6.3777e-06,  1.0096e-06,  ..., -7.6592e-06,
         -8.5495e-07, -3.4720e-06],
        [-1.4484e-05, -1.0133e-05,  1.5907e-06,  ..., -1.2130e-05,
         -1.3709e-06, -5.5134e-06]], device='cuda:0')
Loss: 1.1633474826812744


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   600,  9092,  ...,   554,  1485,   328],
        [    0, 42306,   278,  ...,  1110,  3427,  4224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3242e-05, -1.6062e-05,  2.4118e-05,  ..., -6.1895e-06,
         -2.6427e-06, -1.4681e-06],
        [-8.9407e-06, -6.2287e-06,  9.6858e-07,  ..., -7.5251e-06,
         -8.3447e-07, -3.3751e-06],
        [-9.1344e-06, -6.3628e-06,  9.9279e-07,  ..., -7.6741e-06,
         -8.5682e-07, -3.4571e-06],
        [-1.3679e-05, -9.5367e-06,  1.4938e-06,  ..., -1.1533e-05,
         -1.2722e-06, -5.1782e-06],
        [-2.1785e-05, -1.5169e-05,  2.3544e-06,  ..., -1.8299e-05,
         -2.0415e-06, -8.2254e-06]], device='cuda:0')
Loss: 1.172991156578064


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,   278,  7524,  ...,  1288,   669,   352],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2709e-05, -1.4607e-05,  4.1371e-07,  ...,  2.9778e-05,
         -1.3318e-05,  5.7074e-06],
        [-1.1906e-05, -8.2850e-06,  1.2945e-06,  ..., -1.0028e-05,
         -1.1101e-06, -4.5002e-06],
        [-1.2144e-05, -8.4490e-06,  1.3243e-06,  ..., -1.0207e-05,
         -1.1344e-06, -4.5970e-06],
        [-1.8209e-05, -1.2696e-05,  1.9968e-06,  ..., -1.5378e-05,
         -1.6913e-06, -6.9067e-06],
        [-2.8968e-05, -2.0176e-05,  3.1441e-06,  ..., -2.4378e-05,
         -2.7083e-06, -1.0952e-05]], device='cuda:0')
Loss: 1.1644084453582764


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,   12,  616,  ...,  278,  266, 2907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3878e-05, -1.0306e-05, -3.7927e-05,  ...,  3.4118e-05,
         -2.2051e-05, -3.7492e-05],
        [-1.4886e-05, -1.0341e-05,  1.6354e-06,  ..., -1.2532e-05,
         -1.4044e-06, -5.6103e-06],
        [-1.5169e-05, -1.0550e-05,  1.6708e-06,  ..., -1.2740e-05,
         -1.4324e-06, -5.7295e-06],
        [-2.2799e-05, -1.5885e-05,  2.5257e-06,  ..., -1.9222e-05,
         -2.1439e-06, -8.6278e-06],
        [-3.6180e-05, -2.5183e-05,  3.9674e-06,  ..., -3.0428e-05,
         -3.4235e-06, -1.3649e-05]], device='cuda:0')
Loss: 1.1699435710906982


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,    14,   469,  ...,   367,  2564,   368],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4729e-06,  1.6209e-06, -8.3591e-05,  ...,  3.7394e-05,
          7.5503e-07, -2.9722e-05],
        [-1.7852e-05, -1.2442e-05,  1.9912e-06,  ..., -1.5035e-05,
         -1.7062e-06, -6.7353e-06],
        [-1.8179e-05, -1.2681e-05,  2.0321e-06,  ..., -1.5289e-05,
         -1.7397e-06, -6.8694e-06],
        [-2.7359e-05, -1.9103e-05,  3.0734e-06,  ..., -2.3067e-05,
         -2.6058e-06, -1.0349e-05],
        [-4.3362e-05, -3.0249e-05,  4.8243e-06,  ..., -3.6478e-05,
         -4.1537e-06, -1.6361e-05]], device='cuda:0')
Loss: 1.177926778793335


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  342,  516,  ...,   12,  729, 3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2271e-07,  8.7675e-06, -6.5032e-05,  ...,  5.4040e-05,
          4.2947e-05, -1.3365e-05],
        [-2.0862e-05, -1.4558e-05,  2.3022e-06,  ..., -1.7539e-05,
         -2.0098e-06, -7.8529e-06],
        [-2.1234e-05, -1.4827e-05,  2.3469e-06,  ..., -1.7822e-05,
         -2.0489e-06, -8.0019e-06],
        [-3.1978e-05, -2.2337e-05,  3.5502e-06,  ..., -2.6911e-05,
         -3.0696e-06, -1.2055e-05],
        [-5.0724e-05, -3.5405e-05,  5.5768e-06,  ..., -4.2558e-05,
         -4.8988e-06, -1.9073e-05]], device='cuda:0')
Loss: 1.1734110116958618


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,  1911,   679,  ...,    19,  3737,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5665e-06,  2.6161e-05, -7.9894e-05,  ...,  6.3306e-05,
          4.2060e-05, -8.8961e-06],
        [-2.3827e-05, -1.6600e-05,  2.5928e-06,  ..., -2.0042e-05,
         -2.2724e-06, -8.9407e-06],
        [-2.4274e-05, -1.6913e-05,  2.6450e-06,  ..., -2.0385e-05,
         -2.3171e-06, -9.1121e-06],
        [-3.6538e-05, -2.5481e-05,  4.0010e-06,  ..., -3.0756e-05,
         -3.4720e-06, -1.3724e-05],
        [-5.7995e-05, -4.0412e-05,  6.2846e-06,  ..., -4.8697e-05,
         -5.5432e-06, -2.1726e-05]], device='cuda:0')
Loss: 1.1768982410430908
Graident accumulation at epoch 0, step 255, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4945e-05, -3.7027e-05,  1.1035e-06,  ...,  6.0259e-05,
         -5.1198e-05,  9.4559e-06],
        [-2.6836e-05, -1.9023e-05,  3.0269e-06,  ..., -2.1589e-05,
         -2.6398e-06, -1.0382e-05],
        [ 4.7910e-05,  3.6562e-05, -5.2901e-06,  ...,  4.2556e-05,
          3.1900e-06,  1.7055e-05],
        [-2.5491e-05, -1.5875e-05,  4.2258e-06,  ..., -1.9467e-05,
         -2.6737e-06, -8.8139e-06],
        [-6.0303e-05, -4.2689e-05,  6.0301e-06,  ..., -4.8022e-05,
         -5.9448e-06, -2.3215e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7465e-08, 1.4659e-08, 1.7391e-08,  ..., 1.9957e-08, 3.1584e-08,
         6.5123e-09],
        [2.8343e-11, 1.7801e-11, 9.7924e-13,  ..., 1.8600e-11, 6.0968e-13,
         3.8088e-12],
        [3.1933e-10, 2.1544e-10, 1.1666e-11,  ..., 2.9534e-10, 5.0834e-12,
         6.8122e-11],
        [1.7760e-10, 2.5424e-10, 4.7009e-12,  ..., 1.3137e-10, 6.5923e-12,
         4.6667e-11],
        [1.1671e-10, 6.6722e-11, 4.9370e-12,  ..., 7.7163e-11, 2.0177e-12,
         1.7284e-11]], device='cuda:0')
optimizer state dict: 32.0
lr: [1.7687041437173095e-05, 1.7687041437173095e-05]
scheduler_last_epoch: 32


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8430e-09,  5.5570e-06, -9.5339e-06,  ...,  1.6675e-05,
         -2.7860e-06,  1.1917e-05],
        [-2.9504e-06, -2.0415e-06,  3.5763e-07,  ..., -2.5034e-06,
         -2.8126e-07, -1.1101e-06],
        [ 9.0519e-05,  5.9863e-05, -7.3205e-06,  ...,  6.1886e-05,
          1.8822e-06,  1.8948e-05],
        [-4.5598e-06, -3.1590e-06,  5.5507e-07,  ..., -3.8743e-06,
         -4.3400e-07, -1.7136e-06],
        [-7.1228e-06, -4.9174e-06,  8.6054e-07,  ..., -6.0499e-06,
         -6.8173e-07, -2.6673e-06]], device='cuda:0')
Loss: 1.173764705657959


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[    0,  4902,   518,  ...,  5493,  3227,   278],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3482e-05, -5.1255e-05, -1.5190e-06,  ...,  1.4361e-05,
          1.0826e-05,  3.0706e-05],
        [-5.9307e-06, -4.0829e-06,  7.2084e-07,  ..., -5.0068e-06,
         -6.1840e-07, -2.2426e-06],
        [ 8.7435e-05,  5.7747e-05, -6.9461e-06,  ...,  5.9294e-05,
          1.5339e-06,  1.7779e-05],
        [-9.1791e-06, -6.3330e-06,  1.1213e-06,  ..., -7.7784e-06,
         -9.5554e-07, -3.4645e-06],
        [-1.4365e-05, -9.8944e-06,  1.7397e-06,  ..., -1.2130e-05,
         -1.5013e-06, -5.4091e-06]], device='cuda:0')
Loss: 1.179901123046875


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[   0, 3159,  278,  ...,  266, 2545,  863],
        [   0,  266, 2057,  ...,   88, 1801,   66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2665e-05, -4.7802e-05, -1.4142e-05,  ...,  1.5540e-05,
          3.2961e-05,  5.4042e-05],
        [-8.8811e-06, -6.1095e-06,  1.0785e-06,  ..., -7.4655e-06,
         -8.8289e-07, -3.3528e-06],
        [ 8.4395e-05,  5.5646e-05, -6.5755e-06,  ...,  5.6760e-05,
          1.2620e-06,  1.6631e-05],
        [-1.3769e-05, -9.4920e-06,  1.6838e-06,  ..., -1.1593e-05,
         -1.3635e-06, -5.1931e-06],
        [-2.1547e-05, -1.4842e-05,  2.6077e-06,  ..., -1.8090e-05,
         -2.1420e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1767663955688477


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2651e-05, -3.4460e-05, -5.6254e-05,  ...,  3.3103e-06,
          8.1970e-05,  6.1763e-05],
        [-1.1832e-05, -8.1509e-06,  1.4417e-06,  ..., -9.9689e-06,
         -1.2014e-06, -4.4778e-06],
        [ 8.1400e-05,  5.3575e-05, -6.2067e-06,  ...,  5.4212e-05,
          9.3973e-07,  1.5491e-05],
        [-1.8299e-05, -1.2636e-05,  2.2426e-06,  ..., -1.5438e-05,
         -1.8515e-06, -6.9141e-06],
        [-2.8580e-05, -1.9699e-05,  3.4682e-06,  ..., -2.4050e-05,
         -2.9020e-06, -1.0774e-05]], device='cuda:0')
Loss: 1.1785144805908203


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6907e-05, -1.9801e-05, -5.8283e-05,  ..., -8.2808e-07,
          1.0468e-04,  1.0488e-04],
        [-1.4797e-05, -1.0207e-05,  1.7993e-06,  ..., -1.2502e-05,
         -1.4864e-06, -5.5805e-06],
        [ 7.8345e-05,  5.1459e-05, -5.8397e-06,  ...,  5.1605e-05,
          6.4730e-07,  1.4359e-05],
        [-2.2858e-05, -1.5795e-05,  2.7940e-06,  ..., -1.9342e-05,
         -2.2892e-06, -8.6129e-06],
        [-3.5733e-05, -2.4647e-05,  4.3251e-06,  ..., -3.0160e-05,
         -3.5912e-06, -1.3441e-05]], device='cuda:0')
Loss: 1.192333459854126


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[   0,  895, 4110,  ..., 1578, 1245,   13],
        [   0,   22, 2577,  ..., 4970,    9, 3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0394e-05, -7.1425e-06, -7.8034e-05,  ..., -1.9796e-05,
          1.2707e-04,  1.1728e-04],
        [-1.7777e-05, -1.2249e-05,  2.1365e-06,  ..., -1.4991e-05,
         -1.7695e-06, -6.7055e-06],
        [ 7.5276e-05,  4.9358e-05, -5.4914e-06,  ...,  4.9042e-05,
          3.5486e-07,  1.3204e-05],
        [-2.7448e-05, -1.8954e-05,  3.3192e-06,  ..., -2.3186e-05,
         -2.7232e-06, -1.0341e-05],
        [-4.2915e-05, -2.9564e-05,  5.1335e-06,  ..., -3.6150e-05,
         -4.2729e-06, -1.6138e-05]], device='cuda:0')
Loss: 1.187842607498169


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0, 3825, 1626,  ..., 5096, 3775,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3708e-05, -2.5683e-05, -1.0470e-04,  ..., -1.5697e-05,
          1.0016e-04,  1.4832e-04],
        [-2.0742e-05, -1.4290e-05,  2.4661e-06,  ..., -1.7479e-05,
         -2.0396e-06, -7.8306e-06],
        [ 7.2221e-05,  4.7242e-05, -5.1505e-06,  ...,  4.6479e-05,
          7.7326e-08,  1.2049e-05],
        [-3.2067e-05, -2.2128e-05,  3.8333e-06,  ..., -2.7061e-05,
         -3.1404e-06, -1.2085e-05],
        [-5.0068e-05, -3.4511e-05,  5.9269e-06,  ..., -4.2170e-05,
         -4.9248e-06, -1.8850e-05]], device='cuda:0')
Loss: 1.1670798063278198


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,    13, 41550,  ...,    12,   546,  1996],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9809e-05, -2.0208e-05, -1.0786e-04,  ..., -3.2349e-05,
          1.0834e-04,  1.6490e-04],
        [-2.3693e-05, -1.6347e-05,  2.8070e-06,  ..., -1.9968e-05,
         -2.3358e-06, -8.9481e-06],
        [ 6.9181e-05,  4.5126e-05, -4.7985e-06,  ...,  4.3901e-05,
         -2.2815e-07,  1.0894e-05],
        [-3.6597e-05, -2.5287e-05,  4.3586e-06,  ..., -3.0875e-05,
         -3.5949e-06, -1.3798e-05],
        [-5.7131e-05, -3.9428e-05,  6.7428e-06,  ..., -4.8161e-05,
         -5.6364e-06, -2.1517e-05]], device='cuda:0')
Loss: 1.1667304039001465
Graident accumulation at epoch 0, step 263, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6431e-05, -3.5345e-05, -9.7928e-06,  ...,  5.0998e-05,
         -3.5244e-05,  2.5001e-05],
        [-2.6521e-05, -1.8755e-05,  3.0049e-06,  ..., -2.1427e-05,
         -2.6094e-06, -1.0238e-05],
        [ 5.0037e-05,  3.7418e-05, -5.2409e-06,  ...,  4.2690e-05,
          2.8482e-06,  1.6439e-05],
        [-2.6601e-05, -1.6816e-05,  4.2391e-06,  ..., -2.0608e-05,
         -2.7658e-06, -9.3124e-06],
        [-5.9986e-05, -4.2363e-05,  6.1014e-06,  ..., -4.8036e-05,
         -5.9139e-06, -2.3045e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7428e-08, 1.4645e-08, 1.7385e-08,  ..., 1.9938e-08, 3.1564e-08,
         6.5329e-09],
        [2.8876e-11, 1.8051e-11, 9.8614e-13,  ..., 1.8980e-11, 6.1453e-13,
         3.8850e-12],
        [3.2380e-10, 2.1726e-10, 1.1678e-11,  ..., 2.9698e-10, 5.0783e-12,
         6.8172e-11],
        [1.7876e-10, 2.5462e-10, 4.7152e-12,  ..., 1.3219e-10, 6.5986e-12,
         4.6811e-11],
        [1.1986e-10, 6.8210e-11, 4.9775e-12,  ..., 7.9405e-11, 2.0474e-12,
         1.7729e-11]], device='cuda:0')
optimizer state dict: 33.0
lr: [1.7526576850912724e-05, 1.7526576850912724e-05]
scheduler_last_epoch: 33


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,  1106,   259,  ...,   271,   679,   382]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1423e-05,  1.4340e-05, -2.5028e-05,  ...,  3.5455e-06,
         -1.8421e-06, -3.5337e-05],
        [-2.9355e-06, -2.0266e-06,  3.5763e-07,  ..., -2.5183e-06,
         -2.9430e-07, -1.1176e-06],
        [-3.0398e-06, -2.1160e-06,  3.7067e-07,  ..., -2.6226e-06,
         -3.0547e-07, -1.1623e-06],
        [-4.5002e-06, -3.1143e-06,  5.5134e-07,  ..., -3.8743e-06,
         -4.4890e-07, -1.7136e-06],
        [-7.0333e-06, -4.8578e-06,  8.5309e-07,  ..., -6.0201e-06,
         -7.0408e-07, -2.6673e-06]], device='cuda:0')
Loss: 1.18299400806427


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4828e-05,  4.6304e-06,  9.1063e-06,  ...,  1.9664e-05,
          1.3257e-05, -5.1443e-05],
        [-5.9307e-06, -4.0829e-06,  7.3947e-07,  ..., -5.0515e-06,
         -5.8860e-07, -2.2575e-06],
        [-6.1691e-06, -4.2617e-06,  7.7114e-07,  ..., -5.2750e-06,
         -6.1467e-07, -2.3544e-06],
        [-9.0897e-06, -6.2585e-06,  1.1399e-06,  ..., -7.7486e-06,
         -8.9779e-07, -3.4571e-06],
        [-1.4186e-05, -9.7752e-06,  1.7658e-06,  ..., -1.2100e-05,
         -1.4119e-06, -5.3942e-06]], device='cuda:0')
Loss: 1.177659511566162


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,   266,  2374,  ...,  1551,   518,   638],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6776e-06,  3.7774e-06,  1.0216e-05,  ...,  1.5796e-05,
          2.6811e-05, -1.0028e-05],
        [-8.9258e-06, -6.1393e-06,  1.1288e-06,  ..., -7.5996e-06,
         -9.4436e-07, -3.3751e-06],
        [-9.2536e-06, -6.3777e-06,  1.1716e-06,  ..., -7.8976e-06,
         -9.8161e-07, -3.5092e-06],
        [-1.3679e-05, -9.4026e-06,  1.7360e-06,  ..., -1.1653e-05,
         -1.4417e-06, -5.1633e-06],
        [-2.1219e-05, -1.4603e-05,  2.6748e-06,  ..., -1.8060e-05,
         -2.2464e-06, -8.0168e-06]], device='cuda:0')
Loss: 1.1486340761184692


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[    0,   300,  5201,  ...,  1997,  7423,   417],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1401e-05,  2.2807e-06, -4.7445e-07,  ...,  3.9540e-05,
          2.0757e-05,  7.0908e-06],
        [-1.1876e-05, -8.1658e-06,  1.5367e-06,  ..., -1.0118e-05,
         -1.2256e-06, -4.5151e-06],
        [-1.2308e-05, -8.4639e-06,  1.5944e-06,  ..., -1.0505e-05,
         -1.2722e-06, -4.6864e-06],
        [-1.8209e-05, -1.2502e-05,  2.3618e-06,  ..., -1.5527e-05,
         -1.8720e-06, -6.9067e-06],
        [-2.8193e-05, -1.9372e-05,  3.6359e-06,  ..., -2.4021e-05,
         -2.9132e-06, -1.0699e-05]], device='cuda:0')
Loss: 1.183121919631958


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1401e-05,  1.8911e-06, -3.8924e-06,  ...,  3.9703e-05,
          2.5686e-05,  4.8325e-07],
        [-1.4842e-05, -1.0207e-05,  1.9204e-06,  ..., -1.2651e-05,
         -1.5311e-06, -5.6401e-06],
        [-1.5348e-05, -1.0565e-05,  1.9874e-06,  ..., -1.3113e-05,
         -1.5832e-06, -5.8413e-06],
        [-2.2799e-05, -1.5661e-05,  2.9542e-06,  ..., -1.9461e-05,
         -2.3413e-06, -8.6427e-06],
        [-3.5226e-05, -2.4229e-05,  4.5411e-06,  ..., -3.0041e-05,
         -3.6359e-06, -1.3366e-05]], device='cuda:0')
Loss: 1.1732097864151


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   518,  9048,  ...,  1354,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7139e-05, -8.8916e-06,  5.6014e-06,  ...,  2.5816e-05,
          3.4795e-05,  3.1372e-05],
        [-1.7762e-05, -1.2234e-05,  2.3190e-06,  ..., -1.5140e-05,
         -1.8217e-06, -6.7800e-06],
        [-1.8358e-05, -1.2666e-05,  2.4009e-06,  ..., -1.5676e-05,
         -1.8831e-06, -7.0184e-06],
        [-2.7299e-05, -1.8805e-05,  3.5726e-06,  ..., -2.3305e-05,
         -2.7884e-06, -1.0401e-05],
        [-4.2170e-05, -2.9057e-05,  5.4874e-06,  ..., -3.5942e-05,
         -4.3288e-06, -1.6063e-05]], device='cuda:0')
Loss: 1.1666499376296997


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0,  1485,   271,  ...,  6359,  1799,  5442],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4039e-05, -7.2667e-06, -7.7074e-06,  ...,  7.1924e-06,
          5.0257e-05,  2.4781e-05],
        [-2.0713e-05, -1.4305e-05,  2.6990e-06,  ..., -1.7688e-05,
         -2.1271e-06, -7.9274e-06],
        [-2.1398e-05, -1.4797e-05,  2.7940e-06,  ..., -1.8299e-05,
         -2.1979e-06, -8.1956e-06],
        [-3.1799e-05, -2.1979e-05,  4.1574e-06,  ..., -2.7210e-05,
         -3.2540e-06, -1.2152e-05],
        [-4.9144e-05, -3.3945e-05,  6.3814e-06,  ..., -4.1932e-05,
         -5.0515e-06, -1.8761e-05]], device='cuda:0')
Loss: 1.171975016593933


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  328, 6875,  ...,  369,  654,  300],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0812e-05, -2.5128e-06, -1.9842e-05,  ...,  1.4756e-05,
          6.1416e-05,  6.2429e-05],
        [-2.3678e-05, -1.6347e-05,  3.0864e-06,  ..., -2.0206e-05,
         -2.4233e-06, -9.0450e-06],
        [-2.4468e-05, -1.6913e-05,  3.1944e-06,  ..., -2.0906e-05,
         -2.5053e-06, -9.3505e-06],
        [-3.6359e-05, -2.5123e-05,  4.7572e-06,  ..., -3.1084e-05,
         -3.7104e-06, -1.3873e-05],
        [-5.6207e-05, -3.8803e-05,  7.3016e-06,  ..., -4.7922e-05,
         -5.7593e-06, -2.1413e-05]], device='cuda:0')
Loss: 1.1747244596481323
Graident accumulation at epoch 0, step 271, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.1869e-05, -3.2062e-05, -1.0798e-05,  ...,  4.7374e-05,
         -2.5578e-05,  2.8743e-05],
        [-2.6237e-05, -1.8514e-05,  3.0130e-06,  ..., -2.1305e-05,
         -2.5908e-06, -1.0119e-05],
        [ 4.2586e-05,  3.1985e-05, -4.3974e-06,  ...,  3.6331e-05,
          2.3128e-06,  1.3860e-05],
        [-2.7577e-05, -1.7647e-05,  4.2909e-06,  ..., -2.1655e-05,
         -2.8603e-06, -9.7684e-06],
        [-5.9608e-05, -4.2007e-05,  6.2214e-06,  ..., -4.8024e-05,
         -5.8985e-06, -2.2882e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7382e-08, 1.4630e-08, 1.7368e-08,  ..., 1.9918e-08, 3.1536e-08,
         6.5303e-09],
        [2.9408e-11, 1.8300e-11, 9.9468e-13,  ..., 1.9369e-11, 6.1979e-13,
         3.9630e-12],
        [3.2407e-10, 2.1733e-10, 1.1676e-11,  ..., 2.9712e-10, 5.0795e-12,
         6.8191e-11],
        [1.7990e-10, 2.5500e-10, 4.7332e-12,  ..., 1.3303e-10, 6.6058e-12,
         4.6956e-11],
        [1.2290e-10, 6.9647e-11, 5.0259e-12,  ..., 8.1622e-11, 2.0785e-12,
         1.8170e-11]], device='cuda:0')
optimizer state dict: 34.0
lr: [1.73615119338288e-05, 1.73615119338288e-05]
scheduler_last_epoch: 34


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1379e-05,  2.2750e-05,  1.1303e-05,  ..., -1.4480e-05,
         -9.1760e-06,  3.4199e-05],
        [-2.9355e-06, -2.0415e-06,  4.0233e-07,  ..., -2.4736e-06,
         -3.3528e-07, -1.1027e-06],
        [-3.0547e-06, -2.1160e-06,  4.1537e-07,  ..., -2.5630e-06,
         -3.4831e-07, -1.1474e-06],
        [-4.5002e-06, -3.1292e-06,  6.1467e-07,  ..., -3.7849e-06,
         -5.1409e-07, -1.6913e-06],
        [-6.9439e-06, -4.8280e-06,  9.4250e-07,  ..., -5.8413e-06,
         -7.9349e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1425330638885498


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 7185,  328,  ..., 1427, 1477, 1061],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2763e-05,  1.3421e-05,  4.2589e-05,  ...,  9.8475e-06,
         -1.1289e-05,  3.6198e-05],
        [-5.8562e-06, -4.0680e-06,  7.9907e-07,  ..., -4.9621e-06,
         -6.3889e-07, -2.1905e-06],
        [-6.0797e-06, -4.2170e-06,  8.2701e-07,  ..., -5.1558e-06,
         -6.6310e-07, -2.2724e-06],
        [-9.0003e-06, -6.2436e-06,  1.2256e-06,  ..., -7.6294e-06,
         -9.7789e-07, -3.3602e-06],
        [-1.3858e-05, -9.6262e-06,  1.8775e-06,  ..., -1.1742e-05,
         -1.5125e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.169439673423767


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3205e-05,  1.0411e-05,  5.9618e-05,  ...,  3.4742e-05,
          6.3633e-06,  7.2665e-05],
        [-8.7768e-06, -6.1095e-06,  1.2182e-06,  ..., -7.4655e-06,
         -9.5926e-07, -3.3304e-06],
        [-9.1046e-06, -6.3330e-06,  1.2629e-06,  ..., -7.7486e-06,
         -9.9465e-07, -3.4496e-06],
        [-1.3530e-05, -9.4026e-06,  1.8775e-06,  ..., -1.1504e-05,
         -1.4734e-06, -5.1185e-06],
        [-2.0742e-05, -1.4424e-05,  2.8610e-06,  ..., -1.7643e-05,
         -2.2687e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.1699001789093018


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0, 26473,  2117,  ...,    13,  3292,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1660e-05, -3.1035e-06,  3.2658e-05,  ...,  4.0133e-05,
         -2.9056e-05,  8.3967e-05],
        [-1.1727e-05, -8.1211e-06,  1.6447e-06,  ..., -9.9540e-06,
         -1.2834e-06, -4.4554e-06],
        [-1.2159e-05, -8.4192e-06,  1.7043e-06,  ..., -1.0312e-05,
         -1.3299e-06, -4.6119e-06],
        [-1.8090e-05, -1.2517e-05,  2.5369e-06,  ..., -1.5348e-05,
         -1.9725e-06, -6.8545e-06],
        [-2.7686e-05, -1.9193e-05,  3.8594e-06,  ..., -2.3484e-05,
         -3.0361e-06, -1.0505e-05]], device='cuda:0')
Loss: 1.1591320037841797


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[    0,   685,  2461,  ...,   287,   298,  7943],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7618e-05, -3.3406e-05,  2.9813e-05,  ...,  4.1400e-05,
         -4.2536e-05,  9.3357e-05],
        [-1.4648e-05, -1.0133e-05,  2.0284e-06,  ..., -1.2442e-05,
         -1.6205e-06, -5.5805e-06],
        [-1.5199e-05, -1.0505e-05,  2.1029e-06,  ..., -1.2890e-05,
         -1.6801e-06, -5.7817e-06],
        [-2.2620e-05, -1.5631e-05,  3.1330e-06,  ..., -1.9193e-05,
         -2.4904e-06, -8.5905e-06],
        [-3.4600e-05, -2.3961e-05,  4.7684e-06,  ..., -2.9355e-05,
         -3.8333e-06, -1.3158e-05]], device='cuda:0')
Loss: 1.1683323383331299


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0735e-06, -6.2444e-05,  4.7445e-05,  ...,  3.6561e-05,
         -5.9167e-05,  9.4177e-05],
        [-1.7583e-05, -1.2174e-05,  2.4382e-06,  ..., -1.4961e-05,
         -1.9707e-06, -6.7130e-06],
        [-1.8254e-05, -1.2636e-05,  2.5313e-06,  ..., -1.5512e-05,
         -2.0433e-06, -6.9588e-06],
        [-2.7150e-05, -1.8775e-05,  3.7663e-06,  ..., -2.3067e-05,
         -3.0268e-06, -1.0327e-05],
        [-4.1485e-05, -2.8759e-05,  5.7295e-06,  ..., -3.5286e-05,
         -4.6529e-06, -1.5810e-05]], device='cuda:0')
Loss: 1.1919209957122803


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0, 2310,  292,  ...,  462,  508,  586]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4694e-05, -4.5154e-05,  2.8173e-05,  ...,  4.1069e-05,
         -4.7754e-05,  8.2188e-05],
        [-2.0489e-05, -1.4201e-05,  2.8443e-06,  ..., -1.7449e-05,
         -2.2985e-06, -7.8306e-06],
        [-2.1264e-05, -1.4737e-05,  2.9542e-06,  ..., -1.8090e-05,
         -2.3842e-06, -8.1211e-06],
        [-3.1620e-05, -2.1905e-05,  4.3958e-06,  ..., -2.6911e-05,
         -3.5297e-06, -1.2055e-05],
        [-4.8339e-05, -3.3528e-05,  6.6906e-06,  ..., -4.1157e-05,
         -5.4277e-06, -1.8448e-05]], device='cuda:0')
Loss: 1.16437828540802


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  292,   33,  ...,  352,  266, 9129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6867e-05,  1.4703e-06,  2.5866e-05,  ...,  2.5254e-05,
         -4.4737e-05,  1.1372e-04],
        [-2.3440e-05, -1.6242e-05,  3.2354e-06,  ..., -1.9968e-05,
         -2.6282e-06, -8.9556e-06],
        [-2.4304e-05, -1.6838e-05,  3.3565e-06,  ..., -2.0668e-05,
         -2.7232e-06, -9.2760e-06],
        [-3.6150e-05, -2.5064e-05,  4.9993e-06,  ..., -3.0786e-05,
         -4.0364e-06, -1.3784e-05],
        [-5.5283e-05, -3.8356e-05,  7.6070e-06,  ..., -4.7058e-05,
         -6.2063e-06, -2.1085e-05]], device='cuda:0')
Loss: 1.1686780452728271
Graident accumulation at epoch 0, step 279, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.6369e-05, -2.8709e-05, -7.1314e-06,  ...,  4.5162e-05,
         -2.7494e-05,  3.7241e-05],
        [-2.5957e-05, -1.8287e-05,  3.0353e-06,  ..., -2.1171e-05,
         -2.5945e-06, -1.0003e-05],
        [ 3.5897e-05,  2.7103e-05, -3.6220e-06,  ...,  3.0631e-05,
          1.8092e-06,  1.1546e-05],
        [-2.8434e-05, -1.8388e-05,  4.3618e-06,  ..., -2.2568e-05,
         -2.9779e-06, -1.0170e-05],
        [-5.9175e-05, -4.1642e-05,  6.3600e-06,  ..., -4.7928e-05,
         -5.9293e-06, -2.2702e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7335e-08, 1.4616e-08, 1.7351e-08,  ..., 1.9899e-08, 3.1506e-08,
         6.5367e-09],
        [2.9928e-11, 1.8545e-11, 1.0042e-12,  ..., 1.9749e-11, 6.2608e-13,
         4.0392e-12],
        [3.2434e-10, 2.1740e-10, 1.1676e-11,  ..., 2.9725e-10, 5.0819e-12,
         6.8209e-11],
        [1.8103e-10, 2.5537e-10, 4.7534e-12,  ..., 1.3384e-10, 6.6155e-12,
         4.7099e-11],
        [1.2583e-10, 7.1048e-11, 5.0787e-12,  ..., 8.3755e-11, 2.1150e-12,
         1.8597e-11]], device='cuda:0')
optimizer state dict: 35.0
lr: [1.7191947575507777e-05, 1.7191947575507777e-05]
scheduler_last_epoch: 35


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8984e-06, -1.2248e-05, -1.6180e-05,  ..., -1.9912e-06,
         -4.1883e-06,  3.3089e-05],
        [-2.9355e-06, -2.0564e-06,  4.7311e-07,  ..., -2.5183e-06,
         -3.7998e-07, -1.1325e-06],
        [-3.0100e-06, -2.1160e-06,  4.8429e-07,  ..., -2.5928e-06,
         -3.8929e-07, -1.1623e-06],
        [-4.5002e-06, -3.1590e-06,  7.2643e-07,  ..., -3.8743e-06,
         -5.8115e-07, -1.7360e-06],
        [-6.7949e-06, -4.7684e-06,  1.0878e-06,  ..., -5.8413e-06,
         -8.7917e-07, -2.6226e-06]], device='cuda:0')
Loss: 1.1698123216629028


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[   0, 1850,  311,  ..., 3655, 3133, 9000],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5184e-06, -4.9864e-05, -4.8078e-05,  ..., -1.2262e-06,
         -6.0020e-07,  5.7673e-05],
        [-5.8860e-06, -4.1425e-06,  9.2946e-07,  ..., -5.0366e-06,
         -7.6368e-07, -2.2650e-06],
        [ 3.6012e-04,  2.6826e-04, -5.5252e-05,  ...,  2.9505e-04,
          3.9975e-05,  1.0995e-04],
        [-9.0003e-06, -6.3330e-06,  1.4231e-06,  ..., -7.7188e-06,
         -1.1660e-06, -3.4645e-06],
        [-1.3620e-05, -9.5665e-06,  2.1383e-06,  ..., -1.1683e-05,
         -1.7658e-06, -5.2452e-06]], device='cuda:0')
Loss: 1.1683560609817505


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8662e-05, -4.4306e-05, -4.1693e-05,  ...,  5.5047e-06,
         -5.2185e-05,  5.0560e-05],
        [-8.7917e-06, -6.1691e-06,  1.3728e-06,  ..., -7.5549e-06,
         -1.1194e-06, -3.3975e-06],
        [ 3.5713e-04,  2.6617e-04, -5.4795e-05,  ...,  2.9246e-04,
          3.9609e-05,  1.0878e-04],
        [-1.3471e-05, -9.4324e-06,  2.1048e-06,  ..., -1.1563e-05,
         -1.7099e-06, -5.2005e-06],
        [-2.0355e-05, -1.4246e-05,  3.1665e-06,  ..., -1.7524e-05,
         -2.5891e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.1633987426757812


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0,  461,  654,  ..., 6548, 7171,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6864e-05, -7.7929e-05, -4.6637e-05,  ..., -2.1774e-05,
         -7.2634e-05,  6.9460e-05],
        [-1.1742e-05, -8.2254e-06,  1.8049e-06,  ..., -1.0043e-05,
         -1.5162e-06, -4.5151e-06],
        [ 3.5407e-04,  2.6404e-04, -5.4350e-05,  ...,  2.8988e-04,
          3.9202e-05,  1.0762e-04],
        [-1.8001e-05, -1.2577e-05,  2.7679e-06,  ..., -1.5363e-05,
         -2.3134e-06, -6.9067e-06],
        [-2.7180e-05, -1.9014e-05,  4.1649e-06,  ..., -2.3276e-05,
         -3.5055e-06, -1.0446e-05]], device='cuda:0')
Loss: 1.159032940864563


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5818e-05, -1.1282e-04, -4.7971e-05,  ..., -1.0565e-05,
         -5.1093e-05,  1.0773e-04],
        [ 7.1246e-05,  4.7030e-05, -1.6226e-05,  ...,  6.3477e-05,
          2.1428e-05,  3.3837e-05],
        [ 5.4886e-04,  3.9693e-04, -7.6858e-05,  ...,  4.2718e-04,
          6.1255e-05,  1.7166e-04],
        [-2.2531e-05, -1.5736e-05,  3.4720e-06,  ..., -1.9237e-05,
         -2.9244e-06, -8.6501e-06],
        [-3.4064e-05, -2.3812e-05,  5.2229e-06,  ..., -2.9176e-05,
         -4.4368e-06, -1.3098e-05]], device='cuda:0')
Loss: 1.1839511394500732


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[    0,    14, 49045,  ...,    12,   706,   409],
        [    0,   806,   300,  ...,   360,  4918,  1106]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8728e-06, -1.0225e-04, -5.6430e-05,  ..., -2.5225e-05,
         -4.3812e-05,  1.1328e-04],
        [ 6.8266e-05,  4.4944e-05, -1.5777e-05,  ...,  6.0929e-05,
          2.1024e-05,  3.2704e-05],
        [ 5.4582e-04,  3.9479e-04, -7.6402e-05,  ...,  4.2459e-04,
          6.0841e-05,  1.7051e-04],
        [-2.7031e-05, -1.8895e-05,  4.1500e-06,  ..., -2.3082e-05,
         -3.5353e-06, -1.0364e-05],
        [-4.0919e-05, -2.8610e-05,  6.2510e-06,  ..., -3.5018e-05,
         -5.3681e-06, -1.5706e-05]], device='cuda:0')
Loss: 1.1690109968185425


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[  0, 669,  14,  ..., 596, 292, 494],
        [  0, 432, 984,  ..., 287, 496,  14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5255e-05, -6.5648e-05, -8.0928e-05,  ..., -1.3966e-05,
         -3.9807e-05,  1.2363e-04],
        [ 6.5301e-05,  4.2902e-05, -1.5322e-05,  ...,  5.8366e-05,
          2.0650e-05,  3.1557e-05],
        [ 5.4278e-04,  3.9268e-04, -7.5933e-05,  ...,  4.2195e-04,
          6.0458e-05,  1.6933e-04],
        [-3.1531e-05, -2.2009e-05,  4.8429e-06,  ..., -2.6956e-05,
         -4.1015e-06, -1.2100e-05],
        [-4.7773e-05, -3.3349e-05,  7.3016e-06,  ..., -4.0948e-05,
         -6.2324e-06, -1.8358e-05]], device='cuda:0')
Loss: 1.176885724067688


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  266,  720,  342],
        [   0, 2992,  352,  ...,  259, 2063, 6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1504e-05, -8.6127e-05, -9.6041e-05,  ..., -4.8997e-06,
         -4.0863e-05,  1.0669e-04],
        [ 6.2350e-05,  4.0846e-05, -1.4862e-05,  ...,  5.5848e-05,
          2.0253e-05,  3.0409e-05],
        [ 5.3976e-04,  3.9058e-04, -7.5461e-05,  ...,  4.1937e-04,
          6.0052e-05,  1.6816e-04],
        [-3.6031e-05, -2.5138e-05,  5.5470e-06,  ..., -3.0801e-05,
         -4.7050e-06, -1.3843e-05],
        [-5.4568e-05, -3.8058e-05,  8.3596e-06,  ..., -4.6730e-05,
         -7.1451e-06, -2.0981e-05]], device='cuda:0')
Loss: 1.1701114177703857
Graident accumulation at epoch 0, step 287, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0883e-05, -3.4451e-05, -1.6022e-05,  ...,  4.0156e-05,
         -2.8831e-05,  4.4186e-05],
        [-1.7127e-05, -1.2374e-05,  1.2455e-06,  ..., -1.3469e-05,
         -3.0978e-07, -5.9615e-06],
        [ 8.6283e-05,  6.3450e-05, -1.0806e-05,  ...,  6.9505e-05,
          7.6335e-06,  2.7208e-05],
        [-2.9194e-05, -1.9063e-05,  4.4803e-06,  ..., -2.3392e-05,
         -3.1506e-06, -1.0537e-05],
        [-5.8715e-05, -4.1283e-05,  6.5599e-06,  ..., -4.7808e-05,
         -6.0508e-06, -2.2530e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7288e-08, 1.4608e-08, 1.7343e-08,  ..., 1.9879e-08, 3.1477e-08,
         6.5416e-09],
        [3.3785e-11, 2.0195e-11, 1.2240e-12,  ..., 2.2848e-11, 1.0356e-12,
         4.9599e-12],
        [6.1535e-10, 3.6973e-10, 1.7358e-11,  ..., 4.7282e-10, 8.6830e-12,
         9.6419e-11],
        [1.8215e-10, 2.5575e-10, 4.7794e-12,  ..., 1.3465e-10, 6.6310e-12,
         4.7244e-11],
        [1.2868e-10, 7.2426e-11, 5.1435e-12,  ..., 8.5855e-11, 2.1639e-12,
         1.9018e-11]], device='cuda:0')
optimizer state dict: 36.0
lr: [1.7017987415646643e-05, 1.7017987415646643e-05]
scheduler_last_epoch: 36


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,  1890,   278,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5686e-05, -3.1535e-06, -1.4891e-05,  ..., -6.3715e-06,
          2.1569e-05, -6.8786e-06],
        [-2.9504e-06, -2.0564e-06,  4.8801e-07,  ..., -2.5481e-06,
         -4.2655e-07, -1.1399e-06],
        [-3.0100e-06, -2.1011e-06,  4.9919e-07,  ..., -2.6077e-06,
         -4.3586e-07, -1.1623e-06],
        [-4.4703e-06, -3.0994e-06,  7.4133e-07,  ..., -3.8445e-06,
         -6.4448e-07, -1.7211e-06],
        [-6.7651e-06, -4.7088e-06,  1.1176e-06,  ..., -5.8115e-06,
         -9.8348e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1471234560012817


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,  7849,   278,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7510e-05, -2.2594e-05, -2.7862e-05,  ..., -4.2971e-05,
          3.5054e-05,  5.7346e-07],
        [-5.8860e-06, -4.1127e-06,  1.0245e-06,  ..., -5.0664e-06,
         -8.8289e-07, -2.3022e-06],
        [-6.0201e-06, -4.2170e-06,  1.0505e-06,  ..., -5.2005e-06,
         -9.0338e-07, -2.3544e-06],
        [-8.8811e-06, -6.1989e-06,  1.5497e-06,  ..., -7.6443e-06,
         -1.3299e-06, -3.4720e-06],
        [-1.3441e-05, -9.4175e-06,  2.3395e-06,  ..., -1.1563e-05,
         -2.0266e-06, -5.2601e-06]], device='cuda:0')
Loss: 1.1724557876586914


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  3658,   271,  ...,   278,   970,    12],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1828e-05, -1.3969e-05, -1.7269e-05,  ..., -7.1389e-05,
          4.2048e-05,  1.5698e-05],
        [-8.8811e-06, -6.1989e-06,  1.4808e-06,  ..., -7.6145e-06,
         -1.3318e-06, -3.4496e-06],
        [-9.0748e-06, -6.3330e-06,  1.5162e-06,  ..., -7.8082e-06,
         -1.3597e-06, -3.5241e-06],
        [-1.3381e-05, -9.3132e-06,  2.2352e-06,  ..., -1.1459e-05,
         -2.0005e-06, -5.1931e-06],
        [-2.0295e-05, -1.4186e-05,  3.3751e-06,  ..., -1.7405e-05,
         -3.0547e-06, -7.8827e-06]], device='cuda:0')
Loss: 1.1576026678085327


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   298,   335,   298],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8419e-05, -5.5621e-06, -3.6583e-06,  ..., -6.5911e-05,
          4.2048e-05, -3.3670e-05],
        [-1.1846e-05, -8.2701e-06,  1.9800e-06,  ..., -1.0133e-05,
         -1.8086e-06, -4.5672e-06],
        [-1.2100e-05, -8.4490e-06,  2.0228e-06,  ..., -1.0386e-05,
         -1.8477e-06, -4.6641e-06],
        [-1.7822e-05, -1.2413e-05,  2.9802e-06,  ..., -1.5244e-05,
         -2.7120e-06, -6.8694e-06],
        [-2.7120e-05, -1.8954e-05,  4.5151e-06,  ..., -2.3216e-05,
         -4.1574e-06, -1.0461e-05]], device='cuda:0')
Loss: 1.158742070198059


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[   0,  397, 1267,  ..., 1276,  292,  221],
        [   0,   14,  475,  ..., 6895, 5842, 2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6454e-05,  9.2546e-06, -2.1877e-05,  ..., -9.3094e-05,
          4.1276e-05, -6.2447e-05],
        [-1.4797e-05, -1.0356e-05,  2.4680e-06,  ..., -1.2651e-05,
         -2.2277e-06, -5.6997e-06],
        [-1.5095e-05, -1.0565e-05,  2.5183e-06,  ..., -1.2949e-05,
         -2.2743e-06, -5.8115e-06],
        [-2.2262e-05, -1.5542e-05,  3.7178e-06,  ..., -1.9029e-05,
         -3.3379e-06, -8.5682e-06],
        [-3.3855e-05, -2.3693e-05,  5.6252e-06,  ..., -2.8938e-05,
         -5.1111e-06, -1.3024e-05]], device='cuda:0')
Loss: 1.1676881313323975


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,   741,   266,  ...,   271,  5166,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6414e-05, -1.8873e-05,  1.3739e-06,  ..., -9.9542e-05,
          1.2165e-05, -3.9848e-05],
        [-1.7762e-05, -1.2457e-05,  2.9597e-06,  ..., -1.5184e-05,
         -2.6654e-06, -6.8173e-06],
        [-1.8135e-05, -1.2726e-05,  3.0249e-06,  ..., -1.5542e-05,
         -2.7232e-06, -6.9514e-06],
        [-2.6792e-05, -1.8746e-05,  4.4703e-06,  ..., -2.2873e-05,
         -4.0010e-06, -1.0259e-05],
        [-4.0621e-05, -2.8491e-05,  6.7428e-06,  ..., -3.4690e-05,
         -6.1095e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.1623785495758057


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0,  1737,   278,  ...,  2604,   367,  2002]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8810e-05, -4.5307e-05, -2.0027e-06,  ..., -8.4244e-05,
          2.6726e-05, -9.3355e-06],
        [-2.0757e-05, -1.4573e-05,  3.4738e-06,  ..., -1.7717e-05,
         -3.1237e-06, -7.9572e-06],
        [-2.1189e-05, -1.4886e-05,  3.5502e-06,  ..., -1.8120e-05,
         -3.1907e-06, -8.1137e-06],
        [-3.1322e-05, -2.1949e-05,  5.2527e-06,  ..., -2.6718e-05,
         -4.6939e-06, -1.1988e-05],
        [-4.7445e-05, -3.3289e-05,  7.9125e-06,  ..., -4.0442e-05,
         -7.1526e-06, -1.8150e-05]], device='cuda:0')
Loss: 1.1716704368591309


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3894e-05, -6.5988e-05,  3.5815e-05,  ..., -9.1348e-05,
          1.1661e-05, -9.3355e-06],
        [-2.3708e-05, -1.6645e-05,  3.9767e-06,  ..., -2.0221e-05,
         -3.5558e-06, -9.1046e-06],
        [-2.4199e-05, -1.6987e-05,  4.0606e-06,  ..., -2.0668e-05,
         -3.6303e-06, -9.2834e-06],
        [-3.5822e-05, -2.5108e-05,  6.0201e-06,  ..., -3.0532e-05,
         -5.3495e-06, -1.3739e-05],
        [-5.4210e-05, -3.8028e-05,  9.0599e-06,  ..., -4.6164e-05,
         -8.1435e-06, -2.0772e-05]], device='cuda:0')
Loss: 1.1624201536178589
Graident accumulation at epoch 0, step 295, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9184e-05, -3.7604e-05, -1.0839e-05,  ...,  2.7005e-05,
         -2.4782e-05,  3.8834e-05],
        [-1.7785e-05, -1.2801e-05,  1.5186e-06,  ..., -1.4144e-05,
         -6.3438e-07, -6.2759e-06],
        [ 7.5235e-05,  5.5406e-05, -9.3193e-06,  ...,  6.0487e-05,
          6.5071e-06,  2.3558e-05],
        [-2.9857e-05, -1.9668e-05,  4.6343e-06,  ..., -2.4106e-05,
         -3.3705e-06, -1.0857e-05],
        [-5.8264e-05, -4.0958e-05,  6.8099e-06,  ..., -4.7643e-05,
         -6.2601e-06, -2.2354e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7244e-08, 1.4598e-08, 1.7327e-08,  ..., 1.9868e-08, 3.1445e-08,
         6.5351e-09],
        [3.4314e-11, 2.0452e-11, 1.2386e-12,  ..., 2.3234e-11, 1.0472e-12,
         5.0378e-12],
        [6.1532e-10, 3.6965e-10, 1.7358e-11,  ..., 4.7277e-10, 8.6875e-12,
         9.6408e-11],
        [1.8325e-10, 2.5612e-10, 4.8109e-12,  ..., 1.3545e-10, 6.6530e-12,
         4.7386e-11],
        [1.3149e-10, 7.3800e-11, 5.2205e-12,  ..., 8.7900e-11, 2.2281e-12,
         1.9431e-11]], device='cuda:0')
optimizer state dict: 37.0
lr: [1.6839737780707125e-05, 1.6839737780707125e-05]
scheduler_last_epoch: 37


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,   292,    36,     9],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7465e-05,  1.6617e-05, -8.1757e-06,  ...,  1.5617e-06,
          6.7474e-06, -6.7230e-06],
        [-2.9951e-06, -2.0862e-06,  5.5134e-07,  ..., -2.5481e-06,
         -4.9919e-07, -1.1846e-06],
        [-3.0547e-06, -2.1160e-06,  5.6252e-07,  ..., -2.5928e-06,
         -5.0664e-07, -1.1995e-06],
        [-4.5002e-06, -3.1143e-06,  8.3074e-07,  ..., -3.8147e-06,
         -7.4506e-07, -1.7732e-06],
        [-6.7353e-06, -4.6790e-06,  1.2368e-06,  ..., -5.7220e-06,
         -1.1250e-06, -2.6524e-06]], device='cuda:0')
Loss: 1.1462440490722656


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,  9466,    36,  ...,  1795,   437,   874],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5071e-06,  3.7715e-05, -1.8297e-05,  ...,  8.1663e-06,
         -8.0785e-07,  1.1823e-05],
        [-5.9903e-06, -4.1425e-06,  1.1325e-06,  ..., -5.0962e-06,
         -1.0096e-06, -2.3544e-06],
        [-6.0946e-06, -4.2170e-06,  1.1511e-06,  ..., -5.1856e-06,
         -1.0245e-06, -2.3916e-06],
        [-8.9705e-06, -6.1989e-06,  1.6987e-06,  ..., -7.6294e-06,
         -1.5050e-06, -3.5241e-06],
        [-1.3471e-05, -9.2983e-06,  2.5406e-06,  ..., -1.1444e-05,
         -2.2724e-06, -5.2750e-06]], device='cuda:0')
Loss: 1.1623762845993042


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6596e-06,  5.6340e-05, -5.7913e-05,  ...,  5.6569e-06,
          7.0485e-07, -5.5093e-06],
        [-8.9854e-06, -6.2138e-06,  1.6801e-06,  ..., -7.6741e-06,
         -1.4715e-06, -3.4943e-06],
        [-9.0897e-06, -6.3032e-06,  1.6987e-06,  ..., -7.7635e-06,
         -1.4864e-06, -3.5316e-06],
        [-1.3411e-05, -9.2834e-06,  2.5146e-06,  ..., -1.1444e-05,
         -2.1942e-06, -5.2154e-06],
        [-2.0146e-05, -1.3947e-05,  3.7625e-06,  ..., -1.7196e-05,
         -3.3081e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.1677489280700684


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0,  2344,   271,  ...,  5415,    14,  1075],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9462e-05,  3.5480e-05, -8.0050e-05,  ..., -1.5410e-05,
         -3.2567e-06,  9.0415e-06],
        [-1.1981e-05, -8.2850e-06,  2.2352e-06,  ..., -1.0252e-05,
         -1.9893e-06, -4.6715e-06],
        [-1.2130e-05, -8.4192e-06,  2.2650e-06,  ..., -1.0386e-05,
         -2.0117e-06, -4.7311e-06],
        [-1.7881e-05, -1.2383e-05,  3.3490e-06,  ..., -1.5289e-05,
         -2.9653e-06, -6.9663e-06],
        [-2.6882e-05, -1.8597e-05,  5.0142e-06,  ..., -2.2978e-05,
         -4.4703e-06, -1.0446e-05]], device='cuda:0')
Loss: 1.174958348274231


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0,  578,  221,  ...,  287, 1254, 4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0250e-05,  7.4258e-05, -1.6192e-04,  ...,  2.0183e-05,
          1.8714e-06,  7.0921e-06],
        [-1.4976e-05, -1.0371e-05,  2.8163e-06,  ..., -1.2800e-05,
         -2.4643e-06, -5.8040e-06],
        [-1.5184e-05, -1.0550e-05,  2.8573e-06,  ..., -1.2979e-05,
         -2.4959e-06, -5.8860e-06],
        [-2.2352e-05, -1.5497e-05,  4.2170e-06,  ..., -1.9073e-05,
         -3.6694e-06, -8.6501e-06],
        [-3.3587e-05, -2.3276e-05,  6.3106e-06,  ..., -2.8670e-05,
         -5.5283e-06, -1.2964e-05]], device='cuda:0')
Loss: 1.164495587348938


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  9472, 15004,   511],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7132e-05,  8.4150e-05, -1.9600e-04,  ...,  4.2423e-05,
          8.7239e-06, -6.8098e-05],
        [-1.7986e-05, -1.2457e-05,  3.3639e-06,  ..., -1.5333e-05,
         -2.9597e-06, -6.9886e-06],
        [-1.8239e-05, -1.2666e-05,  3.4161e-06,  ..., -1.5557e-05,
         -2.9989e-06, -7.0855e-06],
        [-2.6882e-05, -1.8626e-05,  5.0440e-06,  ..., -2.2888e-05,
         -4.4145e-06, -1.0423e-05],
        [-4.0352e-05, -2.7955e-05,  7.5400e-06,  ..., -3.4362e-05,
         -6.6459e-06, -1.5602e-05]], device='cuda:0')
Loss: 1.1494168043136597


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   401,  3740,  ...,  5980,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7926e-05,  8.1007e-05, -1.6172e-04,  ...,  2.4028e-05,
          1.8579e-05, -4.9862e-05],
        [-2.0996e-05, -1.4558e-05,  3.9153e-06,  ..., -1.7881e-05,
         -3.4478e-06, -8.1435e-06],
        [-2.1279e-05, -1.4797e-05,  3.9749e-06,  ..., -1.8135e-05,
         -3.4906e-06, -8.2552e-06],
        [-3.1382e-05, -2.1785e-05,  5.8748e-06,  ..., -2.6703e-05,
         -5.1446e-06, -1.2152e-05],
        [-4.7117e-05, -3.2693e-05,  8.7842e-06,  ..., -4.0114e-05,
         -7.7486e-06, -1.8194e-05]], device='cuda:0')
Loss: 1.1567437648773193


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  365, 2849,  ...,    9, 3365, 5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4818e-05,  7.8615e-05, -1.3475e-04,  ...,  2.5103e-06,
          3.9460e-05, -6.6544e-05],
        [-2.4006e-05, -1.6645e-05,  4.4964e-06,  ..., -2.0444e-05,
         -3.9283e-06, -9.3132e-06],
        [-2.4319e-05, -1.6898e-05,  4.5635e-06,  ..., -2.0713e-05,
         -3.9786e-06, -9.4399e-06],
        [-3.5882e-05, -2.4885e-05,  6.7428e-06,  ..., -3.0518e-05,
         -5.8599e-06, -1.3895e-05],
        [-5.3883e-05, -3.7372e-05,  1.0088e-05,  ..., -4.5866e-05,
         -8.8289e-06, -2.0817e-05]], device='cuda:0')
Loss: 1.1741068363189697
Graident accumulation at epoch 0, step 303, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3747e-05, -2.5982e-05, -2.3230e-05,  ...,  2.4556e-05,
         -1.8357e-05,  2.8296e-05],
        [-1.8407e-05, -1.3185e-05,  1.8164e-06,  ..., -1.4774e-05,
         -9.6377e-07, -6.5796e-06],
        [ 6.5280e-05,  4.8176e-05, -7.9310e-06,  ...,  5.2367e-05,
          5.4585e-06,  2.0259e-05],
        [-3.0459e-05, -2.0190e-05,  4.8451e-06,  ..., -2.4747e-05,
         -3.6194e-06, -1.1161e-05],
        [-5.7826e-05, -4.0599e-05,  7.1377e-06,  ..., -4.7466e-05,
         -6.5170e-06, -2.2201e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7197e-08, 1.4590e-08, 1.7328e-08,  ..., 1.9848e-08, 3.1415e-08,
         6.5330e-09],
        [3.4856e-11, 2.0709e-11, 1.2576e-12,  ..., 2.3629e-11, 1.0616e-12,
         5.1195e-12],
        [6.1530e-10, 3.6956e-10, 1.7361e-11,  ..., 4.7273e-10, 8.6946e-12,
         9.6401e-11],
        [1.8435e-10, 2.5649e-10, 4.8516e-12,  ..., 1.3625e-10, 6.6807e-12,
         4.7531e-11],
        [1.3426e-10, 7.5122e-11, 5.3170e-12,  ..., 8.9916e-11, 2.3038e-12,
         1.9845e-11]], device='cuda:0')
optimizer state dict: 38.0
lr: [1.6657307618927726e-05, 1.6657307618927726e-05]
scheduler_last_epoch: 38


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,   377,   472,  ...,  9256,  3807,  5499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1512e-05, -8.9756e-06, -1.2973e-05,  ...,  3.9911e-06,
         -1.0637e-05, -3.0551e-05],
        [-2.9951e-06, -2.1011e-06,  6.1467e-07,  ..., -2.5928e-06,
         -5.2899e-07, -1.1846e-06],
        [-3.0100e-06, -2.1160e-06,  6.1467e-07,  ..., -2.6077e-06,
         -5.2899e-07, -1.1921e-06],
        [-4.4107e-06, -3.0994e-06,  9.0525e-07,  ..., -3.8147e-06,
         -7.7486e-07, -1.7434e-06],
        [-6.5863e-06, -4.6194e-06,  1.3486e-06,  ..., -5.6922e-06,
         -1.1623e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.16884446144104


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[    0, 10565,  2677,  ...,   298,   292, 11188],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4344e-05, -3.5777e-06, -2.6787e-05,  ...,  7.7417e-06,
         -3.2238e-05, -3.0789e-05],
        [-5.9903e-06, -4.2021e-06,  1.2107e-06,  ..., -5.1558e-06,
         -1.0692e-06, -2.3618e-06],
        [-6.0201e-06, -4.2319e-06,  1.2144e-06,  ..., -5.1856e-06,
         -1.0729e-06, -2.3767e-06],
        [-8.8513e-06, -6.2138e-06,  1.7956e-06,  ..., -7.6145e-06,
         -1.5795e-06, -3.4943e-06],
        [-1.3232e-05, -9.2685e-06,  2.6748e-06,  ..., -1.1384e-05,
         -2.3693e-06, -5.2005e-06]], device='cuda:0')
Loss: 1.1561335325241089


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9930e-05,  2.9767e-05, -3.0708e-05,  ..., -2.2043e-05,
         -4.3761e-06, -6.6089e-05],
        [-8.9556e-06, -6.3032e-06,  1.8366e-06,  ..., -7.7486e-06,
         -1.6391e-06, -3.5390e-06],
        [-9.0003e-06, -6.3479e-06,  1.8403e-06,  ..., -7.7784e-06,
         -1.6466e-06, -3.5614e-06],
        [-1.3262e-05, -9.3430e-06,  2.7306e-06,  ..., -1.1459e-05,
         -2.4252e-06, -5.2452e-06],
        [-1.9789e-05, -1.3918e-05,  4.0531e-06,  ..., -1.7107e-05,
         -3.6284e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.157952070236206


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   292,    65,  ...,    12,   857,   344],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7554e-05,  3.3523e-05, -3.6770e-05,  ..., -9.7328e-06,
         -8.0655e-06, -4.8685e-05],
        [-1.1995e-05, -8.4192e-06,  2.4550e-06,  ..., -1.0327e-05,
         -2.1905e-06, -4.7237e-06],
        [-1.2055e-05, -8.4788e-06,  2.4624e-06,  ..., -1.0371e-05,
         -2.2016e-06, -4.7460e-06],
        [-1.7762e-05, -1.2487e-05,  3.6508e-06,  ..., -1.5303e-05,
         -3.2410e-06, -6.9961e-06],
        [-2.6524e-05, -1.8626e-05,  5.4240e-06,  ..., -2.2829e-05,
         -4.8578e-06, -1.0416e-05]], device='cuda:0')
Loss: 1.1668330430984497


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0, 1159,  278,  ...,    9,  271,  266],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7163e-05, -2.5806e-06, -4.2360e-05,  ...,  1.8975e-05,
         -5.3096e-05, -2.4188e-05],
        [-1.4976e-05, -1.0520e-05,  3.0734e-06,  ..., -1.2860e-05,
         -2.7232e-06, -5.8934e-06],
        [-1.5065e-05, -1.0595e-05,  3.0883e-06,  ..., -1.2934e-05,
         -2.7381e-06, -5.9307e-06],
        [-2.2203e-05, -1.5602e-05,  4.5747e-06,  ..., -1.9073e-05,
         -4.0308e-06, -8.7321e-06],
        [-3.3140e-05, -2.3276e-05,  6.7949e-06,  ..., -2.8431e-05,
         -6.0350e-06, -1.2994e-05]], device='cuda:0')
Loss: 1.1568588018417358


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[  0, 342, 726,  ...,  12, 895, 367],
        [  0, 278, 266,  ..., 292, 474, 221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7603e-05, -3.2008e-05, -6.1836e-05,  ...,  1.9026e-05,
         -6.2280e-05, -3.7712e-05],
        [-1.7941e-05, -1.2621e-05,  3.6918e-06,  ..., -1.5408e-05,
         -3.2485e-06, -7.0557e-06],
        [-1.8060e-05, -1.2726e-05,  3.7141e-06,  ..., -1.5512e-05,
         -3.2671e-06, -7.1004e-06],
        [-2.6643e-05, -1.8746e-05,  5.5023e-06,  ..., -2.2888e-05,
         -4.8093e-06, -1.0461e-05],
        [-3.9726e-05, -2.7955e-05,  8.1658e-06,  ..., -3.4094e-05,
         -7.1973e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.1565499305725098


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,   586,   940,  ...,  1471,  2612,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4238e-05, -4.6603e-05, -9.3953e-05,  ...,  2.2651e-05,
         -8.1129e-05, -1.0687e-05],
        [-2.0951e-05, -1.4707e-05,  4.2953e-06,  ..., -1.7986e-05,
         -3.8333e-06, -8.2254e-06],
        [-2.1115e-05, -1.4842e-05,  4.3251e-06,  ..., -1.8120e-05,
         -3.8594e-06, -8.2850e-06],
        [-3.1084e-05, -2.1830e-05,  6.3963e-06,  ..., -2.6688e-05,
         -5.6699e-06, -1.2182e-05],
        [-4.6402e-05, -3.2574e-05,  9.4995e-06,  ..., -3.9756e-05,
         -8.4862e-06, -1.8135e-05]], device='cuda:0')
Loss: 1.1655933856964111


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[   0,  401, 9370,  ...,    9,  287,  518],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3201e-06, -8.5159e-05, -7.8661e-05,  ..., -6.8096e-06,
         -1.0492e-04, -9.1134e-07],
        [-2.3961e-05, -1.6823e-05,  4.9397e-06,  ..., -2.0549e-05,
         -4.3809e-06, -9.4101e-06],
        [-2.4155e-05, -1.6972e-05,  4.9770e-06,  ..., -2.0713e-05,
         -4.4145e-06, -9.4846e-06],
        [-3.5554e-05, -2.4959e-05,  7.3574e-06,  ..., -3.0488e-05,
         -6.4820e-06, -1.3940e-05],
        [-5.3078e-05, -3.7253e-05,  1.0923e-05,  ..., -4.5389e-05,
         -9.7007e-06, -2.0742e-05]], device='cuda:0')
Loss: 1.1463022232055664
Graident accumulation at epoch 0, step 311, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6540e-05, -3.1900e-05, -2.8773e-05,  ...,  2.1419e-05,
         -2.7014e-05,  2.5375e-05],
        [-1.8962e-05, -1.3549e-05,  2.1287e-06,  ..., -1.5352e-05,
         -1.3055e-06, -6.8626e-06],
        [ 5.6336e-05,  4.1661e-05, -6.6402e-06,  ...,  4.5059e-05,
          4.4712e-06,  1.7284e-05],
        [-3.0969e-05, -2.0667e-05,  5.0963e-06,  ..., -2.5321e-05,
         -3.9057e-06, -1.1439e-05],
        [-5.7351e-05, -4.0265e-05,  7.5162e-06,  ..., -4.7258e-05,
         -6.8354e-06, -2.2055e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7150e-08, 1.4582e-08, 1.7317e-08,  ..., 1.9828e-08, 3.1395e-08,
         6.5265e-09],
        [3.5395e-11, 2.0971e-11, 1.2807e-12,  ..., 2.4027e-11, 1.0798e-12,
         5.2030e-12],
        [6.1527e-10, 3.6948e-10, 1.7368e-11,  ..., 4.7269e-10, 8.7054e-12,
         9.6395e-11],
        [1.8543e-10, 2.5685e-10, 4.9008e-12,  ..., 1.3704e-10, 6.7160e-12,
         4.7678e-11],
        [1.3695e-10, 7.6435e-11, 5.4310e-12,  ..., 9.1886e-11, 2.3956e-12,
         2.0255e-11]], device='cuda:0')
optimizer state dict: 39.0
lr: [1.6470808433733317e-05, 1.6470808433733317e-05]
scheduler_last_epoch: 39


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6100e-06, -1.6881e-05, -1.1229e-05,  ...,  2.5042e-05,
          2.2935e-05,  7.7178e-06],
        [-3.0249e-06, -2.1160e-06,  6.5193e-07,  ..., -2.5779e-06,
         -5.4762e-07, -1.1921e-06],
        [-3.0696e-06, -2.1458e-06,  6.6310e-07,  ..., -2.6226e-06,
         -5.5507e-07, -1.2070e-06],
        [-4.5002e-06, -3.1441e-06,  9.7603e-07,  ..., -3.8445e-06,
         -8.1584e-07, -1.7732e-06],
        [-6.6757e-06, -4.6492e-06,  1.4380e-06,  ..., -5.6624e-06,
         -1.2070e-06, -2.6226e-06]], device='cuda:0')
Loss: 1.1641407012939453


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[    0,  1471,   266,  ...,   525,  5202,   292],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8036e-05, -3.7197e-05, -1.5500e-05,  ...,  2.6017e-05,
         -8.7013e-06, -1.4139e-05],
        [-6.0201e-06, -4.2021e-06,  1.3337e-06,  ..., -5.1409e-06,
         -1.1027e-06, -2.3544e-06],
        [-6.0946e-06, -4.2617e-06,  1.3523e-06,  ..., -5.2154e-06,
         -1.1176e-06, -2.3842e-06],
        [-8.9705e-06, -6.2734e-06,  1.9968e-06,  ..., -7.6890e-06,
         -1.6466e-06, -3.5167e-06],
        [-1.3262e-05, -9.2685e-06,  2.9355e-06,  ..., -1.1295e-05,
         -2.4289e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.149956226348877


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0, 3978, 2697,  ...,  461, 5955, 3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3314e-05, -2.6734e-05, -4.2988e-05,  ...,  2.7365e-05,
          3.0836e-05, -8.5161e-05],
        [-9.0152e-06, -6.3032e-06,  1.9819e-06,  ..., -7.7486e-06,
         -1.7062e-06, -3.5539e-06],
        [-9.0897e-06, -6.3628e-06,  2.0005e-06,  ..., -7.8380e-06,
         -1.7211e-06, -3.5837e-06],
        [-1.3381e-05, -9.3728e-06,  2.9579e-06,  ..., -1.1533e-05,
         -2.5369e-06, -5.2825e-06],
        [-1.9819e-05, -1.3858e-05,  4.3511e-06,  ..., -1.7017e-05,
         -3.7551e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.1811885833740234


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1587e-05,  1.8171e-05, -8.8074e-05,  ...,  2.1213e-05,
          4.1650e-05, -7.0203e-05],
        [-1.2025e-05, -8.4043e-06,  2.6263e-06,  ..., -1.0312e-05,
         -2.2538e-06, -4.7237e-06],
        [ 6.0224e-05,  5.2464e-05, -1.9467e-05,  ...,  7.9423e-05,
          3.0669e-05,  4.0228e-05],
        [-1.7881e-05, -1.2532e-05,  3.9265e-06,  ..., -1.5378e-05,
         -3.3565e-06, -7.0333e-06],
        [-2.6464e-05, -1.8507e-05,  5.7742e-06,  ..., -2.2680e-05,
         -4.9695e-06, -1.0371e-05]], device='cuda:0')
Loss: 1.1507225036621094


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0, 2663,  328,  ...,  342,  266, 1163]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8162e-05, -4.1569e-06, -6.6490e-05,  ...,  1.7791e-05,
          3.7426e-05, -7.0203e-05],
        [-1.5035e-05, -1.0520e-05,  3.2857e-06,  ..., -1.2875e-05,
         -2.7940e-06, -5.8934e-06],
        [ 5.7184e-05,  5.0318e-05, -1.8800e-05,  ...,  7.6830e-05,
          3.0121e-05,  3.9043e-05],
        [-2.2382e-05, -1.5706e-05,  4.9174e-06,  ..., -1.9222e-05,
         -4.1649e-06, -8.7842e-06],
        [-3.3110e-05, -2.3186e-05,  7.2271e-06,  ..., -2.8342e-05,
         -6.1616e-06, -1.2949e-05]], device='cuda:0')
Loss: 1.1545343399047852


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7099e-05,  6.6949e-06, -6.7112e-05,  ...,  1.6309e-05,
          2.7966e-05, -1.5499e-05],
        [-1.8016e-05, -1.2636e-05,  3.9600e-06,  ..., -1.5423e-05,
         -3.3565e-06, -7.0482e-06],
        [ 1.5208e-04,  1.0785e-04, -5.5993e-05,  ...,  1.4640e-04,
          6.4905e-05,  8.3497e-05],
        [-2.6882e-05, -1.8910e-05,  5.9381e-06,  ..., -2.3067e-05,
         -5.0142e-06, -1.0528e-05],
        [-3.9667e-05, -2.7865e-05,  8.7097e-06,  ..., -3.3945e-05,
         -7.3984e-06, -1.5482e-05]], device='cuda:0')
Loss: 1.155470609664917


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0706e-06,  9.0363e-06, -5.9952e-05,  ...,  3.9863e-05,
          3.7596e-05,  1.8706e-05],
        [-2.1055e-05, -1.4782e-05,  4.6380e-06,  ..., -1.8001e-05,
         -3.9600e-06, -8.2105e-06],
        [ 1.4898e-04,  1.0566e-04, -5.5300e-05,  ...,  1.4378e-04,
          6.4290e-05,  8.2305e-05],
        [-3.1382e-05, -2.2098e-05,  6.9514e-06,  ..., -2.6882e-05,
         -5.9083e-06, -1.2256e-05],
        [-4.6313e-05, -3.2574e-05,  1.0200e-05,  ..., -3.9577e-05,
         -8.7172e-06, -1.8030e-05]], device='cuda:0')
Loss: 1.151196837425232


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4473e-05,  2.7947e-05, -6.7935e-05,  ...,  1.3017e-05,
          5.9343e-06,  3.2331e-05],
        [-2.4036e-05, -1.6853e-05,  5.2787e-06,  ..., -2.0549e-05,
         -4.5337e-06, -9.4026e-06],
        [ 1.4597e-04,  1.0355e-04, -5.4652e-05,  ...,  1.4120e-04,
          6.3709e-05,  8.1098e-05],
        [-3.5882e-05, -2.5228e-05,  7.9200e-06,  ..., -3.0726e-05,
         -6.7726e-06, -1.4052e-05],
        [-5.2959e-05, -3.7193e-05,  1.1630e-05,  ..., -4.5240e-05,
         -9.9987e-06, -2.0683e-05]], device='cuda:0')
Loss: 1.1469961404800415
Graident accumulation at epoch 0, step 319, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3334e-05, -2.5915e-05, -3.2689e-05,  ...,  2.0579e-05,
         -2.3719e-05,  2.6071e-05],
        [-1.9470e-05, -1.3879e-05,  2.4437e-06,  ..., -1.5871e-05,
         -1.6283e-06, -7.1166e-06],
        [ 6.5299e-05,  4.7850e-05, -1.1441e-05,  ...,  5.4674e-05,
          1.0395e-05,  2.3666e-05],
        [-3.1460e-05, -2.1123e-05,  5.3787e-06,  ..., -2.5861e-05,
         -4.1924e-06, -1.1700e-05],
        [-5.6912e-05, -3.9957e-05,  7.9276e-06,  ..., -4.7056e-05,
         -7.1517e-06, -2.1918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7103e-08, 1.4569e-08, 1.7304e-08,  ..., 1.9808e-08, 3.1364e-08,
         6.5210e-09],
        [3.5937e-11, 2.1234e-11, 1.3073e-12,  ..., 2.4426e-11, 1.0992e-12,
         5.2862e-12],
        [6.3596e-10, 3.7984e-10, 2.0338e-11,  ..., 4.9215e-10, 1.2756e-11,
         1.0288e-10],
        [1.8653e-10, 2.5723e-10, 4.9587e-12,  ..., 1.3785e-10, 6.7552e-12,
         4.7828e-11],
        [1.3962e-10, 7.7742e-11, 5.5608e-12,  ..., 9.3841e-11, 2.4932e-12,
         2.0662e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 40
Epoch 0 | Batch 319/1048 | Training PPL: 9662.127333689465 | time 30.424006938934326
Saving checkpoint at epoch 0, step 319, batch 319
Epoch 0 | Validation PPL: 10.075526624689587 | Learning rate: 1.628035421558293e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319, AFTER epoch 0, step 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  3036,   471,  ...,   287,  1906,    12],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-5.3334e-05, -2.5915e-05, -3.2689e-05,  ...,  2.0579e-05,
         -2.3719e-05,  2.6071e-05],
        [-1.9470e-05, -1.3879e-05,  2.4437e-06,  ..., -1.5871e-05,
         -1.6283e-06, -7.1166e-06],
        [ 6.5299e-05,  4.7850e-05, -1.1441e-05,  ...,  5.4674e-05,
          1.0395e-05,  2.3666e-05],
        [-3.1460e-05, -2.1123e-05,  5.3787e-06,  ..., -2.5861e-05,
         -4.1924e-06, -1.1700e-05],
        [-5.6912e-05, -3.9957e-05,  7.9276e-06,  ..., -4.7056e-05,
         -7.1517e-06, -2.1918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7103e-08, 1.4569e-08, 1.7304e-08,  ..., 1.9808e-08, 3.1364e-08,
         6.5210e-09],
        [3.5937e-11, 2.1234e-11, 1.3073e-12,  ..., 2.4426e-11, 1.0992e-12,
         5.2862e-12],
        [6.3596e-10, 3.7984e-10, 2.0338e-11,  ..., 4.9215e-10, 1.2756e-11,
         1.0288e-10],
        [1.8653e-10, 2.5723e-10, 4.9587e-12,  ..., 1.3785e-10, 6.7552e-12,
         4.7828e-11],
        [1.3962e-10, 7.7742e-11, 5.5608e-12,  ..., 9.3841e-11, 2.4932e-12,
         2.0662e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7765e-06, -7.5377e-06, -2.3073e-05,  ..., -1.8885e-05,
          2.4735e-05,  1.0587e-05],
        [-2.9802e-06, -2.1160e-06,  6.8173e-07,  ..., -2.5630e-06,
         -6.3702e-07, -1.1921e-06],
        [-3.0398e-06, -2.1458e-06,  6.9663e-07,  ..., -2.6226e-06,
         -6.5193e-07, -1.2144e-06],
        [-4.4703e-06, -3.1590e-06,  1.0282e-06,  ..., -3.8445e-06,
         -9.5367e-07, -1.7807e-06],
        [-6.5267e-06, -4.6194e-06,  1.4976e-06,  ..., -5.6326e-06,
         -1.4007e-06, -2.6077e-06]], device='cuda:0')
Loss: 1.1565836668014526


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[   0,  266, 2653,  ...,   29,   16,   14],
        [   0,  591,  953,  ..., 4118, 5750,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0810e-06, -1.9954e-05, -4.0750e-05,  ..., -2.2721e-05,
         -2.1820e-05,  3.4739e-06],
        [-5.9307e-06, -4.1872e-06,  1.3746e-06,  ..., -5.1111e-06,
         -1.2144e-06, -2.3693e-06],
        [-6.0648e-06, -4.2617e-06,  1.4082e-06,  ..., -5.2303e-06,
         -1.2442e-06, -2.4214e-06],
        [-8.9407e-06, -6.3032e-06,  2.0787e-06,  ..., -7.6890e-06,
         -1.8291e-06, -3.5614e-06],
        [-1.2994e-05, -9.1493e-06,  3.0175e-06,  ..., -1.1206e-05,
         -2.6673e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.1449159383773804


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  897,  328,  ...,  908,  696,  688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0021e-05, -3.4422e-05, -7.0384e-05,  ..., -1.0496e-05,
         -6.0229e-05,  2.1352e-05],
        [-8.9258e-06, -6.3032e-06,  2.0601e-06,  ..., -7.6890e-06,
         -1.7993e-06, -3.5316e-06],
        [-9.1046e-06, -6.4075e-06,  2.1011e-06,  ..., -7.8380e-06,
         -1.8366e-06, -3.5986e-06],
        [-1.3471e-05, -9.5069e-06,  3.1143e-06,  ..., -1.1593e-05,
         -2.7083e-06, -5.3197e-06],
        [-1.9580e-05, -1.3798e-05,  4.5151e-06,  ..., -1.6868e-05,
         -3.9488e-06, -7.7337e-06]], device='cuda:0')
Loss: 1.1571334600448608


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6591e-05, -1.8697e-05, -9.0558e-05,  ..., -3.1810e-06,
         -6.0720e-05, -2.2421e-05],
        [-1.1876e-05, -8.4043e-06,  2.7604e-06,  ..., -1.0237e-05,
         -2.3730e-06, -4.6864e-06],
        [ 6.9878e-05,  5.4137e-05, -2.2901e-05,  ...,  6.9306e-05,
          2.4800e-05,  2.8911e-05],
        [-1.7941e-05, -1.2696e-05,  4.1798e-06,  ..., -1.5438e-05,
         -3.5763e-06, -7.0706e-06],
        [-2.6017e-05, -1.8358e-05,  6.0424e-06,  ..., -2.2411e-05,
         -5.2005e-06, -1.0252e-05]], device='cuda:0')
Loss: 1.1565489768981934


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,  6067,  1188,  ...,  5282,   756,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8410e-05, -2.4351e-05, -9.7949e-05,  ..., -5.0567e-05,
         -6.6420e-05, -8.8875e-06],
        [-1.4842e-05, -1.0490e-05,  3.4347e-06,  ..., -1.2800e-05,
         -2.9653e-06, -5.8413e-06],
        [ 6.6868e-05,  5.2021e-05, -2.2216e-05,  ...,  6.6698e-05,
          2.4197e-05,  2.7734e-05],
        [-2.2411e-05, -1.5840e-05,  5.2005e-06,  ..., -1.9312e-05,
         -4.4703e-06, -8.8215e-06],
        [-3.2514e-05, -2.2918e-05,  7.5176e-06,  ..., -2.8044e-05,
         -6.5044e-06, -1.2785e-05]], device='cuda:0')
Loss: 1.1583952903747559


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8410e-05,  1.1103e-05, -4.1220e-05,  ..., -4.5105e-05,
         -6.4750e-05,  2.0851e-05],
        [-1.7837e-05, -1.2591e-05,  4.1127e-06,  ..., -1.5348e-05,
         -3.5577e-06, -7.0184e-06],
        [ 6.3783e-05,  4.9861e-05, -2.1519e-05,  ...,  6.4076e-05,
          2.3590e-05,  2.6527e-05],
        [-2.6971e-05, -1.9029e-05,  6.2287e-06,  ..., -2.3186e-05,
         -5.3644e-06, -1.0610e-05],
        [-3.9101e-05, -2.7537e-05,  9.0078e-06,  ..., -3.3647e-05,
         -7.8008e-06, -1.5363e-05]], device='cuda:0')
Loss: 1.1545274257659912


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[   0,  462,  221,  ...,   29,  413, 1801],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6353e-05, -2.4373e-06, -5.2059e-05,  ..., -3.4385e-05,
         -8.6445e-05,  1.8550e-05],
        [-2.0802e-05, -1.4678e-05,  4.8168e-06,  ..., -1.7896e-05,
         -4.1462e-06, -8.2031e-06],
        [ 6.0729e-05,  4.7700e-05, -2.0793e-05,  ...,  6.1453e-05,
          2.2982e-05,  2.5305e-05],
        [-3.1471e-05, -2.2203e-05,  7.3016e-06,  ..., -2.7061e-05,
         -6.2548e-06, -1.2405e-05],
        [-4.5627e-05, -3.2127e-05,  1.0550e-05,  ..., -3.9250e-05,
         -9.0897e-06, -1.7956e-05]], device='cuda:0')
Loss: 1.1485456228256226


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0, 2018, 4798,  ...,  292, 1919,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0215e-05, -5.7324e-06, -5.0371e-05,  ..., -5.0156e-05,
         -1.1476e-04, -1.3874e-05],
        [-2.3723e-05, -1.6764e-05,  5.5246e-06,  ..., -2.0429e-05,
         -4.7237e-06, -9.3803e-06],
        [ 5.7719e-05,  4.5569e-05, -2.0066e-05,  ...,  5.8845e-05,
          2.2390e-05,  2.4091e-05],
        [-3.5971e-05, -2.5392e-05,  8.3894e-06,  ..., -3.0935e-05,
         -7.1414e-06, -1.4216e-05],
        [-5.2035e-05, -3.6687e-05,  1.2100e-05,  ..., -4.4793e-05,
         -1.0356e-05, -2.0534e-05]], device='cuda:0')
Loss: 1.1498072147369385
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1022e-05, -2.3897e-05, -3.4457e-05,  ...,  1.3506e-05,
         -3.2823e-05,  2.2077e-05],
        [-1.9895e-05, -1.4168e-05,  2.7518e-06,  ..., -1.6327e-05,
         -1.9378e-06, -7.3430e-06],
        [ 6.4541e-05,  4.7622e-05, -1.2304e-05,  ...,  5.5091e-05,
          1.1595e-05,  2.3708e-05],
        [-3.1911e-05, -2.1550e-05,  5.6798e-06,  ..., -2.6369e-05,
         -4.4873e-06, -1.1952e-05],
        [-5.6424e-05, -3.9630e-05,  8.3448e-06,  ..., -4.6830e-05,
         -7.4722e-06, -2.1779e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7057e-08, 1.4554e-08, 1.7289e-08,  ..., 1.9791e-08, 3.1345e-08,
         6.5146e-09],
        [3.6464e-11, 2.1494e-11, 1.3365e-12,  ..., 2.4818e-11, 1.1204e-12,
         5.3689e-12],
        [6.3865e-10, 3.8153e-10, 2.0720e-11,  ..., 4.9512e-10, 1.3244e-11,
         1.0335e-10],
        [1.8764e-10, 2.5762e-10, 5.0241e-12,  ..., 1.3867e-10, 6.7994e-12,
         4.7982e-11],
        [1.4218e-10, 7.9010e-11, 5.7017e-12,  ..., 9.5753e-11, 2.5979e-12,
         2.1063e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.6086061372297498e-05, 1.6086061372297498e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0, 12456,    14,  ...,  1822,  1016,   365],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5239e-05,  4.5282e-05, -2.9190e-06,  ...,  8.2821e-06,
          2.2008e-05,  1.2137e-05],
        [-2.9802e-06, -2.1160e-06,  6.8173e-07,  ..., -2.5630e-06,
         -6.2957e-07, -1.1474e-06],
        [-3.0547e-06, -2.1607e-06,  6.9663e-07,  ..., -2.6226e-06,
         -6.4448e-07, -1.1772e-06],
        [-4.5002e-06, -3.1888e-06,  1.0282e-06,  ..., -3.8743e-06,
         -9.4622e-07, -1.7285e-06],
        [-6.5267e-06, -4.6492e-06,  1.4901e-06,  ..., -5.6028e-06,
         -1.3784e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.1528429985046387


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  1853,    12,   709],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2401e-05,  4.5429e-05, -1.5863e-05,  ..., -1.7743e-05,
          1.0295e-05, -2.8428e-05],
        [-5.9009e-06, -4.2021e-06,  1.3821e-06,  ..., -5.1260e-06,
         -1.2293e-06, -2.2948e-06],
        [-6.0797e-06, -4.3064e-06,  1.4231e-06,  ..., -5.2750e-06,
         -1.2629e-06, -2.3618e-06],
        [-8.9705e-06, -6.3777e-06,  2.1011e-06,  ..., -7.8082e-06,
         -1.8589e-06, -3.4794e-06],
        [-1.2964e-05, -9.2387e-06,  3.0324e-06,  ..., -1.1265e-05,
         -2.6971e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.1718459129333496


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7747e-05,  2.1314e-05, -5.3822e-05,  ..., -8.3403e-06,
          2.8042e-05, -2.5419e-05],
        [-8.8364e-06, -6.2883e-06,  2.0824e-06,  ..., -7.6443e-06,
         -1.8328e-06, -3.4347e-06],
        [-9.1493e-06, -6.4969e-06,  2.1569e-06,  ..., -7.9125e-06,
         -1.8962e-06, -3.5539e-06],
        [-1.3471e-05, -9.5814e-06,  3.1814e-06,  ..., -1.1683e-05,
         -2.7865e-06, -5.2303e-06],
        [-1.9461e-05, -1.3858e-05,  4.5821e-06,  ..., -1.6838e-05,
         -4.0382e-06, -7.5549e-06]], device='cuda:0')
Loss: 1.1370643377304077


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,     9,   287,  ...,   259,  8244,  1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1627e-05,  1.1654e-07, -8.0702e-05,  ..., -2.7185e-05,
          2.8366e-05, -6.3797e-06],
        [-1.1757e-05, -8.3894e-06,  2.8126e-06,  ..., -1.0192e-05,
         -2.4326e-06, -4.5672e-06],
        [-1.2174e-05, -8.6725e-06,  2.9132e-06,  ..., -1.0550e-05,
         -2.5146e-06, -4.7237e-06],
        [-1.8001e-05, -1.2830e-05,  4.3139e-06,  ..., -1.5616e-05,
         -3.7104e-06, -6.9737e-06],
        [-2.5898e-05, -1.8477e-05,  6.1840e-06,  ..., -2.2441e-05,
         -5.3570e-06, -1.0028e-05]], device='cuda:0')
Loss: 1.1497355699539185


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8465e-05,  1.6379e-05, -6.6985e-05,  ..., -1.3981e-05,
          1.8032e-05,  1.4938e-05],
        [-1.4693e-05, -1.0490e-05,  3.5465e-06,  ..., -1.2740e-05,
         -3.0622e-06, -5.6997e-06],
        [-1.5214e-05, -1.0848e-05,  3.6694e-06,  ..., -1.3188e-05,
         -3.1665e-06, -5.9009e-06],
        [-2.2531e-05, -1.6063e-05,  5.4389e-06,  ..., -1.9550e-05,
         -4.6790e-06, -8.7172e-06],
        [-3.2425e-05, -2.3127e-05,  7.8082e-06,  ..., -2.8074e-05,
         -6.7577e-06, -1.2532e-05]], device='cuda:0')
Loss: 1.1509127616882324


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[    0,    34,  3881,  ...,  1027,   271,   266],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5055e-05, -2.5430e-05, -4.3597e-05,  ..., -2.0367e-05,
         -4.0085e-05, -8.2999e-06],
        [-1.7583e-05, -1.2577e-05,  4.2506e-06,  ..., -1.5259e-05,
         -3.6173e-06, -6.8173e-06],
        [-1.8239e-05, -1.3024e-05,  4.3996e-06,  ..., -1.5825e-05,
         -3.7476e-06, -7.0632e-06],
        [-2.7031e-05, -1.9312e-05,  6.5342e-06,  ..., -2.3484e-05,
         -5.5432e-06, -1.0453e-05],
        [-3.8832e-05, -2.7746e-05,  9.3579e-06,  ..., -3.3647e-05,
         -7.9870e-06, -1.4991e-05]], device='cuda:0')
Loss: 1.154139518737793


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 3795,  908,  587],
        [   0,  368, 2418,  ..., 3275, 1116, 5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7945e-05,  1.8134e-05, -1.0151e-04,  ...,  1.0898e-05,
         -3.8950e-05, -6.6567e-06],
        [-2.0504e-05, -1.4648e-05,  4.9770e-06,  ..., -1.7792e-05,
         -4.2245e-06, -7.9796e-06],
        [-2.1294e-05, -1.5184e-05,  5.1595e-06,  ..., -1.8463e-05,
         -4.3809e-06, -8.2776e-06],
        [-3.1501e-05, -2.2486e-05,  7.6443e-06,  ..., -2.7359e-05,
         -6.4708e-06, -1.2234e-05],
        [-4.5270e-05, -3.2306e-05,  1.0960e-05,  ..., -3.9220e-05,
         -9.3207e-06, -1.7554e-05]], device='cuda:0')
Loss: 1.1464800834655762


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  3605,  2572,  ...,   300,   259,  1513],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8838e-05,  1.8453e-05, -8.0197e-05,  ..., -2.6345e-05,
         -3.9049e-05, -2.9926e-05],
        [-2.3440e-05, -1.6749e-05,  5.6773e-06,  ..., -2.0355e-05,
         -4.8131e-06, -9.0972e-06],
        [-2.4319e-05, -1.7345e-05,  5.8822e-06,  ..., -2.1100e-05,
         -4.9844e-06, -9.4250e-06],
        [-3.6001e-05, -2.5719e-05,  8.7246e-06,  ..., -3.1292e-05,
         -7.3723e-06, -1.3955e-05],
        [-5.1767e-05, -3.6985e-05,  1.2510e-05,  ..., -4.4912e-05,
         -1.0625e-05, -2.0027e-05]], device='cuda:0')
Loss: 1.1644364595413208
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9036e-05, -1.9662e-05, -3.9031e-05,  ...,  9.5206e-06,
         -3.3445e-05,  1.6876e-05],
        [-2.0249e-05, -1.4426e-05,  3.0444e-06,  ..., -1.6730e-05,
         -2.2254e-06, -7.5184e-06],
        [ 5.5655e-05,  4.1126e-05, -1.0485e-05,  ...,  4.7472e-05,
          9.9366e-06,  2.0395e-05],
        [-3.2320e-05, -2.1967e-05,  5.9843e-06,  ..., -2.6861e-05,
         -4.7758e-06, -1.2152e-05],
        [-5.5958e-05, -3.9366e-05,  8.7613e-06,  ..., -4.6638e-05,
         -7.7874e-06, -2.1604e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7015e-08, 1.4540e-08, 1.7278e-08,  ..., 1.9772e-08, 3.1316e-08,
         6.5090e-09],
        [3.6977e-11, 2.1753e-11, 1.3674e-12,  ..., 2.5208e-11, 1.1425e-12,
         5.4463e-12],
        [6.3861e-10, 3.8145e-10, 2.0734e-11,  ..., 4.9507e-10, 1.3256e-11,
         1.0334e-10],
        [1.8875e-10, 2.5802e-10, 5.0952e-12,  ..., 1.3951e-10, 6.8470e-12,
         4.8129e-11],
        [1.4472e-10, 8.0299e-11, 5.8525e-12,  ..., 9.7675e-11, 2.7082e-12,
         2.1443e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.5888048657910018e-05, 1.5888048657910018e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[  0, 792,  83,  ..., 300, 768, 932],
        [  0,  76,  15,  ...,  14, 333, 199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0757e-06, -2.3158e-06,  1.7566e-05,  ...,  9.5487e-06,
         -8.5316e-06,  1.8069e-05],
        [-2.8610e-06, -2.0564e-06,  7.0035e-07,  ..., -2.5034e-06,
         -6.0350e-07, -1.1176e-06],
        [-3.0249e-06, -2.1607e-06,  7.4133e-07,  ..., -2.6375e-06,
         -6.3702e-07, -1.1772e-06],
        [-4.5002e-06, -3.2187e-06,  1.1027e-06,  ..., -3.9339e-06,
         -9.4622e-07, -1.7509e-06],
        [-6.3777e-06, -4.5598e-06,  1.5572e-06,  ..., -5.5730e-06,
         -1.3411e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.1644216775894165


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1042, 5738,  ...,   12,  287, 3643],
        [   0,  775,  266,  ...,  409,  328, 5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0008e-05,  2.1604e-05,  2.7073e-05,  ..., -7.0845e-06,
          2.0956e-05,  3.0834e-05],
        [-5.7518e-06, -4.1276e-06,  1.4044e-06,  ..., -5.0664e-06,
         -1.2256e-06, -2.2650e-06],
        [-6.0350e-06, -4.3213e-06,  1.4752e-06,  ..., -5.3197e-06,
         -1.2890e-06, -2.3767e-06],
        [-9.0003e-06, -6.4522e-06,  2.1979e-06,  ..., -7.9274e-06,
         -1.9148e-06, -3.5316e-06],
        [-1.2696e-05, -9.0897e-06,  3.0994e-06,  ..., -1.1206e-05,
         -2.7046e-06, -4.9770e-06]], device='cuda:0')
Loss: 1.1385704278945923


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,  1235,    14,  ...,  3301,   549,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4274e-06,  4.5679e-05,  1.3133e-05,  ..., -2.5388e-05,
          4.7483e-05,  4.0786e-05],
        [-8.6278e-06, -6.1989e-06,  2.1011e-06,  ..., -7.5996e-06,
         -1.8254e-06, -3.3826e-06],
        [-9.0450e-06, -6.4969e-06,  2.2054e-06,  ..., -7.9870e-06,
         -1.9185e-06, -3.5539e-06],
        [-1.3530e-05, -9.7007e-06,  3.2932e-06,  ..., -1.1921e-05,
         -2.8573e-06, -5.2974e-06],
        [-1.9044e-05, -1.3649e-05,  4.6343e-06,  ..., -1.6809e-05,
         -4.0308e-06, -7.4506e-06]], device='cuda:0')
Loss: 1.1522225141525269


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1555e-05,  4.9647e-05,  1.6284e-05,  ..., -3.7147e-05,
          5.6892e-05,  5.1985e-05],
        [-1.1548e-05, -8.2850e-06,  2.7940e-06,  ..., -1.0177e-05,
         -2.4252e-06, -4.5225e-06],
        [-1.2085e-05, -8.6576e-06,  2.9281e-06,  ..., -1.0669e-05,
         -2.5406e-06, -4.7386e-06],
        [-1.8060e-05, -1.2949e-05,  4.3735e-06,  ..., -1.5944e-05,
         -3.7886e-06, -7.0706e-06],
        [-2.5451e-05, -1.8239e-05,  6.1542e-06,  ..., -2.2471e-05,
         -5.3495e-06, -9.9540e-06]], device='cuda:0')
Loss: 1.1540837287902832


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,   300,   344,  ...,    14,  5077,  2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6473e-05,  7.3029e-05,  2.3204e-06,  ..., -5.9644e-05,
          9.5208e-05,  8.6824e-05],
        [-1.4439e-05, -1.0371e-05,  3.4906e-06,  ..., -1.2711e-05,
         -3.0585e-06, -5.6922e-06],
        [-1.5140e-05, -1.0863e-05,  3.6657e-06,  ..., -1.3337e-05,
         -3.2075e-06, -5.9754e-06],
        [-2.2620e-05, -1.6227e-05,  5.4687e-06,  ..., -1.9908e-05,
         -4.7795e-06, -8.9034e-06],
        [-3.1888e-05, -2.2888e-05,  7.7039e-06,  ..., -2.8074e-05,
         -6.7577e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1340752840042114


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 4852,  266,  ..., 2523, 2080, 2632],
        [   0, 1234,  278,  ..., 8635,  271,  546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0435e-06,  4.0523e-05,  3.9860e-06,  ..., -5.4059e-05,
          7.1405e-05,  8.2622e-05],
        [-1.7300e-05, -1.2442e-05,  4.1947e-06,  ..., -1.5259e-05,
         -3.6396e-06, -6.8098e-06],
        [-1.8165e-05, -1.3053e-05,  4.4107e-06,  ..., -1.6019e-05,
         -3.8184e-06, -7.1600e-06],
        [-2.7120e-05, -1.9476e-05,  6.5789e-06,  ..., -2.3901e-05,
         -5.6848e-06, -1.0662e-05],
        [-3.8236e-05, -2.7478e-05,  9.2611e-06,  ..., -3.3706e-05,
         -8.0466e-06, -1.5020e-05]], device='cuda:0')
Loss: 1.1450536251068115


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,  4868,  1027,  ...,   409,  3047,  2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0240e-06,  6.7997e-05, -1.3549e-05,  ..., -4.5553e-05,
          6.3672e-05,  7.0422e-05],
        [-2.0221e-05, -1.4558e-05,  4.8876e-06,  ..., -1.7807e-05,
         -4.2468e-06, -7.9572e-06],
        [-2.1204e-05, -1.5259e-05,  5.1335e-06,  ..., -1.8671e-05,
         -4.4517e-06, -8.3521e-06],
        [-3.1650e-05, -2.2739e-05,  7.6517e-06,  ..., -2.7835e-05,
         -6.6198e-06, -1.2428e-05],
        [-4.4644e-05, -3.2097e-05,  1.0774e-05,  ..., -3.9279e-05,
         -9.3803e-06, -1.7524e-05]], device='cuda:0')
Loss: 1.1484429836273193


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[   0,   12, 1250,  ...,  381, 1524, 2204],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8713e-05,  6.6774e-05, -1.2886e-05,  ..., -3.6969e-05,
          6.2592e-05,  1.3269e-04],
        [-2.3127e-05, -1.6630e-05,  5.6028e-06,  ..., -2.0340e-05,
         -4.8280e-06, -9.0823e-06],
        [-2.4244e-05, -1.7434e-05,  5.8785e-06,  ..., -2.1324e-05,
         -5.0589e-06, -9.5218e-06],
        [-3.6180e-05, -2.5973e-05,  8.7619e-06,  ..., -3.1799e-05,
         -7.5214e-06, -1.4171e-05],
        [-5.1111e-05, -3.6687e-05,  1.2353e-05,  ..., -4.4912e-05,
         -1.0669e-05, -2.0012e-05]], device='cuda:0')
Loss: 1.1643105745315552
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2261e-05, -1.1018e-05, -3.6417e-05,  ...,  4.8716e-06,
         -2.3842e-05,  2.8458e-05],
        [-2.0537e-05, -1.4646e-05,  3.3002e-06,  ..., -1.7091e-05,
         -2.4856e-06, -7.6748e-06],
        [ 4.7665e-05,  3.5270e-05, -8.8489e-06,  ...,  4.0592e-05,
          8.4371e-06,  1.7403e-05],
        [-3.2706e-05, -2.2367e-05,  6.2620e-06,  ..., -2.7355e-05,
         -5.0503e-06, -1.2354e-05],
        [-5.5474e-05, -3.9098e-05,  9.1205e-06,  ..., -4.6466e-05,
         -8.0756e-06, -2.1445e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6969e-08, 1.4530e-08, 1.7261e-08,  ..., 1.9754e-08, 3.1288e-08,
         6.5201e-09],
        [3.7475e-11, 2.2008e-11, 1.3975e-12,  ..., 2.5597e-11, 1.1647e-12,
         5.5233e-12],
        [6.3856e-10, 3.8137e-10, 2.0748e-11,  ..., 4.9503e-10, 1.3268e-11,
         1.0333e-10],
        [1.8987e-10, 2.5844e-10, 5.1669e-12,  ..., 1.4038e-10, 6.8967e-12,
         4.8282e-11],
        [1.4719e-10, 8.1565e-11, 5.9992e-12,  ..., 9.9594e-11, 2.8193e-12,
         2.1823e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.5686437100081734e-05, 1.5686437100081734e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[   0,   14,  560,  ..., 1248, 1398, 1268],
        [   0,  474,  221,  ..., 2945,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5908e-05,  2.9952e-05,  0.0000e+00,  ..., -8.5952e-06,
         -3.9853e-05, -1.0582e-05],
        [-2.8163e-06, -2.0564e-06,  7.3016e-07,  ..., -2.5183e-06,
         -6.2585e-07, -1.1399e-06],
        [-3.0249e-06, -2.2054e-06,  7.8231e-07,  ..., -2.6971e-06,
         -6.7055e-07, -1.2293e-06],
        [-4.5300e-06, -3.3081e-06,  1.1772e-06,  ..., -4.0531e-06,
         -1.0058e-06, -1.8403e-06],
        [-6.2585e-06, -4.5598e-06,  1.6168e-06,  ..., -5.6028e-06,
         -1.3858e-06, -2.5332e-06]], device='cuda:0')
Loss: 1.1413493156433105


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0,   616,  4935,  ...,    89,  4448,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4548e-05,  7.3382e-05,  1.7822e-05,  ..., -9.1517e-07,
         -1.6236e-05, -3.1213e-05],
        [-5.6922e-06, -4.1276e-06,  1.4380e-06,  ..., -5.0664e-06,
         -1.2964e-06, -2.3320e-06],
        [-6.0797e-06, -4.4107e-06,  1.5348e-06,  ..., -5.4091e-06,
         -1.3858e-06, -2.4959e-06],
        [-9.0301e-06, -6.5416e-06,  2.2799e-06,  ..., -8.0168e-06,
         -2.0564e-06, -3.6955e-06],
        [-1.2666e-05, -9.1791e-06,  3.1888e-06,  ..., -1.1265e-05,
         -2.8834e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.167731523513794


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2208e-05,  7.9562e-05, -2.8128e-05,  ..., -3.9187e-06,
         -5.1557e-06, -5.2630e-05],
        [-8.5086e-06, -6.1840e-06,  2.1793e-06,  ..., -7.5996e-06,
         -1.9222e-06, -3.5167e-06],
        [-9.0599e-06, -6.5863e-06,  2.3171e-06,  ..., -8.0764e-06,
         -2.0452e-06, -3.7402e-06],
        [-1.3560e-05, -9.8497e-06,  3.4720e-06,  ..., -1.2070e-05,
         -3.0547e-06, -5.5879e-06],
        [-1.8895e-05, -1.3709e-05,  4.8205e-06,  ..., -1.6838e-05,
         -4.2617e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.150733470916748


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  2192,  2534,   287],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7821e-05,  8.1728e-05, -5.6780e-05,  ..., -2.5897e-05,
         -3.6276e-06, -6.1995e-05],
        [-1.1355e-05, -8.2552e-06,  2.8796e-06,  ..., -1.0148e-05,
         -2.5146e-06, -4.6566e-06],
        [-1.2070e-05, -8.7917e-06,  3.0585e-06,  ..., -1.0774e-05,
         -2.6710e-06, -4.9472e-06],
        [-1.8060e-05, -1.3128e-05,  4.5821e-06,  ..., -1.6093e-05,
         -3.9861e-06, -7.3910e-06],
        [-2.5183e-05, -1.8299e-05,  6.3702e-06,  ..., -2.2441e-05,
         -5.5656e-06, -1.0312e-05]], device='cuda:0')
Loss: 1.1684229373931885


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[   0,  504,  546,  ...,  634,  328,  630],
        [   0, 1730, 2068,  ...,  445, 2704,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7887e-05,  1.2894e-04, -2.6614e-05,  ...,  2.4251e-05,
         -4.5000e-05, -4.0859e-05],
        [-1.4216e-05, -1.0356e-05,  3.6135e-06,  ..., -1.2696e-05,
         -3.1292e-06, -5.8264e-06],
        [-1.5154e-05, -1.1042e-05,  3.8482e-06,  ..., -1.3515e-05,
         -3.3304e-06, -6.1989e-06],
        [-2.2620e-05, -1.6466e-05,  5.7518e-06,  ..., -2.0146e-05,
         -4.9621e-06, -9.2387e-06],
        [-3.1561e-05, -2.2948e-05,  8.0019e-06,  ..., -2.8104e-05,
         -6.9290e-06, -1.2904e-05]], device='cuda:0')
Loss: 1.156140923500061


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  5885,   271,  ...,   278,  1049,    12],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2612e-05,  1.2259e-04, -2.8945e-05,  ...,  3.3021e-05,
         -4.3108e-05, -5.7763e-05],
        [-1.7062e-05, -1.2428e-05,  4.2953e-06,  ..., -1.5229e-05,
         -3.7216e-06, -6.9663e-06],
        [-1.8179e-05, -1.3247e-05,  4.5747e-06,  ..., -1.6212e-05,
         -3.9600e-06, -7.4059e-06],
        [-2.7180e-05, -1.9789e-05,  6.8471e-06,  ..., -2.4199e-05,
         -5.9046e-06, -1.1057e-05],
        [-3.7879e-05, -2.7537e-05,  9.5144e-06,  ..., -3.3736e-05,
         -8.2478e-06, -1.5423e-05]], device='cuda:0')
Loss: 1.1646287441253662


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0,  360,  259,  ...,   12,  358,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6947e-07,  1.0854e-04, -5.9471e-06,  ...,  3.8738e-05,
         -5.9620e-05, -5.6595e-05],
        [-1.9908e-05, -1.4484e-05,  4.9770e-06,  ..., -1.7762e-05,
         -4.3251e-06, -8.1137e-06],
        [-2.1219e-05, -1.5438e-05,  5.3011e-06,  ..., -1.8910e-05,
         -4.6045e-06, -8.6278e-06],
        [-3.1710e-05, -2.3067e-05,  7.9349e-06,  ..., -2.8223e-05,
         -6.8657e-06, -1.2882e-05],
        [-4.4256e-05, -3.2127e-05,  1.1042e-05,  ..., -3.9399e-05,
         -9.5963e-06, -1.7986e-05]], device='cuda:0')
Loss: 1.1497801542282104


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9528e-05,  1.1662e-04, -1.9533e-05,  ...,  4.1519e-05,
         -6.9051e-05, -6.4853e-05],
        [-2.2769e-05, -1.6555e-05,  5.6736e-06,  ..., -2.0280e-05,
         -4.9472e-06, -9.2760e-06],
        [ 7.2657e-05,  4.9322e-05, -1.5814e-05,  ...,  5.6659e-05,
          8.9508e-06,  2.5235e-05],
        [-3.6269e-05, -2.6360e-05,  9.0450e-06,  ..., -3.2216e-05,
         -7.8492e-06, -1.4730e-05],
        [-5.0634e-05, -3.6716e-05,  1.2591e-05,  ..., -4.5002e-05,
         -1.0982e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.145816683769226
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0270,  ...,  0.0282, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7082e-05,  1.7457e-06, -3.4728e-05,  ...,  8.5364e-06,
         -2.8363e-05,  1.9127e-05],
        [-2.0760e-05, -1.4837e-05,  3.5376e-06,  ..., -1.7410e-05,
         -2.7318e-06, -7.8349e-06],
        [ 5.0164e-05,  3.6675e-05, -9.5454e-06,  ...,  4.2199e-05,
          8.4884e-06,  1.8186e-05],
        [-3.3063e-05, -2.2767e-05,  6.5403e-06,  ..., -2.7841e-05,
         -5.3302e-06, -1.2592e-05],
        [-5.4990e-05, -3.8860e-05,  9.4676e-06,  ..., -4.6319e-05,
         -8.3662e-06, -2.1357e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6922e-08, 1.4529e-08, 1.7244e-08,  ..., 1.9736e-08, 3.1262e-08,
         6.5178e-09],
        [3.7956e-11, 2.2260e-11, 1.4283e-12,  ..., 2.5982e-11, 1.1880e-12,
         5.6038e-12],
        [6.4320e-10, 3.8343e-10, 2.0977e-11,  ..., 4.9775e-10, 1.3335e-11,
         1.0386e-10],
        [1.9099e-10, 2.5888e-10, 5.2435e-12,  ..., 1.4128e-10, 6.9514e-12,
         4.8450e-11],
        [1.4961e-10, 8.2831e-11, 6.1518e-12,  ..., 1.0152e-10, 2.9371e-12,
         2.2224e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.5481349926128634e-05, 1.5481349926128634e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2160e-05, -1.6272e-05,  2.4460e-05,  ..., -2.2686e-05,
          4.9263e-06,  2.1370e-05],
        [-2.8610e-06, -2.0713e-06,  7.1526e-07,  ..., -2.5183e-06,
         -6.3330e-07, -1.1548e-06],
        [-3.1143e-06, -2.2650e-06,  7.7859e-07,  ..., -2.7418e-06,
         -6.8918e-07, -1.2591e-06],
        [ 1.9270e-04,  1.1877e-04, -3.2444e-05,  ...,  1.3645e-04,
          3.5897e-05,  6.8474e-05],
        [-6.4373e-06, -4.6492e-06,  1.6019e-06,  ..., -5.6624e-06,
         -1.4231e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.153494954109192


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,  3351,   352,  ...,    17,   287,   357],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4356e-06, -2.7463e-05,  1.1807e-05,  ..., -1.3296e-05,
         -2.6735e-05,  3.0743e-06],
        [-5.6922e-06, -4.1276e-06,  1.4566e-06,  ..., -5.0664e-06,
         -1.2629e-06, -2.3469e-06],
        [-6.1393e-06, -4.4703e-06,  1.5721e-06,  ..., -5.4687e-06,
         -1.3635e-06, -2.5332e-06],
        [ 1.8817e-04,  1.1546e-04, -3.1252e-05,  ...,  1.3236e-04,
          3.4883e-05,  6.6566e-05],
        [-1.2666e-05, -9.1791e-06,  3.2336e-06,  ..., -1.1265e-05,
         -2.8089e-06, -5.2154e-06]], device='cuda:0')
Loss: 1.1517757177352905


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  565,   27,  ...,   88, 4451,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2921e-05, -7.1926e-06,  1.8728e-05,  ..., -2.2576e-05,
         -3.1076e-05, -4.0816e-05],
        [-8.5235e-06, -6.1989e-06,  2.1681e-06,  ..., -7.5847e-06,
         -1.8887e-06, -3.5316e-06],
        [-9.1940e-06, -6.7055e-06,  2.3358e-06,  ..., -8.1807e-06,
         -2.0340e-06, -3.8072e-06],
        [ 1.8364e-04,  1.1214e-04, -3.0112e-05,  ...,  1.2834e-04,
          3.3885e-05,  6.4674e-05],
        [-1.8984e-05, -1.3828e-05,  4.8131e-06,  ..., -1.6868e-05,
         -4.2021e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.1346887350082397


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0, 1445, 3597,  ...,  281,   78,    9],
        [   0, 4448,   12,  ..., 3183,  328, 9559]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5538e-05,  2.2228e-05,  3.2118e-05,  ..., -2.2576e-05,
         -1.4202e-05, -6.7886e-06],
        [-1.1355e-05, -8.2701e-06,  2.9057e-06,  ..., -1.0103e-05,
         -2.5295e-06, -4.7237e-06],
        [-1.2264e-05, -8.9407e-06,  3.1330e-06,  ..., -1.0908e-05,
         -2.7269e-06, -5.0962e-06],
        [ 1.7911e-04,  1.0883e-04, -2.8927e-05,  ...,  1.2432e-04,
          3.2864e-05,  6.2774e-05],
        [-2.5302e-05, -1.8418e-05,  6.4448e-06,  ..., -2.2471e-05,
         -5.6326e-06, -1.0490e-05]], device='cuda:0')
Loss: 1.1548891067504883


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,   292, 15087,  ...,  2675,  1663,    12],
        [    0,  5862,    13,  ..., 12497,   287,  3570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2166e-04,  5.5335e-05,  2.7100e-05,  ..., -4.1734e-05,
         -8.6414e-06, -2.9230e-05],
        [-1.4156e-05, -1.0341e-05,  3.6284e-06,  ..., -1.2606e-05,
         -3.1590e-06, -5.9158e-06],
        [-1.5303e-05, -1.1191e-05,  3.9227e-06,  ..., -1.3635e-05,
         -3.4086e-06, -6.3851e-06],
        [ 1.7458e-04,  1.0549e-04, -2.7757e-05,  ...,  1.2026e-04,
          3.1851e-05,  6.0867e-05],
        [-3.1561e-05, -2.3037e-05,  8.0615e-06,  ..., -2.8074e-05,
         -7.0408e-06, -1.3143e-05]], device='cuda:0')
Loss: 1.145351767539978


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[   0, 5489,   80,  ...,  221,  380,  333],
        [   0,  898,  266,  ...,   12, 3222, 8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2358e-04,  5.7901e-05, -3.1319e-05,  ..., -3.2988e-05,
         -3.0460e-05, -6.9975e-05],
        [-1.6972e-05, -1.2383e-05,  4.3549e-06,  ..., -1.5095e-05,
         -3.7700e-06, -7.1004e-06],
        [-1.8358e-05, -1.3411e-05,  4.7162e-06,  ..., -1.6347e-05,
         -4.0717e-06, -7.6666e-06],
        [ 1.7002e-04,  1.0218e-04, -2.6580e-05,  ...,  1.1624e-04,
          3.0867e-05,  5.8967e-05],
        [-3.7819e-05, -2.7567e-05,  9.6783e-06,  ..., -3.3617e-05,
         -8.3968e-06, -1.5765e-05]], device='cuda:0')
Loss: 1.1610032320022583


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0,   381, 13565,  ...,     9,   847,   300],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2567e-04,  1.0160e-04, -4.1328e-05,  ..., -4.8248e-05,
         -7.2268e-06, -5.2546e-05],
        [-1.9848e-05, -1.4484e-05,  5.0589e-06,  ..., -1.7643e-05,
         -4.4145e-06, -8.2925e-06],
        [-2.1443e-05, -1.5676e-05,  5.4725e-06,  ..., -1.9088e-05,
         -4.7646e-06, -8.9481e-06],
        [ 1.6546e-04,  9.8844e-05, -2.5455e-05,  ...,  1.1219e-04,
          2.9847e-05,  5.7074e-05],
        [-4.4227e-05, -3.2246e-05,  1.1250e-05,  ..., -3.9309e-05,
         -9.8422e-06, -1.8418e-05]], device='cuda:0')
Loss: 1.1505590677261353


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[   0, 5896,  352,  ..., 1168,  767, 1390],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5519e-04,  9.4427e-05, -7.0562e-05,  ..., -5.9031e-05,
         -3.0055e-06, -4.5917e-05],
        [-2.2665e-05, -1.6525e-05,  5.7742e-06,  ..., -2.0161e-05,
         -5.0515e-06, -9.4697e-06],
        [-2.4512e-05, -1.7911e-05,  6.2510e-06,  ..., -2.1830e-05,
         -5.4576e-06, -1.0230e-05],
        [ 1.6093e-04,  9.5551e-05, -2.4300e-05,  ...,  1.0814e-04,
          2.8826e-05,  5.5182e-05],
        [-5.0485e-05, -3.6806e-05,  1.2845e-05,  ..., -4.4912e-05,
         -1.1258e-05, -2.1026e-05]], device='cuda:0')
Loss: 1.1665160655975342
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0270,  ...,  0.0282, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9893e-05,  1.1014e-05, -3.8312e-05,  ...,  1.7796e-06,
         -2.5827e-05,  1.2623e-05],
        [-2.0951e-05, -1.5006e-05,  3.7612e-06,  ..., -1.7685e-05,
         -2.9638e-06, -7.9984e-06],
        [ 4.2697e-05,  3.1216e-05, -7.9658e-06,  ...,  3.5796e-05,
          7.0938e-06,  1.5345e-05],
        [-1.3663e-05, -1.0935e-05,  3.4563e-06,  ..., -1.4243e-05,
         -1.9146e-06, -5.8143e-06],
        [-5.4539e-05, -3.8654e-05,  9.8053e-06,  ..., -4.6178e-05,
         -8.6554e-06, -2.1324e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6899e-08, 1.4523e-08, 1.7232e-08,  ..., 1.9719e-08, 3.1230e-08,
         6.5134e-09],
        [3.8432e-11, 2.2511e-11, 1.4602e-12,  ..., 2.6363e-11, 1.2123e-12,
         5.6879e-12],
        [6.4315e-10, 3.8336e-10, 2.0995e-11,  ..., 4.9773e-10, 1.3351e-11,
         1.0386e-10],
        [2.1670e-10, 2.6775e-10, 5.8287e-12,  ..., 1.5283e-10, 7.7754e-12,
         5.1447e-11],
        [1.5200e-10, 8.4103e-11, 6.3106e-12,  ..., 1.0344e-10, 3.0609e-12,
         2.2643e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.5272912487703465e-05, 1.5272912487703465e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3675e-05, -1.5866e-06, -3.7995e-06,  ..., -4.3154e-06,
          1.5093e-05,  1.6188e-05],
        [-2.8163e-06, -2.0564e-06,  7.4133e-07,  ..., -2.4885e-06,
         -6.7428e-07, -1.2219e-06],
        [-3.0994e-06, -2.2650e-06,  8.1584e-07,  ..., -2.7418e-06,
         -7.3761e-07, -1.3411e-06],
        [-4.5896e-06, -3.3379e-06,  1.2144e-06,  ..., -4.0531e-06,
         -1.0952e-06, -1.9819e-06],
        [-6.3777e-06, -4.6492e-06,  1.6764e-06,  ..., -5.6326e-06,
         -1.5199e-06, -2.7567e-06]], device='cuda:0')
Loss: 1.154062032699585


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    13,   786,  ...,   275,  2623,    13],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4125e-05,  4.7512e-05, -5.2482e-05,  ..., -1.4384e-05,
         -1.1139e-05,  3.4083e-05],
        [-5.6177e-06, -4.1127e-06,  1.4789e-06,  ..., -4.9770e-06,
         -1.3262e-06, -2.4214e-06],
        [ 8.9884e-05,  6.6970e-05, -1.0445e-05,  ...,  8.5845e-05,
          1.6381e-05,  3.5852e-05],
        [-9.1493e-06, -6.6906e-06,  2.4140e-06,  ..., -8.1062e-06,
         -2.1532e-06, -3.9339e-06],
        [-1.2696e-05, -9.2685e-06,  3.3304e-06,  ..., -1.1235e-05,
         -2.9802e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.166426420211792


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,  1304,   292,  ...,  2101,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0249e-06,  3.3679e-05, -6.2745e-05,  ..., -2.8778e-06,
         -1.0764e-05,  3.4083e-05],
        [-8.4490e-06, -6.1542e-06,  2.1979e-06,  ..., -7.4655e-06,
         -1.9893e-06, -3.6210e-06],
        [ 8.6755e-05,  6.4705e-05, -9.6479e-06,  ...,  8.3089e-05,
          1.5647e-05,  3.4526e-05],
        [-1.3798e-05, -1.0058e-05,  3.5986e-06,  ..., -1.2219e-05,
         -3.2410e-06, -5.9158e-06],
        [-1.9014e-05, -1.3828e-05,  4.9397e-06,  ..., -1.6809e-05,
         -4.4629e-06, -8.1360e-06]], device='cuda:0')
Loss: 1.139107346534729


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[   0,   14,   23,  ...,  278,  266, 1462],
        [   0,   14,  381,  ..., 7106,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1766e-05,  5.0846e-05, -7.5956e-05,  ..., -2.0656e-05,
         -1.8800e-05,  6.7812e-05],
        [-1.1250e-05, -8.1956e-06,  2.9169e-06,  ..., -9.9540e-06,
         -2.6301e-06, -4.7982e-06],
        [ 8.3655e-05,  6.2455e-05, -8.8506e-06,  ...,  8.0332e-05,
          1.4939e-05,  3.3230e-05],
        [-1.8388e-05, -1.3411e-05,  4.7833e-06,  ..., -1.6332e-05,
         -4.2915e-06, -7.8380e-06],
        [-2.5302e-05, -1.8418e-05,  6.5565e-06,  ..., -2.2411e-05,
         -5.9083e-06, -1.0774e-05]], device='cuda:0')
Loss: 1.1479365825653076


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[   0, 1101,  300,  ..., 6104,  367,  993],
        [   0, 3592,  417,  ..., 4893,  328,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6690e-05,  5.9579e-05, -9.4407e-05,  ..., -1.4356e-05,
         -6.1539e-05,  7.8207e-05],
        [-1.4067e-05, -1.0222e-05,  3.6582e-06,  ..., -1.2413e-05,
         -3.2783e-06, -5.9977e-06],
        [ 8.0526e-05,  6.0205e-05, -8.0274e-06,  ...,  7.7605e-05,
          1.4220e-05,  3.1903e-05],
        [-2.3037e-05, -1.6734e-05,  6.0052e-06,  ..., -2.0385e-05,
         -5.3570e-06, -9.8050e-06],
        [-3.1590e-05, -2.2948e-05,  8.2105e-06,  ..., -2.7925e-05,
         -7.3612e-06, -1.3441e-05]], device='cuda:0')
Loss: 1.1483858823776245


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  300, 2607,  ..., 1279,  368,  266],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6841e-05,  9.6624e-05, -1.4093e-04,  ..., -5.6156e-06,
         -6.1539e-05,  9.2841e-05],
        [-1.6898e-05, -1.2338e-05,  4.3996e-06,  ..., -1.4916e-05,
         -3.9674e-06, -7.2345e-06],
        [ 7.7412e-05,  5.7880e-05, -7.2078e-06,  ...,  7.4833e-05,
          1.3464e-05,  3.0540e-05],
        [-2.7627e-05, -2.0161e-05,  7.2122e-06,  ..., -2.4468e-05,
         -6.4746e-06, -1.1817e-05],
        [-3.7879e-05, -2.7627e-05,  9.8571e-06,  ..., -3.3498e-05,
         -8.8885e-06, -1.6183e-05]], device='cuda:0')
Loss: 1.157581090927124


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5981e-05,  1.1009e-04, -1.2004e-04,  ..., -1.0062e-05,
         -7.3701e-05,  1.1125e-04],
        [-1.9759e-05, -1.4439e-05,  5.1185e-06,  ..., -1.7434e-05,
         -4.6492e-06, -8.4490e-06],
        [ 7.4267e-05,  5.5570e-05, -6.4180e-06,  ...,  7.2077e-05,
          1.2719e-05,  2.9214e-05],
        [-3.2246e-05, -2.3559e-05,  8.3745e-06,  ..., -2.8521e-05,
         -7.5698e-06, -1.3769e-05],
        [-4.4316e-05, -3.2365e-05,  1.1466e-05,  ..., -3.9160e-05,
         -1.0416e-05, -1.8910e-05]], device='cuda:0')
Loss: 1.1386281251907349


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0,  1716,   271,  ...,   292,    78,  1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8217e-06,  9.6780e-05, -1.2372e-04,  ...,  1.3027e-05,
         -8.3138e-05,  9.2826e-05],
        [-2.2590e-05, -1.6510e-05,  5.8450e-06,  ..., -1.9923e-05,
         -5.3272e-06, -9.6560e-06],
        [ 7.1123e-05,  5.3276e-05, -5.6134e-06,  ...,  6.9320e-05,
          1.1970e-05,  2.7880e-05],
        [-3.6836e-05, -2.6926e-05,  9.5591e-06,  ..., -3.2574e-05,
         -8.6650e-06, -1.5721e-05],
        [-5.0694e-05, -3.7044e-05,  1.3098e-05,  ..., -4.4763e-05,
         -1.1936e-05, -2.1622e-05]], device='cuda:0')
Loss: 1.152794361114502
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6386e-05,  1.9590e-05, -4.6853e-05,  ...,  2.9043e-06,
         -3.1558e-05,  2.0643e-05],
        [-2.1115e-05, -1.5157e-05,  3.9696e-06,  ..., -1.7909e-05,
         -3.2001e-06, -8.1642e-06],
        [ 4.5539e-05,  3.3422e-05, -7.7305e-06,  ...,  3.9148e-05,
          7.5814e-06,  1.6598e-05],
        [-1.5980e-05, -1.2534e-05,  4.0666e-06,  ..., -1.6077e-05,
         -2.5896e-06, -6.8050e-06],
        [-5.4155e-05, -3.8493e-05,  1.0135e-05,  ..., -4.6037e-05,
         -8.9834e-06, -2.1353e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6852e-08, 1.4518e-08, 1.7230e-08,  ..., 1.9700e-08, 3.1206e-08,
         6.5155e-09],
        [3.8903e-11, 2.2761e-11, 1.4929e-12,  ..., 2.6733e-11, 1.2395e-12,
         5.7754e-12],
        [6.4757e-10, 3.8582e-10, 2.1006e-11,  ..., 5.0203e-10, 1.3481e-11,
         1.0453e-10],
        [2.1784e-10, 2.6820e-10, 5.9143e-12,  ..., 1.5374e-10, 7.8427e-12,
         5.1642e-11],
        [1.5442e-10, 8.5391e-11, 6.4758e-12,  ..., 1.0534e-10, 3.2003e-12,
         2.3088e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.5061252184179384e-05, 1.5061252184179384e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   199,  7513,  ...,   271,   259,   957],
        [    0,   266,   858,  ..., 11265,   607,  7455]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9600e-05, -3.1189e-05, -3.2844e-06,  ..., -1.9368e-05,
          3.9930e-06,  2.9637e-05],
        [-2.8014e-06, -2.0415e-06,  7.3016e-07,  ..., -2.4885e-06,
         -7.0035e-07, -1.2591e-06],
        [-3.1441e-06, -2.2948e-06,  8.1956e-07,  ..., -2.8014e-06,
         -7.8231e-07, -1.4156e-06],
        [-4.5896e-06, -3.3379e-06,  1.1995e-06,  ..., -4.0829e-06,
         -1.1399e-06, -2.0564e-06],
        [-6.2883e-06, -4.5896e-06,  1.6391e-06,  ..., -5.6028e-06,
         -1.5646e-06, -2.8312e-06]], device='cuda:0')
Loss: 1.1442193984985352


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5489e-05, -4.6358e-05, -2.1671e-05,  ..., -3.8158e-05,
         -2.1456e-05,  1.9420e-05],
        [-5.5730e-06, -4.0531e-06,  1.4640e-06,  ..., -4.9770e-06,
         -1.3597e-06, -2.4959e-06],
        [-6.2287e-06, -4.5449e-06,  1.6391e-06,  ..., -5.5730e-06,
         -1.5162e-06, -2.8014e-06],
        [-9.1791e-06, -6.6757e-06,  2.4140e-06,  ..., -8.1956e-06,
         -2.2277e-06, -4.0978e-06],
        [-1.2547e-05, -9.1493e-06,  3.3006e-06,  ..., -1.1206e-05,
         -3.0547e-06, -5.6326e-06]], device='cuda:0')
Loss: 1.1609008312225342


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   344,  8260,  ..., 16020, 18216, 11348],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0646e-05, -6.8670e-05, -2.8271e-05,  ..., -1.2798e-05,
         -2.2855e-05,  4.3667e-05],
        [-8.3745e-06, -6.1095e-06,  2.2128e-06,  ..., -7.4357e-06,
         -2.0228e-06, -3.7253e-06],
        [ 8.5966e-05,  6.0718e-05, -2.3468e-05,  ...,  7.4092e-05,
          2.3171e-05,  3.7371e-05],
        [-1.3828e-05, -1.0088e-05,  3.6582e-06,  ..., -1.2279e-05,
         -3.3230e-06, -6.1393e-06],
        [ 7.0900e-05,  4.8990e-05, -2.2288e-05,  ...,  6.3092e-05,
          1.9038e-05,  2.5392e-05]], device='cuda:0')
Loss: 1.152116298675537


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   271,   266,  7246],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9398e-05, -8.9072e-05, -7.0186e-05,  ..., -3.2656e-06,
         -4.0456e-05,  3.4161e-05],
        [-1.1146e-05, -8.1509e-06,  2.9579e-06,  ..., -9.9093e-06,
         -2.7008e-06, -4.9844e-06],
        [ 8.2852e-05,  5.8424e-05, -2.2630e-05,  ...,  7.1305e-05,
          2.2407e-05,  3.5955e-05],
        [-1.8418e-05, -1.3441e-05,  4.8876e-06,  ..., -1.6361e-05,
         -4.4405e-06, -8.2105e-06],
        [ 6.4611e-05,  4.4400e-05, -2.0604e-05,  ...,  5.7489e-05,
          1.7503e-05,  2.2546e-05]], device='cuda:0')
Loss: 1.1469364166259766


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0, 10205,   342,  ...,  2523,  4729, 13753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5910e-05, -1.3229e-04, -5.7413e-05,  ...,  1.7199e-05,
         -5.5491e-05,  3.8956e-05],
        [-1.3947e-05, -1.0222e-05,  3.7067e-06,  ..., -1.2398e-05,
         -3.3677e-06, -6.2361e-06],
        [ 7.9693e-05,  5.6084e-05, -2.1788e-05,  ...,  6.8504e-05,
          2.1655e-05,  3.4547e-05],
        [-2.3037e-05, -1.6853e-05,  6.1244e-06,  ..., -2.0444e-05,
         -5.5358e-06, -1.0267e-05],
        [ 5.8264e-05,  3.9721e-05, -1.8913e-05,  ...,  5.1886e-05,
          1.5991e-05,  1.9730e-05]], device='cuda:0')
Loss: 1.1604551076889038


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6326e-05, -1.4161e-04, -3.9982e-05,  ..., -1.1385e-05,
         -3.7878e-05,  6.0778e-05],
        [-1.6704e-05, -1.2264e-05,  4.4405e-06,  ..., -1.4871e-05,
         -4.0308e-06, -7.4655e-06],
        [ 7.6593e-05,  5.3789e-05, -2.0965e-05,  ...,  6.5732e-05,
          2.0910e-05,  3.3176e-05],
        [-2.7597e-05, -2.0221e-05,  7.3388e-06,  ..., -2.4527e-05,
         -6.6310e-06, -1.2293e-05],
        [ 5.2035e-05,  3.5132e-05, -1.7259e-05,  ...,  4.6313e-05,
          1.4493e-05,  1.6973e-05]], device='cuda:0')
Loss: 1.1320760250091553


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,    14,   747,  ...,   367,   300,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2716e-05, -1.3628e-04, -2.2831e-05,  ...,  2.5351e-06,
         -3.6549e-05,  6.3428e-05],
        [-1.9446e-05, -1.4290e-05,  5.1744e-06,  ..., -1.7330e-05,
         -4.6976e-06, -8.6948e-06],
        [ 7.3509e-05,  5.1509e-05, -2.0134e-05,  ...,  6.2960e-05,
          2.0161e-05,  3.1790e-05],
        [-3.2127e-05, -2.3589e-05,  8.5607e-06,  ..., -2.8610e-05,
         -7.7337e-06, -1.4335e-05],
        [ 4.5836e-05,  3.0542e-05, -1.5590e-05,  ...,  4.0740e-05,
          1.2988e-05,  1.4187e-05]], device='cuda:0')
Loss: 1.152540922164917


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8629e-05, -1.3733e-04, -3.3141e-05,  ...,  2.4227e-06,
         -2.5536e-05,  4.5459e-05],
        [-2.2188e-05, -1.6347e-05,  5.8934e-06,  ..., -1.9789e-05,
         -5.3756e-06, -9.9689e-06],
        [ 7.0424e-05,  4.9200e-05, -1.9326e-05,  ...,  6.0189e-05,
          1.9401e-05,  3.0360e-05],
        [-3.6657e-05, -2.6971e-05,  9.7528e-06,  ..., -3.2663e-05,
         -8.8438e-06, -1.6436e-05],
        [ 3.9637e-05,  2.5923e-05, -1.3966e-05,  ...,  3.5167e-05,
          1.1461e-05,  1.1311e-05]], device='cuda:0')
Loss: 1.1379796266555786
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0885e-05,  3.8989e-06, -4.5481e-05,  ...,  2.8562e-06,
         -3.0956e-05,  2.3125e-05],
        [-2.1222e-05, -1.5276e-05,  4.1620e-06,  ..., -1.8097e-05,
         -3.4176e-06, -8.3446e-06],
        [ 4.8028e-05,  3.5000e-05, -8.8900e-06,  ...,  4.1252e-05,
          8.7634e-06,  1.7974e-05],
        [-1.8048e-05, -1.3978e-05,  4.6352e-06,  ..., -1.7735e-05,
         -3.2151e-06, -7.7681e-06],
        [-4.4776e-05, -3.2052e-05,  7.7246e-06,  ..., -3.7916e-05,
         -6.9390e-06, -1.8087e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6806e-08, 1.4522e-08, 1.7214e-08,  ..., 1.9680e-08, 3.1176e-08,
         6.5111e-09],
        [3.9357e-11, 2.3005e-11, 1.5261e-12,  ..., 2.7098e-11, 1.2671e-12,
         5.8690e-12],
        [6.5188e-10, 3.8785e-10, 2.1358e-11,  ..., 5.0515e-10, 1.3844e-11,
         1.0535e-10],
        [2.1897e-10, 2.6866e-10, 6.0035e-12,  ..., 1.5465e-10, 7.9131e-12,
         5.1861e-11],
        [1.5584e-10, 8.5978e-11, 6.6644e-12,  ..., 1.0647e-10, 3.3285e-12,
         2.3193e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.4846498384781962e-05, 1.4846498384781962e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  747, 7890,  ...,  706, 8667,  271],
        [   0,  328,  490,  ..., 6280, 4283, 4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8922e-05, -7.8385e-06, -1.3154e-05,  ...,  0.0000e+00,
         -2.1185e-05,  2.2913e-05],
        [-2.7567e-06, -2.0415e-06,  7.6368e-07,  ..., -2.4438e-06,
         -6.8173e-07, -1.2890e-06],
        [-3.1590e-06, -2.3395e-06,  8.7172e-07,  ..., -2.7865e-06,
         -7.7859e-07, -1.4678e-06],
        [-4.5896e-06, -3.3975e-06,  1.2740e-06,  ..., -4.0829e-06,
         -1.1325e-06, -2.1458e-06],
        [-6.2883e-06, -4.6492e-06,  1.7285e-06,  ..., -5.5432e-06,
         -1.5497e-06, -2.9206e-06]], device='cuda:0')
Loss: 1.1309934854507446


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,  9058,  5481,  ...,   508, 15074,   300],
        [    0,  2790,   266,  ...,   401,  1496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6587e-06,  3.8314e-05, -1.4726e-05,  ..., -2.6046e-05,
         -6.1285e-06,  1.9486e-05],
        [-5.5283e-06, -4.0978e-06,  1.5050e-06,  ..., -4.9025e-06,
         -1.3597e-06, -2.5779e-06],
        [-6.2734e-06, -4.6492e-06,  1.7062e-06,  ..., -5.5581e-06,
         -1.5385e-06, -2.9132e-06],
        [-9.2089e-06, -6.8098e-06,  2.5108e-06,  ..., -8.1658e-06,
         -2.2575e-06, -4.2766e-06],
        [-1.2606e-05, -9.3281e-06,  3.4124e-06,  ..., -1.1146e-05,
         -3.0920e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.1459407806396484


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,  2834,   266,  ..., 39474,    12, 15441],
        [    0,   292,    46,  ...,  1217,    17,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0144e-05,  9.4188e-05,  8.6466e-06,  ..., -2.5855e-05,
         -5.5280e-06,  7.4661e-06],
        [-8.2999e-06, -6.1244e-06,  2.2352e-06,  ..., -7.3463e-06,
         -2.0191e-06, -3.8520e-06],
        [-9.4026e-06, -6.9439e-06,  2.5295e-06,  ..., -8.3297e-06,
         -2.2836e-06, -4.3511e-06],
        [-1.3828e-05, -1.0192e-05,  3.7253e-06,  ..., -1.2249e-05,
         -3.3528e-06, -6.4075e-06],
        [-1.8924e-05, -1.3947e-05,  5.0664e-06,  ..., -1.6719e-05,
         -4.5896e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.1428663730621338


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0, 23842,   342,  ...,   365,  4011, 10151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1226e-05,  9.0440e-05, -2.2117e-06,  ..., -3.9396e-05,
          6.0563e-06,  2.5978e-05],
        [-1.1072e-05, -8.1658e-06,  2.9802e-06,  ..., -9.8199e-06,
         -2.7008e-06, -5.1335e-06],
        [-1.2547e-05, -9.2536e-06,  3.3714e-06,  ..., -1.1131e-05,
         -3.0547e-06, -5.8040e-06],
        [-1.8418e-05, -1.3560e-05,  4.9546e-06,  ..., -1.6332e-05,
         -4.4778e-06, -8.5235e-06],
        [-2.5183e-05, -1.8567e-05,  6.7502e-06,  ..., -2.2322e-05,
         -6.1318e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.161523461341858


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[   0, 4209,  278,  ...,  287, 9971,  717],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9760e-07,  5.6583e-05, -2.5070e-05,  ..., -3.9151e-05,
          1.8412e-05,  7.1350e-06],
        [-1.3828e-05, -1.0207e-05,  3.7253e-06,  ..., -1.2264e-05,
         -3.3937e-06, -6.4373e-06],
        [-1.5676e-05, -1.1563e-05,  4.2170e-06,  ..., -1.3903e-05,
         -3.8408e-06, -7.2792e-06],
        [-2.3007e-05, -1.6958e-05,  6.1989e-06,  ..., -2.0385e-05,
         -5.6252e-06, -1.0684e-05],
        [-3.1441e-05, -2.3216e-05,  8.4490e-06,  ..., -2.7865e-05,
         -7.7114e-06, -1.4573e-05]], device='cuda:0')
Loss: 1.1240657567977905


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  292,   41,  ...,  271, 9536,  287],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7660e-05,  1.4959e-04, -2.4555e-05,  ..., -3.6709e-05,
          5.9532e-05,  3.4553e-05],
        [-1.6645e-05, -1.2279e-05,  4.4666e-06,  ..., -1.4737e-05,
         -4.1425e-06, -7.7337e-06],
        [-1.8865e-05, -1.3903e-05,  5.0552e-06,  ..., -1.6689e-05,
         -4.6864e-06, -8.7470e-06],
        [-2.7627e-05, -2.0355e-05,  7.4208e-06,  ..., -2.4438e-05,
         -6.8471e-06, -1.2800e-05],
        [-3.7849e-05, -2.7925e-05,  1.0133e-05,  ..., -3.3468e-05,
         -9.4101e-06, -1.7509e-05]], device='cuda:0')
Loss: 1.1367278099060059


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1824,    13,  ...,   266,  5940,    19],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3636e-05,  1.5174e-04, -5.1746e-05,  ..., -2.8683e-05,
          3.9838e-05,  3.3774e-05],
        [-1.9401e-05, -1.4305e-05,  5.2303e-06,  ..., -1.7181e-05,
         -4.7907e-06, -8.9929e-06],
        [-2.1979e-05, -1.6198e-05,  5.9158e-06,  ..., -1.9461e-05,
         -5.4166e-06, -1.0170e-05],
        [-3.2216e-05, -2.3723e-05,  8.6948e-06,  ..., -2.8521e-05,
         -7.9274e-06, -1.4901e-05],
        [-4.4078e-05, -3.2514e-05,  1.1854e-05,  ..., -3.9011e-05,
         -1.0878e-05, -2.0355e-05]], device='cuda:0')
Loss: 1.1469814777374268


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2872e-05,  1.5931e-04, -7.3274e-05,  ..., -2.7234e-05,
          6.7158e-05,  5.5302e-05],
        [-2.2143e-05, -1.6347e-05,  5.9903e-06,  ..., -1.9655e-05,
         -5.4762e-06, -1.0282e-05],
        [-2.5094e-05, -1.8522e-05,  6.7763e-06,  ..., -2.2277e-05,
         -6.1952e-06, -1.1630e-05],
        [-3.6746e-05, -2.7105e-05,  9.9540e-06,  ..., -3.2604e-05,
         -9.0599e-06, -1.7032e-05],
        [-5.0247e-05, -3.7104e-05,  1.3560e-05,  ..., -4.4584e-05,
         -1.2413e-05, -2.3246e-05]], device='cuda:0')
Loss: 1.1296958923339844
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3509e-05,  1.9440e-05, -4.8261e-05,  ..., -1.5285e-07,
         -2.1144e-05,  2.6342e-05],
        [-2.1314e-05, -1.5383e-05,  4.3448e-06,  ..., -1.8253e-05,
         -3.6235e-06, -8.5383e-06],
        [ 4.0716e-05,  2.9648e-05, -7.3234e-06,  ...,  3.4899e-05,
          7.2675e-06,  1.5014e-05],
        [-1.9918e-05, -1.5290e-05,  5.1671e-06,  ..., -1.9222e-05,
         -3.7995e-06, -8.6945e-06],
        [-4.5323e-05, -3.2557e-05,  8.3081e-06,  ..., -3.8583e-05,
         -7.4864e-06, -1.8603e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6761e-08, 1.4533e-08, 1.7202e-08,  ..., 1.9661e-08, 3.1149e-08,
         6.5076e-09],
        [3.9808e-11, 2.3249e-11, 1.5605e-12,  ..., 2.7457e-11, 1.2958e-12,
         5.9689e-12],
        [6.5186e-10, 3.8781e-10, 2.1383e-11,  ..., 5.0515e-10, 1.3869e-11,
         1.0538e-10],
        [2.2010e-10, 2.6913e-10, 6.0966e-12,  ..., 1.5556e-10, 7.9872e-12,
         5.2099e-11],
        [1.5821e-10, 8.7268e-11, 6.8416e-12,  ..., 1.0835e-10, 3.4792e-12,
         2.3710e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.4628782349517233e-05, 1.4628782349517233e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0,   12, 4957,  ...,  944,  278,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5062e-05, -1.2413e-06, -2.5557e-05,  ..., -1.2356e-05,
          1.4218e-05, -1.9281e-05],
        [-2.7418e-06, -2.0564e-06,  7.7859e-07,  ..., -2.4587e-06,
         -7.1898e-07, -1.3486e-06],
        [-3.1441e-06, -2.3395e-06,  8.9034e-07,  ..., -2.8163e-06,
         -8.1956e-07, -1.5423e-06],
        [-4.5598e-06, -3.4124e-06,  1.2964e-06,  ..., -4.0829e-06,
         -1.1921e-06, -2.2501e-06],
        [-6.2287e-06, -4.6492e-06,  1.7583e-06,  ..., -5.5730e-06,
         -1.6242e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.1187162399291992


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,   285,    53,  ...,   259,  5012,  3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3272e-05, -1.4088e-05, -2.8663e-06,  ..., -2.5192e-07,
          4.2820e-05,  1.2386e-06],
        [-5.5134e-06, -4.0978e-06,  1.5385e-06,  ..., -4.9323e-06,
         -1.4268e-06, -2.6971e-06],
        [-6.3032e-06, -4.6641e-06,  1.7583e-06,  ..., -5.6475e-06,
         -1.6242e-06, -3.0771e-06],
        [-9.1493e-06, -6.7949e-06,  2.5630e-06,  ..., -8.1956e-06,
         -2.3618e-06, -4.4852e-06],
        [-1.2487e-05, -9.2387e-06,  3.4720e-06,  ..., -1.1146e-05,
         -3.2187e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.137735366821289


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  292,  263,  ...,  342, 4575,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1755e-05,  3.5091e-05, -3.8454e-06,  ..., -9.2523e-06,
          2.1384e-05, -1.6835e-05],
        [-8.2552e-06, -6.1095e-06,  2.3134e-06,  ..., -7.3910e-06,
         -2.1234e-06, -4.0308e-06],
        [-9.4324e-06, -6.9588e-06,  2.6412e-06,  ..., -8.4490e-06,
         -2.4177e-06, -4.6045e-06],
        [-1.3739e-05, -1.0163e-05,  3.8594e-06,  ..., -1.2308e-05,
         -3.5241e-06, -6.7204e-06],
        [-1.8716e-05, -1.3798e-05,  5.2303e-06,  ..., -1.6719e-05,
         -4.8056e-06, -9.1046e-06]], device='cuda:0')
Loss: 1.1670441627502441


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2339e-05,  3.8541e-05,  1.9797e-05,  ..., -5.9561e-06,
          2.5983e-05,  3.2275e-05],
        [-1.1012e-05, -8.1509e-06,  3.0808e-06,  ..., -9.8497e-06,
         -2.7977e-06, -5.3495e-06],
        [-1.2591e-05, -9.3132e-06,  3.5204e-06,  ..., -1.1265e-05,
         -3.1851e-06, -6.1169e-06],
        [-1.8328e-05, -1.3575e-05,  5.1484e-06,  ..., -1.6391e-05,
         -4.6417e-06, -8.9109e-06],
        [-2.4945e-05, -1.8418e-05,  6.9588e-06,  ..., -2.2262e-05,
         -6.3255e-06, -1.2070e-05]], device='cuda:0')
Loss: 1.1530351638793945


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,   287,  6932,  ...,  1549,  1480,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3323e-05,  2.2237e-05, -9.1345e-06,  ..., -2.6554e-05,
          2.2649e-05,  2.9562e-05],
        [-1.3739e-05, -1.0192e-05,  3.8445e-06,  ..., -1.2293e-05,
         -3.4794e-06, -6.6757e-06],
        [-1.5721e-05, -1.1668e-05,  4.3996e-06,  ..., -1.4082e-05,
         -3.9674e-06, -7.6443e-06],
        [-2.2858e-05, -1.6987e-05,  6.4224e-06,  ..., -2.0474e-05,
         -5.7742e-06, -1.1131e-05],
        [-3.1114e-05, -2.3067e-05,  8.6948e-06,  ..., -2.7835e-05,
         -7.8678e-06, -1.5080e-05]], device='cuda:0')
Loss: 1.14302659034729


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,    12,  1471,  ...,  1356,   600,    12],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5138e-05,  1.1301e-05, -9.1345e-06,  ..., -1.5748e-05,
         -9.1377e-07,  3.5511e-05],
        [-1.6496e-05, -1.2249e-05,  4.5970e-06,  ..., -1.4767e-05,
         -4.1723e-06, -8.0019e-06],
        [-1.8880e-05, -1.4022e-05,  5.2638e-06,  ..., -1.6928e-05,
         -4.7609e-06, -9.1642e-06],
        [-2.7418e-05, -2.0385e-05,  7.6741e-06,  ..., -2.4587e-05,
         -6.9141e-06, -1.3322e-05],
        [-3.7313e-05, -2.7686e-05,  1.0386e-05,  ..., -3.3408e-05,
         -9.4324e-06, -1.8060e-05]], device='cuda:0')
Loss: 1.1319575309753418


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3737e-04,  4.3732e-05, -2.8295e-05,  ..., -1.6847e-05,
          2.5934e-05,  3.7591e-05],
        [-1.9237e-05, -1.4320e-05,  5.3234e-06,  ..., -1.7241e-05,
         -4.8764e-06, -9.3430e-06],
        [-2.2009e-05, -1.6376e-05,  6.0908e-06,  ..., -1.9744e-05,
         -5.5581e-06, -1.0692e-05],
        [-3.1948e-05, -2.3797e-05,  8.8736e-06,  ..., -2.8670e-05,
         -8.0690e-06, -1.5527e-05],
        [-4.3571e-05, -3.2395e-05,  1.2033e-05,  ..., -3.9041e-05,
         -1.1027e-05, -2.1100e-05]], device='cuda:0')
Loss: 1.1586501598358154


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5317e-04, -1.2648e-06, -4.4238e-05,  ..., -1.7816e-05,
          2.9075e-05,  3.8131e-05],
        [-2.1964e-05, -1.6332e-05,  6.0648e-06,  ..., -1.9670e-05,
         -5.5395e-06, -1.0677e-05],
        [-2.5153e-05, -1.8701e-05,  6.9439e-06,  ..., -2.2545e-05,
         -6.3218e-06, -1.2226e-05],
        [-3.6508e-05, -2.7180e-05,  1.0118e-05,  ..., -3.2753e-05,
         -9.1717e-06, -1.7747e-05],
        [-4.9740e-05, -3.6955e-05,  1.3709e-05,  ..., -4.4554e-05,
         -1.2532e-05, -2.4110e-05]], device='cuda:0')
Loss: 1.141487717628479
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6475e-05,  1.7369e-05, -4.7858e-05,  ..., -1.9192e-06,
         -1.6122e-05,  2.7521e-05],
        [-2.1379e-05, -1.5478e-05,  4.5168e-06,  ..., -1.8394e-05,
         -3.8151e-06, -8.7522e-06],
        [ 3.4129e-05,  2.4813e-05, -5.8967e-06,  ...,  2.9155e-05,
          5.9086e-06,  1.2290e-05],
        [-2.1577e-05, -1.6479e-05,  5.6621e-06,  ..., -2.0575e-05,
         -4.3368e-06, -9.5997e-06],
        [-4.5764e-05, -3.2997e-05,  8.8482e-06,  ..., -3.9180e-05,
         -7.9909e-06, -1.9154e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6737e-08, 1.4519e-08, 1.7187e-08,  ..., 1.9642e-08, 3.1119e-08,
         6.5026e-09],
        [4.0250e-11, 2.3493e-11, 1.5957e-12,  ..., 2.7817e-11, 1.3252e-12,
         6.0769e-12],
        [6.5184e-10, 3.8777e-10, 2.1410e-11,  ..., 5.0515e-10, 1.3895e-11,
         1.0542e-10],
        [2.2121e-10, 2.6960e-10, 6.1929e-12,  ..., 1.5648e-10, 8.0634e-12,
         5.2362e-11],
        [1.6052e-10, 8.8547e-11, 7.0227e-12,  ..., 1.1022e-10, 3.6328e-12,
         2.4268e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.4408237148944047e-05, 1.4408237148944047e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   298, 11712,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4867e-05,  8.0236e-05, -2.9589e-05,  ...,  1.9363e-05,
         -1.8852e-05,  4.2102e-05],
        [-2.7120e-06, -2.0266e-06,  7.5623e-07,  ..., -2.4289e-06,
         -7.5251e-07, -1.3933e-06],
        [-3.1292e-06, -2.3395e-06,  8.7544e-07,  ..., -2.8163e-06,
         -8.6799e-07, -1.6093e-06],
        [-4.4703e-06, -3.3528e-06,  1.2517e-06,  ..., -4.0233e-06,
         -1.2368e-06, -2.3097e-06],
        [-6.1691e-06, -4.5896e-06,  1.7211e-06,  ..., -5.5432e-06,
         -1.7062e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.1260110139846802


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0,   13, 4363,  ...,  271, 2462,  709],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4763e-05,  1.0612e-04, -6.5110e-05,  ...,  1.7500e-05,
         -2.2252e-05,  8.0572e-05],
        [-5.4836e-06, -4.0829e-06,  1.4938e-06,  ..., -4.9025e-06,
         -1.4380e-06, -2.7791e-06],
        [-6.2883e-06, -4.6790e-06,  1.7174e-06,  ..., -5.6475e-06,
         -1.6466e-06, -3.1963e-06],
        [-9.1195e-06, -6.7949e-06,  2.4885e-06,  ..., -8.1956e-06,
         -2.3767e-06, -4.6343e-06],
        [-1.2457e-05, -9.2387e-06,  3.3900e-06,  ..., -1.1176e-05,
         -3.2559e-06, -6.3032e-06]], device='cuda:0')
Loss: 1.1438169479370117


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[    0,  7203,   271,  ...,    12,   275,  3338],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8685e-05,  1.1527e-04, -1.0093e-04,  ...,  1.8929e-05,
         -2.2252e-05,  7.3609e-05],
        [-8.1807e-06, -6.1095e-06,  2.2091e-06,  ..., -7.3612e-06,
         -2.1160e-06, -4.1351e-06],
        [-9.3877e-06, -7.0035e-06,  2.5406e-06,  ..., -8.4639e-06,
         -2.4214e-06, -4.7535e-06],
        [-1.3649e-05, -1.0207e-05,  3.6955e-06,  ..., -1.2308e-05,
         -3.5092e-06, -6.9141e-06],
        [-1.8656e-05, -1.3888e-05,  5.0291e-06,  ..., -1.6809e-05,
         -4.8056e-06, -9.4026e-06]], device='cuda:0')
Loss: 1.1380125284194946


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8073e-05,  1.3654e-04, -9.1808e-05,  ..., -2.3229e-05,
         -2.7416e-05,  5.0054e-05],
        [-1.0908e-05, -8.1211e-06,  2.9355e-06,  ..., -9.8199e-06,
         -2.7828e-06, -5.4911e-06],
        [-1.2517e-05, -9.3132e-06,  3.3788e-06,  ..., -1.1295e-05,
         -3.1851e-06, -6.3106e-06],
        [-1.8239e-05, -1.3605e-05,  4.9248e-06,  ..., -1.6481e-05,
         -4.6343e-06, -9.1940e-06],
        [-2.4855e-05, -1.8448e-05,  6.6832e-06,  ..., -2.2411e-05,
         -6.3181e-06, -1.2472e-05]], device='cuda:0')
Loss: 1.1478352546691895


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,   278,  6046,  ...,  1671,   199,   395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0156e-05,  1.3865e-04, -1.1831e-04,  ...,  3.7719e-05,
         -6.2884e-05,  1.8138e-05],
        [-1.3605e-05, -1.0118e-05,  3.6918e-06,  ..., -1.2249e-05,
         -3.4757e-06, -6.8396e-06],
        [-1.5661e-05, -1.1638e-05,  4.2617e-06,  ..., -1.4126e-05,
         -3.9898e-06, -7.8753e-06],
        [-2.2769e-05, -1.6958e-05,  6.1989e-06,  ..., -2.0564e-05,
         -5.7891e-06, -1.1459e-05],
        [-3.1024e-05, -2.3007e-05,  8.4117e-06,  ..., -2.7955e-05,
         -7.8976e-06, -1.5542e-05]], device='cuda:0')
Loss: 1.1220979690551758


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   278,  6318,  ...,   458,    17,     9],
        [    0,  7377, 30662,  ...,   287,   694, 13403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4169e-05,  1.7751e-04, -1.3605e-04,  ...,  9.5744e-05,
         -9.2662e-05,  3.0660e-05],
        [-1.6376e-05, -1.2159e-05,  4.4256e-06,  ..., -1.4707e-05,
         -4.1649e-06, -8.2031e-06],
        [-1.8835e-05, -1.3992e-05,  5.1074e-06,  ..., -1.6943e-05,
         -4.7795e-06, -9.4399e-06],
        [-2.7388e-05, -2.0370e-05,  7.4282e-06,  ..., -2.4676e-05,
         -6.9290e-06, -1.3724e-05],
        [-3.7342e-05, -2.7686e-05,  1.0088e-05,  ..., -3.3557e-05,
         -9.4697e-06, -1.8641e-05]], device='cuda:0')
Loss: 1.1269162893295288


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,   759,  1128,  ...,   221,   474,   221],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4087e-05,  1.3448e-04, -1.3956e-04,  ...,  1.0561e-04,
         -1.1210e-04,  1.7686e-05],
        [-1.9103e-05, -1.4201e-05,  5.1744e-06,  ..., -1.7121e-05,
         -4.8503e-06, -9.5740e-06],
        [-2.1994e-05, -1.6347e-05,  5.9754e-06,  ..., -1.9744e-05,
         -5.5730e-06, -1.1027e-05],
        [-3.1978e-05, -2.3812e-05,  8.6948e-06,  ..., -2.8759e-05,
         -8.0839e-06, -1.6034e-05],
        [-4.3541e-05, -3.2336e-05,  1.1794e-05,  ..., -3.9071e-05,
         -1.1034e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.142667531967163


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0,    12,   287,  ...,   199,   769, 18432]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1421e-04,  1.3997e-04, -1.6172e-04,  ...,  9.3316e-05,
         -1.1510e-04,  3.7435e-05],
        [-2.1890e-05, -1.6242e-05,  5.8971e-06,  ..., -1.9595e-05,
         -5.5395e-06, -1.0937e-05],
        [-2.5168e-05, -1.8671e-05,  6.8024e-06,  ..., -2.2560e-05,
         -6.3591e-06, -1.2577e-05],
        [-3.6627e-05, -2.7210e-05,  9.9018e-06,  ..., -3.2872e-05,
         -9.2313e-06, -1.8299e-05],
        [-4.9829e-05, -3.6925e-05,  1.3426e-05,  ..., -4.4644e-05,
         -1.2584e-05, -2.4825e-05]], device='cuda:0')
Loss: 1.1523239612579346
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4248e-05,  2.9629e-05, -5.9245e-05,  ...,  7.6043e-06,
         -2.6020e-05,  2.8513e-05],
        [-2.1430e-05, -1.5554e-05,  4.6548e-06,  ..., -1.8514e-05,
         -3.9875e-06, -8.9707e-06],
        [ 2.8199e-05,  2.0464e-05, -4.6268e-06,  ...,  2.3983e-05,
          4.6818e-06,  9.8033e-06],
        [-2.3082e-05, -1.7552e-05,  6.0861e-06,  ..., -2.1805e-05,
         -4.8262e-06, -1.0470e-05],
        [-4.6171e-05, -3.3390e-05,  9.3060e-06,  ..., -3.9727e-05,
         -8.4502e-06, -1.9721e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6704e-08, 1.4524e-08, 1.7196e-08,  ..., 1.9631e-08, 3.1101e-08,
         6.4975e-09],
        [4.0689e-11, 2.3733e-11, 1.6289e-12,  ..., 2.8173e-11, 1.3546e-12,
         6.1905e-12],
        [6.5182e-10, 3.8773e-10, 2.1435e-11,  ..., 5.0515e-10, 1.3921e-11,
         1.0548e-10],
        [2.2233e-10, 2.7007e-10, 6.2847e-12,  ..., 1.5740e-10, 8.1405e-12,
         5.2645e-11],
        [1.6285e-10, 8.9822e-11, 7.1959e-12,  ..., 1.1211e-10, 3.7875e-12,
         2.4860e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.418499758283982e-05, 1.418499758283982e-05]
scheduler_last_epoch: 50
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  Learning Rate ‚ñà‚ñà‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÅ
wandb: Validation PPL ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 319
wandb:          Epoch 0
wandb:  Learning Rate 2e-05
wandb:   Training PPL 9662.12733
wandb: Validation PPL 10.07553
wandb: 
wandb: üöÄ View run decent-tree-308 at: https://wandb.ai/kenotron/brainlessgpt/runs/cnbw8ie3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_103944-cnbw8ie3/logs
