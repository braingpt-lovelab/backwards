nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_103152-7dvyvouy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-river-307
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/7dvyvouy
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,   334,   287,  ...,  1348,  6139,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2704e-04,  5.9281e-04, -1.6846e-04,  ...,  2.3848e-04,
          4.8112e-04, -5.9746e-04],
        [ 3.9302e-07, -1.9645e-09,  2.4773e-07,  ..., -1.3225e-07,
         -5.5507e-07, -5.6997e-07],
        [ 1.0133e-06, -8.1025e-08,  5.6624e-07,  ..., -1.8999e-07,
         -1.4305e-06, -1.4380e-06],
        [ 6.2212e-07, -3.1432e-08,  3.7067e-07,  ..., -7.9628e-08,
         -8.3074e-07, -8.4192e-07],
        [ 3.5577e-07, -4.9127e-08,  1.5646e-07,  ..., -6.8918e-08,
         -6.0722e-07, -4.9174e-07]], device='cuda:0')
Loss: 1.3798766136169434


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,  1412, 11275,  ...,   668, 14849,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2516e-03,  4.9267e-04, -3.0263e-04,  ...,  4.9037e-04,
          2.9440e-04, -6.7642e-04],
        [ 4.9127e-07, -2.7019e-07,  5.5134e-07,  ..., -1.5169e-07,
         -1.1101e-06, -1.2144e-06],
        [ 1.2089e-06, -8.0746e-07,  1.1623e-06,  ..., -1.7218e-07,
         -2.6897e-06, -2.9728e-06],
        [ 7.3295e-07, -4.0210e-07,  8.0839e-07,  ..., -3.8883e-08,
         -1.5423e-06, -1.6615e-06],
        [ 3.8533e-07, -3.3784e-07,  3.1199e-07,  ..., -5.0175e-08,
         -1.1586e-06, -1.0356e-06]], device='cuda:0')
Loss: 1.3779733180999756


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7629e-03,  4.1507e-04, -1.6026e-04,  ...,  3.5682e-04,
         -2.1228e-04,  2.5696e-04],
        [ 8.7498e-07, -1.4166e-07,  5.5827e-07,  ...,  3.0850e-08,
         -2.1383e-06, -1.6689e-06],
        [ 2.3711e-06, -3.9581e-07,  1.1192e-06,  ...,  4.0897e-07,
         -5.6103e-06, -4.3660e-06],
        [ 1.3663e-06, -1.7951e-07,  8.6357e-07,  ...,  3.0384e-07,
         -3.0771e-06, -2.4326e-06],
        [ 7.5600e-07, -2.2375e-07,  1.7788e-07,  ...,  1.7800e-07,
         -2.2762e-06, -1.4417e-06]], device='cuda:0')
Loss: 1.3773101568222046


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  7555,  3908,  ...,   259,  8477,   278],
        [    0,  2895,    26,  ..., 11645,  1535,  1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4040e-03, -8.6908e-04,  5.9628e-05,  ...,  4.3353e-04,
         -9.7745e-04,  1.4020e-04],
        [ 9.7090e-07, -1.3977e-07,  6.9238e-07,  ...,  2.2270e-07,
         -2.7642e-06, -2.1420e-06],
        [ 2.6375e-06, -5.2061e-07,  1.3968e-06,  ...,  1.0795e-06,
         -7.3835e-06, -5.7891e-06],
        [ 1.4938e-06, -2.0291e-07,  1.0387e-06,  ...,  6.5961e-07,
         -3.8855e-06, -3.1143e-06],
        [ 7.6194e-07, -2.5635e-07,  1.9441e-07,  ...,  4.0897e-07,
         -2.9393e-06, -1.8366e-06]], device='cuda:0')
Loss: 1.3781037330627441


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[  0, 368, 266,  ..., 591, 767, 824],
        [  0,  25,  26,  ...,   9, 287, 298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0139e-03, -8.3908e-04,  7.8833e-05,  ...,  8.6843e-04,
         -1.3492e-03,  1.0800e-04],
        [ 1.0435e-06,  5.5152e-09,  9.5688e-07,  ...,  9.0455e-08,
         -3.3192e-06, -2.6021e-06],
        [ 2.7847e-06, -2.7288e-07,  1.9444e-06,  ...,  8.9139e-07,
         -8.6874e-06, -7.0110e-06],
        [ 1.6168e-06, -3.6205e-08,  1.4298e-06,  ...,  5.4738e-07,
         -4.5821e-06, -3.8333e-06],
        [ 8.2387e-07, -1.6787e-07,  3.7323e-07,  ...,  2.9721e-07,
         -3.5465e-06, -2.2389e-06]], device='cuda:0')
Loss: 1.3758723735809326


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0,   29,  413,  ..., 1527, 1503,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5182e-03, -8.5118e-04,  2.6907e-04,  ...,  1.3948e-03,
         -4.1878e-04, -8.5133e-04],
        [ 7.5111e-07,  3.7879e-08,  1.0412e-06,  ..., -1.1630e-07,
         -4.0270e-06, -2.9095e-06],
        [ 2.1439e-06, -1.9837e-07,  2.0501e-06,  ...,  4.3504e-07,
         -1.0513e-05, -7.6964e-06],
        [ 1.3020e-06, -4.7730e-09,  1.5816e-06,  ...,  3.4156e-07,
         -5.4911e-06, -4.1779e-06],
        [ 4.8673e-07, -1.3737e-07,  3.4086e-07,  ...,  7.8348e-08,
         -4.4331e-06, -2.4671e-06]], device='cuda:0')
Loss: 1.3811368942260742


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[    0,   221,   825,  ...,   616,  3661,  8052],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3373e-03, -1.0179e-03,  2.7495e-04,  ...,  1.6629e-03,
         -2.8341e-04, -1.0099e-03],
        [ 7.8115e-07, -4.0357e-07,  1.5031e-06,  ..., -2.0757e-07,
         -4.7535e-06, -3.4496e-06],
        [ 2.1923e-06, -1.3160e-06,  3.0186e-06,  ...,  3.5216e-07,
         -1.2197e-05, -9.0376e-06],
        [ 1.3472e-06, -6.3435e-07,  2.2820e-06,  ...,  3.1723e-07,
         -6.4969e-06, -4.9379e-06],
        [ 4.9477e-07, -5.3598e-07,  6.3889e-07,  ...,  3.7835e-08,
         -5.1260e-06, -2.9216e-06]], device='cuda:0')
Loss: 1.37653386592865


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2159e-03, -1.1212e-03,  5.5159e-04,  ...,  9.4682e-04,
         -5.9043e-04, -6.5467e-04],
        [ 1.0214e-06, -3.6515e-07,  2.0209e-06,  ..., -1.4098e-07,
         -5.5693e-06, -4.0978e-06],
        [ 2.8852e-06, -1.3034e-06,  4.3076e-06,  ...,  6.0920e-07,
         -1.4536e-05, -1.0878e-05],
        [ 1.7439e-06, -5.2864e-07,  3.0531e-06,  ...,  4.6904e-07,
         -7.7114e-06, -5.8506e-06],
        [ 7.2480e-07, -5.1595e-07,  1.0990e-06,  ...,  1.2491e-07,
         -6.0871e-06, -3.5623e-06]], device='cuda:0')
Loss: 1.376060128211975
Graident accumulation at epoch 0, step 7, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2159e-04, -1.1212e-04,  5.5159e-05,  ...,  9.4682e-05,
         -5.9043e-05, -6.5467e-05],
        [ 1.0214e-07, -3.6515e-08,  2.0209e-07,  ..., -1.4098e-08,
         -5.5693e-07, -4.0978e-07],
        [ 2.8852e-07, -1.3034e-07,  4.3076e-07,  ...,  6.0920e-08,
         -1.4536e-06, -1.0878e-06],
        [ 1.7439e-07, -5.2864e-08,  3.0531e-07,  ...,  4.6904e-08,
         -7.7114e-07, -5.8506e-07],
        [ 7.2480e-08, -5.1595e-08,  1.0990e-07,  ...,  1.2491e-08,
         -6.0871e-07, -3.5623e-07]], device='cuda:0')
optimizer state dict: tensor([[2.7206e-08, 1.2571e-09, 3.0425e-10,  ..., 8.9647e-10, 3.4860e-10,
         4.2859e-10],
        [1.0433e-15, 1.3334e-16, 4.0841e-15,  ..., 1.9875e-17, 3.1017e-14,
         1.6792e-14],
        [8.3246e-15, 1.6988e-15, 1.8555e-14,  ..., 3.7113e-16, 2.1130e-13,
         1.1833e-13],
        [3.0412e-15, 2.7946e-16, 9.3215e-15,  ..., 2.2000e-16, 5.9465e-14,
         3.4229e-14],
        [5.2534e-16, 2.6621e-16, 1.2077e-15,  ..., 1.5603e-17, 3.7053e-14,
         1.2690e-14]], device='cuda:0')
optimizer state dict: 1.0
lr: [5.0890585241730285e-06, 5.0890585241730285e-06]
scheduler_last_epoch: 1


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[    0,  9657,   300,  ...,    12,   271,   266],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6306e-04, -5.3534e-04,  7.0934e-04,  ...,  6.1408e-04,
         -1.2313e-03, -6.9040e-04],
        [ 2.1304e-08, -1.9791e-08,  3.4645e-07,  ...,  2.8312e-07,
         -1.0431e-06, -4.1537e-07],
        [ 3.9116e-08, -1.9278e-07,  8.0466e-07,  ...,  7.6741e-07,
         -2.6077e-06, -1.1399e-06],
        [ 3.8883e-08, -5.9139e-08,  5.1409e-07,  ...,  4.7125e-07,
         -1.3486e-06, -5.9605e-07],
        [-1.0768e-08, -6.7521e-08,  2.5332e-07,  ...,  3.1665e-07,
         -1.0878e-06, -3.5390e-07]], device='cuda:0')
Loss: 1.3802980184555054


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6015e-04, -1.8815e-03,  5.2908e-04,  ...,  1.2968e-03,
         -1.8072e-03, -7.4086e-04],
        [ 2.5413e-07, -4.9477e-07,  6.9663e-07,  ...,  2.0768e-07,
         -1.8440e-06, -7.9162e-07],
        [ 5.3085e-07, -1.4221e-06,  1.4305e-06,  ...,  6.2492e-07,
         -4.5300e-06, -2.0787e-06],
        [ 3.7230e-07, -8.1537e-07,  9.9093e-07,  ...,  4.2445e-07,
         -2.4438e-06, -1.1213e-06],
        [ 1.4383e-07, -4.9593e-07,  4.4238e-07,  ...,  2.7032e-07,
         -1.8217e-06, -6.1840e-07]], device='cuda:0')
Loss: 1.3787602186203003


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[    0,    73,    30,  ...,  4112,    12,  9416],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5080e-04, -1.9903e-03,  7.3113e-04,  ...,  1.8867e-03,
         -1.9853e-03, -7.9921e-04],
        [ 3.6124e-07, -5.7067e-07,  1.1027e-06,  ...,  6.4261e-08,
         -2.4512e-06, -1.4734e-06],
        [-2.2072e-05, -1.4559e-05, -3.9971e-05,  ...,  1.1649e-04,
         -3.8392e-05, -1.5753e-05],
        [ 5.4925e-07, -9.0478e-07,  1.6093e-06,  ...,  3.2806e-07,
         -3.2485e-06, -2.1644e-06],
        [ 1.8853e-07, -6.4960e-07,  6.8080e-07,  ...,  1.3155e-07,
         -2.4661e-06, -1.2070e-06]], device='cuda:0')
Loss: 1.3779780864715576


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[   0, 8416,  669,  ...,  298,  894,  496],
        [   0, 4890, 1528,  ...,  847,  328, 1703]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6752e-04, -2.1329e-03,  1.6270e-03,  ...,  2.4249e-03,
         -1.4135e-03, -4.3775e-04],
        [ 5.5961e-07, -5.9546e-07,  1.4082e-06,  ..., -1.8533e-07,
         -3.3081e-06, -2.0284e-06],
        [-2.1540e-05, -1.4756e-05, -3.9230e-05,  ...,  1.1591e-04,
         -4.0777e-05, -1.7273e-05],
        [ 9.0688e-07, -1.0096e-06,  2.1197e-06,  ...,  2.2585e-08,
         -4.5300e-06, -2.9877e-06],
        [ 3.3661e-07, -7.2271e-07,  8.8383e-07,  ..., -1.0128e-07,
         -3.4273e-06, -1.6671e-06]], device='cuda:0')
Loss: 1.378662347793579


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4792e-03, -1.8712e-03,  1.6975e-03,  ...,  3.0601e-03,
         -2.3207e-03,  2.6368e-04],
        [ 1.0197e-06, -7.5845e-07,  2.1309e-06,  ..., -3.8557e-07,
         -4.1947e-06, -2.6282e-06],
        [-2.0407e-05, -1.5300e-05, -3.7509e-05,  ...,  1.1553e-04,
         -4.3027e-05, -1.8815e-05],
        [ 1.6445e-06, -1.3746e-06,  3.2745e-06,  ..., -1.6461e-07,
         -5.7667e-06, -3.8818e-06],
        [ 6.7003e-07, -8.6986e-07,  1.3905e-06,  ..., -2.5216e-07,
         -4.2096e-06, -2.1197e-06]], device='cuda:0')
Loss: 1.3743202686309814


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 1871,  401,  ...,   14, 4797,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5415e-03, -1.7045e-03,  1.3520e-03,  ...,  3.3409e-03,
         -2.1817e-03, -1.2775e-03],
        [ 1.2581e-06, -8.5623e-07,  2.5090e-06,  ..., -6.4820e-07,
         -4.9509e-06, -3.0883e-06],
        [-1.9867e-05, -1.5591e-05, -3.6589e-05,  ...,  1.1500e-04,
         -4.4882e-05, -2.0037e-05],
        [ 2.0654e-06, -1.5181e-06,  3.9861e-06,  ..., -4.4401e-07,
         -6.9737e-06, -4.6454e-06],
        [ 9.3080e-07, -1.0123e-06,  1.7332e-06,  ..., -4.9989e-07,
         -5.1036e-06, -2.5760e-06]], device='cuda:0')
Loss: 1.378920078277588


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[    0, 21413,  1735,  ..., 10789, 12523,    12],
        [    0,  2771,    13,  ...,  1412,    35,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5733e-03, -2.7006e-03,  1.8157e-03,  ...,  3.4644e-03,
         -2.6538e-03, -1.9089e-03],
        [ 1.3128e-06, -1.2884e-06,  3.0715e-06,  ..., -3.3155e-07,
         -5.8301e-06, -3.8035e-06],
        [-2.0023e-05, -1.6731e-05, -3.5262e-05,  ...,  1.1589e-04,
         -4.6953e-05, -2.1870e-05],
        [ 2.1846e-06, -2.0992e-06,  4.7386e-06,  ...,  3.2829e-08,
         -8.0913e-06, -5.5879e-06],
        [ 9.1229e-07, -1.4035e-06,  2.1299e-06,  ..., -2.2235e-07,
         -5.8785e-06, -3.1423e-06]], device='cuda:0')
Loss: 1.3786959648132324


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7581e-03, -3.1079e-03,  2.5922e-03,  ...,  3.1623e-03,
         -2.9356e-03, -1.4865e-03],
        [ 1.6351e-06, -1.0984e-06,  3.4645e-06,  ..., -4.6380e-07,
         -6.9775e-06, -4.4033e-06],
        [-1.9315e-05, -1.6427e-05, -3.4491e-05,  ...,  1.1567e-04,
         -4.9635e-05, -2.3345e-05],
        [ 2.6131e-06, -1.8887e-06,  5.2862e-06,  ..., -4.0745e-08,
         -9.5963e-06, -6.4336e-06],
        [ 1.1898e-06, -1.2629e-06,  2.3497e-06,  ..., -2.7474e-07,
         -6.9961e-06, -3.6415e-06]], device='cuda:0')
Loss: 1.374688982963562
Graident accumulation at epoch 0, step 15, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4524e-04, -4.1169e-04,  3.0886e-04,  ...,  4.0145e-04,
         -3.4670e-04, -2.0757e-04],
        [ 2.5543e-07, -1.4270e-07,  5.2833e-07,  ..., -5.9068e-08,
         -1.1990e-06, -8.0913e-07],
        [-1.6718e-06, -1.7600e-06, -3.0614e-06,  ...,  1.1622e-05,
         -6.2718e-06, -3.3135e-06],
        [ 4.1826e-07, -2.3645e-07,  8.0340e-07,  ...,  3.8139e-08,
         -1.6537e-06, -1.1699e-06],
        [ 1.8421e-07, -1.7272e-07,  3.3388e-07,  ..., -1.6232e-08,
         -1.2475e-06, -6.8475e-07]], device='cuda:0')
optimizer state dict: tensor([[3.0270e-08, 1.0915e-08, 7.0232e-09,  ..., 1.0896e-08, 8.9659e-09,
         2.6377e-09],
        [3.7157e-15, 1.3396e-15, 1.6083e-14,  ..., 2.3496e-16, 7.9671e-14,
         3.6164e-14],
        [3.8138e-13, 2.7155e-13, 1.2082e-12,  ..., 1.3379e-11, 2.6747e-12,
         6.6320e-13],
        [9.8662e-15, 3.8465e-15, 3.7256e-14,  ..., 2.2144e-16, 1.5150e-13,
         7.5586e-14],
        [1.9405e-15, 1.8608e-15, 6.7277e-15,  ..., 9.1070e-17, 8.5961e-14,
         2.5938e-14]], device='cuda:0')
optimizer state dict: 2.0
lr: [1.0178117048346057e-05, 1.0178117048346057e-05]
scheduler_last_epoch: 2


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    27,  3961,  ...,   462,   221,   474],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4483e-04, -1.7383e-05,  9.3854e-04,  ...,  3.2340e-04,
         -3.6396e-04, -2.9746e-04],
        [-1.8254e-07, -1.2666e-06,  7.2271e-07,  ..., -2.2817e-07,
         -4.4331e-07, -7.4878e-07],
        [-4.6380e-07, -2.7716e-06,  1.4678e-06,  ..., -4.3772e-07,
         -9.0897e-07, -1.6689e-06],
        [-1.9651e-07, -1.5572e-06,  8.7172e-07,  ..., -1.9837e-07,
         -5.8115e-07, -9.3877e-07],
        [-2.0303e-07, -1.3262e-06,  6.2212e-07,  ..., -2.2072e-07,
         -5.2899e-07, -7.1898e-07]], device='cuda:0')
Loss: 1.3484306335449219


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,   13, 2920,  609],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4122e-04,  8.7827e-05,  1.5309e-03,  ...,  1.3538e-04,
         -7.4824e-04,  2.6012e-04],
        [-4.7497e-07, -2.3246e-06,  1.2517e-06,  ..., -8.2795e-07,
         -1.1027e-06, -1.2107e-06],
        [-1.1045e-06, -5.2601e-06,  2.5779e-06,  ..., -1.7639e-06,
         -2.4587e-06, -2.7716e-06],
        [-4.7591e-07, -2.6152e-06,  1.4715e-06,  ..., -7.7952e-07,
         -1.2629e-06, -1.4268e-06],
        [-5.6438e-07, -2.5630e-06,  1.0226e-06,  ..., -8.4285e-07,
         -1.2927e-06, -1.0598e-06]], device='cuda:0')
Loss: 1.3509353399276733


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0, 3408,  300,  ...,   14, 5870,   12],
        [   0, 2314,  266,  ...,  342, 7299, 1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5777e-03, -5.4679e-04,  1.9527e-03,  ..., -3.0752e-04,
         -1.3388e-03,  9.3224e-04],
        [-9.2946e-07, -3.4869e-06,  1.5739e-06,  ..., -1.1818e-06,
         -2.0564e-06, -1.8366e-06],
        [-2.9867e-05,  1.3636e-04,  5.8682e-06,  ...,  6.5392e-05,
          2.1039e-05,  1.6581e-05],
        [-1.0012e-06, -3.9488e-06,  1.8589e-06,  ..., -1.0906e-06,
         -2.2314e-06, -2.1867e-06],
        [-1.1083e-06, -3.7774e-06,  1.1930e-06,  ..., -1.1558e-06,
         -2.3283e-06, -1.6149e-06]], device='cuda:0')
Loss: 1.3480736017227173


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[    0,   365,  5911,  ...,   925,   408,   266],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9934e-03, -7.1216e-04,  2.3076e-03,  ...,  5.2333e-04,
         -1.5931e-03,  8.9054e-04],
        [-1.2964e-06, -4.8578e-06,  2.2147e-06,  ..., -1.4594e-06,
         -2.8312e-06, -2.7195e-06],
        [-3.0698e-05,  1.3297e-04,  7.2391e-06,  ...,  6.4860e-05,
          1.9280e-05,  1.4435e-05],
        [-1.3402e-06, -5.2825e-06,  2.4661e-06,  ..., -1.2871e-06,
         -2.9020e-06, -2.9989e-06],
        [-1.5143e-06, -5.2452e-06,  1.7220e-06,  ..., -1.4054e-06,
         -3.1628e-06, -2.4457e-06]], device='cuda:0')
Loss: 1.3493497371673584


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 10251,   278,  ...,   278,   319,    13],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1656e-03, -1.6622e-03,  2.4576e-03,  ...,  1.5040e-03,
         -2.0245e-03,  9.2902e-04],
        [-1.8403e-06, -6.2361e-06,  2.7400e-06,  ..., -1.8971e-06,
         -3.3155e-06, -3.6806e-06],
        [-3.1972e-05,  1.2980e-04,  8.3716e-06,  ...,  6.4007e-05,
          1.8192e-05,  1.2185e-05],
        [-1.9474e-06, -6.8918e-06,  3.1441e-06,  ..., -1.7062e-06,
         -3.4049e-06, -4.1015e-06],
        [-2.1439e-06, -6.7353e-06,  2.1635e-06,  ..., -1.8356e-06,
         -3.7067e-06, -3.4068e-06]], device='cuda:0')
Loss: 1.3491437435150146


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3156e-03, -1.6141e-03,  2.5600e-03,  ...,  1.9328e-03,
         -2.1995e-03,  5.7814e-04],
        [-2.2911e-06, -7.7337e-06,  3.5670e-06,  ..., -2.0703e-06,
         -4.5002e-06, -4.7162e-06],
        [-3.2963e-05,  1.2628e-04,  1.0182e-05,  ...,  6.3978e-05,
          1.5629e-05,  9.8754e-06],
        [-2.4540e-06, -8.5309e-06,  4.0941e-06,  ..., -1.7690e-06,
         -4.6864e-06, -5.2117e-06],
        [-2.7064e-06, -8.2776e-06,  2.8824e-06,  ..., -1.9697e-06,
         -5.0031e-06, -4.4052e-06]], device='cuda:0')
Loss: 1.3514447212219238


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[    0,  1635,   266,  ...,   437,  3302,   287],
        [    0,  2278,   292,  ..., 12060,  1319,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4881e-03, -2.4970e-03,  2.5842e-03,  ...,  2.3524e-03,
         -3.7189e-03,  6.1590e-04],
        [-2.5909e-06, -8.9481e-06,  4.1332e-06,  ..., -2.3665e-06,
         -5.2303e-06, -5.5842e-06],
        [-3.3648e-05,  1.2359e-04,  1.1329e-05,  ...,  6.3464e-05,
          1.4072e-05,  8.0128e-06],
        [-2.7185e-06, -9.8124e-06,  4.7199e-06,  ..., -1.9786e-06,
         -5.3942e-06, -6.1058e-06],
        [-3.0603e-06, -9.7007e-06,  3.4040e-06,  ..., -2.2268e-06,
         -5.9083e-06, -5.2433e-06]], device='cuda:0')
Loss: 1.3446325063705444


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   352,   644,  ...,  2928,   590,  3040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8383e-03, -1.4850e-03,  2.7779e-03,  ...,  2.7235e-03,
         -4.0871e-03,  8.9337e-04],
        [-3.1348e-06, -1.0028e-05,  5.2135e-06,  ..., -2.8806e-06,
         -6.0759e-06, -6.2995e-06],
        [-3.4825e-05,  1.2138e-04,  1.3282e-05,  ...,  6.2495e-05,
          1.2411e-05,  6.5748e-06],
        [-3.2922e-06, -1.0997e-05,  5.9865e-06,  ..., -2.4666e-06,
         -6.2883e-06, -6.9216e-06],
        [-3.7011e-06, -1.0908e-05,  4.4173e-06,  ..., -2.7334e-06,
         -6.8508e-06, -5.9027e-06]], device='cuda:0')
Loss: 1.347732424736023
Graident accumulation at epoch 0, step 23, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6455e-04, -5.1903e-04,  5.5577e-04,  ...,  6.3365e-04,
         -7.2074e-04, -9.7473e-05],
        [-8.3593e-08, -1.1313e-06,  9.9685e-07,  ..., -3.4122e-07,
         -1.6867e-06, -1.3582e-06],
        [-4.9871e-06,  1.0554e-05, -1.4271e-06,  ...,  1.6709e-05,
         -4.4035e-06, -2.3247e-06],
        [ 4.7209e-08, -1.3125e-06,  1.3217e-06,  ..., -2.1234e-07,
         -2.1171e-06, -1.7451e-06],
        [-2.0431e-07, -1.2462e-06,  7.4222e-07,  ..., -2.8795e-07,
         -1.8078e-06, -1.2066e-06]], device='cuda:0')
optimizer state dict: tensor([[3.8295e-08, 1.3109e-08, 1.4733e-08,  ..., 1.8302e-08, 2.5661e-08,
         3.4332e-09],
        [1.3539e-14, 1.0191e-13, 4.3248e-14,  ..., 8.5325e-15, 1.1651e-13,
         7.5811e-14],
        [1.5938e-12, 1.5005e-11, 1.3834e-12,  ..., 1.7272e-11, 2.8261e-12,
         7.0576e-13],
        [2.0695e-14, 1.2478e-13, 7.3057e-14,  ..., 6.3054e-15, 1.9089e-13,
         1.2342e-13],
        [1.5637e-14, 1.2084e-13, 2.6233e-14,  ..., 7.5626e-15, 1.3281e-13,
         6.0754e-14]], device='cuda:0')
optimizer state dict: 3.0
lr: [1.5267175572519086e-05, 1.5267175572519086e-05]
scheduler_last_epoch: 3


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[   0,   14,  496,  ...,  368,  259,  490],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3868e-04,  7.3968e-04,  6.8484e-04,  ...,  5.9814e-04,
          3.9275e-04,  3.2364e-04],
        [-2.1458e-06, -3.5465e-06,  6.2399e-08,  ..., -1.3486e-06,
         -3.7625e-07, -9.6858e-07],
        [-3.2783e-06, -5.5730e-06,  7.5903e-08,  ..., -1.9968e-06,
         -5.2899e-07, -1.4976e-06],
        [-1.5423e-06, -2.5928e-06,  6.1002e-08,  ..., -9.3877e-07,
         -2.6636e-07, -6.8545e-07],
        [-3.2485e-06, -5.3346e-06, -3.6554e-08,  ..., -1.9968e-06,
         -6.5565e-07, -1.3560e-06]], device='cuda:0')
Loss: 1.3075193166732788


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[    0,  3037,  4511,  ...,  1711,    12,  2655],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9847e-04,  6.1397e-04,  3.4986e-04,  ...,  7.5629e-04,
         -6.8904e-04,  9.0618e-04],
        [-4.0978e-06, -7.3910e-06,  1.0245e-07,  ..., -2.6897e-06,
         -4.0524e-07, -2.4140e-06],
        [-6.1542e-06, -1.1325e-05,  5.5530e-08,  ..., -3.9190e-06,
         -5.0117e-07, -3.6731e-06],
        [-2.8536e-06, -5.2601e-06,  1.1339e-07,  ..., -1.8291e-06,
         -2.8004e-07, -1.6764e-06],
        [-5.4985e-06, -9.7454e-06, -1.1944e-07,  ..., -3.5167e-06,
         -7.4180e-07, -2.9057e-06]], device='cuda:0')
Loss: 1.3114036321640015


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   369, 17432,  ...,   874,  2577,    14],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9166e-04,  4.3836e-04,  2.9790e-04,  ...,  1.0272e-03,
         -1.0044e-03,  1.1130e-03],
        [-5.6475e-06, -1.0714e-05,  3.9488e-07,  ..., -3.7998e-06,
         -5.2538e-07, -3.2783e-06],
        [-8.6874e-06, -1.6749e-05,  4.6717e-07,  ..., -5.6401e-06,
         -6.7719e-07, -5.0813e-06],
        [-3.9786e-06, -7.7337e-06,  3.4901e-07,  ..., -2.5965e-06,
         -3.9459e-07, -2.2911e-06],
        [-7.4804e-06, -1.3977e-05,  1.2270e-07,  ..., -4.8429e-06,
         -9.6066e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.3113595247268677


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0387e-04,  5.3102e-04, -8.0512e-05,  ...,  1.2560e-03,
         -1.3830e-03,  9.0085e-04],
        [-7.5996e-06, -1.4350e-05,  8.3074e-07,  ..., -4.9546e-06,
         -4.3411e-07, -4.1761e-06],
        [-1.1504e-05, -2.1964e-05,  1.0669e-06,  ..., -7.2271e-06,
         -5.8126e-07, -6.3926e-06],
        [-5.2527e-06, -1.0088e-05,  6.7311e-07,  ..., -3.3006e-06,
         -3.2800e-07, -2.8536e-06],
        [-9.9391e-06, -1.8477e-05,  5.5856e-07,  ..., -6.2734e-06,
         -9.5965e-07, -4.8652e-06]], device='cuda:0')
Loss: 1.308286190032959


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   20,   13,  ...,  496,   14, 1032],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3749e-03,  5.4972e-04, -2.6179e-04,  ...,  1.3544e-03,
         -1.4965e-03,  7.7450e-04],
        [-9.3579e-06, -1.8060e-05,  1.2033e-06,  ..., -6.4746e-06,
         -1.1047e-06, -4.8168e-06],
        [-1.4216e-05, -2.7657e-05,  1.5699e-06,  ..., -9.4771e-06,
         -1.5871e-06, -7.3537e-06],
        [-6.6012e-06, -1.3009e-05,  9.9163e-07,  ..., -4.4182e-06,
         -8.3464e-07, -3.3155e-06],
        [-1.2234e-05, -2.3276e-05,  9.3109e-07,  ..., -8.2105e-06,
         -1.8276e-06, -5.5581e-06]], device='cuda:0')
Loss: 1.3143407106399536


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[   0,  721, 1119,  ...,  600,  328, 3363],
        [   0, 1478,   14,  ...,  266, 9417, 9105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1204e-03,  4.5568e-04, -7.5678e-04,  ...,  1.6251e-03,
         -1.4566e-03,  7.5711e-04],
        [-1.1064e-05, -2.1726e-05,  2.1383e-06,  ..., -7.4953e-06,
         -1.5163e-06, -5.5432e-06],
        [-1.6764e-05, -3.3140e-05,  2.9110e-06,  ..., -1.0945e-05,
         -2.1831e-06, -8.5011e-06],
        [-7.9125e-06, -1.5855e-05,  1.7702e-06,  ..., -5.1335e-06,
         -1.1271e-06, -3.8594e-06],
        [-1.4439e-05, -2.8044e-05,  2.0338e-06,  ..., -9.4473e-06,
         -2.4200e-06, -6.3665e-06]], device='cuda:0')
Loss: 1.309226393699646


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,   12, 2418,  ...,  446,  381, 2204],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0864e-03,  1.2036e-04, -8.8027e-04,  ...,  1.4413e-03,
         -1.4971e-03,  9.0656e-04],
        [-1.3314e-05, -2.5660e-05,  2.8014e-06,  ..., -8.8662e-06,
         -1.6015e-06, -6.4820e-06],
        [-2.0161e-05, -3.9071e-05,  3.8795e-06,  ..., -1.2942e-05,
         -2.3154e-06, -9.9838e-06],
        [-9.4250e-06, -1.8522e-05,  2.2694e-06,  ..., -6.0052e-06,
         -1.1743e-06, -4.4852e-06],
        [-1.7211e-05, -3.2842e-05,  2.7118e-06,  ..., -1.1101e-05,
         -2.5652e-06, -7.4022e-06]], device='cuda:0')
Loss: 1.3009275197982788


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[   0, 2706,  292,  ...,   13, 8954,   13],
        [   0, 1596, 2700,  ...,  943,  266, 4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8310e-03,  5.0697e-04, -8.3467e-04,  ...,  8.5976e-04,
         -2.1446e-03,  2.6743e-04],
        [-1.5520e-05, -2.9415e-05,  3.0808e-06,  ..., -1.0483e-05,
         -2.4919e-06, -7.5176e-06],
        [-2.3574e-05, -4.4912e-05,  4.2334e-06,  ..., -1.5400e-05,
         -3.6937e-06, -1.1608e-05],
        [-6.7135e-05,  1.3835e-04, -2.7031e-05,  ...,  1.5749e-04,
         -4.5617e-05,  9.8809e-05],
        [-2.0251e-05, -3.7998e-05,  2.9232e-06,  ..., -1.3322e-05,
         -3.8989e-06, -8.7284e-06]], device='cuda:0')
Loss: 1.314985990524292
Graident accumulation at epoch 0, step 31, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0612e-03, -4.1643e-04,  4.1672e-04,  ...,  6.5626e-04,
         -8.6312e-04, -6.0983e-05],
        [-1.6272e-06, -3.9596e-06,  1.2053e-06,  ..., -1.3554e-06,
         -1.7672e-06, -1.9741e-06],
        [-6.8458e-06,  5.0075e-06, -8.6108e-07,  ...,  1.3498e-05,
         -4.3325e-06, -3.2530e-06],
        [-6.6710e-06,  1.2653e-05, -1.5135e-06,  ...,  1.5558e-05,
         -6.4671e-06,  8.3103e-06],
        [-2.2090e-06, -4.9214e-06,  9.6031e-07,  ..., -1.5913e-06,
         -2.0169e-06, -1.9587e-06]], device='cuda:0')
optimizer state dict: tensor([[4.6271e-08, 1.3353e-08, 1.5415e-08,  ..., 1.9023e-08, 3.0235e-08,
         3.5013e-09],
        [2.5438e-13, 9.6704e-13, 5.2696e-14,  ..., 1.1842e-13, 1.2260e-13,
         1.3225e-13],
        [2.1479e-12, 1.7007e-11, 1.3999e-12,  ..., 1.7491e-11, 2.8369e-12,
         8.3980e-13],
        [4.5278e-12, 1.9264e-11, 8.0364e-13,  ..., 2.4810e-11, 2.2716e-12,
         9.8865e-12],
        [4.2571e-13, 1.5646e-12, 3.4752e-14,  ..., 1.8502e-13, 1.4788e-13,
         1.3688e-13]], device='cuda:0')
optimizer state dict: 4.0
lr: [1.9999985024557586e-05, 1.9999985024557586e-05]
scheduler_last_epoch: 4


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 6088, 1172,  ...,  546,  401,  925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2200e-04,  5.8591e-05, -9.8375e-05,  ...,  9.5853e-05,
         -3.5894e-05, -7.3428e-05],
        [-4.1425e-06, -5.4240e-06, -5.1409e-07,  ..., -2.6673e-06,
         -4.3213e-07, -6.4075e-07],
        [-4.2915e-06, -5.6326e-06, -5.6997e-07,  ..., -2.7567e-06,
         -4.5262e-07, -6.8545e-07],
        [-2.2948e-06, -3.0398e-06, -2.7940e-07,  ..., -1.4529e-06,
         -2.1979e-07, -3.6508e-07],
        [-5.9903e-06, -7.8678e-06, -8.6799e-07,  ..., -3.8445e-06,
         -6.8545e-07, -8.7544e-07]], device='cuda:0')
Loss: 1.2769209146499634


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9758e-04,  2.2580e-04, -3.8744e-04,  ...,  3.2686e-04,
         -4.1517e-04, -3.0354e-04],
        [-8.3148e-06, -1.0729e-05, -6.0769e-07,  ..., -5.2303e-06,
         -9.0897e-07, -1.4342e-06],
        [-8.4937e-06, -1.0997e-05, -6.8359e-07,  ..., -5.3048e-06,
         -9.4064e-07, -1.4827e-06],
        [-4.7982e-06, -6.2436e-06, -3.2689e-07,  ..., -2.9728e-06,
         -5.0291e-07, -8.2701e-07],
        [-1.2189e-05, -1.5736e-05, -1.1418e-06,  ..., -7.6443e-06,
         -1.4380e-06, -1.9483e-06]], device='cuda:0')
Loss: 1.2707602977752686


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0, 2652,  271,  ...,  634, 1921,  266],
        [   0,  266, 7407,  ...,  287,  365, 4371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8757e-04,  8.2649e-05, -2.3527e-04,  ...,  4.7161e-04,
         -2.2145e-04, -1.1024e-04],
        [-1.2279e-05, -1.6361e-05, -7.1758e-07,  ..., -7.5847e-06,
         -1.1073e-06, -2.5444e-06],
        [-1.2487e-05, -1.6719e-05, -8.0932e-07,  ..., -7.6890e-06,
         -1.1455e-06, -2.6152e-06],
        [ 2.5186e-04,  4.5374e-04,  5.2902e-05,  ...,  1.6775e-04,
          5.8116e-05,  1.3643e-04],
        [-1.8090e-05, -2.4140e-05, -1.3858e-06,  ..., -1.1116e-05,
         -1.7956e-06, -3.4906e-06]], device='cuda:0')
Loss: 1.2812362909317017


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,  4263,  4865,  ...,  1878,   278,  4450],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6966e-04,  3.9345e-04, -2.3527e-04,  ...,  7.4857e-04,
         -1.7311e-04, -7.0592e-04],
        [-1.6391e-05, -2.1845e-05, -1.1981e-06,  ..., -1.0073e-05,
         -8.5961e-07, -3.0845e-06],
        [-1.6659e-05, -2.2292e-05, -1.3197e-06,  ..., -1.0192e-05,
         -9.0152e-07, -3.1590e-06],
        [ 2.4949e-04,  4.5058e-04,  5.2641e-05,  ...,  1.6634e-04,
          5.8262e-05,  1.3615e-04],
        [-2.5064e-05, -3.3379e-05, -2.3469e-06,  ..., -1.5318e-05,
         -1.4193e-06, -4.3139e-06]], device='cuda:0')
Loss: 1.2804598808288574


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,   510,    13,  ...,  3454,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6966e-04,  3.0180e-04, -9.9705e-05,  ...,  8.6434e-04,
          1.5984e-04, -7.7850e-04],
        [-2.0444e-05, -2.7329e-05, -1.7271e-06,  ..., -1.2428e-05,
         -6.2771e-07, -3.7774e-06],
        [-2.0832e-05, -2.7895e-05, -1.8859e-06,  ..., -1.2577e-05,
         -6.9197e-07, -3.8482e-06],
        [ 2.4707e-04,  4.4730e-04,  5.2351e-05,  ...,  1.6496e-04,
          5.8401e-05,  1.3575e-04],
        [-3.1739e-05, -4.2260e-05, -3.3602e-06,  ..., -1.9163e-05,
         -1.1306e-06, -5.2415e-06]], device='cuda:0')
Loss: 1.2660402059555054


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0758e-03,  1.2743e-04, -4.5279e-04,  ...,  9.6178e-04,
         -2.1169e-04, -7.4493e-04],
        [-2.4229e-05, -3.2693e-05, -1.9516e-06,  ..., -1.4618e-05,
         -6.6799e-07, -4.2189e-06],
        [-2.4945e-05, -3.3736e-05, -2.1728e-06,  ..., -1.4916e-05,
         -7.5018e-07, -4.3157e-06],
        [ 2.4485e-04,  4.4414e-04,  5.2230e-05,  ...,  1.6369e-04,
          5.8394e-05,  1.3548e-04],
        [-3.7849e-05, -5.0843e-05, -3.8445e-06,  ..., -2.2665e-05,
         -1.2750e-06, -5.8599e-06]], device='cuda:0')
Loss: 1.2804354429244995


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  4323,  8213,  ...,  1153,   278,  4258],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1932e-03, -2.2136e-04, -7.9140e-04,  ...,  8.0508e-04,
         -6.2864e-04, -6.6964e-04],
        [-2.8402e-05, -3.8385e-05, -2.2422e-06,  ..., -1.7002e-05,
         -6.4494e-07, -5.2694e-06],
        [-2.8998e-05, -3.9369e-05, -2.4876e-06,  ..., -1.7241e-05,
         -7.3569e-07, -5.3588e-06],
        [ 2.4250e-04,  4.4091e-04,  5.2089e-05,  ...,  1.6236e-04,
          5.8419e-05,  1.3491e-04],
        [-4.3929e-05, -5.9128e-05, -4.3921e-06,  ..., -2.6181e-05,
         -1.2676e-06, -7.2829e-06]], device='cuda:0')
Loss: 1.272847056388855


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6897e-04, -4.5701e-04, -4.3708e-04,  ...,  8.3616e-04,
         -7.6366e-04, -7.4282e-04],
        [-3.2872e-05, -4.3958e-05, -2.6911e-06,  ..., -1.9431e-05,
         -1.0324e-06, -5.7537e-06],
        [-3.3379e-05, -4.4852e-05, -2.9979e-06,  ..., -1.9595e-05,
         -1.1268e-06, -5.8357e-06],
        [ 2.3996e-04,  4.3773e-04,  5.1841e-05,  ...,  1.6100e-04,
          5.8202e-05,  1.3464e-04],
        [-5.1022e-05, -6.8009e-05, -5.2936e-06,  ..., -3.0026e-05,
         -1.9195e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.2704589366912842
Graident accumulation at epoch 0, step 39, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0408,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0420e-03, -4.2048e-04,  3.3134e-04,  ...,  6.7425e-04,
         -8.5318e-04, -1.2917e-04],
        [-4.7517e-06, -7.9595e-06,  8.1562e-07,  ..., -3.1630e-06,
         -1.6937e-06, -2.3521e-06],
        [-9.4991e-06,  2.1519e-08, -1.0748e-06,  ...,  1.0189e-05,
         -4.0120e-06, -3.5113e-06],
        [ 1.7993e-05,  5.5161e-05,  3.8219e-06,  ...,  3.0102e-05,
         -1.6635e-10,  2.0944e-05],
        [-7.0902e-06, -1.1230e-05,  3.3492e-07,  ..., -4.4348e-06,
         -2.0072e-06, -2.5586e-06]], device='cuda:0')
optimizer state dict: tensor([[4.6980e-08, 1.3548e-08, 1.5591e-08,  ..., 1.9703e-08, 3.0788e-08,
         4.0496e-09],
        [1.3347e-12, 2.8984e-12, 5.9885e-14,  ..., 4.9587e-13, 1.2354e-13,
         1.6522e-13],
        [3.2599e-12, 1.9002e-11, 1.4075e-12,  ..., 1.7858e-11, 2.8353e-12,
         8.7302e-13],
        [6.2106e-11, 2.1086e-10, 3.4903e-12,  ..., 5.0705e-11, 5.6569e-12,
         2.8005e-11],
        [3.0285e-12, 6.1882e-12, 6.2740e-14,  ..., 1.0864e-12, 1.5141e-13,
         2.0006e-13]], device='cuda:0')
optimizer state dict: 5.0
lr: [1.9996501145215088e-05, 1.9996501145215088e-05]
scheduler_last_epoch: 5


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1760,     9,  ...,  5996,    71,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8324e-04, -1.3944e-04, -4.6265e-04,  ..., -1.5185e-04,
         -1.1103e-04, -1.0212e-04],
        [-5.0962e-06, -5.9903e-06, -9.9093e-07,  ..., -3.2634e-06,
         -7.0035e-07, -3.2410e-07],
        [-4.0531e-06, -4.7684e-06, -7.9721e-07,  ..., -2.5928e-06,
         -5.5879e-07, -2.5891e-07],
        [-3.0696e-06, -3.6210e-06, -5.8487e-07,  ..., -1.9670e-06,
         -4.1351e-07, -1.7881e-07],
        [-8.7619e-06, -1.0252e-05, -1.7658e-06,  ..., -5.5730e-06,
         -1.2293e-06, -4.7311e-07]], device='cuda:0')
Loss: 1.2504453659057617


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0,   266,  7264,  ...,  3211,   328,   275],
        [    0,  2496, 10545,  ...,   287, 13978,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1326e-04, -3.6440e-04, -5.2307e-04,  ..., -1.5185e-04,
         -2.7393e-04, -2.8936e-04],
        [-9.9242e-06, -1.1563e-05, -1.7919e-06,  ..., -6.2287e-06,
         -1.3895e-06, -6.0163e-07],
        [-8.1360e-06, -9.5069e-06, -1.4827e-06,  ..., -5.0962e-06,
         -1.1325e-06, -4.8894e-07],
        [-6.0350e-06, -7.0482e-06, -1.0617e-06,  ..., -3.7849e-06,
         -8.2888e-07, -3.4552e-07],
        [-1.7226e-05, -2.0027e-05, -3.2261e-06,  ..., -1.0788e-05,
         -2.4661e-06, -8.9221e-07]], device='cuda:0')
Loss: 1.2479232549667358


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  6847,   437,  ...,    17,    14,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9359e-04,  1.4099e-05, -4.8148e-04,  ..., -3.5566e-05,
         -2.4287e-04, -5.0802e-04],
        [-1.4782e-05, -1.7226e-05, -2.6710e-06,  ..., -9.7007e-06,
         -2.0489e-06, -1.0375e-06],
        [-1.2189e-05, -1.4216e-05, -2.2203e-06,  ..., -7.9572e-06,
         -1.6838e-06, -8.6706e-07],
        [-8.9407e-06, -1.0416e-05, -1.5758e-06,  ..., -5.8413e-06,
         -1.2089e-06, -6.1374e-07],
        [-2.5749e-05, -2.9922e-05, -4.8205e-06,  ..., -1.6868e-05,
         -3.6433e-06, -1.6149e-06]], device='cuda:0')
Loss: 1.2657170295715332


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 37683,    14,  1500],
        [    0,    12,  2085,  ...,   287,   593,  4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9028e-04, -2.6192e-04, -5.0572e-04,  ...,  5.9450e-06,
         -5.5744e-04, -6.0217e-04],
        [-1.9431e-05, -2.2799e-05, -3.6545e-06,  ..., -1.2949e-05,
         -2.5555e-06, -1.4640e-06],
        [-1.6272e-05, -1.9133e-05, -3.0957e-06,  ..., -1.0803e-05,
         -2.1327e-06, -1.2489e-06],
        [-1.1802e-05, -1.3858e-05, -2.1718e-06,  ..., -7.8231e-06,
         -1.5143e-06, -8.6520e-07],
        [-3.3796e-05, -3.9518e-05, -6.5863e-06,  ..., -2.2501e-05,
         -4.5747e-06, -2.2892e-06]], device='cuda:0')
Loss: 1.2417411804199219


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2457e-04, -3.7367e-04, -6.1023e-04,  ...,  1.4279e-05,
         -8.3512e-04, -7.7055e-04],
        [-2.4259e-05, -2.8402e-05, -4.7348e-06,  ..., -1.6287e-05,
         -3.5055e-06, -1.8869e-06],
        [-2.0504e-05, -2.4021e-05, -4.0494e-06,  ..., -1.3709e-05,
         -2.9635e-06, -1.6214e-06],
        [-1.4827e-05, -1.7345e-05, -2.8349e-06,  ..., -9.8944e-06,
         -2.1029e-06, -1.1241e-06],
        [-4.2439e-05, -4.9472e-05, -8.6129e-06,  ..., -2.8431e-05,
         -6.2808e-06, -2.9709e-06]], device='cuda:0')
Loss: 1.2511290311813354


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   271,  4787,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0771e-04, -5.2427e-04, -9.5647e-04,  ...,  8.8511e-05,
         -8.0655e-04, -9.9176e-04],
        [-2.9027e-05, -3.4094e-05, -5.6960e-06,  ..., -1.9476e-05,
         -4.2170e-06, -2.0992e-06],
        [-2.4438e-05, -2.8700e-05, -4.8541e-06,  ..., -1.6317e-05,
         -3.5483e-06, -1.8012e-06],
        [-1.7703e-05, -2.0772e-05, -3.4049e-06,  ..., -1.1802e-05,
         -2.5332e-06, -1.2536e-06],
        [-5.0843e-05, -5.9485e-05, -1.0356e-05,  ..., -3.4004e-05,
         -7.5772e-06, -3.2559e-06]], device='cuda:0')
Loss: 1.2511932849884033


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[    0,  2923,   391,  ...,    14,  5424,   298],
        [    0,  4988, 36842,  ...,  7630, 18362,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4815e-04, -7.6668e-04, -1.0490e-03,  ...,  6.5570e-05,
         -8.3754e-04, -1.0927e-03],
        [-3.4094e-05, -4.0054e-05, -6.6385e-06,  ..., -2.2843e-05,
         -5.0552e-06, -2.3162e-06],
        [-2.8521e-05, -3.3528e-05, -5.6252e-06,  ..., -1.9029e-05,
         -4.2375e-06, -1.9763e-06],
        [-2.0579e-05, -2.4155e-05, -3.9265e-06,  ..., -1.3694e-05,
         -3.0063e-06, -1.3709e-06],
        [-5.9366e-05, -6.9559e-05, -1.2003e-05,  ..., -3.9697e-05,
         -9.0152e-06, -3.5670e-06]], device='cuda:0')
Loss: 1.2629730701446533


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[    0,   908,    14,  ...,    19,    27,   287],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0664e-04, -8.3014e-04, -1.0588e-03,  ..., -4.6006e-05,
         -8.9854e-04, -1.3462e-03],
        [-3.9130e-05, -4.5598e-05, -7.6890e-06,  ..., -2.6032e-05,
         -5.5358e-06, -2.6012e-06],
        [-3.2663e-05, -3.8058e-05, -6.5006e-06,  ..., -2.1636e-05,
         -4.6454e-06, -2.2277e-06],
        [-2.3574e-05, -2.7433e-05, -4.5374e-06,  ..., -1.5579e-05,
         -3.2894e-06, -1.5367e-06],
        [-6.7949e-05, -7.9036e-05, -1.3873e-05,  ..., -4.5180e-05,
         -9.8720e-06, -4.0010e-06]], device='cuda:0')
Loss: 1.255104899406433
Graident accumulation at epoch 0, step 47, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.7844e-04, -4.6145e-04,  1.9232e-04,  ...,  6.0223e-04,
         -8.5771e-04, -2.5087e-04],
        [-8.1895e-06, -1.1723e-05, -3.4842e-08,  ..., -5.4499e-06,
         -2.0779e-06, -2.3770e-06],
        [-1.1816e-05, -3.7864e-06, -1.6174e-06,  ...,  7.0062e-06,
         -4.0753e-06, -3.3829e-06],
        [ 1.3836e-05,  4.6902e-05,  2.9860e-06,  ...,  2.5534e-05,
         -3.2909e-07,  1.8696e-05],
        [-1.3176e-05, -1.8011e-05, -1.0859e-06,  ..., -8.5093e-06,
         -2.7936e-06, -2.7028e-06]], device='cuda:0')
optimizer state dict: tensor([[4.7099e-08, 1.4224e-08, 1.6696e-08,  ..., 1.9686e-08, 3.1565e-08,
         5.8579e-09],
        [2.8646e-12, 4.9747e-12, 1.1895e-13,  ..., 1.1731e-12, 1.5407e-13,
         1.7182e-13],
        [4.3235e-12, 2.0431e-11, 1.4484e-12,  ..., 1.8308e-11, 2.8541e-12,
         8.7711e-13],
        [6.2600e-11, 2.1140e-10, 3.5074e-12,  ..., 5.0897e-11, 5.6620e-12,
         2.7980e-11],
        [7.6426e-12, 1.2429e-11, 2.5514e-13,  ..., 3.1266e-12, 2.4872e-13,
         2.1587e-13]], device='cuda:0')
optimizer state dict: 6.0
lr: [1.9986907288753243e-05, 1.9986907288753243e-05]
scheduler_last_epoch: 6


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3132e-04,  2.0187e-04, -4.2670e-04,  ..., -1.8124e-05,
          2.6413e-06,  1.2655e-04],
        [-5.0366e-06, -5.1856e-06, -5.8860e-07,  ..., -3.6508e-06,
         -7.3388e-07, -1.3504e-07],
        [-3.9637e-06, -4.0829e-06, -4.7311e-07,  ..., -2.8759e-06,
         -5.7369e-07, -1.1083e-07],
        [-3.5018e-06, -3.6061e-06, -4.0606e-07,  ..., -2.5332e-06,
         -5.0291e-07, -9.2201e-08],
        [-9.2387e-06, -9.5367e-06, -1.1176e-06,  ..., -6.6757e-06,
         -1.3486e-06, -2.1886e-07]], device='cuda:0')
Loss: 1.2450376749038696


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2043e-04,  1.8190e-04, -3.6031e-04,  ..., -1.6336e-05,
         -2.9695e-04, -2.5912e-05],
        [-1.0103e-05, -1.0282e-05, -1.2629e-06,  ..., -7.2867e-06,
         -1.8142e-06, -3.5018e-07],
        [-7.8678e-06, -8.0466e-06, -9.9465e-07,  ..., -5.6922e-06,
         -1.4119e-06, -2.8405e-07],
        [-7.0035e-06, -7.1526e-06, -8.6240e-07,  ..., -5.0515e-06,
         -1.2517e-06, -2.3004e-07],
        [-1.8418e-05, -1.8775e-05, -2.3842e-06,  ..., -1.3292e-05,
         -3.3304e-06, -5.5786e-07]], device='cuda:0')
Loss: 1.2470332384109497


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,    13,  1529,  ...,  8197,  2700,  9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4916e-04,  2.5707e-04, -5.0079e-04,  ...,  7.9674e-06,
         -1.6779e-04,  1.4481e-06],
        [-1.5020e-05, -1.5318e-05, -2.0303e-06,  ..., -1.1012e-05,
         -3.0883e-06, -3.8138e-07],
        [-1.1593e-05, -1.1861e-05, -1.5758e-06,  ..., -8.5086e-06,
         -2.3730e-06, -3.1432e-07],
        [-1.0461e-05, -1.0699e-05, -1.3877e-06,  ..., -7.6592e-06,
         -2.1458e-06, -2.3935e-07],
        [-2.7716e-05, -2.8312e-05, -3.8743e-06,  ..., -2.0295e-05,
         -5.7444e-06, -5.8115e-07]], device='cuda:0')
Loss: 1.2197754383087158


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[   0, 8353, 1842,  ...,   38,  643,  472],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2654e-04,  2.1054e-04, -6.9969e-04,  ..., -1.2329e-04,
         -1.8164e-04, -1.3105e-04],
        [-1.9848e-05, -2.0593e-05, -2.7567e-06,  ..., -1.4782e-05,
         -4.2282e-06, -5.1083e-07],
        [-1.5438e-05, -1.6093e-05, -2.1644e-06,  ..., -1.1519e-05,
         -3.2857e-06, -4.2515e-07],
        [-1.3784e-05, -1.4350e-05, -1.8757e-06,  ..., -1.0267e-05,
         -2.9355e-06, -3.2596e-07],
        [-3.6776e-05, -3.8266e-05, -5.2825e-06,  ..., -2.7418e-05,
         -7.9051e-06, -7.7300e-07]], device='cuda:0')
Loss: 1.2554720640182495


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3943e-04,  4.1280e-06, -6.8473e-04,  ..., -2.3200e-04,
         -3.1862e-04, -8.0796e-05],
        [-2.4855e-05, -2.5839e-05, -3.4757e-06,  ..., -1.8537e-05,
         -5.1968e-06, -6.0862e-07],
        [-1.9282e-05, -2.0117e-05, -2.7269e-06,  ..., -1.4409e-05,
         -4.0345e-06, -4.9686e-07],
        [-1.7226e-05, -1.7956e-05, -2.3600e-06,  ..., -1.2845e-05,
         -3.5949e-06, -3.8254e-07],
        [-4.6015e-05, -4.7863e-05, -6.6534e-06,  ..., -3.4332e-05,
         -9.7081e-06, -8.9873e-07]], device='cuda:0')
Loss: 1.2449110746383667


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0,   271,   266,  ...,   275,  2576,  3588]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0630e-03,  3.6481e-05, -7.4398e-04,  ..., -3.3324e-04,
         -2.7712e-04,  3.8669e-05],
        [-2.9683e-05, -3.0875e-05, -4.2394e-06,  ..., -2.2262e-05,
         -6.1058e-06, -6.6916e-07],
        [-2.3097e-05, -2.4110e-05, -3.3341e-06,  ..., -1.7360e-05,
         -4.7535e-06, -5.4459e-07],
        [-2.0608e-05, -2.1502e-05, -2.8890e-06,  ..., -1.5453e-05,
         -4.2357e-06, -4.1770e-07],
        [-5.5194e-05, -5.7459e-05, -8.1584e-06,  ..., -4.1425e-05,
         -1.1459e-05, -9.6671e-07]], device='cuda:0')
Loss: 1.2522222995758057


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[   0, 3968,  446,  ...,   22,  722,  342],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8408e-04,  6.4926e-06, -7.7799e-04,  ..., -3.2558e-04,
         -4.4705e-04, -6.7284e-05],
        [-3.4720e-05, -3.5912e-05, -5.0962e-06,  ..., -2.5958e-05,
         -7.2755e-06, -7.6788e-07],
        [-2.7150e-05, -2.8163e-05, -4.0345e-06,  ..., -2.0355e-05,
         -5.6922e-06, -6.3074e-07],
        [-2.4080e-05, -2.4989e-05, -3.4701e-06,  ..., -1.8016e-05,
         -5.0366e-06, -4.7870e-07],
        [-6.4552e-05, -6.6876e-05, -9.7975e-06,  ..., -4.8339e-05,
         -1.3664e-05, -1.1083e-06]], device='cuda:0')
Loss: 1.2428922653198242


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[    0,  4538,   271,  ...,  1603,   591,   688],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0398e-03, -3.9963e-05, -8.8372e-04,  ..., -3.7270e-04,
         -5.0812e-04, -8.0689e-05],
        [-3.9876e-05, -4.1127e-05, -5.6103e-06,  ..., -2.9713e-05,
         -8.4899e-06, -8.2189e-07],
        [-3.1114e-05, -3.2187e-05, -4.4294e-06,  ..., -2.3261e-05,
         -6.6310e-06, -6.7800e-07],
        [-2.7582e-05, -2.8521e-05, -3.8110e-06,  ..., -2.0564e-05,
         -5.8562e-06, -5.0990e-07],
        [-7.3850e-05, -7.6294e-05, -1.0766e-05,  ..., -5.5164e-05,
         -1.5900e-05, -1.1493e-06]], device='cuda:0')
Loss: 1.2292808294296265
Graident accumulation at epoch 0, step 55, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.8457e-04, -4.1930e-04,  8.4720e-05,  ...,  5.0473e-04,
         -8.2276e-04, -2.3386e-04],
        [-1.1358e-05, -1.4664e-05, -5.9239e-07,  ..., -7.8762e-06,
         -2.7191e-06, -2.2215e-06],
        [-1.3745e-05, -6.6264e-06, -1.8986e-06,  ...,  3.9795e-06,
         -4.3309e-06, -3.1124e-06],
        [ 9.6941e-06,  3.9360e-05,  2.3063e-06,  ...,  2.0924e-05,
         -8.8180e-07,  1.6775e-05],
        [-1.9244e-05, -2.3839e-05, -2.0539e-06,  ..., -1.3175e-05,
         -4.1042e-06, -2.5475e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8133e-08, 1.4211e-08, 1.7460e-08,  ..., 1.9805e-08, 3.1791e-08,
         5.8585e-09],
        [4.4517e-12, 6.6611e-12, 1.5030e-13,  ..., 2.0547e-12, 2.2599e-13,
         1.7233e-13],
        [5.2873e-12, 2.1447e-11, 1.4665e-12,  ..., 1.8831e-11, 2.8952e-12,
         8.7669e-13],
        [6.3298e-11, 2.1200e-10, 3.5185e-12,  ..., 5.1269e-11, 5.6907e-12,
         2.7952e-11],
        [1.3089e-11, 1.8237e-11, 3.7079e-13,  ..., 6.1665e-12, 5.0127e-13,
         2.1697e-13]], device='cuda:0')
optimizer state dict: 7.0
lr: [1.997120931904809e-05, 1.997120931904809e-05]
scheduler_last_epoch: 7


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,   396,   298,  ...,    52,  5065,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1608e-05,  4.1091e-05,  6.7640e-06,  ..., -1.3784e-04,
         -5.2658e-05, -2.9070e-05],
        [-4.9174e-06, -4.3511e-06, -1.3039e-07,  ..., -3.9339e-06,
         -1.3411e-06, -3.2783e-07],
        [-3.6210e-06, -3.2037e-06, -9.8720e-08,  ..., -2.8908e-06,
         -9.8348e-07, -2.3562e-07],
        [-3.8743e-06, -3.4124e-06, -9.4995e-08,  ..., -3.0845e-06,
         -1.0505e-06, -2.4214e-07],
        [-9.2983e-06, -8.2254e-06, -2.8312e-07,  ..., -7.3910e-06,
         -2.5332e-06, -5.6624e-07]], device='cuda:0')
Loss: 1.208550214767456


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   344, 10706,  ...,  1184,   578,   825],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8988e-05,  1.6821e-04, -7.4609e-05,  ..., -2.9434e-04,
         -1.7589e-04, -1.6593e-04],
        [-9.9540e-06, -8.6725e-06, -3.2131e-07,  ..., -7.9274e-06,
         -2.6003e-06, -4.8801e-07],
        [-7.1973e-06, -6.2883e-06, -2.3935e-07,  ..., -5.7369e-06,
         -1.8887e-06, -3.5763e-07],
        [-7.8976e-06, -6.8694e-06, -2.4214e-07,  ..., -6.2585e-06,
         -2.0564e-06, -3.6974e-07],
        [-1.8597e-05, -1.6212e-05, -6.7800e-07,  ..., -1.4752e-05,
         -4.8876e-06, -8.3819e-07]], device='cuda:0')
Loss: 1.2321109771728516


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0360e-05,  3.3040e-04,  2.6487e-04,  ..., -2.9506e-04,
         -4.8720e-04, -1.3946e-04],
        [-1.5020e-05, -1.3083e-05, -6.1188e-07,  ..., -1.2070e-05,
         -3.9488e-06, -9.3319e-07],
        [-1.0788e-05, -9.4324e-06, -4.4331e-07,  ..., -8.6725e-06,
         -2.8498e-06, -6.7614e-07],
        [-1.1832e-05, -1.0312e-05, -4.5355e-07,  ..., -9.4771e-06,
         -3.1069e-06, -7.1991e-07],
        [-2.8074e-05, -2.4498e-05, -1.2517e-06,  ..., -2.2441e-05,
         -7.4208e-06, -1.6540e-06]], device='cuda:0')
Loss: 1.2457176446914673


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 2728, 3139,  ..., 2254,  221,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5269e-05,  4.7883e-04,  3.1277e-04,  ..., -3.5367e-04,
         -5.5209e-04, -2.8147e-04],
        [-2.0117e-05, -1.7643e-05, -7.6555e-07,  ..., -1.6212e-05,
         -5.3048e-06, -1.3150e-06],
        [-1.4499e-05, -1.2755e-05, -5.5647e-07,  ..., -1.1683e-05,
         -3.8408e-06, -9.5367e-07],
        [-1.5855e-05, -1.3903e-05, -5.6438e-07,  ..., -1.2726e-05,
         -4.1798e-06, -1.0123e-06],
        [-3.7730e-05, -3.3081e-05, -1.5795e-06,  ..., -3.0249e-05,
         -1.0014e-05, -2.3320e-06]], device='cuda:0')
Loss: 1.221134901046753


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,   437,   638,  ...,  4514,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0515e-05,  5.8208e-04,  2.8091e-04,  ..., -4.0529e-04,
         -5.1002e-04, -2.8253e-04],
        [-2.5153e-05, -2.2054e-05, -1.0077e-06,  ..., -2.0176e-05,
         -6.6012e-06, -1.5069e-06],
        [-1.8209e-05, -1.6019e-05, -7.3807e-07,  ..., -1.4618e-05,
         -4.7944e-06, -1.0896e-06],
        [-1.9819e-05, -1.7375e-05, -7.3947e-07,  ..., -1.5855e-05,
         -5.2005e-06, -1.1530e-06],
        [-4.7147e-05, -4.1306e-05, -2.0564e-06,  ..., -3.7640e-05,
         -1.2457e-05, -2.6431e-06]], device='cuda:0')
Loss: 1.2231658697128296


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0,   638,  2708,  ..., 28492,  1814,    12],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1146e-04,  5.3040e-04,  2.1266e-04,  ..., -5.1782e-04,
         -4.8785e-04, -1.7013e-04],
        [-2.9922e-05, -2.6435e-05, -1.1614e-06,  ..., -2.4080e-05,
         -7.9423e-06, -1.8012e-06],
        [-2.1771e-05, -1.9312e-05, -8.5915e-07,  ..., -1.7524e-05,
         -5.8003e-06, -1.3113e-06],
        [-2.3633e-05, -2.0891e-05, -8.4983e-07,  ..., -1.8969e-05,
         -6.2734e-06, -1.3784e-06],
        [-5.6148e-05, -4.9591e-05, -2.3860e-06,  ..., -4.4972e-05,
         -1.5020e-05, -3.1646e-06]], device='cuda:0')
Loss: 1.2336807250976562


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5774e-04,  5.5806e-04,  3.3463e-04,  ..., -4.9532e-04,
         -6.2549e-04, -1.9550e-04],
        [-3.4750e-05, -3.0726e-05, -1.3821e-06,  ..., -2.7984e-05,
         -9.1195e-06, -2.1327e-06],
        [-2.5421e-05, -2.2575e-05, -1.0268e-06,  ..., -2.0474e-05,
         -6.7018e-06, -1.5665e-06],
        [-2.7597e-05, -2.4408e-05, -1.0240e-06,  ..., -2.2158e-05,
         -7.2420e-06, -1.6447e-06],
        [-6.5565e-05, -5.7995e-05, -2.8498e-06,  ..., -5.2571e-05,
         -1.7345e-05, -3.7905e-06]], device='cuda:0')
Loss: 1.2292001247406006


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,  3990,   717,    12],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0196e-04,  6.7984e-04,  1.6746e-04,  ..., -5.7024e-04,
         -5.6356e-04, -1.9565e-04],
        [-3.9607e-05, -3.5226e-05, -1.5553e-06,  ..., -3.2067e-05,
         -1.0438e-05, -2.4568e-06],
        [-2.8983e-05, -2.5898e-05, -1.1609e-06,  ..., -2.3469e-05,
         -7.6704e-06, -1.8030e-06],
        [-3.1501e-05, -2.8029e-05, -1.1590e-06,  ..., -2.5421e-05,
         -8.2925e-06, -1.8999e-06],
        [-7.4506e-05, -6.6340e-05, -3.2075e-06,  ..., -6.0111e-05,
         -1.9774e-05, -4.3418e-06]], device='cuda:0')
Loss: 1.2329025268554688
Graident accumulation at epoch 0, step 63, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0035,  0.0220, -0.0208],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2631e-04, -3.0939e-04,  9.2994e-05,  ...,  3.9724e-04,
         -7.9684e-04, -2.3004e-04],
        [-1.4183e-05, -1.6720e-05, -6.8868e-07,  ..., -1.0295e-05,
         -3.4910e-06, -2.2450e-06],
        [-1.5269e-05, -8.5536e-06, -1.8248e-06,  ...,  1.2346e-06,
         -4.6648e-06, -2.9815e-06],
        [ 5.5746e-06,  3.2621e-05,  1.9598e-06,  ...,  1.6290e-05,
         -1.6229e-06,  1.4908e-05],
        [-2.4770e-05, -2.8089e-05, -2.1693e-06,  ..., -1.7868e-05,
         -5.6712e-06, -2.7269e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8246e-08, 1.4659e-08, 1.7471e-08,  ..., 2.0110e-08, 3.2077e-08,
         5.8910e-09],
        [6.0160e-12, 7.8954e-12, 1.5257e-13,  ..., 3.0810e-12, 3.3472e-13,
         1.7819e-13],
        [6.1220e-12, 2.2096e-11, 1.4664e-12,  ..., 1.9363e-11, 2.9511e-12,
         8.7907e-13],
        [6.4227e-11, 2.1257e-10, 3.5163e-12,  ..., 5.1864e-11, 5.7537e-12,
         2.7928e-11],
        [1.8627e-11, 2.2620e-11, 3.8071e-13,  ..., 9.7737e-12, 8.9177e-13,
         2.3560e-13]], device='cuda:0')
optimizer state dict: 8.0
lr: [1.9949416830880266e-05, 1.9949416830880266e-05]
scheduler_last_epoch: 8


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,   271,  3616,  ...,    12,  1348,  5037],
        [    0,   380,  2114,  ...,   456, 28979,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1908e-04,  2.7941e-05,  5.0084e-05,  ..., -4.8171e-05,
          2.9993e-05, -1.3908e-04],
        [-4.6194e-06, -3.7551e-06,  3.2224e-07,  ..., -3.9935e-06,
         -1.1027e-06, -6.5938e-07],
        [-3.2783e-06, -2.6673e-06,  2.2445e-07,  ..., -2.8461e-06,
         -7.8604e-07, -4.6566e-07],
        [-4.4405e-06, -3.6210e-06,  3.2224e-07,  ..., -3.8743e-06,
         -1.0654e-06, -6.2957e-07],
        [-8.6427e-06, -7.0035e-06,  5.7742e-07,  ..., -7.4804e-06,
         -2.0713e-06, -1.1995e-06]], device='cuda:0')
Loss: 1.2351793050765991


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  5583,   598,  ...,   199,   395,  6551],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0436e-05,  5.8392e-05,  1.2990e-04,  ..., -7.8077e-05,
          1.5104e-04, -2.2613e-04],
        [-9.4473e-06, -7.6592e-06,  6.6124e-07,  ..., -8.1360e-06,
         -2.2352e-06, -1.3150e-06],
        [-6.7055e-06, -5.4389e-06,  4.6194e-07,  ..., -5.7966e-06,
         -1.5870e-06, -9.3132e-07],
        [-8.9407e-06, -7.2420e-06,  6.4448e-07,  ..., -7.7486e-06,
         -2.1234e-06, -1.2293e-06],
        [-1.7583e-05, -1.4186e-05,  1.1735e-06,  ..., -1.5169e-05,
         -4.1723e-06, -2.3767e-06]], device='cuda:0')
Loss: 1.238576889038086


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,  1099,  2851,  ...,   518,   496,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2131e-05,  6.0777e-05,  1.3164e-04,  ..., -1.0398e-04,
          1.0091e-04, -2.4113e-04],
        [-1.4126e-05, -1.1384e-05,  1.0002e-06,  ..., -1.2130e-05,
         -3.4720e-06, -2.0154e-06],
        [-1.0073e-05, -8.1211e-06,  7.0222e-07,  ..., -8.6576e-06,
         -2.4773e-06, -1.4380e-06],
        [-1.3411e-05, -1.0818e-05,  9.8348e-07,  ..., -1.1563e-05,
         -3.3006e-06, -1.8999e-06],
        [-2.6286e-05, -2.1130e-05,  1.7844e-06,  ..., -2.2620e-05,
         -6.4969e-06, -3.6582e-06]], device='cuda:0')
Loss: 1.215898871421814


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,   685,  3482,  ..., 23113,    12,  6481],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2212e-04,  8.7614e-05,  9.1127e-06,  ..., -3.1149e-05,
         -4.4206e-05, -3.7729e-04],
        [-1.8895e-05, -1.5065e-05,  1.3188e-06,  ..., -1.6272e-05,
         -4.6194e-06, -2.6450e-06],
        [-1.3411e-05, -1.0699e-05,  9.2573e-07,  ..., -1.1563e-05,
         -3.2820e-06, -1.8794e-06],
        [-1.7941e-05, -1.4320e-05,  1.2964e-06,  ..., -1.5527e-05,
         -4.3884e-06, -2.4922e-06],
        [-3.5167e-05, -2.7955e-05,  2.3469e-06,  ..., -3.0369e-05,
         -8.6427e-06, -4.8056e-06]], device='cuda:0')
Loss: 1.2236557006835938


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7726e-04,  1.1176e-04,  4.8333e-05,  ..., -1.5633e-04,
          4.5779e-05, -3.6324e-04],
        [-2.3633e-05, -1.8850e-05,  1.6429e-06,  ..., -2.0325e-05,
         -5.7817e-06, -3.3714e-06],
        [-1.6868e-05, -1.3471e-05,  1.1632e-06,  ..., -1.4514e-05,
         -4.1313e-06, -2.4159e-06],
        [-2.2441e-05, -1.7911e-05,  1.6149e-06,  ..., -1.9372e-05,
         -5.4911e-06, -3.1851e-06],
        [-4.3809e-05, -3.4899e-05,  2.9132e-06,  ..., -3.7760e-05,
         -1.0774e-05, -6.1244e-06]], device='cuda:0')
Loss: 1.2306616306304932


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,  3393,  3380,  ...,   292,  6502,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8359e-04,  1.6428e-04,  9.9213e-05,  ..., -2.3046e-04,
          4.5426e-05, -3.6370e-04],
        [-2.8372e-05, -2.2665e-05,  2.0172e-06,  ..., -2.4319e-05,
         -6.8173e-06, -4.0755e-06],
        [-2.0280e-05, -1.6212e-05,  1.4314e-06,  ..., -1.7390e-05,
         -4.8801e-06, -2.9225e-06],
        [-2.6941e-05, -2.1547e-05,  1.9800e-06,  ..., -2.3156e-05,
         -6.4820e-06, -3.8520e-06],
        [-5.2571e-05, -4.1962e-05,  3.5875e-06,  ..., -4.5151e-05,
         -1.2711e-05, -7.4059e-06]], device='cuda:0')
Loss: 1.21747624874115


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4011e-04,  2.2652e-04,  1.5147e-04,  ..., -2.6260e-04,
          6.8293e-07, -5.1729e-04],
        [-3.3110e-05, -2.6539e-05,  2.3730e-06,  ..., -2.8551e-05,
         -7.9945e-06, -4.6715e-06],
        [-2.3589e-05, -1.8895e-05,  1.6754e-06,  ..., -2.0325e-05,
         -5.7034e-06, -3.3360e-06],
        [-3.1352e-05, -2.5123e-05,  2.3190e-06,  ..., -2.7061e-05,
         -7.5772e-06, -4.4033e-06],
        [-6.1393e-05, -4.9144e-05,  4.2319e-06,  ..., -5.3018e-05,
         -1.4916e-05, -8.4862e-06]], device='cuda:0')
Loss: 1.2240327596664429


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[   0,  278,  668,  ..., 2743,  638,  609],
        [   0, 2836, 3084,  ..., 3634, 6464,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1019e-04,  2.5802e-04,  2.8455e-04,  ..., -1.2371e-04,
         -9.9871e-05, -5.2126e-04],
        [-3.7909e-05, -3.0383e-05,  2.8200e-06,  ..., -3.2604e-05,
         -9.0227e-06, -5.3085e-06],
        [-2.7031e-05, -2.1666e-05,  1.9977e-06,  ..., -2.3246e-05,
         -6.4448e-06, -3.7905e-06],
        [-3.5882e-05, -2.8774e-05,  2.7567e-06,  ..., -3.0905e-05,
         -8.5533e-06, -4.9919e-06],
        [-6.9976e-05, -5.6028e-05,  5.0105e-06,  ..., -6.0290e-05,
         -1.6779e-05, -9.5889e-06]], device='cuda:0')
Loss: 1.2039330005645752
Graident accumulation at epoch 0, step 71, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6470e-04, -2.5265e-04,  1.1215e-04,  ...,  3.4514e-04,
         -7.2714e-04, -2.5916e-04],
        [-1.6556e-05, -1.8086e-05, -3.3781e-07,  ..., -1.2526e-05,
         -4.0442e-06, -2.5514e-06],
        [-1.6445e-05, -9.8649e-06, -1.4425e-06,  ..., -1.2134e-06,
         -4.8428e-06, -3.0624e-06],
        [ 1.4290e-06,  2.6481e-05,  2.0395e-06,  ...,  1.1570e-05,
         -2.3159e-06,  1.2918e-05],
        [-2.9290e-05, -3.0883e-05, -1.4513e-06,  ..., -2.2111e-05,
         -6.7819e-06, -3.4131e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8294e-08, 1.4711e-08, 1.7534e-08,  ..., 2.0106e-08, 3.2055e-08,
         6.1568e-09],
        [7.4471e-12, 8.8106e-12, 1.6037e-13,  ..., 4.1409e-12, 4.1580e-13,
         2.0619e-13],
        [6.8465e-12, 2.2543e-11, 1.4689e-12,  ..., 1.9884e-11, 2.9897e-12,
         8.9256e-13],
        [6.5450e-11, 2.1319e-10, 3.5204e-12,  ..., 5.2767e-11, 5.8211e-12,
         2.7925e-11],
        [2.3505e-11, 2.5736e-11, 4.0543e-13,  ..., 1.3399e-11, 1.1724e-12,
         3.2732e-13]], device='cuda:0')
optimizer state dict: 9.0
lr: [1.99215431440706e-05, 1.99215431440706e-05]
scheduler_last_epoch: 9


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4482e-05, -3.1337e-05, -1.9321e-05,  ...,  7.1632e-05,
         -6.8256e-05,  4.2653e-05],
        [-4.3213e-06, -3.0845e-06,  8.3447e-07,  ..., -3.9339e-06,
         -7.6741e-07, -8.1956e-07],
        [-2.9951e-06, -2.1458e-06,  5.8115e-07,  ..., -2.7418e-06,
         -5.3644e-07, -5.6997e-07],
        [-4.6492e-06, -3.3230e-06,  9.1270e-07,  ..., -4.2319e-06,
         -8.3074e-07, -8.8289e-07],
        [-8.1062e-06, -5.7817e-06,  1.5497e-06,  ..., -7.3612e-06,
         -1.4603e-06, -1.5199e-06]], device='cuda:0')
Loss: 1.2189403772354126


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[    0,  3164,    12,  ...,   984,   344,  3993],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0520e-04,  2.1538e-05, -1.4913e-04,  ...,  7.8515e-05,
         -6.7484e-05,  7.0918e-05],
        [-8.6129e-06, -6.1989e-06,  1.6764e-06,  ..., -7.7784e-06,
         -1.5907e-06, -1.8924e-06],
        [-6.0201e-06, -4.3362e-06,  1.1735e-06,  ..., -5.4389e-06,
         -1.1139e-06, -1.3262e-06],
        [-9.5367e-06, -6.8545e-06,  1.8813e-06,  ..., -8.6129e-06,
         -1.7658e-06, -2.0973e-06],
        [-1.6153e-05, -1.1593e-05,  3.1143e-06,  ..., -1.4573e-05,
         -3.0100e-06, -3.5167e-06]], device='cuda:0')
Loss: 1.2203258275985718


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   822,  5085,  ...,   293,  1608,   391],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3143e-04,  4.9917e-05, -1.3167e-04,  ..., -3.8641e-05,
         -2.7434e-05,  1.0823e-04],
        [-1.2964e-05, -9.2238e-06,  2.3954e-06,  ..., -1.1623e-05,
         -2.4103e-06, -2.7716e-06],
        [-9.0748e-06, -6.4820e-06,  1.6764e-06,  ..., -8.1658e-06,
         -1.6950e-06, -1.9483e-06],
        [-1.4335e-05, -1.0192e-05,  2.6785e-06,  ..., -1.2875e-05,
         -2.6710e-06, -3.0659e-06],
        [-2.4259e-05, -1.7256e-05,  4.4331e-06,  ..., -2.1785e-05,
         -4.5598e-06, -5.1335e-06]], device='cuda:0')
Loss: 1.2218562364578247


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4732e-05,  6.8496e-05, -1.4204e-04,  ...,  3.2402e-05,
          7.8660e-05,  2.4640e-04],
        [-1.7405e-05, -1.2442e-05,  3.1553e-06,  ..., -1.5765e-05,
         -3.3081e-06, -3.7476e-06],
        [-1.2055e-05, -8.6576e-06,  2.1867e-06,  ..., -1.0952e-05,
         -2.2948e-06, -2.6040e-06],
        [-1.8984e-05, -1.3575e-05,  3.4831e-06,  ..., -1.7226e-05,
         -3.6135e-06, -4.0866e-06],
        [-3.2425e-05, -2.3216e-05,  5.8189e-06,  ..., -2.9415e-05,
         -6.2212e-06, -6.9216e-06]], device='cuda:0')
Loss: 1.2296196222305298


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0,  7111,   409,  ...,  1908,  1260,   883],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1538e-04,  4.1390e-05, -9.8598e-05,  ..., -1.1657e-05,
          9.2689e-05,  3.0100e-04],
        [-2.1756e-05, -1.5512e-05,  3.8445e-06,  ..., -1.9729e-05,
         -4.0680e-06, -4.5672e-06],
        [-1.5110e-05, -1.0818e-05,  2.6748e-06,  ..., -1.3724e-05,
         -2.8275e-06, -3.1814e-06],
        [-2.3663e-05, -1.6868e-05,  4.2357e-06,  ..., -2.1458e-05,
         -4.4294e-06, -4.9584e-06],
        [ 4.4098e-05,  1.5302e-05, -4.4298e-05,  ...,  6.8559e-05,
          7.1302e-06,  2.9379e-05]], device='cuda:0')
Loss: 1.2374745607376099


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[   0,   12,  358,  ...,  352,  266,  319],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9908e-05,  9.7085e-05, -1.5056e-04,  ...,  5.3934e-05,
          5.5374e-05,  3.2771e-04],
        [-2.6107e-05, -1.8686e-05,  4.6454e-06,  ..., -2.3782e-05,
         -4.8876e-06, -5.4762e-06],
        [-1.8135e-05, -1.3024e-05,  3.2298e-06,  ..., -1.6540e-05,
         -3.3975e-06, -3.8110e-06],
        [-2.8282e-05, -2.0236e-05,  5.0925e-06,  ..., -2.5779e-05,
         -5.2936e-06, -5.9120e-06],
        [ 3.6052e-05,  9.4313e-06, -4.2837e-05,  ...,  6.1079e-05,
          5.6028e-06,  2.7732e-05]], device='cuda:0')
Loss: 1.2275359630584717


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[    0,  1549,   824,  ...,  3609,   720,   417],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8699e-05,  1.4412e-04, -2.6222e-04,  ...,  5.3745e-05,
          1.1501e-05,  4.3445e-04],
        [-3.0339e-05, -2.1771e-05,  5.4352e-06,  ..., -2.7627e-05,
         -5.7109e-06, -6.4671e-06],
        [-2.1145e-05, -1.5214e-05,  3.7923e-06,  ..., -1.9252e-05,
         -3.9786e-06, -4.5113e-06],
        [-3.2961e-05, -2.3663e-05,  5.9791e-06,  ..., -3.0011e-05,
         -6.2026e-06, -6.9998e-06],
        [ 2.8244e-05,  3.7688e-06, -4.1399e-05,  ...,  5.4045e-05,
          4.0829e-06,  2.5944e-05]], device='cuda:0')
Loss: 1.2217402458190918


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   292, 21215,  ...,   266,   818,  1527],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1100e-05,  1.2767e-04, -2.2508e-04,  ...,  1.5985e-04,
         -5.9499e-05,  5.3718e-04],
        [-3.4750e-05, -2.4915e-05,  6.1207e-06,  ..., -3.1561e-05,
         -6.5304e-06, -7.5027e-06],
        [-2.4199e-05, -1.7390e-05,  4.2673e-06,  ..., -2.1964e-05,
         -4.5486e-06, -5.2266e-06],
        [-3.7730e-05, -2.7046e-05,  6.7316e-06,  ..., -3.4213e-05,
         -7.0818e-06, -8.1100e-06],
        [ 2.0018e-05, -2.0724e-06, -4.0125e-05,  ...,  4.6744e-05,
          2.5481e-06,  2.4037e-05]], device='cuda:0')
Loss: 1.2174878120422363
Graident accumulation at epoch 0, step 79, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0294, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0339],
        [ 0.0333, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.8634e-04, -2.1462e-04,  7.8427e-05,  ...,  3.2661e-04,
         -6.6038e-04, -1.7952e-04],
        [-1.8375e-05, -1.8769e-05,  3.0804e-07,  ..., -1.4430e-05,
         -4.2928e-06, -3.0465e-06],
        [-1.7221e-05, -1.0617e-05, -8.7156e-07,  ..., -3.2885e-06,
         -4.8134e-06, -3.2788e-06],
        [-2.4869e-06,  2.1129e-05,  2.5087e-06,  ...,  6.9918e-06,
         -2.7925e-06,  1.0815e-05],
        [-2.4360e-05, -2.8002e-05, -5.3187e-06,  ..., -1.5225e-05,
         -5.8489e-06, -6.6813e-07]], device='cuda:0')
optimizer state dict: tensor([[4.8252e-08, 1.4713e-08, 1.7568e-08,  ..., 2.0111e-08, 3.2026e-08,
         6.4392e-09],
        [8.6472e-12, 9.4226e-12, 1.9767e-13,  ..., 5.1329e-12, 4.5803e-13,
         2.6228e-13],
        [7.4253e-12, 2.2823e-11, 1.4857e-12,  ..., 2.0346e-11, 3.0074e-12,
         9.1898e-13],
        [6.6808e-11, 2.1371e-10, 3.5622e-12,  ..., 5.3885e-11, 5.8655e-12,
         2.7962e-11],
        [2.3882e-11, 2.5715e-11, 2.0151e-12,  ..., 1.5570e-11, 1.1777e-12,
         9.0475e-13]], device='cuda:0')
optimizer state dict: 10.0
lr: [1.9887605295338853e-05, 1.9887605295338853e-05]
scheduler_last_epoch: 10


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0, 5007, 7551,  ...,    9, 2095,  300],
        [   0,  271,  259,  ..., 1345,  352,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0288e-05,  9.3025e-05,  8.7428e-06,  ...,  2.1622e-05,
          1.6810e-05, -1.9567e-05],
        [-4.1425e-06, -2.7120e-06,  1.1101e-06,  ..., -3.8743e-06,
         -5.7742e-07, -1.0729e-06],
        [-2.7716e-06, -1.8179e-06,  7.4133e-07,  ..., -2.5928e-06,
         -3.8929e-07, -7.1526e-07],
        [-4.7684e-06, -3.1143e-06,  1.2815e-06,  ..., -4.4405e-06,
         -6.6683e-07, -1.2293e-06],
        [-7.6890e-06, -5.0068e-06,  2.0266e-06,  ..., -7.1526e-06,
         -1.0729e-06, -1.9670e-06]], device='cuda:0')
Loss: 1.2057009935379028


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[   0, 4337, 2057,  ..., 3020, 1722,  369],
        [   0,   15,   19,  ...,   12,  287, 7897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6287e-04,  1.0981e-04, -1.5116e-04,  ..., -4.3564e-05,
          1.1489e-04,  4.6850e-05],
        [-8.1062e-06, -5.3048e-06,  2.1532e-06,  ..., -7.6294e-06,
         -1.1809e-06, -2.1383e-06],
        [-5.4836e-06, -3.5986e-06,  1.4566e-06,  ..., -5.1558e-06,
         -8.0466e-07, -1.4454e-06],
        [-9.5069e-06, -6.2138e-06,  2.5481e-06,  ..., -8.9407e-06,
         -1.3895e-06, -2.5034e-06],
        [-1.5110e-05, -9.8646e-06,  3.9786e-06,  ..., -1.4156e-05,
         -2.2054e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.2232969999313354


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[   0,   12, 3518,  ..., 1580, 2573,  409],
        [   0,  266, 1784,  ..., 1119, 1276,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1281e-05,  1.8867e-04, -1.3537e-04,  ..., -4.4593e-05,
          2.0050e-05,  6.8181e-05],
        [-1.2159e-05, -7.8976e-06,  3.2187e-06,  ..., -1.1399e-05,
         -1.7993e-06, -3.3006e-06],
        [-8.2254e-06, -5.3570e-06,  2.1756e-06,  ..., -7.7039e-06,
         -1.2219e-06, -2.2314e-06],
        [-1.4305e-05, -9.2834e-06,  3.8147e-06,  ..., -1.3381e-05,
         -2.1197e-06, -3.8743e-06],
        [-2.2739e-05, -1.4752e-05,  5.9754e-06,  ..., -2.1249e-05,
         -3.3826e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.197189450263977


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3865e-04,  1.9317e-04, -8.0002e-05,  ..., -7.6833e-05,
         -1.8289e-05,  1.6337e-04],
        [-1.6242e-05, -1.0535e-05,  4.2394e-06,  ..., -1.5169e-05,
         -2.3097e-06, -4.4182e-06],
        [-1.1027e-05, -7.1749e-06,  2.8759e-06,  ..., -1.0297e-05,
         -1.5739e-06, -2.9951e-06],
        [-1.9282e-05, -1.2517e-05,  5.0738e-06,  ..., -1.7971e-05,
         -2.7418e-06, -5.2303e-06],
        [-3.0369e-05, -1.9699e-05,  7.8827e-06,  ..., -2.8312e-05,
         -4.3511e-06, -8.1807e-06]], device='cuda:0')
Loss: 1.2087726593017578


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8081e-04,  2.8784e-04, -2.3229e-05,  ..., -1.0320e-04,
          5.3230e-05,  1.7533e-04],
        [-2.0355e-05, -1.3247e-05,  5.2527e-06,  ..., -1.9103e-05,
         -2.9318e-06, -5.4762e-06],
        [-1.3798e-05, -9.0003e-06,  3.5539e-06,  ..., -1.2949e-05,
         -1.9912e-06, -3.7067e-06],
        [-2.4110e-05, -1.5706e-05,  6.2659e-06,  ..., -2.2590e-05,
         -3.4645e-06, -6.4671e-06],
        [-3.8117e-05, -2.4825e-05,  9.7677e-06,  ..., -3.5733e-05,
         -5.5283e-06, -1.0163e-05]], device='cuda:0')
Loss: 1.228296160697937


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2623e-05,  2.6804e-04, -6.0730e-05,  ..., -1.5294e-04,
          1.5450e-04,  3.9654e-05],
        [-2.4438e-05, -1.5810e-05,  6.3479e-06,  ..., -2.2843e-05,
         -3.4310e-06, -6.6310e-06],
        [-1.6615e-05, -1.0766e-05,  4.3064e-06,  ..., -1.5512e-05,
         -2.3339e-06, -4.5002e-06],
        [-2.9057e-05, -1.8805e-05,  7.6070e-06,  ..., -2.7120e-05,
         -4.0643e-06, -7.8678e-06],
        [-4.5806e-05, -2.9624e-05,  1.1809e-05,  ..., -4.2737e-05,
         -6.4671e-06, -1.2308e-05]], device='cuda:0')
Loss: 1.2027270793914795


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7134e-05,  2.4907e-04,  1.2151e-05,  ..., -2.2545e-04,
          3.2390e-04, -1.9075e-05],
        [-2.8521e-05, -1.8477e-05,  7.4059e-06,  ..., -2.6658e-05,
         -3.8929e-06, -7.7337e-06],
        [-1.9372e-05, -1.2577e-05,  5.0254e-06,  ..., -1.8090e-05,
         -2.6468e-06, -5.2489e-06],
        [-3.3915e-05, -2.1994e-05,  8.8736e-06,  ..., -3.1650e-05,
         -4.6156e-06, -9.1791e-06],
        [-5.3406e-05, -3.4600e-05,  1.3776e-05,  ..., -4.9829e-05,
         -7.3351e-06, -1.4350e-05]], device='cuda:0')
Loss: 1.2174900770187378


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0, 16064, 10937,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9555e-06,  1.5614e-04, -5.0950e-05,  ..., -1.7573e-04,
          3.4803e-04,  1.5500e-05],
        [-3.2574e-05, -2.1145e-05,  8.4490e-06,  ..., -3.0398e-05,
         -4.4554e-06, -8.8066e-06],
        [-2.2158e-05, -1.4409e-05,  5.7369e-06,  ..., -2.0653e-05,
         -3.0324e-06, -5.9865e-06],
        [-3.8773e-05, -2.5183e-05,  1.0133e-05,  ..., -3.6120e-05,
         -5.2862e-06, -1.0461e-05],
        [-6.1035e-05, -3.9607e-05,  1.5713e-05,  ..., -5.6833e-05,
         -8.3931e-06, -1.6347e-05]], device='cuda:0')
Loss: 1.2220278978347778
Graident accumulation at epoch 0, step 87, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0276, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0860e-04, -1.7754e-04,  6.5489e-05,  ...,  2.7638e-04,
         -5.5953e-04, -1.6002e-04],
        [-1.9795e-05, -1.9007e-05,  1.1221e-06,  ..., -1.6026e-05,
         -4.3091e-06, -3.6225e-06],
        [-1.7714e-05, -1.0997e-05, -2.1071e-07,  ..., -5.0250e-06,
         -4.6353e-06, -3.5496e-06],
        [-6.1155e-06,  1.6497e-05,  3.2711e-06,  ...,  2.6806e-06,
         -3.0419e-06,  8.6873e-06],
        [-2.8027e-05, -2.9163e-05, -3.2155e-06,  ..., -1.9386e-05,
         -6.1034e-06, -2.2360e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8204e-08, 1.4722e-08, 1.7553e-08,  ..., 2.0122e-08, 3.2115e-08,
         6.4330e-09],
        [9.6996e-12, 9.8602e-12, 2.6886e-13,  ..., 6.0518e-12, 4.7742e-13,
         3.3957e-13],
        [7.9088e-12, 2.3008e-11, 1.5171e-12,  ..., 2.0753e-11, 3.0136e-12,
         9.5390e-13],
        [6.8245e-11, 2.1413e-10, 3.6613e-12,  ..., 5.5136e-11, 5.8876e-12,
         2.8044e-11],
        [2.7583e-11, 2.7258e-11, 2.2600e-12,  ..., 1.8785e-11, 1.2470e-12,
         1.1711e-12]], device='cuda:0')
optimizer state dict: 11.0
lr: [1.9847624027890693e-05, 1.9847624027890693e-05]
scheduler_last_epoch: 11


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[   0, 2555,  984,  ..., 5900, 1576,  271],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6389e-09,  4.4544e-05,  9.7376e-05,  ..., -4.3853e-06,
          4.2800e-05,  5.0492e-05],
        [-4.1127e-06, -2.5630e-06,  1.2368e-06,  ..., -3.8147e-06,
         -3.0920e-07, -1.3486e-06],
        [-2.6822e-06, -1.6764e-06,  8.0839e-07,  ..., -2.4885e-06,
         -2.0117e-07, -8.7917e-07],
        [-5.0962e-06, -3.1739e-06,  1.5423e-06,  ..., -4.7088e-06,
         -3.8370e-07, -1.6689e-06],
        [-7.4804e-06, -4.6492e-06,  2.2352e-06,  ..., -6.9141e-06,
         -5.6252e-07, -2.4438e-06]], device='cuda:0')
Loss: 1.2103558778762817


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0, 18905,  2311,  ..., 10213,   908,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3190e-05,  4.7199e-05,  1.3566e-04,  ..., -4.7735e-05,
          1.8514e-05,  8.2314e-05],
        [-8.1360e-06, -5.0813e-06,  2.4289e-06,  ..., -7.6592e-06,
         -5.5134e-07, -2.6450e-06],
        [-5.3793e-06, -3.3677e-06,  1.6093e-06,  ..., -5.0813e-06,
         -3.6322e-07, -1.7472e-06],
        [-1.0073e-05, -6.2883e-06,  3.0324e-06,  ..., -9.4771e-06,
         -6.8173e-07, -3.2634e-06],
        [-1.5080e-05, -9.3877e-06,  4.4703e-06,  ..., -1.4156e-05,
         -1.0245e-06, -4.8578e-06]], device='cuda:0')
Loss: 1.2200783491134644


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2738e-05,  5.4579e-05,  1.4733e-04,  ..., -9.8632e-05,
          3.8827e-07,  5.1357e-05],
        [-1.2219e-05, -7.5996e-06,  3.6582e-06,  ..., -1.1563e-05,
         -7.8045e-07, -4.0531e-06],
        [-8.0168e-06, -4.9993e-06,  2.4028e-06,  ..., -7.6145e-06,
         -5.1036e-07, -2.6561e-06],
        [-1.5110e-05, -9.3877e-06,  4.5598e-06,  ..., -1.4305e-05,
         -9.6299e-07, -4.9919e-06],
        [-2.2560e-05, -1.4007e-05,  6.7204e-06,  ..., -2.1338e-05,
         -1.4454e-06, -7.4357e-06]], device='cuda:0')
Loss: 1.2094837427139282


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,  1083,   287,  ...,    12,   287,  2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8187e-05,  4.4432e-05,  1.4242e-04,  ..., -1.9899e-04,
          5.2004e-05,  2.4667e-04],
        [-1.6361e-05, -1.0237e-05,  4.8727e-06,  ..., -1.5497e-05,
         -1.1176e-06, -5.4315e-06],
        [-1.0714e-05, -6.7204e-06,  3.1926e-06,  ..., -1.0177e-05,
         -7.3109e-07, -3.5539e-06],
        [-2.0236e-05, -1.2651e-05,  6.0722e-06,  ..., -1.9163e-05,
         -1.3784e-06, -6.6981e-06],
        [-3.0160e-05, -1.8835e-05,  8.9407e-06,  ..., -2.8551e-05,
         -2.0675e-06, -9.9540e-06]], device='cuda:0')
Loss: 1.2040921449661255


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[    0,  1265,  1545,  ...,   292, 36667, 36197],
        [    0,  6538,  1805,  ...,   298,   271,   721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9583e-05,  1.3290e-04,  1.2166e-04,  ..., -2.3680e-04,
          5.4962e-05,  2.5279e-04],
        [-2.0474e-05, -1.2800e-05,  5.9977e-06,  ..., -1.9461e-05,
         -1.4212e-06, -6.8173e-06],
        [-1.3411e-05, -8.3968e-06,  3.9265e-06,  ..., -1.2770e-05,
         -9.2853e-07, -4.4592e-06],
        [-2.5213e-05, -1.5736e-05,  7.4357e-06,  ..., -2.3931e-05,
         -1.7378e-06, -8.3521e-06],
        [-3.7730e-05, -2.3514e-05,  1.0997e-05,  ..., -3.5822e-05,
         -2.6301e-06, -1.2472e-05]], device='cuda:0')
Loss: 1.2317346334457397


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0,   634,  1621,  ...,   688,   586,  8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4713e-05,  1.4476e-04,  1.2788e-04,  ..., -2.0881e-04,
          5.4962e-05,  3.0821e-04],
        [-2.4498e-05, -1.5303e-05,  7.2122e-06,  ..., -2.3276e-05,
         -1.7360e-06, -8.1733e-06],
        [-1.6063e-05, -1.0043e-05,  4.7274e-06,  ..., -1.5289e-05,
         -1.1353e-06, -5.3532e-06],
        [-3.0130e-05, -1.8790e-05,  8.9258e-06,  ..., -2.8610e-05,
         -2.1197e-06, -1.0006e-05],
        [-4.5061e-05, -2.8044e-05,  1.3188e-05,  ..., -4.2737e-05,
         -3.2037e-06, -1.4916e-05]], device='cuda:0')
Loss: 1.227596640586853


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,   259,  1380,  ...,   287, 10221,   280],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8763e-05,  1.1199e-04,  1.2357e-04,  ..., -2.9735e-04,
          4.7779e-05,  2.6915e-04],
        [-2.8521e-05, -1.7881e-05,  8.3372e-06,  ..., -2.7150e-05,
         -2.0042e-06, -9.4771e-06],
        [-1.8716e-05, -1.1750e-05,  5.4725e-06,  ..., -1.7837e-05,
         -1.3132e-06, -6.2175e-06],
        [-3.5077e-05, -2.1949e-05,  1.0327e-05,  ..., -3.3379e-05,
         -2.4512e-06, -1.1608e-05],
        [-5.2422e-05, -3.2753e-05,  1.5244e-05,  ..., -4.9800e-05,
         -3.7029e-06, -1.7300e-05]], device='cuda:0')
Loss: 1.2121012210845947


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[   0,  759, 1184,  ...,  472,  346,   14],
        [   0,  560,  199,  ..., 6408,  278, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2764e-04,  1.3377e-04,  1.4458e-04,  ..., -2.6120e-04,
         -4.7879e-05,  2.2297e-04],
        [-3.2544e-05, -2.0310e-05,  9.6187e-06,  ..., -3.0950e-05,
         -2.3115e-06, -1.0900e-05],
        [-2.1398e-05, -1.3366e-05,  6.3255e-06,  ..., -2.0370e-05,
         -1.5190e-06, -7.1600e-06],
        [-4.0203e-05, -2.5019e-05,  1.1966e-05,  ..., -3.8207e-05,
         -2.8405e-06, -1.3396e-05],
        [-5.9903e-05, -3.7223e-05,  1.7613e-05,  ..., -5.6833e-05,
         -4.2841e-06, -1.9908e-05]], device='cuda:0')
Loss: 1.20297372341156
Graident accumulation at epoch 0, step 95, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0171,  0.0141, -0.0267,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5050e-04, -1.4641e-04,  7.3399e-05,  ...,  2.2262e-04,
         -5.0837e-04, -1.2172e-04],
        [-2.1070e-05, -1.9137e-05,  1.9718e-06,  ..., -1.7519e-05,
         -4.1093e-06, -4.3503e-06],
        [-1.8083e-05, -1.1234e-05,  4.4292e-07,  ..., -6.5595e-06,
         -4.3237e-06, -3.9106e-06],
        [-9.5243e-06,  1.2346e-05,  4.1405e-06,  ..., -1.4081e-06,
         -3.0217e-06,  6.4789e-06],
        [-3.1215e-05, -2.9969e-05, -1.1326e-06,  ..., -2.3131e-05,
         -5.9214e-06, -4.0032e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8172e-08, 1.4726e-08, 1.7556e-08,  ..., 2.0170e-08, 3.2086e-08,
         6.4763e-09],
        [1.0749e-11, 1.0263e-11, 3.6111e-13,  ..., 7.0036e-12, 4.8229e-13,
         4.5805e-13],
        [8.3588e-12, 2.3163e-11, 1.5556e-12,  ..., 2.1147e-11, 3.0129e-12,
         1.0042e-12],
        [6.9793e-11, 2.1454e-10, 3.8008e-12,  ..., 5.6541e-11, 5.8897e-12,
         2.8195e-11],
        [3.1144e-11, 2.8616e-11, 2.5679e-12,  ..., 2.1996e-11, 1.2641e-12,
         1.5662e-12]], device='cuda:0')
optimizer state dict: 12.0
lr: [1.9801623778739208e-05, 1.9801623778739208e-05]
scheduler_last_epoch: 12


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[   0, 3119,  278,  ...,  352,  674,  369],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8224e-05, -4.3196e-05,  8.9159e-05,  ..., -8.9203e-05,
         -2.8848e-05,  4.9396e-06],
        [-4.0531e-06, -2.5183e-06,  1.3709e-06,  ..., -3.9041e-06,
         -5.5414e-08, -1.6615e-06],
        [ 1.4773e-04,  1.3076e-04, -3.8979e-05,  ...,  2.1269e-04,
         -2.5851e-05,  1.1925e-04],
        [-5.0664e-06, -3.1441e-06,  1.7211e-06,  ..., -4.8578e-06,
         -6.7055e-08, -2.0713e-06],
        [-7.3612e-06, -4.5598e-06,  2.4736e-06,  ..., -7.0632e-06,
         -1.0384e-07, -2.9951e-06]], device='cuda:0')
Loss: 1.2331515550613403


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1128e-05, -3.2176e-05,  7.4175e-05,  ..., -8.1211e-05,
         -6.1812e-05, -2.2960e-05],
        [-8.0168e-06, -4.9323e-06,  2.7120e-06,  ..., -7.6145e-06,
         -8.7544e-08, -3.3230e-06],
        [ 1.4513e-04,  1.2917e-04, -3.8100e-05,  ...,  2.1026e-04,
         -2.5871e-05,  1.1816e-04],
        [-1.0103e-05, -6.1989e-06,  3.4273e-06,  ..., -9.5367e-06,
         -1.0710e-07, -4.1574e-06],
        [-1.4812e-05, -9.0599e-06,  4.9621e-06,  ..., -1.3977e-05,
         -1.6810e-07, -6.0946e-06]], device='cuda:0')
Loss: 1.212284803390503


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5689e-04, -7.1925e-05,  6.0942e-06,  ..., -3.7749e-06,
         -4.2536e-05,  5.6469e-05],
        [-1.2219e-05, -7.4506e-06,  4.0457e-06,  ..., -1.1548e-05,
         -2.3842e-07, -4.9919e-06],
        [ 1.4243e-04,  1.2755e-04, -3.7243e-05,  ...,  2.0772e-04,
         -2.5969e-05,  1.1708e-04],
        [-1.5169e-05, -9.2387e-06,  5.0440e-06,  ..., -1.4275e-05,
         -2.8498e-07, -6.1691e-06],
        [-2.2501e-05, -1.3649e-05,  7.3910e-06,  ..., -2.1160e-05,
         -4.4564e-07, -9.1344e-06]], device='cuda:0')
Loss: 1.226502776145935


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,   590,    16,  ...,    13,    35,  1151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9635e-05, -1.1614e-04,  2.9537e-05,  ..., -1.0924e-06,
         -4.0349e-05,  5.8634e-05],
        [-1.6421e-05, -9.9242e-06,  5.4091e-06,  ..., -1.5512e-05,
         -3.1339e-07, -6.7353e-06],
        [ 1.3979e-04,  1.2599e-04, -3.6383e-05,  ...,  2.0522e-04,
         -2.6015e-05,  1.1598e-04],
        [-2.0355e-05, -1.2308e-05,  6.7353e-06,  ..., -1.9163e-05,
         -3.7206e-07, -8.3148e-06],
        [-3.0071e-05, -1.8120e-05,  9.8497e-06,  ..., -2.8312e-05,
         -5.8068e-07, -1.2264e-05]], device='cuda:0')
Loss: 1.2263405323028564


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   17,  271,  266],
        [   0, 6112,  278,  ..., 4092,  490, 2774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4585e-04, -1.5542e-04,  9.0014e-06,  ...,  3.5034e-05,
         -9.9486e-05,  1.1134e-04],
        [-2.0534e-05, -1.2398e-05,  6.8098e-06,  ..., -1.9357e-05,
         -3.7812e-07, -8.3372e-06],
        [ 2.0551e-04,  1.8139e-04, -7.2079e-05,  ...,  3.0631e-04,
         -8.5386e-06,  1.7788e-04],
        [-2.5511e-05, -1.5423e-05,  8.5011e-06,  ..., -2.3991e-05,
         -4.4797e-07, -1.0327e-05],
        [-3.7611e-05, -2.2650e-05,  1.2413e-05,  ..., -3.5375e-05,
         -6.9616e-07, -1.5199e-05]], device='cuda:0')
Loss: 1.1983712911605835


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[   0, 1064, 1042,  ...,   12,  259, 4754],
        [   0,  278,  565,  ..., 1125, 5222,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0977e-04, -1.7700e-04,  2.4574e-06,  ...,  5.9165e-05,
         -4.9890e-05,  1.9309e-04],
        [-2.4736e-05, -1.4871e-05,  8.1360e-06,  ..., -2.3156e-05,
         -4.5961e-07, -9.9838e-06],
        [ 2.0279e-04,  1.7978e-04, -7.1215e-05,  ...,  3.0383e-04,
         -8.5912e-06,  1.7681e-04],
        [-3.0667e-05, -1.8463e-05,  1.0140e-05,  ..., -2.8670e-05,
         -5.4715e-07, -1.2353e-05],
        [-4.5359e-05, -2.7210e-05,  1.4856e-05,  ..., -4.2379e-05,
         -8.5821e-07, -1.8224e-05]], device='cuda:0')
Loss: 1.2152860164642334


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,     9,   300,  ...,  6838,   328, 18619],
        [    0,  1934,  2413,  ..., 19697,    13, 16325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8351e-04, -1.7669e-04, -3.7789e-05,  ...,  1.1373e-04,
          6.6684e-06,  2.2431e-04],
        [-2.8878e-05, -1.7330e-05,  9.4399e-06,  ..., -2.7061e-05,
         -5.5647e-07, -1.1653e-05],
        [ 2.0015e-04,  1.7821e-04, -7.0388e-05,  ...,  3.0134e-04,
         -8.6541e-06,  1.7574e-04],
        [-3.5763e-05, -2.1487e-05,  1.1757e-05,  ..., -3.3498e-05,
         -6.6636e-07, -1.4409e-05],
        [-5.2869e-05, -3.1650e-05,  1.7211e-05,  ..., -4.9472e-05,
         -1.0426e-06, -2.1249e-05]], device='cuda:0')
Loss: 1.199967622756958


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0,  4485,   741,  ...,   292,   221,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3121e-04, -2.5398e-04, -8.5733e-06,  ...,  2.8562e-05,
          2.4776e-05,  2.7812e-04],
        [-3.3021e-05, -1.9848e-05,  1.0781e-05,  ..., -3.0994e-05,
         -6.1351e-07, -1.3344e-05],
        [ 1.9750e-04,  1.7660e-04, -6.9523e-05,  ...,  2.9883e-04,
         -8.6897e-06,  1.7466e-04],
        [-4.0889e-05, -2.4617e-05,  1.3433e-05,  ..., -3.8356e-05,
         -7.3249e-07, -1.6496e-05],
        [-6.0409e-05, -3.6240e-05,  1.9640e-05,  ..., -5.6595e-05,
         -1.1493e-06, -2.4304e-05]], device='cuda:0')
Loss: 1.1989336013793945
Graident accumulation at epoch 0, step 103, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0034,  0.0221, -0.0207],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0141, -0.0267,  ...,  0.0277, -0.0160, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.0857e-04, -1.5717e-04,  6.5202e-05,  ...,  2.0321e-04,
         -4.5505e-04, -8.1737e-05],
        [-2.2265e-05, -1.9208e-05,  2.8527e-06,  ..., -1.8866e-05,
         -3.7597e-06, -5.2496e-06],
        [ 3.4753e-06,  7.5496e-06, -6.5537e-06,  ...,  2.3979e-05,
         -4.7603e-06,  1.3947e-05],
        [-1.2661e-05,  8.6496e-06,  5.0698e-06,  ..., -5.1029e-06,
         -2.7928e-06,  4.1815e-06],
        [-3.4134e-05, -3.0596e-05,  9.4463e-07,  ..., -2.6477e-05,
         -5.4442e-06, -6.0332e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8178e-08, 1.4775e-08, 1.7539e-08,  ..., 2.0151e-08, 3.2054e-08,
         6.5472e-09],
        [1.1829e-11, 1.0647e-11, 4.7698e-13,  ..., 7.9573e-12, 4.8218e-13,
         6.3565e-13],
        [4.7356e-11, 5.4327e-11, 6.3875e-12,  ..., 1.1042e-10, 3.0854e-12,
         3.1511e-11],
        [7.1395e-11, 2.1493e-10, 3.9774e-12,  ..., 5.7955e-11, 5.8844e-12,
         2.8439e-11],
        [3.4762e-11, 2.9901e-11, 2.9511e-12,  ..., 2.5177e-11, 1.2642e-12,
         2.1553e-12]], device='cuda:0')
optimizer state dict: 13.0
lr: [1.974963266376872e-05, 1.974963266376872e-05]
scheduler_last_epoch: 13


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3011e-05,  4.5296e-05,  9.4027e-06,  ..., -3.6708e-05,
         -4.2519e-05,  0.0000e+00],
        [-4.1425e-06, -2.4289e-06,  1.3709e-06,  ..., -3.8445e-06,
          6.6590e-08, -1.8477e-06],
        [-2.6822e-06, -1.5646e-06,  8.8289e-07,  ..., -2.4736e-06,
          4.1677e-08, -1.1921e-06],
        [-5.2154e-06, -3.0398e-06,  1.7285e-06,  ..., -4.8280e-06,
          8.7079e-08, -2.3246e-06],
        [-7.6890e-06, -4.4703e-06,  2.5183e-06,  ..., -7.0930e-06,
          1.1548e-07, -3.4124e-06]], device='cuda:0')
Loss: 1.2154868841171265


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1973e-05,  5.7657e-05,  2.1188e-05,  ..., -5.9500e-05,
          2.6106e-05,  2.3044e-05],
        [-8.4043e-06, -5.0068e-06,  2.7791e-06,  ..., -7.8082e-06,
          1.0524e-07, -3.7327e-06],
        [-5.3942e-06, -3.2112e-06,  1.7807e-06,  ..., -4.9919e-06,
          6.9384e-08, -2.3916e-06],
        [-1.0490e-05, -6.2436e-06,  3.4794e-06,  ..., -9.7156e-06,
          1.4179e-07, -4.6641e-06],
        [-1.5616e-05, -9.2685e-06,  5.1409e-06,  ..., -1.4454e-05,
          1.9278e-07, -6.8992e-06]], device='cuda:0')
Loss: 1.2115004062652588


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0,  527, 2811,  ...,  287, 1288,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9166e-05,  4.1751e-05,  7.7109e-05,  ..., -2.0006e-04,
          1.7233e-05,  1.0482e-05],
        [-1.2547e-05, -7.4506e-06,  4.0978e-06,  ..., -1.1653e-05,
          1.6950e-07, -5.5879e-06],
        [-8.0764e-06, -4.7833e-06,  2.6301e-06,  ..., -7.4655e-06,
          1.0990e-07, -3.5912e-06],
        [-1.5706e-05, -9.3132e-06,  5.1409e-06,  ..., -1.4514e-05,
          2.2515e-07, -7.0035e-06],
        [-2.3365e-05, -1.3828e-05,  7.5847e-06,  ..., -2.1577e-05,
          3.0641e-07, -1.0356e-05]], device='cuda:0')
Loss: 1.2058374881744385


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[   0,  300, 1064,  ..., 6953,  944,  278],
        [   0,   35, 3815,  ...,  278, 7097, 4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0914e-04, -2.5729e-06,  7.3865e-05,  ..., -1.5006e-04,
          2.2013e-05,  4.6512e-06],
        [-1.6779e-05, -9.8497e-06,  5.4985e-06,  ..., -1.5527e-05,
          2.3004e-07, -7.4655e-06],
        [-1.0818e-05, -6.3330e-06,  3.5390e-06,  ..., -9.9689e-06,
          1.4855e-07, -4.8056e-06],
        [-2.0862e-05, -1.2249e-05,  6.8694e-06,  ..., -1.9252e-05,
          3.0710e-07, -9.2983e-06],
        [-3.1173e-05, -1.8239e-05,  1.0163e-05,  ..., -2.8700e-05,
          4.1584e-07, -1.3798e-05]], device='cuda:0')
Loss: 1.2011170387268066


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4908e-05, -2.5440e-06,  1.1875e-04,  ..., -2.4247e-04,
          9.3075e-05,  1.7444e-05],
        [-2.0862e-05, -1.2279e-05,  6.8918e-06,  ..., -1.9401e-05,
          3.8836e-07, -9.2685e-06],
        [-1.3456e-05, -7.9051e-06,  4.4368e-06,  ..., -1.2472e-05,
          2.5099e-07, -5.9679e-06],
        [-2.5988e-05, -1.5303e-05,  8.6278e-06,  ..., -2.4110e-05,
          5.1013e-07, -1.1563e-05],
        [-3.8654e-05, -2.2680e-05,  1.2711e-05,  ..., -3.5793e-05,
          7.0268e-07, -1.7092e-05]], device='cuda:0')
Loss: 1.2209093570709229


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,  9017,   600,  ...,  6133,  1098,   352],
        [    0,   287,  6761,  ...,  1918, 33351,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7843e-05,  2.4681e-06,  1.7749e-04,  ..., -2.9667e-04,
          9.2690e-05,  7.5446e-05],
        [-2.5094e-05, -1.4782e-05,  8.2552e-06,  ..., -2.3305e-05,
          4.6054e-07, -1.1176e-05],
        [-1.6183e-05, -9.5218e-06,  5.3197e-06,  ..., -1.4991e-05,
          2.9826e-07, -7.2047e-06],
        [-3.1292e-05, -1.8463e-05,  1.0356e-05,  ..., -2.9027e-05,
          6.0513e-07, -1.3962e-05],
        [-4.6521e-05, -2.7329e-05,  1.5244e-05,  ..., -4.3064e-05,
          8.3959e-07, -2.0638e-05]], device='cuda:0')
Loss: 1.2136069536209106


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[    0,   445,     8,  ...,    13, 25386,    17],
        [    0,  1756,   271,  ...,   259, 48595, 19882]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0477e-05, -3.6386e-05,  1.6661e-04,  ..., -2.3264e-04,
          9.5346e-05,  4.3967e-05],
        [-2.9236e-05, -1.7166e-05,  9.6187e-06,  ..., -2.7120e-05,
          6.0862e-07, -1.3061e-05],
        [-1.8880e-05, -1.1079e-05,  6.2063e-06,  ..., -1.7479e-05,
          3.9418e-07, -8.4341e-06],
        [-3.6508e-05, -2.1473e-05,  1.2077e-05,  ..., -3.3826e-05,
          7.9046e-07, -1.6332e-05],
        [-5.4210e-05, -3.1769e-05,  1.7762e-05,  ..., -5.0128e-05,
          1.1059e-06, -2.4125e-05]], device='cuda:0')
Loss: 1.201424479484558


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,  3380,  1197,  ...,   631,   369,  3123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1005e-04, -6.8045e-05,  1.7635e-04,  ..., -1.5201e-04,
          9.6117e-05,  2.4331e-05],
        [-3.3498e-05, -1.9684e-05,  1.0982e-05,  ..., -3.0965e-05,
          7.2177e-07, -1.4909e-05],
        [-2.1636e-05, -1.2711e-05,  7.0930e-06,  ..., -1.9953e-05,
          4.6543e-07, -9.6262e-06],
        [-4.1753e-05, -2.4572e-05,  1.3769e-05,  ..., -3.8564e-05,
          9.2736e-07, -1.8597e-05],
        [-6.2078e-05, -3.6418e-05,  2.0280e-05,  ..., -5.7220e-05,
          1.3090e-06, -2.7522e-05]], device='cuda:0')
Loss: 1.2080137729644775
Graident accumulation at epoch 0, step 111, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0039,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5872e-04, -1.4825e-04,  7.6316e-05,  ...,  1.6769e-04,
         -3.9994e-04, -7.1131e-05],
        [-2.3388e-05, -1.9256e-05,  3.6657e-06,  ..., -2.0076e-05,
         -3.3116e-06, -6.2155e-06],
        [ 9.6408e-07,  5.5235e-06, -5.1890e-06,  ...,  1.9586e-05,
         -4.2377e-06,  1.1590e-05],
        [-1.5570e-05,  5.3274e-06,  5.9397e-06,  ..., -8.4490e-06,
         -2.4208e-06,  1.9037e-06],
        [-3.6929e-05, -3.1178e-05,  2.8782e-06,  ..., -2.9551e-05,
         -4.7689e-06, -8.1822e-06]], device='cuda:0')
optimizer state dict: tensor([[4.8141e-08, 1.4765e-08, 1.7552e-08,  ..., 2.0154e-08, 3.2031e-08,
         6.5412e-09],
        [1.2939e-11, 1.1023e-11, 5.9711e-13,  ..., 8.9081e-12, 4.8222e-13,
         8.5728e-13],
        [4.7776e-11, 5.4434e-11, 6.4315e-12,  ..., 1.1071e-10, 3.0825e-12,
         3.1572e-11],
        [7.3067e-11, 2.1532e-10, 4.1630e-12,  ..., 5.9385e-11, 5.8794e-12,
         2.8757e-11],
        [3.8581e-11, 3.1197e-11, 3.3594e-12,  ..., 2.8426e-11, 1.2646e-12,
         2.9106e-12]], device='cuda:0')
optimizer state dict: 14.0
lr: [1.9691682460550022e-05, 1.9691682460550022e-05]
scheduler_last_epoch: 14


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9940e-05,  9.7691e-06,  3.8137e-05,  ..., -5.0547e-05,
          0.0000e+00, -1.2541e-04],
        [-4.1723e-06, -2.3842e-06,  1.2219e-06,  ..., -3.6806e-06,
          2.2352e-07, -1.9372e-06],
        [-2.7120e-06, -1.5497e-06,  7.9349e-07,  ..., -2.3842e-06,
          1.4622e-07, -1.2591e-06],
        [-5.2750e-06, -3.0100e-06,  1.5497e-06,  ..., -4.6492e-06,
          2.8498e-07, -2.4438e-06],
        [-7.8678e-06, -4.4703e-06,  2.2948e-06,  ..., -6.8843e-06,
          4.1723e-07, -3.6359e-06]], device='cuda:0')
Loss: 1.1935677528381348


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[   0,  360,  259,  ...,   14,  381, 1371],
        [   0,  995,   13,  ..., 3494,  367, 6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9340e-05,  4.4106e-05,  1.6853e-04,  ..., -1.3488e-04,
         -6.9995e-06, -1.1296e-04],
        [-8.4639e-06, -4.7982e-06,  2.4363e-06,  ..., -7.3761e-06,
          4.5169e-07, -3.8147e-06],
        [-5.4985e-06, -3.1218e-06,  1.5870e-06,  ..., -4.7833e-06,
          2.9523e-07, -2.4810e-06],
        [-1.0639e-05, -6.0499e-06,  3.0845e-06,  ..., -9.2685e-06,
          5.7369e-07, -4.7982e-06],
        [-1.5974e-05, -9.0599e-06,  4.5896e-06,  ..., -1.3888e-05,
          8.4564e-07, -7.1824e-06]], device='cuda:0')
Loss: 1.2194212675094604


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7129e-05,  3.0555e-05,  2.1951e-04,  ..., -1.5253e-04,
         -5.2743e-06, -6.8827e-05],
        [-1.2666e-05, -7.2122e-06,  3.6433e-06,  ..., -1.1086e-05,
          6.0163e-07, -5.7369e-06],
        [-8.2552e-06, -4.7088e-06,  2.3805e-06,  ..., -7.2122e-06,
          3.9442e-07, -3.7402e-06],
        [-1.5825e-05, -9.0450e-06,  4.5821e-06,  ..., -1.3858e-05,
          7.6182e-07, -7.1675e-06],
        [-2.4080e-05, -1.3709e-05,  6.8992e-06,  ..., -2.1011e-05,
          1.1306e-06, -1.0863e-05]], device='cuda:0')
Loss: 1.1960779428482056


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0, 1732,  292,  ..., 3440, 4010, 1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1440e-05,  3.6137e-05,  1.8196e-04,  ..., -1.0307e-04,
          5.4148e-06, -6.1275e-05],
        [-1.6898e-05, -9.6709e-06,  4.9248e-06,  ..., -1.4842e-05,
          7.6368e-07, -7.6592e-06],
        [-1.0997e-05, -6.3032e-06,  3.2075e-06,  ..., -9.6411e-06,
          5.0012e-07, -4.9844e-06],
        [-2.1070e-05, -1.2100e-05,  6.1765e-06,  ..., -1.8507e-05,
          9.6578e-07, -9.5516e-06],
        [-3.2127e-05, -1.8358e-05,  9.3132e-06,  ..., -2.8104e-05,
          1.4342e-06, -1.4499e-05]], device='cuda:0')
Loss: 1.2113531827926636


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  1874,   300,  ...,    14,  5372,    12],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4555e-05,  3.6137e-05,  1.9356e-04,  ..., -1.1468e-04,
          4.6677e-06, -4.8570e-05],
        [-2.1160e-05, -1.2115e-05,  6.2138e-06,  ..., -1.8597e-05,
          9.2201e-07, -9.6112e-06],
        [-1.3754e-05, -7.8827e-06,  4.0419e-06,  ..., -1.2070e-05,
          6.0257e-07, -6.2510e-06],
        [-2.6345e-05, -1.5110e-05,  7.7784e-06,  ..., -2.3156e-05,
          1.1632e-06, -1.1966e-05],
        [-4.0114e-05, -2.2918e-05,  1.1712e-05,  ..., -3.5137e-05,
          1.7248e-06, -1.8150e-05]], device='cuda:0')
Loss: 1.1905664205551147


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,  1103,   271,  ...,   957,   756,   368],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9758e-05, -1.6967e-05,  1.9724e-04,  ..., -1.3082e-04,
         -4.8690e-05, -3.8977e-05],
        [-2.5362e-05, -1.4514e-05,  7.4506e-06,  ..., -2.2337e-05,
          1.1176e-06, -1.1578e-05],
        [-1.6496e-05, -9.4473e-06,  4.8466e-06,  ..., -1.4514e-05,
          7.2923e-07, -7.5325e-06],
        [-3.1650e-05, -1.8150e-05,  9.3505e-06,  ..., -2.7865e-05,
          1.4147e-06, -1.4439e-05],
        [-4.8041e-05, -2.7448e-05,  1.4037e-05,  ..., -4.2200e-05,
          2.0918e-06, -2.1845e-05]], device='cuda:0')
Loss: 1.2105180025100708


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5898e-05, -3.3250e-05,  2.5482e-04,  ..., -6.2195e-05,
         -4.5100e-05,  2.1886e-05],
        [-2.9594e-05, -1.6898e-05,  8.6799e-06,  ..., -2.6062e-05,
          1.3141e-06, -1.3515e-05],
        [-1.9252e-05, -1.1005e-05,  5.6475e-06,  ..., -1.6943e-05,
          8.5775e-07, -8.7991e-06],
        [-3.6955e-05, -2.1130e-05,  1.0900e-05,  ..., -3.2544e-05,
          1.6605e-06, -1.6868e-05],
        [-5.5969e-05, -3.1918e-05,  1.6332e-05,  ..., -4.9204e-05,
          2.4550e-06, -2.5466e-05]], device='cuda:0')
Loss: 1.2021347284317017


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5931e-04, -9.3289e-06,  2.5226e-04,  ..., -8.7534e-05,
          3.4037e-05, -2.4320e-06],
        [-3.3826e-05, -1.9401e-05,  9.9540e-06,  ..., -2.9832e-05,
          1.5208e-06, -1.5408e-05],
        [-2.1964e-05, -1.2614e-05,  6.4634e-06,  ..., -1.9357e-05,
          9.9000e-07, -1.0014e-05],
        [-4.2230e-05, -2.4244e-05,  1.2487e-05,  ..., -3.7223e-05,
          1.9232e-06, -1.9222e-05],
        [-6.3837e-05, -3.6597e-05,  1.8701e-05,  ..., -5.6207e-05,
          2.8387e-06, -2.8983e-05]], device='cuda:0')
Loss: 1.201183557510376
Graident accumulation at epoch 0, step 119, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0047, -0.0153,  0.0039,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0022, -0.0338],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1878e-04, -1.3436e-04,  9.3910e-05,  ...,  1.4217e-04,
         -3.5654e-04, -6.4261e-05],
        [-2.4432e-05, -1.9270e-05,  4.2945e-06,  ..., -2.1052e-05,
         -2.8284e-06, -7.1348e-06],
        [-1.3288e-06,  3.7098e-06, -4.0238e-06,  ...,  1.5692e-05,
         -3.7149e-06,  9.4292e-06],
        [-1.8236e-05,  2.3703e-06,  6.5945e-06,  ..., -1.1326e-05,
         -1.9864e-06, -2.0895e-07],
        [-3.9619e-05, -3.1720e-05,  4.4605e-06,  ..., -3.2217e-05,
         -4.0081e-06, -1.0262e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8119e-08, 1.4751e-08, 1.7598e-08,  ..., 2.0141e-08, 3.2000e-08,
         6.5347e-09],
        [1.4070e-11, 1.1389e-11, 6.9559e-13,  ..., 9.7892e-12, 4.8405e-13,
         1.0938e-12],
        [4.8211e-11, 5.4539e-11, 6.4668e-12,  ..., 1.1097e-10, 3.0804e-12,
         3.1641e-11],
        [7.4777e-11, 2.1569e-10, 4.3148e-12,  ..., 6.0711e-11, 5.8772e-12,
         2.9097e-11],
        [4.2618e-11, 3.2505e-11, 3.7058e-12,  ..., 3.1557e-11, 1.2714e-12,
         3.7477e-12]], device='cuda:0')
optimizer state dict: 15.0
lr: [1.9627808588917577e-05, 1.9627808588917577e-05]
scheduler_last_epoch: 15


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4195e-05,  1.2402e-05, -3.6536e-05,  ...,  1.8488e-05,
          1.4020e-05, -5.0351e-05],
        [-4.2319e-06, -2.4736e-06,  1.1176e-06,  ..., -3.6061e-06,
          1.6391e-07, -2.0117e-06],
        [-2.8014e-06, -1.6391e-06,  7.4133e-07,  ..., -2.3842e-06,
          1.0803e-07, -1.3337e-06],
        [-5.3048e-06, -3.0994e-06,  1.4082e-06,  ..., -4.5300e-06,
          2.0582e-07, -2.5332e-06],
        [-8.2254e-06, -4.8280e-06,  2.1756e-06,  ..., -7.0333e-06,
          3.2037e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.19968843460083


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2448e-05, -5.1071e-05, -9.0656e-06,  ..., -1.7896e-05,
          3.9673e-05, -1.1362e-04],
        [-8.4937e-06, -5.0068e-06,  2.2873e-06,  ..., -7.2420e-06,
          3.3341e-07, -4.0382e-06],
        [-5.6177e-06, -3.3230e-06,  1.5199e-06,  ..., -4.7982e-06,
          2.2165e-07, -2.6822e-06],
        [-1.0550e-05, -6.2287e-06,  2.8610e-06,  ..., -9.0301e-06,
          4.1816e-07, -5.0366e-06],
        [-1.6451e-05, -9.7454e-06,  4.4405e-06,  ..., -1.4096e-05,
          6.5006e-07, -7.8380e-06]], device='cuda:0')
Loss: 1.2007876634597778


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[   0,  462, 9202,  ...,   15, 3256,  271],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7016e-05, -1.3168e-04, -3.3302e-05,  ...,  1.6162e-06,
          5.1652e-06, -1.6250e-05],
        [-1.2755e-05, -7.5251e-06,  3.4422e-06,  ..., -1.0878e-05,
          5.7928e-07, -6.0052e-06],
        [-8.4490e-06, -4.9993e-06,  2.2873e-06,  ..., -7.2122e-06,
          3.8370e-07, -3.9861e-06],
        [-1.5795e-05, -9.3281e-06,  4.2915e-06,  ..., -1.3500e-05,
          7.2177e-07, -7.4506e-06],
        [-2.4796e-05, -1.4693e-05,  6.7055e-06,  ..., -2.1219e-05,
          1.1232e-06, -1.1683e-05]], device='cuda:0')
Loss: 1.200986623764038


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   266,  8802,  ...,  8401,     9,   287],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4371e-05, -1.9337e-04, -5.9015e-05,  ..., -2.5627e-05,
          2.1211e-05,  2.6412e-05],
        [-1.7047e-05, -1.0043e-05,  4.5821e-06,  ..., -1.4573e-05,
          7.8604e-07, -8.0466e-06],
        [-1.1265e-05, -6.6534e-06,  3.0324e-06,  ..., -9.6262e-06,
          5.2154e-07, -5.3272e-06],
        [-2.1040e-05, -1.2413e-05,  5.6848e-06,  ..., -1.8001e-05,
          9.7882e-07, -9.9540e-06],
        [-3.3200e-05, -1.9610e-05,  8.9109e-06,  ..., -2.8402e-05,
          1.5274e-06, -1.5676e-05]], device='cuda:0')
Loss: 1.208747148513794


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8567e-05, -2.2656e-04, -3.4575e-05,  ..., -2.4624e-05,
          3.9524e-05,  3.6609e-05],
        [-2.1368e-05, -1.2502e-05,  5.7295e-06,  ..., -1.8209e-05,
          9.9279e-07, -1.0058e-05],
        [-1.4082e-05, -8.2627e-06,  3.7812e-06,  ..., -1.2010e-05,
          6.5658e-07, -6.6385e-06],
        [-2.6375e-05, -1.5453e-05,  7.1153e-06,  ..., -2.2501e-05,
          1.2377e-06, -1.2442e-05],
        [-4.1544e-05, -2.4378e-05,  1.1131e-05,  ..., -3.5435e-05,
          1.9278e-06, -1.9550e-05]], device='cuda:0')
Loss: 1.1971620321273804


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 1268,  278,  ...,  461,  925,  630],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7857e-05, -2.2510e-04, -2.5954e-05,  ..., -1.2868e-05,
          4.6824e-05,  4.8973e-06],
        [-2.5630e-05, -1.5020e-05,  6.8024e-06,  ..., -2.1860e-05,
          1.1753e-06, -1.2025e-05],
        [-1.6868e-05, -9.9093e-06,  4.4815e-06,  ..., -1.4409e-05,
          7.7579e-07, -7.9274e-06],
        [-3.1650e-05, -1.8552e-05,  8.4415e-06,  ..., -2.7031e-05,
          1.4687e-06, -1.4871e-05],
        [-4.9829e-05, -2.9296e-05,  1.3202e-05,  ..., -4.2558e-05,
          2.2836e-06, -2.3395e-05]], device='cuda:0')
Loss: 1.1919198036193848


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    14,   333,   199],
        [    0,   650,    14,  ...,  3687,   278, 26952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2651e-04, -1.9989e-04, -3.4091e-05,  ..., -1.6761e-05,
          7.8894e-06, -2.6334e-05],
        [-2.9892e-05, -1.7494e-05,  7.9274e-06,  ..., -2.5481e-05,
          1.3132e-06, -1.4037e-05],
        [-1.9699e-05, -1.1541e-05,  5.2266e-06,  ..., -1.6809e-05,
          8.6846e-07, -9.2611e-06],
        [-3.6895e-05, -2.1592e-05,  9.8348e-06,  ..., -3.1501e-05,
          1.6429e-06, -1.7345e-05],
        [-5.8115e-05, -3.4094e-05,  1.5393e-05,  ..., -4.9621e-05,
          2.5518e-06, -2.7299e-05]], device='cuda:0')
Loss: 1.1875700950622559


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  4665,   909,  ...,  3607,   259,  1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8308e-04, -2.2300e-04,  3.4553e-06,  ...,  8.9508e-06,
          3.7284e-05, -3.0034e-05],
        [-3.4243e-05, -1.9968e-05,  9.1195e-06,  ..., -2.9102e-05,
          1.4994e-06, -1.6078e-05],
        [-2.2545e-05, -1.3158e-05,  6.0052e-06,  ..., -1.9178e-05,
          9.8953e-07, -1.0587e-05],
        [-4.2230e-05, -2.4632e-05,  1.1295e-05,  ..., -3.5942e-05,
          1.8720e-06, -1.9848e-05],
        [-6.6578e-05, -3.8892e-05,  1.7703e-05,  ..., -5.6684e-05,
          2.9095e-06, -3.1263e-05]], device='cuda:0')
Loss: 1.19795560836792
Graident accumulation at epoch 0, step 127, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0033,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0267,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8521e-04, -1.4323e-04,  8.4865e-05,  ...,  1.2885e-04,
         -3.1716e-04, -6.0838e-05],
        [-2.5413e-05, -1.9340e-05,  4.7770e-06,  ..., -2.1857e-05,
         -2.3956e-06, -8.0291e-06],
        [-3.4504e-06,  2.0230e-06, -3.0209e-06,  ...,  1.2205e-05,
         -3.2445e-06,  7.4276e-06],
        [-2.0635e-05, -3.2994e-07,  7.0645e-06,  ..., -1.3788e-05,
         -1.6006e-06, -2.1729e-06],
        [-4.2315e-05, -3.2437e-05,  5.7847e-06,  ..., -3.4664e-05,
         -3.3164e-06, -1.2362e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8104e-08, 1.4786e-08, 1.7581e-08,  ..., 2.0121e-08, 3.1970e-08,
         6.5290e-09],
        [1.5229e-11, 1.1776e-11, 7.7806e-13,  ..., 1.0626e-11, 4.8581e-13,
         1.3512e-12],
        [4.8671e-11, 5.4657e-11, 6.4964e-12,  ..., 1.1123e-10, 3.0783e-12,
         3.1721e-11],
        [7.6486e-11, 2.1608e-10, 4.4381e-12,  ..., 6.1942e-11, 5.8748e-12,
         2.9462e-11],
        [4.7008e-11, 3.3986e-11, 4.0155e-12,  ..., 3.4738e-11, 1.2786e-12,
         4.7213e-12]], device='cuda:0')
optimizer state dict: 16.0
lr: [1.9558050089320493e-05, 1.9558050089320493e-05]
scheduler_last_epoch: 16


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9715e-05, -6.0941e-06, -2.8053e-05,  ...,  3.9965e-05,
         -1.6696e-05,  2.7596e-05],
        [-4.3511e-06, -2.6077e-06,  1.0207e-06,  ..., -3.5614e-06,
          1.5181e-07, -2.1309e-06],
        [-2.9355e-06, -1.7583e-06,  6.8918e-07,  ..., -2.4140e-06,
          1.0291e-07, -1.4380e-06],
        [-5.3346e-06, -3.1888e-06,  1.2517e-06,  ..., -4.3809e-06,
          1.8999e-07, -2.6077e-06],
        [-8.7619e-06, -5.2452e-06,  2.0415e-06,  ..., -7.1526e-06,
          3.0175e-07, -4.2915e-06]], device='cuda:0')
Loss: 1.2069752216339111


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1682,  271,  ...,  367, 3210,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0205e-05,  4.6686e-06, -9.1438e-05,  ...,  2.0387e-05,
         -5.2345e-06, -1.3473e-05],
        [-8.5831e-06, -5.1409e-06,  2.0266e-06,  ..., -7.0930e-06,
          2.9430e-07, -4.1425e-06],
        [-5.7667e-06, -3.4571e-06,  1.3597e-06,  ..., -4.7833e-06,
          1.9791e-07, -2.7940e-06],
        [-1.0490e-05, -6.2883e-06,  2.4810e-06,  ..., -8.7023e-06,
          3.6694e-07, -5.0664e-06],
        [-1.7285e-05, -1.0341e-05,  4.0531e-06,  ..., -1.4246e-05,
          5.8487e-07, -8.3447e-06]], device='cuda:0')
Loss: 1.2197201251983643


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0,   471,    12,  ...,    13,  9909,  2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1334e-06,  4.6299e-05, -9.4969e-05,  ...,  1.0296e-05,
         -2.9535e-05, -2.3242e-05],
        [-1.2904e-05, -7.7039e-06,  2.9691e-06,  ..., -1.0625e-05,
          4.6287e-07, -6.2287e-06],
        [-8.6576e-06, -5.1782e-06,  1.9968e-06,  ..., -7.1526e-06,
          3.1153e-07, -4.1872e-06],
        [-1.5765e-05, -9.4175e-06,  3.6433e-06,  ..., -1.3024e-05,
          5.7463e-07, -7.5996e-06],
        [-2.5928e-05, -1.5467e-05,  5.9381e-06,  ..., -2.1309e-05,
          9.2201e-07, -1.2487e-05]], device='cuda:0')
Loss: 1.2179452180862427


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  221,  334,  ...,  706, 2680,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6642e-05,  2.5758e-05, -9.2170e-05,  ...,  5.3197e-05,
         -2.6803e-05, -5.3706e-05],
        [-1.7077e-05, -1.0192e-05,  3.9972e-06,  ..., -1.4082e-05,
          6.1840e-07, -8.3596e-06],
        [-1.1504e-05, -6.8769e-06,  2.6971e-06,  ..., -9.5069e-06,
          4.1677e-07, -5.6401e-06],
        [-2.0951e-05, -1.2502e-05,  4.9248e-06,  ..., -1.7315e-05,
          7.7020e-07, -1.0252e-05],
        [-3.4392e-05, -2.0504e-05,  8.0243e-06,  ..., -2.8342e-05,
          1.2331e-06, -1.6809e-05]], device='cuda:0')
Loss: 1.1880078315734863


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[    0,  2159,   271,  ...,  1268,   344,   259],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0097e-05,  3.5578e-05, -8.8080e-05,  ...,  7.0171e-06,
         -3.2921e-05, -5.9782e-05],
        [-2.1309e-05, -1.2711e-05,  4.9472e-06,  ..., -1.7583e-05,
          7.6368e-07, -1.0386e-05],
        [-1.4365e-05, -8.5831e-06,  3.3416e-06,  ..., -1.1876e-05,
          5.1409e-07, -7.0110e-06],
        [-2.6137e-05, -1.5572e-05,  6.0946e-06,  ..., -2.1607e-05,
          9.5181e-07, -1.2726e-05],
        [-4.2975e-05, -2.5600e-05,  9.9465e-06,  ..., -3.5435e-05,
          1.5274e-06, -2.0891e-05]], device='cuda:0')
Loss: 1.2085061073303223


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0257e-05,  8.8952e-05, -9.7298e-05,  ...,  1.0600e-05,
         -3.3757e-05, -7.2356e-05],
        [-2.5600e-05, -1.5259e-05,  5.9530e-06,  ..., -2.1055e-05,
          9.5181e-07, -1.2442e-05],
        [-1.7285e-05, -1.0319e-05,  4.0308e-06,  ..., -1.4246e-05,
          6.4261e-07, -8.4117e-06],
        [-3.1412e-05, -1.8701e-05,  7.3388e-06,  ..., -2.5868e-05,
          1.1846e-06, -1.5244e-05],
        [-5.1677e-05, -3.0756e-05,  1.1973e-05,  ..., -4.2468e-05,
          1.9036e-06, -2.5034e-05]], device='cuda:0')
Loss: 1.191427230834961


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0, 1979,  352,  ...,  292, 1591,  446],
        [   0,  944,  278,  ..., 2374,  699, 8867]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1618e-05,  1.3188e-04, -1.3897e-04,  ..., -9.4192e-05,
         -1.5753e-05, -8.7292e-05],
        [-2.9862e-05, -1.7807e-05,  6.9812e-06,  ..., -2.4647e-05,
          1.0435e-06, -1.4573e-05],
        [-2.0161e-05, -1.2040e-05,  4.7274e-06,  ..., -1.6659e-05,
          7.0455e-07, -9.8571e-06],
        [-3.6597e-05, -2.1800e-05,  8.5980e-06,  ..., -3.0249e-05,
          1.2992e-06, -1.7837e-05],
        [-6.0141e-05, -3.5822e-05,  1.4015e-05,  ..., -4.9621e-05,
          2.0806e-06, -2.9296e-05]], device='cuda:0')
Loss: 1.2179079055786133


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[    0,  6673,   298,  ...,  4391,   292,   221],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4840e-05,  8.8154e-05, -1.4764e-04,  ..., -1.3417e-04,
         -2.0621e-05, -1.0476e-04],
        [-3.4153e-05, -2.0415e-05,  8.0094e-06,  ..., -2.8163e-05,
          1.1516e-06, -1.6689e-05],
        [-2.3052e-05, -1.3791e-05,  5.4166e-06,  ..., -1.9014e-05,
          7.7672e-07, -1.1280e-05],
        [-4.1872e-05, -2.5004e-05,  9.8646e-06,  ..., -3.4571e-05,
          1.4342e-06, -2.0429e-05],
        [-6.8665e-05, -4.1008e-05,  1.6041e-05,  ..., -5.6595e-05,
          2.2920e-06, -3.3498e-05]], device='cuda:0')
Loss: 1.1913948059082031
Graident accumulation at epoch 0, step 135, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4517e-04, -1.2009e-04,  6.1614e-05,  ...,  1.0255e-04,
         -2.8750e-04, -6.5230e-05],
        [-2.6287e-05, -1.9448e-05,  5.1002e-06,  ..., -2.2487e-05,
         -2.0409e-06, -8.8951e-06],
        [-5.4106e-06,  4.4164e-07, -2.1772e-06,  ...,  9.0829e-06,
         -2.8424e-06,  5.5568e-06],
        [-2.2759e-05, -2.7974e-06,  7.3445e-06,  ..., -1.5866e-05,
         -1.2971e-06, -3.9986e-06],
        [-4.4950e-05, -3.3294e-05,  6.8103e-06,  ..., -3.6857e-05,
         -2.7555e-06, -1.4476e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8063e-08, 1.4779e-08, 1.7585e-08,  ..., 2.0119e-08, 3.1938e-08,
         6.5335e-09],
        [1.6380e-11, 1.2181e-11, 8.4144e-13,  ..., 1.1409e-11, 4.8665e-13,
         1.6284e-12],
        [4.9154e-11, 5.4793e-11, 6.5192e-12,  ..., 1.1148e-10, 3.0759e-12,
         3.1817e-11],
        [7.8163e-11, 2.1649e-10, 4.5309e-12,  ..., 6.3075e-11, 5.8710e-12,
         2.9850e-11],
        [5.1676e-11, 3.5633e-11, 4.2688e-12,  ..., 3.7907e-11, 1.2826e-12,
         5.8387e-12]], device='cuda:0')
optimizer state dict: 17.0
lr: [1.9482449598960544e-05, 1.9482449598960544e-05]
scheduler_last_epoch: 17


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[   0,  471,   14,  ..., 1260, 2129,  367],
        [   0,  278, 1099,  ...,  496,   14,  879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1226e-05, -4.3117e-05, -2.6697e-05,  ...,  3.3773e-05,
          4.8569e-05, -3.3507e-05],
        [-4.2915e-06, -2.6077e-06,  8.3447e-07,  ..., -3.3379e-06,
          1.0990e-07, -2.0713e-06],
        [-2.9504e-06, -1.7881e-06,  5.7369e-07,  ..., -2.2948e-06,
          7.4971e-08, -1.4156e-06],
        [-5.2452e-06, -3.1739e-06,  1.0207e-06,  ..., -4.0531e-06,
          1.3132e-07, -2.5034e-06],
        [-8.7619e-06, -5.3048e-06,  1.6987e-06,  ..., -6.8247e-06,
          2.1793e-07, -4.2021e-06]], device='cuda:0')
Loss: 1.189488172531128


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0,  9430,   287,  ...,  1141,  2280,   408],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6970e-05, -6.3467e-05, -4.6966e-05,  ...,  2.2123e-05,
          7.6233e-05, -6.5677e-05],
        [-8.5235e-06, -5.1856e-06,  1.6727e-06,  ..., -6.7055e-06,
          2.2771e-07, -4.1574e-06],
        [-5.8860e-06, -3.5688e-06,  1.1548e-06,  ..., -4.6194e-06,
          1.5600e-07, -2.8536e-06],
        [-1.0401e-05, -6.3181e-06,  2.0489e-06,  ..., -8.1658e-06,
          2.7660e-07, -5.0366e-06],
        [-1.7405e-05, -1.0550e-05,  3.4049e-06,  ..., -1.3709e-05,
          4.5635e-07, -8.4341e-06]], device='cuda:0')
Loss: 1.1903458833694458


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,    12,   630,  ...,  5049,    14,  2371],
        [    0,  2973, 20362,  ...,   271, 43821, 11776]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2248e-04, -6.7573e-05, -2.1190e-05,  ...,  1.5183e-05,
          8.1651e-05, -5.2274e-05],
        [-1.2815e-05, -7.7188e-06,  2.5369e-06,  ..., -1.0043e-05,
          2.6310e-07, -6.2287e-06],
        [ 1.8931e-04,  1.3580e-04, -5.9449e-05,  ...,  1.4759e-04,
          1.1498e-05,  1.0943e-04],
        [-1.5706e-05, -9.4324e-06,  3.1143e-06,  ..., -1.2279e-05,
          3.2433e-07, -7.5996e-06],
        [-2.6286e-05, -1.5765e-05,  5.1782e-06,  ..., -2.0593e-05,
          5.2620e-07, -1.2726e-05]], device='cuda:0')
Loss: 1.2029625177383423


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  3543,   391,  ...,  3370,  2926,  8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0382e-04, -8.7491e-05,  2.9970e-05,  ...,  1.5183e-05,
          4.3060e-05, -9.2181e-05],
        [-1.7136e-05, -1.0341e-05,  3.3602e-06,  ..., -1.3396e-05,
          3.4925e-07, -8.2999e-06],
        [ 1.8635e-04,  1.3401e-04, -5.8883e-05,  ...,  1.4529e-04,
          1.1558e-05,  1.0801e-04],
        [-2.0921e-05, -1.2591e-05,  4.1127e-06,  ..., -1.6332e-05,
          4.3190e-07, -1.0103e-05],
        [-3.5286e-05, -2.1189e-05,  6.8769e-06,  ..., -2.7537e-05,
          7.0222e-07, -1.7017e-05]], device='cuda:0')
Loss: 1.1924355030059814


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,    13, 26335,  ...,     5,  2570, 34403],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8561e-05, -6.5228e-05,  6.6618e-05,  ..., -1.1378e-05,
          5.4321e-05, -9.1489e-05],
        [-2.1368e-05, -1.2919e-05,  4.2133e-06,  ..., -1.6749e-05,
          3.9116e-07, -1.0371e-05],
        [ 1.8342e-04,  1.3223e-04, -5.8294e-05,  ...,  1.4298e-04,
          1.1586e-05,  1.0658e-04],
        [-2.6107e-05, -1.5751e-05,  5.1633e-06,  ..., -2.0444e-05,
          4.8382e-07, -1.2636e-05],
        [-4.3988e-05, -2.6524e-05,  8.6278e-06,  ..., -3.4451e-05,
          7.8138e-07, -2.1279e-05]], device='cuda:0')
Loss: 1.2121610641479492


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,  1529,  5227,  ...,  1480,   367,   925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2709e-04, -1.0859e-04,  3.2555e-05,  ..., -1.6936e-05,
          6.5155e-05, -4.8507e-05],
        [-2.5630e-05, -1.5512e-05,  5.0552e-06,  ..., -2.0102e-05,
          4.0873e-07, -1.2398e-05],
        [ 1.8050e-04,  1.3044e-04, -5.7717e-05,  ...,  1.4067e-04,
          1.1598e-05,  1.0518e-04],
        [-3.1263e-05, -1.8895e-05,  6.1840e-06,  ..., -2.4498e-05,
          5.0839e-07, -1.5095e-05],
        [-5.2750e-05, -3.1888e-05,  1.0364e-05,  ..., -4.1366e-05,
          8.1351e-07, -2.5451e-05]], device='cuda:0')
Loss: 1.2159769535064697


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,    14,  8058,  ..., 10316,   352,   266],
        [    0,   677,  9606,  ...,  9468,  9268,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0392e-05, -1.3470e-04, -4.2909e-05,  ...,  1.6662e-05,
          1.0275e-04, -5.6978e-05],
        [-2.9862e-05, -1.8045e-05,  5.9307e-06,  ..., -2.3365e-05,
          5.1537e-07, -1.4454e-05],
        [ 1.7758e-04,  1.2868e-04, -5.7110e-05,  ...,  1.3842e-04,
          1.1670e-05,  1.0376e-04],
        [-3.6478e-05, -2.2009e-05,  7.2718e-06,  ..., -2.8491e-05,
          6.3877e-07, -1.7613e-05],
        [-6.1452e-05, -3.7074e-05,  1.2152e-05,  ..., -4.8041e-05,
          1.0259e-06, -2.9624e-05]], device='cuda:0')
Loss: 1.1783533096313477


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[    0,  4108,    85,  ...,    40,    12,  1530],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2719e-05, -1.7918e-04, -5.9880e-05,  ...,  7.9301e-06,
          1.4484e-04, -5.7831e-05],
        [-3.4153e-05, -2.0653e-05,  6.7614e-06,  ..., -2.6762e-05,
          5.8429e-07, -1.6525e-05],
        [ 1.7462e-04,  1.2688e-04, -5.6536e-05,  ...,  1.3607e-04,
          1.1718e-05,  1.0233e-04],
        [-4.1693e-05, -2.5168e-05,  8.2850e-06,  ..., -3.2634e-05,
          7.2445e-07, -2.0117e-05],
        [-7.0333e-05, -4.2468e-05,  1.3858e-05,  ..., -5.5075e-05,
          1.1637e-06, -3.3915e-05]], device='cuda:0')
Loss: 1.204912543296814
Graident accumulation at epoch 0, step 143, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9138e-04, -1.2600e-04,  4.9465e-05,  ...,  9.3085e-05,
         -2.4427e-04, -6.4490e-05],
        [-2.7074e-05, -1.9568e-05,  5.2663e-06,  ..., -2.2915e-05,
         -1.7783e-06, -9.6582e-06],
        [ 1.2592e-05,  1.3085e-05, -7.6130e-06,  ...,  2.1781e-05,
         -1.3863e-06,  1.5234e-05],
        [-2.4652e-05, -5.0344e-06,  7.4386e-06,  ..., -1.7543e-05,
         -1.0949e-06, -5.6104e-06],
        [-4.7488e-05, -3.4212e-05,  7.5151e-06,  ..., -3.8679e-05,
         -2.3636e-06, -1.6420e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8024e-08, 1.4796e-08, 1.7571e-08,  ..., 2.0099e-08, 3.1927e-08,
         6.5303e-09],
        [1.7530e-11, 1.2595e-11, 8.8631e-13,  ..., 1.2114e-11, 4.8651e-13,
         1.8999e-12],
        [7.9596e-11, 7.0836e-11, 9.7090e-12,  ..., 1.2988e-10, 3.2101e-12,
         4.2257e-11],
        [7.9823e-11, 2.1691e-10, 4.5950e-12,  ..., 6.4077e-11, 5.8656e-12,
         3.0225e-11],
        [5.6571e-11, 3.7401e-11, 4.4565e-12,  ..., 4.0902e-11, 1.2826e-12,
         6.9831e-12]], device='cuda:0')
optimizer state dict: 18.0
lr: [1.9401053325731837e-05, 1.9401053325731837e-05]
scheduler_last_epoch: 18


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   452,   298,  ...,   287,  1575,  7856],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8715e-06, -5.1487e-05, -3.9611e-06,  ...,  1.1439e-05,
         -2.2709e-06,  5.0882e-05],
        [-4.1723e-06, -2.6822e-06,  6.7800e-07,  ..., -3.2187e-06,
         -5.3085e-08, -2.0564e-06],
        [-2.9504e-06, -1.8850e-06,  4.7684e-07,  ..., -2.2799e-06,
         -3.6787e-08, -1.4529e-06],
        [-5.1558e-06, -3.2932e-06,  8.3819e-07,  ..., -3.9637e-06,
         -6.4261e-08, -2.5332e-06],
        [-8.8215e-06, -5.6326e-06,  1.4231e-06,  ..., -6.7949e-06,
         -1.1036e-07, -4.3213e-06]], device='cuda:0')
Loss: 1.1922242641448975


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   685,   344,  ...,   680,   401,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4759e-05, -6.5686e-05,  3.4247e-05,  ..., -4.7377e-05,
         -4.8191e-05,  1.5968e-05],
        [-8.3745e-06, -5.3048e-06,  1.3411e-06,  ..., -6.4075e-06,
         -5.4242e-08, -4.1276e-06],
        [-5.9158e-06, -3.7327e-06,  9.4250e-07,  ..., -4.5300e-06,
         -3.8235e-08, -2.9132e-06],
        [-1.0312e-05, -6.4969e-06,  1.6503e-06,  ..., -7.8678e-06,
         -6.3308e-08, -5.0664e-06],
        [-1.7762e-05, -1.1176e-05,  2.8163e-06,  ..., -1.3530e-05,
         -1.1694e-07, -8.7023e-06]], device='cuda:0')
Loss: 1.2001198530197144


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  3001,  3325,  ..., 16332,  2661,  1200],
        [    0,    14,  6436,  ...,   271,  1211,  8917]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7931e-05, -6.6109e-05,  2.1075e-05,  ..., -4.9940e-05,
         -4.2342e-05,  3.4788e-05],
        [-1.2577e-05, -7.8976e-06,  1.9930e-06,  ..., -9.6112e-06,
         -6.3439e-08, -6.1244e-06],
        [-8.8811e-06, -5.5656e-06,  1.4026e-06,  ..., -6.7949e-06,
         -4.5802e-08, -4.3213e-06],
        [-1.5408e-05, -9.6411e-06,  2.4438e-06,  ..., -1.1742e-05,
         -7.3902e-08, -7.4804e-06],
        [-2.6643e-05, -1.6689e-05,  4.1947e-06,  ..., -2.0295e-05,
         -1.4092e-07, -1.2934e-05]], device='cuda:0')
Loss: 1.2117741107940674


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1964e-05, -5.7306e-05,  7.8584e-07,  ..., -4.9940e-05,
         -1.4909e-05,  7.4807e-05],
        [-1.6779e-05, -1.0535e-05,  2.6524e-06,  ..., -1.2860e-05,
         -6.0441e-08, -8.1360e-06],
        [-1.1832e-05, -7.4282e-06,  1.8682e-06,  ..., -9.0748e-06,
         -4.4609e-08, -5.7369e-06],
        [-2.0474e-05, -1.2830e-05,  3.2485e-06,  ..., -1.5676e-05,
         -6.9129e-08, -9.9093e-06],
        [-3.5465e-05, -2.2233e-05,  5.5805e-06,  ..., -2.7120e-05,
         -1.3891e-07, -1.7136e-05]], device='cuda:0')
Loss: 1.1990841627120972


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  266,  997,  ..., 2670,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9162e-05, -5.9756e-05, -8.9747e-05,  ..., -7.8008e-05,
         -5.5978e-06,  5.4858e-05],
        [-2.0921e-05, -1.3128e-05,  3.3230e-06,  ..., -1.6049e-05,
         -1.2796e-07, -1.0148e-05],
        [-1.4782e-05, -9.2760e-06,  2.3451e-06,  ..., -1.1340e-05,
         -9.0244e-08, -7.1600e-06],
        [-2.5600e-05, -1.6034e-05,  4.0792e-06,  ..., -1.9610e-05,
         -1.5015e-07, -1.2383e-05],
        [-4.4286e-05, -2.7746e-05,  6.9961e-06,  ..., -3.3885e-05,
         -2.8140e-07, -2.1398e-05]], device='cuda:0')
Loss: 1.1716240644454956


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0, 1412,   35,  ..., 6077,  298, 1826],
        [   0,  278, 5210,  ..., 1968, 2002,  923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6737e-05, -6.7411e-05, -9.5481e-05,  ..., -1.0460e-04,
          2.1769e-05,  6.4345e-05],
        [-2.5153e-05, -1.5751e-05,  3.9712e-06,  ..., -1.9282e-05,
         -1.6708e-07, -1.2174e-05],
        [-1.7777e-05, -1.1139e-05,  2.8033e-06,  ..., -1.3635e-05,
         -1.1748e-07, -8.5980e-06],
        [-3.0756e-05, -1.9237e-05,  4.8727e-06,  ..., -2.3544e-05,
         -1.9416e-07, -1.4856e-05],
        [-5.3227e-05, -3.3319e-05,  8.3596e-06,  ..., -4.0740e-05,
         -3.6057e-07, -2.5690e-05]], device='cuda:0')
Loss: 1.1908491849899292


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7527e-05, -1.0472e-04, -4.9197e-05,  ..., -1.2895e-04,
          5.5938e-05,  5.2932e-05],
        [-2.9355e-05, -1.8388e-05,  4.6529e-06,  ..., -2.2411e-05,
         -2.2109e-07, -1.4201e-05],
        [-2.0772e-05, -1.3016e-05,  3.2876e-06,  ..., -1.5855e-05,
         -1.5800e-07, -1.0043e-05],
        [-3.5882e-05, -2.2456e-05,  5.7071e-06,  ..., -2.7359e-05,
         -2.6121e-07, -1.7330e-05],
        [-6.2108e-05, -3.8862e-05,  9.7826e-06,  ..., -4.7326e-05,
         -4.8257e-07, -2.9951e-05]], device='cuda:0')
Loss: 1.1904783248901367


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8089e-05, -8.2811e-05, -2.0041e-05,  ..., -1.2096e-04,
          5.5938e-05,  3.5838e-05],
        [-3.3587e-05, -2.1026e-05,  5.2899e-06,  ..., -2.5615e-05,
         -3.0584e-07, -1.6212e-05],
        [-2.3782e-05, -1.4894e-05,  3.7421e-06,  ..., -1.8150e-05,
         -2.1690e-07, -1.1474e-05],
        [-4.1038e-05, -2.5675e-05,  6.4857e-06,  ..., -3.1263e-05,
         -3.6273e-07, -1.9774e-05],
        [-7.1228e-05, -4.4554e-05,  1.1146e-05,  ..., -5.4240e-05,
         -6.6604e-07, -3.4273e-05]], device='cuda:0')
Loss: 1.1980515718460083
Graident accumulation at epoch 0, step 151, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0407,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0142, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4444e-04, -1.2168e-04,  4.2514e-05,  ...,  7.1680e-05,
         -2.1425e-04, -5.4457e-05],
        [-2.7725e-05, -1.9714e-05,  5.2687e-06,  ..., -2.3185e-05,
         -1.6311e-06, -1.0314e-05],
        [ 8.9548e-06,  1.0287e-05, -6.4775e-06,  ...,  1.7788e-05,
         -1.2694e-06,  1.2564e-05],
        [-2.6291e-05, -7.0985e-06,  7.3433e-06,  ..., -1.8915e-05,
         -1.0217e-06, -7.0267e-06],
        [-4.9862e-05, -3.5246e-05,  7.8782e-06,  ..., -4.0235e-05,
         -2.1939e-06, -1.8205e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7982e-08, 1.4788e-08, 1.7554e-08,  ..., 2.0093e-08, 3.1899e-08,
         6.5250e-09],
        [1.8641e-11, 1.3025e-11, 9.1341e-13,  ..., 1.2758e-11, 4.8612e-13,
         2.1608e-12],
        [8.0082e-11, 7.0987e-11, 9.7133e-12,  ..., 1.3008e-10, 3.2069e-12,
         4.2346e-11],
        [8.1427e-11, 2.1735e-10, 4.6325e-12,  ..., 6.4990e-11, 5.8599e-12,
         3.0586e-11],
        [6.1588e-11, 3.9349e-11, 4.5763e-12,  ..., 4.3803e-11, 1.2818e-12,
         8.1507e-12]], device='cuda:0')
optimizer state dict: 19.0
lr: [1.9313911019977992e-05, 1.9313911019977992e-05]
scheduler_last_epoch: 19


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7031e-05,  1.9619e-06,  1.3439e-05,  ..., -5.2543e-05,
          1.8138e-05,  1.9473e-05],
        [-4.0829e-06, -2.6673e-06,  5.4389e-07,  ..., -3.0547e-06,
         -1.7323e-07, -1.9819e-06],
        [ 1.1383e-04,  7.6507e-05, -7.2773e-06,  ...,  5.9789e-05,
          9.7243e-06,  3.4094e-05],
        [-4.8876e-06, -3.1888e-06,  6.5565e-07,  ..., -3.6657e-06,
         -2.0675e-07, -2.3693e-06],
        [-8.7619e-06, -5.6922e-06,  1.1548e-06,  ..., -6.5565e-06,
         -3.7625e-07, -4.2319e-06]], device='cuda:0')
Loss: 1.1906957626342773


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[    0, 39224,    34,  ...,   401,  1716,   271],
        [    0,   741,  4933,  ...,   932,   365,   838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9650e-05,  9.5958e-06,  2.8189e-05,  ..., -6.6303e-05,
         -5.3159e-06,  3.5900e-05],
        [-8.1360e-06, -5.3197e-06,  1.0617e-06,  ..., -6.0946e-06,
         -3.0641e-07, -3.9041e-06],
        [ 1.1084e-04,  7.4555e-05, -6.8955e-06,  ...,  5.7539e-05,
          9.6252e-06,  3.2671e-05],
        [-9.8050e-06, -6.4075e-06,  1.2852e-06,  ..., -7.3463e-06,
         -3.6694e-07, -4.7088e-06],
        [-1.7762e-05, -1.1563e-05,  2.2948e-06,  ..., -1.3292e-05,
         -6.7428e-07, -8.4937e-06]], device='cuda:0')
Loss: 1.2006070613861084


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0, 28590,    12,  ...,   342, 29639,  1693],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5741e-05, -1.6253e-05,  1.9232e-05,  ..., -9.4438e-05,
         -4.0814e-05,  5.0065e-05],
        [-1.2219e-05, -8.0019e-06,  1.5683e-06,  ..., -9.1791e-06,
         -5.1875e-07, -5.8413e-06],
        [ 1.0787e-04,  7.2603e-05, -6.5248e-06,  ...,  5.5289e-05,
          9.4706e-06,  3.1263e-05],
        [-1.4722e-05, -9.6262e-06,  1.8999e-06,  ..., -1.1057e-05,
         -6.2026e-07, -7.0333e-06],
        [-2.6584e-05, -1.7375e-05,  3.3900e-06,  ..., -1.9968e-05,
         -1.1381e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.193820595741272


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0024e-05, -5.9432e-05, -1.6449e-05,  ..., -9.5309e-05,
         -1.6778e-05,  2.1060e-05],
        [-1.6302e-05, -1.0714e-05,  2.1048e-06,  ..., -1.2249e-05,
         -6.4727e-07, -7.7933e-06],
        [ 1.0488e-04,  7.0606e-05, -6.1299e-06,  ...,  5.3024e-05,
          9.3751e-06,  2.9817e-05],
        [-1.9699e-05, -1.2949e-05,  2.5630e-06,  ..., -1.4827e-05,
         -7.7859e-07, -9.4324e-06],
        [-3.5465e-05, -2.3246e-05,  4.5523e-06,  ..., -2.6643e-05,
         -1.4212e-06, -1.6928e-05]], device='cuda:0')
Loss: 1.195592999458313


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3770e-05, -7.6147e-05, -4.7960e-05,  ..., -1.0661e-04,
         -1.9028e-05,  2.3275e-05],
        [-2.0355e-05, -1.3381e-05,  2.6599e-06,  ..., -1.5274e-05,
         -8.6240e-07, -9.7156e-06],
        [ 1.0187e-04,  6.8639e-05, -5.7183e-06,  ...,  5.0789e-05,
          9.2168e-06,  2.8394e-05],
        [-2.4676e-05, -1.6227e-05,  3.2485e-06,  ..., -1.8537e-05,
         -1.0412e-06, -1.1787e-05],
        [-4.4405e-05, -2.9087e-05,  5.7667e-06,  ..., -3.3259e-05,
         -1.8943e-06, -2.1130e-05]], device='cuda:0')
Loss: 1.191038727760315


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  4103,   278,  4190],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9947e-06, -7.3477e-05, -6.6384e-05,  ..., -6.4554e-05,
          3.1762e-06,  2.0589e-05],
        [-2.4408e-05, -1.6004e-05,  3.2261e-06,  ..., -1.8299e-05,
         -1.0505e-06, -1.1653e-05],
        [ 9.8871e-05,  6.6702e-05, -5.2992e-06,  ...,  4.8554e-05,
          9.0771e-06,  2.6964e-05],
        [-2.9624e-05, -1.9431e-05,  3.9414e-06,  ..., -2.2233e-05,
         -1.2694e-06, -1.4156e-05],
        [-5.3287e-05, -3.4839e-05,  6.9961e-06,  ..., -3.9876e-05,
         -2.3060e-06, -2.5362e-05]], device='cuda:0')
Loss: 1.197033405303955


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,   669,   292,  ...,  4032,   271,  4442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8157e-06, -4.6268e-05, -1.2880e-04,  ..., -2.7855e-05,
          3.4116e-05, -7.9180e-06],
        [-2.8402e-05, -1.8656e-05,  3.7476e-06,  ..., -2.1353e-05,
         -1.1986e-06, -1.3575e-05],
        [ 9.5920e-05,  6.4750e-05, -4.9155e-06,  ...,  4.6304e-05,
          8.9672e-06,  2.5555e-05],
        [-3.4541e-05, -2.2680e-05,  4.5858e-06,  ..., -2.5988e-05,
         -1.4519e-06, -1.6510e-05],
        [-6.2048e-05, -4.0650e-05,  8.1286e-06,  ..., -4.6551e-05,
         -2.6319e-06, -2.9564e-05]], device='cuda:0')
Loss: 1.1944910287857056


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,  361, 1224,  ..., 4401, 4261, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9300e-05, -4.4533e-05, -5.2682e-05,  ..., -6.4846e-05,
          6.1279e-05, -5.7931e-06],
        [-3.2425e-05, -2.1324e-05,  4.2189e-06,  ..., -2.4393e-05,
         -1.3635e-06, -1.5467e-05],
        [ 9.2970e-05,  6.2798e-05, -4.5709e-06,  ...,  4.4083e-05,
          8.8480e-06,  2.4177e-05],
        [ 2.8516e-04,  1.7758e-04,  3.3801e-06,  ...,  2.3975e-04,
          2.3966e-05,  1.1826e-04],
        [-7.0930e-05, -4.6521e-05,  9.1717e-06,  ..., -5.3257e-05,
         -2.9970e-06, -3.3706e-05]], device='cuda:0')
Loss: 1.2124555110931396
Graident accumulation at epoch 0, step 159, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1192e-04, -1.1396e-04,  3.2995e-05,  ...,  5.8028e-05,
         -1.8670e-04, -4.9591e-05],
        [-2.8195e-05, -1.9875e-05,  5.1637e-06,  ..., -2.3306e-05,
         -1.6043e-06, -1.0829e-05],
        [ 1.7356e-05,  1.5538e-05, -6.2869e-06,  ...,  2.0418e-05,
         -2.5765e-07,  1.3725e-05],
        [ 4.8537e-06,  1.1370e-05,  6.9470e-06,  ...,  6.9514e-06,
          1.4771e-06,  5.5021e-06],
        [-5.1969e-05, -3.6373e-05,  8.0076e-06,  ..., -4.1537e-05,
         -2.2742e-06, -1.9755e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7934e-08, 1.4775e-08, 1.7539e-08,  ..., 2.0077e-08, 3.1870e-08,
         6.5185e-09],
        [1.9673e-11, 1.3467e-11, 9.3029e-13,  ..., 1.3340e-11, 4.8749e-13,
         2.3979e-12],
        [8.8645e-11, 7.4859e-11, 9.7245e-12,  ..., 1.3190e-10, 3.2820e-12,
         4.2888e-11],
        [1.6266e-10, 2.4867e-10, 4.6393e-12,  ..., 1.2240e-10, 6.4284e-12,
         4.4541e-11],
        [6.6557e-11, 4.1474e-11, 4.6559e-12,  ..., 4.6595e-11, 1.2895e-12,
         9.2787e-12]], device='cuda:0')
optimizer state dict: 20.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 20
Epoch 0 | Batch 159/1048 | Training PPL: 16311.800611336972 | time 11.981629610061646
Saving checkpoint at epoch 0, step 159, batch 159
Epoch 0 | Validation PPL: 10.850048809548472 | Learning rate: 1.9221075944084176e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_159, AFTER epoch 0, step 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[   0, 1487,  409,  ..., 6979, 1273,  496],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4791e-05, -1.4885e-05, -2.4363e-05,  ...,  3.7473e-05,
          3.4583e-05,  1.5396e-05],
        [-3.9935e-06, -2.7269e-06,  3.9674e-07,  ..., -2.9653e-06,
         -2.9244e-07, -1.8254e-06],
        [-3.0398e-06, -2.0862e-06,  3.0361e-07,  ..., -2.2650e-06,
         -2.2259e-07, -1.3858e-06],
        [-4.8578e-06, -3.3379e-06,  4.8801e-07,  ..., -3.6210e-06,
         -3.5390e-07, -2.2203e-06],
        [-8.9407e-06, -6.1095e-06,  8.8289e-07,  ..., -6.6161e-06,
         -6.5193e-07, -4.0531e-06]], device='cuda:0')
Loss: 1.198467493057251


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   298,   527,   496],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7464e-05, -1.5582e-05, -2.1929e-05,  ...,  8.1988e-06,
          4.1920e-05, -6.1099e-05],
        [-7.9572e-06, -5.4091e-06,  8.5123e-07,  ..., -5.9158e-06,
         -5.7556e-07, -3.7104e-06],
        [-6.0797e-06, -4.1425e-06,  6.5006e-07,  ..., -4.5300e-06,
         -4.3958e-07, -2.8312e-06],
        [-9.7454e-06, -6.6459e-06,  1.0468e-06,  ..., -7.2569e-06,
         -7.0035e-07, -4.5449e-06],
        [-1.7822e-05, -1.2100e-05,  1.8887e-06,  ..., -1.3202e-05,
         -1.2815e-06, -8.2552e-06]], device='cuda:0')
Loss: 1.1964521408081055


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4315e-04,  4.0942e-06, -1.0296e-04,  ..., -4.0207e-06,
          6.3366e-05, -9.0164e-05],
        [-1.1891e-05, -8.1211e-06,  1.2778e-06,  ..., -8.8662e-06,
         -8.6613e-07, -5.5581e-06],
        [-9.0748e-06, -6.1989e-06,  9.7416e-07,  ..., -6.7651e-06,
         -6.6031e-07, -4.2394e-06],
        [-1.4544e-05, -9.9391e-06,  1.5683e-06,  ..., -1.0833e-05,
         -1.0524e-06, -6.7949e-06],
        [-2.6584e-05, -1.8120e-05,  2.8349e-06,  ..., -1.9759e-05,
         -1.9297e-06, -1.2368e-05]], device='cuda:0')
Loss: 1.1993893384933472


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[   0, 4215, 1478,  ...,  644,  409, 3803],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1587e-04,  5.1298e-05, -1.3086e-04,  ...,  4.2953e-06,
          1.1603e-04, -1.0951e-04],
        [-1.5795e-05, -1.0803e-05,  1.6633e-06,  ..., -1.1817e-05,
         -1.0729e-06, -7.3910e-06],
        [-1.2070e-05, -8.2552e-06,  1.2703e-06,  ..., -9.0301e-06,
         -8.1863e-07, -5.6475e-06],
        [-1.9372e-05, -1.3262e-05,  2.0489e-06,  ..., -1.4484e-05,
         -1.3076e-06, -9.0748e-06],
        [-3.5465e-05, -2.4199e-05,  3.7067e-06,  ..., -2.6464e-05,
         -2.4028e-06, -1.6540e-05]], device='cuda:0')
Loss: 1.206943154335022


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,    14,  4494,  ...,  4830,   368,   266],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9180e-04,  6.3050e-05, -1.0894e-04,  ..., -7.4927e-06,
          1.7340e-04, -5.4249e-05],
        [-1.9819e-05, -1.3545e-05,  2.0657e-06,  ..., -1.4782e-05,
         -1.3877e-06, -9.2313e-06],
        [-1.5154e-05, -1.0356e-05,  1.5795e-06,  ..., -1.1310e-05,
         -1.0626e-06, -7.0557e-06],
        [-2.4229e-05, -1.6585e-05,  2.5406e-06,  ..., -1.8075e-05,
         -1.6894e-06, -1.1310e-05],
        [-4.4465e-05, -3.0339e-05,  4.6007e-06,  ..., -3.3110e-05,
         -3.1143e-06, -2.0653e-05]], device='cuda:0')
Loss: 1.2009730339050293


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   593,   300,  ...,   278,  4694,    12],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6252e-04, -1.5974e-06, -9.8920e-05,  ..., -3.2523e-06,
          1.5070e-04, -4.6652e-05],
        [-2.3752e-05, -1.6212e-05,  2.5127e-06,  ..., -1.7717e-05,
         -1.6559e-06, -1.1086e-05],
        [-1.8150e-05, -1.2383e-05,  1.9185e-06,  ..., -1.3545e-05,
         -1.2666e-06, -8.4639e-06],
        [-2.9117e-05, -1.9878e-05,  3.0957e-06,  ..., -2.1711e-05,
         -2.0210e-06, -1.3605e-05],
        [-5.3287e-05, -3.6269e-05,  5.5842e-06,  ..., -3.9667e-05,
         -3.7104e-06, -2.4796e-05]], device='cuda:0')
Loss: 1.1884984970092773


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,    15, 35654,     9],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1631e-04, -4.2706e-05, -9.9773e-05,  ..., -5.8464e-05,
          2.0779e-04, -2.8258e-05],
        [-2.7716e-05, -1.8939e-05,  2.9411e-06,  ..., -2.0638e-05,
         -1.9632e-06, -1.2949e-05],
        [-2.1145e-05, -1.4439e-05,  2.2426e-06,  ..., -1.5751e-05,
         -1.4985e-06, -9.8720e-06],
        [-3.3975e-05, -2.3216e-05,  3.6247e-06,  ..., -2.5287e-05,
         -2.3954e-06, -1.5885e-05],
        [-6.2108e-05, -4.2319e-05,  6.5342e-06,  ..., -4.6164e-05,
         -4.3921e-06, -2.8908e-05]], device='cuda:0')
Loss: 1.1804661750793457


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1529,  354,  ...,  709,  271,  266],
        [   0,  409,  699,  ...,   12,  546,  696]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0439e-04, -5.4602e-05, -1.0455e-04,  ..., -1.0856e-04,
          2.1367e-04, -1.7495e-05],
        [-3.1710e-05, -2.1607e-05,  3.3323e-06,  ..., -2.3559e-05,
         -2.1765e-06, -1.4804e-05],
        [-2.4185e-05, -1.6466e-05,  2.5406e-06,  ..., -1.7986e-05,
         -1.6615e-06, -1.1288e-05],
        [-3.8832e-05, -2.6464e-05,  4.1053e-06,  ..., -2.8849e-05,
         -2.6561e-06, -1.8150e-05],
        [-7.1049e-05, -4.8280e-05,  7.4059e-06,  ..., -5.2720e-05,
         -4.8727e-06, -3.3081e-05]], device='cuda:0')
Loss: 1.2027634382247925
Graident accumulation at epoch 0, step 167, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0117e-04, -1.0803e-04,  1.9240e-05,  ...,  4.1369e-05,
         -1.4666e-04, -4.6381e-05],
        [-2.8547e-05, -2.0048e-05,  4.9806e-06,  ..., -2.3331e-05,
         -1.6615e-06, -1.1227e-05],
        [ 1.3202e-05,  1.2338e-05, -5.4041e-06,  ...,  1.6577e-05,
         -3.9803e-07,  1.1224e-05],
        [ 4.8512e-07,  7.5863e-06,  6.6628e-06,  ...,  3.3714e-06,
          1.0637e-06,  3.1370e-06],
        [-5.3877e-05, -3.7564e-05,  7.9474e-06,  ..., -4.2655e-05,
         -2.5340e-06, -2.1088e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7928e-08, 1.4763e-08, 1.7532e-08,  ..., 2.0069e-08, 3.1884e-08,
         6.5123e-09],
        [2.0659e-11, 1.3920e-11, 9.4047e-13,  ..., 1.3882e-11, 4.9174e-13,
         2.6147e-12],
        [8.9142e-11, 7.5056e-11, 9.7212e-12,  ..., 1.3209e-10, 3.2815e-12,
         4.2973e-11],
        [1.6401e-10, 2.4912e-10, 4.6515e-12,  ..., 1.2311e-10, 6.4290e-12,
         4.4826e-11],
        [7.1538e-11, 4.3763e-11, 4.7060e-12,  ..., 4.9328e-11, 1.3120e-12,
         1.0364e-11]], device='cuda:0')
optimizer state dict: 21.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 21


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,  3860,   694,  ...,  1027,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0254e-06,  8.6121e-06,  2.6851e-06,  ...,  2.9462e-05,
          4.0971e-05, -4.3931e-05],
        [-3.8445e-06, -2.6822e-06,  3.2783e-07,  ..., -2.8610e-06,
         -3.9302e-07, -1.6987e-06],
        [-3.0100e-06, -2.1160e-06,  2.5705e-07,  ..., -2.2501e-06,
         -3.0920e-07, -1.3411e-06],
        [-4.7684e-06, -3.3379e-06,  4.1351e-07,  ..., -3.5763e-06,
         -4.8801e-07, -2.1160e-06],
        [-8.8215e-06, -6.1691e-06,  7.4878e-07,  ..., -6.5863e-06,
         -9.0525e-07, -3.9041e-06]], device='cuda:0')
Loss: 1.1868326663970947


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 6132,  300,  ...,   37,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5626e-06,  2.7106e-05, -6.1008e-07,  ...,  2.8337e-05,
          3.2325e-05, -4.7025e-05],
        [-7.7188e-06, -5.4091e-06,  6.3330e-07,  ..., -5.7220e-06,
         -7.9721e-07, -3.4049e-06],
        [-6.1095e-06, -4.2915e-06,  5.0105e-07,  ..., -4.5449e-06,
         -6.3144e-07, -2.7046e-06],
        [-9.5665e-06, -6.7055e-06,  7.9535e-07,  ..., -7.1079e-06,
         -9.8348e-07, -4.2170e-06],
        [-1.7762e-05, -1.2428e-05,  1.4417e-06,  ..., -1.3173e-05,
         -1.8328e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.1874667406082153


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0,    26,   874,  ...,    12, 21591,    12],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6206e-05,  8.3870e-06,  7.1614e-06,  ...,  1.8497e-05,
          4.6144e-05, -1.7047e-05],
        [-1.1593e-05, -8.0913e-06,  9.2946e-07,  ..., -8.5682e-06,
         -1.1642e-06, -5.1036e-06],
        [-9.1493e-06, -6.4075e-06,  7.3481e-07,  ..., -6.7800e-06,
         -9.2015e-07, -4.0382e-06],
        [-1.4365e-05, -1.0043e-05,  1.1660e-06,  ..., -1.0625e-05,
         -1.4361e-06, -6.3181e-06],
        [-2.6643e-05, -1.8597e-05,  2.1197e-06,  ..., -1.9699e-05,
         -2.6785e-06, -1.1712e-05]], device='cuda:0')
Loss: 1.2072488069534302


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2870e-05,  2.9191e-05, -1.3887e-05,  ...,  1.8497e-05,
          4.8343e-05, -6.2025e-05],
        [-1.5408e-05, -1.0818e-05,  1.2256e-06,  ..., -1.1399e-05,
         -1.5441e-06, -6.7726e-06],
        [-1.2174e-05, -8.5682e-06,  9.7137e-07,  ..., -9.0301e-06,
         -1.2238e-06, -5.3570e-06],
        [-1.9133e-05, -1.3456e-05,  1.5404e-06,  ..., -1.4156e-05,
         -1.9111e-06, -8.3894e-06],
        [-3.5465e-05, -2.4885e-05,  2.7977e-06,  ..., -2.6226e-05,
         -3.5577e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.178568959236145


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[   0,  287, 1070,  ...,  292,  221,  374],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4291e-05,  4.8527e-05, -3.3942e-05,  ...,  3.1267e-05,
          2.5727e-05, -4.9248e-05],
        [-1.9282e-05, -1.3471e-05,  1.5441e-06,  ..., -1.4246e-05,
         -1.9334e-06, -8.4713e-06],
        [-1.5184e-05, -1.0639e-05,  1.2210e-06,  ..., -1.1265e-05,
         -1.5292e-06, -6.6832e-06],
        [-2.3901e-05, -1.6749e-05,  1.9390e-06,  ..., -1.7688e-05,
         -2.3916e-06, -1.0490e-05],
        [-4.4286e-05, -3.0965e-05,  3.5167e-06,  ..., -3.2753e-05,
         -4.4480e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1895503997802734


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0, 4100,   12,  ...,   13, 4710, 1558],
        [   0,  278,  266,  ...,   13, 2853,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2974e-05,  3.5321e-05, -7.0073e-05,  ...,  3.1783e-05,
          5.1399e-05, -9.3465e-05],
        [-2.3127e-05, -1.6153e-05,  1.8738e-06,  ..., -1.7107e-05,
         -2.2799e-06, -1.0177e-05],
        [-1.8194e-05, -1.2740e-05,  1.4799e-06,  ..., -1.3500e-05,
         -1.8012e-06, -8.0243e-06],
        [-2.8700e-05, -2.0102e-05,  2.3544e-06,  ..., -2.1264e-05,
         -2.8238e-06, -1.2621e-05],
        [-5.3048e-05, -3.7074e-05,  4.2617e-06,  ..., -3.9250e-05,
         -5.2378e-06, -2.3305e-05]], device='cuda:0')
Loss: 1.2024768590927124


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   344,  8133,  ...,   368,  1119,  5539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6451e-05,  5.4338e-05, -8.0456e-05,  ...,  4.1369e-05,
          7.5704e-05, -8.9045e-05],
        [-2.6971e-05, -1.8865e-05,  2.1514e-06,  ..., -1.9953e-05,
         -2.6505e-06, -1.1861e-05],
        [-2.1249e-05, -1.4901e-05,  1.6997e-06,  ..., -1.5765e-05,
         -2.0955e-06, -9.3654e-06],
        [-3.3468e-05, -2.3469e-05,  2.7008e-06,  ..., -2.4796e-05,
         -3.2801e-06, -1.4707e-05],
        [-6.1929e-05, -4.3362e-05,  4.8988e-06,  ..., -4.5866e-05,
         -6.0946e-06, -2.7210e-05]], device='cuda:0')
Loss: 1.1963508129119873


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0,   342,  4781,  ...,   630,   940,   271],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2460e-05,  9.1193e-05, -1.2124e-04,  ...,  8.4629e-05,
          1.1605e-04, -9.9893e-05],
        [-3.0875e-05, -2.1577e-05,  2.4568e-06,  ..., -2.2814e-05,
         -3.0212e-06, -1.3545e-05],
        [-2.4319e-05, -1.7032e-05,  1.9399e-06,  ..., -1.8016e-05,
         -2.3860e-06, -1.0684e-05],
        [-3.8296e-05, -2.6807e-05,  3.0808e-06,  ..., -2.8327e-05,
         -3.7346e-06, -1.6779e-05],
        [-7.0870e-05, -4.9561e-05,  5.5917e-06,  ..., -5.2422e-05,
         -6.9402e-06, -3.1054e-05]], device='cuda:0')
Loss: 1.192864179611206
Graident accumulation at epoch 0, step 175, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8030e-04, -8.8105e-05,  5.1918e-06,  ...,  4.5695e-05,
         -1.2039e-04, -5.1733e-05],
        [-2.8779e-05, -2.0201e-05,  4.7282e-06,  ..., -2.3279e-05,
         -1.7975e-06, -1.1458e-05],
        [ 9.4501e-06,  9.4009e-06, -4.6697e-06,  ...,  1.3118e-05,
         -5.9683e-07,  9.0329e-06],
        [-3.3930e-06,  4.1470e-06,  6.3046e-06,  ...,  2.0155e-07,
          5.8390e-07,  1.1454e-06],
        [-5.5576e-05, -3.8764e-05,  7.7118e-06,  ..., -4.3632e-05,
         -2.9746e-06, -2.2084e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7889e-08, 1.4757e-08, 1.7529e-08,  ..., 2.0056e-08, 3.1866e-08,
         6.5158e-09],
        [2.1592e-11, 1.4372e-11, 9.4556e-13,  ..., 1.4388e-11, 5.0037e-13,
         2.7955e-12],
        [8.9644e-11, 7.5271e-11, 9.7153e-12,  ..., 1.3228e-10, 3.2839e-12,
         4.3044e-11],
        [1.6531e-10, 2.4959e-10, 4.6564e-12,  ..., 1.2379e-10, 6.4366e-12,
         4.5063e-11],
        [7.6489e-11, 4.6176e-11, 4.7326e-12,  ..., 5.2027e-11, 1.3588e-12,
         1.1318e-11]], device='cuda:0')
optimizer state dict: 22.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 22


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[   0,  275, 2351,  ...,   14, 4520,   12],
        [   0,  292,  685,  ...,  278, 3281,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8906e-06,  6.1820e-05, -1.5528e-05,  ...,  4.0015e-05,
          5.0260e-06,  6.3885e-06],
        [-3.8147e-06, -2.6822e-06,  2.7567e-07,  ..., -2.8014e-06,
         -4.9174e-07, -1.5721e-06],
        [-3.0696e-06, -2.1607e-06,  2.2352e-07,  ..., -2.2501e-06,
         -3.9488e-07, -1.2666e-06],
        [-4.8578e-06, -3.4273e-06,  3.5390e-07,  ..., -3.5614e-06,
         -6.2212e-07, -1.9968e-06],
        [-8.9407e-06, -6.2883e-06,  6.4075e-07,  ..., -6.5565e-06,
         -1.1474e-06, -3.6806e-06]], device='cuda:0')
Loss: 1.185326337814331


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[   0,  638, 1276,  ..., 1589, 2432,  292],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1501e-05,  6.5380e-05,  5.8137e-06,  ...,  6.7892e-05,
         -3.5623e-07,  2.3930e-05],
        [-7.5698e-06, -5.3793e-06,  5.4017e-07,  ..., -5.6028e-06,
         -9.4622e-07, -3.1590e-06],
        [-6.0946e-06, -4.3213e-06,  4.3586e-07,  ..., -4.5002e-06,
         -7.5996e-07, -2.5406e-06],
        [-9.5963e-06, -6.8247e-06,  6.9104e-07,  ..., -7.0930e-06,
         -1.1921e-06, -3.9935e-06],
        [-1.7703e-05, -1.2547e-05,  1.2517e-06,  ..., -1.3053e-05,
         -2.2054e-06, -7.3612e-06]], device='cuda:0')
Loss: 1.196162223815918


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,   598,   696,  ...,  4048,  1795,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6302e-06,  2.8173e-05,  1.1984e-05,  ...,  1.1120e-04,
          4.7684e-06,  2.3656e-05],
        [-1.1340e-05, -8.0466e-06,  8.4378e-07,  ..., -8.3447e-06,
         -1.4566e-06, -4.7609e-06],
        [-9.1344e-06, -6.4671e-06,  6.7987e-07,  ..., -6.7204e-06,
         -1.1735e-06, -3.8370e-06],
        [-1.4395e-05, -1.0207e-05,  1.0785e-06,  ..., -1.0580e-05,
         -1.8440e-06, -6.0350e-06],
        [-2.6464e-05, -1.8746e-05,  1.9521e-06,  ..., -1.9461e-05,
         -3.3975e-06, -1.1101e-05]], device='cuda:0')
Loss: 1.1771272420883179


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9973e-06,  2.3428e-05,  4.4472e-05,  ...,  1.1456e-04,
          1.1517e-06,  2.3656e-05],
        [-1.5154e-05, -1.0759e-05,  1.1325e-06,  ..., -1.1146e-05,
         -1.9856e-06, -6.3404e-06],
        [-1.2204e-05, -8.6427e-06,  9.1176e-07,  ..., -8.9705e-06,
         -1.5963e-06, -5.1036e-06],
        [-1.9163e-05, -1.3590e-05,  1.4435e-06,  ..., -1.4082e-05,
         -2.5034e-06, -8.0168e-06],
        [-3.5405e-05, -2.5094e-05,  2.6263e-06,  ..., -2.6017e-05,
         -4.6417e-06, -1.4797e-05]], device='cuda:0')
Loss: 1.1710186004638672


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  607,  443,  ...,  259, 2646, 1597]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5888e-05,  5.7092e-05,  4.4717e-05,  ...,  1.4040e-04,
         -2.2960e-05,  5.8514e-05],
        [-1.8880e-05, -1.3426e-05,  1.4175e-06,  ..., -1.3933e-05,
         -2.4661e-06, -7.9051e-06],
        [-1.5214e-05, -1.0803e-05,  1.1437e-06,  ..., -1.1221e-05,
         -1.9837e-06, -6.3702e-06],
        [-2.3872e-05, -1.6958e-05,  1.8068e-06,  ..., -1.7613e-05,
         -3.1069e-06, -9.9987e-06],
        [-4.4167e-05, -3.1352e-05,  3.2894e-06,  ..., -3.2544e-05,
         -5.7667e-06, -1.8463e-05]], device='cuda:0')
Loss: 1.2035144567489624


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 46357, 11101, 10621],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4870e-05,  4.3152e-05,  8.8942e-05,  ...,  1.6923e-04,
         -6.6740e-05,  3.9535e-05],
        [-2.2590e-05, -1.6049e-05,  1.6801e-06,  ..., -1.6674e-05,
         -2.9467e-06, -9.4622e-06],
        [-1.8224e-05, -1.2934e-05,  1.3579e-06,  ..., -1.3441e-05,
         -2.3730e-06, -7.6294e-06],
        [-2.8551e-05, -2.0280e-05,  2.1458e-06,  ..., -2.1085e-05,
         -3.7141e-06, -1.1966e-05],
        [-5.2989e-05, -3.7581e-05,  3.9116e-06,  ..., -3.9041e-05,
         -6.9067e-06, -2.2143e-05]], device='cuda:0')
Loss: 1.1805311441421509


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0842e-04,  3.6956e-05,  7.2812e-05,  ...,  1.7661e-04,
         -7.1566e-05,  7.8525e-05],
        [-2.6360e-05, -1.8686e-05,  1.9651e-06,  ..., -1.9461e-05,
         -3.3937e-06, -1.1057e-05],
        [-2.1264e-05, -1.5065e-05,  1.5870e-06,  ..., -1.5676e-05,
         -2.7306e-06, -8.9109e-06],
        [-3.3319e-05, -2.3618e-05,  2.5090e-06,  ..., -2.4602e-05,
         -4.2766e-06, -1.3977e-05],
        [-6.1870e-05, -4.3780e-05,  4.5784e-06,  ..., -4.5598e-05,
         -7.9572e-06, -2.5883e-05]], device='cuda:0')
Loss: 1.1953017711639404


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3975e-04, -2.0959e-05,  1.0005e-04,  ...,  2.0191e-04,
         -1.2046e-04,  9.5911e-05],
        [-3.0115e-05, -2.1353e-05,  2.2370e-06,  ..., -2.2233e-05,
         -3.8855e-06, -1.2644e-05],
        [-2.4289e-05, -1.7211e-05,  1.8058e-06,  ..., -1.7911e-05,
         -3.1255e-06, -1.0185e-05],
        [-3.8087e-05, -2.7001e-05,  2.8536e-06,  ..., -2.8118e-05,
         -4.8950e-06, -1.5989e-05],
        [-7.0691e-05, -5.0038e-05,  5.2080e-06,  ..., -5.2094e-05,
         -9.1121e-06, -2.9609e-05]], device='cuda:0')
Loss: 1.1927088499069214
Graident accumulation at epoch 0, step 183, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0205],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0169,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3829e-04, -8.1391e-05,  1.4677e-05,  ...,  6.1316e-05,
         -1.2039e-04, -3.6968e-05],
        [-2.8913e-05, -2.0316e-05,  4.4791e-06,  ..., -2.3175e-05,
         -2.0063e-06, -1.1577e-05],
        [ 6.0762e-06,  6.7398e-06, -4.0222e-06,  ...,  1.0015e-05,
         -8.4970e-07,  7.1112e-06],
        [-6.8624e-06,  1.0322e-06,  5.9595e-06,  ..., -2.6305e-06,
          3.6009e-08, -5.6803e-07],
        [-5.7088e-05, -3.9891e-05,  7.4614e-06,  ..., -4.4478e-05,
         -3.5884e-06, -2.2837e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7860e-08, 1.4743e-08, 1.7522e-08,  ..., 2.0077e-08, 3.1848e-08,
         6.5185e-09],
        [2.2477e-11, 1.4813e-11, 9.4962e-13,  ..., 1.4868e-11, 5.1497e-13,
         2.9526e-12],
        [9.0144e-11, 7.5492e-11, 9.7088e-12,  ..., 1.3247e-10, 3.2904e-12,
         4.3105e-11],
        [1.6659e-10, 2.5007e-10, 4.6599e-12,  ..., 1.2446e-10, 6.4541e-12,
         4.5273e-11],
        [8.1410e-11, 4.8633e-11, 4.7550e-12,  ..., 5.4689e-11, 1.4405e-12,
         1.2183e-11]], device='cuda:0')
optimizer state dict: 23.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 23


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5897e-06,  2.0432e-05, -4.4808e-05,  ...,  8.5494e-06,
          1.1981e-05,  6.4582e-05],
        [-3.6955e-06, -2.6524e-06,  2.3376e-07,  ..., -2.7567e-06,
         -4.8429e-07, -1.4752e-06],
        [-3.0398e-06, -2.1905e-06,  1.9185e-07,  ..., -2.2799e-06,
         -4.0047e-07, -1.2144e-06],
        [-4.7684e-06, -3.4273e-06,  3.0175e-07,  ..., -3.5763e-06,
         -6.2585e-07, -1.8999e-06],
        [-8.6427e-06, -6.2287e-06,  5.4389e-07,  ..., -6.4671e-06,
         -1.1399e-06, -3.4571e-06]], device='cuda:0')
Loss: 1.1961733102798462


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  285,  590,  ...,  199,  395, 3523],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8127e-05,  4.6731e-05, -6.1589e-05,  ...,  2.5359e-05,
          6.7051e-05,  8.5494e-05],
        [-7.2867e-06, -5.2601e-06,  4.6380e-07,  ..., -5.5134e-06,
         -1.0543e-06, -2.9728e-06],
        [-6.0499e-06, -4.3660e-06,  3.8370e-07,  ..., -4.5747e-06,
         -8.7544e-07, -2.4661e-06],
        [-9.4175e-06, -6.7800e-06,  6.0350e-07,  ..., -7.1228e-06,
         -1.3560e-06, -3.8221e-06],
        [-1.7166e-05, -1.2428e-05,  1.0878e-06,  ..., -1.3024e-05,
         -2.4959e-06, -7.0035e-06]], device='cuda:0')
Loss: 1.1688002347946167


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0,    12, 30621,  ...,   578,  3126,    14],
        [    0,    13,  5005,  ...,   654,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0650e-05,  5.4490e-05, -4.5134e-05,  ...,  7.5088e-06,
          4.4478e-05,  6.7829e-05],
        [-1.0893e-05, -7.8678e-06,  7.3761e-07,  ..., -8.1807e-06,
         -1.5832e-06, -4.4703e-06],
        [ 8.1688e-05,  4.1083e-05, -1.9835e-05,  ...,  7.1015e-05,
          1.2371e-06,  2.9261e-05],
        [-1.4156e-05, -1.0222e-05,  9.7044e-07,  ..., -1.0625e-05,
         -2.0489e-06, -5.7891e-06],
        [-2.5809e-05, -1.8716e-05,  1.7434e-06,  ..., -1.9431e-05,
         -3.7700e-06, -1.0595e-05]], device='cuda:0')
Loss: 1.1861881017684937


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[    0,  6762,   689,  ...,  7061,    14,   381],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3102e-05,  5.4881e-05, -2.5451e-05,  ...,  2.0332e-05,
          1.6543e-05,  6.8258e-05],
        [-1.4499e-05, -1.0505e-05,  9.7603e-07,  ..., -1.0937e-05,
         -2.1085e-06, -5.9754e-06],
        [ 7.8708e-05,  3.8907e-05, -1.9638e-05,  ...,  6.8735e-05,
          7.9937e-07,  2.8017e-05],
        [-1.8835e-05, -1.3649e-05,  1.2834e-06,  ..., -1.4201e-05,
         -2.7344e-06, -7.7412e-06],
        [-3.4392e-05, -2.4945e-05,  2.3022e-06,  ..., -2.5958e-05,
         -5.0217e-06, -1.4156e-05]], device='cuda:0')
Loss: 1.1804877519607544


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0, 13642, 14635,  ...,   367,  1040,  8580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5161e-06,  3.0876e-05,  1.2735e-05,  ...,  2.7476e-05,
         -1.7839e-05,  6.9329e-05],
        [-1.8165e-05, -1.3173e-05,  1.2219e-06,  ..., -1.3679e-05,
         -2.6040e-06, -7.4580e-06],
        [ 7.5668e-05,  3.6687e-05, -1.9434e-05,  ...,  6.6456e-05,
          3.8773e-07,  2.6780e-05],
        [-2.3544e-05, -1.7092e-05,  1.6037e-06,  ..., -1.7717e-05,
         -3.3714e-06, -9.6485e-06],
        [-4.3154e-05, -3.1322e-05,  2.8871e-06,  ..., -3.2514e-05,
         -6.2138e-06, -1.7703e-05]], device='cuda:0')
Loss: 1.1847681999206543


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8981e-06,  4.4135e-05,  3.2822e-06,  ...,  3.2232e-05,
          1.3334e-05,  7.8131e-05],
        [-2.1785e-05, -1.5795e-05,  1.5050e-06,  ..., -1.6406e-05,
         -3.1814e-06, -8.9779e-06],
        [ 7.2628e-05,  3.4482e-05, -1.9193e-05,  ...,  6.4161e-05,
         -1.0028e-07,  2.5506e-05],
        [-2.8253e-05, -2.0489e-05,  1.9763e-06,  ..., -2.1264e-05,
         -4.1202e-06, -1.1615e-05],
        [-5.1856e-05, -3.7611e-05,  3.5651e-06,  ..., -3.9071e-05,
         -7.6070e-06, -2.1338e-05]], device='cuda:0')
Loss: 1.192232608795166


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[    0, 21178,  1952,  ..., 14930,     9,   689],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4440e-05,  3.0932e-05,  2.3387e-06,  ...,  1.8868e-05,
         -4.3210e-05,  9.1885e-05],
        [-2.5392e-05, -1.8418e-05,  1.7714e-06,  ..., -1.9133e-05,
         -3.6769e-06, -1.0461e-05],
        [ 6.9603e-05,  3.2276e-05, -1.8967e-05,  ...,  6.1866e-05,
         -5.1938e-07,  2.4254e-05],
        [-3.2932e-05, -2.3887e-05,  2.3246e-06,  ..., -2.4796e-05,
         -4.7646e-06, -1.3538e-05],
        [-6.0439e-05, -4.3869e-05,  4.1984e-06,  ..., -4.5568e-05,
         -8.7991e-06, -2.4870e-05]], device='cuda:0')
Loss: 1.1841775178909302


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,     9,   391,  ...,   300,  2646,  1717],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7947e-05,  2.8131e-05, -3.0413e-05,  ...,  3.0388e-05,
         -4.2604e-05,  6.5186e-05],
        [-2.9057e-05, -2.1040e-05,  2.0191e-06,  ..., -2.1890e-05,
         -4.2096e-06, -1.1943e-05],
        [ 6.6549e-05,  3.0086e-05, -1.8760e-05,  ...,  5.9571e-05,
         -9.6269e-07,  2.3017e-05],
        [-3.7670e-05, -2.7269e-05,  2.6468e-06,  ..., -2.8357e-05,
         -5.4501e-06, -1.5460e-05],
        [-6.9141e-05, -5.0098e-05,  4.7795e-06,  ..., -5.2124e-05,
         -1.0066e-05, -2.8372e-05]], device='cuda:0')
Loss: 1.1895171403884888
Graident accumulation at epoch 0, step 191, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1167e-04, -7.0438e-05,  1.0168e-05,  ...,  5.8223e-05,
         -1.1262e-04, -2.6753e-05],
        [-2.8927e-05, -2.0389e-05,  4.2331e-06,  ..., -2.3046e-05,
         -2.2266e-06, -1.1614e-05],
        [ 1.2123e-05,  9.0744e-06, -5.4960e-06,  ...,  1.4971e-05,
         -8.6100e-07,  8.7018e-06],
        [-9.9432e-06, -1.7980e-06,  5.6282e-06,  ..., -5.2031e-06,
         -5.1260e-07, -2.0572e-06],
        [-5.8293e-05, -4.0912e-05,  7.1932e-06,  ..., -4.5243e-05,
         -4.2361e-06, -2.3390e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7813e-08, 1.4729e-08, 1.7505e-08,  ..., 2.0058e-08, 3.1818e-08,
         6.5162e-09],
        [2.3299e-11, 1.5241e-11, 9.5275e-13,  ..., 1.5332e-11, 5.3218e-13,
         3.0923e-12],
        [9.4483e-11, 7.6321e-11, 1.0051e-11,  ..., 1.3589e-10, 3.2880e-12,
         4.3592e-11],
        [1.6785e-10, 2.5056e-10, 4.6622e-12,  ..., 1.2514e-10, 6.4773e-12,
         4.5467e-11],
        [8.6109e-11, 5.1095e-11, 4.7731e-12,  ..., 5.7351e-11, 1.5404e-12,
         1.2976e-11]], device='cuda:0')
optimizer state dict: 24.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 24


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  6541,   287,  ...,  1061,  4786,   292],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9612e-05, -2.1825e-05, -1.3687e-05,  ..., -1.9814e-05,
         -7.9619e-06, -7.1455e-06],
        [-3.5018e-06, -2.5779e-06,  2.4214e-07,  ..., -2.6822e-06,
         -5.6252e-07, -1.4007e-06],
        [-3.0249e-06, -2.2352e-06,  2.0955e-07,  ..., -2.3246e-06,
         -4.8801e-07, -1.2144e-06],
        [-4.6790e-06, -3.4422e-06,  3.2596e-07,  ..., -3.5763e-06,
         -7.4878e-07, -1.8701e-06],
        [-8.4043e-06, -6.1989e-06,  5.7742e-07,  ..., -6.4671e-06,
         -1.3560e-06, -3.3528e-06]], device='cuda:0')
Loss: 1.1734110116958618


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[    0,   199,   769,  ..., 12038, 15317,   342],
        [    0,  1832,   292,  ...,  2176,  1345,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6031e-05, -2.7517e-05, -3.3611e-05,  ..., -8.0495e-05,
          1.5058e-05, -3.0440e-05],
        [-7.0333e-06, -5.1260e-06,  4.3493e-07,  ..., -5.4538e-06,
         -1.0617e-06, -2.7940e-06],
        [-6.0350e-06, -4.4107e-06,  3.7439e-07,  ..., -4.6939e-06,
         -9.1456e-07, -2.4065e-06],
        [-9.3877e-06, -6.8396e-06,  5.8673e-07,  ..., -7.2718e-06,
         -1.4156e-06, -3.7327e-06],
        [-1.6868e-05, -1.2308e-05,  1.0356e-06,  ..., -1.3113e-05,
         -2.5630e-06, -6.7055e-06]], device='cuda:0')
Loss: 1.2009117603302002


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[    0, 14949,    12,  ...,   669, 10168,  7166],
        [    0,   401,   266,  ...,   266,  2236,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2968e-05, -8.7797e-05, -4.3691e-05,  ..., -3.0296e-05,
          1.4921e-05,  1.9802e-05],
        [-1.0565e-05, -7.7337e-06,  6.6031e-07,  ..., -8.1360e-06,
         -1.6093e-06, -4.1574e-06],
        [-9.0897e-06, -6.6757e-06,  5.6904e-07,  ..., -7.0184e-06,
         -1.3895e-06, -3.5837e-06],
        [-1.4067e-05, -1.0282e-05,  8.8662e-07,  ..., -1.0833e-05,
         -2.1383e-06, -5.5358e-06],
        [-2.5392e-05, -1.8597e-05,  1.5758e-06,  ..., -1.9610e-05,
         -3.8818e-06, -9.9838e-06]], device='cuda:0')
Loss: 1.1856335401535034


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,   546,   360,  ...,  9107,  2772,  4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1596e-05, -1.0826e-04, -2.1091e-05,  ..., -3.6093e-05,
          4.0086e-05,  4.4239e-05],
        [-1.4096e-05, -1.0312e-05,  8.7824e-07,  ..., -1.0908e-05,
         -2.1420e-06, -5.5805e-06],
        [-1.2130e-05, -8.8960e-06,  7.5623e-07,  ..., -9.4026e-06,
         -1.8459e-06, -4.8056e-06],
        [-1.8716e-05, -1.3679e-05,  1.1791e-06,  ..., -1.4484e-05,
         -2.8349e-06, -7.4059e-06],
        [-3.3855e-05, -2.4796e-05,  2.0973e-06,  ..., -2.6256e-05,
         -5.1558e-06, -1.3396e-05]], device='cuda:0')
Loss: 1.186348557472229


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  7094,   596,  ...,  4764,  9514,    14],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7323e-06, -1.2795e-04,  8.4216e-06,  ...,  2.4690e-05,
          1.9435e-05,  5.0526e-05],
        [-1.7613e-05, -1.2890e-05,  1.1185e-06,  ..., -1.3590e-05,
         -2.6748e-06, -7.0035e-06],
        [-1.5184e-05, -1.1131e-05,  9.6485e-07,  ..., -1.1742e-05,
         -2.3078e-06, -6.0424e-06],
        [-2.3425e-05, -1.7136e-05,  1.5032e-06,  ..., -1.8090e-05,
         -3.5465e-06, -9.3132e-06],
        [-4.2379e-05, -3.1024e-05,  2.6748e-06,  ..., -3.2753e-05,
         -6.4448e-06, -1.6838e-05]], device='cuda:0')
Loss: 1.1965008974075317


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,  360, 2374,  ...,  221,  474,  357],
        [   0, 4882,   12,  ...,   12, 9575,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9083e-06, -1.3649e-04,  5.0368e-05,  ...,  5.5248e-05,
         -1.2919e-05,  2.8875e-05],
        [-2.1145e-05, -1.5408e-05,  1.3607e-06,  ..., -1.6302e-05,
         -3.1851e-06, -8.4043e-06],
        [-1.8224e-05, -1.3307e-05,  1.1753e-06,  ..., -1.4082e-05,
         -2.7474e-06, -7.2494e-06],
        [-2.8133e-05, -2.0504e-05,  1.8273e-06,  ..., -2.1711e-05,
         -4.2282e-06, -1.1183e-05],
        [-5.0902e-05, -3.7104e-05,  3.2559e-06,  ..., -3.9309e-05,
         -7.6815e-06, -2.0221e-05]], device='cuda:0')
Loss: 1.201375961303711


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  2261,     9,  ..., 15008,    14,   333],
        [    0,  2973,    30,  ...,   408,   259,  1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1621e-06, -2.1133e-04,  2.4885e-05,  ...,  8.0611e-05,
         -2.9067e-05,  2.6069e-05],
        [-2.4617e-05, -1.7971e-05,  1.5786e-06,  ..., -1.8984e-05,
         -3.7178e-06, -9.7826e-06],
        [-2.1234e-05, -1.5527e-05,  1.3644e-06,  ..., -1.6406e-05,
         -3.2112e-06, -8.4415e-06],
        [-3.2783e-05, -2.3931e-05,  2.1216e-06,  ..., -2.5302e-05,
         -4.9435e-06, -1.3024e-05],
        [-5.9307e-05, -4.3273e-05,  3.7774e-06,  ..., -4.5776e-05,
         -8.9705e-06, -2.3529e-05]], device='cuda:0')
Loss: 1.1822444200515747


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[   0, 5281, 4452,  ...,   14, 3391,   12],
        [   0, 2220, 1110,  ...,  382,   18,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6807e-05, -2.0492e-04,  6.0657e-05,  ...,  8.8239e-05,
         -3.6593e-05,  7.9814e-05],
        [-2.8074e-05, -2.0534e-05,  1.8170e-06,  ..., -2.1666e-05,
         -4.2394e-06, -1.1168e-05],
        [-2.4259e-05, -1.7762e-05,  1.5730e-06,  ..., -1.8746e-05,
         -3.6675e-06, -9.6560e-06],
        [-3.7432e-05, -2.7373e-05,  2.4457e-06,  ..., -2.8893e-05,
         -5.6438e-06, -1.4894e-05],
        [-6.7711e-05, -4.9502e-05,  4.3549e-06,  ..., -5.2303e-05,
         -1.0245e-05, -2.6911e-05]], device='cuda:0')
Loss: 1.1772549152374268
Graident accumulation at epoch 0, step 199, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0143, -0.0268,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8382e-04, -8.3887e-05,  1.5217e-05,  ...,  6.1225e-05,
         -1.0501e-04, -1.6096e-05],
        [-2.8842e-05, -2.0403e-05,  3.9915e-06,  ..., -2.2908e-05,
         -2.4279e-06, -1.1569e-05],
        [ 8.4852e-06,  6.3907e-06, -4.7891e-06,  ...,  1.1599e-05,
         -1.1417e-06,  6.8660e-06],
        [-1.2692e-05, -4.3555e-06,  5.3100e-06,  ..., -7.5721e-06,
         -1.0257e-06, -3.3409e-06],
        [-5.9235e-05, -4.1771e-05,  6.9094e-06,  ..., -4.5949e-05,
         -4.8370e-06, -2.3742e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7770e-08, 1.4756e-08, 1.7491e-08,  ..., 2.0046e-08, 3.1788e-08,
         6.5161e-09],
        [2.4064e-11, 1.5648e-11, 9.5510e-13,  ..., 1.5786e-11, 5.4962e-13,
         3.2139e-12],
        [9.4977e-11, 7.6561e-11, 1.0043e-11,  ..., 1.3610e-10, 3.2982e-12,
         4.3641e-11],
        [1.6908e-10, 2.5106e-10, 4.6635e-12,  ..., 1.2585e-10, 6.5027e-12,
         4.5643e-11],
        [9.0608e-11, 5.3494e-11, 4.7873e-12,  ..., 6.0029e-11, 1.6438e-12,
         1.3687e-11]], device='cuda:0')
optimizer state dict: 25.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 25


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7458e-05, -1.3071e-05, -2.7398e-05,  ..., -2.6791e-06,
         -1.2295e-05,  1.5044e-05],
        [-3.3975e-06, -2.4736e-06,  2.1607e-07,  ..., -2.6673e-06,
         -5.0664e-07, -1.3188e-06],
        [-3.0398e-06, -2.2203e-06,  1.9185e-07,  ..., -2.3842e-06,
         -4.5449e-07, -1.1772e-06],
        [-4.6492e-06, -3.3826e-06,  2.9616e-07,  ..., -3.6508e-06,
         -6.9290e-07, -1.7956e-06],
        [-8.2850e-06, -6.0201e-06,  5.1782e-07,  ..., -6.4969e-06,
         -1.2368e-06, -3.1888e-06]], device='cuda:0')
Loss: 1.1775978803634644


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,    43,   503,    67],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1416e-05, -3.1567e-05, -1.2852e-05,  ...,  2.1968e-05,
         -8.5058e-06,  2.6324e-06],
        [-6.7800e-06, -4.9621e-06,  4.2003e-07,  ..., -5.3346e-06,
         -9.9465e-07, -2.6152e-06],
        [-6.0648e-06, -4.4405e-06,  3.7439e-07,  ..., -4.7684e-06,
         -8.9221e-07, -2.3395e-06],
        [-9.2983e-06, -6.7949e-06,  5.7742e-07,  ..., -7.3165e-06,
         -1.3672e-06, -3.5763e-06],
        [-1.6510e-05, -1.2070e-05,  1.0096e-06,  ..., -1.2994e-05,
         -2.4363e-06, -6.3479e-06]], device='cuda:0')
Loss: 1.182523488998413


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  2377,   360,  ...,   266,  4745,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0679e-04, -3.6875e-05,  1.4766e-05,  ...,  5.3404e-05,
         -6.6824e-05, -3.4175e-05],
        [-1.0163e-05, -7.4208e-06,  6.5099e-07,  ..., -7.9721e-06,
         -1.4789e-06, -3.9116e-06],
        [-9.0599e-06, -6.6310e-06,  5.7928e-07,  ..., -7.1079e-06,
         -1.3225e-06, -3.4943e-06],
        [-1.4007e-05, -1.0222e-05,  8.9966e-07,  ..., -1.0982e-05,
         -2.0377e-06, -5.3868e-06],
        [-2.4796e-05, -1.8120e-05,  1.5721e-06,  ..., -1.9461e-05,
         -3.6284e-06, -9.5218e-06]], device='cuda:0')
Loss: 1.1994370222091675


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3905e-05, -2.5417e-05, -1.4667e-05,  ...,  7.0150e-05,
         -5.8909e-05, -3.5685e-05],
        [-1.3515e-05, -9.9093e-06,  8.6613e-07,  ..., -1.0639e-05,
         -2.0154e-06, -5.2005e-06],
        [-1.2070e-05, -8.8662e-06,  7.7300e-07,  ..., -9.5069e-06,
         -1.8030e-06, -4.6566e-06],
        [-1.8597e-05, -1.3620e-05,  1.1977e-06,  ..., -1.4618e-05,
         -2.7679e-06, -7.1526e-06],
        [-3.2961e-05, -2.4170e-05,  2.0936e-06,  ..., -2.5958e-05,
         -4.9323e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.1866326332092285


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8877e-05, -3.8395e-06, -6.3195e-06,  ...,  3.9970e-05,
         -5.0959e-05, -2.0407e-05],
        [-1.6928e-05, -1.2398e-05,  1.0487e-06,  ..., -1.3307e-05,
         -2.5295e-06, -6.4895e-06],
        [-1.5110e-05, -1.1072e-05,  9.3505e-07,  ..., -1.1876e-05,
         -2.2613e-06, -5.8040e-06],
        [-2.3276e-05, -1.7032e-05,  1.4529e-06,  ..., -1.8284e-05,
         -3.4757e-06, -8.9258e-06],
        [-4.1306e-05, -3.0220e-05,  2.5369e-06,  ..., -3.2485e-05,
         -6.1914e-06, -1.5810e-05]], device='cuda:0')
Loss: 1.1804741621017456


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0,  300,  369,  ...,   12,  970,   12],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4561e-05,  3.4482e-05, -3.2190e-06,  ...,  1.2186e-05,
         -5.7242e-05, -4.0245e-06],
        [-2.0310e-05, -1.4827e-05,  1.2275e-06,  ..., -1.5974e-05,
         -3.0398e-06, -7.7859e-06],
        [-1.8120e-05, -1.3232e-05,  1.0934e-06,  ..., -1.4246e-05,
         -2.7157e-06, -6.9514e-06],
        [-2.7895e-05, -2.0340e-05,  1.6969e-06,  ..., -2.1935e-05,
         -4.1723e-06, -1.0692e-05],
        [-4.9472e-05, -3.6091e-05,  2.9635e-06,  ..., -3.8952e-05,
         -7.4282e-06, -1.8939e-05]], device='cuda:0')
Loss: 1.1847960948944092


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0,  271, 3421,  ...,  306,  472,  346],
        [   0,  221, 4070,  ..., 1061, 3189,   26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2800e-05,  5.3747e-05, -9.2270e-07,  ...,  1.9212e-05,
         -5.0331e-05, -1.4134e-05],
        [-2.3708e-05, -1.7300e-05,  1.4529e-06,  ..., -1.8626e-05,
         -3.5465e-06, -9.0897e-06],
        [-2.1145e-05, -1.5438e-05,  1.2945e-06,  ..., -1.6600e-05,
         -3.1665e-06, -8.1137e-06],
        [-3.2544e-05, -2.3738e-05,  2.0098e-06,  ..., -2.5570e-05,
         -4.8652e-06, -1.2480e-05],
        [-5.7697e-05, -4.2111e-05,  3.5074e-06,  ..., -4.5359e-05,
         -8.6576e-06, -2.2098e-05]], device='cuda:0')
Loss: 1.1835911273956299


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2951e-04,  4.0086e-05, -9.6558e-06,  ...,  3.9362e-05,
         -8.3695e-05, -2.6008e-05],
        [-2.7031e-05, -1.9759e-05,  1.6931e-06,  ..., -2.1249e-05,
         -4.1090e-06, -1.0379e-05],
        [-2.4155e-05, -1.7658e-05,  1.5125e-06,  ..., -1.8969e-05,
         -3.6769e-06, -9.2760e-06],
        [-3.7134e-05, -2.7135e-05,  2.3451e-06,  ..., -2.9191e-05,
         -5.6401e-06, -1.4253e-05],
        [-6.5804e-05, -4.8071e-05,  4.0885e-06,  ..., -5.1707e-05,
         -1.0028e-05, -2.5213e-05]], device='cuda:0')
Loss: 1.176927924156189
Graident accumulation at epoch 0, step 207, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0152,  0.0037,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0295, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5249e-04, -7.1490e-05,  1.2730e-05,  ...,  5.9038e-05,
         -1.0288e-04, -1.7087e-05],
        [-2.8661e-05, -2.0339e-05,  3.7616e-06,  ..., -2.2742e-05,
         -2.5960e-06, -1.1450e-05],
        [ 5.2212e-06,  3.9858e-06, -4.1589e-06,  ...,  8.5423e-06,
         -1.3952e-06,  5.2518e-06],
        [-1.5136e-05, -6.6335e-06,  5.0135e-06,  ..., -9.7340e-06,
         -1.4872e-06, -4.4321e-06],
        [-5.9892e-05, -4.2401e-05,  6.6273e-06,  ..., -4.6525e-05,
         -5.3561e-06, -2.3889e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7739e-08, 1.4743e-08, 1.7474e-08,  ..., 2.0027e-08, 3.1763e-08,
         6.5102e-09],
        [2.4770e-11, 1.6022e-11, 9.5701e-13,  ..., 1.6222e-11, 5.6595e-13,
         3.3184e-12],
        [9.5465e-11, 7.6796e-11, 1.0036e-11,  ..., 1.3633e-10, 3.3084e-12,
         4.3684e-11],
        [1.7029e-10, 2.5155e-10, 4.6644e-12,  ..., 1.2658e-10, 6.5280e-12,
         4.5801e-11],
        [9.4847e-11, 5.5751e-11, 4.7992e-12,  ..., 6.2643e-11, 1.7427e-12,
         1.4309e-11]], device='cuda:0')
optimizer state dict: 26.0
lr: [1.8547933878823103e-05, 1.8547933878823103e-05]
scheduler_last_epoch: 26


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2394e-05,  1.4589e-05, -9.8311e-06,  ...,  3.7872e-05,
          3.5281e-06,  6.5826e-05],
        [-3.3081e-06, -2.3991e-06,  2.0955e-07,  ..., -2.5779e-06,
         -4.6380e-07, -1.2368e-06],
        [-3.0547e-06, -2.2054e-06,  1.9278e-07,  ..., -2.3842e-06,
         -4.2841e-07, -1.1399e-06],
        [-4.7088e-06, -3.4124e-06,  2.9989e-07,  ..., -3.6806e-06,
         -6.5938e-07, -1.7658e-06],
        [-8.1658e-06, -5.9009e-06,  5.1036e-07,  ..., -6.3479e-06,
         -1.1474e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.1630827188491821


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4745e-05,  3.1384e-05, -5.0185e-06,  ...,  3.3302e-05,
         -6.1851e-06,  6.8851e-05],
        [-6.6012e-06, -4.7386e-06,  4.2934e-07,  ..., -5.1856e-06,
         -9.0525e-07, -2.4661e-06],
        [-6.1095e-06, -4.3809e-06,  3.9674e-07,  ..., -4.8131e-06,
         -8.3819e-07, -2.2799e-06],
        [-9.3877e-06, -6.7353e-06,  6.1654e-07,  ..., -7.4059e-06,
         -1.2890e-06, -3.5167e-06],
        [-1.6332e-05, -1.1683e-05,  1.0505e-06,  ..., -1.2815e-05,
         -2.2426e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.1868629455566406


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0,     9,   870,  ...,  2671,   965,  3229]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0785e-05,  1.6362e-05,  2.2444e-05,  ...,  7.5872e-05,
         -3.1241e-05,  8.1206e-05],
        [-9.8944e-06, -7.1079e-06,  6.4634e-07,  ..., -7.7784e-06,
         -1.3709e-06, -3.6955e-06],
        [-9.1493e-06, -6.5714e-06,  5.9698e-07,  ..., -7.2122e-06,
         -1.2685e-06, -3.4124e-06],
        [-1.4126e-05, -1.0148e-05,  9.3132e-07,  ..., -1.1146e-05,
         -1.9595e-06, -5.2825e-06],
        [-2.4498e-05, -1.7583e-05,  1.5870e-06,  ..., -1.9282e-05,
         -3.4049e-06, -9.1195e-06]], device='cuda:0')
Loss: 1.1745918989181519


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  2314,   516,  ...,  1871,    13,  1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5551e-05,  7.6321e-07, -2.0028e-05,  ...,  1.3064e-04,
         -3.4294e-05,  8.9869e-05],
        [-1.3158e-05, -9.4622e-06,  8.7265e-07,  ..., -1.0401e-05,
         -1.8012e-06, -4.9099e-06],
        [-1.2159e-05, -8.7470e-06,  8.0653e-07,  ..., -9.6411e-06,
         -1.6652e-06, -4.5300e-06],
        [-1.8805e-05, -1.3515e-05,  1.2591e-06,  ..., -1.4901e-05,
         -2.5742e-06, -7.0184e-06],
        [-3.2425e-05, -2.3305e-05,  2.1346e-06,  ..., -2.5660e-05,
         -4.4554e-06, -1.2070e-05]], device='cuda:0')
Loss: 1.1755683422088623


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0, 1358,  367,  ..., 1758, 2921,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7875e-05, -1.4144e-05, -1.6598e-05,  ...,  1.3763e-04,
         -3.4811e-05,  1.1233e-04],
        [-1.6466e-05, -1.1832e-05,  1.0692e-06,  ..., -1.3009e-05,
         -2.2314e-06, -6.1318e-06],
        [-1.5169e-05, -1.0908e-05,  9.8534e-07,  ..., -1.2025e-05,
         -2.0582e-06, -5.6475e-06],
        [-2.3484e-05, -1.6868e-05,  1.5404e-06,  ..., -1.8582e-05,
         -3.1814e-06, -8.7470e-06],
        [-4.0531e-05, -2.9117e-05,  2.6152e-06,  ..., -3.2037e-05,
         -5.5134e-06, -1.5050e-05]], device='cuda:0')
Loss: 1.1848788261413574


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0,  1746,    14,  ...,  3134,  5968,     9],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9464e-05, -4.5877e-06, -3.5278e-05,  ...,  1.4557e-04,
         -2.5280e-05,  1.1690e-04],
        [-1.9774e-05, -1.4246e-05,  1.3076e-06,  ..., -1.5631e-05,
         -2.6971e-06, -7.3835e-06],
        [ 6.7528e-05,  5.4791e-05,  6.2583e-06,  ...,  6.9752e-05,
          8.7394e-06,  2.5928e-05],
        [-2.8133e-05, -2.0266e-05,  1.8775e-06,  ..., -2.2277e-05,
         -3.8370e-06, -1.0513e-05],
        [-4.8637e-05, -3.5018e-05,  3.1963e-06,  ..., -3.8445e-05,
         -6.6534e-06, -1.8105e-05]], device='cuda:0')
Loss: 1.1785171031951904


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0, 16803,   965,  ..., 36064,    12, 13769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4706e-05,  1.6157e-05, -2.4909e-06,  ...,  1.6792e-04,
         -3.9202e-05,  1.3072e-04],
        [-2.3082e-05, -1.6645e-05,  1.5264e-06,  ..., -1.8239e-05,
         -3.1516e-06, -8.6278e-06],
        [ 6.4488e-05,  5.2586e-05,  6.4604e-06,  ...,  6.7353e-05,
          8.3203e-06,  2.4788e-05],
        [-3.2842e-05, -2.3693e-05,  2.1923e-06,  ..., -2.5988e-05,
         -4.4852e-06, -1.2279e-05],
        [-5.6803e-05, -4.0948e-05,  3.7327e-06,  ..., -4.4882e-05,
         -7.7784e-06, -2.1160e-05]], device='cuda:0')
Loss: 1.178139090538025


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8428e-05,  1.6448e-05, -2.6131e-05,  ...,  1.5375e-04,
         -5.5764e-05,  1.2388e-04],
        [-2.6420e-05, -1.9029e-05,  1.7276e-06,  ..., -2.0832e-05,
         -3.6173e-06, -9.8795e-06],
        [ 6.1404e-05,  5.0381e-05,  6.6485e-06,  ...,  6.4954e-05,
          7.8882e-06,  2.3625e-05],
        [-3.7551e-05, -2.7061e-05,  2.4792e-06,  ..., -2.9653e-05,
         -5.1446e-06, -1.4044e-05],
        [-6.5029e-05, -4.6819e-05,  4.2282e-06,  ..., -5.1260e-05,
         -8.9332e-06, -2.4229e-05]], device='cuda:0')
Loss: 1.166017770767212
Graident accumulation at epoch 0, step 215, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0223, -0.0204],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3240e-04, -6.2696e-05,  8.8439e-06,  ...,  6.8510e-05,
         -9.8170e-05, -2.9906e-06],
        [-2.8437e-05, -2.0208e-05,  3.5582e-06,  ..., -2.2551e-05,
         -2.6981e-06, -1.1293e-05],
        [ 1.0839e-05,  8.6253e-06, -3.0782e-06,  ...,  1.4183e-05,
         -4.6684e-07,  7.0892e-06],
        [-1.7378e-05, -8.6762e-06,  4.7601e-06,  ..., -1.1726e-05,
         -1.8529e-06, -5.3933e-06],
        [-6.0405e-05, -4.2843e-05,  6.3874e-06,  ..., -4.6998e-05,
         -5.7138e-06, -2.3923e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7693e-08, 1.4728e-08, 1.7457e-08,  ..., 2.0031e-08, 3.1734e-08,
         6.5191e-09],
        [2.5444e-11, 1.6368e-11, 9.5904e-13,  ..., 1.6640e-11, 5.7847e-13,
         3.4127e-12],
        [9.9140e-11, 7.9257e-11, 1.0070e-11,  ..., 1.4041e-10, 3.3673e-12,
         4.4198e-11],
        [1.7153e-10, 2.5203e-10, 4.6658e-12,  ..., 1.2733e-10, 6.5480e-12,
         4.5952e-11],
        [9.8981e-11, 5.7888e-11, 4.8123e-12,  ..., 6.5208e-11, 1.8208e-12,
         1.4882e-11]], device='cuda:0')
optimizer state dict: 27.0
lr: [1.8417028402436446e-05, 1.8417028402436446e-05]
scheduler_last_epoch: 27


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[   0,  275, 2101,  ..., 1145,  590, 1619],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2542e-06,  6.2477e-06, -4.9526e-05,  ...,  2.5470e-05,
          2.4260e-06,  4.8321e-06],
        [-3.2037e-06, -2.2650e-06,  2.0117e-07,  ..., -2.5779e-06,
         -3.7439e-07, -1.2219e-06],
        [-3.0249e-06, -2.1458e-06,  1.8906e-07,  ..., -2.4438e-06,
         -3.5390e-07, -1.1548e-06],
        [-4.7088e-06, -3.3379e-06,  2.9616e-07,  ..., -3.7849e-06,
         -5.4762e-07, -1.7956e-06],
        [-7.8678e-06, -5.5730e-06,  4.9174e-07,  ..., -6.3479e-06,
         -9.2387e-07, -2.9951e-06]], device='cuda:0')
Loss: 1.1725831031799316


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[   0, 1862,  674,  ...,  391,  266, 7688],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4439e-05, -7.3664e-06, -3.5240e-05,  ...,  1.5146e-05,
          9.2880e-06, -2.0322e-05],
        [-6.4075e-06, -4.5896e-06,  4.3306e-07,  ..., -5.1558e-06,
         -7.7672e-07, -2.3842e-06],
        [-6.0648e-06, -4.3511e-06,  4.1071e-07,  ..., -4.9025e-06,
         -7.3574e-07, -2.2575e-06],
        [-9.4175e-06, -6.7651e-06,  6.4261e-07,  ..., -7.5996e-06,
         -1.1399e-06, -3.5018e-06],
        [-1.5736e-05, -1.1295e-05,  1.0617e-06,  ..., -1.2696e-05,
         -1.9073e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.187293529510498


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8564e-05, -3.5610e-05, -3.6359e-05,  ...,  1.7593e-05,
          6.2695e-06,  1.2379e-06],
        [-9.6112e-06, -6.9141e-06,  6.8080e-07,  ..., -7.7337e-06,
         -1.1623e-06, -3.5986e-06],
        [ 1.1409e-04,  5.4733e-05, -8.2959e-06,  ...,  7.9366e-05,
          7.9505e-06,  2.6104e-05],
        [-1.4096e-05, -1.0177e-05,  1.0096e-06,  ..., -1.1370e-05,
         -1.7025e-06, -5.2750e-06],
        [-2.3603e-05, -1.7017e-05,  1.6689e-06,  ..., -1.9014e-05,
         -2.8536e-06, -8.8066e-06]], device='cuda:0')
Loss: 1.1773284673690796


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,  6491,  3667,  ...,  5042,    14,  2152]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8372e-05, -3.3896e-05,  1.4849e-05,  ...,  2.0869e-05,
         -5.9620e-06, -3.1323e-06],
        [-1.2845e-05, -9.2089e-06,  8.8476e-07,  ..., -1.0327e-05,
         -1.5795e-06, -4.7833e-06],
        [ 1.1101e-04,  5.2543e-05, -8.1004e-06,  ...,  7.6907e-05,
          7.5519e-06,  2.4971e-05],
        [-1.8865e-05, -1.3560e-05,  1.3132e-06,  ..., -1.5184e-05,
         -2.3171e-06, -7.0259e-06],
        [-3.1590e-05, -2.2680e-05,  2.1681e-06,  ..., -2.5392e-05,
         -3.8818e-06, -1.1727e-05]], device='cuda:0')
Loss: 1.1980828046798706


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9401e-05, -5.1654e-05,  4.6230e-05,  ...,  2.8179e-05,
         -1.0000e-05, -3.0310e-05],
        [-1.6049e-05, -1.1519e-05,  1.1064e-06,  ..., -1.2919e-05,
         -1.9465e-06, -5.9456e-06],
        [ 1.0800e-04,  5.0367e-05, -7.8908e-06,  ...,  7.4464e-05,
          7.2073e-06,  2.3876e-05],
        [-2.3574e-05, -1.6958e-05,  1.6410e-06,  ..., -1.8999e-05,
         -2.8536e-06, -8.7321e-06],
        [-3.9458e-05, -2.8372e-05,  2.7083e-06,  ..., -3.1769e-05,
         -4.7833e-06, -1.4573e-05]], device='cuda:0')
Loss: 1.1806581020355225


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0886e-05, -4.0924e-05,  1.9937e-05,  ...,  3.6617e-05,
         -9.4429e-06,  2.7076e-05],
        [-1.9312e-05, -1.3843e-05,  1.3430e-06,  ..., -1.5542e-05,
         -2.3525e-06, -7.1675e-06],
        [ 1.0491e-04,  4.8162e-05, -7.6664e-06,  ...,  7.1990e-05,
          6.8236e-06,  2.2728e-05],
        [-2.8342e-05, -2.0370e-05,  1.9893e-06,  ..., -2.2843e-05,
         -3.4459e-06, -1.0513e-05],
        [-4.7505e-05, -3.4094e-05,  3.2857e-06,  ..., -3.8207e-05,
         -5.7817e-06, -1.7554e-05]], device='cuda:0')
Loss: 1.1725680828094482


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0, 5982, 9385,  ...,   26,  469,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2041e-05, -1.7287e-05,  3.5558e-05,  ...,  4.4578e-05,
         -2.6075e-05,  4.5859e-05],
        [-2.2516e-05, -1.6153e-05,  1.5460e-06,  ..., -1.8150e-05,
         -2.7586e-06, -8.3521e-06],
        [ 1.0189e-04,  4.5986e-05, -7.4755e-06,  ...,  6.9531e-05,
          6.4399e-06,  2.1611e-05],
        [-3.2991e-05, -2.3708e-05,  2.2855e-06,  ..., -2.6628e-05,
         -4.0345e-06, -1.2226e-05],
        [-5.5373e-05, -3.9756e-05,  3.7812e-06,  ..., -4.4644e-05,
         -6.7875e-06, -2.0459e-05]], device='cuda:0')
Loss: 1.171024203300476


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0,  199, 2834,  ...,  287, 3121,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1843e-05, -8.1005e-06, -1.3614e-05,  ...,  8.5364e-05,
         -3.1832e-05, -1.3685e-05],
        [-2.5675e-05, -1.8433e-05,  1.7919e-06,  ..., -2.0713e-05,
         -3.1516e-06, -9.5591e-06],
        [ 9.8909e-05,  4.3825e-05, -7.2436e-06,  ...,  6.7117e-05,
          6.0692e-06,  2.0471e-05],
        [-3.7640e-05, -2.7061e-05,  2.6505e-06,  ..., -3.0398e-05,
         -4.6119e-06, -1.4000e-05],
        [-6.3062e-05, -4.5329e-05,  4.3772e-06,  ..., -5.0902e-05,
         -7.7486e-06, -2.3395e-05]], device='cuda:0')
Loss: 1.1820173263549805
Graident accumulation at epoch 0, step 223, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0168,  0.0144, -0.0269,  ...,  0.0280, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1597e-04, -5.7236e-05,  6.5981e-06,  ...,  7.0195e-05,
         -9.1536e-05, -4.0601e-06],
        [-2.8161e-05, -2.0030e-05,  3.3816e-06,  ..., -2.2367e-05,
         -2.7435e-06, -1.1120e-05],
        [ 1.9646e-05,  1.2145e-05, -3.4947e-06,  ...,  1.9477e-05,
          1.8677e-07,  8.4273e-06],
        [-1.9404e-05, -1.0515e-05,  4.5491e-06,  ..., -1.3593e-05,
         -2.1288e-06, -6.2539e-06],
        [-6.0671e-05, -4.3091e-05,  6.1864e-06,  ..., -4.7389e-05,
         -5.9173e-06, -2.3871e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7647e-08, 1.4714e-08, 1.7440e-08,  ..., 2.0018e-08, 3.1704e-08,
         6.5127e-09],
        [2.6077e-11, 1.6692e-11, 9.6129e-13,  ..., 1.7052e-11, 5.8782e-13,
         3.5007e-12],
        [1.0882e-10, 8.1099e-11, 1.0112e-11,  ..., 1.4477e-10, 3.4008e-12,
         4.4573e-11],
        [1.7277e-10, 2.5251e-10, 4.6682e-12,  ..., 1.2813e-10, 6.5627e-12,
         4.6102e-11],
        [1.0286e-10, 5.9884e-11, 4.8266e-12,  ..., 6.7734e-11, 1.8790e-12,
         1.5414e-11]], device='cuda:0')
optimizer state dict: 28.0
lr: [1.828097834093899e-05, 1.828097834093899e-05]
scheduler_last_epoch: 28


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,  3235,   471,  ...,  1967,  4273,  2738],
        [    0, 21410, 13160,  ...,   292,    69,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9651e-06,  3.7123e-06, -1.6854e-06,  ...,  2.0559e-05,
         -1.7630e-05,  2.9840e-05],
        [-3.1590e-06, -2.2203e-06,  2.5146e-07,  ..., -2.5630e-06,
         -3.3341e-07, -1.1548e-06],
        [-3.0696e-06, -2.1607e-06,  2.4401e-07,  ..., -2.4885e-06,
         -3.2410e-07, -1.1250e-06],
        [-4.7088e-06, -3.3230e-06,  3.7439e-07,  ..., -3.8147e-06,
         -4.9919e-07, -1.7211e-06],
        [-7.7486e-06, -5.4538e-06,  6.1095e-07,  ..., -6.2883e-06,
         -8.2329e-07, -2.8312e-06]], device='cuda:0')
Loss: 1.193020224571228


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[   0,  278, 5717,  ..., 5342, 5147,   14],
        [   0, 6518,  681,  ...,  401, 9748,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8557e-06, -1.0725e-05,  1.9161e-05,  ...,  4.1469e-05,
         -6.3522e-06,  4.0403e-05],
        [-6.2436e-06, -4.4554e-06,  4.8615e-07,  ..., -5.0664e-06,
         -6.4448e-07, -2.2575e-06],
        [-6.0946e-06, -4.3511e-06,  4.7591e-07,  ..., -4.9472e-06,
         -6.2957e-07, -2.2054e-06],
        [-9.3877e-06, -6.7055e-06,  7.3388e-07,  ..., -7.5996e-06,
         -9.6858e-07, -3.3900e-06],
        [-1.5438e-05, -1.0997e-05,  1.1884e-06,  ..., -1.2487e-05,
         -1.5944e-06, -5.5730e-06]], device='cuda:0')
Loss: 1.1732507944107056


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[    0,   292,    48,  ...,   199, 19047,   292],
        [    0, 23809, 27646,  ...,   266,  3373,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5038e-06, -1.2945e-05,  2.7939e-05,  ...,  7.2069e-05,
         -2.2142e-05,  5.2990e-05],
        [-9.3728e-06, -6.7055e-06,  7.2364e-07,  ..., -7.6145e-06,
         -9.7044e-07, -3.3900e-06],
        [-9.1344e-06, -6.5267e-06,  7.0781e-07,  ..., -7.4357e-06,
         -9.4622e-07, -3.3081e-06],
        [-1.4096e-05, -1.0073e-05,  1.0934e-06,  ..., -1.1444e-05,
         -1.4566e-06, -5.0962e-06],
        [-2.3127e-05, -1.6510e-05,  1.7695e-06,  ..., -1.8746e-05,
         -2.3954e-06, -8.3447e-06]], device='cuda:0')
Loss: 1.166809320449829


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2915,   275,  2565],
        [    0, 12472,  1059,  ...,   642,   365,  6517]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3311e-05, -2.6832e-05,  2.2461e-05,  ...,  9.4801e-05,
         -7.2308e-06,  4.4028e-05],
        [-1.2472e-05, -8.9258e-06,  9.5740e-07,  ..., -1.0148e-05,
         -1.3225e-06, -4.5300e-06],
        [-1.2144e-05, -8.6874e-06,  9.3505e-07,  ..., -9.9093e-06,
         -1.2871e-06, -4.4107e-06],
        [-1.8746e-05, -1.3411e-05,  1.4473e-06,  ..., -1.5259e-05,
         -1.9819e-06, -6.8024e-06],
        [-3.0696e-05, -2.1935e-05,  2.3395e-06,  ..., -2.4945e-05,
         -3.2559e-06, -1.1116e-05]], device='cuda:0')
Loss: 1.1905125379562378


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  1034,  5599,  ...,   259,   586,  1403],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4208e-05, -3.0228e-05,  2.7466e-05,  ...,  8.6050e-05,
         -4.3609e-06,  5.2696e-05],
        [-1.5572e-05, -1.1131e-05,  1.1893e-06,  ..., -1.2666e-05,
         -1.6447e-06, -5.6550e-06],
        [-1.5169e-05, -1.0833e-05,  1.1623e-06,  ..., -1.2368e-05,
         -1.6000e-06, -5.5060e-06],
        [-2.3425e-05, -1.6734e-05,  1.7993e-06,  ..., -1.9044e-05,
         -2.4661e-06, -8.5011e-06],
        [-3.8385e-05, -2.7418e-05,  2.9132e-06,  ..., -3.1203e-05,
         -4.0568e-06, -1.3918e-05]], device='cuda:0')
Loss: 1.1801385879516602


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[   0, 4995,  287,  ...,  300, 4531, 4729],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1335e-05, -2.6265e-05,  3.8536e-05,  ...,  1.0146e-04,
         -1.1632e-05,  6.8564e-05],
        [-1.8701e-05, -1.3351e-05,  1.4314e-06,  ..., -1.5199e-05,
         -1.9874e-06, -6.7949e-06],
        [-1.8209e-05, -1.2979e-05,  1.3979e-06,  ..., -1.4812e-05,
         -1.9316e-06, -6.6087e-06],
        [-2.8163e-05, -2.0087e-05,  2.1681e-06,  ..., -2.2858e-05,
         -2.9802e-06, -1.0215e-05],
        [-4.6015e-05, -3.2842e-05,  3.5055e-06,  ..., -3.7372e-05,
         -4.8950e-06, -1.6674e-05]], device='cuda:0')
Loss: 1.1873372793197632


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,    12,  2735,  ...,    12,   344,  1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6737e-05, -2.6265e-05,  4.7709e-05,  ...,  1.0110e-04,
          5.6845e-06,  5.0998e-05],
        [-2.1815e-05, -1.5602e-05,  1.6736e-06,  ..., -1.7747e-05,
         -2.3264e-06, -7.9423e-06],
        [-2.1249e-05, -1.5169e-05,  1.6345e-06,  ..., -1.7285e-05,
         -2.2631e-06, -7.7188e-06],
        [-3.2872e-05, -2.3484e-05,  2.5351e-06,  ..., -2.6703e-05,
         -3.4906e-06, -1.1936e-05],
        [-5.3763e-05, -3.8415e-05,  4.1015e-06,  ..., -4.3660e-05,
         -5.7369e-06, -1.9506e-05]], device='cuda:0')
Loss: 1.1941328048706055


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[    0,   287,  9430,  ...,  3121,   352,   360],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6230e-05,  2.2881e-05,  6.2849e-05,  ...,  7.9347e-05,
         -1.9076e-05,  7.7263e-05],
        [-2.4915e-05, -1.7852e-05,  1.9111e-06,  ..., -2.0266e-05,
         -2.6468e-06, -9.0748e-06],
        [-2.4259e-05, -1.7345e-05,  1.8664e-06,  ..., -1.9744e-05,
         -2.5742e-06, -8.8215e-06],
        [-3.7551e-05, -2.6882e-05,  2.8983e-06,  ..., -3.0518e-05,
         -3.9749e-06, -1.3649e-05],
        [-6.1452e-05, -4.3988e-05,  4.6864e-06,  ..., -4.9919e-05,
         -6.5342e-06, -2.2307e-05]], device='cuda:0')
Loss: 1.1793773174285889
Graident accumulation at epoch 0, step 231, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0275e-04, -4.9225e-05,  1.2223e-05,  ...,  7.1111e-05,
         -8.4290e-05,  4.0722e-06],
        [-2.7836e-05, -1.9812e-05,  3.2345e-06,  ..., -2.2157e-05,
         -2.7338e-06, -1.0915e-05],
        [ 1.5256e-05,  9.1963e-06, -2.9586e-06,  ...,  1.5555e-05,
         -8.9326e-08,  6.7025e-06],
        [-2.1219e-05, -1.2151e-05,  4.3840e-06,  ..., -1.5286e-05,
         -2.3134e-06, -6.9935e-06],
        [-6.0749e-05, -4.3181e-05,  6.0364e-06,  ..., -4.7642e-05,
         -5.9790e-06, -2.3714e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7599e-08, 1.4699e-08, 1.7427e-08,  ..., 2.0004e-08, 3.1672e-08,
         6.5122e-09],
        [2.6672e-11, 1.6994e-11, 9.6398e-13,  ..., 1.7446e-11, 5.9424e-13,
         3.5795e-12],
        [1.0930e-10, 8.1318e-11, 1.0106e-11,  ..., 1.4502e-10, 3.4040e-12,
         4.4606e-11],
        [1.7401e-10, 2.5298e-10, 4.6719e-12,  ..., 1.2893e-10, 6.5719e-12,
         4.6242e-11],
        [1.0653e-10, 6.1759e-11, 4.8438e-12,  ..., 7.0158e-11, 1.9198e-12,
         1.5897e-11]], device='cuda:0')
optimizer state dict: 29.0
lr: [1.8139866849701876e-05, 1.8139866849701876e-05]
scheduler_last_epoch: 29


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,  6909,   352,  ...,  1075,   706,  6909]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3353e-05,  1.8355e-05, -1.4502e-05,  ...,  4.7900e-07,
         -2.6173e-06,  9.7100e-07],
        [-3.0547e-06, -2.1309e-06,  2.7940e-07,  ..., -2.5183e-06,
         -2.9616e-07, -1.1101e-06],
        [-3.0398e-06, -2.1309e-06,  2.7753e-07,  ..., -2.5183e-06,
         -2.9616e-07, -1.1101e-06],
        [-4.6790e-06, -3.2634e-06,  4.3027e-07,  ..., -3.8445e-06,
         -4.5076e-07, -1.6987e-06],
        [-7.4804e-06, -5.2154e-06,  6.8173e-07,  ..., -6.1691e-06,
         -7.3016e-07, -2.7269e-06]], device='cuda:0')
Loss: 1.1743862628936768


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,    12,   546,  ..., 24994, 31107,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8543e-06,  1.4201e-05, -1.5330e-05,  ...,  2.8447e-05,
         -1.2501e-05,  2.3229e-07],
        [-6.0946e-06, -4.2915e-06,  5.7369e-07,  ..., -5.0068e-06,
         -5.9977e-07, -2.2203e-06],
        [-6.0648e-06, -4.2766e-06,  5.6997e-07,  ..., -4.9770e-06,
         -5.9791e-07, -2.2128e-06],
        [-9.3579e-06, -6.5863e-06,  8.8476e-07,  ..., -7.6592e-06,
         -9.1828e-07, -3.4049e-06],
        [-1.4991e-05, -1.0550e-05,  1.4044e-06,  ..., -1.2279e-05,
         -1.4789e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.174289345741272


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0,  642,  287,  ...,  800,   12, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7121e-05, -8.2246e-06,  2.9167e-05,  ...,  2.9225e-05,
         -4.3959e-05,  2.5976e-05],
        [-9.1791e-06, -6.4820e-06,  8.5868e-07,  ..., -7.5251e-06,
         -9.3132e-07, -3.3379e-06],
        [-9.1046e-06, -6.4224e-06,  8.4937e-07,  ..., -7.4506e-06,
         -9.2387e-07, -3.3155e-06],
        [-1.4096e-05, -9.9391e-06,  1.3225e-06,  ..., -1.1504e-05,
         -1.4249e-06, -5.1185e-06],
        [-2.2560e-05, -1.5914e-05,  2.0973e-06,  ..., -1.8448e-05,
         -2.2948e-06, -8.1956e-06]], device='cuda:0')
Loss: 1.168483018875122


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,   266,  2604,  ...,   278,  4035,  4165],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2447e-05, -1.7067e-05,  6.9184e-05,  ..., -1.7465e-05,
         -3.8539e-05,  2.9086e-05],
        [-1.2249e-05, -8.6427e-06,  1.1139e-06,  ..., -1.0028e-05,
         -1.2610e-06, -4.4554e-06],
        [-1.2159e-05, -8.5682e-06,  1.1008e-06,  ..., -9.9391e-06,
         -1.2498e-06, -4.4331e-06],
        [-1.8775e-05, -1.3217e-05,  1.7099e-06,  ..., -1.5303e-05,
         -1.9204e-06, -6.8173e-06],
        [-3.0100e-05, -2.1219e-05,  2.7157e-06,  ..., -2.4587e-05,
         -3.0994e-06, -1.0937e-05]], device='cuda:0')
Loss: 1.1708097457885742


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,  1875,  2117,  ...,  1422,  1059,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0764e-05,  5.0625e-06,  2.7805e-05,  ..., -2.4632e-05,
          1.2427e-05,  8.2107e-06],
        [-1.5303e-05, -1.0818e-05,  1.4380e-06,  ..., -1.2562e-05,
         -1.6056e-06, -5.5954e-06],
        [-1.5169e-05, -1.0714e-05,  1.4212e-06,  ..., -1.2442e-05,
         -1.5907e-06, -5.5581e-06],
        [-2.3425e-05, -1.6540e-05,  2.2054e-06,  ..., -1.9148e-05,
         -2.4419e-06, -8.5533e-06],
        [-3.7521e-05, -2.6524e-05,  3.5018e-06,  ..., -3.0726e-05,
         -3.9376e-06, -1.3709e-05]], device='cuda:0')
Loss: 1.177987813949585


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2301e-05, -6.4042e-05,  5.0557e-05,  ..., -3.0357e-05,
          4.3385e-05, -2.7710e-05],
        [-1.8358e-05, -1.2919e-05,  1.7025e-06,  ..., -1.5050e-05,
         -1.8869e-06, -6.7204e-06],
        [-1.8194e-05, -1.2800e-05,  1.6838e-06,  ..., -1.4916e-05,
         -1.8701e-06, -6.6683e-06],
        [-2.8074e-05, -1.9744e-05,  2.6096e-06,  ..., -2.2948e-05,
         -2.8703e-06, -1.0259e-05],
        [-4.5061e-05, -3.1710e-05,  4.1500e-06,  ..., -3.6895e-05,
         -4.6343e-06, -1.6481e-05]], device='cuda:0')
Loss: 1.179657220840454


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5680e-05, -7.0141e-05,  5.3335e-05,  ..., -6.5356e-05,
          2.3181e-05, -8.1337e-05],
        [-2.1458e-05, -1.5110e-05,  1.9632e-06,  ..., -1.7583e-05,
         -2.2054e-06, -7.8455e-06],
        [-2.1264e-05, -1.4961e-05,  1.9427e-06,  ..., -1.7419e-05,
         -2.1849e-06, -7.7859e-06],
        [-3.2812e-05, -2.3082e-05,  3.0119e-06,  ..., -2.6822e-05,
         -3.3546e-06, -1.1981e-05],
        [-5.2691e-05, -3.7104e-05,  4.7870e-06,  ..., -4.3124e-05,
         -5.4166e-06, -1.9252e-05]], device='cuda:0')
Loss: 1.164473533630371


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0,   11,  360,  ..., 4524, 1553,  401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6968e-05, -5.4229e-05,  7.5628e-05,  ..., -5.7022e-05,
          1.8624e-06, -5.2717e-05],
        [-2.4527e-05, -1.7285e-05,  2.2482e-06,  ..., -2.0117e-05,
         -2.5146e-06, -8.9630e-06],
        [-2.4289e-05, -1.7121e-05,  2.2240e-06,  ..., -1.9923e-05,
         -2.4904e-06, -8.8885e-06],
        [-3.7462e-05, -2.6390e-05,  3.4496e-06,  ..., -3.0667e-05,
         -3.8221e-06, -1.3672e-05],
        [-6.0141e-05, -4.2409e-05,  5.4762e-06,  ..., -4.9263e-05,
         -6.1691e-06, -2.1949e-05]], device='cuda:0')
Loss: 1.1793793439865112
Graident accumulation at epoch 0, step 239, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0144, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.6174e-05, -4.9725e-05,  1.8564e-05,  ...,  5.8297e-05,
         -7.5675e-05, -1.6067e-06],
        [-2.7505e-05, -1.9560e-05,  3.1359e-06,  ..., -2.1953e-05,
         -2.7119e-06, -1.0720e-05],
        [ 1.1301e-05,  6.5645e-06, -2.4403e-06,  ...,  1.2007e-05,
         -3.2943e-07,  5.1434e-06],
        [-2.2843e-05, -1.3575e-05,  4.2906e-06,  ..., -1.6824e-05,
         -2.4643e-06, -7.6613e-06],
        [-6.0688e-05, -4.3104e-05,  5.9804e-06,  ..., -4.7804e-05,
         -5.9980e-06, -2.3538e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7553e-08, 1.4688e-08, 1.7415e-08,  ..., 1.9987e-08, 3.1641e-08,
         6.5085e-09],
        [2.7247e-11, 1.7276e-11, 9.6807e-13,  ..., 1.7833e-11, 5.9997e-13,
         3.6563e-12],
        [1.0978e-10, 8.1530e-11, 1.0101e-11,  ..., 1.4527e-10, 3.4068e-12,
         4.4641e-11],
        [1.7524e-10, 2.5342e-10, 4.6792e-12,  ..., 1.2974e-10, 6.5799e-12,
         4.6383e-11],
        [1.1004e-10, 6.3496e-11, 4.8689e-12,  ..., 7.2515e-11, 1.9559e-12,
         1.6362e-11]], device='cuda:0')
optimizer state dict: 30.0
lr: [1.799378017770064e-05, 1.799378017770064e-05]
scheduler_last_epoch: 30


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8313e-05, -1.4344e-05, -1.4070e-05,  ..., -1.0478e-05,
          1.0856e-05,  1.5770e-05],
        [-2.9802e-06, -2.0862e-06,  3.1479e-07,  ..., -2.5034e-06,
         -2.9616e-07, -1.1250e-06],
        [-3.0100e-06, -2.1160e-06,  3.1851e-07,  ..., -2.5183e-06,
         -2.9989e-07, -1.1399e-06],
        [-4.5896e-06, -3.2187e-06,  4.8801e-07,  ..., -3.8445e-06,
         -4.5635e-07, -1.7360e-06],
        [-7.3612e-06, -5.1558e-06,  7.7486e-07,  ..., -6.1691e-06,
         -7.3761e-07, -2.7716e-06]], device='cuda:0')
Loss: 1.1853222846984863


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0,   278,   554,  ...,   365,  3125,   271],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4424e-05, -3.2237e-07, -1.3136e-05,  ..., -2.1820e-05,
          2.7917e-05,  4.3959e-06],
        [-5.9903e-06, -4.1723e-06,  6.1095e-07,  ..., -5.0068e-06,
         -5.8673e-07, -2.2426e-06],
        [-6.0201e-06, -4.2021e-06,  6.1467e-07,  ..., -5.0217e-06,
         -5.9046e-07, -2.2650e-06],
        [-9.1791e-06, -6.4224e-06,  9.4436e-07,  ..., -7.6890e-06,
         -8.9779e-07, -3.4496e-06],
        [-1.4752e-05, -1.0282e-05,  1.5013e-06,  ..., -1.2338e-05,
         -1.4529e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.207598328590393


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  1067,   271,  ...,   266,   940,   271],
        [    0,   365,  1941,  ..., 38029,  1790, 44066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0251e-07,  4.0934e-06, -2.6906e-05,  ..., -2.9042e-05,
          4.9028e-05,  2.1077e-05],
        [-9.0301e-06, -6.2883e-06,  9.1828e-07,  ..., -7.5251e-06,
         -8.7917e-07, -3.3677e-06],
        [-9.0748e-06, -6.3330e-06,  9.2573e-07,  ..., -7.5549e-06,
         -8.8476e-07, -3.3975e-06],
        [-1.3828e-05, -9.6560e-06,  1.4175e-06,  ..., -1.1533e-05,
         -1.3430e-06, -5.1707e-06],
        [-2.2173e-05, -1.5438e-05,  2.2501e-06,  ..., -1.8477e-05,
         -2.1681e-06, -8.2701e-06]], device='cuda:0')
Loss: 1.1859854459762573


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0,  1688,   790,  ...,   546,   696,    12],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7495e-05,  1.1450e-05, -6.1342e-05,  ..., -1.5436e-05,
          7.7047e-05,  6.9650e-05],
        [-1.2100e-05, -8.4490e-06,  1.2480e-06,  ..., -1.0058e-05,
         -1.1753e-06, -4.4852e-06],
        [ 3.6194e-04,  3.0028e-04, -2.7116e-05,  ...,  3.2054e-04,
          2.9010e-05,  1.3384e-04],
        [-1.8507e-05, -1.2964e-05,  1.9204e-06,  ..., -1.5408e-05,
         -1.7937e-06, -6.8843e-06],
        [-2.9683e-05, -2.0742e-05,  3.0547e-06,  ..., -2.4676e-05,
         -2.8983e-06, -1.1012e-05]], device='cuda:0')
Loss: 1.1796813011169434


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  996, 2226,  ...,  516, 3470,   14],
        [   0, 1561,   14,  ..., 4433,  352, 1561]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0796e-05,  3.2120e-05, -6.7764e-05,  ...,  3.1140e-05,
          7.7047e-05,  1.0930e-04],
        [-1.5110e-05, -1.0535e-05,  1.5758e-06,  ..., -1.2547e-05,
         -1.4845e-06, -5.6252e-06],
        [ 3.5890e-04,  2.9818e-04, -2.6785e-05,  ...,  3.1804e-04,
          2.8699e-05,  1.3270e-04],
        [-2.3156e-05, -1.6168e-05,  2.4270e-06,  ..., -1.9222e-05,
         -2.2687e-06, -8.6352e-06],
        [-3.7074e-05, -2.5839e-05,  3.8557e-06,  ..., -3.0786e-05,
         -3.6582e-06, -1.3798e-05]], device='cuda:0')
Loss: 1.1663614511489868


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    14,  7870,  ...,   284,   830,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9733e-05,  3.6020e-05, -6.9692e-05,  ...,  4.0760e-05,
          7.1051e-05,  1.4083e-04],
        [-1.8105e-05, -1.2636e-05,  1.9092e-06,  ..., -1.5005e-05,
         -1.7881e-06, -6.7428e-06],
        [ 4.6370e-04,  3.7023e-04, -4.0380e-05,  ...,  3.9249e-04,
          4.1602e-05,  1.5555e-04],
        [-2.7776e-05, -1.9401e-05,  2.9448e-06,  ..., -2.3022e-05,
         -2.7362e-06, -1.0364e-05],
        [-4.4495e-05, -3.1024e-05,  4.6752e-06,  ..., -3.6865e-05,
         -4.4070e-06, -1.6555e-05]], device='cuda:0')
Loss: 1.158100962638855


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,  9582,  3645,  ...,  1027,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2014e-05,  2.0618e-05, -8.5224e-05,  ...,  4.0697e-05,
          8.4480e-05,  1.1909e-04],
        [-2.1145e-05, -1.4752e-05,  2.2221e-06,  ..., -1.7539e-05,
         -2.0973e-06, -7.8380e-06],
        [ 4.6066e-04,  3.6811e-04, -4.0067e-05,  ...,  3.8996e-04,
          4.1291e-05,  1.5446e-04],
        [-3.2425e-05, -2.2635e-05,  3.4217e-06,  ..., -2.6897e-05,
         -3.2075e-06, -1.2033e-05],
        [-5.2005e-05, -3.6269e-05,  5.4426e-06,  ..., -4.3124e-05,
         -5.1707e-06, -1.9252e-05]], device='cuda:0')
Loss: 1.1872515678405762


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0, 24063,   717,  ...,  2228,  1416,     9],
        [    0,   741,   300,  ...,    83,  7111,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3337e-05,  7.0467e-06, -6.6041e-05,  ...,  7.4528e-05,
          6.5471e-05,  1.2941e-04],
        [-2.4155e-05, -1.6883e-05,  2.5276e-06,  ..., -2.0027e-05,
         -2.3991e-06, -8.9407e-06],
        [ 4.5759e-04,  3.6595e-04, -3.9754e-05,  ...,  3.8743e-04,
          4.0984e-05,  1.5333e-04],
        [-3.7044e-05, -2.5898e-05,  3.8929e-06,  ..., -3.0711e-05,
         -3.6713e-06, -1.3731e-05],
        [-5.9396e-05, -4.1485e-05,  6.1952e-06,  ..., -4.9233e-05,
         -5.9120e-06, -2.1964e-05]], device='cuda:0')
Loss: 1.1617683172225952
Graident accumulation at epoch 0, step 247, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0075,  0.0034,  ..., -0.0093, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4890e-05, -4.4048e-05,  1.0103e-05,  ...,  5.9920e-05,
         -6.1560e-05,  1.1495e-05],
        [-2.7170e-05, -1.9292e-05,  3.0751e-06,  ..., -2.1761e-05,
         -2.6806e-06, -1.0542e-05],
        [ 5.5930e-05,  4.2503e-05, -6.1717e-06,  ...,  4.9549e-05,
          3.8019e-06,  1.9962e-05],
        [-2.4263e-05, -1.4807e-05,  4.2508e-06,  ..., -1.8212e-05,
         -2.5850e-06, -8.2683e-06],
        [-6.0559e-05, -4.2942e-05,  6.0018e-06,  ..., -4.7947e-05,
         -5.9894e-06, -2.3380e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7513e-08, 1.4673e-08, 1.7402e-08,  ..., 1.9973e-08, 3.1613e-08,
         6.5187e-09],
        [2.7803e-11, 1.7543e-11, 9.7349e-13,  ..., 1.8216e-11, 6.0513e-13,
         3.7326e-12],
        [3.1906e-10, 2.1537e-10, 1.1671e-11,  ..., 2.9522e-10, 5.0831e-12,
         6.8107e-11],
        [1.7644e-10, 2.5384e-10, 4.6896e-12,  ..., 1.3055e-10, 6.5868e-12,
         4.6525e-11],
        [1.1346e-10, 6.5154e-11, 4.9024e-12,  ..., 7.4866e-11, 1.9889e-12,
         1.6828e-11]], device='cuda:0')
optimizer state dict: 31.0
lr: [1.7842807614798848e-05, 1.7842807614798848e-05]
scheduler_last_epoch: 31


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[   0,  266, 3536,  ...,  266, 1883,  266],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4217e-05,  6.7660e-06,  3.1634e-05,  ...,  2.0766e-06,
         -3.8548e-05, -1.1802e-05],
        [-2.9504e-06, -2.0564e-06,  3.2969e-07,  ..., -2.4736e-06,
         -2.6077e-07, -1.1176e-06],
        [-3.0398e-06, -2.1160e-06,  3.4086e-07,  ..., -2.5481e-06,
         -2.7008e-07, -1.1548e-06],
        [-4.5598e-06, -3.1739e-06,  5.1409e-07,  ..., -3.8147e-06,
         -4.0047e-07, -1.7285e-06],
        [-7.2420e-06, -5.0366e-06,  8.0839e-07,  ..., -6.0499e-06,
         -6.4448e-07, -2.7418e-06]], device='cuda:0')
Loss: 1.16513991355896


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3714e-05, -4.4381e-06,  2.8676e-05,  ..., -1.8380e-05,
         -1.7195e-05, -1.9492e-05],
        [-5.9456e-06, -4.1574e-06,  6.5379e-07,  ..., -4.9919e-06,
         -5.6066e-07, -2.2575e-06],
        [-6.0797e-06, -4.2468e-06,  6.7055e-07,  ..., -5.0962e-06,
         -5.7556e-07, -2.3171e-06],
        [-9.1195e-06, -6.3777e-06,  1.0096e-06,  ..., -7.6592e-06,
         -8.5495e-07, -3.4720e-06],
        [-1.4484e-05, -1.0133e-05,  1.5907e-06,  ..., -1.2130e-05,
         -1.3709e-06, -5.5134e-06]], device='cuda:0')
Loss: 1.1633474826812744


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   600,  9092,  ...,   554,  1485,   328],
        [    0, 42306,   278,  ...,  1110,  3427,  4224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3242e-05, -1.6062e-05,  2.4118e-05,  ..., -6.1895e-06,
         -2.6427e-06, -1.4681e-06],
        [-8.9407e-06, -6.2287e-06,  9.6858e-07,  ..., -7.5251e-06,
         -8.3447e-07, -3.3751e-06],
        [-9.1344e-06, -6.3628e-06,  9.9279e-07,  ..., -7.6741e-06,
         -8.5682e-07, -3.4571e-06],
        [-1.3679e-05, -9.5367e-06,  1.4938e-06,  ..., -1.1533e-05,
         -1.2722e-06, -5.1782e-06],
        [-2.1785e-05, -1.5169e-05,  2.3544e-06,  ..., -1.8299e-05,
         -2.0415e-06, -8.2254e-06]], device='cuda:0')
Loss: 1.172991156578064


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,   278,  7524,  ...,  1288,   669,   352],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2709e-05, -1.4607e-05,  4.1371e-07,  ...,  2.9778e-05,
         -1.3318e-05,  5.7074e-06],
        [-1.1906e-05, -8.2850e-06,  1.2945e-06,  ..., -1.0028e-05,
         -1.1101e-06, -4.5002e-06],
        [-1.2144e-05, -8.4490e-06,  1.3243e-06,  ..., -1.0207e-05,
         -1.1344e-06, -4.5970e-06],
        [-1.8209e-05, -1.2696e-05,  1.9968e-06,  ..., -1.5378e-05,
         -1.6913e-06, -6.9067e-06],
        [-2.8968e-05, -2.0176e-05,  3.1441e-06,  ..., -2.4378e-05,
         -2.7083e-06, -1.0952e-05]], device='cuda:0')
Loss: 1.1644084453582764


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,   12,  616,  ...,  278,  266, 2907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3878e-05, -1.0306e-05, -3.7927e-05,  ...,  3.4118e-05,
         -2.2051e-05, -3.7492e-05],
        [-1.4886e-05, -1.0341e-05,  1.6354e-06,  ..., -1.2532e-05,
         -1.4044e-06, -5.6103e-06],
        [-1.5169e-05, -1.0550e-05,  1.6708e-06,  ..., -1.2740e-05,
         -1.4324e-06, -5.7295e-06],
        [-2.2799e-05, -1.5885e-05,  2.5257e-06,  ..., -1.9222e-05,
         -2.1439e-06, -8.6278e-06],
        [-3.6180e-05, -2.5183e-05,  3.9674e-06,  ..., -3.0428e-05,
         -3.4235e-06, -1.3649e-05]], device='cuda:0')
Loss: 1.1699435710906982


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,    14,   469,  ...,   367,  2564,   368],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4729e-06,  1.6209e-06, -8.3591e-05,  ...,  3.7394e-05,
          7.5503e-07, -2.9722e-05],
        [-1.7852e-05, -1.2442e-05,  1.9912e-06,  ..., -1.5035e-05,
         -1.7062e-06, -6.7353e-06],
        [-1.8179e-05, -1.2681e-05,  2.0321e-06,  ..., -1.5289e-05,
         -1.7397e-06, -6.8694e-06],
        [-2.7359e-05, -1.9103e-05,  3.0734e-06,  ..., -2.3067e-05,
         -2.6058e-06, -1.0349e-05],
        [-4.3362e-05, -3.0249e-05,  4.8243e-06,  ..., -3.6478e-05,
         -4.1537e-06, -1.6361e-05]], device='cuda:0')
Loss: 1.177926778793335


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  342,  516,  ...,   12,  729, 3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2271e-07,  8.7675e-06, -6.5032e-05,  ...,  5.4040e-05,
          4.2947e-05, -1.3365e-05],
        [-2.0862e-05, -1.4558e-05,  2.3022e-06,  ..., -1.7539e-05,
         -2.0098e-06, -7.8529e-06],
        [-2.1234e-05, -1.4827e-05,  2.3469e-06,  ..., -1.7822e-05,
         -2.0489e-06, -8.0019e-06],
        [-3.1978e-05, -2.2337e-05,  3.5502e-06,  ..., -2.6911e-05,
         -3.0696e-06, -1.2055e-05],
        [-5.0724e-05, -3.5405e-05,  5.5768e-06,  ..., -4.2558e-05,
         -4.8988e-06, -1.9073e-05]], device='cuda:0')
Loss: 1.1734110116958618


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,  1911,   679,  ...,    19,  3737,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5665e-06,  2.6161e-05, -7.9894e-05,  ...,  6.3306e-05,
          4.2060e-05, -8.8961e-06],
        [-2.3827e-05, -1.6600e-05,  2.5928e-06,  ..., -2.0042e-05,
         -2.2724e-06, -8.9407e-06],
        [-2.4274e-05, -1.6913e-05,  2.6450e-06,  ..., -2.0385e-05,
         -2.3171e-06, -9.1121e-06],
        [-3.6538e-05, -2.5481e-05,  4.0010e-06,  ..., -3.0756e-05,
         -3.4720e-06, -1.3724e-05],
        [-5.7995e-05, -4.0412e-05,  6.2846e-06,  ..., -4.8697e-05,
         -5.5432e-06, -2.1726e-05]], device='cuda:0')
Loss: 1.1768982410430908
Graident accumulation at epoch 0, step 255, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0151,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0334, -0.0098,  0.0406,  ...,  0.0222,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4945e-05, -3.7027e-05,  1.1035e-06,  ...,  6.0259e-05,
         -5.1198e-05,  9.4559e-06],
        [-2.6836e-05, -1.9023e-05,  3.0269e-06,  ..., -2.1589e-05,
         -2.6398e-06, -1.0382e-05],
        [ 4.7910e-05,  3.6562e-05, -5.2901e-06,  ...,  4.2556e-05,
          3.1900e-06,  1.7055e-05],
        [-2.5491e-05, -1.5875e-05,  4.2258e-06,  ..., -1.9467e-05,
         -2.6737e-06, -8.8139e-06],
        [-6.0303e-05, -4.2689e-05,  6.0301e-06,  ..., -4.8022e-05,
         -5.9448e-06, -2.3215e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7465e-08, 1.4659e-08, 1.7391e-08,  ..., 1.9957e-08, 3.1584e-08,
         6.5123e-09],
        [2.8343e-11, 1.7801e-11, 9.7924e-13,  ..., 1.8600e-11, 6.0968e-13,
         3.8088e-12],
        [3.1933e-10, 2.1544e-10, 1.1666e-11,  ..., 2.9534e-10, 5.0834e-12,
         6.8122e-11],
        [1.7760e-10, 2.5424e-10, 4.7009e-12,  ..., 1.3137e-10, 6.5923e-12,
         4.6667e-11],
        [1.1671e-10, 6.6722e-11, 4.9370e-12,  ..., 7.7163e-11, 2.0177e-12,
         1.7284e-11]], device='cuda:0')
optimizer state dict: 32.0
lr: [1.7687041437173095e-05, 1.7687041437173095e-05]
scheduler_last_epoch: 32


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8430e-09,  5.5570e-06, -9.5339e-06,  ...,  1.6675e-05,
         -2.7860e-06,  1.1917e-05],
        [-2.9504e-06, -2.0415e-06,  3.5763e-07,  ..., -2.5034e-06,
         -2.8126e-07, -1.1101e-06],
        [ 9.0519e-05,  5.9863e-05, -7.3205e-06,  ...,  6.1886e-05,
          1.8822e-06,  1.8948e-05],
        [-4.5598e-06, -3.1590e-06,  5.5507e-07,  ..., -3.8743e-06,
         -4.3400e-07, -1.7136e-06],
        [-7.1228e-06, -4.9174e-06,  8.6054e-07,  ..., -6.0499e-06,
         -6.8173e-07, -2.6673e-06]], device='cuda:0')
Loss: 1.173764705657959


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[    0,  4902,   518,  ...,  5493,  3227,   278],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3482e-05, -5.1255e-05, -1.5190e-06,  ...,  1.4361e-05,
          1.0826e-05,  3.0706e-05],
        [-5.9307e-06, -4.0829e-06,  7.2084e-07,  ..., -5.0068e-06,
         -6.1840e-07, -2.2426e-06],
        [ 8.7435e-05,  5.7747e-05, -6.9461e-06,  ...,  5.9294e-05,
          1.5339e-06,  1.7779e-05],
        [-9.1791e-06, -6.3330e-06,  1.1213e-06,  ..., -7.7784e-06,
         -9.5554e-07, -3.4645e-06],
        [-1.4365e-05, -9.8944e-06,  1.7397e-06,  ..., -1.2130e-05,
         -1.5013e-06, -5.4091e-06]], device='cuda:0')
Loss: 1.179901123046875


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[   0, 3159,  278,  ...,  266, 2545,  863],
        [   0,  266, 2057,  ...,   88, 1801,   66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2665e-05, -4.7802e-05, -1.4142e-05,  ...,  1.5540e-05,
          3.2961e-05,  5.4042e-05],
        [-8.8811e-06, -6.1095e-06,  1.0785e-06,  ..., -7.4655e-06,
         -8.8289e-07, -3.3528e-06],
        [ 8.4395e-05,  5.5646e-05, -6.5755e-06,  ...,  5.6760e-05,
          1.2620e-06,  1.6631e-05],
        [-1.3769e-05, -9.4920e-06,  1.6838e-06,  ..., -1.1593e-05,
         -1.3635e-06, -5.1931e-06],
        [-2.1547e-05, -1.4842e-05,  2.6077e-06,  ..., -1.8090e-05,
         -2.1420e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1767663955688477


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2651e-05, -3.4460e-05, -5.6254e-05,  ...,  3.3103e-06,
          8.1970e-05,  6.1763e-05],
        [-1.1832e-05, -8.1509e-06,  1.4417e-06,  ..., -9.9689e-06,
         -1.2014e-06, -4.4778e-06],
        [ 8.1400e-05,  5.3575e-05, -6.2067e-06,  ...,  5.4212e-05,
          9.3973e-07,  1.5491e-05],
        [-1.8299e-05, -1.2636e-05,  2.2426e-06,  ..., -1.5438e-05,
         -1.8515e-06, -6.9141e-06],
        [-2.8580e-05, -1.9699e-05,  3.4682e-06,  ..., -2.4050e-05,
         -2.9020e-06, -1.0774e-05]], device='cuda:0')
Loss: 1.1785144805908203


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6907e-05, -1.9801e-05, -5.8283e-05,  ..., -8.2808e-07,
          1.0468e-04,  1.0488e-04],
        [-1.4797e-05, -1.0207e-05,  1.7993e-06,  ..., -1.2502e-05,
         -1.4864e-06, -5.5805e-06],
        [ 7.8345e-05,  5.1459e-05, -5.8397e-06,  ...,  5.1605e-05,
          6.4730e-07,  1.4359e-05],
        [-2.2858e-05, -1.5795e-05,  2.7940e-06,  ..., -1.9342e-05,
         -2.2892e-06, -8.6129e-06],
        [-3.5733e-05, -2.4647e-05,  4.3251e-06,  ..., -3.0160e-05,
         -3.5912e-06, -1.3441e-05]], device='cuda:0')
Loss: 1.192333459854126


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[   0,  895, 4110,  ..., 1578, 1245,   13],
        [   0,   22, 2577,  ..., 4970,    9, 3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0394e-05, -7.1425e-06, -7.8034e-05,  ..., -1.9796e-05,
          1.2707e-04,  1.1728e-04],
        [-1.7777e-05, -1.2249e-05,  2.1365e-06,  ..., -1.4991e-05,
         -1.7695e-06, -6.7055e-06],
        [ 7.5276e-05,  4.9358e-05, -5.4914e-06,  ...,  4.9042e-05,
          3.5486e-07,  1.3204e-05],
        [-2.7448e-05, -1.8954e-05,  3.3192e-06,  ..., -2.3186e-05,
         -2.7232e-06, -1.0341e-05],
        [-4.2915e-05, -2.9564e-05,  5.1335e-06,  ..., -3.6150e-05,
         -4.2729e-06, -1.6138e-05]], device='cuda:0')
Loss: 1.187842607498169


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0, 3825, 1626,  ..., 5096, 3775,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3708e-05, -2.5683e-05, -1.0470e-04,  ..., -1.5697e-05,
          1.0016e-04,  1.4832e-04],
        [-2.0742e-05, -1.4290e-05,  2.4661e-06,  ..., -1.7479e-05,
         -2.0396e-06, -7.8306e-06],
        [ 7.2221e-05,  4.7242e-05, -5.1505e-06,  ...,  4.6479e-05,
          7.7326e-08,  1.2049e-05],
        [-3.2067e-05, -2.2128e-05,  3.8333e-06,  ..., -2.7061e-05,
         -3.1404e-06, -1.2085e-05],
        [-5.0068e-05, -3.4511e-05,  5.9269e-06,  ..., -4.2170e-05,
         -4.9248e-06, -1.8850e-05]], device='cuda:0')
Loss: 1.1670798063278198


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,    13, 41550,  ...,    12,   546,  1996],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9809e-05, -2.0208e-05, -1.0786e-04,  ..., -3.2349e-05,
          1.0834e-04,  1.6490e-04],
        [-2.3693e-05, -1.6347e-05,  2.8070e-06,  ..., -1.9968e-05,
         -2.3358e-06, -8.9481e-06],
        [ 6.9181e-05,  4.5126e-05, -4.7985e-06,  ...,  4.3901e-05,
         -2.2815e-07,  1.0894e-05],
        [-3.6597e-05, -2.5287e-05,  4.3586e-06,  ..., -3.0875e-05,
         -3.5949e-06, -1.3798e-05],
        [-5.7131e-05, -3.9428e-05,  6.7428e-06,  ..., -4.8161e-05,
         -5.6364e-06, -2.1517e-05]], device='cuda:0')
Loss: 1.1667304039001465
Graident accumulation at epoch 0, step 263, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0167,  0.0145, -0.0269,  ...,  0.0280, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6431e-05, -3.5345e-05, -9.7928e-06,  ...,  5.0998e-05,
         -3.5244e-05,  2.5001e-05],
        [-2.6521e-05, -1.8755e-05,  3.0049e-06,  ..., -2.1427e-05,
         -2.6094e-06, -1.0238e-05],
        [ 5.0037e-05,  3.7418e-05, -5.2409e-06,  ...,  4.2690e-05,
          2.8482e-06,  1.6439e-05],
        [-2.6601e-05, -1.6816e-05,  4.2391e-06,  ..., -2.0608e-05,
         -2.7658e-06, -9.3124e-06],
        [-5.9986e-05, -4.2363e-05,  6.1014e-06,  ..., -4.8036e-05,
         -5.9139e-06, -2.3045e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7428e-08, 1.4645e-08, 1.7385e-08,  ..., 1.9938e-08, 3.1564e-08,
         6.5329e-09],
        [2.8876e-11, 1.8051e-11, 9.8614e-13,  ..., 1.8980e-11, 6.1453e-13,
         3.8850e-12],
        [3.2380e-10, 2.1726e-10, 1.1678e-11,  ..., 2.9698e-10, 5.0783e-12,
         6.8172e-11],
        [1.7876e-10, 2.5462e-10, 4.7152e-12,  ..., 1.3219e-10, 6.5986e-12,
         4.6811e-11],
        [1.1986e-10, 6.8210e-11, 4.9775e-12,  ..., 7.9405e-11, 2.0474e-12,
         1.7729e-11]], device='cuda:0')
optimizer state dict: 33.0
lr: [1.7526576850912724e-05, 1.7526576850912724e-05]
scheduler_last_epoch: 33


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,  1106,   259,  ...,   271,   679,   382]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1423e-05,  1.4340e-05, -2.5028e-05,  ...,  3.5455e-06,
         -1.8421e-06, -3.5337e-05],
        [-2.9355e-06, -2.0266e-06,  3.5763e-07,  ..., -2.5183e-06,
         -2.9430e-07, -1.1176e-06],
        [-3.0398e-06, -2.1160e-06,  3.7067e-07,  ..., -2.6226e-06,
         -3.0547e-07, -1.1623e-06],
        [-4.5002e-06, -3.1143e-06,  5.5134e-07,  ..., -3.8743e-06,
         -4.4890e-07, -1.7136e-06],
        [-7.0333e-06, -4.8578e-06,  8.5309e-07,  ..., -6.0201e-06,
         -7.0408e-07, -2.6673e-06]], device='cuda:0')
Loss: 1.18299400806427


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4828e-05,  4.6304e-06,  9.1063e-06,  ...,  1.9664e-05,
          1.3257e-05, -5.1443e-05],
        [-5.9307e-06, -4.0829e-06,  7.3947e-07,  ..., -5.0515e-06,
         -5.8860e-07, -2.2575e-06],
        [-6.1691e-06, -4.2617e-06,  7.7114e-07,  ..., -5.2750e-06,
         -6.1467e-07, -2.3544e-06],
        [-9.0897e-06, -6.2585e-06,  1.1399e-06,  ..., -7.7486e-06,
         -8.9779e-07, -3.4571e-06],
        [-1.4186e-05, -9.7752e-06,  1.7658e-06,  ..., -1.2100e-05,
         -1.4119e-06, -5.3942e-06]], device='cuda:0')
Loss: 1.177659511566162


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,   266,  2374,  ...,  1551,   518,   638],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6776e-06,  3.7774e-06,  1.0216e-05,  ...,  1.5796e-05,
          2.6811e-05, -1.0028e-05],
        [-8.9258e-06, -6.1393e-06,  1.1288e-06,  ..., -7.5996e-06,
         -9.4436e-07, -3.3751e-06],
        [-9.2536e-06, -6.3777e-06,  1.1716e-06,  ..., -7.8976e-06,
         -9.8161e-07, -3.5092e-06],
        [-1.3679e-05, -9.4026e-06,  1.7360e-06,  ..., -1.1653e-05,
         -1.4417e-06, -5.1633e-06],
        [-2.1219e-05, -1.4603e-05,  2.6748e-06,  ..., -1.8060e-05,
         -2.2464e-06, -8.0168e-06]], device='cuda:0')
Loss: 1.1486340761184692


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[    0,   300,  5201,  ...,  1997,  7423,   417],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1401e-05,  2.2807e-06, -4.7445e-07,  ...,  3.9540e-05,
          2.0757e-05,  7.0908e-06],
        [-1.1876e-05, -8.1658e-06,  1.5367e-06,  ..., -1.0118e-05,
         -1.2256e-06, -4.5151e-06],
        [-1.2308e-05, -8.4639e-06,  1.5944e-06,  ..., -1.0505e-05,
         -1.2722e-06, -4.6864e-06],
        [-1.8209e-05, -1.2502e-05,  2.3618e-06,  ..., -1.5527e-05,
         -1.8720e-06, -6.9067e-06],
        [-2.8193e-05, -1.9372e-05,  3.6359e-06,  ..., -2.4021e-05,
         -2.9132e-06, -1.0699e-05]], device='cuda:0')
Loss: 1.183121919631958


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1401e-05,  1.8911e-06, -3.8924e-06,  ...,  3.9703e-05,
          2.5686e-05,  4.8325e-07],
        [-1.4842e-05, -1.0207e-05,  1.9204e-06,  ..., -1.2651e-05,
         -1.5311e-06, -5.6401e-06],
        [-1.5348e-05, -1.0565e-05,  1.9874e-06,  ..., -1.3113e-05,
         -1.5832e-06, -5.8413e-06],
        [-2.2799e-05, -1.5661e-05,  2.9542e-06,  ..., -1.9461e-05,
         -2.3413e-06, -8.6427e-06],
        [-3.5226e-05, -2.4229e-05,  4.5411e-06,  ..., -3.0041e-05,
         -3.6359e-06, -1.3366e-05]], device='cuda:0')
Loss: 1.1732097864151


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   518,  9048,  ...,  1354,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7139e-05, -8.8916e-06,  5.6014e-06,  ...,  2.5816e-05,
          3.4795e-05,  3.1372e-05],
        [-1.7762e-05, -1.2234e-05,  2.3190e-06,  ..., -1.5140e-05,
         -1.8217e-06, -6.7800e-06],
        [-1.8358e-05, -1.2666e-05,  2.4009e-06,  ..., -1.5676e-05,
         -1.8831e-06, -7.0184e-06],
        [-2.7299e-05, -1.8805e-05,  3.5726e-06,  ..., -2.3305e-05,
         -2.7884e-06, -1.0401e-05],
        [-4.2170e-05, -2.9057e-05,  5.4874e-06,  ..., -3.5942e-05,
         -4.3288e-06, -1.6063e-05]], device='cuda:0')
Loss: 1.1666499376296997


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0,  1485,   271,  ...,  6359,  1799,  5442],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4039e-05, -7.2667e-06, -7.7074e-06,  ...,  7.1924e-06,
          5.0257e-05,  2.4781e-05],
        [-2.0713e-05, -1.4305e-05,  2.6990e-06,  ..., -1.7688e-05,
         -2.1271e-06, -7.9274e-06],
        [-2.1398e-05, -1.4797e-05,  2.7940e-06,  ..., -1.8299e-05,
         -2.1979e-06, -8.1956e-06],
        [-3.1799e-05, -2.1979e-05,  4.1574e-06,  ..., -2.7210e-05,
         -3.2540e-06, -1.2152e-05],
        [-4.9144e-05, -3.3945e-05,  6.3814e-06,  ..., -4.1932e-05,
         -5.0515e-06, -1.8761e-05]], device='cuda:0')
Loss: 1.171975016593933


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  328, 6875,  ...,  369,  654,  300],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0812e-05, -2.5128e-06, -1.9842e-05,  ...,  1.4756e-05,
          6.1416e-05,  6.2429e-05],
        [-2.3678e-05, -1.6347e-05,  3.0864e-06,  ..., -2.0206e-05,
         -2.4233e-06, -9.0450e-06],
        [-2.4468e-05, -1.6913e-05,  3.1944e-06,  ..., -2.0906e-05,
         -2.5053e-06, -9.3505e-06],
        [-3.6359e-05, -2.5123e-05,  4.7572e-06,  ..., -3.1084e-05,
         -3.7104e-06, -1.3873e-05],
        [-5.6207e-05, -3.8803e-05,  7.3016e-06,  ..., -4.7922e-05,
         -5.7593e-06, -2.1413e-05]], device='cuda:0')
Loss: 1.1747244596481323
Graident accumulation at epoch 0, step 271, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.1869e-05, -3.2062e-05, -1.0798e-05,  ...,  4.7374e-05,
         -2.5578e-05,  2.8743e-05],
        [-2.6237e-05, -1.8514e-05,  3.0130e-06,  ..., -2.1305e-05,
         -2.5908e-06, -1.0119e-05],
        [ 4.2586e-05,  3.1985e-05, -4.3974e-06,  ...,  3.6331e-05,
          2.3128e-06,  1.3860e-05],
        [-2.7577e-05, -1.7647e-05,  4.2909e-06,  ..., -2.1655e-05,
         -2.8603e-06, -9.7684e-06],
        [-5.9608e-05, -4.2007e-05,  6.2214e-06,  ..., -4.8024e-05,
         -5.8985e-06, -2.2882e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7382e-08, 1.4630e-08, 1.7368e-08,  ..., 1.9918e-08, 3.1536e-08,
         6.5303e-09],
        [2.9408e-11, 1.8300e-11, 9.9468e-13,  ..., 1.9369e-11, 6.1979e-13,
         3.9630e-12],
        [3.2407e-10, 2.1733e-10, 1.1676e-11,  ..., 2.9712e-10, 5.0795e-12,
         6.8191e-11],
        [1.7990e-10, 2.5500e-10, 4.7332e-12,  ..., 1.3303e-10, 6.6058e-12,
         4.6956e-11],
        [1.2290e-10, 6.9647e-11, 5.0259e-12,  ..., 8.1622e-11, 2.0785e-12,
         1.8170e-11]], device='cuda:0')
optimizer state dict: 34.0
lr: [1.73615119338288e-05, 1.73615119338288e-05]
scheduler_last_epoch: 34


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1379e-05,  2.2750e-05,  1.1303e-05,  ..., -1.4480e-05,
         -9.1760e-06,  3.4199e-05],
        [-2.9355e-06, -2.0415e-06,  4.0233e-07,  ..., -2.4736e-06,
         -3.3528e-07, -1.1027e-06],
        [-3.0547e-06, -2.1160e-06,  4.1537e-07,  ..., -2.5630e-06,
         -3.4831e-07, -1.1474e-06],
        [-4.5002e-06, -3.1292e-06,  6.1467e-07,  ..., -3.7849e-06,
         -5.1409e-07, -1.6913e-06],
        [-6.9439e-06, -4.8280e-06,  9.4250e-07,  ..., -5.8413e-06,
         -7.9349e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1425330638885498


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 7185,  328,  ..., 1427, 1477, 1061],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2763e-05,  1.3421e-05,  4.2589e-05,  ...,  9.8475e-06,
         -1.1289e-05,  3.6198e-05],
        [-5.8562e-06, -4.0680e-06,  7.9907e-07,  ..., -4.9621e-06,
         -6.3889e-07, -2.1905e-06],
        [-6.0797e-06, -4.2170e-06,  8.2701e-07,  ..., -5.1558e-06,
         -6.6310e-07, -2.2724e-06],
        [-9.0003e-06, -6.2436e-06,  1.2256e-06,  ..., -7.6294e-06,
         -9.7789e-07, -3.3602e-06],
        [-1.3858e-05, -9.6262e-06,  1.8775e-06,  ..., -1.1742e-05,
         -1.5125e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.169439673423767


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3205e-05,  1.0411e-05,  5.9618e-05,  ...,  3.4742e-05,
          6.3633e-06,  7.2665e-05],
        [-8.7768e-06, -6.1095e-06,  1.2182e-06,  ..., -7.4655e-06,
         -9.5926e-07, -3.3304e-06],
        [-9.1046e-06, -6.3330e-06,  1.2629e-06,  ..., -7.7486e-06,
         -9.9465e-07, -3.4496e-06],
        [-1.3530e-05, -9.4026e-06,  1.8775e-06,  ..., -1.1504e-05,
         -1.4734e-06, -5.1185e-06],
        [-2.0742e-05, -1.4424e-05,  2.8610e-06,  ..., -1.7643e-05,
         -2.2687e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.1699001789093018


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0, 26473,  2117,  ...,    13,  3292,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1660e-05, -3.1035e-06,  3.2658e-05,  ...,  4.0133e-05,
         -2.9056e-05,  8.3967e-05],
        [-1.1727e-05, -8.1211e-06,  1.6447e-06,  ..., -9.9540e-06,
         -1.2834e-06, -4.4554e-06],
        [-1.2159e-05, -8.4192e-06,  1.7043e-06,  ..., -1.0312e-05,
         -1.3299e-06, -4.6119e-06],
        [-1.8090e-05, -1.2517e-05,  2.5369e-06,  ..., -1.5348e-05,
         -1.9725e-06, -6.8545e-06],
        [-2.7686e-05, -1.9193e-05,  3.8594e-06,  ..., -2.3484e-05,
         -3.0361e-06, -1.0505e-05]], device='cuda:0')
Loss: 1.1591320037841797


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[    0,   685,  2461,  ...,   287,   298,  7943],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7618e-05, -3.3406e-05,  2.9813e-05,  ...,  4.1400e-05,
         -4.2536e-05,  9.3357e-05],
        [-1.4648e-05, -1.0133e-05,  2.0284e-06,  ..., -1.2442e-05,
         -1.6205e-06, -5.5805e-06],
        [-1.5199e-05, -1.0505e-05,  2.1029e-06,  ..., -1.2890e-05,
         -1.6801e-06, -5.7817e-06],
        [-2.2620e-05, -1.5631e-05,  3.1330e-06,  ..., -1.9193e-05,
         -2.4904e-06, -8.5905e-06],
        [-3.4600e-05, -2.3961e-05,  4.7684e-06,  ..., -2.9355e-05,
         -3.8333e-06, -1.3158e-05]], device='cuda:0')
Loss: 1.1683323383331299


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0735e-06, -6.2444e-05,  4.7445e-05,  ...,  3.6561e-05,
         -5.9167e-05,  9.4177e-05],
        [-1.7583e-05, -1.2174e-05,  2.4382e-06,  ..., -1.4961e-05,
         -1.9707e-06, -6.7130e-06],
        [-1.8254e-05, -1.2636e-05,  2.5313e-06,  ..., -1.5512e-05,
         -2.0433e-06, -6.9588e-06],
        [-2.7150e-05, -1.8775e-05,  3.7663e-06,  ..., -2.3067e-05,
         -3.0268e-06, -1.0327e-05],
        [-4.1485e-05, -2.8759e-05,  5.7295e-06,  ..., -3.5286e-05,
         -4.6529e-06, -1.5810e-05]], device='cuda:0')
Loss: 1.1919209957122803


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0, 2310,  292,  ...,  462,  508,  586]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4694e-05, -4.5154e-05,  2.8173e-05,  ...,  4.1069e-05,
         -4.7754e-05,  8.2188e-05],
        [-2.0489e-05, -1.4201e-05,  2.8443e-06,  ..., -1.7449e-05,
         -2.2985e-06, -7.8306e-06],
        [-2.1264e-05, -1.4737e-05,  2.9542e-06,  ..., -1.8090e-05,
         -2.3842e-06, -8.1211e-06],
        [-3.1620e-05, -2.1905e-05,  4.3958e-06,  ..., -2.6911e-05,
         -3.5297e-06, -1.2055e-05],
        [-4.8339e-05, -3.3528e-05,  6.6906e-06,  ..., -4.1157e-05,
         -5.4277e-06, -1.8448e-05]], device='cuda:0')
Loss: 1.16437828540802


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  292,   33,  ...,  352,  266, 9129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6867e-05,  1.4703e-06,  2.5866e-05,  ...,  2.5254e-05,
         -4.4737e-05,  1.1372e-04],
        [-2.3440e-05, -1.6242e-05,  3.2354e-06,  ..., -1.9968e-05,
         -2.6282e-06, -8.9556e-06],
        [-2.4304e-05, -1.6838e-05,  3.3565e-06,  ..., -2.0668e-05,
         -2.7232e-06, -9.2760e-06],
        [-3.6150e-05, -2.5064e-05,  4.9993e-06,  ..., -3.0786e-05,
         -4.0364e-06, -1.3784e-05],
        [-5.5283e-05, -3.8356e-05,  7.6070e-06,  ..., -4.7058e-05,
         -6.2063e-06, -2.1085e-05]], device='cuda:0')
Loss: 1.1686780452728271
Graident accumulation at epoch 0, step 279, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.6369e-05, -2.8709e-05, -7.1314e-06,  ...,  4.5162e-05,
         -2.7494e-05,  3.7241e-05],
        [-2.5957e-05, -1.8287e-05,  3.0353e-06,  ..., -2.1171e-05,
         -2.5945e-06, -1.0003e-05],
        [ 3.5897e-05,  2.7103e-05, -3.6220e-06,  ...,  3.0631e-05,
          1.8092e-06,  1.1546e-05],
        [-2.8434e-05, -1.8388e-05,  4.3618e-06,  ..., -2.2568e-05,
         -2.9779e-06, -1.0170e-05],
        [-5.9175e-05, -4.1642e-05,  6.3600e-06,  ..., -4.7928e-05,
         -5.9293e-06, -2.2702e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7335e-08, 1.4616e-08, 1.7351e-08,  ..., 1.9899e-08, 3.1506e-08,
         6.5367e-09],
        [2.9928e-11, 1.8545e-11, 1.0042e-12,  ..., 1.9749e-11, 6.2608e-13,
         4.0392e-12],
        [3.2434e-10, 2.1740e-10, 1.1676e-11,  ..., 2.9725e-10, 5.0819e-12,
         6.8209e-11],
        [1.8103e-10, 2.5537e-10, 4.7534e-12,  ..., 1.3384e-10, 6.6155e-12,
         4.7099e-11],
        [1.2583e-10, 7.1048e-11, 5.0787e-12,  ..., 8.3755e-11, 2.1150e-12,
         1.8597e-11]], device='cuda:0')
optimizer state dict: 35.0
lr: [1.7191947575507777e-05, 1.7191947575507777e-05]
scheduler_last_epoch: 35


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8984e-06, -1.2248e-05, -1.6180e-05,  ..., -1.9912e-06,
         -4.1883e-06,  3.3089e-05],
        [-2.9355e-06, -2.0564e-06,  4.7311e-07,  ..., -2.5183e-06,
         -3.7998e-07, -1.1325e-06],
        [-3.0100e-06, -2.1160e-06,  4.8429e-07,  ..., -2.5928e-06,
         -3.8929e-07, -1.1623e-06],
        [-4.5002e-06, -3.1590e-06,  7.2643e-07,  ..., -3.8743e-06,
         -5.8115e-07, -1.7360e-06],
        [-6.7949e-06, -4.7684e-06,  1.0878e-06,  ..., -5.8413e-06,
         -8.7917e-07, -2.6226e-06]], device='cuda:0')
Loss: 1.1698123216629028


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[   0, 1850,  311,  ..., 3655, 3133, 9000],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5184e-06, -4.9864e-05, -4.8078e-05,  ..., -1.2262e-06,
         -6.0020e-07,  5.7673e-05],
        [-5.8860e-06, -4.1425e-06,  9.2946e-07,  ..., -5.0366e-06,
         -7.6368e-07, -2.2650e-06],
        [ 3.6012e-04,  2.6826e-04, -5.5252e-05,  ...,  2.9505e-04,
          3.9975e-05,  1.0995e-04],
        [-9.0003e-06, -6.3330e-06,  1.4231e-06,  ..., -7.7188e-06,
         -1.1660e-06, -3.4645e-06],
        [-1.3620e-05, -9.5665e-06,  2.1383e-06,  ..., -1.1683e-05,
         -1.7658e-06, -5.2452e-06]], device='cuda:0')
Loss: 1.1683560609817505


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8662e-05, -4.4306e-05, -4.1693e-05,  ...,  5.5047e-06,
         -5.2185e-05,  5.0560e-05],
        [-8.7917e-06, -6.1691e-06,  1.3728e-06,  ..., -7.5549e-06,
         -1.1194e-06, -3.3975e-06],
        [ 3.5713e-04,  2.6617e-04, -5.4795e-05,  ...,  2.9246e-04,
          3.9609e-05,  1.0878e-04],
        [-1.3471e-05, -9.4324e-06,  2.1048e-06,  ..., -1.1563e-05,
         -1.7099e-06, -5.2005e-06],
        [-2.0355e-05, -1.4246e-05,  3.1665e-06,  ..., -1.7524e-05,
         -2.5891e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.1633987426757812


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0,  461,  654,  ..., 6548, 7171,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6864e-05, -7.7929e-05, -4.6637e-05,  ..., -2.1774e-05,
         -7.2634e-05,  6.9460e-05],
        [-1.1742e-05, -8.2254e-06,  1.8049e-06,  ..., -1.0043e-05,
         -1.5162e-06, -4.5151e-06],
        [ 3.5407e-04,  2.6404e-04, -5.4350e-05,  ...,  2.8988e-04,
          3.9202e-05,  1.0762e-04],
        [-1.8001e-05, -1.2577e-05,  2.7679e-06,  ..., -1.5363e-05,
         -2.3134e-06, -6.9067e-06],
        [-2.7180e-05, -1.9014e-05,  4.1649e-06,  ..., -2.3276e-05,
         -3.5055e-06, -1.0446e-05]], device='cuda:0')
Loss: 1.159032940864563


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5818e-05, -1.1282e-04, -4.7971e-05,  ..., -1.0565e-05,
         -5.1093e-05,  1.0773e-04],
        [ 7.1246e-05,  4.7030e-05, -1.6226e-05,  ...,  6.3477e-05,
          2.1428e-05,  3.3837e-05],
        [ 5.4886e-04,  3.9693e-04, -7.6858e-05,  ...,  4.2718e-04,
          6.1255e-05,  1.7166e-04],
        [-2.2531e-05, -1.5736e-05,  3.4720e-06,  ..., -1.9237e-05,
         -2.9244e-06, -8.6501e-06],
        [-3.4064e-05, -2.3812e-05,  5.2229e-06,  ..., -2.9176e-05,
         -4.4368e-06, -1.3098e-05]], device='cuda:0')
Loss: 1.1839511394500732


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[    0,    14, 49045,  ...,    12,   706,   409],
        [    0,   806,   300,  ...,   360,  4918,  1106]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8728e-06, -1.0225e-04, -5.6430e-05,  ..., -2.5225e-05,
         -4.3812e-05,  1.1328e-04],
        [ 6.8266e-05,  4.4944e-05, -1.5777e-05,  ...,  6.0929e-05,
          2.1024e-05,  3.2704e-05],
        [ 5.4582e-04,  3.9479e-04, -7.6402e-05,  ...,  4.2459e-04,
          6.0841e-05,  1.7051e-04],
        [-2.7031e-05, -1.8895e-05,  4.1500e-06,  ..., -2.3082e-05,
         -3.5353e-06, -1.0364e-05],
        [-4.0919e-05, -2.8610e-05,  6.2510e-06,  ..., -3.5018e-05,
         -5.3681e-06, -1.5706e-05]], device='cuda:0')
Loss: 1.1690109968185425


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[  0, 669,  14,  ..., 596, 292, 494],
        [  0, 432, 984,  ..., 287, 496,  14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5255e-05, -6.5648e-05, -8.0928e-05,  ..., -1.3966e-05,
         -3.9807e-05,  1.2363e-04],
        [ 6.5301e-05,  4.2902e-05, -1.5322e-05,  ...,  5.8366e-05,
          2.0650e-05,  3.1557e-05],
        [ 5.4278e-04,  3.9268e-04, -7.5933e-05,  ...,  4.2195e-04,
          6.0458e-05,  1.6933e-04],
        [-3.1531e-05, -2.2009e-05,  4.8429e-06,  ..., -2.6956e-05,
         -4.1015e-06, -1.2100e-05],
        [-4.7773e-05, -3.3349e-05,  7.3016e-06,  ..., -4.0948e-05,
         -6.2324e-06, -1.8358e-05]], device='cuda:0')
Loss: 1.176885724067688


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  266,  720,  342],
        [   0, 2992,  352,  ...,  259, 2063, 6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1504e-05, -8.6127e-05, -9.6041e-05,  ..., -4.8997e-06,
         -4.0863e-05,  1.0669e-04],
        [ 6.2350e-05,  4.0846e-05, -1.4862e-05,  ...,  5.5848e-05,
          2.0253e-05,  3.0409e-05],
        [ 5.3976e-04,  3.9058e-04, -7.5461e-05,  ...,  4.1937e-04,
          6.0052e-05,  1.6816e-04],
        [-3.6031e-05, -2.5138e-05,  5.5470e-06,  ..., -3.0801e-05,
         -4.7050e-06, -1.3843e-05],
        [-5.4568e-05, -3.8058e-05,  8.3596e-06,  ..., -4.6730e-05,
         -7.1451e-06, -2.0981e-05]], device='cuda:0')
Loss: 1.1701114177703857
Graident accumulation at epoch 0, step 287, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0034,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0269,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0883e-05, -3.4451e-05, -1.6022e-05,  ...,  4.0156e-05,
         -2.8831e-05,  4.4186e-05],
        [-1.7127e-05, -1.2374e-05,  1.2455e-06,  ..., -1.3469e-05,
         -3.0978e-07, -5.9615e-06],
        [ 8.6283e-05,  6.3450e-05, -1.0806e-05,  ...,  6.9505e-05,
          7.6335e-06,  2.7208e-05],
        [-2.9194e-05, -1.9063e-05,  4.4803e-06,  ..., -2.3392e-05,
         -3.1506e-06, -1.0537e-05],
        [-5.8715e-05, -4.1283e-05,  6.5599e-06,  ..., -4.7808e-05,
         -6.0508e-06, -2.2530e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7288e-08, 1.4608e-08, 1.7343e-08,  ..., 1.9879e-08, 3.1477e-08,
         6.5416e-09],
        [3.3785e-11, 2.0195e-11, 1.2240e-12,  ..., 2.2848e-11, 1.0356e-12,
         4.9599e-12],
        [6.1535e-10, 3.6973e-10, 1.7358e-11,  ..., 4.7282e-10, 8.6830e-12,
         9.6419e-11],
        [1.8215e-10, 2.5575e-10, 4.7794e-12,  ..., 1.3465e-10, 6.6310e-12,
         4.7244e-11],
        [1.2868e-10, 7.2426e-11, 5.1435e-12,  ..., 8.5855e-11, 2.1639e-12,
         1.9018e-11]], device='cuda:0')
optimizer state dict: 36.0
lr: [1.7017987415646643e-05, 1.7017987415646643e-05]
scheduler_last_epoch: 36


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,  1890,   278,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5686e-05, -3.1535e-06, -1.4891e-05,  ..., -6.3715e-06,
          2.1569e-05, -6.8786e-06],
        [-2.9504e-06, -2.0564e-06,  4.8801e-07,  ..., -2.5481e-06,
         -4.2655e-07, -1.1399e-06],
        [-3.0100e-06, -2.1011e-06,  4.9919e-07,  ..., -2.6077e-06,
         -4.3586e-07, -1.1623e-06],
        [-4.4703e-06, -3.0994e-06,  7.4133e-07,  ..., -3.8445e-06,
         -6.4448e-07, -1.7211e-06],
        [-6.7651e-06, -4.7088e-06,  1.1176e-06,  ..., -5.8115e-06,
         -9.8348e-07, -2.6077e-06]], device='cuda:0')
Loss: 1.1471234560012817


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,  7849,   278,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7510e-05, -2.2594e-05, -2.7862e-05,  ..., -4.2971e-05,
          3.5054e-05,  5.7346e-07],
        [-5.8860e-06, -4.1127e-06,  1.0245e-06,  ..., -5.0664e-06,
         -8.8289e-07, -2.3022e-06],
        [-6.0201e-06, -4.2170e-06,  1.0505e-06,  ..., -5.2005e-06,
         -9.0338e-07, -2.3544e-06],
        [-8.8811e-06, -6.1989e-06,  1.5497e-06,  ..., -7.6443e-06,
         -1.3299e-06, -3.4720e-06],
        [-1.3441e-05, -9.4175e-06,  2.3395e-06,  ..., -1.1563e-05,
         -2.0266e-06, -5.2601e-06]], device='cuda:0')
Loss: 1.1724557876586914


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  3658,   271,  ...,   278,   970,    12],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1828e-05, -1.3969e-05, -1.7269e-05,  ..., -7.1389e-05,
          4.2048e-05,  1.5698e-05],
        [-8.8811e-06, -6.1989e-06,  1.4808e-06,  ..., -7.6145e-06,
         -1.3318e-06, -3.4496e-06],
        [-9.0748e-06, -6.3330e-06,  1.5162e-06,  ..., -7.8082e-06,
         -1.3597e-06, -3.5241e-06],
        [-1.3381e-05, -9.3132e-06,  2.2352e-06,  ..., -1.1459e-05,
         -2.0005e-06, -5.1931e-06],
        [-2.0295e-05, -1.4186e-05,  3.3751e-06,  ..., -1.7405e-05,
         -3.0547e-06, -7.8827e-06]], device='cuda:0')
Loss: 1.1576026678085327


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   298,   335,   298],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8419e-05, -5.5621e-06, -3.6583e-06,  ..., -6.5911e-05,
          4.2048e-05, -3.3670e-05],
        [-1.1846e-05, -8.2701e-06,  1.9800e-06,  ..., -1.0133e-05,
         -1.8086e-06, -4.5672e-06],
        [-1.2100e-05, -8.4490e-06,  2.0228e-06,  ..., -1.0386e-05,
         -1.8477e-06, -4.6641e-06],
        [-1.7822e-05, -1.2413e-05,  2.9802e-06,  ..., -1.5244e-05,
         -2.7120e-06, -6.8694e-06],
        [-2.7120e-05, -1.8954e-05,  4.5151e-06,  ..., -2.3216e-05,
         -4.1574e-06, -1.0461e-05]], device='cuda:0')
Loss: 1.158742070198059


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[   0,  397, 1267,  ..., 1276,  292,  221],
        [   0,   14,  475,  ..., 6895, 5842, 2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6454e-05,  9.2546e-06, -2.1877e-05,  ..., -9.3094e-05,
          4.1276e-05, -6.2447e-05],
        [-1.4797e-05, -1.0356e-05,  2.4680e-06,  ..., -1.2651e-05,
         -2.2277e-06, -5.6997e-06],
        [-1.5095e-05, -1.0565e-05,  2.5183e-06,  ..., -1.2949e-05,
         -2.2743e-06, -5.8115e-06],
        [-2.2262e-05, -1.5542e-05,  3.7178e-06,  ..., -1.9029e-05,
         -3.3379e-06, -8.5682e-06],
        [-3.3855e-05, -2.3693e-05,  5.6252e-06,  ..., -2.8938e-05,
         -5.1111e-06, -1.3024e-05]], device='cuda:0')
Loss: 1.1676881313323975


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,   741,   266,  ...,   271,  5166,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6414e-05, -1.8873e-05,  1.3739e-06,  ..., -9.9542e-05,
          1.2165e-05, -3.9848e-05],
        [-1.7762e-05, -1.2457e-05,  2.9597e-06,  ..., -1.5184e-05,
         -2.6654e-06, -6.8173e-06],
        [-1.8135e-05, -1.2726e-05,  3.0249e-06,  ..., -1.5542e-05,
         -2.7232e-06, -6.9514e-06],
        [-2.6792e-05, -1.8746e-05,  4.4703e-06,  ..., -2.2873e-05,
         -4.0010e-06, -1.0259e-05],
        [-4.0621e-05, -2.8491e-05,  6.7428e-06,  ..., -3.4690e-05,
         -6.1095e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.1623785495758057


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0,  1737,   278,  ...,  2604,   367,  2002]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8810e-05, -4.5307e-05, -2.0027e-06,  ..., -8.4244e-05,
          2.6726e-05, -9.3355e-06],
        [-2.0757e-05, -1.4573e-05,  3.4738e-06,  ..., -1.7717e-05,
         -3.1237e-06, -7.9572e-06],
        [-2.1189e-05, -1.4886e-05,  3.5502e-06,  ..., -1.8120e-05,
         -3.1907e-06, -8.1137e-06],
        [-3.1322e-05, -2.1949e-05,  5.2527e-06,  ..., -2.6718e-05,
         -4.6939e-06, -1.1988e-05],
        [-4.7445e-05, -3.3289e-05,  7.9125e-06,  ..., -4.0442e-05,
         -7.1526e-06, -1.8150e-05]], device='cuda:0')
Loss: 1.1716704368591309


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3894e-05, -6.5988e-05,  3.5815e-05,  ..., -9.1348e-05,
          1.1661e-05, -9.3355e-06],
        [-2.3708e-05, -1.6645e-05,  3.9767e-06,  ..., -2.0221e-05,
         -3.5558e-06, -9.1046e-06],
        [-2.4199e-05, -1.6987e-05,  4.0606e-06,  ..., -2.0668e-05,
         -3.6303e-06, -9.2834e-06],
        [-3.5822e-05, -2.5108e-05,  6.0201e-06,  ..., -3.0532e-05,
         -5.3495e-06, -1.3739e-05],
        [-5.4210e-05, -3.8028e-05,  9.0599e-06,  ..., -4.6164e-05,
         -8.1435e-06, -2.0772e-05]], device='cuda:0')
Loss: 1.1624201536178589
Graident accumulation at epoch 0, step 295, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0145, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9184e-05, -3.7604e-05, -1.0839e-05,  ...,  2.7005e-05,
         -2.4782e-05,  3.8834e-05],
        [-1.7785e-05, -1.2801e-05,  1.5186e-06,  ..., -1.4144e-05,
         -6.3438e-07, -6.2759e-06],
        [ 7.5235e-05,  5.5406e-05, -9.3193e-06,  ...,  6.0487e-05,
          6.5071e-06,  2.3558e-05],
        [-2.9857e-05, -1.9668e-05,  4.6343e-06,  ..., -2.4106e-05,
         -3.3705e-06, -1.0857e-05],
        [-5.8264e-05, -4.0958e-05,  6.8099e-06,  ..., -4.7643e-05,
         -6.2601e-06, -2.2354e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7244e-08, 1.4598e-08, 1.7327e-08,  ..., 1.9868e-08, 3.1445e-08,
         6.5351e-09],
        [3.4314e-11, 2.0452e-11, 1.2386e-12,  ..., 2.3234e-11, 1.0472e-12,
         5.0378e-12],
        [6.1532e-10, 3.6965e-10, 1.7358e-11,  ..., 4.7277e-10, 8.6875e-12,
         9.6408e-11],
        [1.8325e-10, 2.5612e-10, 4.8109e-12,  ..., 1.3545e-10, 6.6530e-12,
         4.7386e-11],
        [1.3149e-10, 7.3800e-11, 5.2205e-12,  ..., 8.7900e-11, 2.2281e-12,
         1.9431e-11]], device='cuda:0')
optimizer state dict: 37.0
lr: [1.6839737780707125e-05, 1.6839737780707125e-05]
scheduler_last_epoch: 37


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,   292,    36,     9],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7465e-05,  1.6617e-05, -8.1757e-06,  ...,  1.5617e-06,
          6.7474e-06, -6.7230e-06],
        [-2.9951e-06, -2.0862e-06,  5.5134e-07,  ..., -2.5481e-06,
         -4.9919e-07, -1.1846e-06],
        [-3.0547e-06, -2.1160e-06,  5.6252e-07,  ..., -2.5928e-06,
         -5.0664e-07, -1.1995e-06],
        [-4.5002e-06, -3.1143e-06,  8.3074e-07,  ..., -3.8147e-06,
         -7.4506e-07, -1.7732e-06],
        [-6.7353e-06, -4.6790e-06,  1.2368e-06,  ..., -5.7220e-06,
         -1.1250e-06, -2.6524e-06]], device='cuda:0')
Loss: 1.1462440490722656


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,  9466,    36,  ...,  1795,   437,   874],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5071e-06,  3.7715e-05, -1.8297e-05,  ...,  8.1663e-06,
         -8.0785e-07,  1.1823e-05],
        [-5.9903e-06, -4.1425e-06,  1.1325e-06,  ..., -5.0962e-06,
         -1.0096e-06, -2.3544e-06],
        [-6.0946e-06, -4.2170e-06,  1.1511e-06,  ..., -5.1856e-06,
         -1.0245e-06, -2.3916e-06],
        [-8.9705e-06, -6.1989e-06,  1.6987e-06,  ..., -7.6294e-06,
         -1.5050e-06, -3.5241e-06],
        [-1.3471e-05, -9.2983e-06,  2.5406e-06,  ..., -1.1444e-05,
         -2.2724e-06, -5.2750e-06]], device='cuda:0')
Loss: 1.1623762845993042


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6596e-06,  5.6340e-05, -5.7913e-05,  ...,  5.6569e-06,
          7.0485e-07, -5.5093e-06],
        [-8.9854e-06, -6.2138e-06,  1.6801e-06,  ..., -7.6741e-06,
         -1.4715e-06, -3.4943e-06],
        [-9.0897e-06, -6.3032e-06,  1.6987e-06,  ..., -7.7635e-06,
         -1.4864e-06, -3.5316e-06],
        [-1.3411e-05, -9.2834e-06,  2.5146e-06,  ..., -1.1444e-05,
         -2.1942e-06, -5.2154e-06],
        [-2.0146e-05, -1.3947e-05,  3.7625e-06,  ..., -1.7196e-05,
         -3.3081e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.1677489280700684


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0,  2344,   271,  ...,  5415,    14,  1075],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9462e-05,  3.5480e-05, -8.0050e-05,  ..., -1.5410e-05,
         -3.2567e-06,  9.0415e-06],
        [-1.1981e-05, -8.2850e-06,  2.2352e-06,  ..., -1.0252e-05,
         -1.9893e-06, -4.6715e-06],
        [-1.2130e-05, -8.4192e-06,  2.2650e-06,  ..., -1.0386e-05,
         -2.0117e-06, -4.7311e-06],
        [-1.7881e-05, -1.2383e-05,  3.3490e-06,  ..., -1.5289e-05,
         -2.9653e-06, -6.9663e-06],
        [-2.6882e-05, -1.8597e-05,  5.0142e-06,  ..., -2.2978e-05,
         -4.4703e-06, -1.0446e-05]], device='cuda:0')
Loss: 1.174958348274231


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0,  578,  221,  ...,  287, 1254, 4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0250e-05,  7.4258e-05, -1.6192e-04,  ...,  2.0183e-05,
          1.8714e-06,  7.0921e-06],
        [-1.4976e-05, -1.0371e-05,  2.8163e-06,  ..., -1.2800e-05,
         -2.4643e-06, -5.8040e-06],
        [-1.5184e-05, -1.0550e-05,  2.8573e-06,  ..., -1.2979e-05,
         -2.4959e-06, -5.8860e-06],
        [-2.2352e-05, -1.5497e-05,  4.2170e-06,  ..., -1.9073e-05,
         -3.6694e-06, -8.6501e-06],
        [-3.3587e-05, -2.3276e-05,  6.3106e-06,  ..., -2.8670e-05,
         -5.5283e-06, -1.2964e-05]], device='cuda:0')
Loss: 1.164495587348938


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  9472, 15004,   511],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7132e-05,  8.4150e-05, -1.9600e-04,  ...,  4.2423e-05,
          8.7239e-06, -6.8098e-05],
        [-1.7986e-05, -1.2457e-05,  3.3639e-06,  ..., -1.5333e-05,
         -2.9597e-06, -6.9886e-06],
        [-1.8239e-05, -1.2666e-05,  3.4161e-06,  ..., -1.5557e-05,
         -2.9989e-06, -7.0855e-06],
        [-2.6882e-05, -1.8626e-05,  5.0440e-06,  ..., -2.2888e-05,
         -4.4145e-06, -1.0423e-05],
        [-4.0352e-05, -2.7955e-05,  7.5400e-06,  ..., -3.4362e-05,
         -6.6459e-06, -1.5602e-05]], device='cuda:0')
Loss: 1.1494168043136597


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   401,  3740,  ...,  5980,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7926e-05,  8.1007e-05, -1.6172e-04,  ...,  2.4028e-05,
          1.8579e-05, -4.9862e-05],
        [-2.0996e-05, -1.4558e-05,  3.9153e-06,  ..., -1.7881e-05,
         -3.4478e-06, -8.1435e-06],
        [-2.1279e-05, -1.4797e-05,  3.9749e-06,  ..., -1.8135e-05,
         -3.4906e-06, -8.2552e-06],
        [-3.1382e-05, -2.1785e-05,  5.8748e-06,  ..., -2.6703e-05,
         -5.1446e-06, -1.2152e-05],
        [-4.7117e-05, -3.2693e-05,  8.7842e-06,  ..., -4.0114e-05,
         -7.7486e-06, -1.8194e-05]], device='cuda:0')
Loss: 1.1567437648773193


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  365, 2849,  ...,    9, 3365, 5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4818e-05,  7.8615e-05, -1.3475e-04,  ...,  2.5103e-06,
          3.9460e-05, -6.6544e-05],
        [-2.4006e-05, -1.6645e-05,  4.4964e-06,  ..., -2.0444e-05,
         -3.9283e-06, -9.3132e-06],
        [-2.4319e-05, -1.6898e-05,  4.5635e-06,  ..., -2.0713e-05,
         -3.9786e-06, -9.4399e-06],
        [-3.5882e-05, -2.4885e-05,  6.7428e-06,  ..., -3.0518e-05,
         -5.8599e-06, -1.3895e-05],
        [-5.3883e-05, -3.7372e-05,  1.0088e-05,  ..., -4.5866e-05,
         -8.8289e-06, -2.0817e-05]], device='cuda:0')
Loss: 1.1741068363189697
Graident accumulation at epoch 0, step 303, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0021],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3747e-05, -2.5982e-05, -2.3230e-05,  ...,  2.4556e-05,
         -1.8357e-05,  2.8296e-05],
        [-1.8407e-05, -1.3185e-05,  1.8164e-06,  ..., -1.4774e-05,
         -9.6377e-07, -6.5796e-06],
        [ 6.5280e-05,  4.8176e-05, -7.9310e-06,  ...,  5.2367e-05,
          5.4585e-06,  2.0259e-05],
        [-3.0459e-05, -2.0190e-05,  4.8451e-06,  ..., -2.4747e-05,
         -3.6194e-06, -1.1161e-05],
        [-5.7826e-05, -4.0599e-05,  7.1377e-06,  ..., -4.7466e-05,
         -6.5170e-06, -2.2201e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7197e-08, 1.4590e-08, 1.7328e-08,  ..., 1.9848e-08, 3.1415e-08,
         6.5330e-09],
        [3.4856e-11, 2.0709e-11, 1.2576e-12,  ..., 2.3629e-11, 1.0616e-12,
         5.1195e-12],
        [6.1530e-10, 3.6956e-10, 1.7361e-11,  ..., 4.7273e-10, 8.6946e-12,
         9.6401e-11],
        [1.8435e-10, 2.5649e-10, 4.8516e-12,  ..., 1.3625e-10, 6.6807e-12,
         4.7531e-11],
        [1.3426e-10, 7.5122e-11, 5.3170e-12,  ..., 8.9916e-11, 2.3038e-12,
         1.9845e-11]], device='cuda:0')
optimizer state dict: 38.0
lr: [1.6657307618927726e-05, 1.6657307618927726e-05]
scheduler_last_epoch: 38


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,   377,   472,  ...,  9256,  3807,  5499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1512e-05, -8.9756e-06, -1.2973e-05,  ...,  3.9911e-06,
         -1.0637e-05, -3.0551e-05],
        [-2.9951e-06, -2.1011e-06,  6.1467e-07,  ..., -2.5928e-06,
         -5.2899e-07, -1.1846e-06],
        [-3.0100e-06, -2.1160e-06,  6.1467e-07,  ..., -2.6077e-06,
         -5.2899e-07, -1.1921e-06],
        [-4.4107e-06, -3.0994e-06,  9.0525e-07,  ..., -3.8147e-06,
         -7.7486e-07, -1.7434e-06],
        [-6.5863e-06, -4.6194e-06,  1.3486e-06,  ..., -5.6922e-06,
         -1.1623e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.16884446144104


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[    0, 10565,  2677,  ...,   298,   292, 11188],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4344e-05, -3.5777e-06, -2.6787e-05,  ...,  7.7417e-06,
         -3.2238e-05, -3.0789e-05],
        [-5.9903e-06, -4.2021e-06,  1.2107e-06,  ..., -5.1558e-06,
         -1.0692e-06, -2.3618e-06],
        [-6.0201e-06, -4.2319e-06,  1.2144e-06,  ..., -5.1856e-06,
         -1.0729e-06, -2.3767e-06],
        [-8.8513e-06, -6.2138e-06,  1.7956e-06,  ..., -7.6145e-06,
         -1.5795e-06, -3.4943e-06],
        [-1.3232e-05, -9.2685e-06,  2.6748e-06,  ..., -1.1384e-05,
         -2.3693e-06, -5.2005e-06]], device='cuda:0')
Loss: 1.1561335325241089


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9930e-05,  2.9767e-05, -3.0708e-05,  ..., -2.2043e-05,
         -4.3761e-06, -6.6089e-05],
        [-8.9556e-06, -6.3032e-06,  1.8366e-06,  ..., -7.7486e-06,
         -1.6391e-06, -3.5390e-06],
        [-9.0003e-06, -6.3479e-06,  1.8403e-06,  ..., -7.7784e-06,
         -1.6466e-06, -3.5614e-06],
        [-1.3262e-05, -9.3430e-06,  2.7306e-06,  ..., -1.1459e-05,
         -2.4252e-06, -5.2452e-06],
        [-1.9789e-05, -1.3918e-05,  4.0531e-06,  ..., -1.7107e-05,
         -3.6284e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.157952070236206


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   292,    65,  ...,    12,   857,   344],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7554e-05,  3.3523e-05, -3.6770e-05,  ..., -9.7328e-06,
         -8.0655e-06, -4.8685e-05],
        [-1.1995e-05, -8.4192e-06,  2.4550e-06,  ..., -1.0327e-05,
         -2.1905e-06, -4.7237e-06],
        [-1.2055e-05, -8.4788e-06,  2.4624e-06,  ..., -1.0371e-05,
         -2.2016e-06, -4.7460e-06],
        [-1.7762e-05, -1.2487e-05,  3.6508e-06,  ..., -1.5303e-05,
         -3.2410e-06, -6.9961e-06],
        [-2.6524e-05, -1.8626e-05,  5.4240e-06,  ..., -2.2829e-05,
         -4.8578e-06, -1.0416e-05]], device='cuda:0')
Loss: 1.1668330430984497


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0, 1159,  278,  ...,    9,  271,  266],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7163e-05, -2.5806e-06, -4.2360e-05,  ...,  1.8975e-05,
         -5.3096e-05, -2.4188e-05],
        [-1.4976e-05, -1.0520e-05,  3.0734e-06,  ..., -1.2860e-05,
         -2.7232e-06, -5.8934e-06],
        [-1.5065e-05, -1.0595e-05,  3.0883e-06,  ..., -1.2934e-05,
         -2.7381e-06, -5.9307e-06],
        [-2.2203e-05, -1.5602e-05,  4.5747e-06,  ..., -1.9073e-05,
         -4.0308e-06, -8.7321e-06],
        [-3.3140e-05, -2.3276e-05,  6.7949e-06,  ..., -2.8431e-05,
         -6.0350e-06, -1.2994e-05]], device='cuda:0')
Loss: 1.1568588018417358


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[  0, 342, 726,  ...,  12, 895, 367],
        [  0, 278, 266,  ..., 292, 474, 221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7603e-05, -3.2008e-05, -6.1836e-05,  ...,  1.9026e-05,
         -6.2280e-05, -3.7712e-05],
        [-1.7941e-05, -1.2621e-05,  3.6918e-06,  ..., -1.5408e-05,
         -3.2485e-06, -7.0557e-06],
        [-1.8060e-05, -1.2726e-05,  3.7141e-06,  ..., -1.5512e-05,
         -3.2671e-06, -7.1004e-06],
        [-2.6643e-05, -1.8746e-05,  5.5023e-06,  ..., -2.2888e-05,
         -4.8093e-06, -1.0461e-05],
        [-3.9726e-05, -2.7955e-05,  8.1658e-06,  ..., -3.4094e-05,
         -7.1973e-06, -1.5557e-05]], device='cuda:0')
Loss: 1.1565499305725098


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,   586,   940,  ...,  1471,  2612,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4238e-05, -4.6603e-05, -9.3953e-05,  ...,  2.2651e-05,
         -8.1129e-05, -1.0687e-05],
        [-2.0951e-05, -1.4707e-05,  4.2953e-06,  ..., -1.7986e-05,
         -3.8333e-06, -8.2254e-06],
        [-2.1115e-05, -1.4842e-05,  4.3251e-06,  ..., -1.8120e-05,
         -3.8594e-06, -8.2850e-06],
        [-3.1084e-05, -2.1830e-05,  6.3963e-06,  ..., -2.6688e-05,
         -5.6699e-06, -1.2182e-05],
        [-4.6402e-05, -3.2574e-05,  9.4995e-06,  ..., -3.9756e-05,
         -8.4862e-06, -1.8135e-05]], device='cuda:0')
Loss: 1.1655933856964111


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[   0,  401, 9370,  ...,    9,  287,  518],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3201e-06, -8.5159e-05, -7.8661e-05,  ..., -6.8096e-06,
         -1.0492e-04, -9.1134e-07],
        [-2.3961e-05, -1.6823e-05,  4.9397e-06,  ..., -2.0549e-05,
         -4.3809e-06, -9.4101e-06],
        [-2.4155e-05, -1.6972e-05,  4.9770e-06,  ..., -2.0713e-05,
         -4.4145e-06, -9.4846e-06],
        [-3.5554e-05, -2.4959e-05,  7.3574e-06,  ..., -3.0488e-05,
         -6.4820e-06, -1.3940e-05],
        [-5.3078e-05, -3.7253e-05,  1.0923e-05,  ..., -4.5389e-05,
         -9.7007e-06, -2.0742e-05]], device='cuda:0')
Loss: 1.1463022232055664
Graident accumulation at epoch 0, step 311, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0294, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0166,  0.0146, -0.0270,  ...,  0.0281, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6540e-05, -3.1900e-05, -2.8773e-05,  ...,  2.1419e-05,
         -2.7014e-05,  2.5375e-05],
        [-1.8962e-05, -1.3549e-05,  2.1287e-06,  ..., -1.5352e-05,
         -1.3055e-06, -6.8626e-06],
        [ 5.6336e-05,  4.1661e-05, -6.6402e-06,  ...,  4.5059e-05,
          4.4712e-06,  1.7284e-05],
        [-3.0969e-05, -2.0667e-05,  5.0963e-06,  ..., -2.5321e-05,
         -3.9057e-06, -1.1439e-05],
        [-5.7351e-05, -4.0265e-05,  7.5162e-06,  ..., -4.7258e-05,
         -6.8354e-06, -2.2055e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7150e-08, 1.4582e-08, 1.7317e-08,  ..., 1.9828e-08, 3.1395e-08,
         6.5265e-09],
        [3.5395e-11, 2.0971e-11, 1.2807e-12,  ..., 2.4027e-11, 1.0798e-12,
         5.2030e-12],
        [6.1527e-10, 3.6948e-10, 1.7368e-11,  ..., 4.7269e-10, 8.7054e-12,
         9.6395e-11],
        [1.8543e-10, 2.5685e-10, 4.9008e-12,  ..., 1.3704e-10, 6.7160e-12,
         4.7678e-11],
        [1.3695e-10, 7.6435e-11, 5.4310e-12,  ..., 9.1886e-11, 2.3956e-12,
         2.0255e-11]], device='cuda:0')
optimizer state dict: 39.0
lr: [1.6470808433733317e-05, 1.6470808433733317e-05]
scheduler_last_epoch: 39


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6100e-06, -1.6881e-05, -1.1229e-05,  ...,  2.5042e-05,
          2.2935e-05,  7.7178e-06],
        [-3.0249e-06, -2.1160e-06,  6.5193e-07,  ..., -2.5779e-06,
         -5.4762e-07, -1.1921e-06],
        [-3.0696e-06, -2.1458e-06,  6.6310e-07,  ..., -2.6226e-06,
         -5.5507e-07, -1.2070e-06],
        [-4.5002e-06, -3.1441e-06,  9.7603e-07,  ..., -3.8445e-06,
         -8.1584e-07, -1.7732e-06],
        [-6.6757e-06, -4.6492e-06,  1.4380e-06,  ..., -5.6624e-06,
         -1.2070e-06, -2.6226e-06]], device='cuda:0')
Loss: 1.1641407012939453


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[    0,  1471,   266,  ...,   525,  5202,   292],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8036e-05, -3.7197e-05, -1.5500e-05,  ...,  2.6017e-05,
         -8.7013e-06, -1.4139e-05],
        [-6.0201e-06, -4.2021e-06,  1.3337e-06,  ..., -5.1409e-06,
         -1.1027e-06, -2.3544e-06],
        [-6.0946e-06, -4.2617e-06,  1.3523e-06,  ..., -5.2154e-06,
         -1.1176e-06, -2.3842e-06],
        [-8.9705e-06, -6.2734e-06,  1.9968e-06,  ..., -7.6890e-06,
         -1.6466e-06, -3.5167e-06],
        [-1.3262e-05, -9.2685e-06,  2.9355e-06,  ..., -1.1295e-05,
         -2.4289e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.149956226348877


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0, 3978, 2697,  ...,  461, 5955, 3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3314e-05, -2.6734e-05, -4.2988e-05,  ...,  2.7365e-05,
          3.0836e-05, -8.5161e-05],
        [-9.0152e-06, -6.3032e-06,  1.9819e-06,  ..., -7.7486e-06,
         -1.7062e-06, -3.5539e-06],
        [-9.0897e-06, -6.3628e-06,  2.0005e-06,  ..., -7.8380e-06,
         -1.7211e-06, -3.5837e-06],
        [-1.3381e-05, -9.3728e-06,  2.9579e-06,  ..., -1.1533e-05,
         -2.5369e-06, -5.2825e-06],
        [-1.9819e-05, -1.3858e-05,  4.3511e-06,  ..., -1.7017e-05,
         -3.7551e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.1811885833740234


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1587e-05,  1.8171e-05, -8.8074e-05,  ...,  2.1213e-05,
          4.1650e-05, -7.0203e-05],
        [-1.2025e-05, -8.4043e-06,  2.6263e-06,  ..., -1.0312e-05,
         -2.2538e-06, -4.7237e-06],
        [ 6.0224e-05,  5.2464e-05, -1.9467e-05,  ...,  7.9423e-05,
          3.0669e-05,  4.0228e-05],
        [-1.7881e-05, -1.2532e-05,  3.9265e-06,  ..., -1.5378e-05,
         -3.3565e-06, -7.0333e-06],
        [-2.6464e-05, -1.8507e-05,  5.7742e-06,  ..., -2.2680e-05,
         -4.9695e-06, -1.0371e-05]], device='cuda:0')
Loss: 1.1507225036621094


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0, 2663,  328,  ...,  342,  266, 1163]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8162e-05, -4.1569e-06, -6.6490e-05,  ...,  1.7791e-05,
          3.7426e-05, -7.0203e-05],
        [-1.5035e-05, -1.0520e-05,  3.2857e-06,  ..., -1.2875e-05,
         -2.7940e-06, -5.8934e-06],
        [ 5.7184e-05,  5.0318e-05, -1.8800e-05,  ...,  7.6830e-05,
          3.0121e-05,  3.9043e-05],
        [-2.2382e-05, -1.5706e-05,  4.9174e-06,  ..., -1.9222e-05,
         -4.1649e-06, -8.7842e-06],
        [-3.3110e-05, -2.3186e-05,  7.2271e-06,  ..., -2.8342e-05,
         -6.1616e-06, -1.2949e-05]], device='cuda:0')
Loss: 1.1545343399047852


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7099e-05,  6.6949e-06, -6.7112e-05,  ...,  1.6309e-05,
          2.7966e-05, -1.5499e-05],
        [-1.8016e-05, -1.2636e-05,  3.9600e-06,  ..., -1.5423e-05,
         -3.3565e-06, -7.0482e-06],
        [ 1.5208e-04,  1.0785e-04, -5.5993e-05,  ...,  1.4640e-04,
          6.4905e-05,  8.3497e-05],
        [-2.6882e-05, -1.8910e-05,  5.9381e-06,  ..., -2.3067e-05,
         -5.0142e-06, -1.0528e-05],
        [-3.9667e-05, -2.7865e-05,  8.7097e-06,  ..., -3.3945e-05,
         -7.3984e-06, -1.5482e-05]], device='cuda:0')
Loss: 1.155470609664917


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0706e-06,  9.0363e-06, -5.9952e-05,  ...,  3.9863e-05,
          3.7596e-05,  1.8706e-05],
        [-2.1055e-05, -1.4782e-05,  4.6380e-06,  ..., -1.8001e-05,
         -3.9600e-06, -8.2105e-06],
        [ 1.4898e-04,  1.0566e-04, -5.5300e-05,  ...,  1.4378e-04,
          6.4290e-05,  8.2305e-05],
        [-3.1382e-05, -2.2098e-05,  6.9514e-06,  ..., -2.6882e-05,
         -5.9083e-06, -1.2256e-05],
        [-4.6313e-05, -3.2574e-05,  1.0200e-05,  ..., -3.9577e-05,
         -8.7172e-06, -1.8030e-05]], device='cuda:0')
Loss: 1.151196837425232


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4473e-05,  2.7947e-05, -6.7935e-05,  ...,  1.3017e-05,
          5.9343e-06,  3.2331e-05],
        [-2.4036e-05, -1.6853e-05,  5.2787e-06,  ..., -2.0549e-05,
         -4.5337e-06, -9.4026e-06],
        [ 1.4597e-04,  1.0355e-04, -5.4652e-05,  ...,  1.4120e-04,
          6.3709e-05,  8.1098e-05],
        [-3.5882e-05, -2.5228e-05,  7.9200e-06,  ..., -3.0726e-05,
         -6.7726e-06, -1.4052e-05],
        [-5.2959e-05, -3.7193e-05,  1.1630e-05,  ..., -4.5240e-05,
         -9.9987e-06, -2.0683e-05]], device='cuda:0')
Loss: 1.1469961404800415
Graident accumulation at epoch 0, step 319, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0098,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3334e-05, -2.5915e-05, -3.2689e-05,  ...,  2.0579e-05,
         -2.3719e-05,  2.6071e-05],
        [-1.9470e-05, -1.3879e-05,  2.4437e-06,  ..., -1.5871e-05,
         -1.6283e-06, -7.1166e-06],
        [ 6.5299e-05,  4.7850e-05, -1.1441e-05,  ...,  5.4674e-05,
          1.0395e-05,  2.3666e-05],
        [-3.1460e-05, -2.1123e-05,  5.3787e-06,  ..., -2.5861e-05,
         -4.1924e-06, -1.1700e-05],
        [-5.6912e-05, -3.9957e-05,  7.9276e-06,  ..., -4.7056e-05,
         -7.1517e-06, -2.1918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7103e-08, 1.4569e-08, 1.7304e-08,  ..., 1.9808e-08, 3.1364e-08,
         6.5210e-09],
        [3.5937e-11, 2.1234e-11, 1.3073e-12,  ..., 2.4426e-11, 1.0992e-12,
         5.2862e-12],
        [6.3596e-10, 3.7984e-10, 2.0338e-11,  ..., 4.9215e-10, 1.2756e-11,
         1.0288e-10],
        [1.8653e-10, 2.5723e-10, 4.9587e-12,  ..., 1.3785e-10, 6.7552e-12,
         4.7828e-11],
        [1.3962e-10, 7.7742e-11, 5.5608e-12,  ..., 9.3841e-11, 2.4932e-12,
         2.0662e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 40
Epoch 0 | Batch 319/1048 | Training PPL: 9662.127333689465 | time 30.336477994918823
Saving checkpoint at epoch 0, step 319, batch 319
Epoch 0 | Validation PPL: 10.075526624689587 | Learning rate: 1.628035421558293e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319, AFTER epoch 0, step 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  3036,   471,  ...,   287,  1906,    12],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-5.3334e-05, -2.5915e-05, -3.2689e-05,  ...,  2.0579e-05,
         -2.3719e-05,  2.6071e-05],
        [-1.9470e-05, -1.3879e-05,  2.4437e-06,  ..., -1.5871e-05,
         -1.6283e-06, -7.1166e-06],
        [ 6.5299e-05,  4.7850e-05, -1.1441e-05,  ...,  5.4674e-05,
          1.0395e-05,  2.3666e-05],
        [-3.1460e-05, -2.1123e-05,  5.3787e-06,  ..., -2.5861e-05,
         -4.1924e-06, -1.1700e-05],
        [-5.6912e-05, -3.9957e-05,  7.9276e-06,  ..., -4.7056e-05,
         -7.1517e-06, -2.1918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7103e-08, 1.4569e-08, 1.7304e-08,  ..., 1.9808e-08, 3.1364e-08,
         6.5210e-09],
        [3.5937e-11, 2.1234e-11, 1.3073e-12,  ..., 2.4426e-11, 1.0992e-12,
         5.2862e-12],
        [6.3596e-10, 3.7984e-10, 2.0338e-11,  ..., 4.9215e-10, 1.2756e-11,
         1.0288e-10],
        [1.8653e-10, 2.5723e-10, 4.9587e-12,  ..., 1.3785e-10, 6.7552e-12,
         4.7828e-11],
        [1.3962e-10, 7.7742e-11, 5.5608e-12,  ..., 9.3841e-11, 2.4932e-12,
         2.0662e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7765e-06, -7.5377e-06, -2.3073e-05,  ..., -1.8885e-05,
          2.4735e-05,  1.0587e-05],
        [-2.9802e-06, -2.1160e-06,  6.8173e-07,  ..., -2.5630e-06,
         -6.3702e-07, -1.1921e-06],
        [-3.0398e-06, -2.1458e-06,  6.9663e-07,  ..., -2.6226e-06,
         -6.5193e-07, -1.2144e-06],
        [-4.4703e-06, -3.1590e-06,  1.0282e-06,  ..., -3.8445e-06,
         -9.5367e-07, -1.7807e-06],
        [-6.5267e-06, -4.6194e-06,  1.4976e-06,  ..., -5.6326e-06,
         -1.4007e-06, -2.6077e-06]], device='cuda:0')
Loss: 1.1565836668014526


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[   0,  266, 2653,  ...,   29,   16,   14],
        [   0,  591,  953,  ..., 4118, 5750,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0810e-06, -1.9954e-05, -4.0750e-05,  ..., -2.2721e-05,
         -2.1820e-05,  3.4739e-06],
        [-5.9307e-06, -4.1872e-06,  1.3746e-06,  ..., -5.1111e-06,
         -1.2144e-06, -2.3693e-06],
        [-6.0648e-06, -4.2617e-06,  1.4082e-06,  ..., -5.2303e-06,
         -1.2442e-06, -2.4214e-06],
        [-8.9407e-06, -6.3032e-06,  2.0787e-06,  ..., -7.6890e-06,
         -1.8291e-06, -3.5614e-06],
        [-1.2994e-05, -9.1493e-06,  3.0175e-06,  ..., -1.1206e-05,
         -2.6673e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.1449159383773804


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  897,  328,  ...,  908,  696,  688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0021e-05, -3.4422e-05, -7.0384e-05,  ..., -1.0496e-05,
         -6.0229e-05,  2.1352e-05],
        [-8.9258e-06, -6.3032e-06,  2.0601e-06,  ..., -7.6890e-06,
         -1.7993e-06, -3.5316e-06],
        [-9.1046e-06, -6.4075e-06,  2.1011e-06,  ..., -7.8380e-06,
         -1.8366e-06, -3.5986e-06],
        [-1.3471e-05, -9.5069e-06,  3.1143e-06,  ..., -1.1593e-05,
         -2.7083e-06, -5.3197e-06],
        [-1.9580e-05, -1.3798e-05,  4.5151e-06,  ..., -1.6868e-05,
         -3.9488e-06, -7.7337e-06]], device='cuda:0')
Loss: 1.1571334600448608


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6591e-05, -1.8697e-05, -9.0558e-05,  ..., -3.1810e-06,
         -6.0720e-05, -2.2421e-05],
        [-1.1876e-05, -8.4043e-06,  2.7604e-06,  ..., -1.0237e-05,
         -2.3730e-06, -4.6864e-06],
        [ 6.9878e-05,  5.4137e-05, -2.2901e-05,  ...,  6.9306e-05,
          2.4800e-05,  2.8911e-05],
        [-1.7941e-05, -1.2696e-05,  4.1798e-06,  ..., -1.5438e-05,
         -3.5763e-06, -7.0706e-06],
        [-2.6017e-05, -1.8358e-05,  6.0424e-06,  ..., -2.2411e-05,
         -5.2005e-06, -1.0252e-05]], device='cuda:0')
Loss: 1.1565489768981934


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,  6067,  1188,  ...,  5282,   756,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8410e-05, -2.4351e-05, -9.7949e-05,  ..., -5.0567e-05,
         -6.6420e-05, -8.8875e-06],
        [-1.4842e-05, -1.0490e-05,  3.4347e-06,  ..., -1.2800e-05,
         -2.9653e-06, -5.8413e-06],
        [ 6.6868e-05,  5.2021e-05, -2.2216e-05,  ...,  6.6698e-05,
          2.4197e-05,  2.7734e-05],
        [-2.2411e-05, -1.5840e-05,  5.2005e-06,  ..., -1.9312e-05,
         -4.4703e-06, -8.8215e-06],
        [-3.2514e-05, -2.2918e-05,  7.5176e-06,  ..., -2.8044e-05,
         -6.5044e-06, -1.2785e-05]], device='cuda:0')
Loss: 1.1583952903747559


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8410e-05,  1.1103e-05, -4.1220e-05,  ..., -4.5105e-05,
         -6.4750e-05,  2.0851e-05],
        [-1.7837e-05, -1.2591e-05,  4.1127e-06,  ..., -1.5348e-05,
         -3.5577e-06, -7.0184e-06],
        [ 6.3783e-05,  4.9861e-05, -2.1519e-05,  ...,  6.4076e-05,
          2.3590e-05,  2.6527e-05],
        [-2.6971e-05, -1.9029e-05,  6.2287e-06,  ..., -2.3186e-05,
         -5.3644e-06, -1.0610e-05],
        [-3.9101e-05, -2.7537e-05,  9.0078e-06,  ..., -3.3647e-05,
         -7.8008e-06, -1.5363e-05]], device='cuda:0')
Loss: 1.1545274257659912


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[   0,  462,  221,  ...,   29,  413, 1801],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6353e-05, -2.4373e-06, -5.2059e-05,  ..., -3.4385e-05,
         -8.6445e-05,  1.8550e-05],
        [-2.0802e-05, -1.4678e-05,  4.8168e-06,  ..., -1.7896e-05,
         -4.1462e-06, -8.2031e-06],
        [ 6.0729e-05,  4.7700e-05, -2.0793e-05,  ...,  6.1453e-05,
          2.2982e-05,  2.5305e-05],
        [-3.1471e-05, -2.2203e-05,  7.3016e-06,  ..., -2.7061e-05,
         -6.2548e-06, -1.2405e-05],
        [-4.5627e-05, -3.2127e-05,  1.0550e-05,  ..., -3.9250e-05,
         -9.0897e-06, -1.7956e-05]], device='cuda:0')
Loss: 1.1485456228256226


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0, 2018, 4798,  ...,  292, 1919,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0215e-05, -5.7324e-06, -5.0371e-05,  ..., -5.0156e-05,
         -1.1476e-04, -1.3874e-05],
        [-2.3723e-05, -1.6764e-05,  5.5246e-06,  ..., -2.0429e-05,
         -4.7237e-06, -9.3803e-06],
        [ 5.7719e-05,  4.5569e-05, -2.0066e-05,  ...,  5.8845e-05,
          2.2390e-05,  2.4091e-05],
        [-3.5971e-05, -2.5392e-05,  8.3894e-06,  ..., -3.0935e-05,
         -7.1414e-06, -1.4216e-05],
        [-5.2035e-05, -3.6687e-05,  1.2100e-05,  ..., -4.4793e-05,
         -1.0356e-05, -2.0534e-05]], device='cuda:0')
Loss: 1.1498072147369385
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1022e-05, -2.3897e-05, -3.4457e-05,  ...,  1.3506e-05,
         -3.2823e-05,  2.2077e-05],
        [-1.9895e-05, -1.4168e-05,  2.7518e-06,  ..., -1.6327e-05,
         -1.9378e-06, -7.3430e-06],
        [ 6.4541e-05,  4.7622e-05, -1.2304e-05,  ...,  5.5091e-05,
          1.1595e-05,  2.3708e-05],
        [-3.1911e-05, -2.1550e-05,  5.6798e-06,  ..., -2.6369e-05,
         -4.4873e-06, -1.1952e-05],
        [-5.6424e-05, -3.9630e-05,  8.3448e-06,  ..., -4.6830e-05,
         -7.4722e-06, -2.1779e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7057e-08, 1.4554e-08, 1.7289e-08,  ..., 1.9791e-08, 3.1345e-08,
         6.5146e-09],
        [3.6464e-11, 2.1494e-11, 1.3365e-12,  ..., 2.4818e-11, 1.1204e-12,
         5.3689e-12],
        [6.3865e-10, 3.8153e-10, 2.0720e-11,  ..., 4.9512e-10, 1.3244e-11,
         1.0335e-10],
        [1.8764e-10, 2.5762e-10, 5.0241e-12,  ..., 1.3867e-10, 6.7994e-12,
         4.7982e-11],
        [1.4218e-10, 7.9010e-11, 5.7017e-12,  ..., 9.5753e-11, 2.5979e-12,
         2.1063e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.6086061372297498e-05, 1.6086061372297498e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0, 12456,    14,  ...,  1822,  1016,   365],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5239e-05,  4.5282e-05, -2.9190e-06,  ...,  8.2821e-06,
          2.2008e-05,  1.2137e-05],
        [-2.9802e-06, -2.1160e-06,  6.8173e-07,  ..., -2.5630e-06,
         -6.2957e-07, -1.1474e-06],
        [-3.0547e-06, -2.1607e-06,  6.9663e-07,  ..., -2.6226e-06,
         -6.4448e-07, -1.1772e-06],
        [-4.5002e-06, -3.1888e-06,  1.0282e-06,  ..., -3.8743e-06,
         -9.4622e-07, -1.7285e-06],
        [-6.5267e-06, -4.6492e-06,  1.4901e-06,  ..., -5.6028e-06,
         -1.3784e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.1528429985046387


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  1853,    12,   709],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2401e-05,  4.5429e-05, -1.5863e-05,  ..., -1.7743e-05,
          1.0295e-05, -2.8428e-05],
        [-5.9009e-06, -4.2021e-06,  1.3821e-06,  ..., -5.1260e-06,
         -1.2293e-06, -2.2948e-06],
        [-6.0797e-06, -4.3064e-06,  1.4231e-06,  ..., -5.2750e-06,
         -1.2629e-06, -2.3618e-06],
        [-8.9705e-06, -6.3777e-06,  2.1011e-06,  ..., -7.8082e-06,
         -1.8589e-06, -3.4794e-06],
        [-1.2964e-05, -9.2387e-06,  3.0324e-06,  ..., -1.1265e-05,
         -2.6971e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.1718459129333496


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7747e-05,  2.1314e-05, -5.3822e-05,  ..., -8.3403e-06,
          2.8042e-05, -2.5419e-05],
        [-8.8364e-06, -6.2883e-06,  2.0824e-06,  ..., -7.6443e-06,
         -1.8328e-06, -3.4347e-06],
        [-9.1493e-06, -6.4969e-06,  2.1569e-06,  ..., -7.9125e-06,
         -1.8962e-06, -3.5539e-06],
        [-1.3471e-05, -9.5814e-06,  3.1814e-06,  ..., -1.1683e-05,
         -2.7865e-06, -5.2303e-06],
        [-1.9461e-05, -1.3858e-05,  4.5821e-06,  ..., -1.6838e-05,
         -4.0382e-06, -7.5549e-06]], device='cuda:0')
Loss: 1.1370643377304077


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,     9,   287,  ...,   259,  8244,  1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1627e-05,  1.1654e-07, -8.0702e-05,  ..., -2.7185e-05,
          2.8366e-05, -6.3797e-06],
        [-1.1757e-05, -8.3894e-06,  2.8126e-06,  ..., -1.0192e-05,
         -2.4326e-06, -4.5672e-06],
        [-1.2174e-05, -8.6725e-06,  2.9132e-06,  ..., -1.0550e-05,
         -2.5146e-06, -4.7237e-06],
        [-1.8001e-05, -1.2830e-05,  4.3139e-06,  ..., -1.5616e-05,
         -3.7104e-06, -6.9737e-06],
        [-2.5898e-05, -1.8477e-05,  6.1840e-06,  ..., -2.2441e-05,
         -5.3570e-06, -1.0028e-05]], device='cuda:0')
Loss: 1.1497355699539185


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8465e-05,  1.6379e-05, -6.6985e-05,  ..., -1.3981e-05,
          1.8032e-05,  1.4938e-05],
        [-1.4693e-05, -1.0490e-05,  3.5465e-06,  ..., -1.2740e-05,
         -3.0622e-06, -5.6997e-06],
        [-1.5214e-05, -1.0848e-05,  3.6694e-06,  ..., -1.3188e-05,
         -3.1665e-06, -5.9009e-06],
        [-2.2531e-05, -1.6063e-05,  5.4389e-06,  ..., -1.9550e-05,
         -4.6790e-06, -8.7172e-06],
        [-3.2425e-05, -2.3127e-05,  7.8082e-06,  ..., -2.8074e-05,
         -6.7577e-06, -1.2532e-05]], device='cuda:0')
Loss: 1.1509127616882324


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[    0,    34,  3881,  ...,  1027,   271,   266],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5055e-05, -2.5430e-05, -4.3597e-05,  ..., -2.0367e-05,
         -4.0085e-05, -8.2999e-06],
        [-1.7583e-05, -1.2577e-05,  4.2506e-06,  ..., -1.5259e-05,
         -3.6173e-06, -6.8173e-06],
        [-1.8239e-05, -1.3024e-05,  4.3996e-06,  ..., -1.5825e-05,
         -3.7476e-06, -7.0632e-06],
        [-2.7031e-05, -1.9312e-05,  6.5342e-06,  ..., -2.3484e-05,
         -5.5432e-06, -1.0453e-05],
        [-3.8832e-05, -2.7746e-05,  9.3579e-06,  ..., -3.3647e-05,
         -7.9870e-06, -1.4991e-05]], device='cuda:0')
Loss: 1.154139518737793


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 3795,  908,  587],
        [   0,  368, 2418,  ..., 3275, 1116, 5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7945e-05,  1.8134e-05, -1.0151e-04,  ...,  1.0898e-05,
         -3.8950e-05, -6.6567e-06],
        [-2.0504e-05, -1.4648e-05,  4.9770e-06,  ..., -1.7792e-05,
         -4.2245e-06, -7.9796e-06],
        [-2.1294e-05, -1.5184e-05,  5.1595e-06,  ..., -1.8463e-05,
         -4.3809e-06, -8.2776e-06],
        [-3.1501e-05, -2.2486e-05,  7.6443e-06,  ..., -2.7359e-05,
         -6.4708e-06, -1.2234e-05],
        [-4.5270e-05, -3.2306e-05,  1.0960e-05,  ..., -3.9220e-05,
         -9.3207e-06, -1.7554e-05]], device='cuda:0')
Loss: 1.1464800834655762


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  3605,  2572,  ...,   300,   259,  1513],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8838e-05,  1.8453e-05, -8.0197e-05,  ..., -2.6345e-05,
         -3.9049e-05, -2.9926e-05],
        [-2.3440e-05, -1.6749e-05,  5.6773e-06,  ..., -2.0355e-05,
         -4.8131e-06, -9.0972e-06],
        [-2.4319e-05, -1.7345e-05,  5.8822e-06,  ..., -2.1100e-05,
         -4.9844e-06, -9.4250e-06],
        [-3.6001e-05, -2.5719e-05,  8.7246e-06,  ..., -3.1292e-05,
         -7.3723e-06, -1.3955e-05],
        [-5.1767e-05, -3.6985e-05,  1.2510e-05,  ..., -4.4912e-05,
         -1.0625e-05, -2.0027e-05]], device='cuda:0')
Loss: 1.1644364595413208
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9036e-05, -1.9662e-05, -3.9031e-05,  ...,  9.5206e-06,
         -3.3445e-05,  1.6876e-05],
        [-2.0249e-05, -1.4426e-05,  3.0444e-06,  ..., -1.6730e-05,
         -2.2254e-06, -7.5184e-06],
        [ 5.5655e-05,  4.1126e-05, -1.0485e-05,  ...,  4.7472e-05,
          9.9366e-06,  2.0395e-05],
        [-3.2320e-05, -2.1967e-05,  5.9843e-06,  ..., -2.6861e-05,
         -4.7758e-06, -1.2152e-05],
        [-5.5958e-05, -3.9366e-05,  8.7613e-06,  ..., -4.6638e-05,
         -7.7874e-06, -2.1604e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7015e-08, 1.4540e-08, 1.7278e-08,  ..., 1.9772e-08, 3.1316e-08,
         6.5090e-09],
        [3.6977e-11, 2.1753e-11, 1.3674e-12,  ..., 2.5208e-11, 1.1425e-12,
         5.4463e-12],
        [6.3861e-10, 3.8145e-10, 2.0734e-11,  ..., 4.9507e-10, 1.3256e-11,
         1.0334e-10],
        [1.8875e-10, 2.5802e-10, 5.0952e-12,  ..., 1.3951e-10, 6.8470e-12,
         4.8129e-11],
        [1.4472e-10, 8.0299e-11, 5.8525e-12,  ..., 9.7675e-11, 2.7082e-12,
         2.1443e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.5888048657910018e-05, 1.5888048657910018e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[  0, 792,  83,  ..., 300, 768, 932],
        [  0,  76,  15,  ...,  14, 333, 199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0757e-06, -2.3158e-06,  1.7566e-05,  ...,  9.5487e-06,
         -8.5316e-06,  1.8069e-05],
        [-2.8610e-06, -2.0564e-06,  7.0035e-07,  ..., -2.5034e-06,
         -6.0350e-07, -1.1176e-06],
        [-3.0249e-06, -2.1607e-06,  7.4133e-07,  ..., -2.6375e-06,
         -6.3702e-07, -1.1772e-06],
        [-4.5002e-06, -3.2187e-06,  1.1027e-06,  ..., -3.9339e-06,
         -9.4622e-07, -1.7509e-06],
        [-6.3777e-06, -4.5598e-06,  1.5572e-06,  ..., -5.5730e-06,
         -1.3411e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.1644216775894165


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1042, 5738,  ...,   12,  287, 3643],
        [   0,  775,  266,  ...,  409,  328, 5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0008e-05,  2.1604e-05,  2.7073e-05,  ..., -7.0845e-06,
          2.0956e-05,  3.0834e-05],
        [-5.7518e-06, -4.1276e-06,  1.4044e-06,  ..., -5.0664e-06,
         -1.2256e-06, -2.2650e-06],
        [-6.0350e-06, -4.3213e-06,  1.4752e-06,  ..., -5.3197e-06,
         -1.2890e-06, -2.3767e-06],
        [-9.0003e-06, -6.4522e-06,  2.1979e-06,  ..., -7.9274e-06,
         -1.9148e-06, -3.5316e-06],
        [-1.2696e-05, -9.0897e-06,  3.0994e-06,  ..., -1.1206e-05,
         -2.7046e-06, -4.9770e-06]], device='cuda:0')
Loss: 1.1385704278945923


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,  1235,    14,  ...,  3301,   549,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4274e-06,  4.5679e-05,  1.3133e-05,  ..., -2.5388e-05,
          4.7483e-05,  4.0786e-05],
        [-8.6278e-06, -6.1989e-06,  2.1011e-06,  ..., -7.5996e-06,
         -1.8254e-06, -3.3826e-06],
        [-9.0450e-06, -6.4969e-06,  2.2054e-06,  ..., -7.9870e-06,
         -1.9185e-06, -3.5539e-06],
        [-1.3530e-05, -9.7007e-06,  3.2932e-06,  ..., -1.1921e-05,
         -2.8573e-06, -5.2974e-06],
        [-1.9044e-05, -1.3649e-05,  4.6343e-06,  ..., -1.6809e-05,
         -4.0308e-06, -7.4506e-06]], device='cuda:0')
Loss: 1.1522225141525269


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1555e-05,  4.9647e-05,  1.6284e-05,  ..., -3.7147e-05,
          5.6892e-05,  5.1985e-05],
        [-1.1548e-05, -8.2850e-06,  2.7940e-06,  ..., -1.0177e-05,
         -2.4252e-06, -4.5225e-06],
        [-1.2085e-05, -8.6576e-06,  2.9281e-06,  ..., -1.0669e-05,
         -2.5406e-06, -4.7386e-06],
        [-1.8060e-05, -1.2949e-05,  4.3735e-06,  ..., -1.5944e-05,
         -3.7886e-06, -7.0706e-06],
        [-2.5451e-05, -1.8239e-05,  6.1542e-06,  ..., -2.2471e-05,
         -5.3495e-06, -9.9540e-06]], device='cuda:0')
Loss: 1.1540837287902832


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,   300,   344,  ...,    14,  5077,  2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6473e-05,  7.3029e-05,  2.3204e-06,  ..., -5.9644e-05,
          9.5208e-05,  8.6824e-05],
        [-1.4439e-05, -1.0371e-05,  3.4906e-06,  ..., -1.2711e-05,
         -3.0585e-06, -5.6922e-06],
        [-1.5140e-05, -1.0863e-05,  3.6657e-06,  ..., -1.3337e-05,
         -3.2075e-06, -5.9754e-06],
        [-2.2620e-05, -1.6227e-05,  5.4687e-06,  ..., -1.9908e-05,
         -4.7795e-06, -8.9034e-06],
        [-3.1888e-05, -2.2888e-05,  7.7039e-06,  ..., -2.8074e-05,
         -6.7577e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1340752840042114


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 4852,  266,  ..., 2523, 2080, 2632],
        [   0, 1234,  278,  ..., 8635,  271,  546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0435e-06,  4.0523e-05,  3.9860e-06,  ..., -5.4059e-05,
          7.1405e-05,  8.2622e-05],
        [-1.7300e-05, -1.2442e-05,  4.1947e-06,  ..., -1.5259e-05,
         -3.6396e-06, -6.8098e-06],
        [-1.8165e-05, -1.3053e-05,  4.4107e-06,  ..., -1.6019e-05,
         -3.8184e-06, -7.1600e-06],
        [-2.7120e-05, -1.9476e-05,  6.5789e-06,  ..., -2.3901e-05,
         -5.6848e-06, -1.0662e-05],
        [-3.8236e-05, -2.7478e-05,  9.2611e-06,  ..., -3.3706e-05,
         -8.0466e-06, -1.5020e-05]], device='cuda:0')
Loss: 1.1450536251068115


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,  4868,  1027,  ...,   409,  3047,  2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0240e-06,  6.7997e-05, -1.3549e-05,  ..., -4.5553e-05,
          6.3672e-05,  7.0422e-05],
        [-2.0221e-05, -1.4558e-05,  4.8876e-06,  ..., -1.7807e-05,
         -4.2468e-06, -7.9572e-06],
        [-2.1204e-05, -1.5259e-05,  5.1335e-06,  ..., -1.8671e-05,
         -4.4517e-06, -8.3521e-06],
        [-3.1650e-05, -2.2739e-05,  7.6517e-06,  ..., -2.7835e-05,
         -6.6198e-06, -1.2428e-05],
        [-4.4644e-05, -3.2097e-05,  1.0774e-05,  ..., -3.9279e-05,
         -9.3803e-06, -1.7524e-05]], device='cuda:0')
Loss: 1.1484429836273193


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[   0,   12, 1250,  ...,  381, 1524, 2204],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8713e-05,  6.6774e-05, -1.2886e-05,  ..., -3.6969e-05,
          6.2592e-05,  1.3269e-04],
        [-2.3127e-05, -1.6630e-05,  5.6028e-06,  ..., -2.0340e-05,
         -4.8280e-06, -9.0823e-06],
        [-2.4244e-05, -1.7434e-05,  5.8785e-06,  ..., -2.1324e-05,
         -5.0589e-06, -9.5218e-06],
        [-3.6180e-05, -2.5973e-05,  8.7619e-06,  ..., -3.1799e-05,
         -7.5214e-06, -1.4171e-05],
        [-5.1111e-05, -3.6687e-05,  1.2353e-05,  ..., -4.4912e-05,
         -1.0669e-05, -2.0012e-05]], device='cuda:0')
Loss: 1.1643105745315552
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2261e-05, -1.1018e-05, -3.6417e-05,  ...,  4.8716e-06,
         -2.3842e-05,  2.8458e-05],
        [-2.0537e-05, -1.4646e-05,  3.3002e-06,  ..., -1.7091e-05,
         -2.4856e-06, -7.6748e-06],
        [ 4.7665e-05,  3.5270e-05, -8.8489e-06,  ...,  4.0592e-05,
          8.4371e-06,  1.7403e-05],
        [-3.2706e-05, -2.2367e-05,  6.2620e-06,  ..., -2.7355e-05,
         -5.0503e-06, -1.2354e-05],
        [-5.5474e-05, -3.9098e-05,  9.1205e-06,  ..., -4.6466e-05,
         -8.0756e-06, -2.1445e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6969e-08, 1.4530e-08, 1.7261e-08,  ..., 1.9754e-08, 3.1288e-08,
         6.5201e-09],
        [3.7475e-11, 2.2008e-11, 1.3975e-12,  ..., 2.5597e-11, 1.1647e-12,
         5.5233e-12],
        [6.3856e-10, 3.8137e-10, 2.0748e-11,  ..., 4.9503e-10, 1.3268e-11,
         1.0333e-10],
        [1.8987e-10, 2.5844e-10, 5.1669e-12,  ..., 1.4038e-10, 6.8967e-12,
         4.8282e-11],
        [1.4719e-10, 8.1565e-11, 5.9992e-12,  ..., 9.9594e-11, 2.8193e-12,
         2.1823e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.5686437100081734e-05, 1.5686437100081734e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[   0,   14,  560,  ..., 1248, 1398, 1268],
        [   0,  474,  221,  ..., 2945,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5908e-05,  2.9952e-05,  0.0000e+00,  ..., -8.5952e-06,
         -3.9853e-05, -1.0582e-05],
        [-2.8163e-06, -2.0564e-06,  7.3016e-07,  ..., -2.5183e-06,
         -6.2585e-07, -1.1399e-06],
        [-3.0249e-06, -2.2054e-06,  7.8231e-07,  ..., -2.6971e-06,
         -6.7055e-07, -1.2293e-06],
        [-4.5300e-06, -3.3081e-06,  1.1772e-06,  ..., -4.0531e-06,
         -1.0058e-06, -1.8403e-06],
        [-6.2585e-06, -4.5598e-06,  1.6168e-06,  ..., -5.6028e-06,
         -1.3858e-06, -2.5332e-06]], device='cuda:0')
Loss: 1.1413493156433105


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0,   616,  4935,  ...,    89,  4448,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4548e-05,  7.3382e-05,  1.7822e-05,  ..., -9.1517e-07,
         -1.6236e-05, -3.1213e-05],
        [-5.6922e-06, -4.1276e-06,  1.4380e-06,  ..., -5.0664e-06,
         -1.2964e-06, -2.3320e-06],
        [-6.0797e-06, -4.4107e-06,  1.5348e-06,  ..., -5.4091e-06,
         -1.3858e-06, -2.4959e-06],
        [-9.0301e-06, -6.5416e-06,  2.2799e-06,  ..., -8.0168e-06,
         -2.0564e-06, -3.6955e-06],
        [-1.2666e-05, -9.1791e-06,  3.1888e-06,  ..., -1.1265e-05,
         -2.8834e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.167731523513794


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2208e-05,  7.9562e-05, -2.8128e-05,  ..., -3.9187e-06,
         -5.1557e-06, -5.2630e-05],
        [-8.5086e-06, -6.1840e-06,  2.1793e-06,  ..., -7.5996e-06,
         -1.9222e-06, -3.5167e-06],
        [-9.0599e-06, -6.5863e-06,  2.3171e-06,  ..., -8.0764e-06,
         -2.0452e-06, -3.7402e-06],
        [-1.3560e-05, -9.8497e-06,  3.4720e-06,  ..., -1.2070e-05,
         -3.0547e-06, -5.5879e-06],
        [-1.8895e-05, -1.3709e-05,  4.8205e-06,  ..., -1.6838e-05,
         -4.2617e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.150733470916748


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  2192,  2534,   287],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7821e-05,  8.1728e-05, -5.6780e-05,  ..., -2.5897e-05,
         -3.6276e-06, -6.1995e-05],
        [-1.1355e-05, -8.2552e-06,  2.8796e-06,  ..., -1.0148e-05,
         -2.5146e-06, -4.6566e-06],
        [-1.2070e-05, -8.7917e-06,  3.0585e-06,  ..., -1.0774e-05,
         -2.6710e-06, -4.9472e-06],
        [-1.8060e-05, -1.3128e-05,  4.5821e-06,  ..., -1.6093e-05,
         -3.9861e-06, -7.3910e-06],
        [-2.5183e-05, -1.8299e-05,  6.3702e-06,  ..., -2.2441e-05,
         -5.5656e-06, -1.0312e-05]], device='cuda:0')
Loss: 1.1684229373931885


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[   0,  504,  546,  ...,  634,  328,  630],
        [   0, 1730, 2068,  ...,  445, 2704,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7887e-05,  1.2894e-04, -2.6614e-05,  ...,  2.4251e-05,
         -4.5000e-05, -4.0859e-05],
        [-1.4216e-05, -1.0356e-05,  3.6135e-06,  ..., -1.2696e-05,
         -3.1292e-06, -5.8264e-06],
        [-1.5154e-05, -1.1042e-05,  3.8482e-06,  ..., -1.3515e-05,
         -3.3304e-06, -6.1989e-06],
        [-2.2620e-05, -1.6466e-05,  5.7518e-06,  ..., -2.0146e-05,
         -4.9621e-06, -9.2387e-06],
        [-3.1561e-05, -2.2948e-05,  8.0019e-06,  ..., -2.8104e-05,
         -6.9290e-06, -1.2904e-05]], device='cuda:0')
Loss: 1.156140923500061


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  5885,   271,  ...,   278,  1049,    12],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2612e-05,  1.2259e-04, -2.8945e-05,  ...,  3.3021e-05,
         -4.3108e-05, -5.7763e-05],
        [-1.7062e-05, -1.2428e-05,  4.2953e-06,  ..., -1.5229e-05,
         -3.7216e-06, -6.9663e-06],
        [-1.8179e-05, -1.3247e-05,  4.5747e-06,  ..., -1.6212e-05,
         -3.9600e-06, -7.4059e-06],
        [-2.7180e-05, -1.9789e-05,  6.8471e-06,  ..., -2.4199e-05,
         -5.9046e-06, -1.1057e-05],
        [-3.7879e-05, -2.7537e-05,  9.5144e-06,  ..., -3.3736e-05,
         -8.2478e-06, -1.5423e-05]], device='cuda:0')
Loss: 1.1646287441253662


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0,  360,  259,  ...,   12,  358,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6947e-07,  1.0854e-04, -5.9471e-06,  ...,  3.8738e-05,
         -5.9620e-05, -5.6595e-05],
        [-1.9908e-05, -1.4484e-05,  4.9770e-06,  ..., -1.7762e-05,
         -4.3251e-06, -8.1137e-06],
        [-2.1219e-05, -1.5438e-05,  5.3011e-06,  ..., -1.8910e-05,
         -4.6045e-06, -8.6278e-06],
        [-3.1710e-05, -2.3067e-05,  7.9349e-06,  ..., -2.8223e-05,
         -6.8657e-06, -1.2882e-05],
        [-4.4256e-05, -3.2127e-05,  1.1042e-05,  ..., -3.9399e-05,
         -9.5963e-06, -1.7986e-05]], device='cuda:0')
Loss: 1.1497801542282104


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9528e-05,  1.1662e-04, -1.9533e-05,  ...,  4.1519e-05,
         -6.9051e-05, -6.4853e-05],
        [-2.2769e-05, -1.6555e-05,  5.6736e-06,  ..., -2.0280e-05,
         -4.9472e-06, -9.2760e-06],
        [ 7.2657e-05,  4.9322e-05, -1.5814e-05,  ...,  5.6659e-05,
          8.9508e-06,  2.5235e-05],
        [-3.6269e-05, -2.6360e-05,  9.0450e-06,  ..., -3.2216e-05,
         -7.8492e-06, -1.4730e-05],
        [-5.0634e-05, -3.6716e-05,  1.2591e-05,  ..., -4.5002e-05,
         -1.0982e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.145816683769226
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0270,  ...,  0.0282, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7082e-05,  1.7457e-06, -3.4728e-05,  ...,  8.5364e-06,
         -2.8363e-05,  1.9127e-05],
        [-2.0760e-05, -1.4837e-05,  3.5376e-06,  ..., -1.7410e-05,
         -2.7318e-06, -7.8349e-06],
        [ 5.0164e-05,  3.6675e-05, -9.5454e-06,  ...,  4.2199e-05,
          8.4884e-06,  1.8186e-05],
        [-3.3063e-05, -2.2767e-05,  6.5403e-06,  ..., -2.7841e-05,
         -5.3302e-06, -1.2592e-05],
        [-5.4990e-05, -3.8860e-05,  9.4676e-06,  ..., -4.6319e-05,
         -8.3662e-06, -2.1357e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6922e-08, 1.4529e-08, 1.7244e-08,  ..., 1.9736e-08, 3.1262e-08,
         6.5178e-09],
        [3.7956e-11, 2.2260e-11, 1.4283e-12,  ..., 2.5982e-11, 1.1880e-12,
         5.6038e-12],
        [6.4320e-10, 3.8343e-10, 2.0977e-11,  ..., 4.9775e-10, 1.3335e-11,
         1.0386e-10],
        [1.9099e-10, 2.5888e-10, 5.2435e-12,  ..., 1.4128e-10, 6.9514e-12,
         4.8450e-11],
        [1.4961e-10, 8.2831e-11, 6.1518e-12,  ..., 1.0152e-10, 2.9371e-12,
         2.2224e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.5481349926128634e-05, 1.5481349926128634e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2160e-05, -1.6272e-05,  2.4460e-05,  ..., -2.2686e-05,
          4.9263e-06,  2.1370e-05],
        [-2.8610e-06, -2.0713e-06,  7.1526e-07,  ..., -2.5183e-06,
         -6.3330e-07, -1.1548e-06],
        [-3.1143e-06, -2.2650e-06,  7.7859e-07,  ..., -2.7418e-06,
         -6.8918e-07, -1.2591e-06],
        [ 1.9270e-04,  1.1877e-04, -3.2444e-05,  ...,  1.3645e-04,
          3.5897e-05,  6.8474e-05],
        [-6.4373e-06, -4.6492e-06,  1.6019e-06,  ..., -5.6624e-06,
         -1.4231e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.153494954109192


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,  3351,   352,  ...,    17,   287,   357],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4356e-06, -2.7463e-05,  1.1807e-05,  ..., -1.3296e-05,
         -2.6735e-05,  3.0743e-06],
        [-5.6922e-06, -4.1276e-06,  1.4566e-06,  ..., -5.0664e-06,
         -1.2629e-06, -2.3469e-06],
        [-6.1393e-06, -4.4703e-06,  1.5721e-06,  ..., -5.4687e-06,
         -1.3635e-06, -2.5332e-06],
        [ 1.8817e-04,  1.1546e-04, -3.1252e-05,  ...,  1.3236e-04,
          3.4883e-05,  6.6566e-05],
        [-1.2666e-05, -9.1791e-06,  3.2336e-06,  ..., -1.1265e-05,
         -2.8089e-06, -5.2154e-06]], device='cuda:0')
Loss: 1.1517757177352905


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  565,   27,  ...,   88, 4451,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2921e-05, -7.1926e-06,  1.8728e-05,  ..., -2.2576e-05,
         -3.1076e-05, -4.0816e-05],
        [-8.5235e-06, -6.1989e-06,  2.1681e-06,  ..., -7.5847e-06,
         -1.8887e-06, -3.5316e-06],
        [-9.1940e-06, -6.7055e-06,  2.3358e-06,  ..., -8.1807e-06,
         -2.0340e-06, -3.8072e-06],
        [ 1.8364e-04,  1.1214e-04, -3.0112e-05,  ...,  1.2834e-04,
          3.3885e-05,  6.4674e-05],
        [-1.8984e-05, -1.3828e-05,  4.8131e-06,  ..., -1.6868e-05,
         -4.2021e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.1346887350082397


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0, 1445, 3597,  ...,  281,   78,    9],
        [   0, 4448,   12,  ..., 3183,  328, 9559]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5538e-05,  2.2228e-05,  3.2118e-05,  ..., -2.2576e-05,
         -1.4202e-05, -6.7886e-06],
        [-1.1355e-05, -8.2701e-06,  2.9057e-06,  ..., -1.0103e-05,
         -2.5295e-06, -4.7237e-06],
        [-1.2264e-05, -8.9407e-06,  3.1330e-06,  ..., -1.0908e-05,
         -2.7269e-06, -5.0962e-06],
        [ 1.7911e-04,  1.0883e-04, -2.8927e-05,  ...,  1.2432e-04,
          3.2864e-05,  6.2774e-05],
        [-2.5302e-05, -1.8418e-05,  6.4448e-06,  ..., -2.2471e-05,
         -5.6326e-06, -1.0490e-05]], device='cuda:0')
Loss: 1.1548891067504883


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,   292, 15087,  ...,  2675,  1663,    12],
        [    0,  5862,    13,  ..., 12497,   287,  3570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2166e-04,  5.5335e-05,  2.7100e-05,  ..., -4.1734e-05,
         -8.6414e-06, -2.9230e-05],
        [-1.4156e-05, -1.0341e-05,  3.6284e-06,  ..., -1.2606e-05,
         -3.1590e-06, -5.9158e-06],
        [-1.5303e-05, -1.1191e-05,  3.9227e-06,  ..., -1.3635e-05,
         -3.4086e-06, -6.3851e-06],
        [ 1.7458e-04,  1.0549e-04, -2.7757e-05,  ...,  1.2026e-04,
          3.1851e-05,  6.0867e-05],
        [-3.1561e-05, -2.3037e-05,  8.0615e-06,  ..., -2.8074e-05,
         -7.0408e-06, -1.3143e-05]], device='cuda:0')
Loss: 1.145351767539978


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[   0, 5489,   80,  ...,  221,  380,  333],
        [   0,  898,  266,  ...,   12, 3222, 8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2358e-04,  5.7901e-05, -3.1319e-05,  ..., -3.2988e-05,
         -3.0460e-05, -6.9975e-05],
        [-1.6972e-05, -1.2383e-05,  4.3549e-06,  ..., -1.5095e-05,
         -3.7700e-06, -7.1004e-06],
        [-1.8358e-05, -1.3411e-05,  4.7162e-06,  ..., -1.6347e-05,
         -4.0717e-06, -7.6666e-06],
        [ 1.7002e-04,  1.0218e-04, -2.6580e-05,  ...,  1.1624e-04,
          3.0867e-05,  5.8967e-05],
        [-3.7819e-05, -2.7567e-05,  9.6783e-06,  ..., -3.3617e-05,
         -8.3968e-06, -1.5765e-05]], device='cuda:0')
Loss: 1.1610032320022583


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0,   381, 13565,  ...,     9,   847,   300],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2567e-04,  1.0160e-04, -4.1328e-05,  ..., -4.8248e-05,
         -7.2268e-06, -5.2546e-05],
        [-1.9848e-05, -1.4484e-05,  5.0589e-06,  ..., -1.7643e-05,
         -4.4145e-06, -8.2925e-06],
        [-2.1443e-05, -1.5676e-05,  5.4725e-06,  ..., -1.9088e-05,
         -4.7646e-06, -8.9481e-06],
        [ 1.6546e-04,  9.8844e-05, -2.5455e-05,  ...,  1.1219e-04,
          2.9847e-05,  5.7074e-05],
        [-4.4227e-05, -3.2246e-05,  1.1250e-05,  ..., -3.9309e-05,
         -9.8422e-06, -1.8418e-05]], device='cuda:0')
Loss: 1.1505590677261353


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[   0, 5896,  352,  ..., 1168,  767, 1390],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5519e-04,  9.4427e-05, -7.0562e-05,  ..., -5.9031e-05,
         -3.0055e-06, -4.5917e-05],
        [-2.2665e-05, -1.6525e-05,  5.7742e-06,  ..., -2.0161e-05,
         -5.0515e-06, -9.4697e-06],
        [-2.4512e-05, -1.7911e-05,  6.2510e-06,  ..., -2.1830e-05,
         -5.4576e-06, -1.0230e-05],
        [ 1.6093e-04,  9.5551e-05, -2.4300e-05,  ...,  1.0814e-04,
          2.8826e-05,  5.5182e-05],
        [-5.0485e-05, -3.6806e-05,  1.2845e-05,  ..., -4.4912e-05,
         -1.1258e-05, -2.1026e-05]], device='cuda:0')
Loss: 1.1665160655975342
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0270,  ...,  0.0282, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9893e-05,  1.1014e-05, -3.8312e-05,  ...,  1.7796e-06,
         -2.5827e-05,  1.2623e-05],
        [-2.0951e-05, -1.5006e-05,  3.7612e-06,  ..., -1.7685e-05,
         -2.9638e-06, -7.9984e-06],
        [ 4.2697e-05,  3.1216e-05, -7.9658e-06,  ...,  3.5796e-05,
          7.0938e-06,  1.5345e-05],
        [-1.3663e-05, -1.0935e-05,  3.4563e-06,  ..., -1.4243e-05,
         -1.9146e-06, -5.8143e-06],
        [-5.4539e-05, -3.8654e-05,  9.8053e-06,  ..., -4.6178e-05,
         -8.6554e-06, -2.1324e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6899e-08, 1.4523e-08, 1.7232e-08,  ..., 1.9719e-08, 3.1230e-08,
         6.5134e-09],
        [3.8432e-11, 2.2511e-11, 1.4602e-12,  ..., 2.6363e-11, 1.2123e-12,
         5.6879e-12],
        [6.4315e-10, 3.8336e-10, 2.0995e-11,  ..., 4.9773e-10, 1.3351e-11,
         1.0386e-10],
        [2.1670e-10, 2.6775e-10, 5.8287e-12,  ..., 1.5283e-10, 7.7754e-12,
         5.1447e-11],
        [1.5200e-10, 8.4103e-11, 6.3106e-12,  ..., 1.0344e-10, 3.0609e-12,
         2.2643e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.5272912487703465e-05, 1.5272912487703465e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3675e-05, -1.5866e-06, -3.7995e-06,  ..., -4.3154e-06,
          1.5093e-05,  1.6188e-05],
        [-2.8163e-06, -2.0564e-06,  7.4133e-07,  ..., -2.4885e-06,
         -6.7428e-07, -1.2219e-06],
        [-3.0994e-06, -2.2650e-06,  8.1584e-07,  ..., -2.7418e-06,
         -7.3761e-07, -1.3411e-06],
        [-4.5896e-06, -3.3379e-06,  1.2144e-06,  ..., -4.0531e-06,
         -1.0952e-06, -1.9819e-06],
        [-6.3777e-06, -4.6492e-06,  1.6764e-06,  ..., -5.6326e-06,
         -1.5199e-06, -2.7567e-06]], device='cuda:0')
Loss: 1.154062032699585


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    13,   786,  ...,   275,  2623,    13],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4125e-05,  4.7512e-05, -5.2482e-05,  ..., -1.4384e-05,
         -1.1139e-05,  3.4083e-05],
        [-5.6177e-06, -4.1127e-06,  1.4789e-06,  ..., -4.9770e-06,
         -1.3262e-06, -2.4214e-06],
        [ 8.9884e-05,  6.6970e-05, -1.0445e-05,  ...,  8.5845e-05,
          1.6381e-05,  3.5852e-05],
        [-9.1493e-06, -6.6906e-06,  2.4140e-06,  ..., -8.1062e-06,
         -2.1532e-06, -3.9339e-06],
        [-1.2696e-05, -9.2685e-06,  3.3304e-06,  ..., -1.1235e-05,
         -2.9802e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.166426420211792


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,  1304,   292,  ...,  2101,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0249e-06,  3.3679e-05, -6.2745e-05,  ..., -2.8778e-06,
         -1.0764e-05,  3.4083e-05],
        [-8.4490e-06, -6.1542e-06,  2.1979e-06,  ..., -7.4655e-06,
         -1.9893e-06, -3.6210e-06],
        [ 8.6755e-05,  6.4705e-05, -9.6479e-06,  ...,  8.3089e-05,
          1.5647e-05,  3.4526e-05],
        [-1.3798e-05, -1.0058e-05,  3.5986e-06,  ..., -1.2219e-05,
         -3.2410e-06, -5.9158e-06],
        [-1.9014e-05, -1.3828e-05,  4.9397e-06,  ..., -1.6809e-05,
         -4.4629e-06, -8.1360e-06]], device='cuda:0')
Loss: 1.139107346534729


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[   0,   14,   23,  ...,  278,  266, 1462],
        [   0,   14,  381,  ..., 7106,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1766e-05,  5.0846e-05, -7.5956e-05,  ..., -2.0656e-05,
         -1.8800e-05,  6.7812e-05],
        [-1.1250e-05, -8.1956e-06,  2.9169e-06,  ..., -9.9540e-06,
         -2.6301e-06, -4.7982e-06],
        [ 8.3655e-05,  6.2455e-05, -8.8506e-06,  ...,  8.0332e-05,
          1.4939e-05,  3.3230e-05],
        [-1.8388e-05, -1.3411e-05,  4.7833e-06,  ..., -1.6332e-05,
         -4.2915e-06, -7.8380e-06],
        [-2.5302e-05, -1.8418e-05,  6.5565e-06,  ..., -2.2411e-05,
         -5.9083e-06, -1.0774e-05]], device='cuda:0')
Loss: 1.1479365825653076


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[   0, 1101,  300,  ..., 6104,  367,  993],
        [   0, 3592,  417,  ..., 4893,  328,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6690e-05,  5.9579e-05, -9.4407e-05,  ..., -1.4356e-05,
         -6.1539e-05,  7.8207e-05],
        [-1.4067e-05, -1.0222e-05,  3.6582e-06,  ..., -1.2413e-05,
         -3.2783e-06, -5.9977e-06],
        [ 8.0526e-05,  6.0205e-05, -8.0274e-06,  ...,  7.7605e-05,
          1.4220e-05,  3.1903e-05],
        [-2.3037e-05, -1.6734e-05,  6.0052e-06,  ..., -2.0385e-05,
         -5.3570e-06, -9.8050e-06],
        [-3.1590e-05, -2.2948e-05,  8.2105e-06,  ..., -2.7925e-05,
         -7.3612e-06, -1.3441e-05]], device='cuda:0')
Loss: 1.1483858823776245


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  300, 2607,  ..., 1279,  368,  266],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6841e-05,  9.6624e-05, -1.4093e-04,  ..., -5.6156e-06,
         -6.1539e-05,  9.2841e-05],
        [-1.6898e-05, -1.2338e-05,  4.3996e-06,  ..., -1.4916e-05,
         -3.9674e-06, -7.2345e-06],
        [ 7.7412e-05,  5.7880e-05, -7.2078e-06,  ...,  7.4833e-05,
          1.3464e-05,  3.0540e-05],
        [-2.7627e-05, -2.0161e-05,  7.2122e-06,  ..., -2.4468e-05,
         -6.4746e-06, -1.1817e-05],
        [-3.7879e-05, -2.7627e-05,  9.8571e-06,  ..., -3.3498e-05,
         -8.8885e-06, -1.6183e-05]], device='cuda:0')
Loss: 1.157581090927124


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5981e-05,  1.1009e-04, -1.2004e-04,  ..., -1.0062e-05,
         -7.3701e-05,  1.1125e-04],
        [-1.9759e-05, -1.4439e-05,  5.1185e-06,  ..., -1.7434e-05,
         -4.6492e-06, -8.4490e-06],
        [ 7.4267e-05,  5.5570e-05, -6.4180e-06,  ...,  7.2077e-05,
          1.2719e-05,  2.9214e-05],
        [-3.2246e-05, -2.3559e-05,  8.3745e-06,  ..., -2.8521e-05,
         -7.5698e-06, -1.3769e-05],
        [-4.4316e-05, -3.2365e-05,  1.1466e-05,  ..., -3.9160e-05,
         -1.0416e-05, -1.8910e-05]], device='cuda:0')
Loss: 1.1386281251907349


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0,  1716,   271,  ...,   292,    78,  1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8217e-06,  9.6780e-05, -1.2372e-04,  ...,  1.3027e-05,
         -8.3138e-05,  9.2826e-05],
        [-2.2590e-05, -1.6510e-05,  5.8450e-06,  ..., -1.9923e-05,
         -5.3272e-06, -9.6560e-06],
        [ 7.1123e-05,  5.3276e-05, -5.6134e-06,  ...,  6.9320e-05,
          1.1970e-05,  2.7880e-05],
        [-3.6836e-05, -2.6926e-05,  9.5591e-06,  ..., -3.2574e-05,
         -8.6650e-06, -1.5721e-05],
        [-5.0694e-05, -3.7044e-05,  1.3098e-05,  ..., -4.4763e-05,
         -1.1936e-05, -2.1622e-05]], device='cuda:0')
Loss: 1.152794361114502
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0165,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6386e-05,  1.9590e-05, -4.6853e-05,  ...,  2.9043e-06,
         -3.1558e-05,  2.0643e-05],
        [-2.1115e-05, -1.5157e-05,  3.9696e-06,  ..., -1.7909e-05,
         -3.2001e-06, -8.1642e-06],
        [ 4.5539e-05,  3.3422e-05, -7.7305e-06,  ...,  3.9148e-05,
          7.5814e-06,  1.6598e-05],
        [-1.5980e-05, -1.2534e-05,  4.0666e-06,  ..., -1.6077e-05,
         -2.5896e-06, -6.8050e-06],
        [-5.4155e-05, -3.8493e-05,  1.0135e-05,  ..., -4.6037e-05,
         -8.9834e-06, -2.1353e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6852e-08, 1.4518e-08, 1.7230e-08,  ..., 1.9700e-08, 3.1206e-08,
         6.5155e-09],
        [3.8903e-11, 2.2761e-11, 1.4929e-12,  ..., 2.6733e-11, 1.2395e-12,
         5.7754e-12],
        [6.4757e-10, 3.8582e-10, 2.1006e-11,  ..., 5.0203e-10, 1.3481e-11,
         1.0453e-10],
        [2.1784e-10, 2.6820e-10, 5.9143e-12,  ..., 1.5374e-10, 7.8427e-12,
         5.1642e-11],
        [1.5442e-10, 8.5391e-11, 6.4758e-12,  ..., 1.0534e-10, 3.2003e-12,
         2.3088e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.5061252184179384e-05, 1.5061252184179384e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   199,  7513,  ...,   271,   259,   957],
        [    0,   266,   858,  ..., 11265,   607,  7455]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9600e-05, -3.1189e-05, -3.2844e-06,  ..., -1.9368e-05,
          3.9930e-06,  2.9637e-05],
        [-2.8014e-06, -2.0415e-06,  7.3016e-07,  ..., -2.4885e-06,
         -7.0035e-07, -1.2591e-06],
        [-3.1441e-06, -2.2948e-06,  8.1956e-07,  ..., -2.8014e-06,
         -7.8231e-07, -1.4156e-06],
        [-4.5896e-06, -3.3379e-06,  1.1995e-06,  ..., -4.0829e-06,
         -1.1399e-06, -2.0564e-06],
        [-6.2883e-06, -4.5896e-06,  1.6391e-06,  ..., -5.6028e-06,
         -1.5646e-06, -2.8312e-06]], device='cuda:0')
Loss: 1.1442193984985352


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5489e-05, -4.6358e-05, -2.1671e-05,  ..., -3.8158e-05,
         -2.1456e-05,  1.9420e-05],
        [-5.5730e-06, -4.0531e-06,  1.4640e-06,  ..., -4.9770e-06,
         -1.3597e-06, -2.4959e-06],
        [-6.2287e-06, -4.5449e-06,  1.6391e-06,  ..., -5.5730e-06,
         -1.5162e-06, -2.8014e-06],
        [-9.1791e-06, -6.6757e-06,  2.4140e-06,  ..., -8.1956e-06,
         -2.2277e-06, -4.0978e-06],
        [-1.2547e-05, -9.1493e-06,  3.3006e-06,  ..., -1.1206e-05,
         -3.0547e-06, -5.6326e-06]], device='cuda:0')
Loss: 1.1609008312225342


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   344,  8260,  ..., 16020, 18216, 11348],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0646e-05, -6.8670e-05, -2.8271e-05,  ..., -1.2798e-05,
         -2.2855e-05,  4.3667e-05],
        [-8.3745e-06, -6.1095e-06,  2.2128e-06,  ..., -7.4357e-06,
         -2.0228e-06, -3.7253e-06],
        [ 8.5966e-05,  6.0718e-05, -2.3468e-05,  ...,  7.4092e-05,
          2.3171e-05,  3.7371e-05],
        [-1.3828e-05, -1.0088e-05,  3.6582e-06,  ..., -1.2279e-05,
         -3.3230e-06, -6.1393e-06],
        [ 7.0900e-05,  4.8990e-05, -2.2288e-05,  ...,  6.3092e-05,
          1.9038e-05,  2.5392e-05]], device='cuda:0')
Loss: 1.152116298675537


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   271,   266,  7246],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9398e-05, -8.9072e-05, -7.0186e-05,  ..., -3.2656e-06,
         -4.0456e-05,  3.4161e-05],
        [-1.1146e-05, -8.1509e-06,  2.9579e-06,  ..., -9.9093e-06,
         -2.7008e-06, -4.9844e-06],
        [ 8.2852e-05,  5.8424e-05, -2.2630e-05,  ...,  7.1305e-05,
          2.2407e-05,  3.5955e-05],
        [-1.8418e-05, -1.3441e-05,  4.8876e-06,  ..., -1.6361e-05,
         -4.4405e-06, -8.2105e-06],
        [ 6.4611e-05,  4.4400e-05, -2.0604e-05,  ...,  5.7489e-05,
          1.7503e-05,  2.2546e-05]], device='cuda:0')
Loss: 1.1469364166259766


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0, 10205,   342,  ...,  2523,  4729, 13753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5910e-05, -1.3229e-04, -5.7413e-05,  ...,  1.7199e-05,
         -5.5491e-05,  3.8956e-05],
        [-1.3947e-05, -1.0222e-05,  3.7067e-06,  ..., -1.2398e-05,
         -3.3677e-06, -6.2361e-06],
        [ 7.9693e-05,  5.6084e-05, -2.1788e-05,  ...,  6.8504e-05,
          2.1655e-05,  3.4547e-05],
        [-2.3037e-05, -1.6853e-05,  6.1244e-06,  ..., -2.0444e-05,
         -5.5358e-06, -1.0267e-05],
        [ 5.8264e-05,  3.9721e-05, -1.8913e-05,  ...,  5.1886e-05,
          1.5991e-05,  1.9730e-05]], device='cuda:0')
Loss: 1.1604551076889038


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6326e-05, -1.4161e-04, -3.9982e-05,  ..., -1.1385e-05,
         -3.7878e-05,  6.0778e-05],
        [-1.6704e-05, -1.2264e-05,  4.4405e-06,  ..., -1.4871e-05,
         -4.0308e-06, -7.4655e-06],
        [ 7.6593e-05,  5.3789e-05, -2.0965e-05,  ...,  6.5732e-05,
          2.0910e-05,  3.3176e-05],
        [-2.7597e-05, -2.0221e-05,  7.3388e-06,  ..., -2.4527e-05,
         -6.6310e-06, -1.2293e-05],
        [ 5.2035e-05,  3.5132e-05, -1.7259e-05,  ...,  4.6313e-05,
          1.4493e-05,  1.6973e-05]], device='cuda:0')
Loss: 1.1320760250091553


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,    14,   747,  ...,   367,   300,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2716e-05, -1.3628e-04, -2.2831e-05,  ...,  2.5351e-06,
         -3.6549e-05,  6.3428e-05],
        [-1.9446e-05, -1.4290e-05,  5.1744e-06,  ..., -1.7330e-05,
         -4.6976e-06, -8.6948e-06],
        [ 7.3509e-05,  5.1509e-05, -2.0134e-05,  ...,  6.2960e-05,
          2.0161e-05,  3.1790e-05],
        [-3.2127e-05, -2.3589e-05,  8.5607e-06,  ..., -2.8610e-05,
         -7.7337e-06, -1.4335e-05],
        [ 4.5836e-05,  3.0542e-05, -1.5590e-05,  ...,  4.0740e-05,
          1.2988e-05,  1.4187e-05]], device='cuda:0')
Loss: 1.152540922164917


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8629e-05, -1.3733e-04, -3.3141e-05,  ...,  2.4227e-06,
         -2.5536e-05,  4.5459e-05],
        [-2.2188e-05, -1.6347e-05,  5.8934e-06,  ..., -1.9789e-05,
         -5.3756e-06, -9.9689e-06],
        [ 7.0424e-05,  4.9200e-05, -1.9326e-05,  ...,  6.0189e-05,
          1.9401e-05,  3.0360e-05],
        [-3.6657e-05, -2.6971e-05,  9.7528e-06,  ..., -3.2663e-05,
         -8.8438e-06, -1.6436e-05],
        [ 3.9637e-05,  2.5923e-05, -1.3966e-05,  ...,  3.5167e-05,
          1.1461e-05,  1.1311e-05]], device='cuda:0')
Loss: 1.1379796266555786
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0885e-05,  3.8989e-06, -4.5481e-05,  ...,  2.8562e-06,
         -3.0956e-05,  2.3125e-05],
        [-2.1222e-05, -1.5276e-05,  4.1620e-06,  ..., -1.8097e-05,
         -3.4176e-06, -8.3446e-06],
        [ 4.8028e-05,  3.5000e-05, -8.8900e-06,  ...,  4.1252e-05,
          8.7634e-06,  1.7974e-05],
        [-1.8048e-05, -1.3978e-05,  4.6352e-06,  ..., -1.7735e-05,
         -3.2151e-06, -7.7681e-06],
        [-4.4776e-05, -3.2052e-05,  7.7246e-06,  ..., -3.7916e-05,
         -6.9390e-06, -1.8087e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6806e-08, 1.4522e-08, 1.7214e-08,  ..., 1.9680e-08, 3.1176e-08,
         6.5111e-09],
        [3.9357e-11, 2.3005e-11, 1.5261e-12,  ..., 2.7098e-11, 1.2671e-12,
         5.8690e-12],
        [6.5188e-10, 3.8785e-10, 2.1358e-11,  ..., 5.0515e-10, 1.3844e-11,
         1.0535e-10],
        [2.1897e-10, 2.6866e-10, 6.0035e-12,  ..., 1.5465e-10, 7.9131e-12,
         5.1861e-11],
        [1.5584e-10, 8.5978e-11, 6.6644e-12,  ..., 1.0647e-10, 3.3285e-12,
         2.3193e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.4846498384781962e-05, 1.4846498384781962e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  747, 7890,  ...,  706, 8667,  271],
        [   0,  328,  490,  ..., 6280, 4283, 4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8922e-05, -7.8385e-06, -1.3154e-05,  ...,  0.0000e+00,
         -2.1185e-05,  2.2913e-05],
        [-2.7567e-06, -2.0415e-06,  7.6368e-07,  ..., -2.4438e-06,
         -6.8173e-07, -1.2890e-06],
        [-3.1590e-06, -2.3395e-06,  8.7172e-07,  ..., -2.7865e-06,
         -7.7859e-07, -1.4678e-06],
        [-4.5896e-06, -3.3975e-06,  1.2740e-06,  ..., -4.0829e-06,
         -1.1325e-06, -2.1458e-06],
        [-6.2883e-06, -4.6492e-06,  1.7285e-06,  ..., -5.5432e-06,
         -1.5497e-06, -2.9206e-06]], device='cuda:0')
Loss: 1.1309934854507446


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,  9058,  5481,  ...,   508, 15074,   300],
        [    0,  2790,   266,  ...,   401,  1496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6587e-06,  3.8314e-05, -1.4726e-05,  ..., -2.6046e-05,
         -6.1285e-06,  1.9486e-05],
        [-5.5283e-06, -4.0978e-06,  1.5050e-06,  ..., -4.9025e-06,
         -1.3597e-06, -2.5779e-06],
        [-6.2734e-06, -4.6492e-06,  1.7062e-06,  ..., -5.5581e-06,
         -1.5385e-06, -2.9132e-06],
        [-9.2089e-06, -6.8098e-06,  2.5108e-06,  ..., -8.1658e-06,
         -2.2575e-06, -4.2766e-06],
        [-1.2606e-05, -9.3281e-06,  3.4124e-06,  ..., -1.1146e-05,
         -3.0920e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.1459407806396484


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,  2834,   266,  ..., 39474,    12, 15441],
        [    0,   292,    46,  ...,  1217,    17,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0144e-05,  9.4188e-05,  8.6466e-06,  ..., -2.5855e-05,
         -5.5280e-06,  7.4661e-06],
        [-8.2999e-06, -6.1244e-06,  2.2352e-06,  ..., -7.3463e-06,
         -2.0191e-06, -3.8520e-06],
        [-9.4026e-06, -6.9439e-06,  2.5295e-06,  ..., -8.3297e-06,
         -2.2836e-06, -4.3511e-06],
        [-1.3828e-05, -1.0192e-05,  3.7253e-06,  ..., -1.2249e-05,
         -3.3528e-06, -6.4075e-06],
        [-1.8924e-05, -1.3947e-05,  5.0664e-06,  ..., -1.6719e-05,
         -4.5896e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.1428663730621338


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0, 23842,   342,  ...,   365,  4011, 10151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1226e-05,  9.0440e-05, -2.2117e-06,  ..., -3.9396e-05,
          6.0563e-06,  2.5978e-05],
        [-1.1072e-05, -8.1658e-06,  2.9802e-06,  ..., -9.8199e-06,
         -2.7008e-06, -5.1335e-06],
        [-1.2547e-05, -9.2536e-06,  3.3714e-06,  ..., -1.1131e-05,
         -3.0547e-06, -5.8040e-06],
        [-1.8418e-05, -1.3560e-05,  4.9546e-06,  ..., -1.6332e-05,
         -4.4778e-06, -8.5235e-06],
        [-2.5183e-05, -1.8567e-05,  6.7502e-06,  ..., -2.2322e-05,
         -6.1318e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.161523461341858


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[   0, 4209,  278,  ...,  287, 9971,  717],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9760e-07,  5.6583e-05, -2.5070e-05,  ..., -3.9151e-05,
          1.8412e-05,  7.1350e-06],
        [-1.3828e-05, -1.0207e-05,  3.7253e-06,  ..., -1.2264e-05,
         -3.3937e-06, -6.4373e-06],
        [-1.5676e-05, -1.1563e-05,  4.2170e-06,  ..., -1.3903e-05,
         -3.8408e-06, -7.2792e-06],
        [-2.3007e-05, -1.6958e-05,  6.1989e-06,  ..., -2.0385e-05,
         -5.6252e-06, -1.0684e-05],
        [-3.1441e-05, -2.3216e-05,  8.4490e-06,  ..., -2.7865e-05,
         -7.7114e-06, -1.4573e-05]], device='cuda:0')
Loss: 1.1240657567977905


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  292,   41,  ...,  271, 9536,  287],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7660e-05,  1.4959e-04, -2.4555e-05,  ..., -3.6709e-05,
          5.9532e-05,  3.4553e-05],
        [-1.6645e-05, -1.2279e-05,  4.4666e-06,  ..., -1.4737e-05,
         -4.1425e-06, -7.7337e-06],
        [-1.8865e-05, -1.3903e-05,  5.0552e-06,  ..., -1.6689e-05,
         -4.6864e-06, -8.7470e-06],
        [-2.7627e-05, -2.0355e-05,  7.4208e-06,  ..., -2.4438e-05,
         -6.8471e-06, -1.2800e-05],
        [-3.7849e-05, -2.7925e-05,  1.0133e-05,  ..., -3.3468e-05,
         -9.4101e-06, -1.7509e-05]], device='cuda:0')
Loss: 1.1367278099060059


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1824,    13,  ...,   266,  5940,    19],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3636e-05,  1.5174e-04, -5.1746e-05,  ..., -2.8683e-05,
          3.9838e-05,  3.3774e-05],
        [-1.9401e-05, -1.4305e-05,  5.2303e-06,  ..., -1.7181e-05,
         -4.7907e-06, -8.9929e-06],
        [-2.1979e-05, -1.6198e-05,  5.9158e-06,  ..., -1.9461e-05,
         -5.4166e-06, -1.0170e-05],
        [-3.2216e-05, -2.3723e-05,  8.6948e-06,  ..., -2.8521e-05,
         -7.9274e-06, -1.4901e-05],
        [-4.4078e-05, -3.2514e-05,  1.1854e-05,  ..., -3.9011e-05,
         -1.0878e-05, -2.0355e-05]], device='cuda:0')
Loss: 1.1469814777374268


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2872e-05,  1.5931e-04, -7.3274e-05,  ..., -2.7234e-05,
          6.7158e-05,  5.5302e-05],
        [-2.2143e-05, -1.6347e-05,  5.9903e-06,  ..., -1.9655e-05,
         -5.4762e-06, -1.0282e-05],
        [-2.5094e-05, -1.8522e-05,  6.7763e-06,  ..., -2.2277e-05,
         -6.1952e-06, -1.1630e-05],
        [-3.6746e-05, -2.7105e-05,  9.9540e-06,  ..., -3.2604e-05,
         -9.0599e-06, -1.7032e-05],
        [-5.0247e-05, -3.7104e-05,  1.3560e-05,  ..., -4.4584e-05,
         -1.2413e-05, -2.3246e-05]], device='cuda:0')
Loss: 1.1296958923339844
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3509e-05,  1.9440e-05, -4.8261e-05,  ..., -1.5285e-07,
         -2.1144e-05,  2.6342e-05],
        [-2.1314e-05, -1.5383e-05,  4.3448e-06,  ..., -1.8253e-05,
         -3.6235e-06, -8.5383e-06],
        [ 4.0716e-05,  2.9648e-05, -7.3234e-06,  ...,  3.4899e-05,
          7.2675e-06,  1.5014e-05],
        [-1.9918e-05, -1.5290e-05,  5.1671e-06,  ..., -1.9222e-05,
         -3.7995e-06, -8.6945e-06],
        [-4.5323e-05, -3.2557e-05,  8.3081e-06,  ..., -3.8583e-05,
         -7.4864e-06, -1.8603e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6761e-08, 1.4533e-08, 1.7202e-08,  ..., 1.9661e-08, 3.1149e-08,
         6.5076e-09],
        [3.9808e-11, 2.3249e-11, 1.5605e-12,  ..., 2.7457e-11, 1.2958e-12,
         5.9689e-12],
        [6.5186e-10, 3.8781e-10, 2.1383e-11,  ..., 5.0515e-10, 1.3869e-11,
         1.0538e-10],
        [2.2010e-10, 2.6913e-10, 6.0966e-12,  ..., 1.5556e-10, 7.9872e-12,
         5.2099e-11],
        [1.5821e-10, 8.7268e-11, 6.8416e-12,  ..., 1.0835e-10, 3.4792e-12,
         2.3710e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.4628782349517233e-05, 1.4628782349517233e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0,   12, 4957,  ...,  944,  278,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5062e-05, -1.2413e-06, -2.5557e-05,  ..., -1.2356e-05,
          1.4218e-05, -1.9281e-05],
        [-2.7418e-06, -2.0564e-06,  7.7859e-07,  ..., -2.4587e-06,
         -7.1898e-07, -1.3486e-06],
        [-3.1441e-06, -2.3395e-06,  8.9034e-07,  ..., -2.8163e-06,
         -8.1956e-07, -1.5423e-06],
        [-4.5598e-06, -3.4124e-06,  1.2964e-06,  ..., -4.0829e-06,
         -1.1921e-06, -2.2501e-06],
        [-6.2287e-06, -4.6492e-06,  1.7583e-06,  ..., -5.5730e-06,
         -1.6242e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.1187162399291992


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,   285,    53,  ...,   259,  5012,  3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3272e-05, -1.4088e-05, -2.8663e-06,  ..., -2.5192e-07,
          4.2820e-05,  1.2386e-06],
        [-5.5134e-06, -4.0978e-06,  1.5385e-06,  ..., -4.9323e-06,
         -1.4268e-06, -2.6971e-06],
        [-6.3032e-06, -4.6641e-06,  1.7583e-06,  ..., -5.6475e-06,
         -1.6242e-06, -3.0771e-06],
        [-9.1493e-06, -6.7949e-06,  2.5630e-06,  ..., -8.1956e-06,
         -2.3618e-06, -4.4852e-06],
        [-1.2487e-05, -9.2387e-06,  3.4720e-06,  ..., -1.1146e-05,
         -3.2187e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.137735366821289


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  292,  263,  ...,  342, 4575,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1755e-05,  3.5091e-05, -3.8454e-06,  ..., -9.2523e-06,
          2.1384e-05, -1.6835e-05],
        [-8.2552e-06, -6.1095e-06,  2.3134e-06,  ..., -7.3910e-06,
         -2.1234e-06, -4.0308e-06],
        [-9.4324e-06, -6.9588e-06,  2.6412e-06,  ..., -8.4490e-06,
         -2.4177e-06, -4.6045e-06],
        [-1.3739e-05, -1.0163e-05,  3.8594e-06,  ..., -1.2308e-05,
         -3.5241e-06, -6.7204e-06],
        [-1.8716e-05, -1.3798e-05,  5.2303e-06,  ..., -1.6719e-05,
         -4.8056e-06, -9.1046e-06]], device='cuda:0')
Loss: 1.1670441627502441


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2339e-05,  3.8541e-05,  1.9797e-05,  ..., -5.9561e-06,
          2.5983e-05,  3.2275e-05],
        [-1.1012e-05, -8.1509e-06,  3.0808e-06,  ..., -9.8497e-06,
         -2.7977e-06, -5.3495e-06],
        [-1.2591e-05, -9.3132e-06,  3.5204e-06,  ..., -1.1265e-05,
         -3.1851e-06, -6.1169e-06],
        [-1.8328e-05, -1.3575e-05,  5.1484e-06,  ..., -1.6391e-05,
         -4.6417e-06, -8.9109e-06],
        [-2.4945e-05, -1.8418e-05,  6.9588e-06,  ..., -2.2262e-05,
         -6.3255e-06, -1.2070e-05]], device='cuda:0')
Loss: 1.1530351638793945


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,   287,  6932,  ...,  1549,  1480,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3323e-05,  2.2237e-05, -9.1345e-06,  ..., -2.6554e-05,
          2.2649e-05,  2.9562e-05],
        [-1.3739e-05, -1.0192e-05,  3.8445e-06,  ..., -1.2293e-05,
         -3.4794e-06, -6.6757e-06],
        [-1.5721e-05, -1.1668e-05,  4.3996e-06,  ..., -1.4082e-05,
         -3.9674e-06, -7.6443e-06],
        [-2.2858e-05, -1.6987e-05,  6.4224e-06,  ..., -2.0474e-05,
         -5.7742e-06, -1.1131e-05],
        [-3.1114e-05, -2.3067e-05,  8.6948e-06,  ..., -2.7835e-05,
         -7.8678e-06, -1.5080e-05]], device='cuda:0')
Loss: 1.14302659034729


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,    12,  1471,  ...,  1356,   600,    12],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5138e-05,  1.1301e-05, -9.1345e-06,  ..., -1.5748e-05,
         -9.1377e-07,  3.5511e-05],
        [-1.6496e-05, -1.2249e-05,  4.5970e-06,  ..., -1.4767e-05,
         -4.1723e-06, -8.0019e-06],
        [-1.8880e-05, -1.4022e-05,  5.2638e-06,  ..., -1.6928e-05,
         -4.7609e-06, -9.1642e-06],
        [-2.7418e-05, -2.0385e-05,  7.6741e-06,  ..., -2.4587e-05,
         -6.9141e-06, -1.3322e-05],
        [-3.7313e-05, -2.7686e-05,  1.0386e-05,  ..., -3.3408e-05,
         -9.4324e-06, -1.8060e-05]], device='cuda:0')
Loss: 1.1319575309753418


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3737e-04,  4.3732e-05, -2.8295e-05,  ..., -1.6847e-05,
          2.5934e-05,  3.7591e-05],
        [-1.9237e-05, -1.4320e-05,  5.3234e-06,  ..., -1.7241e-05,
         -4.8764e-06, -9.3430e-06],
        [-2.2009e-05, -1.6376e-05,  6.0908e-06,  ..., -1.9744e-05,
         -5.5581e-06, -1.0692e-05],
        [-3.1948e-05, -2.3797e-05,  8.8736e-06,  ..., -2.8670e-05,
         -8.0690e-06, -1.5527e-05],
        [-4.3571e-05, -3.2395e-05,  1.2033e-05,  ..., -3.9041e-05,
         -1.1027e-05, -2.1100e-05]], device='cuda:0')
Loss: 1.1586501598358154


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5317e-04, -1.2648e-06, -4.4238e-05,  ..., -1.7816e-05,
          2.9075e-05,  3.8131e-05],
        [-2.1964e-05, -1.6332e-05,  6.0648e-06,  ..., -1.9670e-05,
         -5.5395e-06, -1.0677e-05],
        [-2.5153e-05, -1.8701e-05,  6.9439e-06,  ..., -2.2545e-05,
         -6.3218e-06, -1.2226e-05],
        [-3.6508e-05, -2.7180e-05,  1.0118e-05,  ..., -3.2753e-05,
         -9.1717e-06, -1.7747e-05],
        [-4.9740e-05, -3.6955e-05,  1.3709e-05,  ..., -4.4554e-05,
         -1.2532e-05, -2.4110e-05]], device='cuda:0')
Loss: 1.141487717628479
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6475e-05,  1.7369e-05, -4.7858e-05,  ..., -1.9192e-06,
         -1.6122e-05,  2.7521e-05],
        [-2.1379e-05, -1.5478e-05,  4.5168e-06,  ..., -1.8394e-05,
         -3.8151e-06, -8.7522e-06],
        [ 3.4129e-05,  2.4813e-05, -5.8967e-06,  ...,  2.9155e-05,
          5.9086e-06,  1.2290e-05],
        [-2.1577e-05, -1.6479e-05,  5.6621e-06,  ..., -2.0575e-05,
         -4.3368e-06, -9.5997e-06],
        [-4.5764e-05, -3.2997e-05,  8.8482e-06,  ..., -3.9180e-05,
         -7.9909e-06, -1.9154e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6737e-08, 1.4519e-08, 1.7187e-08,  ..., 1.9642e-08, 3.1119e-08,
         6.5026e-09],
        [4.0250e-11, 2.3493e-11, 1.5957e-12,  ..., 2.7817e-11, 1.3252e-12,
         6.0769e-12],
        [6.5184e-10, 3.8777e-10, 2.1410e-11,  ..., 5.0515e-10, 1.3895e-11,
         1.0542e-10],
        [2.2121e-10, 2.6960e-10, 6.1929e-12,  ..., 1.5648e-10, 8.0634e-12,
         5.2362e-11],
        [1.6052e-10, 8.8547e-11, 7.0227e-12,  ..., 1.1022e-10, 3.6328e-12,
         2.4268e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.4408237148944047e-05, 1.4408237148944047e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   298, 11712,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4867e-05,  8.0236e-05, -2.9589e-05,  ...,  1.9363e-05,
         -1.8852e-05,  4.2102e-05],
        [-2.7120e-06, -2.0266e-06,  7.5623e-07,  ..., -2.4289e-06,
         -7.5251e-07, -1.3933e-06],
        [-3.1292e-06, -2.3395e-06,  8.7544e-07,  ..., -2.8163e-06,
         -8.6799e-07, -1.6093e-06],
        [-4.4703e-06, -3.3528e-06,  1.2517e-06,  ..., -4.0233e-06,
         -1.2368e-06, -2.3097e-06],
        [-6.1691e-06, -4.5896e-06,  1.7211e-06,  ..., -5.5432e-06,
         -1.7062e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.1260110139846802


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0,   13, 4363,  ...,  271, 2462,  709],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4763e-05,  1.0612e-04, -6.5110e-05,  ...,  1.7500e-05,
         -2.2252e-05,  8.0572e-05],
        [-5.4836e-06, -4.0829e-06,  1.4938e-06,  ..., -4.9025e-06,
         -1.4380e-06, -2.7791e-06],
        [-6.2883e-06, -4.6790e-06,  1.7174e-06,  ..., -5.6475e-06,
         -1.6466e-06, -3.1963e-06],
        [-9.1195e-06, -6.7949e-06,  2.4885e-06,  ..., -8.1956e-06,
         -2.3767e-06, -4.6343e-06],
        [-1.2457e-05, -9.2387e-06,  3.3900e-06,  ..., -1.1176e-05,
         -3.2559e-06, -6.3032e-06]], device='cuda:0')
Loss: 1.1438169479370117


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[    0,  7203,   271,  ...,    12,   275,  3338],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8685e-05,  1.1527e-04, -1.0093e-04,  ...,  1.8929e-05,
         -2.2252e-05,  7.3609e-05],
        [-8.1807e-06, -6.1095e-06,  2.2091e-06,  ..., -7.3612e-06,
         -2.1160e-06, -4.1351e-06],
        [-9.3877e-06, -7.0035e-06,  2.5406e-06,  ..., -8.4639e-06,
         -2.4214e-06, -4.7535e-06],
        [-1.3649e-05, -1.0207e-05,  3.6955e-06,  ..., -1.2308e-05,
         -3.5092e-06, -6.9141e-06],
        [-1.8656e-05, -1.3888e-05,  5.0291e-06,  ..., -1.6809e-05,
         -4.8056e-06, -9.4026e-06]], device='cuda:0')
Loss: 1.1380125284194946


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8073e-05,  1.3654e-04, -9.1808e-05,  ..., -2.3229e-05,
         -2.7416e-05,  5.0054e-05],
        [-1.0908e-05, -8.1211e-06,  2.9355e-06,  ..., -9.8199e-06,
         -2.7828e-06, -5.4911e-06],
        [-1.2517e-05, -9.3132e-06,  3.3788e-06,  ..., -1.1295e-05,
         -3.1851e-06, -6.3106e-06],
        [-1.8239e-05, -1.3605e-05,  4.9248e-06,  ..., -1.6481e-05,
         -4.6343e-06, -9.1940e-06],
        [-2.4855e-05, -1.8448e-05,  6.6832e-06,  ..., -2.2411e-05,
         -6.3181e-06, -1.2472e-05]], device='cuda:0')
Loss: 1.1478352546691895


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,   278,  6046,  ...,  1671,   199,   395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0156e-05,  1.3865e-04, -1.1831e-04,  ...,  3.7719e-05,
         -6.2884e-05,  1.8138e-05],
        [-1.3605e-05, -1.0118e-05,  3.6918e-06,  ..., -1.2249e-05,
         -3.4757e-06, -6.8396e-06],
        [-1.5661e-05, -1.1638e-05,  4.2617e-06,  ..., -1.4126e-05,
         -3.9898e-06, -7.8753e-06],
        [-2.2769e-05, -1.6958e-05,  6.1989e-06,  ..., -2.0564e-05,
         -5.7891e-06, -1.1459e-05],
        [-3.1024e-05, -2.3007e-05,  8.4117e-06,  ..., -2.7955e-05,
         -7.8976e-06, -1.5542e-05]], device='cuda:0')
Loss: 1.1220979690551758


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   278,  6318,  ...,   458,    17,     9],
        [    0,  7377, 30662,  ...,   287,   694, 13403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4169e-05,  1.7751e-04, -1.3605e-04,  ...,  9.5744e-05,
         -9.2662e-05,  3.0660e-05],
        [-1.6376e-05, -1.2159e-05,  4.4256e-06,  ..., -1.4707e-05,
         -4.1649e-06, -8.2031e-06],
        [-1.8835e-05, -1.3992e-05,  5.1074e-06,  ..., -1.6943e-05,
         -4.7795e-06, -9.4399e-06],
        [-2.7388e-05, -2.0370e-05,  7.4282e-06,  ..., -2.4676e-05,
         -6.9290e-06, -1.3724e-05],
        [-3.7342e-05, -2.7686e-05,  1.0088e-05,  ..., -3.3557e-05,
         -9.4697e-06, -1.8641e-05]], device='cuda:0')
Loss: 1.1269162893295288


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,   759,  1128,  ...,   221,   474,   221],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4087e-05,  1.3448e-04, -1.3956e-04,  ...,  1.0561e-04,
         -1.1210e-04,  1.7686e-05],
        [-1.9103e-05, -1.4201e-05,  5.1744e-06,  ..., -1.7121e-05,
         -4.8503e-06, -9.5740e-06],
        [-2.1994e-05, -1.6347e-05,  5.9754e-06,  ..., -1.9744e-05,
         -5.5730e-06, -1.1027e-05],
        [-3.1978e-05, -2.3812e-05,  8.6948e-06,  ..., -2.8759e-05,
         -8.0839e-06, -1.6034e-05],
        [-4.3541e-05, -3.2336e-05,  1.1794e-05,  ..., -3.9071e-05,
         -1.1034e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.142667531967163


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0,    12,   287,  ...,   199,   769, 18432]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1421e-04,  1.3997e-04, -1.6172e-04,  ...,  9.3316e-05,
         -1.1510e-04,  3.7435e-05],
        [-2.1890e-05, -1.6242e-05,  5.8971e-06,  ..., -1.9595e-05,
         -5.5395e-06, -1.0937e-05],
        [-2.5168e-05, -1.8671e-05,  6.8024e-06,  ..., -2.2560e-05,
         -6.3591e-06, -1.2577e-05],
        [-3.6627e-05, -2.7210e-05,  9.9018e-06,  ..., -3.2872e-05,
         -9.2313e-06, -1.8299e-05],
        [-4.9829e-05, -3.6925e-05,  1.3426e-05,  ..., -4.4644e-05,
         -1.2584e-05, -2.4825e-05]], device='cuda:0')
Loss: 1.1523239612579346
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0035,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4248e-05,  2.9629e-05, -5.9245e-05,  ...,  7.6043e-06,
         -2.6020e-05,  2.8513e-05],
        [-2.1430e-05, -1.5554e-05,  4.6548e-06,  ..., -1.8514e-05,
         -3.9875e-06, -8.9707e-06],
        [ 2.8199e-05,  2.0464e-05, -4.6268e-06,  ...,  2.3983e-05,
          4.6818e-06,  9.8033e-06],
        [-2.3082e-05, -1.7552e-05,  6.0861e-06,  ..., -2.1805e-05,
         -4.8262e-06, -1.0470e-05],
        [-4.6171e-05, -3.3390e-05,  9.3060e-06,  ..., -3.9727e-05,
         -8.4502e-06, -1.9721e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6704e-08, 1.4524e-08, 1.7196e-08,  ..., 1.9631e-08, 3.1101e-08,
         6.4975e-09],
        [4.0689e-11, 2.3733e-11, 1.6289e-12,  ..., 2.8173e-11, 1.3546e-12,
         6.1905e-12],
        [6.5182e-10, 3.8773e-10, 2.1435e-11,  ..., 5.0515e-10, 1.3921e-11,
         1.0548e-10],
        [2.2233e-10, 2.7007e-10, 6.2847e-12,  ..., 1.5740e-10, 8.1405e-12,
         5.2645e-11],
        [1.6285e-10, 8.9822e-11, 7.1959e-12,  ..., 1.1211e-10, 3.7875e-12,
         2.4860e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.418499758283982e-05, 1.418499758283982e-05]
scheduler_last_epoch: 50


Running epoch 0, step 400, batch 400
Sampled inputs[:2]: tensor([[   0, 1883, 1090,  ...,  365, 1943,  298],
        [   0, 6957,  271,  ..., 9094,  266, 4320]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6650e-05, -2.0421e-06, -1.2647e-05,  ..., -3.7267e-06,
         -2.5576e-05, -4.5299e-05],
        [-2.6524e-06, -1.9819e-06,  7.3016e-07,  ..., -2.4289e-06,
         -6.4448e-07, -1.3784e-06],
        [-3.0845e-06, -2.2948e-06,  8.4564e-07,  ..., -2.8163e-06,
         -7.4878e-07, -1.5944e-06],
        [-4.5598e-06, -3.3975e-06,  1.2517e-06,  ..., -4.1723e-06,
         -1.1027e-06, -2.3544e-06],
        [-6.1095e-06, -4.5896e-06,  1.6838e-06,  ..., -5.6028e-06,
         -1.4901e-06, -3.1739e-06]], device='cuda:0')
Loss: 1.15364670753479


Running epoch 0, step 401, batch 401
Sampled inputs[:2]: tensor([[    0,   508,  3282,  ...,   334,   287, 31884],
        [    0,  4994,  8429,  ...,    12,   795,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3105e-05, -1.0659e-05, -3.1796e-05,  ...,  2.1681e-05,
         -3.3450e-05, -6.9099e-05],
        [-5.3644e-06, -4.0084e-06,  1.4566e-06,  ..., -4.8876e-06,
         -1.3150e-06, -2.7865e-06],
        [-6.2138e-06, -4.6343e-06,  1.6838e-06,  ..., -5.6624e-06,
         -1.5236e-06, -3.2187e-06],
        [-9.1493e-06, -6.8098e-06,  2.4810e-06,  ..., -8.3148e-06,
         -2.2277e-06, -4.7237e-06],
        [-1.2308e-05, -9.2089e-06,  3.3379e-06,  ..., -1.1206e-05,
         -3.0175e-06, -6.3777e-06]], device='cuda:0')
Loss: 1.1346559524536133


Running epoch 0, step 402, batch 402
Sampled inputs[:2]: tensor([[   0,    9, 1471,  ...,  741,  266, 5821],
        [   0,  287, 2421,  ..., 6612,  352,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5003e-05,  4.8007e-05, -8.1188e-05,  ..., -1.2681e-05,
         -2.6616e-05, -4.4612e-05],
        [-8.1062e-06, -6.0350e-06,  2.1644e-06,  ..., -7.3314e-06,
         -2.0489e-06, -4.1947e-06],
        [-9.3728e-06, -6.9737e-06,  2.4997e-06,  ..., -8.4788e-06,
         -2.3656e-06, -4.8429e-06],
        [ 1.7558e-04,  1.2393e-04, -4.4611e-05,  ...,  1.5329e-04,
          3.4888e-05,  9.8110e-05],
        [-1.8597e-05, -1.3858e-05,  4.9621e-06,  ..., -1.6809e-05,
         -4.6939e-06, -9.5963e-06]], device='cuda:0')
Loss: 1.1308526992797852


Running epoch 0, step 403, batch 403
Sampled inputs[:2]: tensor([[    0,  3532,   300,  ...,    12,   461,   806],
        [    0, 14161,  1241,  ..., 15255,   768,  4239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1478e-05,  3.7076e-05, -7.5017e-05,  ..., -3.5623e-05,
         -6.7207e-05, -2.7831e-05],
        [-1.0788e-05, -8.0466e-06,  2.8871e-06,  ..., -9.7454e-06,
         -2.7157e-06, -5.5879e-06],
        [-1.2487e-05, -9.3132e-06,  3.3379e-06,  ..., -1.1280e-05,
         -3.1367e-06, -6.4597e-06],
        [ 1.7099e-04,  1.2051e-04, -4.3374e-05,  ...,  1.4915e-04,
          3.3755e-05,  9.5725e-05],
        [-2.4796e-05, -1.8477e-05,  6.6236e-06,  ..., -2.2382e-05,
         -6.2212e-06, -1.2800e-05]], device='cuda:0')
Loss: 1.1273113489151


Running epoch 0, step 404, batch 404
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,   741,  2985,  ...,   199,   769,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1956e-05,  2.1184e-05, -4.2229e-05,  ..., -5.3922e-05,
         -6.8114e-05, -8.1907e-05],
        [-1.3486e-05, -1.0073e-05,  3.6098e-06,  ..., -1.2189e-05,
         -3.3565e-06, -6.9961e-06],
        [-1.5602e-05, -1.1668e-05,  4.1761e-06,  ..., -1.4111e-05,
         -3.8743e-06, -8.0988e-06],
        [ 1.6637e-04,  1.1701e-04, -4.2130e-05,  ...,  1.4494e-04,
          3.2660e-05,  9.3296e-05],
        [-3.0994e-05, -2.3156e-05,  8.2925e-06,  ..., -2.8014e-05,
         -7.6964e-06, -1.6049e-05]], device='cuda:0')
Loss: 1.1230335235595703


Running epoch 0, step 405, batch 405
Sampled inputs[:2]: tensor([[    0,  7294, 23782,  ...,   471, 11528,  3437],
        [    0,   287, 39084,  ...,   266,  1817,  1589]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3132e-05,  1.4658e-05, -4.2248e-05,  ..., -6.3514e-05,
         -9.3863e-05, -3.2827e-05],
        [-1.6198e-05, -1.2130e-05,  4.3400e-06,  ..., -1.4663e-05,
         -4.0606e-06, -8.3819e-06],
        [-1.8716e-05, -1.4037e-05,  5.0180e-06,  ..., -1.6958e-05,
         -4.6864e-06, -9.7007e-06],
        [ 1.6181e-04,  1.1353e-04, -4.0893e-05,  ...,  1.4077e-04,
          3.1475e-05,  9.0957e-05],
        [-3.7134e-05, -2.7835e-05,  9.9465e-06,  ..., -3.3617e-05,
         -9.2983e-06, -1.9193e-05]], device='cuda:0')
Loss: 1.1461491584777832


Running epoch 0, step 406, batch 406
Sampled inputs[:2]: tensor([[    0,  3070,  9719,  ...,   600,  4207,  4293],
        [    0,   292, 23950,  ...,  9305,   287,  4401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0585e-04,  7.4364e-06, -7.1348e-05,  ..., -3.5147e-05,
         -8.9411e-05, -1.0295e-05],
        [-1.8924e-05, -1.4156e-05,  5.0738e-06,  ..., -1.7107e-05,
         -4.7348e-06, -9.7826e-06],
        [-2.1860e-05, -1.6391e-05,  5.8636e-06,  ..., -1.9774e-05,
         -5.4650e-06, -1.1317e-05],
        [ 1.5719e-04,  1.1008e-04, -3.9649e-05,  ...,  1.3663e-04,
          3.0336e-05,  8.8588e-05],
        [-4.3392e-05, -3.2514e-05,  1.1623e-05,  ..., -3.9220e-05,
         -1.0848e-05, -2.2396e-05]], device='cuda:0')
Loss: 1.1342532634735107


Running epoch 0, step 407, batch 407
Sampled inputs[:2]: tensor([[    0,    12, 17340,  ...,   408,  1550,  2415],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4830e-05,  3.0461e-05, -1.0682e-04,  ..., -5.1333e-05,
         -1.2373e-04,  1.1713e-06],
        [-2.1622e-05, -1.6183e-05,  5.8077e-06,  ..., -1.9535e-05,
         -5.4203e-06, -1.1183e-05],
        [-2.4974e-05, -1.8731e-05,  6.7092e-06,  ..., -2.2575e-05,
         -6.2510e-06, -1.2927e-05],
        [ 1.5263e-04,  1.0665e-04, -3.8405e-05,  ...,  1.3252e-04,
          2.9181e-05,  8.6218e-05],
        [-4.9591e-05, -3.7163e-05,  1.3307e-05,  ..., -4.4793e-05,
         -1.2420e-05, -2.5600e-05]], device='cuda:0')
Loss: 1.1476664543151855
Graident accumulation at epoch 0, step 407, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0155, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0341e-05,  2.9712e-05, -6.4002e-05,  ...,  1.7106e-06,
         -3.5792e-05,  2.5778e-05],
        [-2.1449e-05, -1.5617e-05,  4.7701e-06,  ..., -1.8616e-05,
         -4.1308e-06, -9.1920e-06],
        [ 2.2882e-05,  1.6545e-05, -3.4932e-06,  ...,  1.9328e-05,
          3.5886e-06,  7.5303e-06],
        [-5.5106e-06, -5.1322e-06,  1.6370e-06,  ..., -6.3728e-06,
         -1.4255e-06, -8.0082e-07],
        [-4.6513e-05, -3.3767e-05,  9.7061e-06,  ..., -4.0233e-05,
         -8.8472e-06, -2.0309e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6666e-08, 1.4510e-08, 1.7190e-08,  ..., 1.9614e-08, 3.1085e-08,
         6.4910e-09],
        [4.1116e-11, 2.3971e-11, 1.6610e-12,  ..., 2.8526e-11, 1.3826e-12,
         6.3093e-12],
        [6.5179e-10, 3.8769e-10, 2.1458e-11,  ..., 5.0516e-10, 1.3947e-11,
         1.0554e-10],
        [2.4541e-10, 2.8117e-10, 7.7533e-12,  ..., 1.7480e-10, 8.9839e-12,
         6.0026e-11],
        [1.6514e-10, 9.1113e-11, 7.3658e-12,  ..., 1.1400e-10, 3.9380e-12,
         2.5490e-11]], device='cuda:0')
optimizer state dict: 51.0
lr: [1.3959200097809337e-05, 1.3959200097809337e-05]
scheduler_last_epoch: 51


Running epoch 0, step 408, batch 408
Sampled inputs[:2]: tensor([[   0,   18,   66,  ...,   65,   17,  287],
        [   0,   17,  292,  ..., 8055,  365, 3125]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1928e-05,  2.2518e-05, -1.8464e-05,  ..., -6.1561e-06,
         -2.1937e-05,  2.4003e-05],
        [-2.7418e-06, -2.0415e-06,  7.2643e-07,  ..., -2.4736e-06,
         -7.0408e-07, -1.4529e-06],
        [-3.1590e-06, -2.3693e-06,  8.3819e-07,  ..., -2.8461e-06,
         -8.0839e-07, -1.6764e-06],
        [ 4.4621e-04,  2.9846e-04, -1.4817e-04,  ...,  4.2321e-04,
          9.3494e-05,  2.6281e-04],
        [-6.2883e-06, -4.7088e-06,  1.6689e-06,  ..., -5.6922e-06,
         -1.6093e-06, -3.3379e-06]], device='cuda:0')
Loss: 1.1564834117889404


Running epoch 0, step 409, batch 409
Sampled inputs[:2]: tensor([[    0,  2173,   292,  ...,   344,  8106,   344],
        [    0,   957,  1357,  ..., 26179,   287,  6458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6222e-06,  2.1531e-05, -3.3039e-05,  ..., -2.4473e-05,
         -9.3009e-06,  2.9959e-05],
        [-5.4687e-06, -4.0829e-06,  1.4566e-06,  ..., -4.9323e-06,
         -1.3933e-06, -2.9206e-06],
        [-6.2883e-06, -4.7237e-06,  1.6801e-06,  ..., -5.6773e-06,
         -1.5981e-06, -3.3602e-06],
        [ 4.4162e-04,  2.9502e-04, -1.4693e-04,  ...,  4.1907e-04,
          9.2339e-05,  2.6034e-04],
        [-1.2517e-05, -9.3877e-06,  3.3379e-06,  ..., -1.1325e-05,
         -3.1814e-06, -6.6906e-06]], device='cuda:0')
Loss: 1.136348843574524


Running epoch 0, step 410, batch 410
Sampled inputs[:2]: tensor([[    0,   266,  6737,  ...,  2409,    12,   287],
        [    0,    14,   747,  ..., 12545,    12, 15209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3580e-05,  1.1929e-05, -5.8177e-05,  ..., -2.1156e-05,
         -1.4571e-05,  4.5544e-05],
        [-8.1509e-06, -6.1244e-06,  2.1905e-06,  ..., -7.3761e-06,
         -2.0675e-06, -4.3884e-06],
        [-9.3877e-06, -7.0930e-06,  2.5257e-06,  ..., -8.4937e-06,
         -2.3730e-06, -5.0515e-06],
        [ 4.3706e-04,  2.9153e-04, -1.4568e-04,  ...,  4.1489e-04,
          9.1199e-05,  2.5785e-04],
        [-1.8656e-05, -1.4067e-05,  5.0142e-06,  ..., -1.6928e-05,
         -4.7162e-06, -1.0043e-05]], device='cuda:0')
Loss: 1.12785804271698


Running epoch 0, step 411, batch 411
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,    21,    66,  ...,  1377,   278,  1634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8925e-05,  1.3008e-05, -9.0323e-05,  ..., -2.7805e-05,
         -2.8240e-05,  6.1049e-05],
        [-1.0833e-05, -8.1211e-06,  2.9095e-06,  ..., -9.8497e-06,
         -2.7902e-06, -5.8413e-06],
        [-1.2502e-05, -9.4175e-06,  3.3639e-06,  ..., -1.1370e-05,
         -3.2112e-06, -6.7428e-06],
        [ 4.3256e-04,  2.8815e-04, -1.4445e-04,  ...,  4.1072e-04,
          8.9985e-05,  2.5541e-04],
        [-2.4825e-05, -1.8686e-05,  6.6757e-06,  ..., -2.2620e-05,
         -6.3851e-06, -1.3396e-05]], device='cuda:0')
Loss: 1.1604386568069458


Running epoch 0, step 412, batch 412
Sampled inputs[:2]: tensor([[    0,   278,  9939,  ...,  1238,    14,   445],
        [    0,  2467, 18011,  ...,  5913,  9281,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4548e-05,  2.9338e-05, -1.1018e-04,  ..., -1.8700e-05,
         -2.5969e-05,  5.9277e-05],
        [-1.3530e-05, -1.0148e-05,  3.6471e-06,  ..., -1.2338e-05,
         -3.5167e-06, -7.3314e-06],
        [-1.5616e-05, -1.1757e-05,  4.2133e-06,  ..., -1.4246e-05,
         -4.0457e-06, -8.4639e-06],
        [ 4.2800e-04,  2.8472e-04, -1.4320e-04,  ...,  4.0652e-04,
          8.8763e-05,  2.5289e-04],
        [-3.0994e-05, -2.3305e-05,  8.3596e-06,  ..., -2.8312e-05,
         -8.0392e-06, -1.6794e-05]], device='cuda:0')
Loss: 1.1464931964874268


Running epoch 0, step 413, batch 413
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,   259,  5918,  ...,   508,  3433,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4506e-05,  5.9291e-05, -1.2148e-04,  ..., -6.0246e-05,
         -1.6227e-05,  3.6674e-05],
        [-1.6183e-05, -1.2130e-05,  4.3660e-06,  ..., -1.4767e-05,
         -4.1872e-06, -8.7693e-06],
        [-1.8731e-05, -1.4096e-05,  5.0589e-06,  ..., -1.7092e-05,
         -4.8317e-06, -1.0148e-05],
        [ 4.2344e-04,  2.8132e-04, -1.4197e-04,  ...,  4.0238e-04,
          8.7623e-05,  2.5043e-04],
        [-3.7163e-05, -2.7925e-05,  1.0028e-05,  ..., -3.3945e-05,
         -9.5889e-06, -2.0117e-05]], device='cuda:0')
Loss: 1.1450519561767578


Running epoch 0, step 414, batch 414
Sampled inputs[:2]: tensor([[    0,  5625,  2558,  ...,   680,   292,   494],
        [    0,   721,  1717,  ...,   278, 26029,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0664e-05,  5.3970e-05, -1.3423e-04,  ..., -8.5079e-05,
         -1.9920e-05,  4.0596e-05],
        [-1.8850e-05, -1.4141e-05,  5.0776e-06,  ..., -1.7211e-05,
         -4.8466e-06, -1.0192e-05],
        [-2.1845e-05, -1.6451e-05,  5.8897e-06,  ..., -1.9938e-05,
         -5.5991e-06, -1.1809e-05],
        [ 4.1888e-04,  2.7788e-04, -1.4074e-04,  ...,  3.9820e-04,
          8.6498e-05,  2.4800e-04],
        [-4.3392e-05, -3.2604e-05,  1.1683e-05,  ..., -3.9607e-05,
         -1.1116e-05, -2.3425e-05]], device='cuda:0')
Loss: 1.137347936630249


Running epoch 0, step 415, batch 415
Sampled inputs[:2]: tensor([[    0,   266,  5232,  ...,  2719,    13, 25385],
        [    0,  1265,   328,  ...,  2282, 35414,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0123e-05,  7.8762e-05, -1.8840e-04,  ..., -8.4986e-05,
         -1.6182e-05,  4.2839e-05],
        [-2.1517e-05, -1.6153e-05,  5.7966e-06,  ..., -1.9655e-05,
         -5.5134e-06, -1.1615e-05],
        [-2.4930e-05, -1.8775e-05,  6.7204e-06,  ..., -2.2769e-05,
         -6.3702e-06, -1.3456e-05],
        [ 4.1432e-04,  2.7445e-04, -1.3952e-04,  ...,  3.9403e-04,
          8.5365e-05,  2.4556e-04],
        [-4.9561e-05, -3.7223e-05,  1.3337e-05,  ..., -4.5240e-05,
         -1.2651e-05, -2.6703e-05]], device='cuda:0')
Loss: 1.1284468173980713
Graident accumulation at epoch 0, step 415, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0155, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4319e-05,  3.4617e-05, -7.6441e-05,  ..., -6.9591e-06,
         -3.3831e-05,  2.7485e-05],
        [-2.1456e-05, -1.5670e-05,  4.8728e-06,  ..., -1.8720e-05,
         -4.2691e-06, -9.4343e-06],
        [ 1.8101e-05,  1.3013e-05, -2.4718e-06,  ...,  1.5118e-05,
          2.5927e-06,  5.4317e-06],
        [ 3.6473e-05,  2.2827e-05, -1.2478e-05,  ...,  3.3668e-05,
          7.2536e-06,  2.3835e-05],
        [-4.6818e-05, -3.4113e-05,  1.0069e-05,  ..., -4.0734e-05,
         -9.2276e-06, -2.0948e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6624e-08, 1.4502e-08, 1.7209e-08,  ..., 1.9601e-08, 3.1054e-08,
         6.4863e-09],
        [4.1538e-11, 2.4208e-11, 1.6929e-12,  ..., 2.8884e-11, 1.4116e-12,
         6.4380e-12],
        [6.5176e-10, 3.8766e-10, 2.1482e-11,  ..., 5.0517e-10, 1.3973e-11,
         1.0561e-10],
        [4.1682e-10, 3.5622e-10, 2.7210e-11,  ..., 3.2989e-10, 1.6262e-11,
         1.2026e-10],
        [1.6743e-10, 9.2408e-11, 7.5363e-12,  ..., 1.1593e-10, 4.0941e-12,
         2.6178e-11]], device='cuda:0')
optimizer state dict: 52.0
lr: [1.3730982703887026e-05, 1.3730982703887026e-05]
scheduler_last_epoch: 52


Running epoch 0, step 416, batch 416
Sampled inputs[:2]: tensor([[    0,   706,  6989,  ...,  6914,    15,  2537],
        [    0, 48705,   292,  ...,   266,  2548,  2697]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2127e-05, -1.0617e-05, -7.2451e-06,  ..., -1.6830e-05,
         -1.3521e-07,  3.3897e-05],
        [-2.7269e-06, -2.0266e-06,  7.3388e-07,  ..., -2.4885e-06,
         -6.8173e-07, -1.4901e-06],
        [-3.1590e-06, -2.3544e-06,  8.4937e-07,  ..., -2.8908e-06,
         -7.8976e-07, -1.7211e-06],
        [-4.6492e-06, -3.4720e-06,  1.2517e-06,  ..., -4.2617e-06,
         -1.1623e-06, -2.5332e-06],
        [-6.2585e-06, -4.6492e-06,  1.6764e-06,  ..., -5.7220e-06,
         -1.5646e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.1369974613189697


Running epoch 0, step 417, batch 417
Sampled inputs[:2]: tensor([[   0,  298,  696,  ..., 3502,  287, 1047],
        [   0,  278, 5492,  ...,  328,  995,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8711e-06,  3.0577e-06, -2.3902e-05,  ..., -2.2736e-05,
         -3.1376e-06,  4.0640e-05],
        [-5.4389e-06, -4.0531e-06,  1.4417e-06,  ..., -4.9472e-06,
         -1.3523e-06, -2.9653e-06],
        [-6.2734e-06, -4.6790e-06,  1.6652e-06,  ..., -5.7220e-06,
         -1.5609e-06, -3.4124e-06],
        [-9.2983e-06, -6.9439e-06,  2.4661e-06,  ..., -8.4937e-06,
         -2.3022e-06, -5.0515e-06],
        [-1.2487e-05, -9.2983e-06,  3.3081e-06,  ..., -1.1384e-05,
         -3.1069e-06, -6.7949e-06]], device='cuda:0')
Loss: 1.1437785625457764


Running epoch 0, step 418, batch 418
Sampled inputs[:2]: tensor([[   0, 1171, 2926,  ...,  259, 4288,  654],
        [   0,  443,   40,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2166e-05, -2.0412e-06, -8.1207e-05,  ..., -3.3781e-05,
         -1.6573e-05,  4.6676e-05],
        [-8.1211e-06, -6.0797e-06,  2.1569e-06,  ..., -7.4208e-06,
         -2.0079e-06, -4.4256e-06],
        [-9.3430e-06, -7.0035e-06,  2.4848e-06,  ..., -8.5533e-06,
         -2.3097e-06, -5.0813e-06],
        [-1.3888e-05, -1.0416e-05,  3.6955e-06,  ..., -1.2726e-05,
         -3.4198e-06, -7.5549e-06],
        [-1.8626e-05, -1.3947e-05,  4.9472e-06,  ..., -1.7047e-05,
         -4.6045e-06, -1.0133e-05]], device='cuda:0')
Loss: 1.1522516012191772


Running epoch 0, step 419, batch 419
Sampled inputs[:2]: tensor([[    0,   409, 22809,  ...,   342,   720,    14],
        [    0,    14,   560,  ...,    12,  8593,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9717e-05, -1.0800e-05, -1.0505e-04,  ..., -3.2204e-05,
         -1.6549e-05,  5.4303e-05],
        [-1.0833e-05, -8.1509e-06,  2.8834e-06,  ..., -9.8944e-06,
         -2.6710e-06, -5.9083e-06],
        [-1.2457e-05, -9.3877e-06,  3.3230e-06,  ..., -1.1399e-05,
         -3.0734e-06, -6.7875e-06],
        [-1.8537e-05, -1.3977e-05,  4.9546e-06,  ..., -1.6987e-05,
         -4.5598e-06, -1.0103e-05],
        [-2.4855e-05, -1.8686e-05,  6.6161e-06,  ..., -2.2709e-05,
         -6.1318e-06, -1.3530e-05]], device='cuda:0')
Loss: 1.1429294347763062


Running epoch 0, step 420, batch 420
Sampled inputs[:2]: tensor([[   0, 2579,  278,  ...,   56,    9,  271],
        [   0,  221,  422,  ..., 2693,  733,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3767e-05,  3.4022e-06, -9.6562e-05,  ..., -4.9981e-06,
         -4.6184e-05,  6.4555e-05],
        [-1.3500e-05, -1.0148e-05,  3.5949e-06,  ..., -1.2308e-05,
         -3.3639e-06, -7.3686e-06],
        [-1.5572e-05, -1.1712e-05,  4.1537e-06,  ..., -1.4216e-05,
         -3.8780e-06, -8.4937e-06],
        [-2.3156e-05, -1.7434e-05,  6.1914e-06,  ..., -2.1160e-05,
         -5.7518e-06, -1.2636e-05],
        [-3.1024e-05, -2.3305e-05,  8.2552e-06,  ..., -2.8282e-05,
         -7.7263e-06, -1.6898e-05]], device='cuda:0')
Loss: 1.1285382509231567


Running epoch 0, step 421, batch 421
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   300,   275,  1184],
        [    0, 49141,    14,  ...,   342,   259,  1943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0625e-05,  4.5854e-05, -7.0233e-05,  ..., -2.6191e-05,
         -6.4456e-05,  4.8223e-05],
        [-1.6212e-05, -1.2204e-05,  4.3213e-06,  ..., -1.4782e-05,
         -4.0457e-06, -8.8513e-06],
        [-1.8686e-05, -1.4082e-05,  4.9882e-06,  ..., -1.7062e-05,
         -4.6603e-06, -1.0200e-05],
        [-2.7835e-05, -2.0981e-05,  7.4506e-06,  ..., -2.5421e-05,
         -6.9290e-06, -1.5199e-05],
        [-3.7283e-05, -2.8044e-05,  9.9316e-06,  ..., -3.4004e-05,
         -9.2983e-06, -2.0325e-05]], device='cuda:0')
Loss: 1.1553701162338257


Running epoch 0, step 422, batch 422
Sampled inputs[:2]: tensor([[   0,  300, 1635,  ...,  437,  266, 1136],
        [   0, 6574, 1707,  ...,   14, 5077,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3557e-05,  6.3602e-05, -8.7476e-05,  ..., -1.9348e-05,
         -5.1209e-05,  1.3714e-05],
        [-1.8924e-05, -1.4216e-05,  5.0589e-06,  ..., -1.7241e-05,
         -4.7386e-06, -1.0349e-05],
        [-2.1800e-05, -1.6391e-05,  5.8338e-06,  ..., -1.9878e-05,
         -5.4538e-06, -1.1913e-05],
        [-3.2514e-05, -2.4453e-05,  8.7246e-06,  ..., -2.9653e-05,
         -8.1137e-06, -1.7762e-05],
        [-4.3511e-05, -3.2663e-05,  1.1623e-05,  ..., -3.9637e-05,
         -1.0885e-05, -2.3752e-05]], device='cuda:0')
Loss: 1.1346980333328247


Running epoch 0, step 423, batch 423
Sampled inputs[:2]: tensor([[    0,  3473,   278,  ..., 11743,   472,   346],
        [    0,    43,   527,  ...,  4309,    14,  8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2676e-05,  3.4124e-05, -1.1835e-04,  ..., -2.9083e-05,
         -6.7855e-05,  8.1826e-06],
        [-2.1636e-05, -1.6257e-05,  5.7966e-06,  ..., -1.9714e-05,
         -5.4352e-06, -1.1817e-05],
        [-2.4945e-05, -1.8761e-05,  6.6869e-06,  ..., -2.2739e-05,
         -6.2585e-06, -1.3612e-05],
        [-3.7163e-05, -2.7969e-05,  9.9912e-06,  ..., -3.3885e-05,
         -9.3058e-06, -2.0280e-05],
        [-4.9740e-05, -3.7372e-05,  1.3314e-05,  ..., -4.5329e-05,
         -1.2487e-05, -2.7120e-05]], device='cuda:0')
Loss: 1.1459017992019653
Graident accumulation at epoch 0, step 423, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0164,  0.0148, -0.0271,  ...,  0.0283, -0.0155, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6155e-05,  3.4568e-05, -8.0632e-05,  ..., -9.1715e-06,
         -3.7233e-05,  2.5554e-05],
        [-2.1474e-05, -1.5729e-05,  4.9652e-06,  ..., -1.8820e-05,
         -4.3857e-06, -9.6725e-06],
        [ 1.3796e-05,  9.8355e-06, -1.5559e-06,  ...,  1.1332e-05,
          1.7076e-06,  3.5273e-06],
        [ 2.9109e-05,  1.7747e-05, -1.0231e-05,  ...,  2.6912e-05,
          5.5976e-06,  1.9423e-05],
        [-4.7110e-05, -3.4439e-05,  1.0394e-05,  ..., -4.1194e-05,
         -9.5536e-06, -2.1565e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6580e-08, 1.4489e-08, 1.7205e-08,  ..., 1.9583e-08, 3.1028e-08,
         6.4799e-09],
        [4.1965e-11, 2.4448e-11, 1.7248e-12,  ..., 2.9244e-11, 1.4398e-12,
         6.5711e-12],
        [6.5173e-10, 3.8762e-10, 2.1505e-11,  ..., 5.0518e-10, 1.3998e-11,
         1.0569e-10],
        [4.1779e-10, 3.5664e-10, 2.7283e-11,  ..., 3.3071e-10, 1.6333e-11,
         1.2055e-10],
        [1.6974e-10, 9.3712e-11, 7.7060e-12,  ..., 1.1787e-10, 4.2459e-12,
         2.6887e-11]], device='cuda:0')
optimizer state dict: 53.0
lr: [1.3500484890183603e-05, 1.3500484890183603e-05]
scheduler_last_epoch: 53


Running epoch 0, step 424, batch 424
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2208,   12, 1901],
        [   0,  368, 2035,  ...,  266, 1122,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5205e-05,  1.5034e-05, -5.6069e-05,  ...,  8.2398e-06,
         -3.1728e-05, -1.3963e-05],
        [-2.6673e-06, -2.0564e-06,  6.8918e-07,  ..., -2.4736e-06,
         -6.8918e-07, -1.4976e-06],
        [-3.0845e-06, -2.3842e-06,  7.9721e-07,  ..., -2.8610e-06,
         -7.9721e-07, -1.7285e-06],
        [-4.6194e-06, -3.5763e-06,  1.1921e-06,  ..., -4.2915e-06,
         -1.1846e-06, -2.5928e-06],
        [-6.1095e-06, -4.7088e-06,  1.5721e-06,  ..., -5.6624e-06,
         -1.5721e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.1331437826156616


Running epoch 0, step 425, batch 425
Sampled inputs[:2]: tensor([[    0,   278,  6653,  ...,  7524,   271, 28279],
        [    0,  1802,  4165,  ...,   298,   445,    28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6040e-05,  3.5264e-05, -4.2357e-05,  ...,  8.0991e-06,
         -3.4308e-05, -2.3599e-05],
        [-5.3942e-06, -4.1276e-06,  1.4417e-06,  ..., -4.9323e-06,
         -1.3746e-06, -3.0026e-06],
        [-6.1989e-06, -4.7535e-06,  1.6540e-06,  ..., -5.6773e-06,
         -1.5758e-06, -3.4496e-06],
        [-9.2685e-06, -7.0930e-06,  2.4736e-06,  ..., -8.4937e-06,
         -2.3395e-06, -5.1558e-06],
        [-1.2368e-05, -9.4473e-06,  3.2932e-06,  ..., -1.1295e-05,
         -3.1367e-06, -6.8545e-06]], device='cuda:0')
Loss: 1.156541109085083


Running epoch 0, step 426, batch 426
Sampled inputs[:2]: tensor([[   0,   40,  568,  ..., 3750,  300, 3421],
        [   0,  271, 5738,  ...,   12,   21, 9023]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3136e-05,  3.2732e-05, -5.0025e-05,  ...,  2.9087e-05,
         -4.4510e-05, -2.8885e-05],
        [-8.1062e-06, -6.1840e-06,  2.1495e-06,  ..., -7.3910e-06,
         -2.0489e-06, -4.4629e-06],
        [-9.3132e-06, -7.1079e-06,  2.4661e-06,  ..., -8.5086e-06,
         -2.3469e-06, -5.1260e-06],
        [-1.3977e-05, -1.0639e-05,  3.7029e-06,  ..., -1.2755e-05,
         -3.5018e-06, -7.6741e-06],
        [-1.8656e-05, -1.4186e-05,  4.9323e-06,  ..., -1.6987e-05,
         -4.6939e-06, -1.0237e-05]], device='cuda:0')
Loss: 1.1379233598709106


Running epoch 0, step 427, batch 427
Sampled inputs[:2]: tensor([[    0,   382,    17,  ...,  8733,    13,  9306],
        [    0, 29258,   765,  ...,  4196,    19,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0324e-05,  1.5686e-05, -7.3336e-05,  ...,  6.3572e-06,
         -7.1328e-05, -3.9418e-05],
        [-1.0818e-05, -8.2701e-06,  2.8908e-06,  ..., -9.8795e-06,
         -2.6971e-06, -5.9903e-06],
        [-1.2428e-05, -9.5069e-06,  3.3192e-06,  ..., -1.1370e-05,
         -3.0883e-06, -6.8769e-06],
        [-1.8656e-05, -1.4246e-05,  4.9844e-06,  ..., -1.7047e-05,
         -4.6119e-06, -1.0297e-05],
        [-2.4825e-05, -1.8924e-05,  6.6161e-06,  ..., -2.2650e-05,
         -6.1616e-06, -1.3709e-05]], device='cuda:0')
Loss: 1.1169402599334717


Running epoch 0, step 428, batch 428
Sampled inputs[:2]: tensor([[   0,  287, 1477,  ...,  997,  292, 4471],
        [   0,   12,  266,  ...,  287, 2888, 4845]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5738e-06, -1.7698e-06, -7.3734e-05,  ...,  6.3201e-06,
         -9.3434e-05, -2.3997e-05],
        [-1.3545e-05, -1.0327e-05,  3.6098e-06,  ..., -1.2323e-05,
         -3.3528e-06, -7.4506e-06],
        [-1.5587e-05, -1.1876e-05,  4.1500e-06,  ..., -1.4201e-05,
         -3.8445e-06, -8.5682e-06],
        [-2.3454e-05, -1.7852e-05,  6.2510e-06,  ..., -2.1338e-05,
         -5.7593e-06, -1.2860e-05],
        [-3.1114e-05, -2.3663e-05,  8.2776e-06,  ..., -2.8312e-05,
         -7.6741e-06, -1.7092e-05]], device='cuda:0')
Loss: 1.1353099346160889


Running epoch 0, step 429, batch 429
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   266,  2025,   287],
        [    0,   266,  2552,  ...,    13, 16179,   800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4565e-05, -1.0126e-05, -1.2858e-04,  ...,  4.0867e-05,
         -1.0979e-04, -4.5497e-05],
        [-1.6242e-05, -1.2368e-05,  4.3362e-06,  ..., -1.4797e-05,
         -4.0419e-06, -8.9407e-06],
        [-1.8731e-05, -1.4260e-05,  4.9956e-06,  ..., -1.7077e-05,
         -4.6454e-06, -1.0304e-05],
        [-2.8104e-05, -2.1368e-05,  7.5027e-06,  ..., -2.5600e-05,
         -6.9365e-06, -1.5423e-05],
        [-3.7342e-05, -2.8372e-05,  9.9465e-06,  ..., -3.4034e-05,
         -9.2611e-06, -2.0519e-05]], device='cuda:0')
Loss: 1.1428251266479492


Running epoch 0, step 430, batch 430
Sampled inputs[:2]: tensor([[   0, 1086,  292,  ..., 1400,  367, 1874],
        [   0, 2140,   12,  ...,  696,  688, 1998]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9942e-05, -8.2857e-06, -1.5723e-04,  ...,  5.1134e-05,
         -1.6071e-04,  4.0507e-05],
        [-1.8984e-05, -1.4395e-05,  5.0552e-06,  ..., -1.7256e-05,
         -4.7050e-06, -1.0416e-05],
        [-2.1875e-05, -1.6600e-05,  5.8264e-06,  ..., -1.9923e-05,
         -5.4054e-06, -1.2003e-05],
        [-3.2872e-05, -2.4915e-05,  8.7544e-06,  ..., -2.9892e-05,
         -8.0839e-06, -1.7986e-05],
        [-4.3631e-05, -3.3051e-05,  1.1601e-05,  ..., -3.9697e-05,
         -1.0781e-05, -2.3901e-05]], device='cuda:0')
Loss: 1.1225333213806152


Running epoch 0, step 431, batch 431
Sampled inputs[:2]: tensor([[    0,  1356,    18,  ...,    31,   333,   199],
        [    0,  7527,    15,  ...,  2677,   292, 30654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4617e-05,  9.0805e-07, -2.0818e-04,  ...,  4.2056e-05,
         -2.0885e-04,  2.7801e-05],
        [-2.1681e-05, -1.6481e-05,  5.8040e-06,  ..., -1.9744e-05,
         -5.3905e-06, -1.1928e-05],
        [-2.4974e-05, -1.9014e-05,  6.6906e-06,  ..., -2.2799e-05,
         -6.1952e-06, -1.3746e-05],
        [ 1.2512e-04,  1.0194e-04, -3.6135e-05,  ...,  1.5241e-04,
          4.9425e-05,  8.6397e-05],
        [-4.9800e-05, -3.7819e-05,  1.3314e-05,  ..., -4.5389e-05,
         -1.2353e-05, -2.7359e-05]], device='cuda:0')
Loss: 1.1388074159622192
Graident accumulation at epoch 0, step 431, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0164,  0.0148, -0.0271,  ...,  0.0284, -0.0155, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7001e-05,  3.1202e-05, -9.3387e-05,  ..., -4.0487e-06,
         -5.4395e-05,  2.5779e-05],
        [-2.1495e-05, -1.5804e-05,  5.0490e-06,  ..., -1.8912e-05,
         -4.4862e-06, -9.8981e-06],
        [ 9.9191e-06,  6.9506e-06, -7.3128e-07,  ...,  7.9191e-06,
          9.1729e-07,  1.7999e-06],
        [ 3.8710e-05,  2.6167e-05, -1.2822e-05,  ...,  3.9463e-05,
          9.9804e-06,  2.6121e-05],
        [-4.7379e-05, -3.4777e-05,  1.0686e-05,  ..., -4.1613e-05,
         -9.8335e-06, -2.2145e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6536e-08, 1.4474e-08, 1.7231e-08,  ..., 1.9565e-08, 3.1040e-08,
         6.4742e-09],
        [4.2393e-11, 2.4695e-11, 1.7568e-12,  ..., 2.9604e-11, 1.4674e-12,
         6.7069e-12],
        [6.5171e-10, 3.8760e-10, 2.1528e-11,  ..., 5.0520e-10, 1.4023e-11,
         1.0578e-10],
        [4.3302e-10, 3.6668e-10, 2.8561e-11,  ..., 3.5361e-10, 1.8759e-11,
         1.2790e-10],
        [1.7205e-10, 9.5048e-11, 7.8756e-12,  ..., 1.1982e-10, 4.3943e-12,
         2.7609e-11]], device='cuda:0')
optimizer state dict: 54.0
lr: [1.3267847539628745e-05, 1.3267847539628745e-05]
scheduler_last_epoch: 54


Running epoch 0, step 432, batch 432
Sampled inputs[:2]: tensor([[    0,   346,    14,  ...,   381,   535,   505],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5316e-05, -3.8342e-06, -5.2852e-07,  ..., -3.2006e-05,
         -6.0126e-06,  5.0536e-07],
        [-2.7269e-06, -2.0713e-06,  7.4506e-07,  ..., -2.4885e-06,
         -6.7055e-07, -1.5199e-06],
        [-3.1143e-06, -2.3544e-06,  8.4937e-07,  ..., -2.8461e-06,
         -7.6368e-07, -1.7360e-06],
        [-4.7386e-06, -3.5912e-06,  1.2964e-06,  ..., -4.3213e-06,
         -1.1548e-06, -2.6375e-06],
        [-6.1989e-06, -4.7088e-06,  1.6913e-06,  ..., -5.6624e-06,
         -1.5199e-06, -3.4571e-06]], device='cuda:0')
Loss: 1.1237287521362305


Running epoch 0, step 433, batch 433
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0,   374,   298,  ..., 11183,    12,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0999e-05,  4.7289e-05,  2.2788e-05,  ..., -5.5514e-05,
         -1.8370e-05,  4.9221e-05],
        [-5.4836e-06, -4.1276e-06,  1.4715e-06,  ..., -5.0068e-06,
         -1.4007e-06, -3.0622e-06],
        [-6.2287e-06, -4.6790e-06,  1.6727e-06,  ..., -5.6922e-06,
         -1.5907e-06, -3.4794e-06],
        [-9.4473e-06, -7.0930e-06,  2.5406e-06,  ..., -8.6129e-06,
         -2.3916e-06, -5.2601e-06],
        [-1.2457e-05, -9.3579e-06,  3.3453e-06,  ..., -1.1384e-05,
         -3.1739e-06, -6.9439e-06]], device='cuda:0')
Loss: 1.134066104888916


Running epoch 0, step 434, batch 434
Sampled inputs[:2]: tensor([[   0,   12, 1197,  ...,  516, 1136, 9774],
        [   0,  278, 5798,  ...,  266,  729, 1798]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1553e-05,  4.6744e-05,  3.8399e-06,  ..., -3.7439e-05,
         -2.8943e-05,  5.8141e-05],
        [-8.1807e-06, -6.1542e-06,  2.1942e-06,  ..., -7.4506e-06,
         -2.0191e-06, -4.5672e-06],
        [-9.3281e-06, -7.0184e-06,  2.5034e-06,  ..., -8.5086e-06,
         -2.2985e-06, -5.2080e-06],
        [-1.4216e-05, -1.0669e-05,  3.8147e-06,  ..., -1.2934e-05,
         -3.4720e-06, -7.9125e-06],
        [-1.8626e-05, -1.4007e-05,  4.9919e-06,  ..., -1.6958e-05,
         -4.5821e-06, -1.0371e-05]], device='cuda:0')
Loss: 1.115381121635437


Running epoch 0, step 435, batch 435
Sampled inputs[:2]: tensor([[    0,    13, 23070,  ...,   266,   319,    13],
        [    0,  1192, 11929,  ...,   266,  1551,  1860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2421e-05,  7.3107e-05, -1.6551e-05,  ..., -3.7439e-05,
         -5.5052e-05,  9.5693e-05],
        [-1.0878e-05, -8.1956e-06,  2.9467e-06,  ..., -9.8944e-06,
         -2.7120e-06, -6.0722e-06],
        [-1.2428e-05, -9.3579e-06,  3.3714e-06,  ..., -1.1310e-05,
         -3.0883e-06, -6.9365e-06],
        [-1.8924e-05, -1.4246e-05,  5.1409e-06,  ..., -1.7226e-05,
         -4.6715e-06, -1.0550e-05],
        [-2.4796e-05, -1.8686e-05,  6.7204e-06,  ..., -2.2531e-05,
         -6.1616e-06, -1.3813e-05]], device='cuda:0')
Loss: 1.1318738460540771


Running epoch 0, step 436, batch 436
Sampled inputs[:2]: tensor([[    0,   380,   981,  ...,   567,  5407,   472],
        [    0,  4154, 14296,  ...,   516,  1796, 18233]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4452e-05,  9.4762e-05, -3.0989e-05,  ..., -2.5819e-05,
         -6.1455e-05,  6.4548e-05],
        [-1.3635e-05, -1.0237e-05,  3.6806e-06,  ..., -1.2353e-05,
         -3.3937e-06, -7.5996e-06],
        [-1.5631e-05, -1.1742e-05,  4.2282e-06,  ..., -1.4171e-05,
         -3.8780e-06, -8.7097e-06],
        [-2.3752e-05, -1.7837e-05,  6.4373e-06,  ..., -2.1547e-05,
         -5.8562e-06, -1.3217e-05],
        [-3.1114e-05, -2.3365e-05,  8.4043e-06,  ..., -2.8163e-05,
         -7.7188e-06, -1.7300e-05]], device='cuda:0')
Loss: 1.1180362701416016


Running epoch 0, step 437, batch 437
Sampled inputs[:2]: tensor([[    0,     9,   278,  ...,   278,   298,   452],
        [    0,   292, 44809,  ...,   642,   437,  9038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6813e-05,  1.3536e-04, -8.8672e-05,  ..., -4.0745e-06,
         -6.3053e-05,  5.8294e-05],
        [-1.6361e-05, -1.2279e-05,  4.4331e-06,  ..., -1.4856e-05,
         -4.0978e-06, -9.1419e-06],
        [-1.8731e-05, -1.4052e-05,  5.0813e-06,  ..., -1.7002e-05,
         -4.6715e-06, -1.0461e-05],
        [-2.8491e-05, -2.1368e-05,  7.7486e-06,  ..., -2.5898e-05,
         -7.0706e-06, -1.5900e-05],
        [-3.7283e-05, -2.7955e-05,  1.0103e-05,  ..., -3.3796e-05,
         -9.3058e-06, -2.0787e-05]], device='cuda:0')
Loss: 1.1365143060684204


Running epoch 0, step 438, batch 438
Sampled inputs[:2]: tensor([[    0,  3665,  1419,  ...,   600,   847,   328],
        [    0,    13, 20054,  ...,    19,     9,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9678e-05,  1.8776e-04, -1.0304e-04,  ..., -3.7404e-05,
         -6.1797e-05,  5.0571e-05],
        [-1.9088e-05, -1.4305e-05,  5.1633e-06,  ..., -1.7375e-05,
         -4.7646e-06, -1.0654e-05],
        [-2.1845e-05, -1.6376e-05,  5.9158e-06,  ..., -1.9878e-05,
         -5.4240e-06, -1.2189e-05],
        [-3.3230e-05, -2.4900e-05,  9.0152e-06,  ..., -3.0249e-05,
         -8.2105e-06, -1.8522e-05],
        [-4.3482e-05, -3.2574e-05,  1.1764e-05,  ..., -3.9518e-05,
         -1.0811e-05, -2.4229e-05]], device='cuda:0')
Loss: 1.1224077939987183


Running epoch 0, step 439, batch 439
Sampled inputs[:2]: tensor([[    0,   341,  2802,  ...,  1798,    12,   266],
        [    0,  5301,   792,  ..., 27135, 34090,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8306e-05,  2.0023e-04, -1.0761e-04,  ..., -4.2190e-05,
         -5.8801e-05,  2.1776e-05],
        [-2.1800e-05, -1.6361e-05,  5.8822e-06,  ..., -1.9863e-05,
         -5.4277e-06, -1.2182e-05],
        [-2.4974e-05, -1.8746e-05,  6.7428e-06,  ..., -2.2739e-05,
         -6.1840e-06, -1.3947e-05],
        [-3.7909e-05, -2.8446e-05,  1.0259e-05,  ..., -3.4541e-05,
         -9.3430e-06, -2.1145e-05],
        [-4.9740e-05, -3.7313e-05,  1.3411e-05,  ..., -4.5240e-05,
         -1.2331e-05, -2.7731e-05]], device='cuda:0')
Loss: 1.1466518640518188
Graident accumulation at epoch 0, step 439, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0155, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7131e-05,  4.8105e-05, -9.4809e-05,  ..., -7.8629e-06,
         -5.4836e-05,  2.5379e-05],
        [-2.1525e-05, -1.5860e-05,  5.1324e-06,  ..., -1.9007e-05,
         -4.5803e-06, -1.0126e-05],
        [ 6.4298e-06,  4.3810e-06,  1.6127e-08,  ...,  4.8533e-06,
          2.0716e-07,  2.2517e-07],
        [ 3.1048e-05,  2.0705e-05, -1.0514e-05,  ...,  3.2062e-05,
          8.0480e-06,  2.1394e-05],
        [-4.7615e-05, -3.5030e-05,  1.0958e-05,  ..., -4.1976e-05,
         -1.0083e-05, -2.2703e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6491e-08, 1.4500e-08, 1.7226e-08,  ..., 1.9547e-08, 3.1013e-08,
         6.4682e-09],
        [4.2826e-11, 2.4938e-11, 1.7896e-12,  ..., 2.9969e-11, 1.4954e-12,
         6.8485e-12],
        [6.5168e-10, 3.8756e-10, 2.1552e-11,  ..., 5.0521e-10, 1.4047e-11,
         1.0587e-10],
        [4.3403e-10, 3.6712e-10, 2.8638e-11,  ..., 3.5445e-10, 1.8828e-11,
         1.2822e-10],
        [1.7435e-10, 9.6346e-11, 8.0476e-12,  ..., 1.2174e-10, 4.5419e-12,
         2.8350e-11]], device='cuda:0')
optimizer state dict: 55.0
lr: [1.3033212842861785e-05, 1.3033212842861785e-05]
scheduler_last_epoch: 55


Running epoch 0, step 440, batch 440
Sampled inputs[:2]: tensor([[   0,  266, 1441,  ..., 1817, 1589,  278],
        [   0, 2771, 2070,  ...,  221,  396,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7011e-05, -4.7071e-05, -1.9932e-05,  ...,  2.0076e-05,
         -3.4942e-05, -2.1798e-05],
        [-2.7716e-06, -2.0564e-06,  7.6368e-07,  ..., -2.4587e-06,
         -6.6683e-07, -1.5199e-06],
        [-3.1739e-06, -2.3544e-06,  8.7172e-07,  ..., -2.8014e-06,
         -7.5996e-07, -1.7360e-06],
        [ 8.0741e-05,  5.9285e-05, -2.9130e-05,  ...,  7.9694e-05,
          3.1260e-05,  6.4619e-05],
        [-6.3181e-06, -4.6790e-06,  1.7360e-06,  ..., -5.6028e-06,
         -1.5199e-06, -3.4571e-06]], device='cuda:0')
Loss: 1.1276326179504395


Running epoch 0, step 441, batch 441
Sampled inputs[:2]: tensor([[    0,  5506,   696,  ...,   607, 11129,   276],
        [    0,   342,   266,  ...,  4998,  4756,  5139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3204e-05, -9.2386e-05, -5.2653e-05,  ...,  1.4302e-05,
         -1.1324e-05, -5.9861e-05],
        [-5.5432e-06, -4.1425e-06,  1.5199e-06,  ..., -4.9472e-06,
         -1.3560e-06, -3.0547e-06],
        [-6.3479e-06, -4.7237e-06,  1.7323e-06,  ..., -5.6475e-06,
         -1.5423e-06, -3.4869e-06],
        [ 7.5883e-05,  5.5649e-05, -2.7811e-05,  ...,  7.5313e-05,
          3.0068e-05,  6.1952e-05],
        [-1.2666e-05, -9.4175e-06,  3.4571e-06,  ..., -1.1295e-05,
         -3.0920e-06, -6.9439e-06]], device='cuda:0')
Loss: 1.1440802812576294


Running epoch 0, step 442, batch 442
Sampled inputs[:2]: tensor([[   0,  275, 4452,  ...,   12, 3516, 5227],
        [   0,   12,  287,  ...,  278, 4697,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0875e-05, -9.8076e-05, -8.7486e-05,  ...,  4.2672e-05,
         -3.5544e-05, -3.8428e-05],
        [-8.2999e-06, -6.2138e-06,  2.2948e-06,  ..., -7.4059e-06,
         -2.0303e-06, -4.5672e-06],
        [-9.4920e-06, -7.0781e-06,  2.6152e-06,  ..., -8.4639e-06,
         -2.3097e-06, -5.2080e-06],
        [ 7.1026e-05,  5.2028e-05, -2.6455e-05,  ...,  7.0992e-05,
          2.8891e-05,  5.9299e-05],
        [-1.8924e-05, -1.4096e-05,  5.2154e-06,  ..., -1.6898e-05,
         -4.6194e-06, -1.0371e-05]], device='cuda:0')
Loss: 1.1428090333938599


Running epoch 0, step 443, batch 443
Sampled inputs[:2]: tensor([[   0, 1867,  300,  ...,  259, 3095, 1842],
        [   0, 1416,  367,  ...,  555,  764,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3637e-05, -7.5029e-05, -9.4978e-05,  ...,  6.0590e-05,
         -3.6696e-05, -3.0079e-05],
        [-1.1072e-05, -8.2701e-06,  3.0436e-06,  ..., -9.8497e-06,
         -2.6859e-06, -6.1095e-06],
        [-1.2651e-05, -9.4175e-06,  3.4682e-06,  ..., -1.1250e-05,
         -3.0510e-06, -6.9588e-06],
        [ 6.6168e-05,  4.8407e-05, -2.5136e-05,  ...,  6.6700e-05,
          2.7751e-05,  5.6602e-05],
        [-2.5243e-05, -1.8805e-05,  6.9216e-06,  ..., -2.2501e-05,
         -6.1095e-06, -1.3873e-05]], device='cuda:0')
Loss: 1.1297537088394165


Running epoch 0, step 444, batch 444
Sampled inputs[:2]: tensor([[    0,  2348,   565,  ...,    12,   709,   266],
        [    0,   341, 22766,  ...,   271,   266,  1176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7040e-05, -6.1380e-05, -1.3433e-04,  ...,  4.4332e-05,
         -3.8514e-05, -3.7120e-05],
        [-1.3828e-05, -1.0356e-05,  3.8370e-06,  ..., -1.2383e-05,
         -3.3937e-06, -7.6666e-06],
        [-1.5751e-05, -1.1757e-05,  4.3623e-06,  ..., -1.4096e-05,
         -3.8408e-06, -8.7097e-06],
        [ 6.1400e-05,  4.4816e-05, -2.3758e-05,  ...,  6.2349e-05,
          2.6544e-05,  5.3920e-05],
        [-3.1501e-05, -2.3514e-05,  8.7172e-06,  ..., -2.8223e-05,
         -7.7039e-06, -1.7390e-05]], device='cuda:0')
Loss: 1.1128493547439575


Running epoch 0, step 445, batch 445
Sampled inputs[:2]: tensor([[   0,  894,  496,  ...,  266,  623,  587],
        [   0,  266, 2967,  ...,  287, 4432,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4438e-05, -5.8235e-05, -1.7040e-04,  ..., -2.4969e-05,
         -3.7105e-05, -7.0894e-05],
        [-1.6570e-05, -1.2383e-05,  4.5858e-06,  ..., -1.4827e-05,
         -4.0792e-06, -9.2164e-06],
        [ 8.1438e-05,  4.7384e-05, -2.1089e-05,  ...,  6.4853e-05,
          1.3276e-05,  1.4065e-05],
        [ 5.6572e-05,  4.1240e-05, -2.2447e-05,  ...,  5.8058e-05,
          2.5360e-05,  5.1208e-05],
        [-3.7789e-05, -2.8163e-05,  1.0423e-05,  ..., -3.3826e-05,
         -9.2611e-06, -2.0921e-05]], device='cuda:0')
Loss: 1.092692494392395


Running epoch 0, step 446, batch 446
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,  1049,   292,  ...,   221,   380,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2216e-07, -3.9718e-05, -2.0473e-04,  ..., -2.5493e-05,
         -4.1725e-05, -7.9014e-05],
        [-1.9312e-05, -1.4469e-05,  5.3532e-06,  ..., -1.7300e-05,
         -4.8056e-06, -1.0788e-05],
        [ 7.8323e-05,  4.5015e-05, -2.0217e-05,  ...,  6.2051e-05,
          1.2457e-05,  1.2284e-05],
        [ 5.1803e-05,  3.7619e-05, -2.1106e-05,  ...,  5.3766e-05,
          2.4108e-05,  4.8481e-05],
        [-4.3988e-05, -3.2872e-05,  1.2159e-05,  ..., -3.9399e-05,
         -1.0893e-05, -2.4468e-05]], device='cuda:0')
Loss: 1.1334025859832764


Running epoch 0, step 447, batch 447
Sampled inputs[:2]: tensor([[    0, 13466,    14,  ..., 11227,  1966,  4039],
        [    0,   273,    14,  ...,   271,   266, 25408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4463e-05, -2.8038e-05, -2.0823e-04,  ..., -3.3264e-05,
         -5.0959e-05, -8.1499e-05],
        [-2.2084e-05, -1.6570e-05,  6.1132e-06,  ..., -1.9774e-05,
         -5.5097e-06, -1.2338e-05],
        [ 7.5179e-05,  4.2616e-05, -1.9357e-05,  ...,  5.9235e-05,
          1.1660e-05,  1.0518e-05],
        [ 4.7005e-05,  3.3968e-05, -1.9794e-05,  ...,  4.9474e-05,
          2.2901e-05,  4.5799e-05],
        [-5.0336e-05, -3.7700e-05,  1.3895e-05,  ..., -4.5061e-05,
         -1.2502e-05, -2.7999e-05]], device='cuda:0')
Loss: 1.1267590522766113
Graident accumulation at epoch 0, step 447, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0972e-05,  4.0491e-05, -1.0615e-04,  ..., -1.0403e-05,
         -5.4448e-05,  1.4691e-05],
        [-2.1581e-05, -1.5931e-05,  5.2304e-06,  ..., -1.9084e-05,
         -4.6733e-06, -1.0348e-05],
        [ 1.3305e-05,  8.2044e-06, -1.9212e-06,  ...,  1.0291e-05,
          1.3524e-06,  1.2545e-06],
        [ 3.2644e-05,  2.2032e-05, -1.1442e-05,  ...,  3.3803e-05,
          9.5333e-06,  2.3835e-05],
        [-4.7887e-05, -3.5297e-05,  1.1252e-05,  ..., -4.2284e-05,
         -1.0325e-05, -2.3233e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6445e-08, 1.4486e-08, 1.7252e-08,  ..., 1.9529e-08, 3.0984e-08,
         6.4683e-09],
        [4.3270e-11, 2.5188e-11, 1.8252e-12,  ..., 3.0330e-11, 1.5242e-12,
         6.9939e-12],
        [6.5668e-10, 3.8899e-10, 2.1905e-11,  ..., 5.0821e-10, 1.4169e-11,
         1.0587e-10],
        [4.3580e-10, 3.6791e-10, 2.9001e-11,  ..., 3.5654e-10, 1.9333e-11,
         1.3019e-10],
        [1.7671e-10, 9.7670e-11, 8.2326e-12,  ..., 1.2365e-10, 4.6937e-12,
         2.9106e-11]], device='cuda:0')
optimizer state dict: 56.0
lr: [1.2796724211323173e-05, 1.2796724211323173e-05]
scheduler_last_epoch: 56


Running epoch 0, step 448, batch 448
Sampled inputs[:2]: tensor([[    0,    45,  6556,  ...,  1477,   352,  1611],
        [    0,   401,  3408,  ...,   287, 19892,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3757e-05,  4.8382e-05, -1.1181e-05,  ...,  3.8349e-06,
         -5.4159e-06,  1.3350e-05],
        [-2.7865e-06, -2.0564e-06,  7.8231e-07,  ..., -2.5034e-06,
         -6.8173e-07, -1.6019e-06],
        [-3.1590e-06, -2.3246e-06,  8.8289e-07,  ..., -2.8312e-06,
         -7.6741e-07, -1.8030e-06],
        [-4.9472e-06, -3.6359e-06,  1.3858e-06,  ..., -4.4405e-06,
         -1.1995e-06, -2.8312e-06],
        [-6.3479e-06, -4.6492e-06,  1.7732e-06,  ..., -5.6624e-06,
         -1.5423e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.1321210861206055


Running epoch 0, step 449, batch 449
Sampled inputs[:2]: tensor([[    0, 14867,   278,  ...,   674,   369,  4127],
        [    0,  5646,    12,  ...,  1952,   287,  3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1113e-05,  5.0313e-05, -1.2135e-05,  ...,  4.3253e-05,
          3.8633e-07,  5.6279e-05],
        [-5.5879e-06, -4.1276e-06,  1.5832e-06,  ..., -4.9919e-06,
         -1.3821e-06, -3.1814e-06],
        [-6.3330e-06, -4.6790e-06,  1.7881e-06,  ..., -5.6475e-06,
         -1.5572e-06, -3.5912e-06],
        [-9.8944e-06, -7.2867e-06,  2.7940e-06,  ..., -8.8215e-06,
         -2.4214e-06, -5.6028e-06],
        [-1.2696e-05, -9.3281e-06,  3.5763e-06,  ..., -1.1295e-05,
         -3.1218e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.140899658203125


Running epoch 0, step 450, batch 450
Sampled inputs[:2]: tensor([[    0, 10511,  3887,  ...,  3504,   298,   422],
        [    0,  5332,   391,  ...,   221,   334,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2169e-05,  6.3671e-05, -4.3922e-05,  ..., -1.9553e-06,
          8.6396e-06,  5.5311e-05],
        [-8.3745e-06, -6.1840e-06,  2.3581e-06,  ..., -7.4953e-06,
         -2.1122e-06, -4.7460e-06],
        [-9.4771e-06, -6.9886e-06,  2.6636e-06,  ..., -8.4639e-06,
         -2.3730e-06, -5.3570e-06],
        [-1.4722e-05, -1.0833e-05,  4.1351e-06,  ..., -1.3143e-05,
         -3.6731e-06, -8.2999e-06],
        [-1.9044e-05, -1.3977e-05,  5.3346e-06,  ..., -1.6987e-05,
         -4.7758e-06, -1.0744e-05]], device='cuda:0')
Loss: 1.1261247396469116


Running epoch 0, step 451, batch 451
Sampled inputs[:2]: tensor([[   0,  365, 5392,  ...,   14,  333,  199],
        [   0,  278, 4191,  ...,  381, 3020,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0264e-05,  7.3006e-05, -3.9566e-05,  ...,  1.6305e-05,
          3.0469e-06,  4.7590e-05],
        [-1.1131e-05, -8.1658e-06,  3.1069e-06,  ..., -9.9987e-06,
         -2.7604e-06, -6.3032e-06],
        [-1.2562e-05, -9.2238e-06,  3.5018e-06,  ..., -1.1265e-05,
         -3.0957e-06, -7.1079e-06],
        [-1.9580e-05, -1.4350e-05,  5.4613e-06,  ..., -1.7554e-05,
         -4.8131e-06, -1.1057e-05],
        [-2.5302e-05, -1.8507e-05,  7.0333e-06,  ..., -2.2680e-05,
         -6.2436e-06, -1.4275e-05]], device='cuda:0')
Loss: 1.129254937171936


Running epoch 0, step 452, batch 452
Sampled inputs[:2]: tensor([[   0,  391, 1351,  ...,   13,   40,    9],
        [   0,  300, 5864,  ...,   12, 3667,  796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4344e-05,  5.2258e-05, -4.3544e-05,  ...,  1.9128e-05,
          5.8538e-06,  5.4042e-05],
        [-1.3933e-05, -1.0222e-05,  3.8967e-06,  ..., -1.2472e-05,
         -3.4459e-06, -7.8827e-06],
        [-1.5765e-05, -1.1563e-05,  4.4070e-06,  ..., -1.4096e-05,
         -3.8743e-06, -8.9183e-06],
        [-2.4527e-05, -1.7956e-05,  6.8545e-06,  ..., -2.1905e-05,
         -6.0052e-06, -1.3843e-05],
        [-3.1710e-05, -2.3186e-05,  8.8438e-06,  ..., -2.8342e-05,
         -7.8008e-06, -1.7881e-05]], device='cuda:0')
Loss: 1.1406753063201904


Running epoch 0, step 453, batch 453
Sampled inputs[:2]: tensor([[   0,   14, 4746,  ...,  266, 1119, 1705],
        [   0,  271,  266,  ...,  984,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9411e-05,  7.8516e-05, -2.8407e-05,  ...,  3.1847e-05,
         -6.7592e-08,  6.1547e-05],
        [-1.6719e-05, -1.2264e-05,  4.6939e-06,  ..., -1.4931e-05,
         -4.1351e-06, -9.4399e-06],
        [-1.8939e-05, -1.3888e-05,  5.3123e-06,  ..., -1.6898e-05,
         -4.6566e-06, -1.0692e-05],
        [-2.9445e-05, -2.1577e-05,  8.2627e-06,  ..., -2.6256e-05,
         -7.2196e-06, -1.6600e-05],
        [-3.8087e-05, -2.7865e-05,  1.0662e-05,  ..., -3.3975e-05,
         -9.3728e-06, -2.1428e-05]], device='cuda:0')
Loss: 1.1453295946121216


Running epoch 0, step 454, batch 454
Sampled inputs[:2]: tensor([[    0,   221,   381,  ...,   360,  8978,    14],
        [    0,   634, 10095,  ...,   367, 24607,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4288e-05,  3.6151e-05, -3.4682e-06,  ...,  5.9140e-05,
         -5.1953e-05,  7.2108e-05],
        [-1.9461e-05, -1.4290e-05,  5.4613e-06,  ..., -1.7360e-05,
         -4.8056e-06, -1.0960e-05],
        [-2.2098e-05, -1.6227e-05,  6.1989e-06,  ..., -1.9684e-05,
         -5.4240e-06, -1.2442e-05],
        [-3.4302e-05, -2.5183e-05,  9.6336e-06,  ..., -3.0547e-05,
         -8.3968e-06, -1.9297e-05],
        [-4.4346e-05, -3.2514e-05,  1.2420e-05,  ..., -3.9518e-05,
         -1.0893e-05, -2.4900e-05]], device='cuda:0')
Loss: 1.118865966796875


Running epoch 0, step 455, batch 455
Sampled inputs[:2]: tensor([[    0,  6803,  6298,  ...,   490,  1781,    12],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7035e-05,  2.7733e-05,  2.9827e-05,  ...,  7.3198e-05,
         -6.8437e-05,  7.6056e-05],
        [-2.2188e-05, -1.6332e-05,  6.2250e-06,  ..., -1.9789e-05,
         -5.4464e-06, -1.2480e-05],
        [-2.5213e-05, -1.8552e-05,  7.0669e-06,  ..., -2.2456e-05,
         -6.1467e-06, -1.4164e-05],
        [-3.9160e-05, -2.8804e-05,  1.0997e-05,  ..., -3.4869e-05,
         -9.5293e-06, -2.1994e-05],
        [-5.0634e-05, -3.7193e-05,  1.4171e-05,  ..., -4.5091e-05,
         -1.2361e-05, -2.8387e-05]], device='cuda:0')
Loss: 1.1310524940490723
Graident accumulation at epoch 0, step 455, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3578e-05,  3.9215e-05, -9.2554e-05,  ..., -2.0428e-06,
         -5.5847e-05,  2.0827e-05],
        [-2.1642e-05, -1.5971e-05,  5.3299e-06,  ..., -1.9154e-05,
         -4.7506e-06, -1.0561e-05],
        [ 9.4529e-06,  5.5288e-06, -1.0224e-06,  ...,  7.0167e-06,
          6.0250e-07, -2.8732e-07],
        [ 2.5463e-05,  1.6948e-05, -9.1977e-06,  ...,  2.6936e-05,
          7.6271e-06,  1.9252e-05],
        [-4.8162e-05, -3.5487e-05,  1.1544e-05,  ..., -4.2565e-05,
         -1.0529e-05, -2.3748e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6402e-08, 1.4472e-08, 1.7236e-08,  ..., 1.9515e-08, 3.0958e-08,
         6.4677e-09],
        [4.3720e-11, 2.5430e-11, 1.8621e-12,  ..., 3.0692e-11, 1.5524e-12,
         7.1427e-12],
        [6.5666e-10, 3.8894e-10, 2.1933e-11,  ..., 5.0821e-10, 1.4192e-11,
         1.0597e-10],
        [4.3690e-10, 3.6837e-10, 2.9093e-11,  ..., 3.5740e-10, 1.9405e-11,
         1.3054e-10],
        [1.7910e-10, 9.8956e-11, 8.4252e-12,  ..., 1.2556e-10, 4.8418e-12,
         2.9883e-11]], device='cuda:0')
optimizer state dict: 57.0
lr: [1.255852618959973e-05, 1.255852618959973e-05]
scheduler_last_epoch: 57


Running epoch 0, step 456, batch 456
Sampled inputs[:2]: tensor([[    0,   266,  1890,  ...,   287, 38242,    13],
        [    0,   607,   259,  ...,   995,    13,  6507]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1909e-06,  8.2384e-06,  1.9571e-05,  ..., -2.8301e-05,
          9.1767e-06,  3.6307e-05],
        [-2.7865e-06, -1.9968e-06,  7.8604e-07,  ..., -2.4438e-06,
         -6.9290e-07, -1.5795e-06],
        [-3.1739e-06, -2.2650e-06,  8.9779e-07,  ..., -2.7865e-06,
         -7.8604e-07, -1.7956e-06],
        [-5.0068e-06, -3.5763e-06,  1.4156e-06,  ..., -4.3809e-06,
         -1.2293e-06, -2.8312e-06],
        [-6.4075e-06, -4.5896e-06,  1.8105e-06,  ..., -5.6326e-06,
         -1.5870e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.131845235824585


Running epoch 0, step 457, batch 457
Sampled inputs[:2]: tensor([[    0,  1477,    12,  ..., 31038,   408,   298],
        [    0,   367,  3704,  ...,  1746,    14,   759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6754e-05,  1.5781e-05,  9.3539e-06,  ..., -2.4005e-05,
         -1.4988e-06,  3.8638e-05],
        [-5.5730e-06, -4.0382e-06,  1.5646e-06,  ..., -4.9174e-06,
         -1.3821e-06, -3.1739e-06],
        [-6.3181e-06, -4.5747e-06,  1.7770e-06,  ..., -5.5879e-06,
         -1.5609e-06, -3.5986e-06],
        [-9.9838e-06, -7.2122e-06,  2.8089e-06,  ..., -8.7917e-06,
         -2.4438e-06, -5.6773e-06],
        [-1.2755e-05, -9.2089e-06,  3.5763e-06,  ..., -1.1235e-05,
         -3.1441e-06, -7.2420e-06]], device='cuda:0')
Loss: 1.1282955408096313


Running epoch 0, step 458, batch 458
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  654,  271, 4483],
        [   0, 6203,  352,  ...,  266, 3437,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9123e-05,  2.3787e-05, -2.5033e-05,  ..., -2.6073e-05,
         -1.3274e-06,  3.1781e-05],
        [-8.3745e-06, -6.0350e-06,  2.3805e-06,  ..., -7.3761e-06,
         -2.0899e-06, -4.7684e-06],
        [-9.4622e-06, -6.8098e-06,  2.6897e-06,  ..., -8.3447e-06,
         -2.3469e-06, -5.3868e-06],
        [-1.4991e-05, -1.0774e-05,  4.2692e-06,  ..., -1.3202e-05,
         -3.6955e-06, -8.5235e-06],
        [-1.9103e-05, -1.3739e-05,  5.4166e-06,  ..., -1.6809e-05,
         -4.7311e-06, -1.0848e-05]], device='cuda:0')
Loss: 1.1196751594543457


Running epoch 0, step 459, batch 459
Sampled inputs[:2]: tensor([[   0, 8290,  391,  ...,  298, 1253,    7],
        [   0, 1760,  446,  ...,  329, 1405,  422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3428e-05,  3.3856e-05, -4.9449e-05,  ..., -3.6549e-05,
          1.1189e-05, -6.2796e-06],
        [-1.1161e-05, -8.0615e-06,  3.1888e-06,  ..., -9.8646e-06,
         -2.8051e-06, -6.4000e-06],
        [-1.2577e-05, -9.0748e-06,  3.5949e-06,  ..., -1.1131e-05,
         -3.1404e-06, -7.2122e-06],
        [-1.9848e-05, -1.4305e-05,  5.6773e-06,  ..., -1.7554e-05,
         -4.9248e-06, -1.1370e-05],
        [-2.5392e-05, -1.8328e-05,  7.2420e-06,  ..., -2.2441e-05,
         -6.3404e-06, -1.4544e-05]], device='cuda:0')
Loss: 1.1317880153656006


Running epoch 0, step 460, batch 460
Sampled inputs[:2]: tensor([[    0,  2853, 21042,  ...,  4120,   607, 11176],
        [    0,    12,  4567,  ...,  4154,  1799, 11883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2361e-05,  1.4285e-05, -4.6775e-05,  ..., -6.2083e-05,
          3.0654e-05, -1.5260e-05],
        [-1.3947e-05, -1.0043e-05,  3.9712e-06,  ..., -1.2308e-05,
         -3.4682e-06, -7.9647e-06],
        [-1.5736e-05, -1.1325e-05,  4.4815e-06,  ..., -1.3903e-05,
         -3.8892e-06, -8.9854e-06],
        [-2.4825e-05, -1.7866e-05,  7.0781e-06,  ..., -2.1935e-05,
         -6.1020e-06, -1.4171e-05],
        [-3.1799e-05, -2.2888e-05,  9.0301e-06,  ..., -2.8044e-05,
         -7.8604e-06, -1.8135e-05]], device='cuda:0')
Loss: 1.1368976831436157


Running epoch 0, step 461, batch 461
Sampled inputs[:2]: tensor([[   0,   13, 2549,  ...,  221,  382,  298],
        [   0, 4599, 9005,  ...,  809,   13, 1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3556e-05,  4.4636e-05, -5.5052e-05,  ..., -8.5264e-05,
          3.9535e-05, -1.4535e-05],
        [-1.6749e-05, -1.2040e-05,  4.7646e-06,  ..., -1.4782e-05,
         -4.1388e-06, -9.5591e-06],
        [-1.8880e-05, -1.3560e-05,  5.3719e-06,  ..., -1.6674e-05,
         -4.6380e-06, -1.0774e-05],
        [-2.9802e-05, -2.1398e-05,  8.4862e-06,  ..., -2.6315e-05,
         -7.2792e-06, -1.6987e-05],
        [-3.8147e-05, -2.7418e-05,  1.0826e-05,  ..., -3.3647e-05,
         -9.3728e-06, -2.1741e-05]], device='cuda:0')
Loss: 1.1226683855056763


Running epoch 0, step 462, batch 462
Sampled inputs[:2]: tensor([[   0,  266, 1790,  ...,  292,   78,  527],
        [   0, 9818,  347,  ...,  413, 7359,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6730e-05,  3.2317e-05, -7.2739e-05,  ..., -1.0210e-04,
          5.3099e-05, -3.8231e-05],
        [-1.9565e-05, -1.4052e-05,  5.5507e-06,  ..., -1.7256e-05,
         -4.8317e-06, -1.1176e-05],
        [-2.2054e-05, -1.5840e-05,  6.2622e-06,  ..., -1.9476e-05,
         -5.4166e-06, -1.2599e-05],
        [-3.4750e-05, -2.4945e-05,  9.8720e-06,  ..., -3.0696e-05,
         -8.4862e-06, -1.9833e-05],
        [-4.4495e-05, -3.1948e-05,  1.2591e-05,  ..., -3.9220e-05,
         -1.0930e-05, -2.5377e-05]], device='cuda:0')
Loss: 1.137628436088562


Running epoch 0, step 463, batch 463
Sampled inputs[:2]: tensor([[    0, 47354,  5923,  ...,   266, 14679,  8137],
        [    0,    13, 20773,  ..., 22463,  2587,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5964e-05,  4.6958e-05, -6.7786e-05,  ..., -9.1951e-05,
          3.4852e-05, -1.9456e-05],
        [-2.2352e-05, -1.6063e-05,  6.3367e-06,  ..., -1.9744e-05,
         -5.4985e-06, -1.2770e-05],
        [-2.5213e-05, -1.8120e-05,  7.1488e-06,  ..., -2.2292e-05,
         -6.1654e-06, -1.4402e-05],
        [-3.9697e-05, -2.8521e-05,  1.1273e-05,  ..., -3.5137e-05,
         -9.6634e-06, -2.2665e-05],
        [-5.0843e-05, -3.6538e-05,  1.4372e-05,  ..., -4.4882e-05,
         -1.2442e-05, -2.8998e-05]], device='cuda:0')
Loss: 1.125495433807373
Graident accumulation at epoch 0, step 463, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0154, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2817e-05,  3.9989e-05, -9.0077e-05,  ..., -1.1034e-05,
         -4.6777e-05,  1.6799e-05],
        [-2.1713e-05, -1.5980e-05,  5.4306e-06,  ..., -1.9213e-05,
         -4.8254e-06, -1.0782e-05],
        [ 5.9864e-06,  3.1639e-06, -2.0524e-07,  ...,  4.0858e-06,
         -7.4288e-08, -1.6988e-06],
        [ 1.8947e-05,  1.2401e-05, -7.1507e-06,  ...,  2.0729e-05,
          5.8980e-06,  1.5060e-05],
        [-4.8430e-05, -3.5592e-05,  1.1827e-05,  ..., -4.2797e-05,
         -1.0720e-05, -2.4273e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6356e-08, 1.4460e-08, 1.7223e-08,  ..., 1.9503e-08, 3.0928e-08,
         6.4616e-09],
        [4.4175e-11, 2.5662e-11, 1.9004e-12,  ..., 3.1051e-11, 1.5811e-12,
         7.2986e-12],
        [6.5664e-10, 3.8888e-10, 2.1963e-11,  ..., 5.0820e-10, 1.4216e-11,
         1.0607e-10],
        [4.3804e-10, 3.6882e-10, 2.9191e-11,  ..., 3.5828e-10, 1.9479e-11,
         1.3092e-10],
        [1.8150e-10, 1.0019e-10, 8.6233e-12,  ..., 1.2745e-10, 4.9918e-12,
         3.0694e-11]], device='cuda:0')
optimizer state dict: 58.0
lr: [1.2318764367077325e-05, 1.2318764367077325e-05]
scheduler_last_epoch: 58


Running epoch 0, step 464, batch 464
Sampled inputs[:2]: tensor([[   0, 3059, 2013,  ...,  278, 1997,   14],
        [   0,  344, 2574,  ..., 2558, 2663,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7952e-05,  3.8986e-05, -5.9450e-05,  ..., -2.2574e-06,
         -2.2644e-05, -4.1212e-05],
        [-2.7865e-06, -1.9819e-06,  7.8604e-07,  ..., -2.4289e-06,
         -6.8545e-07, -1.6391e-06],
        [-3.1590e-06, -2.2501e-06,  8.9034e-07,  ..., -2.7418e-06,
         -7.7486e-07, -1.8552e-06],
        [-4.9472e-06, -3.5316e-06,  1.4007e-06,  ..., -4.2915e-06,
         -1.2070e-06, -2.9057e-06],
        [-6.3777e-06, -4.5300e-06,  1.7956e-06,  ..., -5.5432e-06,
         -1.5646e-06, -3.7402e-06]], device='cuda:0')
Loss: 1.1315488815307617


Running epoch 0, step 465, batch 465
Sampled inputs[:2]: tensor([[   0,   13, 3105,  ...,  496,   14,  879],
        [   0,  221,  474,  ...,  287,  271, 2540]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9287e-05,  7.4526e-05, -1.0876e-04,  ...,  6.4118e-05,
         -8.9599e-05, -5.4839e-05],
        [-5.5879e-06, -3.9935e-06,  1.5944e-06,  ..., -4.8727e-06,
         -1.3933e-06, -3.2783e-06],
        [-6.3181e-06, -4.5151e-06,  1.7993e-06,  ..., -5.4836e-06,
         -1.5609e-06, -3.6955e-06],
        [-9.9540e-06, -7.1228e-06,  2.8461e-06,  ..., -8.6427e-06,
         -2.4587e-06, -5.8264e-06],
        [-1.2726e-05, -9.0599e-06,  3.6135e-06,  ..., -1.1057e-05,
         -3.1516e-06, -7.4357e-06]], device='cuda:0')
Loss: 1.1310409307479858


Running epoch 0, step 466, batch 466
Sampled inputs[:2]: tensor([[    0,   437,   266,  ...,   266, 16084,  1781],
        [    0,   281,   221,  ...,  2236, 15064,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8971e-05,  8.7003e-05, -1.2475e-04,  ...,  5.7042e-05,
         -7.8065e-05, -1.2587e-04],
        [-8.3745e-06, -5.9903e-06,  2.3916e-06,  ..., -7.3016e-06,
         -2.1197e-06, -4.9397e-06],
        [-9.4324e-06, -6.7502e-06,  2.6934e-06,  ..., -8.2105e-06,
         -2.3656e-06, -5.5581e-06],
        [-1.4931e-05, -1.0699e-05,  4.2766e-06,  ..., -1.2994e-05,
         -3.7402e-06, -8.8066e-06],
        [-1.9044e-05, -1.3590e-05,  5.4166e-06,  ..., -1.6570e-05,
         -4.7833e-06, -1.1206e-05]], device='cuda:0')
Loss: 1.1251215934753418


Running epoch 0, step 467, batch 467
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0,   729,  3430,  ...,  9715,    13, 42383]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1552e-05,  8.8566e-05, -1.3013e-04,  ...,  1.1561e-04,
         -1.1203e-04, -1.0479e-04],
        [-1.1161e-05, -8.0168e-06,  3.1851e-06,  ..., -9.7305e-06,
         -2.8238e-06, -6.5640e-06],
        [-1.2591e-05, -9.0599e-06,  3.5949e-06,  ..., -1.0952e-05,
         -3.1590e-06, -7.3984e-06],
        [-1.9968e-05, -1.4365e-05,  5.7146e-06,  ..., -1.7375e-05,
         -4.9993e-06, -1.1742e-05],
        [-2.5421e-05, -1.8239e-05,  7.2271e-06,  ..., -2.2113e-05,
         -6.3851e-06, -1.4916e-05]], device='cuda:0')
Loss: 1.1311423778533936


Running epoch 0, step 468, batch 468
Sampled inputs[:2]: tensor([[   0,   14,  298,  ...,  333,  199,  769],
        [   0,  278, 1059,  ...,  300, 1877,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0851e-05,  1.6113e-04, -2.6097e-04,  ...,  1.2481e-04,
         -8.4473e-05, -6.6703e-05],
        [-1.4007e-05, -1.0014e-05,  3.9786e-06,  ..., -1.2234e-05,
         -3.5763e-06, -8.2552e-06],
        [-1.5706e-05, -1.1250e-05,  4.4666e-06,  ..., -1.3709e-05,
         -3.9749e-06, -9.2462e-06],
        [-2.4945e-05, -1.7852e-05,  7.1153e-06,  ..., -2.1785e-05,
         -6.3032e-06, -1.4693e-05],
        [-3.1710e-05, -2.2650e-05,  8.9854e-06,  ..., -2.7657e-05,
         -8.0317e-06, -1.8641e-05]], device='cuda:0')
Loss: 1.1140007972717285


Running epoch 0, step 469, batch 469
Sampled inputs[:2]: tensor([[    0,   677,  6499,  ...,  2738,    12,   287],
        [    0,   376,   283,  ..., 29188,   292,  7627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0362e-04,  1.9446e-04, -2.5803e-04,  ...,  1.1264e-04,
         -9.6890e-05, -7.9666e-05],
        [-1.6868e-05, -1.2010e-05,  4.7870e-06,  ..., -1.4737e-05,
         -4.2990e-06, -9.9018e-06],
        [-1.8910e-05, -1.3500e-05,  5.3719e-06,  ..., -1.6510e-05,
         -4.7795e-06, -1.1094e-05],
        [-3.0011e-05, -2.1413e-05,  8.5533e-06,  ..., -2.6226e-05,
         -7.5698e-06, -1.7613e-05],
        [-3.8207e-05, -2.7210e-05,  1.0818e-05,  ..., -3.3349e-05,
         -9.6634e-06, -2.2382e-05]], device='cuda:0')
Loss: 1.1392220258712769


Running epoch 0, step 470, batch 470
Sampled inputs[:2]: tensor([[    0,   287,   266,  ...,   998,   342, 17709],
        [    0,  8158,  1416,  ...,   413,    29,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4618e-05,  2.0598e-04, -2.9331e-04,  ...,  1.1780e-04,
         -8.3260e-05, -1.0538e-04],
        [-1.9684e-05, -1.4007e-05,  5.5730e-06,  ..., -1.7196e-05,
         -4.9733e-06, -1.1489e-05],
        [-2.2069e-05, -1.5736e-05,  6.2510e-06,  ..., -1.9267e-05,
         -5.5321e-06, -1.2867e-05],
        [-3.5048e-05, -2.4974e-05,  9.9540e-06,  ..., -3.0607e-05,
         -8.7619e-06, -2.0429e-05],
        [-4.4614e-05, -3.1739e-05,  1.2599e-05,  ..., -3.8922e-05,
         -1.1191e-05, -2.5973e-05]], device='cuda:0')
Loss: 1.1381433010101318


Running epoch 0, step 471, batch 471
Sampled inputs[:2]: tensor([[    0, 13312,  9048,  ..., 33470,  8672,  3524],
        [    0,   409,   394,  ...,   475,  5458,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1205e-04,  2.6076e-04, -2.7306e-04,  ...,  1.2954e-04,
         -7.1883e-05, -9.5708e-05],
        [-2.2501e-05, -1.5959e-05,  6.3553e-06,  ..., -1.9640e-05,
         -5.6550e-06, -1.3150e-05],
        [-2.5213e-05, -1.7911e-05,  7.1228e-06,  ..., -2.1994e-05,
         -6.2883e-06, -1.4722e-05],
        [-3.9995e-05, -2.8387e-05,  1.1332e-05,  ..., -3.4899e-05,
         -9.9465e-06, -2.3350e-05],
        [-5.1051e-05, -3.6210e-05,  1.4387e-05,  ..., -4.4525e-05,
         -1.2740e-05, -2.9773e-05]], device='cuda:0')
Loss: 1.1518454551696777
Graident accumulation at epoch 0, step 471, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0034,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0154, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0740e-05,  6.2067e-05, -1.0838e-04,  ...,  3.0242e-06,
         -4.9288e-05,  5.5483e-06],
        [-2.1792e-05, -1.5978e-05,  5.5231e-06,  ..., -1.9256e-05,
         -4.9083e-06, -1.1019e-05],
        [ 2.8665e-06,  1.0564e-06,  5.2756e-07,  ...,  1.4778e-06,
         -6.9569e-07, -3.0011e-06],
        [ 1.3053e-05,  8.3224e-06, -5.3024e-06,  ...,  1.5166e-05,
          4.3136e-06,  1.1219e-05],
        [-4.8692e-05, -3.5654e-05,  1.2083e-05,  ..., -4.2969e-05,
         -1.0922e-05, -2.4823e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6322e-08, 1.4513e-08, 1.7280e-08,  ..., 1.9501e-08, 3.0902e-08,
         6.4643e-09],
        [4.4638e-11, 2.5891e-11, 1.9389e-12,  ..., 3.1406e-11, 1.6115e-12,
         7.4642e-12],
        [6.5662e-10, 3.8882e-10, 2.1991e-11,  ..., 5.0817e-10, 1.4242e-11,
         1.0618e-10],
        [4.3920e-10, 3.6925e-10, 2.9290e-11,  ..., 3.5914e-10, 1.9558e-11,
         1.3134e-10],
        [1.8393e-10, 1.0140e-10, 8.8217e-12,  ..., 1.2930e-10, 5.1491e-12,
         3.1549e-11]], device='cuda:0')
optimizer state dict: 59.0
lr: [1.2077585288954968e-05, 1.2077585288954968e-05]
scheduler_last_epoch: 59


Running epoch 0, step 472, batch 472
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 35247,  2547,   278],
        [    0,   199,  2834,  ...,  1236,   768,  4316]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1831e-05, -8.2864e-06, -9.3125e-05,  ..., -1.9856e-05,
         -3.9828e-05, -2.7717e-05],
        [-2.8014e-06, -1.9819e-06,  8.4937e-07,  ..., -2.4736e-06,
         -7.7859e-07, -1.7136e-06],
        [-3.1441e-06, -2.2203e-06,  9.5367e-07,  ..., -2.7716e-06,
         -8.6427e-07, -1.9222e-06],
        [-5.0068e-06, -3.5167e-06,  1.5199e-06,  ..., -4.4107e-06,
         -1.3635e-06, -3.0398e-06],
        [-6.3479e-06, -4.4703e-06,  1.9222e-06,  ..., -5.6028e-06,
         -1.7434e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.1242625713348389


Running epoch 0, step 473, batch 473
Sampled inputs[:2]: tensor([[    0,   677, 35427,  ..., 30465,  2783,     9],
        [    0,   287,  1410,  ...,  1255,  1699,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6195e-05, -3.6099e-05, -1.1264e-04,  ..., -1.7931e-05,
         -6.4614e-05, -5.3266e-05],
        [-5.6177e-06, -3.9339e-06,  1.6727e-06,  ..., -4.8876e-06,
         -1.4566e-06, -3.3602e-06],
        [-6.3330e-06, -4.4256e-06,  1.8850e-06,  ..., -5.4985e-06,
         -1.6205e-06, -3.7774e-06],
        [-1.0133e-05, -7.0632e-06,  3.0175e-06,  ..., -8.7917e-06,
         -2.5779e-06, -6.0201e-06],
        [-1.2845e-05, -8.9407e-06,  3.8147e-06,  ..., -1.1146e-05,
         -3.2857e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.1240510940551758


Running epoch 0, step 474, batch 474
Sampled inputs[:2]: tensor([[   0,  367, 2063,  ..., 3022,  221,  733],
        [   0, 1241, 2098,  ..., 1862,  631,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4613e-05, -8.1635e-05, -1.1748e-04,  ...,  1.7162e-05,
         -4.6864e-05, -5.6667e-05],
        [-8.3894e-06, -5.8860e-06,  2.5034e-06,  ..., -7.2569e-06,
         -2.1160e-06, -4.9919e-06],
        [-9.4920e-06, -6.6459e-06,  2.8312e-06,  ..., -8.1956e-06,
         -2.3618e-06, -5.6326e-06],
        [-1.5169e-05, -1.0610e-05,  4.5300e-06,  ..., -1.3083e-05,
         -3.7551e-06, -8.9705e-06],
        [-1.9222e-05, -1.3411e-05,  5.7220e-06,  ..., -1.6600e-05,
         -4.7833e-06, -1.1370e-05]], device='cuda:0')
Loss: 1.1203296184539795


Running epoch 0, step 475, batch 475
Sampled inputs[:2]: tensor([[    0,    25,     5,  ...,  3935,    14,    16],
        [    0,  3217, 16714,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4240e-05, -8.3021e-05, -1.3675e-04,  ...,  7.5871e-05,
         -9.9211e-05, -2.5099e-05],
        [-1.1221e-05, -7.8231e-06,  3.3379e-06,  ..., -9.6709e-06,
         -2.8163e-06, -6.6459e-06],
        [-1.2681e-05, -8.8215e-06,  3.7737e-06,  ..., -1.0908e-05,
         -3.1441e-06, -7.5027e-06],
        [-2.0236e-05, -1.4082e-05,  6.0350e-06,  ..., -1.7405e-05,
         -4.9993e-06, -1.1951e-05],
        [-2.5690e-05, -1.7852e-05,  7.6443e-06,  ..., -2.2113e-05,
         -6.3777e-06, -1.5154e-05]], device='cuda:0')
Loss: 1.1378836631774902


Running epoch 0, step 476, batch 476
Sampled inputs[:2]: tensor([[   0,   15, 2537,  ...,   14, 3544,  417],
        [   0,  623,   12,  ..., 4792, 6572,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6297e-05, -9.9755e-05, -1.2106e-04,  ...,  4.4450e-05,
         -1.2190e-04, -3.7002e-05],
        [-1.3977e-05, -9.7752e-06,  4.1351e-06,  ..., -1.2040e-05,
         -3.4720e-06, -8.2478e-06],
        [-1.5825e-05, -1.1057e-05,  4.6827e-06,  ..., -1.3620e-05,
         -3.8855e-06, -9.3281e-06],
        [-2.5272e-05, -1.7658e-05,  7.4878e-06,  ..., -2.1756e-05,
         -6.1765e-06, -1.4871e-05],
        [-3.2067e-05, -2.2382e-05,  9.4846e-06,  ..., -2.7597e-05,
         -7.8827e-06, -1.8865e-05]], device='cuda:0')
Loss: 1.1161854267120361


Running epoch 0, step 477, batch 477
Sampled inputs[:2]: tensor([[    0,  2386,  4012,  ...,   300, 15480,  1036],
        [    0,    12,   266,  ...,  1125,   609,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8739e-05, -1.2797e-04, -1.1927e-04,  ...,  6.2819e-05,
         -1.4491e-04, -7.6162e-05],
        [-1.6779e-05, -1.1742e-05,  4.9621e-06,  ..., -1.4439e-05,
         -4.1761e-06, -9.8646e-06],
        [-1.8984e-05, -1.3277e-05,  5.6177e-06,  ..., -1.6332e-05,
         -4.6715e-06, -1.1154e-05],
        [-3.0309e-05, -2.1189e-05,  8.9779e-06,  ..., -2.6077e-05,
         -7.4208e-06, -1.7762e-05],
        [-3.8445e-05, -2.6852e-05,  1.1362e-05,  ..., -3.3051e-05,
         -9.4697e-06, -2.2531e-05]], device='cuda:0')
Loss: 1.1128287315368652


Running epoch 0, step 478, batch 478
Sampled inputs[:2]: tensor([[    0,  1477,   591,  ...,  4111, 18012, 11991],
        [    0,   756,   943,  ...,  4016,    12,   627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2382e-04, -1.4099e-04, -9.4509e-05,  ...,  8.0560e-05,
         -1.4871e-04, -7.8689e-05],
        [-1.9580e-05, -1.3709e-05,  5.7705e-06,  ..., -1.6853e-05,
         -4.8913e-06, -1.1526e-05],
        [-2.2128e-05, -1.5482e-05,  6.5267e-06,  ..., -1.9044e-05,
         -5.4687e-06, -1.3016e-05],
        [-3.5346e-05, -2.4721e-05,  1.0431e-05,  ..., -3.0398e-05,
         -8.6948e-06, -2.0728e-05],
        [-4.4882e-05, -3.1382e-05,  1.3225e-05,  ..., -3.8594e-05,
         -1.1109e-05, -2.6345e-05]], device='cuda:0')
Loss: 1.1338187456130981


Running epoch 0, step 479, batch 479
Sampled inputs[:2]: tensor([[    0,    33,    12,  ...,  1110,   467, 17467],
        [    0,   277,   279,  ...,    12,   287,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0275e-04, -2.0861e-04, -1.2393e-04,  ...,  1.1909e-04,
         -1.6072e-04, -1.0532e-04],
        [-2.2352e-05, -1.5661e-05,  6.5602e-06,  ..., -1.9267e-05,
         -5.5842e-06, -1.3150e-05],
        [-2.5287e-05, -1.7717e-05,  7.4320e-06,  ..., -2.1800e-05,
         -6.2510e-06, -1.4871e-05],
        [-4.0352e-05, -2.8253e-05,  1.1869e-05,  ..., -3.4779e-05,
         -9.9316e-06, -2.3663e-05],
        [-5.1260e-05, -3.5882e-05,  1.5043e-05,  ..., -4.4137e-05,
         -1.2688e-05, -3.0071e-05]], device='cuda:0')
Loss: 1.1031140089035034
Graident accumulation at epoch 0, step 479, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0154, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6942e-05,  3.4999e-05, -1.0993e-04,  ...,  1.4631e-05,
         -6.0431e-05, -5.5386e-06],
        [-2.1848e-05, -1.5946e-05,  5.6268e-06,  ..., -1.9257e-05,
         -4.9759e-06, -1.1232e-05],
        [ 5.1089e-08, -8.2097e-07,  1.2180e-06,  ..., -8.5000e-07,
         -1.2512e-06, -4.1882e-06],
        [ 7.7126e-06,  4.6649e-06, -3.5853e-06,  ...,  1.0172e-05,
          2.8890e-06,  7.7309e-06],
        [-4.8949e-05, -3.5676e-05,  1.2379e-05,  ..., -4.3086e-05,
         -1.1099e-05, -2.5348e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6286e-08, 1.4543e-08, 1.7278e-08,  ..., 1.9495e-08, 3.0897e-08,
         6.4689e-09],
        [4.5092e-11, 2.6111e-11, 1.9800e-12,  ..., 3.1745e-11, 1.6410e-12,
         7.6297e-12],
        [6.5660e-10, 3.8874e-10, 2.2025e-11,  ..., 5.0814e-10, 1.4266e-11,
         1.0629e-10],
        [4.4039e-10, 3.6968e-10, 2.9402e-11,  ..., 3.5999e-10, 1.9637e-11,
         1.3177e-10],
        [1.8637e-10, 1.0259e-10, 9.0392e-12,  ..., 1.3112e-10, 5.3049e-12,
         3.2422e-11]], device='cuda:0')
optimizer state dict: 60.0
lr: [1.1835136366674677e-05, 1.1835136366674677e-05]
scheduler_last_epoch: 60
Epoch 0 | Batch 479/1048 | Training PPL: 6801.592610905623 | time 48.61168670654297
Saving checkpoint at epoch 0, step 479, batch 479
Epoch 0 | Validation PPL: 9.463093424998604 | Learning rate: 1.1835136366674677e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_479, AFTER epoch 0, step 479


Running epoch 0, step 480, batch 480
Sampled inputs[:2]: tensor([[   0, 5221, 7166,  ..., 4309,  342,  996],
        [   0, 1615,  287,  ...,  259,  623,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0064e-05, -2.2875e-05, -5.8087e-05,  ...,  1.0040e-05,
          1.8251e-05,  1.3538e-05],
        [-2.8610e-06, -1.9372e-06,  8.4564e-07,  ..., -2.4140e-06,
         -7.3761e-07, -1.6913e-06],
        [-3.2187e-06, -2.1756e-06,  9.4995e-07,  ..., -2.7120e-06,
         -8.1956e-07, -1.8924e-06],
        [-5.0962e-06, -3.4273e-06,  1.5050e-06,  ..., -4.2915e-06,
         -1.2890e-06, -2.9802e-06],
        [-6.5565e-06, -4.4107e-06,  1.9372e-06,  ..., -5.5432e-06,
         -1.6764e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.1267744302749634


Running epoch 0, step 481, batch 481
Sampled inputs[:2]: tensor([[   0, 1552,  271,  ...,   13,  287,  995],
        [   0,   17,   14,  ...,  650, 1711,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1954e-05, -8.0002e-06, -4.4457e-05,  ...,  3.7623e-05,
          2.5593e-05,  1.7089e-05],
        [-5.7071e-06, -3.8892e-06,  1.6689e-06,  ..., -4.8280e-06,
         -1.4417e-06, -3.3900e-06],
        [-6.4075e-06, -4.3660e-06,  1.8701e-06,  ..., -5.4091e-06,
         -1.5981e-06, -3.7923e-06],
        [-1.0163e-05, -6.8992e-06,  2.9653e-06,  ..., -8.5831e-06,
         -2.5183e-06, -5.9903e-06],
        [-1.3083e-05, -8.8811e-06,  3.8147e-06,  ..., -1.1057e-05,
         -3.2708e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.1493760347366333


Running epoch 0, step 482, batch 482
Sampled inputs[:2]: tensor([[    0,   365,  1462,  ...,   518,  6104,   278],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4287e-05, -3.6034e-05, -9.9990e-05,  ...,  5.2429e-05,
          5.7195e-05,  2.1592e-05],
        [-8.5384e-06, -5.8562e-06,  2.4848e-06,  ..., -7.2271e-06,
         -2.1346e-06, -5.0738e-06],
        [-9.5963e-06, -6.5714e-06,  2.7865e-06,  ..., -8.1062e-06,
         -2.3693e-06, -5.6848e-06],
        [-1.5199e-05, -1.0386e-05,  4.4182e-06,  ..., -1.2815e-05,
         -3.7327e-06, -8.9705e-06],
        [-1.9610e-05, -1.3411e-05,  5.6922e-06,  ..., -1.6570e-05,
         -4.8578e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.133710503578186


Running epoch 0, step 483, batch 483
Sampled inputs[:2]: tensor([[    0,    13,  1311,  ...,   271,   795,   957],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9394e-05, -4.9928e-05, -1.3570e-04,  ...,  3.8240e-05,
          6.8035e-05,  5.5906e-05],
        [-1.1340e-05, -7.7635e-06,  3.3043e-06,  ..., -9.6262e-06,
         -2.8349e-06, -6.7577e-06],
        [ 1.8068e-04,  1.1060e-04, -6.0079e-05,  ...,  1.3697e-04,
          3.4830e-05,  1.0267e-04],
        [-2.0206e-05, -1.3798e-05,  5.8785e-06,  ..., -1.7107e-05,
         -4.9546e-06, -1.1966e-05],
        [-2.6047e-05, -1.7822e-05,  7.5772e-06,  ..., -2.2084e-05,
         -6.4448e-06, -1.5467e-05]], device='cuda:0')
Loss: 1.1304669380187988


Running epoch 0, step 484, batch 484
Sampled inputs[:2]: tensor([[   0,   12, 9248,  ..., 2673, 4239,  292],
        [   0, 4110,  271,  ...,  944,  278, 3230]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1997e-05, -4.8070e-05, -1.5548e-04,  ...,  1.1412e-04,
          8.5378e-05,  6.2648e-05],
        [-1.4171e-05, -9.7007e-06,  4.1462e-06,  ..., -1.2055e-05,
         -3.5986e-06, -8.4490e-06],
        [ 7.1378e-04,  4.8965e-04, -1.9886e-04,  ...,  6.1573e-04,
          1.9782e-04,  4.6194e-04],
        [-2.5213e-05, -1.7226e-05,  7.3686e-06,  ..., -2.1398e-05,
         -6.2957e-06, -1.4961e-05],
        [-3.2544e-05, -2.2262e-05,  9.4995e-06,  ..., -2.7657e-05,
         -8.1956e-06, -1.9342e-05]], device='cuda:0')
Loss: 1.1611980199813843


Running epoch 0, step 485, batch 485
Sampled inputs[:2]: tensor([[    0,   266,  6449,  ...,   474,   221,   474],
        [    0,    14,  3228,  ..., 13747,   287, 20295]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7663e-05, -8.7428e-05, -1.3975e-04,  ...,  1.2658e-04,
          1.1351e-04,  5.6930e-05],
        [-1.6987e-05, -1.1623e-05,  4.9882e-06,  ..., -1.4454e-05,
         -4.3213e-06, -1.0118e-05],
        [ 1.0098e-03,  6.7943e-04, -2.8991e-04,  ...,  8.6688e-04,
          2.7837e-04,  6.0091e-04],
        [-3.0249e-05, -2.0653e-05,  8.8736e-06,  ..., -2.5690e-05,
         -7.5772e-06, -1.7941e-05],
        [-3.9071e-05, -2.6703e-05,  1.1437e-05,  ..., -3.3170e-05,
         -9.8571e-06, -2.3186e-05]], device='cuda:0')
Loss: 1.127948522567749


Running epoch 0, step 486, batch 486
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2840e-04, -9.4248e-05, -1.5604e-04,  ...,  1.4243e-04,
          9.2455e-05,  6.9130e-05],
        [-1.9804e-05, -1.3530e-05,  5.8115e-06,  ..., -1.6853e-05,
         -5.0291e-06, -1.1802e-05],
        [ 1.0066e-03,  6.7728e-04, -2.8898e-04,  ...,  8.6418e-04,
          2.7758e-04,  5.9901e-04],
        [-3.5286e-05, -2.4080e-05,  1.0356e-05,  ..., -2.9981e-05,
         -8.8289e-06, -2.0951e-05],
        [-4.5538e-05, -3.1114e-05,  1.3329e-05,  ..., -3.8683e-05,
         -1.1474e-05, -2.7061e-05]], device='cuda:0')
Loss: 1.1232775449752808


Running epoch 0, step 487, batch 487
Sampled inputs[:2]: tensor([[   0, 7061,  437,  ...,  278, 9500,   18],
        [   0,    9,  287,  ...,  369, 2968, 8347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3227e-04,  8.1521e-06, -1.5783e-04,  ...,  1.5645e-04,
          9.0424e-05,  1.1945e-04],
        [-2.2709e-05, -1.5467e-05,  6.6310e-06,  ..., -1.9282e-05,
         -5.8450e-06, -1.3553e-05],
        [ 1.0034e-03,  6.7512e-04, -2.8806e-04,  ...,  8.6145e-04,
          2.7667e-04,  5.9705e-04],
        [-4.0323e-05, -2.7448e-05,  1.1779e-05,  ..., -3.4213e-05,
         -1.0230e-05, -2.3991e-05],
        [-5.2154e-05, -3.5524e-05,  1.5192e-05,  ..., -4.4256e-05,
         -1.3322e-05, -3.1054e-05]], device='cuda:0')
Loss: 1.1217466592788696
Graident accumulation at epoch 0, step 487, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0293, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0340],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0154, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9021e-05,  3.2314e-05, -1.1472e-04,  ...,  2.8812e-05,
         -4.5346e-05,  6.9603e-06],
        [-2.1934e-05, -1.5899e-05,  5.7272e-06,  ..., -1.9260e-05,
         -5.0628e-06, -1.1464e-05],
        [ 1.0038e-04,  6.6773e-05, -2.7710e-05,  ...,  8.5380e-05,
          2.6541e-05,  5.5936e-05],
        [ 2.9090e-06,  1.4536e-06, -2.0488e-06,  ...,  5.7332e-06,
          1.5772e-06,  4.5587e-06],
        [-4.9269e-05, -3.5661e-05,  1.2660e-05,  ..., -4.3203e-05,
         -1.1321e-05, -2.5918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6258e-08, 1.4528e-08, 1.7286e-08,  ..., 1.9500e-08, 3.0875e-08,
         6.4767e-09],
        [4.5563e-11, 2.6324e-11, 2.0220e-12,  ..., 3.2085e-11, 1.6735e-12,
         7.8058e-12],
        [1.6627e-09, 8.4414e-10, 1.0498e-10,  ..., 1.2497e-09, 9.0800e-11,
         4.6266e-10],
        [4.4158e-10, 3.7007e-10, 2.9511e-11,  ..., 3.6080e-10, 1.9722e-11,
         1.3221e-10],
        [1.8891e-10, 1.0375e-10, 9.2609e-12,  ..., 1.3295e-10, 5.4771e-12,
         3.3354e-11]], device='cuda:0')
optimizer state dict: 61.0
lr: [1.1591565787821919e-05, 1.1591565787821919e-05]
scheduler_last_epoch: 61


Running epoch 0, step 488, batch 488
Sampled inputs[:2]: tensor([[   0,  394,  292,  ..., 1711,  365,  897],
        [   0,  328, 6379,  ...,  287, 1342,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0884e-05, -5.2158e-06, -1.9327e-05,  ..., -1.2099e-07,
         -6.0994e-06,  5.2856e-05],
        [-2.8312e-06, -1.9073e-06,  8.4937e-07,  ..., -2.3842e-06,
         -7.1898e-07, -1.7211e-06],
        [-3.2187e-06, -2.1756e-06,  9.6858e-07,  ..., -2.7269e-06,
         -8.0839e-07, -1.9670e-06],
        [-5.0962e-06, -3.4422e-06,  1.5423e-06,  ..., -4.3213e-06,
         -1.2740e-06, -3.0994e-06],
        [-6.4969e-06, -4.3511e-06,  1.9521e-06,  ..., -5.4836e-06,
         -1.6317e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.121012568473816


Running epoch 0, step 489, batch 489
Sampled inputs[:2]: tensor([[   0,   12,  287,  ..., 2336,  221,  334],
        [   0, 2286, 1085,  ..., 1387, 1184, 1802]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0805e-05,  9.5289e-06, -7.6001e-05,  ...,  3.6533e-06,
         -3.5554e-05,  7.8849e-05],
        [-5.6773e-06, -3.8072e-06,  1.6838e-06,  ..., -4.7982e-06,
         -1.5087e-06, -3.4422e-06],
        [-6.3926e-06, -4.2915e-06,  1.8999e-06,  ..., -5.4091e-06,
         -1.6801e-06, -3.8743e-06],
        [-1.0103e-05, -6.7800e-06,  3.0175e-06,  ..., -8.5533e-06,
         -2.6450e-06, -6.1244e-06],
        [-1.2904e-05, -8.6129e-06,  3.8296e-06,  ..., -1.0908e-05,
         -3.3975e-06, -7.7784e-06]], device='cuda:0')
Loss: 1.1217113733291626


Running epoch 0, step 490, batch 490
Sampled inputs[:2]: tensor([[    0,   806,  1255,  ...,   474,   221,   380],
        [    0, 13509,   472,  ...,  1805,    13, 27816]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6870e-04,  9.5289e-06, -1.9801e-04,  ..., -2.8651e-05,
         -7.6089e-05,  4.4114e-05],
        [-8.4788e-06, -5.7146e-06,  2.5444e-06,  ..., -7.2271e-06,
         -2.3283e-06, -5.2005e-06],
        [-9.5516e-06, -6.4373e-06,  2.8685e-06,  ..., -8.1360e-06,
         -2.5928e-06, -5.8562e-06],
        [-1.5080e-05, -1.0163e-05,  4.5449e-06,  ..., -1.2845e-05,
         -4.0755e-06, -9.2387e-06],
        [-1.9282e-05, -1.2934e-05,  5.7817e-06,  ..., -1.6391e-05,
         -5.2303e-06, -1.1742e-05]], device='cuda:0')
Loss: 1.133771538734436


Running epoch 0, step 491, batch 491
Sampled inputs[:2]: tensor([[   0,  587,  292,  ...,   12,  287, 2261],
        [   0,  278, 1041,  ..., 2098, 1837,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3463e-04, -4.5324e-05, -2.4717e-04,  ..., -1.8631e-06,
         -6.0400e-05,  8.4748e-05],
        [-1.1325e-05, -7.6368e-06,  3.3639e-06,  ..., -9.6262e-06,
         -3.0808e-06, -6.9141e-06],
        [-1.2755e-05, -8.6129e-06,  3.7923e-06,  ..., -1.0848e-05,
         -3.4347e-06, -7.7784e-06],
        [-2.0117e-05, -1.3575e-05,  5.9903e-06,  ..., -1.7107e-05,
         -5.3868e-06, -1.2249e-05],
        [-2.5719e-05, -1.7285e-05,  7.6294e-06,  ..., -2.1815e-05,
         -6.9216e-06, -1.5587e-05]], device='cuda:0')
Loss: 1.0947558879852295


Running epoch 0, step 492, batch 492
Sampled inputs[:2]: tensor([[    0,  3634,  3444,  ...,   642,  2156,   266],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4814e-04, -6.2787e-05, -2.6908e-04,  ...,  1.5772e-05,
         -6.2970e-05,  9.6841e-05],
        [-1.4171e-05, -9.5442e-06,  4.2059e-06,  ..., -1.2040e-05,
         -3.7923e-06, -8.6054e-06],
        [-1.6004e-05, -1.0788e-05,  4.7609e-06,  ..., -1.3620e-05,
         -4.2394e-06, -9.7156e-06],
        [-2.5243e-05, -1.7017e-05,  7.5102e-06,  ..., -2.1458e-05,
         -6.6459e-06, -1.5289e-05],
        [-3.2276e-05, -2.1696e-05,  9.5814e-06,  ..., -2.7418e-05,
         -8.5458e-06, -1.9491e-05]], device='cuda:0')
Loss: 1.1354033946990967


Running epoch 0, step 493, batch 493
Sampled inputs[:2]: tensor([[    0,   298,   301,  ...,    13, 10308,  2129],
        [    0,   221,   380,  ..., 10022,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5342e-04, -2.6383e-06, -3.2850e-04,  ...,  5.9259e-05,
         -5.6735e-05,  1.8257e-04],
        [-1.7032e-05, -1.1437e-05,  5.0552e-06,  ..., -1.4484e-05,
         -4.5821e-06, -1.0349e-05],
        [-1.9267e-05, -1.2949e-05,  5.7220e-06,  ..., -1.6406e-05,
         -5.1260e-06, -1.1697e-05],
        [-3.0309e-05, -2.0370e-05,  9.0152e-06,  ..., -2.5779e-05,
         -8.0168e-06, -1.8358e-05],
        [-3.8832e-05, -2.6017e-05,  1.1519e-05,  ..., -3.3021e-05,
         -1.0334e-05, -2.3454e-05]], device='cuda:0')
Loss: 1.1035778522491455


Running epoch 0, step 494, batch 494
Sampled inputs[:2]: tensor([[    0, 20080, 11069,  ...,   300,  5768,   271],
        [    0,   927,   259,  ...,   328,  9430,  2330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0849e-04, -1.5501e-05, -3.3614e-04,  ...,  1.1254e-04,
         -1.0300e-04,  1.0465e-04],
        [-1.9833e-05, -1.3299e-05,  5.9195e-06,  ..., -1.6868e-05,
         -5.2527e-06, -1.2040e-05],
        [-2.2441e-05, -1.5050e-05,  6.6981e-06,  ..., -1.9088e-05,
         -5.8711e-06, -1.3597e-05],
        [-3.5435e-05, -2.3782e-05,  1.0602e-05,  ..., -3.0130e-05,
         -9.2164e-06, -2.1443e-05],
        [-4.5240e-05, -3.0279e-05,  1.3486e-05,  ..., -3.8445e-05,
         -1.1846e-05, -2.7299e-05]], device='cuda:0')
Loss: 1.1088262796401978


Running epoch 0, step 495, batch 495
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,   957,  1231,  ...,   800,   342,  1398]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0152e-04,  6.7816e-05, -3.5544e-04,  ...,  1.4031e-04,
         -8.5270e-05,  1.0624e-04],
        [-2.2650e-05, -1.5184e-05,  6.7540e-06,  ..., -1.9267e-05,
         -6.0126e-06, -1.3791e-05],
        [-2.5615e-05, -1.7181e-05,  7.6406e-06,  ..., -2.1785e-05,
         -6.7167e-06, -1.5564e-05],
        [-4.0412e-05, -2.7105e-05,  1.2077e-05,  ..., -3.4362e-05,
         -1.0535e-05, -2.4512e-05],
        [-5.1618e-05, -3.4541e-05,  1.5371e-05,  ..., -4.3869e-05,
         -1.3553e-05, -3.1233e-05]], device='cuda:0')
Loss: 1.1103557348251343
Graident accumulation at epoch 0, step 495, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0293, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0163,  0.0149, -0.0272,  ...,  0.0284, -0.0154, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6271e-05,  3.5865e-05, -1.3879e-04,  ...,  3.9962e-05,
         -4.9338e-05,  1.6888e-05],
        [-2.2005e-05, -1.5827e-05,  5.8299e-06,  ..., -1.9260e-05,
         -5.1578e-06, -1.1697e-05],
        [ 8.7782e-05,  5.8378e-05, -2.4175e-05,  ...,  7.4664e-05,
          2.3215e-05,  4.8786e-05],
        [-1.4231e-06, -1.4023e-06, -6.3618e-07,  ...,  1.7236e-06,
          3.6594e-07,  1.6516e-06],
        [-4.9504e-05, -3.5549e-05,  1.2931e-05,  ..., -4.3270e-05,
         -1.1544e-05, -2.6450e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6222e-08, 1.4518e-08, 1.7395e-08,  ..., 1.9501e-08, 3.0851e-08,
         6.4815e-09],
        [4.6031e-11, 2.6528e-11, 2.0656e-12,  ..., 3.2425e-11, 1.7080e-12,
         7.9881e-12],
        [1.6617e-09, 8.4359e-10, 1.0494e-10,  ..., 1.2490e-09, 9.0754e-11,
         4.6244e-10],
        [4.4277e-10, 3.7043e-10, 2.9627e-11,  ..., 3.6162e-10, 1.9813e-11,
         1.3268e-10],
        [1.9138e-10, 1.0484e-10, 9.4879e-12,  ..., 1.3474e-10, 5.6553e-12,
         3.4296e-11]], device='cuda:0')
optimizer state dict: 62.0
lr: [1.1347022425551613e-05, 1.1347022425551613e-05]
scheduler_last_epoch: 62


Running epoch 0, step 496, batch 496
Sampled inputs[:2]: tensor([[    0, 15912,    14,  ..., 25535,    18,  3947],
        [    0,  1590,  2140,  ...,   287,  5342,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7632e-05,  1.4426e-05, -2.0468e-05,  ...,  2.7318e-05,
         -2.7106e-05, -1.1983e-05],
        [-2.8014e-06, -1.8626e-06,  8.4192e-07,  ..., -2.3991e-06,
         -6.8545e-07, -1.6764e-06],
        [-3.2187e-06, -2.1309e-06,  9.6112e-07,  ..., -2.7418e-06,
         -7.7114e-07, -1.9073e-06],
        [-5.1558e-06, -3.4124e-06,  1.5423e-06,  ..., -4.3809e-06,
         -1.2293e-06, -3.0547e-06],
        [-6.4373e-06, -4.2617e-06,  1.9222e-06,  ..., -5.4836e-06,
         -1.5423e-06, -3.8147e-06]], device='cuda:0')
Loss: 1.1082395315170288


Running epoch 0, step 497, batch 497
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0,    12,   496,  ...,   437,   266,  3767]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3880e-05, -6.3894e-06,  1.1987e-06,  ...,  4.3779e-05,
         -7.1071e-05,  3.9989e-06],
        [-5.6177e-06, -3.7253e-06,  1.7174e-06,  ..., -4.7833e-06,
         -1.4193e-06, -3.3826e-06],
        [-6.4671e-06, -4.2915e-06,  1.9744e-06,  ..., -5.4985e-06,
         -1.6093e-06, -3.8743e-06],
        [-1.0252e-05, -6.7800e-06,  3.1218e-06,  ..., -8.7023e-06,
         -2.5332e-06, -6.1244e-06],
        [-1.2904e-05, -8.5533e-06,  3.9339e-06,  ..., -1.0967e-05,
         -3.2112e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.129686713218689


Running epoch 0, step 498, batch 498
Sampled inputs[:2]: tensor([[    0,  5260,   365,  ...,  7242,   471,   391],
        [    0,  4645,  7688,  ..., 26535,   471,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0474e-05, -1.7415e-05,  2.4054e-05,  ...,  3.0161e-05,
         -1.0372e-04,  3.3965e-05],
        [-8.4788e-06, -5.5879e-06,  2.5593e-06,  ..., -7.1824e-06,
         -2.1830e-06, -5.0813e-06],
        [-9.7603e-06, -6.4373e-06,  2.9430e-06,  ..., -8.2701e-06,
         -2.4773e-06, -5.8264e-06],
        [-1.5318e-05, -1.0088e-05,  4.6119e-06,  ..., -1.2964e-05,
         -3.8669e-06, -9.1344e-06],
        [-1.9431e-05, -1.2815e-05,  5.8562e-06,  ..., -1.6481e-05,
         -4.9472e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.1305261850357056


Running epoch 0, step 499, batch 499
Sampled inputs[:2]: tensor([[    0,   266,  3574,  ...,  7052,  3829,   292],
        [    0, 11657,   367,  ..., 31468,    26,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7834e-05, -5.8809e-05,  2.4054e-05,  ..., -1.4848e-05,
         -1.8060e-04, -4.0302e-06],
        [-1.1265e-05, -7.4580e-06,  3.4086e-06,  ..., -9.5516e-06,
         -2.8946e-06, -6.7353e-06],
        [-1.2994e-05, -8.6129e-06,  3.9265e-06,  ..., -1.1012e-05,
         -3.2894e-06, -7.7337e-06],
        [-2.0325e-05, -1.3441e-05,  6.1393e-06,  ..., -1.7196e-05,
         -5.1185e-06, -1.2085e-05],
        [-2.5868e-05, -1.7136e-05,  7.8082e-06,  ..., -2.1935e-05,
         -6.5714e-06, -1.5393e-05]], device='cuda:0')
Loss: 1.1201071739196777


Running epoch 0, step 500, batch 500
Sampled inputs[:2]: tensor([[    0,  3756,    13,  ...,  1704,   278,  5851],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2200e-06, -6.7347e-05, -5.2042e-05,  ..., -4.0525e-05,
         -1.7910e-04, -1.9018e-05],
        [-1.4067e-05, -9.3207e-06,  4.2878e-06,  ..., -1.1966e-05,
         -3.6508e-06, -8.4564e-06],
        [-1.6183e-05, -1.0729e-05,  4.9248e-06,  ..., -1.3754e-05,
         -4.1313e-06, -9.6858e-06],
        [-2.5362e-05, -1.6764e-05,  7.7114e-06,  ..., -2.1517e-05,
         -6.4373e-06, -1.5154e-05],
        [-3.2216e-05, -2.1338e-05,  9.7901e-06,  ..., -2.7388e-05,
         -8.2552e-06, -1.9267e-05]], device='cuda:0')
Loss: 1.143394112586975


Running epoch 0, step 501, batch 501
Sampled inputs[:2]: tensor([[    0, 45050,   342,  ...,  3729,   287, 27888],
        [    0,  1428,   266,  ...,  3169,  3058,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0474e-05, -1.3641e-04, -4.8345e-05,  ..., -2.0042e-05,
         -2.0677e-04, -3.5869e-05],
        [-1.6868e-05, -1.1183e-05,  5.1446e-06,  ..., -1.4365e-05,
         -4.3809e-06, -1.0148e-05],
        [-1.9386e-05, -1.2860e-05,  5.9009e-06,  ..., -1.6496e-05,
         -4.9546e-06, -1.1623e-05],
        [-3.0398e-05, -2.0117e-05,  9.2611e-06,  ..., -2.5839e-05,
         -7.7337e-06, -1.8194e-05],
        [-3.8564e-05, -2.5570e-05,  1.1727e-05,  ..., -3.2842e-05,
         -9.8944e-06, -2.3112e-05]], device='cuda:0')
Loss: 1.13271963596344


Running epoch 0, step 502, batch 502
Sampled inputs[:2]: tensor([[   0, 4868, 3106,  ..., 2637,  278,  521],
        [   0,  287,  552,  ..., 7407, 2401,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8118e-05, -1.8525e-04, -7.9645e-05,  ...,  3.2060e-05,
         -2.4738e-04, -2.1172e-05],
        [-1.9684e-05, -1.3016e-05,  5.9977e-06,  ..., -1.6779e-05,
         -5.0776e-06, -1.1839e-05],
        [-2.2590e-05, -1.4946e-05,  6.8694e-06,  ..., -1.9237e-05,
         -5.7369e-06, -1.3545e-05],
        [-3.5435e-05, -2.3410e-05,  1.0788e-05,  ..., -3.0160e-05,
         -8.9630e-06, -2.1219e-05],
        [-4.4912e-05, -2.9713e-05,  1.3649e-05,  ..., -3.8296e-05,
         -1.1452e-05, -2.6926e-05]], device='cuda:0')
Loss: 1.1313380002975464


Running epoch 0, step 503, batch 503
Sampled inputs[:2]: tensor([[    0,  2950,    13,  ..., 16513,   300,  2205],
        [    0,    14,   221,  ...,   298,   408,  1849]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5302e-05, -1.3414e-04, -8.6888e-05,  ..., -2.7968e-06,
         -2.3373e-04,  2.4158e-05],
        [-2.2545e-05, -1.4909e-05,  6.8583e-06,  ..., -1.9178e-05,
         -5.8934e-06, -1.3575e-05],
        [-2.5868e-05, -1.7107e-05,  7.8529e-06,  ..., -2.1979e-05,
         -6.6571e-06, -1.5527e-05],
        [-4.0442e-05, -2.6718e-05,  1.2286e-05,  ..., -3.4362e-05,
         -1.0364e-05, -2.4229e-05],
        [-5.1409e-05, -3.4004e-05,  1.5602e-05,  ..., -4.3750e-05,
         -1.3284e-05, -3.0860e-05]], device='cuda:0')
Loss: 1.106130599975586
Graident accumulation at epoch 0, step 503, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0293, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0149, -0.0273,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9113e-05,  1.8864e-05, -1.3360e-04,  ...,  3.5686e-05,
         -6.7778e-05,  1.7615e-05],
        [-2.2059e-05, -1.5735e-05,  5.9327e-06,  ..., -1.9252e-05,
         -5.2314e-06, -1.1884e-05],
        [ 7.6417e-05,  5.0829e-05, -2.0972e-05,  ...,  6.5000e-05,
          2.0228e-05,  4.2355e-05],
        [-5.3249e-06, -3.9338e-06,  6.5604e-07,  ..., -1.8849e-06,
         -7.0703e-07, -9.3648e-07],
        [-4.9695e-05, -3.5395e-05,  1.3198e-05,  ..., -4.3318e-05,
         -1.1718e-05, -2.6891e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6177e-08, 1.4522e-08, 1.7385e-08,  ..., 1.9481e-08, 3.0875e-08,
         6.4756e-09],
        [4.6493e-11, 2.6724e-11, 2.1106e-12,  ..., 3.2760e-11, 1.7411e-12,
         8.1644e-12],
        [1.6607e-09, 8.4304e-10, 1.0489e-10,  ..., 1.2482e-09, 9.0708e-11,
         4.6222e-10],
        [4.4396e-10, 3.7077e-10, 2.9749e-11,  ..., 3.6244e-10, 1.9901e-11,
         1.3313e-10],
        [1.9383e-10, 1.0589e-10, 9.7218e-12,  ..., 1.3652e-10, 5.8261e-12,
         3.5214e-11]], device='cuda:0')
optimizer state dict: 63.0
lr: [1.1101655747595168e-05, 1.1101655747595168e-05]
scheduler_last_epoch: 63


Running epoch 0, step 504, batch 504
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5315e-05, -2.9372e-06,  6.3756e-06,  ...,  2.3993e-05,
         -5.4999e-06, -2.1606e-05],
        [-2.8312e-06, -1.8030e-06,  8.6799e-07,  ..., -2.3842e-06,
         -7.7486e-07, -1.7658e-06],
        [-3.2634e-06, -2.0862e-06,  9.9838e-07,  ..., -2.7567e-06,
         -8.8289e-07, -2.0266e-06],
        [-5.0366e-06, -3.2187e-06,  1.5497e-06,  ..., -4.2617e-06,
         -1.3635e-06, -3.1441e-06],
        [-6.4373e-06, -4.1127e-06,  1.9670e-06,  ..., -5.4240e-06,
         -1.7434e-06, -3.9935e-06]], device='cuda:0')
Loss: 1.1234239339828491


Running epoch 0, step 505, batch 505
Sampled inputs[:2]: tensor([[   0, 1098,  259,  ..., 6572, 1477,   12],
        [   0, 1070, 5746,  ...,  278,  689,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3581e-05,  5.3036e-05, -4.1062e-05,  ...,  3.4078e-05,
         -6.3716e-06, -3.2981e-05],
        [-5.7220e-06, -3.6508e-06,  1.7174e-06,  ..., -4.7833e-06,
         -1.5609e-06, -3.5241e-06],
        [-6.5863e-06, -4.2170e-06,  1.9819e-06,  ..., -5.5432e-06,
         -1.7770e-06, -4.0531e-06],
        [-1.0192e-05, -6.5267e-06,  3.0696e-06,  ..., -8.5533e-06,
         -2.7344e-06, -6.2734e-06],
        [-1.3024e-05, -8.3148e-06,  3.9041e-06,  ..., -1.0908e-05,
         -3.5167e-06, -7.9870e-06]], device='cuda:0')
Loss: 1.0993430614471436


Running epoch 0, step 506, batch 506
Sampled inputs[:2]: tensor([[    0,   271,  8130,  ...,   609, 28676,   965],
        [    0,  2816,   292,  ...,  3662,   461,  2723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0987e-05,  5.3036e-05, -5.8937e-05,  ...,  6.8964e-05,
         -3.2541e-05, -6.1802e-05],
        [-8.5086e-06, -5.4613e-06,  2.5891e-06,  ..., -7.1973e-06,
         -2.2911e-06, -5.2676e-06],
        [-9.7752e-06, -6.2883e-06,  2.9802e-06,  ..., -8.2999e-06,
         -2.6003e-06, -6.0499e-06],
        [-1.5199e-05, -9.7901e-06,  4.6492e-06,  ..., -1.2904e-05,
         -4.0233e-06, -9.4026e-06],
        [-1.9342e-05, -1.2428e-05,  5.8860e-06,  ..., -1.6391e-05,
         -5.1558e-06, -1.1921e-05]], device='cuda:0')
Loss: 1.131073236465454


Running epoch 0, step 507, batch 507
Sampled inputs[:2]: tensor([[    0,   749,     9,  ...,  2756,    14,  1062],
        [    0,  9423,   298,  ...,  5274, 37902,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5666e-05,  8.2148e-05, -7.4381e-05,  ...,  8.8546e-05,
         -6.5419e-06, -1.3824e-05],
        [-1.1355e-05, -7.2867e-06,  3.4645e-06,  ..., -9.6262e-06,
         -3.0585e-06, -6.9961e-06],
        [-1.3024e-05, -8.3745e-06,  3.9786e-06,  ..., -1.1072e-05,
         -3.4682e-06, -8.0168e-06],
        [-2.0206e-05, -1.3009e-05,  6.1914e-06,  ..., -1.7196e-05,
         -5.3495e-06, -1.2442e-05],
        [-2.5839e-05, -1.6600e-05,  7.8827e-06,  ..., -2.1935e-05,
         -6.8918e-06, -1.5855e-05]], device='cuda:0')
Loss: 1.1421024799346924


Running epoch 0, step 508, batch 508
Sampled inputs[:2]: tensor([[   0, 3125,  271,  ..., 1041, 1032,   15],
        [   0,  328,  843,  ...,  298,  292,   37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4691e-05,  9.2384e-05, -9.6120e-05,  ...,  8.4753e-05,
         -3.5988e-06, -4.3569e-05],
        [-1.4216e-05, -9.1568e-06,  4.3437e-06,  ..., -1.2100e-05,
         -3.8594e-06, -8.7693e-06],
        [-1.6287e-05, -1.0505e-05,  4.9844e-06,  ..., -1.3903e-05,
         -4.3698e-06, -1.0043e-05],
        [-2.5332e-05, -1.6347e-05,  7.7635e-06,  ..., -2.1636e-05,
         -6.7502e-06, -1.5602e-05],
        [-3.2336e-05, -2.0832e-05,  9.8646e-06,  ..., -2.7537e-05,
         -8.6799e-06, -1.9848e-05]], device='cuda:0')
Loss: 1.1302510499954224


Running epoch 0, step 509, batch 509
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    12,   259,  1220],
        [    0,   278,   266,  ..., 10639,   292,  4723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2474e-06,  9.1228e-05, -1.4396e-04,  ...,  1.1670e-04,
         -3.9984e-05, -2.7636e-05],
        [-1.7032e-05, -1.1005e-05,  5.2303e-06,  ..., -1.4514e-05,
         -4.6268e-06, -1.0505e-05],
        [-1.9535e-05, -1.2636e-05,  6.0052e-06,  ..., -1.6689e-05,
         -5.2415e-06, -1.2040e-05],
        [-3.0369e-05, -1.9655e-05,  9.3505e-06,  ..., -2.5958e-05,
         -8.0913e-06, -1.8686e-05],
        [-3.8713e-05, -2.5034e-05,  1.1876e-05,  ..., -3.3021e-05,
         -1.0401e-05, -2.3782e-05]], device='cuda:0')
Loss: 1.093589425086975


Running epoch 0, step 510, batch 510
Sampled inputs[:2]: tensor([[    0, 17301,   300,  ...,   278,   546,  1576],
        [    0,   266,  9823,  ...,    14,  1062,  7676]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9235e-05,  2.4521e-05, -1.5220e-04,  ...,  9.6029e-05,
         -7.5557e-05, -4.9303e-05],
        [-1.9848e-05, -1.2815e-05,  6.0759e-06,  ..., -1.6943e-05,
         -5.3719e-06, -1.2279e-05],
        [-2.2754e-05, -1.4707e-05,  6.9737e-06,  ..., -1.9461e-05,
         -6.0834e-06, -1.4067e-05],
        [-3.5435e-05, -2.2903e-05,  1.0878e-05,  ..., -3.0309e-05,
         -9.4101e-06, -2.1875e-05],
        [-4.5121e-05, -2.9147e-05,  1.3798e-05,  ..., -3.8534e-05,
         -1.2085e-05, -2.7806e-05]], device='cuda:0')
Loss: 1.1265333890914917


Running epoch 0, step 511, batch 511
Sampled inputs[:2]: tensor([[    0,   368, 46614,  ...,  1070,   278,  1028],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8733e-05,  1.2303e-04, -1.8563e-04,  ...,  1.4668e-04,
         -9.5051e-05, -1.4226e-05],
        [-2.2694e-05, -1.4648e-05,  6.9551e-06,  ..., -1.9386e-05,
         -6.1356e-06, -1.4007e-05],
        [-2.6032e-05, -1.6823e-05,  7.9870e-06,  ..., -2.2277e-05,
         -6.9477e-06, -1.6049e-05],
        [-4.0501e-05, -2.6166e-05,  1.2450e-05,  ..., -3.4660e-05,
         -1.0736e-05, -2.4945e-05],
        [-5.1558e-05, -3.3289e-05,  1.5780e-05,  ..., -4.4018e-05,
         -1.3776e-05, -3.1680e-05]], device='cuda:0')
Loss: 1.1052639484405518
Graident accumulation at epoch 0, step 511, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0163,  0.0149, -0.0273,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1329e-05,  2.9280e-05, -1.3881e-04,  ...,  4.6785e-05,
         -7.0505e-05,  1.4431e-05],
        [-2.2123e-05, -1.5627e-05,  6.0349e-06,  ..., -1.9266e-05,
         -5.3218e-06, -1.2097e-05],
        [ 6.6172e-05,  4.4064e-05, -1.8076e-05,  ...,  5.6272e-05,
          1.7511e-05,  3.6514e-05],
        [-8.8426e-06, -6.1571e-06,  1.8354e-06,  ..., -5.1625e-06,
         -1.7100e-06, -3.3373e-06],
        [-4.9881e-05, -3.5184e-05,  1.3456e-05,  ..., -4.3388e-05,
         -1.1924e-05, -2.7370e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6133e-08, 1.4522e-08, 1.7402e-08,  ..., 1.9483e-08, 3.0853e-08,
         6.4693e-09],
        [4.6961e-11, 2.6912e-11, 2.1568e-12,  ..., 3.3103e-11, 1.7770e-12,
         8.3525e-12],
        [1.6597e-09, 8.4248e-10, 1.0485e-10,  ..., 1.2474e-09, 9.0665e-11,
         4.6201e-10],
        [4.4516e-10, 3.7109e-10, 2.9874e-11,  ..., 3.6328e-10, 1.9996e-11,
         1.3362e-10],
        [1.9630e-10, 1.0689e-10, 9.9611e-12,  ..., 1.3832e-10, 6.0101e-12,
         3.6183e-11]], device='cuda:0')
optimizer state dict: 64.0
lr: [1.085561572490406e-05, 1.085561572490406e-05]
scheduler_last_epoch: 64


Running epoch 0, step 512, batch 512
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0416e-04,  4.0168e-05,  3.7477e-05,  ..., -9.8766e-06,
         -4.8107e-05, -2.0662e-05],
        [-2.8163e-06, -1.7732e-06,  8.7917e-07,  ..., -2.4289e-06,
         -8.0839e-07, -1.7732e-06],
        [-3.2932e-06, -2.0713e-06,  1.0282e-06,  ..., -2.8461e-06,
         -9.3132e-07, -2.0713e-06],
        [-5.0664e-06, -3.1888e-06,  1.5870e-06,  ..., -4.3809e-06,
         -1.4231e-06, -3.1739e-06],
        [-6.4671e-06, -4.0829e-06,  2.0117e-06,  ..., -5.5730e-06,
         -1.8328e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.1228801012039185


Running epoch 0, step 513, batch 513
Sampled inputs[:2]: tensor([[   0, 2355, 2728,  ...,  554, 9025,  368],
        [   0, 4929, 4214,  ..., 1172,  591, 4422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0712e-05,  5.8620e-05, -3.4531e-06,  ...,  3.3639e-05,
         -7.4262e-05,  8.0777e-06],
        [-5.6326e-06, -3.5465e-06,  1.7472e-06,  ..., -4.8280e-06,
         -1.6019e-06, -3.5241e-06],
        [-6.6012e-06, -4.1574e-06,  2.0415e-06,  ..., -5.6624e-06,
         -1.8477e-06, -4.1276e-06],
        [-1.0133e-05, -6.3777e-06,  3.1441e-06,  ..., -8.7023e-06,
         -2.8163e-06, -6.3181e-06],
        [-1.2934e-05, -8.1360e-06,  3.9935e-06,  ..., -1.1057e-05,
         -3.6210e-06, -8.0466e-06]], device='cuda:0')
Loss: 1.1254324913024902


Running epoch 0, step 514, batch 514
Sampled inputs[:2]: tensor([[    0,  1644,  1742,  ...,   287,  1704,  2044],
        [    0, 17900,   554,  ...,   266,  7912,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6221e-04, -1.0123e-05, -2.3988e-06,  ...,  6.1083e-05,
         -1.1456e-04,  2.3820e-05],
        [-8.3894e-06, -5.2899e-06,  2.6114e-06,  ..., -7.1526e-06,
         -2.3395e-06, -5.2154e-06],
        [-9.9242e-06, -6.2734e-06,  3.0845e-06,  ..., -8.4788e-06,
         -2.7232e-06, -6.1691e-06],
        [-1.5259e-05, -9.6262e-06,  4.7535e-06,  ..., -1.3053e-05,
         -4.1574e-06, -9.4622e-06],
        [-1.9431e-05, -1.2249e-05,  6.0201e-06,  ..., -1.6540e-05,
         -5.3346e-06, -1.2010e-05]], device='cuda:0')
Loss: 1.1065577268600464


Running epoch 0, step 515, batch 515
Sampled inputs[:2]: tensor([[    0,  7333,   342,  ...,    13,  1818,  6183],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2609e-04, -5.2633e-05, -2.0644e-05,  ...,  5.3255e-05,
         -1.0157e-04,  2.5795e-05],
        [-1.1176e-05, -7.0632e-06,  3.4831e-06,  ..., -9.5218e-06,
         -3.0659e-06, -6.8992e-06],
        [-1.3188e-05, -8.3596e-06,  4.1053e-06,  ..., -1.1250e-05,
         -3.5614e-06, -8.1360e-06],
        [-2.0295e-05, -1.2830e-05,  6.3330e-06,  ..., -1.7345e-05,
         -5.4389e-06, -1.2502e-05],
        [-2.5779e-05, -1.6302e-05,  8.0168e-06,  ..., -2.1935e-05,
         -6.9737e-06, -1.5855e-05]], device='cuda:0')
Loss: 1.109619379043579


Running epoch 0, step 516, batch 516
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,   508, 12163,  ...,  4920,   344, 11003]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2634e-04, -9.8568e-05, -4.0886e-05,  ...,  6.3185e-05,
         -1.2515e-04,  1.0232e-05],
        [-1.3992e-05, -8.8364e-06,  4.3623e-06,  ..., -1.1936e-05,
         -3.8557e-06, -8.6352e-06],
        [-1.6466e-05, -1.0431e-05,  5.1335e-06,  ..., -1.4067e-05,
         -4.4741e-06, -1.0163e-05],
        [-2.5332e-05, -1.6019e-05,  7.9125e-06,  ..., -2.1666e-05,
         -6.8322e-06, -1.5602e-05],
        [-3.2216e-05, -2.0355e-05,  1.0028e-05,  ..., -2.7448e-05,
         -8.7693e-06, -1.9819e-05]], device='cuda:0')
Loss: 1.130467414855957


Running epoch 0, step 517, batch 517
Sampled inputs[:2]: tensor([[    0,  1176,    13,  ...,  1919,   221,   380],
        [    0, 18971,   278,  ...,  1934,  1916,  2612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2637e-05, -1.3596e-04, -5.0081e-05,  ...,  4.5110e-05,
         -1.2586e-04,  6.4898e-06],
        [-1.6823e-05, -1.0617e-05,  5.2378e-06,  ..., -1.4320e-05,
         -4.6231e-06, -1.0386e-05],
        [ 7.5053e-05,  4.9005e-05, -7.2287e-06,  ...,  6.8027e-05,
          1.7319e-05,  4.7122e-05],
        [-3.0458e-05, -1.9237e-05,  9.4995e-06,  ..., -2.5988e-05,
         -8.1956e-06, -1.8761e-05],
        [-3.8773e-05, -2.4468e-05,  1.2055e-05,  ..., -3.2961e-05,
         -1.0528e-05, -2.3842e-05]], device='cuda:0')
Loss: 1.0974823236465454


Running epoch 0, step 518, batch 518
Sampled inputs[:2]: tensor([[    0,  5129,  1245,  ...,   292, 24298,    13],
        [    0,    14,   475,  ...,  2117,  2792, 12848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3639e-05, -9.2220e-05, -6.5341e-05,  ...,  6.9994e-05,
         -1.9519e-04,  5.9443e-06],
        [-1.9640e-05, -1.2398e-05,  6.1244e-06,  ..., -1.6734e-05,
         -5.4426e-06, -1.2144e-05],
        [ 7.1775e-05,  4.6934e-05, -6.1930e-06,  ...,  6.5210e-05,
          1.6384e-05,  4.5081e-05],
        [-3.5465e-05, -2.2411e-05,  1.1086e-05,  ..., -3.0309e-05,
         -9.6187e-06, -2.1875e-05],
        [-4.5180e-05, -2.8521e-05,  1.4082e-05,  ..., -3.8475e-05,
         -1.2361e-05, -2.7806e-05]], device='cuda:0')
Loss: 1.1231127977371216


Running epoch 0, step 519, batch 519
Sampled inputs[:2]: tensor([[    0, 21748,   792,  ...,   408,   266, 31879],
        [    0,    21,   292,  ...,    13,  1861,  4254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1097e-05,  1.1488e-04, -8.6532e-05,  ...,  7.1105e-05,
         -1.7160e-04,  8.2002e-05],
        [-2.2516e-05, -1.4193e-05,  6.9961e-06,  ..., -1.9163e-05,
         -6.3889e-06, -1.4029e-05],
        [ 1.4672e-04,  1.2587e-04, -4.5083e-05,  ...,  1.3234e-04,
          6.5133e-05,  1.0945e-04],
        [-4.0442e-05, -2.5526e-05,  1.2599e-05,  ..., -3.4511e-05,
         -1.1228e-05, -2.5123e-05],
        [-5.1677e-05, -3.2574e-05,  1.6049e-05,  ..., -4.3958e-05,
         -1.4476e-05, -3.2067e-05]], device='cuda:0')
Loss: 1.1264500617980957
Graident accumulation at epoch 0, step 519, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3306e-05,  3.7840e-05, -1.3358e-04,  ...,  4.9217e-05,
         -8.0615e-05,  2.1188e-05],
        [-2.2162e-05, -1.5483e-05,  6.1311e-06,  ..., -1.9255e-05,
         -5.4285e-06, -1.2290e-05],
        [ 7.4227e-05,  5.2244e-05, -2.0777e-05,  ...,  6.3879e-05,
          2.2273e-05,  4.3808e-05],
        [-1.2002e-05, -8.0939e-06,  2.9118e-06,  ..., -8.0973e-06,
         -2.6618e-06, -5.5159e-06],
        [-5.0061e-05, -3.4923e-05,  1.3716e-05,  ..., -4.3445e-05,
         -1.2179e-05, -2.7840e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6088e-08, 1.4521e-08, 1.7392e-08,  ..., 1.9469e-08, 3.0852e-08,
         6.4696e-09],
        [4.7421e-11, 2.7086e-11, 2.2036e-12,  ..., 3.3437e-11, 1.8160e-12,
         8.5409e-12],
        [1.6796e-09, 8.5748e-10, 1.0678e-10,  ..., 1.2637e-09, 9.4817e-11,
         4.7353e-10],
        [4.4635e-10, 3.7137e-10, 3.0003e-11,  ..., 3.6410e-10, 2.0102e-11,
         1.3412e-10],
        [1.9877e-10, 1.0785e-10, 1.0209e-11,  ..., 1.4012e-10, 6.2136e-12,
         3.7175e-11]], device='cuda:0')
optimizer state dict: 65.0
lr: [1.060905273998585e-05, 1.060905273998585e-05]
scheduler_last_epoch: 65


Running epoch 0, step 520, batch 520
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   12, 5576,   12],
        [   0,   12,  342,  ..., 3458,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3068e-05,  6.3634e-05, -2.4839e-05,  ...,  4.2071e-05,
         -5.3157e-05,  7.0462e-06],
        [-2.7716e-06, -1.7434e-06,  8.6427e-07,  ..., -2.4438e-06,
         -8.5682e-07, -1.7807e-06],
        [-3.2187e-06, -2.0266e-06,  1.0058e-06,  ..., -2.8312e-06,
         -9.8348e-07, -2.0713e-06],
        [-4.9472e-06, -3.1143e-06,  1.5497e-06,  ..., -4.3511e-06,
         -1.4976e-06, -3.1739e-06],
        [-6.3479e-06, -3.9935e-06,  1.9819e-06,  ..., -5.5730e-06,
         -1.9372e-06, -4.0829e-06]], device='cuda:0')
Loss: 1.1284931898117065


Running epoch 0, step 521, batch 521
Sampled inputs[:2]: tensor([[    0,   733,   560,  ...,  1172, 22808,   271],
        [    0,    14, 38914,  ...,   266,  5690,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9430e-05,  1.6737e-04, -6.9794e-06,  ...,  8.0654e-05,
         -8.2106e-05,  3.7752e-05],
        [-5.6326e-06, -3.4720e-06,  1.7732e-06,  ..., -4.8429e-06,
         -1.8179e-06, -3.6657e-06],
        [-6.5565e-06, -4.0531e-06,  2.0713e-06,  ..., -5.6475e-06,
         -2.0936e-06, -4.2766e-06],
        [-9.9838e-06, -6.1691e-06,  3.1590e-06,  ..., -8.5831e-06,
         -3.1590e-06, -6.4969e-06],
        [-1.2845e-05, -7.9274e-06,  4.0382e-06,  ..., -1.1027e-05,
         -4.0978e-06, -8.3447e-06]], device='cuda:0')
Loss: 1.1057755947113037


Running epoch 0, step 522, batch 522
Sampled inputs[:2]: tensor([[   0,  554, 1034,  ..., 3313,  365,  654],
        [   0, 1575, 4384,  ...,  328,  722, 6124]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3872e-05,  1.7351e-04,  1.1803e-05,  ...,  1.0419e-04,
         -7.3103e-05,  3.6644e-05],
        [-8.4043e-06, -5.1931e-06,  2.6561e-06,  ..., -7.2271e-06,
         -2.6561e-06, -5.4613e-06],
        [-9.8348e-06, -6.0946e-06,  3.1143e-06,  ..., -8.4788e-06,
         -3.0696e-06, -6.4075e-06],
        [-1.4991e-05, -9.2685e-06,  4.7535e-06,  ..., -1.2875e-05,
         -4.6343e-06, -9.7305e-06],
        [-1.9222e-05, -1.1891e-05,  6.0648e-06,  ..., -1.6510e-05,
         -6.0052e-06, -1.2457e-05]], device='cuda:0')
Loss: 1.1147615909576416


Running epoch 0, step 523, batch 523
Sampled inputs[:2]: tensor([[    0,   389, 18984,  ...,   287,   768,  1070],
        [    0,    13,  2497,  ...,   943,   259,  2646]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5846e-05,  2.4536e-04, -5.7097e-05,  ...,  1.3766e-04,
         -7.4191e-05,  9.8004e-05],
        [-1.1235e-05, -6.9588e-06,  3.5428e-06,  ..., -9.6411e-06,
         -3.5502e-06, -7.2867e-06],
        [-1.3158e-05, -8.1807e-06,  4.1574e-06,  ..., -1.1310e-05,
         -4.1053e-06, -8.5533e-06],
        [-1.9968e-05, -1.2383e-05,  6.3181e-06,  ..., -1.7136e-05,
         -6.1765e-06, -1.2949e-05],
        [-2.5719e-05, -1.5944e-05,  8.1062e-06,  ..., -2.2054e-05,
         -8.0317e-06, -1.6630e-05]], device='cuda:0')
Loss: 1.1233532428741455


Running epoch 0, step 524, batch 524
Sampled inputs[:2]: tensor([[    0,  9577,  2789,  ...,  1042,  9086,   623],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1321e-05,  2.1868e-04, -6.4039e-05,  ...,  1.8133e-04,
         -1.6261e-04,  1.2736e-04],
        [-1.4022e-05, -8.6650e-06,  4.4182e-06,  ..., -1.2010e-05,
         -4.3102e-06, -9.0152e-06],
        [ 4.0785e-04,  3.2006e-04, -7.1652e-05,  ...,  3.6177e-04,
          1.3704e-04,  3.0321e-04],
        [-2.5034e-05, -1.5497e-05,  7.9125e-06,  ..., -2.1458e-05,
         -7.5251e-06, -1.6093e-05],
        [-3.2127e-05, -1.9878e-05,  1.0118e-05,  ..., -2.7508e-05,
         -9.7454e-06, -2.0593e-05]], device='cuda:0')
Loss: 1.1261390447616577


Running epoch 0, step 525, batch 525
Sampled inputs[:2]: tensor([[   0, 5150, 1030,  ...,   14,  475, 1763],
        [   0,  278, 2097,  ..., 1754,  287,  631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0584e-05,  2.6791e-04, -1.4932e-04,  ...,  2.1802e-04,
         -1.7049e-04,  1.1398e-04],
        [-1.6794e-05, -1.0364e-05,  5.3048e-06,  ..., -1.4395e-05,
         -5.1223e-06, -1.0774e-05],
        [ 4.0457e-04,  3.1805e-04, -7.0601e-05,  ...,  3.5896e-04,
          1.3611e-04,  3.0114e-04],
        [-3.0100e-05, -1.8597e-05,  9.5367e-06,  ..., -2.5809e-05,
         -8.9705e-06, -1.9297e-05],
        [-3.8475e-05, -2.3782e-05,  1.2159e-05,  ..., -3.2961e-05,
         -1.1578e-05, -2.4617e-05]], device='cuda:0')
Loss: 1.1156702041625977


Running epoch 0, step 526, batch 526
Sampled inputs[:2]: tensor([[   0,    9,  292,  ...,  944,  278, 1758],
        [   0,  391, 9095,  ...,  417,  199, 2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0584e-05,  2.5516e-04, -1.7036e-04,  ...,  2.9144e-04,
         -2.2188e-04,  1.0460e-04],
        [-1.9535e-05, -1.2100e-05,  6.1989e-06,  ..., -1.6764e-05,
         -5.8524e-06, -1.2502e-05],
        [ 4.0132e-04,  3.1598e-04, -6.9543e-05,  ...,  3.5614e-04,
          1.3525e-04,  2.9910e-04],
        [ 5.5416e-05,  2.8693e-05, -2.8782e-05,  ...,  5.0429e-05,
          3.2328e-05,  3.2562e-05],
        [-4.4823e-05, -2.7806e-05,  1.4231e-05,  ..., -3.8475e-05,
         -1.3247e-05, -2.8610e-05]], device='cuda:0')
Loss: 1.1268123388290405


Running epoch 0, step 527, batch 527
Sampled inputs[:2]: tensor([[    0,  2162,    73,  ...,   278,   266,  1059],
        [    0,   269,    12,  ..., 45645,    14,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5313e-04,  3.4632e-04, -2.0983e-04,  ...,  2.8064e-04,
         -2.4754e-04,  1.1942e-04],
        [-2.2337e-05, -1.3836e-05,  7.0669e-06,  ..., -1.9178e-05,
         -6.7316e-06, -1.4275e-05],
        [ 3.9804e-04,  3.1395e-04, -6.8523e-05,  ...,  3.5333e-04,
          1.3425e-04,  2.9705e-04],
        [ 5.0469e-05,  2.5623e-05, -2.7239e-05,  ...,  4.6167e-05,
          3.0816e-05,  2.9447e-05],
        [-5.1230e-05, -3.1769e-05,  1.6227e-05,  ..., -4.3988e-05,
         -1.5229e-05, -3.2663e-05]], device='cuda:0')
Loss: 1.1162335872650146
Graident accumulation at epoch 0, step 527, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6288e-05,  6.8688e-05, -1.4120e-04,  ...,  7.2359e-05,
         -9.7307e-05,  3.1011e-05],
        [-2.2180e-05, -1.5318e-05,  6.2246e-06,  ..., -1.9248e-05,
         -5.5588e-06, -1.2489e-05],
        [ 1.0661e-04,  7.8415e-05, -2.5552e-05,  ...,  9.2824e-05,
          3.3470e-05,  6.9132e-05],
        [-5.7553e-06, -4.7222e-06, -1.0333e-07,  ..., -2.6709e-06,
          6.8601e-07, -2.0196e-06],
        [-5.0178e-05, -3.4608e-05,  1.3967e-05,  ..., -4.3499e-05,
         -1.2484e-05, -2.8322e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6066e-08, 1.4626e-08, 1.7419e-08,  ..., 1.9528e-08, 3.0882e-08,
         6.4774e-09],
        [4.7873e-11, 2.7250e-11, 2.2514e-12,  ..., 3.3771e-11, 1.8595e-12,
         8.7362e-12],
        [1.8363e-09, 9.5519e-10, 1.1137e-10,  ..., 1.3873e-09, 1.1274e-10,
         5.6129e-10],
        [4.4845e-10, 3.7165e-10, 3.0715e-11,  ..., 3.6587e-10, 2.1032e-11,
         1.3485e-10],
        [2.0120e-10, 1.0875e-10, 1.0462e-11,  ..., 1.4191e-10, 6.4393e-12,
         3.8204e-11]], device='cuda:0')
optimizer state dict: 66.0
lr: [1.0362117494988668e-05, 1.0362117494988668e-05]
scheduler_last_epoch: 66


Running epoch 0, step 528, batch 528
Sampled inputs[:2]: tensor([[   0,  689, 3953,  ...,  461,  943,  352],
        [   0, 1726, 3775,  ...,  300,  266, 1686]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1655e-05,  8.2171e-06,  5.8086e-05,  ..., -1.2345e-05,
         -5.4387e-05, -3.2310e-07],
        [-2.6971e-06, -1.7360e-06,  8.9407e-07,  ..., -2.3693e-06,
         -8.2701e-07, -1.7285e-06],
        [-3.2485e-06, -2.1011e-06,  1.0729e-06,  ..., -2.8610e-06,
         -9.7603e-07, -2.0713e-06],
        [-4.9472e-06, -3.1888e-06,  1.6391e-06,  ..., -4.3511e-06,
         -1.4827e-06, -3.1590e-06],
        [-6.2585e-06, -4.0531e-06,  2.0713e-06,  ..., -5.5134e-06,
         -1.8924e-06, -3.9935e-06]], device='cuda:0')
Loss: 1.13407301902771


Running epoch 0, step 529, batch 529
Sampled inputs[:2]: tensor([[    0,  1136,   944,  ...,   401, 13771,    12],
        [    0, 47831,   266,  ...,    66,    17, 20005]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1535e-05,  1.0537e-05,  2.2083e-05,  ...,  4.4799e-05,
         -7.5312e-05,  1.1275e-05],
        [-5.4240e-06, -3.4049e-06,  1.7509e-06,  ..., -4.7386e-06,
         -1.6838e-06, -3.5018e-06],
        [-6.5267e-06, -4.0978e-06,  2.1011e-06,  ..., -5.7071e-06,
         -1.9893e-06, -4.2021e-06],
        [-9.9242e-06, -6.2287e-06,  3.2112e-06,  ..., -8.6725e-06,
         -3.0175e-06, -6.3926e-06],
        [-1.2547e-05, -7.8976e-06,  4.0531e-06,  ..., -1.0967e-05,
         -3.8445e-06, -8.0764e-06]], device='cuda:0')
Loss: 1.1199345588684082


Running epoch 0, step 530, batch 530
Sampled inputs[:2]: tensor([[   0, 6328,   12,  ...,  417,  199, 1726],
        [   0,  494,  221,  ...,  437,  266, 2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1807e-05,  2.4063e-05,  2.2083e-05,  ...,  5.9438e-05,
         -1.4784e-04, -7.6379e-06],
        [-8.1360e-06, -5.0962e-06,  2.6524e-06,  ..., -7.1079e-06,
         -2.5742e-06, -5.3272e-06],
        [-9.7603e-06, -6.1244e-06,  3.1739e-06,  ..., -8.5384e-06,
         -3.0324e-06, -6.3628e-06],
        [-1.4871e-05, -9.3132e-06,  4.8578e-06,  ..., -1.2994e-05,
         -4.5970e-06, -9.7007e-06],
        [-1.8746e-05, -1.1772e-05,  6.1095e-06,  ..., -1.6391e-05,
         -5.8562e-06, -1.2219e-05]], device='cuda:0')
Loss: 1.1330089569091797


Running epoch 0, step 531, batch 531
Sampled inputs[:2]: tensor([[    0,  1480,   518,  ...,   445,    28,   445],
        [    0,   271,   266,  ...,  2805,   607, 10848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8944e-06, -3.1342e-05, -1.4316e-05,  ...,  9.1702e-05,
         -2.0573e-04, -2.0543e-06],
        [-1.0788e-05, -6.7577e-06,  3.5055e-06,  ..., -9.4324e-06,
         -3.3230e-06, -7.0408e-06],
        [ 7.6158e-05,  4.4780e-05, -2.7824e-05,  ...,  8.4510e-05,
          3.5158e-05,  4.3730e-05],
        [-1.9819e-05, -1.2428e-05,  6.4522e-06,  ..., -1.7345e-05,
         -5.9530e-06, -1.2890e-05],
        [-2.4974e-05, -1.5706e-05,  8.1062e-06,  ..., -2.1845e-05,
         -7.5772e-06, -1.6242e-05]], device='cuda:0')
Loss: 1.102908730506897


Running epoch 0, step 532, batch 532
Sampled inputs[:2]: tensor([[    0,   969,   258,  ...,   726,  5303,  6512],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3324e-05, -6.5740e-06, -1.7671e-05,  ...,  4.8853e-05,
         -1.6286e-04, -2.0543e-06],
        [-1.3530e-05, -8.4564e-06,  4.3698e-06,  ..., -1.1802e-05,
         -4.1798e-06, -8.7917e-06],
        [ 7.2895e-05,  4.2753e-05, -2.6796e-05,  ...,  8.1694e-05,
          3.4152e-05,  4.1643e-05],
        [-2.4766e-05, -1.5512e-05,  8.0243e-06,  ..., -2.1636e-05,
         -7.4729e-06, -1.6049e-05],
        [-3.1292e-05, -1.9640e-05,  1.0103e-05,  ..., -2.7299e-05,
         -9.5293e-06, -2.0266e-05]], device='cuda:0')
Loss: 1.1462180614471436


Running epoch 0, step 533, batch 533
Sampled inputs[:2]: tensor([[    0,   278,   264,  ..., 21836,   344,   259],
        [    0,  2523, 10780,  ...,  1041,    26, 13745]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6337e-05, -3.6376e-06, -3.4710e-05,  ...,  9.1463e-05,
         -2.0202e-04,  3.2738e-05],
        [-1.6212e-05, -1.0125e-05,  5.2340e-06,  ..., -1.4171e-05,
         -4.9360e-06, -1.0535e-05],
        [ 6.9662e-05,  4.0726e-05, -2.5753e-05,  ...,  7.8832e-05,
          3.3258e-05,  3.9542e-05],
        [-2.9743e-05, -1.8626e-05,  9.6336e-06,  ..., -2.6047e-05,
         -8.8364e-06, -1.9282e-05],
        [-3.7551e-05, -2.3544e-05,  1.2115e-05,  ..., -3.2812e-05,
         -1.1265e-05, -2.4319e-05]], device='cuda:0')
Loss: 1.1064019203186035


Running epoch 0, step 534, batch 534
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   908,  1086,    12],
        [    0,   475,   668,  ..., 17680,   368,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9234e-05,  9.6908e-07, -2.5940e-05,  ...,  1.2710e-04,
         -2.4044e-04,  6.1709e-05],
        [-1.8939e-05, -1.1817e-05,  6.1244e-06,  ..., -1.6525e-05,
         -5.7891e-06, -1.2264e-05],
        [ 6.6339e-05,  3.8655e-05, -2.4665e-05,  ...,  7.5957e-05,
          3.2238e-05,  3.7441e-05],
        [-3.4720e-05, -2.1741e-05,  1.1265e-05,  ..., -3.0369e-05,
         -1.0349e-05, -2.2426e-05],
        [-4.3869e-05, -2.7478e-05,  1.4186e-05,  ..., -3.8266e-05,
         -1.3217e-05, -2.8312e-05]], device='cuda:0')
Loss: 1.1145546436309814


Running epoch 0, step 535, batch 535
Sampled inputs[:2]: tensor([[    0,  3231,   271,  ...,  9279,  8231, 28871],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9512e-05,  4.3640e-05, -1.3340e-04,  ...,  1.6829e-04,
         -2.7571e-04,  5.4763e-05],
        [-2.1577e-05, -1.3493e-05,  6.9886e-06,  ..., -1.8880e-05,
         -6.6645e-06, -1.4029e-05],
        [ 6.3165e-05,  3.6644e-05, -2.3622e-05,  ...,  7.3125e-05,
          3.1209e-05,  3.5340e-05],
        [-3.9518e-05, -2.4781e-05,  1.2837e-05,  ..., -3.4660e-05,
         -1.1891e-05, -2.5615e-05],
        [-4.9949e-05, -3.1322e-05,  1.6183e-05,  ..., -4.3690e-05,
         -1.5199e-05, -3.2336e-05]], device='cuda:0')
Loss: 1.1217350959777832
Graident accumulation at epoch 0, step 535, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5611e-05,  6.6183e-05, -1.4042e-04,  ...,  8.1952e-05,
         -1.1515e-04,  3.3387e-05],
        [-2.2119e-05, -1.5136e-05,  6.3010e-06,  ..., -1.9211e-05,
         -5.6694e-06, -1.2643e-05],
        [ 1.0226e-04,  7.4238e-05, -2.5359e-05,  ...,  9.0854e-05,
          3.3244e-05,  6.5753e-05],
        [-9.1316e-06, -6.7280e-06,  1.1907e-06,  ..., -5.8698e-06,
         -5.7170e-07, -4.3791e-06],
        [-5.0155e-05, -3.4279e-05,  1.4188e-05,  ..., -4.3518e-05,
         -1.2756e-05, -2.8723e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6021e-08, 1.4614e-08, 1.7419e-08,  ..., 1.9537e-08, 3.0927e-08,
         6.4739e-09],
        [4.8291e-11, 2.7405e-11, 2.2979e-12,  ..., 3.4094e-11, 1.9021e-12,
         8.9243e-12],
        [1.8385e-09, 9.5558e-10, 1.1182e-10,  ..., 1.3912e-09, 1.1361e-10,
         5.6198e-10],
        [4.4956e-10, 3.7190e-10, 3.0849e-11,  ..., 3.6671e-10, 2.1152e-11,
         1.3537e-10],
        [2.0349e-10, 1.0962e-10, 1.0713e-11,  ..., 1.4368e-10, 6.6639e-12,
         3.9212e-11]], device='cuda:0')
optimizer state dict: 67.0
lr: [1.01149609195903e-05, 1.01149609195903e-05]
scheduler_last_epoch: 67


Running epoch 0, step 536, batch 536
Sampled inputs[:2]: tensor([[   0, 1555,   12,  ...,  809,  287,  259],
        [   0, 6275,   12,  ..., 2027, 2887,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8252e-05, -1.9385e-05,  1.5967e-06,  ...,  1.1655e-05,
         -1.0096e-05,  7.5072e-06],
        [-2.6822e-06, -1.7136e-06,  8.9034e-07,  ..., -2.3395e-06,
         -8.2329e-07, -1.7434e-06],
        [-3.3081e-06, -2.1160e-06,  1.0952e-06,  ..., -2.8759e-06,
         -9.9093e-07, -2.1458e-06],
        [-4.9174e-06, -3.1292e-06,  1.6242e-06,  ..., -4.2915e-06,
         -1.4603e-06, -3.1888e-06],
        [-6.2585e-06, -3.9935e-06,  2.0713e-06,  ..., -5.4538e-06,
         -1.8924e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.1379704475402832


Running epoch 0, step 537, batch 537
Sampled inputs[:2]: tensor([[    0,   494,   360,  ...,   391, 24104, 35211],
        [    0,  2827,  5744,  ...,   365,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4922e-05,  3.0193e-05, -5.0189e-05,  ...,  9.7185e-06,
         -7.3642e-05,  3.8190e-05],
        [-5.3942e-06, -3.3900e-06,  1.7732e-06,  ..., -4.7088e-06,
         -1.7658e-06, -3.5837e-06],
        [-6.5863e-06, -4.1574e-06,  2.1681e-06,  ..., -5.7518e-06,
         -2.1085e-06, -4.3660e-06],
        [-9.8348e-06, -6.1691e-06,  3.2261e-06,  ..., -8.5831e-06,
         -3.1143e-06, -6.5118e-06],
        [-1.2457e-05, -7.8380e-06,  4.0978e-06,  ..., -1.0878e-05,
         -4.0084e-06, -8.2552e-06]], device='cuda:0')
Loss: 1.103247880935669


Running epoch 0, step 538, batch 538
Sampled inputs[:2]: tensor([[   0,  767, 9289,  ...,  494,  287, 8957],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3374e-04,  7.0943e-05, -7.7356e-05,  ...,  1.2844e-05,
         -1.3653e-04, -2.9605e-05],
        [-8.0913e-06, -5.0738e-06,  2.6785e-06,  ..., -7.0632e-06,
         -2.5816e-06, -5.3346e-06],
        [-9.8795e-06, -6.2138e-06,  3.2708e-06,  ..., -8.6278e-06,
         -3.0845e-06, -6.4969e-06],
        [-1.4812e-05, -9.2834e-06,  4.9025e-06,  ..., -1.2934e-05,
         -4.5747e-06, -9.7305e-06],
        [-1.8686e-05, -1.1742e-05,  6.1989e-06,  ..., -1.6302e-05,
         -5.8636e-06, -1.2279e-05]], device='cuda:0')
Loss: 1.111510992050171


Running epoch 0, step 539, batch 539
Sampled inputs[:2]: tensor([[    0,  3889,  4039,  ...,   616, 22910,   259],
        [    0,    14,  3948,  ...,   571, 10097,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3182e-04,  5.0250e-05, -9.2855e-05,  ...,  4.3309e-05,
         -1.3335e-04, -5.4940e-05],
        [-1.0744e-05, -6.7130e-06,  3.5316e-06,  ..., -9.4324e-06,
         -3.4496e-06, -7.0632e-06],
        [-1.3158e-05, -8.2403e-06,  4.3213e-06,  ..., -1.1563e-05,
         -4.1276e-06, -8.6278e-06],
        [-1.9699e-05, -1.2308e-05,  6.4746e-06,  ..., -1.7315e-05,
         -6.1169e-06, -1.2904e-05],
        [-2.4945e-05, -1.5616e-05,  8.2105e-06,  ..., -2.1875e-05,
         -7.8604e-06, -1.6332e-05]], device='cuda:0')
Loss: 1.1203489303588867


Running epoch 0, step 540, batch 540
Sampled inputs[:2]: tensor([[   0, 7070,   86,  ...,  298, 4930,  518],
        [   0, 5319,   14,  ..., 2372, 2356, 4093]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0245e-04,  1.3357e-05, -9.7226e-05,  ...,  7.6265e-05,
         -1.4198e-04, -4.2122e-05],
        [-1.3411e-05, -8.3819e-06,  4.3996e-06,  ..., -1.1772e-05,
         -4.2617e-06, -8.8066e-06],
        [-1.6436e-05, -1.0297e-05,  5.3868e-06,  ..., -1.4439e-05,
         -5.0962e-06, -1.0759e-05],
        [-2.4647e-05, -1.5408e-05,  8.0839e-06,  ..., -2.1636e-05,
         -7.5698e-06, -1.6108e-05],
        [-3.1114e-05, -1.9491e-05,  1.0207e-05,  ..., -2.7269e-05,
         -9.6932e-06, -2.0325e-05]], device='cuda:0')
Loss: 1.1229016780853271


Running epoch 0, step 541, batch 541
Sampled inputs[:2]: tensor([[   0,   27,  417,  ...,   18,  365,  806],
        [   0,  493,  221,  ...,  259,  726, 2786]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1929e-05, -6.4540e-06, -1.1887e-04,  ...,  1.2551e-04,
         -1.8342e-04, -5.0438e-05],
        [-1.6078e-05, -1.0051e-05,  5.2974e-06,  ..., -1.4111e-05,
         -5.0962e-06, -1.0587e-05],
        [-1.9729e-05, -1.2368e-05,  6.4969e-06,  ..., -1.7345e-05,
         -6.1020e-06, -1.2949e-05],
        [-2.9594e-05, -1.8507e-05,  9.7454e-06,  ..., -2.5988e-05,
         -9.0748e-06, -1.9401e-05],
        [-3.7372e-05, -2.3425e-05,  1.2308e-05,  ..., -3.2783e-05,
         -1.1630e-05, -2.4498e-05]], device='cuda:0')
Loss: 1.125179409980774


Running epoch 0, step 542, batch 542
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,   271, 16217,  ...,  6352,  4546,  2558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8953e-05,  5.0286e-06, -1.8498e-04,  ...,  7.5875e-05,
         -1.5522e-04, -6.7075e-05],
        [-1.8775e-05, -1.1712e-05,  6.1616e-06,  ..., -1.6466e-05,
         -6.0089e-06, -1.2383e-05],
        [-2.2963e-05, -1.4365e-05,  7.5325e-06,  ..., -2.0161e-05,
         -7.1749e-06, -1.5095e-05],
        [-3.4481e-05, -2.1517e-05,  1.1310e-05,  ..., -3.0249e-05,
         -1.0684e-05, -2.2635e-05],
        [-4.3541e-05, -2.7224e-05,  1.4275e-05,  ..., -3.8147e-05,
         -1.3672e-05, -2.8580e-05]], device='cuda:0')
Loss: 1.139102578163147


Running epoch 0, step 543, batch 543
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   292,   334,   674],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8064e-05,  9.2877e-05, -2.4363e-04,  ...,  6.3437e-05,
         -1.8729e-04, -2.3381e-05],
        [-2.1458e-05, -1.3374e-05,  7.0110e-06,  ..., -1.8820e-05,
         -6.8508e-06, -1.4171e-05],
        [-2.6211e-05, -1.6391e-05,  8.5607e-06,  ..., -2.3022e-05,
         -8.1807e-06, -1.7256e-05],
        [-3.9399e-05, -2.4572e-05,  1.2867e-05,  ..., -3.4571e-05,
         -1.2189e-05, -2.5913e-05],
        [-4.9680e-05, -3.1039e-05,  1.6227e-05,  ..., -4.3541e-05,
         -1.5572e-05, -3.2663e-05]], device='cuda:0')
Loss: 1.1223819255828857
Graident accumulation at epoch 0, step 543, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7243e-05,  6.8853e-05, -1.5074e-04,  ...,  8.0101e-05,
         -1.2236e-04,  2.7710e-05],
        [-2.2053e-05, -1.4960e-05,  6.3720e-06,  ..., -1.9172e-05,
         -5.7875e-06, -1.2795e-05],
        [ 8.9416e-05,  6.5175e-05, -2.1967e-05,  ...,  7.9466e-05,
          2.9102e-05,  5.7452e-05],
        [-1.2158e-05, -8.5124e-06,  2.3584e-06,  ..., -8.7399e-06,
         -1.7334e-06, -6.5325e-06],
        [-5.0107e-05, -3.3955e-05,  1.4392e-05,  ..., -4.3521e-05,
         -1.3037e-05, -2.9117e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5977e-08, 1.4608e-08, 1.7461e-08,  ..., 1.9521e-08, 3.0931e-08,
         6.4680e-09],
        [4.8703e-11, 2.7557e-11, 2.3448e-12,  ..., 3.4414e-11, 1.9471e-12,
         9.1162e-12],
        [1.8373e-09, 9.5489e-10, 1.1178e-10,  ..., 1.3904e-09, 1.1356e-10,
         5.6172e-10],
        [4.5066e-10, 3.7213e-10, 3.0984e-11,  ..., 3.6753e-10, 2.1280e-11,
         1.3591e-10],
        [2.0576e-10, 1.1047e-10, 1.0966e-11,  ..., 1.4543e-10, 6.8997e-12,
         4.0239e-11]], device='cuda:0')
optimizer state dict: 68.0
lr: [9.867734078748245e-06, 9.867734078748245e-06]
scheduler_last_epoch: 68


Running epoch 0, step 544, batch 544
Sampled inputs[:2]: tensor([[   0, 2612,  271,  ...,  369, 9862,  287],
        [   0,  756,  401,  ..., 8385, 1004,  775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1127e-05,  1.7225e-05,  3.1780e-05,  ...,  9.1790e-06,
         -4.8081e-05, -4.0623e-05],
        [-2.5779e-06, -1.6540e-06,  8.2329e-07,  ..., -2.2650e-06,
         -8.1584e-07, -1.7285e-06],
        [-3.1888e-06, -2.0564e-06,  1.0207e-06,  ..., -2.8163e-06,
         -9.7603e-07, -2.1309e-06],
        [-4.7982e-06, -3.0845e-06,  1.5423e-06,  ..., -4.2319e-06,
         -1.4603e-06, -3.2037e-06],
        [-5.9605e-06, -3.8147e-06,  1.9073e-06,  ..., -5.2452e-06,
         -1.8403e-06, -3.9637e-06]], device='cuda:0')
Loss: 1.09877610206604


Running epoch 0, step 545, batch 545
Sampled inputs[:2]: tensor([[    0,   271, 10474,  ...,   298,  2286,    29],
        [    0,  1041,    14,  ...,   360,   266, 14966]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4700e-05,  4.8477e-05,  3.2326e-05,  ...,  5.4808e-05,
         -7.9303e-05, -1.1965e-05],
        [-5.1856e-06, -3.2783e-06,  1.6913e-06,  ..., -4.5449e-06,
         -1.7025e-06, -3.4645e-06],
        [-6.4373e-06, -4.0829e-06,  2.1085e-06,  ..., -5.6624e-06,
         -2.0564e-06, -4.2915e-06],
        [-9.6560e-06, -6.1244e-06,  3.1665e-06,  ..., -8.4937e-06,
         -3.0547e-06, -6.4224e-06],
        [-1.2040e-05, -7.5996e-06,  3.9339e-06,  ..., -1.0550e-05,
         -3.8669e-06, -7.9870e-06]], device='cuda:0')
Loss: 1.1193790435791016


Running epoch 0, step 546, batch 546
Sampled inputs[:2]: tensor([[   0,  298,  374,  ...,  298,  413,   28],
        [   0,  352,  927,  ..., 1521, 3513,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7717e-06,  1.9389e-04, -3.5853e-05,  ...,  7.5992e-05,
         -9.3092e-05,  7.8075e-05],
        [-7.8529e-06, -4.9397e-06,  2.5593e-06,  ..., -6.8843e-06,
         -2.6710e-06, -5.2527e-06],
        [-9.6411e-06, -6.0797e-06,  3.1516e-06,  ..., -8.4788e-06,
         -3.1888e-06, -6.4373e-06],
        [-1.4484e-05, -9.1195e-06,  4.7386e-06,  ..., -1.2726e-05,
         -4.7386e-06, -9.6411e-06],
        [-1.8001e-05, -1.1310e-05,  5.8711e-06,  ..., -1.5765e-05,
         -5.9828e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1071953773498535


Running epoch 0, step 547, batch 547
Sampled inputs[:2]: tensor([[    0,  3699,  3058,  ...,   820,  5327,  8055],
        [    0,   266, 11692,  ...,   278, 14620, 12718]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8548e-05,  1.5753e-04,  3.3981e-06,  ...,  3.0165e-05,
         -3.4452e-05,  2.4355e-05],
        [-1.0476e-05, -6.5863e-06,  3.4124e-06,  ..., -9.2089e-06,
         -3.5241e-06, -6.9812e-06],
        [-1.2875e-05, -8.1211e-06,  4.2021e-06,  ..., -1.1340e-05,
         -4.2170e-06, -8.5682e-06],
        [-1.9312e-05, -1.2159e-05,  6.3106e-06,  ..., -1.7017e-05,
         -6.2734e-06, -1.2815e-05],
        [-2.4110e-05, -1.5154e-05,  7.8529e-06,  ..., -2.1189e-05,
         -7.9349e-06, -1.5974e-05]], device='cuda:0')
Loss: 1.1305837631225586


Running epoch 0, step 548, batch 548
Sampled inputs[:2]: tensor([[    0, 13856,   278,  ...,    14,    69,   462],
        [    0,   437,   266,  ...,  5512,   822,    89]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1087e-04,  1.7146e-04,  2.5247e-05,  ...,  2.4151e-05,
         -2.4898e-05,  5.7199e-05],
        [-1.3128e-05, -8.2180e-06,  4.2766e-06,  ..., -1.1548e-05,
         -4.4219e-06, -8.7321e-06],
        [-1.6123e-05, -1.0133e-05,  5.2601e-06,  ..., -1.4201e-05,
         -5.2899e-06, -1.0699e-05],
        [-2.4170e-05, -1.5154e-05,  7.8976e-06,  ..., -2.1309e-05,
         -7.8678e-06, -1.6004e-05],
        [-3.0220e-05, -1.8910e-05,  9.8348e-06,  ..., -2.6554e-05,
         -9.9614e-06, -1.9968e-05]], device='cuda:0')
Loss: 1.1113321781158447


Running epoch 0, step 549, batch 549
Sampled inputs[:2]: tensor([[    0,  1197, 10640,  ...,  2405,   437,  5880],
        [    0,   694,   266,  ...,  3007,   300,  5726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2851e-04,  1.3053e-04,  1.9906e-05,  ...,  6.6969e-05,
         -2.3754e-05,  6.7391e-05],
        [-1.5736e-05, -9.8422e-06,  5.1297e-06,  ..., -1.3828e-05,
         -5.2266e-06, -1.0408e-05],
        [-1.9386e-05, -1.2159e-05,  6.3255e-06,  ..., -1.7062e-05,
         -6.2659e-06, -1.2785e-05],
        [-2.9087e-05, -1.8224e-05,  9.5069e-06,  ..., -2.5660e-05,
         -9.3356e-06, -1.9163e-05],
        [-3.6299e-05, -2.2694e-05,  1.1817e-05,  ..., -3.1888e-05,
         -1.1794e-05, -2.3872e-05]], device='cuda:0')
Loss: 1.1200978755950928


Running epoch 0, step 550, batch 550
Sampled inputs[:2]: tensor([[    0,   259,  2416,  ..., 14474,    12,   259],
        [    0,   266,  4411,  ...,   368,  6388,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5884e-04, -4.1382e-06,  3.6564e-05,  ...,  1.1192e-04,
         -9.9360e-05, -5.3185e-05],
        [-1.8269e-05, -1.1444e-05,  5.9642e-06,  ..., -1.6093e-05,
         -5.9642e-06, -1.2070e-05],
        [-2.2545e-05, -1.4156e-05,  7.3686e-06,  ..., -1.9893e-05,
         -7.1563e-06, -1.4856e-05],
        [-3.3945e-05, -2.1294e-05,  1.1116e-05,  ..., -3.0011e-05,
         -1.0684e-05, -2.2337e-05],
        [-4.2200e-05, -2.6420e-05,  1.3754e-05,  ..., -3.7163e-05,
         -1.3463e-05, -2.7716e-05]], device='cuda:0')
Loss: 1.1112874746322632


Running epoch 0, step 551, batch 551
Sampled inputs[:2]: tensor([[    0,  3227,   278,  ...,  2950,    14, 15544],
        [    0,   287,  3284,  ...,   221,   493,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9201e-04, -4.9459e-05,  2.2933e-05,  ...,  1.5213e-04,
         -1.2266e-04, -8.0188e-05],
        [-2.0906e-05, -1.3061e-05,  6.8136e-06,  ..., -1.8403e-05,
         -6.7614e-06, -1.3769e-05],
        [-2.5854e-05, -1.6198e-05,  8.4341e-06,  ..., -2.2799e-05,
         -8.1323e-06, -1.6987e-05],
        [-3.8922e-05, -2.4363e-05,  1.2718e-05,  ..., -3.4362e-05,
         -1.2137e-05, -2.5526e-05],
        [-4.8429e-05, -3.0264e-05,  1.5765e-05,  ..., -4.2617e-05,
         -1.5311e-05, -3.1710e-05]], device='cuda:0')
Loss: 1.1127943992614746
Graident accumulation at epoch 0, step 551, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3174e-06,  5.7022e-05, -1.3338e-04,  ...,  8.7304e-05,
         -1.2239e-04,  1.6920e-05],
        [-2.1939e-05, -1.4770e-05,  6.4162e-06,  ..., -1.9095e-05,
         -5.8849e-06, -1.2893e-05],
        [ 7.7889e-05,  5.7038e-05, -1.8927e-05,  ...,  6.9240e-05,
          2.5378e-05,  5.0008e-05],
        [-1.4835e-05, -1.0098e-05,  3.3944e-06,  ..., -1.1302e-05,
         -2.7738e-06, -8.4319e-06],
        [-4.9939e-05, -3.3586e-05,  1.4530e-05,  ..., -4.3430e-05,
         -1.3265e-05, -2.9377e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5968e-08, 1.4595e-08, 1.7444e-08,  ..., 1.9525e-08, 3.0915e-08,
         6.4679e-09],
        [4.9091e-11, 2.7700e-11, 2.3889e-12,  ..., 3.4718e-11, 1.9909e-12,
         9.2966e-12],
        [1.8361e-09, 9.5420e-10, 1.1174e-10,  ..., 1.3895e-09, 1.1351e-10,
         5.6144e-10],
        [4.5173e-10, 3.7235e-10, 3.1114e-11,  ..., 3.6835e-10, 2.1406e-11,
         1.3643e-10],
        [2.0790e-10, 1.1128e-10, 1.1203e-11,  ..., 1.4710e-10, 7.1272e-12,
         4.1205e-11]], device='cuda:0')
optimizer state dict: 69.0
lr: [9.620588080367043e-06, 9.620588080367043e-06]
scheduler_last_epoch: 69


Running epoch 0, step 552, batch 552
Sampled inputs[:2]: tensor([[    0,  1167,   278,  ...,   278,  1853,  1424],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8015e-05, -7.9842e-05, -3.9184e-05,  ...,  3.5760e-05,
         -7.5936e-06, -3.8738e-05],
        [-2.4885e-06, -1.6019e-06,  8.4564e-07,  ..., -2.2054e-06,
         -8.0839e-07, -1.6615e-06],
        [-3.1441e-06, -2.0266e-06,  1.0654e-06,  ..., -2.7865e-06,
         -9.7603e-07, -2.0862e-06],
        [-4.7088e-06, -3.0398e-06,  1.6019e-06,  ..., -4.2021e-06,
         -1.4529e-06, -3.1292e-06],
        [-5.7817e-06, -3.7104e-06,  1.9521e-06,  ..., -5.1260e-06,
         -1.8179e-06, -3.8147e-06]], device='cuda:0')
Loss: 1.1274206638336182


Running epoch 0, step 553, batch 553
Sampled inputs[:2]: tensor([[   0, 1927,  287,  ..., 1027,  271,  266],
        [   0,  328,  266,  ...,  271,  706,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5489e-05, -1.0021e-04, -7.1882e-05,  ...,  3.9665e-05,
         -3.8146e-05, -3.1918e-05],
        [-5.0217e-06, -3.2261e-06,  1.6838e-06,  ..., -4.4554e-06,
         -1.6689e-06, -3.3900e-06],
        [-6.3777e-06, -4.0978e-06,  2.1383e-06,  ..., -5.6624e-06,
         -2.0340e-06, -4.2766e-06],
        [-9.5069e-06, -6.1095e-06,  3.1888e-06,  ..., -8.4639e-06,
         -3.0026e-06, -6.3777e-06],
        [-1.1653e-05, -7.4655e-06,  3.8892e-06,  ..., -1.0341e-05,
         -3.7402e-06, -7.7784e-06]], device='cuda:0')
Loss: 1.0898408889770508


Running epoch 0, step 554, batch 554
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3203e-05, -5.6755e-05, -5.8741e-05,  ..., -5.1316e-05,
         -5.0278e-05, -5.8567e-05],
        [-7.5698e-06, -4.8131e-06,  2.5108e-06,  ..., -6.6906e-06,
         -2.5257e-06, -5.0738e-06],
        [-9.5665e-06, -6.0797e-06,  3.1665e-06,  ..., -8.4490e-06,
         -3.0696e-06, -6.3628e-06],
        [-1.4275e-05, -9.0748e-06,  4.7237e-06,  ..., -1.2636e-05,
         -4.5374e-06, -9.4920e-06],
        [-1.7524e-05, -1.1101e-05,  5.7742e-06,  ..., -1.5467e-05,
         -5.6624e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.0955265760421753


Running epoch 0, step 555, batch 555
Sampled inputs[:2]: tensor([[    0,   413,    28,  ...,   328, 37605,  6499],
        [    0,   494,   221,  ...,   298,  1062,  4923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2219e-04,  8.1475e-05, -1.2574e-04,  ...,  4.8788e-05,
         -1.1189e-04, -3.5390e-05],
        [-1.0103e-05, -6.4299e-06,  3.4012e-06,  ..., -9.0003e-06,
         -3.6210e-06, -6.8843e-06],
        [-1.2711e-05, -8.0913e-06,  4.2692e-06,  ..., -1.1310e-05,
         -4.3884e-06, -8.5980e-06],
        [-1.8835e-05, -1.1981e-05,  6.3181e-06,  ..., -1.6779e-05,
         -6.4373e-06, -1.2726e-05],
        [-2.3246e-05, -1.4752e-05,  7.7710e-06,  ..., -2.0653e-05,
         -8.0615e-06, -1.5676e-05]], device='cuda:0')
Loss: 1.124332070350647


Running epoch 0, step 556, batch 556
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1450e-04,  2.3345e-04, -2.1055e-04,  ...,  1.1270e-04,
         -1.9295e-04,  1.9185e-05],
        [-1.2696e-05, -8.0466e-06,  4.3064e-06,  ..., -1.1310e-05,
         -4.6715e-06, -8.7023e-06],
        [-1.5885e-05, -1.0073e-05,  5.3793e-06,  ..., -1.4141e-05,
         -5.6326e-06, -1.0818e-05],
        [-2.3514e-05, -1.4916e-05,  7.9572e-06,  ..., -2.0951e-05,
         -8.2627e-06, -1.5989e-05],
        [-2.8998e-05, -1.8343e-05,  9.7826e-06,  ..., -2.5749e-05,
         -1.0327e-05, -1.9699e-05]], device='cuda:0')
Loss: 1.1092777252197266


Running epoch 0, step 557, batch 557
Sampled inputs[:2]: tensor([[    0,   720,  1122,  ...,   656,   287, 14258],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0135e-04,  1.8416e-04, -2.2434e-04,  ...,  1.3490e-04,
         -3.1022e-04,  3.4232e-05],
        [-1.5229e-05, -9.6112e-06,  5.1744e-06,  ..., -1.3530e-05,
         -5.4426e-06, -1.0341e-05],
        [-1.9133e-05, -1.2085e-05,  6.4895e-06,  ..., -1.6987e-05,
         -6.5938e-06, -1.2919e-05],
        [-2.8491e-05, -1.7986e-05,  9.6560e-06,  ..., -2.5302e-05,
         -9.7156e-06, -1.9178e-05],
        [-3.4928e-05, -2.1994e-05,  1.1809e-05,  ..., -3.0935e-05,
         -1.2092e-05, -2.3514e-05]], device='cuda:0')
Loss: 1.0869066715240479


Running epoch 0, step 558, batch 558
Sampled inputs[:2]: tensor([[   0, 2319,   30,  ...,  508, 6703,   12],
        [   0, 1732,  699,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1384e-04,  2.4671e-04, -2.5457e-04,  ...,  1.5186e-04,
         -3.7525e-04,  4.6004e-05],
        [-1.7717e-05, -1.1198e-05,  6.0201e-06,  ..., -1.5795e-05,
         -6.1579e-06, -1.1951e-05],
        [-2.2307e-05, -1.4111e-05,  7.5698e-06,  ..., -1.9878e-05,
         -7.4804e-06, -1.4976e-05],
        [-3.3349e-05, -2.1100e-05,  1.1317e-05,  ..., -2.9743e-05,
         -1.1064e-05, -2.2322e-05],
        [-4.0710e-05, -2.5690e-05,  1.3776e-05,  ..., -3.6180e-05,
         -1.3717e-05, -2.7239e-05]], device='cuda:0')
Loss: 1.108389139175415


Running epoch 0, step 559, batch 559
Sampled inputs[:2]: tensor([[   0,   21,   13,  ...,   14,  747,  806],
        [   0,  278, 6481,  ...,   13, 8970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9525e-05,  2.1422e-04, -2.8407e-04,  ...,  1.5992e-04,
         -4.4022e-04,  8.0771e-05],
        [-2.0206e-05, -1.2770e-05,  6.8285e-06,  ..., -1.7986e-05,
         -6.8769e-06, -1.3545e-05],
        [-2.5496e-05, -1.6138e-05,  8.6054e-06,  ..., -2.2680e-05,
         -8.3707e-06, -1.7017e-05],
        [-3.8147e-05, -2.4155e-05,  1.2875e-05,  ..., -3.3975e-05,
         -1.2390e-05, -2.5392e-05],
        [-4.6641e-05, -2.9460e-05,  1.5698e-05,  ..., -4.1395e-05,
         -1.5385e-05, -3.1024e-05]], device='cuda:0')
Loss: 1.1076703071594238
Graident accumulation at epoch 0, step 559, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2738e-05,  7.2741e-05, -1.4844e-04,  ...,  9.4565e-05,
         -1.5417e-04,  2.3305e-05],
        [-2.1765e-05, -1.4570e-05,  6.4574e-06,  ..., -1.8984e-05,
         -5.9841e-06, -1.2958e-05],
        [ 6.7551e-05,  4.9720e-05, -1.6173e-05,  ...,  6.0048e-05,
          2.2003e-05,  4.3306e-05],
        [-1.7166e-05, -1.1503e-05,  4.3424e-06,  ..., -1.3569e-05,
         -3.7355e-06, -1.0128e-05],
        [-4.9610e-05, -3.3173e-05,  1.4646e-05,  ..., -4.3227e-05,
         -1.3477e-05, -2.9541e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5928e-08, 1.4627e-08, 1.7508e-08,  ..., 1.9531e-08, 3.1078e-08,
         6.4680e-09],
        [4.9450e-11, 2.7835e-11, 2.4331e-12,  ..., 3.5007e-11, 2.0362e-12,
         9.4708e-12],
        [1.8350e-09, 9.5350e-10, 1.1170e-10,  ..., 1.3886e-09, 1.1347e-10,
         5.6117e-10],
        [4.5273e-10, 3.7256e-10, 3.1249e-11,  ..., 3.6913e-10, 2.1538e-11,
         1.3693e-10],
        [2.0986e-10, 1.1204e-10, 1.1439e-11,  ..., 1.4867e-10, 7.3568e-12,
         4.2126e-11]], device='cuda:0')
optimizer state dict: 70.0
lr: [9.373673982939395e-06, 9.373673982939395e-06]
scheduler_last_epoch: 70


Running epoch 0, step 560, batch 560
Sampled inputs[:2]: tensor([[   0,  259, 6022,  ..., 1871, 1209, 1241],
        [   0, 1765, 5370,  ..., 1711,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5012e-05, -9.9622e-06,  2.8259e-05,  ...,  2.0371e-05,
         -2.1912e-05,  1.5916e-05],
        [-2.4587e-06, -1.5944e-06,  8.4564e-07,  ..., -2.1756e-06,
         -7.5996e-07, -1.6093e-06],
        [ 8.1534e-05,  4.6226e-05, -4.5588e-05,  ...,  1.0638e-04,
          3.1096e-05,  7.5159e-05],
        [-4.7684e-06, -3.0845e-06,  1.6317e-06,  ..., -4.2319e-06,
         -1.4082e-06, -3.0845e-06],
        [-5.7817e-06, -3.7402e-06,  1.9670e-06,  ..., -5.1260e-06,
         -1.7360e-06, -3.7402e-06]], device='cuda:0')
Loss: 1.1306686401367188


Running epoch 0, step 561, batch 561
Sampled inputs[:2]: tensor([[    0,   391,  4356,  ...,   287, 32873,  5362],
        [    0,   474,   706,  ...,    83, 38084,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6426e-05,  1.2974e-04, -8.9634e-06,  ...,  9.6164e-06,
         -3.9831e-05,  1.8250e-05],
        [-4.9323e-06, -3.1888e-06,  1.6913e-06,  ..., -4.3958e-06,
         -1.7583e-06, -3.3528e-06],
        [ 7.8450e-05,  4.4244e-05, -4.4530e-05,  ...,  1.0361e-04,
          2.9904e-05,  7.2999e-05],
        [-9.2685e-06, -5.9903e-06,  3.1814e-06,  ..., -8.2850e-06,
         -3.1367e-06, -6.2287e-06],
        [-1.1355e-05, -7.3165e-06,  3.8743e-06,  ..., -1.0103e-05,
         -3.8967e-06, -7.6145e-06]], device='cuda:0')
Loss: 1.0971189737319946


Running epoch 0, step 562, batch 562
Sampled inputs[:2]: tensor([[    0,   409,  3669,  ...,    12,   374,    20],
        [    0,    67,   695,  ...,   437,   266, 44563]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3631e-05,  1.1474e-04, -2.3982e-05,  ...,  2.2747e-06,
         -6.3407e-05,  3.5115e-05],
        [-7.3314e-06, -4.7609e-06,  2.4848e-06,  ..., -6.5267e-06,
         -2.4624e-06, -4.9025e-06],
        [ 7.5276e-05,  4.2158e-05, -4.3479e-05,  ...,  1.0078e-04,
          2.9002e-05,  7.0957e-05],
        [-1.3977e-05, -9.0897e-06,  4.7460e-06,  ..., -1.2487e-05,
         -4.4554e-06, -9.2536e-06],
        [-1.7047e-05, -1.1057e-05,  5.7593e-06,  ..., -1.5169e-05,
         -5.5283e-06, -1.1265e-05]], device='cuda:0')
Loss: 1.1005632877349854


Running epoch 0, step 563, batch 563
Sampled inputs[:2]: tensor([[    0,  1016,  1387,  ..., 12156, 14838,  3550],
        [    0,  2228,  1416,  ...,  3766,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6601e-05,  1.4311e-04, -5.4007e-05,  ...,  3.5507e-05,
         -2.5815e-05,  1.0039e-05],
        [-9.7901e-06, -6.3255e-06,  3.3267e-06,  ..., -8.7321e-06,
         -3.3192e-06, -6.5640e-06],
        [ 7.2161e-05,  4.0161e-05, -4.2414e-05,  ...,  9.7975e-05,
          2.7952e-05,  6.8856e-05],
        [-1.8686e-05, -1.2085e-05,  6.3628e-06,  ..., -1.6719e-05,
         -6.0201e-06, -1.2428e-05],
        [-2.2620e-05, -1.4603e-05,  7.6666e-06,  ..., -2.0176e-05,
         -7.4133e-06, -1.5020e-05]], device='cuda:0')
Loss: 1.1171553134918213


Running epoch 0, step 564, batch 564
Sampled inputs[:2]: tensor([[   0,  278,  634,  ...,  598, 1722,  591],
        [   0, 1603,  694,  ...,   36,   18,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2106e-05,  2.1466e-04, -9.5425e-05,  ...,  5.9679e-05,
         -3.9602e-05,  4.3889e-05],
        [-1.2264e-05, -7.9274e-06,  4.1835e-06,  ..., -1.0952e-05,
         -4.2133e-06, -8.2701e-06],
        [ 6.9032e-05,  3.8135e-05, -4.1326e-05,  ...,  9.5158e-05,
          2.6864e-05,  6.6710e-05],
        [-2.3305e-05, -1.5065e-05,  7.9721e-06,  ..., -2.0862e-05,
         -7.6070e-06, -1.5587e-05],
        [-2.8193e-05, -1.8209e-05,  9.6187e-06,  ..., -2.5183e-05,
         -9.3505e-06, -1.8835e-05]], device='cuda:0')
Loss: 1.1053887605667114


Running epoch 0, step 565, batch 565
Sampled inputs[:2]: tensor([[    0, 11661,    12,  ...,  1707,   394,   264],
        [    0,  1295,  1178,  ...,  4808,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7895e-05,  1.5776e-04, -1.5953e-04,  ...,  1.2745e-04,
         -5.2001e-05,  2.7535e-05],
        [-1.4678e-05, -9.4771e-06,  5.0068e-06,  ..., -1.3098e-05,
         -4.9211e-06, -9.8199e-06],
        [ 6.5843e-05,  3.6093e-05, -4.0245e-05,  ...,  9.2327e-05,
          2.5970e-05,  6.4684e-05],
        [-2.8044e-05, -1.8105e-05,  9.5814e-06,  ..., -2.5094e-05,
         -8.9258e-06, -1.8597e-05],
        [-3.3885e-05, -2.1845e-05,  1.1556e-05,  ..., -3.0249e-05,
         -1.0967e-05, -2.2456e-05]], device='cuda:0')
Loss: 1.1065622568130493


Running epoch 0, step 566, batch 566
Sampled inputs[:2]: tensor([[   0,   14,  381,  ..., 2195,  278,  266],
        [   0, 3448,  278,  ...,  380,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3015e-05,  1.8184e-04, -2.3940e-04,  ...,  1.2204e-04,
         -2.5693e-06,  4.5454e-05],
        [-1.7092e-05, -1.1042e-05,  5.8189e-06,  ..., -1.5244e-05,
         -5.6066e-06, -1.1370e-05],
        [ 6.2714e-05,  3.4052e-05, -3.9187e-05,  ...,  8.9541e-05,
          2.5117e-05,  6.2687e-05],
        [-3.2753e-05, -2.1160e-05,  1.1176e-05,  ..., -2.9296e-05,
         -1.0185e-05, -2.1607e-05],
        [-3.9488e-05, -2.5466e-05,  1.3441e-05,  ..., -3.5226e-05,
         -1.2502e-05, -2.6017e-05]], device='cuda:0')
Loss: 1.104485034942627


Running epoch 0, step 567, batch 567
Sampled inputs[:2]: tensor([[    0, 41010,  6737,  ...,   963,   409,   382],
        [    0,   292, 15156,  ...,    35,  3815,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1170e-05,  2.5195e-04, -2.4406e-04,  ...,  1.5416e-04,
         -3.8019e-05,  1.1725e-04],
        [-1.9550e-05, -1.2599e-05,  6.6832e-06,  ..., -1.7449e-05,
         -6.4597e-06, -1.2979e-05],
        [ 5.9585e-05,  3.2070e-05, -3.8085e-05,  ...,  8.6724e-05,
          2.4081e-05,  6.0646e-05],
        [-3.7402e-05, -2.4095e-05,  1.2808e-05,  ..., -3.3468e-05,
         -1.1705e-05, -2.4632e-05],
        [-4.5121e-05, -2.9027e-05,  1.5423e-05,  ..., -4.0293e-05,
         -1.4387e-05, -2.9683e-05]], device='cuda:0')
Loss: 1.1078875064849854
Graident accumulation at epoch 0, step 567, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0153, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3474e-06,  9.0662e-05, -1.5801e-04,  ...,  1.0052e-04,
         -1.4256e-04,  3.2700e-05],
        [-2.1544e-05, -1.4373e-05,  6.4800e-06,  ..., -1.8830e-05,
         -6.0317e-06, -1.2960e-05],
        [ 6.6754e-05,  4.7955e-05, -1.8365e-05,  ...,  6.2716e-05,
          2.2211e-05,  4.5040e-05],
        [-1.9189e-05, -1.2762e-05,  5.1889e-06,  ..., -1.5559e-05,
         -4.5324e-06, -1.1578e-05],
        [-4.9161e-05, -3.2759e-05,  1.4724e-05,  ..., -4.2933e-05,
         -1.3568e-05, -2.9555e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5885e-08, 1.4676e-08, 1.7550e-08,  ..., 1.9535e-08, 3.1049e-08,
         6.4753e-09],
        [4.9783e-11, 2.7966e-11, 2.4754e-12,  ..., 3.5277e-11, 2.0758e-12,
         9.6298e-12],
        [1.8367e-09, 9.5358e-10, 1.1304e-10,  ..., 1.3948e-09, 1.1393e-10,
         5.6429e-10],
        [4.5368e-10, 3.7277e-10, 3.1382e-11,  ..., 3.6988e-10, 2.1653e-11,
         1.3740e-10],
        [2.1169e-10, 1.1277e-10, 1.1665e-11,  ..., 1.5014e-10, 7.5565e-12,
         4.2965e-11]], device='cuda:0')
optimizer state dict: 71.0
lr: [9.12714270321745e-06, 9.12714270321745e-06]
scheduler_last_epoch: 71


Running epoch 0, step 568, batch 568
Sampled inputs[:2]: tensor([[    0,   221,   264,  ...,  3613,  3222, 14000],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5380e-05,  6.9164e-05, -5.7979e-05,  ...,  4.3919e-05,
         -2.4877e-05,  1.0207e-04],
        [-2.4140e-06, -1.5572e-06,  8.7172e-07,  ..., -2.1905e-06,
         -9.2760e-07, -1.6168e-06],
        [-3.1441e-06, -2.0266e-06,  1.1325e-06,  ..., -2.8461e-06,
         -1.1548e-06, -2.0862e-06],
        [-4.6194e-06, -2.9653e-06,  1.6615e-06,  ..., -4.1723e-06,
         -1.6764e-06, -3.0547e-06],
        [-5.4836e-06, -3.5316e-06,  1.9819e-06,  ..., -4.9472e-06,
         -2.0266e-06, -3.6359e-06]], device='cuda:0')
Loss: 1.110721468925476


Running epoch 0, step 569, batch 569
Sampled inputs[:2]: tensor([[    0,   680,   271,  ..., 12942,   271,   266],
        [    0,  1477,  3205,  ...,  6441,  9363,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0261e-05,  2.6737e-05, -8.7142e-05,  ...,  4.2453e-05,
         -4.0783e-05,  7.6354e-05],
        [-4.7535e-06, -3.0845e-06,  1.7062e-06,  ..., -4.2766e-06,
         -1.6429e-06, -3.0845e-06],
        [-6.2585e-06, -4.0680e-06,  2.2352e-06,  ..., -5.6326e-06,
         -2.0564e-06, -4.0233e-06],
        [-9.2089e-06, -5.9754e-06,  3.2857e-06,  ..., -8.2850e-06,
         -2.9951e-06, -5.9009e-06],
        [-1.1027e-05, -7.1377e-06,  3.9339e-06,  ..., -9.8944e-06,
         -3.6508e-06, -7.0632e-06]], device='cuda:0')
Loss: 1.120605707168579


Running epoch 0, step 570, batch 570
Sampled inputs[:2]: tensor([[   0,  944,  278,  ..., 5755,  292,  221],
        [   0, 3019,  278,  ...,  365, 1770,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2809e-05, -2.3492e-05, -6.3736e-05,  ...,  5.3030e-05,
         -4.7773e-05, -7.8978e-06],
        [-7.1228e-06, -4.5896e-06,  2.5555e-06,  ..., -6.3926e-06,
         -2.4028e-06, -4.6790e-06],
        [-9.3877e-06, -6.0499e-06,  3.3528e-06,  ..., -8.4341e-06,
         -3.0175e-06, -6.1095e-06],
        [-1.3798e-05, -8.8960e-06,  4.9248e-06,  ..., -1.2398e-05,
         -4.3958e-06, -8.9705e-06],
        [-1.6510e-05, -1.0625e-05,  5.9009e-06,  ..., -1.4812e-05,
         -5.3570e-06, -1.0729e-05]], device='cuda:0')
Loss: 1.1100540161132812


Running epoch 0, step 571, batch 571
Sampled inputs[:2]: tensor([[   0, 1500,  367,  ...,  344, 4250,  287],
        [   0, 1042, 2548,  ...,  328,  259, 2771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3296e-04,  1.1858e-05,  7.3541e-06,  ...,  3.8053e-05,
         -6.1124e-06,  2.5571e-05],
        [-9.5516e-06, -6.1542e-06,  3.3714e-06,  ..., -8.5831e-06,
         -3.2634e-06, -6.2808e-06],
        [-1.2532e-05, -8.0764e-06,  4.4107e-06,  ..., -1.1265e-05,
         -4.0978e-06, -8.1807e-06],
        [-1.8388e-05, -1.1861e-05,  6.4671e-06,  ..., -1.6540e-05,
         -5.9530e-06, -1.1981e-05],
        [-2.2024e-05, -1.4171e-05,  7.7486e-06,  ..., -1.9759e-05,
         -7.2569e-06, -1.4335e-05]], device='cuda:0')
Loss: 1.1125344038009644


Running epoch 0, step 572, batch 572
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  391,  721, 1576],
        [   0, 2626,   13,  ...,  300,  369,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3589e-04,  2.2851e-05, -1.6263e-05,  ...,  8.4789e-05,
         -5.0269e-06, -4.6020e-07],
        [-1.1921e-05, -7.7337e-06,  4.2059e-06,  ..., -1.0699e-05,
         -3.9861e-06, -7.7859e-06],
        [-1.5676e-05, -1.0177e-05,  5.5134e-06,  ..., -1.4067e-05,
         -5.0142e-06, -1.0163e-05],
        [-2.3067e-05, -1.4976e-05,  8.1062e-06,  ..., -2.0713e-05,
         -7.3016e-06, -1.4931e-05],
        [-2.7508e-05, -1.7822e-05,  9.6709e-06,  ..., -2.4647e-05,
         -8.8736e-06, -1.7807e-05]], device='cuda:0')
Loss: 1.106059193611145


Running epoch 0, step 573, batch 573
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0,   709,   630,  ...,  6263,   409,   508]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7683e-05,  5.9064e-06, -8.9734e-05,  ...,  1.4159e-04,
         -6.1108e-05, -1.6549e-07],
        [-1.4231e-05, -9.2387e-06,  5.0105e-06,  ..., -1.2770e-05,
         -4.6603e-06, -9.2760e-06],
        [-1.8761e-05, -1.2189e-05,  6.5863e-06,  ..., -1.6838e-05,
         -5.8673e-06, -1.2144e-05],
        [-2.7657e-05, -1.7971e-05,  9.7007e-06,  ..., -2.4825e-05,
         -8.5607e-06, -1.7866e-05],
        [-3.2902e-05, -2.1353e-05,  1.1556e-05,  ..., -2.9504e-05,
         -1.0394e-05, -2.1279e-05]], device='cuda:0')
Loss: 1.1056712865829468


Running epoch 0, step 574, batch 574
Sampled inputs[:2]: tensor([[    0,   471,    14,  ..., 27104,     9,   631],
        [    0,    12, 17906,  ...,  2086,   287,  4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0201e-05,  2.7710e-06, -9.8573e-05,  ...,  1.7355e-04,
         -6.1108e-05,  7.9035e-05],
        [-1.6555e-05, -1.0766e-05,  5.8189e-06,  ..., -1.4842e-05,
         -5.3123e-06, -1.0759e-05],
        [ 8.0422e-05,  3.1380e-05, -1.3516e-05,  ...,  7.2297e-05,
          2.1432e-05,  4.0935e-05],
        [-3.2365e-05, -2.1055e-05,  1.1332e-05,  ..., -2.8998e-05,
         -9.7975e-06, -2.0832e-05],
        [ 1.2909e-04,  1.3235e-04, -4.3033e-05,  ...,  1.2768e-04,
          3.5758e-05,  7.4330e-05]], device='cuda:0')
Loss: 1.1002386808395386


Running epoch 0, step 575, batch 575
Sampled inputs[:2]: tensor([[    0,  1266,  2257,  ..., 27146,  1141,  1196],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6393e-05, -1.7874e-06, -1.4079e-04,  ...,  2.1367e-04,
         -8.1849e-05,  6.8562e-05],
        [-1.8880e-05, -1.2338e-05,  6.6347e-06,  ..., -1.6943e-05,
         -6.0797e-06, -1.2323e-05],
        [ 7.7367e-05,  2.9309e-05, -1.2443e-05,  ...,  6.9525e-05,
          2.0478e-05,  3.8893e-05],
        [-3.6865e-05, -2.4080e-05,  1.2904e-05,  ..., -3.3081e-05,
         -1.1191e-05, -2.3827e-05],
        [ 1.2378e-04,  1.2875e-04, -4.1170e-05,  ...,  1.2288e-04,
          3.4074e-05,  7.0798e-05]], device='cuda:0')
Loss: 1.0964775085449219
Graident accumulation at epoch 0, step 575, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3352e-05,  8.1417e-05, -1.5628e-04,  ...,  1.1184e-04,
         -1.3649e-04,  3.6286e-05],
        [-2.1277e-05, -1.4169e-05,  6.4955e-06,  ..., -1.8642e-05,
         -6.0365e-06, -1.2896e-05],
        [ 6.7816e-05,  4.6091e-05, -1.7772e-05,  ...,  6.3397e-05,
          2.2038e-05,  4.4425e-05],
        [-2.0957e-05, -1.3894e-05,  5.9604e-06,  ..., -1.7311e-05,
         -5.1982e-06, -1.2803e-05],
        [-3.1866e-05, -1.6607e-05,  9.1346e-06,  ..., -2.6352e-05,
         -8.8036e-06, -1.9520e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5845e-08, 1.4661e-08, 1.7552e-08,  ..., 1.9561e-08, 3.1024e-08,
         6.4735e-09],
        [5.0090e-11, 2.8090e-11, 2.5169e-12,  ..., 3.5529e-11, 2.1107e-12,
         9.7720e-12],
        [1.8408e-09, 9.5349e-10, 1.1308e-10,  ..., 1.3982e-09, 1.1424e-10,
         5.6524e-10],
        [4.5458e-10, 3.7298e-10, 3.1517e-11,  ..., 3.7061e-10, 2.1757e-11,
         1.3783e-10],
        [2.2680e-10, 1.2923e-10, 1.3348e-11,  ..., 1.6509e-10, 8.7100e-12,
         4.7934e-11]], device='cuda:0')
optimizer state dict: 72.0
lr: [8.881144923970756e-06, 8.881144923970756e-06]
scheduler_last_epoch: 72


Running epoch 0, step 576, batch 576
Sampled inputs[:2]: tensor([[    0,    19,    14,  ...,    13,  6673,   298],
        [    0, 39004,   266,  ...,   287, 21972,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1149e-04,  7.3228e-05, -8.3720e-05,  ...,  5.6987e-05,
         -4.4355e-05, -8.1469e-05],
        [-2.3246e-06, -1.4976e-06,  8.4564e-07,  ..., -2.1756e-06,
         -1.0654e-06, -1.6987e-06],
        [-2.9653e-06, -1.8924e-06,  1.0729e-06,  ..., -2.7418e-06,
         -1.2815e-06, -2.1160e-06],
        [-4.2617e-06, -2.7120e-06,  1.5423e-06,  ..., -3.9339e-06,
         -1.8179e-06, -3.0398e-06],
        [-5.1260e-06, -3.2634e-06,  1.8552e-06,  ..., -4.7386e-06,
         -2.2054e-06, -3.6657e-06]], device='cuda:0')
Loss: 1.094947338104248


Running epoch 0, step 577, batch 577
Sampled inputs[:2]: tensor([[   0, 5182,  446,  ...,  417,  199,   50],
        [   0, 7230,   13,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8455e-05,  5.7154e-05, -1.2768e-04,  ...,  8.9856e-05,
         -9.6555e-05, -1.2384e-04],
        [-4.5598e-06, -2.9579e-06,  1.6652e-06,  ..., -4.2021e-06,
         -1.7025e-06, -3.1441e-06],
        [-5.9605e-06, -3.8594e-06,  2.1681e-06,  ..., -5.4687e-06,
         -2.0862e-06, -4.0382e-06],
        [-8.7023e-06, -5.6177e-06,  3.1665e-06,  ..., -7.9870e-06,
         -2.9951e-06, -5.8860e-06],
        [-1.0341e-05, -6.6608e-06,  3.7625e-06,  ..., -9.4771e-06,
         -3.6210e-06, -7.0035e-06]], device='cuda:0')
Loss: 1.1022486686706543


Running epoch 0, step 578, batch 578
Sampled inputs[:2]: tensor([[   0,  380, 8157,  ...,  943,  352, 2278],
        [   0,   14, 1266,  ..., 2288,  417,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7002e-04,  1.0577e-04, -1.4046e-04,  ...,  9.8959e-05,
          3.1276e-06, -6.0708e-05],
        [-6.8843e-06, -4.4778e-06,  2.5183e-06,  ..., -6.3330e-06,
         -2.5742e-06, -4.7237e-06],
        [-9.0301e-06, -5.8711e-06,  3.2932e-06,  ..., -8.2701e-06,
         -3.1814e-06, -6.1095e-06],
        [-1.3083e-05, -8.4937e-06,  4.7833e-06,  ..., -1.2010e-05,
         -4.5598e-06, -8.8513e-06],
        [-1.5616e-05, -1.0118e-05,  5.7146e-06,  ..., -1.4305e-05,
         -5.5283e-06, -1.0565e-05]], device='cuda:0')
Loss: 1.1189993619918823


Running epoch 0, step 579, batch 579
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   630,  3765, 19107],
        [    0,   365,  8790,  ...,  1172,  8806,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9742e-04,  1.0300e-04, -1.7927e-04,  ...,  1.4233e-04,
          1.1484e-05, -4.0935e-05],
        [-9.1642e-06, -5.9456e-06,  3.3490e-06,  ..., -8.3596e-06,
         -3.3081e-06, -6.1765e-06],
        [-1.2130e-05, -7.8678e-06,  4.4182e-06,  ..., -1.1027e-05,
         -4.1239e-06, -8.0615e-06],
        [-1.7613e-05, -1.1414e-05,  6.4299e-06,  ..., -1.6063e-05,
         -5.9232e-06, -1.1712e-05],
        [-2.0921e-05, -1.3530e-05,  7.6368e-06,  ..., -1.9014e-05,
         -7.1600e-06, -1.3903e-05]], device='cuda:0')
Loss: 1.1046847105026245


Running epoch 0, step 580, batch 580
Sampled inputs[:2]: tensor([[    0, 48545,    26,  ...,  1471,   266,   319],
        [    0,    20,  2637,  ..., 14044,     9,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1240e-04,  1.8164e-04, -2.3584e-04,  ...,  1.4392e-04,
         -1.3367e-05, -4.0395e-06],
        [-1.1519e-05, -7.4506e-06,  4.2990e-06,  ..., -1.0505e-05,
         -4.3213e-06, -7.8753e-06],
        [-1.5140e-05, -9.7901e-06,  5.6326e-06,  ..., -1.3769e-05,
         -5.3681e-06, -1.0222e-05],
        [-2.1964e-05, -1.4171e-05,  8.1807e-06,  ..., -2.0027e-05,
         -7.7039e-06, -1.4827e-05],
        [-2.6137e-05, -1.6838e-05,  9.7379e-06,  ..., -2.3752e-05,
         -9.3207e-06, -1.7628e-05]], device='cuda:0')
Loss: 1.104990839958191


Running epoch 0, step 581, batch 581
Sampled inputs[:2]: tensor([[    0,    12,   221,  ...,   292, 27729,  9837],
        [    0, 11853,  1611,  ...,  4413,  4240,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7402e-04,  1.2998e-04, -3.0778e-04,  ...,  1.9781e-04,
         -6.8469e-05, -7.2781e-05],
        [-1.3739e-05, -8.9034e-06,  5.1074e-06,  ..., -1.2517e-05,
         -5.0217e-06, -9.3207e-06],
        [-1.8120e-05, -1.1757e-05,  6.7204e-06,  ..., -1.6481e-05,
         -6.2622e-06, -1.2159e-05],
        [-2.6405e-05, -1.7077e-05,  9.8050e-06,  ..., -2.4080e-05,
         -9.0152e-06, -1.7703e-05],
        [-3.1352e-05, -2.0251e-05,  1.1638e-05,  ..., -2.8491e-05,
         -1.0893e-05, -2.0996e-05]], device='cuda:0')
Loss: 1.1009093523025513


Running epoch 0, step 582, batch 582
Sampled inputs[:2]: tensor([[    0, 28107,    14,  ...,   864,   298,   413],
        [    0,  2372,  1319,  ...,  1253,   292, 34166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3972e-04,  1.8691e-04, -3.7287e-04,  ...,  2.4785e-04,
         -1.0552e-04,  1.1881e-05],
        [-1.6138e-05, -1.0431e-05,  5.9307e-06,  ..., -1.4603e-05,
         -5.9418e-06, -1.0863e-05],
        [-2.1264e-05, -1.3769e-05,  7.8082e-06,  ..., -1.9222e-05,
         -7.4245e-06, -1.4171e-05],
        [-3.0935e-05, -1.9968e-05,  1.1370e-05,  ..., -2.8044e-05,
         -1.0669e-05, -2.0608e-05],
        [-3.6836e-05, -2.3738e-05,  1.3530e-05,  ..., -3.3259e-05,
         -1.2934e-05, -2.4498e-05]], device='cuda:0')
Loss: 1.0838125944137573


Running epoch 0, step 583, batch 583
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3972e-04,  2.7655e-04, -3.3494e-04,  ...,  2.3374e-04,
         -9.1546e-05,  1.1218e-04],
        [-1.8522e-05, -1.1966e-05,  6.8061e-06,  ..., -1.6719e-05,
         -6.7949e-06, -1.2435e-05],
        [-2.4408e-05, -1.5795e-05,  8.9630e-06,  ..., -2.2009e-05,
         -8.5048e-06, -1.6227e-05],
        [-3.5495e-05, -2.2903e-05,  1.3046e-05,  ..., -3.2097e-05,
         -1.2226e-05, -2.3589e-05],
        [-4.2319e-05, -2.7254e-05,  1.5542e-05,  ..., -3.8087e-05,
         -1.4834e-05, -2.8074e-05]], device='cuda:0')
Loss: 1.1424992084503174
Graident accumulation at epoch 0, step 583, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5989e-05,  1.0093e-04, -1.7415e-04,  ...,  1.2403e-04,
         -1.3199e-04,  4.3876e-05],
        [-2.1002e-05, -1.3949e-05,  6.5265e-06,  ..., -1.8449e-05,
         -6.1123e-06, -1.2850e-05],
        [ 5.8593e-05,  3.9902e-05, -1.5099e-05,  ...,  5.4856e-05,
          1.8984e-05,  3.8360e-05],
        [-2.2411e-05, -1.4795e-05,  6.6690e-06,  ..., -1.8790e-05,
         -5.9010e-06, -1.3882e-05],
        [-3.2912e-05, -1.7672e-05,  9.7754e-06,  ..., -2.7526e-05,
         -9.4066e-06, -2.0375e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5856e-08, 1.4723e-08, 1.7647e-08,  ..., 1.9596e-08, 3.1002e-08,
         6.4796e-09],
        [5.0383e-11, 2.8205e-11, 2.5607e-12,  ..., 3.5773e-11, 2.1548e-12,
         9.9169e-12],
        [1.8396e-09, 9.5278e-10, 1.1305e-10,  ..., 1.3973e-09, 1.1420e-10,
         5.6494e-10],
        [4.5539e-10, 3.7313e-10, 3.1656e-11,  ..., 3.7127e-10, 2.1885e-11,
         1.3825e-10],
        [2.2836e-10, 1.2984e-10, 1.3577e-11,  ..., 1.6638e-10, 8.9213e-12,
         4.8675e-11]], device='cuda:0')
optimizer state dict: 73.0
lr: [8.635831001887192e-06, 8.635831001887192e-06]
scheduler_last_epoch: 73


Running epoch 0, step 584, batch 584
Sampled inputs[:2]: tensor([[    0, 16765,   367,  ..., 30192,  7038,  8135],
        [    0,   199, 11296,  ...,   266, 10463,  8256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6582e-05,  3.1019e-05, -1.9461e-05,  ...,  3.6937e-05,
          5.0891e-05,  4.2848e-05],
        [-2.2799e-06, -1.4603e-06,  8.3819e-07,  ..., -2.0266e-06,
         -7.3388e-07, -1.4305e-06],
        [-3.1143e-06, -1.9968e-06,  1.1399e-06,  ..., -2.7716e-06,
         -9.5367e-07, -1.9372e-06],
        [-4.5002e-06, -2.8908e-06,  1.6615e-06,  ..., -4.0233e-06,
         -1.3635e-06, -2.8014e-06],
        [-5.3346e-06, -3.4124e-06,  1.9521e-06,  ..., -4.7386e-06,
         -1.6466e-06, -3.3081e-06]], device='cuda:0')
Loss: 1.1002871990203857


Running epoch 0, step 585, batch 585
Sampled inputs[:2]: tensor([[    0,   381,  1795,  ...,    12,   344,   593],
        [    0,    12,   496,  ..., 11354,  4856,  1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6475e-04,  7.2962e-05,  1.9171e-05,  ..., -9.0795e-06,
          6.7307e-05,  9.1122e-05],
        [-4.5449e-06, -2.9355e-06,  1.7583e-06,  ..., -4.0382e-06,
         -1.4976e-06, -2.8759e-06],
        [ 2.2992e-04,  2.0181e-04, -1.0535e-04,  ...,  2.4941e-04,
          6.0441e-05,  2.0001e-04],
        [-8.9407e-06, -5.7966e-06,  3.4645e-06,  ..., -7.9870e-06,
         -2.7567e-06, -5.6177e-06],
        [-1.0639e-05, -6.8843e-06,  4.0978e-06,  ..., -9.4771e-06,
         -3.3602e-06, -6.6757e-06]], device='cuda:0')
Loss: 1.124590277671814


Running epoch 0, step 586, batch 586
Sampled inputs[:2]: tensor([[    0,    12, 32425,  ...,   389,   221,   494],
        [    0,   346,   462,  ...,   474, 38333,    87]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9155e-04,  1.8070e-04,  1.4127e-06,  ...,  2.3031e-05,
          2.8398e-05,  1.0287e-04],
        [-6.7800e-06, -4.3511e-06,  2.5779e-06,  ..., -6.0797e-06,
         -2.2165e-06, -4.3288e-06],
        [ 2.2691e-04,  1.9990e-04, -1.0425e-04,  ...,  2.4664e-04,
          5.9525e-05,  1.9807e-04],
        [-1.3292e-05, -8.5533e-06,  5.0589e-06,  ..., -1.1981e-05,
         -4.0680e-06, -8.4192e-06],
        [-1.5795e-05, -1.0133e-05,  5.9828e-06,  ..., -1.4186e-05,
         -4.9472e-06, -9.9838e-06]], device='cuda:0')
Loss: 1.111995816230774


Running epoch 0, step 587, batch 587
Sampled inputs[:2]: tensor([[    0,    13,  1581,  ...,    13, 11628, 14876],
        [    0,  5116,  4330,  ...,   925,   699,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2275e-04,  2.2437e-04, -5.7733e-05,  ...,  3.5756e-05,
          2.1599e-05,  9.5742e-05],
        [-8.9705e-06, -5.7295e-06,  3.3975e-06,  ..., -8.0913e-06,
         -3.0138e-06, -5.7891e-06],
        [ 2.2399e-04,  1.9805e-04, -1.0315e-04,  ...,  2.4394e-04,
          5.8526e-05,  1.9614e-04],
        [-1.7583e-05, -1.1265e-05,  6.6608e-06,  ..., -1.5944e-05,
         -5.5134e-06, -1.1250e-05],
        [-2.0921e-05, -1.3351e-05,  7.8902e-06,  ..., -1.8895e-05,
         -6.7204e-06, -1.3351e-05]], device='cuda:0')
Loss: 1.1084067821502686


Running epoch 0, step 588, batch 588
Sampled inputs[:2]: tensor([[    0,   259, 19567,  ...,   266,  3899,  2123],
        [    0,  4350,    14,  ...,   266,  9479,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9715e-04,  2.2164e-04, -2.3139e-05,  ...,  4.2443e-05,
          7.4036e-06,  1.0996e-04],
        [-1.1191e-05, -7.1526e-06,  4.2059e-06,  ..., -1.0103e-05,
         -3.7141e-06, -7.1749e-06],
        [ 2.2095e-04,  1.9610e-04, -1.0205e-04,  ...,  2.4120e-04,
          5.7625e-05,  1.9426e-04],
        [-2.2024e-05, -1.4111e-05,  8.2701e-06,  ..., -1.9968e-05,
         -6.8173e-06, -1.3992e-05],
        [-2.6166e-05, -1.6719e-05,  9.7901e-06,  ..., -2.3633e-05,
         -8.3148e-06, -1.6600e-05]], device='cuda:0')
Loss: 1.1100386381149292


Running epoch 0, step 589, batch 589
Sampled inputs[:2]: tensor([[    0, 21930,    12,  ...,  2849,   863,   578],
        [    0,  1196,  2612,  ...,  2489,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0132e-04,  2.2687e-04, -1.4057e-04,  ...,  7.9595e-05,
         -9.9982e-06,  8.3044e-05],
        [-1.3366e-05, -8.5756e-06,  5.0552e-06,  ..., -1.2100e-05,
         -4.4741e-06, -8.5756e-06],
        [ 2.1797e-04,  1.9415e-04, -1.0090e-04,  ...,  2.3847e-04,
          5.6656e-05,  1.9236e-04],
        [-2.6435e-05, -1.6987e-05,  9.9763e-06,  ..., -2.3991e-05,
         -8.2403e-06, -1.6794e-05],
        [-3.1292e-05, -2.0072e-05,  1.1772e-05,  ..., -2.8312e-05,
         -1.0014e-05, -1.9863e-05]], device='cuda:0')
Loss: 1.1174275875091553


Running epoch 0, step 590, batch 590
Sampled inputs[:2]: tensor([[   0,  600,   14,  ...,  221, 8187, 1802],
        [   0, 9116,  278,  ..., 6997, 3244, 1192]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1064e-04,  2.4426e-04, -2.2521e-04,  ...,  1.1938e-04,
         -1.5098e-05,  1.0941e-04],
        [-1.5527e-05, -9.9763e-06,  5.8748e-06,  ..., -1.4111e-05,
         -5.1856e-06, -9.9763e-06],
        [ 2.1502e-04,  1.9223e-04, -9.9782e-05,  ...,  2.3572e-04,
          5.5747e-05,  1.9046e-04],
        [-3.0816e-05, -1.9833e-05,  1.1638e-05,  ..., -2.8074e-05,
         -9.5740e-06, -1.9610e-05],
        [-3.6389e-05, -2.3380e-05,  1.3709e-05,  ..., -3.3051e-05,
         -1.1601e-05, -2.3142e-05]], device='cuda:0')
Loss: 1.0901578664779663


Running epoch 0, step 591, batch 591
Sampled inputs[:2]: tensor([[   0,  271, 2862,  ...,  287, 5699,   18],
        [   0, 2530,  634,  ...,   15, 8808,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0125e-04,  2.8620e-04, -2.3128e-04,  ...,  1.0928e-04,
          3.1552e-05,  1.6427e-04],
        [-1.7792e-05, -1.1481e-05,  6.7204e-06,  ..., -1.6212e-05,
         -6.0648e-06, -1.1466e-05],
        [ 2.1201e-04,  1.9023e-04, -9.8665e-05,  ...,  2.3291e-04,
          5.4630e-05,  1.8849e-04],
        [-3.5137e-05, -2.2709e-05,  1.3247e-05,  ..., -3.2097e-05,
         -1.1168e-05, -2.2426e-05],
        [-4.1574e-05, -2.6822e-05,  1.5646e-05,  ..., -3.7879e-05,
         -1.3553e-05, -2.6524e-05]], device='cuda:0')
Loss: 1.1209453344345093
Graident accumulation at epoch 0, step 591, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0335, -0.0097,  0.0405,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0285, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.2515e-05,  1.1946e-04, -1.7986e-04,  ...,  1.2255e-04,
         -1.1564e-04,  5.5915e-05],
        [-2.0681e-05, -1.3702e-05,  6.5459e-06,  ..., -1.8226e-05,
         -6.1076e-06, -1.2712e-05],
        [ 7.3935e-05,  5.4935e-05, -2.3455e-05,  ...,  7.2662e-05,
          2.2548e-05,  5.3373e-05],
        [-2.3683e-05, -1.5587e-05,  7.3268e-06,  ..., -2.0121e-05,
         -6.4278e-06, -1.4736e-05],
        [-3.3778e-05, -1.8587e-05,  1.0362e-05,  ..., -2.8561e-05,
         -9.8212e-06, -2.0990e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5972e-08, 1.4790e-08, 1.7682e-08,  ..., 1.9589e-08, 3.0972e-08,
         6.5001e-09],
        [5.0649e-11, 2.8309e-11, 2.6033e-12,  ..., 3.6000e-11, 2.1894e-12,
         1.0038e-11],
        [1.8827e-09, 9.8802e-10, 1.2267e-10,  ..., 1.4501e-09, 1.1707e-10,
         5.9990e-10],
        [4.5617e-10, 3.7327e-10, 3.1800e-11,  ..., 3.7193e-10, 2.1988e-11,
         1.3862e-10],
        [2.2986e-10, 1.3043e-10, 1.3808e-11,  ..., 1.6765e-10, 9.0961e-12,
         4.9329e-11]], device='cuda:0')
optimizer state dict: 74.0
lr: [8.391350875673234e-06, 8.391350875673234e-06]
scheduler_last_epoch: 74


Running epoch 0, step 592, batch 592
Sampled inputs[:2]: tensor([[    0,  4494,    12,  ...,   341,  1619,    12],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0716e-04,  7.4368e-05, -1.0110e-04,  ..., -4.0127e-05,
          2.8591e-05,  1.5108e-05],
        [-2.1607e-06, -1.3709e-06,  8.6427e-07,  ..., -1.9968e-06,
         -8.1956e-07, -1.4231e-06],
        [-2.8461e-06, -1.8179e-06,  1.1399e-06,  ..., -2.6375e-06,
         -1.0058e-06, -1.8626e-06],
        [-4.2021e-06, -2.6822e-06,  1.6913e-06,  ..., -3.9041e-06,
         -1.4678e-06, -2.7418e-06],
        [-4.9174e-06, -3.1292e-06,  1.9819e-06,  ..., -4.5598e-06,
         -1.7583e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.1163954734802246


Running epoch 0, step 593, batch 593
Sampled inputs[:2]: tensor([[   0, 1477, 5648,  ..., 4391, 1722,  369],
        [   0, 6416,  367,  ...,  496,   14,   20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4973e-04,  1.0657e-04, -9.3138e-05,  ..., -6.5679e-05,
          1.2743e-04,  1.4944e-05],
        [-4.4107e-06, -2.7791e-06,  1.7211e-06,  ..., -4.0531e-06,
         -1.6764e-06, -2.8387e-06],
        [-5.8413e-06, -3.6955e-06,  2.2799e-06,  ..., -5.3793e-06,
         -2.0862e-06, -3.7327e-06],
        [-8.5533e-06, -5.4091e-06,  3.3453e-06,  ..., -7.8976e-06,
         -3.0249e-06, -5.4538e-06],
        [-1.0073e-05, -6.3479e-06,  3.9339e-06,  ..., -9.2685e-06,
         -3.6433e-06, -6.4373e-06]], device='cuda:0')
Loss: 1.1352672576904297


Running epoch 0, step 594, batch 594
Sampled inputs[:2]: tensor([[   0,  694, 2326,  ...,  278, 1781, 9660],
        [   0,  287, 7763,  ...,  689, 2409,  699]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8626e-04,  1.6479e-04, -1.7730e-04,  ..., -1.0802e-05,
          1.0926e-04,  5.1270e-06],
        [-6.5863e-06, -4.2170e-06,  2.5928e-06,  ..., -6.0797e-06,
         -2.5295e-06, -4.2394e-06],
        [-8.7917e-06, -5.6326e-06,  3.4571e-06,  ..., -8.1062e-06,
         -3.1590e-06, -5.6028e-06],
        [-1.2815e-05, -8.2254e-06,  5.0515e-06,  ..., -1.1861e-05,
         -4.5672e-06, -8.1509e-06],
        [-1.5050e-05, -9.6262e-06,  5.9307e-06,  ..., -1.3888e-05,
         -5.4836e-06, -9.5963e-06]], device='cuda:0')
Loss: 1.103294849395752


Running epoch 0, step 595, batch 595
Sampled inputs[:2]: tensor([[    0,    14,  1845,  ...,   806,   352,   408],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3394e-04,  2.3038e-04, -1.7113e-04,  ..., -2.7536e-05,
          1.6817e-04,  1.6211e-04],
        [-8.8364e-06, -5.6773e-06,  3.4980e-06,  ..., -8.1211e-06,
         -3.4235e-06, -5.6550e-06],
        [-1.1817e-05, -7.5996e-06,  4.6715e-06,  ..., -1.0863e-05,
         -4.3064e-06, -7.4878e-06],
        [-1.7196e-05, -1.1057e-05,  6.8024e-06,  ..., -1.5855e-05,
         -6.2063e-06, -1.0863e-05],
        [-2.0236e-05, -1.2979e-05,  8.0019e-06,  ..., -1.8597e-05,
         -7.4655e-06, -1.2815e-05]], device='cuda:0')
Loss: 1.1139148473739624


Running epoch 0, step 596, batch 596
Sampled inputs[:2]: tensor([[   0, 7926, 6750,  ...,  259, 1524, 6257],
        [   0, 9855,  278,  ...,  266, 3134,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6365e-04,  2.9483e-04, -1.9318e-04,  ..., -4.7390e-05,
          1.9246e-04,  1.4482e-04],
        [-1.1027e-05, -7.0855e-06,  4.3586e-06,  ..., -1.0118e-05,
         -4.3027e-06, -7.0557e-06],
        [-1.4827e-05, -9.5367e-06,  5.8487e-06,  ..., -1.3620e-05,
         -5.4315e-06, -9.3877e-06],
        [-2.1517e-05, -1.3843e-05,  8.5011e-06,  ..., -1.9819e-05,
         -7.8231e-06, -1.3605e-05],
        [-2.5362e-05, -1.6272e-05,  1.0014e-05,  ..., -2.3276e-05,
         -9.4175e-06, -1.6049e-05]], device='cuda:0')
Loss: 1.1072173118591309


Running epoch 0, step 597, batch 597
Sampled inputs[:2]: tensor([[    0,  2380,  2667,  ...,    14,   381,  5621],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0282e-04,  3.3500e-04, -1.8411e-04,  ..., -3.9233e-05,
          2.1620e-04,  1.1952e-04],
        [-1.3247e-05, -8.5011e-06,  5.2154e-06,  ..., -1.2100e-05,
         -5.1558e-06, -8.4713e-06],
        [-1.7822e-05, -1.1444e-05,  7.0035e-06,  ..., -1.6302e-05,
         -6.5267e-06, -1.1280e-05],
        [-2.5839e-05, -1.6585e-05,  1.0155e-05,  ..., -2.3693e-05,
         -9.3728e-06, -1.6317e-05],
        [-3.0458e-05, -1.9506e-05,  1.1981e-05,  ..., -2.7835e-05,
         -1.1295e-05, -1.9267e-05]], device='cuda:0')
Loss: 1.1335437297821045


Running epoch 0, step 598, batch 598
Sampled inputs[:2]: tensor([[   0, 4137,  300,  ..., 2579,  278,  266],
        [   0,  401, 3704,  ...,   14, 1062, 1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6298e-04,  4.3790e-04, -2.0680e-04,  ..., -2.0630e-05,
          2.0096e-04,  1.3265e-04],
        [-1.5363e-05, -9.8422e-06,  6.0163e-06,  ..., -1.4082e-05,
         -5.8524e-06, -9.7826e-06],
        [-2.0698e-05, -1.3269e-05,  8.0913e-06,  ..., -1.9014e-05,
         -7.3947e-06, -1.3039e-05],
        [-3.0130e-05, -1.9312e-05,  1.1779e-05,  ..., -2.7746e-05,
         -1.0654e-05, -1.8954e-05],
        [-3.5375e-05, -2.2620e-05,  1.3843e-05,  ..., -3.2455e-05,
         -1.2808e-05, -2.2277e-05]], device='cuda:0')
Loss: 1.0968372821807861


Running epoch 0, step 599, batch 599
Sampled inputs[:2]: tensor([[   0,   24,   15,  ...,  221,  380,  417],
        [   0, 4359, 5768,  ..., 4402,  292,   69]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0284e-04,  5.8632e-04, -2.9628e-04,  ..., -1.8665e-05,
          2.2785e-04,  1.2521e-04],
        [-1.7539e-05, -1.1198e-05,  6.8583e-06,  ..., -1.6123e-05,
         -6.7577e-06, -1.1258e-05],
        [-2.3589e-05, -1.5073e-05,  9.2015e-06,  ..., -2.1711e-05,
         -8.5197e-06, -1.4961e-05],
        [-3.4302e-05, -2.1905e-05,  1.3381e-05,  ..., -3.1650e-05,
         -1.2241e-05, -2.1726e-05],
        [-4.0323e-05, -2.5690e-05,  1.5751e-05,  ..., -3.7074e-05,
         -1.4745e-05, -2.5555e-05]], device='cuda:0')
Loss: 1.0764036178588867
Graident accumulation at epoch 0, step 599, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0335, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0285, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5548e-05,  1.6614e-04, -1.9150e-04,  ...,  1.0843e-04,
         -8.1290e-05,  6.2844e-05],
        [-2.0367e-05, -1.3452e-05,  6.5772e-06,  ..., -1.8015e-05,
         -6.1726e-06, -1.2566e-05],
        [ 6.4183e-05,  4.7934e-05, -2.0190e-05,  ...,  6.3224e-05,
          1.9441e-05,  4.6540e-05],
        [-2.4745e-05, -1.6218e-05,  7.9323e-06,  ..., -2.1274e-05,
         -7.0091e-06, -1.5435e-05],
        [-3.4432e-05, -1.9297e-05,  1.0901e-05,  ..., -2.9412e-05,
         -1.0314e-05, -2.1447e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6017e-08, 1.5119e-08, 1.7753e-08,  ..., 1.9569e-08, 3.0993e-08,
         6.5093e-09],
        [5.0906e-11, 2.8406e-11, 2.6477e-12,  ..., 3.6224e-11, 2.2329e-12,
         1.0155e-11],
        [1.8814e-09, 9.8726e-10, 1.2263e-10,  ..., 1.4492e-09, 1.1702e-10,
         5.9952e-10],
        [4.5689e-10, 3.7338e-10, 3.1947e-11,  ..., 3.7256e-10, 2.2115e-11,
         1.3895e-10],
        [2.3126e-10, 1.3096e-10, 1.4042e-11,  ..., 1.6885e-10, 9.3044e-12,
         4.9933e-11]], device='cuda:0')
optimizer state dict: 75.0
lr: [8.147853974409676e-06, 8.147853974409676e-06]
scheduler_last_epoch: 75


Running epoch 0, step 600, batch 600
Sampled inputs[:2]: tensor([[    0,   328,   471,  ..., 11137,   679,  6585],
        [    0,   546,   360,  ...,    12,   461,  8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8941e-05,  1.6966e-05,  2.4183e-05,  ...,  5.6429e-05,
         -1.6119e-05, -5.9586e-05],
        [-2.1905e-06, -1.3635e-06,  9.1270e-07,  ..., -2.0266e-06,
         -9.3877e-07, -1.4007e-06],
        [-2.8461e-06, -1.7658e-06,  1.1846e-06,  ..., -2.6226e-06,
         -1.1474e-06, -1.7956e-06],
        [-4.1723e-06, -2.5928e-06,  1.7434e-06,  ..., -3.8743e-06,
         -1.6838e-06, -2.6524e-06],
        [-4.9174e-06, -3.0547e-06,  2.0564e-06,  ..., -4.5598e-06,
         -2.0266e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.1207404136657715


Running epoch 0, step 601, batch 601
Sampled inputs[:2]: tensor([[    0, 38717,  1679,  ...,   472,   346,   462],
        [    0,  5105,   271,  ...,   308,  3056,  3640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1899e-04,  8.4775e-05, -2.9066e-05,  ...,  7.3528e-05,
         -5.5166e-05, -4.8972e-05],
        [-4.3660e-06, -2.7418e-06,  1.8217e-06,  ..., -4.0084e-06,
         -1.8589e-06, -2.7865e-06],
        [-5.6773e-06, -3.5614e-06,  2.3693e-06,  ..., -5.2154e-06,
         -2.2650e-06, -3.5763e-06],
        [-8.3148e-06, -5.2154e-06,  3.4794e-06,  ..., -7.6592e-06,
         -3.2932e-06, -5.2452e-06],
        [-9.7752e-06, -6.1393e-06,  4.0978e-06,  ..., -9.0003e-06,
         -3.9637e-06, -6.1691e-06]], device='cuda:0')
Loss: 1.101750373840332


Running epoch 0, step 602, batch 602
Sampled inputs[:2]: tensor([[   0, 5159,  292,  ...,  772,  271, 3728],
        [   0, 3440, 5745,  ...,  360, 4998,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0470e-04, -2.6037e-05, -6.4522e-05,  ...,  8.4073e-05,
         -4.4514e-05, -1.0245e-04],
        [-6.4969e-06, -4.0904e-06,  2.6748e-06,  ..., -5.9307e-06,
         -2.5891e-06, -4.0904e-06],
        [-8.5682e-06, -5.4017e-06,  3.5241e-06,  ..., -7.8231e-06,
         -3.1851e-06, -5.3346e-06],
        [-1.2517e-05, -7.8827e-06,  5.1633e-06,  ..., -1.1459e-05,
         -4.6194e-06, -7.7933e-06],
        [-1.4693e-05, -9.2685e-06,  6.0648e-06,  ..., -1.3441e-05,
         -5.5656e-06, -9.1642e-06]], device='cuda:0')
Loss: 1.092529058456421


Running epoch 0, step 603, batch 603
Sampled inputs[:2]: tensor([[    0,  1188,   278,  ...,   271,  8368,   292],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1921e-04,  3.6953e-05, -7.3471e-05,  ...,  4.4107e-05,
         -1.4936e-05,  4.3101e-05],
        [-8.8215e-06, -5.5432e-06,  3.5949e-06,  ..., -8.0466e-06,
         -3.8110e-06, -5.6028e-06],
        [-1.1563e-05, -7.2792e-06,  4.7013e-06,  ..., -1.0535e-05,
         -4.6678e-06, -7.2569e-06],
        [-1.6600e-05, -1.0446e-05,  6.7577e-06,  ..., -1.5169e-05,
         -6.6012e-06, -1.0386e-05],
        [-1.9819e-05, -1.2457e-05,  8.0764e-06,  ..., -1.8090e-05,
         -8.1286e-06, -1.2442e-05]], device='cuda:0')
Loss: 1.1253935098648071


Running epoch 0, step 604, batch 604
Sampled inputs[:2]: tensor([[    0,  2013,    13,  ...,   271,   266,   908],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5972e-04,  7.6187e-05, -1.9564e-04,  ...,  3.5615e-05,
         -5.5849e-05,  8.3111e-06],
        [-1.0982e-05, -6.9365e-06,  4.5113e-06,  ..., -1.0073e-05,
         -4.7088e-06, -6.9737e-06],
        [-1.4395e-05, -9.1195e-06,  5.9009e-06,  ..., -1.3188e-05,
         -5.7705e-06, -9.0376e-06],
        [-2.0683e-05, -1.3098e-05,  8.5011e-06,  ..., -1.9014e-05,
         -8.1807e-06, -1.2964e-05],
        [-2.4617e-05, -1.5557e-05,  1.0118e-05,  ..., -2.2560e-05,
         -1.0014e-05, -1.5453e-05]], device='cuda:0')
Loss: 1.0996776819229126


Running epoch 0, step 605, batch 605
Sampled inputs[:2]: tensor([[    0, 24086,   266,  ..., 18814,    19,   292],
        [    0,   607,   259,  ...,   271,   669,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5534e-04, -1.4370e-04, -1.6319e-04,  ...,  1.8445e-05,
          5.9951e-05, -3.7757e-05],
        [-1.3083e-05, -8.2627e-06,  5.3942e-06,  ..., -1.1966e-05,
         -5.5172e-06, -8.2701e-06],
        [-1.7285e-05, -1.0952e-05,  7.1079e-06,  ..., -1.5795e-05,
         -6.8061e-06, -1.0796e-05],
        [-2.4825e-05, -1.5721e-05,  1.0230e-05,  ..., -2.2769e-05,
         -9.6411e-06, -1.5482e-05],
        [-2.9564e-05, -1.8671e-05,  1.2174e-05,  ..., -2.7031e-05,
         -1.1824e-05, -1.8463e-05]], device='cuda:0')
Loss: 1.1082497835159302


Running epoch 0, step 606, batch 606
Sampled inputs[:2]: tensor([[    0,  1086,    14,  ...,   963,   292,   221],
        [    0,    13,  2615,  ..., 31594, 15867,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0652e-04, -1.6530e-05, -1.2256e-04,  ..., -4.9262e-05,
          1.5981e-05,  1.4425e-04],
        [-1.5348e-05, -9.7007e-06,  6.3851e-06,  ..., -1.4007e-05,
         -6.7316e-06, -9.7826e-06],
        [-2.0131e-05, -1.2770e-05,  8.3521e-06,  ..., -1.8358e-05,
         -8.2441e-06, -1.2673e-05],
        [-2.8789e-05, -1.8269e-05,  1.1966e-05,  ..., -2.6375e-05,
         -1.1623e-05, -1.8105e-05],
        [-3.4481e-05, -2.1800e-05,  1.4320e-05,  ..., -3.1441e-05,
         -1.4327e-05, -2.1696e-05]], device='cuda:0')
Loss: 1.0862925052642822


Running epoch 0, step 607, batch 607
Sampled inputs[:2]: tensor([[    0, 25938,   359,  ...,    36, 15859,   504],
        [    0, 11325,   278,  ...,   446,  1869,   642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3703e-04, -3.1249e-05, -1.6450e-04,  ..., -5.2328e-06,
         -4.1270e-06,  1.3914e-04],
        [-1.7494e-05, -1.1064e-05,  7.2904e-06,  ..., -1.5959e-05,
         -7.5847e-06, -1.1116e-05],
        [-2.2992e-05, -1.4596e-05,  9.5516e-06,  ..., -2.0951e-05,
         -9.2946e-06, -1.4424e-05],
        [-3.2932e-05, -2.0906e-05,  1.3702e-05,  ..., -3.0145e-05,
         -1.3128e-05, -2.0638e-05],
        [-3.9339e-05, -2.4885e-05,  1.6361e-05,  ..., -3.5852e-05,
         -1.6131e-05, -2.4661e-05]], device='cuda:0')
Loss: 1.1194058656692505
Graident accumulation at epoch 0, step 607, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0147,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1970e-04,  1.4640e-04, -1.8880e-04,  ...,  9.7066e-05,
         -7.3574e-05,  7.0474e-05],
        [-2.0079e-05, -1.3213e-05,  6.6485e-06,  ..., -1.7810e-05,
         -6.3138e-06, -1.2421e-05],
        [ 5.5465e-05,  4.1681e-05, -1.7216e-05,  ...,  5.4807e-05,
          1.6568e-05,  4.0443e-05],
        [-2.5564e-05, -1.6687e-05,  8.5092e-06,  ..., -2.2161e-05,
         -7.6210e-06, -1.5955e-05],
        [-3.4923e-05, -1.9856e-05,  1.1447e-05,  ..., -3.0056e-05,
         -1.0895e-05, -2.1768e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6085e-08, 1.5105e-08, 1.7762e-08,  ..., 1.9550e-08, 3.0962e-08,
         6.5222e-09],
        [5.1161e-11, 2.8500e-11, 2.6982e-12,  ..., 3.6442e-11, 2.2882e-12,
         1.0269e-11],
        [1.8800e-09, 9.8648e-10, 1.2260e-10,  ..., 1.4482e-09, 1.1699e-10,
         5.9913e-10],
        [4.5752e-10, 3.7344e-10, 3.2103e-11,  ..., 3.7309e-10, 2.2266e-11,
         1.3924e-10],
        [2.3258e-10, 1.3145e-10, 1.4296e-11,  ..., 1.6997e-10, 9.5553e-12,
         5.0491e-11]], device='cuda:0')
optimizer state dict: 76.0
lr: [7.905489126218852e-06, 7.905489126218852e-06]
scheduler_last_epoch: 76


Running epoch 0, step 608, batch 608
Sampled inputs[:2]: tensor([[   0, 1751,  287,  ..., 6079, 1059,  287],
        [   0,  292, 3030,  ..., 1231, 2156,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6101e-05, -1.2483e-04,  4.2029e-05,  ..., -1.0745e-04,
          3.3464e-05,  1.3090e-05],
        [-2.2650e-06, -1.3784e-06,  9.8348e-07,  ..., -1.9670e-06,
         -1.0505e-06, -1.3784e-06],
        [-2.8908e-06, -1.7658e-06,  1.2442e-06,  ..., -2.5183e-06,
         -1.2591e-06, -1.7360e-06],
        [-4.0829e-06, -2.4885e-06,  1.7583e-06,  ..., -3.5614e-06,
         -1.7583e-06, -2.4438e-06],
        [-5.0962e-06, -3.0994e-06,  2.2054e-06,  ..., -4.4405e-06,
         -2.2501e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.1218760013580322


Running epoch 0, step 609, batch 609
Sampled inputs[:2]: tensor([[    0,   491, 10524,  ...,  2218,  5627,  4199],
        [    0,  1253,   287,  ...,  2988,    14,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2860e-04, -1.6075e-04,  1.0124e-04,  ..., -1.2006e-04,
          1.1973e-05, -3.8875e-05],
        [-4.5747e-06, -2.7344e-06,  1.9595e-06,  ..., -3.9935e-06,
         -2.2054e-06, -2.8387e-06],
        [-5.8711e-06, -3.5241e-06,  2.4959e-06,  ..., -5.1409e-06,
         -2.6822e-06, -3.6061e-06],
        [-8.3148e-06, -4.9621e-06,  3.5316e-06,  ..., -7.2718e-06,
         -3.7551e-06, -5.0813e-06],
        [-1.0282e-05, -6.1393e-06,  4.3809e-06,  ..., -8.9705e-06,
         -4.7386e-06, -6.2883e-06]], device='cuda:0')
Loss: 1.1120367050170898


Running epoch 0, step 610, batch 610
Sampled inputs[:2]: tensor([[    0,  6192,   266,  ...,  3318,  9872, 10931],
        [    0,   714,    14,  ...,  1501, 11397, 31940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4026e-04, -1.1263e-04, -1.2514e-05,  ..., -7.6926e-05,
         -9.5972e-05, -6.7202e-05],
        [-6.7502e-06, -4.0606e-06,  2.9095e-06,  ..., -5.9456e-06,
         -3.1590e-06, -4.1574e-06],
        [-8.7321e-06, -5.2601e-06,  3.7402e-06,  ..., -7.7039e-06,
         -3.8594e-06, -5.3123e-06],
        [-1.2398e-05, -7.4506e-06,  5.3123e-06,  ..., -1.0952e-05,
         -5.4166e-06, -7.5251e-06],
        [-1.5199e-05, -9.1195e-06,  6.5267e-06,  ..., -1.3381e-05,
         -6.7800e-06, -9.2238e-06]], device='cuda:0')
Loss: 1.1007452011108398


Running epoch 0, step 611, batch 611
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0,   870,   278,  ...,   478,   401,   897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7115e-05, -7.0330e-05, -6.8907e-05,  ..., -8.5957e-05,
         -9.5972e-05, -5.9881e-05],
        [-8.9109e-06, -5.4240e-06,  3.8520e-06,  ..., -7.9721e-06,
         -4.1351e-06, -5.5358e-06],
        [-1.1504e-05, -7.0110e-06,  4.9472e-06,  ..., -1.0297e-05,
         -5.0217e-06, -7.0482e-06],
        [-1.6391e-05, -9.9540e-06,  7.0408e-06,  ..., -1.4678e-05,
         -7.0781e-06, -1.0014e-05],
        [-1.9968e-05, -1.2115e-05,  8.5980e-06,  ..., -1.7822e-05,
         -8.8066e-06, -1.2204e-05]], device='cuda:0')
Loss: 1.1139857769012451


Running epoch 0, step 612, batch 612
Sampled inputs[:2]: tensor([[    0,  1234,   408,  ...,   292, 17323,   221],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3085e-06,  2.3287e-07, -2.1281e-04,  ..., -7.3424e-05,
         -5.9337e-05,  3.1619e-05],
        [-1.1057e-05, -6.7353e-06,  4.8056e-06,  ..., -9.9540e-06,
         -5.2378e-06, -6.9141e-06],
        [-1.4216e-05, -8.6725e-06,  6.1542e-06,  ..., -1.2785e-05,
         -6.3032e-06, -8.7470e-06],
        [-2.0325e-05, -1.2353e-05,  8.7917e-06,  ..., -1.8299e-05,
         -8.9183e-06, -1.2487e-05],
        [-2.4706e-05, -1.5005e-05,  1.0714e-05,  ..., -2.2173e-05,
         -1.1072e-05, -1.5169e-05]], device='cuda:0')
Loss: 1.0754282474517822


Running epoch 0, step 613, batch 613
Sampled inputs[:2]: tensor([[    0,   344,  2183,  ...,    14,   759,   596],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8640e-06,  1.6067e-04, -3.3586e-04,  ..., -8.1749e-05,
         -1.1725e-04,  8.7486e-05],
        [-1.3232e-05, -8.0466e-06,  5.7593e-06,  ..., -1.1995e-05,
         -6.4149e-06, -8.3819e-06],
        [-1.6913e-05, -1.0312e-05,  7.3388e-06,  ..., -1.5318e-05,
         -7.6741e-06, -1.0550e-05],
        [-2.4229e-05, -1.4707e-05,  1.0505e-05,  ..., -2.1964e-05,
         -1.0885e-05, -1.5095e-05],
        [-2.9415e-05, -1.7852e-05,  1.2785e-05,  ..., -2.6613e-05,
         -1.3500e-05, -1.8328e-05]], device='cuda:0')
Loss: 1.103774070739746


Running epoch 0, step 614, batch 614
Sampled inputs[:2]: tensor([[    0,   266,  1176,  ...,   199, 17791,  3662],
        [    0,  1356,   634,  ...,  6604,   634,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1077e-05,  4.6877e-05, -4.3999e-04,  ..., -8.9086e-05,
         -1.7597e-04, -6.4799e-05],
        [-1.5393e-05, -9.4175e-06,  6.7353e-06,  ..., -1.3933e-05,
         -7.4208e-06, -9.7454e-06],
        [-1.9714e-05, -1.2100e-05,  8.5980e-06,  ..., -1.7837e-05,
         -8.8736e-06, -1.2293e-05],
        [-2.8253e-05, -1.7285e-05,  1.2331e-05,  ..., -2.5600e-05,
         -1.2591e-05, -1.7613e-05],
        [-3.4183e-05, -2.0891e-05,  1.4946e-05,  ..., -3.0905e-05,
         -1.5587e-05, -2.1309e-05]], device='cuda:0')
Loss: 1.08182692527771


Running epoch 0, step 615, batch 615
Sampled inputs[:2]: tensor([[   0, 1197,  729,  ...,  674,  369, 8222],
        [   0, 3152, 1385,  ..., 1403,  518, 2088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0347e-05,  5.3058e-05, -4.5110e-04,  ..., -1.2548e-04,
         -1.4409e-04, -1.8257e-04],
        [-1.7509e-05, -1.0692e-05,  7.6890e-06,  ..., -1.5855e-05,
         -8.2664e-06, -1.1027e-05],
        [ 5.5312e-04,  3.1624e-04, -3.0920e-04,  ...,  4.7162e-04,
          2.1745e-04,  2.6796e-04],
        [-3.2425e-05, -1.9789e-05,  1.4193e-05,  ..., -2.9385e-05,
         -1.4134e-05, -2.0117e-05],
        [-3.9160e-05, -2.3887e-05,  1.7181e-05,  ..., -3.5405e-05,
         -1.7479e-05, -2.4289e-05]], device='cuda:0')
Loss: 1.1183217763900757
Graident accumulation at epoch 0, step 615, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0169e-04,  1.3707e-04, -2.1503e-04,  ...,  7.4812e-05,
         -8.0626e-05,  4.5169e-05],
        [-1.9822e-05, -1.2961e-05,  6.7525e-06,  ..., -1.7614e-05,
         -6.5090e-06, -1.2282e-05],
        [ 1.0523e-04,  6.9138e-05, -4.6414e-05,  ...,  9.6488e-05,
          3.6656e-05,  6.3195e-05],
        [-2.6250e-05, -1.6997e-05,  9.0776e-06,  ..., -2.2883e-05,
         -8.2723e-06, -1.6371e-05],
        [-3.5347e-05, -2.0259e-05,  1.2021e-05,  ..., -3.0591e-05,
         -1.1554e-05, -2.2020e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6042e-08, 1.5092e-08, 1.7948e-08,  ..., 1.9546e-08, 3.0951e-08,
         6.5490e-09],
        [5.1416e-11, 2.8586e-11, 2.7547e-12,  ..., 3.6657e-11, 2.3542e-12,
         1.0380e-11],
        [2.1841e-09, 1.0855e-09, 2.1808e-10,  ..., 1.6691e-09, 1.6416e-10,
         6.7034e-10],
        [4.5811e-10, 3.7346e-10, 3.2272e-11,  ..., 3.7358e-10, 2.2443e-11,
         1.3950e-10],
        [2.3388e-10, 1.3189e-10, 1.4577e-11,  ..., 1.7105e-10, 9.8512e-12,
         5.1031e-11]], device='cuda:0')
optimizer state dict: 77.0
lr: [7.664404467299166e-06, 7.664404467299166e-06]
scheduler_last_epoch: 77


Running epoch 0, step 616, batch 616
Sampled inputs[:2]: tensor([[   0,  365, 1110,  ..., 4130,  221,  199],
        [   0, 2296,  446,  ..., 2937,  287, 2795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8203e-05, -1.0848e-04,  7.2185e-05,  ..., -4.8459e-05,
          6.5472e-06,  8.5794e-06],
        [-2.2203e-06, -1.2964e-06,  1.0207e-06,  ..., -1.9521e-06,
         -1.0878e-06, -1.4007e-06],
        [-2.7865e-06, -1.6391e-06,  1.2666e-06,  ..., -2.4587e-06,
         -1.2666e-06, -1.7285e-06],
        [-3.9935e-06, -2.3395e-06,  1.8179e-06,  ..., -3.5316e-06,
         -1.7881e-06, -2.4736e-06],
        [-4.9472e-06, -2.8908e-06,  2.2501e-06,  ..., -4.3511e-06,
         -2.2799e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.1037259101867676


Running epoch 0, step 617, batch 617
Sampled inputs[:2]: tensor([[   0, 2341, 7956,  ..., 2355,  413,   72],
        [   0,   12,  689,  ..., 1110, 1712, 2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0253e-04, -1.8805e-04,  7.4467e-05,  ..., -5.4607e-05,
          2.7062e-06,  3.7542e-06],
        [-4.4256e-06, -2.6077e-06,  1.9595e-06,  ..., -3.8594e-06,
         -2.0862e-06, -2.6971e-06],
        [-5.6326e-06, -3.3453e-06,  2.4661e-06,  ..., -4.9323e-06,
         -2.4587e-06, -3.3826e-06],
        [ 8.8443e-05,  4.1798e-05, -2.3781e-05,  ...,  6.3354e-05,
          3.0248e-05,  4.4647e-05],
        [-9.9838e-06, -5.9009e-06,  4.3809e-06,  ..., -8.7321e-06,
         -4.4256e-06, -6.0052e-06]], device='cuda:0')
Loss: 1.1099724769592285


Running epoch 0, step 618, batch 618
Sampled inputs[:2]: tensor([[   0,  266, 1634,  ...,  310, 1372,  287],
        [   0,  858,   13,  ..., 2253,  847,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9975e-04, -2.0590e-04,  5.5742e-06,  ..., -1.2701e-04,
          1.1811e-04, -7.3457e-05],
        [-6.7353e-06, -4.0084e-06,  3.0398e-06,  ..., -5.9754e-06,
         -3.4496e-06, -4.2096e-06],
        [-8.4043e-06, -5.0291e-06,  3.7700e-06,  ..., -7.4655e-06,
         -3.9935e-06, -5.1707e-06],
        [ 8.4479e-05,  3.9414e-05, -2.1918e-05,  ...,  5.9748e-05,
          2.8073e-05,  4.2099e-05],
        [-1.4842e-05, -8.8364e-06,  6.6757e-06,  ..., -1.3143e-05,
         -7.1377e-06, -9.1344e-06]], device='cuda:0')
Loss: 1.1173831224441528


Running epoch 0, step 619, batch 619
Sampled inputs[:2]: tensor([[    0,  1431,   221,  ...,   756,   409,   275],
        [    0, 26074,   486,  ...,  2314,   266,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4949e-04, -2.4625e-04, -2.3388e-05,  ..., -1.3494e-04,
          1.5662e-04, -9.3624e-05],
        [-8.8811e-06, -5.3197e-06,  4.0233e-06,  ..., -7.8976e-06,
         -4.4331e-06, -5.5060e-06],
        [-1.1235e-05, -6.7577e-06,  5.0664e-06,  ..., -9.9987e-06,
         -5.1931e-06, -6.8545e-06],
        [ 8.0486e-05,  3.6970e-05, -2.0093e-05,  ...,  5.6157e-05,
          2.6389e-05,  3.9730e-05],
        [-1.9670e-05, -1.1772e-05,  8.8811e-06,  ..., -1.7434e-05,
         -9.2089e-06, -1.1981e-05]], device='cuda:0')
Loss: 1.0947105884552002


Running epoch 0, step 620, batch 620
Sampled inputs[:2]: tensor([[    0,    13, 37178,  ...,  1692,  3287, 10652],
        [    0,   598,   278,  ...,   437,   266,  2388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3896e-04, -1.8694e-04, -6.2666e-05,  ..., -1.0479e-04,
          1.0413e-04, -5.8784e-05],
        [-1.1146e-05, -6.6459e-06,  5.0291e-06,  ..., -9.8944e-06,
         -5.5730e-06, -6.8769e-06],
        [-1.4067e-05, -8.4266e-06,  6.3181e-06,  ..., -1.2502e-05,
         -6.5342e-06, -8.5458e-06],
        [ 7.6343e-05,  3.4541e-05, -1.8252e-05,  ...,  5.2491e-05,
          2.4437e-05,  3.7256e-05],
        [-2.4676e-05, -1.4707e-05,  1.1101e-05,  ..., -2.1845e-05,
         -1.1608e-05, -1.4961e-05]], device='cuda:0')
Loss: 1.1208630800247192


Running epoch 0, step 621, batch 621
Sampled inputs[:2]: tensor([[    0,   790, 43134,  ...,   446,   381,  1034],
        [    0,  1550,  2013,  ...,  9970,   638,  6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3043e-04, -2.8219e-04, -5.7704e-05,  ..., -1.5133e-04,
          8.6825e-05, -8.5219e-05],
        [-1.3322e-05, -7.9572e-06,  6.0275e-06,  ..., -1.1817e-05,
         -6.5789e-06, -8.2031e-06],
        [-1.6883e-05, -1.0133e-05,  7.6070e-06,  ..., -1.4991e-05,
         -7.7337e-06, -1.0237e-05],
        [ 7.2261e-05,  3.2068e-05, -1.6382e-05,  ...,  4.8870e-05,
          2.2708e-05,  3.4798e-05],
        [-2.9624e-05, -1.7688e-05,  1.3366e-05,  ..., -2.6196e-05,
         -1.3754e-05, -1.7926e-05]], device='cuda:0')
Loss: 1.099034309387207


Running epoch 0, step 622, batch 622
Sampled inputs[:2]: tensor([[    0,   365,   984,  ..., 18562,  4237, 31813],
        [    0,    14,   357,  ...,    30,   287,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7626e-04, -2.6336e-04,  5.3275e-05,  ..., -2.3948e-04,
          2.2567e-04, -7.6742e-06],
        [-1.5482e-05, -9.3281e-06,  7.0184e-06,  ..., -1.3769e-05,
         -7.4990e-06, -9.5293e-06],
        [ 2.7782e-04,  1.5379e-04, -1.3456e-04,  ...,  1.9981e-04,
          4.9666e-05,  2.5299e-05],
        [ 6.8207e-05,  2.9505e-05, -1.4542e-05,  ...,  4.5189e-05,
          2.1129e-05,  3.2339e-05],
        [-3.4511e-05, -2.0787e-05,  1.5587e-05,  ..., -3.0607e-05,
         -1.5721e-05, -2.0891e-05]], device='cuda:0')
Loss: 1.1224137544631958


Running epoch 0, step 623, batch 623
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0,    14,  6707,  ..., 17771,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1541e-04, -9.1139e-05, -6.1062e-05,  ..., -1.6124e-04,
          1.6991e-04, -3.8913e-05],
        [-1.7732e-05, -1.0654e-05,  8.0839e-06,  ..., -1.5870e-05,
         -8.9966e-06, -1.1139e-05],
        [ 2.7518e-04,  1.5223e-04, -1.3331e-04,  ...,  1.9735e-04,
          4.8012e-05,  2.3444e-05],
        [ 6.4437e-05,  2.7284e-05, -1.2746e-05,  ...,  4.1658e-05,
          1.8774e-05,  2.9687e-05],
        [-3.9130e-05, -2.3499e-05,  1.7777e-05,  ..., -3.4899e-05,
         -1.8612e-05, -2.4125e-05]], device='cuda:0')
Loss: 1.116133689880371
Graident accumulation at epoch 0, step 623, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0223,  0.0064, -0.0020],
        [-0.0162,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9981e-05,  1.1425e-04, -1.9964e-04,  ...,  5.1207e-05,
         -5.5572e-05,  3.6761e-05],
        [-1.9613e-05, -1.2730e-05,  6.8857e-06,  ..., -1.7440e-05,
         -6.7578e-06, -1.2168e-05],
        [ 1.2223e-04,  7.7447e-05, -5.5103e-05,  ...,  1.0657e-04,
          3.7791e-05,  5.9220e-05],
        [-1.7181e-05, -1.2569e-05,  6.8952e-06,  ..., -1.6429e-05,
         -5.5676e-06, -1.1766e-05],
        [-3.5725e-05, -2.0583e-05,  1.2596e-05,  ..., -3.1022e-05,
         -1.2259e-05, -2.2231e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6262e-08, 1.5086e-08, 1.7933e-08,  ..., 1.9553e-08, 3.0949e-08,
         6.5439e-09],
        [5.1679e-11, 2.8671e-11, 2.8173e-12,  ..., 3.6872e-11, 2.4328e-12,
         1.0494e-11],
        [2.2576e-09, 1.1076e-09, 2.3563e-10,  ..., 1.7064e-09, 1.6630e-10,
         6.7022e-10],
        [4.6180e-10, 3.7383e-10, 3.2402e-11,  ..., 3.7495e-10, 2.2773e-11,
         1.4025e-10],
        [2.3518e-10, 1.3231e-10, 1.4878e-11,  ..., 1.7210e-10, 1.0188e-11,
         5.1562e-11]], device='cuda:0')
optimizer state dict: 78.0
lr: [7.424747351382533e-06, 7.424747351382533e-06]
scheduler_last_epoch: 78


Running epoch 0, step 624, batch 624
Sampled inputs[:2]: tensor([[    0, 35449,   824,  ...,   278, 30449,  3659],
        [    0,  6508,  4305,  ...,   806,  3888,  4431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2616e-05,  1.5133e-04, -3.2992e-05,  ...,  2.5738e-05,
         -4.3693e-05,  1.4024e-05],
        [-2.3246e-06, -1.3188e-06,  9.5367e-07,  ..., -1.9670e-06,
         -1.2070e-06, -1.3337e-06],
        [-3.0100e-06, -1.7211e-06,  1.2293e-06,  ..., -2.5481e-06,
         -1.4603e-06, -1.7062e-06],
        [-4.1425e-06, -2.3693e-06,  1.6987e-06,  ..., -3.5167e-06,
         -1.9819e-06, -2.3395e-06],
        [-5.3346e-06, -3.0100e-06,  2.1756e-06,  ..., -4.4703e-06,
         -2.6375e-06, -3.0100e-06]], device='cuda:0')
Loss: 1.0803934335708618


Running epoch 0, step 625, batch 625
Sampled inputs[:2]: tensor([[    0,   380,   333,  ...,  8127,   504,   679],
        [    0,   292, 17181,  ...,   634,  5039,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0915e-04,  1.6860e-04, -1.4293e-04,  ...,  1.0208e-04,
         -8.8798e-05,  3.5505e-05],
        [-4.5002e-06, -2.6301e-06,  1.9670e-06,  ..., -3.9488e-06,
         -2.2873e-06, -2.6599e-06],
        [-5.6922e-06, -3.3453e-06,  2.4810e-06,  ..., -4.9919e-06,
         -2.6971e-06, -3.3230e-06],
        [-8.0764e-06, -4.7535e-06,  3.5390e-06,  ..., -7.1228e-06,
         -3.7849e-06, -4.7088e-06],
        [-1.0014e-05, -5.8413e-06,  4.3660e-06,  ..., -8.7321e-06,
         -4.8429e-06, -5.8264e-06]], device='cuda:0')
Loss: 1.089328408241272


Running epoch 0, step 626, batch 626
Sampled inputs[:2]: tensor([[    0,  1039,   259,  ...,   221,   685,   546],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2718e-05,  2.3261e-04, -1.9150e-04,  ...,  1.3328e-04,
         -1.1958e-04,  2.8742e-05],
        [-6.7055e-06, -3.9190e-06,  2.9653e-06,  ..., -5.8711e-06,
         -3.3379e-06, -3.9935e-06],
        [-8.5235e-06, -4.9919e-06,  3.7551e-06,  ..., -7.4506e-06,
         -3.9488e-06, -5.0068e-06],
        [-1.2010e-05, -7.0333e-06,  5.2974e-06,  ..., -1.0535e-05,
         -5.4985e-06, -7.0482e-06],
        [-1.4901e-05, -8.6725e-06,  6.5714e-06,  ..., -1.2964e-05,
         -7.0333e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.1026040315628052


Running epoch 0, step 627, batch 627
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 8271,  365,  437],
        [   0, 7432,  287,  ...,   12,  461, 2652]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4331e-04,  3.2979e-04, -1.8921e-04,  ...,  1.3889e-04,
         -1.6613e-04,  6.1343e-05],
        [-8.9407e-06, -5.2974e-06,  3.9637e-06,  ..., -7.8827e-06,
         -4.4703e-06, -5.3197e-06],
        [-1.1250e-05, -6.6832e-06,  4.9770e-06,  ..., -9.9093e-06,
         -5.2452e-06, -6.6087e-06],
        [-1.6004e-05, -9.4920e-06,  7.0781e-06,  ..., -1.4126e-05,
         -7.3761e-06, -9.3728e-06],
        [-1.9759e-05, -1.1668e-05,  8.7470e-06,  ..., -1.7345e-05,
         -9.3728e-06, -1.1578e-05]], device='cuda:0')
Loss: 1.1181869506835938


Running epoch 0, step 628, batch 628
Sampled inputs[:2]: tensor([[    0, 15666,   609,  ...,   527,  4486,     9],
        [    0,   367,  1236,  ...,   344,   292,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4778e-04,  4.3542e-04, -2.9380e-04,  ...,  2.1449e-04,
         -2.2628e-04,  9.8034e-05],
        [-1.1146e-05, -6.6459e-06,  4.9844e-06,  ..., -9.8497e-06,
         -5.5358e-06, -6.6385e-06],
        [ 7.7137e-05,  6.2801e-05, -1.6197e-05,  ...,  8.0379e-05,
          5.9507e-06,  3.9299e-05],
        [-2.0057e-05, -1.1966e-05,  8.9481e-06,  ..., -1.7747e-05,
         -9.1717e-06, -1.1772e-05],
        [-2.4557e-05, -1.4588e-05,  1.0967e-05,  ..., -2.1607e-05,
         -1.1533e-05, -1.4409e-05]], device='cuda:0')
Loss: 1.0972414016723633


Running epoch 0, step 629, batch 629
Sampled inputs[:2]: tensor([[    0,  4441,  1821,  ...,   642,  2310,    14],
        [    0,  1869,   596,  ..., 13055, 17051,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1588e-04,  3.0521e-04, -2.9652e-04,  ...,  2.5893e-04,
         -2.7543e-04,  7.8800e-05],
        [-1.3322e-05, -8.0019e-06,  5.9605e-06,  ..., -1.1727e-05,
         -6.5044e-06, -7.8678e-06],
        [ 7.4261e-05,  6.1006e-05, -1.4908e-05,  ...,  7.7890e-05,
          4.7586e-06,  3.7690e-05],
        [-2.4170e-05, -1.4529e-05,  1.0796e-05,  ..., -2.1309e-05,
         -1.0863e-05, -1.4067e-05],
        [-2.9594e-05, -1.7732e-05,  1.3232e-05,  ..., -2.5958e-05,
         -1.3664e-05, -1.7211e-05]], device='cuda:0')
Loss: 1.0915940999984741


Running epoch 0, step 630, batch 630
Sampled inputs[:2]: tensor([[    0,   287,   266,  ...,   333,   199,  3217],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9386e-04,  3.6491e-04, -3.4018e-04,  ...,  2.2915e-04,
         -2.7664e-04,  1.5936e-04],
        [-1.5542e-05, -9.4175e-06,  6.9663e-06,  ..., -1.3679e-05,
         -7.5623e-06, -9.1344e-06],
        [ 7.1489e-05,  5.9240e-05, -1.3656e-05,  ...,  7.5446e-05,
          3.5591e-06,  3.6133e-05],
        [-2.8133e-05, -1.7032e-05,  1.2584e-05,  ..., -2.4796e-05,
         -1.2569e-05, -1.6287e-05],
        [-3.4451e-05, -2.0802e-05,  1.5438e-05,  ..., -3.0220e-05,
         -1.5810e-05, -1.9938e-05]], device='cuda:0')
Loss: 1.1006412506103516


Running epoch 0, step 631, batch 631
Sampled inputs[:2]: tensor([[    0, 24414,  4865,  ...,  8720,   344,  1566],
        [    0,  2698,   221,  ...,  8352,  5680,   782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3412e-04,  3.6192e-04, -3.4591e-04,  ...,  2.3616e-04,
         -3.3266e-04,  1.2911e-04],
        [-1.7673e-05, -1.0774e-05,  8.0168e-06,  ..., -1.5564e-05,
         -8.5756e-06, -1.0446e-05],
        [ 6.8792e-05,  5.7526e-05, -1.2330e-05,  ...,  7.3062e-05,
          2.3819e-06,  3.4494e-05],
        [-3.2127e-05, -1.9565e-05,  1.4551e-05,  ..., -2.8342e-05,
         -1.4298e-05, -1.8716e-05],
        [-3.9190e-05, -2.3797e-05,  1.7777e-05,  ..., -3.4392e-05,
         -1.7926e-05, -2.2814e-05]], device='cuda:0')
Loss: 1.0811752080917358
Graident accumulation at epoch 0, step 631, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0162,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.9395e-05,  1.3902e-04, -2.1426e-04,  ...,  6.9702e-05,
         -8.3281e-05,  4.5995e-05],
        [-1.9419e-05, -1.2535e-05,  6.9988e-06,  ..., -1.7252e-05,
         -6.9396e-06, -1.1995e-05],
        [ 1.1688e-04,  7.5455e-05, -5.0826e-05,  ...,  1.0322e-04,
          3.4250e-05,  5.6748e-05],
        [-1.8676e-05, -1.3269e-05,  7.6608e-06,  ..., -1.7620e-05,
         -6.4406e-06, -1.2461e-05],
        [-3.6072e-05, -2.0905e-05,  1.3114e-05,  ..., -3.1359e-05,
         -1.2826e-05, -2.2289e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6404e-08, 1.5202e-08, 1.8035e-08,  ..., 1.9589e-08, 3.1029e-08,
         6.5541e-09],
        [5.1940e-11, 2.8758e-11, 2.8787e-12,  ..., 3.7077e-11, 2.5039e-12,
         1.0592e-11],
        [2.2601e-09, 1.1098e-09, 2.3555e-10,  ..., 1.7100e-09, 1.6614e-10,
         6.7074e-10],
        [4.6237e-10, 3.7384e-10, 3.2581e-11,  ..., 3.7537e-10, 2.2955e-11,
         1.4046e-10],
        [2.3648e-10, 1.3275e-10, 1.5179e-11,  ..., 1.7311e-10, 1.0499e-11,
         5.2031e-11]], device='cuda:0')
optimizer state dict: 79.0
lr: [7.186664259670068e-06, 7.186664259670068e-06]
scheduler_last_epoch: 79


Running epoch 0, step 632, batch 632
Sampled inputs[:2]: tensor([[   0, 2680,  271,  ..., 4971,  278,  266],
        [   0, 1760,    9,  ...,  278, 6607,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6405e-05,  2.8428e-05, -8.2086e-05,  ..., -2.2202e-05,
         -5.1494e-05,  3.2239e-06],
        [-2.2203e-06, -1.4007e-06,  1.0729e-06,  ..., -1.9521e-06,
         -1.2815e-06, -1.3784e-06],
        [-2.7418e-06, -1.7285e-06,  1.3113e-06,  ..., -2.3991e-06,
         -1.4454e-06, -1.6689e-06],
        [-3.8743e-06, -2.4438e-06,  1.8701e-06,  ..., -3.4273e-06,
         -2.0117e-06, -2.3544e-06],
        [-4.7982e-06, -3.0100e-06,  2.3097e-06,  ..., -4.2021e-06,
         -2.5481e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.0795236825942993


Running epoch 0, step 633, batch 633
Sampled inputs[:2]: tensor([[   0, 2379,   13,  ...,  287,  259, 2193],
        [   0, 2734, 2703,  ..., 7851,  280, 1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7157e-05,  2.3017e-05, -5.8413e-05,  ..., -8.1449e-05,
         -5.7876e-05, -6.2864e-05],
        [-4.4107e-06, -2.7642e-06,  2.0713e-06,  ..., -3.8445e-06,
         -2.3767e-06, -2.6226e-06],
        [-5.5283e-06, -3.4571e-06,  2.5779e-06,  ..., -4.7982e-06,
         -2.7195e-06, -3.2336e-06],
        [-7.8082e-06, -4.8727e-06,  3.6582e-06,  ..., -6.8098e-06,
         -3.7998e-06, -4.5598e-06],
        [-9.7156e-06, -6.0648e-06,  4.5598e-06,  ..., -8.4341e-06,
         -4.8429e-06, -5.6475e-06]], device='cuda:0')
Loss: 1.1108393669128418


Running epoch 0, step 634, batch 634
Sampled inputs[:2]: tensor([[    0,    17,  2736,  ...,   352,   422,    13],
        [    0,    15, 14761,  ...,   278,  3218,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4318e-05,  8.0548e-06,  1.3856e-05,  ..., -1.6290e-04,
          7.9883e-05, -9.4458e-05],
        [-6.6906e-06, -4.1947e-06,  3.1516e-06,  ..., -5.8115e-06,
         -3.7178e-06, -4.0606e-06],
        [-8.2403e-06, -5.1558e-06,  3.8594e-06,  ..., -7.1377e-06,
         -4.1723e-06, -4.9174e-06],
        [-1.1683e-05, -7.3016e-06,  5.4911e-06,  ..., -1.0163e-05,
         -5.8711e-06, -6.9588e-06],
        [-1.4573e-05, -9.0897e-06,  6.8545e-06,  ..., -1.2606e-05,
         -7.4804e-06, -8.6427e-06]], device='cuda:0')
Loss: 1.0857702493667603


Running epoch 0, step 635, batch 635
Sampled inputs[:2]: tensor([[    0,   300, 11040,  ...,   266,  1736,  3487],
        [    0,   792,   287,  ...,   706,  9751,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0671e-05, -8.2333e-06, -5.8371e-05,  ..., -1.3162e-04,
          2.3464e-05, -5.8054e-05],
        [-8.9407e-06, -5.5656e-06,  4.1574e-06,  ..., -7.7337e-06,
         -4.7758e-06, -5.2750e-06],
        [-1.1072e-05, -6.8843e-06,  5.1185e-06,  ..., -9.5516e-06,
         -5.4017e-06, -6.4299e-06],
        [-1.5706e-05, -9.7603e-06,  7.2867e-06,  ..., -1.3620e-05,
         -7.5996e-06, -9.1046e-06],
        [-1.9521e-05, -1.2100e-05,  9.0599e-06,  ..., -1.6838e-05,
         -9.6709e-06, -1.1280e-05]], device='cuda:0')
Loss: 1.0956393480300903


Running epoch 0, step 636, batch 636
Sampled inputs[:2]: tensor([[    0,   531,  9804,  ...,  1027,   360,  1576],
        [    0, 25241,   717,  ...,   413,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4885e-05,  1.8541e-05, -8.9208e-05,  ..., -1.5617e-04,
          3.2663e-05, -3.0189e-05],
        [-1.1072e-05, -6.9067e-06,  5.1260e-06,  ..., -9.5963e-06,
         -5.8189e-06, -6.5193e-06],
        [-1.3873e-05, -8.6352e-06,  6.3851e-06,  ..., -1.1995e-05,
         -6.6608e-06, -8.0317e-06],
        [-1.9729e-05, -1.2264e-05,  9.1046e-06,  ..., -1.7121e-05,
         -9.3877e-06, -1.1399e-05],
        [-2.4498e-05, -1.5199e-05,  1.1325e-05,  ..., -2.1160e-05,
         -1.1936e-05, -1.4126e-05]], device='cuda:0')
Loss: 1.111557960510254


Running epoch 0, step 637, batch 637
Sampled inputs[:2]: tensor([[   0,  992,  409,  ..., 5843,  344,  259],
        [   0,  292,  380,  ..., 6156,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3191e-04,  8.5882e-05, -3.1750e-04,  ..., -1.3188e-04,
         -9.6503e-06, -3.6475e-05],
        [-1.3292e-05, -8.2329e-06,  6.1244e-06,  ..., -1.1504e-05,
         -7.0632e-06, -7.8306e-06],
        [-1.6630e-05, -1.0274e-05,  7.6219e-06,  ..., -1.4335e-05,
         -8.0615e-06, -9.6187e-06],
        [-2.3693e-05, -1.4618e-05,  1.0885e-05,  ..., -2.0504e-05,
         -1.1399e-05, -1.3679e-05],
        [-2.9445e-05, -1.8135e-05,  1.3545e-05,  ..., -2.5362e-05,
         -1.4484e-05, -1.6972e-05]], device='cuda:0')
Loss: 1.1019985675811768


Running epoch 0, step 638, batch 638
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,  9161,   300,  4460],
        [    0,   287, 17044,  ...,   496,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0409e-05,  1.3746e-04, -3.7649e-04,  ..., -1.3985e-04,
         -1.0652e-04, -6.3045e-05],
        [-1.5467e-05, -9.5665e-06,  7.1526e-06,  ..., -1.3396e-05,
         -8.1211e-06, -9.1493e-06],
        [-1.9372e-05, -1.1951e-05,  8.9034e-06,  ..., -1.6704e-05,
         -9.2760e-06, -1.1250e-05],
        [-2.7746e-05, -1.7092e-05,  1.2793e-05,  ..., -2.4021e-05,
         -1.3188e-05, -1.6093e-05],
        [-3.4362e-05, -2.1115e-05,  1.5855e-05,  ..., -2.9594e-05,
         -1.6704e-05, -1.9878e-05]], device='cuda:0')
Loss: 1.1101248264312744


Running epoch 0, step 639, batch 639
Sampled inputs[:2]: tensor([[   0, 7180,  266,  ..., 1805,   12,  221],
        [   0,  765,  292,  ...,  623,   12, 7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5164e-04,  2.2693e-04, -6.7141e-04,  ..., -1.0541e-04,
         -2.0616e-04, -2.1786e-04],
        [-1.7688e-05, -1.0870e-05,  8.1956e-06,  ..., -1.5393e-05,
         -9.3952e-06, -1.0617e-05],
        [-2.2098e-05, -1.3545e-05,  1.0185e-05,  ..., -1.9148e-05,
         -1.0706e-05, -1.3016e-05],
        [-3.1650e-05, -1.9386e-05,  1.4640e-05,  ..., -2.7537e-05,
         -1.5214e-05, -1.8626e-05],
        [-3.9041e-05, -2.3842e-05,  1.8060e-05,  ..., -3.3766e-05,
         -1.9193e-05, -2.2888e-05]], device='cuda:0')
Loss: 1.096611738204956
Graident accumulation at epoch 0, step 639, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6619e-05,  1.4781e-04, -2.5998e-04,  ...,  5.2191e-05,
         -9.5569e-05,  1.9610e-05],
        [-1.9246e-05, -1.2368e-05,  7.1185e-06,  ..., -1.7066e-05,
         -7.1851e-06, -1.1858e-05],
        [ 1.0298e-04,  6.6555e-05, -4.4725e-05,  ...,  9.0986e-05,
          2.9755e-05,  4.9771e-05],
        [-1.9973e-05, -1.3881e-05,  8.3588e-06,  ..., -1.8612e-05,
         -7.3180e-06, -1.3077e-05],
        [-3.6369e-05, -2.1198e-05,  1.3609e-05,  ..., -3.1600e-05,
         -1.3463e-05, -2.2349e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6381e-08, 1.5238e-08, 1.8468e-08,  ..., 1.9580e-08, 3.1040e-08,
         6.5950e-09],
        [5.2201e-11, 2.8848e-11, 2.9430e-12,  ..., 3.7277e-11, 2.5897e-12,
         1.0694e-11],
        [2.2583e-09, 1.1089e-09, 2.3542e-10,  ..., 1.7087e-09, 1.6609e-10,
         6.7024e-10],
        [4.6291e-10, 3.7384e-10, 3.2763e-11,  ..., 3.7576e-10, 2.3163e-11,
         1.4066e-10],
        [2.3776e-10, 1.3318e-10, 1.5490e-11,  ..., 1.7408e-10, 1.0857e-11,
         5.2503e-11]], device='cuda:0')
optimizer state dict: 80.0
lr: [6.950300711301095e-06, 6.950300711301095e-06]
scheduler_last_epoch: 80
Epoch 0 | Batch 639/1048 | Training PPL: 6456.830912840064 | time 67.19092106819153
Saving checkpoint at epoch 0, step 639, batch 639
Epoch 0 | Validation PPL: 8.980351588032015 | Learning rate: 6.950300711301095e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_639, AFTER epoch 0, step 639


Running epoch 0, step 640, batch 640
Sampled inputs[:2]: tensor([[    0,   270,   472,  ...,   292,    73,    14],
        [    0,   292,   380,  ...,   527, 37357,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1267e-04,  2.7761e-05, -4.6792e-05,  ..., -8.0759e-05,
          2.6348e-05, -7.5787e-06],
        [-2.2203e-06, -1.3486e-06,  9.9093e-07,  ..., -1.9073e-06,
         -1.2591e-06, -1.3337e-06],
        [-2.7418e-06, -1.6615e-06,  1.2368e-06,  ..., -2.3544e-06,
         -1.4380e-06, -1.6242e-06],
        [-3.9339e-06, -2.3693e-06,  1.7658e-06,  ..., -3.3677e-06,
         -2.0415e-06, -2.3246e-06],
        [-4.8876e-06, -2.9504e-06,  2.2054e-06,  ..., -4.1723e-06,
         -2.5779e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.101613163948059


Running epoch 0, step 641, batch 641
Sampled inputs[:2]: tensor([[    0,   342,   721,  ...,  2429,    14,   475],
        [    0,  1607, 26394,  ...,    19,   471,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1569e-04,  1.1739e-04, -8.8089e-05,  ..., -2.8338e-05,
         -3.0030e-05, -2.4119e-05],
        [-4.3362e-06, -2.7120e-06,  1.9372e-06,  ..., -3.7700e-06,
         -2.1420e-06, -2.5108e-06],
        [-5.4687e-06, -3.4198e-06,  2.4587e-06,  ..., -4.7535e-06,
         -2.4661e-06, -3.1218e-06],
        [-7.9274e-06, -4.9323e-06,  3.5539e-06,  ..., -6.8843e-06,
         -3.5241e-06, -4.5151e-06],
        [-9.7454e-06, -6.0648e-06,  4.3809e-06,  ..., -8.4043e-06,
         -4.4405e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.0893983840942383


Running epoch 0, step 642, batch 642
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2930e-04,  4.5970e-05, -4.8500e-05,  ...,  3.0030e-05,
         -1.2039e-04, -6.1780e-05],
        [-6.4373e-06, -4.0382e-06,  2.8796e-06,  ..., -5.5879e-06,
         -2.9579e-06, -3.6582e-06],
        [-8.2552e-06, -5.1856e-06,  3.7029e-06,  ..., -7.1675e-06,
         -3.4422e-06, -4.6268e-06],
        [-1.1981e-05, -7.4953e-06,  5.3644e-06,  ..., -1.0401e-05,
         -4.9323e-06, -6.7055e-06],
        [-1.4663e-05, -9.1642e-06,  6.5714e-06,  ..., -1.2636e-05,
         -6.2138e-06, -8.1658e-06]], device='cuda:0')
Loss: 1.0933382511138916


Running epoch 0, step 643, batch 643
Sampled inputs[:2]: tensor([[    0,  8125,  5241,  ...,   328,  3227,   278],
        [    0,   949, 11135,  ...,   278,   772,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0112e-05,  1.5457e-04, -5.6865e-05,  ...,  1.6905e-05,
         -1.7689e-04,  4.4470e-05],
        [-8.6725e-06, -5.4091e-06,  3.8706e-06,  ..., -7.5102e-06,
         -4.1425e-06, -4.9472e-06],
        [-1.1042e-05, -6.9067e-06,  4.9546e-06,  ..., -9.5665e-06,
         -4.7982e-06, -6.2212e-06],
        [-1.6004e-05, -9.9838e-06,  7.1675e-06,  ..., -1.3873e-05,
         -6.8694e-06, -9.0003e-06],
        [-1.9670e-05, -1.2234e-05,  8.8215e-06,  ..., -1.6928e-05,
         -8.6725e-06, -1.0997e-05]], device='cuda:0')
Loss: 1.107180118560791


Running epoch 0, step 644, batch 644
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  266, 2105, 3925],
        [   0,  475,  266,  ...,  843,  287, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6316e-04,  1.0990e-04, -7.4056e-05,  ...,  1.8057e-05,
         -1.2791e-04,  4.0441e-05],
        [-1.0848e-05, -6.8024e-06,  4.8466e-06,  ..., -9.3579e-06,
         -5.1558e-06, -6.1542e-06],
        [-1.3843e-05, -8.7023e-06,  6.2063e-06,  ..., -1.1936e-05,
         -5.9903e-06, -7.7486e-06],
        [-1.9938e-05, -1.2517e-05,  8.9407e-06,  ..., -1.7241e-05,
         -8.5384e-06, -1.1161e-05],
        [-2.4647e-05, -1.5423e-05,  1.1057e-05,  ..., -2.1160e-05,
         -1.0833e-05, -1.3709e-05]], device='cuda:0')
Loss: 1.1090208292007446


Running epoch 0, step 645, batch 645
Sampled inputs[:2]: tensor([[   0, 1603,   12,  ...,   12,  756,  437],
        [   0, 2085,   12,  ...,  496,   14,  747]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7259e-04,  3.1507e-04, -1.4500e-04,  ...,  9.1885e-05,
         -2.2585e-04,  1.9510e-04],
        [-1.3158e-05, -8.1658e-06,  5.9195e-06,  ..., -1.1295e-05,
         -6.4969e-06, -7.4878e-06],
        [-1.6630e-05, -1.0349e-05,  7.4953e-06,  ..., -1.4260e-05,
         -7.4878e-06, -9.3281e-06],
        [-2.3812e-05, -1.4812e-05,  1.0736e-05,  ..., -2.0489e-05,
         -1.0580e-05, -1.3351e-05],
        [-2.9653e-05, -1.8373e-05,  1.3381e-05,  ..., -2.5332e-05,
         -1.3545e-05, -1.6540e-05]], device='cuda:0')
Loss: 1.0968581438064575


Running epoch 0, step 646, batch 646
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,  8821,  6845,   278],
        [    0, 10296,   809,  ..., 27683,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6176e-05,  4.8044e-04, -2.7315e-04,  ...,  1.5193e-04,
         -2.8213e-04,  1.8559e-04],
        [-1.5453e-05, -9.5963e-06,  6.9328e-06,  ..., -1.3262e-05,
         -7.8678e-06, -8.9034e-06],
        [-1.9342e-05, -1.2033e-05,  8.7023e-06,  ..., -1.6585e-05,
         -9.0003e-06, -1.0982e-05],
        [-2.7657e-05, -1.7196e-05,  1.2457e-05,  ..., -2.3797e-05,
         -1.2696e-05, -1.5706e-05],
        [-3.4571e-05, -2.1413e-05,  1.5572e-05,  ..., -2.9534e-05,
         -1.6302e-05, -1.9506e-05]], device='cuda:0')
Loss: 1.0930496454238892


Running epoch 0, step 647, batch 647
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6806e-05,  5.3713e-04, -2.8334e-04,  ...,  1.1654e-04,
         -2.7208e-04,  2.0380e-04],
        [-1.7732e-05, -1.1049e-05,  7.9982e-06,  ..., -1.5184e-05,
         -9.0376e-06, -1.0252e-05],
        [-2.2233e-05, -1.3880e-05,  1.0043e-05,  ..., -1.9014e-05,
         -1.0364e-05, -1.2666e-05],
        [-3.1680e-05, -1.9759e-05,  1.4320e-05,  ..., -2.7180e-05,
         -1.4566e-05, -1.8030e-05],
        [-3.9667e-05, -2.4647e-05,  1.7941e-05,  ..., -3.3796e-05,
         -1.8731e-05, -2.2456e-05]], device='cuda:0')
Loss: 1.1067488193511963
Graident accumulation at epoch 0, step 647, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.5277e-05,  1.8674e-04, -2.6231e-04,  ...,  5.8626e-05,
         -1.1322e-04,  3.8029e-05],
        [-1.9095e-05, -1.2236e-05,  7.2064e-06,  ..., -1.6878e-05,
         -7.3704e-06, -1.1697e-05],
        [ 9.0462e-05,  5.8511e-05, -3.9248e-05,  ...,  7.9986e-05,
          2.5743e-05,  4.3527e-05],
        [-2.1144e-05, -1.4468e-05,  8.9549e-06,  ..., -1.9469e-05,
         -8.0428e-06, -1.3573e-05],
        [-3.6698e-05, -2.1543e-05,  1.4042e-05,  ..., -3.1819e-05,
         -1.3990e-05, -2.2360e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6335e-08, 1.5511e-08, 1.8530e-08,  ..., 1.9574e-08, 3.1083e-08,
         6.6299e-09],
        [5.2463e-11, 2.8941e-11, 3.0040e-12,  ..., 3.7471e-11, 2.6688e-12,
         1.0789e-11],
        [2.2565e-09, 1.1080e-09, 2.3528e-10,  ..., 1.7074e-09, 1.6603e-10,
         6.6973e-10],
        [4.6345e-10, 3.7386e-10, 3.2935e-11,  ..., 3.7612e-10, 2.3352e-11,
         1.4085e-10],
        [2.3910e-10, 1.3366e-10, 1.5797e-11,  ..., 1.7505e-10, 1.1197e-11,
         5.2954e-11]], device='cuda:0')
optimizer state dict: 81.0
lr: [6.715801174410152e-06, 6.715801174410152e-06]
scheduler_last_epoch: 81


Running epoch 0, step 648, batch 648
Sampled inputs[:2]: tensor([[    0,   515,   266,  ...,    18,  3770,  1345],
        [    0,  1978, 20360,  ...,   898,   699, 10262]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2718e-05, -1.0037e-04,  7.0760e-05,  ..., -4.1811e-05,
          2.1870e-05,  3.7357e-05],
        [-2.1607e-06, -1.3560e-06,  9.8348e-07,  ..., -1.7658e-06,
         -9.5367e-07, -1.1995e-06],
        [-2.8014e-06, -1.7658e-06,  1.2740e-06,  ..., -2.3097e-06,
         -1.1325e-06, -1.5348e-06],
        [-3.9637e-06, -2.5034e-06,  1.8030e-06,  ..., -3.2634e-06,
         -1.5944e-06, -2.1756e-06],
        [-5.0962e-06, -3.1888e-06,  2.3246e-06,  ..., -4.1723e-06,
         -2.1011e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.1004788875579834


Running epoch 0, step 649, batch 649
Sampled inputs[:2]: tensor([[   0, 6584,  278,  ..., 1039,  965, 1410],
        [   0, 6809,  344,  ...,   14, 1266,  795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7366e-05,  8.0090e-05,  2.9592e-05,  ..., -5.8093e-05,
          1.4040e-05,  2.1508e-04],
        [-4.4405e-06, -2.7791e-06,  1.9521e-06,  ..., -3.7327e-06,
         -2.4438e-06, -2.6822e-06],
        [-5.4836e-06, -3.4496e-06,  2.4289e-06,  ..., -4.6194e-06,
         -2.7418e-06, -3.2485e-06],
        [-7.6890e-06, -4.8280e-06,  3.4124e-06,  ..., -6.4671e-06,
         -3.8147e-06, -4.5449e-06],
        [-9.8646e-06, -6.1691e-06,  4.3958e-06,  ..., -8.2552e-06,
         -4.9919e-06, -5.7966e-06]], device='cuda:0')
Loss: 1.084044098854065


Running epoch 0, step 650, batch 650
Sampled inputs[:2]: tensor([[    0,  3227,   300,  ...,  1817,  5709,   300],
        [    0,  1795,   365,  ...,   266, 46932,   293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5126e-05,  2.1014e-04,  4.8422e-05,  ..., -6.3976e-05,
          1.2596e-04,  3.7804e-04],
        [-6.6310e-06, -4.1202e-06,  2.8685e-06,  ..., -5.5656e-06,
         -3.4347e-06, -3.8892e-06],
        [-8.2850e-06, -5.1707e-06,  3.6061e-06,  ..., -6.9737e-06,
         -3.8818e-06, -4.7684e-06],
        [-1.1712e-05, -7.2867e-06,  5.0962e-06,  ..., -9.8348e-06,
         -5.4389e-06, -6.7204e-06],
        [-1.4931e-05, -9.2685e-06,  6.5267e-06,  ..., -1.2487e-05,
         -7.0930e-06, -8.5384e-06]], device='cuda:0')
Loss: 1.1075485944747925


Running epoch 0, step 651, batch 651
Sampled inputs[:2]: tensor([[    0,  3398,   271,  ...,    13,  1581, 13600],
        [    0,  1064,   266,  ...,  2971,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5809e-04,  2.1784e-04, -5.8379e-06,  ..., -1.0771e-04,
          2.5577e-04,  3.6399e-04],
        [-8.7172e-06, -5.4687e-06,  3.8110e-06,  ..., -7.3537e-06,
         -4.2617e-06, -5.0366e-06],
        [-1.1057e-05, -6.9588e-06,  4.8503e-06,  ..., -9.3430e-06,
         -4.8727e-06, -6.2659e-06],
        [-1.5676e-05, -9.8348e-06,  6.8843e-06,  ..., -1.3247e-05,
         -6.8620e-06, -8.8662e-06],
        [-1.9997e-05, -1.2517e-05,  8.8066e-06,  ..., -1.6809e-05,
         -8.9556e-06, -1.1265e-05]], device='cuda:0')
Loss: 1.0952881574630737


Running epoch 0, step 652, batch 652
Sampled inputs[:2]: tensor([[   0,  591, 1545,  ...,   71,  462,  221],
        [   0, 4154,   12,  ...,   14,  560,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6266e-04,  2.9786e-04, -1.9152e-05,  ..., -9.8334e-05,
          3.2763e-04,  3.9443e-04],
        [-1.0774e-05, -6.8024e-06,  4.6790e-06,  ..., -9.1419e-06,
         -4.9919e-06, -6.0946e-06],
        [-1.3813e-05, -8.7470e-06,  6.0201e-06,  ..., -1.1727e-05,
         -5.7183e-06, -7.6517e-06],
        [-1.9699e-05, -1.2442e-05,  8.6054e-06,  ..., -1.6749e-05,
         -8.1062e-06, -1.0893e-05],
        [-2.4945e-05, -1.5721e-05,  1.0937e-05,  ..., -2.1100e-05,
         -1.0543e-05, -1.3754e-05]], device='cuda:0')
Loss: 1.0815329551696777


Running epoch 0, step 653, batch 653
Sampled inputs[:2]: tensor([[   0, 6640,   13,  ...,  292,  221,  273],
        [   0,  266, 1194,  ..., 2267,   15, 1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7809e-04,  3.6119e-04, -5.2325e-05,  ..., -1.0179e-04,
          2.9812e-04,  3.8222e-04],
        [-1.2875e-05, -8.2329e-06,  5.6550e-06,  ..., -1.0982e-05,
         -6.1095e-06, -7.3835e-06],
        [-1.6451e-05, -1.0543e-05,  7.2420e-06,  ..., -1.4022e-05,
         -6.9626e-06, -9.2313e-06],
        [-2.3469e-05, -1.5005e-05,  1.0356e-05,  ..., -2.0042e-05,
         -9.8795e-06, -1.3143e-05],
        [-2.9564e-05, -1.8865e-05,  1.3098e-05,  ..., -2.5123e-05,
         -1.2778e-05, -1.6510e-05]], device='cuda:0')
Loss: 1.079789400100708


Running epoch 0, step 654, batch 654
Sampled inputs[:2]: tensor([[    0,   266,  1422,  ...,   446,  1992,   586],
        [    0,  7935,  6521,  ..., 41312,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2353e-04,  3.3791e-04, -1.8583e-05,  ..., -1.3474e-04,
          2.6966e-04,  4.2688e-04],
        [-1.5035e-05, -9.6112e-06,  6.5602e-06,  ..., -1.2800e-05,
         -7.0706e-06, -8.5905e-06],
        [-1.9252e-05, -1.2338e-05,  8.4192e-06,  ..., -1.6391e-05,
         -8.1100e-06, -1.0781e-05],
        [-2.7433e-05, -1.7539e-05,  1.2025e-05,  ..., -2.3395e-05,
         -1.1481e-05, -1.5333e-05],
        [-3.4630e-05, -2.2098e-05,  1.5229e-05,  ..., -2.9385e-05,
         -1.4879e-05, -1.9297e-05]], device='cuda:0')
Loss: 1.0931986570358276


Running epoch 0, step 655, batch 655
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,    12,  3367,  ..., 16917, 12221, 12138]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5216e-04,  2.3255e-04,  8.4384e-06,  ..., -1.6881e-04,
          4.0231e-04,  4.2688e-04],
        [-1.7226e-05, -1.0997e-05,  7.5363e-06,  ..., -1.4618e-05,
         -8.1658e-06, -9.8646e-06],
        [-2.2084e-05, -1.4126e-05,  9.6783e-06,  ..., -1.8731e-05,
         -9.3840e-06, -1.2398e-05],
        [-3.1367e-05, -2.0027e-05,  1.3784e-05,  ..., -2.6658e-05,
         -1.3255e-05, -1.7583e-05],
        [-3.9726e-05, -2.5317e-05,  1.7524e-05,  ..., -3.3587e-05,
         -1.7218e-05, -2.2203e-05]], device='cuda:0')
Loss: 1.1069438457489014
Graident accumulation at epoch 0, step 655, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2965e-05,  1.9132e-04, -2.3524e-04,  ...,  3.5883e-05,
         -6.1668e-05,  7.6914e-05],
        [-1.8908e-05, -1.2112e-05,  7.2394e-06,  ..., -1.6652e-05,
         -7.4499e-06, -1.1514e-05],
        [ 7.9208e-05,  5.1248e-05, -3.4355e-05,  ...,  7.0114e-05,
          2.2230e-05,  3.7935e-05],
        [-2.2166e-05, -1.5024e-05,  9.4377e-06,  ..., -2.0188e-05,
         -8.5639e-06, -1.3974e-05],
        [-3.7001e-05, -2.1921e-05,  1.4390e-05,  ..., -3.1996e-05,
         -1.4312e-05, -2.2344e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6352e-08, 1.5550e-08, 1.8511e-08,  ..., 1.9583e-08, 3.1214e-08,
         6.8055e-09],
        [5.2707e-11, 2.9033e-11, 3.0578e-12,  ..., 3.7647e-11, 2.7328e-12,
         1.0875e-11],
        [2.2548e-09, 1.1070e-09, 2.3514e-10,  ..., 1.7060e-09, 1.6595e-10,
         6.6921e-10],
        [4.6398e-10, 3.7388e-10, 3.3093e-11,  ..., 3.7645e-10, 2.3505e-11,
         1.4101e-10],
        [2.4044e-10, 1.3416e-10, 1.6088e-11,  ..., 1.7600e-10, 1.1482e-11,
         5.3394e-11]], device='cuda:0')
optimizer state dict: 82.0
lr: [6.4833089778264036e-06, 6.4833089778264036e-06]
scheduler_last_epoch: 82


Running epoch 0, step 656, batch 656
Sampled inputs[:2]: tensor([[   0,   26, 4044,  ..., 9531,  365,  993],
        [   0, 1796,  342,  ...,  668, 2903,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9758e-05,  7.5377e-05,  6.4089e-05,  ...,  5.3067e-05,
         -1.7923e-05,  1.4283e-04],
        [-2.3395e-06, -1.4603e-06,  9.7603e-07,  ..., -1.9073e-06,
         -1.2293e-06, -1.3262e-06],
        [-2.8908e-06, -1.8105e-06,  1.2144e-06,  ..., -2.3544e-06,
         -1.4156e-06, -1.6093e-06],
        [-4.1425e-06, -2.5928e-06,  1.7509e-06,  ..., -3.4124e-06,
         -2.0415e-06, -2.3246e-06],
        [-5.3644e-06, -3.3528e-06,  2.2650e-06,  ..., -4.3809e-06,
         -2.6673e-06, -2.9951e-06]], device='cuda:0')
Loss: 1.1260637044906616


Running epoch 0, step 657, batch 657
Sampled inputs[:2]: tensor([[    0,    14, 13078,  ...,  1994,    12,   287],
        [    0,   199,  5990,  ...,   278,   638,  5513]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3213e-05,  1.6061e-05,  7.4342e-05,  ..., -2.2707e-05,
          3.0781e-05,  9.0756e-05],
        [-4.5598e-06, -2.8312e-06,  1.9036e-06,  ..., -3.7849e-06,
         -2.2575e-06, -2.6301e-06],
        [-5.7071e-06, -3.5614e-06,  2.3916e-06,  ..., -4.7386e-06,
         -2.5705e-06, -3.2336e-06],
        [-8.1956e-06, -5.0962e-06,  3.4422e-06,  ..., -6.8545e-06,
         -3.6806e-06, -4.6492e-06],
        [-1.0461e-05, -6.5118e-06,  4.3958e-06,  ..., -8.6725e-06,
         -4.7982e-06, -5.9158e-06]], device='cuda:0')
Loss: 1.0914493799209595


Running epoch 0, step 658, batch 658
Sampled inputs[:2]: tensor([[    0,   902, 11331,  ...,  1795,   365,   654],
        [    0,     5,  7523,  ...,   199,  8871,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2101e-05,  1.3188e-05,  2.0270e-04,  ..., -5.7524e-05,
          6.4371e-05,  1.2342e-04],
        [-6.8098e-06, -4.2692e-06,  2.8349e-06,  ..., -5.6848e-06,
         -3.2783e-06, -3.8892e-06],
        [-8.5831e-06, -5.3942e-06,  3.5837e-06,  ..., -7.1675e-06,
         -3.7700e-06, -4.8131e-06],
        [-1.2368e-05, -7.7337e-06,  5.1633e-06,  ..., -1.0371e-05,
         -5.4017e-06, -6.9439e-06],
        [-1.5765e-05, -9.8795e-06,  6.6012e-06,  ..., -1.3143e-05,
         -7.0482e-06, -8.8215e-06]], device='cuda:0')
Loss: 1.1140632629394531


Running epoch 0, step 659, batch 659
Sampled inputs[:2]: tensor([[   0,  259, 5112,  ..., 3520,  278,  298],
        [   0, 1611,  266,  ...,  266, 2673, 6277]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0557e-05,  3.7892e-05,  2.4608e-04,  ..., -4.4270e-05,
         -2.4905e-05,  9.0982e-05],
        [-8.9854e-06, -5.6922e-06,  3.7551e-06,  ..., -7.5176e-06,
         -4.2394e-06, -5.0962e-06],
        [ 8.6402e-05,  5.4181e-05, -2.8171e-05,  ...,  6.3814e-05,
          2.7039e-05,  2.2873e-05],
        [-1.6391e-05, -1.0371e-05,  6.8769e-06,  ..., -1.3769e-05,
         -6.9737e-06, -9.1493e-06],
        [-2.0891e-05, -1.3232e-05,  8.7768e-06,  ..., -1.7434e-05,
         -9.1195e-06, -1.1608e-05]], device='cuda:0')
Loss: 1.1075717210769653


Running epoch 0, step 660, batch 660
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    14, 10961,    12],
        [    0,    14,   475,  ..., 44038,    12,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0415e-05,  6.0037e-05,  2.3989e-04,  ..., -3.1559e-05,
         -8.2397e-05,  1.4545e-04],
        [-1.1161e-05, -7.1302e-06,  4.7162e-06,  ..., -9.3132e-06,
         -5.1521e-06, -6.3181e-06],
        [ 8.3555e-05,  5.2289e-05, -2.6912e-05,  ...,  6.1459e-05,
          2.5944e-05,  2.1286e-05],
        [-2.0385e-05, -1.3024e-05,  8.6501e-06,  ..., -1.7092e-05,
         -8.5086e-06, -1.1384e-05],
        [-2.6017e-05, -1.6630e-05,  1.1042e-05,  ..., -2.1666e-05,
         -1.1131e-05, -1.4439e-05]], device='cuda:0')
Loss: 1.0983233451843262


Running epoch 0, step 661, batch 661
Sampled inputs[:2]: tensor([[   0,   14,  333,  ...,  328, 5453, 4713],
        [   0,   17,  292,  ..., 2269, 3887,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0033e-05,  2.5920e-04,  2.6532e-04,  ..., -2.7216e-05,
         -9.7804e-05,  2.5053e-04],
        [-1.3292e-05, -8.5309e-06,  5.6401e-06,  ..., -1.1101e-05,
         -6.1654e-06, -7.5251e-06],
        [ 8.0799e-05,  5.0471e-05, -2.5712e-05,  ...,  5.9150e-05,
          2.4774e-05,  1.9759e-05],
        [-2.4289e-05, -1.5587e-05,  1.0341e-05,  ..., -2.0370e-05,
         -1.0148e-05, -1.3560e-05],
        [-3.1024e-05, -1.9908e-05,  1.3217e-05,  ..., -2.5839e-05,
         -1.3292e-05, -1.7196e-05]], device='cuda:0')
Loss: 1.0982216596603394


Running epoch 0, step 662, batch 662
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0, 39200,  1828,  ...,   300,  3067,  4443]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5308e-04,  4.5484e-04,  2.1923e-04,  ...,  3.8562e-05,
         -5.2721e-05,  2.4967e-04],
        [-1.5393e-05, -9.9391e-06,  6.6012e-06,  ..., -1.2979e-05,
         -7.2829e-06, -8.7619e-06],
        [ 7.8161e-05,  4.8712e-05, -2.4498e-05,  ...,  5.6825e-05,
          2.3515e-05,  1.8246e-05],
        [-2.8133e-05, -1.8165e-05,  1.2122e-05,  ..., -2.3797e-05,
         -1.1981e-05, -1.5780e-05],
        [-3.5703e-05, -2.3037e-05,  1.5393e-05,  ..., -2.9981e-05,
         -1.5557e-05, -1.9878e-05]], device='cuda:0')
Loss: 1.0961458683013916


Running epoch 0, step 663, batch 663
Sampled inputs[:2]: tensor([[   0, 2202,  292,  ..., 2431, 2267, 3423],
        [   0,  266, 2086,  ..., 4283,  720,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0414e-04,  2.7917e-04,  2.6597e-04,  ..., -4.6104e-05,
          1.5574e-04,  2.0324e-04],
        [-1.7613e-05, -1.1340e-05,  7.5847e-06,  ..., -1.4812e-05,
         -8.3111e-06, -1.0014e-05],
        [ 7.5345e-05,  4.6932e-05, -2.3261e-05,  ...,  5.4500e-05,
          2.2323e-05,  1.6682e-05],
        [-3.2216e-05, -2.0742e-05,  1.3918e-05,  ..., -2.7180e-05,
         -1.3702e-05, -1.8045e-05],
        [-4.0978e-05, -2.6360e-05,  1.7717e-05,  ..., -3.4332e-05,
         -1.7852e-05, -2.2799e-05]], device='cuda:0')
Loss: 1.118784785270691
Graident accumulation at epoch 0, step 663, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4082e-05,  2.0010e-04, -1.8512e-04,  ...,  2.7684e-05,
         -3.9927e-05,  8.9546e-05],
        [-1.8778e-05, -1.2035e-05,  7.2739e-06,  ..., -1.6468e-05,
         -7.5360e-06, -1.1364e-05],
        [ 7.8821e-05,  5.0816e-05, -3.3246e-05,  ...,  6.8553e-05,
          2.2239e-05,  3.5810e-05],
        [-2.3171e-05, -1.5596e-05,  9.8857e-06,  ..., -2.0887e-05,
         -9.0777e-06, -1.4381e-05],
        [-3.7399e-05, -2.2364e-05,  1.4723e-05,  ..., -3.2230e-05,
         -1.4666e-05, -2.2389e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6317e-08, 1.5612e-08, 1.8563e-08,  ..., 1.9566e-08, 3.1207e-08,
         6.8400e-09],
        [5.2965e-11, 2.9132e-11, 3.1123e-12,  ..., 3.7829e-11, 2.7991e-12,
         1.0965e-11],
        [2.2582e-09, 1.1081e-09, 2.3545e-10,  ..., 1.7073e-09, 1.6628e-10,
         6.6882e-10],
        [4.6455e-10, 3.7394e-10, 3.3253e-11,  ..., 3.7682e-10, 2.3669e-11,
         1.4120e-10],
        [2.4188e-10, 1.3472e-10, 1.6386e-11,  ..., 1.7700e-10, 1.1789e-11,
         5.3861e-11]], device='cuda:0')
optimizer state dict: 83.0
lr: [6.252966223469409e-06, 6.252966223469409e-06]
scheduler_last_epoch: 83


Running epoch 0, step 664, batch 664
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0, 41921,  1955,  ...,    75,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9900e-06,  7.0623e-05, -1.3192e-04,  ..., -3.6963e-05,
         -1.5566e-04, -1.4692e-04],
        [-2.0713e-06, -1.4007e-06,  9.3505e-07,  ..., -1.8030e-06,
         -8.4564e-07, -1.1623e-06],
        [-1.4514e-05,  6.7711e-05, -4.6176e-05,  ...,  3.8802e-05,
          6.0781e-05,  4.7573e-05],
        [-4.0233e-06, -2.7120e-06,  1.8552e-06,  ..., -3.4720e-06,
         -1.4454e-06, -2.2054e-06],
        [-5.0664e-06, -3.4124e-06,  2.3395e-06,  ..., -4.3511e-06,
         -1.8477e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.1048835515975952


Running epoch 0, step 665, batch 665
Sampled inputs[:2]: tensor([[    0,  1456, 32380,  ...,    12,  1172, 12557],
        [    0,   380,   333,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0725e-05,  1.8684e-04, -1.2480e-04,  ...,  6.6813e-05,
         -2.0486e-04, -1.7439e-04],
        [-4.2468e-06, -2.7716e-06,  1.8403e-06,  ..., -3.6210e-06,
         -1.7546e-06, -2.3395e-06],
        [-1.7331e-05,  6.5930e-05, -4.5007e-05,  ...,  3.6447e-05,
          5.9708e-05,  4.6068e-05],
        [-8.1360e-06, -5.3048e-06,  3.5688e-06,  ..., -6.9290e-06,
         -3.0026e-06, -4.4107e-06],
        [-1.0252e-05, -6.6906e-06,  4.5002e-06,  ..., -8.6725e-06,
         -3.8594e-06, -5.5134e-06]], device='cuda:0')
Loss: 1.088990330696106


Running epoch 0, step 666, batch 666
Sampled inputs[:2]: tensor([[   0,  266, 1513,  ...,  367, 1941,  344],
        [   0, 3103, 2134,  ..., 6627,  275, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9582e-05,  8.9978e-05, -1.1123e-04,  ...,  1.1903e-05,
         -1.3432e-04, -1.5503e-04],
        [-6.5267e-06, -4.2021e-06,  2.7865e-06,  ..., -5.5134e-06,
         -2.8871e-06, -3.6135e-06],
        [-2.0177e-05,  6.4142e-05, -4.3822e-05,  ...,  3.4093e-05,
          5.8404e-05,  4.4504e-05],
        [-1.2130e-05, -7.8231e-06,  5.2378e-06,  ..., -1.0267e-05,
         -4.8354e-06, -6.6310e-06],
        [-1.5557e-05, -1.0014e-05,  6.7204e-06,  ..., -1.3053e-05,
         -6.3330e-06, -8.4192e-06]], device='cuda:0')
Loss: 1.1107680797576904


Running epoch 0, step 667, batch 667
Sampled inputs[:2]: tensor([[   0,   14,  292,  ..., 1385,   12,  287],
        [   0, 2974,  278,  ...,  365, 8758,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9582e-05,  2.0275e-04, -2.8255e-04,  ...,  1.6973e-04,
         -2.4570e-04, -1.5975e-04],
        [-8.6725e-06, -5.5954e-06,  3.6694e-06,  ..., -7.3463e-06,
         -3.8296e-06, -4.8056e-06],
        [-2.2948e-05,  6.2332e-05, -4.2667e-05,  ...,  3.1739e-05,
          5.7316e-05,  4.2991e-05],
        [-1.6183e-05, -1.0461e-05,  6.9290e-06,  ..., -1.3724e-05,
         -6.4149e-06, -8.8513e-06],
        [-2.0593e-05, -1.3292e-05,  8.8364e-06,  ..., -1.7315e-05,
         -8.3297e-06, -1.1146e-05]], device='cuda:0')
Loss: 1.0928950309753418


Running epoch 0, step 668, batch 668
Sampled inputs[:2]: tensor([[    0, 21540,   527,  ...,   824,    14,   381],
        [    0,   456,    17,  ...,  1553, 29477,  2713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7466e-05,  4.6352e-04, -2.4244e-04,  ...,  2.1073e-04,
         -3.6084e-04, -7.4530e-05],
        [-1.1027e-05, -7.0035e-06,  4.6678e-06,  ..., -9.2834e-06,
         -5.3123e-06, -6.2361e-06],
        [-2.5824e-05,  6.0618e-05, -4.1445e-05,  ...,  2.9399e-05,
          5.5633e-05,  4.1277e-05],
        [-2.0117e-05, -1.2800e-05,  8.5980e-06,  ..., -1.6928e-05,
         -8.7097e-06, -1.1206e-05],
        [-2.5898e-05, -1.6451e-05,  1.1101e-05,  ..., -2.1607e-05,
         -1.1459e-05, -1.4290e-05]], device='cuda:0')
Loss: 1.0877383947372437


Running epoch 0, step 669, batch 669
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0, 13649,  7841,  ...,   287,  4713,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9702e-05,  4.2060e-04, -1.4561e-04,  ...,  1.6797e-04,
         -3.6084e-04, -3.4627e-05],
        [-1.3202e-05, -8.4713e-06,  5.6066e-06,  ..., -1.1079e-05,
         -6.1840e-06, -7.3910e-06],
        [-2.8685e-05,  5.8666e-05, -4.0201e-05,  ...,  2.7015e-05,
          5.4582e-05,  3.9772e-05],
        [-2.4199e-05, -1.5572e-05,  1.0379e-05,  ..., -2.0340e-05,
         -1.0192e-05, -1.3366e-05],
        [-3.1203e-05, -2.0027e-05,  1.3411e-05,  ..., -2.5988e-05,
         -1.3456e-05, -1.7062e-05]], device='cuda:0')
Loss: 1.0998163223266602


Running epoch 0, step 670, batch 670
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   394,   360,  5911],
        [    0,   631,   516,  ..., 13374,   898,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0977e-05,  3.5248e-04, -1.1743e-04,  ...,  2.0057e-04,
         -3.6748e-04, -1.7816e-04],
        [-1.5408e-05, -9.9316e-06,  6.5453e-06,  ..., -1.2897e-05,
         -7.1228e-06, -8.6278e-06],
        [ 5.2928e-05,  1.0404e-04, -8.1032e-05,  ...,  8.0716e-05,
          1.0433e-04,  7.7681e-05],
        [-2.8253e-05, -1.8254e-05,  1.2115e-05,  ..., -2.3678e-05,
         -1.1735e-05, -1.5602e-05],
        [-3.6389e-05, -2.3469e-05,  1.5631e-05,  ..., -3.0249e-05,
         -1.5482e-05, -1.9908e-05]], device='cuda:0')
Loss: 1.091133713722229


Running epoch 0, step 671, batch 671
Sampled inputs[:2]: tensor([[    0,   654,   300,  ..., 21762,  3597, 11117],
        [    0,  6795,  1728,  ...,   578,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7871e-05,  4.1757e-04, -5.3493e-05,  ...,  2.2447e-04,
         -3.8457e-04, -1.3196e-04],
        [-1.7658e-05, -1.1355e-05,  7.5139e-06,  ..., -1.4752e-05,
         -8.2701e-06, -9.9540e-06],
        [ 5.0111e-05,  1.0226e-04, -7.9810e-05,  ...,  7.8407e-05,
          1.0302e-04,  7.6042e-05],
        [-3.2365e-05, -2.0847e-05,  1.3888e-05,  ..., -2.7046e-05,
         -1.3642e-05, -1.7986e-05],
        [-4.1723e-05, -2.6822e-05,  1.7941e-05,  ..., -3.4600e-05,
         -1.8016e-05, -2.2992e-05]], device='cuda:0')
Loss: 1.1104611158370972
Graident accumulation at epoch 0, step 671, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.6461e-05,  2.2185e-04, -1.7196e-04,  ...,  4.7363e-05,
         -7.4392e-05,  6.7395e-05],
        [-1.8666e-05, -1.1967e-05,  7.2979e-06,  ..., -1.6297e-05,
         -7.6095e-06, -1.1223e-05],
        [ 7.5950e-05,  5.5960e-05, -3.7902e-05,  ...,  6.9538e-05,
          3.0317e-05,  3.9833e-05],
        [-2.4091e-05, -1.6121e-05,  1.0286e-05,  ..., -2.1503e-05,
         -9.5341e-06, -1.4741e-05],
        [-3.7831e-05, -2.2810e-05,  1.5045e-05,  ..., -3.2467e-05,
         -1.5001e-05, -2.2450e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6271e-08, 1.5771e-08, 1.8548e-08,  ..., 1.9597e-08, 3.1324e-08,
         6.8506e-09],
        [5.3224e-11, 2.9232e-11, 3.1656e-12,  ..., 3.8008e-11, 2.8647e-12,
         1.1053e-11],
        [2.2584e-09, 1.1175e-09, 2.4158e-10,  ..., 1.7117e-09, 1.7673e-10,
         6.7393e-10],
        [4.6513e-10, 3.7400e-10, 3.3413e-11,  ..., 3.7717e-10, 2.3831e-11,
         1.4138e-10],
        [2.4338e-10, 1.3531e-10, 1.6691e-11,  ..., 1.7802e-10, 1.2102e-11,
         5.4336e-11]], device='cuda:0')
optimizer state dict: 84.0
lr: [6.0249136994947676e-06, 6.0249136994947676e-06]
scheduler_last_epoch: 84


Running epoch 0, step 672, batch 672
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,  1806,   319,  ...,  3427,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7644e-05,  2.7317e-05, -1.1290e-05,  ..., -2.8226e-05,
         -1.0772e-04, -1.2608e-04],
        [-2.2054e-06, -1.5199e-06,  9.2387e-07,  ..., -1.7658e-06,
         -8.9779e-07, -1.1846e-06],
        [-2.9653e-06, -2.0266e-06,  1.2517e-06,  ..., -2.3544e-06,
         -1.0803e-06, -1.5721e-06],
        [-4.1425e-06, -2.8312e-06,  1.7509e-06,  ..., -3.3081e-06,
         -1.4976e-06, -2.2054e-06],
        [-5.4538e-06, -3.7104e-06,  2.3097e-06,  ..., -4.3213e-06,
         -2.0266e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.0702248811721802


Running epoch 0, step 673, batch 673
Sampled inputs[:2]: tensor([[    0,    15,    72,  ...,   380, 22463,  2587],
        [    0,   342,   266,  ...,   586,  1944,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7303e-05,  1.2022e-04, -1.4767e-05,  ...,  6.3023e-05,
         -1.7342e-04, -1.6663e-04],
        [-4.2915e-06, -3.0324e-06,  1.7919e-06,  ..., -3.5390e-06,
         -1.7397e-06, -2.3246e-06],
        [-5.7369e-06, -4.0233e-06,  2.4289e-06,  ..., -4.6790e-06,
         -2.0340e-06, -3.0398e-06],
        [-8.0764e-06, -5.6624e-06,  3.4198e-06,  ..., -6.6310e-06,
         -2.8610e-06, -4.3064e-06],
        [-1.0550e-05, -7.3612e-06,  4.4703e-06,  ..., -8.5831e-06,
         -3.8296e-06, -5.5730e-06]], device='cuda:0')
Loss: 1.073971152305603


Running epoch 0, step 674, batch 674
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   266,  6071,  ...,  1061,  1107,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6993e-05, -7.8244e-06,  5.5217e-05,  ...,  1.0389e-04,
         -2.0547e-04, -2.1623e-04],
        [-6.4075e-06, -4.4480e-06,  2.7306e-06,  ..., -5.3421e-06,
         -2.9467e-06, -3.6359e-06],
        [-8.4341e-06, -5.8264e-06,  3.6433e-06,  ..., -6.9737e-06,
         -3.4198e-06, -4.6790e-06],
        [-1.1891e-05, -8.2105e-06,  5.1409e-06,  ..., -9.8646e-06,
         -4.8131e-06, -6.6310e-06],
        [-1.5497e-05, -1.0654e-05,  6.7055e-06,  ..., -1.2755e-05,
         -6.4224e-06, -8.5682e-06]], device='cuda:0')
Loss: 1.089066743850708


Running epoch 0, step 675, batch 675
Sampled inputs[:2]: tensor([[    0, 11752,   280,  ..., 14814,  1128,   360],
        [    0,   259,  2283,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4696e-04,  4.0383e-04, -2.0658e-04,  ...,  2.7789e-04,
         -3.3132e-04, -2.8057e-04],
        [-8.5086e-06, -5.8934e-06,  3.6359e-06,  ..., -7.1973e-06,
         -4.1164e-06, -4.9174e-06],
        [-1.1072e-05, -7.6219e-06,  4.8056e-06,  ..., -9.2685e-06,
         -4.7237e-06, -6.2436e-06],
        [-1.5691e-05, -1.0788e-05,  6.8247e-06,  ..., -1.3188e-05,
         -6.6906e-06, -8.8960e-06],
        [-2.0266e-05, -1.3873e-05,  8.8215e-06,  ..., -1.6868e-05,
         -8.7917e-06, -1.1370e-05]], device='cuda:0')
Loss: 1.0799299478530884


Running epoch 0, step 676, batch 676
Sampled inputs[:2]: tensor([[   0,  365,  925,  ...,  909,  598,  328],
        [   0, 3761,   12,  ...,   14,   22,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6688e-04,  4.8898e-04, -2.5088e-04,  ...,  3.5282e-04,
         -3.2177e-04, -3.2553e-04],
        [-1.0759e-05, -7.4059e-06,  4.5672e-06,  ..., -9.0376e-06,
         -5.3160e-06, -6.2510e-06],
        [-1.3873e-05, -9.5069e-06,  5.9828e-06,  ..., -1.1533e-05,
         -6.0722e-06, -7.8678e-06],
        [-1.9625e-05, -1.3426e-05,  8.4788e-06,  ..., -1.6376e-05,
         -8.5756e-06, -1.1191e-05],
        [-2.5392e-05, -1.7315e-05,  1.0997e-05,  ..., -2.1011e-05,
         -1.1310e-05, -1.4335e-05]], device='cuda:0')
Loss: 1.0795390605926514


Running epoch 0, step 677, batch 677
Sampled inputs[:2]: tensor([[   0, 4304, 7406,  ...,  957, 7366,  328],
        [   0, 3308,  259,  ...,   14, 6349, 1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2056e-04,  4.4355e-04, -2.0093e-04,  ...,  3.6944e-04,
         -3.2675e-04, -4.2156e-04],
        [-1.3083e-05, -8.9705e-06,  5.5432e-06,  ..., -1.0915e-05,
         -6.4485e-06, -7.4878e-06],
        [-1.6868e-05, -1.1533e-05,  7.2494e-06,  ..., -1.3947e-05,
         -7.4059e-06, -9.4473e-06],
        [-2.3797e-05, -1.6227e-05,  1.0237e-05,  ..., -1.9744e-05,
         -1.0423e-05, -1.3381e-05],
        [-3.0965e-05, -2.1055e-05,  1.3351e-05,  ..., -2.5481e-05,
         -1.3843e-05, -1.7256e-05]], device='cuda:0')
Loss: 1.0942637920379639


Running epoch 0, step 678, batch 678
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0, 29368,    13,  ...,   376,    88,  3333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7604e-04,  5.1593e-04, -1.5401e-04,  ...,  4.1129e-04,
         -3.5457e-04, -3.8492e-04],
        [-1.5259e-05, -1.0401e-05,  6.4895e-06,  ..., -1.2703e-05,
         -7.3351e-06, -8.5905e-06],
        [-1.9759e-05, -1.3441e-05,  8.5160e-06,  ..., -1.6332e-05,
         -8.4862e-06, -1.0893e-05],
        [-2.7969e-05, -1.8969e-05,  1.2062e-05,  ..., -2.3186e-05,
         -1.1973e-05, -1.5467e-05],
        [-3.6448e-05, -2.4661e-05,  1.5751e-05,  ..., -2.9981e-05,
         -1.5944e-05, -1.9982e-05]], device='cuda:0')
Loss: 1.1332601308822632


Running epoch 0, step 679, batch 679
Sampled inputs[:2]: tensor([[    0, 19641,   437,  ...,  2992,   518,   266],
        [    0,  1420,  2337,  ...,   722, 28860,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7106e-04,  6.0041e-04, -1.5401e-04,  ...,  4.6180e-04,
         -3.4476e-04, -2.8665e-04],
        [-1.7479e-05, -1.1899e-05,  7.3835e-06,  ..., -1.4462e-05,
         -8.2962e-06, -9.7528e-06],
        [-2.2724e-05, -1.5438e-05,  9.7156e-06,  ..., -1.8686e-05,
         -9.6634e-06, -1.2428e-05],
        [-3.2112e-05, -2.1756e-05,  1.3739e-05,  ..., -2.6464e-05,
         -1.3605e-05, -1.7613e-05],
        [-4.1932e-05, -2.8357e-05,  1.7986e-05,  ..., -3.4332e-05,
         -1.8165e-05, -2.2799e-05]], device='cuda:0')
Loss: 1.1095463037490845
Graident accumulation at epoch 0, step 679, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4921e-05,  2.5971e-04, -1.7016e-04,  ...,  8.8806e-05,
         -1.0143e-04,  3.1990e-05],
        [-1.8548e-05, -1.1960e-05,  7.3065e-06,  ..., -1.6113e-05,
         -7.6781e-06, -1.1076e-05],
        [ 6.6083e-05,  4.8820e-05, -3.3141e-05,  ...,  6.0716e-05,
          2.6319e-05,  3.4607e-05],
        [-2.4893e-05, -1.6685e-05,  1.0631e-05,  ..., -2.1999e-05,
         -9.9412e-06, -1.5028e-05],
        [-3.8241e-05, -2.3365e-05,  1.5339e-05,  ..., -3.2653e-05,
         -1.5318e-05, -2.2485e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6254e-08, 1.6116e-08, 1.8553e-08,  ..., 1.9790e-08, 3.1412e-08,
         6.9259e-09],
        [5.3476e-11, 2.9345e-11, 3.2170e-12,  ..., 3.8180e-11, 2.9307e-12,
         1.1137e-11],
        [2.2567e-09, 1.1166e-09, 2.4143e-10,  ..., 1.7103e-09, 1.7665e-10,
         6.7341e-10],
        [4.6570e-10, 3.7410e-10, 3.3568e-11,  ..., 3.7749e-10, 2.3993e-11,
         1.4155e-10],
        [2.4489e-10, 1.3598e-10, 1.6998e-11,  ..., 1.7902e-10, 1.2420e-11,
         5.4801e-11]], device='cuda:0')
optimizer state dict: 85.0
lr: [5.799290794242787e-06, 5.799290794242787e-06]
scheduler_last_epoch: 85


Running epoch 0, step 680, batch 680
Sampled inputs[:2]: tensor([[   0,  374, 5195,  ...,  266, 5555,   14],
        [   0,  266, 1254,  ...,  369, 2870,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1442e-05,  9.2626e-05, -4.7228e-05,  ..., -1.3048e-05,
         -3.3933e-05, -4.9049e-05],
        [-2.1458e-06, -1.4752e-06,  8.9779e-07,  ..., -1.8775e-06,
         -1.0058e-06, -1.1623e-06],
        [-2.7567e-06, -1.8850e-06,  1.1623e-06,  ..., -2.3842e-06,
         -1.1548e-06, -1.4603e-06],
        [-4.1127e-06, -2.8014e-06,  1.7360e-06,  ..., -3.5614e-06,
         -1.7211e-06, -2.1905e-06],
        [-5.2154e-06, -3.5465e-06,  2.2203e-06,  ..., -4.4703e-06,
         -2.2352e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0941630601882935


Running epoch 0, step 681, batch 681
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0,  278, 1478,  ...,  266, 1607, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4098e-05, -3.9082e-06, -6.1359e-05,  ..., -1.1588e-04,
          4.6290e-05, -1.1383e-05],
        [-4.3362e-06, -2.9579e-06,  1.8105e-06,  ..., -3.8147e-06,
         -2.4661e-06, -2.6226e-06],
        [-5.4538e-06, -3.7178e-06,  2.3022e-06,  ..., -4.7237e-06,
         -2.7046e-06, -3.1814e-06],
        [-7.8529e-06, -5.3346e-06,  3.3230e-06,  ..., -6.7949e-06,
         -3.8221e-06, -4.5598e-06],
        [-1.0252e-05, -6.9588e-06,  4.3660e-06,  ..., -8.7917e-06,
         -5.0962e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.0706651210784912


Running epoch 0, step 682, batch 682
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,   221, 18844,  ...,   199, 10174,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5493e-05,  5.8159e-06, -2.2450e-05,  ..., -1.0312e-04,
          1.4435e-06,  8.1234e-05],
        [-6.4969e-06, -4.4778e-06,  2.7530e-06,  ..., -5.6401e-06,
         -3.5241e-06, -3.8147e-06],
        [-8.2552e-06, -5.6997e-06,  3.5390e-06,  ..., -7.0781e-06,
         -3.9488e-06, -4.7013e-06],
        [-1.1817e-05, -8.1211e-06,  5.0738e-06,  ..., -1.0148e-05,
         -5.5730e-06, -6.7055e-06],
        [-1.5557e-05, -1.0684e-05,  6.7055e-06,  ..., -1.3232e-05,
         -7.4953e-06, -8.7470e-06]], device='cuda:0')
Loss: 1.098876953125


Running epoch 0, step 683, batch 683
Sampled inputs[:2]: tensor([[    0,   287,  2997,  ...,   437,   266,  1040],
        [    0,  4347,   638,  ...,  1345,   292, 15343]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1934e-04, -4.1986e-05, -8.3787e-05,  ..., -4.0177e-05,
         -1.1682e-04,  9.2329e-05],
        [-8.6576e-06, -5.9828e-06,  3.6955e-06,  ..., -7.5102e-06,
         -4.6343e-06, -5.0068e-06],
        [-1.1027e-05, -7.6368e-06,  4.7609e-06,  ..., -9.4771e-06,
         -5.2527e-06, -6.2063e-06],
        [-1.5780e-05, -1.0878e-05,  6.8322e-06,  ..., -1.3590e-05,
         -7.4357e-06, -8.8662e-06],
        [-2.0772e-05, -1.4290e-05,  9.0152e-06,  ..., -1.7703e-05,
         -9.9838e-06, -1.1563e-05]], device='cuda:0')
Loss: 1.1195871829986572


Running epoch 0, step 684, batch 684
Sampled inputs[:2]: tensor([[   0,   45,   17,  ...,  278, 4112,   14],
        [   0,   12,  344,  ...,  824,   12,  968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1172e-04,  1.9580e-04, -2.9788e-05,  ...,  4.7021e-05,
         -3.0617e-04,  1.4750e-06],
        [-1.0788e-05, -7.4506e-06,  4.5747e-06,  ..., -9.2983e-06,
         -5.5656e-06, -6.1616e-06],
        [-1.3858e-05, -9.5889e-06,  5.9456e-06,  ..., -1.1832e-05,
         -6.3702e-06, -7.7114e-06],
        [-1.9833e-05, -1.3649e-05,  8.5309e-06,  ..., -1.6958e-05,
         -9.0227e-06, -1.1027e-05],
        [-2.6137e-05, -1.7971e-05,  1.1280e-05,  ..., -2.2143e-05,
         -1.2130e-05, -1.4395e-05]], device='cuda:0')
Loss: 1.0917046070098877


Running epoch 0, step 685, batch 685
Sampled inputs[:2]: tensor([[    0,    13,    19,  ..., 22111,  2489,    14],
        [    0,   360,   508,  ...,   259,   554,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9182e-04,  1.4417e-04,  6.0331e-05,  ...,  3.8065e-05,
         -3.6561e-04,  2.8187e-05],
        [-1.2934e-05, -8.9481e-06,  5.4799e-06,  ..., -1.1124e-05,
         -6.5565e-06, -7.3016e-06],
        [-1.6704e-05, -1.1586e-05,  7.1526e-06,  ..., -1.4260e-05,
         -7.5772e-06, -9.2015e-06],
        [-2.3797e-05, -1.6421e-05,  1.0215e-05,  ..., -2.0355e-05,
         -1.0692e-05, -1.3113e-05],
        [-3.1441e-05, -2.1666e-05,  1.3530e-05,  ..., -2.6643e-05,
         -1.4424e-05, -1.7151e-05]], device='cuda:0')
Loss: 1.0996161699295044


Running epoch 0, step 686, batch 686
Sampled inputs[:2]: tensor([[    0,  1340,   800,  ...,   259, 13583,   422],
        [    0,  5775,   292,  ...,  8671,  1339,   642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8697e-04,  9.9759e-05,  5.6841e-05,  ...,  1.0311e-04,
         -4.7283e-04, -7.0111e-06],
        [-1.5005e-05, -1.0453e-05,  6.4000e-06,  ..., -1.2897e-05,
         -7.5400e-06, -8.5235e-06],
        [-1.9446e-05, -1.3568e-05,  8.3819e-06,  ..., -1.6585e-05,
         -8.7321e-06, -1.0781e-05],
        [-2.7761e-05, -1.9297e-05,  1.2003e-05,  ..., -2.3738e-05,
         -1.2368e-05, -1.5423e-05],
        [-3.6508e-05, -2.5302e-05,  1.5810e-05,  ..., -3.0905e-05,
         -1.6585e-05, -2.0042e-05]], device='cuda:0')
Loss: 1.0910401344299316


Running epoch 0, step 687, batch 687
Sampled inputs[:2]: tensor([[    0,   586,  1016,  ...,  7151,  8280,   300],
        [    0,   446, 21112,  ..., 22092,    22,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2417e-04,  6.5624e-05,  7.4857e-05,  ...,  8.5424e-05,
         -4.6926e-04, -5.9487e-05],
        [-1.7196e-05, -1.1958e-05,  7.3016e-06,  ..., -1.4700e-05,
         -8.5011e-06, -9.6560e-06],
        [-2.2396e-05, -1.5594e-05,  9.6038e-06,  ..., -1.8999e-05,
         -9.9242e-06, -1.2279e-05],
        [-3.1903e-05, -2.2143e-05,  1.3724e-05,  ..., -2.7150e-05,
         -1.4044e-05, -1.7539e-05],
        [-4.2021e-05, -2.9087e-05,  1.8105e-05,  ..., -3.5435e-05,
         -1.8865e-05, -2.2843e-05]], device='cuda:0')
Loss: 1.1099358797073364
Graident accumulation at epoch 0, step 687, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0152, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3012e-05,  2.4030e-04, -1.4566e-04,  ...,  8.8468e-05,
         -1.3821e-04,  2.2843e-05],
        [-1.8412e-05, -1.1960e-05,  7.3060e-06,  ..., -1.5972e-05,
         -7.7604e-06, -1.0934e-05],
        [ 5.7235e-05,  4.2379e-05, -2.8866e-05,  ...,  5.2744e-05,
          2.2695e-05,  2.9918e-05],
        [-2.5594e-05, -1.7230e-05,  1.0941e-05,  ..., -2.2514e-05,
         -1.0352e-05, -1.5280e-05],
        [-3.8619e-05, -2.3937e-05,  1.5616e-05,  ..., -3.2931e-05,
         -1.5672e-05, -2.2521e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6258e-08, 1.6104e-08, 1.8540e-08,  ..., 1.9778e-08, 3.1600e-08,
         6.9225e-09],
        [5.3718e-11, 2.9458e-11, 3.2671e-12,  ..., 3.8357e-11, 3.0000e-12,
         1.1219e-11],
        [2.2550e-09, 1.1157e-09, 2.4128e-10,  ..., 1.7090e-09, 1.7657e-10,
         6.7289e-10],
        [4.6625e-10, 3.7422e-10, 3.3723e-11,  ..., 3.7785e-10, 2.4166e-11,
         1.4172e-10],
        [2.4641e-10, 1.3669e-10, 1.7309e-11,  ..., 1.8010e-10, 1.2763e-11,
         5.5268e-11]], device='cuda:0')
optimizer state dict: 86.0
lr: [5.576235411042707e-06, 5.576235411042707e-06]
scheduler_last_epoch: 86


Running epoch 0, step 688, batch 688
Sampled inputs[:2]: tensor([[    0,    30,  1869,  ...,  4998, 44266,    12],
        [    0,   266, 30368,  ...,   950,   266,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1118e-05, -1.3860e-05,  3.8686e-05,  ...,  5.9381e-06,
          9.0873e-06, -4.6698e-05],
        [-2.1011e-06, -1.5572e-06,  8.9779e-07,  ..., -1.7732e-06,
         -8.8289e-07, -1.1027e-06],
        [-2.8461e-06, -2.1011e-06,  1.2219e-06,  ..., -2.3842e-06,
         -1.0952e-06, -1.4678e-06],
        [-4.0233e-06, -2.9802e-06,  1.7360e-06,  ..., -3.3826e-06,
         -1.5423e-06, -2.0862e-06],
        [-5.4240e-06, -3.9935e-06,  2.3246e-06,  ..., -4.5300e-06,
         -2.1160e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.1004753112792969


Running epoch 0, step 689, batch 689
Sampled inputs[:2]: tensor([[    0,     9, 25368,  ...,   271,   266,  1136],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8204e-05,  3.1192e-05, -7.8864e-05,  ...,  2.4493e-06,
         -8.2243e-05, -8.6442e-05],
        [-4.1127e-06, -3.1069e-06,  1.7472e-06,  ..., -3.5465e-06,
         -1.8366e-06, -2.2724e-06],
        [-5.5432e-06, -4.1723e-06,  2.3991e-06,  ..., -4.7386e-06,
         -2.2203e-06, -3.0026e-06],
        [-7.8976e-06, -5.9307e-06,  3.4273e-06,  ..., -6.7651e-06,
         -3.1590e-06, -4.2915e-06],
        [-1.0461e-05, -7.8082e-06,  4.5300e-06,  ..., -8.8811e-06,
         -4.2468e-06, -5.6028e-06]], device='cuda:0')
Loss: 1.0908496379852295


Running epoch 0, step 690, batch 690
Sampled inputs[:2]: tensor([[   0,  273,  298,  ..., 7437, 2767,  518],
        [   0, 1217,    9,  ..., 1821,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8496e-04,  2.3246e-04, -1.7347e-04,  ...,  7.0923e-05,
         -2.4536e-04, -6.2772e-05],
        [-6.1542e-06, -4.5896e-06,  2.6412e-06,  ..., -5.3719e-06,
         -2.8573e-06, -3.4571e-06],
        [-8.2552e-06, -6.1393e-06,  3.6135e-06,  ..., -7.1377e-06,
         -3.4273e-06, -4.5300e-06],
        [-1.1772e-05, -8.7470e-06,  5.1633e-06,  ..., -1.0222e-05,
         -4.8950e-06, -6.4969e-06],
        [-1.5527e-05, -1.1474e-05,  6.8098e-06,  ..., -1.3351e-05,
         -6.5416e-06, -8.4341e-06]], device='cuda:0')
Loss: 1.0899624824523926


Running epoch 0, step 691, batch 691
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0,  809,  367,  ...,  717,  287, 1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0900e-05,  4.3944e-04, -4.0765e-04,  ...,  2.2172e-04,
         -4.2321e-04, -2.0488e-05],
        [-8.2999e-06, -6.1020e-06,  3.4757e-06,  ..., -7.1898e-06,
         -4.0568e-06, -4.7386e-06],
        [-1.1042e-05, -8.0913e-06,  4.7237e-06,  ..., -9.4622e-06,
         -4.8205e-06, -6.1467e-06],
        [-1.5706e-05, -1.1504e-05,  6.7353e-06,  ..., -1.3515e-05,
         -6.8620e-06, -8.7917e-06],
        [-2.0713e-05, -1.5095e-05,  8.8811e-06,  ..., -1.7643e-05,
         -9.1642e-06, -1.1414e-05]], device='cuda:0')
Loss: 1.0505282878875732


Running epoch 0, step 692, batch 692
Sampled inputs[:2]: tensor([[    0,   266, 20604,  ...,   409, 13764,  6048],
        [    0,  1690,  2558,  ...,  2025,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7787e-05,  2.0104e-04, -4.0382e-04,  ...,  2.0130e-04,
         -3.9039e-04, -7.7394e-05],
        [-1.0520e-05, -7.7114e-06,  4.3809e-06,  ..., -8.9854e-06,
         -5.0999e-06, -5.9158e-06],
        [-1.3947e-05, -1.0192e-05,  5.9232e-06,  ..., -1.1817e-05,
         -6.0946e-06, -7.6815e-06],
        [-1.9848e-05, -1.4499e-05,  8.4341e-06,  ..., -1.6868e-05,
         -8.6576e-06, -1.0967e-05],
        [-2.6256e-05, -1.9088e-05,  1.1161e-05,  ..., -2.2113e-05,
         -1.1638e-05, -1.4320e-05]], device='cuda:0')
Loss: 1.1264827251434326


Running epoch 0, step 693, batch 693
Sampled inputs[:2]: tensor([[    0,   516,   596,  ...,  3109,   287,   394],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5523e-05,  1.1868e-04, -4.5242e-04,  ...,  2.0685e-04,
         -4.1072e-04, -4.7747e-05],
        [-1.2740e-05, -9.3207e-06,  5.3719e-06,  ..., -1.0729e-05,
         -6.1132e-06, -7.1153e-06],
        [-1.6928e-05, -1.2353e-05,  7.2643e-06,  ..., -1.4156e-05,
         -7.3165e-06, -9.2685e-06],
        [ 4.7406e-04,  3.4921e-04, -1.2837e-04,  ...,  3.8767e-04,
          1.9134e-04,  2.7730e-04],
        [-3.1829e-05, -2.3142e-05,  1.3664e-05,  ..., -2.6464e-05,
         -1.3977e-05, -1.7270e-05]], device='cuda:0')
Loss: 1.102962851524353


Running epoch 0, step 694, batch 694
Sampled inputs[:2]: tensor([[    0,  2923,   266,  ...,  7763,   360,  1255],
        [    0,   266,   944,  ..., 14981,  1952,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9172e-05,  8.1442e-05, -4.5572e-04,  ...,  2.2342e-04,
         -4.0556e-04, -5.7026e-05],
        [-1.5050e-05, -1.0900e-05,  6.3106e-06,  ..., -1.2569e-05,
         -7.3202e-06, -8.3894e-06],
        [-1.9848e-05, -1.4365e-05,  8.4713e-06,  ..., -1.6481e-05,
         -8.7172e-06, -1.0855e-05],
        [ 4.6998e-04,  3.4641e-04, -1.2669e-04,  ...,  3.8442e-04,
          1.8941e-04,  2.7508e-04],
        [-3.7313e-05, -2.6882e-05,  1.5944e-05,  ..., -3.0816e-05,
         -1.6630e-05, -2.0221e-05]], device='cuda:0')
Loss: 1.1082323789596558


Running epoch 0, step 695, batch 695
Sampled inputs[:2]: tensor([[    0,   527,   496,  ...,    12,   795,  8296],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0854e-04, -1.0634e-04, -3.7766e-04,  ...,  2.2977e-04,
         -3.4043e-04, -8.0801e-06],
        [-1.7256e-05, -1.2502e-05,  7.2271e-06,  ..., -1.4402e-05,
         -8.4601e-06, -9.6411e-06],
        [-2.2739e-05, -1.6466e-05,  9.6783e-06,  ..., -1.8880e-05,
         -1.0058e-05, -1.2465e-05],
        [ 4.6599e-04,  3.4350e-04, -1.2501e-04,  ...,  3.8110e-04,
          1.8756e-04,  2.7285e-04],
        [-4.2766e-05, -3.0845e-05,  1.8239e-05,  ..., -3.5316e-05,
         -1.9222e-05, -2.3246e-05]], device='cuda:0')
Loss: 1.1141637563705444
Graident accumulation at epoch 0, step 695, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5857e-05,  2.0563e-04, -1.6886e-04,  ...,  1.0260e-04,
         -1.5843e-04,  1.9750e-05],
        [-1.8297e-05, -1.2014e-05,  7.2981e-06,  ..., -1.5815e-05,
         -7.8304e-06, -1.0805e-05],
        [ 4.9237e-05,  3.6495e-05, -2.5012e-05,  ...,  4.5582e-05,
          1.9419e-05,  2.5680e-05],
        [ 2.3564e-05,  1.8843e-05, -2.6547e-06,  ...,  1.7847e-05,
          9.4395e-06,  1.3533e-05],
        [-3.9034e-05, -2.4628e-05,  1.5878e-05,  ..., -3.3170e-05,
         -1.6027e-05, -2.2593e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6255e-08, 1.6099e-08, 1.8664e-08,  ..., 1.9811e-08, 3.1685e-08,
         6.9156e-09],
        [5.3962e-11, 2.9585e-11, 3.3160e-12,  ..., 3.8527e-11, 3.0686e-12,
         1.1301e-11],
        [2.2532e-09, 1.1149e-09, 2.4114e-10,  ..., 1.7076e-09, 1.7649e-10,
         6.7237e-10],
        [6.8293e-10, 4.9184e-10, 4.9317e-11,  ..., 5.2271e-10, 5.9320e-11,
         2.1602e-10],
        [2.4799e-10, 1.3750e-10, 1.7624e-11,  ..., 1.8117e-10, 1.3120e-11,
         5.5753e-11]], device='cuda:0')
optimizer state dict: 87.0
lr: [5.355883883924591e-06, 5.355883883924591e-06]
scheduler_last_epoch: 87


Running epoch 0, step 696, batch 696
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0,   300,  3808,  ...,   496,    14,  1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7154e-05,  1.6178e-04, -8.3543e-05,  ...,  1.1445e-04,
         -3.6756e-05,  8.1907e-07],
        [-2.0564e-06, -1.4976e-06,  8.2329e-07,  ..., -1.8030e-06,
         -1.0431e-06, -1.1846e-06],
        [-2.7418e-06, -1.9968e-06,  1.1325e-06,  ..., -2.3544e-06,
         -1.2293e-06, -1.5348e-06],
        [-3.9041e-06, -2.8312e-06,  1.6093e-06,  ..., -3.3975e-06,
         -1.7732e-06, -2.2054e-06],
        [-5.1856e-06, -3.7700e-06,  2.1607e-06,  ..., -4.4703e-06,
         -2.3693e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.107661247253418


Running epoch 0, step 697, batch 697
Sampled inputs[:2]: tensor([[    0,  1008,   266,  ...,  1941,   437,  1626],
        [    0,   328, 16219,  ..., 14559,   351,   587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4396e-05,  1.4930e-04, -1.7174e-04,  ...,  1.7190e-04,
         -9.8087e-06, -3.3453e-05],
        [-4.2021e-06, -3.0696e-06,  1.7174e-06,  ..., -3.5539e-06,
         -1.9670e-06, -2.3842e-06],
        [-5.6177e-06, -4.1127e-06,  2.3469e-06,  ..., -4.6939e-06,
         -2.3469e-06, -3.1143e-06],
        [-8.1360e-06, -5.9307e-06,  3.3900e-06,  ..., -6.8396e-06,
         -3.4198e-06, -4.5449e-06],
        [-1.0729e-05, -7.8529e-06,  4.5151e-06,  ..., -8.9705e-06,
         -4.5747e-06, -5.9307e-06]], device='cuda:0')
Loss: 1.0879586935043335


Running epoch 0, step 698, batch 698
Sampled inputs[:2]: tensor([[   0,  638, 1862,  ...,   14, 7869,   14],
        [   0,  635,   13,  ...,   13, 4710, 1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7260e-05,  2.5545e-04, -1.0444e-04,  ...,  1.1396e-04,
         -1.3722e-05,  5.9774e-05],
        [-6.2734e-06, -4.6715e-06,  2.5667e-06,  ..., -5.3272e-06,
         -3.0100e-06, -3.5688e-06],
        [-8.3894e-06, -6.2585e-06,  3.5018e-06,  ..., -7.0482e-06,
         -3.6210e-06, -4.6641e-06],
        [-1.2040e-05, -8.9258e-06,  5.0142e-06,  ..., -1.0148e-05,
         -5.2154e-06, -6.7353e-06],
        [-1.6004e-05, -1.1906e-05,  6.7204e-06,  ..., -1.3441e-05,
         -7.0333e-06, -8.8662e-06]], device='cuda:0')
Loss: 1.0871024131774902


Running epoch 0, step 699, batch 699
Sampled inputs[:2]: tensor([[    0,   266, 28695,  ...,   278,   266,  6087],
        [    0,  1487,  2511,  ..., 27735,   760,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2361e-07,  2.3829e-04, -1.1916e-04,  ...,  1.7622e-04,
          1.0705e-05, -3.4388e-05],
        [-8.4192e-06, -6.1914e-06,  3.5055e-06,  ..., -7.0482e-06,
         -3.8408e-06, -4.6641e-06],
        [-1.1474e-05, -8.4341e-06,  4.8578e-06,  ..., -9.5069e-06,
         -4.7013e-06, -6.2138e-06],
        [-1.6302e-05, -1.1921e-05,  6.8918e-06,  ..., -1.3560e-05,
         -6.6981e-06, -8.8811e-06],
        [-2.1815e-05, -1.5989e-05,  9.2685e-06,  ..., -1.8060e-05,
         -9.1195e-06, -1.1772e-05]], device='cuda:0')
Loss: 1.0986998081207275


Running epoch 0, step 700, batch 700
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ...,   298,   365,  4920],
        [    0, 23749, 27341,  ..., 34110,   342,  9672]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9805e-05,  4.7291e-04, -1.9554e-04,  ...,  3.2544e-04,
         -1.8716e-04, -5.5155e-05],
        [-1.0461e-05, -7.6964e-06,  4.3549e-06,  ..., -8.8364e-06,
         -4.6752e-06, -5.7891e-06],
        [-1.4216e-05, -1.0446e-05,  6.0275e-06,  ..., -1.1876e-05,
         -5.6699e-06, -7.6741e-06],
        [-2.0325e-05, -1.4856e-05,  8.6054e-06,  ..., -1.7047e-05,
         -8.1137e-06, -1.1027e-05],
        [-2.7090e-05, -1.9833e-05,  1.1533e-05,  ..., -2.2590e-05,
         -1.1027e-05, -1.4573e-05]], device='cuda:0')
Loss: 1.0917370319366455


Running epoch 0, step 701, batch 701
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0,   494,   298,  ...,   408, 32859, 14550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0414e-04,  6.4268e-04, -3.5820e-04,  ...,  4.7948e-04,
         -3.1823e-04, -7.9848e-05],
        [-1.2562e-05, -9.1940e-06,  5.1670e-06,  ..., -1.0669e-05,
         -6.1132e-06, -7.1377e-06],
        [-1.6943e-05, -1.2383e-05,  7.1302e-06,  ..., -1.4201e-05,
         -7.3314e-06, -9.3579e-06],
        [-2.4170e-05, -1.7583e-05,  1.0155e-05,  ..., -2.0340e-05,
         -1.0423e-05, -1.3396e-05],
        [-3.2246e-05, -2.3469e-05,  1.3620e-05,  ..., -2.6971e-05,
         -1.4171e-05, -1.7717e-05]], device='cuda:0')
Loss: 1.0911368131637573


Running epoch 0, step 702, batch 702
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,  3253,  1573,  ...,   298,   358,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4267e-04,  8.0596e-04, -2.9165e-04,  ...,  4.6525e-04,
         -3.9361e-04, -4.9716e-05],
        [-1.4722e-05, -1.0766e-05,  6.0424e-06,  ..., -1.2465e-05,
         -7.1041e-06, -8.2925e-06],
        [-1.9923e-05, -1.4544e-05,  8.3521e-06,  ..., -1.6659e-05,
         -8.5756e-06, -1.0930e-05],
        [-2.8282e-05, -2.0564e-05,  1.1839e-05,  ..., -2.3738e-05,
         -1.2122e-05, -1.5572e-05],
        [ 1.6086e-04,  6.5631e-05, -4.3971e-05,  ...,  1.1872e-04,
          1.2919e-04,  7.9599e-05]], device='cuda:0')
Loss: 1.1150535345077515


Running epoch 0, step 703, batch 703
Sampled inputs[:2]: tensor([[    0,  1049,    12,  ...,   292,  3963,   755],
        [    0, 10446,    14,  ...,   266,  1164,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4165e-04,  9.1873e-04, -3.7498e-04,  ...,  5.4440e-04,
         -4.8959e-04, -1.1992e-04],
        [-1.6749e-05, -1.2323e-05,  6.8955e-06,  ..., -1.4253e-05,
         -8.1398e-06, -9.4771e-06],
        [-2.2605e-05, -1.6585e-05,  9.5218e-06,  ..., -1.8984e-05,
         -9.7752e-06, -1.2450e-05],
        [-3.2097e-05, -2.3469e-05,  1.3500e-05,  ..., -2.7061e-05,
         -1.3843e-05, -1.7762e-05],
        [ 1.5585e-04,  6.1831e-05, -4.1781e-05,  ...,  1.1440e-04,
          1.2693e-04,  7.6782e-05]], device='cuda:0')
Loss: 1.101457953453064
Graident accumulation at epoch 0, step 703, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6436e-05,  2.7694e-04, -1.8947e-04,  ...,  1.4678e-04,
         -1.9155e-04,  5.7834e-06],
        [-1.8142e-05, -1.2045e-05,  7.2579e-06,  ..., -1.5659e-05,
         -7.8613e-06, -1.0672e-05],
        [ 4.2053e-05,  3.1187e-05, -2.1558e-05,  ...,  3.9125e-05,
          1.6500e-05,  2.1867e-05],
        [ 1.7998e-05,  1.4612e-05, -1.0392e-06,  ...,  1.3356e-05,
          7.1112e-06,  1.0403e-05],
        [-1.9546e-05, -1.5982e-05,  1.0112e-05,  ..., -1.8413e-05,
         -1.7320e-06, -1.2656e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6267e-08, 1.6927e-08, 1.8786e-08,  ..., 2.0087e-08, 3.1893e-08,
         6.9231e-09],
        [5.4189e-11, 2.9707e-11, 3.3603e-12,  ..., 3.8691e-11, 3.1318e-12,
         1.1379e-11],
        [2.2515e-09, 1.1141e-09, 2.4099e-10,  ..., 1.7063e-09, 1.7641e-10,
         6.7186e-10],
        [6.8328e-10, 4.9190e-10, 4.9450e-11,  ..., 5.2292e-10, 5.9452e-11,
         2.1612e-10],
        [2.7204e-10, 1.4119e-10, 1.9352e-11,  ..., 1.9407e-10, 2.9217e-11,
         6.1593e-11]], device='cuda:0')
optimizer state dict: 88.0
lr: [5.1383708942904056e-06, 5.1383708942904056e-06]
scheduler_last_epoch: 88


Running epoch 0, step 704, batch 704
Sampled inputs[:2]: tensor([[   0,  266, 5528,  ...,  685,  266, 1231],
        [   0, 3468,  278,  ..., 2442,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1790e-04, -9.2002e-05, -7.3962e-05,  ...,  3.5984e-05,
         -5.6011e-05, -1.4727e-04],
        [-2.0266e-06, -1.5572e-06,  8.3447e-07,  ..., -1.6987e-06,
         -8.0466e-07, -1.1697e-06],
        [-2.8312e-06, -2.1756e-06,  1.1921e-06,  ..., -2.3544e-06,
         -9.9093e-07, -1.6019e-06],
        [-4.0233e-06, -3.0696e-06,  1.6838e-06,  ..., -3.3677e-06,
         -1.4156e-06, -2.2948e-06],
        [-5.3942e-06, -4.1127e-06,  2.2650e-06,  ..., -4.4703e-06,
         -1.9372e-06, -3.0249e-06]], device='cuda:0')
Loss: 1.0925952196121216


Running epoch 0, step 705, batch 705
Sampled inputs[:2]: tensor([[    0,   680,   401,  ...,  2872,   292, 23535],
        [    0,   271,  4219,  ...,   644,    14,  3607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2631e-04,  4.3128e-04, -5.0985e-04,  ...,  3.9792e-04,
         -5.4340e-04, -3.8642e-04],
        [-3.9116e-06, -3.0771e-06,  1.6727e-06,  ..., -3.5167e-06,
         -2.0042e-06, -2.4959e-06],
        [-5.2601e-06, -4.1127e-06,  2.3171e-06,  ..., -4.6194e-06,
         -2.3097e-06, -3.2261e-06],
        [-7.5996e-06, -5.9158e-06,  3.3528e-06,  ..., -6.7353e-06,
         -3.3975e-06, -4.7237e-06],
        [-1.0073e-05, -7.8231e-06,  4.4554e-06,  ..., -8.7917e-06,
         -4.5002e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.0793620347976685


Running epoch 0, step 706, batch 706
Sampled inputs[:2]: tensor([[    0,   894,    73,  ...,  2323,   909,  4103],
        [    0, 12165,    12,  ...,  2860, 10718,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0956e-04,  3.7701e-04, -5.0538e-04,  ...,  3.8560e-04,
         -5.1395e-04, -3.8642e-04],
        [-6.0573e-06, -4.6343e-06,  2.5779e-06,  ..., -5.2154e-06,
         -2.8424e-06, -3.6731e-06],
        [ 9.3784e-05,  7.5713e-05,  1.0308e-06,  ...,  4.8576e-05,
          7.6447e-05,  4.2550e-05],
        [-1.1772e-05, -8.9556e-06,  5.1335e-06,  ..., -1.0058e-05,
         -4.8652e-06, -7.0035e-06],
        [-1.5706e-05, -1.1936e-05,  6.8545e-06,  ..., -1.3262e-05,
         -6.5565e-06, -9.1493e-06]], device='cuda:0')
Loss: 1.1097056865692139


Running epoch 0, step 707, batch 707
Sampled inputs[:2]: tensor([[    0,  1188,    12,  ...,   292, 23032,   689],
        [    0, 34525,  2008,  ...,  1194,   300, 11120]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2174e-04,  5.4694e-04, -6.3653e-04,  ...,  6.0215e-04,
         -6.0540e-04, -3.4835e-04],
        [-8.0392e-06, -6.1393e-06,  3.4571e-06,  ..., -6.9290e-06,
         -3.7178e-06, -4.8280e-06],
        [ 9.1087e-05,  7.3672e-05,  2.2527e-06,  ...,  4.6267e-05,
          7.5397e-05,  4.1023e-05],
        [-1.5676e-05, -1.1921e-05,  6.9067e-06,  ..., -1.3426e-05,
         -6.4000e-06, -9.2387e-06],
        [-2.0951e-05, -1.5900e-05,  9.2387e-06,  ..., -1.7732e-05,
         -8.6427e-06, -1.2085e-05]], device='cuda:0')
Loss: 1.1002522706985474


Running epoch 0, step 708, batch 708
Sampled inputs[:2]: tensor([[    0,   292,   960,  ...,   271,  1356,    14],
        [    0,   292, 41192,  ..., 34298,  8741,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6303e-04,  6.3234e-04, -6.9659e-04,  ...,  5.6427e-04,
         -6.8675e-04, -4.9666e-04],
        [-1.0021e-05, -7.6294e-06,  4.2915e-06,  ..., -8.6650e-06,
         -4.6715e-06, -6.0648e-06],
        [ 8.8360e-05,  7.1631e-05,  3.4299e-06,  ...,  4.3897e-05,
          7.4235e-05,  3.9361e-05],
        [-1.9521e-05, -1.4797e-05,  8.5607e-06,  ..., -1.6779e-05,
         -8.0466e-06, -1.1608e-05],
        [-2.6196e-05, -1.9804e-05,  1.1489e-05,  ..., -2.2262e-05,
         -1.0937e-05, -1.5244e-05]], device='cuda:0')
Loss: 1.0915464162826538


Running epoch 0, step 709, batch 709
Sampled inputs[:2]: tensor([[    0,    73,    14,  ...,   650,    13,  3658],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3097e-04,  6.7656e-04, -5.6719e-04,  ...,  6.3792e-04,
         -7.3258e-04, -5.4272e-04],
        [-1.2152e-05, -9.2760e-06,  5.2191e-06,  ..., -1.0431e-05,
         -5.6326e-06, -7.3090e-06],
        [ 8.5454e-05,  6.9380e-05,  4.7189e-06,  ...,  4.1513e-05,
          7.3065e-05,  3.7700e-05],
        [-2.3603e-05, -1.7956e-05,  1.0371e-05,  ..., -2.0161e-05,
         -9.7007e-06, -1.3962e-05],
        [-3.1739e-05, -2.4065e-05,  1.3947e-05,  ..., -2.6792e-05,
         -1.3217e-05, -1.8388e-05]], device='cuda:0')
Loss: 1.091130018234253


Running epoch 0, step 710, batch 710
Sampled inputs[:2]: tensor([[    0,    80, 10802,  ...,   287, 28533, 25359],
        [    0,  2544,   394,  ...,    14,  1062,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4511e-04,  6.0599e-04, -5.8854e-04,  ...,  6.5465e-04,
         -5.8259e-04, -5.7545e-04],
        [-1.4178e-05, -1.0826e-05,  6.0387e-06,  ..., -1.2167e-05,
         -6.7130e-06, -8.5533e-06],
        [ 8.2623e-05,  6.7235e-05,  5.8961e-06,  ...,  3.9144e-05,
          7.1716e-05,  3.6009e-05],
        [-2.7478e-05, -2.0891e-05,  1.1981e-05,  ..., -2.3425e-05,
         -1.1548e-05, -1.6287e-05],
        [-3.7163e-05, -2.8118e-05,  1.6212e-05,  ..., -3.1322e-05,
         -1.5810e-05, -2.1577e-05]], device='cuda:0')
Loss: 1.1227350234985352


Running epoch 0, step 711, batch 711
Sampled inputs[:2]: tensor([[    0, 32878,   593,  ...,   437,  1329,   644],
        [    0, 23230,    12,  ...,  5092,   741,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6489e-04,  5.9040e-04, -5.1157e-04,  ...,  7.2123e-04,
         -5.9634e-04, -6.1150e-04],
        [-1.6160e-05, -1.2361e-05,  6.9216e-06,  ..., -1.3866e-05,
         -7.5400e-06, -9.7379e-06],
        [ 7.9821e-05,  6.5089e-05,  7.1552e-06,  ...,  3.6760e-05,
          7.0673e-05,  3.4377e-05],
        [-3.1441e-05, -2.3946e-05,  1.3776e-05,  ..., -2.6822e-05,
         -1.3024e-05, -1.8612e-05],
        [-4.2409e-05, -3.2142e-05,  1.8582e-05,  ..., -3.5763e-05,
         -1.7807e-05, -2.4602e-05]], device='cuda:0')
Loss: 1.0921448469161987
Graident accumulation at epoch 0, step 711, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.7282e-05,  3.0829e-04, -2.2168e-04,  ...,  2.0422e-04,
         -2.3203e-04, -5.5945e-05],
        [-1.7944e-05, -1.2077e-05,  7.2242e-06,  ..., -1.5479e-05,
         -7.8292e-06, -1.0578e-05],
        [ 4.5830e-05,  3.4577e-05, -1.8687e-05,  ...,  3.8889e-05,
          2.1917e-05,  2.3118e-05],
        [ 1.3054e-05,  1.0756e-05,  4.4237e-07,  ...,  9.3386e-06,
          5.0978e-06,  7.5020e-06],
        [-2.1832e-05, -1.7598e-05,  1.0959e-05,  ..., -2.0148e-05,
         -3.3395e-06, -1.3850e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6248e-08, 1.7259e-08, 1.9029e-08,  ..., 2.0587e-08, 3.2216e-08,
         7.2901e-09],
        [5.4396e-11, 2.9830e-11, 3.4048e-12,  ..., 3.8845e-11, 3.1855e-12,
         1.1463e-11],
        [2.2556e-09, 1.1172e-09, 2.4080e-10,  ..., 1.7059e-09, 1.8123e-10,
         6.7237e-10],
        [6.8358e-10, 4.9198e-10, 4.9590e-11,  ..., 5.2312e-10, 5.9562e-11,
         2.1625e-10],
        [2.7356e-10, 1.4208e-10, 1.9678e-11,  ..., 1.9516e-10, 2.9505e-11,
         6.2137e-11]], device='cuda:0')
optimizer state dict: 89.0
lr: [4.9238293885951606e-06, 4.9238293885951606e-06]
scheduler_last_epoch: 89


Running epoch 0, step 712, batch 712
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,    12,   287,  ...,  4626,    27,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4891e-05,  1.0616e-04,  4.6815e-05,  ..., -7.9281e-05,
         -5.6291e-05, -3.0010e-05],
        [-1.9819e-06, -1.5497e-06,  8.4192e-07,  ..., -1.6913e-06,
         -9.4250e-07, -1.2293e-06],
        [-2.7269e-06, -2.1160e-06,  1.1772e-06,  ..., -2.2799e-06,
         -1.1399e-06, -1.6391e-06],
        [-3.9339e-06, -3.0398e-06,  1.7136e-06,  ..., -3.3081e-06,
         -1.6466e-06, -2.3842e-06],
        [-5.2452e-06, -4.0531e-06,  2.2948e-06,  ..., -4.3809e-06,
         -2.2501e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.0965811014175415


Running epoch 0, step 713, batch 713
Sampled inputs[:2]: tensor([[    0,    14, 30840,  ...,   287,   932,    14],
        [    0, 20202,   300,  ..., 15185,   287,  6573]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8965e-05,  2.2884e-04,  7.5885e-05,  ...,  1.4956e-05,
         -1.7395e-04,  1.1840e-04],
        [-3.8818e-06, -3.1143e-06,  1.6466e-06,  ..., -3.3453e-06,
         -1.8552e-06, -2.4587e-06],
        [-5.4091e-06, -4.3213e-06,  2.3469e-06,  ..., -4.5896e-06,
         -2.2724e-06, -3.3304e-06],
        [-7.7188e-06, -6.1393e-06,  3.3602e-06,  ..., -6.5714e-06,
         -3.2336e-06, -4.7833e-06],
        [-1.0341e-05, -8.1956e-06,  4.5300e-06,  ..., -8.7321e-06,
         -4.4405e-06, -6.3032e-06]], device='cuda:0')
Loss: 1.0820198059082031


Running epoch 0, step 714, batch 714
Sampled inputs[:2]: tensor([[   0,  531,   20,  ...,   12, 1644,  680],
        [   0,   14, 3445,  ...,  298,  527, 2732]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3421e-05,  2.9300e-04,  7.7896e-05,  ...,  6.2548e-05,
         -2.3692e-04,  1.0723e-04],
        [-5.7593e-06, -4.6343e-06,  2.4065e-06,  ..., -5.0217e-06,
         -2.7418e-06, -3.6582e-06],
        [-8.1062e-06, -6.4820e-06,  3.4869e-06,  ..., -6.9290e-06,
         -3.3379e-06, -4.9695e-06],
        [-1.1504e-05, -9.1791e-06,  4.9546e-06,  ..., -9.8944e-06,
         -4.7609e-06, -7.1228e-06],
        [-1.5408e-05, -1.2249e-05,  6.6757e-06,  ..., -1.3113e-05,
         -6.4969e-06, -9.3579e-06]], device='cuda:0')
Loss: 1.075735092163086


Running epoch 0, step 715, batch 715
Sampled inputs[:2]: tensor([[    0,  1665,  6306,  ...,   300, 10204,   582],
        [    0, 10084,    12,  ..., 24717,   365,  1616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2856e-05,  3.0978e-04, -4.3231e-05,  ...,  2.0319e-04,
         -3.0442e-04,  5.3638e-05],
        [-7.7561e-06, -6.2212e-06,  3.2969e-06,  ..., -6.7353e-06,
         -3.7104e-06, -4.9025e-06],
        [-1.0878e-05, -8.6576e-06,  4.7386e-06,  ..., -9.2685e-06,
         -4.5225e-06, -6.6534e-06],
        [-1.5497e-05, -1.2308e-05,  6.7651e-06,  ..., -1.3292e-05,
         -6.4746e-06, -9.5665e-06],
        [-2.0742e-05, -1.6451e-05,  9.1046e-06,  ..., -1.7613e-05,
         -8.8364e-06, -1.2562e-05]], device='cuda:0')
Loss: 1.0984973907470703


Running epoch 0, step 716, batch 716
Sampled inputs[:2]: tensor([[   0,  367, 3675,  ...,   22, 3180,   14],
        [   0,  446, 1115,  ..., 1869, 4971, 1954]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4816e-05,  2.7468e-04, -6.2008e-05,  ...,  2.8522e-04,
         -3.0439e-04,  8.0786e-05],
        [-9.8273e-06, -7.8380e-06,  4.1835e-06,  ..., -8.3968e-06,
         -4.5113e-06, -6.0275e-06],
        [-1.3798e-05, -1.0952e-05,  6.0052e-06,  ..., -1.1608e-05,
         -5.5432e-06, -8.2254e-06],
        [-1.9640e-05, -1.5542e-05,  8.5607e-06,  ..., -1.6630e-05,
         -7.9274e-06, -1.1802e-05],
        [-2.6464e-05, -2.0921e-05,  1.1578e-05,  ..., -2.2203e-05,
         -1.0893e-05, -1.5616e-05]], device='cuda:0')
Loss: 1.1162056922912598


Running epoch 0, step 717, batch 717
Sampled inputs[:2]: tensor([[   0, 1932,  278,  ...,  609,  271,  266],
        [   0, 1486,  292,  ..., 7484,   15, 5357]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1183e-04,  3.0064e-04,  7.1618e-06,  ...,  2.9506e-04,
         -2.3032e-04,  1.2709e-04],
        [-1.1869e-05, -9.4697e-06,  5.0366e-06,  ..., -1.0133e-05,
         -5.6289e-06, -7.3910e-06],
        [-1.6585e-05, -1.3173e-05,  7.2047e-06,  ..., -1.3933e-05,
         -6.8694e-06, -1.0021e-05],
        [-2.3425e-05, -1.8552e-05,  1.0192e-05,  ..., -1.9804e-05,
         -9.7528e-06, -1.4275e-05],
        [-3.1739e-05, -2.5123e-05,  1.3858e-05,  ..., -2.6584e-05,
         -1.3471e-05, -1.8984e-05]], device='cuda:0')
Loss: 1.0882142782211304


Running epoch 0, step 718, batch 718
Sampled inputs[:2]: tensor([[   0,  271,  957,  ..., 1597, 1276,  292],
        [   0, 1527,  292,  ..., 2122,  278, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0417e-04,  4.0479e-04, -4.8118e-05,  ...,  4.8131e-04,
         -4.0029e-04,  1.6953e-04],
        [-1.3851e-05, -1.0975e-05,  5.8860e-06,  ..., -1.1884e-05,
         -6.9253e-06, -8.8289e-06],
        [-1.9252e-05, -1.5184e-05,  8.3819e-06,  ..., -1.6198e-05,
         -8.3596e-06, -1.1861e-05],
        [-2.7195e-05, -2.1383e-05,  1.1854e-05,  ..., -2.3037e-05,
         -1.1884e-05, -1.6913e-05],
        [-3.6836e-05, -2.8923e-05,  1.6108e-05,  ..., -3.0875e-05,
         -1.6347e-05, -2.2456e-05]], device='cuda:0')
Loss: 1.0697158575057983


Running epoch 0, step 719, batch 719
Sampled inputs[:2]: tensor([[    0,   298,   669,  ...,   287, 19731,    13],
        [    0,   344,  8133,  ...,   278,  1603,   674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9099e-04,  4.7581e-04, -1.1724e-05,  ...,  5.4149e-04,
         -4.0694e-04,  2.9339e-04],
        [-1.5758e-05, -1.2465e-05,  6.6757e-06,  ..., -1.3620e-05,
         -8.0802e-06, -1.0155e-05],
        [-2.1935e-05, -1.7270e-05,  9.5293e-06,  ..., -1.8567e-05,
         -9.7454e-06, -1.3642e-05],
        [-3.0875e-05, -2.4229e-05,  1.3418e-05,  ..., -2.6315e-05,
         -1.3806e-05, -1.9386e-05],
        [-4.1842e-05, -3.2797e-05,  1.8269e-05,  ..., -3.5286e-05,
         -1.8984e-05, -2.5749e-05]], device='cuda:0')
Loss: 1.0716805458068848
Graident accumulation at epoch 0, step 719, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1454e-05,  3.2504e-04, -2.0069e-04,  ...,  2.3795e-04,
         -2.4952e-04, -2.1012e-05],
        [-1.7725e-05, -1.2115e-05,  7.1694e-06,  ..., -1.5293e-05,
         -7.8543e-06, -1.0536e-05],
        [ 3.9054e-05,  2.9392e-05, -1.5865e-05,  ...,  3.3143e-05,
          1.8751e-05,  1.9442e-05],
        [ 8.6613e-06,  7.2575e-06,  1.7400e-06,  ...,  5.7732e-06,
          3.2074e-06,  4.8131e-06],
        [-2.3833e-05, -1.9118e-05,  1.1690e-05,  ..., -2.1661e-05,
         -4.9040e-06, -1.5040e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6238e-08, 1.7468e-08, 1.9010e-08,  ..., 2.0860e-08, 3.2350e-08,
         7.3689e-09],
        [5.4590e-11, 2.9956e-11, 3.4460e-12,  ..., 3.8991e-11, 3.2476e-12,
         1.1554e-11],
        [2.2538e-09, 1.1164e-09, 2.4065e-10,  ..., 1.7046e-09, 1.8114e-10,
         6.7188e-10],
        [6.8385e-10, 4.9207e-10, 4.9721e-11,  ..., 5.2329e-10, 5.9694e-11,
         2.1641e-10],
        [2.7504e-10, 1.4301e-10, 1.9992e-11,  ..., 1.9621e-10, 2.9836e-11,
         6.2737e-11]], device='cuda:0')
optimizer state dict: 90.0
lr: [4.712390497088522e-06, 4.712390497088522e-06]
scheduler_last_epoch: 90


Running epoch 0, step 720, batch 720
Sampled inputs[:2]: tensor([[    0,  8588,  3937,  ...,   516,  1128,  2341],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4102e-05,  5.2927e-05,  3.4354e-05,  ...,  1.2803e-04,
         -9.4994e-05,  3.2765e-05],
        [-2.0117e-06, -1.5870e-06,  8.7172e-07,  ..., -1.7136e-06,
         -1.0133e-06, -1.3188e-06],
        [-2.7567e-06, -2.1607e-06,  1.2293e-06,  ..., -2.2948e-06,
         -1.2219e-06, -1.7360e-06],
        [-3.9637e-06, -3.0845e-06,  1.7658e-06,  ..., -3.3081e-06,
         -1.7658e-06, -2.5183e-06],
        [-5.3644e-06, -4.2021e-06,  2.4140e-06,  ..., -4.4703e-06,
         -2.4289e-06, -3.3528e-06]], device='cuda:0')
Loss: 1.10087251663208


Running epoch 0, step 721, batch 721
Sampled inputs[:2]: tensor([[    0,  1550,   685,  ...,   943,  1239,   996],
        [    0, 24674,   513,  ...,  6099,    12,  4863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0702e-04, -3.9782e-05,  5.9241e-05,  ...,  2.1286e-04,
         -1.3284e-04, -1.7888e-05],
        [-3.8967e-06, -3.1441e-06,  1.7174e-06,  ..., -3.2783e-06,
         -1.6131e-06, -2.3991e-06],
        [-5.5581e-06, -4.4703e-06,  2.5034e-06,  ..., -4.6045e-06,
         -2.0117e-06, -3.3230e-06],
        [-7.8678e-06, -6.3032e-06,  3.5539e-06,  ..., -6.5565e-06,
         -2.8536e-06, -4.7535e-06],
        [-1.0669e-05, -8.5831e-06,  4.8280e-06,  ..., -8.8513e-06,
         -4.0010e-06, -6.3479e-06]], device='cuda:0')
Loss: 1.0676947832107544


Running epoch 0, step 722, batch 722
Sampled inputs[:2]: tensor([[    0, 10893, 10997,  ...,   367,   616,  7903],
        [    0,  9342,   600,  ...,   199, 12095,   291]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3805e-04,  2.9646e-04, -3.0027e-04,  ...,  5.7262e-04,
         -5.1941e-04, -3.0296e-05],
        [-5.6848e-06, -4.7088e-06,  2.5332e-06,  ..., -4.9844e-06,
         -2.5071e-06, -3.7029e-06],
        [-8.0168e-06, -6.5863e-06,  3.6657e-06,  ..., -6.8843e-06,
         -3.0026e-06, -5.0291e-06],
        [-1.1489e-05, -9.4175e-06,  5.2676e-06,  ..., -9.9391e-06,
         -4.3437e-06, -7.3016e-06],
        [-1.5348e-05, -1.2606e-05,  7.0632e-06,  ..., -1.3173e-05,
         -5.9232e-06, -9.5665e-06]], device='cuda:0')
Loss: 1.054572343826294


Running epoch 0, step 723, batch 723
Sampled inputs[:2]: tensor([[    0,    34,     9,  ...,    19,    14, 45576],
        [    0,   259,  1329,  ...,   266,   706,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9728e-05,  9.2868e-05, -2.1140e-04,  ...,  5.2336e-04,
         -5.7394e-04, -1.7092e-05],
        [-7.6666e-06, -6.3106e-06,  3.3677e-06,  ..., -6.6534e-06,
         -3.4682e-06, -4.9621e-06],
        [-1.0848e-05, -8.8811e-06,  4.8727e-06,  ..., -9.2536e-06,
         -4.2543e-06, -6.8024e-06],
        [-1.5363e-05, -1.2532e-05,  6.9290e-06,  ..., -1.3202e-05,
         -6.0499e-06, -9.7454e-06],
        [-2.0802e-05, -1.6987e-05,  9.4026e-06,  ..., -1.7732e-05,
         -8.3968e-06, -1.2949e-05]], device='cuda:0')
Loss: 1.1031333208084106


Running epoch 0, step 724, batch 724
Sampled inputs[:2]: tensor([[    0,    14,  3741,  ...,   278, 12472, 10257],
        [    0,   292,  2860,  ...,   266,  7000,  7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5484e-04,  3.4141e-04, -2.6782e-04,  ...,  4.5375e-04,
         -7.0889e-04,  2.1129e-05],
        [-9.4250e-06, -7.7933e-06,  4.1761e-06,  ..., -8.2552e-06,
         -4.2655e-06, -6.1840e-06],
        [-1.3337e-05, -1.0967e-05,  6.0573e-06,  ..., -1.1459e-05,
         -5.1968e-06, -8.4639e-06],
        [-1.8969e-05, -1.5527e-05,  8.6427e-06,  ..., -1.6421e-05,
         -7.4282e-06, -1.2174e-05],
        [-2.5660e-05, -2.1011e-05,  1.1712e-05,  ..., -2.1994e-05,
         -1.0304e-05, -1.6123e-05]], device='cuda:0')
Loss: 1.082464337348938


Running epoch 0, step 725, batch 725
Sampled inputs[:2]: tensor([[   0,  829,  874,  ...,  292,  380,  759],
        [   0, 2736, 2523,  ..., 4086, 4798, 7701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3707e-04,  2.5306e-04, -2.0996e-04,  ...,  3.8472e-04,
         -6.1691e-04,  5.0970e-05],
        [-1.1437e-05, -9.4175e-06,  5.0627e-06,  ..., -9.9242e-06,
         -5.1446e-06, -7.3910e-06],
        [-1.6138e-05, -1.3217e-05,  7.3090e-06,  ..., -1.3754e-05,
         -6.3069e-06, -1.0110e-05],
        [ 1.4309e-04,  1.0739e-04, -1.1947e-04,  ...,  2.4422e-04,
          9.3380e-05,  7.6512e-05],
        [-3.1084e-05, -2.5362e-05,  1.4156e-05,  ..., -2.6464e-05,
         -1.2524e-05, -1.9282e-05]], device='cuda:0')
Loss: 1.115162968635559


Running epoch 0, step 726, batch 726
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,  4014,    88,  ...,  1103,    14,  1771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9926e-04,  3.0767e-04, -2.9826e-04,  ...,  5.2483e-04,
         -6.2830e-04,  1.3277e-04],
        [-1.3389e-05, -1.0960e-05,  5.9195e-06,  ..., -1.1548e-05,
         -5.9120e-06, -8.5533e-06],
        [ 6.1778e-05,  3.8953e-05, -3.1022e-05,  ...,  6.1764e-05,
          3.7324e-05,  4.2298e-05],
        [ 1.3913e-04,  1.0424e-04, -1.1768e-04,  ...,  2.4093e-04,
          9.1994e-05,  7.4187e-05],
        [-3.6508e-05, -2.9624e-05,  1.6600e-05,  ..., -3.0935e-05,
         -1.4476e-05, -2.2396e-05]], device='cuda:0')
Loss: 1.1182111501693726


Running epoch 0, step 727, batch 727
Sampled inputs[:2]: tensor([[    0,  9829,   292,  ...,  2928,  1029,   271],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0302e-04,  2.7058e-04, -2.9849e-04,  ...,  5.4221e-04,
         -6.4999e-04,  1.1114e-04],
        [-1.5251e-05, -1.2480e-05,  6.7167e-06,  ..., -1.3173e-05,
         -6.8210e-06, -9.8720e-06],
        [ 5.9155e-05,  3.6808e-05, -2.9867e-05,  ...,  5.9514e-05,
          3.6191e-05,  4.0480e-05],
        [ 1.3542e-04,  1.0123e-04, -1.1606e-04,  ...,  2.3772e-04,
          9.0377e-05,  7.1594e-05],
        [-4.1604e-05, -3.3766e-05,  1.8850e-05,  ..., -3.5286e-05,
         -1.6727e-05, -2.5868e-05]], device='cuda:0')
Loss: 1.0983846187591553
Graident accumulation at epoch 0, step 727, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.7611e-05,  3.1960e-04, -2.1047e-04,  ...,  2.6838e-04,
         -2.8957e-04, -7.7968e-06],
        [-1.7478e-05, -1.2152e-05,  7.1241e-06,  ..., -1.5081e-05,
         -7.7510e-06, -1.0470e-05],
        [ 4.1064e-05,  3.0134e-05, -1.7265e-05,  ...,  3.5780e-05,
          2.0495e-05,  2.1546e-05],
        [ 2.1337e-05,  1.6655e-05, -1.0040e-05,  ...,  2.8968e-05,
          1.1924e-05,  1.1491e-05],
        [-2.5610e-05, -2.0583e-05,  1.2406e-05,  ..., -2.3024e-05,
         -6.0862e-06, -1.6123e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6233e-08, 1.7523e-08, 1.9080e-08,  ..., 2.1133e-08, 3.2740e-08,
         7.3739e-09],
        [5.4768e-11, 3.0082e-11, 3.4877e-12,  ..., 3.9126e-11, 3.2909e-12,
         1.1640e-11],
        [2.2551e-09, 1.1166e-09, 2.4130e-10,  ..., 1.7064e-09, 1.8227e-10,
         6.7285e-10],
        [7.0151e-10, 5.0183e-10, 6.3141e-11,  ..., 5.7928e-10, 6.7802e-11,
         2.2132e-10],
        [2.7650e-10, 1.4401e-10, 2.0328e-11,  ..., 1.9726e-10, 3.0086e-11,
         6.3344e-11]], device='cuda:0')
optimizer state dict: 91.0
lr: [4.504183453666481e-06, 4.504183453666481e-06]
scheduler_last_epoch: 91


Running epoch 0, step 728, batch 728
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,   768,  3227,  ...,  3487,    13, 31431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 0.0000e+00, -2.0939e-04,  1.9105e-04,  ...,  1.5123e-05,
          1.1887e-04, -4.6480e-05],
        [-1.9372e-06, -1.5274e-06,  7.9349e-07,  ..., -1.7211e-06,
         -1.0282e-06, -1.3709e-06],
        [-2.5779e-06, -2.0415e-06,  1.1027e-06,  ..., -2.2501e-06,
         -1.2070e-06, -1.7658e-06],
        [-3.7104e-06, -2.9206e-06,  1.5944e-06,  ..., -3.2783e-06,
         -1.7583e-06, -2.5630e-06],
        [-5.0962e-06, -4.0233e-06,  2.2054e-06,  ..., -4.4703e-06,
         -2.4587e-06, -3.4422e-06]], device='cuda:0')
Loss: 1.102513313293457


Running epoch 0, step 729, batch 729
Sampled inputs[:2]: tensor([[    0,   221,  6872,  ...,   806,   518,   266],
        [    0, 12440,   578,  ..., 25918,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8570e-04, -4.8231e-04,  2.3403e-04,  ...,  3.1180e-05,
          2.2990e-04,  5.3260e-05],
        [-3.5986e-06, -2.9951e-06,  1.6429e-06,  ..., -3.3453e-06,
         -1.8030e-06, -2.6524e-06],
        [-4.9323e-06, -4.0829e-06,  2.3320e-06,  ..., -4.4554e-06,
         -2.0973e-06, -3.4720e-06],
        [-7.1824e-06, -5.9307e-06,  3.4273e-06,  ..., -6.5863e-06,
         -3.0845e-06, -5.1260e-06],
        [-9.5963e-06, -7.9274e-06,  4.5747e-06,  ..., -8.6725e-06,
         -4.1947e-06, -6.6459e-06]], device='cuda:0')
Loss: 1.0732316970825195


Running epoch 0, step 730, batch 730
Sampled inputs[:2]: tensor([[    0,  1167,  2667,  ...,  4769,    13,  5019],
        [    0,   565,  5539,  ...,    12,   516, 14426]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0331e-04, -7.5651e-04,  3.6553e-04,  ..., -1.3154e-04,
          3.6453e-04,  1.1190e-04],
        [-5.4985e-06, -4.5374e-06,  2.5220e-06,  ..., -4.9770e-06,
         -2.6748e-06, -3.9116e-06],
        [-7.5698e-06, -6.2436e-06,  3.5837e-06,  ..., -6.7055e-06,
         -3.1702e-06, -5.1856e-06],
        [-1.0923e-05, -8.9705e-06,  5.1931e-06,  ..., -9.7901e-06,
         -4.5896e-06, -7.5698e-06],
        [-1.4782e-05, -1.2130e-05,  7.0333e-06,  ..., -1.3083e-05,
         -6.3404e-06, -9.9689e-06]], device='cuda:0')
Loss: 1.103139877319336


Running epoch 0, step 731, batch 731
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,   299,   292,  ...,   266,  2474,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0385e-04, -1.0168e-03,  2.4087e-04,  ..., -1.7034e-04,
          3.1709e-04,  1.6463e-05],
        [-7.1526e-06, -5.9158e-06,  3.3081e-06,  ..., -6.7651e-06,
         -4.0829e-06, -5.4166e-06],
        [-9.7454e-06, -8.0392e-06,  4.6492e-06,  ..., -8.9109e-06,
         -4.6752e-06, -7.0035e-06],
        [-1.4082e-05, -1.1563e-05,  6.7577e-06,  ..., -1.3053e-05,
         -6.8396e-06, -1.0267e-05],
        [-1.8984e-05, -1.5572e-05,  9.1046e-06,  ..., -1.7285e-05,
         -9.1717e-06, -1.3381e-05]], device='cuda:0')
Loss: 1.0958489179611206


Running epoch 0, step 732, batch 732
Sampled inputs[:2]: tensor([[    0,  2939,    14,  ...,  1702,  1481,   278],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1838e-04, -1.2367e-03,  3.0320e-04,  ..., -3.3873e-04,
          3.6949e-04,  1.0389e-06],
        [-8.8736e-06, -7.3984e-06,  4.0382e-06,  ..., -8.4639e-06,
         -4.9993e-06, -6.7502e-06],
        [-1.2144e-05, -1.0051e-05,  5.7593e-06,  ..., -1.1146e-05,
         -5.6885e-06, -8.7172e-06],
        [-1.7613e-05, -1.4514e-05,  8.3819e-06,  ..., -1.6376e-05,
         -8.3596e-06, -1.2845e-05],
        [-2.3693e-05, -1.9476e-05,  1.1310e-05,  ..., -2.1636e-05,
         -1.1168e-05, -1.6659e-05]], device='cuda:0')
Loss: 1.085868239402771


Running epoch 0, step 733, batch 733
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,   298, 49038,  ...,   288,  1690,  2736]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8179e-04, -1.5472e-03,  4.5958e-04,  ..., -5.9743e-04,
          6.5699e-04,  1.7963e-04],
        [-1.0677e-05, -8.8736e-06,  4.8093e-06,  ..., -1.0192e-05,
         -6.3553e-06, -8.3372e-06],
        [-1.4603e-05, -1.2048e-05,  6.8620e-06,  ..., -1.3396e-05,
         -7.2829e-06, -1.0744e-05],
        [-2.0966e-05, -1.7226e-05,  9.8944e-06,  ..., -1.9491e-05,
         -1.0565e-05, -1.5676e-05],
        [-2.8372e-05, -2.3246e-05,  1.3426e-05,  ..., -2.5898e-05,
         -1.4193e-05, -2.0429e-05]], device='cuda:0')
Loss: 1.077322244644165


Running epoch 0, step 734, batch 734
Sampled inputs[:2]: tensor([[    0, 38136,    12,  ...,   367, 12851,  1040],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1515e-04, -1.8763e-03,  4.6933e-04,  ..., -6.5843e-04,
          8.5544e-04,  2.7386e-04],
        [-1.2554e-05, -1.0423e-05,  5.6550e-06,  ..., -1.1794e-05,
         -7.0706e-06, -9.4920e-06],
        [-1.7285e-05, -1.4253e-05,  8.0988e-06,  ..., -1.5661e-05,
         -8.1696e-06, -1.2346e-05],
        [-2.4706e-05, -2.0295e-05,  1.1623e-05,  ..., -2.2665e-05,
         -1.1794e-05, -1.7926e-05],
        [-3.3647e-05, -2.7567e-05,  1.5870e-05,  ..., -3.0339e-05,
         -1.6026e-05, -2.3544e-05]], device='cuda:0')
Loss: 1.0936766862869263


Running epoch 0, step 735, batch 735
Sampled inputs[:2]: tensor([[    0, 21801, 13084,  ...,  1738,  2946,    12],
        [    0,    12,  9328,  ...,    20,   408,   790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4921e-04, -1.6261e-03,  4.8259e-04,  ..., -6.7387e-04,
          1.0055e-03,  4.5219e-04],
        [-1.4611e-05, -1.2003e-05,  6.4075e-06,  ..., -1.3635e-05,
         -8.6352e-06, -1.1101e-05],
        [-1.9923e-05, -1.6265e-05,  9.1121e-06,  ..., -1.7941e-05,
         -9.9875e-06, -1.4327e-05],
        [-2.8253e-05, -2.3022e-05,  1.2994e-05,  ..., -2.5779e-05,
         -1.4253e-05, -2.0608e-05],
        [-3.8713e-05, -3.1412e-05,  1.7837e-05,  ..., -3.4690e-05,
         -1.9483e-05, -2.7269e-05]], device='cuda:0')
Loss: 1.0909596681594849
Graident accumulation at epoch 0, step 735, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3071e-05,  1.2502e-04, -1.4116e-04,  ...,  1.7415e-04,
         -1.6006e-04,  3.8202e-05],
        [-1.7191e-05, -1.2137e-05,  7.0524e-06,  ..., -1.4937e-05,
         -7.8394e-06, -1.0533e-05],
        [ 3.4965e-05,  2.5494e-05, -1.4628e-05,  ...,  3.0408e-05,
          1.7447e-05,  1.7958e-05],
        [ 1.6378e-05,  1.2687e-05, -7.7366e-06,  ...,  2.3493e-05,
          9.3066e-06,  8.2813e-06],
        [-2.6920e-05, -2.1666e-05,  1.2949e-05,  ..., -2.4190e-05,
         -7.4259e-06, -1.7238e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6609e-08, 2.0150e-08, 1.9294e-08,  ..., 2.1566e-08, 3.3718e-08,
         7.5710e-09],
        [5.4927e-11, 3.0196e-11, 3.5252e-12,  ..., 3.9273e-11, 3.3622e-12,
         1.1752e-11],
        [2.2532e-09, 1.1157e-09, 2.4114e-10,  ..., 1.7050e-09, 1.8219e-10,
         6.7238e-10],
        [7.0160e-10, 5.0186e-10, 6.3247e-11,  ..., 5.7936e-10, 6.7937e-11,
         2.2152e-10],
        [2.7772e-10, 1.4485e-10, 2.0625e-11,  ..., 1.9826e-10, 3.0435e-11,
         6.4024e-11]], device='cuda:0')
optimizer state dict: 92.0
lr: [4.299335516882092e-06, 4.299335516882092e-06]
scheduler_last_epoch: 92


Running epoch 0, step 736, batch 736
Sampled inputs[:2]: tensor([[    0, 47684,   292,  ...,   287, 49958, 22022],
        [    0,  3386, 43625,  ...,    19,  2125,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4484e-04,  2.6342e-04, -9.0157e-05,  ...,  4.9789e-05,
         -1.5382e-04, -6.5077e-05],
        [-1.6987e-06, -1.3411e-06,  7.7114e-07,  ..., -1.6764e-06,
         -1.0431e-06, -1.3560e-06],
        [-2.3246e-06, -1.8030e-06,  1.1325e-06,  ..., -2.1607e-06,
         -1.1548e-06, -1.7062e-06],
        [-3.5167e-06, -2.7269e-06,  1.7211e-06,  ..., -3.3379e-06,
         -1.8254e-06, -2.6673e-06],
        [-4.5002e-06, -3.4571e-06,  2.2203e-06,  ..., -4.1127e-06,
         -2.2352e-06, -3.2037e-06]], device='cuda:0')
Loss: 1.0846986770629883


Running epoch 0, step 737, batch 737
Sampled inputs[:2]: tensor([[    0,  1254,  1773,  ..., 19459,  2447,  2613],
        [    0,  2906, 46441,  ..., 39156,   287, 11452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2886e-05, -1.3346e-04,  2.0140e-04,  ..., -1.3198e-04,
          2.3271e-04,  3.9690e-05],
        [-3.6210e-06, -2.9206e-06,  1.6689e-06,  ..., -3.3453e-06,
         -2.1905e-06, -2.7716e-06],
        [-5.0068e-06, -4.0084e-06,  2.4289e-06,  ..., -4.4405e-06,
         -2.5630e-06, -3.5986e-06],
        [-7.1526e-06, -5.7071e-06,  3.4794e-06,  ..., -6.4522e-06,
         -3.7476e-06, -5.2452e-06],
        [-9.6560e-06, -7.6592e-06,  4.7237e-06,  ..., -8.4937e-06,
         -4.9770e-06, -6.7651e-06]], device='cuda:0')
Loss: 1.1087085008621216


Running epoch 0, step 738, batch 738
Sampled inputs[:2]: tensor([[    0,  1099,   644,  ...,  5481,    14,  8782],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1848e-04, -2.8881e-05,  2.0150e-04,  ..., -1.3555e-04,
          1.6975e-04, -4.1565e-05],
        [-5.3644e-06, -4.3884e-06,  2.4512e-06,  ..., -4.9695e-06,
         -3.1963e-06, -4.1053e-06],
        [-7.4953e-06, -6.0797e-06,  3.6061e-06,  ..., -6.6757e-06,
         -3.7625e-06, -5.4091e-06],
        [-1.0729e-05, -8.6874e-06,  5.1782e-06,  ..., -9.7156e-06,
         -5.5134e-06, -7.8976e-06],
        [-1.4454e-05, -1.1623e-05,  7.0333e-06,  ..., -1.2785e-05,
         -7.3463e-06, -1.0192e-05]], device='cuda:0')
Loss: 1.0826798677444458


Running epoch 0, step 739, batch 739
Sampled inputs[:2]: tensor([[   0,  221,  709,  ..., 3365, 3504,  278],
        [   0, 1531,   14,  ..., 6169,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4091e-05, -9.4250e-05,  3.2662e-04,  ...,  3.2353e-05,
          2.6916e-04,  5.5980e-05],
        [-7.1526e-06, -5.8338e-06,  3.2112e-06,  ..., -6.5416e-06,
         -3.8370e-06, -5.2229e-06],
        [-1.0103e-05, -8.1807e-06,  4.7684e-06,  ..., -8.9407e-06,
         -4.5635e-06, -6.9961e-06],
        [-1.4395e-05, -1.1638e-05,  6.8098e-06,  ..., -1.2919e-05,
         -6.6459e-06, -1.0148e-05],
        [-1.9759e-05, -1.5885e-05,  9.4026e-06,  ..., -1.7375e-05,
         -9.0450e-06, -1.3351e-05]], device='cuda:0')
Loss: 1.0764979124069214


Running epoch 0, step 740, batch 740
Sampled inputs[:2]: tensor([[    0,   300, 16683,  ...,  8709,    40,  9817],
        [    0,   825,  1243,  ...,    15,    22,    42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1582e-05, -1.8074e-05,  2.9394e-04,  ...,  1.1383e-04,
          3.0940e-04,  1.2698e-04],
        [-9.0078e-06, -7.3016e-06,  4.0308e-06,  ..., -8.0839e-06,
         -4.3511e-06, -6.3181e-06],
        [-1.2919e-05, -1.0416e-05,  6.0350e-06,  ..., -1.1280e-05,
         -5.2415e-06, -8.6427e-06],
        [-1.8418e-05, -1.4812e-05,  8.6278e-06,  ..., -1.6257e-05,
         -7.6070e-06, -1.2502e-05],
        [-2.5094e-05, -2.0117e-05,  1.1817e-05,  ..., -2.1815e-05,
         -1.0416e-05, -1.6406e-05]], device='cuda:0')
Loss: 1.0682837963104248


Running epoch 0, step 741, batch 741
Sampled inputs[:2]: tensor([[    0,   266,  2109,  ...,  6730, 11558,   287],
        [    0,    14,  8383,  ...,   266,  1717,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1396e-04, -1.1592e-04,  2.4126e-04,  ...,  1.5656e-04,
          2.5597e-04,  1.0800e-04],
        [-1.0833e-05, -8.7395e-06,  4.8354e-06,  ..., -9.6336e-06,
         -5.0254e-06, -7.4059e-06],
        [-1.5616e-05, -1.2532e-05,  7.2494e-06,  ..., -1.3545e-05,
         -6.1169e-06, -1.0215e-05],
        [-2.2292e-05, -1.7852e-05,  1.0394e-05,  ..., -1.9535e-05,
         -8.8736e-06, -1.4797e-05],
        [-3.0279e-05, -2.4199e-05,  1.4171e-05,  ..., -2.6166e-05,
         -1.2174e-05, -1.9401e-05]], device='cuda:0')
Loss: 1.1110938787460327


Running epoch 0, step 742, batch 742
Sampled inputs[:2]: tensor([[    0,   923,    13,  ...,   199,   677,  3826],
        [    0, 10205,   342,  ...,  6354, 12230,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0405e-04, -9.8024e-05,  3.5631e-04,  ...,  2.0695e-04,
          1.9939e-04,  1.8983e-04],
        [-1.2636e-05, -1.0289e-05,  5.6811e-06,  ..., -1.1221e-05,
         -5.7481e-06, -8.6352e-06],
        [-1.8254e-05, -1.4797e-05,  8.5086e-06,  ..., -1.5840e-05,
         -7.0222e-06, -1.1966e-05],
        [-2.6107e-05, -2.1115e-05,  1.2219e-05,  ..., -2.2888e-05,
         -1.0200e-05, -1.7345e-05],
        [-3.5495e-05, -2.8640e-05,  1.6674e-05,  ..., -3.0696e-05,
         -1.4037e-05, -2.2814e-05]], device='cuda:0')
Loss: 1.068952202796936


Running epoch 0, step 743, batch 743
Sampled inputs[:2]: tensor([[    0,  9010,    17,  ...,  3813,  1147,   199],
        [    0,   278, 30377,  ...,    13,    83,  2908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2971e-04, -2.1275e-04,  5.6097e-04,  ...,  5.6149e-05,
          4.2989e-04,  3.1449e-04],
        [-1.4648e-05, -1.1824e-05,  6.5304e-06,  ..., -1.2934e-05,
         -6.8210e-06, -1.0036e-05],
        [-2.0966e-05, -1.6853e-05,  9.7007e-06,  ..., -1.8105e-05,
         -8.2962e-06, -1.3776e-05],
        [-2.9922e-05, -2.4021e-05,  1.3895e-05,  ..., -2.6107e-05,
         -1.2018e-05, -1.9938e-05],
        [-4.0889e-05, -3.2753e-05,  1.9059e-05,  ..., -3.5197e-05,
         -1.6615e-05, -2.6360e-05]], device='cuda:0')
Loss: 1.1255273818969727
Graident accumulation at epoch 0, step 743, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.4735e-05,  9.1244e-05, -7.0947e-05,  ...,  1.6235e-04,
         -1.0107e-04,  6.5831e-05],
        [-1.6937e-05, -1.2106e-05,  7.0002e-06,  ..., -1.4736e-05,
         -7.7376e-06, -1.0483e-05],
        [ 2.9372e-05,  2.1259e-05, -1.2195e-05,  ...,  2.5557e-05,
          1.4872e-05,  1.4785e-05],
        [ 1.1748e-05,  9.0165e-06, -5.5734e-06,  ...,  1.8533e-05,
          7.1742e-06,  5.4594e-06],
        [-2.8317e-05, -2.2774e-05,  1.3560e-05,  ..., -2.5291e-05,
         -8.3448e-06, -1.8150e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6615e-08, 2.0175e-08, 1.9589e-08,  ..., 2.1548e-08, 3.3869e-08,
         7.6623e-09],
        [5.5086e-11, 3.0305e-11, 3.5643e-12,  ..., 3.9401e-11, 3.4053e-12,
         1.1841e-11],
        [2.2514e-09, 1.1149e-09, 2.4099e-10,  ..., 1.7037e-09, 1.8208e-10,
         6.7190e-10],
        [7.0180e-10, 5.0193e-10, 6.3376e-11,  ..., 5.7946e-10, 6.8014e-11,
         2.2170e-10],
        [2.7911e-10, 1.4578e-10, 2.0968e-11,  ..., 1.9930e-10, 3.0681e-11,
         6.4655e-11]], device='cuda:0')
optimizer state dict: 93.0
lr: [4.097971892163585e-06, 4.097971892163585e-06]
scheduler_last_epoch: 93


Running epoch 0, step 744, batch 744
Sampled inputs[:2]: tensor([[   0, 3441,  796,  ..., 7561, 1711,  857],
        [   0,  494,  825,  ...,  897,  328,  275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6678e-05,  1.9790e-04, -1.0227e-04,  ...,  2.7321e-05,
         -8.2462e-05,  4.3801e-06],
        [-1.9073e-06, -1.4827e-06,  7.7859e-07,  ..., -1.6540e-06,
         -8.0839e-07, -1.1846e-06],
        [-2.7865e-06, -2.1458e-06,  1.1921e-06,  ..., -2.3395e-06,
         -1.0058e-06, -1.6466e-06],
        [-4.0233e-06, -3.0994e-06,  1.7211e-06,  ..., -3.4273e-06,
         -1.4752e-06, -2.4140e-06],
        [-5.4240e-06, -4.1425e-06,  2.3246e-06,  ..., -4.5300e-06,
         -2.0117e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.1217421293258667


Running epoch 0, step 745, batch 745
Sampled inputs[:2]: tensor([[   0,  320,  472,  ..., 1345,   14, 1869],
        [   0, 1250, 1797,  ...,  266, 1417,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4655e-04,  3.8835e-04, -1.7583e-04,  ...,  2.9480e-04,
         -3.2673e-04,  1.3921e-04],
        [-3.6806e-06, -2.8759e-06,  1.5683e-06,  ..., -3.2783e-06,
         -1.6205e-06, -2.4214e-06],
        [-5.4389e-06, -4.2170e-06,  2.4065e-06,  ..., -4.6939e-06,
         -2.0415e-06, -3.4124e-06],
        [-7.7784e-06, -6.0201e-06,  3.4496e-06,  ..., -6.7949e-06,
         -2.9653e-06, -4.9621e-06],
        [-1.0401e-05, -7.9870e-06,  4.6045e-06,  ..., -8.9109e-06,
         -3.9935e-06, -6.4075e-06]], device='cuda:0')
Loss: 1.0839693546295166


Running epoch 0, step 746, batch 746
Sampled inputs[:2]: tensor([[    0,   300, 12579,  ...,  1722,   369,  5049],
        [    0,  2793,   271,  ...,   374,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5940e-04,  5.7154e-04, -1.8899e-04,  ...,  2.4920e-04,
         -2.7676e-04,  1.6912e-04],
        [-5.3570e-06, -4.3288e-06,  2.2873e-06,  ..., -4.8727e-06,
         -2.4848e-06, -3.5837e-06],
        [-7.9423e-06, -6.3181e-06,  3.5614e-06,  ..., -6.9439e-06,
         -3.0771e-06, -5.0142e-06],
        [-1.1384e-05, -9.0450e-06,  5.1111e-06,  ..., -1.0073e-05,
         -4.4852e-06, -7.3165e-06],
        [-1.5289e-05, -1.2040e-05,  6.8843e-06,  ..., -1.3232e-05,
         -6.0350e-06, -9.4622e-06]], device='cuda:0')
Loss: 1.09326171875


Running epoch 0, step 747, batch 747
Sampled inputs[:2]: tensor([[   0, 9677,  609,  ...,  199, 1919,  298],
        [   0,  668, 2474,  ...,  668, 4599,  360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1386e-04,  4.2117e-04, -1.0305e-04,  ...,  2.2825e-04,
         -1.9615e-04,  9.7259e-05],
        [-7.1451e-06, -5.7817e-06,  3.1181e-06,  ..., -6.4224e-06,
         -3.0920e-06, -4.6939e-06],
        [-1.0699e-05, -8.5384e-06,  4.8727e-06,  ..., -9.2834e-06,
         -3.8818e-06, -6.6757e-06],
        [-1.5199e-05, -1.2130e-05,  6.9365e-06,  ..., -1.3351e-05,
         -5.6028e-06, -9.6560e-06],
        [-2.0504e-05, -1.6242e-05,  9.3877e-06,  ..., -1.7673e-05,
         -7.6219e-06, -1.2562e-05]], device='cuda:0')
Loss: 1.0902539491653442


Running epoch 0, step 748, batch 748
Sampled inputs[:2]: tensor([[    0,  2823,   287,  ...,  3504,     9, 13910],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8656e-04,  7.8167e-04, -2.8145e-04,  ...,  4.3703e-04,
         -3.4876e-04,  2.6311e-05],
        [-8.8289e-06, -7.2569e-06,  3.8855e-06,  ..., -8.0392e-06,
         -4.1425e-06, -5.9828e-06],
        [-1.3158e-05, -1.0639e-05,  6.0573e-06,  ..., -1.1504e-05,
         -5.1484e-06, -8.4117e-06],
        [-1.8671e-05, -1.5110e-05,  8.6129e-06,  ..., -1.6555e-05,
         -7.4506e-06, -1.2174e-05],
        [-2.5183e-05, -2.0236e-05,  1.1668e-05,  ..., -2.1875e-05,
         -1.0051e-05, -1.5810e-05]], device='cuda:0')
Loss: 1.0959495306015015


Running epoch 0, step 749, batch 749
Sampled inputs[:2]: tensor([[    0,   266,  1658,  ...,   278,  1083,  5993],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7143e-04,  1.0552e-03, -4.2730e-04,  ...,  5.4866e-04,
         -3.8632e-04, -5.3625e-05],
        [-1.0617e-05, -8.7693e-06,  4.6454e-06,  ..., -9.6560e-06,
         -4.9956e-06, -7.2569e-06],
        [-1.5765e-05, -1.2815e-05,  7.2271e-06,  ..., -1.3784e-05,
         -6.1840e-06, -1.0170e-05],
        [-2.2277e-05, -1.8120e-05,  1.0230e-05,  ..., -1.9759e-05,
         -8.9183e-06, -1.4648e-05],
        [-3.0100e-05, -2.4319e-05,  1.3888e-05,  ..., -2.6137e-05,
         -1.2048e-05, -1.9059e-05]], device='cuda:0')
Loss: 1.0751243829727173


Running epoch 0, step 750, batch 750
Sampled inputs[:2]: tensor([[    0, 10676,   328,  ...,     9,   360,  2583],
        [    0,   965,   300,  ...,   546,   857,  4350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8444e-04,  9.8509e-04, -2.6761e-04,  ...,  5.1894e-04,
         -2.1024e-04,  6.6216e-05],
        [-1.2510e-05, -1.0327e-05,  5.4874e-06,  ..., -1.1250e-05,
         -5.7034e-06, -8.3670e-06],
        [-1.8701e-05, -1.5229e-05,  8.5607e-06,  ..., -1.6257e-05,
         -7.1824e-06, -1.1876e-05],
        [-2.6211e-05, -2.1338e-05,  1.2018e-05,  ..., -2.3067e-05,
         -1.0245e-05, -1.6943e-05],
        [-3.5644e-05, -2.8878e-05,  1.6421e-05,  ..., -3.0786e-05,
         -1.3985e-05, -2.2247e-05]], device='cuda:0')
Loss: 1.0989750623703003


Running epoch 0, step 751, batch 751
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,  2720,    14,  ...,   300, 15867,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0629e-03,  1.1902e-03, -5.6672e-04,  ...,  6.5854e-04,
         -4.2573e-04, -8.5332e-05],
        [-1.4298e-05, -1.1735e-05,  6.2287e-06,  ..., -1.2852e-05,
         -6.4895e-06, -9.5591e-06],
        [-2.1294e-05, -1.7270e-05,  9.7081e-06,  ..., -1.8522e-05,
         -8.1249e-06, -1.3515e-05],
        [-2.9922e-05, -2.4229e-05,  1.3657e-05,  ..., -2.6330e-05,
         -1.1615e-05, -1.9327e-05],
        [-4.0591e-05, -3.2753e-05,  1.8612e-05,  ..., -3.5077e-05,
         -1.5840e-05, -2.5317e-05]], device='cuda:0')
Loss: 1.0929077863693237
Graident accumulation at epoch 0, step 751, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0292, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.5024e-05,  2.0114e-04, -1.2052e-04,  ...,  2.1197e-04,
         -1.3353e-04,  5.0715e-05],
        [-1.6673e-05, -1.2069e-05,  6.9231e-06,  ..., -1.4548e-05,
         -7.6127e-06, -1.0391e-05],
        [ 2.4305e-05,  1.7406e-05, -1.0005e-05,  ...,  2.1149e-05,
          1.2573e-05,  1.1955e-05],
        [ 7.5809e-06,  5.6919e-06, -3.6504e-06,  ...,  1.4047e-05,
          5.2952e-06,  2.9808e-06],
        [-2.9545e-05, -2.3772e-05,  1.4065e-05,  ..., -2.6270e-05,
         -9.0943e-06, -1.8867e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7698e-08, 2.1572e-08, 1.9891e-08,  ..., 2.1960e-08, 3.4017e-08,
         7.6620e-09],
        [5.5235e-11, 3.0413e-11, 3.5996e-12,  ..., 3.9526e-11, 3.4440e-12,
         1.1920e-11],
        [2.2496e-09, 1.1141e-09, 2.4084e-10,  ..., 1.7023e-09, 1.8196e-10,
         6.7141e-10],
        [7.0199e-10, 5.0202e-10, 6.3500e-11,  ..., 5.7958e-10, 6.8081e-11,
         2.2185e-10],
        [2.8048e-10, 1.4671e-10, 2.1293e-11,  ..., 2.0033e-10, 3.0901e-11,
         6.5231e-11]], device='cuda:0')
optimizer state dict: 94.0
lr: [3.900215655287364e-06, 3.900215655287364e-06]
scheduler_last_epoch: 94


Running epoch 0, step 752, batch 752
Sampled inputs[:2]: tensor([[   0,  259, 6887,  ..., 1400,  292,  474],
        [   0,  272,  352,  ...,  590, 4361,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5675e-04,  3.5429e-04, -5.2735e-05,  ...,  2.5478e-04,
         -2.3228e-04, -4.2548e-05],
        [-1.8105e-06, -1.4603e-06,  7.7859e-07,  ..., -1.6242e-06,
         -7.4506e-07, -1.0878e-06],
        [-2.7269e-06, -2.1756e-06,  1.2144e-06,  ..., -2.3991e-06,
         -9.8348e-07, -1.5870e-06],
        [-3.9041e-06, -3.1143e-06,  1.7360e-06,  ..., -3.4869e-06,
         -1.4380e-06, -2.3097e-06],
        [-5.1558e-06, -4.0829e-06,  2.2948e-06,  ..., -4.5300e-06,
         -1.9073e-06, -2.9504e-06]], device='cuda:0')
Loss: 1.0991318225860596


Running epoch 0, step 753, batch 753
Sampled inputs[:2]: tensor([[    0,   892,   271,  ...,   278,   266, 10237],
        [    0,   365,  2714,  ...,   298,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8627e-04,  4.4635e-04, -6.5695e-05,  ...,  3.4840e-04,
         -2.0789e-04, -1.3739e-04],
        [-3.6657e-06, -2.9579e-06,  1.5981e-06,  ..., -3.2112e-06,
         -1.5013e-06, -2.2575e-06],
        [-5.5134e-06, -4.3958e-06,  2.4959e-06,  ..., -4.7237e-06,
         -1.9521e-06, -3.2708e-06],
        [-7.8678e-06, -6.2734e-06,  3.5688e-06,  ..., -6.8396e-06,
         -2.8312e-06, -4.7535e-06],
        [-1.0401e-05, -8.2552e-06,  4.7386e-06,  ..., -8.9109e-06,
         -3.7923e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.064052939414978


Running epoch 0, step 754, batch 754
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  266,  818,  278],
        [   0, 1184, 1451,  ...,  934,  352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5576e-04,  7.3646e-04, -1.6547e-04,  ...,  4.3226e-04,
         -2.0780e-04, -4.1420e-04],
        [-5.5879e-06, -4.4107e-06,  2.3469e-06,  ..., -4.8727e-06,
         -2.5518e-06, -3.6284e-06],
        [-8.4043e-06, -6.5863e-06,  3.6806e-06,  ..., -7.1824e-06,
         -3.3602e-06, -5.2527e-06],
        [-1.1653e-05, -9.1344e-06,  5.1335e-06,  ..., -1.0058e-05,
         -4.6343e-06, -7.3463e-06],
        [-1.5765e-05, -1.2308e-05,  6.9588e-06,  ..., -1.3441e-05,
         -6.4299e-06, -9.6858e-06]], device='cuda:0')
Loss: 1.0800683498382568


Running epoch 0, step 755, batch 755
Sampled inputs[:2]: tensor([[   0,  257,  298,  ..., 1878,  328,  259],
        [   0, 1295,  508,  ...,  829,  772,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1616e-04,  7.6127e-04, -2.0028e-04,  ...,  4.8067e-04,
         -3.2557e-04, -2.9486e-04],
        [-7.4655e-06, -5.8487e-06,  3.2075e-06,  ..., -6.5044e-06,
         -3.6322e-06, -4.9844e-06],
        [-1.1221e-05, -8.7321e-06,  5.0142e-06,  ..., -9.5516e-06,
         -4.7684e-06, -7.1749e-06],
        [-1.5467e-05, -1.2025e-05,  6.9439e-06,  ..., -1.3277e-05,
         -6.5267e-06, -9.9540e-06],
        [-2.0921e-05, -1.6212e-05,  9.4175e-06,  ..., -1.7762e-05,
         -9.0078e-06, -1.3128e-05]], device='cuda:0')
Loss: 1.0623939037322998


Running epoch 0, step 756, batch 756
Sampled inputs[:2]: tensor([[    0,    89,  2023,  ...,  3230,   328,   790],
        [    0,    12, 12774,  ...,  1231,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4195e-04,  1.1639e-03, -3.1866e-04,  ...,  6.1419e-04,
         -4.8126e-04, -2.8025e-04],
        [-9.3728e-06, -7.3314e-06,  3.9898e-06,  ..., -8.1360e-06,
         -4.6305e-06, -6.2361e-06],
        [-1.3873e-05, -1.0759e-05,  6.1467e-06,  ..., -1.1742e-05,
         -5.9381e-06, -8.8140e-06],
        [-1.9312e-05, -1.4976e-05,  8.5980e-06,  ..., -1.6496e-05,
         -8.2701e-06, -1.2383e-05],
        [-2.6077e-05, -2.0146e-05,  1.1653e-05,  ..., -2.1994e-05,
         -1.1347e-05, -1.6257e-05]], device='cuda:0')
Loss: 1.0852530002593994


Running epoch 0, step 757, batch 757
Sampled inputs[:2]: tensor([[    0, 28684,   472,  ...,   317,     9,  1926],
        [    0, 33315,   266,  ...,    12,  1126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2998e-04,  1.3767e-03, -6.2502e-04,  ...,  6.4715e-04,
         -7.5307e-04, -4.6416e-04],
        [-1.1183e-05, -8.7842e-06,  4.7944e-06,  ..., -9.7677e-06,
         -5.5507e-06, -7.4655e-06],
        [-1.6496e-05, -1.2845e-05,  7.3761e-06,  ..., -1.4037e-05,
         -7.0557e-06, -1.0490e-05],
        [-2.3082e-05, -1.7986e-05,  1.0364e-05,  ..., -1.9848e-05,
         -9.9391e-06, -1.4856e-05],
        [-3.0994e-05, -2.4050e-05,  1.3962e-05,  ..., -2.6256e-05,
         -1.3478e-05, -1.9327e-05]], device='cuda:0')
Loss: 1.0870952606201172


Running epoch 0, step 758, batch 758
Sampled inputs[:2]: tensor([[    0,   445,   749,  ...,   850,  1028,    13],
        [    0,  6022,   644,  ..., 14834,  3554,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2823e-04,  1.7594e-03, -7.2657e-04,  ...,  8.1069e-04,
         -8.3510e-04, -5.7381e-04],
        [-1.3031e-05, -1.0319e-05,  5.5358e-06,  ..., -1.1325e-05,
         -6.1505e-06, -8.5980e-06],
        [-1.9282e-05, -1.5125e-05,  8.5384e-06,  ..., -1.6332e-05,
         -7.8194e-06, -1.2137e-05],
        [-2.6956e-05, -2.1160e-05,  1.1988e-05,  ..., -2.3082e-05,
         -1.0997e-05, -1.7166e-05],
        [-3.6180e-05, -2.8282e-05,  1.6138e-05,  ..., -3.0518e-05,
         -1.4968e-05, -2.2337e-05]], device='cuda:0')
Loss: 1.0636459589004517


Running epoch 0, step 759, batch 759
Sampled inputs[:2]: tensor([[    0,   292, 49760,  ...,   275,  4474,    13],
        [    0,  4371,  4806,  ...,   685,   461,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7658e-04,  1.7854e-03, -7.8253e-04,  ...,  7.9415e-04,
         -7.9193e-04, -6.4835e-04],
        [-1.4901e-05, -1.1809e-05,  6.3591e-06,  ..., -1.2904e-05,
         -6.6645e-06, -9.6858e-06],
        [-2.2128e-05, -1.7390e-05,  9.8273e-06,  ..., -1.8716e-05,
         -8.5048e-06, -1.3761e-05],
        [-3.1009e-05, -2.4378e-05,  1.3828e-05,  ..., -2.6494e-05,
         -1.1966e-05, -1.9506e-05],
        [-4.1515e-05, -3.2514e-05,  1.8567e-05,  ..., -3.4958e-05,
         -1.6317e-05, -2.5332e-05]], device='cuda:0')
Loss: 1.074130892753601
Graident accumulation at epoch 0, step 759, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5518e-04,  3.5957e-04, -1.8673e-04,  ...,  2.7019e-04,
         -1.9937e-04, -1.9192e-05],
        [-1.6496e-05, -1.2043e-05,  6.8667e-06,  ..., -1.4384e-05,
         -7.5179e-06, -1.0320e-05],
        [ 1.9662e-05,  1.3927e-05, -8.0214e-06,  ...,  1.7162e-05,
          1.0465e-05,  9.3833e-06],
        [ 3.7219e-06,  2.6849e-06, -1.9025e-06,  ...,  9.9929e-06,
          3.5691e-06,  7.3214e-07],
        [-3.0742e-05, -2.4646e-05,  1.4515e-05,  ..., -2.7139e-05,
         -9.8166e-06, -1.9513e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8419e-08, 2.4738e-08, 2.0483e-08,  ..., 2.2569e-08, 3.4610e-08,
         8.0746e-09],
        [5.5402e-11, 3.0522e-11, 3.6364e-12,  ..., 3.9653e-11, 3.4850e-12,
         1.2002e-11],
        [2.2478e-09, 1.1133e-09, 2.4070e-10,  ..., 1.7009e-09, 1.8185e-10,
         6.7092e-10],
        [7.0225e-10, 5.0211e-10, 6.3627e-11,  ..., 5.7970e-10, 6.8156e-11,
         2.2201e-10],
        [2.8192e-10, 1.4762e-10, 2.1617e-11,  ..., 2.0136e-10, 3.1136e-11,
         6.5808e-11]], device='cuda:0')
optimizer state dict: 95.0
lr: [3.7061876771526483e-06, 3.7061876771526483e-06]
scheduler_last_epoch: 95


Running epoch 0, step 760, batch 760
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0, 49018,   292,  ...,  8774,   642,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8646e-05,  4.6281e-05,  1.2250e-05,  ...,  1.1351e-05,
         -7.9012e-06, -1.0394e-06],
        [-1.9073e-06, -1.5274e-06,  8.3074e-07,  ..., -1.5646e-06,
         -7.6368e-07, -1.1250e-06],
        [-2.9355e-06, -2.3395e-06,  1.3113e-06,  ..., -2.3842e-06,
         -1.0505e-06, -1.6838e-06],
        [-4.0233e-06, -3.2187e-06,  1.8105e-06,  ..., -3.2932e-06,
         -1.4529e-06, -2.3395e-06],
        [-5.5432e-06, -4.4107e-06,  2.4885e-06,  ..., -4.5002e-06,
         -2.0564e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0919770002365112


Running epoch 0, step 761, batch 761
Sampled inputs[:2]: tensor([[   0,  756,  401,  ...,  271, 7272, 1663],
        [   0, 5136,  446,  ..., 1173,  300,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2786e-04,  2.9554e-04, -1.8990e-04,  ...,  2.4676e-04,
         -3.8666e-04,  7.8174e-05],
        [-3.6806e-06, -2.9057e-06,  1.5907e-06,  ..., -3.1590e-06,
         -1.8366e-06, -2.4661e-06],
        [-5.5134e-06, -4.3362e-06,  2.4810e-06,  ..., -4.6194e-06,
         -2.3767e-06, -3.5167e-06],
        [-7.7933e-06, -6.1244e-06,  3.5241e-06,  ..., -6.6012e-06,
         -3.4347e-06, -5.0813e-06],
        [-1.0371e-05, -8.1211e-06,  4.6790e-06,  ..., -8.6725e-06,
         -4.5747e-06, -6.4522e-06]], device='cuda:0')
Loss: 1.068210244178772


Running epoch 0, step 762, batch 762
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,  221,  380,  560],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3582e-05,  1.0229e-04, -1.4069e-04,  ...,  6.5004e-05,
         -9.2396e-05,  1.2862e-04],
        [-5.5283e-06, -4.2841e-06,  2.3544e-06,  ..., -4.8950e-06,
         -2.9840e-06, -3.8445e-06],
        [-8.1360e-06, -6.3032e-06,  3.6508e-06,  ..., -6.9886e-06,
         -3.7402e-06, -5.3421e-06],
        [-1.1533e-05, -8.9258e-06,  5.1782e-06,  ..., -1.0014e-05,
         -5.4315e-06, -7.7337e-06],
        [-1.5229e-05, -1.1772e-05,  6.8545e-06,  ..., -1.3024e-05,
         -7.1228e-06, -9.7454e-06]], device='cuda:0')
Loss: 1.0943270921707153


Running epoch 0, step 763, batch 763
Sampled inputs[:2]: tensor([[   0, 1712,   12,  ..., 1255, 1688,  266],
        [   0, 3767, 2337,  ...,  950,  847,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0476e-05,  3.6118e-04, -1.6772e-04,  ...,  1.3604e-04,
         -1.2062e-06,  2.5855e-04],
        [-7.4133e-06, -5.7817e-06,  3.1441e-06,  ..., -6.5044e-06,
         -3.9972e-06, -5.1111e-06],
        [-1.0908e-05, -8.4788e-06,  4.8652e-06,  ..., -9.2834e-06,
         -5.0440e-06, -7.1228e-06],
        [-1.5378e-05, -1.1951e-05,  6.8694e-06,  ..., -1.3247e-05,
         -7.2271e-06, -1.0237e-05],
        [-2.0355e-05, -1.5765e-05,  9.1046e-06,  ..., -1.7226e-05,
         -9.5516e-06, -1.2964e-05]], device='cuda:0')
Loss: 1.078674077987671


Running epoch 0, step 764, batch 764
Sampled inputs[:2]: tensor([[    0,    12,  3570,  ...,   273,   298,   894],
        [    0,  8450,   292,  ...,   352,   722, 37719]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5273e-04,  3.2251e-04, -2.2485e-04,  ...,  7.2175e-05,
          3.4549e-05,  3.0861e-04],
        [-9.2238e-06, -7.1600e-06,  3.8631e-06,  ..., -8.1956e-06,
         -5.2713e-06, -6.5640e-06],
        [-1.3471e-05, -1.0416e-05,  5.9679e-06,  ..., -1.1563e-05,
         -6.6161e-06, -9.0450e-06],
        [-1.8939e-05, -1.4633e-05,  8.4117e-06,  ..., -1.6466e-05,
         -9.4622e-06, -1.2964e-05],
        [-2.5243e-05, -1.9431e-05,  1.1235e-05,  ..., -2.1517e-05,
         -1.2532e-05, -1.6525e-05]], device='cuda:0')
Loss: 1.0733555555343628


Running epoch 0, step 765, batch 765
Sampled inputs[:2]: tensor([[    0,  5332,   266,  ...,   300,   259, 15369],
        [    0,   381,  1659,  ...,  1403,   271,  6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3570e-04,  2.4898e-04, -2.1522e-04,  ...,  3.7087e-05,
         -1.2555e-05,  2.9494e-04],
        [-1.1161e-05, -8.6352e-06,  4.7088e-06,  ..., -9.7677e-06,
         -6.0052e-06, -7.6815e-06],
        [-1.6302e-05, -1.2577e-05,  7.2345e-06,  ..., -1.3843e-05,
         -7.5661e-06, -1.0632e-05],
        [-2.3052e-05, -1.7762e-05,  1.0267e-05,  ..., -1.9789e-05,
         -1.0848e-05, -1.5303e-05],
        [-3.0607e-05, -2.3514e-05,  1.3649e-05,  ..., -2.5839e-05,
         -1.4387e-05, -1.9491e-05]], device='cuda:0')
Loss: 1.0866035223007202


Running epoch 0, step 766, batch 766
Sampled inputs[:2]: tensor([[    0, 15165,   287,  ..., 15049,   278,   266],
        [    0,  3406,   300,  ...,  1726,  3521,  4481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9879e-04,  6.5111e-04, -4.0202e-04,  ...,  3.2495e-04,
         -2.1713e-04,  3.4993e-04],
        [-1.2852e-05, -1.0014e-05,  5.4985e-06,  ..., -1.1370e-05,
         -6.7763e-06, -8.8066e-06],
        [-1.8820e-05, -1.4603e-05,  8.4639e-06,  ..., -1.6123e-05,
         -8.5011e-06, -1.2197e-05],
        [-2.6792e-05, -2.0757e-05,  1.2100e-05,  ..., -2.3246e-05,
         -1.2286e-05, -1.7703e-05],
        [-3.5375e-05, -2.7329e-05,  1.6019e-05,  ..., -3.0160e-05,
         -1.6212e-05, -2.2396e-05]], device='cuda:0')
Loss: 1.070831060409546


Running epoch 0, step 767, batch 767
Sampled inputs[:2]: tensor([[    0,     9,  9925,  ...,   527, 23286,  6062],
        [    0,  2088,  5370,  ...,  1110,  3380,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6893e-04,  5.4549e-04, -3.6540e-04,  ...,  2.8355e-04,
         -2.2751e-04,  3.1954e-04],
        [-1.4722e-05, -1.1519e-05,  6.2883e-06,  ..., -1.3009e-05,
         -7.6927e-06, -9.9912e-06],
        [-2.1562e-05, -1.6794e-05,  9.6709e-06,  ..., -1.8477e-05,
         -9.6783e-06, -1.3873e-05],
        [-3.0696e-05, -2.3872e-05,  1.3828e-05,  ..., -2.6628e-05,
         -1.3992e-05, -2.0117e-05],
        [-4.0531e-05, -3.1412e-05,  1.8299e-05,  ..., -3.4571e-05,
         -1.8477e-05, -2.5496e-05]], device='cuda:0')
Loss: 1.0859885215759277
Graident accumulation at epoch 0, step 767, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6655e-04,  3.7816e-04, -2.0459e-04,  ...,  2.7152e-04,
         -2.0219e-04,  1.4682e-05],
        [-1.6318e-05, -1.1990e-05,  6.8088e-06,  ..., -1.4246e-05,
         -7.5354e-06, -1.0287e-05],
        [ 1.5540e-05,  1.0855e-05, -6.2522e-06,  ...,  1.3599e-05,
          8.4506e-06,  7.0577e-06],
        [ 2.8008e-07,  2.9230e-08, -3.2942e-07,  ...,  6.3308e-06,
          1.8130e-06, -1.3527e-06],
        [-3.1721e-05, -2.5323e-05,  1.4894e-05,  ..., -2.7882e-05,
         -1.0683e-05, -2.0111e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8442e-08, 2.5011e-08, 2.0596e-08,  ..., 2.2626e-08, 3.4627e-08,
         8.1687e-09],
        [5.5564e-11, 3.0624e-11, 3.6723e-12,  ..., 3.9783e-11, 3.5407e-12,
         1.2090e-11],
        [2.2461e-09, 1.1125e-09, 2.4055e-10,  ..., 1.6996e-09, 1.8176e-10,
         6.7045e-10],
        [7.0249e-10, 5.0218e-10, 6.3755e-11,  ..., 5.7983e-10, 6.8283e-11,
         2.2219e-10],
        [2.8328e-10, 1.4846e-10, 2.1930e-11,  ..., 2.0235e-10, 3.1447e-11,
         6.6392e-11]], device='cuda:0')
optimizer state dict: 96.0
lr: [3.5160065499038043e-06, 3.5160065499038043e-06]
scheduler_last_epoch: 96


Running epoch 0, step 768, batch 768
Sampled inputs[:2]: tensor([[    0,    12,  1197,  ...,   352,  2513,   266],
        [    0, 18717,  2837,  ...,    48,    18,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0625e-05,  1.7897e-04, -9.8035e-05,  ...,  9.5263e-05,
         -1.4257e-04, -8.4613e-07],
        [-1.9521e-06, -1.5050e-06,  8.2701e-07,  ..., -1.7211e-06,
         -9.7603e-07, -1.3113e-06],
        [-2.8461e-06, -2.1756e-06,  1.2740e-06,  ..., -2.4438e-06,
         -1.2517e-06, -1.8105e-06],
        [-4.0829e-06, -3.1143e-06,  1.8179e-06,  ..., -3.5316e-06,
         -1.8254e-06, -2.6524e-06],
        [-5.2154e-06, -3.9637e-06,  2.3395e-06,  ..., -4.4405e-06,
         -2.3246e-06, -3.2634e-06]], device='cuda:0')
Loss: 1.1073395013809204


Running epoch 0, step 769, batch 769
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,  5775,    12,  ...,    12,  1034,  9257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8336e-05,  4.9462e-04, -2.7745e-04,  ...,  9.5263e-05,
         -5.0324e-04, -1.0560e-05],
        [-3.8445e-06, -2.9951e-06,  1.6540e-06,  ..., -3.2932e-06,
         -1.7025e-06, -2.4512e-06],
        [ 9.5034e-05,  5.3889e-05, -2.3555e-05,  ...,  5.5964e-05,
          2.8602e-05,  4.3017e-05],
        [-8.1956e-06, -6.3479e-06,  3.7029e-06,  ..., -6.9439e-06,
         -3.2187e-06, -5.1111e-06],
        [-1.0550e-05, -8.1360e-06,  4.7833e-06,  ..., -8.8215e-06,
         -4.1723e-06, -6.3330e-06]], device='cuda:0')
Loss: 1.0830650329589844


Running epoch 0, step 770, batch 770
Sampled inputs[:2]: tensor([[    0,  1336,   287,  ..., 15920,    12,   287],
        [    0,   591,  2036,  ...,   266,  1027,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7822e-07,  4.5687e-04, -2.9270e-04,  ...,  1.5604e-04,
         -6.1159e-04, -4.2471e-06],
        [-5.8115e-06, -4.4778e-06,  2.5146e-06,  ..., -4.9025e-06,
         -2.3767e-06, -3.4794e-06],
        [ 9.2069e-05,  5.1639e-05, -2.2229e-05,  ...,  5.3535e-05,
          2.7663e-05,  4.1482e-05],
        [-1.2457e-05, -9.5814e-06,  5.6252e-06,  ..., -1.0446e-05,
         -4.5523e-06, -7.3463e-06],
        [-1.6123e-05, -1.2368e-05,  7.2867e-06,  ..., -1.3381e-05,
         -5.9977e-06, -9.1791e-06]], device='cuda:0')
Loss: 1.0769826173782349


Running epoch 0, step 771, batch 771
Sampled inputs[:2]: tensor([[    0,    55,  2258,  ..., 32764,    75,   338],
        [    0,   266,  1211,  ...,  1336,   694,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5028e-04,  3.3901e-04, -4.1654e-04,  ...,  5.3051e-05,
         -5.2013e-04, -5.7316e-05],
        [-7.6666e-06, -5.8711e-06,  3.3900e-06,  ..., -6.4969e-06,
         -3.3155e-06, -4.7982e-06],
        [ 8.9357e-05,  4.9597e-05, -2.0880e-05,  ...,  5.1255e-05,
          2.6463e-05,  3.9642e-05],
        [-1.6391e-05, -1.2517e-05,  7.5772e-06,  ..., -1.3769e-05,
         -6.3181e-06, -1.0058e-05],
        [-2.1130e-05, -1.6123e-05,  9.7752e-06,  ..., -1.7554e-05,
         -8.2478e-06, -1.2487e-05]], device='cuda:0')
Loss: 1.0908708572387695


Running epoch 0, step 772, batch 772
Sampled inputs[:2]: tensor([[    0, 31571,    13,  ...,   367,  2177,   271],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0196e-04,  3.3901e-04, -5.0869e-04,  ...,  9.4206e-05,
         -7.1449e-04, -1.9137e-04],
        [-9.5740e-06, -7.3463e-06,  4.2468e-06,  ..., -8.1286e-06,
         -4.1984e-06, -6.0201e-06],
        [ 8.6511e-05,  4.7422e-05, -1.9546e-05,  ...,  4.8901e-05,
          2.5323e-05,  3.7906e-05],
        [-2.0474e-05, -1.5631e-05,  9.4846e-06,  ..., -1.7181e-05,
         -7.9647e-06, -1.2591e-05],
        [-2.6435e-05, -2.0176e-05,  1.2279e-05,  ..., -2.1935e-05,
         -1.0423e-05, -1.5661e-05]], device='cuda:0')
Loss: 1.0795888900756836


Running epoch 0, step 773, batch 773
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,   508,   586,  ...,  6157,  3146,  7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9428e-05,  3.4336e-04, -5.7092e-04,  ...,  1.6091e-04,
         -7.3652e-04, -1.8419e-04],
        [-1.1511e-05, -8.8364e-06,  5.0813e-06,  ..., -9.7677e-06,
         -5.0776e-06, -7.1898e-06],
        [ 8.3635e-05,  4.5216e-05, -1.8272e-05,  ...,  4.6516e-05,
          2.4161e-05,  3.6237e-05],
        [-2.4587e-05, -1.8775e-05,  1.1310e-05,  ..., -2.0623e-05,
         -9.6560e-06, -1.5035e-05],
        [-3.1829e-05, -2.4319e-05,  1.4678e-05,  ..., -2.6405e-05,
         -1.2644e-05, -1.8761e-05]], device='cuda:0')
Loss: 1.0876591205596924


Running epoch 0, step 774, batch 774
Sampled inputs[:2]: tensor([[   0,  278, 7914,  ..., 1194,  300, 4419],
        [   0, 3630, 2199,  ..., 4157,   27, 4765]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7352e-04,  5.2254e-04, -6.1047e-04,  ...,  3.7866e-04,
         -8.0544e-04, -1.0555e-04],
        [-1.3396e-05, -1.0319e-05,  5.9232e-06,  ..., -1.1437e-05,
         -5.8934e-06, -8.3447e-06],
        [ 8.0908e-05,  4.3085e-05, -1.7013e-05,  ...,  4.4147e-05,
          2.3140e-05,  3.4628e-05],
        [-2.8580e-05, -2.1905e-05,  1.3158e-05,  ..., -2.4125e-05,
         -1.1168e-05, -1.7449e-05],
        [-3.7104e-05, -2.8431e-05,  1.7136e-05,  ..., -3.0965e-05,
         -1.4685e-05, -2.1830e-05]], device='cuda:0')
Loss: 1.0876719951629639


Running epoch 0, step 775, batch 775
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  266, 1040, 1679],
        [   0,  328, 2097,  ...,  365, 1941,  607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7351e-04,  6.1866e-04, -6.1297e-04,  ...,  5.1373e-04,
         -9.7411e-04, -1.1626e-04],
        [-1.5333e-05, -1.1750e-05,  6.7800e-06,  ..., -1.3098e-05,
         -6.8098e-06, -9.6112e-06],
        [ 7.8092e-05,  4.0999e-05, -1.5724e-05,  ...,  4.1763e-05,
          2.1956e-05,  3.2847e-05],
        [-3.2663e-05, -2.4930e-05,  1.5035e-05,  ..., -2.7612e-05,
         -1.2897e-05, -2.0087e-05],
        [-4.2558e-05, -3.2485e-05,  1.9640e-05,  ..., -3.5554e-05,
         -1.7039e-05, -2.5228e-05]], device='cuda:0')
Loss: 1.0933294296264648
Graident accumulation at epoch 0, step 775, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2255e-04,  4.0221e-04, -2.4543e-04,  ...,  2.9574e-04,
         -2.7938e-04,  1.5872e-06],
        [-1.6220e-05, -1.1966e-05,  6.8060e-06,  ..., -1.4131e-05,
         -7.4628e-06, -1.0220e-05],
        [ 2.1795e-05,  1.3869e-05, -7.1994e-06,  ...,  1.6415e-05,
          9.8011e-06,  9.6366e-06],
        [-3.0143e-06, -2.4667e-06,  1.2070e-06,  ...,  2.9365e-06,
          3.4202e-07, -3.2261e-06],
        [-3.2804e-05, -2.6039e-05,  1.5368e-05,  ..., -2.8649e-05,
         -1.1318e-05, -2.0623e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8469e-08, 2.5368e-08, 2.0952e-08,  ..., 2.2868e-08, 3.5541e-08,
         8.1740e-09],
        [5.5743e-11, 3.0731e-11, 3.7146e-12,  ..., 3.9915e-11, 3.5835e-12,
         1.2170e-11],
        [2.2499e-09, 1.1130e-09, 2.4056e-10,  ..., 1.6996e-09, 1.8206e-10,
         6.7085e-10],
        [7.0285e-10, 5.0230e-10, 6.3917e-11,  ..., 5.8001e-10, 6.8381e-11,
         2.2237e-10],
        [2.8481e-10, 1.4936e-10, 2.2294e-11,  ..., 2.0341e-10, 3.1706e-11,
         6.6962e-11]], device='cuda:0')
optimizer state dict: 97.0
lr: [3.32978851444543e-06, 3.32978851444543e-06]
scheduler_last_epoch: 97


Running epoch 0, step 776, batch 776
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  631, 3370,  940],
        [   0, 2732,  413,  ...,  287,  266, 3668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4961e-06,  1.4743e-04, -9.7488e-05,  ...,  1.0166e-04,
         -7.5891e-05,  9.4956e-06],
        [-1.8626e-06, -1.4007e-06,  8.1956e-07,  ..., -1.6689e-06,
         -1.0356e-06, -1.3560e-06],
        [-2.6226e-06, -1.9670e-06,  1.2219e-06,  ..., -2.2501e-06,
         -1.2368e-06, -1.7732e-06],
        [-3.8743e-06, -2.8908e-06,  1.8105e-06,  ..., -3.3677e-06,
         -1.8701e-06, -2.6822e-06],
        [-4.8876e-06, -3.6657e-06,  2.3097e-06,  ..., -4.1723e-06,
         -2.3544e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.0580023527145386


Running epoch 0, step 777, batch 777
Sampled inputs[:2]: tensor([[   0,   19,    9,  ..., 4971,  367, 1675],
        [   0, 3804,  300,  ..., 5062, 9848, 3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6184e-06,  2.0897e-04, -6.7114e-05,  ...,  1.1209e-04,
         -1.9118e-04,  4.7174e-05],
        [-3.8892e-06, -2.8908e-06,  1.6540e-06,  ..., -3.3826e-06,
         -2.0340e-06, -2.6599e-06],
        [-5.4836e-06, -4.0829e-06,  2.4363e-06,  ..., -4.6194e-06,
         -2.4885e-06, -3.5465e-06],
        [-8.0764e-06, -5.9754e-06,  3.5912e-06,  ..., -6.8694e-06,
         -3.7327e-06, -5.3346e-06],
        [-1.0133e-05, -7.5400e-06,  4.5598e-06,  ..., -8.4937e-06,
         -4.6790e-06, -6.4075e-06]], device='cuda:0')
Loss: 1.0852267742156982


Running epoch 0, step 778, batch 778
Sampled inputs[:2]: tensor([[    0,  1858,   499,  ...,    14,  1032,    14],
        [    0, 41638,  4573,  ...,   259,   790,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1478e-04,  1.2098e-04, -9.3799e-05,  ...,  1.2579e-05,
         -1.2632e-04,  1.2663e-04],
        [-5.7593e-06, -4.2990e-06,  2.4103e-06,  ..., -5.0291e-06,
         -3.1590e-06, -3.9488e-06],
        [-8.1956e-06, -6.1095e-06,  3.6284e-06,  ..., -6.8843e-06,
         -3.8818e-06, -5.2825e-06],
        [-1.1861e-05, -8.7917e-06,  5.2527e-06,  ..., -1.0073e-05,
         -5.6997e-06, -7.7933e-06],
        [-1.5169e-05, -1.1280e-05,  6.8098e-06,  ..., -1.2666e-05,
         -7.2569e-06, -9.5367e-06]], device='cuda:0')
Loss: 1.075634479522705


Running epoch 0, step 779, batch 779
Sampled inputs[:2]: tensor([[    0,  1172,   365,  ...,  1119, 15573,  3701],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2647e-04,  1.8329e-04, -1.3495e-04,  ...,  9.0396e-05,
         -1.0473e-04,  1.4738e-04],
        [-7.6815e-06, -5.7593e-06,  3.1963e-06,  ..., -6.7279e-06,
         -4.1574e-06, -5.1484e-06],
        [-1.0878e-05, -8.1509e-06,  4.7907e-06,  ..., -9.1940e-06,
         -5.0962e-06, -6.8694e-06],
        [-1.5825e-05, -1.1802e-05,  6.9663e-06,  ..., -1.3545e-05,
         -7.5325e-06, -1.0207e-05],
        [-2.0176e-05, -1.5065e-05,  8.9854e-06,  ..., -1.6958e-05,
         -9.5516e-06, -1.2428e-05]], device='cuda:0')
Loss: 1.1061490774154663


Running epoch 0, step 780, batch 780
Sampled inputs[:2]: tensor([[    0,  4258,   717,  ...,    34,   609,  1169],
        [    0,   313,    66,  ...,   894,  2973, 25074]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2678e-05,  1.2177e-04, -1.1957e-04,  ...,  3.7896e-05,
          5.1657e-05,  1.3042e-04],
        [-9.5814e-06, -7.2420e-06,  4.0904e-06,  ..., -8.4192e-06,
         -5.0440e-06, -6.3479e-06],
        [-1.3649e-05, -1.0297e-05,  6.1467e-06,  ..., -1.1578e-05,
         -6.1914e-06, -8.5086e-06],
        [-1.9819e-05, -1.4901e-05,  8.9332e-06,  ..., -1.7032e-05,
         -9.1344e-06, -1.2621e-05],
        [-2.5302e-05, -1.9029e-05,  1.1504e-05,  ..., -2.1338e-05,
         -1.1578e-05, -1.5378e-05]], device='cuda:0')
Loss: 1.1001964807510376


Running epoch 0, step 781, batch 781
Sampled inputs[:2]: tensor([[   0,  409,  729,  ...,  391,  266,  996],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1409e-04, -1.2482e-04, -5.8468e-05,  ..., -3.1052e-05,
          1.1289e-04,  1.5440e-04],
        [-1.1519e-05, -8.7619e-06,  4.9509e-06,  ..., -1.0014e-05,
         -5.7593e-06, -7.4878e-06],
        [-1.6525e-05, -1.2562e-05,  7.4506e-06,  ..., -1.3933e-05,
         -7.1600e-06, -1.0163e-05],
        [-2.3931e-05, -1.8135e-05,  1.0811e-05,  ..., -2.0429e-05,
         -1.0513e-05, -1.5035e-05],
        [-3.0696e-05, -2.3261e-05,  1.3962e-05,  ..., -2.5749e-05,
         -1.3471e-05, -1.8448e-05]], device='cuda:0')
Loss: 1.0855783224105835


Running epoch 0, step 782, batch 782
Sampled inputs[:2]: tensor([[   0,   12,  722,  ...,  674,  369,  897],
        [   0,  360,  259,  ..., 5710,  278, 2433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0335e-05, -8.6318e-05, -2.9596e-04,  ...,  1.4581e-04,
         -1.5368e-04,  8.4683e-05],
        [-1.3299e-05, -1.0125e-05,  5.7928e-06,  ..., -1.1720e-05,
         -6.8694e-06, -8.8885e-06],
        [-1.9059e-05, -1.4499e-05,  8.7097e-06,  ..., -1.6272e-05,
         -8.5458e-06, -1.2040e-05],
        [-2.7746e-05, -2.1040e-05,  1.2733e-05,  ..., -2.4006e-05,
         -1.2659e-05, -1.7926e-05],
        [-3.5346e-05, -2.6807e-05,  1.6287e-05,  ..., -3.0011e-05,
         -1.6049e-05, -2.1800e-05]], device='cuda:0')
Loss: 1.0734583139419556


Running epoch 0, step 783, batch 783
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0,   14, 3080,  ...,  910,  266, 5275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2199e-05,  1.7057e-04, -4.0779e-04,  ...,  3.8547e-04,
         -3.0617e-04, -2.6388e-05],
        [-1.5251e-05, -1.1668e-05,  6.6236e-06,  ..., -1.3359e-05,
         -7.9796e-06, -1.0282e-05],
        [-2.1949e-05, -1.6764e-05,  9.9987e-06,  ..., -1.8626e-05,
         -1.0014e-05, -1.4022e-05],
        [-3.1739e-05, -2.4185e-05,  1.4551e-05,  ..., -2.7314e-05,
         -1.4655e-05, -2.0683e-05],
        [-4.0650e-05, -3.0920e-05,  1.8671e-05,  ..., -3.4302e-05,
         -1.8731e-05, -2.5332e-05]], device='cuda:0')
Loss: 1.0668399333953857
Graident accumulation at epoch 0, step 783, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0107e-04,  3.7905e-04, -2.6167e-04,  ...,  3.0472e-04,
         -2.8206e-04, -1.2104e-06],
        [-1.6123e-05, -1.1936e-05,  6.7877e-06,  ..., -1.4054e-05,
         -7.5145e-06, -1.0226e-05],
        [ 1.7420e-05,  1.0806e-05, -5.4796e-06,  ...,  1.2911e-05,
          7.8197e-06,  7.2708e-06],
        [-5.8868e-06, -4.6384e-06,  2.5414e-06,  ..., -8.8541e-08,
         -1.1577e-06, -4.9718e-06],
        [-3.3589e-05, -2.6527e-05,  1.5699e-05,  ..., -2.9214e-05,
         -1.2060e-05, -2.1094e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8429e-08, 2.5372e-08, 2.1097e-08,  ..., 2.2993e-08, 3.5599e-08,
         8.1666e-09],
        [5.5920e-11, 3.0837e-11, 3.7548e-12,  ..., 4.0053e-11, 3.6436e-12,
         1.2264e-11],
        [2.2481e-09, 1.1122e-09, 2.4042e-10,  ..., 1.6983e-09, 1.8198e-10,
         6.7038e-10],
        [7.0316e-10, 5.0238e-10, 6.4065e-11,  ..., 5.8018e-10, 6.8528e-11,
         2.2258e-10],
        [2.8618e-10, 1.5017e-10, 2.2620e-11,  ..., 2.0439e-10, 3.2025e-11,
         6.7537e-11]], device='cuda:0')
optimizer state dict: 98.0
lr: [3.1476473893945937e-06, 3.1476473893945937e-06]
scheduler_last_epoch: 98


Running epoch 0, step 784, batch 784
Sampled inputs[:2]: tensor([[   0, 9611,  278,  ...,  278,  638,  600],
        [   0, 2587,   27,  ...,  259, 2462, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0325e-05,  9.3907e-05, -6.8241e-05,  ...,  4.5129e-05,
          1.9605e-05,  1.8683e-05],
        [-1.9670e-06, -1.4678e-06,  7.4878e-07,  ..., -1.7136e-06,
         -1.1623e-06, -1.3784e-06],
        [-2.8610e-06, -2.1160e-06,  1.1697e-06,  ..., -2.3991e-06,
         -1.4752e-06, -1.8626e-06],
        [-3.9339e-06, -2.8908e-06,  1.6019e-06,  ..., -3.3081e-06,
         -2.0415e-06, -2.5928e-06],
        [-5.1856e-06, -3.8147e-06,  2.1458e-06,  ..., -4.2915e-06,
         -2.6673e-06, -3.3081e-06]], device='cuda:0')
Loss: 1.0814354419708252


Running epoch 0, step 785, batch 785
Sampled inputs[:2]: tensor([[   0,  380,  759,  ..., 1420, 1804,  490],
        [   0, 5902,  518,  ..., 3126,   12,  497]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3279e-05,  2.8132e-04, -5.9329e-05,  ...,  6.5297e-05,
         -3.6521e-05,  1.1326e-04],
        [-3.8221e-06, -2.8834e-06,  1.4789e-06,  ..., -3.4273e-06,
         -2.2948e-06, -2.7046e-06],
        [-5.5134e-06, -4.1127e-06,  2.2948e-06,  ..., -4.7237e-06,
         -2.8461e-06, -3.6061e-06],
        [-7.7188e-06, -5.7518e-06,  3.2112e-06,  ..., -6.6906e-06,
         -4.0531e-06, -5.1707e-06],
        [-9.8944e-06, -7.3761e-06,  4.1723e-06,  ..., -8.4043e-06,
         -5.1111e-06, -6.3479e-06]], device='cuda:0')
Loss: 1.0828251838684082


Running epoch 0, step 786, batch 786
Sampled inputs[:2]: tensor([[    0,   607, 27288,  ...,   445,  4712,   278],
        [    0,  8754,    14,  ...,  6125,   394,   927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7618e-05,  2.1547e-04, -1.3994e-04,  ...,  1.2478e-04,
          1.3178e-05,  8.8692e-05],
        [-5.8189e-06, -4.3809e-06,  2.3916e-06,  ..., -5.1185e-06,
         -3.2410e-06, -3.9190e-06],
        [-8.4639e-06, -6.3330e-06,  3.6806e-06,  ..., -7.1824e-06,
         -4.0904e-06, -5.3197e-06],
        [-1.1891e-05, -8.8811e-06,  5.1782e-06,  ..., -1.0177e-05,
         -5.8264e-06, -7.6443e-06],
        [-1.5199e-05, -1.1399e-05,  6.6757e-06,  ..., -1.2815e-05,
         -7.3910e-06, -9.3877e-06]], device='cuda:0')
Loss: 1.1078376770019531


Running epoch 0, step 787, batch 787
Sampled inputs[:2]: tensor([[    0, 15033,   278,  ...,   266,  2937,    14],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3704e-04,  3.2730e-04, -3.5059e-04,  ...,  3.2929e-04,
         -2.2848e-04,  8.4057e-05],
        [-7.7188e-06, -5.8264e-06,  3.2373e-06,  ..., -6.8471e-06,
         -4.2915e-06, -5.1633e-06],
        [-1.1176e-05, -8.3894e-06,  4.9397e-06,  ..., -9.5814e-06,
         -5.4091e-06, -7.0035e-06],
        [-1.5795e-05, -1.1832e-05,  7.0035e-06,  ..., -1.3694e-05,
         -7.7635e-06, -1.0133e-05],
        [-2.0266e-05, -1.5214e-05,  9.0450e-06,  ..., -1.7256e-05,
         -9.8795e-06, -1.2457e-05]], device='cuda:0')
Loss: 1.085091233253479


Running epoch 0, step 788, batch 788
Sampled inputs[:2]: tensor([[    0,  1234,   278,  ...,  1237,  1008,   417],
        [    0,   287,  1790,  ..., 11367,  9476,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5627e-04,  6.7187e-04, -3.3080e-04,  ...,  3.8912e-04,
         -4.8620e-04,  4.5765e-05],
        [-9.6112e-06, -7.3090e-06,  4.0866e-06,  ..., -8.5682e-06,
         -5.2750e-06, -6.3628e-06],
        [-1.3903e-05, -1.0535e-05,  6.2212e-06,  ..., -1.1966e-05,
         -6.6161e-06, -8.6203e-06],
        [-1.9729e-05, -1.4886e-05,  8.8438e-06,  ..., -1.7181e-05,
         -9.5591e-06, -1.2517e-05],
        [-2.5272e-05, -1.9148e-05,  1.1414e-05,  ..., -2.1636e-05,
         -1.2130e-05, -1.5348e-05]], device='cuda:0')
Loss: 1.0795526504516602


Running epoch 0, step 789, batch 789
Sampled inputs[:2]: tensor([[    0,  1581, 11884,  ...,  7031,   689,   527],
        [    0,    18,    14,  ...,   380,   981,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6071e-04,  7.3667e-04, -4.8715e-04,  ...,  5.1929e-04,
         -8.6292e-04, -2.9495e-06],
        [-1.1362e-05, -8.5831e-06,  4.8913e-06,  ..., -1.0259e-05,
         -6.5789e-06, -7.8753e-06],
        [-1.6347e-05, -1.2316e-05,  7.4208e-06,  ..., -1.4201e-05,
         -8.1658e-06, -1.0572e-05],
        [-2.3425e-05, -1.7554e-05,  1.0677e-05,  ..., -2.0608e-05,
         -1.1958e-05, -1.5542e-05],
        [-2.9743e-05, -2.2382e-05,  1.3620e-05,  ..., -2.5660e-05,
         -1.4976e-05, -1.8790e-05]], device='cuda:0')
Loss: 1.070053219795227


Running epoch 0, step 790, batch 790
Sampled inputs[:2]: tensor([[    0,    15, 43895,  ...,   292,   380, 16795],
        [    0,  2025,   287,  ...,   381,  1487,  3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3268e-04,  5.0094e-04, -4.9175e-04,  ...,  3.7729e-04,
         -7.3584e-04,  1.0578e-04],
        [-1.3247e-05, -1.0028e-05,  5.7146e-06,  ..., -1.1951e-05,
         -7.6890e-06, -9.1493e-06],
        [-1.9148e-05, -1.4462e-05,  8.6948e-06,  ..., -1.6645e-05,
         -9.6411e-06, -1.2383e-05],
        [-2.7329e-05, -2.0534e-05,  1.2465e-05,  ..., -2.4050e-05,
         -1.4044e-05, -1.8120e-05],
        [-3.4899e-05, -2.6315e-05,  1.5974e-05,  ..., -3.0130e-05,
         -1.7717e-05, -2.2054e-05]], device='cuda:0')
Loss: 1.1026760339736938


Running epoch 0, step 791, batch 791
Sampled inputs[:2]: tensor([[   0, 1231,  352,  ..., 8524,   14,  381],
        [   0,   14, 3609,  ...,  298,  413,   29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2557e-04,  5.8386e-04, -4.3092e-04,  ...,  4.4707e-04,
         -8.1849e-04,  1.8974e-04],
        [-1.5110e-05, -1.1466e-05,  6.4969e-06,  ..., -1.3605e-05,
         -8.6017e-06, -1.0319e-05],
        [-2.1890e-05, -1.6578e-05,  9.9167e-06,  ..., -1.9029e-05,
         -1.0826e-05, -1.4037e-05],
        [-3.1129e-05, -2.3454e-05,  1.4164e-05,  ..., -2.7403e-05,
         -1.5713e-05, -2.0459e-05],
        [-3.9905e-05, -3.0160e-05,  1.8209e-05,  ..., -3.4481e-05,
         -1.9938e-05, -2.4989e-05]], device='cuda:0')
Loss: 1.0729753971099854
Graident accumulation at epoch 0, step 791, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0352e-04,  3.9953e-04, -2.7859e-04,  ...,  3.1895e-04,
         -3.3570e-04,  1.7885e-05],
        [-1.6022e-05, -1.1889e-05,  6.7586e-06,  ..., -1.4009e-05,
         -7.6232e-06, -1.0235e-05],
        [ 1.3489e-05,  8.0674e-06, -3.9399e-06,  ...,  9.7168e-06,
          5.9551e-06,  5.1400e-06],
        [-8.4110e-06, -6.5200e-06,  3.7037e-06,  ..., -2.8200e-06,
         -2.6133e-06, -6.5205e-06],
        [-3.4221e-05, -2.6890e-05,  1.5950e-05,  ..., -2.9741e-05,
         -1.2847e-05, -2.1483e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8396e-08, 2.5688e-08, 2.1261e-08,  ..., 2.3170e-08, 3.6234e-08,
         8.1944e-09],
        [5.6092e-11, 3.0937e-11, 3.7932e-12,  ..., 4.0198e-11, 3.7140e-12,
         1.2358e-11],
        [2.2464e-09, 1.1114e-09, 2.4028e-10,  ..., 1.6969e-09, 1.8192e-10,
         6.6991e-10],
        [7.0342e-10, 5.0243e-10, 6.4201e-11,  ..., 5.8035e-10, 6.8706e-11,
         2.2277e-10],
        [2.8749e-10, 1.5093e-10, 2.2929e-11,  ..., 2.0537e-10, 3.2390e-11,
         6.8094e-11]], device='cuda:0')
optimizer state dict: 99.0
lr: [2.969694501513574e-06, 2.969694501513574e-06]
scheduler_last_epoch: 99


Running epoch 0, step 792, batch 792
Sampled inputs[:2]: tensor([[    0,   461,  1169,  ..., 14135,  2771,    13],
        [    0,  1774,  1781,  ...,  4685,   409,  4614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2850e-07,  2.0689e-06, -3.7033e-05,  ...,  1.9116e-04,
         -1.6694e-04,  8.7344e-05],
        [-1.8030e-06, -1.4678e-06,  8.8662e-07,  ..., -1.5870e-06,
         -8.8289e-07, -1.2666e-06],
        [-2.6822e-06, -2.1905e-06,  1.3709e-06,  ..., -2.3097e-06,
         -1.1474e-06, -1.8030e-06],
        [-3.9339e-06, -3.1888e-06,  2.0266e-06,  ..., -3.4273e-06,
         -1.7062e-06, -2.6971e-06],
        [-4.8578e-06, -3.9339e-06,  2.4885e-06,  ..., -4.1723e-06,
         -2.1458e-06, -3.1888e-06]], device='cuda:0')
Loss: 1.0756988525390625


Running epoch 0, step 793, batch 793
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,   328,   266,  ...,   382,    17,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7416e-05,  2.0689e-06, -6.6173e-05,  ...,  1.9603e-04,
         -5.5791e-05,  2.2215e-04],
        [-3.7029e-06, -2.8312e-06,  1.6764e-06,  ..., -3.3379e-06,
         -2.2389e-06, -2.6897e-06],
        [-5.3793e-06, -4.1276e-06,  2.5630e-06,  ..., -4.6641e-06,
         -2.7940e-06, -3.6508e-06],
        [-7.7784e-06, -5.9307e-06,  3.7178e-06,  ..., -6.8545e-06,
         -4.1202e-06, -5.4240e-06],
        [-9.8646e-06, -7.5251e-06,  4.7237e-06,  ..., -8.4937e-06,
         -5.2154e-06, -6.5267e-06]], device='cuda:0')
Loss: 1.0897666215896606


Running epoch 0, step 794, batch 794
Sampled inputs[:2]: tensor([[   0,  461, 4182,  ..., 7461,  292, 4895],
        [   0,  278, 1295,  ..., 4337,  271, 1268]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5203e-05,  4.8765e-05, -3.5050e-05,  ...,  2.8442e-04,
         -9.9066e-05,  3.6743e-04],
        [-5.6252e-06, -4.2319e-06,  2.5295e-06,  ..., -4.9844e-06,
         -3.1218e-06, -3.8892e-06],
        [-8.2105e-06, -6.1840e-06,  3.8669e-06,  ..., -7.0333e-06,
         -3.9414e-06, -5.3346e-06],
        [-1.1891e-05, -8.9109e-06,  5.6252e-06,  ..., -1.0327e-05,
         -5.8040e-06, -7.9274e-06],
        [-1.5140e-05, -1.1370e-05,  7.1526e-06,  ..., -1.2904e-05,
         -7.4059e-06, -9.5963e-06]], device='cuda:0')
Loss: 1.0874600410461426


Running epoch 0, step 795, batch 795
Sampled inputs[:2]: tensor([[    0,    12,   638,  ...,   374,   221,   527],
        [    0, 11054,    12,  ...,   560,   199,   677]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7838e-04, -5.1982e-05,  1.4391e-04,  ...,  9.7611e-05,
          8.9663e-05,  4.5016e-04],
        [-7.5027e-06, -5.6028e-06,  3.2783e-06,  ..., -6.6906e-06,
         -4.0829e-06, -5.1260e-06],
        [-1.0893e-05, -8.1360e-06,  5.0142e-06,  ..., -9.3877e-06,
         -5.1111e-06, -6.9961e-06],
        [-1.5825e-05, -1.1757e-05,  7.3165e-06,  ..., -1.3798e-05,
         -7.5474e-06, -1.0401e-05],
        [-2.0295e-05, -1.5095e-05,  9.3877e-06,  ..., -1.7375e-05,
         -9.7007e-06, -1.2681e-05]], device='cuda:0')
Loss: 1.0696834325790405


Running epoch 0, step 796, batch 796
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,  1725,   271,   266],
        [    0,   968,   266,  ...,   287,  2143, 15228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5900e-04,  1.8335e-04, -1.1254e-05,  ...,  1.2152e-04,
         -2.5030e-04,  3.6722e-04],
        [-9.2983e-06, -6.9290e-06,  4.1761e-06,  ..., -8.2925e-06,
         -4.9472e-06, -6.3032e-06],
        [-1.3575e-05, -1.0118e-05,  6.3926e-06,  ..., -1.1712e-05,
         -6.2212e-06, -8.6650e-06],
        [-1.9848e-05, -1.4707e-05,  9.3877e-06,  ..., -1.7330e-05,
         -9.2387e-06, -1.2964e-05],
        [-2.5272e-05, -1.8746e-05,  1.1951e-05,  ..., -2.1696e-05,
         -1.1802e-05, -1.5691e-05]], device='cuda:0')
Loss: 1.0754964351654053


Running epoch 0, step 797, batch 797
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,    19,   669,  ...,    14,  4053,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4183e-04,  2.3338e-04, -1.3298e-05,  ...,  4.4063e-06,
         -1.5371e-04,  4.6127e-04],
        [-1.1183e-05, -8.3148e-06,  4.9956e-06,  ..., -9.9242e-06,
         -5.9754e-06, -7.6219e-06],
        [-1.6332e-05, -1.2130e-05,  7.6666e-06,  ..., -1.3992e-05,
         -7.5027e-06, -1.0453e-05],
        [-2.3752e-05, -1.7539e-05,  1.1191e-05,  ..., -2.0608e-05,
         -1.1079e-05, -1.5557e-05],
        [-3.0369e-05, -2.2456e-05,  1.4335e-05,  ..., -2.5898e-05,
         -1.4186e-05, -1.8939e-05]], device='cuda:0')
Loss: 1.0948888063430786


Running epoch 0, step 798, batch 798
Sampled inputs[:2]: tensor([[   0,   21, 1304,  ..., 3577,   13, 2497],
        [   0,  259, 1143,  ...,  593,  360,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0732e-04,  6.0721e-04,  5.1675e-05,  ...,  1.1014e-04,
         -3.8809e-04,  4.5483e-04],
        [-1.3240e-05, -9.8497e-06,  5.9120e-06,  ..., -1.1578e-05,
         -7.0557e-06, -8.8438e-06],
        [-1.9401e-05, -1.4424e-05,  9.0674e-06,  ..., -1.6436e-05,
         -8.9929e-06, -1.2226e-05],
        [-2.7984e-05, -2.0698e-05,  1.3143e-05,  ..., -2.4021e-05,
         -1.3135e-05, -1.8045e-05],
        [-3.6061e-05, -2.6688e-05,  1.6943e-05,  ..., -3.0428e-05,
         -1.6972e-05, -2.2173e-05]], device='cuda:0')
Loss: 1.0890005826950073


Running epoch 0, step 799, batch 799
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  292,   86,   16],
        [   0,  259,  587,  ...,   14,   71,  462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8199e-04,  5.7041e-04,  8.8425e-05,  ...,  5.4391e-05,
         -2.5775e-04,  4.1704e-04],
        [-1.5132e-05, -1.1265e-05,  6.7726e-06,  ..., -1.3225e-05,
         -7.8492e-06, -1.0058e-05],
        [-2.2173e-05, -1.6510e-05,  1.0394e-05,  ..., -1.8805e-05,
         -1.0021e-05, -1.3925e-05],
        [-3.1948e-05, -2.3648e-05,  1.5035e-05,  ..., -2.7433e-05,
         -1.4611e-05, -2.0504e-05],
        [-4.1217e-05, -3.0532e-05,  1.9416e-05,  ..., -3.4809e-05,
         -1.8939e-05, -2.5228e-05]], device='cuda:0')
Loss: 1.0764045715332031
Graident accumulation at epoch 0, step 799, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4971e-05,  4.1662e-04, -2.4189e-04,  ...,  2.9250e-04,
         -3.2791e-04,  5.7800e-05],
        [-1.5933e-05, -1.1827e-05,  6.7600e-06,  ..., -1.3931e-05,
         -7.6458e-06, -1.0218e-05],
        [ 9.9232e-06,  5.6096e-06, -2.5066e-06,  ...,  6.8646e-06,
          4.3575e-06,  3.2335e-06],
        [-1.0765e-05, -8.2329e-06,  4.8368e-06,  ..., -5.2813e-06,
         -3.8130e-06, -7.9189e-06],
        [-3.4920e-05, -2.7255e-05,  1.6296e-05,  ..., -3.0248e-05,
         -1.3457e-05, -2.1858e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8427e-08, 2.5987e-08, 2.1248e-08,  ..., 2.3150e-08, 3.6264e-08,
         8.3601e-09],
        [5.6265e-11, 3.1033e-11, 3.8353e-12,  ..., 4.0333e-11, 3.7719e-12,
         1.2447e-11],
        [2.2446e-09, 1.1105e-09, 2.4015e-10,  ..., 1.6956e-09, 1.8184e-10,
         6.6943e-10],
        [7.0374e-10, 5.0248e-10, 6.4363e-11,  ..., 5.8052e-10, 6.8851e-11,
         2.2297e-10],
        [2.8890e-10, 1.5171e-10, 2.3283e-11,  ..., 2.0638e-10, 3.2717e-11,
         6.8662e-11]], device='cuda:0')
optimizer state dict: 100.0
lr: [2.796038617665642e-06, 2.796038617665642e-06]
scheduler_last_epoch: 100
Epoch 0 | Batch 799/1048 | Training PPL: 5493.0370170287915 | time 85.50886797904968
Saving checkpoint at epoch 0, step 799, batch 799
Epoch 0 | Validation PPL: 8.6366841824694 | Learning rate: 2.796038617665642e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_799, AFTER epoch 0, step 799


Running epoch 0, step 800, batch 800
Sampled inputs[:2]: tensor([[    0,  1144,  2680,  ...,   963,     9,  1184],
        [    0,  5685,   565,  ..., 23968,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9532e-04,  7.3433e-05, -1.5563e-04,  ...,  2.6389e-04,
         -3.8430e-04, -1.0548e-04],
        [-1.7136e-06, -1.2740e-06,  8.1211e-07,  ..., -1.7062e-06,
         -1.1697e-06, -1.4007e-06],
        [-2.4438e-06, -1.8179e-06,  1.2517e-06,  ..., -2.2650e-06,
         -1.3858e-06, -1.7881e-06],
        [-3.6806e-06, -2.7120e-06,  1.8999e-06,  ..., -3.5018e-06,
         -2.1607e-06, -2.8014e-06],
        [-4.4703e-06, -3.3081e-06,  2.3097e-06,  ..., -4.0829e-06,
         -2.5481e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.0677369832992554


Running epoch 0, step 801, batch 801
Sampled inputs[:2]: tensor([[    0,   266,   283,  ...,   271, 48829,   580],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2467e-05,  7.4585e-05, -1.5635e-04,  ...,  7.5562e-05,
         -2.0218e-04, -3.1622e-04],
        [-3.5763e-06, -2.7344e-06,  1.7211e-06,  ..., -3.3230e-06,
         -1.9744e-06, -2.6077e-06],
        [-5.3197e-06, -4.0382e-06,  2.7120e-06,  ..., -4.6641e-06,
         -2.4810e-06, -3.5390e-06],
        [-7.5847e-06, -5.7369e-06,  3.9116e-06,  ..., -6.8098e-06,
         -3.6433e-06, -5.2303e-06],
        [-9.5963e-06, -7.2420e-06,  4.9323e-06,  ..., -8.3447e-06,
         -4.5300e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.0869169235229492


Running epoch 0, step 802, batch 802
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,  3047,  4878,  ...,   352, 10854, 34025]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5223e-05,  2.6682e-04, -2.0855e-04,  ...,  1.4992e-04,
         -3.4965e-04, -3.1561e-04],
        [-5.3719e-06, -4.0531e-06,  2.5518e-06,  ..., -4.9695e-06,
         -3.0622e-06, -3.8818e-06],
        [-7.9125e-06, -5.9456e-06,  3.9861e-06,  ..., -6.9290e-06,
         -3.7923e-06, -5.2378e-06],
        [-1.1399e-05, -8.5384e-06,  5.7966e-06,  ..., -1.0177e-05,
         -5.6103e-06, -7.7933e-06],
        [-1.4484e-05, -1.0818e-05,  7.3463e-06,  ..., -1.2577e-05,
         -7.0184e-06, -9.2834e-06]], device='cuda:0')
Loss: 1.0719012022018433


Running epoch 0, step 803, batch 803
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ...,  271,   13,  704],
        [   0,  596,  292,  ...,   13, 6673,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0072e-05,  2.6682e-04, -2.4126e-04,  ...,  1.8944e-04,
         -4.6587e-04, -2.2706e-04],
        [-7.2047e-06, -5.4166e-06,  3.3863e-06,  ..., -6.5789e-06,
         -3.9823e-06, -5.0962e-06],
        [-1.0669e-05, -7.9870e-06,  5.2974e-06,  ..., -9.2834e-06,
         -5.0068e-06, -6.9588e-06],
        [-1.5303e-05, -1.1429e-05,  7.6666e-06,  ..., -1.3545e-05,
         -7.3537e-06, -1.0297e-05],
        [-1.9580e-05, -1.4573e-05,  9.7752e-06,  ..., -1.6898e-05,
         -9.2983e-06, -1.2368e-05]], device='cuda:0')
Loss: 1.0881049633026123


Running epoch 0, step 804, batch 804
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,  9636,   417,   199],
        [    0,  2270,   278,  ..., 36325,  5892,  3558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0826e-04,  5.0623e-04, -2.6644e-04,  ...,  3.5953e-04,
         -7.0866e-04, -1.8130e-04],
        [-9.0301e-06, -6.8471e-06,  4.2878e-06,  ..., -8.2180e-06,
         -4.8801e-06, -6.2585e-06],
        [-1.3396e-05, -1.0103e-05,  6.6981e-06,  ..., -1.1653e-05,
         -6.1691e-06, -8.5980e-06],
        [-1.9148e-05, -1.4424e-05,  9.6634e-06,  ..., -1.6943e-05,
         -9.0376e-06, -1.2666e-05],
        [-2.4527e-05, -1.8448e-05,  1.2338e-05,  ..., -2.1219e-05,
         -1.1459e-05, -1.5274e-05]], device='cuda:0')
Loss: 1.0890579223632812


Running epoch 0, step 805, batch 805
Sampled inputs[:2]: tensor([[    0,   266,  3634,  ...,   694,   266,  1784],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8388e-04,  4.2196e-04, -2.0933e-04,  ...,  2.8654e-04,
         -7.0363e-04, -2.0486e-04],
        [-1.1086e-05, -8.2925e-06,  5.0962e-06,  ..., -9.9614e-06,
         -5.9530e-06, -7.5549e-06],
        [-1.6361e-05, -1.2204e-05,  7.9274e-06,  ..., -1.4111e-05,
         -7.5623e-06, -1.0371e-05],
        [-2.3231e-05, -1.7285e-05,  1.1355e-05,  ..., -2.0355e-05,
         -1.0975e-05, -1.5154e-05],
        [-3.0011e-05, -2.2322e-05,  1.4618e-05,  ..., -2.5749e-05,
         -1.4082e-05, -1.8463e-05]], device='cuda:0')
Loss: 1.0917216539382935


Running epoch 0, step 806, batch 806
Sampled inputs[:2]: tensor([[    0, 40995,  5863,  ...,    13,  9819,   609],
        [    0,   292,    17,  ...,  5760,  1345,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5898e-05,  3.4390e-04, -8.9654e-05,  ...,  5.3769e-05,
         -6.2154e-04, -1.4405e-04],
        [-1.3039e-05, -9.6932e-06,  5.9567e-06,  ..., -1.1623e-05,
         -6.9067e-06, -8.8289e-06],
        [-1.9222e-05, -1.4246e-05,  9.2611e-06,  ..., -1.6481e-05,
         -8.7917e-06, -1.2137e-05],
        [-2.7254e-05, -2.0146e-05,  1.3247e-05,  ..., -2.3693e-05,
         -1.2696e-05, -1.7673e-05],
        [-3.5375e-05, -2.6137e-05,  1.7136e-05,  ..., -3.0160e-05,
         -1.6436e-05, -2.1681e-05]], device='cuda:0')
Loss: 1.0679863691329956


Running epoch 0, step 807, batch 807
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0,    52, 26766,  ...,  4411,  4226,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8569e-05,  5.2746e-04,  6.8228e-05,  ...,  1.5661e-04,
         -5.9429e-04, -7.0129e-05],
        [-1.5020e-05, -1.1154e-05,  6.8322e-06,  ..., -1.3269e-05,
         -7.8082e-06, -1.0021e-05],
        [-2.2233e-05, -1.6466e-05,  1.0647e-05,  ..., -1.8924e-05,
         -9.9987e-06, -1.3866e-05],
        [-3.1337e-05, -2.3156e-05,  1.5140e-05,  ..., -2.7061e-05,
         -1.4357e-05, -2.0072e-05],
        [-4.0859e-05, -3.0160e-05,  1.9670e-05,  ..., -3.4630e-05,
         -1.8686e-05, -2.4766e-05]], device='cuda:0')
Loss: 1.0975005626678467
Graident accumulation at epoch 0, step 807, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.0331e-05,  4.2770e-04, -2.1088e-04,  ...,  2.7891e-04,
         -3.5455e-04,  4.5007e-05],
        [-1.5841e-05, -1.1760e-05,  6.7673e-06,  ..., -1.3865e-05,
         -7.6621e-06, -1.0198e-05],
        [ 6.7076e-06,  3.4021e-06, -1.1912e-06,  ...,  4.2857e-06,
          2.9219e-06,  1.5236e-06],
        [-1.2822e-05, -9.7252e-06,  5.8671e-06,  ..., -7.4592e-06,
         -4.8674e-06, -9.1342e-06],
        [-3.5514e-05, -2.7545e-05,  1.6634e-05,  ..., -3.0686e-05,
         -1.3980e-05, -2.2149e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8379e-08, 2.6240e-08, 2.1231e-08,  ..., 2.3151e-08, 3.6581e-08,
         8.3567e-09],
        [5.6435e-11, 3.1127e-11, 3.8781e-12,  ..., 4.0469e-11, 3.8291e-12,
         1.2535e-11],
        [2.2429e-09, 1.1097e-09, 2.4002e-10,  ..., 1.6943e-09, 1.8175e-10,
         6.6895e-10],
        [7.0402e-10, 5.0252e-10, 6.4528e-11,  ..., 5.8067e-10, 6.8988e-11,
         2.2315e-10],
        [2.9028e-10, 1.5247e-10, 2.3647e-11,  ..., 2.0737e-10, 3.3033e-11,
         6.9207e-11]], device='cuda:0')
optimizer state dict: 101.0
lr: [2.626785878335505e-06, 2.626785878335505e-06]
scheduler_last_epoch: 101


Running epoch 0, step 808, batch 808
Sampled inputs[:2]: tensor([[    0,  2919,  1482,  ...,   587, 20186,   275],
        [    0,  8840,    26,  ...,    28,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3511e-05, -7.9854e-05, -7.2820e-05,  ..., -5.2392e-05,
          1.3482e-04,  1.7440e-04],
        [-1.9222e-06, -1.4082e-06,  9.1270e-07,  ..., -1.6391e-06,
         -1.0878e-06, -1.2740e-06],
        [-2.8014e-06, -2.0564e-06,  1.3933e-06,  ..., -2.3395e-06,
         -1.4156e-06, -1.7583e-06],
        [-3.9041e-06, -2.8610e-06,  1.9521e-06,  ..., -3.2932e-06,
         -1.9968e-06, -2.5034e-06],
        [-5.1260e-06, -3.7700e-06,  2.5779e-06,  ..., -4.2617e-06,
         -2.6077e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0932039022445679


Running epoch 0, step 809, batch 809
Sampled inputs[:2]: tensor([[    0,  1871,   518,  ...,   271,   259,  1110],
        [    0, 17561,    12,  ...,   741,   496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6942e-04,  8.4203e-05, -1.7860e-04,  ...,  1.3690e-04,
         -1.9730e-04,  1.7266e-04],
        [-3.9041e-06, -2.8089e-06,  1.8179e-06,  ..., -3.3081e-06,
         -2.0228e-06, -2.4661e-06],
        [-5.7220e-06, -4.1276e-06,  2.7940e-06,  ..., -4.7386e-06,
         -2.6226e-06, -3.4198e-06],
        [-7.9572e-06, -5.7071e-06,  3.9041e-06,  ..., -6.6608e-06,
         -3.7029e-06, -4.8727e-06],
        [-1.0550e-05, -7.5847e-06,  5.1856e-06,  ..., -8.7023e-06,
         -4.8876e-06, -6.1244e-06]], device='cuda:0')
Loss: 1.079840898513794


Running epoch 0, step 810, batch 810
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,  3141,   311,  ...,   328,  7818,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3267e-04,  8.6872e-05, -1.2709e-04,  ...,  1.1974e-04,
         -1.4000e-04,  3.1204e-04],
        [-5.9605e-06, -4.2543e-06,  2.6934e-06,  ..., -5.0142e-06,
         -2.9840e-06, -3.6508e-06],
        [-8.6725e-06, -6.2138e-06,  4.1127e-06,  ..., -7.1526e-06,
         -3.8743e-06, -5.0515e-06],
        [-1.2159e-05, -8.6725e-06,  5.7742e-06,  ..., -1.0148e-05,
         -5.5060e-06, -7.2569e-06],
        [-1.5974e-05, -1.1429e-05,  7.6145e-06,  ..., -1.3143e-05,
         -7.2271e-06, -9.0599e-06]], device='cuda:0')
Loss: 1.1107804775238037


Running epoch 0, step 811, batch 811
Sampled inputs[:2]: tensor([[   0,  287,  768,  ...,  221,  474,  221],
        [   0,  266, 4908,  ..., 1209,  328, 1603]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1299e-04,  3.2841e-04, -1.8146e-04,  ...,  3.4534e-04,
         -2.9547e-04,  1.6750e-04],
        [-7.7859e-06, -5.6550e-06,  3.5688e-06,  ..., -6.6906e-06,
         -3.7998e-06, -4.8578e-06],
        [-1.1325e-05, -8.2105e-06,  5.4464e-06,  ..., -9.4771e-06,
         -4.9025e-06, -6.6832e-06],
        [-1.6034e-05, -1.1608e-05,  7.7561e-06,  ..., -1.3605e-05,
         -7.0333e-06, -9.7156e-06],
        [-2.0891e-05, -1.5125e-05,  1.0103e-05,  ..., -1.7434e-05,
         -9.2089e-06, -1.1995e-05]], device='cuda:0')
Loss: 1.0579431056976318


Running epoch 0, step 812, batch 812
Sampled inputs[:2]: tensor([[    0,   287, 19777,  ...,   266,  5061,   278],
        [    0,    13,  7805,  ...,  2733,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3020e-04,  3.5309e-04, -1.1115e-04,  ...,  2.6807e-04,
         -2.5029e-04,  1.2110e-04],
        [-9.7975e-06, -7.1824e-06,  4.3884e-06,  ..., -8.3372e-06,
         -4.7013e-06, -6.0871e-06],
        [-1.4439e-05, -1.0580e-05,  6.7800e-06,  ..., -1.1981e-05,
         -6.1616e-06, -8.5011e-06],
        [-2.0057e-05, -1.4678e-05,  9.4995e-06,  ..., -1.6883e-05,
         -8.6576e-06, -1.2115e-05],
        [-2.6464e-05, -1.9357e-05,  1.2517e-05,  ..., -2.1905e-05,
         -1.1489e-05, -1.5169e-05]], device='cuda:0')
Loss: 1.0846399068832397


Running epoch 0, step 813, batch 813
Sampled inputs[:2]: tensor([[    0,   689,    13,  ...,   756,   271, 31773],
        [    0,  2416,   352,  ...,   278,  1036, 16832]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0553e-05,  4.6440e-04, -2.0848e-04,  ...,  5.3917e-04,
         -5.5788e-04,  8.2692e-05],
        [-1.1593e-05, -8.5607e-06,  5.1893e-06,  ..., -9.9018e-06,
         -5.4352e-06, -7.1600e-06],
        [-1.7121e-05, -1.2636e-05,  8.0243e-06,  ..., -1.4290e-05,
         -7.1526e-06, -1.0058e-05],
        [-2.4021e-05, -1.7703e-05,  1.1347e-05,  ..., -2.0310e-05,
         -1.0133e-05, -1.4439e-05],
        [-3.1501e-05, -2.3201e-05,  1.4856e-05,  ..., -2.6226e-05,
         -1.3396e-05, -1.8016e-05]], device='cuda:0')
Loss: 1.0861971378326416


Running epoch 0, step 814, batch 814
Sampled inputs[:2]: tensor([[    0,    61, 22315,  ..., 36901,    17,   360],
        [    0,  1481,   278,  ...,  3940,  4938,     5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8996e-05,  4.9908e-04, -2.7997e-04,  ...,  5.9784e-04,
         -7.0630e-04,  5.9569e-05],
        [-1.3515e-05, -9.9838e-06,  6.0759e-06,  ..., -1.1526e-05,
         -6.2734e-06, -8.2999e-06],
        [ 5.2632e-05,  6.1052e-05, -5.1381e-05,  ...,  6.6360e-05,
          6.3896e-05,  1.0803e-05],
        [-2.8074e-05, -2.0683e-05,  1.3299e-05,  ..., -2.3693e-05,
         -1.1712e-05, -1.6794e-05],
        [-3.6806e-05, -2.7105e-05,  1.7419e-05,  ..., -3.0577e-05,
         -1.5497e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.0927451848983765


Running epoch 0, step 815, batch 815
Sampled inputs[:2]: tensor([[    0,  3829,   278,  ..., 11978,     9,   968],
        [    0,   328,  5180,  ...,   344,  2356,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0677e-04,  4.1886e-04, -3.5017e-04,  ...,  6.2911e-04,
         -7.0838e-04,  7.0564e-05],
        [-1.5438e-05, -1.1422e-05,  6.9849e-06,  ..., -1.3240e-05,
         -7.1228e-06, -9.4920e-06],
        [ 4.9801e-05,  5.8966e-05, -4.9973e-05,  ...,  6.3946e-05,
          6.2816e-05,  9.1642e-06],
        [-3.2127e-05, -2.3678e-05,  1.5326e-05,  ..., -2.7210e-05,
         -1.3284e-05, -1.9222e-05],
        [-4.1932e-05, -3.0875e-05,  1.9968e-05,  ..., -3.4928e-05,
         -1.7494e-05, -2.3797e-05]], device='cuda:0')
Loss: 1.1065114736557007
Graident accumulation at epoch 0, step 815, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4975e-05,  4.2682e-04, -2.2481e-04,  ...,  3.1393e-04,
         -3.8993e-04,  4.7563e-05],
        [-1.5801e-05, -1.1726e-05,  6.7890e-06,  ..., -1.3802e-05,
         -7.6081e-06, -1.0127e-05],
        [ 1.1017e-05,  8.9584e-06, -6.0694e-06,  ...,  1.0252e-05,
          8.9113e-06,  2.2877e-06],
        [-1.4752e-05, -1.1120e-05,  6.8130e-06,  ..., -9.4343e-06,
         -5.7091e-06, -1.0143e-05],
        [-3.6156e-05, -2.7878e-05,  1.6967e-05,  ..., -3.1110e-05,
         -1.4331e-05, -2.2313e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8342e-08, 2.6389e-08, 2.1333e-08,  ..., 2.3524e-08, 3.7046e-08,
         8.3533e-09],
        [5.6617e-11, 3.1226e-11, 3.9231e-12,  ..., 4.0604e-11, 3.8760e-12,
         1.2612e-11],
        [2.2431e-09, 1.1120e-09, 2.4228e-10,  ..., 1.6967e-09, 1.8552e-10,
         6.6837e-10],
        [7.0435e-10, 5.0258e-10, 6.4699e-11,  ..., 5.8083e-10, 6.9096e-11,
         2.2330e-10],
        [2.9175e-10, 1.5327e-10, 2.4022e-11,  ..., 2.0838e-10, 3.3306e-11,
         6.9704e-11]], device='cuda:0')
optimizer state dict: 102.0
lr: [2.4620397327550194e-06, 2.4620397327550194e-06]
scheduler_last_epoch: 102


Running epoch 0, step 816, batch 816
Sampled inputs[:2]: tensor([[    0,   689,  2149,  ...,  4263,    14,   292],
        [    0,  6481,   298,  ...,  6145, 16858,   824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4863e-05,  3.1956e-04, -6.4141e-05,  ...,  2.3510e-04,
         -3.8001e-04,  1.7359e-05],
        [-1.8999e-06, -1.3635e-06,  8.0094e-07,  ..., -1.6391e-06,
         -8.2701e-07, -1.3113e-06],
        [-2.8759e-06, -2.0564e-06,  1.2890e-06,  ..., -2.3842e-06,
         -1.0803e-06, -1.8626e-06],
        [-3.7998e-06, -2.7120e-06,  1.7211e-06,  ..., -3.2037e-06,
         -1.4529e-06, -2.5332e-06],
        [-5.1558e-06, -3.6806e-06,  2.3395e-06,  ..., -4.2617e-06,
         -1.9968e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.0636881589889526


Running epoch 0, step 817, batch 817
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  365, 1943,  259],
        [   0,   17,   12,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5949e-05,  4.3958e-04, -2.4129e-05,  ...,  1.9386e-04,
         -2.4710e-04,  1.9601e-04],
        [-4.0010e-06, -2.9132e-06,  1.7509e-06,  ..., -3.4198e-06,
         -1.9372e-06, -2.6375e-06],
        [-5.9307e-06, -4.3064e-06,  2.7195e-06,  ..., -4.9025e-06,
         -2.5481e-06, -3.6880e-06],
        [-8.0317e-06, -5.8115e-06,  3.7178e-06,  ..., -6.7353e-06,
         -3.5092e-06, -5.1111e-06],
        [-1.0788e-05, -7.8231e-06,  4.9919e-06,  ..., -8.9109e-06,
         -4.7386e-06, -6.5118e-06]], device='cuda:0')
Loss: 1.0981866121292114


Running epoch 0, step 818, batch 818
Sampled inputs[:2]: tensor([[   0, 5440,   13,  ..., 1878,  342, 2060],
        [   0, 6477,   12,  ..., 2931,  221,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7581e-05,  2.7133e-04,  2.9601e-05,  ...,  2.1678e-04,
         -2.2393e-04,  1.8714e-04],
        [-5.9977e-06, -4.3437e-06,  2.6636e-06,  ..., -5.0962e-06,
         -2.7753e-06, -3.7998e-06],
        [-8.8811e-06, -6.4224e-06,  4.1127e-06,  ..., -7.3314e-06,
         -3.6582e-06, -5.3346e-06],
        [ 5.6987e-05,  3.4780e-05, -5.2480e-05,  ...,  6.5477e-05,
          1.6940e-05,  2.1604e-05],
        [-1.6361e-05, -1.1817e-05,  7.6145e-06,  ..., -1.3471e-05,
         -6.8992e-06, -9.5516e-06]], device='cuda:0')
Loss: 1.087912917137146


Running epoch 0, step 819, batch 819
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,    19,    14,  ...,   278,  2588,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6435e-04,  8.2264e-05,  6.5447e-05,  ...,  8.7701e-05,
         -1.9995e-04,  3.3560e-04],
        [-8.0392e-06, -5.8264e-06,  3.5688e-06,  ..., -6.8396e-06,
         -3.6731e-06, -5.0068e-06],
        [-1.1936e-05, -8.6725e-06,  5.5209e-06,  ..., -9.9093e-06,
         -4.8950e-06, -7.0706e-06],
        [ 5.2904e-05,  3.1785e-05, -5.0587e-05,  ...,  6.2005e-05,
          1.5279e-05,  1.9250e-05],
        [-2.2024e-05, -1.5989e-05,  1.0237e-05,  ..., -1.8269e-05,
         -9.2685e-06, -1.2696e-05]], device='cuda:0')
Loss: 1.092926263809204


Running epoch 0, step 820, batch 820
Sampled inputs[:2]: tensor([[   0, 3306, 4057,  ...,  287,  266, 1692],
        [   0, 3951,   77,  ..., 7062,  278,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3639e-04, -6.4419e-05,  2.5878e-04,  ...,  2.1978e-05,
         -1.7489e-04,  4.6639e-04],
        [-1.0066e-05, -7.3537e-06,  4.4927e-06,  ..., -8.4788e-06,
         -4.4033e-06, -6.1318e-06],
        [-1.4871e-05, -1.0908e-05,  6.9141e-06,  ..., -1.2293e-05,
         -5.8860e-06, -8.6725e-06],
        [ 4.8761e-05,  2.8656e-05, -4.8620e-05,  ...,  5.8638e-05,
          1.3893e-05,  1.6970e-05],
        [-2.7478e-05, -2.0131e-05,  1.2830e-05,  ..., -2.2680e-05,
         -1.1191e-05, -1.5602e-05]], device='cuda:0')
Loss: 1.0832264423370361


Running epoch 0, step 821, batch 821
Sampled inputs[:2]: tensor([[    0,   342,  4014,  ...,   368,   408,  2105],
        [    0,   266, 10262,  ...,   271,  3437,  4392]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8263e-04, -7.5921e-05,  4.1899e-04,  ..., -2.0311e-04,
          4.1907e-04,  5.6180e-04],
        [-1.1973e-05, -8.8289e-06,  5.2825e-06,  ..., -1.0148e-05,
         -5.5879e-06, -7.4431e-06],
        [-1.7777e-05, -1.3143e-05,  8.2031e-06,  ..., -1.4722e-05,
         -7.4357e-06, -1.0520e-05],
        [ 4.4976e-05,  2.5750e-05, -4.6929e-05,  ...,  5.5434e-05,
          1.1896e-05,  1.4526e-05],
        [-3.2693e-05, -2.4095e-05,  1.5169e-05,  ..., -2.7001e-05,
         -1.3962e-05, -1.8805e-05]], device='cuda:0')
Loss: 1.079827070236206


Running epoch 0, step 822, batch 822
Sampled inputs[:2]: tensor([[    0,  1238,    14,  ...,   368,   940,   437],
        [    0, 10026,   992,  ...,   273,  2831,  8716]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3486e-04, -1.7421e-05,  4.7113e-04,  ..., -2.2266e-04,
          5.1008e-04,  6.6040e-04],
        [-1.3761e-05, -1.0222e-05,  6.0685e-06,  ..., -1.1861e-05,
         -6.6236e-06, -8.6948e-06],
        [-2.0444e-05, -1.5184e-05,  9.4548e-06,  ..., -1.7107e-05,
         -8.7321e-06, -1.2212e-05],
        [ 4.1415e-05,  2.3038e-05, -4.5245e-05,  ...,  5.2200e-05,
          1.0130e-05,  1.2217e-05],
        [-3.7611e-05, -2.7835e-05,  1.7509e-05,  ..., -3.1352e-05,
         -1.6376e-05, -2.1800e-05]], device='cuda:0')
Loss: 1.0902360677719116


Running epoch 0, step 823, batch 823
Sampled inputs[:2]: tensor([[    0,   850,    13,  ..., 11823,    13, 30706],
        [    0,   380,  6119,  ..., 11823,   287,  6797]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8740e-04,  5.0071e-05,  5.3624e-04,  ..., -2.5887e-04,
          6.3937e-04,  6.9672e-04],
        [-1.5646e-05, -1.1615e-05,  6.9328e-06,  ..., -1.3441e-05,
         -7.4953e-06, -9.9093e-06],
        [-2.3201e-05, -1.7241e-05,  1.0774e-05,  ..., -1.9372e-05,
         -9.8422e-06, -1.3888e-05],
        [ 3.7451e-05,  2.0118e-05, -4.3338e-05,  ...,  4.8907e-05,
          8.4985e-06,  9.7429e-06],
        [-4.2766e-05, -3.1680e-05,  1.9982e-05,  ..., -3.5584e-05,
         -1.8492e-05, -2.4840e-05]], device='cuda:0')
Loss: 1.0954035520553589
Graident accumulation at epoch 0, step 823, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9738e-05,  3.8914e-04, -1.4870e-04,  ...,  2.5665e-04,
         -2.8700e-04,  1.1248e-04],
        [-1.5786e-05, -1.1715e-05,  6.8034e-06,  ..., -1.3766e-05,
         -7.5969e-06, -1.0106e-05],
        [ 7.5951e-06,  6.3385e-06, -4.3851e-06,  ...,  7.2894e-06,
          7.0359e-06,  6.7010e-07],
        [-9.5320e-06, -7.9967e-06,  1.7979e-06,  ..., -3.6001e-06,
         -4.2884e-06, -8.1544e-06],
        [-3.6817e-05, -2.8258e-05,  1.7269e-05,  ..., -3.1558e-05,
         -1.4747e-05, -2.2566e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8377e-08, 2.6365e-08, 2.1599e-08,  ..., 2.3568e-08, 3.7418e-08,
         8.8304e-09],
        [5.6805e-11, 3.1330e-11, 3.9672e-12,  ..., 4.0744e-11, 3.9283e-12,
         1.2698e-11],
        [2.2414e-09, 1.1112e-09, 2.4215e-10,  ..., 1.6953e-09, 1.8543e-10,
         6.6789e-10],
        [7.0505e-10, 5.0248e-10, 6.6512e-11,  ..., 5.8264e-10, 6.9099e-11,
         2.2317e-10],
        [2.9328e-10, 1.5412e-10, 2.4397e-11,  ..., 2.0944e-10, 3.3615e-11,
         7.0251e-11]], device='cuda:0')
optimizer state dict: 103.0
lr: [2.3019008756738137e-06, 2.3019008756738137e-06]
scheduler_last_epoch: 103


Running epoch 0, step 824, batch 824
Sampled inputs[:2]: tensor([[    0, 16847,  2027,  ...,     5,  1460,   496],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3949e-05,  1.5536e-04,  3.1571e-05,  ..., -4.0963e-05,
         -2.3214e-04,  7.6577e-06],
        [-1.8105e-06, -1.3486e-06,  8.0094e-07,  ..., -1.6093e-06,
         -9.7603e-07, -1.2890e-06],
        [ 8.6421e-05,  5.9156e-05, -3.3818e-05,  ...,  6.7771e-05,
          6.0320e-05,  4.2278e-05],
        [-3.7998e-06, -2.8014e-06,  1.7956e-06,  ..., -3.2783e-06,
         -1.8328e-06, -2.5779e-06],
        [-5.0664e-06, -3.7253e-06,  2.3991e-06,  ..., -4.2617e-06,
         -2.4140e-06, -3.2336e-06]], device='cuda:0')
Loss: 1.074304223060608


Running epoch 0, step 825, batch 825
Sampled inputs[:2]: tensor([[   0,  221,  374,  ..., 2296,  365, 4579],
        [   0, 7117,  278,  ...,  287,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2467e-04, -5.1487e-05,  2.5480e-04,  ..., -3.6840e-04,
          4.7459e-04, -1.2779e-04],
        [-3.7327e-06, -2.7418e-06,  1.4864e-06,  ..., -3.3304e-06,
         -2.1383e-06, -2.6152e-06],
        [ 8.3530e-05,  5.7070e-05, -3.2648e-05,  ...,  6.5327e-05,
          5.8822e-05,  4.0483e-05],
        [-7.5847e-06, -5.5134e-06,  3.3081e-06,  ..., -6.5267e-06,
         -3.8296e-06, -5.0068e-06],
        [-1.0371e-05, -7.5400e-06,  4.5896e-06,  ..., -8.6725e-06,
         -5.1707e-06, -6.3926e-06]], device='cuda:0')
Loss: 1.0711308717727661


Running epoch 0, step 826, batch 826
Sampled inputs[:2]: tensor([[   0,   14,   69,  ...,  287,  259, 5158],
        [   0,  391, 7750,  ..., 4133,  271,  668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7685e-04,  1.4370e-05,  1.2819e-04,  ..., -4.3811e-04,
          5.5419e-04, -4.8703e-05],
        [-5.7295e-06, -4.1053e-06,  2.3209e-06,  ..., -5.0217e-06,
         -3.2708e-06, -4.0308e-06],
        [ 8.0520e-05,  5.4984e-05, -3.1314e-05,  ...,  6.2853e-05,
          5.7310e-05,  3.8456e-05],
        [-1.1638e-05, -8.2999e-06,  5.1111e-06,  ..., -9.8944e-06,
         -5.9009e-06, -7.8082e-06],
        [-1.5706e-05, -1.1235e-05,  6.9588e-06,  ..., -1.3024e-05,
         -7.8976e-06, -9.8795e-06]], device='cuda:0')
Loss: 1.0585660934448242


Running epoch 0, step 827, batch 827
Sampled inputs[:2]: tensor([[    0, 27366,   504,  ...,  1358,   365,  6883],
        [    0,   278,  4452,  ...,    14,    18,  3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9247e-04, -1.3565e-04,  2.0429e-04,  ..., -4.4750e-04,
          4.6880e-04,  4.8960e-05],
        [-7.6517e-06, -5.5134e-06,  3.2522e-06,  ..., -6.6981e-06,
         -4.1202e-06, -5.2303e-06],
        [ 7.7644e-05,  5.2883e-05, -2.9884e-05,  ...,  6.0410e-05,
          5.6177e-05,  3.6758e-05],
        [-1.5661e-05, -1.1235e-05,  7.1377e-06,  ..., -1.3381e-05,
         -7.5027e-06, -1.0267e-05],
        [-2.1011e-05, -1.5110e-05,  9.5963e-06,  ..., -1.7524e-05,
         -1.0058e-05, -1.2934e-05]], device='cuda:0')
Loss: 1.1107099056243896


Running epoch 0, step 828, batch 828
Sampled inputs[:2]: tensor([[    0,  3256,   221,  ..., 18116,   292, 47989],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4050e-04, -5.9031e-05, -1.0513e-05,  ..., -2.3771e-04,
          7.6787e-05, -8.6495e-05],
        [-9.3728e-06, -6.7577e-06,  4.0270e-06,  ..., -8.5682e-06,
         -5.5283e-06, -6.7800e-06],
        [ 7.5230e-05,  5.1124e-05, -2.8692e-05,  ...,  5.8025e-05,
          5.4553e-05,  3.4865e-05],
        [-1.9103e-05, -1.3709e-05,  8.8736e-06,  ..., -1.6943e-05,
         -9.9763e-06, -1.3143e-05],
        [-2.5451e-05, -1.8328e-05,  1.1772e-05,  ..., -2.1815e-05,
         -1.2994e-05, -1.6212e-05]], device='cuda:0')
Loss: 1.0761045217514038


Running epoch 0, step 829, batch 829
Sampled inputs[:2]: tensor([[   0,   14, 2787,  ..., 9674, 2491,   12],
        [   0, 1197, 3025,  ...,   14,  747, 3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5773e-04, -2.6649e-04,  1.1724e-04,  ..., -2.9884e-04,
          4.0599e-05, -1.3796e-05],
        [-1.1325e-05, -8.1733e-06,  4.8913e-06,  ..., -1.0155e-05,
         -6.1393e-06, -7.9125e-06],
        [ 7.2280e-05,  4.8979e-05, -2.7321e-05,  ...,  5.5656e-05,
          5.3726e-05,  3.3204e-05],
        [-2.3186e-05, -1.6659e-05,  1.0781e-05,  ..., -2.0236e-05,
         -1.1101e-05, -1.5482e-05],
        [-3.0905e-05, -2.2292e-05,  1.4320e-05,  ..., -2.6196e-05,
         -1.4603e-05, -1.9208e-05]], device='cuda:0')
Loss: 1.089094638824463


Running epoch 0, step 830, batch 830
Sampled inputs[:2]: tensor([[    0,    16,    14,  ...,   300,  9283,    14],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0126e-04, -3.4310e-04,  5.3838e-05,  ..., -3.1931e-04,
          8.1056e-05,  1.3695e-05],
        [-1.3337e-05, -9.6411e-06,  5.7705e-06,  ..., -1.1906e-05,
         -7.2345e-06, -9.2462e-06],
        [ 6.9493e-05,  4.6937e-05, -2.6017e-05,  ...,  5.3302e-05,
          5.2378e-05,  3.1453e-05],
        [-2.7061e-05, -1.9476e-05,  1.2599e-05,  ..., -2.3559e-05,
         -1.3001e-05, -1.7956e-05],
        [-3.6180e-05, -2.6166e-05,  1.6809e-05,  ..., -3.0637e-05,
         -1.7211e-05, -2.2396e-05]], device='cuda:0')
Loss: 1.084591031074524


Running epoch 0, step 831, batch 831
Sampled inputs[:2]: tensor([[    0,  1074,  1593,  ...,   992,  1810,   300],
        [    0,  2734,  2338,  ...,  3977,   970, 10537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8725e-04, -8.7692e-05, -1.8469e-05,  ..., -1.9826e-04,
         -1.9581e-04,  3.4907e-05],
        [-1.5199e-05, -1.1012e-05,  6.5938e-06,  ..., -1.3620e-05,
         -8.3074e-06, -1.0461e-05],
        [ 6.6781e-05,  4.4940e-05, -2.4735e-05,  ...,  5.0933e-05,
          5.1029e-05,  2.9828e-05],
        [-3.0860e-05, -2.2247e-05,  1.4402e-05,  ..., -2.6941e-05,
         -1.4953e-05, -2.0310e-05],
        [-4.1157e-05, -2.9817e-05,  1.9178e-05,  ..., -3.4958e-05,
         -1.9714e-05, -2.5272e-05]], device='cuda:0')
Loss: 1.0987955331802368
Graident accumulation at epoch 0, step 831, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1961e-05,  3.4146e-04, -1.3568e-04,  ...,  2.1116e-04,
         -2.7788e-04,  1.0472e-04],
        [-1.5727e-05, -1.1644e-05,  6.7824e-06,  ..., -1.3751e-05,
         -7.6679e-06, -1.0141e-05],
        [ 1.3514e-05,  1.0199e-05, -6.4202e-06,  ...,  1.1654e-05,
          1.1435e-05,  3.5859e-06],
        [-1.1665e-05, -9.4217e-06,  3.0583e-06,  ..., -5.9342e-06,
         -5.3549e-06, -9.3700e-06],
        [-3.7251e-05, -2.8414e-05,  1.7459e-05,  ..., -3.1898e-05,
         -1.5244e-05, -2.2837e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8566e-08, 2.6346e-08, 2.1578e-08,  ..., 2.3583e-08, 3.7419e-08,
         8.8227e-09],
        [5.6979e-11, 3.1420e-11, 4.0067e-12,  ..., 4.0888e-11, 3.9934e-12,
         1.2795e-11],
        [2.2436e-09, 1.1121e-09, 2.4252e-10,  ..., 1.6962e-09, 1.8785e-10,
         6.6812e-10],
        [7.0529e-10, 5.0247e-10, 6.6653e-11,  ..., 5.8279e-10, 6.9253e-11,
         2.2336e-10],
        [2.9468e-10, 1.5486e-10, 2.4740e-11,  ..., 2.1045e-10, 3.3970e-11,
         7.0820e-11]], device='cuda:0')
optimizer state dict: 104.0
lr: [2.1464671858134968e-06, 2.1464671858134968e-06]
scheduler_last_epoch: 104


Running epoch 0, step 832, batch 832
Sampled inputs[:2]: tensor([[    0,  1235,   368,  ..., 12152,  8498,   287],
        [    0,   271,   768,  ..., 15555,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0025e-05,  7.5703e-06, -4.6266e-05,  ...,  1.1265e-05,
         -1.1841e-04,  5.7662e-05],
        [-2.0117e-06, -1.4454e-06,  8.6427e-07,  ..., -1.6540e-06,
         -7.1526e-07, -1.2070e-06],
        [-3.0249e-06, -2.1756e-06,  1.3635e-06,  ..., -2.4587e-06,
         -9.6858e-07, -1.7434e-06],
        [-4.2021e-06, -2.9951e-06,  1.8924e-06,  ..., -3.4273e-06,
         -1.3486e-06, -2.4736e-06],
        [-5.6028e-06, -3.9935e-06,  2.5332e-06,  ..., -4.5300e-06,
         -1.8626e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.0843144655227661


Running epoch 0, step 833, batch 833
Sampled inputs[:2]: tensor([[    0,  6532,  6984,  ...,   271,  8212, 14409],
        [    0,   278,  3358,  ...,    12,   287,  9612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5559e-05,  1.6285e-04, -1.0269e-04,  ...,  6.1473e-05,
         -1.9460e-04,  5.9733e-06],
        [-3.9637e-06, -2.8759e-06,  1.6280e-06,  ..., -3.3975e-06,
         -1.7434e-06, -2.4587e-06],
        [-5.9009e-06, -4.2766e-06,  2.5630e-06,  ..., -4.9174e-06,
         -2.2799e-06, -3.4496e-06],
        [-8.0019e-06, -5.7518e-06,  3.4869e-06,  ..., -6.7353e-06,
         -3.1069e-06, -4.8131e-06],
        [-1.0848e-05, -7.8082e-06,  4.7535e-06,  ..., -9.0003e-06,
         -4.3064e-06, -6.1542e-06]], device='cuda:0')
Loss: 1.0961940288543701


Running epoch 0, step 834, batch 834
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,  1040,   287,  ...,    14, 10209,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8318e-04,  2.3939e-04, -1.4708e-04,  ...,  2.1401e-04,
         -3.3892e-04, -7.6146e-06],
        [-5.9605e-06, -4.3362e-06,  2.5183e-06,  ..., -5.0887e-06,
         -2.5444e-06, -3.6508e-06],
        [-8.8662e-06, -6.4671e-06,  3.9414e-06,  ..., -7.3910e-06,
         -3.3602e-06, -5.1409e-06],
        [-1.2085e-05, -8.7470e-06,  5.4091e-06,  ..., -1.0177e-05,
         -4.5970e-06, -7.1973e-06],
        [-1.6212e-05, -1.1802e-05,  7.2718e-06,  ..., -1.3500e-05,
         -6.3330e-06, -9.1493e-06]], device='cuda:0')
Loss: 1.0709043741226196


Running epoch 0, step 835, batch 835
Sampled inputs[:2]: tensor([[    0,  6660, 13165,  ...,   380,   333,   199],
        [    0,  2336,    26,  ...,  2564,   271,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9741e-04,  3.7856e-04, -3.9136e-05,  ...,  2.7194e-04,
         -4.6155e-04, -9.6448e-05],
        [-7.9870e-06, -5.7146e-06,  3.4086e-06,  ..., -6.7875e-06,
         -3.5129e-06, -4.9695e-06],
        [-1.1832e-05, -8.4937e-06,  5.3048e-06,  ..., -9.8199e-06,
         -4.6417e-06, -6.9737e-06],
        [-1.6198e-05, -1.1548e-05,  7.3090e-06,  ..., -1.3575e-05,
         -6.3926e-06, -9.7901e-06],
        [-2.1666e-05, -1.5557e-05,  9.8050e-06,  ..., -1.7971e-05,
         -8.7470e-06, -1.2428e-05]], device='cuda:0')
Loss: 1.0786924362182617


Running epoch 0, step 836, batch 836
Sampled inputs[:2]: tensor([[    0, 13576,   431,  ...,    14,   475,   298],
        [    0,  4889,  3593,  ..., 19787,   287, 22475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5541e-04,  3.6759e-04, -1.1525e-04,  ...,  2.9609e-04,
         -4.7936e-04, -1.8234e-04],
        [-9.9540e-06, -7.1898e-06,  4.2878e-06,  ..., -8.4490e-06,
         -4.1947e-06, -6.0648e-06],
        [-1.4797e-05, -1.0714e-05,  6.6906e-06,  ..., -1.2279e-05,
         -5.5470e-06, -8.5533e-06],
        [-2.0310e-05, -1.4603e-05,  9.2313e-06,  ..., -1.7002e-05,
         -7.6368e-06, -1.2010e-05],
        [-2.7090e-05, -1.9610e-05,  1.2353e-05,  ..., -2.2471e-05,
         -1.0476e-05, -1.5244e-05]], device='cuda:0')
Loss: 1.09645676612854


Running epoch 0, step 837, batch 837
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0, 14652,    12,  ..., 17330,   996,  3294]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8395e-04,  5.8580e-04, -1.1702e-04,  ...,  3.3938e-04,
         -6.1791e-04, -5.5367e-05],
        [-1.1817e-05, -8.6129e-06,  5.1446e-06,  ..., -1.0103e-05,
         -5.0701e-06, -7.2718e-06],
        [-1.7568e-05, -1.2815e-05,  8.0392e-06,  ..., -1.4633e-05,
         -6.6645e-06, -1.0215e-05],
        [-2.4155e-05, -1.7509e-05,  1.1139e-05,  ..., -2.0340e-05,
         -9.2164e-06, -1.4409e-05],
        [-3.2187e-05, -2.3454e-05,  1.4856e-05,  ..., -2.6762e-05,
         -1.2621e-05, -1.8194e-05]], device='cuda:0')
Loss: 1.073839783668518


Running epoch 0, step 838, batch 838
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,   609,   271,  ...,  4684, 14107,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6753e-04,  5.4147e-04, -7.1735e-05,  ...,  3.4732e-04,
         -4.9125e-04, -5.4404e-05],
        [-1.3813e-05, -1.0081e-05,  6.0238e-06,  ..., -1.1817e-05,
         -5.9269e-06, -8.4639e-06],
        [-2.0608e-05, -1.5065e-05,  9.4399e-06,  ..., -1.7181e-05,
         -7.8194e-06, -1.1943e-05],
        [-2.8238e-05, -2.0519e-05,  1.3016e-05,  ..., -2.3797e-05,
         -1.0803e-05, -1.6809e-05],
        [-3.7640e-05, -2.7478e-05,  1.7360e-05,  ..., -3.1322e-05,
         -1.4752e-05, -2.1204e-05]], device='cuda:0')
Loss: 1.0968214273452759


Running epoch 0, step 839, batch 839
Sampled inputs[:2]: tensor([[    0,    13,  4467,  ...,  2390, 47857,   287],
        [    0,  3087,   401,  ...,  1875,  4122,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3706e-04,  8.1650e-04, -7.1162e-05,  ...,  3.1644e-04,
         -4.2101e-04, -7.0068e-05],
        [-1.5825e-05, -1.1533e-05,  6.8322e-06,  ..., -1.3553e-05,
         -7.0594e-06, -9.7826e-06],
        [-2.3574e-05, -1.7196e-05,  1.0721e-05,  ..., -1.9640e-05,
         -9.2871e-06, -1.3754e-05],
        [-3.2350e-05, -2.3454e-05,  1.4789e-05,  ..., -2.7239e-05,
         -1.2845e-05, -1.9386e-05],
        [-4.3064e-05, -3.1352e-05,  1.9729e-05,  ..., -3.5793e-05,
         -1.7449e-05, -2.4423e-05]], device='cuda:0')
Loss: 1.1057565212249756
Graident accumulation at epoch 0, step 839, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3941e-05,  3.8896e-04, -1.2923e-04,  ...,  2.2169e-04,
         -2.9219e-04,  8.7242e-05],
        [-1.5737e-05, -1.1633e-05,  6.7874e-06,  ..., -1.3731e-05,
         -7.6071e-06, -1.0105e-05],
        [ 9.8050e-06,  7.4592e-06, -4.7060e-06,  ...,  8.5244e-06,
          9.3630e-06,  1.8520e-06],
        [-1.3733e-05, -1.0825e-05,  4.2314e-06,  ..., -8.0647e-06,
         -6.1039e-06, -1.0372e-05],
        [-3.7832e-05, -2.8708e-05,  1.7686e-05,  ..., -3.2287e-05,
         -1.5464e-05, -2.2995e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8631e-08, 2.6987e-08, 2.1561e-08,  ..., 2.3660e-08, 3.7559e-08,
         8.8188e-09],
        [5.7172e-11, 3.1521e-11, 4.0494e-12,  ..., 4.1031e-11, 4.0392e-12,
         1.2878e-11],
        [2.2419e-09, 1.1113e-09, 2.4239e-10,  ..., 1.6949e-09, 1.8775e-10,
         6.6764e-10],
        [7.0563e-10, 5.0252e-10, 6.6805e-11,  ..., 5.8295e-10, 6.9349e-11,
         2.2351e-10],
        [2.9624e-10, 1.5568e-10, 2.5105e-11,  ..., 2.1152e-10, 3.4240e-11,
         7.1345e-11]], device='cuda:0')
optimizer state dict: 105.0
lr: [1.995833666043061e-06, 1.995833666043061e-06]
scheduler_last_epoch: 105


Running epoch 0, step 840, batch 840
Sampled inputs[:2]: tensor([[    0, 25778,  3804,  ...,  2354,    12,   554],
        [    0,   271, 21394,  ...,  1487,   287,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1903e-05, -1.5359e-04,  1.6144e-04,  ..., -8.7484e-05,
          1.1772e-04,  1.0324e-04],
        [-2.0862e-06, -1.5199e-06,  8.1211e-07,  ..., -1.7732e-06,
         -1.0058e-06, -1.2517e-06],
        [-3.0845e-06, -2.2650e-06,  1.2591e-06,  ..., -2.5481e-06,
         -1.3337e-06, -1.7509e-06],
        [-4.1425e-06, -3.0249e-06,  1.6838e-06,  ..., -3.4720e-06,
         -1.8179e-06, -2.4140e-06],
        [-5.6624e-06, -4.1425e-06,  2.3246e-06,  ..., -4.6492e-06,
         -2.5034e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.1025776863098145


Running epoch 0, step 841, batch 841
Sampled inputs[:2]: tensor([[    0, 43788,    12,  ...,    12,  6288,   391],
        [    0,  2440,  1458,  ...,  7650,   328,  2297]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5384e-04, -2.7444e-04,  1.2938e-04,  ..., -1.0773e-04,
          3.3366e-04,  2.3183e-04],
        [-3.9935e-06, -2.8834e-06,  1.6429e-06,  ..., -3.5167e-06,
         -1.9670e-06, -2.4885e-06],
        [-5.9009e-06, -4.2766e-06,  2.5630e-06,  ..., -5.0217e-06,
         -2.5779e-06, -3.4422e-06],
        [-8.0764e-06, -5.8264e-06,  3.5167e-06,  ..., -6.9886e-06,
         -3.5912e-06, -4.8578e-06],
        [-1.0967e-05, -7.9274e-06,  4.7833e-06,  ..., -9.2685e-06,
         -4.9025e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.0892478227615356


Running epoch 0, step 842, batch 842
Sampled inputs[:2]: tensor([[    0,  3352,   259,  ...,  3565,    12,   409],
        [    0,   352,   721,  ...,   634, 17642,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4950e-04, -4.5840e-04,  4.0591e-06,  ..., -1.3199e-04,
          2.9294e-04,  2.5904e-04],
        [-5.9158e-06, -4.3735e-06,  2.5332e-06,  ..., -5.2005e-06,
         -2.6412e-06, -3.6284e-06],
        [-8.8513e-06, -6.5565e-06,  3.9786e-06,  ..., -7.5549e-06,
         -3.5167e-06, -5.1111e-06],
        [-1.2040e-05, -8.8811e-06,  5.4538e-06,  ..., -1.0446e-05,
         -4.8503e-06, -7.1675e-06],
        [-1.6272e-05, -1.2010e-05,  7.3463e-06,  ..., -1.3828e-05,
         -6.6683e-06, -9.1195e-06]], device='cuda:0')
Loss: 1.0829957723617554


Running epoch 0, step 843, batch 843
Sampled inputs[:2]: tensor([[   0, 1920,   19,  ..., 5232,  796, 1303],
        [   0,  266,  824,  ..., 1799,  287, 6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8758e-04, -7.7035e-04, -5.3134e-05,  ..., -3.2452e-04,
          5.3809e-04,  2.7048e-04],
        [-7.7412e-06, -5.7742e-06,  3.3304e-06,  ..., -6.8694e-06,
         -3.4794e-06, -4.7684e-06],
        [-1.1623e-05, -8.6576e-06,  5.2601e-06,  ..., -9.9987e-06,
         -4.6045e-06, -6.7353e-06],
        [-1.5751e-05, -1.1697e-05,  7.1824e-06,  ..., -1.3754e-05,
         -6.3181e-06, -9.4026e-06],
        [-2.1368e-05, -1.5885e-05,  9.7156e-06,  ..., -1.8299e-05,
         -8.7395e-06, -1.1995e-05]], device='cuda:0')
Loss: 1.1004060506820679


Running epoch 0, step 844, batch 844
Sampled inputs[:2]: tensor([[    0,   508,  2322,  ...,   968,   266, 15123],
        [    0,  4191,   368,  ...,   367,  4182,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9083e-04, -6.2322e-04, -1.5683e-07,  ..., -4.9657e-04,
          5.8477e-04,  2.2659e-04],
        [-9.7230e-06, -7.2569e-06,  4.1649e-06,  ..., -8.5384e-06,
         -4.2841e-06, -5.9605e-06],
        [-1.4648e-05, -1.0923e-05,  6.6012e-06,  ..., -1.2472e-05,
         -5.6699e-06, -8.4639e-06],
        [-1.9804e-05, -1.4693e-05,  8.9929e-06,  ..., -1.7092e-05,
         -7.7561e-06, -1.1772e-05],
        [-2.6882e-05, -1.9968e-05,  1.2159e-05,  ..., -2.2799e-05,
         -1.0751e-05, -1.5050e-05]], device='cuda:0')
Loss: 1.0675890445709229


Running epoch 0, step 845, batch 845
Sampled inputs[:2]: tensor([[    0,  2280,   344,  ...,   287,   266,  3344],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6818e-04, -4.3007e-04, -8.4246e-05,  ..., -2.3673e-04,
          2.8205e-04,  2.1188e-04],
        [-1.1593e-05, -8.6427e-06,  4.9584e-06,  ..., -1.0267e-05,
         -5.2974e-06, -7.2420e-06],
        [-1.7330e-05, -1.2919e-05,  7.8231e-06,  ..., -1.4842e-05,
         -6.9290e-06, -1.0155e-05],
        [-2.3529e-05, -1.7449e-05,  1.0706e-05,  ..., -2.0444e-05,
         -9.5442e-06, -1.4201e-05],
        [-3.1769e-05, -2.3589e-05,  1.4395e-05,  ..., -2.7061e-05,
         -1.3091e-05, -1.8016e-05]], device='cuda:0')
Loss: 1.0728375911712646


Running epoch 0, step 846, batch 846
Sampled inputs[:2]: tensor([[    0,  3058,   292,  ...,  1387,  1236,   369],
        [    0,  4710,    12,  ...,  3969,     9, 11692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5597e-04, -4.8029e-04, -2.9121e-05,  ..., -3.6209e-04,
          2.1359e-04,  2.5679e-04],
        [-1.3590e-05, -1.0066e-05,  5.7481e-06,  ..., -1.2085e-05,
         -6.4969e-06, -8.6278e-06],
        [-2.0280e-05, -1.5035e-05,  9.0972e-06,  ..., -1.7419e-05,
         -8.4937e-06, -1.2062e-05],
        [-2.7284e-05, -2.0131e-05,  1.2323e-05,  ..., -2.3752e-05,
         -1.1541e-05, -1.6659e-05],
        [-3.7044e-05, -2.7373e-05,  1.6704e-05,  ..., -3.1620e-05,
         -1.5907e-05, -2.1294e-05]], device='cuda:0')
Loss: 1.0744538307189941


Running epoch 0, step 847, batch 847
Sampled inputs[:2]: tensor([[    0,  7011,   650,  ..., 28839, 11610,  3222],
        [    0, 16763,  1538,  ...,   631,  3299,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2430e-04, -4.9799e-04, -2.0094e-04,  ..., -2.5118e-04,
          2.3356e-04,  3.3874e-04],
        [-1.5557e-05, -1.1452e-05,  6.5751e-06,  ..., -1.3806e-05,
         -7.4655e-06, -9.9242e-06],
        [-2.3142e-05, -1.7062e-05,  1.0371e-05,  ..., -1.9848e-05,
         -9.7603e-06, -1.3836e-05],
        [-3.1307e-05, -2.2963e-05,  1.4126e-05,  ..., -2.7224e-05,
         -1.3344e-05, -1.9237e-05],
        [-4.2260e-05, -3.1084e-05,  1.9044e-05,  ..., -3.6031e-05,
         -1.8261e-05, -2.4423e-05]], device='cuda:0')
Loss: 1.0973495244979858
Graident accumulation at epoch 0, step 847, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9883e-05,  3.0027e-04, -1.3640e-04,  ...,  1.7440e-04,
         -2.3962e-04,  1.1239e-04],
        [-1.5719e-05, -1.1615e-05,  6.7662e-06,  ..., -1.3739e-05,
         -7.5929e-06, -1.0087e-05],
        [ 6.5103e-06,  5.0071e-06, -3.1983e-06,  ...,  5.6871e-06,
          7.4507e-06,  2.8320e-07],
        [-1.5491e-05, -1.2039e-05,  5.2209e-06,  ..., -9.9807e-06,
         -6.8279e-06, -1.1258e-05],
        [-3.8275e-05, -2.8946e-05,  1.7822e-05,  ..., -3.2662e-05,
         -1.5744e-05, -2.3138e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8762e-08, 2.7208e-08, 2.1580e-08,  ..., 2.3699e-08, 3.7576e-08,
         8.9248e-09],
        [5.7357e-11, 3.1621e-11, 4.0886e-12,  ..., 4.1181e-11, 4.0909e-12,
         1.2963e-11],
        [2.2402e-09, 1.1105e-09, 2.4226e-10,  ..., 1.6936e-09, 1.8765e-10,
         6.6716e-10],
        [7.0591e-10, 5.0254e-10, 6.6938e-11,  ..., 5.8310e-10, 6.9458e-11,
         2.2366e-10],
        [2.9773e-10, 1.5649e-10, 2.5443e-11,  ..., 2.1261e-10, 3.4539e-11,
         7.1870e-11]], device='cuda:0')
optimizer state dict: 106.0
lr: [1.8500923853120123e-06, 1.8500923853120123e-06]
scheduler_last_epoch: 106


Running epoch 0, step 848, batch 848
Sampled inputs[:2]: tensor([[   0,   16,   52,  ...,   12,  298,  374],
        [   0,  790, 2816,  ...,   14, 1062,  668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4942e-05,  1.6798e-04, -7.8991e-05,  ...,  1.9039e-04,
         -1.6424e-04, -1.1617e-06],
        [-1.9968e-06, -1.4529e-06,  8.7172e-07,  ..., -1.7583e-06,
         -7.7114e-07, -1.1474e-06],
        [-2.9951e-06, -2.1905e-06,  1.3709e-06,  ..., -2.5779e-06,
         -1.0356e-06, -1.6391e-06],
        [-4.1127e-06, -2.9802e-06,  1.8850e-06,  ..., -3.5763e-06,
         -1.4305e-06, -2.3097e-06],
        [-5.3942e-06, -3.9339e-06,  2.4885e-06,  ..., -4.6492e-06,
         -1.9222e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.0971412658691406


Running epoch 0, step 849, batch 849
Sampled inputs[:2]: tensor([[   0,  266, 4616,  ..., 1906, 7256,  287],
        [   0, 1415,  300,  ..., 1497, 5715, 4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1986e-05, -1.2028e-04, -2.1258e-04,  ...,  1.8529e-04,
         -2.1652e-05, -7.6563e-05],
        [-3.9041e-06, -2.8610e-06,  1.7136e-06,  ..., -3.4347e-06,
         -1.4529e-06, -2.3171e-06],
        [-5.8562e-06, -4.3064e-06,  2.7120e-06,  ..., -5.0217e-06,
         -1.9409e-06, -3.3006e-06],
        [-8.1062e-06, -5.9158e-06,  3.7625e-06,  ..., -7.0333e-06,
         -2.7046e-06, -4.6939e-06],
        [-1.0610e-05, -7.7486e-06,  4.9174e-06,  ..., -9.0599e-06,
         -3.6284e-06, -5.7817e-06]], device='cuda:0')
Loss: 1.0724295377731323


Running epoch 0, step 850, batch 850
Sampled inputs[:2]: tensor([[    0,   857,   352,  ...,  3608,   271,   995],
        [    0,  1171,   341,  ...,   278, 14713,    18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1986e-05, -1.3646e-04, -1.5752e-04,  ...,  4.0143e-04,
         -1.8768e-04, -7.6563e-05],
        [-5.9158e-06, -4.3213e-06,  2.5630e-06,  ..., -5.1409e-06,
         -2.3209e-06, -3.4496e-06],
        [-8.8662e-06, -6.5118e-06,  4.0531e-06,  ..., -7.5400e-06,
         -3.1330e-06, -4.9323e-06],
        [-1.2308e-05, -8.9705e-06,  5.6326e-06,  ..., -1.0580e-05,
         -4.3735e-06, -7.0184e-06],
        [-1.6063e-05, -1.1712e-05,  7.3314e-06,  ..., -1.3590e-05,
         -5.8338e-06, -8.6576e-06]], device='cuda:0')
Loss: 1.0903511047363281


Running epoch 0, step 851, batch 851
Sampled inputs[:2]: tensor([[    0,  2645,    12,  ...,     5,  1239,  7200],
        [    0, 17471,  7279,  ...,   328,  6179,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5439e-05,  8.1189e-05, -1.1784e-04,  ...,  4.5579e-04,
         -3.5528e-04,  1.5917e-05],
        [-8.1658e-06, -5.8636e-06,  3.3937e-06,  ..., -7.0110e-06,
         -3.3788e-06, -4.7013e-06],
        [-1.2130e-05, -8.7619e-06,  5.3123e-06,  ..., -1.0237e-05,
         -4.5784e-06, -6.6832e-06],
        [-1.6689e-05, -1.1981e-05,  7.3239e-06,  ..., -1.4216e-05,
         -6.3106e-06, -9.4175e-06],
        [-2.2084e-05, -1.5885e-05,  9.6560e-06,  ..., -1.8567e-05,
         -8.5458e-06, -1.1832e-05]], device='cuda:0')
Loss: 1.110139012336731


Running epoch 0, step 852, batch 852
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0, 19191,   266,  ...,   287,   843,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1875e-06, -3.4945e-05, -7.7393e-05,  ...,  4.1881e-04,
         -2.9660e-04, -2.5570e-05],
        [-1.0237e-05, -7.3463e-06,  4.2804e-06,  ..., -8.7246e-06,
         -4.1425e-06, -5.8413e-06],
        [-1.5214e-05, -1.0982e-05,  6.6757e-06,  ..., -1.2770e-05,
         -5.6438e-06, -8.3372e-06],
        [-2.0951e-05, -1.5035e-05,  9.2238e-06,  ..., -1.7732e-05,
         -7.7635e-06, -1.1757e-05],
        [-2.7746e-05, -1.9938e-05,  1.2159e-05,  ..., -2.3216e-05,
         -1.0572e-05, -1.4797e-05]], device='cuda:0')
Loss: 1.0995712280273438


Running epoch 0, step 853, batch 853
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,   12, 1631,  ..., 1143,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6164e-04,  5.6457e-04, -2.5895e-04,  ...,  5.9656e-04,
         -5.4473e-04, -4.5491e-05],
        [-1.2085e-05, -8.7023e-06,  5.0664e-06,  ..., -1.0461e-05,
         -5.2750e-06, -7.2643e-06],
        [-1.8001e-05, -1.3009e-05,  7.9423e-06,  ..., -1.5184e-05,
         -7.0594e-06, -1.0245e-05],
        [-2.4661e-05, -1.7717e-05,  1.0937e-05,  ..., -2.1055e-05,
         -9.7156e-06, -1.4439e-05],
        [-3.2753e-05, -2.3544e-05,  1.4439e-05,  ..., -2.7508e-05,
         -1.3135e-05, -1.8075e-05]], device='cuda:0')
Loss: 1.048024296760559


Running epoch 0, step 854, batch 854
Sampled inputs[:2]: tensor([[    0,   287,  4599,  ..., 11812,   266,  1036],
        [    0,    17,  3978,  ...,  3988,   598,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6442e-04,  8.5065e-04, -3.0852e-04,  ...,  7.7095e-04,
         -7.1271e-04, -9.9819e-05],
        [-1.4096e-05, -1.0110e-05,  5.8822e-06,  ..., -1.2249e-05,
         -6.0834e-06, -8.4266e-06],
        [-2.1026e-05, -1.5154e-05,  9.2462e-06,  ..., -1.7807e-05,
         -8.1398e-06, -1.1906e-05],
        [-2.8834e-05, -2.0653e-05,  1.2718e-05,  ..., -2.4721e-05,
         -1.1221e-05, -1.6794e-05],
        [-3.8236e-05, -2.7418e-05,  1.6794e-05,  ..., -3.2246e-05,
         -1.5132e-05, -2.0996e-05]], device='cuda:0')
Loss: 1.0906633138656616


Running epoch 0, step 855, batch 855
Sampled inputs[:2]: tensor([[    0,  1985,   278,  ...,   677, 12292, 17956],
        [    0,  2548,   720,  ...,  1795,  1109, 32948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3953e-04,  8.5852e-04, -4.5843e-04,  ...,  7.7167e-04,
         -9.5891e-04, -3.6610e-04],
        [-1.6004e-05, -1.1504e-05,  6.7465e-06,  ..., -1.3955e-05,
         -7.0892e-06, -9.7156e-06],
        [-2.3872e-05, -1.7241e-05,  1.0610e-05,  ..., -2.0280e-05,
         -9.4660e-06, -1.3702e-05],
        [-3.2738e-05, -2.3499e-05,  1.4603e-05,  ..., -2.8178e-05,
         -1.3083e-05, -1.9342e-05],
        [-4.3482e-05, -3.1233e-05,  1.9297e-05,  ..., -3.6776e-05,
         -1.7621e-05, -2.4185e-05]], device='cuda:0')
Loss: 1.089931607246399
Graident accumulation at epoch 0, step 855, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7058e-05,  3.5609e-04, -1.6860e-04,  ...,  2.3413e-04,
         -3.1155e-04,  6.4543e-05],
        [-1.5747e-05, -1.1604e-05,  6.7642e-06,  ..., -1.3761e-05,
         -7.5425e-06, -1.0050e-05],
        [ 3.4721e-06,  2.7824e-06, -1.8175e-06,  ...,  3.0904e-06,
          5.7590e-06, -1.1153e-06],
        [-1.7216e-05, -1.3185e-05,  6.1591e-06,  ..., -1.1800e-05,
         -7.4534e-06, -1.2067e-05],
        [-3.8796e-05, -2.9174e-05,  1.7970e-05,  ..., -3.3073e-05,
         -1.5932e-05, -2.3243e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9122e-08, 2.7917e-08, 2.1769e-08,  ..., 2.4271e-08, 3.8457e-08,
         9.0499e-09],
        [5.7556e-11, 3.1722e-11, 4.1300e-12,  ..., 4.1334e-11, 4.1371e-12,
         1.3045e-11],
        [2.2385e-09, 1.1097e-09, 2.4213e-10,  ..., 1.6923e-09, 1.8756e-10,
         6.6668e-10],
        [7.0627e-10, 5.0259e-10, 6.7084e-11,  ..., 5.8332e-10, 6.9559e-11,
         2.2381e-10],
        [2.9933e-10, 1.5731e-10, 2.5789e-11,  ..., 2.1375e-10, 3.4815e-11,
         7.2383e-11]], device='cuda:0')
optimizer state dict: 107.0
lr: [1.7093324223767748e-06, 1.7093324223767748e-06]
scheduler_last_epoch: 107


Running epoch 0, step 856, batch 856
Sampled inputs[:2]: tensor([[    0,   266, 15794,  ...,  3128,  6479,  2626],
        [    0,  1057,    14,  ...,    14,  4735,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6399e-05,  1.0033e-05,  9.9862e-05,  ..., -9.0176e-05,
          1.6644e-04, -1.0362e-04],
        [-1.9819e-06, -1.4156e-06,  8.0839e-07,  ..., -1.7211e-06,
         -9.4995e-07, -1.2666e-06],
        [-3.0249e-06, -2.1607e-06,  1.2890e-06,  ..., -2.5630e-06,
         -1.3188e-06, -1.8477e-06],
        [-4.0531e-06, -2.8908e-06,  1.7434e-06,  ..., -3.4720e-06,
         -1.7583e-06, -2.5332e-06],
        [-5.3048e-06, -3.7998e-06,  2.2650e-06,  ..., -4.4703e-06,
         -2.3395e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.0862171649932861


Running epoch 0, step 857, batch 857
Sampled inputs[:2]: tensor([[    0,    69,   462,  ...,   437,   266,   634],
        [    0, 33119,   391,  ...,   292,  4462,  2721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6688e-05,  4.2365e-05,  1.3698e-04,  ..., -1.9264e-05,
          1.2535e-04, -4.7595e-05],
        [-4.0233e-06, -2.9281e-06,  1.5944e-06,  ..., -3.4943e-06,
         -1.9856e-06, -2.5705e-06],
        [-6.0201e-06, -4.3958e-06,  2.5406e-06,  ..., -5.0813e-06,
         -2.6375e-06, -3.6284e-06],
        [-8.0466e-06, -5.8413e-06,  3.4049e-06,  ..., -6.8694e-06,
         -3.5465e-06, -4.9770e-06],
        [-1.0639e-05, -7.7337e-06,  4.5002e-06,  ..., -8.9109e-06,
         -4.7088e-06, -6.1691e-06]], device='cuda:0')
Loss: 1.0700504779815674


Running epoch 0, step 858, batch 858
Sampled inputs[:2]: tensor([[    0, 15152,  1106,  ...,   607,   266,  2529],
        [    0,  1854,   292,  ...,   328,  1360,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1823e-05,  2.2236e-04,  6.1595e-05,  ..., -1.5895e-04,
          4.0041e-04, -7.8230e-05],
        [-5.8338e-06, -4.3288e-06,  2.3693e-06,  ..., -5.1931e-06,
         -2.9542e-06, -3.8967e-06],
        [-8.7470e-06, -6.4671e-06,  3.8072e-06,  ..., -7.4655e-06,
         -3.8594e-06, -5.4091e-06],
        [-1.1712e-05, -8.6129e-06,  5.1111e-06,  ..., -1.0118e-05,
         -5.1782e-06, -7.4357e-06],
        [-1.5557e-05, -1.1429e-05,  6.8098e-06,  ..., -1.3173e-05,
         -6.9290e-06, -9.2387e-06]], device='cuda:0')
Loss: 1.0872806310653687


Running epoch 0, step 859, batch 859
Sampled inputs[:2]: tensor([[    0,   266,   452,  ...,  1725,  2200,   342],
        [    0, 29883,   680,  ...,  3363,  1049,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1743e-04,  3.2249e-04, -9.4147e-05,  ..., -7.5669e-05,
          2.4687e-04, -2.0260e-04],
        [-7.7263e-06, -5.7220e-06,  3.1441e-06,  ..., -6.9141e-06,
         -3.7439e-06, -5.0291e-06],
        [-1.1578e-05, -8.5682e-06,  5.0440e-06,  ..., -9.9689e-06,
         -4.9025e-06, -6.9961e-06],
        [-1.5706e-05, -1.1548e-05,  6.8620e-06,  ..., -1.3694e-05,
         -6.6757e-06, -9.7454e-06],
        [-2.0683e-05, -1.5229e-05,  9.0748e-06,  ..., -1.7703e-05,
         -8.8960e-06, -1.2025e-05]], device='cuda:0')
Loss: 1.084152340888977


Running epoch 0, step 860, batch 860
Sampled inputs[:2]: tensor([[    0,   287,   271,  ...,  1039,  4186,    13],
        [    0,    14, 49601,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0199e-04,  3.6086e-04,  3.5974e-07,  ..., -7.7386e-05,
          4.8236e-04, -7.8468e-05],
        [-9.8124e-06, -7.2867e-06,  4.0196e-06,  ..., -8.6725e-06,
         -4.5821e-06, -6.2063e-06],
        [-1.4678e-05, -1.0908e-05,  6.4000e-06,  ..., -1.2532e-05,
         -6.0499e-06, -8.6799e-06],
        [-1.9908e-05, -1.4693e-05,  8.7172e-06,  ..., -1.7211e-05,
         -8.2254e-06, -1.2085e-05],
        [-2.6345e-05, -1.9461e-05,  1.1548e-05,  ..., -2.2382e-05,
         -1.1027e-05, -1.5005e-05]], device='cuda:0')
Loss: 1.0969575643539429


Running epoch 0, step 861, batch 861
Sampled inputs[:2]: tensor([[   0, 8822, 1486,  ...,   12,  287, 6903],
        [   0,  271, 3403,  ..., 6168,  300, 2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0538e-04,  1.2742e-04,  9.3214e-05,  ..., -7.8672e-05,
          6.1117e-04,  2.5173e-05],
        [-1.1884e-05, -8.7768e-06,  4.8913e-06,  ..., -1.0423e-05,
         -5.5321e-06, -7.4878e-06],
        [-1.7777e-05, -1.3158e-05,  7.7635e-06,  ..., -1.5125e-05,
         -7.3984e-06, -1.0543e-05],
        [-2.3901e-05, -1.7583e-05,  1.0513e-05,  ..., -2.0608e-05,
         -9.9689e-06, -1.4529e-05],
        [-3.1948e-05, -2.3514e-05,  1.4022e-05,  ..., -2.7090e-05,
         -1.3530e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.0784436464309692


Running epoch 0, step 862, batch 862
Sampled inputs[:2]: tensor([[    0,   278,   490,  ...,   434,   472,   346],
        [    0,  7120,   344,  ...,  6273,    52, 22639]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8375e-04,  1.3879e-04,  9.9889e-05,  ..., -2.4105e-06,
          6.3831e-04, -1.3308e-07],
        [-1.3709e-05, -1.0096e-05,  5.6289e-06,  ..., -1.2182e-05,
         -6.4671e-06, -8.7023e-06],
        [-2.0459e-05, -1.5095e-05,  8.9556e-06,  ..., -1.7583e-05,
         -8.5980e-06, -1.2159e-05],
        [-2.7657e-05, -2.0280e-05,  1.2189e-05,  ..., -2.4110e-05,
         -1.1690e-05, -1.6868e-05],
        [-3.6985e-05, -2.7135e-05,  1.6272e-05,  ..., -3.1650e-05,
         -1.5810e-05, -2.1160e-05]], device='cuda:0')
Loss: 1.0945903062820435


Running epoch 0, step 863, batch 863
Sampled inputs[:2]: tensor([[    0, 20596,  2943,  ...,  5560,  2512,   266],
        [    0,  1371,   287,  ...,   689,   278, 12774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7336e-04,  6.1399e-04,  7.3295e-06,  ...,  8.0866e-05,
          1.9935e-04, -1.5029e-04],
        [-1.5765e-05, -1.1533e-05,  6.4746e-06,  ..., -1.3895e-05,
         -7.4506e-06, -9.9316e-06],
        [-2.3484e-05, -1.7196e-05,  1.0259e-05,  ..., -2.0027e-05,
         -9.8869e-06, -1.3866e-05],
        [-3.1859e-05, -2.3186e-05,  1.4007e-05,  ..., -2.7552e-05,
         -1.3500e-05, -1.9297e-05],
        [-4.2439e-05, -3.0935e-05,  1.8641e-05,  ..., -3.6061e-05,
         -1.8179e-05, -2.4155e-05]], device='cuda:0')
Loss: 1.0797001123428345
Graident accumulation at epoch 0, step 863, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.0688e-05,  3.8188e-04, -1.5101e-04,  ...,  2.1880e-04,
         -2.6046e-04,  4.3059e-05],
        [-1.5749e-05, -1.1597e-05,  6.7352e-06,  ..., -1.3774e-05,
         -7.5333e-06, -1.0038e-05],
        [ 7.7650e-07,  7.8452e-07, -6.0979e-07,  ...,  7.7861e-07,
          4.1944e-06, -2.3903e-06],
        [-1.8680e-05, -1.4185e-05,  6.9439e-06,  ..., -1.3376e-05,
         -8.0581e-06, -1.2790e-05],
        [-3.9160e-05, -2.9350e-05,  1.8037e-05,  ..., -3.3372e-05,
         -1.6156e-05, -2.3334e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9402e-08, 2.8267e-08, 2.1747e-08,  ..., 2.4253e-08, 3.8459e-08,
         9.0634e-09],
        [5.7747e-11, 3.1823e-11, 4.1678e-12,  ..., 4.1486e-11, 4.1884e-12,
         1.3130e-11],
        [2.2369e-09, 1.1089e-09, 2.4199e-10,  ..., 1.6910e-09, 1.8747e-10,
         6.6621e-10],
        [7.0658e-10, 5.0263e-10, 6.7213e-11,  ..., 5.8349e-10, 6.9672e-11,
         2.2396e-10],
        [3.0083e-10, 1.5811e-10, 2.6111e-11,  ..., 2.1484e-10, 3.5111e-11,
         7.2895e-11]], device='cuda:0')
optimizer state dict: 108.0
lr: [1.5736398113547236e-06, 1.5736398113547236e-06]
scheduler_last_epoch: 108


Running epoch 0, step 864, batch 864
Sampled inputs[:2]: tensor([[    0, 11541,  4784,  ...,  2837, 38541,    12],
        [    0,   380,  1075,  ..., 16948,   266,  1751]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1895e-05,  5.2586e-05, -7.3195e-05,  ...,  1.8180e-04,
         -1.9954e-04, -1.1624e-05],
        [-1.9222e-06, -1.4827e-06,  8.0466e-07,  ..., -1.8254e-06,
         -9.8348e-07, -1.1921e-06],
        [-2.7567e-06, -2.1607e-06,  1.2293e-06,  ..., -2.5481e-06,
         -1.2517e-06, -1.6168e-06],
        [-3.7849e-06, -2.9355e-06,  1.6913e-06,  ..., -3.5316e-06,
         -1.7509e-06, -2.2650e-06],
        [-5.0664e-06, -3.9339e-06,  2.2650e-06,  ..., -4.6492e-06,
         -2.3693e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1068768501281738


Running epoch 0, step 865, batch 865
Sampled inputs[:2]: tensor([[    0,   278,  4575,  ...,  1220,   278,  4575],
        [    0, 10288,   300,  ...,  5365,    12,  3539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5152e-04,  4.5197e-04, -9.4810e-05,  ...,  5.8092e-04,
         -5.0867e-04, -1.6345e-04],
        [-3.8221e-06, -2.9504e-06,  1.5832e-06,  ..., -3.5167e-06,
         -1.7621e-06, -2.4736e-06],
        [-5.5432e-06, -4.3064e-06,  2.4512e-06,  ..., -4.9621e-06,
         -2.2575e-06, -3.3751e-06],
        [-7.7784e-06, -5.9903e-06,  3.4645e-06,  ..., -7.0333e-06,
         -3.2187e-06, -4.8876e-06],
        [-1.0133e-05, -7.8082e-06,  4.5002e-06,  ..., -9.0003e-06,
         -4.2543e-06, -5.9903e-06]], device='cuda:0')
Loss: 1.0750107765197754


Running epoch 0, step 866, batch 866
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0,   380,   341,  ...,   955,   644,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7955e-04,  5.6846e-04, -5.3657e-05,  ...,  8.9638e-04,
         -7.4874e-04, -2.3725e-04],
        [-5.8338e-06, -4.4256e-06,  2.4959e-06,  ..., -5.2825e-06,
         -2.7232e-06, -3.7923e-06],
        [ 7.3612e-05,  4.2910e-05, -1.1056e-05,  ...,  7.0375e-05,
          1.0120e-05,  4.6642e-05],
        [-1.1891e-05, -8.9854e-06,  5.3868e-06,  ..., -1.0610e-05,
         -5.0440e-06, -7.5400e-06],
        [-1.5587e-05, -1.1802e-05,  7.0333e-06,  ..., -1.3709e-05,
         -6.6981e-06, -9.3281e-06]], device='cuda:0')
Loss: 1.0914260149002075


Running epoch 0, step 867, batch 867
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 19245,   565,    14],
        [    0, 16028,   669,  ...,   292,  6502,  7050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2154e-04,  2.7676e-04,  5.6834e-05,  ...,  1.0229e-03,
         -6.1986e-04, -2.0538e-04],
        [-7.7561e-06, -5.7817e-06,  3.3006e-06,  ..., -7.1004e-06,
         -4.0121e-06, -5.2825e-06],
        [ 7.0781e-05,  4.0884e-05, -9.8038e-06,  ...,  6.7812e-05,
          8.4286e-06,  4.4616e-05],
        [-1.5631e-05, -1.1608e-05,  7.0706e-06,  ..., -1.4037e-05,
         -7.3090e-06, -1.0282e-05],
        [-2.0534e-05, -1.5333e-05,  9.2238e-06,  ..., -1.8150e-05,
         -9.6485e-06, -1.2711e-05]], device='cuda:0')
Loss: 1.0577377080917358


Running epoch 0, step 868, batch 868
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  365, 2463,  391],
        [   0,  894,   16,  ...,  892,  300,  722]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6610e-05, -5.9865e-05,  5.6834e-05,  ...,  8.8742e-04,
         -1.5080e-04, -1.2450e-05],
        [-9.6783e-06, -7.2271e-06,  4.0792e-06,  ..., -8.8513e-06,
         -5.1521e-06, -6.6981e-06],
        [ 6.7994e-05,  3.8798e-05, -8.5745e-06,  ...,  6.5398e-05,
          6.9906e-06,  4.2746e-05],
        [-1.9401e-05, -1.4395e-05,  8.7470e-06,  ..., -1.7330e-05,
         -9.2760e-06, -1.2860e-05],
        [-2.5570e-05, -1.9073e-05,  1.1474e-05,  ..., -2.2471e-05,
         -1.2271e-05, -1.5914e-05]], device='cuda:0')
Loss: 1.0686490535736084


Running epoch 0, step 869, batch 869
Sampled inputs[:2]: tensor([[    0,    14,  5551,  ...,   668, 11988,  2538],
        [    0,   266, 27347,  ...,   368,  3367,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3680e-04, -1.7113e-04, -2.6316e-05,  ...,  7.8990e-04,
          7.5479e-05, -7.8691e-05],
        [ 1.3801e-05,  5.3410e-05, -1.0061e-05,  ...,  6.8396e-05,
          6.1302e-05,  6.2312e-05],
        [ 6.5327e-05,  3.6831e-05, -7.3898e-06,  ...,  6.2999e-05,
          5.8507e-06,  4.1092e-05],
        [-2.3127e-05, -1.7151e-05,  1.0423e-05,  ..., -2.0772e-05,
         -1.0893e-05, -1.5274e-05],
        [-3.0369e-05, -2.2605e-05,  1.3635e-05,  ..., -2.6762e-05,
         -1.4357e-05, -1.8761e-05]], device='cuda:0')
Loss: 1.0778794288635254


Running epoch 0, step 870, batch 870
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,    12,   850,  4952],
        [    0,    83,   292,  ...,   445,    11, 16109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0453e-04, -2.3760e-04, -9.1808e-05,  ...,  8.3517e-04,
          5.2939e-05, -1.3616e-04],
        [ 1.1864e-05,  5.1964e-05, -9.2270e-06,  ...,  6.6720e-05,
          6.0564e-05,  6.1082e-05],
        [ 6.2466e-05,  3.4685e-05, -6.0711e-06,  ...,  6.0585e-05,
          4.8746e-06,  3.9385e-05],
        [-2.7001e-05, -2.0042e-05,  1.2219e-05,  ..., -2.4065e-05,
         -1.2212e-05, -1.7643e-05],
        [-3.5584e-05, -2.6509e-05,  1.6063e-05,  ..., -3.1143e-05,
         -1.6205e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.063523292541504


Running epoch 0, step 871, batch 871
Sampled inputs[:2]: tensor([[    0,   874,   445,  ...,    14, 16205,  8510],
        [    0,  1619,   938,  ...,   292, 10026, 14367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2757e-04, -1.1365e-04, -2.1183e-04,  ...,  9.1523e-04,
         -1.6986e-04, -6.9276e-05],
        [ 9.8521e-06,  5.0578e-05, -8.4037e-06,  ...,  6.4887e-05,
          5.9432e-05,  5.9786e-05],
        [ 5.9515e-05,  3.2644e-05, -4.7747e-06,  ...,  5.7992e-05,
          3.3771e-06,  3.7605e-05],
        [-3.0935e-05, -2.2739e-05,  1.3955e-05,  ..., -2.7582e-05,
         -1.4223e-05, -2.0102e-05],
        [-4.0829e-05, -3.0145e-05,  1.8388e-05,  ..., -3.5733e-05,
         -1.8917e-05, -2.4825e-05]], device='cuda:0')
Loss: 1.0742977857589722
Graident accumulation at epoch 0, step 871, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0438e-04,  3.3233e-04, -1.5709e-04,  ...,  2.8844e-04,
         -2.5140e-04,  3.1825e-05],
        [-1.3189e-05, -5.3794e-06,  5.2213e-06,  ..., -5.9079e-06,
         -8.3681e-07, -3.0557e-06],
        [ 6.6504e-06,  3.9704e-06, -1.0263e-06,  ...,  6.5000e-06,
          4.1127e-06,  1.6092e-06],
        [-1.9905e-05, -1.5040e-05,  7.6450e-06,  ..., -1.4796e-05,
         -8.6746e-06, -1.3521e-05],
        [-3.9327e-05, -2.9430e-05,  1.8072e-05,  ..., -3.3608e-05,
         -1.6433e-05, -2.3483e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9404e-08, 2.8251e-08, 2.1770e-08,  ..., 2.5067e-08, 3.8449e-08,
         9.0591e-09],
        [5.7786e-11, 3.4349e-11, 4.2342e-12,  ..., 4.5655e-11, 7.7164e-12,
         1.6691e-11],
        [2.2382e-09, 1.1088e-09, 2.4177e-10,  ..., 1.6927e-09, 1.8729e-10,
         6.6695e-10],
        [7.0683e-10, 5.0264e-10, 6.7341e-11,  ..., 5.8367e-10, 6.9805e-11,
         2.2414e-10],
        [3.0219e-10, 1.5886e-10, 2.6423e-11,  ..., 2.1590e-10, 3.5434e-11,
         7.3438e-11]], device='cuda:0')
optimizer state dict: 109.0
lr: [1.4430974891391325e-06, 1.4430974891391325e-06]
scheduler_last_epoch: 109


Running epoch 0, step 872, batch 872
Sampled inputs[:2]: tensor([[    0,  9792,  3239,  ...,   699,  3636,  1761],
        [    0,     5,  4413,  ...,  9205, 16744,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2730e-04, -3.0558e-04,  7.4642e-05,  ..., -2.1199e-04,
          2.6401e-04,  2.6397e-04],
        [-1.9521e-06, -1.4082e-06,  7.6741e-07,  ..., -1.7956e-06,
         -1.1250e-06, -1.3560e-06],
        [-2.8610e-06, -2.0564e-06,  1.2219e-06,  ..., -2.4885e-06,
         -1.4231e-06, -1.8179e-06],
        [-3.8445e-06, -2.7567e-06,  1.6466e-06,  ..., -3.4124e-06,
         -1.9521e-06, -2.5332e-06],
        [-5.1260e-06, -3.6806e-06,  2.2203e-06,  ..., -4.4107e-06,
         -2.5481e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0846415758132935


Running epoch 0, step 873, batch 873
Sampled inputs[:2]: tensor([[    0,   508,  1548,  ...,   494, 10792,     9],
        [    0,  4845,  1521,  ...,   963,   292,  6414]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6411e-05, -4.3961e-04,  1.3550e-05,  ..., -3.5448e-04,
          4.3876e-04,  3.1313e-04],
        [-3.5763e-06, -2.6673e-06,  1.3448e-06,  ..., -3.4645e-06,
         -2.0713e-06, -2.6152e-06],
        [-5.4091e-06, -3.9786e-06,  2.2873e-06,  ..., -4.8280e-06,
         -2.5630e-06, -3.4794e-06],
        [-7.3463e-06, -5.3793e-06,  3.1069e-06,  ..., -6.6906e-06,
         -3.5614e-06, -4.9323e-06],
        [-9.6858e-06, -7.0781e-06,  4.1574e-06,  ..., -8.4937e-06,
         -4.5747e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.0531563758850098


Running epoch 0, step 874, batch 874
Sampled inputs[:2]: tensor([[    0,  2352,  4275,  ..., 10518,   342,   266],
        [    0,   935,   508,  ...,   287, 41582,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5647e-04,  4.1328e-05, -2.3593e-04,  ..., -9.4963e-05,
         -1.8882e-04,  1.0930e-04],
        [-5.4091e-06, -4.0308e-06,  2.1048e-06,  ..., -5.2378e-06,
         -3.2410e-06, -4.0084e-06],
        [-8.0317e-06, -5.9158e-06,  3.4720e-06,  ..., -7.1973e-06,
         -3.9861e-06, -5.2378e-06],
        [-1.1042e-05, -8.0913e-06,  4.7907e-06,  ..., -1.0133e-05,
         -5.6475e-06, -7.5698e-06],
        [-1.4335e-05, -1.0505e-05,  6.2734e-06,  ..., -1.2636e-05,
         -7.0930e-06, -8.8662e-06]], device='cuda:0')
Loss: 1.0729936361312866


Running epoch 0, step 875, batch 875
Sampled inputs[:2]: tensor([[   0,  280, 5656,  ..., 7369, 2276,   12],
        [   0, 6010,  829,  ...,  668, 1784,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6997e-04,  1.0722e-04, -2.9559e-04,  ..., -9.7501e-05,
         -1.8706e-04,  1.1547e-04],
        [-7.5251e-06, -5.5060e-06,  2.9653e-06,  ..., -6.9961e-06,
         -4.1947e-06, -5.3123e-06],
        [-1.1101e-05, -8.0764e-06,  4.7907e-06,  ..., -9.7156e-06,
         -5.2601e-06, -7.0482e-06],
        [-1.5214e-05, -1.1027e-05,  6.5863e-06,  ..., -1.3590e-05,
         -7.3835e-06, -1.0088e-05],
        [-1.9848e-05, -1.4409e-05,  8.6427e-06,  ..., -1.7166e-05,
         -9.4324e-06, -1.2040e-05]], device='cuda:0')
Loss: 1.0939586162567139


Running epoch 0, step 876, batch 876
Sampled inputs[:2]: tensor([[    0,  1125,   278,  ...,  6447,   609,    14],
        [    0,   680,   993,  ...,   699, 11426,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1245e-04, -1.2902e-04, -1.0139e-04,  ..., -1.7942e-04,
         -8.4553e-05,  2.5294e-04],
        [-9.5069e-06, -6.9737e-06,  3.8259e-06,  ..., -8.7321e-06,
         -5.0738e-06, -6.6757e-06],
        [-1.4126e-05, -1.0327e-05,  6.1691e-06,  ..., -1.2308e-05,
         -6.4895e-06, -9.0450e-06],
        [-1.9357e-05, -1.4067e-05,  8.4788e-06,  ..., -1.7166e-05,
         -9.0599e-06, -1.2904e-05],
        [-2.5183e-05, -1.8343e-05,  1.1057e-05,  ..., -2.1726e-05,
         -1.1683e-05, -1.5453e-05]], device='cuda:0')
Loss: 1.07662832736969


Running epoch 0, step 877, batch 877
Sampled inputs[:2]: tensor([[   0,   12,  271,  ...,   12,  298,  273],
        [   0,  259, 2561,  ...,   77, 4830,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8663e-04,  1.6247e-04, -1.9252e-04,  ..., -1.7942e-04,
         -1.9302e-04,  1.6894e-05],
        [-1.1429e-05, -8.4192e-06,  4.5635e-06,  ..., -1.0535e-05,
         -6.1840e-06, -8.1137e-06],
        [-1.6928e-05, -1.2428e-05,  7.3239e-06,  ..., -1.4797e-05,
         -7.9051e-06, -1.0967e-05],
        [-2.3067e-05, -1.6838e-05,  1.0021e-05,  ..., -2.0534e-05,
         -1.0952e-05, -1.5527e-05],
        [-3.0130e-05, -2.2069e-05,  1.3128e-05,  ..., -2.6107e-05,
         -1.4216e-05, -1.8701e-05]], device='cuda:0')
Loss: 1.0576835870742798


Running epoch 0, step 878, batch 878
Sampled inputs[:2]: tensor([[    0,   278, 14971,  ...,  2341,   266,   717],
        [    0,   452,    13,  ...,   358,    13, 12347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3689e-04,  1.6247e-04, -1.9849e-04,  ..., -2.3865e-04,
         -1.9238e-05, -7.6471e-05],
        [-1.3240e-05, -9.6932e-06,  5.2638e-06,  ..., -1.2286e-05,
         -7.3165e-06, -9.5218e-06],
        [-1.9610e-05, -1.4320e-05,  8.4862e-06,  ..., -1.7196e-05,
         -9.3281e-06, -1.2822e-05],
        [-2.6733e-05, -1.9401e-05,  1.1615e-05,  ..., -2.3901e-05,
         -1.2949e-05, -1.8194e-05],
        [-3.5107e-05, -2.5570e-05,  1.5303e-05,  ..., -3.0518e-05,
         -1.6883e-05, -2.1994e-05]], device='cuda:0')
Loss: 1.0519152879714966


Running epoch 0, step 879, batch 879
Sampled inputs[:2]: tensor([[    0, 21448,   344,  ...,   365,  1501,   271],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5823e-04,  1.2800e-04, -2.5687e-04,  ..., -2.4202e-04,
         -7.1662e-05, -7.2235e-05],
        [-1.5266e-05, -1.1139e-05,  6.1542e-06,  ..., -1.3985e-05,
         -8.1360e-06, -1.0751e-05],
        [ 4.1875e-04,  2.2081e-04, -1.8366e-04,  ...,  3.9761e-04,
          2.1237e-04,  2.2544e-04],
        [-3.0905e-05, -2.2382e-05,  1.3523e-05,  ..., -2.7388e-05,
         -1.4499e-05, -2.0698e-05],
        [-4.0501e-05, -2.9474e-05,  1.7762e-05,  ..., -3.5018e-05,
         -1.8984e-05, -2.5064e-05]], device='cuda:0')
Loss: 1.0854865312576294
Graident accumulation at epoch 0, step 879, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4976e-04,  3.1190e-04, -1.6707e-04,  ...,  2.3540e-04,
         -2.3342e-04,  2.1419e-05],
        [-1.3397e-05, -5.9554e-06,  5.3146e-06,  ..., -6.7156e-06,
         -1.5667e-06, -3.8252e-06],
        [ 4.7861e-05,  2.5654e-05, -1.9289e-05,  ...,  4.5611e-05,
          2.4938e-05,  2.3992e-05],
        [-2.1005e-05, -1.5774e-05,  8.2328e-06,  ..., -1.6055e-05,
         -9.2570e-06, -1.4239e-05],
        [-3.9444e-05, -2.9434e-05,  1.8041e-05,  ..., -3.3749e-05,
         -1.6688e-05, -2.3641e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9666e-08, 2.8239e-08, 2.1814e-08,  ..., 2.5100e-08, 3.8416e-08,
         9.0553e-09],
        [5.7962e-11, 3.4439e-11, 4.2679e-12,  ..., 4.5805e-11, 7.7749e-12,
         1.6790e-11],
        [2.4113e-09, 1.1565e-09, 2.7526e-10,  ..., 1.8491e-09, 2.3220e-10,
         7.1711e-10],
        [7.0708e-10, 5.0264e-10, 6.7456e-11,  ..., 5.8383e-10, 6.9945e-11,
         2.2434e-10],
        [3.0353e-10, 1.5957e-10, 2.6712e-11,  ..., 2.1691e-10, 3.5759e-11,
         7.3993e-11]], device='cuda:0')
optimizer state dict: 110.0
lr: [1.3177852447071903e-06, 1.3177852447071903e-06]
scheduler_last_epoch: 110


Running epoch 0, step 880, batch 880
Sampled inputs[:2]: tensor([[    0,  1029,  6068,  ..., 18017,   300,   259],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0552e-05, -8.6907e-05, -1.0409e-06,  ..., -2.9300e-05,
         -1.3289e-04,  1.0615e-04],
        [-2.0117e-06, -1.4529e-06,  7.8604e-07,  ..., -1.7658e-06,
         -9.1270e-07, -1.3411e-06],
        [-2.9206e-06, -2.1458e-06,  1.2219e-06,  ..., -2.4885e-06,
         -1.1846e-06, -1.8105e-06],
        [-4.0531e-06, -2.9355e-06,  1.6987e-06,  ..., -3.5018e-06,
         -1.6838e-06, -2.6077e-06],
        [-5.2154e-06, -3.8147e-06,  2.1905e-06,  ..., -4.4405e-06,
         -2.1607e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0786406993865967


Running epoch 0, step 881, batch 881
Sampled inputs[:2]: tensor([[    0,   292,   494,  ...,   259, 14134, 11544],
        [    0,   298,   894,  ...,  7605,  3220,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2244e-04, -1.1893e-04,  8.2862e-05,  ..., -1.7669e-04,
          6.7067e-05,  3.2617e-04],
        [-4.0382e-06, -2.8536e-06,  1.5758e-06,  ..., -3.6359e-06,
         -2.2165e-06, -2.8759e-06],
        [-5.7966e-06, -4.1574e-06,  2.4214e-06,  ..., -5.0366e-06,
         -2.8610e-06, -3.8370e-06],
        [-7.9274e-06, -5.6177e-06,  3.3230e-06,  ..., -6.9886e-06,
         -3.9637e-06, -5.3942e-06],
        [-1.0431e-05, -7.4953e-06,  4.3809e-06,  ..., -9.0599e-06,
         -5.2452e-06, -6.6161e-06]], device='cuda:0')
Loss: 1.0792680978775024


Running epoch 0, step 882, batch 882
Sampled inputs[:2]: tensor([[    0,   609,    12,  ...,   409, 11041,   292],
        [    0,  1420,  6319,  ...,   292,  4895,  4050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5992e-04, -2.5315e-04,  1.0856e-04,  ..., -4.2268e-04,
          4.2270e-04,  5.8220e-04],
        [-5.8785e-06, -4.1425e-06,  2.1979e-06,  ..., -5.3793e-06,
         -3.2969e-06, -4.1947e-06],
        [-8.5235e-06, -6.0573e-06,  3.4720e-06,  ..., -7.4655e-06,
         -4.2245e-06, -5.6103e-06],
        [-1.1697e-05, -8.2254e-06,  4.7684e-06,  ..., -1.0386e-05,
         -5.9009e-06, -7.9274e-06],
        [-1.5467e-05, -1.0997e-05,  6.3479e-06,  ..., -1.3471e-05,
         -7.7486e-06, -9.7454e-06]], device='cuda:0')
Loss: 1.0790261030197144


Running epoch 0, step 883, batch 883
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,   278,   269, 10376],
        [    0, 13706,  1862,  ...,   275,  1036, 42948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8987e-04, -3.4620e-04,  9.4012e-05,  ..., -5.6978e-04,
          4.4144e-04,  4.4217e-04],
        [-7.8008e-06, -5.5954e-06,  3.1218e-06,  ..., -7.0557e-06,
         -4.1537e-06, -5.5209e-06],
        [-1.1459e-05, -8.2776e-06,  4.9397e-06,  ..., -9.9689e-06,
         -5.4389e-06, -7.5325e-06],
        [-1.5691e-05, -1.1235e-05,  6.7949e-06,  ..., -1.3858e-05,
         -7.5623e-06, -1.0625e-05],
        [-2.0653e-05, -1.4901e-05,  8.9407e-06,  ..., -1.7881e-05,
         -9.9242e-06, -1.3024e-05]], device='cuda:0')
Loss: 1.073738694190979


Running epoch 0, step 884, batch 884
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   680,   401,  1407],
        [    0,    20,     9,  ...,    12,  2212, 24950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7550e-04, -2.9809e-04,  2.3975e-04,  ..., -5.5422e-04,
          4.5670e-04,  4.9770e-04],
        [-9.7230e-06, -6.9737e-06,  3.8706e-06,  ..., -8.7842e-06,
         -5.1372e-06, -6.7949e-06],
        [-1.4424e-05, -1.0394e-05,  6.2063e-06,  ..., -1.2502e-05,
         -6.7577e-06, -9.3430e-06],
        [-1.9565e-05, -1.3992e-05,  8.4490e-06,  ..., -1.7226e-05,
         -9.2983e-06, -1.3083e-05],
        [-2.5868e-05, -1.8626e-05,  1.1191e-05,  ..., -2.2292e-05,
         -1.2279e-05, -1.6093e-05]], device='cuda:0')
Loss: 1.0752977132797241


Running epoch 0, step 885, batch 885
Sampled inputs[:2]: tensor([[    0,   292,    40,  ..., 26995,   278,   717],
        [    0,  2958,   298,  ...,    12,   709,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2341e-03, -1.2592e-03,  5.0406e-04,  ..., -1.6343e-03,
          2.2546e-03,  1.5085e-03],
        [-1.1206e-05, -7.9870e-06,  4.4778e-06,  ..., -1.0610e-05,
         -6.5081e-06, -8.5607e-06],
        [-1.6645e-05, -1.1921e-05,  7.2718e-06,  ..., -1.4916e-05,
         -8.3745e-06, -1.1548e-05],
        [-2.2709e-05, -1.6093e-05,  9.9689e-06,  ..., -2.0728e-05,
         -1.1668e-05, -1.6361e-05],
        [-2.9862e-05, -2.1383e-05,  1.3128e-05,  ..., -2.6524e-05,
         -1.5140e-05, -1.9759e-05]], device='cuda:0')
Loss: 1.0669578313827515


Running epoch 0, step 886, batch 886
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 8122, 1387,  616],
        [   0, 1943, 1837,  ...,  870,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1856e-03, -1.3053e-03,  3.7959e-04,  ..., -1.6343e-03,
          2.2167e-03,  1.6128e-03],
        [-1.3202e-05, -9.4399e-06,  5.3197e-06,  ..., -1.2293e-05,
         -7.3127e-06, -9.7826e-06],
        [-1.9595e-05, -1.4096e-05,  8.5831e-06,  ..., -1.7390e-05,
         -9.4771e-06, -1.3284e-05],
        [-2.6852e-05, -1.9103e-05,  1.1809e-05,  ..., -2.4214e-05,
         -1.3202e-05, -1.8850e-05],
        [-3.5167e-05, -2.5257e-05,  1.5467e-05,  ..., -3.0935e-05,
         -1.7196e-05, -2.2784e-05]], device='cuda:0')
Loss: 1.0956147909164429


Running epoch 0, step 887, batch 887
Sampled inputs[:2]: tensor([[    0,  1853,  3373,  ...,  3020,  6695,   300],
        [    0,  2670, 31283,  ...,    18,  9106,  1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0150e-03, -1.2191e-03,  4.6224e-04,  ..., -1.6352e-03,
          2.1666e-03,  1.6160e-03],
        [-1.5244e-05, -1.0923e-05,  6.1467e-06,  ..., -1.4022e-05,
         -8.3111e-06, -1.1116e-05],
        [-2.2680e-05, -1.6332e-05,  9.9093e-06,  ..., -1.9923e-05,
         -1.0841e-05, -1.5162e-05],
        [-3.0994e-05, -2.2098e-05,  1.3605e-05,  ..., -2.7671e-05,
         -1.5065e-05, -2.1458e-05],
        [-4.0591e-05, -2.9191e-05,  1.7807e-05,  ..., -3.5375e-05,
         -1.9625e-05, -2.5988e-05]], device='cuda:0')
Loss: 1.1059889793395996
Graident accumulation at epoch 0, step 887, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3286e-05,  1.5879e-04, -1.0414e-04,  ...,  4.8336e-05,
          6.5767e-06,  1.8087e-04],
        [-1.3581e-05, -6.4521e-06,  5.3978e-06,  ..., -7.4462e-06,
         -2.2412e-06, -4.5543e-06],
        [ 4.0807e-05,  2.1455e-05, -1.6369e-05,  ...,  3.9058e-05,
          2.1360e-05,  2.0077e-05],
        [-2.2004e-05, -1.6407e-05,  8.7700e-06,  ..., -1.7217e-05,
         -9.8378e-06, -1.4960e-05],
        [-3.9559e-05, -2.9410e-05,  1.8018e-05,  ..., -3.3912e-05,
         -1.6981e-05, -2.3876e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0647e-08, 2.9697e-08, 2.2006e-08,  ..., 2.7749e-08, 4.3071e-08,
         1.1658e-08],
        [5.8136e-11, 3.4524e-11, 4.3014e-12,  ..., 4.5956e-11, 7.8362e-12,
         1.6897e-11],
        [2.4094e-09, 1.1556e-09, 2.7508e-10,  ..., 1.8477e-09, 2.3209e-10,
         7.1662e-10],
        [7.0734e-10, 5.0263e-10, 6.7574e-11,  ..., 5.8402e-10, 7.0102e-11,
         2.2458e-10],
        [3.0488e-10, 1.6027e-10, 2.7003e-11,  ..., 2.1794e-10, 3.6108e-11,
         7.4594e-11]], device='cuda:0')
optimizer state dict: 111.0
lr: [1.1977796703520529e-06, 1.1977796703520529e-06]
scheduler_last_epoch: 111


Running epoch 0, step 888, batch 888
Sampled inputs[:2]: tensor([[    0,   292,   221,  ...,   796, 12886,   694],
        [    0,   266, 15324,  ...,   943,  1613,  7178]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1823e-04, -4.8267e-04,  2.3736e-04,  ..., -2.6042e-04,
          4.6543e-04,  2.9757e-04],
        [-2.0266e-06, -1.4007e-06,  7.6741e-07,  ..., -1.7956e-06,
         -1.1623e-06, -1.5572e-06],
        [-2.9802e-06, -2.0862e-06,  1.2219e-06,  ..., -2.5332e-06,
         -1.5125e-06, -2.1160e-06],
        [-4.0233e-06, -2.8014e-06,  1.6615e-06,  ..., -3.4571e-06,
         -2.0415e-06, -2.9206e-06],
        [-5.3644e-06, -3.7700e-06,  2.2054e-06,  ..., -4.5598e-06,
         -2.7418e-06, -3.6359e-06]], device='cuda:0')
Loss: 1.0754272937774658


Running epoch 0, step 889, batch 889
Sampled inputs[:2]: tensor([[    0,  6668,   565,  ...,   360,   259,  8166],
        [    0,  4566,   300,  ...,   271,  1644, 16473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2333e-04, -1.6729e-04,  1.8943e-05,  ..., -2.6958e-05,
          1.4404e-04, -5.2895e-05],
        [-3.8669e-06, -2.6375e-06,  1.5460e-06,  ..., -3.5465e-06,
         -2.1383e-06, -2.8238e-06],
        [-5.7518e-06, -3.9786e-06,  2.4810e-06,  ..., -5.0664e-06,
         -2.7940e-06, -3.8743e-06],
        [-7.8678e-06, -5.3942e-06,  3.4049e-06,  ..., -7.0184e-06,
         -3.8445e-06, -5.4538e-06],
        [-1.0312e-05, -7.1377e-06,  4.4405e-06,  ..., -9.0301e-06,
         -5.0813e-06, -6.6459e-06]], device='cuda:0')
Loss: 1.090108036994934


Running epoch 0, step 890, batch 890
Sampled inputs[:2]: tensor([[   0, 2785, 1061,  ..., 1194,  692, 4339],
        [   0, 2615,   13,  ...,  940, 3661, 6837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5494e-04, -1.3067e-03,  2.7639e-05,  ..., -6.1364e-04,
         -5.1160e-05,  3.0402e-04],
        [-5.3570e-06, -3.7774e-06,  2.2911e-06,  ..., -5.2303e-06,
         -3.0100e-06, -4.3213e-06],
        [-8.1062e-06, -5.7518e-06,  3.7253e-06,  ..., -7.4953e-06,
         -3.9339e-06, -5.9605e-06],
        [-1.1235e-05, -7.9125e-06,  5.2229e-06,  ..., -1.0565e-05,
         -5.4985e-06, -8.5533e-06],
        [-1.4424e-05, -1.0222e-05,  6.6012e-06,  ..., -1.3173e-05,
         -7.0632e-06, -1.0043e-05]], device='cuda:0')
Loss: 1.0536390542984009


Running epoch 0, step 891, batch 891
Sampled inputs[:2]: tensor([[    0,  1855,    14,  ...,    12,   287, 16479],
        [    0,   300, 13523,  ..., 42438,   786,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3176e-04, -1.1523e-03, -1.2500e-04,  ..., -4.0720e-04,
         -2.7230e-04,  2.6247e-04],
        [-7.2494e-06, -5.1185e-06,  3.0696e-06,  ..., -6.9365e-06,
         -3.8408e-06, -5.4836e-06],
        [-1.0923e-05, -7.7635e-06,  4.9546e-06,  ..., -9.9391e-06,
         -5.0217e-06, -7.5698e-06],
        [-1.5199e-05, -1.0714e-05,  6.9663e-06,  ..., -1.4067e-05,
         -7.0557e-06, -1.0908e-05],
        [-1.9550e-05, -1.3873e-05,  8.8513e-06,  ..., -1.7583e-05,
         -9.0897e-06, -1.2845e-05]], device='cuda:0')
Loss: 1.0664063692092896


Running epoch 0, step 892, batch 892
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,    18,   998,  ...,  5322,   504,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5789e-04, -1.0577e-03, -2.0191e-05,  ..., -2.8850e-04,
         -4.1057e-04,  2.3067e-04],
        [-9.3058e-06, -6.6012e-06,  3.9339e-06,  ..., -8.7544e-06,
         -4.7497e-06, -6.6832e-06],
        [-1.3977e-05, -9.9838e-06,  6.2957e-06,  ..., -1.2606e-05,
         -6.2734e-06, -9.2834e-06],
        [-1.9372e-05, -1.3724e-05,  8.7991e-06,  ..., -1.7732e-05,
         -8.7619e-06, -1.3307e-05],
        [-2.5064e-05, -1.7896e-05,  1.1280e-05,  ..., -2.2382e-05,
         -1.1399e-05, -1.5840e-05]], device='cuda:0')
Loss: 1.1029696464538574


Running epoch 0, step 893, batch 893
Sampled inputs[:2]: tensor([[   0,  275,  467,  ...,  298,  365, 2714],
        [   0,  421, 6007,  ...,  408, 2105,  843]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5177e-04, -1.0481e-03,  5.9618e-05,  ..., -4.8304e-04,
         -2.6456e-04,  2.6073e-04],
        [-1.1273e-05, -8.0243e-06,  4.7274e-06,  ..., -1.0498e-05,
         -5.5917e-06, -7.9051e-06],
        [-1.6958e-05, -1.2159e-05,  7.5698e-06,  ..., -1.5184e-05,
         -7.4208e-06, -1.1042e-05],
        [-2.3454e-05, -1.6659e-05,  1.0550e-05,  ..., -2.1294e-05,
         -1.0327e-05, -1.5795e-05],
        [-3.0309e-05, -2.1711e-05,  1.3530e-05,  ..., -2.6911e-05,
         -1.3486e-05, -1.8850e-05]], device='cuda:0')
Loss: 1.077298641204834


Running epoch 0, step 894, batch 894
Sampled inputs[:2]: tensor([[    0,  2165,  1323,  ...,   199,   677,  8376],
        [    0,   271, 36770,  ...,   278,  1398,  4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2002e-03, -1.2017e-03, -2.8835e-05,  ..., -4.4550e-04,
         -3.6834e-04,  3.3979e-04],
        [-1.3299e-05, -9.4548e-06,  5.5768e-06,  ..., -1.2204e-05,
         -6.5751e-06, -9.1866e-06],
        [-2.0102e-05, -1.4409e-05,  8.9481e-06,  ..., -1.7807e-05,
         -8.8364e-06, -1.2949e-05],
        [-2.7567e-05, -1.9580e-05,  1.2368e-05,  ..., -2.4736e-05,
         -1.2167e-05, -1.8358e-05],
        [-3.5912e-05, -2.5705e-05,  1.5989e-05,  ..., -3.1590e-05,
         -1.6078e-05, -2.2143e-05]], device='cuda:0')
Loss: 1.0858705043792725


Running epoch 0, step 895, batch 895
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1454e-03, -1.2442e-03, -4.0075e-05,  ..., -3.8021e-04,
         -3.6834e-04,  3.4339e-04],
        [-1.5326e-05, -1.0893e-05,  6.4000e-06,  ..., -1.3970e-05,
         -7.4320e-06, -1.0416e-05],
        [-2.3067e-05, -1.6525e-05,  1.0215e-05,  ..., -2.0370e-05,
         -1.0014e-05, -1.4685e-05],
        [ 5.8989e-05,  3.1756e-05, -1.0888e-05,  ...,  3.8718e-05,
          5.5415e-05,  1.8213e-05],
        [-4.1276e-05, -2.9519e-05,  1.8299e-05,  ..., -3.6210e-05,
         -1.8284e-05, -2.5183e-05]], device='cuda:0')
Loss: 1.0954408645629883
Graident accumulation at epoch 0, step 895, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.4584e-05,  1.8494e-05, -9.7732e-05,  ...,  5.4815e-06,
         -3.0915e-05,  1.9712e-04],
        [-1.3756e-05, -6.8961e-06,  5.4981e-06,  ..., -8.0986e-06,
         -2.7603e-06, -5.1405e-06],
        [ 3.4419e-05,  1.7657e-05, -1.3711e-05,  ...,  3.3115e-05,
          1.8223e-05,  1.6601e-05],
        [-1.3905e-05, -1.1591e-05,  6.8042e-06,  ..., -1.1624e-05,
         -3.3125e-06, -1.1643e-05],
        [-3.9731e-05, -2.9421e-05,  1.8046e-05,  ..., -3.4141e-05,
         -1.7112e-05, -2.4007e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1908e-08, 3.1216e-08, 2.1986e-08,  ..., 2.7866e-08, 4.3164e-08,
         1.1764e-08],
        [5.8313e-11, 3.4608e-11, 4.3381e-12,  ..., 4.6105e-11, 7.8836e-12,
         1.6989e-11],
        [2.4075e-09, 1.1547e-09, 2.7491e-10,  ..., 1.8462e-09, 2.3195e-10,
         7.1612e-10],
        [7.1011e-10, 5.0313e-10, 6.7625e-11,  ..., 5.8493e-10, 7.3103e-11,
         2.2468e-10],
        [3.0628e-10, 1.6098e-10, 2.7310e-11,  ..., 2.1904e-10, 3.6406e-11,
         7.5154e-11]], device='cuda:0')
optimizer state dict: 112.0
lr: [1.083154114868752e-06, 1.083154114868752e-06]
scheduler_last_epoch: 112


Running epoch 0, step 896, batch 896
Sampled inputs[:2]: tensor([[   0,  368,  729,  ...,  221,  380, 2830],
        [   0,   12,  401,  ...,  504,  565,  590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0520e-04,  8.9606e-05, -1.3779e-04,  ...,  1.4146e-04,
         -2.1882e-04,  3.5462e-06],
        [-1.9968e-06, -1.4305e-06,  7.8604e-07,  ..., -1.7434e-06,
         -8.0839e-07, -1.1995e-06],
        [-2.9802e-06, -2.1607e-06,  1.2517e-06,  ..., -2.5481e-06,
         -1.1027e-06, -1.6987e-06],
        [-4.0829e-06, -2.9355e-06,  1.7211e-06,  ..., -3.5316e-06,
         -1.5050e-06, -2.3991e-06],
        [-5.2452e-06, -3.7998e-06,  2.2054e-06,  ..., -4.4703e-06,
         -1.9968e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.0766907930374146


Running epoch 0, step 897, batch 897
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,   287,  5724,  ...,   298,   591,  2609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0165e-04,  1.4970e-04, -2.3591e-04,  ...,  2.0210e-04,
         -2.9137e-04,  7.7157e-05],
        [-4.1276e-06, -2.9206e-06,  1.6876e-06,  ..., -3.5688e-06,
         -1.7770e-06, -2.4140e-06],
        [-6.1095e-06, -4.3660e-06,  2.6226e-06,  ..., -5.1707e-06,
         -2.4214e-06, -3.3975e-06],
        [-8.3148e-06, -5.9009e-06,  3.5837e-06,  ..., -7.1228e-06,
         -3.3081e-06, -4.7535e-06],
        [-1.0848e-05, -7.7635e-06,  4.6641e-06,  ..., -9.1791e-06,
         -4.3958e-06, -5.8264e-06]], device='cuda:0')
Loss: 1.0959619283676147


Running epoch 0, step 898, batch 898
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,  2346, 17886,  ...,   287,  6769,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5323e-04,  2.4607e-04, -3.1041e-04,  ...,  3.9558e-04,
         -5.1268e-04,  8.0487e-05],
        [-6.1393e-06, -4.3586e-06,  2.4848e-06,  ..., -5.3123e-06,
         -2.6338e-06, -3.5241e-06],
        [-9.0450e-06, -6.4820e-06,  3.8445e-06,  ..., -7.6741e-06,
         -3.5688e-06, -4.9472e-06],
        [-1.2487e-05, -8.8811e-06,  5.3272e-06,  ..., -1.0699e-05,
         -4.9323e-06, -7.0184e-06],
        [-1.6183e-05, -1.1608e-05,  6.8992e-06,  ..., -1.3709e-05,
         -6.5416e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.0871264934539795


Running epoch 0, step 899, batch 899
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,  3988,  1049,   935],
        [    0,    19,     9,  ..., 11504,   446,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8281e-04,  3.0888e-04, -4.2780e-04,  ...,  3.9558e-04,
         -5.6873e-04,  2.6836e-05],
        [-8.0615e-06, -5.8338e-06,  3.2969e-06,  ..., -7.0110e-06,
         -3.5278e-06, -4.8727e-06],
        [-1.1951e-05, -8.7172e-06,  5.1409e-06,  ..., -1.0163e-05,
         -4.7982e-06, -6.8545e-06],
        [-1.6391e-05, -1.1846e-05,  7.0781e-06,  ..., -1.4082e-05,
         -6.5789e-06, -9.6709e-06],
        [-2.1189e-05, -1.5453e-05,  9.1344e-06,  ..., -1.8001e-05,
         -8.7023e-06, -1.1727e-05]], device='cuda:0')
Loss: 1.0609874725341797


Running epoch 0, step 900, batch 900
Sampled inputs[:2]: tensor([[    0,   266,  1234,  ...,   908,   328, 26300],
        [    0,  5603,  6598,  ...,  1692,  1713,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5883e-04,  3.8853e-04, -4.9303e-04,  ...,  5.8929e-04,
         -7.5142e-04, -7.7808e-05],
        [-1.0028e-05, -7.2867e-06,  4.0904e-06,  ..., -8.7470e-06,
         -4.3027e-06, -6.0275e-06],
        [ 2.1598e-04,  1.7392e-04, -1.1533e-04,  ...,  2.9386e-04,
          5.6427e-05,  1.1393e-04],
        [-2.0415e-05, -1.4827e-05,  8.8066e-06,  ..., -1.7598e-05,
         -7.9870e-06, -1.1995e-05],
        [-2.6464e-05, -1.9357e-05,  1.1399e-05,  ..., -2.2531e-05,
         -1.0602e-05, -1.4558e-05]], device='cuda:0')
Loss: 1.079248309135437


Running epoch 0, step 901, batch 901
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,   257,    13,  ...,   328,   630,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3723e-04,  6.2221e-04, -3.9256e-04,  ...,  8.0227e-04,
         -1.0941e-03,  7.0827e-05],
        [-1.2159e-05, -8.8289e-06,  4.9807e-06,  ..., -1.0528e-05,
         -5.2229e-06, -7.2122e-06],
        [ 2.1282e-04,  1.7161e-04, -1.1398e-04,  ...,  2.9124e-04,
          5.5145e-05,  1.1222e-04],
        [-2.4706e-05, -1.7941e-05,  1.0662e-05,  ..., -2.1189e-05,
         -9.7230e-06, -1.4365e-05],
        [-3.2216e-05, -2.3559e-05,  1.3858e-05,  ..., -2.7299e-05,
         -1.2986e-05, -1.7583e-05]], device='cuda:0')
Loss: 1.118287444114685


Running epoch 0, step 902, batch 902
Sampled inputs[:2]: tensor([[    0, 29073,   916,  ...,    12,   287,   850],
        [    0,   341,   298,  ...,   298,  1304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3156e-04,  9.6299e-04, -7.0047e-04,  ...,  1.0521e-03,
         -1.1051e-03,  1.1789e-05],
        [-1.4022e-05, -1.0110e-05,  5.6736e-06,  ..., -1.2346e-05,
         -6.5044e-06, -8.7470e-06],
        [ 2.1000e-04,  1.6964e-04, -1.1283e-04,  ...,  2.8864e-04,
          5.3424e-05,  1.1010e-04],
        [-2.8282e-05, -2.0400e-05,  1.2144e-05,  ..., -2.4572e-05,
         -1.1943e-05, -1.7151e-05],
        [-3.6985e-05, -2.6882e-05,  1.5825e-05,  ..., -3.1650e-05,
         -1.5877e-05, -2.0996e-05]], device='cuda:0')
Loss: 1.0555363893508911


Running epoch 0, step 903, batch 903
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,   287, 20640,   292],
        [    0,   445,    16,  ...,  7747,  5308,  6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1447e-04,  1.2041e-03, -7.8185e-04,  ...,  1.2783e-03,
         -1.4470e-03, -3.4130e-05],
        [-1.5773e-05, -1.1414e-05,  6.4373e-06,  ..., -1.4141e-05,
         -7.3612e-06, -9.9614e-06],
        [ 2.0741e-04,  1.6770e-04, -1.1159e-04,  ...,  2.8614e-04,
          5.2344e-05,  1.0847e-04],
        [-3.1993e-05, -2.3142e-05,  1.3910e-05,  ..., -2.8238e-05,
         -1.3530e-05, -1.9580e-05],
        [-4.1693e-05, -3.0398e-05,  1.8075e-05,  ..., -3.6150e-05,
         -1.7889e-05, -2.3827e-05]], device='cuda:0')
Loss: 1.0650612115859985
Graident accumulation at epoch 0, step 903, batch 903
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3212e-06,  1.3706e-04, -1.6614e-04,  ...,  1.3277e-04,
         -1.7253e-04,  1.7400e-04],
        [-1.3958e-05, -7.3480e-06,  5.5920e-06,  ..., -8.7028e-06,
         -3.2203e-06, -5.6226e-06],
        [ 5.1718e-05,  3.2662e-05, -2.3499e-05,  ...,  5.8418e-05,
          2.1635e-05,  2.5788e-05],
        [-1.5714e-05, -1.2746e-05,  7.5148e-06,  ..., -1.3285e-05,
         -4.3343e-06, -1.2437e-05],
        [-3.9927e-05, -2.9519e-05,  1.8049e-05,  ..., -3.4342e-05,
         -1.7189e-05, -2.3989e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2520e-08, 3.2634e-08, 2.2575e-08,  ..., 2.9472e-08, 4.5215e-08,
         1.1753e-08],
        [5.8503e-11, 3.4704e-11, 4.3752e-12,  ..., 4.6259e-11, 7.9299e-12,
         1.7071e-11],
        [2.4481e-09, 1.1817e-09, 2.8709e-10,  ..., 1.9263e-09, 2.3446e-10,
         7.2717e-10],
        [7.1042e-10, 5.0316e-10, 6.7751e-11,  ..., 5.8514e-10, 7.3213e-11,
         2.2484e-10],
        [3.0771e-10, 1.6174e-10, 2.7610e-11,  ..., 2.2012e-10, 3.6690e-11,
         7.5646e-11]], device='cuda:0')
optimizer state dict: 113.0
lr: [9.739786387225548e-07, 9.739786387225548e-07]
scheduler_last_epoch: 113


Running epoch 0, step 904, batch 904
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,   266, 15258,  ...,  2366,   368,  3988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2543e-05, -5.8508e-04,  7.3805e-05,  ..., -3.1782e-04,
          6.7986e-04,  2.0927e-04],
        [-1.7136e-06, -1.2740e-06,  7.1526e-07,  ..., -1.6466e-06,
         -7.8976e-07, -1.3709e-06],
        [-2.5481e-06, -1.8924e-06,  1.1623e-06,  ..., -2.3246e-06,
         -9.9093e-07, -1.8254e-06],
        [-3.6359e-06, -2.6673e-06,  1.6689e-06,  ..., -3.3528e-06,
         -1.4231e-06, -2.7120e-06],
        [-4.6492e-06, -3.4422e-06,  2.1160e-06,  ..., -4.1425e-06,
         -1.8477e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0565615892410278


Running epoch 0, step 905, batch 905
Sampled inputs[:2]: tensor([[    0, 13964,    13,  ...,    14,   560,   199],
        [    0,    12,   266,  ...,   278,   266, 10995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9876e-06, -4.5355e-04, -2.4631e-05,  ..., -2.8523e-04,
          3.8007e-04,  5.6514e-05],
        [-3.6210e-06, -2.6822e-06,  1.5795e-06,  ..., -3.3453e-06,
         -1.4529e-06, -2.4959e-06],
        [-5.4389e-06, -4.0382e-06,  2.5183e-06,  ..., -4.8578e-06,
         -1.9222e-06, -3.4720e-06],
        [-7.6592e-06, -5.6326e-06,  3.5763e-06,  ..., -6.9141e-06,
         -2.6971e-06, -5.0664e-06],
        [-9.6858e-06, -7.1675e-06,  4.4852e-06,  ..., -8.5533e-06,
         -3.5465e-06, -5.8860e-06]], device='cuda:0')
Loss: 1.0399070978164673


Running epoch 0, step 906, batch 906
Sampled inputs[:2]: tensor([[   0, 1231,  278,  ...,   12, 2606,  266],
        [   0, 7219,  591,  ...,  278,  266, 5908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3546e-05, -4.3448e-04, -1.3963e-04,  ..., -9.8446e-05,
          1.5668e-04,  9.1948e-06],
        [-5.6624e-06, -4.2021e-06,  2.4587e-06,  ..., -5.1558e-06,
         -2.2203e-06, -3.7029e-06],
        [-8.4639e-06, -6.3181e-06,  3.8818e-06,  ..., -7.5400e-06,
         -3.0026e-06, -5.2080e-06],
        [-1.1772e-05, -8.7172e-06,  5.4538e-06,  ..., -1.0580e-05,
         -4.1500e-06, -7.4804e-06],
        [-1.4991e-05, -1.1161e-05,  6.8843e-06,  ..., -1.3262e-05,
         -5.5432e-06, -8.8364e-06]], device='cuda:0')
Loss: 1.095751404762268


Running epoch 0, step 907, batch 907
Sampled inputs[:2]: tensor([[   0, 5998,  591,  ..., 3126,   12,  358],
        [   0,  767, 1811,  ..., 1441, 1428,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7137e-04, -1.7603e-04, -2.5782e-04,  ...,  5.3081e-06,
         -5.0933e-04, -4.0817e-04],
        [-7.4804e-06, -5.5507e-06,  3.2112e-06,  ..., -6.9886e-06,
         -3.2187e-06, -4.9621e-06],
        [-1.1101e-05, -8.2552e-06,  5.0887e-06,  ..., -1.0014e-05,
         -4.2319e-06, -6.8322e-06],
        [-1.5512e-05, -1.1459e-05,  7.1749e-06,  ..., -1.4201e-05,
         -5.9679e-06, -9.9242e-06],
        [-1.9729e-05, -1.4648e-05,  9.0748e-06,  ..., -1.7673e-05,
         -7.7635e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.0552517175674438


Running epoch 0, step 908, batch 908
Sampled inputs[:2]: tensor([[   0,  395, 4973,  ..., 5851,  409, 4370],
        [   0,  729, 3084,  ...,  381, 1445,  642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2018e-04, -2.4322e-04, -3.1538e-04,  ...,  8.0376e-05,
         -5.6917e-04, -3.8624e-04],
        [-9.4175e-06, -6.9365e-06,  4.0419e-06,  ..., -8.6650e-06,
         -3.9153e-06, -6.1467e-06],
        [ 1.4370e-04,  1.1549e-04, -7.8796e-05,  ...,  1.1816e-04,
          5.0977e-05,  5.8877e-05],
        [-1.9655e-05, -1.4409e-05,  9.0450e-06,  ..., -1.7762e-05,
         -7.3463e-06, -1.2428e-05],
        [-2.4974e-05, -1.8418e-05,  1.1429e-05,  ..., -2.2143e-05,
         -9.5889e-06, -1.4603e-05]], device='cuda:0')
Loss: 1.0984699726104736


Running epoch 0, step 909, batch 909
Sampled inputs[:2]: tensor([[   0,  396,  221,  ..., 1279,  720,  292],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2441e-04, -8.5547e-05, -4.2452e-04,  ...,  1.2448e-04,
         -8.2971e-04, -5.7502e-04],
        [ 8.8313e-06,  8.8224e-05, -3.7681e-05,  ...,  4.6015e-05,
          4.1875e-06,  4.4698e-06],
        [ 1.4096e-04,  1.1358e-04, -7.7634e-05,  ...,  1.1570e-04,
          4.9404e-05,  5.6821e-05],
        [-2.3425e-05, -1.7017e-05,  1.0654e-05,  ..., -2.1264e-05,
         -9.5963e-06, -1.5438e-05],
        [-2.9862e-05, -2.1830e-05,  1.3530e-05,  ..., -2.6464e-05,
         -1.2375e-05, -1.8105e-05]], device='cuda:0')
Loss: 1.0405654907226562


Running epoch 0, step 910, batch 910
Sampled inputs[:2]: tensor([[   0, 4385,  342,  ..., 3644,  775,  874],
        [   0, 1336,  278,  ...,  266, 3269,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8769e-04, -1.2233e-04, -4.4923e-04,  ...,  9.8366e-05,
         -8.5870e-04, -6.2206e-04],
        [ 6.7899e-06,  8.6794e-05, -3.6810e-05,  ...,  4.4265e-05,
          3.4275e-06,  3.3001e-06],
        [ 1.3792e-04,  1.1142e-04, -7.6285e-05,  ...,  1.1311e-04,
          4.8332e-05,  5.5115e-05],
        [-2.7716e-05, -2.0057e-05,  1.2577e-05,  ..., -2.4959e-05,
         -1.1086e-05, -1.7911e-05],
        [-3.5286e-05, -2.5675e-05,  1.5929e-05,  ..., -3.1114e-05,
         -1.4357e-05, -2.1070e-05]], device='cuda:0')
Loss: 1.0950653553009033


Running epoch 0, step 911, batch 911
Sampled inputs[:2]: tensor([[   0,  422,   14,  ...,  271, 1360,   12],
        [   0,  199,  769,  ...,  685, 1423,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1655e-04, -1.7884e-04, -5.8889e-04,  ...,  1.9265e-05,
         -7.3780e-04, -7.1519e-04],
        [ 4.9198e-06,  8.5512e-05, -3.6091e-05,  ...,  4.2454e-05,
          2.2951e-06,  1.9143e-06],
        [ 1.3507e-04,  1.0947e-04, -7.5078e-05,  ...,  1.1055e-04,
          4.6871e-05,  5.3237e-05],
        [-3.1531e-05, -2.2665e-05,  1.4216e-05,  ..., -2.8476e-05,
         -1.3068e-05, -2.0534e-05],
        [-4.0412e-05, -2.9176e-05,  1.8135e-05,  ..., -3.5673e-05,
         -1.6995e-05, -2.4289e-05]], device='cuda:0')
Loss: 1.0737183094024658
Graident accumulation at epoch 0, step 911, batch 911
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6444e-05,  1.0547e-04, -2.0842e-04,  ...,  1.2142e-04,
         -2.2905e-04,  8.5081e-05],
        [-1.2070e-05,  1.9381e-06,  1.4237e-06,  ..., -3.5871e-06,
         -2.6688e-06, -4.8689e-06],
        [ 6.0053e-05,  4.0343e-05, -2.8657e-05,  ...,  6.3630e-05,
          2.4158e-05,  2.8533e-05],
        [-1.7295e-05, -1.3738e-05,  8.1849e-06,  ..., -1.4804e-05,
         -5.2077e-06, -1.3246e-05],
        [-3.9975e-05, -2.9484e-05,  1.8057e-05,  ..., -3.4475e-05,
         -1.7170e-05, -2.4019e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2734e-08, 3.2634e-08, 2.2899e-08,  ..., 2.9443e-08, 4.5714e-08,
         1.2253e-08],
        [5.8469e-11, 4.1981e-11, 5.6733e-12,  ..., 4.8015e-11, 7.9272e-12,
         1.7058e-11],
        [2.4639e-09, 1.1925e-09, 2.9244e-10,  ..., 1.9366e-09, 2.3642e-10,
         7.2928e-10],
        [7.1070e-10, 5.0318e-10, 6.7885e-11,  ..., 5.8537e-10, 7.3311e-11,
         2.2504e-10],
        [3.0903e-10, 1.6243e-10, 2.7911e-11,  ..., 2.2118e-10, 3.6942e-11,
         7.6161e-11]], device='cuda:0')
optimizer state dict: 114.0
lr: [8.703199712272026e-07, 8.703199712272026e-07]
scheduler_last_epoch: 114


Running epoch 0, step 912, batch 912
Sampled inputs[:2]: tensor([[    0,   342,  3001,  ...,   369, 11195,   367],
        [    0,   266,  1624,  ...,    14,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2207e-04,  8.3726e-05, -1.1027e-04,  ...,  6.7124e-05,
         -1.0152e-05, -1.2661e-05],
        [-1.8626e-06, -1.3635e-06,  8.3447e-07,  ..., -1.7285e-06,
         -7.8231e-07, -1.2890e-06],
        [-2.8163e-06, -2.0713e-06,  1.3337e-06,  ..., -2.5332e-06,
         -1.0729e-06, -1.8403e-06],
        [-3.9637e-06, -2.8908e-06,  1.8999e-06,  ..., -3.6210e-06,
         -1.5125e-06, -2.6822e-06],
        [-4.9472e-06, -3.6210e-06,  2.3395e-06,  ..., -4.4107e-06,
         -1.9521e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.059944987297058


Running epoch 0, step 913, batch 913
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  2165,  9311,  ..., 10570,   437,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0255e-04,  2.5597e-04, -2.7949e-04,  ...,  3.1147e-04,
         -1.7474e-04, -6.0955e-05],
        [-3.8892e-06, -2.8014e-06,  1.6540e-06,  ..., -3.5614e-06,
         -1.6876e-06, -2.6152e-06],
        [-5.7817e-06, -4.1872e-06,  2.5928e-06,  ..., -5.1409e-06,
         -2.2724e-06, -3.6582e-06],
        [-8.1360e-06, -5.8264e-06,  3.6806e-06,  ..., -7.3314e-06,
         -3.2112e-06, -5.3346e-06],
        [-1.0163e-05, -7.3463e-06,  4.5747e-06,  ..., -9.0003e-06,
         -4.1276e-06, -6.2138e-06]], device='cuda:0')
Loss: 1.065307378768921


Running epoch 0, step 914, batch 914
Sampled inputs[:2]: tensor([[    0,  6693,  1235,  ..., 10814,  1810,   367],
        [    0,   271,  4728,  ...,   344,   259,  1774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8333e-04,  5.6109e-04, -4.7624e-04,  ...,  4.6364e-04,
         -3.4809e-04, -9.4645e-05],
        [-5.6624e-06, -4.1425e-06,  2.4326e-06,  ..., -5.2452e-06,
         -2.5332e-06, -3.8892e-06],
        [-8.5086e-06, -6.2287e-06,  3.8669e-06,  ..., -7.6145e-06,
         -3.4273e-06, -5.4687e-06],
        [-1.1861e-05, -8.5980e-06,  5.4464e-06,  ..., -1.0759e-05,
         -4.7982e-06, -7.8827e-06],
        [-1.5020e-05, -1.0952e-05,  6.8396e-06,  ..., -1.3351e-05,
         -6.2287e-06, -9.2983e-06]], device='cuda:0')
Loss: 1.0556858777999878


Running epoch 0, step 915, batch 915
Sampled inputs[:2]: tensor([[    0,   445,    29,  ..., 20247,   272,   298],
        [    0,   474,   513,  ...,   221,  2951,  7773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4655e-04,  1.0277e-03, -8.6804e-04,  ...,  9.2781e-04,
         -7.7531e-04, -1.2635e-04],
        [-7.6443e-06, -5.4464e-06,  3.1367e-06,  ..., -7.0333e-06,
         -3.5986e-06, -5.3048e-06],
        [-1.1578e-05, -8.2552e-06,  5.0664e-06,  ..., -1.0237e-05,
         -4.8950e-06, -7.4953e-06],
        [-1.5825e-05, -1.1191e-05,  6.9886e-06,  ..., -1.4201e-05,
         -6.7204e-06, -1.0565e-05],
        [-2.0355e-05, -1.4469e-05,  8.9407e-06,  ..., -1.7852e-05,
         -8.7768e-06, -1.2651e-05]], device='cuda:0')
Loss: 1.0478448867797852


Running epoch 0, step 916, batch 916
Sampled inputs[:2]: tensor([[   0, 7018,   14,  ..., 8288,   12, 1250],
        [   0, 7066, 2737,  ..., 2269,  271,  927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0556e-04,  9.0855e-04, -8.8464e-04,  ...,  9.5847e-04,
         -8.0970e-04, -1.8481e-04],
        [-9.6858e-06, -6.8918e-06,  4.0047e-06,  ..., -8.8066e-06,
         -4.4517e-06, -6.5640e-06],
        [-1.4603e-05, -1.0431e-05,  6.4075e-06,  ..., -1.2830e-05,
         -6.0722e-06, -9.2909e-06],
        [-1.9968e-05, -1.4126e-05,  8.8289e-06,  ..., -1.7777e-05,
         -8.3223e-06, -1.3083e-05],
        [-2.5719e-05, -1.8314e-05,  1.1310e-05,  ..., -2.2441e-05,
         -1.0937e-05, -1.5736e-05]], device='cuda:0')
Loss: 1.072921633720398


Running epoch 0, step 917, batch 917
Sampled inputs[:2]: tensor([[    0, 17694,    12,  ..., 12452,   446,   475],
        [    0,  6184,  1412,  ...,    12,   266,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9127e-04,  9.1669e-04, -9.2466e-04,  ...,  1.0088e-03,
         -9.0733e-04, -1.9061e-04],
        [-1.1593e-05, -8.1956e-06,  4.8019e-06,  ..., -1.0595e-05,
         -5.4203e-06, -7.9051e-06],
        [-1.7390e-05, -1.2368e-05,  7.6517e-06,  ..., -1.5318e-05,
         -7.3165e-06, -1.1072e-05],
        [-2.3961e-05, -1.6883e-05,  1.0625e-05,  ..., -2.1428e-05,
         -1.0155e-05, -1.5751e-05],
        [-3.0726e-05, -2.1800e-05,  1.3560e-05,  ..., -2.6911e-05,
         -1.3232e-05, -1.8820e-05]], device='cuda:0')
Loss: 1.069878101348877


Running epoch 0, step 918, batch 918
Sampled inputs[:2]: tensor([[    0,  9088,  7217,  ...,   199, 17822,   278],
        [    0,   344,   259,  ..., 47553,   287, 28978]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8983e-04,  1.5685e-03, -1.1576e-03,  ...,  1.3918e-03,
         -1.1052e-03, -4.7062e-04],
        [-1.3404e-05, -9.5963e-06,  5.6028e-06,  ..., -1.2353e-05,
         -6.3218e-06, -9.2313e-06],
        [-2.0042e-05, -1.4409e-05,  8.8960e-06,  ..., -1.7762e-05,
         -8.4788e-06, -1.2830e-05],
        [-2.7671e-05, -1.9714e-05,  1.2398e-05,  ..., -2.4930e-05,
         -1.1802e-05, -1.8343e-05],
        [-3.5584e-05, -2.5511e-05,  1.5825e-05,  ..., -3.1352e-05,
         -1.5393e-05, -2.1905e-05]], device='cuda:0')
Loss: 1.0764912366867065


Running epoch 0, step 919, batch 919
Sampled inputs[:2]: tensor([[    0,  4014,    88,  ...,    14, 11961,    13],
        [    0,    14,  1075,  ..., 22182,  5948,  8401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6357e-04,  1.7628e-03, -1.1329e-03,  ...,  1.5668e-03,
         -1.2803e-03, -5.0795e-04],
        [-1.5430e-05, -1.1116e-05,  6.5379e-06,  ..., -1.4126e-05,
         -7.0632e-06, -1.0349e-05],
        [-2.3082e-05, -1.6719e-05,  1.0341e-05,  ..., -2.0400e-05,
         -9.5144e-06, -1.4476e-05],
        [-3.2023e-05, -2.2992e-05,  1.4484e-05,  ..., -2.8729e-05,
         -1.3269e-05, -2.0757e-05],
        [-4.0889e-05, -2.9504e-05,  1.8343e-05,  ..., -3.5971e-05,
         -1.7278e-05, -2.4691e-05]], device='cuda:0')
Loss: 1.0631771087646484
Graident accumulation at epoch 0, step 919, batch 919
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3716e-04,  2.7120e-04, -3.0087e-04,  ...,  2.6596e-04,
         -3.3418e-04,  2.5778e-05],
        [-1.2406e-05,  6.3264e-07,  1.9351e-06,  ..., -4.6411e-06,
         -3.1082e-06, -5.4169e-06],
        [ 5.1740e-05,  3.4636e-05, -2.4757e-05,  ...,  5.5227e-05,
          2.0791e-05,  2.4232e-05],
        [-1.8768e-05, -1.4663e-05,  8.8148e-06,  ..., -1.6197e-05,
         -6.0139e-06, -1.3998e-05],
        [-4.0067e-05, -2.9486e-05,  1.8086e-05,  ..., -3.4625e-05,
         -1.7181e-05, -2.4086e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3427e-08, 3.5708e-08, 2.4160e-08,  ..., 3.1869e-08, 4.7307e-08,
         1.2499e-08],
        [5.8649e-11, 4.2063e-11, 5.7104e-12,  ..., 4.8166e-11, 7.9692e-12,
         1.7148e-11],
        [2.4620e-09, 1.1916e-09, 2.9225e-10,  ..., 1.9350e-09, 2.3628e-10,
         7.2876e-10],
        [7.1102e-10, 5.0320e-10, 6.8027e-11,  ..., 5.8561e-10, 7.3413e-11,
         2.2525e-10],
        [3.1040e-10, 1.6314e-10, 2.8220e-11,  ..., 2.2225e-10, 3.7204e-11,
         7.6694e-11]], device='cuda:0')
optimizer state dict: 115.0
lr: [7.722414697591851e-07, 7.722414697591851e-07]
scheduler_last_epoch: 115


Running epoch 0, step 920, batch 920
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0, 49570,   644,  ...,   461,   800,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0261e-05,  4.8694e-05,  3.6037e-05,  ...,  1.3195e-04,
         -1.3420e-05,  6.3548e-05],
        [-2.0266e-06, -1.4082e-06,  8.5682e-07,  ..., -1.7434e-06,
         -7.1898e-07, -1.1921e-06],
        [-2.9206e-06, -2.0564e-06,  1.2964e-06,  ..., -2.5034e-06,
         -9.6858e-07, -1.6540e-06],
        [-4.1127e-06, -2.8759e-06,  1.8403e-06,  ..., -3.5316e-06,
         -1.3560e-06, -2.3842e-06],
        [-5.3048e-06, -3.7253e-06,  2.3544e-06,  ..., -4.5300e-06,
         -1.8403e-06, -2.9057e-06]], device='cuda:0')
Loss: 1.0709950923919678


Running epoch 0, step 921, batch 921
Sampled inputs[:2]: tensor([[   0,   13, 1320,  ..., 8686, 6851,   13],
        [   0,  591,  688,  ...,  271, 3390,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2407e-05,  1.0858e-04, -1.3470e-05,  ...,  1.8137e-04,
         -3.5702e-04, -1.5800e-04],
        [-3.9935e-06, -2.8685e-06,  1.6689e-06,  ..., -3.4943e-06,
         -1.4491e-06, -2.3544e-06],
        [-5.9307e-06, -4.3064e-06,  2.6077e-06,  ..., -5.1111e-06,
         -1.9670e-06, -3.3230e-06],
        [-8.2254e-06, -5.9307e-06,  3.6359e-06,  ..., -7.1526e-06,
         -2.7493e-06, -4.7535e-06],
        [-1.0699e-05, -7.7188e-06,  4.7088e-06,  ..., -9.1791e-06,
         -3.6806e-06, -5.7817e-06]], device='cuda:0')
Loss: 1.0694931745529175


Running epoch 0, step 922, batch 922
Sampled inputs[:2]: tensor([[    0, 36122,  1085,  ...,  6231,     9,  7794],
        [    0,  1526,   422,  ..., 22454,   409, 31482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3403e-04,  2.5317e-04, -7.2056e-05,  ...,  2.9970e-04,
         -5.1563e-04, -2.3292e-04],
        [-5.8115e-06, -4.1574e-06,  2.4922e-06,  ..., -5.2899e-06,
         -2.4773e-06, -3.7402e-06],
        [-8.6874e-06, -6.2734e-06,  3.9265e-06,  ..., -7.7188e-06,
         -3.3677e-06, -5.2750e-06],
        [-1.2040e-05, -8.6278e-06,  5.4762e-06,  ..., -1.0818e-05,
         -4.7162e-06, -7.5549e-06],
        [-1.5527e-05, -1.1176e-05,  7.0333e-06,  ..., -1.3709e-05,
         -6.1840e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.1122013330459595


Running epoch 0, step 923, batch 923
Sampled inputs[:2]: tensor([[    0,   504,   409,  ...,  5863,  2621,   824],
        [    0,   437,  1119,  ..., 32831,    83,   623]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5333e-04,  4.6781e-04, -1.1374e-04,  ...,  3.8035e-04,
         -6.5824e-04, -3.3495e-04],
        [-7.8976e-06, -5.6028e-06,  3.3490e-06,  ..., -7.0706e-06,
         -3.4608e-06, -5.0887e-06],
        [-1.1832e-05, -8.4788e-06,  5.2899e-06,  ..., -1.0341e-05,
         -4.7386e-06, -7.1973e-06],
        [-1.6212e-05, -1.1533e-05,  7.3016e-06,  ..., -1.4350e-05,
         -6.5565e-06, -1.0207e-05],
        [-2.1070e-05, -1.5050e-05,  9.4324e-06,  ..., -1.8328e-05,
         -8.6427e-06, -1.2383e-05]], device='cuda:0')
Loss: 1.0714548826217651


Running epoch 0, step 924, batch 924
Sampled inputs[:2]: tensor([[   0, 4672,  278,  ..., 7523, 2305,   13],
        [   0,  843, 3365,  ..., 1136, 1615,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0189e-04,  6.4486e-04, -1.9393e-04,  ...,  4.0077e-04,
         -8.1295e-04, -3.7842e-04],
        [-9.9093e-06, -7.0482e-06,  4.1798e-06,  ..., -8.8587e-06,
         -4.4107e-06, -6.3479e-06],
        [-1.4812e-05, -1.0639e-05,  6.6012e-06,  ..., -1.2919e-05,
         -6.0201e-06, -8.9630e-06],
        [-2.0266e-05, -1.4439e-05,  9.0748e-06,  ..., -1.7896e-05,
         -8.3223e-06, -1.2681e-05],
        [ 1.4509e-04,  1.1049e-04, -4.6542e-05,  ...,  1.4170e-04,
          9.7900e-05,  5.3747e-05]], device='cuda:0')
Loss: 1.1098108291625977


Running epoch 0, step 925, batch 925
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,  1034,   287,  ...,  9677,    13,  6687]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4478e-04,  7.3888e-04, -3.7562e-04,  ...,  4.7000e-04,
         -9.3251e-04, -3.9281e-04],
        [-1.1653e-05, -8.2925e-06,  4.9099e-06,  ..., -1.0580e-05,
         -5.3868e-06, -7.7188e-06],
        [-1.7464e-05, -1.2524e-05,  7.8157e-06,  ..., -1.5378e-05,
         -7.2941e-06, -1.0848e-05],
        [-2.3976e-05, -1.7047e-05,  1.0788e-05,  ..., -2.1413e-05,
         -1.0118e-05, -1.5438e-05],
        [ 1.4041e-04,  1.0717e-04, -4.4396e-05,  ...,  1.3741e-04,
          9.5650e-05,  5.0558e-05]], device='cuda:0')
Loss: 1.0610506534576416


Running epoch 0, step 926, batch 926
Sampled inputs[:2]: tensor([[    0,   259,  3022,  ...,   437,  5100,  1782],
        [    0,  3703,   278,  ...,  9807,    14, 10365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5838e-04,  6.3173e-04, -5.4938e-04,  ...,  4.9325e-04,
         -1.0326e-03, -4.9770e-04],
        [-1.3620e-05, -9.7156e-06,  5.7556e-06,  ..., -1.2368e-05,
         -6.1952e-06, -8.9929e-06],
        [-2.0295e-05, -1.4596e-05,  9.1195e-06,  ..., -1.7866e-05,
         -8.3223e-06, -1.2562e-05],
        [-2.7910e-05, -1.9908e-05,  1.2614e-05,  ..., -2.4945e-05,
         -1.1571e-05, -1.7941e-05],
        [ 1.3531e-04,  1.0346e-04, -4.2057e-05,  ...,  1.3297e-04,
          9.3757e-05,  4.7623e-05]], device='cuda:0')
Loss: 1.0890568494796753


Running epoch 0, step 927, batch 927
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,   271,   266,  ...,    70,    27,  5311]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8736e-04,  7.1221e-04, -5.9225e-04,  ...,  4.9642e-04,
         -9.6379e-04, -4.4334e-04],
        [-1.5602e-05, -1.1176e-05,  6.6459e-06,  ..., -1.4082e-05,
         -6.9737e-06, -1.0148e-05],
        [-2.3246e-05, -1.6786e-05,  1.0490e-05,  ..., -2.0400e-05,
         -9.3952e-06, -1.4238e-05],
        [-3.1993e-05, -2.2933e-05,  1.4551e-05,  ..., -2.8491e-05,
         -1.3046e-05, -2.0325e-05],
        [ 1.3010e-04,  9.9585e-05, -3.9628e-05,  ...,  1.2847e-04,
          9.1775e-05,  4.4747e-05]], device='cuda:0')
Loss: 1.094752311706543
Graident accumulation at epoch 0, step 927, batch 927
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6218e-04,  3.1530e-04, -3.3000e-04,  ...,  2.8901e-04,
         -3.9714e-04, -2.1134e-05],
        [-1.2725e-05, -5.4821e-07,  2.4062e-06,  ..., -5.5851e-06,
         -3.4948e-06, -5.8900e-06],
        [ 4.4241e-05,  2.9494e-05, -2.1232e-05,  ...,  4.7665e-05,
          1.7773e-05,  2.0385e-05],
        [-2.0091e-05, -1.5490e-05,  9.3884e-06,  ..., -1.7426e-05,
         -6.7171e-06, -1.4630e-05],
        [-2.3050e-05, -1.6579e-05,  1.2314e-05,  ..., -1.8316e-05,
         -6.2851e-06, -1.7203e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3524e-08, 3.6180e-08, 2.4486e-08,  ..., 3.2083e-08, 4.8189e-08,
         1.2683e-08],
        [5.8833e-11, 4.2146e-11, 5.7488e-12,  ..., 4.8317e-11, 8.0098e-12,
         1.7233e-11],
        [2.4601e-09, 1.1906e-09, 2.9207e-10,  ..., 1.9335e-09, 2.3613e-10,
         7.2823e-10],
        [7.1133e-10, 5.0322e-10, 6.8171e-11,  ..., 5.8584e-10, 7.3510e-11,
         2.2543e-10],
        [3.2701e-10, 1.7289e-10, 2.9762e-11,  ..., 2.3853e-10, 4.5589e-11,
         7.8620e-11]], device='cuda:0')
optimizer state dict: 116.0
lr: [6.798030810329725e-07, 6.798030810329725e-07]
scheduler_last_epoch: 116


Running epoch 0, step 928, batch 928
Sampled inputs[:2]: tensor([[   0,  287,  358,  ...,  328, 1704, 3227],
        [   0,  278, 1620,  ...,  360, 1758,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5279e-05,  2.6498e-04,  5.4792e-05,  ...,  0.0000e+00,
         -1.8640e-05,  4.0483e-05],
        [-2.0415e-06, -1.4678e-06,  8.1211e-07,  ..., -1.7434e-06,
         -1.0356e-06, -1.3560e-06],
        [-2.9951e-06, -2.1607e-06,  1.2591e-06,  ..., -2.4736e-06,
         -1.3784e-06, -1.8552e-06],
        [-4.0531e-06, -2.8759e-06,  1.6987e-06,  ..., -3.3826e-06,
         -1.8775e-06, -2.5928e-06],
        [-5.4240e-06, -3.9041e-06,  2.2948e-06,  ..., -4.4703e-06,
         -2.5332e-06, -3.2336e-06]], device='cuda:0')
Loss: 1.0702515840530396


Running epoch 0, step 929, batch 929
Sampled inputs[:2]: tensor([[    0,    14, 21687,  ...,   943,  2153,  4089],
        [    0,   287, 21212,  ...,  3123,   944,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3508e-05,  1.8305e-04, -4.3483e-05,  ...,  1.5959e-05,
          1.0002e-04,  1.4318e-04],
        [-4.1276e-06, -2.9430e-06,  1.6913e-06,  ..., -3.4943e-06,
         -1.8328e-06, -2.5928e-06],
        [-6.0946e-06, -4.3809e-06,  2.6152e-06,  ..., -5.0515e-06,
         -2.4959e-06, -3.6508e-06],
        [-8.2552e-06, -5.8711e-06,  3.5539e-06,  ..., -6.9141e-06,
         -3.3677e-06, -5.0813e-06],
        [-1.0878e-05, -7.8082e-06,  4.6790e-06,  ..., -9.0003e-06,
         -4.5747e-06, -6.3032e-06]], device='cuda:0')
Loss: 1.106224536895752


Running epoch 0, step 930, batch 930
Sampled inputs[:2]: tensor([[   0,  298,  894,  ...,  266, 2904, 1679],
        [   0, 6294,  367,  ...,  496,   14,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1484e-04,  4.2254e-04, -1.1711e-04,  ...,  5.0119e-05,
          1.0429e-04,  1.3190e-04],
        [-6.1691e-06, -4.2692e-06,  2.4475e-06,  ..., -5.3644e-06,
         -2.9653e-06, -4.0457e-06],
        [-9.0599e-06, -6.3479e-06,  3.8072e-06,  ..., -7.6443e-06,
         -3.9637e-06, -5.6028e-06],
        [-1.2279e-05, -8.4937e-06,  5.1558e-06,  ..., -1.0490e-05,
         -5.3644e-06, -7.7933e-06],
        [-1.6004e-05, -1.1221e-05,  6.7502e-06,  ..., -1.3471e-05,
         -7.1377e-06, -9.5367e-06]], device='cuda:0')
Loss: 1.0660183429718018


Running epoch 0, step 931, batch 931
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,   266, 10726,  ..., 13973, 22191, 15913]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6326e-04,  3.1703e-04, -3.2401e-04,  ...,  2.8811e-04,
         -5.3064e-05,  2.9728e-05],
        [-8.0243e-06, -5.6326e-06,  3.3341e-06,  ..., -7.1153e-06,
         -3.9116e-06, -5.3570e-06],
        [-1.1772e-05, -8.3596e-06,  5.1707e-06,  ..., -1.0118e-05,
         -5.2005e-06, -7.3761e-06],
        [-1.6153e-05, -1.1340e-05,  7.1228e-06,  ..., -1.4096e-05,
         -7.1824e-06, -1.0461e-05],
        [-2.0802e-05, -1.4782e-05,  9.1493e-06,  ..., -1.7792e-05,
         -9.3728e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.0905886888504028


Running epoch 0, step 932, batch 932
Sampled inputs[:2]: tensor([[   0,  342,  970,  ...,  401, 2907, 1657],
        [   0,   13,   41,  ...,    5,  271, 2936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5441e-04,  4.3438e-04, -2.2530e-04,  ...,  2.1815e-04,
          4.0266e-05,  9.5217e-05],
        [-1.0200e-05, -7.1526e-06,  4.1947e-06,  ..., -8.8662e-06,
         -4.8652e-06, -6.6087e-06],
        [-1.5080e-05, -1.0684e-05,  6.5193e-06,  ..., -1.2740e-05,
         -6.5640e-06, -9.2164e-06],
        [-2.0534e-05, -1.4409e-05,  8.9258e-06,  ..., -1.7613e-05,
         -8.9929e-06, -1.2964e-05],
        [-2.6673e-05, -1.8924e-05,  1.1563e-05,  ..., -2.2471e-05,
         -1.1846e-05, -1.5736e-05]], device='cuda:0')
Loss: 1.0901955366134644


Running epoch 0, step 933, batch 933
Sampled inputs[:2]: tensor([[    0,  1304,  1040,  ...,   287,  1665,   741],
        [    0,   775,   721,  ...,  5650,   518, 11548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6064e-04,  5.3274e-04, -3.3909e-04,  ...,  3.9023e-04,
         -4.8710e-05, -2.7779e-05],
        [-1.2167e-05, -8.6576e-06,  4.9956e-06,  ..., -1.0625e-05,
         -5.7593e-06, -7.8902e-06],
        [-1.8016e-05, -1.2964e-05,  7.7784e-06,  ..., -1.5318e-05,
         -7.7933e-06, -1.1049e-05],
        [-2.4587e-05, -1.7524e-05,  1.0684e-05,  ..., -2.1204e-05,
         -1.0706e-05, -1.5572e-05],
        [-3.1769e-05, -2.2858e-05,  1.3754e-05,  ..., -2.6941e-05,
         -1.4052e-05, -1.8805e-05]], device='cuda:0')
Loss: 1.0827484130859375


Running epoch 0, step 934, batch 934
Sampled inputs[:2]: tensor([[    0, 14700,   717,  ..., 10570,   292,   221],
        [    0,   271,   266,  ...,  4298,  1231,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0200e-04,  5.3052e-04, -4.1763e-04,  ...,  1.3741e-04,
          3.3633e-04,  8.6672e-05],
        [-1.4000e-05, -9.9093e-06,  5.6177e-06,  ..., -1.2487e-05,
         -7.0557e-06, -9.4771e-06],
        [-2.0638e-05, -1.4782e-05,  8.8438e-06,  ..., -1.7762e-05,
         -9.3356e-06, -1.3016e-05],
        [-2.8163e-05, -1.9968e-05,  1.2115e-05,  ..., -2.4617e-05,
         -1.2882e-05, -1.8388e-05],
        [-3.6359e-05, -2.6062e-05,  1.5676e-05,  ..., -3.1143e-05,
         -1.6734e-05, -2.2069e-05]], device='cuda:0')
Loss: 1.0508801937103271


Running epoch 0, step 935, batch 935
Sampled inputs[:2]: tensor([[   0, 7963,   17,  ...,   50,   13,   18],
        [   0,  221,  380,  ...,  508, 1853,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2391e-04,  6.9303e-04, -4.2477e-04,  ...,  5.2149e-04,
          3.7433e-06,  2.4334e-04],
        [-1.5877e-05, -1.1243e-05,  6.4336e-06,  ..., -1.4268e-05,
         -7.9982e-06, -1.0736e-05],
        [-2.3410e-05, -1.6764e-05,  1.0118e-05,  ..., -2.0310e-05,
         -1.0580e-05, -1.4760e-05],
        [-3.2037e-05, -2.2709e-05,  1.3903e-05,  ..., -2.8223e-05,
         -1.4633e-05, -2.0921e-05],
        [-4.1276e-05, -2.9594e-05,  1.7926e-05,  ..., -3.5614e-05,
         -1.9014e-05, -2.5049e-05]], device='cuda:0')
Loss: 1.0698626041412354
Graident accumulation at epoch 0, step 935, batch 935
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8835e-04,  3.5307e-04, -3.3948e-04,  ...,  3.1225e-04,
         -3.5705e-04,  5.3138e-06],
        [-1.3041e-05, -1.6177e-06,  2.8090e-06,  ..., -6.4534e-06,
         -3.9451e-06, -6.3746e-06],
        [ 3.7476e-05,  2.4868e-05, -1.8097e-05,  ...,  4.0867e-05,
          1.4937e-05,  1.6871e-05],
        [-2.1285e-05, -1.6212e-05,  9.8399e-06,  ..., -1.8506e-05,
         -7.5087e-06, -1.5259e-05],
        [-2.4873e-05, -1.7881e-05,  1.2876e-05,  ..., -2.0046e-05,
         -7.5580e-06, -1.7987e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3650e-08, 3.6624e-08, 2.4642e-08,  ..., 3.2323e-08, 4.8141e-08,
         1.2729e-08],
        [5.9027e-11, 4.2230e-11, 5.7845e-12,  ..., 4.8472e-11, 8.0658e-12,
         1.7332e-11],
        [2.4581e-09, 1.1897e-09, 2.9188e-10,  ..., 1.9320e-09, 2.3601e-10,
         7.2772e-10],
        [7.1165e-10, 5.0324e-10, 6.8296e-11,  ..., 5.8605e-10, 7.3651e-11,
         2.2565e-10],
        [3.2839e-10, 1.7359e-10, 3.0053e-11,  ..., 2.3956e-10, 4.5905e-11,
         7.9168e-11]], device='cuda:0')
optimizer state dict: 117.0
lr: [5.930613044608946e-07, 5.930613044608946e-07]
scheduler_last_epoch: 117


Running epoch 0, step 936, batch 936
Sampled inputs[:2]: tensor([[    0,  1640,  1103,  ...,   685,  1478,    14],
        [    0,    12,   696,  ..., 14275,  2661,  6129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2209e-05,  8.3725e-05, -7.9419e-05,  ..., -5.2589e-05,
          2.2855e-04, -7.0898e-05],
        [-1.9073e-06, -1.4231e-06,  7.7486e-07,  ..., -1.8254e-06,
         -9.6112e-07, -1.3784e-06],
        [-2.8163e-06, -2.0862e-06,  1.2517e-06,  ..., -2.5481e-06,
         -1.2368e-06, -1.8179e-06],
        [-3.7849e-06, -2.8014e-06,  1.6838e-06,  ..., -3.5018e-06,
         -1.6838e-06, -2.5630e-06],
        [-5.0962e-06, -3.7700e-06,  2.2948e-06,  ..., -4.5598e-06,
         -2.2650e-06, -3.1441e-06]], device='cuda:0')
Loss: 1.0634053945541382


Running epoch 0, step 937, batch 937
Sampled inputs[:2]: tensor([[    0,   413,    29,  ...,   818,   278,   970],
        [    0,    13, 23904,  ...,   560,  8840,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7451e-05,  1.4184e-04, -1.1181e-05,  ..., -6.6536e-05,
          3.4914e-04,  1.7993e-05],
        [-3.9786e-06, -2.8908e-06,  1.5274e-06,  ..., -3.6061e-06,
         -2.0340e-06, -2.8014e-06],
        [-5.9307e-06, -4.3064e-06,  2.4810e-06,  ..., -5.1409e-06,
         -2.7046e-06, -3.8147e-06],
        [-7.8678e-06, -5.7071e-06,  3.3155e-06,  ..., -6.9588e-06,
         -3.6359e-06, -5.2899e-06],
        [-1.0669e-05, -7.7635e-06,  4.5300e-06,  ..., -9.2089e-06,
         -4.9472e-06, -6.6161e-06]], device='cuda:0')
Loss: 1.0513676404953003


Running epoch 0, step 938, batch 938
Sampled inputs[:2]: tensor([[    0, 25845,  4034,  ...,   474,   221,   474],
        [    0,  1067,   408,  ...,  4657,  1016,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4751e-04,  3.9573e-04, -8.1276e-05,  ...,  5.1463e-05,
          8.9160e-05, -9.8315e-05],
        [-5.9456e-06, -4.2841e-06,  2.2650e-06,  ..., -5.3793e-06,
         -2.8946e-06, -4.0606e-06],
        [-8.9109e-06, -6.4075e-06,  3.7104e-06,  ..., -7.6890e-06,
         -3.8520e-06, -5.5730e-06],
        [-1.2010e-05, -8.5980e-06,  5.0068e-06,  ..., -1.0550e-05,
         -5.2676e-06, -7.8231e-06],
        [-1.5914e-05, -1.1444e-05,  6.7055e-06,  ..., -1.3649e-05,
         -7.0184e-06, -9.5516e-06]], device='cuda:0')
Loss: 1.0507519245147705


Running epoch 0, step 939, batch 939
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   493,  2104,    14],
        [    0,   409, 35049,  ...,    12,   699,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1495e-04,  7.6954e-04,  9.2491e-05,  ...,  2.2330e-04,
          5.3369e-05, -7.2560e-05],
        [-8.0168e-06, -5.6997e-06,  2.9951e-06,  ..., -7.1377e-06,
         -3.8780e-06, -5.4538e-06],
        [-1.2115e-05, -8.6129e-06,  4.9248e-06,  ..., -1.0341e-05,
         -5.2527e-06, -7.6145e-06],
        [-1.6242e-05, -1.1489e-05,  6.6087e-06,  ..., -1.4082e-05,
         -7.1228e-06, -1.0595e-05],
        [-2.1607e-05, -1.5348e-05,  8.8811e-06,  ..., -1.8328e-05,
         -9.5516e-06, -1.3068e-05]], device='cuda:0')
Loss: 1.0756430625915527


Running epoch 0, step 940, batch 940
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7821e-04,  7.3069e-04,  7.9028e-05,  ...,  2.3454e-04,
          2.4198e-05, -6.4275e-05],
        [-1.0043e-05, -7.1377e-06,  3.8780e-06,  ..., -8.9034e-06,
         -4.6454e-06, -6.6757e-06],
        [-1.5154e-05, -1.0788e-05,  6.3032e-06,  ..., -1.2949e-05,
         -6.3181e-06, -9.3877e-06],
        [-2.0444e-05, -1.4484e-05,  8.5309e-06,  ..., -1.7717e-05,
         -8.5831e-06, -1.3113e-05],
        [-2.7001e-05, -1.9222e-05,  1.1325e-05,  ..., -2.2948e-05,
         -1.1519e-05, -1.6108e-05]], device='cuda:0')
Loss: 1.086158275604248


Running epoch 0, step 941, batch 941
Sampled inputs[:2]: tensor([[    0, 18322,   287,  ...,   953,   271,   221],
        [    0,  2518,   437,  ...,    12,  1041,   283]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5688e-05,  6.3259e-04, -3.9726e-05,  ...,  3.2914e-04,
         -8.6012e-05, -7.4463e-05],
        [-1.1995e-05, -8.5309e-06,  4.7460e-06,  ..., -1.0625e-05,
         -5.4576e-06, -8.1137e-06],
        [-1.8135e-05, -1.2934e-05,  7.6890e-06,  ..., -1.5497e-05,
         -7.4282e-06, -1.1444e-05],
        [-2.4498e-05, -1.7375e-05,  1.0453e-05,  ..., -2.1234e-05,
         -1.0096e-05, -1.6004e-05],
        [-3.2127e-05, -2.2903e-05,  1.3709e-05,  ..., -2.7299e-05,
         -1.3500e-05, -1.9491e-05]], device='cuda:0')
Loss: 1.0514163970947266


Running epoch 0, step 942, batch 942
Sampled inputs[:2]: tensor([[    0,   795,  3185,  ...,    14,  1671,   199],
        [    0,   634,   631,  ...,  3431,   287, 27947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2104e-06,  6.5744e-04,  3.5650e-05,  ...,  3.8446e-04,
         -2.0573e-04, -2.5213e-05],
        [-1.3992e-05, -9.9763e-06,  5.6326e-06,  ..., -1.2361e-05,
         -6.2846e-06, -9.3207e-06],
        [-2.1130e-05, -1.5110e-05,  9.0823e-06,  ..., -1.8060e-05,
         -8.5831e-06, -1.3180e-05],
        [-2.8670e-05, -2.0385e-05,  1.2390e-05,  ..., -2.4840e-05,
         -1.1690e-05, -1.8477e-05],
        [-3.7521e-05, -2.6807e-05,  1.6212e-05,  ..., -3.1888e-05,
         -1.5631e-05, -2.2516e-05]], device='cuda:0')
Loss: 1.072414755821228


Running epoch 0, step 943, batch 943
Sampled inputs[:2]: tensor([[    0,  3544,   417,  ...,   380,   381,  3794],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0988e-04,  7.2974e-04, -3.1966e-04,  ...,  9.3633e-04,
         -1.2065e-03, -3.7163e-04],
        [-1.5728e-05, -1.1139e-05,  6.3851e-06,  ..., -1.4134e-05,
         -7.2680e-06, -1.0736e-05],
        [-2.3648e-05, -1.6823e-05,  1.0252e-05,  ..., -2.0474e-05,
         -9.7975e-06, -1.5013e-05],
        [-3.2395e-05, -2.2888e-05,  1.4164e-05,  ..., -2.8566e-05,
         -1.3612e-05, -2.1398e-05],
        [-4.2051e-05, -2.9892e-05,  1.8314e-05,  ..., -3.6150e-05,
         -1.7822e-05, -2.5660e-05]], device='cuda:0')
Loss: 1.0671199560165405
Graident accumulation at epoch 0, step 943, batch 943
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1050e-04,  3.9074e-04, -3.3750e-04,  ...,  3.7466e-04,
         -4.4199e-04, -3.2380e-05],
        [-1.3309e-05, -2.5698e-06,  3.1666e-06,  ..., -7.2214e-06,
         -4.2774e-06, -6.8108e-06],
        [ 3.1364e-05,  2.0699e-05, -1.5263e-05,  ...,  3.4733e-05,
          1.2464e-05,  1.3682e-05],
        [-2.2396e-05, -1.6880e-05,  1.0272e-05,  ..., -1.9512e-05,
         -8.1190e-06, -1.5873e-05],
        [-2.6591e-05, -1.9082e-05,  1.3419e-05,  ..., -2.1656e-05,
         -8.5844e-06, -1.8754e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3764e-08, 3.7120e-08, 2.4720e-08,  ..., 3.3167e-08, 4.9548e-08,
         1.2855e-08],
        [5.9215e-11, 4.2312e-11, 5.8195e-12,  ..., 4.8623e-11, 8.1106e-12,
         1.7429e-11],
        [2.4562e-09, 1.1888e-09, 2.9169e-10,  ..., 1.9305e-09, 2.3587e-10,
         7.2722e-10],
        [7.1198e-10, 5.0326e-10, 6.8428e-11,  ..., 5.8628e-10, 7.3762e-11,
         2.2588e-10],
        [3.2983e-10, 1.7431e-10, 3.0359e-11,  ..., 2.4063e-10, 4.6177e-11,
         7.9748e-11]], device='cuda:0')
optimizer state dict: 118.0
lr: [5.120691576200498e-07, 5.120691576200498e-07]
scheduler_last_epoch: 118


Running epoch 0, step 944, batch 944
Sampled inputs[:2]: tensor([[    0,   834,    89,  ...,  4030,    12,  6528],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7497e-04,  3.4402e-04, -2.9259e-04,  ...,  3.1064e-04,
         -5.9381e-04, -1.0590e-04],
        [-1.8701e-06, -1.2964e-06,  7.7114e-07,  ..., -1.8254e-06,
         -9.2387e-07, -1.4156e-06],
        [-2.7418e-06, -1.9073e-06,  1.2219e-06,  ..., -2.5183e-06,
         -1.1772e-06, -1.8775e-06],
        [-4.0233e-06, -2.7567e-06,  1.7956e-06,  ..., -3.7849e-06,
         -1.7732e-06, -2.8908e-06],
        [-4.7982e-06, -3.3528e-06,  2.1458e-06,  ..., -4.3511e-06,
         -2.1011e-06, -3.1441e-06]], device='cuda:0')
Loss: 1.071277141571045


Running epoch 0, step 945, batch 945
Sampled inputs[:2]: tensor([[    0,   199,  3289,  ...,  2269,  6476,   271],
        [    0,   300,  6263,  ..., 18488,  1665,  1640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2357e-04,  7.8886e-04, -5.6214e-04,  ...,  4.3632e-04,
         -1.0073e-03, -3.9512e-04],
        [-3.9414e-06, -2.6152e-06,  1.4938e-06,  ..., -3.6508e-06,
         -1.8105e-06, -2.8089e-06],
        [-5.7817e-06, -3.8743e-06,  2.3618e-06,  ..., -5.1409e-06,
         -2.3916e-06, -3.8296e-06],
        [-8.1062e-06, -5.3644e-06,  3.3304e-06,  ..., -7.3463e-06,
         -3.3900e-06, -5.5730e-06],
        [-1.0133e-05, -6.7949e-06,  4.1574e-06,  ..., -8.9109e-06,
         -4.2617e-06, -6.4224e-06]], device='cuda:0')
Loss: 1.0729382038116455


Running epoch 0, step 946, batch 946
Sampled inputs[:2]: tensor([[   0,  380,  560,  ...,  287, 6769,  806],
        [   0, 6418,  446,  ...,  413,   29,  413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7443e-04,  1.1768e-03, -5.4235e-04,  ...,  6.7618e-04,
         -1.3375e-03, -4.6059e-04],
        [-6.1318e-06, -4.0680e-06,  2.3171e-06,  ..., -5.4613e-06,
         -2.8163e-06, -4.0531e-06],
        [-9.0599e-06, -6.0797e-06,  3.6433e-06,  ..., -7.8231e-06,
         -3.8072e-06, -5.6252e-06],
        [-1.2577e-05, -8.3596e-06,  5.0887e-06,  ..., -1.1027e-05,
         -5.3123e-06, -8.0764e-06],
        [-1.6034e-05, -1.0759e-05,  6.4671e-06,  ..., -1.3709e-05,
         -6.8545e-06, -9.5665e-06]], device='cuda:0')
Loss: 1.0967684984207153


Running epoch 0, step 947, batch 947
Sampled inputs[:2]: tensor([[   0, 4175,  437,  ..., 1700,   14,  381],
        [   0, 3353,   17,  ...,  596,   12,  461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6605e-04,  1.2825e-03, -6.1032e-04,  ...,  6.6909e-04,
         -1.4131e-03, -4.9400e-04],
        [-8.2031e-06, -5.5656e-06,  3.2000e-06,  ..., -7.2122e-06,
         -3.5912e-06, -5.2229e-06],
        [ 9.7607e-05,  6.3865e-05, -2.2768e-05,  ...,  6.5039e-05,
          2.8492e-05,  5.3734e-05],
        [-1.6838e-05, -1.1444e-05,  6.9737e-06,  ..., -1.4618e-05,
         -6.7875e-06, -1.0505e-05],
        [-2.1487e-05, -1.4752e-05,  8.8513e-06,  ..., -1.8299e-05,
         -8.8662e-06, -1.2532e-05]], device='cuda:0')
Loss: 1.0990873575210571


Running epoch 0, step 948, batch 948
Sampled inputs[:2]: tensor([[   0, 1549, 7052,  ..., 2529, 3958,   37],
        [   0,  516,  689,  ...,  278,  516, 6137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4822e-04,  1.5726e-03, -9.4639e-04,  ...,  9.2258e-04,
         -1.8522e-03, -6.6571e-04],
        [-9.9763e-06, -6.8024e-06,  4.0159e-06,  ..., -8.9556e-06,
         -4.5225e-06, -6.6459e-06],
        [ 9.4984e-05,  6.2018e-05, -2.1479e-05,  ...,  6.2625e-05,
          2.7323e-05,  5.1863e-05],
        [-2.0608e-05, -1.4067e-05,  8.8587e-06,  ..., -1.8209e-05,
         -8.5309e-06, -1.3381e-05],
        [-2.6196e-05, -1.8075e-05,  1.1161e-05,  ..., -2.2590e-05,
         -1.1027e-05, -1.5736e-05]], device='cuda:0')
Loss: 1.0843509435653687


Running epoch 0, step 949, batch 949
Sampled inputs[:2]: tensor([[   0, 3101,  275,  ..., 2345,  609,  287],
        [   0,   14,   22,  ..., 1319,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7962e-04,  1.8279e-03, -8.5624e-04,  ...,  9.7341e-04,
         -1.9891e-03, -6.8791e-04],
        [-1.1854e-05, -8.1956e-06,  4.7833e-06,  ..., -1.0684e-05,
         -5.4911e-06, -8.0541e-06],
        [ 9.2019e-05,  5.9842e-05, -2.0175e-05,  ...,  6.0047e-05,
          2.5982e-05,  4.9852e-05],
        [-2.4512e-05, -1.6913e-05,  1.0602e-05,  ..., -2.1681e-05,
         -1.0341e-05, -1.6153e-05],
        [-3.1292e-05, -2.1830e-05,  1.3426e-05,  ..., -2.7001e-05,
         -1.3381e-05, -1.9059e-05]], device='cuda:0')
Loss: 1.0633116960525513


Running epoch 0, step 950, batch 950
Sampled inputs[:2]: tensor([[    0,  1145,    13,  ...,   721,  1119,  3495],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8215e-04,  2.3934e-03, -1.2362e-03,  ...,  1.3254e-03,
         -2.7564e-03, -9.5591e-04],
        [-1.3702e-05, -9.4622e-06,  5.5693e-06,  ..., -1.2420e-05,
         -6.3181e-06, -9.3728e-06],
        [ 8.9262e-05,  5.7935e-05, -1.8916e-05,  ...,  5.7558e-05,
          2.4879e-05,  4.8026e-05],
        [-2.8476e-05, -1.9625e-05,  1.2435e-05,  ..., -2.5332e-05,
         -1.1958e-05, -1.8910e-05],
        [-3.6269e-05, -2.5257e-05,  1.5706e-05,  ..., -3.1471e-05,
         -1.5423e-05, -2.2247e-05]], device='cuda:0')
Loss: 1.0710268020629883


Running epoch 0, step 951, batch 951
Sampled inputs[:2]: tensor([[   0, 2302,  287,  ..., 1522, 1666,  300],
        [   0, 1795,  650,  ...,  516, 2793, 1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2075e-03,  2.7427e-03, -1.3062e-03,  ...,  1.5910e-03,
         -3.1501e-03, -1.0324e-03],
        [-1.5587e-05, -1.0796e-05,  6.4075e-06,  ..., -1.4096e-05,
         -6.9998e-06, -1.0617e-05],
        [ 8.6401e-05,  5.5893e-05, -1.7575e-05,  ...,  5.5055e-05,
          2.3933e-05,  4.6231e-05],
        [-3.2499e-05, -2.2471e-05,  1.4335e-05,  ..., -2.8893e-05,
         -1.3277e-05, -2.1517e-05],
        [-4.1336e-05, -2.8878e-05,  1.8090e-05,  ..., -3.5912e-05,
         -1.7188e-05, -2.5302e-05]], device='cuda:0')
Loss: 1.053719162940979
Graident accumulation at epoch 0, step 951, batch 951
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1020e-04,  6.2594e-04, -4.3437e-04,  ...,  4.9630e-04,
         -7.1280e-04, -1.3238e-04],
        [-1.3537e-05, -3.3924e-06,  3.4907e-06,  ..., -7.9089e-06,
         -4.5497e-06, -7.1914e-06],
        [ 3.6867e-05,  2.4219e-05, -1.5494e-05,  ...,  3.6765e-05,
          1.3611e-05,  1.6937e-05],
        [-2.3407e-05, -1.7439e-05,  1.0678e-05,  ..., -2.0450e-05,
         -8.6348e-06, -1.6438e-05],
        [-2.8065e-05, -2.0061e-05,  1.3886e-05,  ..., -2.3082e-05,
         -9.4448e-06, -1.9409e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5169e-08, 4.4606e-08, 2.6401e-08,  ..., 3.5666e-08, 5.9422e-08,
         1.3908e-08],
        [5.9399e-11, 4.2386e-11, 5.8547e-12,  ..., 4.8773e-11, 8.1515e-12,
         1.7525e-11],
        [2.4613e-09, 1.1908e-09, 2.9171e-10,  ..., 1.9316e-09, 2.3620e-10,
         7.2863e-10],
        [7.1233e-10, 5.0326e-10, 6.8565e-11,  ..., 5.8653e-10, 7.3865e-11,
         2.2612e-10],
        [3.3121e-10, 1.7497e-10, 3.0656e-11,  ..., 2.4168e-10, 4.6426e-11,
         8.0308e-11]], device='cuda:0')
optimizer state dict: 119.0
lr: [4.36876143847339e-07, 4.36876143847339e-07]
scheduler_last_epoch: 119


Running epoch 0, step 952, batch 952
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,   409,  4146,  ...,     9,   360,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8636e-05,  1.2948e-04,  2.7430e-05,  ..., -4.2318e-06,
         -1.6173e-04, -6.8125e-05],
        [-1.9670e-06, -1.3709e-06,  8.3447e-07,  ..., -1.7881e-06,
         -9.3132e-07, -1.2964e-06],
        [-2.9504e-06, -2.0713e-06,  1.3113e-06,  ..., -2.6226e-06,
         -1.2890e-06, -1.8477e-06],
        [-4.2319e-06, -2.9504e-06,  1.9073e-06,  ..., -3.8147e-06,
         -1.8552e-06, -2.7418e-06],
        [-5.3048e-06, -3.6955e-06,  2.3544e-06,  ..., -4.6790e-06,
         -2.3693e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.0723456144332886


Running epoch 0, step 953, batch 953
Sampled inputs[:2]: tensor([[   0,  825, 3066,  ..., 1184,  266, 7964],
        [   0, 3933, 6394,  ..., 1364,  950,  847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5826e-05,  2.7007e-04, -1.0699e-04,  ...,  1.2691e-04,
         -4.8378e-04, -1.6325e-04],
        [-3.9637e-06, -2.7716e-06,  1.7583e-06,  ..., -3.5018e-06,
         -1.6578e-06, -2.5555e-06],
        [ 1.8992e-04,  1.2552e-04, -9.2794e-05,  ...,  1.6546e-04,
          7.0057e-05,  6.8101e-05],
        [-8.4937e-06, -5.9456e-06,  3.9637e-06,  ..., -7.4506e-06,
         -3.3081e-06, -5.3942e-06],
        [-1.0610e-05, -7.4804e-06,  4.8876e-06,  ..., -9.1791e-06,
         -4.2394e-06, -6.3628e-06]], device='cuda:0')
Loss: 1.0963300466537476


Running epoch 0, step 954, batch 954
Sampled inputs[:2]: tensor([[    0,   446, 28686,  ...,    35,  2706, 19712],
        [    0,    15,    83,  ...,  6030,    14, 14080]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8549e-05,  3.0751e-04, -9.6293e-05,  ...,  1.2147e-04,
         -5.1188e-04, -1.8264e-04],
        [-5.9009e-06, -4.1872e-06,  2.6226e-06,  ..., -5.2676e-06,
         -2.5593e-06, -3.8147e-06],
        [ 1.8698e-04,  1.2336e-04, -9.1431e-05,  ...,  1.6287e-04,
          6.8820e-05,  6.6290e-05],
        [-1.2547e-05, -8.8960e-06,  5.8413e-06,  ..., -1.1086e-05,
         -5.0291e-06, -7.9870e-06],
        [-1.5825e-05, -1.1295e-05,  7.2867e-06,  ..., -1.3769e-05,
         -6.4895e-06, -9.4622e-06]], device='cuda:0')
Loss: 1.1212750673294067


Running epoch 0, step 955, batch 955
Sampled inputs[:2]: tensor([[   0, 1062,  648,  ...,  266, 4939,  278],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1262e-05,  3.1711e-04, -3.4628e-04,  ...,  4.3397e-04,
         -8.1073e-04, -3.0132e-04],
        [-7.5996e-06, -5.4538e-06,  3.4720e-06,  ..., -6.9961e-06,
         -3.4496e-06, -5.2750e-06],
        [ 1.8448e-04,  1.2148e-04, -9.0097e-05,  ...,  1.6045e-04,
          6.7657e-05,  6.4338e-05],
        [-1.6153e-05, -1.1563e-05,  7.8082e-06,  ..., -1.4648e-05,
         -6.7279e-06, -1.0937e-05],
        [-2.0236e-05, -1.4603e-05,  9.6411e-06,  ..., -1.7971e-05,
         -8.5905e-06, -1.2740e-05]], device='cuda:0')
Loss: 1.0582258701324463


Running epoch 0, step 956, batch 956
Sampled inputs[:2]: tensor([[    0,   292, 17190,  ...,  3078,     9,   287],
        [    0,   221,  1771,  ..., 14547,  1705,  1003]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4249e-04,  1.3886e-04, -1.0015e-04,  ...,  3.0320e-04,
         -4.4986e-04,  5.8487e-05],
        [-9.5665e-06, -6.8247e-06,  4.2580e-06,  ..., -8.7917e-06,
         -4.5747e-06, -6.7875e-06],
        [ 1.8157e-04,  1.1943e-04, -8.8860e-05,  ...,  1.5790e-04,
          6.6175e-05,  6.2267e-05],
        [-2.0117e-05, -1.4335e-05,  9.4995e-06,  ..., -1.8179e-05,
         -8.7991e-06, -1.3873e-05],
        [-2.5421e-05, -1.8269e-05,  1.1861e-05,  ..., -2.2501e-05,
         -1.1273e-05, -1.6287e-05]], device='cuda:0')
Loss: 1.093408465385437


Running epoch 0, step 957, batch 957
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,    12,   344,  ...,    14,  2295,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0627e-04, -1.6382e-04,  5.2909e-06,  ...,  2.2549e-04,
         -2.0808e-04,  1.0438e-04],
        [-1.1593e-05, -8.2850e-06,  5.2117e-06,  ..., -1.0595e-05,
         -5.4687e-06, -8.0466e-06],
        [ 3.8394e-04,  2.6623e-04, -1.4070e-04,  ...,  3.2210e-04,
          2.1231e-04,  1.1260e-04],
        [-2.4170e-05, -1.7300e-05,  1.1511e-05,  ..., -2.1800e-05,
         -1.0468e-05, -1.6406e-05],
        [-3.0696e-05, -2.2143e-05,  1.4424e-05,  ..., -2.7120e-05,
         -1.3508e-05, -1.9342e-05]], device='cuda:0')
Loss: 1.0884478092193604


Running epoch 0, step 958, batch 958
Sampled inputs[:2]: tensor([[   0, 4213, 1921,  ..., 1340, 1049,  292],
        [   0,  292,   58,  ...,  319,  221, 1061]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0598e-04, -6.0045e-04, -7.0848e-05,  ...,  1.2586e-04,
          7.1823e-05,  1.6376e-04],
        [-1.3560e-05, -9.6411e-06,  6.0946e-06,  ..., -1.2361e-05,
         -6.3591e-06, -9.3877e-06],
        [ 3.8099e-04,  2.6416e-04, -1.3932e-04,  ...,  3.1952e-04,
          2.1110e-04,  1.1070e-04],
        [-2.8282e-05, -2.0146e-05,  1.3463e-05,  ..., -2.5436e-05,
         -1.2174e-05, -1.9148e-05],
        [-3.5971e-05, -2.5854e-05,  1.6898e-05,  ..., -3.1710e-05,
         -1.5743e-05, -2.2590e-05]], device='cuda:0')
Loss: 1.0854005813598633


Running epoch 0, step 959, batch 959
Sampled inputs[:2]: tensor([[    0,  2229,   352,  ...,  4988,    33,    13],
        [    0,   292, 12522,  ...,   266,  1977,  8481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2152e-04, -3.7474e-04,  2.3478e-05,  ...,  2.2376e-04,
         -1.0140e-05,  3.9120e-04],
        [-1.5572e-05, -1.1034e-05,  6.9551e-06,  ..., -1.4178e-05,
         -7.3947e-06, -1.0826e-05],
        [ 3.7816e-04,  2.6215e-04, -1.3801e-04,  ...,  3.1706e-04,
          2.0980e-04,  1.0885e-04],
        [-3.2246e-05, -2.2918e-05,  1.5296e-05,  ..., -2.8938e-05,
         -1.4052e-05, -2.1875e-05],
        [-4.1068e-05, -2.9489e-05,  1.9252e-05,  ..., -3.6091e-05,
         -1.8157e-05, -2.5779e-05]], device='cuda:0')
Loss: 1.0743558406829834
Graident accumulation at epoch 0, step 959, batch 959
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8703e-04,  5.2587e-04, -3.8858e-04,  ...,  4.6904e-04,
         -6.4253e-04, -8.0024e-05],
        [-1.3741e-05, -4.1566e-06,  3.8371e-06,  ..., -8.5359e-06,
         -4.8342e-06, -7.5548e-06],
        [ 7.0997e-05,  4.8011e-05, -2.7746e-05,  ...,  6.4795e-05,
          3.3230e-05,  2.6128e-05],
        [-2.4291e-05, -1.7987e-05,  1.1140e-05,  ..., -2.1299e-05,
         -9.1765e-06, -1.6981e-05],
        [-2.9365e-05, -2.1004e-05,  1.4423e-05,  ..., -2.4382e-05,
         -1.0316e-05, -2.0046e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5963e-08, 4.4701e-08, 2.6375e-08,  ..., 3.5680e-08, 5.9362e-08,
         1.4047e-08],
        [5.9582e-11, 4.2465e-11, 5.8972e-12,  ..., 4.8925e-11, 8.1980e-12,
         1.7624e-11],
        [2.6018e-09, 1.2583e-09, 3.1047e-10,  ..., 2.0302e-09, 2.7998e-10,
         7.3975e-10],
        [7.1266e-10, 5.0328e-10, 6.8731e-11,  ..., 5.8678e-10, 7.3988e-11,
         2.2637e-10],
        [3.3256e-10, 1.7567e-10, 3.0996e-11,  ..., 2.4274e-10, 4.6709e-11,
         8.0892e-11]], device='cuda:0')
optimizer state dict: 120.0
lr: [3.6752822198246384e-07, 3.6752822198246384e-07]
scheduler_last_epoch: 120
Epoch 0 | Batch 959/1048 | Training PPL: 5403.740756796401 | time 103.80038547515869
Saving checkpoint at epoch 0, step 959, batch 959
Epoch 0 | Validation PPL: 8.522219659204813 | Learning rate: 3.6752822198246384e-07
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_959, AFTER epoch 0, step 959


Running epoch 0, step 960, batch 960
Sampled inputs[:2]: tensor([[    0,  1943,   300,  ..., 43803,   368,  2400],
        [    0,  7110,   278,  ...,    66,    13,  9070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6774e-05,  4.3203e-05, -2.5320e-05,  ...,  1.9381e-04,
         -1.6087e-04,  2.3311e-06],
        [-1.8626e-06, -1.3858e-06,  7.8604e-07,  ..., -1.6764e-06,
         -6.9663e-07, -1.2964e-06],
        [-2.7865e-06, -2.0713e-06,  1.2740e-06,  ..., -2.3991e-06,
         -8.8662e-07, -1.7956e-06],
        [-3.8743e-06, -2.8759e-06,  1.7881e-06,  ..., -3.3975e-06,
         -1.2442e-06, -2.5928e-06],
        [-4.9770e-06, -3.7104e-06,  2.2799e-06,  ..., -4.2617e-06,
         -1.6615e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.0567461252212524


Running epoch 0, step 961, batch 961
Sampled inputs[:2]: tensor([[    0, 32444,    41,  ...,    14,    18,    59],
        [    0,  2738,   278,  ...,   292,    35,  2147]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9910e-04, -3.0861e-04,  1.3038e-04,  ...,  7.7382e-05,
          1.2816e-04,  3.2352e-04],
        [-3.9786e-06, -2.9355e-06,  1.7397e-06,  ..., -3.4645e-06,
         -1.6503e-06, -2.6375e-06],
        [ 1.9762e-04,  1.3031e-04, -4.1547e-05,  ...,  1.1372e-04,
          9.9152e-05,  7.5044e-05],
        [-7.8678e-06, -5.8115e-06,  3.6657e-06,  ..., -6.7651e-06,
         -2.9355e-06, -5.0962e-06],
        [-1.0550e-05, -7.8231e-06,  4.8727e-06,  ..., -8.9407e-06,
         -4.1053e-06, -6.3777e-06]], device='cuda:0')
Loss: 1.1002845764160156


Running epoch 0, step 962, batch 962
Sampled inputs[:2]: tensor([[   0, 6945, 2360,  ...,   30,  413,   16],
        [   0,  726, 3979,  ...,   27, 2085,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5954e-04, -4.9832e-04,  2.5573e-04,  ..., -1.4509e-04,
          6.4853e-04,  6.3064e-04],
        [-6.0201e-06, -4.4033e-06,  2.5444e-06,  ..., -5.2005e-06,
         -2.6487e-06, -4.0978e-06],
        [ 1.9459e-04,  1.2812e-04, -4.0281e-05,  ...,  1.1121e-04,
          9.7796e-05,  7.2987e-05],
        [-1.1921e-05, -8.7172e-06,  5.3719e-06,  ..., -1.0177e-05,
         -4.7609e-06, -7.9274e-06],
        [-1.6004e-05, -1.1817e-05,  7.1675e-06,  ..., -1.3471e-05,
         -6.6236e-06, -9.9540e-06]], device='cuda:0')
Loss: 1.0643259286880493


Running epoch 0, step 963, batch 963
Sampled inputs[:2]: tensor([[   0, 1823,   12,  ..., 1874,  271,  266],
        [   0, 5522, 5662,  ...,  638, 1231, 1098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3723e-04, -5.6973e-04,  2.5736e-04,  ..., -1.6997e-04,
          6.2803e-04,  7.1013e-04],
        [-8.0615e-06, -5.9158e-06,  3.4720e-06,  ..., -6.9365e-06,
         -3.5167e-06, -5.3197e-06],
        [ 1.9155e-04,  1.2585e-04, -3.8858e-05,  ...,  1.0865e-04,
          9.6596e-05,  7.1214e-05],
        [-1.6123e-05, -1.1817e-05,  7.3537e-06,  ..., -1.3724e-05,
         -6.4000e-06, -1.0431e-05],
        [-2.1517e-05, -1.5900e-05,  9.7305e-06,  ..., -1.8120e-05,
         -8.8885e-06, -1.3083e-05]], device='cuda:0')
Loss: 1.080272912979126


Running epoch 0, step 964, batch 964
Sampled inputs[:2]: tensor([[   0,  874,  590,  ...,  300,  867,  638],
        [   0,  706, 1005,  ...,  278,  266, 5590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3191e-04, -3.4326e-04,  1.1897e-04,  ..., -2.8144e-04,
          5.7935e-04,  7.3164e-04],
        [-9.8720e-06, -7.2643e-06,  4.2878e-06,  ..., -8.7246e-06,
         -4.5225e-06, -6.7353e-06],
        [ 1.8895e-04,  1.2390e-04, -3.7598e-05,  ...,  1.0619e-04,
          9.5322e-05,  6.9329e-05],
        [-1.9878e-05, -1.4603e-05,  9.1866e-06,  ..., -1.7345e-05,
         -8.2776e-06, -1.3277e-05],
        [-2.6166e-05, -1.9386e-05,  1.1966e-05,  ..., -2.2441e-05,
         -1.1228e-05, -1.6287e-05]], device='cuda:0')
Loss: 1.0807671546936035


Running epoch 0, step 965, batch 965
Sampled inputs[:2]: tensor([[    0,  5750,   642,  ...,   221, 15441,   644],
        [    0,  1921,   843,  ...,  9420,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0115e-04, -3.2582e-04,  3.1537e-05,  ..., -1.3146e-04,
          4.5831e-04,  5.5670e-04],
        [-1.1913e-05, -8.7023e-06,  5.1744e-06,  ..., -1.0476e-05,
         -5.2229e-06, -7.9572e-06],
        [ 1.8589e-04,  1.2173e-04, -3.6213e-05,  ...,  1.0358e-04,
          9.4354e-05,  6.7563e-05],
        [-2.4021e-05, -1.7524e-05,  1.1079e-05,  ..., -2.0891e-05,
         -9.5814e-06, -1.5751e-05],
        [-3.1680e-05, -2.3320e-05,  1.4469e-05,  ..., -2.7120e-05,
         -1.3053e-05, -1.9386e-05]], device='cuda:0')
Loss: 1.0845081806182861


Running epoch 0, step 966, batch 966
Sampled inputs[:2]: tensor([[    0,  4653, 21419,  ...,  7845,   300,   565],
        [    0,  2027,   365,  ...,   368,  1782,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4070e-04,  1.2816e-04,  2.8345e-05,  ..., -6.6695e-05,
          4.3077e-04,  5.9017e-04],
        [-1.4104e-05, -1.0200e-05,  5.9940e-06,  ..., -1.2264e-05,
         -6.3330e-06, -9.3654e-06],
        [ 1.8258e-04,  1.1943e-04, -3.4901e-05,  ...,  1.0095e-04,
          9.2789e-05,  6.5552e-05],
        [ 2.8219e-04,  1.9490e-04, -1.9400e-04,  ...,  2.4325e-04,
          1.4022e-04,  1.8116e-04],
        [-3.7581e-05, -2.7403e-05,  1.6838e-05,  ..., -3.1799e-05,
         -1.5870e-05, -2.2843e-05]], device='cuda:0')
Loss: 1.0606012344360352


Running epoch 0, step 967, batch 967
Sampled inputs[:2]: tensor([[    0,  2383,  9843,  ...,   401,  3959,   300],
        [    0,   944,   278,  ..., 17330,  1683,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2340e-04,  1.6967e-04,  1.0006e-04,  ...,  1.8117e-05,
          2.6225e-04,  5.6516e-04],
        [-1.6086e-05, -1.1571e-05,  6.8620e-06,  ..., -1.3985e-05,
         -7.0743e-06, -1.0639e-05],
        [ 1.7959e-04,  1.1733e-04, -3.3538e-05,  ...,  9.8382e-05,
          9.1753e-05,  6.3689e-05],
        [ 2.7792e-04,  1.9194e-04, -1.9204e-04,  ...,  2.3956e-04,
          1.3875e-04,  1.7842e-04],
        [-4.2826e-05, -3.1084e-05,  1.9222e-05,  ..., -3.6299e-05,
         -1.7777e-05, -2.6003e-05]], device='cuda:0')
Loss: 1.0767896175384521
Graident accumulation at epoch 0, step 967, batch 967
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0599e-04,  4.9025e-04, -3.3972e-04,  ...,  4.2395e-04,
         -5.5206e-04, -1.5506e-05],
        [-1.3975e-05, -4.8980e-06,  4.1396e-06,  ..., -9.0808e-06,
         -5.0582e-06, -7.8633e-06],
        [ 8.1856e-05,  5.4943e-05, -2.8325e-05,  ...,  6.8153e-05,
          3.9082e-05,  2.9884e-05],
        [ 5.9309e-06,  3.0056e-06, -9.1774e-06,  ...,  4.7867e-06,
          5.6165e-06,  2.5587e-06],
        [-3.0711e-05, -2.2012e-05,  1.4903e-05,  ..., -2.5574e-05,
         -1.1062e-05, -2.0642e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6295e-08, 4.4685e-08, 2.6359e-08,  ..., 3.5645e-08, 5.9372e-08,
         1.4352e-08],
        [5.9781e-11, 4.2557e-11, 5.9384e-12,  ..., 4.9072e-11, 8.2398e-12,
         1.7720e-11],
        [2.6315e-09, 1.2708e-09, 3.1128e-10,  ..., 2.0378e-09, 2.8812e-10,
         7.4307e-10],
        [7.8919e-10, 5.3962e-10, 1.0554e-10,  ..., 6.4358e-10, 9.3167e-11,
         2.5797e-10],
        [3.3406e-10, 1.7646e-10, 3.1334e-11,  ..., 2.4381e-10, 4.6979e-11,
         8.1488e-11]], device='cuda:0')
optimizer state dict: 121.0
lr: [3.040677782773405e-07, 3.040677782773405e-07]
scheduler_last_epoch: 121


Running epoch 0, step 968, batch 968
Sampled inputs[:2]: tensor([[    0,    12,  6426,  ...,  2629, 13422,    12],
        [    0,   287,   298,  ..., 14121,  3121,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0966e-04,  5.0938e-04, -1.9545e-04,  ...,  3.4360e-04,
         -5.1554e-04, -1.1614e-04],
        [-1.8850e-06, -1.3188e-06,  8.3447e-07,  ..., -1.6913e-06,
         -9.2760e-07, -1.3933e-06],
        [-2.8610e-06, -2.0117e-06,  1.3262e-06,  ..., -2.4736e-06,
         -1.2815e-06, -1.9670e-06],
        [-3.8743e-06, -2.6971e-06,  1.8403e-06,  ..., -3.4124e-06,
         -1.7583e-06, -2.7716e-06],
        [-5.0366e-06, -3.5465e-06,  2.3395e-06,  ..., -4.3213e-06,
         -2.3097e-06, -3.3081e-06]], device='cuda:0')
Loss: 1.0574871301651


Running epoch 0, step 969, batch 969
Sampled inputs[:2]: tensor([[    0,  4878,   607,  ...,    14, 17331,   287],
        [    0,    47,    12,  ...,  4367,   278,   471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1757e-04,  5.0309e-04, -1.3090e-04,  ...,  3.5384e-04,
         -4.6651e-04, -1.7825e-04],
        [-3.8967e-06, -2.7567e-06,  1.6913e-06,  ..., -3.4720e-06,
         -1.7285e-06, -2.6450e-06],
        [-5.8860e-06, -4.1872e-06,  2.6822e-06,  ..., -5.0962e-06,
         -2.3991e-06, -3.7700e-06],
        [-7.9870e-06, -5.6326e-06,  3.6880e-06,  ..., -7.0184e-06,
         -3.2783e-06, -5.3048e-06],
        [-1.0371e-05, -7.3910e-06,  4.7386e-06,  ..., -8.9407e-06,
         -4.3362e-06, -6.3777e-06]], device='cuda:0')
Loss: 1.098547101020813


Running epoch 0, step 970, batch 970
Sampled inputs[:2]: tensor([[   0,   12,  895,  ...,   13, 2900,   14],
        [   0, 7110,  437,  ...,  266, 6724, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5003e-05,  5.4425e-04, -1.2703e-04,  ...,  2.9677e-04,
         -4.7410e-04, -9.7261e-05],
        [-5.8189e-06, -4.1947e-06,  2.6524e-06,  ..., -5.2378e-06,
         -2.5034e-06, -3.9637e-06],
        [-8.7470e-06, -6.3330e-06,  4.1500e-06,  ..., -7.6592e-06,
         -3.4571e-06, -5.6326e-06],
        [-1.2040e-05, -8.6576e-06,  5.8040e-06,  ..., -1.0729e-05,
         -4.7758e-06, -8.0466e-06],
        [-1.5527e-05, -1.1265e-05,  7.3761e-06,  ..., -1.3560e-05,
         -6.3181e-06, -9.6262e-06]], device='cuda:0')
Loss: 1.087874412536621


Running epoch 0, step 971, batch 971
Sampled inputs[:2]: tensor([[    0, 26138,    17,  ...,   401,  1867,  4977],
        [    0,  3412,  1707,  ..., 11114,    15,  1821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9920e-05,  4.5138e-04, -9.9218e-05,  ...,  2.5826e-04,
         -5.2085e-04, -2.0469e-04],
        [-7.8157e-06, -5.5432e-06,  3.5502e-06,  ..., -6.9886e-06,
         -3.3602e-06, -5.2303e-06],
        [-1.1683e-05, -8.3447e-06,  5.5283e-06,  ..., -1.0192e-05,
         -4.6343e-06, -7.4208e-06],
        [-1.6093e-05, -1.1429e-05,  7.7412e-06,  ..., -1.4275e-05,
         -6.4075e-06, -1.0595e-05],
        [-2.0891e-05, -1.4961e-05,  9.8944e-06,  ..., -1.8179e-05,
         -8.5235e-06, -1.2770e-05]], device='cuda:0')
Loss: 1.0952577590942383


Running epoch 0, step 972, batch 972
Sampled inputs[:2]: tensor([[    0,  5160,   278,  ...,   496,    14, 46919],
        [    0,    12,   344,  ...,  2337,  1122,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5077e-05,  5.9270e-04, -2.5362e-04,  ...,  5.1998e-04,
         -7.4609e-04, -1.9920e-04],
        [-9.8124e-06, -6.9290e-06,  4.4666e-06,  ..., -8.7321e-06,
         -4.0978e-06, -6.4820e-06],
        [-1.4618e-05, -1.0401e-05,  6.9290e-06,  ..., -1.2696e-05,
         -5.6103e-06, -9.1568e-06],
        [-2.0325e-05, -1.4380e-05,  9.7826e-06,  ..., -1.7956e-05,
         -7.8231e-06, -1.3202e-05],
        [-2.6196e-05, -1.8671e-05,  1.2428e-05,  ..., -2.2680e-05,
         -1.0356e-05, -1.5795e-05]], device='cuda:0')
Loss: 1.0985523462295532


Running epoch 0, step 973, batch 973
Sampled inputs[:2]: tensor([[    0,   301,   298,  ...,   806,   352, 22105],
        [    0,   767,  4478,  ...,   278,   266, 19201]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0424e-04,  5.7527e-04, -2.7629e-04,  ...,  4.5515e-04,
         -8.1419e-04, -1.0873e-04],
        [-1.1779e-05, -8.2999e-06,  5.3197e-06,  ..., -1.0483e-05,
         -4.9695e-06, -7.8380e-06],
        [-1.7628e-05, -1.2517e-05,  8.3297e-06,  ..., -1.5303e-05,
         -6.8322e-06, -1.1109e-05],
        [-2.4319e-05, -1.7166e-05,  1.1645e-05,  ..., -2.1458e-05,
         -9.4324e-06, -1.5885e-05],
        [-3.1471e-05, -2.2396e-05,  1.4886e-05,  ..., -2.7239e-05,
         -1.2562e-05, -1.9073e-05]], device='cuda:0')
Loss: 1.0567063093185425


Running epoch 0, step 974, batch 974
Sampled inputs[:2]: tensor([[   0, 1270,  413,  ...,  413,  711,   14],
        [   0, 7692,   12,  ...,  266, 2042,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0563e-04,  9.8627e-04, -4.6224e-04,  ...,  7.1274e-04,
         -8.1419e-04, -3.9161e-04],
        [-1.3776e-05, -9.6709e-06,  6.1654e-06,  ..., -1.2226e-05,
         -5.8226e-06, -9.1940e-06],
        [-2.0713e-05, -1.4663e-05,  9.7007e-06,  ..., -1.7911e-05,
         -8.0168e-06, -1.3061e-05],
        [-2.8491e-05, -2.0042e-05,  1.3568e-05,  ..., -2.5079e-05,
         -1.1072e-05, -1.8686e-05],
        [-3.6776e-05, -2.6092e-05,  1.7256e-05,  ..., -3.1680e-05,
         -1.4663e-05, -2.2262e-05]], device='cuda:0')
Loss: 1.0638999938964844


Running epoch 0, step 975, batch 975
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,    20, 13016,  ...,    14,  2743,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7136e-04,  1.1395e-03, -4.8407e-04,  ...,  7.9221e-04,
         -6.6088e-04, -3.1221e-04],
        [-1.5594e-05, -1.0937e-05,  6.9067e-06,  ..., -1.4007e-05,
         -6.9402e-06, -1.0699e-05],
        [-2.3469e-05, -1.6615e-05,  1.0945e-05,  ..., -2.0444e-05,
         -9.4697e-06, -1.5132e-05],
        [-3.2187e-05, -2.2620e-05,  1.5244e-05,  ..., -2.8551e-05,
         -1.3068e-05, -2.1577e-05],
        [-4.1604e-05, -2.9519e-05,  1.9446e-05,  ..., -3.6061e-05,
         -1.7256e-05, -2.5690e-05]], device='cuda:0')
Loss: 1.0651148557662964
Graident accumulation at epoch 0, step 975, batch 975
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1252e-04,  5.5518e-04, -3.5415e-04,  ...,  4.6078e-04,
         -5.6294e-04, -4.5176e-05],
        [-1.4137e-05, -5.5019e-06,  4.4163e-06,  ..., -9.5734e-06,
         -5.2464e-06, -8.1469e-06],
        [ 7.1323e-05,  4.7787e-05, -2.4398e-05,  ...,  5.9294e-05,
          3.4227e-05,  2.5382e-05],
        [ 2.1192e-06,  4.4302e-07, -6.7353e-06,  ...,  1.4530e-06,
          3.7480e-06,  1.4513e-07],
        [-3.1801e-05, -2.2763e-05,  1.5357e-05,  ..., -2.6623e-05,
         -1.1681e-05, -2.1147e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6268e-08, 4.5939e-08, 2.6567e-08,  ..., 3.6237e-08, 5.9749e-08,
         1.4435e-08],
        [5.9964e-11, 4.2634e-11, 5.9802e-12,  ..., 4.9219e-11, 8.2798e-12,
         1.7817e-11],
        [2.6294e-09, 1.2698e-09, 3.1109e-10,  ..., 2.0362e-09, 2.8792e-10,
         7.4255e-10],
        [7.8943e-10, 5.3959e-10, 1.0567e-10,  ..., 6.4375e-10, 9.3245e-11,
         2.5818e-10],
        [3.3546e-10, 1.7715e-10, 3.1681e-11,  ..., 2.4487e-10, 4.7229e-11,
         8.2066e-11]], device='cuda:0')
optimizer state dict: 122.0
lr: [2.465336004891461e-07, 2.465336004891461e-07]
scheduler_last_epoch: 122


Running epoch 0, step 976, batch 976
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  381, 3513, 1501],
        [   0,  560,  199,  ...,   29,  445,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0624e-05, -2.2418e-06, -4.4549e-05,  ..., -7.1267e-06,
          2.3686e-04,  1.0694e-04],
        [-2.0415e-06, -1.4082e-06,  8.1211e-07,  ..., -1.7509e-06,
         -9.2387e-07, -1.3933e-06],
        [ 9.0710e-04,  6.8554e-04, -3.0008e-04,  ...,  7.3051e-04,
          2.0291e-04,  4.1575e-04],
        [-4.1425e-06, -2.8610e-06,  1.7509e-06,  ..., -3.5018e-06,
         -1.7360e-06, -2.7418e-06],
        [-5.3346e-06, -3.7700e-06,  2.2948e-06,  ..., -4.5002e-06,
         -2.2948e-06, -3.3081e-06]], device='cuda:0')
Loss: 1.081059455871582


Running epoch 0, step 977, batch 977
Sampled inputs[:2]: tensor([[    0,   278, 11554,  ...,  4713,  1039, 17088],
        [    0,    13,  4831,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7387e-05,  3.6998e-05, -1.7556e-04,  ...,  4.4735e-05,
          1.7105e-04,  1.4723e-04],
        [-3.9339e-06, -2.8610e-06,  1.6876e-06,  ..., -3.4645e-06,
         -1.7993e-06, -2.7716e-06],
        [ 9.0424e-04,  6.8335e-04, -2.9869e-04,  ...,  7.2801e-04,
          2.0172e-04,  4.1382e-04],
        [-7.9274e-06, -5.7518e-06,  3.6210e-06,  ..., -6.8694e-06,
         -3.3155e-06, -5.4240e-06],
        [-1.0371e-05, -7.6145e-06,  4.7386e-06,  ..., -8.9109e-06,
         -4.4852e-06, -6.5863e-06]], device='cuda:0')
Loss: 1.0699301958084106


Running epoch 0, step 978, batch 978
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   352, 10572,   345],
        [    0,  2328,   271,  ...,   706,    13,  8961]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7532e-05, -7.1288e-05, -2.8404e-04,  ...,  1.4259e-05,
          2.8358e-04,  1.3807e-04],
        [-5.8413e-06, -4.2096e-06,  2.5593e-06,  ..., -5.1633e-06,
         -2.7381e-06, -4.1649e-06],
        [ 9.0144e-04,  6.8136e-04, -2.9735e-04,  ...,  7.2560e-04,
          2.0049e-04,  4.1192e-04],
        [-1.1861e-05, -8.5086e-06,  5.5209e-06,  ..., -1.0282e-05,
         -5.0440e-06, -8.1956e-06],
        [-1.5497e-05, -1.1295e-05,  7.1973e-06,  ..., -1.3322e-05,
         -6.8247e-06, -9.9242e-06]], device='cuda:0')
Loss: 1.0650527477264404


Running epoch 0, step 979, batch 979
Sampled inputs[:2]: tensor([[   0,  298, 2230,  ..., 2300, 3698, 4764],
        [   0, 9058, 4048,  ...,   14,  759, 1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0265e-05, -1.2961e-04, -1.7222e-04,  ..., -1.1458e-04,
          3.6850e-04,  2.2197e-04],
        [-7.7784e-06, -5.5879e-06,  3.4124e-06,  ..., -6.9737e-06,
         -3.6769e-06, -5.5581e-06],
        [ 8.9856e-04,  6.7927e-04, -2.9601e-04,  ...,  7.2297e-04,
          1.9921e-04,  4.0995e-04],
        [-1.5736e-05, -1.1295e-05,  7.3537e-06,  ..., -1.3873e-05,
         -6.7577e-06, -1.0923e-05],
        [-2.0534e-05, -1.4946e-05,  9.5367e-06,  ..., -1.7911e-05,
         -9.1195e-06, -1.3202e-05]], device='cuda:0')
Loss: 1.0748624801635742


Running epoch 0, step 980, batch 980
Sampled inputs[:2]: tensor([[   0,  391, 1761,  ...,  346,   14,  292],
        [   0,  560,  199,  ...,  266, 1371, 4811]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3277e-04, -4.9104e-04, -7.6750e-05,  ..., -3.5503e-04,
          5.1542e-04,  2.5484e-04],
        [-9.7752e-06, -7.0035e-06,  4.2692e-06,  ..., -8.7246e-06,
         -4.5598e-06, -6.8918e-06],
        [ 8.9566e-04,  6.7717e-04, -2.9469e-04,  ...,  7.2046e-04,
          1.9802e-04,  4.0809e-04],
        [-1.9759e-05, -1.4171e-05,  9.1940e-06,  ..., -1.7390e-05,
         -8.4043e-06, -1.3575e-05],
        [-2.5719e-05, -1.8716e-05,  1.1906e-05,  ..., -2.2411e-05,
         -1.1310e-05, -1.6406e-05]], device='cuda:0')
Loss: 1.0878331661224365


Running epoch 0, step 981, batch 981
Sampled inputs[:2]: tensor([[   0, 1781,  659,  ...,   12, 1478,   14],
        [   0,   13,  711,  ...,  591,  953,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0604e-04, -4.7148e-04,  2.5977e-06,  ..., -3.8478e-04,
          4.5598e-04,  4.0770e-04],
        [-1.1727e-05, -8.4192e-06,  5.0776e-06,  ..., -1.0446e-05,
         -5.5805e-06, -8.2478e-06],
        [ 8.9262e-04,  6.7495e-04, -2.9333e-04,  ...,  7.1786e-04,
          1.9658e-04,  4.0614e-04],
        [-2.3752e-05, -1.7062e-05,  1.0982e-05,  ..., -2.0847e-05,
         -1.0312e-05, -1.6242e-05],
        [-3.0965e-05, -2.2560e-05,  1.4275e-05,  ..., -2.6852e-05,
         -1.3843e-05, -1.9655e-05]], device='cuda:0')
Loss: 1.0704982280731201


Running epoch 0, step 982, batch 982
Sampled inputs[:2]: tensor([[    0,   759,  4585,  ...,   360,   300,   670],
        [    0,  1278,    69,  ...,    15,  7377, 20524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9957e-04, -3.3146e-04, -1.1662e-04,  ..., -1.9209e-04,
          2.4379e-04,  3.1257e-04],
        [-1.3649e-05, -9.7975e-06,  5.9865e-06,  ..., -1.2144e-05,
         -6.3851e-06, -9.4473e-06],
        [ 8.8976e-04,  6.7286e-04, -2.9192e-04,  ...,  7.1536e-04,
          1.9545e-04,  4.0443e-04],
        [-2.7686e-05, -1.9908e-05,  1.2949e-05,  ..., -2.4334e-05,
         -1.1861e-05, -1.8671e-05],
        [-3.6120e-05, -2.6315e-05,  1.6823e-05,  ..., -3.1352e-05,
         -1.5929e-05, -2.2635e-05]], device='cuda:0')
Loss: 1.0852868556976318


Running epoch 0, step 983, batch 983
Sampled inputs[:2]: tensor([[   0,  635,   13,  ...,  292,   20,  445],
        [   0, 9029,  634,  ..., 1424, 6872,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1246e-04, -3.4279e-04, -8.1922e-05,  ..., -2.2897e-04,
          2.4810e-04,  5.0436e-04],
        [-1.5430e-05, -1.1072e-05,  6.7689e-06,  ..., -1.3858e-05,
         -7.2606e-06, -1.0878e-05],
        [ 8.8703e-04,  6.7090e-04, -2.9063e-04,  ...,  7.1287e-04,
          1.9427e-04,  4.0243e-04],
        [-3.1441e-05, -2.2590e-05,  1.4745e-05,  ..., -2.7820e-05,
         -1.3493e-05, -2.1547e-05],
        [-4.0978e-05, -2.9817e-05,  1.9133e-05,  ..., -3.5733e-05,
         -1.8045e-05, -2.6017e-05]], device='cuda:0')
Loss: 1.0532376766204834
Graident accumulation at epoch 0, step 983, batch 983
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.0027e-05,  4.6538e-04, -3.2693e-04,  ...,  3.9180e-04,
         -4.8183e-04,  9.7775e-06],
        [-1.4266e-05, -6.0589e-06,  4.6516e-06,  ..., -1.0002e-05,
         -5.4478e-06, -8.4200e-06],
        [ 1.5289e-04,  1.1010e-04, -5.1021e-05,  ...,  1.2465e-04,
          5.0231e-05,  6.3087e-05],
        [-1.2369e-06, -1.8603e-06, -4.5873e-06,  ..., -1.4744e-06,
          2.0239e-06, -2.0241e-06],
        [-3.2718e-05, -2.3468e-05,  1.5735e-05,  ..., -2.7534e-05,
         -1.2318e-05, -2.1634e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6257e-08, 4.6011e-08, 2.6547e-08,  ..., 3.6253e-08, 5.9751e-08,
         1.4675e-08],
        [6.0142e-11, 4.2714e-11, 6.0200e-12,  ..., 4.9362e-11, 8.3242e-12,
         1.7917e-11],
        [3.4136e-09, 1.7186e-09, 3.9524e-10,  ..., 2.5424e-09, 3.2538e-10,
         9.0376e-10],
        [7.8963e-10, 5.3956e-10, 1.0578e-10,  ..., 6.4388e-10, 9.3333e-11,
         2.5839e-10],
        [3.3680e-10, 1.7787e-10, 3.2015e-11,  ..., 2.4590e-10, 4.7508e-11,
         8.2661e-11]], device='cuda:0')
optimizer state dict: 123.0
lr: [1.9496085417278542e-07, 1.9496085417278542e-07]
scheduler_last_epoch: 123


Running epoch 0, step 984, batch 984
Sampled inputs[:2]: tensor([[    0,  3615,    16,  ...,  2140,  1098,   352],
        [    0, 13081,   278,  ...,   368,   266,  1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5433e-04, -2.1325e-04, -8.8161e-05,  ...,  6.4920e-05,
          8.1022e-05, -6.1401e-06],
        [-1.9670e-06, -1.4156e-06,  8.4564e-07,  ..., -1.7807e-06,
         -8.4937e-07, -1.4082e-06],
        [-2.9206e-06, -2.1309e-06,  1.3113e-06,  ..., -2.5779e-06,
         -1.1250e-06, -1.9670e-06],
        [-4.0531e-06, -2.9206e-06,  1.8403e-06,  ..., -3.6359e-06,
         -1.5870e-06, -2.8461e-06],
        [-5.1856e-06, -3.7700e-06,  2.3246e-06,  ..., -4.5598e-06,
         -2.0713e-06, -3.3677e-06]], device='cuda:0')
Loss: 1.0789850950241089


Running epoch 0, step 985, batch 985
Sampled inputs[:2]: tensor([[    0,  1890,   278,  ...,   578,    72,   815],
        [    0,   360,  5323,  ..., 29974,    25,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4575e-04, -5.0924e-04, -1.4367e-04,  ...,  3.0819e-05,
          5.3637e-04,  2.4106e-04],
        [-3.7700e-06, -2.7642e-06,  1.6764e-06,  ..., -3.4124e-06,
         -1.6168e-06, -2.7716e-06],
        [-5.5432e-06, -4.1127e-06,  2.6077e-06,  ..., -4.8876e-06,
         -2.1011e-06, -3.8147e-06],
        [-7.9274e-06, -5.8264e-06,  3.7774e-06,  ..., -7.1079e-06,
         -3.0398e-06, -5.7369e-06],
        [-9.9242e-06, -7.3612e-06,  4.6641e-06,  ..., -8.7023e-06,
         -3.9414e-06, -6.5416e-06]], device='cuda:0')
Loss: 1.0642013549804688


Running epoch 0, step 986, batch 986
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0,   271,   259,  ...,  4511,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4716e-04, -4.0128e-04,  2.2423e-05,  ..., -8.6162e-05,
          7.8567e-04,  4.5582e-04],
        [-5.9158e-06, -4.3213e-06,  2.5220e-06,  ..., -5.2229e-06,
         -2.6822e-06, -4.1574e-06],
        [-8.6129e-06, -6.3479e-06,  3.8967e-06,  ..., -7.4059e-06,
         -3.4869e-06, -5.6624e-06],
        [-1.2130e-05, -8.8513e-06,  5.5432e-06,  ..., -1.0580e-05,
         -4.9621e-06, -8.3596e-06],
        [-1.5467e-05, -1.1384e-05,  7.0035e-06,  ..., -1.3232e-05,
         -6.4895e-06, -9.7454e-06]], device='cuda:0')
Loss: 1.0815541744232178


Running epoch 0, step 987, batch 987
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,    13,   635,    13],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9515e-04, -5.5133e-04,  5.9532e-05,  ..., -3.7566e-05,
          8.2440e-04,  4.2142e-04],
        [-7.7337e-06, -5.6475e-06,  3.3043e-06,  ..., -6.9663e-06,
         -3.6433e-06, -5.5432e-06],
        [-1.1310e-05, -8.3148e-06,  5.1707e-06,  ..., -9.8199e-06,
         -4.6939e-06, -7.4953e-06],
        [-1.5914e-05, -1.1593e-05,  7.3537e-06,  ..., -1.4067e-05,
         -6.7055e-06, -1.1057e-05],
        [-2.0325e-05, -1.4931e-05,  9.3132e-06,  ..., -1.7554e-05,
         -8.7246e-06, -1.2890e-05]], device='cuda:0')
Loss: 1.0675526857376099


Running epoch 0, step 988, batch 988
Sampled inputs[:2]: tensor([[    0,    12,   297,  ...,  2980,  1145, 17207],
        [    0,   257,   221,  ...,  1474,  2044,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9806e-03, -3.8106e-03,  1.0387e-03,  ..., -2.2553e-03,
          5.3363e-03,  4.7020e-04],
        [-9.3132e-06, -6.5342e-06,  3.9451e-06,  ..., -8.8066e-06,
         -4.9472e-06, -7.3537e-06],
        [-1.3605e-05, -9.7007e-06,  6.2287e-06,  ..., -1.2219e-05,
         -6.2361e-06, -9.7007e-06],
        [-1.9118e-05, -1.3463e-05,  8.8513e-06,  ..., -1.7479e-05,
         -8.8960e-06, -1.4246e-05],
        [-2.4438e-05, -1.7494e-05,  1.1213e-05,  ..., -2.1636e-05,
         -1.1362e-05, -1.6406e-05]], device='cuda:0')
Loss: 1.0701202154159546


Running epoch 0, step 989, batch 989
Sampled inputs[:2]: tensor([[    0,   221,   451,  ...,   741, 25712,   950],
        [    0,  4823,    12,  ...,  1756,  3406,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9551e-03, -5.8398e-03,  1.2228e-03,  ..., -3.3514e-03,
          7.6086e-03,  2.3411e-03],
        [-1.0855e-05, -7.5921e-06,  4.6380e-06,  ..., -1.0647e-05,
         -6.1393e-06, -9.0227e-06],
        [ 2.4647e-04,  2.0214e-04, -1.3977e-04,  ...,  1.3339e-04,
          1.6545e-04,  1.0497e-04],
        [-2.2516e-05, -1.5773e-05,  1.0543e-05,  ..., -2.1219e-05,
         -1.1072e-05, -1.7524e-05],
        [-2.8551e-05, -2.0400e-05,  1.3210e-05,  ..., -2.5779e-05,
         -1.3761e-05, -1.9714e-05]], device='cuda:0')
Loss: 1.0682103633880615


Running epoch 0, step 990, batch 990
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,  271, 4111, 1188],
        [   0,   13, 6913,  ...,  278, 1317, 4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8809e-03, -5.7153e-03,  1.2172e-03,  ..., -3.2247e-03,
          7.4613e-03,  2.4044e-03],
        [-1.2659e-05, -8.8811e-06,  5.4054e-06,  ..., -1.2442e-05,
         -7.0706e-06, -1.0267e-05],
        [ 2.4385e-04,  2.0024e-04, -1.3857e-04,  ...,  1.3090e-04,
          1.6428e-04,  1.0331e-04],
        [-2.6330e-05, -1.8500e-05,  1.2286e-05,  ..., -2.4945e-05,
         -1.2830e-05, -2.0072e-05],
        [-3.3319e-05, -2.3857e-05,  1.5400e-05,  ..., -3.0249e-05,
         -1.5952e-05, -2.2605e-05]], device='cuda:0')
Loss: 1.0843735933303833


Running epoch 0, step 991, batch 991
Sampled inputs[:2]: tensor([[    0,  3611, 10765,  ...,   271,  4317,    13],
        [    0,   278,   266,  ...,  5503,   259,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4944e-03, -5.7430e-03,  1.2408e-03,  ..., -3.7481e-03,
          8.4305e-03,  2.8958e-03],
        [-1.4514e-05, -1.0096e-05,  6.1989e-06,  ..., -1.4246e-05,
         -8.2329e-06, -1.1921e-05],
        [ 2.4106e-04,  1.9837e-04, -1.3727e-04,  ...,  1.2832e-04,
          1.6273e-04,  1.0104e-04],
        [-3.0041e-05, -2.0958e-05,  1.4029e-05,  ..., -2.8417e-05,
         -1.4856e-05, -2.3142e-05],
        [-3.8236e-05, -2.7180e-05,  1.7710e-05,  ..., -3.4720e-05,
         -1.8679e-05, -2.6330e-05]], device='cuda:0')
Loss: 1.066532850265503
Graident accumulation at epoch 0, step 991, batch 991
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.7742e-04, -1.5546e-04, -1.7015e-04,  ..., -2.2191e-05,
          4.0940e-04,  2.9838e-04],
        [-1.4291e-05, -6.4626e-06,  4.8063e-06,  ..., -1.0426e-05,
         -5.7263e-06, -8.7701e-06],
        [ 1.6171e-04,  1.1893e-04, -5.9645e-05,  ...,  1.2502e-04,
          6.1482e-05,  6.6883e-05],
        [-4.1173e-06, -3.7701e-06, -2.7256e-06,  ..., -4.1686e-06,
          3.3587e-07, -4.1358e-06],
        [-3.3270e-05, -2.3839e-05,  1.5932e-05,  ..., -2.8252e-05,
         -1.2954e-05, -2.2103e-05]], device='cuda:0')
optimizer state dict: tensor([[6.8412e-08, 7.8947e-08, 2.8060e-08,  ..., 5.0265e-08, 1.3076e-07,
         2.3046e-08],
        [6.0293e-11, 4.2773e-11, 6.0524e-12,  ..., 4.9516e-11, 8.3836e-12,
         1.8041e-11],
        [3.4683e-09, 1.7563e-09, 4.1369e-10,  ..., 2.5563e-09, 3.5153e-10,
         9.1307e-10],
        [7.8974e-10, 5.3946e-10, 1.0587e-10,  ..., 6.4404e-10, 9.3461e-11,
         2.5867e-10],
        [3.3793e-10, 1.7843e-10, 3.2297e-11,  ..., 2.4686e-10, 4.7809e-11,
         8.3272e-11]], device='cuda:0')
optimizer state dict: 124.0
lr: [1.493810611872959e-07, 1.493810611872959e-07]
scheduler_last_epoch: 124


Running epoch 0, step 992, batch 992
Sampled inputs[:2]: tensor([[    0,  2258, 10315,  ...,  4185,  9433,   221],
        [    0,    17,  4110,  ...,   287,  7115,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6110e-05,  6.9776e-05,  2.6008e-05,  ...,  1.1317e-04,
         -1.3656e-04, -2.8903e-05],
        [-1.9372e-06, -1.4082e-06,  9.3132e-07,  ..., -1.6764e-06,
         -7.1526e-07, -1.1399e-06],
        [-2.9504e-06, -2.1458e-06,  1.4529e-06,  ..., -2.5183e-06,
         -1.0207e-06, -1.6838e-06],
        [-4.2021e-06, -3.0249e-06,  2.0862e-06,  ..., -3.6359e-06,
         -1.4380e-06, -2.4587e-06],
        [-5.2452e-06, -3.8147e-06,  2.5630e-06,  ..., -4.5002e-06,
         -1.8924e-06, -2.9057e-06]], device='cuda:0')
Loss: 1.1119986772537231


Running epoch 0, step 993, batch 993
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,  266, 4505,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3092e-04, -2.6700e-04, -1.0827e-04,  ..., -1.4020e-04,
          3.1535e-04,  5.4185e-05],
        [-3.7476e-06, -2.8312e-06,  1.8813e-06,  ..., -3.3304e-06,
         -1.4156e-06, -2.3618e-06],
        [-5.6326e-06, -4.2468e-06,  2.9281e-06,  ..., -4.8876e-06,
         -1.9595e-06, -3.3975e-06],
        [-8.1658e-06, -6.1095e-06,  4.2915e-06,  ..., -7.1824e-06,
         -2.8238e-06, -5.0515e-06],
        [-1.0133e-05, -7.6145e-06,  5.2452e-06,  ..., -8.7917e-06,
         -3.6806e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.0812580585479736


Running epoch 0, step 994, batch 994
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0, 4120,  278,  ...,  298,  273,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5426e-04, -4.7477e-04, -1.7069e-04,  ..., -2.4934e-04,
          4.0004e-04, -8.9953e-05],
        [-5.5730e-06, -4.2319e-06,  2.6971e-06,  ..., -5.0068e-06,
         -2.1495e-06, -3.6806e-06],
        [-8.3297e-06, -6.2883e-06,  4.2096e-06,  ..., -7.2420e-06,
         -2.8946e-06, -5.2005e-06],
        [-1.1966e-05, -8.9854e-06,  6.1318e-06,  ..., -1.0565e-05,
         -4.1649e-06, -7.6890e-06],
        [-1.4961e-05, -1.1265e-05,  7.5549e-06,  ..., -1.2994e-05,
         -5.4091e-06, -8.9854e-06]], device='cuda:0')
Loss: 1.0779505968093872


Running epoch 0, step 995, batch 995
Sampled inputs[:2]: tensor([[    0,  2426,   699,  ...,   221,  1551,   720],
        [    0,  6904,  6069,  ..., 17196,   471,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3997e-04, -4.9747e-04,  6.7997e-05,  ..., -3.0927e-04,
          8.3306e-04,  6.8285e-05],
        [ 5.5070e-05,  6.1095e-05, -4.3912e-05,  ...,  5.5335e-05,
          6.0344e-05,  5.8952e-05],
        [-1.1101e-05, -8.2552e-06,  5.4762e-06,  ..., -9.7752e-06,
         -4.3027e-06, -7.2718e-06],
        [-1.5691e-05, -1.1608e-05,  7.8455e-06,  ..., -1.4022e-05,
         -6.1020e-06, -1.0595e-05],
        [-1.9938e-05, -1.4812e-05,  9.8348e-06,  ..., -1.7494e-05,
         -7.9870e-06, -1.2517e-05]], device='cuda:0')
Loss: 1.0752638578414917


Running epoch 0, step 996, batch 996
Sampled inputs[:2]: tensor([[    0,  7712, 31756,  ...,   895,   360,   630],
        [    0,  5841,   328,  ...,  2051,   266,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5874e-04, -4.2979e-04,  3.4387e-05,  ..., -2.4160e-04,
          6.6271e-04,  2.9174e-05],
        [ 5.3133e-05,  5.9783e-05, -4.3011e-05,  ...,  5.3621e-05,
          5.9644e-05,  5.7730e-05],
        [-1.3962e-05, -1.0207e-05,  6.8694e-06,  ..., -1.2264e-05,
         -5.2452e-06, -8.9929e-06],
        [-1.9953e-05, -1.4484e-05,  9.9316e-06,  ..., -1.7762e-05,
         -7.5102e-06, -1.3247e-05],
        [-2.5153e-05, -1.8373e-05,  1.2368e-05,  ..., -2.1994e-05,
         -9.7752e-06, -1.5527e-05]], device='cuda:0')
Loss: 1.0717155933380127


Running epoch 0, step 997, batch 997
Sampled inputs[:2]: tensor([[   0,  953,  328,  ..., 2245,   12, 1253],
        [   0, 3408,  300,  ..., 3868,  300, 2932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5874e-04, -4.7379e-04,  3.0051e-05,  ..., -2.5957e-04,
          7.2193e-04,  7.3646e-05],
        [ 5.1233e-05,  5.8405e-05, -4.2180e-05,  ...,  5.1878e-05,
          5.8872e-05,  5.6403e-05],
        [-1.6734e-05, -1.2234e-05,  8.1435e-06,  ..., -1.4722e-05,
         -6.2585e-06, -1.0803e-05],
        [-2.3827e-05, -1.7285e-05,  1.1742e-05,  ..., -2.1249e-05,
         -8.9183e-06, -1.5870e-05],
        [-3.0249e-05, -2.2084e-05,  1.4707e-05,  ..., -2.6494e-05,
         -1.1712e-05, -1.8701e-05]], device='cuda:0')
Loss: 1.0820302963256836


Running epoch 0, step 998, batch 998
Sampled inputs[:2]: tensor([[    0,  1526,  3502,  ..., 11727,  3736,  1661],
        [    0,  2546,   300,  ...,    14,  1075,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1509e-03, -9.3707e-04,  2.0064e-04,  ..., -4.7800e-04,
          1.0916e-03,  4.0776e-04],
        [ 4.9311e-05,  5.7079e-05, -4.1442e-05,  ...,  5.0023e-05,
          5.7747e-05,  5.4861e-05],
        [-1.9506e-05, -1.4186e-05,  9.3207e-06,  ..., -1.7256e-05,
         -7.6592e-06, -1.2815e-05],
        [-2.7642e-05, -1.9953e-05,  1.3366e-05,  ..., -2.4840e-05,
         -1.0900e-05, -1.8790e-05],
        [-3.5286e-05, -2.5630e-05,  1.6868e-05,  ..., -3.1054e-05,
         -1.4320e-05, -2.2158e-05]], device='cuda:0')
Loss: 1.0731749534606934


Running epoch 0, step 999, batch 999
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0, 25409,   287,  ...,  1005,   344,  3493]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2142e-03, -9.2860e-04,  1.8297e-04,  ..., -3.4075e-04,
          9.3255e-04,  4.4674e-04],
        [ 4.7426e-05,  5.5656e-05, -4.0593e-05,  ...,  4.8249e-05,
          5.6883e-05,  5.3535e-05],
        [-2.2292e-05, -1.6287e-05,  1.0647e-05,  ..., -1.9774e-05,
         -8.7917e-06, -1.4648e-05],
        [-3.1576e-05, -2.2903e-05,  1.5251e-05,  ..., -2.8461e-05,
         -1.2524e-05, -2.1473e-05],
        [-4.0174e-05, -2.9325e-05,  1.9193e-05,  ..., -3.5465e-05,
         -1.6361e-05, -2.5243e-05]], device='cuda:0')
Loss: 1.0790948867797852
Graident accumulation at epoch 0, step 999, batch 999
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.7110e-04, -2.3277e-04, -1.3484e-04,  ..., -5.4047e-05,
          4.6171e-04,  3.1321e-04],
        [-8.1194e-06, -2.5073e-07,  2.6636e-07,  ..., -4.5587e-06,
          5.3463e-07, -2.5396e-06],
        [ 1.4331e-04,  1.0540e-04, -5.2616e-05,  ...,  1.1054e-04,
          5.4454e-05,  5.8730e-05],
        [-6.8631e-06, -5.6834e-06, -9.2790e-07,  ..., -6.5979e-06,
         -9.5016e-07, -5.8695e-06],
        [-3.3961e-05, -2.4388e-05,  1.6258e-05,  ..., -2.8974e-05,
         -1.3295e-05, -2.2417e-05]], device='cuda:0')
optimizer state dict: tensor([[6.9818e-08, 7.9730e-08, 2.8066e-08,  ..., 5.0331e-08, 1.3150e-07,
         2.3223e-08],
        [6.2482e-11, 4.5828e-11, 7.6942e-12,  ..., 5.1794e-11, 1.1611e-11,
         2.0889e-11],
        [3.4653e-09, 1.7548e-09, 4.1339e-10,  ..., 2.5541e-09, 3.5126e-10,
         9.1237e-10],
        [7.8995e-10, 5.3944e-10, 1.0600e-10,  ..., 6.4421e-10, 9.3524e-11,
         2.5887e-10],
        [3.3920e-10, 1.7911e-10, 3.2633e-11,  ..., 2.4787e-10, 4.8029e-11,
         8.3826e-11]], device='cuda:0')
optimizer state dict: 125.0
lr: [1.0982208042932707e-07, 1.0982208042932707e-07]
scheduler_last_epoch: 125


Running epoch 0, step 1000, batch 1000
Sampled inputs[:2]: tensor([[    0,   344, 14017,  ...,    65,   298,   634],
        [    0,  3087,   342,  ...,    14,   381,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5710e-05,  2.0770e-04, -5.8407e-05,  ...,  7.0722e-05,
         -3.3042e-04, -1.3440e-04],
        [-1.9670e-06, -1.4156e-06,  8.8289e-07,  ..., -1.7509e-06,
         -8.0094e-07, -1.2517e-06],
        [-2.8908e-06, -2.0862e-06,  1.3635e-06,  ..., -2.5183e-06,
         -1.0803e-06, -1.7509e-06],
        [-4.0233e-06, -2.9057e-06,  1.9073e-06,  ..., -3.5465e-06,
         -1.4901e-06, -2.5183e-06],
        [-5.2750e-06, -3.8147e-06,  2.4736e-06,  ..., -4.5598e-06,
         -2.0415e-06, -3.0845e-06]], device='cuda:0')
Loss: 1.080752968788147


Running epoch 0, step 1001, batch 1001
Sampled inputs[:2]: tensor([[    0,   199,   769,  ...,   380,   560,   199],
        [    0,  3504,     9,  ...,  7166, 10945,  3119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5695e-04,  4.1409e-05, -1.3248e-04,  ..., -7.2623e-05,
         -3.2067e-04, -2.6619e-04],
        [-3.8892e-06, -2.7865e-06,  1.6987e-06,  ..., -3.3826e-06,
         -1.5423e-06, -2.5630e-06],
        [-5.9009e-06, -4.2617e-06,  2.7046e-06,  ..., -5.0217e-06,
         -2.1160e-06, -3.7029e-06],
        [-8.1956e-06, -5.8860e-06,  3.7849e-06,  ..., -7.0482e-06,
         -2.9355e-06, -5.3197e-06],
        [-1.0699e-05, -7.7188e-06,  4.8876e-06,  ..., -9.0599e-06,
         -3.9786e-06, -6.4820e-06]], device='cuda:0')
Loss: 1.0829541683197021


Running epoch 0, step 1002, batch 1002
Sampled inputs[:2]: tensor([[    0, 15003, 19278,  ...,   287,   847,   328],
        [    0,    15,  4291,  ...,  1685,   278,  2101]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5657e-04,  2.2706e-05, -4.7131e-05,  ..., -9.7965e-05,
         -2.1296e-04, -1.7689e-04],
        [-5.8562e-06, -4.2841e-06,  2.5705e-06,  ..., -5.1633e-06,
         -2.3916e-06, -3.8892e-06],
        [-8.8066e-06, -6.4820e-06,  4.0457e-06,  ..., -7.5847e-06,
         -3.2485e-06, -5.5432e-06],
        [-1.2219e-05, -8.9556e-06,  5.6699e-06,  ..., -1.0654e-05,
         -4.5225e-06, -7.9870e-06],
        [-1.5825e-05, -1.1653e-05,  7.2420e-06,  ..., -1.3560e-05,
         -6.0499e-06, -9.6112e-06]], device='cuda:0')
Loss: 1.0795800685882568


Running epoch 0, step 1003, batch 1003
Sampled inputs[:2]: tensor([[    0,   266,  1586,  ...,  1888,  2117,   328],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1146e-04, -1.6308e-04, -5.9154e-05,  ..., -2.1082e-04,
         -2.6229e-04, -2.0944e-04],
        [-7.7412e-06, -5.7295e-06,  3.3900e-06,  ..., -6.7800e-06,
         -3.1628e-06, -5.2303e-06],
        [-1.1653e-05, -8.6725e-06,  5.3570e-06,  ..., -9.9540e-06,
         -4.2841e-06, -7.4282e-06],
        [-1.6153e-05, -1.1966e-05,  7.4953e-06,  ..., -1.3977e-05,
         -5.9754e-06, -1.0684e-05],
        [-2.0921e-05, -1.5587e-05,  9.6112e-06,  ..., -1.7792e-05,
         -7.9870e-06, -1.2845e-05]], device='cuda:0')
Loss: 1.0605597496032715


Running epoch 0, step 1004, batch 1004
Sampled inputs[:2]: tensor([[    0,   591, 18622,  ...,   955,  6118,  9191],
        [    0,   292,   474,  ...,   446, 14932,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4583e-04, -6.3392e-04,  1.2773e-04,  ..., -4.0012e-04,
          2.3184e-04, -1.5210e-04],
        [-9.6336e-06, -7.0259e-06,  4.1910e-06,  ..., -8.5607e-06,
         -4.0419e-06, -6.6236e-06],
        [-1.4424e-05, -1.0610e-05,  6.6087e-06,  ..., -1.2457e-05,
         -5.4166e-06, -9.3132e-06],
        [-2.0057e-05, -1.4663e-05,  9.2834e-06,  ..., -1.7583e-05,
         -7.6070e-06, -1.3456e-05],
        [-2.5868e-05, -1.9073e-05,  1.1846e-05,  ..., -2.2262e-05,
         -1.0088e-05, -1.6078e-05]], device='cuda:0')
Loss: 1.0773448944091797


Running epoch 0, step 1005, batch 1005
Sampled inputs[:2]: tensor([[    0,   369,   726,  ...,   292,   221,   358],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0072e-03, -9.2994e-04,  4.1479e-05,  ..., -4.8068e-04,
          3.7197e-04,  1.0528e-04],
        [-1.1355e-05, -8.3297e-06,  4.9919e-06,  ..., -1.0222e-05,
         -4.9770e-06, -8.0094e-06],
        [-1.6972e-05, -1.2547e-05,  7.8753e-06,  ..., -1.4797e-05,
         -6.6087e-06, -1.1176e-05],
        [-2.3767e-05, -1.7449e-05,  1.1139e-05,  ..., -2.1040e-05,
         -9.3803e-06, -1.6302e-05],
        [-3.0518e-05, -2.2605e-05,  1.4141e-05,  ..., -2.6464e-05,
         -1.2323e-05, -1.9312e-05]], device='cuda:0')
Loss: 1.0693202018737793


Running epoch 0, step 1006, batch 1006
Sampled inputs[:2]: tensor([[    0,  7994,    12,  ..., 13800,   278,   795],
        [    0,    14, 15670,  ...,  2027,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2253e-04, -8.4018e-04, -1.0522e-04,  ..., -4.7088e-04,
          8.9450e-05, -7.6782e-05],
        [-1.3098e-05, -9.5516e-06,  5.7705e-06,  ..., -1.2003e-05,
         -6.1095e-06, -9.5293e-06],
        [-1.9521e-05, -1.4380e-05,  9.1046e-06,  ..., -1.7241e-05,
         -8.0392e-06, -1.3158e-05],
        [-2.7418e-05, -2.0012e-05,  1.2904e-05,  ..., -2.4632e-05,
         -1.1526e-05, -1.9297e-05],
        [-3.5077e-05, -2.5898e-05,  1.6347e-05,  ..., -3.0786e-05,
         -1.4886e-05, -2.2665e-05]], device='cuda:0')
Loss: 1.0733623504638672


Running epoch 0, step 1007, batch 1007
Sampled inputs[:2]: tensor([[    0,  5024,  3846,  ...,  5880,  1377,    12],
        [    0, 38460,     9,  ...,   829,   870,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9848e-04, -7.2213e-04, -1.5718e-04,  ..., -4.7371e-04,
          8.5089e-06,  6.0064e-05],
        [-1.5169e-05, -1.1027e-05,  6.6571e-06,  ..., -1.3813e-05,
         -7.0445e-06, -1.0863e-05],
        [-2.2486e-05, -1.6525e-05,  1.0438e-05,  ..., -1.9789e-05,
         -9.2834e-06, -1.4968e-05],
        [-3.1650e-05, -2.3067e-05,  1.4842e-05,  ..., -2.8342e-05,
         -1.3322e-05, -2.1994e-05],
        [-4.0412e-05, -2.9773e-05,  1.8761e-05,  ..., -3.5375e-05,
         -1.7211e-05, -2.5824e-05]], device='cuda:0')
Loss: 1.0860627889633179
Graident accumulation at epoch 0, step 1007, batch 1007
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.2384e-04, -2.8171e-04, -1.3707e-04,  ..., -9.6013e-05,
          4.1639e-04,  2.8790e-04],
        [-8.8244e-06, -1.3283e-06,  9.0543e-07,  ..., -5.4841e-06,
         -2.2328e-07, -3.3719e-06],
        [ 1.2673e-04,  9.3211e-05, -4.6311e-05,  ...,  9.7506e-05,
          4.8081e-05,  5.1360e-05],
        [-9.3418e-06, -7.4218e-06,  6.4904e-07,  ..., -8.7723e-06,
         -2.1873e-06, -7.4820e-06],
        [-3.4606e-05, -2.4927e-05,  1.6509e-05,  ..., -2.9614e-05,
         -1.3686e-05, -2.2758e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0555e-08, 8.0172e-08, 2.8062e-08,  ..., 5.0505e-08, 1.3137e-07,
         2.3203e-08],
        [6.2649e-11, 4.5904e-11, 7.7308e-12,  ..., 5.1933e-11, 1.1649e-11,
         2.0986e-11],
        [3.4623e-09, 1.7533e-09, 4.1308e-10,  ..., 2.5520e-09, 3.5100e-10,
         9.1168e-10],
        [7.9016e-10, 5.3944e-10, 1.0611e-10,  ..., 6.4437e-10, 9.3608e-11,
         2.5909e-10],
        [3.4050e-10, 1.7982e-10, 3.2952e-11,  ..., 2.4888e-10, 4.8277e-11,
         8.4409e-11]], device='cuda:0')
optimizer state dict: 126.0
lr: [7.630809080545365e-08, 7.630809080545365e-08]
scheduler_last_epoch: 126


Running epoch 0, step 1008, batch 1008
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0,  1594,   586,  ...,    13,   701,   308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6618e-04, -1.2912e-04,  2.5675e-04,  ..., -2.0234e-04,
          1.3388e-04,  3.8761e-05],
        [-1.8328e-06, -1.3411e-06,  8.1584e-07,  ..., -1.7434e-06,
         -9.7603e-07, -1.3933e-06],
        [-2.8461e-06, -2.0862e-06,  1.3486e-06,  ..., -2.5928e-06,
         -1.3262e-06, -2.0266e-06],
        [-3.8743e-06, -2.8312e-06,  1.8626e-06,  ..., -3.5912e-06,
         -1.8254e-06, -2.8759e-06],
        [-5.0068e-06, -3.6657e-06,  2.3544e-06,  ..., -4.5002e-06,
         -2.3991e-06, -3.3826e-06]], device='cuda:0')
Loss: 1.0757578611373901


Running epoch 0, step 1009, batch 1009
Sampled inputs[:2]: tensor([[    0,  1086,  5564,  ..., 29319, 32982,   344],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6164e-05,  2.0936e-04,  2.0491e-04,  ..., -1.2091e-05,
         -1.3863e-04,  5.7184e-05],
        [-3.8743e-06, -2.8089e-06,  1.6317e-06,  ..., -3.5092e-06,
         -2.0042e-06, -2.8834e-06],
        [-5.9009e-06, -4.3064e-06,  2.6450e-06,  ..., -5.1707e-06,
         -2.7344e-06, -4.1276e-06],
        [-7.9870e-06, -5.8115e-06,  3.6210e-06,  ..., -7.1079e-06,
         -3.7253e-06, -5.8264e-06],
        [-1.0252e-05, -7.4804e-06,  4.5896e-06,  ..., -8.9109e-06,
         -4.8578e-06, -6.8545e-06]], device='cuda:0')
Loss: 1.0669161081314087


Running epoch 0, step 1010, batch 1010
Sampled inputs[:2]: tensor([[    0,   287, 14752,  ...,   910, 26097,  1477],
        [    0,   333,   199,  ...,   292,    48,  1792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1859e-05,  5.0597e-04,  2.9686e-04,  ...,  2.6198e-04,
         -2.7252e-04,  1.2204e-04],
        [-6.0648e-06, -4.3213e-06,  2.4959e-06,  ..., -5.3048e-06,
         -3.0100e-06, -4.2543e-06],
        [ 1.1880e-04,  4.5602e-05,  1.9595e-05,  ...,  7.9173e-05,
          8.1800e-05,  5.5837e-05],
        [-1.2308e-05, -8.7917e-06,  5.4017e-06,  ..., -1.0625e-05,
         -5.6028e-06, -8.4639e-06],
        [-1.6034e-05, -1.1563e-05,  6.9886e-06,  ..., -1.3620e-05,
         -7.4208e-06, -1.0192e-05]], device='cuda:0')
Loss: 1.0864627361297607


Running epoch 0, step 1011, batch 1011
Sampled inputs[:2]: tensor([[   0,  287,  271,  ..., 5090,  631, 3276],
        [   0,  287, 4579,  ...,  909,   12,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9872e-04,  6.2294e-04,  2.1102e-04,  ...,  2.6198e-04,
         -5.2307e-04,  1.3271e-05],
        [-7.8976e-06, -5.6624e-06,  3.3453e-06,  ..., -7.0333e-06,
         -3.7290e-06, -5.5507e-06],
        [ 1.1602e-04,  4.3560e-05,  2.0966e-05,  ...,  7.6654e-05,
          8.0846e-05,  5.4012e-05],
        [-1.6302e-05, -1.1697e-05,  7.3835e-06,  ..., -1.4275e-05,
         -6.9663e-06, -1.1176e-05],
        [-2.1100e-05, -1.5244e-05,  9.4622e-06,  ..., -1.8150e-05,
         -9.2238e-06, -1.3351e-05]], device='cuda:0')
Loss: 1.0735113620758057


Running epoch 0, step 1012, batch 1012
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  380,  560,  199],
        [   0,   12, 5820,  ...,    5, 2122,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9688e-04,  9.8551e-04,  5.8684e-05,  ...,  4.4893e-04,
         -4.3802e-04, -7.5439e-05],
        [-9.8497e-06, -7.0408e-06,  4.1984e-06,  ..., -8.8587e-06,
         -4.8615e-06, -6.9961e-06],
        [ 1.1326e-04,  4.1578e-05,  2.2262e-05,  ...,  7.4181e-05,
          7.9401e-05,  5.2119e-05],
        [-2.0266e-05, -1.4514e-05,  9.2685e-06,  ..., -1.7911e-05,
         -9.0972e-06, -1.4022e-05],
        [-2.6107e-05, -1.8820e-05,  1.1817e-05,  ..., -2.2590e-05,
         -1.1876e-05, -1.6630e-05]], device='cuda:0')
Loss: 1.0772818326950073


Running epoch 0, step 1013, batch 1013
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0,  5143,  3877,  ...,   292, 44003,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7996e-04,  1.3347e-03, -6.6971e-05,  ...,  5.6150e-04,
         -5.7698e-04,  4.1708e-06],
        [-1.1817e-05, -8.4862e-06,  5.0887e-06,  ..., -1.0625e-05,
         -5.7817e-06, -8.3968e-06],
        [ 1.1018e-04,  3.9313e-05,  2.3760e-05,  ...,  7.1528e-05,
          7.8097e-05,  5.0078e-05],
        [-2.4259e-05, -1.7419e-05,  1.1206e-05,  ..., -2.1368e-05,
         -1.0781e-05, -1.6734e-05],
        [-3.1441e-05, -2.2724e-05,  1.4380e-05,  ..., -2.7120e-05,
         -1.4171e-05, -1.9982e-05]], device='cuda:0')
Loss: 1.049162745475769


Running epoch 0, step 1014, batch 1014
Sampled inputs[:2]: tensor([[   0,  593, 1387,  ...,  508, 8222, 1415],
        [   0,  221,  380,  ..., 5543,  768, 6375]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1312e-04,  1.3080e-03, -1.2173e-04,  ...,  7.1929e-04,
         -7.5969e-04,  9.2647e-06],
        [-1.3597e-05, -9.7528e-06,  5.9418e-06,  ..., -1.2338e-05,
         -6.4857e-06, -9.7007e-06],
        [ 1.0761e-04,  3.7466e-05,  2.5064e-05,  ...,  6.9159e-05,
          7.7199e-05,  4.8350e-05],
        [-2.8133e-05, -2.0191e-05,  1.3202e-05,  ..., -2.5034e-05,
         -1.2167e-05, -1.9476e-05],
        [-3.6150e-05, -2.6122e-05,  1.6764e-05,  ..., -3.1441e-05,
         -1.5914e-05, -2.3022e-05]], device='cuda:0')
Loss: 1.0577412843704224


Running epoch 0, step 1015, batch 1015
Sampled inputs[:2]: tensor([[   0,  292,  474,  ..., 1085,  494, 2665],
        [   0,   12,  287,  ..., 3359, 1751, 5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6099e-04,  9.6467e-04, -1.8320e-04,  ...,  6.2429e-04,
         -8.1781e-04,  3.4466e-04],
        [-1.5534e-05, -1.1086e-05,  6.8098e-06,  ..., -1.4037e-05,
         -7.4394e-06, -1.1101e-05],
        [ 1.0480e-04,  3.5499e-05,  2.6397e-05,  ...,  6.6775e-05,
          7.5948e-05,  4.6442e-05],
        [-3.2127e-05, -2.2963e-05,  1.5140e-05,  ..., -2.8476e-05,
         -1.3962e-05, -2.2262e-05],
        [-4.1217e-05, -2.9713e-05,  1.9193e-05,  ..., -3.5733e-05,
         -1.8239e-05, -2.6315e-05]], device='cuda:0')
Loss: 1.0451308488845825
Graident accumulation at epoch 0, step 1015, batch 1015
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.6536e-04, -1.5707e-04, -1.4169e-04,  ..., -2.3983e-05,
          2.9297e-04,  2.9357e-04],
        [-9.4954e-06, -2.3042e-06,  1.4959e-06,  ..., -6.3394e-06,
         -9.4490e-07, -4.1448e-06],
        [ 1.2454e-04,  8.7440e-05, -3.9040e-05,  ...,  9.4433e-05,
          5.0867e-05,  5.0868e-05],
        [-1.1620e-05, -8.9759e-06,  2.0981e-06,  ..., -1.0743e-05,
         -3.3648e-06, -8.9600e-06],
        [-3.5267e-05, -2.5405e-05,  1.6777e-05,  ..., -3.0226e-05,
         -1.4142e-05, -2.3114e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0511e-08, 8.1023e-08, 2.8068e-08,  ..., 5.0844e-08, 1.3191e-07,
         2.3299e-08],
        [6.2828e-11, 4.5981e-11, 7.7694e-12,  ..., 5.2078e-11, 1.1693e-11,
         2.1089e-11],
        [3.4699e-09, 1.7528e-09, 4.1337e-10,  ..., 2.5539e-09, 3.5641e-10,
         9.1293e-10],
        [7.9041e-10, 5.3942e-10, 1.0623e-10,  ..., 6.4453e-10, 9.3709e-11,
         2.5933e-10],
        [3.4186e-10, 1.8052e-10, 3.3288e-11,  ..., 2.4990e-10, 4.8562e-11,
         8.5017e-11]], device='cuda:0')
optimizer state dict: 127.0
lr: [4.885957645375916e-08, 4.885957645375916e-08]
scheduler_last_epoch: 127


Running epoch 0, step 1016, batch 1016
Sampled inputs[:2]: tensor([[    0,   273,   298,  ..., 23554,    12,  1530],
        [    0,  1716,  1773,  ...,  5014,    12,   847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6185e-05,  6.5981e-05,  9.2162e-05,  ...,  3.6491e-05,
          1.9062e-04,  6.9919e-05],
        [-2.0862e-06, -1.4454e-06,  7.8231e-07,  ..., -1.8850e-06,
         -1.1772e-06, -1.5348e-06],
        [-3.0547e-06, -2.1458e-06,  1.2368e-06,  ..., -2.6822e-06,
         -1.6093e-06, -2.1160e-06],
        [-3.8743e-06, -2.7269e-06,  1.5944e-06,  ..., -3.4869e-06,
         -2.0564e-06, -2.7716e-06],
        [-5.3942e-06, -3.8445e-06,  2.2203e-06,  ..., -4.7386e-06,
         -2.8908e-06, -3.5763e-06]], device='cuda:0')
Loss: 1.0839587450027466


Running epoch 0, step 1017, batch 1017
Sampled inputs[:2]: tensor([[    0,  2270,  3279,  ...,   380,   475,   768],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1081e-04,  6.8932e-04, -1.6030e-04,  ...,  3.8170e-04,
         -5.1352e-04, -3.7448e-04],
        [-3.9712e-06, -2.8014e-06,  1.5944e-06,  ..., -3.6955e-06,
         -2.0824e-06, -2.7716e-06],
        [-5.8264e-06, -4.1723e-06,  2.4959e-06,  ..., -5.2601e-06,
         -2.8014e-06, -3.8147e-06],
        [-7.8082e-06, -5.5730e-06,  3.3900e-06,  ..., -7.2271e-06,
         -3.7849e-06, -5.3048e-06],
        [-1.0371e-05, -7.4953e-06,  4.4852e-06,  ..., -9.3579e-06,
         -5.0813e-06, -6.5118e-06]], device='cuda:0')
Loss: 1.0755215883255005


Running epoch 0, step 1018, batch 1018
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0,   600,   518,  ...,  3134,   278, 37342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0734e-04,  5.5898e-04, -2.1670e-04,  ...,  2.3015e-04,
         -5.1671e-04, -3.3671e-04],
        [-5.8562e-06, -4.1723e-06,  2.4959e-06,  ..., -5.3868e-06,
         -2.8051e-06, -4.0010e-06],
        [-8.6129e-06, -6.2138e-06,  3.8818e-06,  ..., -7.7337e-06,
         -3.8072e-06, -5.5730e-06],
        [-1.1712e-05, -8.4341e-06,  5.3570e-06,  ..., -1.0744e-05,
         -5.1931e-06, -7.8529e-06],
        [-1.5378e-05, -1.1176e-05,  6.9588e-06,  ..., -1.3798e-05,
         -6.9812e-06, -9.5516e-06]], device='cuda:0')
Loss: 1.089018702507019


Running epoch 0, step 1019, batch 1019
Sampled inputs[:2]: tensor([[    0,  6408,   391,  ...,   870,   278,   266],
        [    0,   292,   380,  ...,   287, 10086,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7123e-04,  1.8702e-04, -3.1650e-05,  ..., -1.9033e-04,
          2.3150e-04, -3.6747e-05],
        [-7.8231e-06, -5.5730e-06,  3.2708e-06,  ..., -7.3090e-06,
         -3.9004e-06, -5.4166e-06],
        [-1.1519e-05, -8.2999e-06,  5.1260e-06,  ..., -1.0431e-05,
         -5.2750e-06, -7.4953e-06],
        [-1.5527e-05, -1.1161e-05,  6.9886e-06,  ..., -1.4380e-05,
         -7.1600e-06, -1.0490e-05],
        [-2.0593e-05, -1.4931e-05,  9.2089e-06,  ..., -1.8597e-05,
         -9.6038e-06, -1.2845e-05]], device='cuda:0')
Loss: 1.082067847251892


Running epoch 0, step 1020, batch 1020
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,   369,   726,  ...,    83,   409,   729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2905e-04,  1.8702e-04, -2.1182e-04,  ..., -1.5189e-04,
         -6.3463e-06, -4.1261e-04],
        [-9.6112e-06, -6.8992e-06,  4.0568e-06,  ..., -9.0450e-06,
         -4.6939e-06, -6.7875e-06],
        [-1.4082e-05, -1.0207e-05,  6.3479e-06,  ..., -1.2770e-05,
         -6.2585e-06, -9.2611e-06],
        [-1.9297e-05, -1.3947e-05,  8.8140e-06,  ..., -1.7926e-05,
         -8.6427e-06, -1.3232e-05],
        [-2.5153e-05, -1.8328e-05,  1.1399e-05,  ..., -2.2739e-05,
         -1.1437e-05, -1.5825e-05]], device='cuda:0')
Loss: 1.0702112913131714


Running epoch 0, step 1021, batch 1021
Sampled inputs[:2]: tensor([[    0,  1016,   271,  ...,   461,   616,   993],
        [    0, 18981,    13,  ...,   365,  2714,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4469e-04,  2.9738e-04, -2.0252e-04,  ..., -1.1583e-04,
         -5.3389e-05, -4.5509e-04],
        [-1.1653e-05, -8.3819e-06,  4.9137e-06,  ..., -1.0848e-05,
         -5.7295e-06, -8.0541e-06],
        [-1.7092e-05, -1.2398e-05,  7.6741e-06,  ..., -1.5348e-05,
         -7.6592e-06, -1.1034e-05],
        [-2.3380e-05, -1.6898e-05,  1.0617e-05,  ..., -2.1458e-05,
         -1.0550e-05, -1.5706e-05],
        [-3.0667e-05, -2.2352e-05,  1.3828e-05,  ..., -2.7448e-05,
         -1.4059e-05, -1.8954e-05]], device='cuda:0')
Loss: 1.0943127870559692


Running epoch 0, step 1022, batch 1022
Sampled inputs[:2]: tensor([[   0,  266,  996,  ...,  709,  616, 9378],
        [   0, 1530,   17,  ...,  409, 1611,  895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8524e-05, -1.6618e-04, -8.0933e-05,  ..., -4.7341e-04,
         -7.6364e-06, -2.9477e-04],
        [-1.3545e-05, -9.8720e-06,  5.8115e-06,  ..., -1.2554e-05,
         -6.5118e-06, -9.2909e-06],
        [-1.9789e-05, -1.4529e-05,  9.0152e-06,  ..., -1.7747e-05,
         -8.6874e-06, -1.2733e-05],
        [-2.7180e-05, -1.9893e-05,  1.2524e-05,  ..., -2.4870e-05,
         -1.1995e-05, -1.8165e-05],
        [-3.5644e-05, -2.6286e-05,  1.6302e-05,  ..., -3.1859e-05,
         -1.6056e-05, -2.1979e-05]], device='cuda:0')
Loss: 1.0931040048599243


Running epoch 0, step 1023, batch 1023
Sampled inputs[:2]: tensor([[   0,   12,  221,  ...,  593,  360,  726],
        [   0,  271,  266,  ..., 1034, 1928,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2736e-04, -3.7052e-04, -1.6178e-04,  ..., -4.1978e-04,
          7.6293e-05, -2.6930e-04],
        [-1.5385e-05, -1.1213e-05,  6.6720e-06,  ..., -1.4238e-05,
         -7.3016e-06, -1.0617e-05],
        [ 6.7318e-05,  8.7790e-06, -2.8178e-05,  ...,  6.5876e-05,
          1.0315e-05,  2.9554e-05],
        [-3.1054e-05, -2.2724e-05,  1.4447e-05,  ..., -2.8387e-05,
         -1.3500e-05, -2.0906e-05],
        [-4.0650e-05, -2.9981e-05,  1.8716e-05,  ..., -3.6299e-05,
         -1.8068e-05, -2.5213e-05]], device='cuda:0')
Loss: 1.0747177600860596
Graident accumulation at epoch 0, step 1023, batch 1023
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.4156e-04, -1.7842e-04, -1.4370e-04,  ..., -6.3563e-05,
          2.7130e-04,  2.3729e-04],
        [-1.0084e-05, -3.1950e-06,  2.0135e-06,  ..., -7.1293e-06,
         -1.5806e-06, -4.7921e-06],
        [ 1.1882e-04,  7.9574e-05, -3.7954e-05,  ...,  9.1577e-05,
          4.6812e-05,  4.8737e-05],
        [-1.3564e-05, -1.0351e-05,  3.3330e-06,  ..., -1.2507e-05,
         -4.3784e-06, -1.0155e-05],
        [-3.5805e-05, -2.5863e-05,  1.6971e-05,  ..., -3.0833e-05,
         -1.4534e-05, -2.3324e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0457e-08, 8.1079e-08, 2.8066e-08,  ..., 5.0970e-08, 1.3178e-07,
         2.3348e-08],
        [6.3002e-11, 4.6060e-11, 7.8062e-12,  ..., 5.2229e-11, 1.1734e-11,
         2.1180e-11],
        [3.4709e-09, 1.7511e-09, 4.1375e-10,  ..., 2.5556e-09, 3.5616e-10,
         9.1289e-10],
        [7.9058e-10, 5.3940e-10, 1.0634e-10,  ..., 6.4469e-10, 9.3798e-11,
         2.5951e-10],
        [3.4317e-10, 1.8124e-10, 3.3605e-11,  ..., 2.5097e-10, 4.8840e-11,
         8.5567e-11]], device='cuda:0')
optimizer state dict: 128.0
lr: [2.7493314223681067e-08, 2.7493314223681067e-08]
scheduler_last_epoch: 128


Running epoch 0, step 1024, batch 1024
Sampled inputs[:2]: tensor([[   0,  328, 1410,  ..., 7344,   12, 5067],
        [   0,   13, 4596,  ...,  408,  689,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7253e-04,  3.0402e-04, -2.1498e-04,  ...,  1.9756e-04,
         -3.6580e-04, -3.0282e-04],
        [-1.6987e-06, -1.3188e-06,  7.3761e-07,  ..., -1.7881e-06,
         -9.4995e-07, -1.4678e-06],
        [-2.4289e-06, -1.8701e-06,  1.1846e-06,  ..., -2.3544e-06,
         -1.1474e-06, -1.8254e-06],
        [-3.6061e-06, -2.7418e-06,  1.7732e-06,  ..., -3.6359e-06,
         -1.7732e-06, -2.9057e-06],
        [-4.4107e-06, -3.3677e-06,  2.1756e-06,  ..., -4.2021e-06,
         -2.1011e-06, -3.1292e-06]], device='cuda:0')
Loss: 1.0541939735412598


Running epoch 0, step 1025, batch 1025
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0,  642,  271,  ..., 5430, 2314, 6431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6154e-04,  4.7377e-04, -4.4009e-04,  ...,  2.6393e-04,
         -6.4516e-04, -5.6257e-04],
        [-3.6210e-06, -2.7269e-06,  1.6354e-06,  ..., -3.5018e-06,
         -1.8477e-06, -2.8610e-06],
        [-5.2005e-06, -3.9414e-06,  2.5630e-06,  ..., -4.7684e-06,
         -2.3246e-06, -3.7253e-06],
        [-7.5400e-06, -5.6475e-06,  3.7551e-06,  ..., -7.1079e-06,
         -3.4571e-06, -5.7220e-06],
        [-9.4175e-06, -7.0930e-06,  4.6492e-06,  ..., -8.5235e-06,
         -4.3213e-06, -6.4075e-06]], device='cuda:0')
Loss: 1.0562328100204468


Running epoch 0, step 1026, batch 1026
Sampled inputs[:2]: tensor([[    0,   857,   344,  ...,  1529,  9106,  1447],
        [    0, 22390,   292,  ...,  3552,   278,   317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9792e-04,  4.4074e-04, -3.6733e-04,  ...,  4.0946e-04,
         -6.0585e-04, -4.7675e-04],
        [-5.6773e-06, -4.1947e-06,  2.5555e-06,  ..., -5.2825e-06,
         -2.6897e-06, -4.0755e-06],
        [-8.2403e-06, -6.1467e-06,  3.9935e-06,  ..., -7.3612e-06,
         -3.4645e-06, -5.4538e-06],
        [-1.1683e-05, -8.6427e-06,  5.7220e-06,  ..., -1.0699e-05,
         -5.0217e-06, -8.1807e-06],
        [-1.4901e-05, -1.1086e-05,  7.2271e-06,  ..., -1.3202e-05,
         -6.4671e-06, -9.4175e-06]], device='cuda:0')
Loss: 1.0764684677124023


Running epoch 0, step 1027, batch 1027
Sampled inputs[:2]: tensor([[   0,  368,  275,  ..., 6389, 9102,   12],
        [   0,  471,  590,  ..., 5007,   13, 2920]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7593e-04,  5.5369e-04, -2.9146e-04,  ...,  2.5301e-04,
         -2.8924e-04, -3.3584e-04],
        [-7.8231e-06, -5.6326e-06,  3.3453e-06,  ..., -7.0408e-06,
         -3.7849e-06, -5.6326e-06],
        [-1.1340e-05, -8.2776e-06,  5.2452e-06,  ..., -9.8348e-06,
         -4.9099e-06, -7.5400e-06],
        [-1.5855e-05, -1.1474e-05,  7.4133e-06,  ..., -1.4082e-05,
         -6.9737e-06, -1.1086e-05],
        [-2.0474e-05, -1.4931e-05,  9.5218e-06,  ..., -1.7643e-05,
         -9.1344e-06, -1.3009e-05]], device='cuda:0')
Loss: 1.0561227798461914


Running epoch 0, step 1028, batch 1028
Sampled inputs[:2]: tensor([[    0,  1607,    12,  ...,   895,  1503,   369],
        [    0,   278, 39533,  ...,   277,  1395, 47607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2549e-04,  3.1362e-04, -2.4563e-04,  ...,  1.0076e-04,
         -3.3184e-04, -3.7858e-04],
        [-9.8348e-06, -7.0482e-06,  4.2245e-06,  ..., -8.8140e-06,
         -4.5449e-06, -6.8843e-06],
        [-1.4290e-05, -1.0394e-05,  6.5938e-06,  ..., -1.2413e-05,
         -5.9456e-06, -9.3058e-06],
        [-1.9968e-05, -1.4380e-05,  9.3058e-06,  ..., -1.7703e-05,
         -8.4266e-06, -1.3635e-05],
        [-2.5779e-05, -1.8716e-05,  1.1921e-05,  ..., -2.2233e-05,
         -1.1072e-05, -1.6049e-05]], device='cuda:0')
Loss: 1.082737684249878


Running epoch 0, step 1029, batch 1029
Sampled inputs[:2]: tensor([[    0, 20291,  1990,  ...,   298,   732,   298],
        [    0, 38495, 36253,  ..., 11006,  5699,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1710e-04,  3.2821e-04, -2.5332e-04,  ...,  1.4652e-04,
         -4.3887e-04, -2.7689e-04],
        [-1.1981e-05, -8.5458e-06,  5.1297e-06,  ..., -1.0639e-05,
         -5.4799e-06, -8.2403e-06],
        [-1.7375e-05, -1.2569e-05,  7.9498e-06,  ..., -1.5005e-05,
         -7.2196e-06, -1.1183e-05],
        [-2.4199e-05, -1.7345e-05,  1.1176e-05,  ..., -2.1294e-05,
         -1.0170e-05, -1.6272e-05],
        [-3.1352e-05, -2.2680e-05,  1.4365e-05,  ..., -2.6911e-05,
         -1.3426e-05, -1.9327e-05]], device='cuda:0')
Loss: 1.1177724599838257


Running epoch 0, step 1030, batch 1030
Sampled inputs[:2]: tensor([[    0,   344,   259,  ...,  6787, 10045,  9799],
        [    0,   266, 12080,  ...,   674,   369, 10956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3041e-05,  3.2927e-04, -5.6750e-04,  ...,  2.8210e-04,
         -6.6254e-04, -4.4291e-04],
        [-1.3694e-05, -9.7081e-06,  5.8897e-06,  ..., -1.2420e-05,
         -6.4932e-06, -9.6783e-06],
        [-1.9848e-05, -1.4283e-05,  9.1493e-06,  ..., -1.7390e-05,
         -8.4415e-06, -1.3001e-05],
        [-2.7791e-05, -1.9774e-05,  1.2942e-05,  ..., -2.4870e-05,
         -1.2033e-05, -1.9088e-05],
        [-3.5912e-05, -2.5839e-05,  1.6600e-05,  ..., -3.1233e-05,
         -1.5706e-05, -2.2501e-05]], device='cuda:0')
Loss: 1.0640010833740234


Running epoch 0, step 1031, batch 1031
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0,   367,   925,  ..., 25491,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0834e-05,  2.2104e-04, -3.9979e-04,  ...,  2.6718e-04,
         -4.6725e-04, -2.1124e-04],
        [-1.5721e-05, -1.1146e-05,  6.7316e-06,  ..., -1.4111e-05,
         -7.3463e-06, -1.0893e-05],
        [-2.2933e-05, -1.6503e-05,  1.0490e-05,  ..., -1.9938e-05,
         -9.6411e-06, -1.4775e-05],
        [-3.1903e-05, -2.2709e-05,  1.4730e-05,  ..., -2.8297e-05,
         -1.3635e-05, -2.1517e-05],
        [-4.1395e-05, -2.9802e-05,  1.8984e-05,  ..., -3.5763e-05,
         -1.7911e-05, -2.5541e-05]], device='cuda:0')
Loss: 1.09720778465271
Graident accumulation at epoch 0, step 1031, batch 1031
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1149e-04, -1.3847e-04, -1.6931e-04,  ..., -3.0489e-05,
          1.9745e-04,  1.9243e-04],
        [-1.0648e-05, -3.9902e-06,  2.4853e-06,  ..., -7.8275e-06,
         -2.1571e-06, -5.4021e-06],
        [ 1.0464e-04,  6.9966e-05, -3.3109e-05,  ...,  8.0426e-05,
          4.1167e-05,  4.2386e-05],
        [-1.5398e-05, -1.1587e-05,  4.4726e-06,  ..., -1.4086e-05,
         -5.3040e-06, -1.1291e-05],
        [-3.6364e-05, -2.6257e-05,  1.7172e-05,  ..., -3.1326e-05,
         -1.4872e-05, -2.3545e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0388e-08, 8.1047e-08, 2.8198e-08,  ..., 5.0990e-08, 1.3187e-07,
         2.3369e-08],
        [6.3186e-11, 4.6139e-11, 7.8437e-12,  ..., 5.2376e-11, 1.1777e-11,
         2.1278e-11],
        [3.4680e-09, 1.7497e-09, 4.1345e-10,  ..., 2.5535e-09, 3.5590e-10,
         9.1219e-10],
        [7.9081e-10, 5.3938e-10, 1.0645e-10,  ..., 6.4485e-10, 9.3890e-11,
         2.5971e-10],
        [3.4454e-10, 1.8194e-10, 3.3932e-11,  ..., 2.5200e-10, 4.9112e-11,
         8.6134e-11]], device='cuda:0')
optimizer state dict: 129.0
lr: [1.2222363421819928e-08, 1.2222363421819928e-08]
scheduler_last_epoch: 129


Running epoch 0, step 1032, batch 1032
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,    14,  3352,   266],
        [    0,   688,  2353,  ..., 20538, 10393,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8856e-05,  2.8443e-05,  8.3123e-05,  ..., -7.5506e-06,
          1.0191e-04,  1.5625e-04],
        [-1.9222e-06, -1.4305e-06,  8.1211e-07,  ..., -1.7434e-06,
         -7.1153e-07, -1.2219e-06],
        [-2.7120e-06, -2.0415e-06,  1.2144e-06,  ..., -2.4289e-06,
         -9.3505e-07, -1.6317e-06],
        [-3.9637e-06, -2.9802e-06,  1.8105e-06,  ..., -3.5912e-06,
         -1.3635e-06, -2.4587e-06],
        [-5.0366e-06, -3.8147e-06,  2.2650e-06,  ..., -4.5002e-06,
         -1.8179e-06, -2.9206e-06]], device='cuda:0')
Loss: 1.0833345651626587


Running epoch 0, step 1033, batch 1033
Sampled inputs[:2]: tensor([[    0,     9,   342,  ...,    12,   709,   857],
        [    0,   278, 19142,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8174e-04, -4.5924e-06, -8.8577e-05,  ..., -1.3054e-04,
          1.7425e-04,  1.2242e-04],
        [-3.8072e-06, -2.7418e-06,  1.5534e-06,  ..., -3.5316e-06,
         -1.5832e-06, -2.7120e-06],
        [-5.3942e-06, -3.9414e-06,  2.3842e-06,  ..., -4.8429e-06,
         -2.0228e-06, -3.5390e-06],
        [-7.8082e-06, -5.6773e-06,  3.5092e-06,  ..., -7.1377e-06,
         -2.9728e-06, -5.3346e-06],
        [-9.8944e-06, -7.2718e-06,  4.4107e-06,  ..., -8.8513e-06,
         -3.8594e-06, -6.2138e-06]], device='cuda:0')
Loss: 1.0390766859054565


Running epoch 0, step 1034, batch 1034
Sampled inputs[:2]: tensor([[    0,  2366,  5036,  ...,  1477,   352,   631],
        [    0,   677, 25912,  ...,  2337,   292,  4462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8293e-04,  3.4070e-04, -2.5948e-04,  ..., -7.9726e-05,
          5.2354e-05,  2.1193e-04],
        [-5.7891e-06, -4.1351e-06,  2.3320e-06,  ..., -5.2750e-06,
         -2.6189e-06, -4.2319e-06],
        [-8.3894e-06, -6.0573e-06,  3.6433e-06,  ..., -7.3761e-06,
         -3.4235e-06, -5.6848e-06],
        [-1.1683e-05, -8.3894e-06,  5.1484e-06,  ..., -1.0461e-05,
         -4.8056e-06, -8.2105e-06],
        [-1.5080e-05, -1.0937e-05,  6.6012e-06,  ..., -1.3202e-05,
         -6.3181e-06, -9.7752e-06]], device='cuda:0')
Loss: 1.0580317974090576


Running epoch 0, step 1035, batch 1035
Sampled inputs[:2]: tensor([[    0,   508, 22318,  ...,    13,  1107,  4093],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5587e-04,  2.3723e-04, -3.4941e-04,  ..., -2.6738e-04,
          1.4255e-04,  2.5923e-04],
        [-7.6815e-06, -5.5358e-06,  3.2075e-06,  ..., -6.9737e-06,
         -3.5353e-06, -5.6699e-06],
        [-1.1250e-05, -8.1882e-06,  5.0440e-06,  ..., -9.8795e-06,
         -4.6603e-06, -7.7263e-06],
        [-1.5616e-05, -1.1295e-05,  7.1004e-06,  ..., -1.3933e-05,
         -6.4895e-06, -1.1086e-05],
        [-2.0146e-05, -1.4707e-05,  9.0748e-06,  ..., -1.7613e-05,
         -8.5831e-06, -1.3232e-05]], device='cuda:0')
Loss: 1.0581231117248535


Running epoch 0, step 1036, batch 1036
Sampled inputs[:2]: tensor([[    0,    71,    14,  ...,  1770,   391, 39516],
        [    0,  7428,  1566,  ...,   199,  1726,  5647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6520e-04,  2.5695e-04, -3.9447e-04,  ..., -3.3573e-04,
         -1.2197e-04,  2.6920e-04],
        [-9.6038e-06, -6.9961e-06,  4.1127e-06,  ..., -8.7395e-06,
         -4.3996e-06, -6.9365e-06],
        [-1.4171e-05, -1.0408e-05,  6.5044e-06,  ..., -1.2457e-05,
         -5.8301e-06, -9.5218e-06],
        [-1.9580e-05, -1.4290e-05,  9.0972e-06,  ..., -1.7494e-05,
         -8.0913e-06, -1.3620e-05],
        [-2.5183e-05, -1.8522e-05,  1.1608e-05,  ..., -2.2024e-05,
         -1.0654e-05, -1.6198e-05]], device='cuda:0')
Loss: 1.0771009922027588


Running epoch 0, step 1037, batch 1037
Sampled inputs[:2]: tensor([[   0, 4834,  278,  ...,   13, 8382,  669],
        [   0, 2422,  300,  ...,  630,  729, 3400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5845e-04,  4.7164e-04, -5.3549e-04,  ..., -1.8866e-04,
         -3.9431e-04,  3.5946e-04],
        [-1.1675e-05, -8.5086e-06,  4.9323e-06,  ..., -1.0535e-05,
         -5.4203e-06, -8.2999e-06],
        [-1.7285e-05, -1.2718e-05,  7.8306e-06,  ..., -1.5095e-05,
         -7.2457e-06, -1.1459e-05],
        [-2.3693e-05, -1.7300e-05,  1.0848e-05,  ..., -2.1026e-05,
         -9.9689e-06, -1.6257e-05],
        [-3.0726e-05, -2.2635e-05,  1.3977e-05,  ..., -2.6703e-05,
         -1.3232e-05, -1.9506e-05]], device='cuda:0')
Loss: 1.0572081804275513


Running epoch 0, step 1038, batch 1038
Sampled inputs[:2]: tensor([[   0,  266, 3727,  ..., 1143,  271, 5213],
        [   0,  300, 4402,  ..., 2013,   13, 6825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2508e-04,  4.3601e-04, -4.3338e-04,  ..., -2.6699e-04,
         -4.2543e-04,  2.9319e-04],
        [-1.3746e-05, -9.9093e-06,  5.7556e-06,  ..., -1.2331e-05,
         -6.3069e-06, -9.5740e-06],
        [-2.0459e-05, -1.4894e-05,  9.1493e-06,  ..., -1.7792e-05,
         -8.5123e-06, -1.3329e-05],
        [ 9.9778e-05,  3.9285e-05, -2.8229e-05,  ...,  6.0757e-05,
          4.6192e-05,  1.0793e-05],
        [-3.6389e-05, -2.6539e-05,  1.6347e-05,  ..., -3.1531e-05,
         -1.5542e-05, -2.2739e-05]], device='cuda:0')
Loss: 1.0945370197296143


Running epoch 0, step 1039, batch 1039
Sampled inputs[:2]: tensor([[    0,   431, 19346,  ...,    14,  3237, 18548],
        [    0,   587,   300,  ...,  4325,   278, 12564]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2101e-04,  4.9909e-04, -5.3028e-04,  ..., -2.9007e-04,
         -2.9440e-04,  3.3373e-04],
        [-1.5728e-05, -1.1265e-05,  6.4895e-06,  ..., -1.4223e-05,
         -7.3053e-06, -1.1064e-05],
        [-2.3261e-05, -1.6816e-05,  1.0334e-05,  ..., -2.0310e-05,
         -9.7640e-06, -1.5222e-05],
        [ 9.5784e-05,  3.6558e-05, -2.6537e-05,  ...,  5.7091e-05,
          4.4360e-05,  7.9920e-06],
        [-4.1485e-05, -3.0071e-05,  1.8537e-05,  ..., -3.6061e-05,
         -1.7881e-05, -2.6017e-05]], device='cuda:0')
Loss: 1.0844793319702148
Graident accumulation at epoch 0, step 1039, batch 1039
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9244e-04, -7.4715e-05, -2.0540e-04,  ..., -5.6447e-05,
          1.4826e-04,  2.0656e-04],
        [-1.1156e-05, -4.7177e-06,  2.8857e-06,  ..., -8.4671e-06,
         -2.6719e-06, -5.9683e-06],
        [ 9.1850e-05,  6.1288e-05, -2.8765e-05,  ...,  7.0352e-05,
          3.6074e-05,  3.6625e-05],
        [-4.2795e-06, -6.7721e-06,  1.3716e-06,  ..., -6.9683e-06,
         -3.3764e-07, -9.3626e-06],
        [-3.6876e-05, -2.6638e-05,  1.7309e-05,  ..., -3.1800e-05,
         -1.5173e-05, -2.3793e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0332e-08, 8.1215e-08, 2.8451e-08,  ..., 5.1023e-08, 1.3182e-07,
         2.3457e-08],
        [6.3370e-11, 4.6219e-11, 7.8780e-12,  ..., 5.2526e-11, 1.1818e-11,
         2.1379e-11],
        [3.4650e-09, 1.7482e-09, 4.1314e-10,  ..., 2.5513e-09, 3.5564e-10,
         9.1151e-10],
        [7.9919e-10, 5.4017e-10, 1.0704e-10,  ..., 6.4747e-10, 9.5764e-11,
         2.5951e-10],
        [3.4591e-10, 1.8267e-10, 3.4241e-11,  ..., 2.5305e-10, 4.9382e-11,
         8.6725e-11]], device='cuda:0')
optimizer state dict: 130.0
lr: [3.0560578299276833e-09, 3.0560578299276833e-09]
scheduler_last_epoch: 130


Running epoch 0, step 1040, batch 1040
Sampled inputs[:2]: tensor([[   0,  287,  516,  ..., 2386, 3492, 1663],
        [   0,  767, 1345,  ...,  276,  327,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2512e-04,  9.9795e-05,  1.3546e-04,  ...,  3.1849e-04,
         -2.7024e-04,  7.1875e-05],
        [-2.0266e-06, -1.4007e-06,  8.6054e-07,  ..., -1.7583e-06,
         -9.6112e-07, -1.2964e-06],
        [-3.1292e-06, -2.1756e-06,  1.3933e-06,  ..., -2.6524e-06,
         -1.3858e-06, -1.9073e-06],
        [-4.2617e-06, -2.9355e-06,  1.9222e-06,  ..., -3.6508e-06,
         -1.8701e-06, -2.6822e-06],
        [-5.4836e-06, -3.8147e-06,  2.4438e-06,  ..., -4.6194e-06,
         -2.4885e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.0684705972671509


Running epoch 0, step 1041, batch 1041
Sampled inputs[:2]: tensor([[   0, 2377,  271,  ...,  395,  394,   14],
        [   0, 5699,   20,  ..., 3502, 2051,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5889e-05,  8.2127e-05, -5.3443e-05,  ...,  3.7085e-04,
         -3.1320e-04,  8.0988e-05],
        [-4.0084e-06, -2.8163e-06,  1.7770e-06,  ..., -3.4794e-06,
         -1.5981e-06, -2.5332e-06],
        [-6.0946e-06, -4.3213e-06,  2.8163e-06,  ..., -5.2005e-06,
         -2.2873e-06, -3.6955e-06],
        [-8.3447e-06, -5.8711e-06,  3.9190e-06,  ..., -7.1973e-06,
         -3.0994e-06, -5.2154e-06],
        [-1.0580e-05, -7.5251e-06,  4.9025e-06,  ..., -9.0301e-06,
         -4.1425e-06, -6.2138e-06]], device='cuda:0')
Loss: 1.0737699270248413


Running epoch 0, step 1042, batch 1042
Sampled inputs[:2]: tensor([[   0, 3115, 1640,  ...,  300,  266, 5453],
        [   0,  298, 2587,  ...,  298,  894,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9427e-04,  4.9160e-04, -1.4206e-04,  ...,  7.1243e-04,
         -1.1027e-03, -1.8251e-04],
        [-5.8562e-06, -4.0978e-06,  2.5518e-06,  ..., -5.2378e-06,
         -2.5593e-06, -4.0308e-06],
        [-8.8811e-06, -6.2734e-06,  4.0904e-06,  ..., -7.6890e-06,
         -3.5614e-06, -5.7518e-06],
        [-1.2070e-05, -8.4192e-06,  5.6550e-06,  ..., -1.0625e-05,
         -4.8503e-06, -8.0764e-06],
        [-1.5467e-05, -1.0923e-05,  7.1377e-06,  ..., -1.3351e-05,
         -6.4075e-06, -9.6262e-06]], device='cuda:0')
Loss: 1.0505027770996094


Running epoch 0, step 1043, batch 1043
Sampled inputs[:2]: tensor([[   0,   28, 2973,  ..., 8762, 2134,   27],
        [   0,  879,   27,  ..., 3958, 2875,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4744e-04,  4.5819e-04, -7.0907e-05,  ...,  7.4933e-04,
         -1.1881e-03, -1.8251e-04],
        [-8.0019e-06, -5.5730e-06,  3.4049e-06,  ..., -7.0035e-06,
         -3.4384e-06, -5.4017e-06],
        [-1.2204e-05, -8.5980e-06,  5.4762e-06,  ..., -1.0401e-05,
         -4.8429e-06, -7.8231e-06],
        [-1.6451e-05, -1.1444e-05,  7.4878e-06,  ..., -1.4216e-05,
         -6.5267e-06, -1.0848e-05],
        [-2.1189e-05, -1.4916e-05,  9.5367e-06,  ..., -1.8030e-05,
         -8.6725e-06, -1.3053e-05]], device='cuda:0')
Loss: 1.0975161790847778


Running epoch 0, step 1044, batch 1044
Sampled inputs[:2]: tensor([[   0,  300, 5631,  ..., 2278, 2669, 3011],
        [   0, 1615,  328,  ...,  266, 3133,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7786e-05,  3.5278e-04, -2.9443e-05,  ...,  7.8000e-04,
         -1.2552e-03, -2.2325e-04],
        [-9.8944e-06, -6.9290e-06,  4.2543e-06,  ..., -8.6874e-06,
         -4.0941e-06, -6.6236e-06],
        [-1.5020e-05, -1.0625e-05,  6.8098e-06,  ..., -1.2860e-05,
         -5.7369e-06, -9.5591e-06],
        [-2.0564e-05, -1.4365e-05,  9.4548e-06,  ..., -1.7807e-05,
         -7.8008e-06, -1.3441e-05],
        [-2.6286e-05, -1.8582e-05,  1.1951e-05,  ..., -2.2471e-05,
         -1.0386e-05, -1.6093e-05]], device='cuda:0')
Loss: 1.067337155342102


Running epoch 0, step 1045, batch 1045
Sampled inputs[:2]: tensor([[   0,  445,   18,  ..., 1478,  578,  494],
        [   0,  515,  352,  ..., 2326, 3595, 6887]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4255e-04,  6.0554e-04,  1.2375e-05,  ...,  7.9072e-04,
         -1.1774e-03, -1.2751e-04],
        [-1.1966e-05, -8.3745e-06,  5.0850e-06,  ..., -1.0423e-05,
         -4.9807e-06, -7.9200e-06],
        [-1.8209e-05, -1.2845e-05,  8.1286e-06,  ..., -1.5467e-05,
         -7.0333e-06, -1.1496e-05],
        [-2.4825e-05, -1.7315e-05,  1.1258e-05,  ..., -2.1324e-05,
         -9.4920e-06, -1.6063e-05],
        [-3.1918e-05, -2.2516e-05,  1.4305e-05,  ..., -2.7090e-05,
         -1.2726e-05, -1.9401e-05]], device='cuda:0')
Loss: 1.0945338010787964


Running epoch 0, step 1046, batch 1046
Sampled inputs[:2]: tensor([[    0,  3984, 13077,  ...,   287,   650,   413],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1150e-04,  7.6909e-04,  6.8110e-05,  ...,  8.7133e-04,
         -1.1935e-03,  6.9412e-05],
        [-1.3977e-05, -9.7901e-06,  5.9493e-06,  ..., -1.2234e-05,
         -5.8897e-06, -9.2536e-06],
        [-2.1175e-05, -1.4946e-05,  9.4697e-06,  ..., -1.8075e-05,
         -8.2776e-06, -1.3366e-05],
        [-2.8938e-05, -2.0206e-05,  1.3128e-05,  ..., -2.4974e-05,
         -1.1213e-05, -1.8731e-05],
        [-3.7223e-05, -2.6271e-05,  1.6689e-05,  ..., -3.1739e-05,
         -1.5020e-05, -2.2620e-05]], device='cuda:0')
Loss: 1.083852767944336


Running epoch 0, step 1047, batch 1047
Sampled inputs[:2]: tensor([[    0,   474,   221,  ..., 32291,   360,  2458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2595e-04,  7.2224e-04,  1.6162e-04,  ...,  1.1168e-03,
         -1.2788e-03,  1.3262e-04],
        [-1.5855e-05, -1.1124e-05,  6.8024e-06,  ..., -1.3910e-05,
         -6.5155e-06, -1.0520e-05],
        [-2.3946e-05, -1.6943e-05,  1.0788e-05,  ..., -2.0519e-05,
         -9.1530e-06, -1.5154e-05],
        [-3.2872e-05, -2.3037e-05,  1.5035e-05,  ..., -2.8491e-05,
         -1.2435e-05, -2.1353e-05],
        [-4.2260e-05, -2.9922e-05,  1.9103e-05,  ..., -3.6180e-05,
         -1.6734e-05, -2.5749e-05]], device='cuda:0')
Loss: 1.053901195526123
Graident accumulation at epoch 0, step 1047, batch 1047
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0336, -0.0097,  0.0404,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0287, -0.0151, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.7579e-04,  4.9797e-06, -1.6870e-04,  ...,  6.0878e-05,
          5.5514e-06,  1.9917e-04],
        [-1.1626e-05, -5.3583e-06,  3.2774e-06,  ..., -9.0114e-06,
         -3.0563e-06, -6.4235e-06],
        [ 8.0271e-05,  5.3465e-05, -2.4810e-05,  ...,  6.1265e-05,
          3.1551e-05,  3.1447e-05],
        [-7.1387e-06, -8.3986e-06,  2.7380e-06,  ..., -9.1206e-06,
         -1.5474e-06, -1.0562e-05],
        [-3.7415e-05, -2.6966e-05,  1.7488e-05,  ..., -3.2238e-05,
         -1.5329e-05, -2.3988e-05]], device='cuda:0')
optimizer state dict: tensor([[7.0278e-08, 8.1655e-08, 2.8448e-08,  ..., 5.2219e-08, 1.3333e-07,
         2.3451e-08],
        [6.3558e-11, 4.6297e-11, 7.9163e-12,  ..., 5.2667e-11, 1.1849e-11,
         2.1468e-11],
        [3.4621e-09, 1.7467e-09, 4.1284e-10,  ..., 2.5492e-09, 3.5537e-10,
         9.1083e-10],
        [7.9947e-10, 5.4017e-10, 1.0716e-10,  ..., 6.4763e-10, 9.5823e-11,
         2.5971e-10],
        [3.4735e-10, 1.8338e-10, 3.4572e-11,  ..., 2.5410e-10, 4.9613e-11,
         8.7301e-11]], device='cuda:0')
optimizer state dict: 131.0
lr: [0.0, 0.0]
scheduler_last_epoch: 131
End of epoch 0 | Validation PPL: 8.51653845385154 | Learning rate: 0.0
[2025-03-25 10:34:02,505] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
End of epoch checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/end_of_epoch_checkpoint.0, AFTER epoch 0
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà
wandb:   End of epoch ‚ñÅ
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  Learning Rate ‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: Validation PPL ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 959
wandb:   End of epoch 0
wandb:          Epoch 0
wandb:  Learning Rate 0.0
wandb:   Training PPL 5403.74076
wandb: Validation PPL 8.51654
wandb: 
wandb: üöÄ View run vague-river-307 at: https://wandb.ai/kenotron/brainlessgpt/runs/7dvyvouy
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_103152-7dvyvouy/logs
