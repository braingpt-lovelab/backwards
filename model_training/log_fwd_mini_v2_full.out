nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_134538-nqpb042a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-plasma-311
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/nqpb042a
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0,  8158,  1416,  ...,   413,    29,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5899e-04, -6.6102e-04, -7.2001e-05,  ...,  6.3213e-04,
         -2.1844e-04, -2.9654e-04],
        [-1.9372e-07, -1.3225e-07,  1.5087e-07,  ..., -7.1712e-08,
         -4.9546e-07, -4.7497e-07],
        [-5.4389e-07, -4.4517e-07,  3.5577e-07,  ..., -1.0990e-07,
         -1.4454e-06, -1.5125e-06],
        [-2.4773e-07, -1.8813e-07,  2.5332e-07,  ..., -2.4680e-08,
         -6.9663e-07, -6.4820e-07],
        [-2.4214e-07, -2.0955e-07,  2.7940e-08,  ..., -4.1211e-08,
         -6.3330e-07, -4.5821e-07]], device='cuda:0')
Loss: 1.381668210029602


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[   0,  266, 1422,  ...,  446, 1992,  586],
        [   0, 1607,   12,  ...,  895, 1503,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3242e-04, -1.1336e-03, -2.3221e-04,  ...,  3.8676e-04,
         -1.0851e-03, -2.9566e-04],
        [-4.7497e-08, -3.7253e-07,  4.4145e-07,  ...,  4.7497e-08,
         -1.3411e-06, -1.0226e-06],
        [-2.4773e-07, -1.0598e-06,  9.7044e-07,  ...,  3.3528e-07,
         -3.4720e-06, -2.8610e-06],
        [-2.0489e-08, -4.9174e-07,  6.6496e-07,  ...,  2.9942e-07,
         -1.8664e-06, -1.4044e-06],
        [-1.5926e-07, -4.6100e-07,  1.7229e-07,  ...,  1.3853e-07,
         -1.5087e-06, -8.8476e-07]], device='cuda:0')
Loss: 1.3763436079025269


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0,    14,   381,  ...,  2195,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5004e-04, -1.8562e-03, -1.4519e-04,  ...,  7.4696e-04,
         -2.1892e-03,  6.2129e-04],
        [ 6.3796e-07, -3.4890e-07,  1.1344e-06,  ...,  3.5483e-07,
         -2.3246e-06, -1.7565e-06],
        [ 1.6745e-06, -1.0903e-06,  2.6990e-06,  ...,  1.3262e-06,
         -6.1393e-06, -4.8727e-06],
        [ 1.0449e-06, -4.8085e-07,  1.5963e-06,  ...,  8.6939e-07,
         -3.1553e-06, -2.4401e-06],
        [ 4.2189e-07, -4.8790e-07,  6.5658e-07,  ...,  4.2538e-07,
         -2.3991e-06, -1.4286e-06]], device='cuda:0')
Loss: 1.37981116771698


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,   995,    13,  ...,  3494,   367,  6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0775e-05, -1.9968e-03,  7.1347e-04,  ...,  1.5155e-03,
         -3.4760e-03,  9.0713e-04],
        [ 5.7416e-07, -6.9721e-07,  1.1905e-06,  ...,  4.2096e-07,
         -2.7772e-06, -2.1476e-06],
        [ 1.4780e-06, -2.0813e-06,  2.7193e-06,  ...,  1.5851e-06,
         -7.2718e-06, -5.9605e-06],
        [ 9.5973e-07, -9.9867e-07,  1.6694e-06,  ...,  1.0920e-06,
         -3.7365e-06, -2.9579e-06],
        [ 3.2410e-07, -8.6601e-07,  6.0163e-07,  ...,  5.3435e-07,
         -2.8722e-06, -1.7490e-06]], device='cuda:0')
Loss: 1.375632643699646


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[    0,   287,  1477,  ...,   997,   292,  4471],
        [    0,  3227,   278,  ...,  2950,    14, 15544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9980e-04, -2.4794e-03,  1.1284e-03,  ...,  2.0875e-03,
         -4.4334e-03,  8.0653e-04],
        [ 8.2748e-07, -7.6753e-07,  1.6338e-06,  ...,  5.7556e-07,
         -3.6191e-06, -2.6114e-06],
        [ 2.1299e-06, -2.4408e-06,  3.6954e-06,  ...,  2.1067e-06,
         -9.4324e-06, -7.2494e-06],
        [ 1.2969e-06, -1.1579e-06,  2.2543e-06,  ...,  1.3397e-06,
         -4.7274e-06, -3.5539e-06],
        [ 5.1782e-07, -9.8243e-07,  8.9407e-07,  ...,  7.0198e-07,
         -3.7216e-06, -2.1365e-06]], device='cuda:0')
Loss: 1.377576470375061


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[    0, 29368,    13,  ...,   376,    88,  3333],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1308e-03, -2.4198e-03,  1.9124e-03,  ...,  2.4509e-03,
         -5.7382e-03,  1.3137e-03],
        [ 1.0566e-06, -7.0466e-07,  1.9989e-06,  ...,  5.8621e-07,
         -4.2748e-06, -3.2857e-06],
        [ 2.5956e-06, -2.4250e-06,  4.5187e-06,  ...,  2.2007e-06,
         -1.0990e-05, -8.9630e-06],
        [ 1.6005e-06, -1.1055e-06,  2.7684e-06,  ...,  1.4422e-06,
         -5.5172e-06, -4.4443e-06],
        [ 6.8638e-07, -9.5484e-07,  1.1194e-06,  ...,  7.0256e-07,
         -4.3325e-06, -2.6692e-06]], device='cuda:0')
Loss: 1.3740524053573608


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[   0,   12, 4957,  ...,  944,  278,  609],
        [   0, 5054, 3945,  ...,  272,  278,  516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9466e-03, -2.5292e-03,  2.1365e-03,  ...,  2.5738e-03,
         -5.9757e-03,  1.5254e-03],
        [ 1.1954e-06, -7.5449e-07,  2.2261e-06,  ...,  5.6083e-07,
         -4.7516e-06, -3.6545e-06],
        [ 2.8973e-06, -2.5750e-06,  4.9433e-06,  ...,  2.2176e-06,
         -1.2003e-05, -9.9391e-06],
        [ 1.7895e-06, -1.2108e-06,  3.1036e-06,  ...,  1.4785e-06,
         -6.1467e-06, -5.0254e-06],
        [ 7.7067e-07, -1.0363e-06,  1.2191e-06,  ...,  6.8510e-07,
         -4.8243e-06, -3.0044e-06]], device='cuda:0')
Loss: 1.379459261894226


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 3164,   12,  ...,  984,  344, 3993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3608e-03, -2.9404e-03,  1.5434e-03,  ...,  2.8837e-03,
         -6.7349e-03,  6.1856e-04],
        [ 1.4310e-06, -7.1793e-07,  2.6005e-06,  ...,  8.2346e-07,
         -5.8096e-06, -4.5970e-06],
        [ 3.5195e-06, -2.6067e-06,  5.7741e-06,  ...,  3.0223e-06,
         -1.4745e-05, -1.2428e-05],
        [ 2.1099e-06, -1.1372e-06,  3.6624e-06,  ...,  1.9125e-06,
         -7.5623e-06, -6.2995e-06],
        [ 9.5880e-07, -1.0687e-06,  1.4231e-06,  ...,  9.6822e-07,
         -5.8152e-06, -3.7830e-06]], device='cuda:0')
Loss: 1.3757104873657227
Graident accumulation at epoch 0, step 7, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3608e-04, -2.9404e-04,  1.5434e-04,  ...,  2.8837e-04,
         -6.7349e-04,  6.1856e-05],
        [ 1.4310e-07, -7.1793e-08,  2.6005e-07,  ...,  8.2346e-08,
         -5.8096e-07, -4.5970e-07],
        [ 3.5195e-07, -2.6067e-07,  5.7741e-07,  ...,  3.0223e-07,
         -1.4745e-06, -1.2428e-06],
        [ 2.1099e-07, -1.1372e-07,  3.6624e-07,  ...,  1.9125e-07,
         -7.5623e-07, -6.2995e-07],
        [ 9.5880e-08, -1.0687e-07,  1.4231e-07,  ...,  9.6822e-08,
         -5.8152e-07, -3.7830e-07]], device='cuda:0')
optimizer state dict: tensor([[5.5732e-09, 8.6460e-09, 2.3822e-09,  ..., 8.3158e-09, 4.5359e-08,
         3.8262e-10],
        [2.0477e-15, 5.1543e-16, 6.7625e-15,  ..., 6.7809e-16, 3.3751e-14,
         2.1132e-14],
        [1.2387e-14, 6.7947e-15, 3.3340e-14,  ..., 9.1340e-15, 2.1741e-13,
         1.5444e-13],
        [4.4517e-15, 1.2932e-15, 1.3413e-14,  ..., 3.6575e-15, 5.7189e-14,
         3.9683e-14],
        [9.1929e-16, 1.1421e-15, 2.0251e-15,  ..., 9.3745e-16, 3.3816e-14,
         1.4311e-14]], device='cuda:0')
optimizer state dict: 1.0
lr: [2.5445292620865143e-06, 2.5445292620865143e-06]
scheduler_last_epoch: 1


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0, 1594,  586,  ...,   13,  701,  308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1718e-04, -1.1122e-04,  6.3018e-04,  ..., -3.6876e-04,
          8.5216e-04, -1.0828e-03],
        [ 3.0175e-07, -5.1782e-07,  3.5390e-07,  ...,  3.3062e-08,
         -7.7486e-07, -3.5577e-07],
        [ 7.1153e-07, -1.4082e-06,  8.3074e-07,  ...,  1.9372e-07,
         -1.8030e-06, -8.6427e-07],
        [ 4.5449e-07, -7.8604e-07,  5.2899e-07,  ...,  1.5460e-07,
         -1.0803e-06, -5.4389e-07],
        [ 2.7381e-07, -5.7369e-07,  2.4028e-07,  ...,  9.5926e-08,
         -8.4192e-07, -3.4086e-07]], device='cuda:0')
Loss: 1.3777410984039307


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[    0,  3806,    13,  ..., 11786,  2254,   221],
        [    0,    18,   271,  ...,  4868,   963,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4393e-04, -9.4308e-04,  7.7401e-04,  ..., -7.8146e-04,
          5.1069e-04, -1.7971e-03],
        [ 6.8918e-07, -8.0094e-07,  8.2701e-07,  ..., -1.4110e-07,
         -1.4491e-06, -1.0375e-06],
        [ 1.6876e-06, -2.2277e-06,  1.8738e-06,  ..., -1.2666e-07,
         -3.4869e-06, -2.5555e-06],
        [ 1.0394e-06, -1.2163e-06,  1.2107e-06,  ...,  7.4506e-09,
         -2.0154e-06, -1.5348e-06],
        [ 5.8115e-07, -8.5123e-07,  5.5693e-07,  ..., -2.9802e-08,
         -1.4789e-06, -8.8103e-07]], device='cuda:0')
Loss: 1.3762363195419312


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[   0, 2383, 9843,  ...,  401, 3959,  300],
        [   0, 2816,  292,  ..., 3662,  461, 2723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2450e-04, -1.5055e-03,  1.5079e-03,  ..., -9.1928e-04,
          1.1402e-04, -1.9344e-03],
        [ 1.1493e-06, -5.6066e-07,  1.0580e-06,  ..., -1.1409e-07,
         -1.8999e-06, -1.6633e-06],
        [ 2.9020e-06, -1.7099e-06,  2.4140e-06,  ...,  7.6368e-08,
         -4.7684e-06, -4.2766e-06],
        [ 1.8217e-06, -8.8476e-07,  1.5609e-06,  ...,  1.8254e-07,
         -2.7157e-06, -2.5406e-06],
        [ 1.0282e-06, -6.5006e-07,  6.2771e-07,  ...,  4.6100e-08,
         -1.9819e-06, -1.3765e-06]], device='cuda:0')
Loss: 1.3759078979492188


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0, 32878,   593,  ...,   437,  1329,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0863e-04, -1.7732e-03,  1.5526e-03,  ..., -3.7463e-04,
         -1.2299e-03, -1.5848e-03],
        [ 1.1937e-06, -5.3586e-07,  1.2247e-06,  ...,  0.0000e+00,
         -2.6636e-06, -2.1961e-06],
        [ 3.1115e-06, -1.8091e-06,  2.9169e-06,  ...,  5.1782e-07,
         -7.0035e-06, -6.0201e-06],
        [ 1.9185e-06, -9.1293e-07,  1.8608e-06,  ...,  3.9395e-07,
         -3.6471e-06, -3.2969e-06],
        [ 1.0678e-06, -7.0198e-07,  6.9570e-07,  ...,  1.9418e-07,
         -2.7604e-06, -1.8086e-06]], device='cuda:0')
Loss: 1.3792593479156494


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,    5, 2122,  271],
        [   0,  381, 1659,  ..., 1403,  271, 6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5158e-03, -1.4043e-03,  1.3883e-03,  ...,  8.5730e-05,
         -1.6945e-03, -2.2113e-03],
        [ 1.2298e-06, -6.4762e-07,  1.7015e-06,  ..., -1.3225e-07,
         -3.2336e-06, -2.5686e-06],
        [ 3.1639e-06, -2.0410e-06,  3.8408e-06,  ...,  2.8312e-07,
         -8.1584e-06, -6.8881e-06],
        [ 1.9786e-06, -1.0759e-06,  2.5425e-06,  ...,  2.8498e-07,
         -4.3772e-06, -3.8184e-06],
        [ 1.0391e-06, -8.0490e-07,  1.0235e-06,  ...,  1.1176e-07,
         -3.3416e-06, -2.0992e-06]], device='cuda:0')
Loss: 1.3794163465499878


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[    0,   266,  2552,  ...,    13, 16179,   800],
        [    0, 17471,  7279,  ...,   328,  6179,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6272e-03, -1.6848e-03,  2.7662e-03,  ...,  3.0864e-04,
         -1.7143e-03, -1.8994e-03],
        [ 1.2647e-06, -6.8883e-07,  2.0014e-06,  ..., -2.1793e-07,
         -3.9451e-06, -3.4738e-06],
        [ 3.1700e-06, -2.2217e-06,  4.5560e-06,  ...,  1.4063e-07,
         -9.9912e-06, -9.3020e-06],
        [ 1.9962e-06, -1.0990e-06,  2.8983e-06,  ...,  2.2934e-07,
         -5.1521e-06, -4.8615e-06],
        [ 1.0170e-06, -8.5775e-07,  1.1837e-06,  ..., -1.8626e-09,
         -4.0419e-06, -2.8852e-06]], device='cuda:0')
Loss: 1.3737728595733643


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0, 9855,  278,  ...,  266, 3134,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8792e-03, -1.9441e-03,  3.3766e-03,  ...,  2.4101e-04,
         -1.8999e-03, -2.0666e-03],
        [ 1.4268e-06, -8.2946e-07,  2.3218e-06,  ..., -2.2852e-07,
         -4.9062e-06, -4.2859e-06],
        [ 3.3916e-06, -2.6948e-06,  5.4166e-06,  ...,  1.9209e-07,
         -1.2405e-05, -1.1507e-05],
        [ 2.2588e-06, -1.2740e-06,  3.3751e-06,  ...,  2.5670e-07,
         -6.4932e-06, -6.0461e-06],
        [ 1.1283e-06, -1.0161e-06,  1.3830e-06,  ...,  4.3947e-09,
         -4.9546e-06, -3.5856e-06]], device='cuda:0')
Loss: 1.376901626586914


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[    0,   199, 14973,  ...,   638,  1119,  1329],
        [    0,  2352,  4275,  ..., 10518,   342,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9947e-03, -2.7854e-03,  3.0757e-03,  ...,  5.7582e-04,
         -2.7795e-03, -2.5766e-03],
        [ 1.7732e-06, -8.6369e-07,  2.5518e-06,  ..., -3.1467e-07,
         -5.4389e-06, -4.9416e-06],
        [ 4.3527e-06, -2.8438e-06,  5.8897e-06,  ...,  5.3318e-08,
         -1.3791e-05, -1.3236e-05],
        [ 2.8288e-06, -1.3004e-06,  3.7737e-06,  ...,  2.0477e-07,
         -7.2084e-06, -7.0147e-06],
        [ 1.4487e-06, -1.0458e-06,  1.5018e-06,  ..., -3.7282e-08,
         -5.5172e-06, -4.1630e-06]], device='cuda:0')
Loss: 1.3801281452178955
Graident accumulation at epoch 0, step 15, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0151,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0023, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0021],
        [-0.0172,  0.0140, -0.0267,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.1194e-04, -5.4317e-04,  4.4648e-04,  ...,  3.1712e-04,
         -8.8410e-04, -2.0199e-04],
        [ 3.0611e-07, -1.5098e-07,  4.8923e-07,  ...,  4.2645e-08,
         -1.0668e-06, -9.0789e-07],
        [ 7.5203e-07, -5.1898e-07,  1.1086e-06,  ...,  2.7734e-07,
         -2.7061e-06, -2.4421e-06],
        [ 4.7277e-07, -2.3238e-07,  7.0699e-07,  ...,  1.9260e-07,
         -1.4015e-06, -1.2684e-06],
        [ 2.3116e-07, -2.0076e-07,  2.7825e-07,  ...,  8.3412e-08,
         -1.0751e-06, -7.5677e-07]], device='cuda:0')
optimizer state dict: tensor([[2.1525e-08, 1.6396e-08, 1.1840e-08,  ..., 8.6391e-09, 5.3040e-08,
         7.0213e-09],
        [5.1900e-15, 1.2609e-15, 1.3268e-14,  ..., 7.7643e-16, 6.3299e-14,
         4.5531e-14],
        [3.1321e-14, 1.4875e-14, 6.7995e-14,  ..., 9.1278e-15, 4.0738e-13,
         3.2948e-13],
        [1.2449e-14, 2.9829e-15, 2.7641e-14,  ..., 3.6958e-15, 1.0909e-13,
         8.8850e-14],
        [3.0170e-15, 2.2346e-15, 4.2784e-15,  ..., 9.3790e-16, 6.4221e-14,
         3.1628e-14]], device='cuda:0')
optimizer state dict: 2.0
lr: [5.0890585241730285e-06, 5.0890585241730285e-06]
scheduler_last_epoch: 2


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8874e-04, -4.6690e-04,  5.7047e-04,  ...,  2.9755e-04,
         -5.0614e-05,  2.1809e-04],
        [-9.9652e-08, -6.5193e-07,  7.3761e-07,  ..., -1.4901e-07,
         -6.6310e-07, -6.0350e-07],
        [-2.3376e-07, -1.5423e-06,  1.5348e-06,  ..., -2.3562e-07,
         -1.4156e-06, -1.5050e-06],
        [-7.2177e-08, -7.3388e-07,  9.0897e-07,  ..., -5.3784e-08,
         -7.5623e-07, -7.2643e-07],
        [-1.1129e-07, -5.5134e-07,  5.1036e-07,  ..., -7.9628e-08,
         -5.7742e-07, -4.3027e-07]], device='cuda:0')
Loss: 1.3625731468200684


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[    0,  1581, 11884,  ...,  7031,   689,   527],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9855e-04, -1.6552e-03,  2.4885e-04,  ..., -1.2108e-04,
         -3.4685e-04,  3.6158e-05],
        [-3.2969e-07, -1.6056e-06,  1.1921e-06,  ..., -5.1223e-07,
         -1.7062e-06, -8.0280e-07],
        [-8.1118e-07, -4.0457e-06,  2.5928e-06,  ..., -1.0328e-06,
         -3.9786e-06, -2.0526e-06],
        [-3.5716e-07, -2.0154e-06,  1.5460e-06,  ..., -4.5798e-07,
         -2.0973e-06, -9.9465e-07],
        [-3.9814e-07, -1.6615e-06,  8.9221e-07,  ..., -4.3726e-07,
         -1.8068e-06, -5.4622e-07]], device='cuda:0')
Loss: 1.36259925365448


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  365, 1943,  259],
        [   0,  360,  259,  ...,   14,  381, 1371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9189e-04, -1.9352e-03,  1.8489e-03,  ..., -2.5120e-04,
         -6.4746e-04, -3.7440e-04],
        [-8.5868e-07, -2.2501e-06,  1.6913e-06,  ..., -5.7789e-07,
         -2.7344e-06, -1.4249e-06],
        [-2.0629e-06, -5.6177e-06,  3.6806e-06,  ..., -1.0999e-06,
         -6.4224e-06, -3.5800e-06],
        [-9.5321e-07, -2.8163e-06,  2.1718e-06,  ..., -4.7847e-07,
         -3.3192e-06, -1.7695e-06],
        [-9.7929e-07, -2.3432e-06,  1.2722e-06,  ..., -4.6578e-07,
         -2.9244e-06, -1.1013e-06]], device='cuda:0')
Loss: 1.3605363368988037


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[   0, 1304,  292,  ..., 2101,  292,  474],
        [   0, 2088, 5370,  ..., 1110, 3380,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1132e-03, -1.5650e-03,  1.9859e-03,  ...,  3.3433e-05,
         -1.8649e-03, -6.2931e-04],
        [-9.5600e-07, -2.9765e-06,  2.3469e-06,  ..., -9.5414e-07,
         -3.5055e-06, -2.5127e-06],
        [-2.1509e-06, -7.3314e-06,  5.2527e-06,  ..., -1.8971e-06,
         -8.3074e-06, -6.2473e-06],
        [-1.0217e-06, -3.6359e-06,  2.9206e-06,  ..., -8.1002e-07,
         -4.1425e-06, -2.9765e-06],
        [-1.1944e-06, -3.1106e-06,  1.7155e-06,  ..., -7.6566e-07,
         -3.7067e-06, -2.0773e-06]], device='cuda:0')
Loss: 1.3606832027435303


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 42329,   472,  ...,   292,    33,  3092],
        [    0,  3889,  4039,  ...,   616, 22910,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4719e-03, -2.3097e-03,  2.5237e-03,  ..., -2.2847e-05,
         -2.6834e-03, -9.5378e-04],
        [-1.1059e-06, -3.4980e-06,  2.6859e-06,  ..., -1.4049e-06,
         -4.1686e-06, -2.9001e-06],
        [-2.5048e-06, -8.7246e-06,  6.0499e-06,  ..., -2.9476e-06,
         -9.9987e-06, -7.2233e-06],
        [-1.1688e-06, -4.2729e-06,  3.3975e-06,  ..., -1.2552e-06,
         -4.8727e-06, -3.4571e-06],
        [-1.3919e-06, -3.7439e-06,  1.9744e-06,  ..., -1.2406e-06,
         -4.5113e-06, -2.3940e-06]], device='cuda:0')
Loss: 1.3635929822921753


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,   892,   271,  ...,   278,   266, 10237],
        [    0,    14,  8383,  ...,   266,  1717,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7586e-03, -2.8073e-03,  3.0003e-03,  ...,  2.9360e-04,
         -2.7228e-03, -1.6377e-03],
        [-1.5865e-06, -4.1574e-06,  3.3453e-06,  ..., -1.3884e-06,
         -5.0589e-06, -3.6117e-06],
        [-3.6596e-06, -1.0327e-05,  7.5325e-06,  ..., -2.7698e-06,
         -1.2144e-05, -9.0040e-06],
        [-1.6904e-06, -5.0701e-06,  4.2170e-06,  ..., -1.1681e-06,
         -5.9381e-06, -4.3511e-06],
        [-1.8668e-06, -4.3996e-06,  2.5108e-06,  ..., -1.2085e-06,
         -5.4203e-06, -3.0459e-06]], device='cuda:0')
Loss: 1.360700011253357


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[   0,  586, 1016,  ..., 7151, 8280,  300],
        [   0, 7070,   86,  ...,  298, 4930,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7672e-03, -3.4596e-03,  2.9256e-03,  ..., -1.2931e-04,
         -3.3297e-03, -1.4508e-03],
        [-1.5733e-06, -4.7944e-06,  3.8370e-06,  ..., -1.5150e-06,
         -6.0648e-06, -4.3977e-06],
        [-3.6725e-06, -1.1958e-05,  8.5309e-06,  ..., -2.9560e-06,
         -1.4424e-05, -1.0956e-05],
        [-1.6785e-06, -5.8487e-06,  4.8093e-06,  ..., -1.2245e-06,
         -7.0781e-06, -5.3346e-06],
        [-1.8997e-06, -5.0589e-06,  2.8536e-06,  ..., -1.3026e-06,
         -6.4261e-06, -3.7537e-06]], device='cuda:0')
Loss: 1.3630280494689941


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[   0, 3699, 3058,  ...,  820, 5327, 8055],
        [   0,  287,  271,  ..., 1039, 4186,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8030e-03, -3.2233e-03,  3.9658e-03,  ...,  6.1071e-04,
         -3.5021e-03, -5.6378e-04],
        [-1.7828e-06, -5.5656e-06,  4.5225e-06,  ..., -1.7078e-06,
         -6.8396e-06, -4.8783e-06],
        [-4.2052e-06, -1.3687e-05,  9.9838e-06,  ..., -3.2429e-06,
         -1.6116e-05, -1.2066e-05],
        [-1.8834e-06, -6.6683e-06,  5.6289e-06,  ..., -1.3567e-06,
         -7.9051e-06, -5.8822e-06],
        [-2.1278e-06, -5.7705e-06,  3.3379e-06,  ..., -1.4208e-06,
         -7.1749e-06, -4.1113e-06]], device='cuda:0')
Loss: 1.3629704713821411
Graident accumulation at epoch 0, step 23, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0156,  0.0039,  ..., -0.0036,  0.0219, -0.0209],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.3105e-04, -8.1118e-04,  7.9841e-04,  ...,  3.4648e-04,
         -1.1459e-03, -2.3817e-04],
        [ 9.7216e-08, -6.9244e-07,  8.9255e-07,  ..., -1.3240e-07,
         -1.6440e-06, -1.3049e-06],
        [ 2.5630e-07, -1.8358e-06,  1.9962e-06,  ..., -7.4685e-08,
         -4.0471e-06, -3.4045e-06],
        [ 2.3715e-07, -8.7597e-07,  1.1992e-06,  ...,  3.7669e-08,
         -2.0518e-06, -1.7298e-06],
        [-4.7409e-09, -7.5773e-07,  5.8421e-07,  ..., -6.7015e-08,
         -1.6851e-06, -1.0922e-06]], device='cuda:0')
optimizer state dict: tensor([[2.4755e-08, 2.6769e-08, 2.7555e-08,  ..., 9.0034e-09, 6.5251e-08,
         7.3322e-09],
        [8.3634e-15, 3.2235e-14, 3.3707e-14,  ..., 3.6923e-15, 1.1002e-13,
         6.9283e-14],
        [4.8973e-14, 2.0219e-13, 1.6760e-13,  ..., 1.9635e-14, 6.6669e-13,
         4.7474e-13],
        [1.5984e-14, 4.7446e-14, 5.9298e-14,  ..., 5.5328e-15, 1.7147e-13,
         1.2336e-13],
        [7.5417e-15, 3.5531e-14, 1.5415e-14,  ..., 2.9558e-15, 1.1564e-13,
         4.8499e-14]], device='cuda:0')
optimizer state dict: 3.0
lr: [7.633587786259543e-06, 7.633587786259543e-06]
scheduler_last_epoch: 3


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[    0,  2919,  1482,  ...,   587, 20186,   275],
        [    0,  1552,   271,  ...,    13,   287,   995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3917e-04,  6.5980e-04, -2.7737e-04,  ...,  5.4120e-04,
         -3.5274e-04,  3.5727e-04],
        [-9.7603e-07, -1.8924e-06,  1.1846e-06,  ..., -3.3341e-07,
         -9.8348e-07, -9.3132e-07],
        [-1.8701e-06, -3.7253e-06,  2.2203e-06,  ..., -4.6566e-07,
         -1.9222e-06, -1.8626e-06],
        [-8.4937e-07, -1.6317e-06,  1.0431e-06,  ..., -2.1141e-07,
         -8.3074e-07, -8.1584e-07],
        [-9.7603e-07, -1.8701e-06,  1.0356e-06,  ..., -2.7753e-07,
         -1.0282e-06, -8.6054e-07]], device='cuda:0')
Loss: 1.3362468481063843


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[   0, 6275,   12,  ..., 2027, 2887,  287],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1908e-05,  5.0246e-04, -4.9524e-04,  ...,  9.8193e-04,
         -1.0209e-03,  5.2860e-04],
        [-2.3022e-06, -3.7923e-06,  1.9856e-06,  ..., -8.2888e-07,
         -1.3728e-06, -1.8179e-06],
        [-4.3884e-06, -7.3016e-06,  3.5837e-06,  ..., -1.3076e-06,
         -2.6450e-06, -3.5688e-06],
        [-2.0489e-06, -3.3677e-06,  1.7956e-06,  ..., -5.8208e-07,
         -1.1548e-06, -1.6019e-06],
        [-2.4661e-06, -3.9563e-06,  1.8217e-06,  ..., -7.6555e-07,
         -1.5497e-06, -1.8105e-06]], device='cuda:0')
Loss: 1.3406950235366821


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,   706,  1005,  ...,   278,   266,  5590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5532e-04, -4.4468e-05, -1.2943e-04,  ...,  9.3684e-04,
         -1.9184e-03,  4.9841e-04],
        [-3.5912e-06, -5.9679e-06,  2.4047e-06,  ..., -1.5106e-06,
         -2.2631e-06, -2.5779e-06],
        [-6.7577e-06, -1.1414e-05,  4.2878e-06,  ..., -2.5295e-06,
         -4.2766e-06, -4.9844e-06],
        [-3.2187e-06, -5.4240e-06,  2.2370e-06,  ..., -1.1893e-06,
         -2.0154e-06, -2.2948e-06],
        [-3.7998e-06, -6.2510e-06,  2.1253e-06,  ..., -1.4845e-06,
         -2.5406e-06, -2.5444e-06]], device='cuda:0')
Loss: 1.3319610357284546


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[    0,  1619,   938,  ...,   292, 10026, 14367],
        [    0,   368,  2418,  ...,  3275,  1116,  5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7823e-04, -1.9681e-04,  1.5147e-04,  ...,  3.8823e-05,
         -2.2945e-03, -5.6369e-04],
        [-4.7982e-06, -8.2031e-06,  3.3882e-06,  ..., -2.3004e-06,
         -3.1497e-06, -3.8669e-06],
        [-9.0227e-06, -1.5855e-05,  6.2250e-06,  ..., -3.9153e-06,
         -5.9679e-06, -7.6368e-06],
        [-4.0568e-06, -7.0035e-06,  3.0044e-06,  ..., -1.7108e-06,
         -2.6003e-06, -3.1814e-06],
        [-5.0515e-06, -8.4564e-06,  2.9784e-06,  ..., -2.2519e-06,
         -3.5167e-06, -3.6173e-06]], device='cuda:0')
Loss: 1.3356143236160278


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   14,  560,  ...,   12, 8593,  266],
        [   0, 3544,  417,  ...,  380,  381, 3794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2372e-04, -6.3692e-04, -1.8280e-04,  ..., -1.7384e-04,
         -2.5462e-03, -5.9191e-04],
        [-5.9903e-06, -1.0408e-05,  3.8370e-06,  ..., -3.1982e-06,
         -3.7048e-06, -4.6901e-06],
        [-1.1452e-05, -2.0593e-05,  7.1190e-06,  ..., -5.6885e-06,
         -7.0855e-06, -9.4399e-06],
        [-5.1074e-06, -9.1046e-06,  3.4738e-06,  ..., -2.4596e-06,
         -3.1181e-06, -3.8929e-06],
        [-6.3628e-06, -1.0915e-05,  3.3788e-06,  ..., -3.2205e-06,
         -4.1872e-06, -4.3921e-06]], device='cuda:0')
Loss: 1.3416661024093628


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,  2587,    27,  ...,   259,  2462,  1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0264e-03, -2.3574e-04, -2.3098e-04,  ...,  2.0166e-04,
         -3.0190e-03, -4.9201e-04],
        [-7.2122e-06, -1.2189e-05,  4.2021e-06,  ..., -4.0028e-06,
         -4.0084e-06, -5.3532e-06],
        [-1.3761e-05, -2.3931e-05,  7.6927e-06,  ..., -7.1563e-06,
         -7.6033e-06, -1.0654e-05],
        [-6.2026e-06, -1.0692e-05,  3.8389e-06,  ..., -3.0855e-06,
         -3.3621e-06, -4.4815e-06],
        [-7.8827e-06, -1.3046e-05,  3.6824e-06,  ..., -4.1630e-06,
         -4.5747e-06, -5.0776e-06]], device='cuda:0')
Loss: 1.3349192142486572


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,  266, 1371, 4811],
        [   0,  278, 6046,  ..., 1671,  199,  395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8105e-03, -4.3805e-04, -6.1679e-04,  ...,  6.0762e-04,
         -3.7943e-03, -2.8954e-04],
        [-8.5756e-06, -1.4186e-05,  4.9882e-06,  ..., -4.5244e-06,
         -4.8839e-06, -6.5230e-06],
        [-1.6369e-05, -2.7895e-05,  9.2499e-06,  ..., -8.1249e-06,
         -9.3020e-06, -1.3128e-05],
        [-7.2084e-06, -1.2212e-05,  4.4685e-06,  ..., -3.4468e-06,
         -3.9991e-06, -5.3979e-06],
        [-9.2760e-06, -1.5058e-05,  4.3754e-06,  ..., -4.7106e-06,
         -5.4948e-06, -6.1803e-06]], device='cuda:0')
Loss: 1.3320298194885254


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[    0, 41010,  6737,  ...,   963,   409,   382],
        [    0,   380,   333,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9135e-03, -1.0262e-03, -1.2266e-03,  ...,  1.3180e-03,
         -4.3903e-03, -2.5079e-04],
        [-9.6187e-06, -1.5952e-05,  5.5432e-06,  ..., -5.0869e-06,
         -5.6326e-06, -7.2531e-06],
        [-1.8425e-05, -3.1412e-05,  1.0278e-05,  ..., -9.1381e-06,
         -1.0710e-05, -1.4633e-05],
        [-8.1360e-06, -1.3791e-05,  4.9938e-06,  ..., -3.8957e-06,
         -4.6175e-06, -6.0648e-06],
        [-1.0371e-05, -1.6809e-05,  4.8075e-06,  ..., -5.2359e-06,
         -6.2846e-06, -6.8508e-06]], device='cuda:0')
Loss: 1.3351662158966064
Graident accumulation at epoch 0, step 31, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0408,  ...,  0.0222,  0.0062, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4929e-04, -8.3268e-04,  5.9591e-04,  ...,  4.4362e-04,
         -1.4703e-03, -2.3943e-04],
        [-8.7438e-07, -2.2184e-06,  1.3576e-06,  ..., -6.2785e-07,
         -2.0429e-06, -1.8997e-06],
        [-1.6119e-06, -4.7933e-06,  2.8243e-06,  ..., -9.8103e-07,
         -4.7134e-06, -4.5273e-06],
        [-6.0017e-07, -2.1675e-06,  1.5786e-06,  ..., -3.5567e-07,
         -2.3084e-06, -2.1633e-06],
        [-1.0414e-06, -2.3628e-06,  1.0065e-06,  ..., -5.8390e-07,
         -2.1450e-06, -1.6681e-06]], device='cuda:0')
optimizer state dict: tensor([[2.8391e-08, 2.7795e-08, 2.9032e-08,  ..., 1.0731e-08, 8.4461e-08,
         7.3877e-09],
        [1.0087e-13, 2.8666e-13, 6.4401e-14,  ..., 2.9565e-14, 1.4163e-13,
         1.2182e-13],
        [3.8842e-13, 1.1887e-12, 2.7307e-13,  ..., 1.0312e-13, 7.8073e-13,
         6.8839e-13],
        [8.2163e-14, 2.3759e-13, 8.4176e-14,  ..., 2.0704e-14, 1.9262e-13,
         1.6002e-13],
        [1.1510e-13, 3.1802e-13, 3.8512e-14,  ..., 3.0367e-14, 1.5502e-13,
         9.5384e-14]], device='cuda:0')
optimizer state dict: 4.0
lr: [1.0178117048346057e-05, 1.0178117048346057e-05]
scheduler_last_epoch: 4


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,     9,   300,  ...,  6838,   328, 18619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7005e-04, -3.7711e-04,  1.7922e-04,  ...,  2.5884e-04,
          1.4648e-04,  1.6331e-05],
        [-2.3693e-06, -3.6061e-06,  5.8487e-07,  ..., -1.2740e-06,
         -5.2154e-07, -9.4250e-07],
        [-3.2634e-06, -5.0068e-06,  7.7114e-07,  ..., -1.7211e-06,
         -7.4133e-07, -1.3560e-06],
        [-1.4380e-06, -2.2203e-06,  3.6508e-07,  ..., -7.5623e-07,
         -3.1479e-07, -5.8860e-07],
        [-2.6971e-06, -4.0233e-06,  4.8056e-07,  ..., -1.4305e-06,
         -5.8115e-07, -9.5367e-07]], device='cuda:0')
Loss: 1.3013445138931274


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0,  7994,    12,  ..., 13800,   278,   795],
        [    0,    25,    26,  ...,     9,   287,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3308e-03, -9.2919e-05, -2.0045e-04,  ..., -4.2472e-04,
          1.8812e-04, -1.9125e-04],
        [-4.4554e-06, -6.8545e-06,  1.2256e-06,  ..., -2.8387e-06,
         -1.0356e-06, -1.7248e-06],
        [-6.0648e-06, -9.4473e-06,  1.5832e-06,  ..., -3.7625e-06,
         -1.4231e-06, -2.4363e-06],
        [-2.8163e-06, -4.4107e-06,  8.1584e-07,  ..., -1.7546e-06,
         -6.5565e-07, -1.1288e-06],
        [-5.1707e-06, -7.8976e-06,  1.1250e-06,  ..., -3.2559e-06,
         -1.2554e-06, -1.7770e-06]], device='cuda:0')
Loss: 1.3158239126205444


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0,  689, 2149,  ..., 4263,   14,  292],
        [   0, 8754,   14,  ..., 6125,  394,  927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4990e-03, -3.7944e-04, -3.2583e-04,  ..., -3.9769e-04,
          2.1652e-04, -6.2080e-04],
        [-7.0632e-06, -1.0476e-05,  1.8701e-06,  ..., -4.1053e-06,
         -1.1171e-06, -2.9989e-06],
        [-9.9689e-06, -1.4961e-05,  2.5071e-06,  ..., -5.6177e-06,
         -1.5241e-06, -4.3884e-06],
        [-4.6492e-06, -6.9439e-06,  1.3001e-06,  ..., -2.5965e-06,
         -6.9290e-07, -2.0452e-06],
        [-8.1509e-06, -1.2010e-05,  1.7248e-06,  ..., -4.6417e-06,
         -1.3895e-06, -3.1404e-06]], device='cuda:0')
Loss: 1.305778980255127


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,   266,   554,  ..., 10679,  3790,   857],
        [    0,   659,   278,  ...,   769,  1728,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0918e-03, -4.4932e-06, -3.2583e-04,  ..., -3.5086e-04,
         -2.8128e-04, -6.7311e-04],
        [-9.7007e-06, -1.4141e-05,  2.4661e-06,  ..., -5.2378e-06,
         -1.5828e-06, -4.0196e-06],
        [-1.3813e-05, -2.0385e-05,  3.3453e-06,  ..., -7.2569e-06,
         -2.1574e-06, -5.9456e-06],
        [-6.2957e-06, -9.2685e-06,  1.6857e-06,  ..., -3.2522e-06,
         -9.7789e-07, -2.6859e-06],
        [-1.1116e-05, -1.6093e-05,  2.2613e-06,  ..., -5.9009e-06,
         -1.9744e-06, -4.2208e-06]], device='cuda:0')
Loss: 1.3038296699523926


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[   0,   17,  292,  ..., 8055,  365, 3125],
        [   0,   12, 2735,  ...,   12,  344, 1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0918e-03,  1.5234e-04, -2.3118e-04,  ..., -3.6479e-04,
         -2.3106e-04, -1.3473e-03],
        [-1.2442e-05, -1.7762e-05,  3.2336e-06,  ..., -7.3239e-06,
         -2.3241e-06, -5.0999e-06],
        [-1.7554e-05, -2.5392e-05,  4.3437e-06,  ..., -1.0073e-05,
         -3.2005e-06, -7.4282e-06],
        [-8.0094e-06, -1.1578e-05,  2.2072e-06,  ..., -4.5635e-06,
         -1.4324e-06, -3.3453e-06],
        [-1.4246e-05, -2.0117e-05,  3.0063e-06,  ..., -8.2254e-06,
         -2.8275e-06, -5.3234e-06]], device='cuda:0')
Loss: 1.3153663873672485


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,   342,  4014,  ...,   368,   408,  2105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5666e-03, -1.5280e-04, -5.6434e-04,  ..., -6.3249e-05,
         -5.2631e-04, -1.8109e-03],
        [-1.4737e-05, -2.1547e-05,  3.9823e-06,  ..., -8.4266e-06,
         -2.4084e-06, -5.7854e-06],
        [-2.1040e-05, -3.1233e-05,  5.4464e-06,  ..., -1.1720e-05,
         -3.3188e-06, -8.5160e-06],
        [-9.6112e-06, -1.4231e-05,  2.7623e-06,  ..., -5.2787e-06,
         -1.4629e-06, -3.8091e-06],
        [-1.7002e-05, -2.4647e-05,  3.7625e-06,  ..., -9.5218e-06,
         -2.9719e-06, -6.0461e-06]], device='cuda:0')
Loss: 1.3098199367523193


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  5024,  3846,  ...,  5880,  1377,    12],
        [    0, 31571,    13,  ...,   367,  2177,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8650e-03,  1.6610e-04, -3.4712e-04,  ..., -1.3600e-04,
         -1.2174e-03, -1.4067e-03],
        [-1.7196e-05, -2.5570e-05,  4.9099e-06,  ..., -9.7677e-06,
         -2.7306e-06, -6.8359e-06],
        [-2.4512e-05, -3.6925e-05,  6.7055e-06,  ..., -1.3553e-05,
         -3.7807e-06, -1.0066e-05],
        [-1.1288e-05, -1.7032e-05,  3.4329e-06,  ..., -6.1579e-06,
         -1.6808e-06, -4.5244e-06],
        [-1.9699e-05, -2.9027e-05,  4.6529e-06,  ..., -1.0990e-05,
         -3.3760e-06, -7.0967e-06]], device='cuda:0')
Loss: 1.3075282573699951


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0, 29073,   916,  ...,    12,   287,   850],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0280e-03,  1.3750e-04, -9.5960e-04,  ..., -4.2230e-04,
         -2.0969e-03, -1.5911e-03],
        [-1.9908e-05, -2.9594e-05,  5.8934e-06,  ..., -1.1206e-05,
         -3.8110e-06, -7.4245e-06],
        [-2.8476e-05, -4.2915e-05,  8.0839e-06,  ..., -1.5609e-05,
         -5.4124e-06, -1.1001e-05],
        [-1.2957e-05, -1.9550e-05,  4.0662e-06,  ..., -7.0073e-06,
         -2.3551e-06, -4.8969e-06],
        [-2.2620e-05, -3.3468e-05,  5.5954e-06,  ..., -1.2532e-05,
         -4.6054e-06, -7.6480e-06]], device='cuda:0')
Loss: 1.3024098873138428
Graident accumulation at epoch 0, step 39, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0032, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0045, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0222,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0672e-03, -7.3566e-04,  4.4036e-04,  ...,  3.5703e-04,
         -1.5330e-03, -3.7460e-04],
        [-2.7777e-06, -4.9559e-06,  1.8112e-06,  ..., -1.6856e-06,
         -2.2197e-06, -2.4522e-06],
        [-4.2983e-06, -8.6055e-06,  3.3503e-06,  ..., -2.4438e-06,
         -4.7833e-06, -5.1747e-06],
        [-1.8358e-06, -3.9058e-06,  1.8274e-06,  ..., -1.0208e-06,
         -2.3131e-06, -2.4367e-06],
        [-3.1992e-06, -5.4733e-06,  1.4654e-06,  ..., -1.7787e-06,
         -2.3911e-06, -2.2661e-06]], device='cuda:0')
optimizer state dict: tensor([[3.7532e-08, 2.7786e-08, 2.9924e-08,  ..., 1.0899e-08, 8.8774e-08,
         9.9120e-09],
        [4.9710e-13, 1.1622e-12, 9.9069e-14,  ..., 1.5510e-13, 1.5602e-13,
         1.7682e-13],
        [1.1989e-12, 3.0292e-12, 3.3815e-13,  ..., 3.4666e-13, 8.0924e-13,
         8.0872e-13],
        [2.4995e-13, 6.1957e-13, 1.0063e-13,  ..., 6.9785e-14, 1.9798e-13,
         1.8384e-13],
        [6.2664e-13, 1.4378e-12, 6.9782e-14,  ..., 1.8738e-13, 1.7607e-13,
         1.5378e-13]], device='cuda:0')
optimizer state dict: 5.0
lr: [1.2722646310432571e-05, 1.2722646310432571e-05]
scheduler_last_epoch: 5


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0,  401, 3704,  ...,   14, 1062, 1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0413e-04,  5.7657e-04,  1.2707e-04,  ..., -1.9771e-04,
         -7.5889e-05, -2.2969e-04],
        [-3.9339e-06, -4.9472e-06,  4.1910e-07,  ..., -1.9521e-06,
         -1.2107e-07, -6.2957e-07],
        [-4.2617e-06, -5.3644e-06,  4.4890e-07,  ..., -2.0713e-06,
         -1.2759e-07, -6.7428e-07],
        [-2.2799e-06, -2.8610e-06,  2.6450e-07,  ..., -1.1027e-06,
         -6.5193e-08, -3.3714e-07],
        [-5.6922e-06, -7.0333e-06,  5.0664e-07,  ..., -2.8461e-06,
         -2.2165e-07, -7.5251e-07]], device='cuda:0')
Loss: 1.2790526151657104


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0, 38495, 36253,  ..., 11006,  5699,    19],
        [    0,     5,  7523,  ...,   199,  8871,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9696e-04,  5.5073e-04, -1.3862e-04,  ..., -1.9771e-04,
          5.8935e-05, -9.2613e-07],
        [-8.2552e-06, -1.0580e-05,  8.7172e-07,  ..., -4.2766e-06,
         -7.0222e-07, -1.7025e-06],
        [-8.8513e-06, -1.1355e-05,  8.8103e-07,  ..., -4.5300e-06,
         -7.1991e-07, -1.8068e-06],
        [-4.5449e-06, -5.7966e-06,  5.2527e-07,  ..., -2.2873e-06,
         -3.7067e-07, -8.9593e-07],
        [-1.1653e-05, -1.4782e-05,  9.7603e-07,  ..., -6.1095e-06,
         -1.0487e-06, -2.1309e-06]], device='cuda:0')
Loss: 1.2893766164779663


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  5353,  5234,  ...,  1458,    14,  7157],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9322e-04,  9.5483e-04,  1.7768e-04,  ...,  2.1327e-04,
         -1.9842e-04, -2.8742e-04],
        [-1.2159e-05, -1.5497e-05,  1.4342e-06,  ..., -6.5565e-06,
         -1.0934e-06, -2.7083e-06],
        [-1.3322e-05, -1.7017e-05,  1.5181e-06,  ..., -7.1377e-06,
         -1.1930e-06, -3.0063e-06],
        [-6.6310e-06, -8.4341e-06,  8.5309e-07,  ..., -3.5018e-06,
         -5.7649e-07, -1.4286e-06],
        [-1.6689e-05, -2.1130e-05,  1.6242e-06,  ..., -9.1195e-06,
         -1.6373e-06, -3.3602e-06]], device='cuda:0')
Loss: 1.2851920127868652


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,  1901, 11083,  ...,   360,  6055,  2374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6341e-04,  6.2147e-04, -2.1682e-04,  ...,  2.2378e-05,
         -1.3402e-04, -4.3165e-04],
        [-1.5974e-05, -2.0564e-05,  2.2538e-06,  ..., -8.5682e-06,
         -1.9129e-06, -3.4794e-06],
        [ 1.0259e-04,  1.0303e-04,  1.9333e-05,  ...,  7.5866e-05,
          7.6884e-06, -4.2619e-05],
        [-8.7321e-06, -1.1265e-05,  1.3001e-06,  ..., -4.5896e-06,
         -1.0030e-06, -1.8347e-06],
        [-2.2143e-05, -2.8312e-05,  2.6375e-06,  ..., -1.1936e-05,
         -2.8592e-06, -4.3437e-06]], device='cuda:0')
Loss: 1.277353048324585


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  1387,   369,  ..., 15722,    14,  8157],
        [    0,  4100,    12,  ...,    13,  4710,  1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3857e-03,  3.2882e-04, -6.0079e-04,  ..., -8.1072e-04,
          2.7744e-05, -1.1646e-03],
        [-1.9655e-05, -2.5392e-05,  2.8498e-06,  ..., -1.0595e-05,
         -2.6692e-06, -4.6492e-06],
        [ 9.8660e-05,  9.7813e-05,  1.9940e-05,  ...,  7.3720e-05,
          6.8801e-06, -4.3893e-05],
        [-1.0803e-05, -1.3992e-05,  1.6615e-06,  ..., -5.7146e-06,
         -1.4277e-06, -2.4717e-06],
        [-2.7120e-05, -3.4809e-05,  3.2969e-06,  ..., -1.4618e-05,
         -3.9022e-06, -5.8189e-06]], device='cuda:0')
Loss: 1.2921456098556519


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[   0,  515,  266,  ...,   18, 3770, 1345],
        [   0, 6945, 2360,  ...,   30,  413,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2822e-03,  3.9307e-04, -6.3346e-04,  ..., -5.3327e-04,
         -5.1168e-04, -1.2839e-03],
        [-2.3827e-05, -3.0607e-05,  3.3006e-06,  ..., -1.2815e-05,
         -3.4478e-06, -6.0201e-06],
        [ 9.4368e-05,  9.2419e-05,  2.0404e-05,  ...,  7.1515e-05,
          6.0679e-06, -4.5338e-05],
        [-1.2890e-05, -1.6615e-05,  1.9222e-06,  ..., -6.7800e-06,
         -1.8058e-06, -3.1646e-06],
        [-3.2961e-05, -4.2111e-05,  3.8333e-06,  ..., -1.7717e-05,
         -5.0571e-06, -7.6368e-06]], device='cuda:0')
Loss: 1.2784932851791382


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[   0,  292,  685,  ...,  278, 3281,  298],
        [   0, 2530,  634,  ...,   15, 8808,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8776e-03,  5.4310e-04, -1.0105e-03,  ...,  8.8677e-05,
         -9.5718e-04, -9.1999e-04],
        [-2.7820e-05, -3.5584e-05,  3.4962e-06,  ..., -1.4991e-05,
         -3.6918e-06, -6.7279e-06],
        [ 9.0047e-05,  8.7025e-05,  2.0606e-05,  ...,  6.9190e-05,
          5.7662e-06, -4.6113e-05],
        [-1.5005e-05, -1.9267e-05,  2.0424e-06,  ..., -7.9200e-06,
         -1.9241e-06, -3.5353e-06],
        [-3.8564e-05, -4.9084e-05,  3.9954e-06,  ..., -2.0772e-05,
         -5.4855e-06, -8.5272e-06]], device='cuda:0')
Loss: 1.2850874662399292


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[   0, 4834,  278,  ...,   13, 8382,  669],
        [   0,  298,  894,  ..., 7605, 3220,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8899e-03,  6.4339e-04, -1.7657e-03,  ...,  4.3832e-04,
         -9.1477e-04, -5.3093e-04],
        [-3.1963e-05, -4.0770e-05,  4.0103e-06,  ..., -1.7032e-05,
         -4.2580e-06, -7.4357e-06],
        [ 8.5815e-05,  8.1720e-05,  2.1091e-05,  ...,  6.7134e-05,
          5.2037e-06, -4.6877e-05],
        [-1.7241e-05, -2.2039e-05,  2.3236e-06,  ..., -8.9929e-06,
         -2.2240e-06, -3.8967e-06],
        [-4.4614e-05, -5.6565e-05,  4.6100e-06,  ..., -2.3723e-05,
         -6.3945e-06, -9.4883e-06]], device='cuda:0')
Loss: 1.2751591205596924
Graident accumulation at epoch 0, step 47, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0033,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0275, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1494e-03, -5.9776e-04,  2.1975e-04,  ...,  3.6516e-04,
         -1.4712e-03, -3.9023e-04],
        [-5.6963e-06, -8.5373e-06,  2.0311e-06,  ..., -3.2203e-06,
         -2.4235e-06, -2.9506e-06],
        [ 4.7131e-06,  4.2699e-07,  5.1243e-06,  ...,  4.5139e-06,
         -3.7846e-06, -9.3449e-06],
        [-3.3763e-06, -5.7191e-06,  1.8770e-06,  ..., -1.8180e-06,
         -2.3041e-06, -2.5827e-06],
        [-7.3407e-06, -1.0582e-05,  1.7799e-06,  ..., -3.9731e-06,
         -2.7914e-06, -2.9883e-06]], device='cuda:0')
optimizer state dict: tensor([[4.1066e-08, 2.8172e-08, 3.3012e-08,  ..., 1.1080e-08, 8.9522e-08,
         1.0184e-08],
        [1.5182e-12, 2.8232e-12, 1.1505e-13,  ..., 4.4504e-13, 1.7399e-13,
         2.3194e-13],
        [8.5619e-12, 9.7043e-12, 7.8263e-13,  ..., 4.8532e-12, 8.3551e-13,
         3.0054e-12],
        [5.4694e-13, 1.1047e-12, 1.0592e-13,  ..., 1.5059e-13, 2.0273e-13,
         1.9884e-13],
        [2.6164e-12, 4.6360e-12, 9.0964e-14,  ..., 7.4996e-13, 2.1678e-13,
         2.4366e-13]], device='cuda:0')
optimizer state dict: 6.0
lr: [1.5267175572519086e-05, 1.5267175572519086e-05]
scheduler_last_epoch: 6


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,   680,   401,  ...,  2872,   292, 23535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5653e-04, -2.5848e-04, -4.4173e-04,  ..., -2.3710e-04,
          2.2132e-04, -5.2105e-04],
        [-4.9174e-06, -5.4240e-06,  3.4645e-07,  ..., -2.8759e-06,
         -8.5309e-07, -9.5367e-07],
        [-4.3809e-06, -4.9174e-06,  2.9989e-07,  ..., -2.5779e-06,
         -7.6368e-07, -8.7544e-07],
        [-2.5630e-06, -2.8610e-06,  1.9558e-07,  ..., -1.4976e-06,
         -4.5449e-07, -4.8801e-07],
        [-7.8678e-06, -8.7023e-06,  4.6752e-07,  ..., -4.6194e-06,
         -1.4156e-06, -1.4603e-06]], device='cuda:0')
Loss: 1.2634332180023193


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[   0, 6132,  300,  ...,   37,  271,  259],
        [   0,   25,    5,  ..., 3935,   14,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1421e-04, -4.0013e-04, -5.6866e-04,  ..., -4.3177e-04,
          1.4547e-04, -9.3817e-04],
        [-1.0520e-05, -1.0997e-05,  3.4497e-07,  ..., -6.3330e-06,
         -1.4789e-06, -2.1309e-06],
        [-9.1493e-06, -9.6560e-06,  3.0384e-07,  ..., -5.5134e-06,
         -1.3150e-06, -1.8813e-06],
        [-5.4836e-06, -5.7966e-06,  2.1211e-07,  ..., -3.3006e-06,
         -7.7300e-07, -1.0990e-06],
        [-1.6630e-05, -1.7464e-05,  3.9674e-07,  ..., -1.0103e-05,
         -2.4140e-06, -3.2410e-06]], device='cuda:0')
Loss: 1.2700655460357666


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  413,   16,  ...,  493, 2104,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3055e-04, -6.1019e-04, -6.2826e-04,  ..., -3.5716e-04,
          3.6507e-04, -1.2088e-03],
        [-1.5587e-05, -1.6361e-05,  5.0422e-07,  ..., -9.5367e-06,
         -1.9372e-06, -3.1367e-06],
        [-1.3649e-05, -1.4454e-05,  4.3050e-07,  ..., -8.3447e-06,
         -1.7155e-06, -2.7940e-06],
        [-8.1956e-06, -8.6874e-06,  3.1781e-07,  ..., -5.0217e-06,
         -1.0105e-06, -1.6354e-06],
        [-2.5332e-05, -2.6703e-05,  5.7090e-07,  ..., -1.5557e-05,
         -3.2634e-06, -4.9174e-06]], device='cuda:0')
Loss: 1.2677818536758423


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0406e-05, -9.3668e-04, -1.1237e-03,  ..., -3.3358e-04,
          7.1667e-04, -1.5146e-03],
        [-2.0653e-05, -2.1845e-05,  7.3612e-07,  ..., -1.2532e-05,
         -2.2855e-06, -4.2990e-06],
        [-1.8030e-05, -1.9222e-05,  6.0652e-07,  ..., -1.0937e-05,
         -2.0191e-06, -3.8221e-06],
        [-1.0923e-05, -1.1653e-05,  4.5937e-07,  ..., -6.6385e-06,
         -1.2079e-06, -2.2613e-06],
        [-3.3319e-05, -3.5405e-05,  8.3540e-07,  ..., -2.0325e-05,
         -3.8520e-06, -6.6981e-06]], device='cuda:0')
Loss: 1.2679698467254639


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[   0, 1145,   35,  ...,  300, 5192,  518],
        [   0,   19,  669,  ...,   14, 4053,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6562e-05, -4.9409e-04, -1.1890e-03,  ..., -4.5521e-04,
          8.1123e-04, -1.6303e-03],
        [-2.5451e-05, -2.7031e-05,  6.6069e-07,  ..., -1.5497e-05,
         -2.6952e-06, -5.3495e-06],
        [-2.2143e-05, -2.3663e-05,  5.5321e-07,  ..., -1.3471e-05,
         -2.3805e-06, -4.7423e-06],
        [-1.3396e-05, -1.4320e-05,  4.4709e-07,  ..., -8.1584e-06,
         -1.4156e-06, -2.7940e-06],
        [-4.1544e-05, -4.4227e-05,  6.2399e-07,  ..., -2.5421e-05,
         -4.5858e-06, -8.4341e-06]], device='cuda:0')
Loss: 1.2737042903900146


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   437, 38603,  ..., 37253, 10432,   278],
        [    0,   266,  1194,  ...,  2267,    15,  1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5744e-04, -3.5795e-04, -1.2472e-03,  ..., -4.7537e-04,
          1.1529e-03, -1.2223e-03],
        [-3.0607e-05, -3.2693e-05,  5.6802e-07,  ..., -1.8373e-05,
         -2.9840e-06, -6.2399e-06],
        [-2.6762e-05, -2.8759e-05,  4.5542e-07,  ..., -1.6049e-05,
         -2.6338e-06, -5.5507e-06],
        [-1.6063e-05, -1.7270e-05,  4.0286e-07,  ..., -9.6336e-06,
         -1.5572e-06, -3.2410e-06],
        [-4.9651e-05, -5.3167e-05,  3.7253e-07,  ..., -2.9951e-05,
         -5.0850e-06, -9.7156e-06]], device='cuda:0')
Loss: 1.2723544836044312


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[    0,  6010,   829,  ...,   668,  1784,   587],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7173e-04, -2.7326e-04, -1.5252e-03,  ..., -2.7720e-04,
          1.0380e-03, -1.4117e-03],
        [-3.5912e-05, -3.8266e-05,  6.2949e-07,  ..., -2.1547e-05,
         -3.5763e-06, -7.2904e-06],
        [-3.1322e-05, -3.3557e-05,  4.9383e-07,  ..., -1.8746e-05,
         -3.1479e-06, -6.4597e-06],
        [-1.8775e-05, -2.0117e-05,  4.4593e-07,  ..., -1.1228e-05,
         -1.8384e-06, -3.7663e-06],
        [-5.7995e-05, -6.1929e-05,  4.1211e-07,  ..., -3.4928e-05,
         -6.0610e-06, -1.1303e-05]], device='cuda:0')
Loss: 1.270372986793518


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[   0,  328,  957,  ...,  298,  275, 8570],
        [   0,  741,  300,  ...,   83, 7111,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2740e-03, -1.8047e-04, -1.6725e-03,  ..., -1.3202e-04,
          1.0920e-03, -1.1901e-03],
        [-4.0919e-05, -4.4286e-05,  7.0446e-07,  ..., -2.4587e-05,
         -3.9563e-06, -8.2143e-06],
        [-3.5882e-05, -3.9041e-05,  5.4599e-07,  ..., -2.1502e-05,
         -3.4906e-06, -7.3016e-06],
        [-2.1324e-05, -2.3186e-05,  4.9156e-07,  ..., -1.2770e-05,
         -2.0210e-06, -4.2263e-06],
        [-6.5982e-05, -7.1466e-05,  4.4331e-07,  ..., -3.9786e-05,
         -6.7055e-06, -1.2688e-05]], device='cuda:0')
Loss: 1.2612338066101074
Graident accumulation at epoch 0, step 55, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0162, -0.0191]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1619e-03, -5.5603e-04,  3.0529e-05,  ...,  3.1544e-04,
         -1.2149e-03, -4.7022e-04],
        [-9.2185e-06, -1.2112e-05,  1.8984e-06,  ..., -5.3569e-06,
         -2.5768e-06, -3.4769e-06],
        [ 6.5355e-07, -3.5198e-06,  4.6665e-06,  ...,  1.9123e-06,
         -3.7552e-06, -9.1406e-06],
        [-5.1710e-06, -7.4658e-06,  1.7385e-06,  ..., -2.9133e-06,
         -2.2758e-06, -2.7470e-06],
        [-1.3205e-05, -1.6671e-05,  1.6462e-06,  ..., -7.5544e-06,
         -3.1828e-06, -3.9583e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2648e-08, 2.8177e-08, 3.5776e-08,  ..., 1.1087e-08, 9.0625e-08,
         1.1590e-08],
        [3.1910e-12, 4.7816e-12, 1.1543e-13,  ..., 1.0491e-12, 1.8947e-13,
         2.9918e-13],
        [9.8409e-12, 1.1219e-11, 7.8214e-13,  ..., 5.3107e-12, 8.4686e-13,
         3.0557e-12],
        [1.0011e-12, 1.6412e-12, 1.0606e-13,  ..., 3.1352e-13, 2.0661e-13,
         2.1650e-13],
        [6.9675e-12, 9.7387e-12, 9.1070e-14,  ..., 2.3321e-12, 2.6153e-13,
         4.0441e-13]], device='cuda:0')
optimizer state dict: 7.0
lr: [1.78117048346056e-05, 1.78117048346056e-05]
scheduler_last_epoch: 7


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,    14,    23,  ...,   278,   266,  1462],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4085e-04,  1.2752e-04, -5.4260e-07,  ...,  3.0539e-05,
          7.2305e-06,  2.9546e-04],
        [-5.9009e-06, -5.5730e-06, -2.7753e-07,  ..., -3.8445e-06,
         -9.6858e-07, -1.2368e-06],
        [-4.5598e-06, -4.3213e-06, -2.3190e-07,  ..., -2.9802e-06,
         -7.5251e-07, -9.6112e-07],
        [-3.1292e-06, -2.9653e-06, -1.4435e-07,  ..., -2.0415e-06,
         -5.1409e-07, -6.4448e-07],
        [-1.0192e-05, -9.6560e-06, -5.6252e-07,  ..., -6.6757e-06,
         -1.7062e-06, -2.1011e-06]], device='cuda:0')
Loss: 1.2467494010925293


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   271,  2862,  ...,   287,  5699,    18],
        [    0,  9458,   278,  ...,    15,  5251, 27858]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6754e-04,  1.7927e-04,  1.1910e-04,  ...,  1.5945e-04,
          1.3666e-04,  3.9993e-04],
        [-1.1623e-05, -1.0997e-05, -4.6846e-07,  ..., -7.4804e-06,
         -1.8403e-06, -2.3320e-06],
        [-9.1493e-06, -8.6725e-06, -3.9581e-07,  ..., -5.9009e-06,
         -1.4752e-06, -1.8552e-06],
        [-6.3777e-06, -6.0499e-06, -2.4727e-07,  ..., -4.0978e-06,
         -1.0170e-06, -1.2591e-06],
        [-2.0266e-05, -1.9193e-05, -9.7230e-07,  ..., -1.3113e-05,
         -3.3230e-06, -3.9563e-06]], device='cuda:0')
Loss: 1.2520456314086914


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[   0,   13, 5005,  ...,  654,  344,  259],
        [   0, 1238,   14,  ...,  368,  940,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4538e-04,  1.8089e-04, -5.8327e-06,  ...,  3.9326e-04,
          2.4775e-05,  2.1546e-04],
        [-1.7166e-05, -1.6451e-05, -8.4098e-07,  ..., -1.1086e-05,
         -2.6636e-06, -3.3677e-06],
        [-1.3679e-05, -1.3173e-05, -7.0129e-07,  ..., -8.8662e-06,
         -2.1532e-06, -2.7046e-06],
        [-9.4026e-06, -9.0450e-06, -4.3726e-07,  ..., -6.0648e-06,
         -1.4640e-06, -1.8179e-06],
        [-3.0398e-05, -2.9206e-05, -1.7323e-06,  ..., -1.9729e-05,
         -4.8429e-06, -5.7891e-06]], device='cuda:0')
Loss: 1.248404860496521


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0,  344, 3693,  ..., 1782, 3679,  292],
        [   0, 2027,  365,  ...,  368, 1782,  394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4702e-05,  2.2710e-04, -2.8984e-05,  ...,  7.8692e-04,
          1.9217e-04,  4.0183e-04],
        [-2.2560e-05, -2.1309e-05, -1.0766e-06,  ..., -1.4424e-05,
         -3.4757e-06, -4.4852e-06],
        [-1.8150e-05, -1.7196e-05, -9.0618e-07,  ..., -1.1623e-05,
         -2.8387e-06, -3.6247e-06],
        [-1.2562e-05, -1.1891e-05, -5.6857e-07,  ..., -8.0019e-06,
         -1.9521e-06, -2.4624e-06],
        [-4.0650e-05, -3.8445e-05, -2.2464e-06,  ..., -2.6077e-05,
         -6.5044e-06, -7.8157e-06]], device='cuda:0')
Loss: 1.2457482814788818


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0,   266,   283,  ...,   271, 48829,   580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8045e-04,  2.8562e-04,  7.1124e-05,  ...,  7.5604e-04,
          2.6225e-04,  3.3864e-04],
        [-2.8431e-05, -2.6494e-05, -1.3784e-06,  ..., -1.8075e-05,
         -4.3847e-06, -5.4613e-06],
        [-2.2769e-05, -2.1309e-05, -1.1446e-06,  ..., -1.4484e-05,
         -3.5614e-06, -4.3996e-06],
        [-1.5870e-05, -1.4812e-05, -7.2597e-07,  ..., -1.0043e-05,
         -2.4624e-06, -2.9989e-06],
        [-5.1022e-05, -4.7624e-05, -2.8275e-06,  ..., -3.2485e-05,
         -8.1360e-06, -9.4473e-06]], device='cuda:0')
Loss: 1.2538642883300781


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0, 21930,    12,  ...,  2849,   863,   578],
        [    0,   278, 14971,  ...,  2341,   266,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0759e-05,  3.1102e-04,  9.9973e-05,  ...,  5.8930e-04,
          1.7759e-04,  1.3759e-04],
        [-3.4064e-05, -3.1710e-05, -1.5628e-06,  ..., -2.1607e-05,
         -5.2638e-06, -6.3665e-06],
        [-2.7210e-05, -2.5421e-05, -1.2899e-06,  ..., -1.7285e-05,
         -4.2580e-06, -5.1185e-06],
        [-1.9103e-05, -1.7822e-05, -8.1630e-07,  ..., -1.2085e-05,
         -2.9616e-06, -3.5129e-06],
        [-6.1452e-05, -5.7280e-05, -3.2373e-06,  ..., -3.9071e-05,
         -9.8199e-06, -1.1057e-05]], device='cuda:0')
Loss: 1.2508753538131714


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[   0,  741, 2985,  ...,  199,  769,  278],
        [   0,  278, 4452,  ...,   14,   18, 3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9090e-04,  4.1728e-05,  3.5908e-04,  ...,  5.5293e-04,
          1.8698e-05,  7.2425e-05],
        [-3.9846e-05, -3.7044e-05, -1.9185e-06,  ..., -2.5198e-05,
         -5.9791e-06, -7.5810e-06],
        [-3.1918e-05, -2.9743e-05, -1.5823e-06,  ..., -2.0191e-05,
         -4.8466e-06, -6.1095e-06],
        [-2.2456e-05, -2.0906e-05, -1.0091e-06,  ..., -1.4156e-05,
         -3.3788e-06, -4.2096e-06],
        [-7.2181e-05, -6.7174e-05, -3.9823e-06,  ..., -4.5687e-05,
         -1.1168e-05, -1.3217e-05]], device='cuda:0')
Loss: 1.235849380493164


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0, 3119,  278,  ...,  352,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6431e-04, -8.3537e-05,  5.2022e-04,  ...,  4.0979e-04,
         -8.0363e-05,  1.9687e-04],
        [-4.5329e-05, -4.2260e-05, -2.1411e-06,  ..., -2.8804e-05,
         -6.7428e-06, -8.5793e-06],
        [-3.6269e-05, -3.3915e-05, -1.7677e-06,  ..., -2.3067e-05,
         -5.4576e-06, -6.9104e-06],
        [-2.5630e-05, -2.3931e-05, -1.1199e-06,  ..., -1.6242e-05,
         -3.8259e-06, -4.7833e-06],
        [-8.1956e-05, -7.6473e-05, -4.4592e-06,  ..., -5.2154e-05,
         -1.2577e-05, -1.4916e-05]], device='cuda:0')
Loss: 1.2525867223739624
Graident accumulation at epoch 0, step 63, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0538, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0172,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0821e-03, -5.0878e-04,  7.9498e-05,  ...,  3.2488e-04,
         -1.1014e-03, -4.0352e-04],
        [-1.2830e-05, -1.5127e-05,  1.4945e-06,  ..., -7.7016e-06,
         -2.9934e-06, -3.9872e-06],
        [-3.0387e-06, -6.5593e-06,  4.0231e-06,  ..., -5.8563e-07,
         -3.9254e-06, -8.9176e-06],
        [-7.2169e-06, -9.1123e-06,  1.4526e-06,  ..., -4.2462e-06,
         -2.4308e-06, -2.9507e-06],
        [-2.0080e-05, -2.2651e-05,  1.0357e-06,  ..., -1.2014e-05,
         -4.1222e-06, -5.0541e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2738e-08, 2.8155e-08, 3.6011e-08,  ..., 1.1243e-08, 9.0541e-08,
         1.1617e-08],
        [5.2426e-12, 6.5627e-12, 1.1990e-13,  ..., 1.8777e-12, 2.3474e-13,
         3.7248e-13],
        [1.1147e-11, 1.2358e-11, 7.8449e-13,  ..., 5.8375e-12, 8.7580e-13,
         3.1004e-12],
        [1.6570e-12, 2.2122e-12, 1.0721e-13,  ..., 5.7701e-13, 2.2104e-13,
         2.3917e-13],
        [1.3677e-11, 1.5577e-11, 1.1086e-13,  ..., 5.0499e-12, 4.1944e-13,
         6.2649e-13]], device='cuda:0')
optimizer state dict: 8.0
lr: [1.9999985024557586e-05, 1.9999985024557586e-05]
scheduler_last_epoch: 8


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,    13,    19,  ..., 22111,  2489,    14],
        [    0,  1854,   292,  ...,   328,  1360,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6356e-05,  2.7158e-04,  8.5619e-05,  ...,  8.6750e-05,
         -5.2903e-05, -1.0299e-04],
        [-5.4538e-06, -4.6492e-06, -3.4459e-07,  ..., -4.0829e-06,
         -8.7917e-07, -8.7917e-07],
        [-4.0531e-06, -3.4571e-06, -2.6450e-07,  ..., -3.0398e-06,
         -6.5565e-07, -6.5193e-07],
        [-3.2783e-06, -2.7865e-06, -1.9278e-07,  ..., -2.4438e-06,
         -5.2154e-07, -5.2154e-07],
        [-1.0371e-05, -8.7619e-06, -7.0781e-07,  ..., -7.6890e-06,
         -1.6764e-06, -1.6019e-06]], device='cuda:0')
Loss: 1.248663067817688


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  1253,   287,  ...,  2988,    14,   417],
        [    0,   461,  1169,  ..., 14135,  2771,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0331e-05,  1.0098e-04,  1.5885e-04,  ...,  9.2330e-05,
         -2.1633e-04, -2.6794e-04],
        [-1.1027e-05, -9.2983e-06, -6.6496e-07,  ..., -7.8976e-06,
         -2.0117e-06, -1.8477e-06],
        [-8.3148e-06, -7.0333e-06, -5.0850e-07,  ..., -5.9605e-06,
         -1.5199e-06, -1.4007e-06],
        [-6.9141e-06, -5.8264e-06, -3.8743e-07,  ..., -4.9323e-06,
         -1.2554e-06, -1.1474e-06],
        [-2.0981e-05, -1.7643e-05, -1.3597e-06,  ..., -1.4961e-05,
         -3.8669e-06, -3.4049e-06]], device='cuda:0')
Loss: 1.2286880016326904


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[   0, 1356,  634,  ..., 6604,  634,   14],
        [   0,  759, 1184,  ...,  472,  346,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0953e-04,  3.8932e-06,  1.6977e-04,  ...,  1.4342e-04,
         -4.0828e-04, -3.8058e-04],
        [-1.6332e-05, -1.3739e-05, -8.8010e-07,  ..., -1.1474e-05,
         -2.9355e-06, -2.7157e-06],
        [-1.2487e-05, -1.0520e-05, -6.8080e-07,  ..., -8.7768e-06,
         -2.2464e-06, -2.0824e-06],
        [-1.0476e-05, -8.8066e-06, -5.2340e-07,  ..., -7.3314e-06,
         -1.8738e-06, -1.7248e-06],
        [-3.1173e-05, -2.6166e-05, -1.8161e-06,  ..., -2.1845e-05,
         -5.6624e-06, -5.0217e-06]], device='cuda:0')
Loss: 1.2218878269195557


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,    14,  3741,  ...,   278, 12472, 10257],
        [    0,  1295,  1178,  ...,  4808,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9734e-05,  1.4760e-04,  2.4762e-05,  ...,  9.4009e-05,
         -4.4760e-04, -3.4675e-04],
        [-2.1666e-05, -1.8150e-05, -1.0887e-06,  ..., -1.5318e-05,
         -3.7141e-06, -3.5167e-06],
        [-1.6570e-05, -1.3888e-05, -8.4471e-07,  ..., -1.1712e-05,
         -2.8461e-06, -2.6897e-06],
        [-1.4022e-05, -1.1742e-05, -6.5472e-07,  ..., -9.8795e-06,
         -2.3916e-06, -2.2389e-06],
        [-4.1485e-05, -3.4690e-05, -2.2668e-06,  ..., -2.9266e-05,
         -7.2047e-06, -6.5044e-06]], device='cuda:0')
Loss: 1.242640733718872


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0,  493,  221,  ...,  259,  726, 2786],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5154e-04,  6.0933e-05,  4.6923e-05,  ...,  1.1278e-04,
         -3.2423e-04, -2.6252e-04],
        [-2.7090e-05, -2.2858e-05, -1.3756e-06,  ..., -1.9252e-05,
         -4.6119e-06, -4.6119e-06],
        [-2.0713e-05, -1.7479e-05, -1.0664e-06,  ..., -1.4722e-05,
         -3.5278e-06, -3.5353e-06],
        [-1.7479e-05, -1.4737e-05, -8.2795e-07,  ..., -1.2383e-05,
         -2.9579e-06, -2.9355e-06],
        [-5.1618e-05, -4.3452e-05, -2.8517e-06,  ..., -3.6627e-05,
         -8.8811e-06, -8.5011e-06]], device='cuda:0')
Loss: 1.245609998703003


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[   0, 9010,   17,  ..., 3813, 1147,  199],
        [   0, 2793,  271,  ...,  374,  298,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0193e-04,  2.5924e-04,  6.0095e-05,  ..., -6.1282e-05,
         -9.0143e-05, -1.6429e-04],
        [-3.2693e-05, -2.7657e-05, -1.5907e-06,  ..., -2.3305e-05,
         -5.4576e-06, -5.5730e-06],
        [-2.4825e-05, -2.0996e-05, -1.2275e-06,  ..., -1.7703e-05,
         -4.1537e-06, -4.2357e-06],
        [-2.1055e-05, -1.7792e-05, -9.5461e-07,  ..., -1.4976e-05,
         -3.4980e-06, -3.5353e-06],
        [-6.2108e-05, -5.2392e-05, -3.3043e-06,  ..., -4.4227e-05,
         -1.0483e-05, -1.0230e-05]], device='cuda:0')
Loss: 1.250514268875122


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[   0, 1171, 2926,  ...,  259, 4288,  654],
        [   0,  266, 1441,  ..., 1817, 1589,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4753e-04,  3.2157e-05, -2.5481e-05,  ...,  1.1583e-04,
         -2.3271e-04, -1.8311e-04],
        [-3.7998e-05, -3.2306e-05, -1.9297e-06,  ..., -2.7105e-05,
         -6.3963e-06, -6.5416e-06],
        [-2.8729e-05, -2.4438e-05, -1.4827e-06,  ..., -2.0504e-05,
         -4.8503e-06, -4.9509e-06],
        [-2.4498e-05, -2.0802e-05, -1.1697e-06,  ..., -1.7434e-05,
         -4.1015e-06, -4.1574e-06],
        [-7.2181e-05, -6.1214e-05, -3.9935e-06,  ..., -5.1439e-05,
         -1.2286e-05, -1.2025e-05]], device='cuda:0')
Loss: 1.2396451234817505


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[    0,   741,   266,  ...,   271,  5166,   596],
        [    0, 14349,   278,  ...,   365,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9584e-04, -4.5106e-05, -1.0049e-04,  ...,  1.3625e-04,
         -2.8137e-04, -6.2372e-05],
        [-4.3571e-05, -3.7104e-05, -2.2016e-06,  ..., -3.1069e-05,
         -7.1637e-06, -7.4953e-06],
        [-3.2872e-05, -2.8029e-05, -1.6866e-06,  ..., -2.3454e-05,
         -5.4277e-06, -5.6550e-06],
        [-2.8133e-05, -2.3931e-05, -1.3411e-06,  ..., -2.0012e-05,
         -4.6007e-06, -4.7684e-06],
        [-8.2433e-05, -7.0035e-05, -4.5523e-06,  ..., -5.8711e-05,
         -1.3724e-05, -1.3709e-05]], device='cuda:0')
Loss: 1.2226446866989136
Graident accumulation at epoch 0, step 71, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0036,  0.0220, -0.0208],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0339],
        [ 0.0334, -0.0097,  0.0407,  ...,  0.0223,  0.0063, -0.0020],
        [-0.0171,  0.0140, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0235e-03, -4.6241e-04,  6.1500e-05,  ...,  3.0601e-04,
         -1.0194e-03, -3.6940e-04],
        [-1.5904e-05, -1.7325e-05,  1.1249e-06,  ..., -1.0038e-05,
         -3.4104e-06, -4.3380e-06],
        [-6.0221e-06, -8.7063e-06,  3.4521e-06,  ..., -2.8725e-06,
         -4.0757e-06, -8.5913e-06],
        [-9.3086e-06, -1.0594e-05,  1.1733e-06,  ..., -5.8228e-06,
         -2.6478e-06, -3.1324e-06],
        [-2.6315e-05, -2.7389e-05,  4.7689e-07,  ..., -1.6684e-05,
         -5.0824e-06, -5.9196e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2941e-08, 2.8129e-08, 3.5985e-08,  ..., 1.1251e-08, 9.0529e-08,
         1.1610e-08],
        [7.1358e-12, 7.9328e-12, 1.2463e-13,  ..., 2.8411e-12, 2.8583e-13,
         4.2829e-13],
        [1.2216e-11, 1.3131e-11, 7.8655e-13,  ..., 6.3818e-12, 9.0438e-13,
         3.1292e-12],
        [2.4468e-12, 2.7827e-12, 1.0890e-13,  ..., 9.7693e-13, 2.4198e-13,
         2.6166e-13],
        [2.0459e-11, 2.0466e-11, 1.3148e-13,  ..., 8.4917e-12, 6.0737e-13,
         8.1380e-13]], device='cuda:0')
optimizer state dict: 9.0
lr: [1.999900705266644e-05, 1.999900705266644e-05]
scheduler_last_epoch: 9


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0, 45050,   342,  ...,  3729,   287, 27888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7672e-05, -1.2870e-04, -2.2138e-04,  ...,  1.9908e-04,
          8.8755e-05,  1.6581e-05],
        [-4.7684e-06, -3.4720e-06,  5.2154e-08,  ..., -3.7849e-06,
         -5.6997e-07, -8.6799e-07],
        [-3.5465e-06, -2.5779e-06,  3.3295e-08,  ..., -2.8014e-06,
         -4.2841e-07, -6.3702e-07],
        [-3.7700e-06, -2.7269e-06,  4.5169e-08,  ..., -2.9802e-06,
         -4.5449e-07, -6.7055e-07],
        [-9.7156e-06, -7.0333e-06,  8.1956e-08,  ..., -7.6890e-06,
         -1.1921e-06, -1.7136e-06]], device='cuda:0')
Loss: 1.2366336584091187


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[   0, 6668,  565,  ...,  360,  259, 8166],
        [   0,   17,  292,  ..., 2269, 3887,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8935e-05, -3.8070e-05, -2.1354e-04,  ...,  2.6919e-04,
          1.5769e-04, -1.4675e-04],
        [-9.6858e-06, -6.9737e-06, -3.6322e-08,  ..., -7.5698e-06,
         -1.0915e-06, -1.5348e-06],
        [-7.2122e-06, -5.2005e-06, -3.3760e-08,  ..., -5.6177e-06,
         -8.1770e-07, -1.1325e-06],
        [-7.6145e-06, -5.4687e-06, -1.7229e-08,  ..., -5.9456e-06,
         -8.6240e-07, -1.1846e-06],
        [-1.9610e-05, -1.4067e-05, -1.2852e-07,  ..., -1.5289e-05,
         -2.2575e-06, -3.0026e-06]], device='cuda:0')
Loss: 1.2341158390045166


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0,   266,   824,  ...,  1799,   287,  6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7636e-04, -6.9506e-05, -2.2484e-04,  ...,  2.5093e-04,
          2.8939e-05, -1.1043e-04],
        [-1.4633e-05, -1.0490e-05, -8.1724e-08,  ..., -1.1221e-05,
         -1.8440e-06, -2.3358e-06],
        [-1.0908e-05, -7.8231e-06, -7.4040e-08,  ..., -8.3447e-06,
         -1.3877e-06, -1.7360e-06],
        [-1.1429e-05, -8.1807e-06, -4.6333e-08,  ..., -8.7619e-06,
         -1.4435e-06, -1.7993e-06],
        [-2.9445e-05, -2.1040e-05, -2.6450e-07,  ..., -2.2560e-05,
         -3.7700e-06, -4.5598e-06]], device='cuda:0')
Loss: 1.2281776666641235


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   409, 35049,  ...,    12,   699,   394],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0213e-04, -4.2872e-05, -2.5284e-04,  ...,  2.1058e-04,
         -2.8808e-05, -2.7660e-04],
        [-1.9550e-05, -1.4111e-05, -2.1770e-07,  ..., -1.5214e-05,
         -2.5295e-06, -3.2149e-06],
        [-1.4424e-05, -1.0416e-05, -1.7649e-07,  ..., -1.1206e-05,
         -1.8720e-06, -2.3618e-06],
        [-1.5140e-05, -1.0923e-05, -1.4459e-07,  ..., -1.1787e-05,
         -1.9539e-06, -2.4550e-06],
        [-3.9279e-05, -2.8282e-05, -5.7556e-07,  ..., -3.0547e-05,
         -5.1409e-06, -6.2883e-06]], device='cuda:0')
Loss: 1.2318204641342163


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,    13,  4467,  ...,  2390, 47857,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6891e-05,  2.8657e-05, -3.0233e-04,  ...,  4.4438e-05,
          9.6714e-05, -3.0814e-04],
        [-2.4647e-05, -1.7762e-05, -1.8824e-07,  ..., -1.8984e-05,
         -3.3826e-06, -4.1835e-06],
        [-1.8209e-05, -1.3128e-05, -1.5274e-07,  ..., -1.4007e-05,
         -2.5053e-06, -3.0883e-06],
        [-1.9044e-05, -1.3709e-05, -1.1222e-07,  ..., -1.4678e-05,
         -2.6096e-06, -3.1926e-06],
        [-4.9531e-05, -3.5584e-05, -5.3784e-07,  ..., -3.8087e-05,
         -6.8769e-06, -8.1956e-06]], device='cuda:0')
Loss: 1.2396296262741089


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[    0,  1497, 16170,  ...,  1888,  2350,   578],
        [    0,  2366,  5036,  ...,  1477,   352,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3089e-05, -5.2875e-05, -3.4770e-04,  ...,  2.2829e-04,
         -7.2182e-05, -3.5280e-04],
        [-2.9624e-05, -2.1249e-05, -2.4296e-07,  ..., -2.2799e-05,
         -4.1872e-06, -5.1260e-06],
        [-2.1920e-05, -1.5736e-05, -2.0396e-07,  ..., -1.6868e-05,
         -3.1088e-06, -3.7886e-06],
        [-2.3007e-05, -1.6481e-05, -1.5786e-07,  ..., -1.7717e-05,
         -3.2540e-06, -3.9376e-06],
        [-5.9605e-05, -4.2677e-05, -6.9989e-07,  ..., -4.5836e-05,
         -8.5384e-06, -1.0073e-05]], device='cuda:0')
Loss: 1.2327312231063843


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[   0, 4014,   88,  ..., 1103,   14, 1771],
        [   0, 1086,   14,  ...,  963,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5687e-05, -3.7996e-05, -3.6915e-04,  ...,  2.7318e-04,
         -5.4318e-05, -9.9269e-05],
        [-3.4660e-05, -2.5019e-05, -1.0233e-07,  ..., -2.6762e-05,
         -4.8801e-06, -5.9828e-06],
        [ 8.1697e-05,  5.2069e-05,  1.1001e-05,  ...,  1.0216e-04,
         -1.4078e-05, -4.4503e-07],
        [-2.6882e-05, -1.9386e-05, -4.3306e-08,  ..., -2.0757e-05,
         -3.7830e-06, -4.5933e-06],
        [-6.9499e-05, -5.0098e-05, -4.5775e-07,  ..., -5.3644e-05,
         -9.9242e-06, -1.1735e-05]], device='cuda:0')
Loss: 1.223325252532959


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   437,  1119,  ..., 32831,    83,   623],
        [    0, 13856,   278,  ...,    14,    69,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4427e-04, -1.3726e-04, -3.5012e-04,  ...,  3.3771e-04,
         -6.0221e-05, -1.0088e-04],
        [-3.9816e-05, -2.8789e-05, -1.1211e-07,  ..., -3.0696e-05,
         -5.5991e-06, -6.8881e-06],
        [ 7.7987e-05,  4.9357e-05,  1.0994e-05,  ...,  9.9311e-05,
         -1.4596e-05, -1.0970e-06],
        [-3.0726e-05, -2.2188e-05, -4.1990e-08,  ..., -2.3678e-05,
         -4.3083e-06, -5.2601e-06],
        [-7.9572e-05, -5.7429e-05, -4.9872e-07,  ..., -6.1333e-05,
         -1.1340e-05, -1.3463e-05]], device='cuda:0')
Loss: 1.230455994606018
Graident accumulation at epoch 0, step 79, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0164],
        [ 0.0046, -0.0155,  0.0039,  ..., -0.0035,  0.0220, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0338],
        [ 0.0334, -0.0096,  0.0407,  ...,  0.0223,  0.0063, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.0673e-04, -4.2990e-04,  2.0338e-05,  ...,  3.0918e-04,
         -9.2349e-04, -3.4255e-04],
        [-1.8295e-05, -1.8471e-05,  1.0012e-06,  ..., -1.2104e-05,
         -3.6293e-06, -4.5930e-06],
        [ 2.3788e-06, -2.9000e-06,  4.2063e-06,  ...,  7.3458e-06,
         -5.1277e-06, -7.8419e-06],
        [-1.1450e-05, -1.1754e-05,  1.0517e-06,  ..., -7.6083e-06,
         -2.8139e-06, -3.3452e-06],
        [-3.1641e-05, -3.0393e-05,  3.7933e-07,  ..., -2.1149e-05,
         -5.7081e-06, -6.6739e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2919e-08, 2.8120e-08, 3.6071e-08,  ..., 1.1354e-08, 9.0442e-08,
         1.1608e-08],
        [8.7140e-12, 8.7537e-12, 1.2452e-13,  ..., 3.7806e-12, 3.1689e-13,
         4.7531e-13],
        [1.8286e-11, 1.5554e-11, 9.0663e-13,  ..., 1.6238e-11, 1.1165e-12,
         3.1273e-12],
        [3.3885e-12, 3.2722e-12, 1.0879e-13,  ..., 1.5366e-12, 2.6030e-13,
         2.8907e-13],
        [2.6770e-11, 2.3744e-11, 1.3159e-13,  ..., 1.2245e-11, 7.3535e-13,
         9.9425e-13]], device='cuda:0')
optimizer state dict: 10.0
lr: [1.9996501145215088e-05, 1.9996501145215088e-05]
scheduler_last_epoch: 10


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  894,   16,  ...,  892,  300,  722]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4182e-05,  7.2969e-05,  4.8510e-05,  ..., -7.0122e-05,
          2.3306e-05,  5.6965e-05],
        [-4.7982e-06, -3.0845e-06,  3.5949e-07,  ..., -3.7849e-06,
         -6.1840e-07, -9.1642e-07],
        [-3.4273e-06, -2.2054e-06,  2.5518e-07,  ..., -2.7120e-06,
         -4.4331e-07, -6.5565e-07],
        [-4.1425e-06, -2.6673e-06,  3.1665e-07,  ..., -3.2634e-06,
         -5.3272e-07, -7.8976e-07],
        [-9.7752e-06, -6.2883e-06,  7.1153e-07,  ..., -7.6890e-06,
         -1.2666e-06, -1.8403e-06]], device='cuda:0')
Loss: 1.2216856479644775


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[    0,   360,  2374,  ...,   221,   474,   357],
        [    0,  3001,  3325,  ..., 16332,  2661,  1200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0527e-05,  9.7422e-05,  1.4640e-05,  ..., -1.5485e-04,
          9.6302e-05,  1.2612e-04],
        [-9.4771e-06, -6.0499e-06,  6.3516e-07,  ..., -7.5251e-06,
         -1.1809e-06, -1.7360e-06],
        [-6.9439e-06, -4.4256e-06,  4.6380e-07,  ..., -5.5134e-06,
         -8.6799e-07, -1.2703e-06],
        [-8.2850e-06, -5.2750e-06,  5.6624e-07,  ..., -6.5565e-06,
         -1.0319e-06, -1.5087e-06],
        [-1.9491e-05, -1.2457e-05,  1.2666e-06,  ..., -1.5438e-05,
         -2.4587e-06, -3.5241e-06]], device='cuda:0')
Loss: 1.2353297472000122


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1314e-04,  2.0729e-04, -1.4075e-06,  ..., -1.2578e-04,
          8.2362e-05,  1.7026e-04],
        [-1.4007e-05, -8.9258e-06,  9.7789e-07,  ..., -1.1161e-05,
         -1.7844e-06, -2.6338e-06],
        [-1.0282e-05, -6.5416e-06,  7.1712e-07,  ..., -8.1807e-06,
         -1.3094e-06, -1.9334e-06],
        [-1.2338e-05, -7.8529e-06,  8.8103e-07,  ..., -9.8050e-06,
         -1.5721e-06, -2.3022e-06],
        [-2.8729e-05, -1.8328e-05,  1.9483e-06,  ..., -2.2858e-05,
         -3.6955e-06, -5.3197e-06]], device='cuda:0')
Loss: 1.2365825176239014


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,    13,  3105,  ...,   496,    14,   879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2520e-04,  1.1123e-04, -1.2074e-05,  ..., -1.6140e-04,
          3.0964e-05,  2.0730e-04],
        [-1.8716e-05, -1.2010e-05,  1.2890e-06,  ..., -1.4976e-05,
         -2.3954e-06, -3.6396e-06],
        [-1.3843e-05, -8.8811e-06,  9.5181e-07,  ..., -1.1072e-05,
         -1.7714e-06, -2.6934e-06],
        [-1.6510e-05, -1.0565e-05,  1.1697e-06,  ..., -1.3173e-05,
         -2.1048e-06, -3.1814e-06],
        [-3.8445e-05, -2.4647e-05,  2.5705e-06,  ..., -3.0726e-05,
         -4.9695e-06, -7.3612e-06]], device='cuda:0')
Loss: 1.2262126207351685


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0, 31550,    14,  ...,   278,   266,  4901],
        [    0,   278,  1099,  ...,   496,    14,   879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8285e-04,  1.0651e-04, -1.2313e-04,  ..., -2.2568e-04,
          1.5386e-04,  1.7081e-04],
        [-2.3276e-05, -1.5020e-05,  1.5404e-06,  ..., -1.8716e-05,
         -2.9020e-06, -4.4852e-06],
        [-1.7211e-05, -1.1101e-05,  1.1362e-06,  ..., -1.3843e-05,
         -2.1514e-06, -3.3230e-06],
        [-2.0564e-05, -1.3232e-05,  1.3998e-06,  ..., -1.6496e-05,
         -2.5537e-06, -3.9302e-06],
        [-4.7922e-05, -3.0875e-05,  3.0696e-06,  ..., -3.8475e-05,
         -6.0350e-06, -9.0823e-06]], device='cuda:0')
Loss: 1.2265921831130981


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 1415,  300,  ..., 1497, 5715, 4555],
        [   0,  368,  266,  ...,  591,  767,  824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0078e-04,  2.3803e-04, -3.4522e-05,  ..., -2.4752e-04,
          4.6858e-05,  6.7764e-05],
        [-2.8074e-05, -1.8030e-05,  1.8682e-06,  ..., -2.2501e-05,
         -3.4720e-06, -5.4985e-06],
        [-2.0787e-05, -1.3337e-05,  1.3765e-06,  ..., -1.6659e-05,
         -2.5760e-06, -4.0717e-06],
        [-2.4676e-05, -1.5810e-05,  1.6885e-06,  ..., -1.9744e-05,
         -3.0380e-06, -4.7907e-06],
        [-5.7638e-05, -3.6925e-05,  3.7029e-06,  ..., -4.6104e-05,
         -7.1824e-06, -1.1079e-05]], device='cuda:0')
Loss: 1.218894362449646


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,   300, 13523,  ..., 42438,   786,  1416],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2746e-04,  3.2278e-04, -1.8153e-04,  ..., -3.4250e-04,
          2.6003e-04,  3.5529e-05],
        [-3.2693e-05, -2.0981e-05,  2.1141e-06,  ..., -2.6256e-05,
         -4.0494e-06, -6.4597e-06],
        [-2.4229e-05, -1.5527e-05,  1.5581e-06,  ..., -1.9446e-05,
         -3.0119e-06, -4.7870e-06],
        [-2.8759e-05, -1.8418e-05,  1.9101e-06,  ..., -2.3052e-05,
         -3.5558e-06, -5.6289e-06],
        [-6.7174e-05, -4.3005e-05,  4.1835e-06,  ..., -5.3853e-05,
         -8.3968e-06, -1.3016e-05]], device='cuda:0')
Loss: 1.230224847793579


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[   0, 3101,  275,  ..., 2345,  609,  287],
        [   0,  591, 1545,  ...,   71,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7423e-04,  3.4821e-04, -2.2002e-04,  ..., -3.5644e-04,
          2.7096e-04, -1.9521e-05],
        [-3.7402e-05, -2.3946e-05,  2.3320e-06,  ..., -2.9966e-05,
         -4.6678e-06, -7.3500e-06],
        [-2.7731e-05, -1.7732e-05,  1.7192e-06,  ..., -2.2218e-05,
         -3.4720e-06, -5.4501e-06],
        [-3.2872e-05, -2.1011e-05,  2.1104e-06,  ..., -2.6286e-05,
         -4.0960e-06, -6.4000e-06],
        [-7.6771e-05, -4.9084e-05,  4.6100e-06,  ..., -6.1452e-05,
         -9.6709e-06, -1.4812e-05]], device='cuda:0')
Loss: 1.2184165716171265
Graident accumulation at epoch 0, step 87, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0046, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0022, -0.0338],
        [ 0.0335, -0.0096,  0.0407,  ...,  0.0223,  0.0063, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4348e-04, -3.5209e-04, -3.6979e-06,  ...,  2.4262e-04,
         -8.0405e-04, -3.1025e-04],
        [-2.0206e-05, -1.9019e-05,  1.1343e-06,  ..., -1.3890e-05,
         -3.7332e-06, -4.8687e-06],
        [-6.3220e-07, -4.3832e-06,  3.9576e-06,  ...,  4.3895e-06,
         -4.9621e-06, -7.6027e-06],
        [-1.3592e-05, -1.2679e-05,  1.1576e-06,  ..., -9.4760e-06,
         -2.9421e-06, -3.6507e-06],
        [-3.6154e-05, -3.2263e-05,  8.0240e-07,  ..., -2.5179e-05,
         -6.1044e-06, -7.4877e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2951e-08, 2.8213e-08, 3.6084e-08,  ..., 1.1469e-08, 9.0425e-08,
         1.1597e-08],
        [1.0104e-11, 9.3184e-12, 1.2983e-13,  ..., 4.6747e-12, 3.3836e-13,
         5.2885e-13],
        [1.9036e-11, 1.5853e-11, 9.0868e-13,  ..., 1.6715e-11, 1.1275e-12,
         3.1539e-12],
        [4.4656e-12, 3.7104e-12, 1.1314e-13,  ..., 2.2260e-12, 2.7682e-13,
         3.2974e-13],
        [3.2637e-11, 2.6130e-11, 1.5271e-13,  ..., 1.6009e-11, 8.2814e-13,
         1.2126e-12]], device='cuda:0')
optimizer state dict: 11.0
lr: [1.999246768512805e-05, 1.999246768512805e-05]
scheduler_last_epoch: 11


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,  5625,  2558,  ...,   680,   292,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2323e-05,  2.5563e-05,  2.2798e-06,  ..., -9.1002e-05,
          2.3160e-05,  4.4088e-06],
        [-4.5598e-06, -2.5481e-06,  5.7369e-07,  ..., -3.6210e-06,
         -4.8429e-07, -1.0580e-06],
        [-3.2485e-06, -1.8179e-06,  4.0978e-07,  ..., -2.5779e-06,
         -3.4459e-07, -7.5623e-07],
        [-4.3511e-06, -2.4289e-06,  5.5879e-07,  ..., -3.4571e-06,
         -4.5635e-07, -1.0058e-06],
        [-9.0003e-06, -5.0068e-06,  1.1176e-06,  ..., -7.1526e-06,
         -9.6112e-07, -2.0713e-06]], device='cuda:0')
Loss: 1.2240058183670044


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,  1477,  5648,  ...,  4391,  1722,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8353e-05, -3.6532e-07,  1.4268e-05,  ..., -1.4103e-04,
          6.2668e-05, -3.5783e-05],
        [-9.0003e-06, -5.0813e-06,  1.1884e-06,  ..., -7.2420e-06,
         -9.8720e-07, -2.0079e-06],
        [-6.4522e-06, -3.6508e-06,  8.5495e-07,  ..., -5.2005e-06,
         -7.1153e-07, -1.4380e-06],
        [-8.6725e-06, -4.9025e-06,  1.1660e-06,  ..., -6.9886e-06,
         -9.4809e-07, -1.9260e-06],
        [-1.7881e-05, -1.0043e-05,  2.3320e-06,  ..., -1.4365e-05,
         -1.9744e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.2217415571212769


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,  3217, 16714,  ...,   462,   221,   474],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0156e-04,  3.1188e-05,  6.3935e-05,  ..., -1.2019e-04,
         -2.0335e-05, -2.1750e-04],
        [-1.3351e-05, -7.5549e-06,  1.7434e-06,  ..., -1.0729e-05,
         -1.4398e-06, -2.9989e-06],
        [-9.6112e-06, -5.4613e-06,  1.2610e-06,  ..., -7.7486e-06,
         -1.0412e-06, -2.1607e-06],
        [-1.2904e-05, -7.3165e-06,  1.7136e-06,  ..., -1.0371e-05,
         -1.3895e-06, -2.8796e-06],
        [-2.6643e-05, -1.5020e-05,  3.4422e-06,  ..., -2.1368e-05,
         -2.8871e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.2152314186096191


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   334,   287,  ...,  1348,  6139,   342],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6165e-04,  1.4612e-04,  5.3521e-05,  ..., -2.3181e-04,
          8.6631e-05, -1.4780e-04],
        [-1.7852e-05, -1.0177e-05,  2.3507e-06,  ..., -1.4499e-05,
         -1.9427e-06, -4.0196e-06],
        [-1.2755e-05, -7.3016e-06,  1.6857e-06,  ..., -1.0401e-05,
         -1.3933e-06, -2.8759e-06],
        [-1.7256e-05, -9.8646e-06,  2.3060e-06,  ..., -1.4022e-05,
         -1.8738e-06, -3.8631e-06],
        [-3.5524e-05, -2.0236e-05,  4.6343e-06,  ..., -2.8849e-05,
         -3.8855e-06, -7.9125e-06]], device='cuda:0')
Loss: 1.223623514175415


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,  221,  380,  560],
        [   0,  298,  374,  ...,  298,  413,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5115e-04,  3.5560e-04,  8.7286e-05,  ..., -2.6316e-04,
          1.6743e-04, -1.5255e-04],
        [-2.2382e-05, -1.2726e-05,  2.8834e-06,  ..., -1.8120e-05,
         -2.3823e-06, -4.8839e-06],
        [-1.5974e-05, -9.1046e-06,  2.0619e-06,  ..., -1.2964e-05,
         -1.7062e-06, -3.4869e-06],
        [-2.1547e-05, -1.2264e-05,  2.8163e-06,  ..., -1.7449e-05,
         -2.2836e-06, -4.6715e-06],
        [-4.4584e-05, -2.5302e-05,  5.6848e-06,  ..., -3.6091e-05,
         -4.7721e-06, -9.6187e-06]], device='cuda:0')
Loss: 1.210854172706604


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,   269,    12,  ..., 45645,    14,   298],
        [    0,  1526,   422,  ..., 22454,   409, 31482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8851e-05,  4.9827e-04,  8.6096e-06,  ..., -2.7654e-04,
          1.6743e-04, -1.1619e-04],
        [-2.6643e-05, -1.5169e-05,  3.4384e-06,  ..., -2.1785e-05,
         -2.9858e-06, -5.7891e-06],
        [-1.8969e-05, -1.0826e-05,  2.4531e-06,  ..., -1.5542e-05,
         -2.1290e-06, -4.1239e-06],
        [-2.5630e-05, -1.4603e-05,  3.3490e-06,  ..., -2.0951e-05,
         -2.8573e-06, -5.5321e-06],
        [-5.2989e-05, -3.0130e-05,  6.7577e-06,  ..., -4.3333e-05,
         -5.9716e-06, -1.1384e-05]], device='cuda:0')
Loss: 1.2201310396194458


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,    71,    14,  ...,  1770,   391, 39516],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0932e-05,  5.2692e-04,  3.8067e-05,  ..., -3.3550e-04,
          2.4691e-04, -4.1172e-05],
        [-3.1173e-05, -1.7732e-05,  4.0084e-06,  ..., -2.5451e-05,
         -3.4254e-06, -6.7651e-06],
        [-2.2188e-05, -1.2644e-05,  2.8536e-06,  ..., -1.8120e-05,
         -2.4401e-06, -4.8093e-06],
        [-2.9981e-05, -1.7047e-05,  3.9004e-06,  ..., -2.4438e-05,
         -3.2727e-06, -6.4522e-06],
        [-6.1989e-05, -3.5226e-05,  7.8753e-06,  ..., -5.0575e-05,
         -6.8471e-06, -1.3292e-05]], device='cuda:0')
Loss: 1.2225180864334106


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[    0,   275,  2101,  ...,  1145,   590,  1619],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6942e-04,  6.9129e-04,  7.3442e-05,  ..., -4.7762e-04,
          2.3235e-04, -1.0180e-04],
        [-3.5733e-05, -2.0266e-05,  4.6641e-06,  ..., -2.9162e-05,
         -3.9209e-06, -7.8604e-06],
        [ 1.0084e-04,  6.0757e-05, -3.9276e-07,  ...,  5.6471e-05,
          4.7423e-06,  1.9999e-05],
        [-3.4302e-05, -1.9461e-05,  4.5374e-06,  ..., -2.7969e-05,
         -3.7458e-06, -7.4953e-06],
        [-7.0810e-05, -4.0114e-05,  9.1419e-06,  ..., -5.7757e-05,
         -7.8157e-06, -1.5393e-05]], device='cuda:0')
Loss: 1.214664101600647
Graident accumulation at epoch 0, step 95, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0092, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0407,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0276, -0.0161, -0.0190]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.4219e-04, -2.4775e-04,  4.0161e-06,  ...,  1.7060e-04,
         -7.0041e-04, -2.8940e-04],
        [-2.1758e-05, -1.9143e-05,  1.4872e-06,  ..., -1.5417e-05,
         -3.7519e-06, -5.1679e-06],
        [ 9.5146e-06,  2.1308e-06,  3.5226e-06,  ...,  9.5976e-06,
         -3.9917e-06, -4.8425e-06],
        [-1.5663e-05, -1.3357e-05,  1.4956e-06,  ..., -1.1325e-05,
         -3.0224e-06, -4.0351e-06],
        [-3.9620e-05, -3.3048e-05,  1.6363e-06,  ..., -2.8437e-05,
         -6.2755e-06, -8.2782e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2937e-08, 2.8663e-08, 3.6053e-08,  ..., 1.1686e-08, 9.0389e-08,
         1.1596e-08],
        [1.1371e-11, 9.7198e-12, 1.5146e-13,  ..., 5.5205e-12, 3.5340e-13,
         5.9011e-13],
        [2.9185e-11, 1.9529e-11, 9.0793e-13,  ..., 1.9888e-11, 1.1488e-12,
         3.5507e-12],
        [5.6378e-12, 4.0854e-12, 1.3361e-13,  ..., 3.0061e-12, 2.9057e-13,
         3.8559e-13],
        [3.7619e-11, 2.7713e-11, 2.3613e-13,  ..., 1.9329e-11, 8.8840e-13,
         1.4484e-12]], device='cuda:0')
optimizer state dict: 12.0
lr: [1.9986907288753243e-05, 1.9986907288753243e-05]
scheduler_last_epoch: 12


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[    0,  1943,   300,  ..., 43803,   368,  2400],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5255e-05, -4.3250e-06,  2.4926e-05,  ..., -1.8580e-05,
         -2.3767e-05, -1.2665e-05],
        [-4.2319e-06, -2.3544e-06,  7.7486e-07,  ..., -3.5465e-06,
         -5.0291e-07, -1.1101e-06],
        [-2.9951e-06, -1.6689e-06,  5.4762e-07,  ..., -2.5034e-06,
         -3.5577e-07, -7.8604e-07],
        [-4.3511e-06, -2.4140e-06,  8.0094e-07,  ..., -3.6210e-06,
         -5.1409e-07, -1.1325e-06],
        [-8.2850e-06, -4.5896e-06,  1.5050e-06,  ..., -6.9141e-06,
         -9.8348e-07, -2.1458e-06]], device='cuda:0')
Loss: 1.2147639989852905


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[    0,  7185,   328,  ...,  1427,  1477,  1061],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4961e-05,  8.4826e-05, -2.4425e-05,  ..., -8.2647e-05,
         -5.3771e-05, -1.8591e-05],
        [-8.3447e-06, -4.7088e-06,  1.5311e-06,  ..., -7.0930e-06,
         -8.9779e-07, -2.1160e-06],
        [-5.8413e-06, -3.3006e-06,  1.0654e-06,  ..., -4.9472e-06,
         -6.2771e-07, -1.4827e-06],
        [-8.5831e-06, -4.8280e-06,  1.5795e-06,  ..., -7.2569e-06,
         -9.1828e-07, -2.1607e-06],
        [-1.6332e-05, -9.2089e-06,  2.9653e-06,  ..., -1.3828e-05,
         -1.7583e-06, -4.0978e-06]], device='cuda:0')
Loss: 1.2179569005966187


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[   0, 1034,  287,  ..., 9677,   13, 6687],
        [   0, 5603, 6598,  ..., 1692, 1713,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0664e-05,  6.7303e-05, -1.1942e-04,  ..., -1.1830e-04,
         -5.5844e-05, -6.8558e-05],
        [-1.2547e-05, -7.0035e-06,  2.3022e-06,  ..., -1.0625e-05,
         -1.3728e-06, -3.0920e-06],
        [-8.8215e-06, -4.9323e-06,  1.6131e-06,  ..., -7.4655e-06,
         -9.6858e-07, -2.1793e-06],
        [-1.2934e-05, -7.2122e-06,  2.3842e-06,  ..., -1.0923e-05,
         -1.4063e-06, -3.1665e-06],
        [-2.4617e-05, -1.3769e-05,  4.4852e-06,  ..., -2.0832e-05,
         -2.7046e-06, -6.0201e-06]], device='cuda:0')
Loss: 1.2120574712753296


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[   0,   17, 3978,  ..., 3988,  598,   12],
        [   0,  278, 7914,  ..., 1194,  300, 4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2482e-05,  1.2890e-04, -1.3926e-04,  ..., -1.1202e-04,
          5.8291e-06, -1.1273e-04],
        [-1.6809e-05, -9.3579e-06,  3.0771e-06,  ..., -1.4260e-05,
         -1.7751e-06, -4.1574e-06],
        [-1.1757e-05, -6.5491e-06,  2.1458e-06,  ..., -9.9689e-06,
         -1.2442e-06, -2.9132e-06],
        [-1.7285e-05, -9.6112e-06,  3.1814e-06,  ..., -1.4633e-05,
         -1.8161e-06, -4.2468e-06],
        [-3.3081e-05, -1.8418e-05,  6.0126e-06,  ..., -2.8044e-05,
         -3.5055e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.2258104085922241


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[    0,   380,   333,  ...,  8127,   504,   679],
        [    0, 28590,    12,  ...,   342, 29639,  1693]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6147e-04,  1.3033e-04, -2.1917e-04,  ..., -1.6868e-04,
          1.3428e-05, -4.9442e-05],
        [-2.1040e-05, -1.1712e-05,  3.8445e-06,  ..., -1.7881e-05,
         -2.3078e-06, -5.2378e-06],
        [-1.4722e-05, -8.1882e-06,  2.6822e-06,  ..., -1.2502e-05,
         -1.6149e-06, -3.6657e-06],
        [-2.1607e-05, -1.2010e-05,  3.9749e-06,  ..., -1.8328e-05,
         -2.3525e-06, -5.3495e-06],
        [-4.1306e-05, -2.2978e-05,  7.5027e-06,  ..., -3.5077e-05,
         -4.5411e-06, -1.0177e-05]], device='cuda:0')
Loss: 1.1993809938430786


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[    0,   300,   344,  ...,    14,  5077,  2715],
        [    0,    12,   287,  ...,    15, 35654,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5850e-04,  1.0050e-04, -1.9047e-04,  ..., -1.7648e-04,
          1.7797e-04, -6.8675e-05],
        [-2.5213e-05, -1.4052e-05,  4.6156e-06,  ..., -2.1338e-05,
         -2.7623e-06, -6.3479e-06],
        [-1.7658e-05, -9.8273e-06,  3.2261e-06,  ..., -1.4931e-05,
         -1.9297e-06, -4.4443e-06],
        [-2.5928e-05, -1.4439e-05,  4.7833e-06,  ..., -2.1920e-05,
         -2.8182e-06, -6.4969e-06],
        [-4.9412e-05, -2.7508e-05,  8.9929e-06,  ..., -4.1783e-05,
         -5.4277e-06, -1.2308e-05]], device='cuda:0')
Loss: 1.2073496580123901


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,    14, 49601,  ...,    12,   298,   374],
        [    0,  6408,   391,  ...,   870,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0302e-04,  1.1119e-04, -1.6754e-04,  ..., -2.2838e-04,
          2.9927e-04, -5.0883e-05],
        [-2.9564e-05, -1.6510e-05,  5.3234e-06,  ..., -2.4989e-05,
         -3.2876e-06, -7.3910e-06],
        [-2.0698e-05, -1.1541e-05,  3.7178e-06,  ..., -1.7479e-05,
         -2.2985e-06, -5.1782e-06],
        [-3.0279e-05, -1.6898e-05,  5.4948e-06,  ..., -2.5570e-05,
         -3.3397e-06, -7.5474e-06],
        [-5.7936e-05, -3.2306e-05,  1.0356e-05,  ..., -4.8935e-05,
         -6.4634e-06, -1.4335e-05]], device='cuda:0')
Loss: 1.2015633583068848


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[   0, 1611,  266,  ...,  266, 2673, 6277],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2170e-04,  1.4642e-04, -2.1260e-04,  ..., -2.0580e-04,
          2.2364e-04, -1.0632e-04],
        [-3.3677e-05, -1.8805e-05,  6.1430e-06,  ..., -2.8476e-05,
         -3.7439e-06, -8.5384e-06],
        [ 9.7952e-05,  4.7913e-05, -2.5135e-05,  ...,  8.5014e-05,
         -1.0274e-05,  1.2897e-05],
        [-3.4541e-05, -1.9267e-05,  6.3479e-06,  ..., -2.9162e-05,
         -3.8054e-06, -8.7246e-06],
        [-6.5923e-05, -3.6746e-05,  1.1936e-05,  ..., -5.5701e-05,
         -7.3500e-06, -1.6540e-05]], device='cuda:0')
Loss: 1.21623694896698
Graident accumulation at epoch 0, step 103, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0406,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0171,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9014e-04, -2.0833e-04, -1.7645e-05,  ...,  1.3296e-04,
         -6.0800e-04, -2.7109e-04],
        [-2.2950e-05, -1.9109e-05,  1.9528e-06,  ..., -1.6723e-05,
         -3.7511e-06, -5.5049e-06],
        [ 1.8358e-05,  6.7091e-06,  6.5678e-07,  ...,  1.7139e-05,
         -4.6199e-06, -3.0685e-06],
        [-1.7551e-05, -1.3948e-05,  1.9808e-06,  ..., -1.3109e-05,
         -3.1007e-06, -4.5041e-06],
        [-4.2250e-05, -3.3418e-05,  2.6663e-06,  ..., -3.1163e-05,
         -6.3830e-06, -9.1044e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2943e-08, 2.8656e-08, 3.6062e-08,  ..., 1.1717e-08, 9.0349e-08,
         1.1595e-08],
        [1.2494e-11, 1.0064e-11, 1.8904e-13,  ..., 6.3258e-12, 3.6706e-13,
         6.6242e-13],
        [3.8751e-11, 2.1805e-11, 1.5388e-12,  ..., 2.7095e-11, 1.2532e-12,
         3.7135e-12],
        [6.8253e-12, 4.4526e-12, 1.7377e-13,  ..., 3.8535e-12, 3.0476e-13,
         4.6133e-13],
        [4.1927e-11, 2.9035e-11, 3.7836e-13,  ..., 2.2412e-11, 9.4153e-13,
         1.7205e-12]], device='cuda:0')
optimizer state dict: 13.0
lr: [1.9979820805767768e-05, 1.9979820805767768e-05]
scheduler_last_epoch: 13


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,  3634,  3444,  ...,   642,  2156,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2414e-05,  2.6295e-05, -2.9867e-05,  ...,  2.5833e-05,
          2.2353e-06,  0.0000e+00],
        [-4.0233e-06, -2.2203e-06,  9.2760e-07,  ..., -3.5167e-06,
         -4.8056e-07, -1.1474e-06],
        [-2.7567e-06, -1.5125e-06,  6.3330e-07,  ..., -2.3991e-06,
         -3.2969e-07, -7.8231e-07],
        [-4.3511e-06, -2.3991e-06,  1.0058e-06,  ..., -3.7849e-06,
         -5.1782e-07, -1.2368e-06],
        [-7.8082e-06, -4.2915e-06,  1.7881e-06,  ..., -6.7949e-06,
         -9.3877e-07, -2.2054e-06]], device='cuda:0')
Loss: 1.225634217262268


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[   0, 7333,  342,  ...,   13, 1818, 6183],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4430e-05, -2.5576e-05,  1.3842e-05,  ..., -8.7279e-05,
          5.2773e-05,  3.8437e-07],
        [-8.1062e-06, -4.4405e-06,  1.8291e-06,  ..., -7.0035e-06,
         -8.8289e-07, -2.2054e-06],
        [-5.5283e-06, -3.0249e-06,  1.2442e-06,  ..., -4.7684e-06,
         -6.0350e-07, -1.4976e-06],
        [-8.6427e-06, -4.7386e-06,  1.9670e-06,  ..., -7.4655e-06,
         -9.3877e-07, -2.3469e-06],
        [-1.5616e-05, -8.5533e-06,  3.5018e-06,  ..., -1.3471e-05,
         -1.7136e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.203037977218628


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0, 2923,  391,  ...,   14, 5424,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1721e-05, -3.7625e-05, -1.4895e-05,  ..., -9.2946e-05,
          3.8441e-05,  2.1793e-05],
        [-1.2338e-05, -6.6757e-06,  2.7455e-06,  ..., -1.0550e-05,
         -1.3784e-06, -3.2857e-06],
        [-8.3745e-06, -4.5374e-06,  1.8626e-06,  ..., -7.1675e-06,
         -9.4064e-07, -2.2277e-06],
        [-1.3053e-05, -7.0781e-06,  2.9355e-06,  ..., -1.1176e-05,
         -1.4566e-06, -3.4720e-06],
        [-2.3484e-05, -1.2726e-05,  5.1931e-06,  ..., -2.0057e-05,
         -2.6412e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.2011780738830566


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[    0,    14,  3080,  ...,   910,   266,  5275],
        [    0, 11054,    12,  ...,   560,   199,   677]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7684e-05,  1.2908e-06,  1.8088e-05,  ..., -1.8905e-04,
          9.9911e-05, -2.2750e-05],
        [-1.6391e-05, -8.8513e-06,  3.6135e-06,  ..., -1.4022e-05,
         -1.8515e-06, -4.4629e-06],
        [-1.1176e-05, -6.0424e-06,  2.4624e-06,  ..., -9.5665e-06,
         -1.2685e-06, -3.0473e-06],
        [-1.7375e-05, -9.4026e-06,  3.8706e-06,  ..., -1.4886e-05,
         -1.9595e-06, -4.7311e-06],
        [-3.1471e-05, -1.7017e-05,  6.8992e-06,  ..., -2.6911e-05,
         -3.5763e-06, -8.5086e-06]], device='cuda:0')
Loss: 1.2019983530044556


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0, 10084,    12,  ..., 24717,   365,  1616],
        [    0,   367,  3675,  ...,    22,  3180,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3866e-05, -1.6172e-05,  1.5259e-05,  ..., -1.4771e-04,
          1.5737e-04,  3.2631e-05],
        [-2.0474e-05, -1.1101e-05,  4.5039e-06,  ..., -1.7539e-05,
         -2.3227e-06, -5.5730e-06],
        [-1.3977e-05, -7.5921e-06,  3.0734e-06,  ..., -1.1995e-05,
         -1.5926e-06, -3.8147e-06],
        [-2.1756e-05, -1.1817e-05,  4.8392e-06,  ..., -1.8671e-05,
         -2.4624e-06, -5.9232e-06],
        [-3.9339e-05, -2.1368e-05,  8.6129e-06,  ..., -3.3736e-05,
         -4.4964e-06, -1.0654e-05]], device='cuda:0')
Loss: 1.2306740283966064


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,   475,   668,  ..., 17680,   368,  1351],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2847e-05, -8.0109e-05,  2.4177e-06,  ..., -1.2949e-04,
          1.4793e-04, -1.5131e-05],
        [-2.4498e-05, -1.3277e-05,  5.4315e-06,  ..., -2.0996e-05,
         -2.8331e-06, -6.6310e-06],
        [-1.6734e-05, -9.0897e-06,  3.7104e-06,  ..., -1.4365e-05,
         -1.9409e-06, -4.5374e-06],
        [-2.6017e-05, -1.4141e-05,  5.8301e-06,  ..., -2.2352e-05,
         -3.0026e-06, -7.0408e-06],
        [-4.7088e-05, -2.5570e-05,  1.0394e-05,  ..., -4.0412e-05,
         -5.4874e-06, -1.2666e-05]], device='cuda:0')
Loss: 1.2173869609832764


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[   0, 6909,  352,  ..., 1075,  706, 6909],
        [   0, 4672,  278,  ..., 7523, 2305,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2740e-04,  1.1971e-05, -6.6118e-05,  ..., -1.9240e-04,
          1.3452e-04,  4.1440e-05],
        [-2.8551e-05, -1.5408e-05,  6.3628e-06,  ..., -2.4572e-05,
         -3.2522e-06, -7.7784e-06],
        [-1.9521e-05, -1.0557e-05,  4.3511e-06,  ..., -1.6823e-05,
         -2.2277e-06, -5.3309e-06],
        [-3.0339e-05, -1.6421e-05,  6.8285e-06,  ..., -2.6166e-05,
         -3.4496e-06, -8.2627e-06],
        [ 1.3718e-04,  9.2933e-05, -5.0317e-05,  ...,  1.4135e-04,
          3.7764e-05,  4.7084e-05]], device='cuda:0')
Loss: 1.2203092575073242


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[   0,  600, 9092,  ...,  554, 1485,  328],
        [   0,  516,  596,  ..., 3109,  287,  394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0410e-04, -2.1426e-05, -1.0322e-04,  ..., -3.1826e-04,
          1.7255e-04,  3.2738e-05],
        [-3.2753e-05, -1.7717e-05,  7.3165e-06,  ..., -2.8133e-05,
         -3.6694e-06, -8.9034e-06],
        [-2.2337e-05, -1.2107e-05,  4.9956e-06,  ..., -1.9222e-05,
         -2.5071e-06, -6.0871e-06],
        [-3.4750e-05, -1.8850e-05,  7.8417e-06,  ..., -2.9922e-05,
         -3.8873e-06, -9.4399e-06],
        [ 1.2932e-04,  8.8611e-05, -4.8529e-05,  ...,  1.3465e-04,
          3.6982e-05,  4.4998e-05]], device='cuda:0')
Loss: 1.2190684080123901
Graident accumulation at epoch 0, step 111, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0096,  0.0406,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3154e-04, -1.8964e-04, -2.6203e-05,  ...,  8.7837e-05,
         -5.2995e-04, -2.4071e-04],
        [-2.3930e-05, -1.8970e-05,  2.4892e-06,  ..., -1.7864e-05,
         -3.7430e-06, -5.8448e-06],
        [ 1.4289e-05,  4.8274e-06,  1.0907e-06,  ...,  1.3503e-05,
         -4.4086e-06, -3.3704e-06],
        [-1.9271e-05, -1.4439e-05,  2.5669e-06,  ..., -1.4790e-05,
         -3.1794e-06, -4.9977e-06],
        [-2.5093e-05, -2.1215e-05, -2.4533e-06,  ..., -1.4582e-05,
         -2.0465e-06, -3.6942e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2911e-08, 2.8627e-08, 3.6037e-08,  ..., 1.1806e-08, 9.0288e-08,
         1.1585e-08],
        [1.3554e-11, 1.0368e-11, 2.4238e-13,  ..., 7.1110e-12, 3.8016e-13,
         7.4103e-13],
        [3.9211e-11, 2.1929e-11, 1.5622e-12,  ..., 2.7438e-11, 1.2583e-12,
         3.7468e-12],
        [8.0260e-12, 4.8034e-12, 2.3509e-13,  ..., 4.7449e-12, 3.1957e-13,
         5.4998e-13],
        [5.8608e-11, 3.6858e-11, 2.7331e-12,  ..., 4.0519e-11, 2.3083e-12,
         3.7436e-12]], device='cuda:0')
optimizer state dict: 14.0
lr: [1.997120931904809e-05, 1.997120931904809e-05]
scheduler_last_epoch: 14


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4464e-05,  2.5093e-05,  3.1658e-05,  ..., -4.2697e-05,
          0.0000e+00,  6.6364e-05],
        [-3.9637e-06, -2.1756e-06,  1.0729e-06,  ..., -3.5167e-06,
         -4.8429e-07, -1.2815e-06],
        [-2.6822e-06, -1.4678e-06,  7.2271e-07,  ..., -2.3693e-06,
         -3.2596e-07, -8.6427e-07],
        [-4.2915e-06, -2.3544e-06,  1.1697e-06,  ..., -3.7849e-06,
         -5.2527e-07, -1.3784e-06],
        [-7.6294e-06, -4.1723e-06,  2.0564e-06,  ..., -6.7353e-06,
         -9.3877e-07, -2.4438e-06]], device='cuda:0')
Loss: 1.2251358032226562


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[    0,   221,  6872,  ...,   806,   518,   266],
        [    0,  1985,   278,  ...,   677, 12292, 17956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0879e-05, -4.2036e-06,  3.7748e-05,  ...,  1.5084e-05,
          2.2272e-05, -1.8659e-05],
        [-7.8678e-06, -4.3511e-06,  2.1458e-06,  ..., -6.9886e-06,
         -9.1642e-07, -2.5257e-06],
        [-5.2899e-06, -2.9281e-06,  1.4417e-06,  ..., -4.6939e-06,
         -6.1654e-07, -1.6987e-06],
        [-8.5235e-06, -4.7088e-06,  2.3395e-06,  ..., -7.5400e-06,
         -9.9279e-07, -2.7269e-06],
        [-1.5050e-05, -8.3148e-06,  4.0829e-06,  ..., -1.3351e-05,
         -1.7658e-06, -4.7982e-06]], device='cuda:0')
Loss: 1.2102715969085693


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6726e-04,  1.0304e-05, -5.8059e-05,  ..., -9.4456e-05,
          2.5000e-05, -6.0347e-05],
        [ 8.1297e-05,  2.8647e-05, -4.1962e-05,  ...,  6.8483e-05,
          2.4342e-05,  3.5710e-05],
        [-8.0317e-06, -4.4033e-06,  2.1532e-06,  ..., -7.0632e-06,
         -9.9279e-07, -2.5481e-06],
        [-1.2934e-05, -7.0930e-06,  3.4943e-06,  ..., -1.1355e-05,
         -1.6000e-06, -4.0978e-06],
        [-2.2799e-05, -1.2487e-05,  6.0797e-06,  ..., -2.0057e-05,
         -2.8312e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.1964582204818726


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0900e-04, -1.9740e-05, -3.8183e-05,  ..., -5.6154e-05,
         -4.3104e-05, -3.0849e-05],
        [ 7.7303e-05,  2.6486e-05, -4.0911e-05,  ...,  6.5041e-05,
          2.3866e-05,  3.4503e-05],
        [-1.0729e-05, -5.8711e-06,  2.8647e-06,  ..., -9.3877e-06,
         -1.3150e-06, -3.3639e-06],
        [-1.7256e-05, -9.4473e-06,  4.6417e-06,  ..., -1.5080e-05,
         -2.1141e-06, -5.4091e-06],
        [-3.0488e-05, -1.6659e-05,  8.0913e-06,  ..., -2.6673e-05,
         -3.7551e-06, -9.5069e-06]], device='cuda:0')
Loss: 1.2125771045684814


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,   380,   981,  ...,   567,  5407,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7186e-04, -1.9740e-05, -2.0117e-05,  ..., -1.1686e-04,
         -5.0043e-05, -5.5316e-05],
        [ 7.3340e-05,  2.4311e-05, -3.9890e-05,  ...,  6.1613e-05,
          2.3378e-05,  3.3244e-05],
        [-1.3441e-05, -7.3612e-06,  3.5577e-06,  ..., -1.1727e-05,
         -1.6484e-06, -4.2245e-06],
        [-2.1607e-05, -1.1832e-05,  5.7667e-06,  ..., -1.8835e-05,
         -2.6468e-06, -6.7949e-06],
        [-3.8177e-05, -2.0891e-05,  1.0058e-05,  ..., -3.3319e-05,
         -4.7088e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1983031034469604


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,  4902,   518,  ...,  5493,  3227,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4955e-05, -2.0113e-05,  1.1977e-06,  ..., -2.1217e-04,
          1.7855e-05, -4.1591e-05],
        [ 6.9316e-05,  2.2195e-05, -3.8877e-05,  ...,  5.8097e-05,
          2.2882e-05,  3.1955e-05],
        [-1.6183e-05, -8.8066e-06,  4.2506e-06,  ..., -1.4126e-05,
         -1.9874e-06, -5.1036e-06],
        [-2.6047e-05, -1.4171e-05,  6.8918e-06,  ..., -2.2709e-05,
         -3.1870e-06, -8.2105e-06],
        [-4.5985e-05, -2.5004e-05,  1.2025e-05,  ..., -4.0144e-05,
         -5.6773e-06, -1.4439e-05]], device='cuda:0')
Loss: 1.2089574337005615


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0, 2228, 1416,  ..., 3766,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8558e-05,  1.1241e-05, -6.2618e-05,  ..., -1.5947e-04,
         -2.1616e-05,  6.9632e-06],
        [ 6.5293e-05,  2.0064e-05, -3.7849e-05,  ...,  5.4625e-05,
          2.2439e-05,  3.0711e-05],
        [-1.8895e-05, -1.0245e-05,  4.9472e-06,  ..., -1.6466e-05,
         -2.2873e-06, -5.9418e-06],
        [-3.0369e-05, -1.6466e-05,  8.0094e-06,  ..., -2.6464e-05,
         -3.6638e-06, -9.5516e-06],
        [-5.3734e-05, -2.9117e-05,  1.4007e-05,  ..., -4.6819e-05,
         -6.5416e-06, -1.6823e-05]], device='cuda:0')
Loss: 1.2021266222000122


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  1265,  1545,  ...,   292, 36667, 36197]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5590e-04,  6.0170e-06, -7.2928e-05,  ..., -1.9568e-04,
          1.6563e-05, -4.0528e-05],
        [ 6.1240e-05,  1.7844e-05, -3.6813e-05,  ...,  5.1049e-05,
          2.1955e-05,  2.9429e-05],
        [-2.1622e-05, -1.1750e-05,  5.6475e-06,  ..., -1.8880e-05,
         -2.6133e-06, -6.8061e-06],
        [-3.4660e-05, -1.8835e-05,  9.1195e-06,  ..., -3.0249e-05,
         -4.1705e-06, -1.0908e-05],
        [-6.1363e-05, -3.3319e-05,  1.5959e-05,  ..., -5.3525e-05,
         -7.4580e-06, -1.9222e-05]], device='cuda:0')
Loss: 1.2176851034164429
Graident accumulation at epoch 0, step 119, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0150,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0035,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0335, -0.0095,  0.0406,  ...,  0.0224,  0.0064, -0.0018],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.8397e-04, -1.7008e-04, -3.0875e-05,  ...,  5.9484e-05,
         -4.7530e-04, -2.2069e-04],
        [-1.5413e-05, -1.5289e-05, -1.4411e-06,  ..., -1.0973e-05,
         -1.1732e-06, -2.3174e-06],
        [ 1.0698e-05,  3.1697e-06,  1.5464e-06,  ...,  1.0265e-05,
         -4.2291e-06, -3.7140e-06],
        [-2.0810e-05, -1.4878e-05,  3.2222e-06,  ..., -1.6336e-05,
         -3.2785e-06, -5.5887e-06],
        [-2.8720e-05, -2.2425e-05, -6.1201e-07,  ..., -1.8477e-05,
         -2.5876e-06, -5.2470e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2893e-08, 2.8599e-08, 3.6006e-08,  ..., 1.1833e-08, 9.0198e-08,
         1.1575e-08],
        [1.7291e-11, 1.0676e-11, 1.5974e-12,  ..., 9.7098e-12, 8.6178e-13,
         1.6064e-12],
        [3.9639e-11, 2.2046e-11, 1.5926e-12,  ..., 2.7767e-11, 1.2638e-12,
         3.7894e-12],
        [9.2193e-12, 5.1534e-12, 3.1802e-13,  ..., 5.6552e-12, 3.3664e-13,
         6.6840e-13],
        [6.2315e-11, 3.7931e-11, 2.9850e-12,  ..., 4.3344e-11, 2.3616e-12,
         4.1094e-12]], device='cuda:0')
optimizer state dict: 15.0
lr: [1.996107414450454e-05, 1.996107414450454e-05]
scheduler_last_epoch: 15


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,    14,  1364, 19388],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3870e-05, -2.0028e-05, -5.2460e-07,  ...,  6.6685e-06,
         -3.7046e-05, -7.3824e-05],
        [-3.9935e-06, -2.2352e-06,  1.1772e-06,  ..., -3.5465e-06,
         -5.0291e-07, -1.3709e-06],
        [-2.6971e-06, -1.5050e-06,  7.8976e-07,  ..., -2.3842e-06,
         -3.3714e-07, -9.2015e-07],
        [-4.3809e-06, -2.4438e-06,  1.2890e-06,  ..., -3.8743e-06,
         -5.4389e-07, -1.4901e-06],
        [-7.7486e-06, -4.3213e-06,  2.2650e-06,  ..., -6.8247e-06,
         -9.6112e-07, -2.6226e-06]], device='cuda:0')
Loss: 1.221095323562622


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0, 1716, 1773,  ..., 5014,   12,  847],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3516e-05, -7.0016e-06,  2.7201e-05,  ..., -1.5155e-05,
         -1.7969e-05, -1.2875e-04],
        [-7.9572e-06, -4.5151e-06,  2.2724e-06,  ..., -7.0930e-06,
         -1.0096e-06, -2.6971e-06],
        [-5.3644e-06, -3.0324e-06,  1.5236e-06,  ..., -4.7684e-06,
         -6.7428e-07, -1.8105e-06],
        [-8.6427e-06, -4.8876e-06,  2.4661e-06,  ..., -7.6741e-06,
         -1.0841e-06, -2.9057e-06],
        [-1.5438e-05, -8.7023e-06,  4.3660e-06,  ..., -1.3649e-05,
         -1.9372e-06, -5.1707e-06]], device='cuda:0')
Loss: 1.1975698471069336


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[    0,  2013,    13,  ...,   271,   266,   908],
        [    0, 12456,    14,  ...,  1822,  1016,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9425e-05,  3.1180e-05,  8.9120e-05,  ..., -1.0578e-05,
         -3.8533e-05, -7.2715e-05],
        [-1.1951e-05, -6.7651e-06,  3.4124e-06,  ..., -1.0669e-05,
         -1.4901e-06, -4.0308e-06],
        [-8.0317e-06, -4.5300e-06,  2.2799e-06,  ..., -7.1377e-06,
         -9.9093e-07, -2.6971e-06],
        [-1.2934e-05, -7.2867e-06,  3.6880e-06,  ..., -1.1474e-05,
         -1.5907e-06, -4.3213e-06],
        [-2.3067e-05, -1.2994e-05,  6.5267e-06,  ..., -2.0444e-05,
         -2.8536e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.2002543210983276


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1615,   287,  ...,   259,   623,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7832e-06,  7.8905e-05, -2.4697e-05,  ...,  6.0346e-06,
          2.0459e-06, -4.6783e-05],
        [-1.5914e-05, -9.0003e-06,  4.6268e-06,  ..., -1.4260e-05,
         -1.9781e-06, -5.3868e-06],
        [-1.0654e-05, -6.0126e-06,  3.0808e-06,  ..., -9.5218e-06,
         -1.3113e-06, -3.5949e-06],
        [-1.7256e-05, -9.7305e-06,  5.0142e-06,  ..., -1.5378e-05,
         -2.1197e-06, -5.7966e-06],
        [-3.0518e-05, -1.7196e-05,  8.7917e-06,  ..., -2.7210e-05,
         -3.7663e-06, -1.0237e-05]], device='cuda:0')
Loss: 1.204085111618042


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[    0,    29,   413,  ...,  2001,  1027,   292],
        [    0,  1526,  3502,  ..., 11727,  3736,  1661]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5132e-04,  1.3013e-04,  4.9000e-05,  ..., -1.3185e-05,
          5.9400e-05, -8.4576e-05],
        [-2.0057e-05, -1.1221e-05,  5.7146e-06,  ..., -1.7777e-05,
         -2.4922e-06, -6.7130e-06],
        [-1.3366e-05, -7.4729e-06,  3.7961e-06,  ..., -1.1832e-05,
         -1.6503e-06, -4.4666e-06],
        [-2.1726e-05, -1.2115e-05,  6.1914e-06,  ..., -1.9178e-05,
         -2.6710e-06, -7.2196e-06],
        [-3.8505e-05, -2.1458e-05,  1.0893e-05,  ..., -3.4004e-05,
         -4.7646e-06, -1.2785e-05]], device='cuda:0')
Loss: 1.1906014680862427


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 2336,   26,  ..., 2564,  271, 1422],
        [   0, 6538, 1805,  ...,  298,  271,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1293e-04,  1.4202e-04,  1.0004e-04,  ...,  2.1730e-05,
          1.0658e-04, -5.9946e-05],
        [-2.4050e-05, -1.3486e-05,  6.8396e-06,  ..., -2.1324e-05,
         -2.9840e-06, -8.1062e-06],
        [-1.6019e-05, -8.9779e-06,  4.5411e-06,  ..., -1.4186e-05,
         -1.9763e-06, -5.3942e-06],
        [-2.6047e-05, -1.4558e-05,  7.4133e-06,  ..., -2.3022e-05,
         -3.1963e-06, -8.7246e-06],
        [-4.6194e-05, -2.5809e-05,  1.3039e-05,  ..., -4.0829e-05,
         -5.7109e-06, -1.5467e-05]], device='cuda:0')
Loss: 1.2002562284469604


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[   0,  600,   14,  ...,  221, 8187, 1802],
        [   0, 3058,  292,  ..., 1387, 1236,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7362e-04,  1.6744e-04,  1.6215e-04,  ..., -1.4948e-05,
          6.4301e-05, -8.6663e-05],
        [-2.7984e-05, -1.5676e-05,  7.9870e-06,  ..., -2.4810e-05,
         -3.4589e-06, -9.4548e-06],
        [-1.8656e-05, -1.0438e-05,  5.3123e-06,  ..., -1.6525e-05,
         -2.2948e-06, -6.2995e-06],
        [-3.0339e-05, -1.6943e-05,  8.6725e-06,  ..., -2.6822e-05,
         -3.7141e-06, -1.0192e-05],
        [-5.3883e-05, -3.0071e-05,  1.5259e-05,  ..., -4.7624e-05,
         -6.6385e-06, -1.8075e-05]], device='cuda:0')
Loss: 1.2040667533874512


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[   0, 2377,  360,  ...,  266, 4745,  963],
        [   0, 1890,  278,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9822e-04,  2.2028e-04,  1.8769e-04,  ..., -1.3827e-05,
         -1.3787e-05, -2.1025e-04],
        [-3.1948e-05, -1.7866e-05,  9.1344e-06,  ..., -2.8238e-05,
         -3.8594e-06, -1.0811e-05],
        [-2.1338e-05, -1.1928e-05,  6.0908e-06,  ..., -1.8850e-05,
         -2.5686e-06, -7.2196e-06],
        [-3.4690e-05, -1.9357e-05,  9.9391e-06,  ..., -3.0592e-05,
         -4.1593e-06, -1.1683e-05],
        [-6.1631e-05, -3.4362e-05,  1.7494e-05,  ..., -5.4330e-05,
         -7.4282e-06, -2.0713e-05]], device='cuda:0')
Loss: 1.2111308574676514
Graident accumulation at epoch 0, step 127, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0047, -0.0154,  0.0038,  ..., -0.0034,  0.0221, -0.0207],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0224,  0.0064, -0.0018],
        [-0.0170,  0.0141, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0575e-04, -1.3104e-04, -9.0185e-06,  ...,  5.2153e-05,
         -4.2915e-04, -2.1965e-04],
        [-1.7067e-05, -1.5547e-05, -3.8352e-07,  ..., -1.2700e-05,
         -1.4418e-06, -3.1667e-06],
        [ 7.4942e-06,  1.6599e-06,  2.0008e-06,  ...,  7.3533e-06,
         -4.0630e-06, -4.0645e-06],
        [-2.2198e-05, -1.5326e-05,  3.8939e-06,  ..., -1.7762e-05,
         -3.3666e-06, -6.1980e-06],
        [-3.2011e-05, -2.3619e-05,  1.1986e-06,  ..., -2.2062e-05,
         -3.0717e-06, -6.7936e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2889e-08, 2.8619e-08, 3.6005e-08,  ..., 1.1821e-08, 9.0108e-08,
         1.1608e-08],
        [1.8294e-11, 1.0984e-11, 1.6792e-12,  ..., 1.0498e-11, 8.7582e-13,
         1.7216e-12],
        [4.0055e-11, 2.2166e-11, 1.6281e-12,  ..., 2.8094e-11, 1.2692e-12,
         3.8377e-12],
        [1.0413e-11, 5.5229e-12, 4.1649e-13,  ..., 6.5854e-12, 3.5361e-13,
         8.0422e-13],
        [6.6051e-11, 3.9074e-11, 3.2881e-12,  ..., 4.6252e-11, 2.4144e-12,
         4.5343e-12]], device='cuda:0')
optimizer state dict: 16.0
lr: [1.9949416830880266e-05, 1.9949416830880266e-05]
scheduler_last_epoch: 16


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,  1478,    14,  ...,   266,  9417,  9105],
        [    0,   266,  1890,  ...,   287, 38242,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1566e-05,  2.4132e-05,  4.9323e-05,  ..., -5.6518e-05,
          1.7343e-05, -1.3965e-05],
        [-4.0531e-06, -2.3991e-06,  1.2219e-06,  ..., -3.5167e-06,
         -4.4517e-07, -1.4976e-06],
        [-2.6971e-06, -1.5944e-06,  8.1584e-07,  ..., -2.3544e-06,
         -2.9989e-07, -9.9838e-07],
        [-4.3809e-06, -2.5779e-06,  1.3262e-06,  ..., -3.7998e-06,
         -4.7684e-07, -1.6093e-06],
        [-7.8678e-06, -4.6194e-06,  2.3544e-06,  ..., -6.8247e-06,
         -8.6799e-07, -2.8908e-06]], device='cuda:0')
Loss: 1.2089474201202393


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3112e-05,  4.5850e-06,  4.4510e-05,  ..., -2.7706e-05,
          4.6852e-05, -2.0351e-05],
        [-8.0466e-06, -4.7982e-06,  2.4363e-06,  ..., -7.0930e-06,
         -8.7917e-07, -2.9355e-06],
        [-5.3346e-06, -3.1739e-06,  1.6168e-06,  ..., -4.7237e-06,
         -5.8673e-07, -1.9446e-06],
        [-8.6725e-06, -5.1409e-06,  2.6375e-06,  ..., -7.6443e-06,
         -9.4436e-07, -3.1516e-06],
        [-1.5438e-05, -9.1791e-06,  4.6492e-06,  ..., -1.3620e-05,
         -1.6987e-06, -5.6028e-06]], device='cuda:0')
Loss: 1.222679853439331


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0,   292,    33,  ...,   352,   266,  9129],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1893e-04,  4.5430e-05,  6.9701e-05,  ...,  5.9738e-07,
          4.7908e-06, -1.2896e-05],
        [-1.2249e-05, -7.1824e-06,  3.5986e-06,  ..., -1.0744e-05,
         -1.4231e-06, -4.4703e-06],
        [-8.0913e-06, -4.7386e-06,  2.3767e-06,  ..., -7.1079e-06,
         -9.4250e-07, -2.9504e-06],
        [-1.3143e-05, -7.6741e-06,  3.8743e-06,  ..., -1.1519e-05,
         -1.5218e-06, -4.7833e-06],
        [-2.3603e-05, -1.3798e-05,  6.8992e-06,  ..., -2.0713e-05,
         -2.7642e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.2007588148117065


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0,   14,  469,  ...,  367, 2564,  368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6664e-04,  2.9228e-05,  4.7258e-05,  ...,  3.4855e-05,
          1.4749e-05,  1.1359e-05],
        [-1.6332e-05, -9.5814e-06,  4.7907e-06,  ..., -1.4320e-05,
         -1.8757e-06, -5.9828e-06],
        [-1.0788e-05, -6.3255e-06,  3.1665e-06,  ..., -9.4771e-06,
         -1.2424e-06, -3.9563e-06],
        [-1.7524e-05, -1.0252e-05,  5.1633e-06,  ..., -1.5363e-05,
         -2.0061e-06, -6.4075e-06],
        [-3.1412e-05, -1.8388e-05,  9.1791e-06,  ..., -2.7567e-05,
         -3.6322e-06, -1.1474e-05]], device='cuda:0')
Loss: 1.207656741142273


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  995,   13, 6507],
        [   0,   43,  527,  ..., 4309,   14, 8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6356e-04, -3.4133e-05,  8.9529e-05,  ...,  1.0483e-05,
         -2.4772e-05,  2.0227e-05],
        [-2.0474e-05, -1.1936e-05,  6.0201e-06,  ..., -1.7941e-05,
         -2.3786e-06, -7.4431e-06],
        [-1.3545e-05, -7.8827e-06,  3.9823e-06,  ..., -1.1876e-05,
         -1.5795e-06, -4.9248e-06],
        [-2.1964e-05, -1.2770e-05,  6.4820e-06,  ..., -1.9237e-05,
         -2.5425e-06, -7.9647e-06],
        [-3.9399e-05, -2.2918e-05,  1.1548e-05,  ..., -3.4511e-05,
         -4.6082e-06, -1.4260e-05]], device='cuda:0')
Loss: 1.204783320426941


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,  1188,   278,  ...,   271,  8368,   292],
        [    0,   843, 17111,  ...,    12,   461,  6176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0240e-04, -3.9732e-05,  8.7657e-05,  ...,  2.8608e-05,
         -8.5357e-06,  4.8636e-05],
        [-2.4647e-05, -1.4305e-05,  7.2196e-06,  ..., -2.1562e-05,
         -2.8592e-06, -8.9332e-06],
        [-1.6287e-05, -9.4473e-06,  4.7721e-06,  ..., -1.4260e-05,
         -1.8962e-06, -5.9083e-06],
        [-2.6286e-05, -1.5244e-05,  7.7337e-06,  ..., -2.3007e-05,
         -3.0417e-06, -9.5144e-06],
        [-4.7386e-05, -2.7478e-05,  1.3858e-05,  ..., -4.1485e-05,
         -5.5321e-06, -1.7121e-05]], device='cuda:0')
Loss: 1.214857816696167


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0,    9,  391,  ...,  300, 2646, 1717],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1926e-04,  1.3207e-05,  4.4940e-05,  ..., -2.4426e-05,
          1.3936e-05,  8.3876e-05],
        [-2.8759e-05, -1.6689e-05,  8.4415e-06,  ..., -2.5213e-05,
         -3.3099e-06, -1.0408e-05],
        [-1.8984e-05, -1.1012e-05,  5.5768e-06,  ..., -1.6659e-05,
         -2.1942e-06, -6.8769e-06],
        [-3.0667e-05, -1.7777e-05,  9.0450e-06,  ..., -2.6882e-05,
         -3.5185e-06, -1.1079e-05],
        [-5.5254e-05, -3.2037e-05,  1.6198e-05,  ..., -4.8488e-05,
         -6.4000e-06, -1.9938e-05]], device='cuda:0')
Loss: 1.2219111919403076


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[   0, 1824,   13,  ...,  266, 5940,   19],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2582e-04, -9.6810e-08, -9.4132e-07,  ...,  6.7606e-08,
          2.7424e-05,  1.2127e-04],
        [-3.2783e-05, -1.9044e-05,  9.6634e-06,  ..., -2.8715e-05,
         -3.8054e-06, -1.1876e-05],
        [-2.1666e-05, -1.2591e-05,  6.3926e-06,  ..., -1.8999e-05,
         -2.5239e-06, -7.8604e-06],
        [-3.4988e-05, -2.0310e-05,  1.0364e-05,  ..., -3.0652e-05,
         -4.0475e-06, -1.2659e-05],
        [-6.3002e-05, -3.6597e-05,  1.8552e-05,  ..., -5.5283e-05,
         -7.3537e-06, -2.2769e-05]], device='cuda:0')
Loss: 1.196455717086792
Graident accumulation at epoch 0, step 135, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0047, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0225,  0.0064, -0.0018],
        [-0.0170,  0.0142, -0.0268,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4260e-04, -1.1795e-04, -8.2108e-06,  ...,  4.6945e-05,
         -3.8349e-04, -1.8556e-04],
        [-1.8638e-05, -1.5896e-05,  6.2117e-07,  ..., -1.4301e-05,
         -1.6782e-06, -4.0377e-06],
        [ 4.5781e-06,  2.3478e-07,  2.4400e-06,  ...,  4.7181e-06,
         -3.9091e-06, -4.4441e-06],
        [-2.3477e-05, -1.5824e-05,  4.5408e-06,  ..., -1.9051e-05,
         -3.4347e-06, -6.8441e-06],
        [-3.5110e-05, -2.4917e-05,  2.9339e-06,  ..., -2.5384e-05,
         -3.4999e-06, -8.3911e-06]], device='cuda:0')
optimizer state dict: tensor([[4.2862e-08, 2.8590e-08, 3.5969e-08,  ..., 1.1809e-08, 9.0019e-08,
         1.1611e-08],
        [1.9350e-11, 1.1336e-11, 1.7709e-12,  ..., 1.1312e-11, 8.8942e-13,
         1.8610e-12],
        [4.0484e-11, 2.2302e-11, 1.6673e-12,  ..., 2.8427e-11, 1.2743e-12,
         3.8957e-12],
        [1.1627e-11, 5.9299e-12, 5.2348e-13,  ..., 7.5183e-12, 3.6964e-13,
         9.6365e-13],
        [6.9954e-11, 4.0375e-11, 3.6290e-12,  ..., 4.9262e-11, 2.4660e-12,
         5.0482e-12]], device='cuda:0')
optimizer state dict: 17.0
lr: [1.993623915951455e-05, 1.993623915951455e-05]
scheduler_last_epoch: 17


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[    0, 17262,   342,  ...,   472,   346,   462],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2139e-05, -7.3832e-06,  3.3647e-05,  ...,  2.2109e-05,
         -1.2003e-05, -1.7065e-05],
        [-4.1723e-06, -2.5630e-06,  1.2070e-06,  ..., -3.6210e-06,
         -4.9546e-07, -1.6168e-06],
        [-2.7716e-06, -1.6913e-06,  8.0094e-07,  ..., -2.3991e-06,
         -3.2969e-07, -1.0654e-06],
        [-4.2915e-06, -2.6375e-06,  1.2517e-06,  ..., -3.7402e-06,
         -5.1036e-07, -1.6615e-06],
        [-8.0466e-06, -4.9472e-06,  2.3246e-06,  ..., -7.0035e-06,
         -9.6112e-07, -3.1143e-06]], device='cuda:0')
Loss: 1.2072744369506836


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,    18,    14,  ...,   446,   747,  1193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5207e-05,  1.8639e-05,  3.6626e-05,  ...,  2.2277e-05,
         -4.7963e-05,  3.0866e-05],
        [-8.2552e-06, -5.0515e-06,  2.3693e-06,  ..., -7.1824e-06,
         -1.0394e-06, -3.1665e-06],
        [-5.5432e-06, -3.3826e-06,  1.5907e-06,  ..., -4.8131e-06,
         -6.9849e-07, -2.1160e-06],
        [-8.5533e-06, -5.2452e-06,  2.4736e-06,  ..., -7.4655e-06,
         -1.0766e-06, -3.2783e-06],
        [-1.6034e-05, -9.8050e-06,  4.5896e-06,  ..., -1.3947e-05,
         -2.0340e-06, -6.1393e-06]], device='cuda:0')
Loss: 1.189347743988037


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2208,    12,  1901],
        [    0,  2950,    13,  ..., 16513,   300,  2205]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9944e-06, -3.3869e-06,  3.3300e-05,  ..., -2.0121e-05,
         -5.6294e-05,  1.0138e-05],
        [-1.2368e-05, -7.6145e-06,  3.5763e-06,  ..., -1.0759e-05,
         -1.5199e-06, -4.7609e-06],
        [-8.3297e-06, -5.1185e-06,  2.4103e-06,  ..., -7.2271e-06,
         -1.0245e-06, -3.1963e-06],
        [-1.2845e-05, -7.9125e-06,  3.7402e-06,  ..., -1.1191e-05,
         -1.5758e-06, -4.9397e-06],
        [-2.3961e-05, -1.4752e-05,  6.9141e-06,  ..., -2.0862e-05,
         -2.9653e-06, -9.2089e-06]], device='cuda:0')
Loss: 1.1952375173568726


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,   259,  2416,  ..., 14474,    12,   259],
        [    0,  6803,  6298,  ...,   490,  1781,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1633e-05,  5.0117e-06,  9.7677e-05,  ..., -2.0121e-05,
         -4.2916e-05, -3.0969e-05],
        [-1.6481e-05, -1.0118e-05,  4.7758e-06,  ..., -1.4320e-05,
         -1.9595e-06, -6.3404e-06],
        [-1.1057e-05, -6.7726e-06,  3.2075e-06,  ..., -9.5814e-06,
         -1.3169e-06, -4.2394e-06],
        [-1.7196e-05, -1.0550e-05,  5.0142e-06,  ..., -1.4946e-05,
         -2.0377e-06, -6.6012e-06],
        [-3.2008e-05, -1.9610e-05,  9.2536e-06,  ..., -2.7806e-05,
         -3.8259e-06, -1.2279e-05]], device='cuda:0')
Loss: 1.1961286067962646


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   271,   957,  ...,  1597,  1276,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0297e-05,  7.7037e-06,  1.1872e-04,  ..., -1.4537e-05,
         -5.5999e-05, -5.7700e-05],
        [-2.0593e-05, -1.2651e-05,  6.0722e-06,  ..., -1.7896e-05,
         -2.4699e-06, -7.9870e-06],
        [-1.3798e-05, -8.4564e-06,  4.0717e-06,  ..., -1.1966e-05,
         -1.6578e-06, -5.3346e-06],
        [-2.1517e-05, -1.3202e-05,  6.3851e-06,  ..., -1.8716e-05,
         -2.5742e-06, -8.3297e-06],
        [-3.9995e-05, -2.4498e-05,  1.1742e-05,  ..., -3.4720e-05,
         -4.8168e-06, -1.5453e-05]], device='cuda:0')
Loss: 1.197644591331482


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6726e-05,  2.2904e-06,  1.6061e-04,  ..., -2.6243e-05,
         -6.0543e-05,  4.2457e-06],
        [-2.4736e-05, -1.5169e-05,  7.2718e-06,  ..., -2.1473e-05,
         -2.9653e-06, -9.5591e-06],
        [-1.6570e-05, -1.0140e-05,  4.8764e-06,  ..., -1.4350e-05,
         -1.9893e-06, -6.3851e-06],
        [-2.5809e-05, -1.5810e-05,  7.6368e-06,  ..., -2.2411e-05,
         -3.0845e-06, -9.9614e-06],
        [-4.8101e-05, -2.9415e-05,  1.4082e-05,  ..., -4.1693e-05,
         -5.7854e-06, -1.8522e-05]], device='cuda:0')
Loss: 1.2023824453353882


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8002e-05,  3.0654e-06,  2.0237e-04,  ..., -4.7356e-05,
         -4.6205e-05,  2.9696e-05],
        [-2.8819e-05, -1.7643e-05,  8.5384e-06,  ..., -2.5019e-05,
         -3.5204e-06, -1.1168e-05],
        [-1.9327e-05, -1.1802e-05,  5.7295e-06,  ..., -1.6734e-05,
         -2.3637e-06, -7.4729e-06],
        [-3.0071e-05, -1.8388e-05,  8.9630e-06,  ..., -2.6122e-05,
         -3.6620e-06, -1.1645e-05],
        [-5.5969e-05, -3.4183e-05,  1.6510e-05,  ..., -4.8518e-05,
         -6.8583e-06, -2.1607e-05]], device='cuda:0')
Loss: 1.1882233619689941


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0,  397, 1267,  ..., 1276,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0063e-05,  1.3149e-05,  2.2310e-04,  ..., -7.7804e-05,
         -8.3082e-05, -2.1496e-05],
        [-3.2932e-05, -2.0161e-05,  9.7379e-06,  ..., -2.8580e-05,
         -4.0755e-06, -1.2740e-05],
        [-2.2098e-05, -1.3500e-05,  6.5379e-06,  ..., -1.9133e-05,
         -2.7362e-06, -8.5309e-06],
        [-3.4332e-05, -2.0996e-05,  1.0215e-05,  ..., -2.9817e-05,
         -4.2319e-06, -1.3269e-05],
        [-6.3956e-05, -3.9071e-05,  1.8835e-05,  ..., -5.5432e-05,
         -7.9311e-06, -2.4647e-05]], device='cuda:0')
Loss: 1.1984469890594482
Graident accumulation at epoch 0, step 143, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0406,  ...,  0.0225,  0.0065, -0.0018],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0434e-04, -1.0484e-04,  1.4921e-05,  ...,  3.4470e-05,
         -3.5345e-04, -1.6915e-04],
        [-2.0068e-05, -1.6323e-05,  1.5328e-06,  ..., -1.5729e-05,
         -1.9179e-06, -4.9080e-06],
        [ 1.9105e-06, -1.1387e-06,  2.8498e-06,  ...,  2.3330e-06,
         -3.7918e-06, -4.8528e-06],
        [-2.4562e-05, -1.6342e-05,  5.1082e-06,  ..., -2.0127e-05,
         -3.5144e-06, -7.4866e-06],
        [-3.7995e-05, -2.6332e-05,  4.5240e-06,  ..., -2.8389e-05,
         -3.9430e-06, -1.0017e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2823e-08, 2.8562e-08, 3.5983e-08,  ..., 1.1803e-08, 8.9936e-08,
         1.1600e-08],
        [2.0416e-11, 1.1731e-11, 1.8640e-12,  ..., 1.2117e-11, 9.0514e-13,
         2.0214e-12],
        [4.0932e-11, 2.2462e-11, 1.7084e-12,  ..., 2.8765e-11, 1.2805e-12,
         3.9646e-12],
        [1.2794e-11, 6.3648e-12, 6.2730e-13,  ..., 8.3999e-12, 3.8718e-13,
         1.1388e-12],
        [7.3974e-11, 4.1861e-11, 3.9801e-12,  ..., 5.2286e-11, 2.5265e-12,
         5.6506e-12]], device='cuda:0')
optimizer state dict: 18.0
lr: [1.99215431440706e-05, 1.99215431440706e-05]
scheduler_last_epoch: 18


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   281,   221,  ...,  2236, 15064,  1458],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9535e-06,  2.6266e-05,  2.2569e-06,  ..., -4.0173e-05,
         -1.3518e-05, -9.4243e-05],
        [-4.0829e-06, -2.5332e-06,  1.1548e-06,  ..., -3.5465e-06,
         -4.5635e-07, -1.6987e-06],
        [-2.7865e-06, -1.7211e-06,  7.8976e-07,  ..., -2.4140e-06,
         -3.1106e-07, -1.1548e-06],
        [-4.3213e-06, -2.6673e-06,  1.2293e-06,  ..., -3.7551e-06,
         -4.8056e-07, -1.7956e-06],
        [-8.1062e-06, -5.0068e-06,  2.2799e-06,  ..., -7.0035e-06,
         -8.9779e-07, -3.3379e-06]], device='cuda:0')
Loss: 1.2054047584533691


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0, 12472,  1059,  ...,   642,   365,  6517],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7080e-06,  6.1897e-06,  3.1102e-06,  ..., -1.9570e-05,
         -1.1080e-06, -1.3894e-04],
        [-8.2254e-06, -5.1707e-06,  2.3246e-06,  ..., -7.0930e-06,
         -8.9966e-07, -3.4049e-06],
        [-5.6475e-06, -3.5465e-06,  1.5981e-06,  ..., -4.8727e-06,
         -6.1840e-07, -2.3395e-06],
        [-8.6725e-06, -5.4389e-06,  2.4661e-06,  ..., -7.4953e-06,
         -9.4622e-07, -3.5912e-06],
        [-1.6272e-05, -1.0222e-05,  4.5747e-06,  ..., -1.4007e-05,
         -1.7770e-06, -6.6906e-06]], device='cuda:0')
Loss: 1.2096750736236572


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  5150,  1030,  ...,    14,   475,  1763],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2642e-04, -2.2768e-06,  6.9454e-06,  ...,  2.1595e-06,
          1.6140e-05, -6.1083e-05],
        [-1.2368e-05, -7.7188e-06,  3.4720e-06,  ..., -1.0654e-05,
         -1.3690e-06, -5.0664e-06],
        [-8.4937e-06, -5.2974e-06,  2.3879e-06,  ..., -7.3165e-06,
         -9.4064e-07, -3.4794e-06],
        [-1.3024e-05, -8.1211e-06,  3.6806e-06,  ..., -1.1235e-05,
         -1.4380e-06, -5.3346e-06],
        [-2.4557e-05, -1.5318e-05,  6.8694e-06,  ..., -2.1130e-05,
         -2.7157e-06, -9.9987e-06]], device='cuda:0')
Loss: 1.21113920211792


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0,   292,   960,  ...,   271,  1356,    14],
        [    0,   342,  8514,  ...,   266, 46850,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5304e-04,  5.3739e-06,  1.2721e-07,  ...,  2.1595e-06,
          5.9353e-06, -5.0862e-05],
        [-1.6481e-05, -1.0252e-05,  4.5970e-06,  ..., -1.4171e-05,
         -1.8198e-06, -6.7353e-06],
        [-1.1340e-05, -7.0482e-06,  3.1665e-06,  ..., -9.7454e-06,
         -1.2517e-06, -4.6343e-06],
        [-1.7345e-05, -1.0788e-05,  4.8727e-06,  ..., -1.4931e-05,
         -1.9092e-06, -7.0855e-06],
        [-3.2723e-05, -2.0325e-05,  9.1046e-06,  ..., -2.8074e-05,
         -3.6098e-06, -1.3292e-05]], device='cuda:0')
Loss: 1.1870664358139038


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  292,  334,  674],
        [   0,  300,  266,  ...,  271, 4111, 1188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0899e-04, -7.1924e-06,  1.5579e-05,  ...,  8.0115e-06,
         -1.6290e-05, -9.0019e-05],
        [-2.0623e-05, -1.2845e-05,  5.7146e-06,  ..., -1.7747e-05,
         -2.2650e-06, -8.3894e-06],
        [-1.4216e-05, -8.8364e-06,  3.9376e-06,  ..., -1.2204e-05,
         -1.5572e-06, -5.7742e-06],
        [-2.1666e-05, -1.3486e-05,  6.0350e-06,  ..., -1.8641e-05,
         -2.3711e-06, -8.8066e-06],
        [-4.1068e-05, -2.5541e-05,  1.1340e-05,  ..., -3.5256e-05,
         -4.5113e-06, -1.6615e-05]], device='cuda:0')
Loss: 1.2080308198928833


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0,  344, 8133,  ...,  368, 1119, 5539],
        [   0,  825, 1243,  ...,   15,   22,   42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5242e-04,  1.3247e-05,  2.0874e-05,  ..., -7.3142e-06,
         -3.4305e-05, -3.5983e-05],
        [-2.4706e-05, -1.5393e-05,  6.8471e-06,  ..., -2.1294e-05,
         -2.6952e-06, -1.0006e-05],
        [-1.7032e-05, -1.0595e-05,  4.7199e-06,  ..., -1.4648e-05,
         -1.8533e-06, -6.8918e-06],
        [-2.5958e-05, -1.6153e-05,  7.2271e-06,  ..., -2.2352e-05,
         -2.8200e-06, -1.0498e-05],
        [-4.9233e-05, -3.0607e-05,  1.3590e-05,  ..., -4.2319e-05,
         -5.3719e-06, -1.9833e-05]], device='cuda:0')
Loss: 1.1903893947601318


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0, 1167, 2667,  ..., 4769,   13, 5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8160e-04,  1.0071e-05,  6.0206e-05,  ..., -2.8680e-05,
         -1.9736e-05,  8.6964e-06],
        [-2.8819e-05, -1.7986e-05,  7.9796e-06,  ..., -2.4810e-05,
         -3.1684e-06, -1.1675e-05],
        [-1.9878e-05, -1.2383e-05,  5.4985e-06,  ..., -1.7077e-05,
         -2.1793e-06, -8.0392e-06],
        [-3.0249e-05, -1.8865e-05,  8.4117e-06,  ..., -2.6017e-05,
         -3.3118e-06, -1.2234e-05],
        [-5.7399e-05, -3.5733e-05,  1.5825e-05,  ..., -4.9263e-05,
         -6.3144e-06, -2.3112e-05]], device='cuda:0')
Loss: 1.1947449445724487


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,   275,   467,  ...,   298,   365,  2714],
        [    0,  3386, 43625,  ...,    19,  2125,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7209e-04,  8.2720e-05,  7.7449e-05,  ..., -2.9390e-05,
         -1.9736e-05,  1.9197e-05],
        [-3.2932e-05, -2.0519e-05,  9.1195e-06,  ..., -2.8357e-05,
         -3.5632e-06, -1.3366e-05],
        [-2.2724e-05, -1.4134e-05,  6.2883e-06,  ..., -1.9521e-05,
         -2.4494e-06, -9.2089e-06],
        [-3.4541e-05, -2.1502e-05,  9.6038e-06,  ..., -2.9713e-05,
         -3.7197e-06, -1.3992e-05],
        [-6.5625e-05, -4.0799e-05,  1.8090e-05,  ..., -5.6326e-05,
         -7.1004e-06, -2.6479e-05]], device='cuda:0')
Loss: 1.204606056213379
Graident accumulation at epoch 0, step 151, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0539, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0221, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0336, -0.0095,  0.0405,  ...,  0.0225,  0.0065, -0.0018],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.8112e-04, -8.6081e-05,  2.1174e-05,  ...,  2.8084e-05,
         -3.2008e-04, -1.5032e-04],
        [-2.1354e-05, -1.6742e-05,  2.2915e-06,  ..., -1.6992e-05,
         -2.0824e-06, -5.7538e-06],
        [-5.5299e-07, -2.4382e-06,  3.1936e-06,  ...,  1.4763e-07,
         -3.6576e-06, -5.2884e-06],
        [-2.5560e-05, -1.6858e-05,  5.5578e-06,  ..., -2.1086e-05,
         -3.5349e-06, -8.1372e-06],
        [-4.0758e-05, -2.7779e-05,  5.8806e-06,  ..., -3.1183e-05,
         -4.2588e-06, -1.1663e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2809e-08, 2.8540e-08, 3.5953e-08,  ..., 1.1792e-08, 8.9846e-08,
         1.1588e-08],
        [2.1480e-11, 1.2140e-11, 1.9453e-12,  ..., 1.2909e-11, 9.1693e-13,
         2.1981e-12],
        [4.1408e-11, 2.2639e-11, 1.7462e-12,  ..., 2.9117e-11, 1.2852e-12,
         4.0454e-12],
        [1.3975e-11, 6.8208e-12, 7.1891e-13,  ..., 9.2743e-12, 4.0062e-13,
         1.3334e-12],
        [7.8207e-11, 4.3483e-11, 4.3034e-12,  ..., 5.5406e-11, 2.5744e-12,
         6.3461e-12]], device='cuda:0')
optimizer state dict: 19.0
lr: [1.9905331030227867e-05, 1.9905331030227867e-05]
scheduler_last_epoch: 19


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,  1527,   292,  ...,  2122,   278,  1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8992e-05, -5.4470e-06,  6.0540e-05,  ...,  2.0776e-05,
          4.0542e-05,  5.8314e-06],
        [-4.1127e-06, -2.6375e-06,  1.0133e-06,  ..., -3.3975e-06,
         -3.6508e-07, -1.6838e-06],
        [-2.9355e-06, -1.8850e-06,  7.1898e-07,  ..., -2.4289e-06,
         -2.6077e-07, -1.1995e-06],
        [-4.2915e-06, -2.7567e-06,  1.0580e-06,  ..., -3.5465e-06,
         -3.7812e-07, -1.7583e-06],
        [-8.4639e-06, -5.4240e-06,  2.0713e-06,  ..., -6.9737e-06,
         -7.5251e-07, -3.4571e-06]], device='cuda:0')
Loss: 1.1819803714752197


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1862,  674,  ...,  391,  266, 7688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8791e-05, -4.4714e-05,  9.3111e-05,  ...,  2.2282e-06,
          7.0730e-05, -3.5857e-05],
        [-8.2254e-06, -5.2750e-06,  2.0564e-06,  ..., -6.8247e-06,
         -7.5810e-07, -3.4347e-06],
        [-5.8860e-06, -3.7774e-06,  1.4640e-06,  ..., -4.8876e-06,
         -5.4203e-07, -2.4512e-06],
        [-8.6129e-06, -5.5432e-06,  2.1607e-06,  ..., -7.1526e-06,
         -7.8976e-07, -3.5986e-06],
        [-1.6987e-05, -1.0878e-05,  4.2170e-06,  ..., -1.4067e-05,
         -1.5646e-06, -7.0632e-06]], device='cuda:0')
Loss: 1.1996870040893555


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0,  2496, 10545,  ...,   287, 13978,   408],
        [    0,    14,   747,  ...,  2039,   287,  8053]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2381e-05, -4.6742e-05,  1.2408e-04,  ..., -4.5514e-07,
          5.3233e-05, -6.1922e-05],
        [-1.2249e-05, -7.8827e-06,  3.0845e-06,  ..., -1.0267e-05,
         -1.1027e-06, -5.0813e-06],
        [-8.7321e-06, -5.6252e-06,  2.1942e-06,  ..., -7.3165e-06,
         -7.8604e-07, -3.6210e-06],
        [-1.2785e-05, -8.2403e-06,  3.2336e-06,  ..., -1.0714e-05,
         -1.1455e-06, -5.3048e-06],
        [-2.5392e-05, -1.6302e-05,  6.3628e-06,  ..., -2.1219e-05,
         -2.2873e-06, -1.0490e-05]], device='cuda:0')
Loss: 1.2045069932937622


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[   0,  668, 2474,  ...,  668, 4599,  360],
        [   0,   13, 1529,  ..., 8197, 2700, 9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1062e-05, -8.6505e-05,  1.1118e-04,  ..., -2.4516e-05,
          5.9260e-05, -3.9658e-05],
        [-1.6332e-05, -1.0520e-05,  4.1053e-06,  ..., -1.3709e-05,
         -1.4734e-06, -6.7279e-06],
        [-1.1653e-05, -7.5102e-06,  2.9206e-06,  ..., -9.7603e-06,
         -1.0505e-06, -4.7982e-06],
        [-1.7047e-05, -1.0982e-05,  4.2990e-06,  ..., -1.4290e-05,
         -1.5330e-06, -7.0184e-06],
        [-3.3855e-05, -2.1756e-05,  8.4639e-06,  ..., -2.8342e-05,
         -3.0585e-06, -1.3888e-05]], device='cuda:0')
Loss: 1.1897884607315063


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,  5143,  3877,  ...,   292, 44003,    12],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0258e-05, -9.0325e-05,  1.3322e-04,  ..., -2.4705e-05,
          4.8671e-05, -3.6301e-05],
        [-2.0474e-05, -1.3143e-05,  5.0589e-06,  ..., -1.7211e-05,
         -1.8533e-06, -8.3968e-06],
        [-1.4588e-05, -9.3654e-06,  3.5986e-06,  ..., -1.2249e-05,
         -1.3206e-06, -5.9828e-06],
        [-2.1338e-05, -1.3694e-05,  5.2974e-06,  ..., -1.7911e-05,
         -1.9278e-06, -8.7470e-06],
        [-4.2439e-05, -2.7180e-05,  1.0446e-05,  ..., -3.5614e-05,
         -3.8520e-06, -1.7345e-05]], device='cuda:0')
Loss: 1.1967936754226685


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0, 5221, 7166,  ..., 4309,  342,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6557e-05, -9.0483e-05,  1.5422e-04,  ..., -4.7596e-05,
          3.2570e-05, -5.7258e-05],
        [-2.4587e-05, -1.5765e-05,  6.0871e-06,  ..., -2.0698e-05,
         -2.2519e-06, -1.0118e-05],
        [-1.7479e-05, -1.1213e-05,  4.3213e-06,  ..., -1.4707e-05,
         -1.6000e-06, -7.1973e-06],
        [-2.5541e-05, -1.6376e-05,  6.3553e-06,  ..., -2.1502e-05,
         -2.3320e-06, -1.0513e-05],
        [-5.0843e-05, -3.2544e-05,  1.2547e-05,  ..., -4.2796e-05,
         -4.6678e-06, -2.0877e-05]], device='cuda:0')
Loss: 1.2013086080551147


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0,  1217,     9,  ...,  1821,     5,   278],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3908e-06, -8.4175e-05,  1.0603e-04,  ..., -4.5510e-05,
          4.0776e-05, -3.7722e-05],
        [-2.8670e-05, -1.8314e-05,  7.0632e-06,  ..., -2.4170e-05,
         -2.6245e-06, -1.1824e-05],
        [-2.0400e-05, -1.3039e-05,  5.0142e-06,  ..., -1.7181e-05,
         -1.8664e-06, -8.4117e-06],
        [-2.9802e-05, -1.9029e-05,  7.3686e-06,  ..., -2.5108e-05,
         -2.7176e-06, -1.2279e-05],
        [-5.9307e-05, -3.7819e-05,  1.4544e-05,  ..., -4.9949e-05,
         -5.4352e-06, -2.4378e-05]], device='cuda:0')
Loss: 1.1996577978134155


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ...,  271,   13,  704],
        [   0, 2159,  271,  ..., 1268,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1758e-05, -4.8882e-05,  1.2231e-04,  ..., -1.0378e-04,
          4.8703e-05, -2.5622e-05],
        [-3.2723e-05, -2.0862e-05,  8.0168e-06,  ..., -2.7657e-05,
         -3.0398e-06, -1.3433e-05],
        [-2.3276e-05, -1.4849e-05,  5.6922e-06,  ..., -1.9655e-05,
         -2.1607e-06, -9.5591e-06],
        [-3.4004e-05, -2.1681e-05,  8.3670e-06,  ..., -2.8729e-05,
         -3.1441e-06, -1.3955e-05],
        [-6.7711e-05, -4.3124e-05,  1.6525e-05,  ..., -5.7161e-05,
         -6.2995e-06, -2.7716e-05]], device='cuda:0')
Loss: 1.2128735780715942
Graident accumulation at epoch 0, step 159, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0225,  0.0065, -0.0017],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0277, -0.0160, -0.0189]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4418e-04, -8.2361e-05,  3.1288e-05,  ...,  1.4897e-05,
         -2.8320e-04, -1.3785e-04],
        [-2.2491e-05, -1.7154e-05,  2.8640e-06,  ..., -1.8058e-05,
         -2.1782e-06, -6.5218e-06],
        [-2.8253e-06, -3.6793e-06,  3.4435e-06,  ..., -1.8326e-06,
         -3.5079e-06, -5.7155e-06],
        [-2.6405e-05, -1.7340e-05,  5.8387e-06,  ..., -2.1850e-05,
         -3.4959e-06, -8.7190e-06],
        [-4.3453e-05, -2.9313e-05,  6.9451e-06,  ..., -3.3781e-05,
         -4.4628e-06, -1.3268e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2767e-08, 2.8514e-08, 3.5932e-08,  ..., 1.1791e-08, 8.9758e-08,
         1.1577e-08],
        [2.2529e-11, 1.2563e-11, 2.0076e-12,  ..., 1.3661e-11, 9.2526e-13,
         2.3763e-12],
        [4.1908e-11, 2.2837e-11, 1.7769e-12,  ..., 2.9474e-11, 1.2886e-12,
         4.1327e-12],
        [1.5117e-11, 7.2840e-12, 7.8819e-13,  ..., 1.0090e-11, 4.1011e-13,
         1.5268e-12],
        [8.2713e-11, 4.5300e-11, 4.5722e-12,  ..., 5.8618e-11, 2.6115e-12,
         7.1079e-12]], device='cuda:0')
optimizer state dict: 20.0
lr: [1.9887605295338853e-05, 1.9887605295338853e-05]
scheduler_last_epoch: 20
Epoch 0 | Batch 159/1048 | Training PPL: 16366.44734593623 | time 11.985222101211548
Saving checkpoint at epoch 0, step 159, batch 159
Epoch 0 | Validation PPL: 10.902067974460058 | Learning rate: 1.9887605295338853e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_159, AFTER epoch 0, step 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,    13,  6913,  ...,   278,  1317,  4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1532e-05,  5.7355e-05,  6.2920e-06,  ..., -2.3375e-05,
         -2.1437e-05, -5.4110e-05],
        [-4.0233e-06, -2.6226e-06,  9.0525e-07,  ..., -3.5018e-06,
         -3.3900e-07, -1.7956e-06],
        [-2.9057e-06, -1.8924e-06,  6.5193e-07,  ..., -2.5183e-06,
         -2.4214e-07, -1.2964e-06],
        [-4.1425e-06, -2.6971e-06,  9.3132e-07,  ..., -3.5763e-06,
         -3.4273e-07, -1.8403e-06],
        [-8.4043e-06, -5.4538e-06,  1.8775e-06,  ..., -7.2718e-06,
         -6.9663e-07, -3.7253e-06]], device='cuda:0')
Loss: 1.2193163633346558


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,   578,   221,  ...,   287,  1254,  4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5455e-05,  8.3513e-05, -5.6863e-05,  ...,  6.5240e-05,
         -3.3483e-05, -5.0211e-05],
        [-8.1062e-06, -5.2303e-06,  1.8664e-06,  ..., -6.8694e-06,
         -5.8860e-07, -3.5539e-06],
        [-5.9456e-06, -3.8296e-06,  1.3672e-06,  ..., -5.0217e-06,
         -4.2655e-07, -2.6003e-06],
        [-8.3745e-06, -5.3942e-06,  1.9297e-06,  ..., -7.0632e-06,
         -5.9791e-07, -3.6582e-06],
        [-1.7226e-05, -1.1057e-05,  3.9488e-06,  ..., -1.4514e-05,
         -1.2331e-06, -7.5102e-06]], device='cuda:0')
Loss: 1.19813072681427


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0,  504,  409,  ..., 5863, 2621,  824],
        [   0,  593, 1387,  ...,  508, 8222, 1415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8123e-05,  6.2965e-05, -5.0034e-05,  ...,  5.6003e-05,
         -2.4365e-06, -6.2535e-05],
        [-1.2189e-05, -7.8976e-06,  2.7344e-06,  ..., -1.0312e-05,
         -9.0152e-07, -5.3719e-06],
        [-8.9258e-06, -5.7817e-06,  2.0005e-06,  ..., -7.5400e-06,
         -6.5379e-07, -3.9339e-06],
        [-1.2606e-05, -8.1509e-06,  2.8312e-06,  ..., -1.0625e-05,
         -9.1828e-07, -5.5432e-06],
        [-2.5809e-05, -1.6689e-05,  5.7742e-06,  ..., -2.1756e-05,
         -1.8887e-06, -1.1325e-05]], device='cuda:0')
Loss: 1.1939586400985718


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,    13, 36961,  ...,  6671, 13711,  4568]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2214e-04,  6.0473e-05, -2.7550e-05,  ...,  4.0014e-05,
          2.2754e-05, -8.9303e-05],
        [-1.6242e-05, -1.0535e-05,  3.5688e-06,  ..., -1.3754e-05,
         -1.1772e-06, -7.1526e-06],
        [-1.1861e-05, -7.7039e-06,  2.6077e-06,  ..., -1.0043e-05,
         -8.5402e-07, -5.2303e-06],
        [-1.6779e-05, -1.0863e-05,  3.6918e-06,  ..., -1.4171e-05,
         -1.2014e-06, -7.3835e-06],
        [-3.4511e-05, -2.2352e-05,  7.5549e-06,  ..., -2.9147e-05,
         -2.4848e-06, -1.5140e-05]], device='cuda:0')
Loss: 1.207492470741272


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   292,   474,   221],
        [    0,  3761,   527,  ..., 24518,   391,   638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0983e-05,  5.2547e-05, -1.1455e-05,  ...,  4.5892e-05,
          5.7029e-05, -6.3293e-05],
        [-2.0325e-05, -1.3188e-05,  4.4294e-06,  ..., -1.7136e-05,
         -1.4678e-06, -8.8960e-06],
        [-1.4856e-05, -9.6560e-06,  3.2410e-06,  ..., -1.2532e-05,
         -1.0682e-06, -6.5118e-06],
        [-2.1040e-05, -1.3635e-05,  4.5970e-06,  ..., -1.7703e-05,
         -1.5032e-06, -9.2089e-06],
        [-4.3273e-05, -2.8074e-05,  9.4026e-06,  ..., -3.6418e-05,
         -3.1106e-06, -1.8895e-05]], device='cuda:0')
Loss: 1.1713643074035645


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   360,   259,  ...,  5710,   278,  2433],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7460e-05,  8.3348e-05,  3.6542e-05,  ...,  4.7939e-05,
          4.7717e-05, -8.4977e-05],
        [-2.4348e-05, -1.5795e-05,  5.3197e-06,  ..., -2.0564e-05,
         -1.7527e-06, -1.0692e-05],
        [-1.7792e-05, -1.1563e-05,  3.8967e-06,  ..., -1.5035e-05,
         -1.2796e-06, -7.8306e-06],
        [-2.5213e-05, -1.6347e-05,  5.5283e-06,  ..., -2.1264e-05,
         -1.7993e-06, -1.1079e-05],
        [-5.1856e-05, -3.3647e-05,  1.1303e-05,  ..., -4.3750e-05,
         -3.7253e-06, -2.2709e-05]], device='cuda:0')
Loss: 1.1960279941558838


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2063e-04,  6.2561e-05,  5.4117e-06,  ...,  2.8016e-05,
          3.1311e-05, -1.0541e-04],
        [-2.8402e-05, -1.8433e-05,  6.2175e-06,  ..., -2.4006e-05,
         -2.0619e-06, -1.2435e-05],
        [ 7.0459e-05,  6.2186e-05, -1.4532e-05,  ...,  6.0858e-05,
          9.7097e-06,  1.9223e-05],
        [-2.9445e-05, -1.9103e-05,  6.4671e-06,  ..., -2.4840e-05,
         -2.1216e-06, -1.2897e-05],
        [-6.0558e-05, -3.9309e-05,  1.3225e-05,  ..., -5.1141e-05,
         -4.3884e-06, -2.6450e-05]], device='cuda:0')
Loss: 1.2071577310562134


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 1083,  287,  ...,   12,  287, 2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3830e-04,  6.7717e-05,  1.4142e-05,  ..., -1.2302e-05,
          3.3677e-05, -6.6679e-05],
        [-3.2514e-05, -2.1130e-05,  7.1153e-06,  ..., -2.7448e-05,
         -2.3507e-06, -1.4238e-05],
        [ 6.7434e-05,  6.0219e-05, -1.3872e-05,  ...,  5.8339e-05,
          9.4983e-06,  1.7905e-05],
        [-3.3677e-05, -2.1875e-05,  7.3984e-06,  ..., -2.8387e-05,
         -2.4177e-06, -1.4752e-05],
        [-6.9320e-05, -4.5002e-05,  1.5117e-05,  ..., -5.8413e-05,
         -5.0068e-06, -3.0264e-05]], device='cuda:0')
Loss: 1.1818737983703613
Graident accumulation at epoch 0, step 167, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0034,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0170,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2359e-04, -6.7353e-05,  2.9573e-05,  ...,  1.2177e-05,
         -2.5151e-04, -1.3073e-04],
        [-2.3493e-05, -1.7552e-05,  3.2892e-06,  ..., -1.8997e-05,
         -2.1954e-06, -7.2934e-06],
        [ 4.2006e-06,  2.7105e-06,  1.7119e-06,  ...,  4.1846e-06,
         -2.2073e-06, -3.3535e-06],
        [-2.7132e-05, -1.7794e-05,  5.9947e-06,  ..., -2.2504e-05,
         -3.3880e-06, -9.3223e-06],
        [-4.6040e-05, -3.0882e-05,  7.7623e-06,  ..., -3.6244e-05,
         -4.5172e-06, -1.4968e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2743e-08, 2.8490e-08, 3.5896e-08,  ..., 1.1780e-08, 8.9670e-08,
         1.1570e-08],
        [2.3564e-11, 1.2997e-11, 2.0562e-12,  ..., 1.4401e-11, 9.2986e-13,
         2.5767e-12],
        [4.6413e-11, 2.6441e-11, 1.9675e-12,  ..., 3.2848e-11, 1.3775e-12,
         4.4492e-12],
        [1.6236e-11, 7.7553e-12, 8.4214e-13,  ..., 1.0886e-11, 4.1555e-13,
         1.7429e-12],
        [8.7436e-11, 4.7279e-11, 4.7961e-12,  ..., 6.1971e-11, 2.6339e-12,
         8.0167e-12]], device='cuda:0')
optimizer state dict: 21.0
lr: [1.9868368648050586e-05, 1.9868368648050586e-05]
scheduler_last_epoch: 21


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  1832,   292,  ...,  2176,  1345,    14],
        [    0,   767,  4478,  ...,   278,   266, 19201]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4191e-05, -1.6499e-05, -3.3142e-05,  ...,  3.3248e-06,
         -2.3714e-05, -2.0763e-06],
        [-4.0233e-06, -2.6673e-06,  7.1898e-07,  ..., -3.4124e-06,
         -2.1793e-07, -1.8477e-06],
        [-3.0398e-06, -2.0266e-06,  5.4389e-07,  ..., -2.5779e-06,
         -1.6298e-07, -1.3933e-06],
        [-4.1425e-06, -2.7418e-06,  7.4133e-07,  ..., -3.5018e-06,
         -2.2259e-07, -1.8999e-06],
        [-8.9407e-06, -5.9307e-06,  1.5870e-06,  ..., -7.5698e-06,
         -4.8429e-07, -4.0829e-06]], device='cuda:0')
Loss: 1.189488410949707


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 6112,  278,  ..., 4092,  490, 2774],
        [   0, 2319,   30,  ...,  508, 6703,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2221e-05, -3.0153e-06, -3.5707e-05,  ...,  1.7869e-05,
         -3.1714e-05,  6.2515e-05],
        [-8.0466e-06, -5.3793e-06,  1.5013e-06,  ..., -6.8545e-06,
         -4.3865e-07, -3.6731e-06],
        [ 8.8806e-05,  6.5684e-05, -1.9128e-05,  ...,  7.3533e-05,
          2.1321e-05,  6.3665e-05],
        [-8.2552e-06, -5.5283e-06,  1.5460e-06,  ..., -7.0333e-06,
         -4.4610e-07, -3.7625e-06],
        [-1.7762e-05, -1.1891e-05,  3.2932e-06,  ..., -1.5110e-05,
         -9.6858e-07, -8.0764e-06]], device='cuda:0')
Loss: 1.1957449913024902


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  2523,  4729, 13753],
        [    0,   259,  1380,  ...,   287, 10221,   280]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0698e-04, -2.2208e-05, -1.7630e-05,  ...,  1.9208e-05,
         -4.9940e-05,  4.4762e-05],
        [-1.2130e-05, -8.1360e-06,  2.2054e-06,  ..., -1.0237e-05,
         -6.4634e-07, -5.5060e-06],
        [ 8.5722e-05,  6.3598e-05, -1.8596e-05,  ...,  7.0970e-05,
          2.1164e-05,  6.2272e-05],
        [-1.2398e-05, -8.3297e-06,  2.2650e-06,  ..., -1.0476e-05,
         -6.5565e-07, -5.6252e-06],
        [-2.6703e-05, -1.7941e-05,  4.8354e-06,  ..., -2.2560e-05,
         -1.4249e-06, -1.2100e-05]], device='cuda:0')
Loss: 1.2016322612762451


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1016,   271,  ...,   461,   616,   993],
        [    0,   278,   264,  ..., 21836,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1097e-04, -6.3307e-05,  2.8008e-06,  ...,  1.9208e-05,
         -6.6510e-05,  4.5041e-05],
        [-1.6183e-05, -1.0848e-05,  2.9467e-06,  ..., -1.3590e-05,
         -9.0152e-07, -7.3761e-06],
        [ 8.2682e-05,  6.1572e-05, -1.8037e-05,  ...,  6.8452e-05,
          2.0972e-05,  6.0871e-05],
        [-1.6540e-05, -1.1101e-05,  3.0287e-06,  ..., -1.3903e-05,
         -9.1828e-07, -7.5325e-06],
        [-3.5584e-05, -2.3872e-05,  6.4597e-06,  ..., -2.9892e-05,
         -1.9874e-06, -1.6183e-05]], device='cuda:0')
Loss: 1.1901304721832275


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2336,   221,   334],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2126e-04, -1.1754e-04, -3.4805e-06,  ..., -1.5838e-05,
         -5.6518e-05,  5.1268e-05],
        [-2.0295e-05, -1.3590e-05,  3.6843e-06,  ..., -1.7017e-05,
         -1.1297e-06, -9.2387e-06],
        [ 7.9642e-05,  5.9530e-05, -1.7489e-05,  ...,  6.5918e-05,
          2.0801e-05,  5.9493e-05],
        [-2.0683e-05, -1.3873e-05,  3.7774e-06,  ..., -1.7360e-05,
         -1.1483e-06, -9.4175e-06],
        [-4.4584e-05, -2.9862e-05,  8.0690e-06,  ..., -3.7372e-05,
         -2.4904e-06, -2.0236e-05]], device='cuda:0')
Loss: 1.1952227354049683


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0,  278, 1059,  ...,  300, 1877,   13],
        [   0, 1125,  278,  ..., 6447,  609,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3068e-04, -1.3275e-04, -3.2699e-05,  ...,  6.3457e-06,
         -2.4337e-05,  7.3156e-05],
        [-2.4378e-05, -1.6227e-05,  4.4145e-06,  ..., -2.0385e-05,
         -1.3812e-06, -1.1109e-05],
        [ 7.6542e-05,  5.7518e-05, -1.6934e-05,  ...,  6.3370e-05,
          2.0610e-05,  5.8070e-05],
        [-2.4945e-05, -1.6615e-05,  4.5411e-06,  ..., -2.0847e-05,
         -1.4091e-06, -1.1355e-05],
        [-5.3585e-05, -3.5673e-05,  9.6634e-06,  ..., -4.4763e-05,
         -3.0492e-06, -2.4348e-05]], device='cuda:0')
Loss: 1.188739538192749


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[   0, 2314,  266,  ...,  342, 7299, 1099],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2225e-04, -1.6197e-04, -2.0901e-05,  ..., -2.0250e-05,
         -3.4285e-06,  8.3014e-05],
        [-2.8461e-05, -1.8924e-05,  5.1558e-06,  ..., -2.3767e-05,
         -1.6401e-06, -1.2971e-05],
        [ 2.8560e-04,  1.8062e-04, -1.3345e-05,  ...,  2.5634e-04,
          3.6302e-05,  1.3805e-04],
        [-2.9087e-05, -1.9372e-05,  5.3011e-06,  ..., -2.4289e-05,
         -1.6736e-06, -1.3247e-05],
        [-6.2466e-05, -4.1574e-05,  1.1280e-05,  ..., -5.2154e-05,
         -3.6154e-06, -2.8402e-05]], device='cuda:0')
Loss: 1.1941349506378174


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0, 19191,   266,  ...,   287,   843,  1528],
        [    0,   380,  6119,  ..., 11823,   287,  6797]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5563e-04, -2.1828e-04, -4.3589e-05,  ..., -2.6878e-05,
          2.3977e-06,  5.6729e-05],
        [-3.2455e-05, -2.1562e-05,  5.8860e-06,  ..., -2.7090e-05,
         -1.9027e-06, -1.4842e-05],
        [ 2.8257e-04,  1.7862e-04, -1.2794e-05,  ...,  2.5384e-04,
          3.6103e-05,  1.3663e-04],
        [-3.3230e-05, -2.2098e-05,  6.0573e-06,  ..., -2.7716e-05,
         -1.9418e-06, -1.5184e-05],
        [-7.1406e-05, -4.7445e-05,  1.2897e-05,  ..., -5.9545e-05,
         -4.2003e-06, -3.2574e-05]], device='cuda:0')
Loss: 1.2051953077316284
Graident accumulation at epoch 0, step 175, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0153,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0169,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0680e-04, -8.2446e-05,  2.2257e-05,  ...,  8.2717e-06,
         -2.2612e-04, -1.1198e-04],
        [-2.4389e-05, -1.7953e-05,  3.5488e-06,  ..., -1.9807e-05,
         -2.1662e-06, -8.0482e-06],
        [ 3.2038e-05,  2.0302e-05,  2.6135e-07,  ...,  2.9150e-05,
          1.6238e-06,  1.0645e-05],
        [-2.7742e-05, -1.8224e-05,  6.0009e-06,  ..., -2.3025e-05,
         -3.2434e-06, -9.9085e-06],
        [-4.8577e-05, -3.2538e-05,  8.2758e-06,  ..., -3.8574e-05,
         -4.4855e-06, -1.6728e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2725e-08, 2.8509e-08, 3.5862e-08,  ..., 1.1769e-08, 8.9580e-08,
         1.1562e-08],
        [2.4593e-11, 1.3449e-11, 2.0888e-12,  ..., 1.5120e-11, 9.3255e-13,
         2.7944e-12],
        [1.2621e-10, 5.8321e-11, 2.1292e-12,  ..., 9.7249e-11, 2.6796e-12,
         2.3114e-11],
        [1.7324e-11, 8.2358e-12, 8.7799e-13,  ..., 1.1643e-11, 4.1890e-13,
         1.9717e-12],
        [9.2447e-11, 4.9483e-11, 4.9576e-12,  ..., 6.5455e-11, 2.6489e-12,
         9.0698e-12]], device='cuda:0')
optimizer state dict: 22.0
lr: [1.9847624027890693e-05, 1.9847624027890693e-05]
scheduler_last_epoch: 22


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[    0, 18837,   394,  ...,   271,  1398,  1871],
        [    0,  3070,  9719,  ...,   600,  4207,  4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1399e-06,  7.5371e-06, -2.3635e-05,  ...,  7.7769e-05,
          1.3793e-05, -5.6754e-05],
        [-4.0531e-06, -2.6822e-06,  5.7369e-07,  ..., -3.2932e-06,
         -2.3376e-07, -1.9372e-06],
        [-3.1888e-06, -2.1160e-06,  4.5076e-07,  ..., -2.5928e-06,
         -1.8440e-07, -1.5274e-06],
        [-4.1425e-06, -2.7418e-06,  5.8860e-07,  ..., -3.3677e-06,
         -2.3749e-07, -1.9819e-06],
        [-9.1791e-06, -6.0797e-06,  1.2964e-06,  ..., -7.4804e-06,
         -5.2899e-07, -4.4107e-06]], device='cuda:0')
Loss: 1.1817307472229004


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[    0,   266, 11692,  ...,   278, 14620, 12718],
        [    0,  3646,  1340,  ...,    13,  7800,  2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9986e-05, -7.2459e-05,  2.4264e-06,  ...,  4.3053e-05,
          1.1465e-05, -7.0366e-05],
        [-8.0168e-06, -5.3495e-06,  1.0766e-06,  ..., -6.5565e-06,
         -4.8336e-07, -3.8296e-06],
        [-6.3181e-06, -4.2319e-06,  8.4750e-07,  ..., -5.1707e-06,
         -3.8184e-07, -3.0249e-06],
        [-8.1658e-06, -5.4538e-06,  1.0990e-06,  ..., -6.6757e-06,
         -4.8894e-07, -3.9041e-06],
        [-1.8179e-05, -1.2159e-05,  2.4363e-06,  ..., -1.4871e-05,
         -1.0952e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.190208911895752


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[   0,  898,  266,  ...,   12, 3222, 8095],
        [   0, 2296,  446,  ..., 2937,  287, 2795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2705e-05, -1.1177e-04,  1.1024e-05,  ...,  6.3952e-05,
          2.4599e-06, -5.0363e-05],
        [-1.1951e-05, -7.9572e-06,  1.6578e-06,  ..., -9.7603e-06,
         -6.8452e-07, -5.7369e-06],
        [-9.4324e-06, -6.2883e-06,  1.3076e-06,  ..., -7.7039e-06,
         -5.4203e-07, -4.5300e-06],
        [-1.2219e-05, -8.1360e-06,  1.6987e-06,  ..., -9.9689e-06,
         -6.9663e-07, -5.8711e-06],
        [-2.7299e-05, -1.8179e-05,  3.7700e-06,  ..., -2.2262e-05,
         -1.5665e-06, -1.3083e-05]], device='cuda:0')
Loss: 1.200032114982605


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,   638,  1276,  ...,  1589,  2432,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8126e-06, -1.1708e-04,  2.8261e-05,  ...,  7.3450e-05,
          1.3050e-05, -5.0363e-05],
        [-1.5944e-05, -1.0625e-05,  2.2352e-06,  ..., -1.3024e-05,
         -8.5495e-07, -7.6592e-06],
        [-1.2562e-05, -8.3745e-06,  1.7621e-06,  ..., -1.0252e-05,
         -6.7428e-07, -6.0275e-06],
        [-1.6302e-05, -1.0863e-05,  2.2948e-06,  ..., -1.3307e-05,
         -8.6892e-07, -7.8380e-06],
        [-3.6418e-05, -2.4259e-05,  5.0887e-06,  ..., -2.9683e-05,
         -1.9576e-06, -1.7434e-05]], device='cuda:0')
Loss: 1.1955487728118896


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0,   278,   490,  ...,   434,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1367e-05, -1.4487e-04,  6.6157e-09,  ...,  1.2241e-04,
         -1.9722e-05, -1.0315e-05],
        [-1.9878e-05, -1.3232e-05,  2.7865e-06,  ..., -1.6317e-05,
         -1.1344e-06, -9.5665e-06],
        [-1.5646e-05, -1.0416e-05,  2.1961e-06,  ..., -1.2830e-05,
         -8.9314e-07, -7.5251e-06],
        [-2.0355e-05, -1.3545e-05,  2.8610e-06,  ..., -1.6689e-05,
         -1.1539e-06, -9.7901e-06],
        [-4.5419e-05, -3.0220e-05,  6.3404e-06,  ..., -3.7193e-05,
         -2.5984e-06, -2.1785e-05]], device='cuda:0')
Loss: 1.2013083696365356


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,  5281,  4452,  ...,    14,  3391,    12],
        [    0,  2386,  4012,  ...,   300, 15480,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2759e-05, -1.3390e-04, -1.1830e-06,  ...,  1.0576e-04,
         -5.1887e-05,  3.3793e-05],
        [-2.3901e-05, -1.5929e-05,  3.3975e-06,  ..., -1.9595e-05,
         -1.3970e-06, -1.1489e-05],
        [-1.8775e-05, -1.2532e-05,  2.6766e-06,  ..., -1.5393e-05,
         -1.0980e-06, -9.0301e-06],
        [-2.4438e-05, -1.6302e-05,  3.4906e-06,  ..., -2.0042e-05,
         -1.4221e-06, -1.1757e-05],
        [-5.4538e-05, -3.6359e-05,  7.7263e-06,  ..., -4.4644e-05,
         -3.1982e-06, -2.6137e-05]], device='cuda:0')
Loss: 1.1843031644821167


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[   0, 9466,   36,  ..., 1795,  437,  874],
        [   0, 1875, 2117,  ..., 1422, 1059,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5914e-05, -9.0039e-05, -2.4693e-05,  ...,  9.1458e-05,
         -1.4421e-05, -2.4271e-06],
        [-2.7835e-05, -1.8537e-05,  4.0010e-06,  ..., -2.2843e-05,
         -1.6224e-06, -1.3411e-05],
        [-2.1905e-05, -1.4618e-05,  3.1572e-06,  ..., -1.7986e-05,
         -1.2768e-06, -1.0565e-05],
        [-2.8491e-05, -1.8999e-05,  4.1164e-06,  ..., -2.3395e-05,
         -1.6531e-06, -1.3739e-05],
        [-6.3598e-05, -4.2379e-05,  9.1195e-06,  ..., -5.2124e-05,
         -3.7160e-06, -3.0547e-05]], device='cuda:0')
Loss: 1.1905269622802734


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,  3059,  2013,  ...,   278,  1997,    14],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4504e-06, -9.2342e-05, -2.1052e-05,  ...,  9.7836e-05,
         -2.0556e-05, -5.5315e-05],
        [-3.1859e-05, -2.1189e-05,  4.5933e-06,  ..., -2.6152e-05,
         -1.8775e-06, -1.5363e-05],
        [-2.5064e-05, -1.6719e-05,  3.6266e-06,  ..., -2.0593e-05,
         -1.4789e-06, -1.2107e-05],
        [-3.2604e-05, -2.1711e-05,  4.7274e-06,  ..., -2.6777e-05,
         -1.9120e-06, -1.5736e-05],
        [-7.2718e-05, -4.8399e-05,  1.0461e-05,  ..., -5.9634e-05,
         -4.2971e-06, -3.4988e-05]], device='cuda:0')
Loss: 1.1843678951263428
Graident accumulation at epoch 0, step 183, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0206],
        [ 0.0293, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0405,  ...,  0.0226,  0.0065, -0.0017],
        [-0.0169,  0.0142, -0.0269,  ...,  0.0278, -0.0160, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7706e-04, -8.3436e-05,  1.7926e-05,  ...,  1.7228e-05,
         -2.0556e-04, -1.0632e-04],
        [-2.5136e-05, -1.8277e-05,  3.6533e-06,  ..., -2.0441e-05,
         -2.1373e-06, -8.7797e-06],
        [ 2.6328e-05,  1.6600e-05,  5.9787e-07,  ...,  2.4176e-05,
          1.3135e-06,  8.3700e-06],
        [-2.8228e-05, -1.8573e-05,  5.8736e-06,  ..., -2.3400e-05,
         -3.1103e-06, -1.0491e-05],
        [-5.0991e-05, -3.4124e-05,  8.4943e-06,  ..., -4.0680e-05,
         -4.4667e-06, -1.8554e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2682e-08, 2.8489e-08, 3.5827e-08,  ..., 1.1767e-08, 8.9491e-08,
         1.1553e-08],
        [2.5584e-11, 1.3885e-11, 2.1078e-12,  ..., 1.5789e-11, 9.3514e-13,
         3.0276e-12],
        [1.2672e-10, 5.8542e-11, 2.1403e-12,  ..., 9.7576e-11, 2.6791e-12,
         2.3237e-11],
        [1.8370e-11, 8.6990e-12, 8.9946e-13,  ..., 1.2349e-11, 4.2214e-13,
         2.2174e-12],
        [9.7643e-11, 5.1776e-11, 5.0621e-12,  ..., 6.8946e-11, 2.6648e-12,
         1.0285e-11]], device='cuda:0')
optimizer state dict: 23.0
lr: [1.982537460481821e-05, 1.982537460481821e-05]
scheduler_last_epoch: 23


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[    0, 14652,    12,  ..., 17330,   996,  3294],
        [    0,   677,  8708,  ..., 19891,   267,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1030e-05,  7.9231e-06, -3.7509e-05,  ..., -3.9381e-06,
          3.0186e-06, -2.3857e-05],
        [-3.9339e-06, -2.6524e-06,  4.7125e-07,  ..., -3.2336e-06,
         -1.3970e-07, -1.9521e-06],
        [-3.1888e-06, -2.1458e-06,  3.8370e-07,  ..., -2.6226e-06,
         -1.1362e-07, -1.5870e-06],
        [-4.0531e-06, -2.7269e-06,  4.8801e-07,  ..., -3.3379e-06,
         -1.4249e-07, -2.0117e-06],
        [-9.1791e-06, -6.1691e-06,  1.0952e-06,  ..., -7.5698e-06,
         -3.2969e-07, -4.5598e-06]], device='cuda:0')
Loss: 1.1968201398849487


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1555,   12,  ...,  809,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8102e-05, -2.1987e-05, -8.1233e-05,  ...,  7.8537e-07,
          3.1475e-05, -1.8084e-05],
        [-7.8380e-06, -5.3495e-06,  9.3691e-07,  ..., -6.4969e-06,
         -3.3062e-07, -3.8743e-06],
        [-6.4075e-06, -4.3511e-06,  7.6555e-07,  ..., -5.3048e-06,
         -2.7101e-07, -3.1739e-06],
        [-8.0466e-06, -5.4687e-06,  9.6485e-07,  ..., -6.6608e-06,
         -3.3528e-07, -3.9786e-06],
        [-1.8358e-05, -1.2457e-05,  2.1830e-06,  ..., -1.5199e-05,
         -7.8045e-07, -9.0599e-06]], device='cuda:0')
Loss: 1.2194416522979736


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0, 43587,  1390,  ...,    12,   768,  1952],
        [    0,   266,  1634,  ...,   310,  1372,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7337e-05,  1.5651e-06, -7.5615e-05,  ...,  1.3384e-05,
          4.9433e-05, -3.9168e-05],
        [-1.1772e-05, -8.0019e-06,  1.3951e-06,  ..., -9.7454e-06,
         -5.2806e-07, -5.7966e-06],
        [-9.6262e-06, -6.5118e-06,  1.1399e-06,  ..., -7.9572e-06,
         -4.3306e-07, -4.7460e-06],
        [-1.2070e-05, -8.1807e-06,  1.4380e-06,  ..., -9.9838e-06,
         -5.3644e-07, -5.9605e-06],
        [-2.7537e-05, -1.8626e-05,  3.2485e-06,  ..., -2.2769e-05,
         -1.2405e-06, -1.3560e-05]], device='cuda:0')
Loss: 1.1863499879837036


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[  0,  12, 271,  ...,  12, 298, 273],
        [  0,  12, 287,  ...,  17, 271, 266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5785e-05,  8.0930e-06, -8.4281e-05,  ...,  2.8199e-05,
          3.0798e-05,  6.8264e-06],
        [-1.5706e-05, -1.0654e-05,  1.8571e-06,  ..., -1.2979e-05,
         -7.4413e-07, -7.7486e-06],
        [-1.2830e-05, -8.6725e-06,  1.5162e-06,  ..., -1.0580e-05,
         -6.0908e-07, -6.3330e-06],
        [-1.6093e-05, -1.0893e-05,  1.9129e-06,  ..., -1.3277e-05,
         -7.5437e-07, -7.9572e-06],
        [-3.6657e-05, -2.4766e-05,  4.3139e-06,  ..., -3.0220e-05,
         -1.7397e-06, -1.8060e-05]], device='cuda:0')
Loss: 1.1738216876983643


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9668e-05,  3.9203e-05, -8.9966e-05,  ...,  6.5489e-05,
          1.8456e-05, -2.3997e-05],
        [-1.9640e-05, -1.3307e-05,  2.3264e-06,  ..., -1.6198e-05,
         -9.2201e-07, -9.7305e-06],
        [-1.6034e-05, -1.0833e-05,  1.8980e-06,  ..., -1.3202e-05,
         -7.5437e-07, -7.9498e-06],
        [-2.0087e-05, -1.3590e-05,  2.3935e-06,  ..., -1.6555e-05,
         -9.3412e-07, -9.9838e-06],
        [-4.5776e-05, -3.0875e-05,  5.3942e-06,  ..., -3.7640e-05,
         -2.1551e-06, -2.2650e-05]], device='cuda:0')
Loss: 1.1948845386505127


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[   0,  635,   13,  ...,  292,   20,  445],
        [   0,  271, 8429,  ..., 9404,  963,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9440e-05,  2.2607e-05, -6.9873e-05,  ...,  9.3579e-05,
         -2.3874e-06, -4.1197e-05],
        [-2.3574e-05, -1.5944e-05,  2.7865e-06,  ..., -1.9431e-05,
         -1.0906e-06, -1.1697e-05],
        [-1.9222e-05, -1.2979e-05,  2.2724e-06,  ..., -1.5825e-05,
         -8.9128e-07, -9.5442e-06],
        [-2.4110e-05, -1.6287e-05,  2.8666e-06,  ..., -1.9848e-05,
         -1.1045e-06, -1.1995e-05],
        [-5.4955e-05, -3.7014e-05,  6.4597e-06,  ..., -4.5151e-05,
         -2.5518e-06, -2.7210e-05]], device='cuda:0')
Loss: 1.1847690343856812


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[   0, 1062,  648,  ...,  266, 4939,  278],
        [   0, 8840,   26,  ...,   28,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8245e-05,  3.8222e-05, -6.3637e-05,  ...,  1.2412e-04,
          1.2794e-05, -3.8544e-05],
        [-2.7448e-05, -1.8582e-05,  3.2354e-06,  ..., -2.2635e-05,
         -1.2908e-06, -1.3620e-05],
        [-2.2382e-05, -1.5140e-05,  2.6412e-06,  ..., -1.8448e-05,
         -1.0552e-06, -1.1124e-05],
        [-2.8074e-05, -1.8984e-05,  3.3285e-06,  ..., -2.3142e-05,
         -1.3085e-06, -1.3962e-05],
        [-6.4015e-05, -4.3213e-05,  7.5102e-06,  ..., -5.2691e-05,
         -3.0249e-06, -3.1739e-05]], device='cuda:0')
Loss: 1.1910382509231567


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,   266,  2511,  ...,  3220,  4164,  1173],
        [    0,    12,   496,  ..., 11354,  4856,  1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2685e-05,  5.2686e-05,  1.6256e-05,  ...,  1.0820e-04,
          5.8177e-06, -5.4097e-05],
        [-3.1352e-05, -2.1219e-05,  3.7197e-06,  ..., -2.5779e-05,
         -1.4724e-06, -1.5557e-05],
        [ 2.7138e-04,  2.1412e-04, -6.6107e-05,  ...,  2.5674e-04,
         -8.4032e-06,  1.3646e-04],
        [-3.2067e-05, -2.1696e-05,  3.8277e-06,  ..., -2.6360e-05,
         -1.4920e-06, -1.5959e-05],
        [-7.3195e-05, -4.9442e-05,  8.6427e-06,  ..., -6.0081e-05,
         -3.4533e-06, -3.6299e-05]], device='cuda:0')
Loss: 1.1866689920425415
Graident accumulation at epoch 0, step 191, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0048, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0337, -0.0094,  0.0404,  ...,  0.0226,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4209e-04, -6.9823e-05,  1.7759e-05,  ...,  2.6325e-05,
         -1.8443e-04, -1.0109e-04],
        [-2.5758e-05, -1.8571e-05,  3.6599e-06,  ..., -2.0975e-05,
         -2.0708e-06, -9.4574e-06],
        [ 5.0833e-05,  3.6352e-05, -6.0726e-06,  ...,  4.7432e-05,
          3.4184e-07,  2.1179e-05],
        [-2.8612e-05, -1.8885e-05,  5.6690e-06,  ..., -2.3696e-05,
         -2.9484e-06, -1.1038e-05],
        [-5.3211e-05, -3.5656e-05,  8.5091e-06,  ..., -4.2620e-05,
         -4.3654e-06, -2.0329e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2645e-08, 2.8463e-08, 3.5791e-08,  ..., 1.1767e-08, 8.9402e-08,
         1.1545e-08],
        [2.6541e-11, 1.4321e-11, 2.1195e-12,  ..., 1.6438e-11, 9.3637e-13,
         3.2666e-12],
        [2.0024e-10, 1.0433e-10, 6.5082e-12,  ..., 1.6340e-10, 2.7470e-12,
         4.1834e-11],
        [1.9379e-11, 9.1610e-12, 9.1321e-13,  ..., 1.3031e-11, 4.2394e-13,
         2.4698e-12],
        [1.0290e-10, 5.4169e-11, 5.1317e-12,  ..., 7.2487e-11, 2.6740e-12,
         1.1592e-11]], device='cuda:0')
optimizer state dict: 24.0
lr: [1.9801623778739208e-05, 1.9801623778739208e-05]
scheduler_last_epoch: 24


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  2346, 17886,  ...,   287,  6769,   806],
        [    0, 38717,  1679,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7704e-05, -6.6058e-05, -1.1911e-05,  ...,  3.0209e-05,
          2.4434e-05, -3.9143e-06],
        [-3.7998e-06, -2.5481e-06,  3.4459e-07,  ..., -3.1143e-06,
         -1.0990e-07, -1.9073e-06],
        [-3.2187e-06, -2.1607e-06,  2.9244e-07,  ..., -2.6375e-06,
         -9.1735e-08, -1.6168e-06],
        [-3.9637e-06, -2.6524e-06,  3.6135e-07,  ..., -3.2336e-06,
         -1.1083e-07, -1.9819e-06],
        [-9.0599e-06, -6.0499e-06,  8.1211e-07,  ..., -7.3910e-06,
         -2.5891e-07, -4.5300e-06]], device='cuda:0')
Loss: 1.202036738395691


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0, 1086,  292,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2437e-05, -7.3945e-05, -1.1627e-05,  ...,  4.8315e-05,
          8.7335e-06,  2.5960e-05],
        [-7.6741e-06, -5.1111e-06,  7.2084e-07,  ..., -6.2734e-06,
         -2.6636e-07, -3.8296e-06],
        [-6.5118e-06, -4.3362e-06,  6.1467e-07,  ..., -5.3346e-06,
         -2.2678e-07, -3.2559e-06],
        [-8.0168e-06, -5.3346e-06,  7.5810e-07,  ..., -6.5565e-06,
         -2.7567e-07, -4.0084e-06],
        [-1.8239e-05, -1.2130e-05,  1.7025e-06,  ..., -1.4901e-05,
         -6.3702e-07, -9.0897e-06]], device='cuda:0')
Loss: 1.1852984428405762


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  271,  669,   12],
        [   0, 2388, 6604,  ..., 5005, 1196,  717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3937e-05, -9.6195e-05, -9.0314e-06,  ...,  6.9516e-05,
          1.0340e-05,  4.1552e-05],
        [-1.1519e-05, -7.7039e-06,  1.0803e-06,  ..., -9.4175e-06,
         -4.0233e-07, -5.7518e-06],
        [-9.7603e-06, -6.5416e-06,  9.2015e-07,  ..., -8.0019e-06,
         -3.4366e-07, -4.8950e-06],
        [-1.1981e-05, -8.0317e-06,  1.1344e-06,  ..., -9.8199e-06,
         -4.1630e-07, -6.0052e-06],
        [-2.7299e-05, -1.8269e-05,  2.5481e-06,  ..., -2.2352e-05,
         -9.6112e-07, -1.3649e-05]], device='cuda:0')
Loss: 1.1857975721359253


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5259e-05, -1.1735e-04, -3.7697e-05,  ...,  8.7240e-05,
          2.1464e-05,  8.8932e-05],
        [-1.5363e-05, -1.0312e-05,  1.4678e-06,  ..., -1.2577e-05,
         -5.6438e-07, -7.6517e-06],
        [-1.3024e-05, -8.7619e-06,  1.2517e-06,  ..., -1.0699e-05,
         -4.8149e-07, -6.5193e-06],
        [-1.5974e-05, -1.0729e-05,  1.5404e-06,  ..., -1.3098e-05,
         -5.8115e-07, -7.9870e-06],
        [-3.6418e-05, -2.4468e-05,  3.4682e-06,  ..., -2.9862e-05,
         -1.3467e-06, -1.8179e-05]], device='cuda:0')
Loss: 1.2008891105651855


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  1550,   685,  ...,   943,  1239,   996],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9103e-05, -1.9714e-04, -5.7867e-05,  ...,  1.2122e-04,
          9.1686e-06,  1.3540e-04],
        [-1.9208e-05, -1.2904e-05,  1.8701e-06,  ..., -1.5706e-05,
         -7.1246e-07, -9.5293e-06],
        [-1.6332e-05, -1.0997e-05,  1.5963e-06,  ..., -1.3381e-05,
         -6.0908e-07, -8.1360e-06],
        [-1.9968e-05, -1.3426e-05,  1.9595e-06,  ..., -1.6332e-05,
         -7.3388e-07, -9.9391e-06],
        [-4.5478e-05, -3.0577e-05,  4.4107e-06,  ..., -3.7223e-05,
         -1.6987e-06, -2.2590e-05]], device='cuda:0')
Loss: 1.1852425336837769


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  696,  700,  328],
        [   0,  266, 1790,  ...,  292,   78,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0448e-04, -2.2653e-04, -4.1614e-05,  ...,  1.0743e-04,
          2.6558e-05,  1.5891e-04],
        [-2.3022e-05, -1.5542e-05,  2.2873e-06,  ..., -1.8865e-05,
         -8.6613e-07, -1.1481e-05],
        [-1.9580e-05, -1.3247e-05,  1.9539e-06,  ..., -1.6078e-05,
         -7.4226e-07, -9.7975e-06],
        [-2.3931e-05, -1.6168e-05,  2.3972e-06,  ..., -1.9625e-05,
         -8.9314e-07, -1.1966e-05],
        [-5.4419e-05, -3.6776e-05,  5.3868e-06,  ..., -4.4644e-05,
         -2.0638e-06, -2.7150e-05]], device='cuda:0')
Loss: 1.1976226568222046


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  1197, 10640,  ...,  2405,   437,  5880],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9289e-05, -2.5604e-04, -2.9416e-05,  ...,  1.2000e-04,
          1.4435e-05,  1.4699e-04],
        [-2.6822e-05, -1.8120e-05,  2.6822e-06,  ..., -2.1979e-05,
         -1.0263e-06, -1.3404e-05],
        [-2.2829e-05, -1.5453e-05,  2.2911e-06,  ..., -1.8731e-05,
         -8.7731e-07, -1.1437e-05],
        [-2.7925e-05, -1.8880e-05,  2.8126e-06,  ..., -2.2888e-05,
         -1.0589e-06, -1.3977e-05],
        [-6.3479e-05, -4.2915e-05,  6.3181e-06,  ..., -5.2035e-05,
         -2.4438e-06, -3.1710e-05]], device='cuda:0')
Loss: 1.178024411201477


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   706,  2680,   365],
        [    0,    33,    12,  ...,  1110,   467, 17467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8122e-05, -2.6668e-04, -5.1928e-05,  ...,  1.2738e-04,
          8.3208e-06,  1.8024e-04],
        [-3.0637e-05, -2.0683e-05,  3.0827e-06,  ..., -2.5123e-05,
         -1.2107e-06, -1.5341e-05],
        [-2.6077e-05, -1.7628e-05,  2.6338e-06,  ..., -2.1413e-05,
         -1.0338e-06, -1.3098e-05],
        [-3.1918e-05, -2.1547e-05,  3.2354e-06,  ..., -2.6181e-05,
         -1.2508e-06, -1.6004e-05],
        [-7.2479e-05, -4.8935e-05,  7.2606e-06,  ..., -5.9456e-05,
         -2.8796e-06, -3.6269e-05]], device='cuda:0')
Loss: 1.1606292724609375
Graident accumulation at epoch 0, step 199, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0038,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0031,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0226,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0188]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2769e-04, -8.9509e-05,  1.0790e-05,  ...,  3.6431e-05,
         -1.6515e-04, -7.2961e-05],
        [-2.6246e-05, -1.8782e-05,  3.6022e-06,  ..., -2.1390e-05,
         -1.9848e-06, -1.0046e-05],
        [ 4.3142e-05,  3.0954e-05, -5.2020e-06,  ...,  4.0548e-05,
          2.0428e-07,  1.7751e-05],
        [-2.8942e-05, -1.9151e-05,  5.4256e-06,  ..., -2.3945e-05,
         -2.7787e-06, -1.1535e-05],
        [-5.5138e-05, -3.6984e-05,  8.3843e-06,  ..., -4.4304e-05,
         -4.2168e-06, -2.1923e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2612e-08, 2.8506e-08, 3.5758e-08,  ..., 1.1771e-08, 8.9312e-08,
         1.1566e-08],
        [2.7453e-11, 1.4734e-11, 2.1269e-12,  ..., 1.7053e-11, 9.3690e-13,
         3.4986e-12],
        [2.0072e-10, 1.0454e-10, 6.5087e-12,  ..., 1.6369e-10, 2.7453e-12,
         4.1964e-11],
        [2.0379e-11, 9.6161e-12, 9.2277e-13,  ..., 1.3704e-11, 4.2508e-13,
         2.7235e-12],
        [1.0805e-10, 5.6509e-11, 5.1793e-12,  ..., 7.5949e-11, 2.6796e-12,
         1.2896e-11]], device='cuda:0')
optimizer state dict: 25.0
lr: [1.9776375178987234e-05, 1.9776375178987234e-05]
scheduler_last_epoch: 25


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,   12,  850, 4952],
        [   0, 4350,   14,  ...,  266, 9479,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2385e-06, -2.0429e-05, -3.8474e-05,  ...,  4.8768e-06,
          1.7557e-05,  1.5317e-05],
        [-3.7253e-06, -2.5183e-06,  3.3341e-07,  ..., -3.0845e-06,
         -1.4994e-07, -1.8552e-06],
        [-3.3081e-06, -2.2352e-06,  2.9616e-07,  ..., -2.7418e-06,
         -1.3318e-07, -1.6466e-06],
        [-3.9339e-06, -2.6524e-06,  3.5204e-07,  ..., -3.2485e-06,
         -1.5646e-07, -1.9521e-06],
        [-8.8811e-06, -6.0201e-06,  7.8976e-07,  ..., -7.3612e-06,
         -3.6322e-07, -4.4107e-06]], device='cuda:0')
Loss: 1.163453221321106


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[   0, 6067, 1188,  ..., 5282,  756,  342],
        [   0,  328,  266,  ...,   14, 3352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4235e-05, -5.0084e-05, -6.7153e-05,  ...,  2.0024e-05,
          2.7950e-05,  1.2952e-05],
        [-7.4804e-06, -5.0664e-06,  6.7614e-07,  ..., -6.1542e-06,
         -3.0827e-07, -3.6955e-06],
        [-6.6459e-06, -4.5002e-06,  6.0163e-07,  ..., -5.4538e-06,
         -2.7288e-07, -3.2783e-06],
        [-7.8678e-06, -5.3197e-06,  7.1339e-07,  ..., -6.4671e-06,
         -3.2224e-07, -3.8892e-06],
        [-1.7881e-05, -1.2100e-05,  1.6019e-06,  ..., -1.4663e-05,
         -7.4133e-07, -8.7917e-06]], device='cuda:0')
Loss: 1.184167742729187


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0, 39224,    34,  ...,   401,  1716,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2335e-05, -6.3071e-05, -3.6533e-05,  ...,  4.8004e-05,
         -1.4647e-06,  1.2391e-05],
        [-1.1265e-05, -7.6145e-06,  9.9279e-07,  ..., -9.2387e-06,
         -4.3958e-07, -5.5581e-06],
        [-9.9689e-06, -6.7502e-06,  8.7917e-07,  ..., -8.1509e-06,
         -3.8790e-07, -4.9174e-06],
        [-1.1832e-05, -7.9870e-06,  1.0468e-06,  ..., -9.6858e-06,
         -4.5728e-07, -5.8413e-06],
        [-2.6941e-05, -1.8239e-05,  2.3581e-06,  ..., -2.2054e-05,
         -1.0598e-06, -1.3262e-05]], device='cuda:0')
Loss: 1.1733125448226929


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   266, 27347,  ...,   368,  3367,    13],
        [    0, 11661,    12,  ...,  1707,   394,   264]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8685e-05, -9.6566e-05, -8.3764e-05,  ...,  5.9988e-05,
          3.9773e-06,  2.9780e-05],
        [ 8.0651e-05,  6.4099e-05, -5.4301e-06,  ...,  5.7929e-05,
          1.5663e-05,  2.4592e-05],
        [-1.3262e-05, -8.9854e-06,  1.1697e-06,  ..., -1.0863e-05,
         -5.1549e-07, -6.5789e-06],
        [-1.5765e-05, -1.0639e-05,  1.3951e-06,  ..., -1.2919e-05,
         -6.0815e-07, -7.8231e-06],
        [-3.5822e-05, -2.4229e-05,  3.1330e-06,  ..., -2.9325e-05,
         -1.4026e-06, -1.7703e-05]], device='cuda:0')
Loss: 1.176944375038147


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1976,  1329,  ...,   278,  9469,   292],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9801e-05, -1.0460e-04, -1.2527e-04,  ...,  6.8580e-05,
          2.3087e-05,  3.2160e-05],
        [ 7.6866e-05,  6.1580e-05, -5.0818e-06,  ...,  5.4874e-05,
          1.5527e-05,  2.2714e-05],
        [-1.6615e-05, -1.1221e-05,  1.4789e-06,  ..., -1.3575e-05,
         -6.3563e-07, -8.2478e-06],
        [-1.9729e-05, -1.3292e-05,  1.7639e-06,  ..., -1.6123e-05,
         -7.5065e-07, -9.7901e-06],
        [-4.4823e-05, -3.0249e-05,  3.9600e-06,  ..., -3.6597e-05,
         -1.7304e-06, -2.2173e-05]], device='cuda:0')
Loss: 1.1870214939117432


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0, 1358,  367,  ..., 1758, 2921,   12],
        [   0,  278,  554,  ...,  365, 3125,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5774e-05, -1.0649e-04, -1.1463e-04,  ...,  8.8877e-05,
          3.9539e-05,  4.1324e-05],
        [ 7.3096e-05,  5.9062e-05, -4.7726e-06,  ...,  5.1790e-05,
          1.5414e-05,  2.0844e-05],
        [-1.9923e-05, -1.3426e-05,  1.7509e-06,  ..., -1.6287e-05,
         -7.3574e-07, -9.8944e-06],
        [-2.3633e-05, -1.5914e-05,  2.0880e-06,  ..., -1.9342e-05,
         -8.6706e-07, -1.1742e-05],
        [-5.3823e-05, -3.6240e-05,  4.6976e-06,  ..., -4.3958e-05,
         -2.0005e-06, -2.6643e-05]], device='cuda:0')
Loss: 1.1999824047088623


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0, 4154,   12,  ...,   14,  560,  199],
        [   0,  554, 1034,  ..., 3313,  365,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8940e-05, -1.0296e-04, -1.0521e-04,  ...,  1.1740e-04,
          6.3249e-05,  1.8198e-05],
        [ 6.9326e-05,  5.6514e-05, -4.4262e-06,  ...,  4.8690e-05,
          1.5310e-05,  1.8989e-05],
        [-2.3216e-05, -1.5661e-05,  2.0545e-06,  ..., -1.8999e-05,
         -8.2608e-07, -1.1519e-05],
        [-2.7537e-05, -1.8567e-05,  2.4512e-06,  ..., -2.2560e-05,
         -9.7370e-07, -1.3664e-05],
        [-6.2823e-05, -4.2319e-05,  5.5209e-06,  ..., -5.1349e-05,
         -2.2501e-06, -3.1054e-05]], device='cuda:0')
Loss: 1.188599705696106


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[    0,  3408,   300,  ...,    14,  5870,    12],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1017e-05, -1.2674e-04, -1.4589e-04,  ...,  1.0669e-04,
          6.2859e-05,  7.0057e-06],
        [ 6.5571e-05,  5.3966e-05, -4.1095e-06,  ...,  4.5621e-05,
          1.5169e-05,  1.7119e-05],
        [-2.6524e-05, -1.7911e-05,  2.3358e-06,  ..., -2.1711e-05,
         -9.4902e-07, -1.3173e-05],
        [-3.1471e-05, -2.1249e-05,  2.7884e-06,  ..., -2.5794e-05,
         -1.1199e-06, -1.5631e-05],
        [-7.1764e-05, -4.8399e-05,  6.2771e-06,  ..., -5.8681e-05,
         -2.5835e-06, -3.5524e-05]], device='cuda:0')
Loss: 1.185383677482605
Graident accumulation at epoch 0, step 207, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0169,  0.0143, -0.0269,  ...,  0.0278, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1402e-04, -9.3232e-05, -4.8776e-06,  ...,  4.3457e-05,
         -1.4235e-04, -6.4965e-05],
        [-1.7064e-05, -1.1507e-05,  2.8310e-06,  ..., -1.4689e-05,
         -2.6941e-07, -7.3293e-06],
        [ 3.6176e-05,  2.6067e-05, -4.4482e-06,  ...,  3.4322e-05,
          8.8947e-08,  1.4659e-05],
        [-2.9195e-05, -1.9361e-05,  5.1619e-06,  ..., -2.4130e-05,
         -2.6128e-06, -1.1944e-05],
        [-5.6800e-05, -3.8126e-05,  8.1735e-06,  ..., -4.5741e-05,
         -4.0535e-06, -2.3283e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2577e-08, 2.8494e-08, 3.5744e-08,  ..., 1.1771e-08, 8.9227e-08,
         1.1554e-08],
        [3.1725e-11, 1.7632e-11, 2.1417e-12,  ..., 1.9117e-11, 1.1661e-12,
         3.7882e-12],
        [2.0122e-10, 1.0475e-10, 6.5076e-12,  ..., 1.6400e-10, 2.7435e-12,
         4.2095e-11],
        [2.1349e-11, 1.0058e-11, 9.2962e-13,  ..., 1.4355e-11, 4.2591e-13,
         2.9651e-12],
        [1.1310e-10, 5.8795e-11, 5.2136e-12,  ..., 7.9317e-11, 2.6836e-12,
         1.4145e-11]], device='cuda:0')
optimizer state dict: 26.0
lr: [1.974963266376872e-05, 1.974963266376872e-05]
scheduler_last_epoch: 26


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 33792,   352,  ...,   278,   546, 30495],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3176e-05, -3.4477e-05,  2.0018e-06,  ...,  3.3099e-06,
         -1.8806e-05, -3.3498e-05],
        [-3.6806e-06, -2.5034e-06,  3.1479e-07,  ..., -2.9951e-06,
         -9.4995e-08, -1.7956e-06],
        [-3.3081e-06, -2.2501e-06,  2.8312e-07,  ..., -2.6822e-06,
         -8.5216e-08, -1.6093e-06],
        [-3.9041e-06, -2.6524e-06,  3.3528e-07,  ..., -3.1590e-06,
         -9.9186e-08, -1.8999e-06],
        [-8.7619e-06, -5.9605e-06,  7.4878e-07,  ..., -7.1228e-06,
         -2.2817e-07, -4.2617e-06]], device='cuda:0')
Loss: 1.1751855611801147


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,    22,  2577,  ...,  4970,     9,  3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1131e-06, -3.7600e-05, -1.8727e-05,  ...,  2.0205e-05,
          8.6319e-06, -3.0733e-05],
        [-7.3612e-06, -5.0068e-06,  6.3702e-07,  ..., -6.0350e-06,
         -2.1793e-07, -3.6433e-06],
        [-6.6459e-06, -4.5300e-06,  5.7742e-07,  ..., -5.4538e-06,
         -1.9651e-07, -3.2857e-06],
        [-7.7784e-06, -5.2899e-06,  6.7800e-07,  ..., -6.3628e-06,
         -2.2771e-07, -3.8370e-06],
        [-1.7524e-05, -1.1891e-05,  1.5087e-06,  ..., -1.4335e-05,
         -5.2061e-07, -8.6427e-06]], device='cuda:0')
Loss: 1.188888430595398


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   409, 15720,  ...,    12,   287,  2350],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6167e-05, -5.9100e-05, -4.2560e-05,  ...,  5.0853e-06,
         -2.6821e-06, -2.1530e-05],
        [-1.1042e-05, -7.5251e-06,  9.7416e-07,  ..., -9.0748e-06,
         -3.9209e-07, -5.5134e-06],
        [-9.9987e-06, -6.8247e-06,  8.8476e-07,  ..., -8.2254e-06,
         -3.5577e-07, -4.9919e-06],
        [-1.1653e-05, -7.9572e-06,  1.0375e-06,  ..., -9.5665e-06,
         -4.1118e-07, -5.8040e-06],
        [-2.6345e-05, -1.7941e-05,  2.3171e-06,  ..., -2.1636e-05,
         -9.4343e-07, -1.3113e-05]], device='cuda:0')
Loss: 1.1995048522949219


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,   944,   278,  ..., 17330,  1683,   360],
        [    0,    13,  7805,  ...,  2733,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1959e-05, -5.0089e-05, -3.9173e-05,  ...,  2.0360e-05,
          1.2598e-05, -4.2802e-06],
        [-1.4663e-05, -1.0028e-05,  1.2778e-06,  ..., -1.2055e-05,
         -5.1502e-07, -7.3314e-06],
        [-1.3322e-05, -9.1344e-06,  1.1642e-06,  ..., -1.0952e-05,
         -4.6939e-07, -6.6608e-06],
        [-1.5497e-05, -1.0639e-05,  1.3635e-06,  ..., -1.2740e-05,
         -5.4343e-07, -7.7412e-06],
        [-3.5048e-05, -2.3991e-05,  3.0436e-06,  ..., -2.8789e-05,
         -1.2415e-06, -1.7494e-05]], device='cuda:0')
Loss: 1.1817182302474976


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[    0,   344, 14017,  ...,    65,   298,   634],
        [    0,  5699,    20,  ...,  3502,  2051,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2393e-07, -7.1213e-05, -5.1243e-05,  ..., -1.2700e-05,
          2.0652e-05, -3.1057e-05],
        [-1.8373e-05, -1.2532e-05,  1.5758e-06,  ..., -1.5095e-05,
         -6.3982e-07, -9.1419e-06],
        [-1.6659e-05, -1.1384e-05,  1.4324e-06,  ..., -1.3694e-05,
         -5.8208e-07, -8.2925e-06],
        [-1.9372e-05, -1.3262e-05,  1.6782e-06,  ..., -1.5929e-05,
         -6.7381e-07, -9.6411e-06],
        [-4.3809e-05, -2.9922e-05,  3.7439e-06,  ..., -3.6001e-05,
         -1.5395e-06, -2.1785e-05]], device='cuda:0')
Loss: 1.1958709955215454


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   685,  2461,  ...,   287,   298,  7943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5287e-05, -1.0474e-04, -7.4867e-05,  ..., -2.6377e-06,
          2.3574e-05, -4.0249e-05],
        [-2.2024e-05, -1.5020e-05,  1.8924e-06,  ..., -1.8135e-05,
         -7.8604e-07, -1.0945e-05],
        [-1.9968e-05, -1.3635e-05,  1.7211e-06,  ..., -1.6466e-05,
         -7.1526e-07, -9.9316e-06],
        [-2.3186e-05, -1.5870e-05,  2.0117e-06,  ..., -1.9118e-05,
         -8.2562e-07, -1.1526e-05],
        [-5.2512e-05, -3.5852e-05,  4.4964e-06,  ..., -4.3273e-05,
         -1.8897e-06, -2.6077e-05]], device='cuda:0')
Loss: 1.1927465200424194


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[   0,  270,  472,  ...,  292,   73,   14],
        [   0,  265, 1781,  ...,  334,  344,  984]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8710e-05, -6.3793e-05, -8.3378e-05,  ...,  2.8301e-05,
         -3.7609e-06, -5.3491e-05],
        [-2.5660e-05, -1.7524e-05,  2.2184e-06,  ..., -2.1204e-05,
         -9.5088e-07, -1.2778e-05],
        [-2.3246e-05, -1.5885e-05,  2.0154e-06,  ..., -1.9222e-05,
         -8.6427e-07, -1.1586e-05],
        [-2.6986e-05, -1.8477e-05,  2.3544e-06,  ..., -2.2307e-05,
         -9.9698e-07, -1.3433e-05],
        [-6.1274e-05, -4.1842e-05,  5.2787e-06,  ..., -5.0634e-05,
         -2.2883e-06, -3.0488e-05]], device='cuda:0')
Loss: 1.192609190940857


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,   12,  221,  ...,  593,  360,  726],
        [   0,   14,  333,  ...,  328, 5453, 4713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6799e-05, -6.8646e-05, -9.0644e-05,  ...,  3.8184e-05,
          7.2399e-06, -3.4314e-05],
        [-2.9311e-05, -2.0027e-05,  2.5202e-06,  ..., -2.4185e-05,
         -1.0980e-06, -1.4596e-05],
        [-2.6584e-05, -1.8165e-05,  2.2929e-06,  ..., -2.1949e-05,
         -9.9838e-07, -1.3247e-05],
        [-3.0860e-05, -2.1130e-05,  2.6785e-06,  ..., -2.5466e-05,
         -1.1516e-06, -1.5356e-05],
        [-7.0035e-05, -4.7833e-05,  6.0052e-06,  ..., -5.7787e-05,
         -2.6440e-06, -3.4839e-05]], device='cuda:0')
Loss: 1.1863031387329102
Graident accumulation at epoch 0, step 215, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.9730e-04, -9.0773e-05, -1.3454e-05,  ...,  4.2929e-05,
         -1.2739e-04, -6.1900e-05],
        [-1.8289e-05, -1.2359e-05,  2.7999e-06,  ..., -1.5638e-05,
         -3.5227e-07, -8.0559e-06],
        [ 2.9900e-05,  2.1644e-05, -3.7741e-06,  ...,  2.8695e-05,
         -1.9785e-08,  1.1868e-05],
        [-2.9362e-05, -1.9538e-05,  4.9136e-06,  ..., -2.4263e-05,
         -2.4667e-06, -1.2285e-05],
        [-5.8124e-05, -3.9096e-05,  7.9567e-06,  ..., -4.6946e-05,
         -3.9125e-06, -2.4439e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2537e-08, 2.8470e-08, 3.5716e-08,  ..., 1.1760e-08, 8.9138e-08,
         1.1544e-08],
        [3.2553e-11, 1.8015e-11, 2.1459e-12,  ..., 1.9683e-11, 1.1661e-12,
         3.9975e-12],
        [2.0173e-10, 1.0498e-10, 6.5064e-12,  ..., 1.6432e-10, 2.7418e-12,
         4.2229e-11],
        [2.2280e-11, 1.0494e-11, 9.3586e-13,  ..., 1.4990e-11, 4.2681e-13,
         3.1979e-12],
        [1.1789e-10, 6.1025e-11, 5.2444e-12,  ..., 8.2577e-11, 2.6879e-12,
         1.5345e-11]], device='cuda:0')
optimizer state dict: 27.0
lr: [1.972140031957344e-05, 1.972140031957344e-05]
scheduler_last_epoch: 27


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[    0,  1690,  2558,  ...,  2025,    12,   266],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4867e-05, -3.2973e-05, -2.0042e-05,  ..., -8.4288e-06,
          3.1569e-05, -3.9779e-06],
        [-3.6061e-06, -2.4587e-06,  3.0734e-07,  ..., -2.9653e-06,
         -1.5553e-07, -1.7956e-06],
        [-3.3528e-06, -2.2948e-06,  2.8498e-07,  ..., -2.7567e-06,
         -1.4529e-07, -1.6689e-06],
        [-3.7998e-06, -2.5928e-06,  3.2410e-07,  ..., -3.1143e-06,
         -1.6298e-07, -1.8850e-06],
        [-8.7023e-06, -5.9307e-06,  7.3388e-07,  ..., -7.1228e-06,
         -3.7812e-07, -4.3213e-06]], device='cuda:0')
Loss: 1.1919078826904297


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[    0,   709,   630,  ...,  6263,   409,   508],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4377e-05, -3.9679e-05,  3.2808e-06,  ..., -3.5445e-06,
          2.2875e-05, -4.6812e-07],
        [-7.1973e-06, -4.9323e-06,  6.4261e-07,  ..., -5.9158e-06,
         -3.1013e-07, -3.5539e-06],
        [-6.6906e-06, -4.6045e-06,  5.9605e-07,  ..., -5.4836e-06,
         -2.8871e-07, -3.3006e-06],
        [-7.5996e-06, -5.2154e-06,  6.8173e-07,  ..., -6.2287e-06,
         -3.2689e-07, -3.7476e-06],
        [-1.7226e-05, -1.1832e-05,  1.5236e-06,  ..., -1.4126e-05,
         -7.4692e-07, -8.4937e-06]], device='cuda:0')
Loss: 1.1728084087371826


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   292,    58,  ...,   319,   221,  1061],
        [    0,   344, 10706,  ...,  1184,   578,   825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2920e-05, -2.5643e-05,  1.0683e-05,  ...,  3.1693e-05,
          3.7728e-06,  4.1526e-06],
        [-1.0803e-05, -7.4059e-06,  1.0058e-06,  ..., -8.9109e-06,
         -4.8801e-07, -5.3272e-06],
        [-1.0028e-05, -6.8843e-06,  9.3319e-07,  ..., -8.2552e-06,
         -4.5449e-07, -4.9397e-06],
        [-1.1414e-05, -7.8231e-06,  1.0692e-06,  ..., -9.3877e-06,
         -5.1316e-07, -5.6177e-06],
        [-2.5749e-05, -1.7673e-05,  2.3805e-06,  ..., -2.1219e-05,
         -1.1660e-06, -1.2696e-05]], device='cuda:0')
Loss: 1.1713662147521973


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[   0, 6541,  287,  ..., 1061, 4786,  292],
        [   0,  285,  590,  ...,  199,  395, 3523]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2908e-06,  1.0605e-05,  1.9633e-05,  ...,  3.4473e-05,
          3.8620e-05,  4.1725e-05],
        [-1.4409e-05, -9.8944e-06,  1.3392e-06,  ..., -1.1906e-05,
         -6.9477e-07, -7.1079e-06],
        [-1.3351e-05, -9.1791e-06,  1.2424e-06,  ..., -1.1012e-05,
         -6.4448e-07, -6.5863e-06],
        [-1.5199e-05, -1.0431e-05,  1.4212e-06,  ..., -1.2532e-05,
         -7.2829e-07, -7.4878e-06],
        [-3.4213e-05, -2.3514e-05,  3.1590e-06,  ..., -2.8253e-05,
         -1.6503e-06, -1.6868e-05]], device='cuda:0')
Loss: 1.162642002105713


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2537e-05,  6.7227e-07,  4.0687e-06,  ...,  2.1371e-05,
          1.0812e-05,  3.2933e-05],
        [-1.8016e-05, -1.2368e-05,  1.6969e-06,  ..., -1.4901e-05,
         -8.6054e-07, -8.8960e-06],
        [-1.6659e-05, -1.1444e-05,  1.5702e-06,  ..., -1.3769e-05,
         -7.9628e-07, -8.2254e-06],
        [-1.8969e-05, -1.3009e-05,  1.7956e-06,  ..., -1.5676e-05,
         -8.9966e-07, -9.3505e-06],
        [-4.2737e-05, -2.9355e-05,  3.9972e-06,  ..., -3.5346e-05,
         -2.0415e-06, -2.1070e-05]], device='cuda:0')
Loss: 1.1739205121994019


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0,    83,    12,  ...,  3781,   292, 27247]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1382e-05, -2.4112e-06,  2.1762e-05,  ...,  2.5780e-05,
          2.5016e-05,  4.2275e-05],
        [-2.1696e-05, -1.4856e-05,  2.0266e-06,  ..., -1.7911e-05,
         -1.0710e-06, -1.0736e-05],
        [-2.0057e-05, -1.3754e-05,  1.8775e-06,  ..., -1.6570e-05,
         -9.9093e-07, -9.9242e-06],
        [-2.2843e-05, -1.5631e-05,  2.1458e-06,  ..., -1.8850e-05,
         -1.1204e-06, -1.1288e-05],
        [-5.1558e-05, -3.5316e-05,  4.7870e-06,  ..., -4.2588e-05,
         -2.5481e-06, -2.5481e-05]], device='cuda:0')
Loss: 1.1923210620880127


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[    0,   298,   369,  ...,  5936,   968,   259],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5031e-05, -8.6540e-07, -9.3530e-06,  ...,  4.1999e-05,
          3.5082e-05,  3.8496e-06],
        [-2.5272e-05, -1.7330e-05,  2.3432e-06,  ..., -2.0906e-05,
         -1.2349e-06, -1.2517e-05],
        [-2.3335e-05, -1.6019e-05,  2.1700e-06,  ..., -1.9312e-05,
         -1.1427e-06, -1.1563e-05],
        [-2.6613e-05, -1.8239e-05,  2.4829e-06,  ..., -2.2009e-05,
         -1.2927e-06, -1.3165e-05],
        [-6.0022e-05, -4.1157e-05,  5.5321e-06,  ..., -4.9651e-05,
         -2.9374e-06, -2.9683e-05]], device='cuda:0')
Loss: 1.1857447624206543


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 1276,  292,  ...,   83, 1837,   13],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7551e-05,  6.3443e-07, -3.9632e-05,  ...,  7.4279e-05,
          6.9536e-05,  8.7938e-06],
        [-2.8908e-05, -1.9774e-05,  2.6785e-06,  ..., -2.3842e-05,
         -1.3569e-06, -1.4335e-05],
        [-2.6673e-05, -1.8269e-05,  2.4773e-06,  ..., -2.2009e-05,
         -1.2536e-06, -1.3225e-05],
        [-3.0518e-05, -2.0847e-05,  2.8424e-06,  ..., -2.5153e-05,
         -1.4193e-06, -1.5102e-05],
        [-6.8724e-05, -4.6998e-05,  6.3255e-06,  ..., -5.6654e-05,
         -3.2261e-06, -3.4004e-05]], device='cuda:0')
Loss: 1.1804741621017456
Graident accumulation at epoch 0, step 223, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0107, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0033,  0.0222, -0.0205],
        [ 0.0292, -0.0075,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0404,  ...,  0.0227,  0.0066, -0.0016],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.7582e-04, -8.1632e-05, -1.6072e-05,  ...,  4.6064e-05,
         -1.0770e-04, -5.4830e-05],
        [-1.9351e-05, -1.3101e-05,  2.7878e-06,  ..., -1.6459e-05,
         -4.5274e-07, -8.6838e-06],
        [ 2.4242e-05,  1.7653e-05, -3.1489e-06,  ...,  2.3624e-05,
         -1.4316e-07,  9.3588e-06],
        [-2.9477e-05, -1.9669e-05,  4.7065e-06,  ..., -2.4352e-05,
         -2.3619e-06, -1.2567e-05],
        [-5.9184e-05, -3.9887e-05,  7.7936e-06,  ..., -4.7917e-05,
         -3.8439e-06, -2.5395e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2495e-08, 2.8441e-08, 3.5682e-08,  ..., 1.1754e-08, 8.9053e-08,
         1.1532e-08],
        [3.3356e-11, 1.8388e-11, 2.1509e-12,  ..., 2.0231e-11, 1.1668e-12,
         4.1989e-12],
        [2.0224e-10, 1.0521e-10, 6.5060e-12,  ..., 1.6464e-10, 2.7406e-12,
         4.2361e-11],
        [2.3189e-11, 1.0919e-11, 9.4301e-13,  ..., 1.5607e-11, 4.2840e-13,
         3.4228e-12],
        [1.2249e-10, 6.3172e-11, 5.2792e-12,  ..., 8.5704e-11, 2.6957e-12,
         1.6486e-11]], device='cuda:0')
optimizer state dict: 28.0
lr: [1.9691682460550022e-05, 1.9691682460550022e-05]
scheduler_last_epoch: 28


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0,   199,   769,  ..., 12038, 15317,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1211e-05,  9.9412e-06,  2.3962e-06,  ..., -4.6237e-05,
          2.2088e-05, -2.7460e-05],
        [-3.5465e-06, -2.4438e-06,  3.2224e-07,  ..., -2.9206e-06,
         -1.9465e-07, -1.7211e-06],
        [-3.3230e-06, -2.2948e-06,  3.0175e-07,  ..., -2.7418e-06,
         -1.8254e-07, -1.6168e-06],
        [-3.7402e-06, -2.5779e-06,  3.4086e-07,  ..., -3.0845e-06,
         -2.0396e-07, -1.8179e-06],
        [-8.4043e-06, -5.7817e-06,  7.5623e-07,  ..., -6.9141e-06,
         -4.6194e-07, -4.0829e-06]], device='cuda:0')
Loss: 1.1967830657958984


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[    0,  1128,  3231,  ...,  8375,   199,  2038],
        [    0, 27342,    17,  ...,  5125,  3244,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3523e-07,  3.3294e-05,  3.9901e-06,  ..., -4.3986e-05,
          1.2250e-05, -2.1646e-05],
        [-7.0781e-06, -4.8727e-06,  6.5006e-07,  ..., -5.8264e-06,
         -4.2003e-07, -3.4794e-06],
        [-6.6608e-06, -4.5747e-06,  6.1095e-07,  ..., -5.4836e-06,
         -3.9395e-07, -3.2708e-06],
        [-7.4506e-06, -5.1260e-06,  6.8918e-07,  ..., -6.1393e-06,
         -4.3865e-07, -3.6582e-06],
        [-1.6809e-05, -1.1533e-05,  1.5311e-06,  ..., -1.3828e-05,
         -9.9465e-07, -8.2552e-06]], device='cuda:0')
Loss: 1.1821467876434326


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[   0,  365, 5392,  ...,   14,  333,  199],
        [   0,  271,  266,  ...,  984,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2394e-05,  3.5160e-05,  3.5089e-05,  ..., -3.4414e-05,
         -6.1157e-06, -5.8847e-05],
        [-1.0639e-05, -7.3016e-06,  9.7975e-07,  ..., -8.7172e-06,
         -6.3051e-07, -5.2229e-06],
        [-1.0028e-05, -6.8694e-06,  9.2387e-07,  ..., -8.2105e-06,
         -5.9418e-07, -4.9174e-06],
        [-1.1235e-05, -7.7039e-06,  1.0412e-06,  ..., -9.2089e-06,
         -6.6031e-07, -5.5060e-06],
        [-2.5332e-05, -1.7315e-05,  2.3209e-06,  ..., -2.0713e-05,
         -1.4976e-06, -1.2398e-05]], device='cuda:0')
Loss: 1.1746701002120972


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[   0, 6294,  367,  ...,  496,   14,   18],
        [   0,  417,  199,  ...,   13,   20, 6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1440e-05,  3.5976e-05,  6.4697e-05,  ..., -5.5658e-05,
          1.5405e-05, -5.4207e-05],
        [-1.4260e-05, -9.7752e-06,  1.3039e-06,  ..., -1.1683e-05,
         -8.5589e-07, -6.9663e-06],
        [-1.3411e-05, -9.1791e-06,  1.2293e-06,  ..., -1.0982e-05,
         -8.0466e-07, -6.5565e-06],
        [-1.4991e-05, -1.0267e-05,  1.3802e-06,  ..., -1.2293e-05,
         -8.9221e-07, -7.3239e-06],
        [-3.3855e-05, -2.3127e-05,  3.0771e-06,  ..., -2.7686e-05,
         -2.0266e-06, -1.6481e-05]], device='cuda:0')
Loss: 1.1839121580123901


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  2958,   298,  ...,    12,   709,   616],
        [    0, 48705,   292,  ...,   266,  2548,  2697]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3331e-05,  4.1046e-05,  4.5671e-05,  ..., -6.2395e-05,
         -2.6471e-06, -2.8283e-05],
        [-1.7837e-05, -1.2204e-05,  1.6876e-06,  ..., -1.4603e-05,
         -1.0831e-06, -8.7693e-06],
        [-1.6794e-05, -1.1489e-05,  1.5944e-06,  ..., -1.3754e-05,
         -1.0198e-06, -8.2627e-06],
        [-1.8731e-05, -1.2800e-05,  1.7844e-06,  ..., -1.5348e-05,
         -1.1288e-06, -9.2089e-06],
        [-4.2319e-05, -2.8849e-05,  3.9749e-06,  ..., -3.4571e-05,
         -2.5630e-06, -2.0713e-05]], device='cuda:0')
Loss: 1.152995228767395


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,   381,  1795,  ...,    12,   344,   593]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0612e-05,  5.5934e-05,  4.3232e-05,  ..., -7.1995e-05,
         -4.2035e-06, -1.5261e-05],
        [-2.1428e-05, -1.4648e-05,  1.9968e-06,  ..., -1.7539e-05,
         -1.3141e-06, -1.0498e-05],
        [-2.0191e-05, -1.3798e-05,  1.8869e-06,  ..., -1.6540e-05,
         -1.2368e-06, -9.8944e-06],
        [-2.2501e-05, -1.5378e-05,  2.1122e-06,  ..., -1.8448e-05,
         -1.3690e-06, -1.1027e-05],
        [-5.0843e-05, -3.4660e-05,  4.7088e-06,  ..., -4.1574e-05,
         -3.1143e-06, -2.4825e-05]], device='cuda:0')
Loss: 1.1886335611343384


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,   471,   590,  ...,  5007,    13,  2920],
        [    0, 47684,   292,  ...,   287, 49958, 22022]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7080e-05,  5.5934e-05,  8.2224e-05,  ..., -5.1440e-05,
          2.2507e-05,  8.3935e-06],
        [-2.4989e-05, -1.7092e-05,  2.3469e-06,  ..., -2.0429e-05,
         -1.5749e-06, -1.2241e-05],
        [-2.3574e-05, -1.6123e-05,  2.2203e-06,  ..., -1.9282e-05,
         -1.4827e-06, -1.1548e-05],
        [-2.6256e-05, -1.7956e-05,  2.4848e-06,  ..., -2.1487e-05,
         -1.6429e-06, -1.2860e-05],
        [-5.9247e-05, -4.0442e-05,  5.5321e-06,  ..., -4.8399e-05,
         -3.7290e-06, -2.8938e-05]], device='cuda:0')
Loss: 1.1638119220733643


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[   0,   12,  630,  ..., 5049,   14, 2371],
        [   0,   18,   14,  ...,  300,  275, 1184]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0166e-05,  3.5163e-05,  1.0167e-04,  ..., -4.4165e-05,
          5.7614e-05,  4.1058e-05],
        [-2.8580e-05, -1.9595e-05,  2.7008e-06,  ..., -2.3350e-05,
         -1.8151e-06, -1.4007e-05],
        [ 4.5532e-05,  5.2749e-05,  5.9698e-06,  ...,  5.0755e-05,
          1.7130e-05,  4.4550e-05],
        [-3.0041e-05, -2.0593e-05,  2.8592e-06,  ..., -2.4572e-05,
         -1.8943e-06, -1.4722e-05],
        [-6.7830e-05, -4.6402e-05,  6.3702e-06,  ..., -5.5373e-05,
         -4.3064e-06, -3.3140e-05]], device='cuda:0')
Loss: 1.1863635778427124
Graident accumulation at epoch 0, step 231, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0205],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0093, -0.0021, -0.0338],
        [ 0.0338, -0.0093,  0.0403,  ...,  0.0227,  0.0066, -0.0015],
        [-0.0168,  0.0143, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6325e-04, -6.9953e-05, -4.2978e-06,  ...,  3.7041e-05,
         -9.1167e-05, -4.5241e-05],
        [-2.0274e-05, -1.3750e-05,  2.7791e-06,  ..., -1.7148e-05,
         -5.8898e-07, -9.2161e-06],
        [ 2.6371e-05,  2.1162e-05, -2.2371e-06,  ...,  2.6338e-05,
          1.5842e-06,  1.2878e-05],
        [-2.9534e-05, -1.9761e-05,  4.5217e-06,  ..., -2.4374e-05,
         -2.3152e-06, -1.2783e-05],
        [-6.0049e-05, -4.0538e-05,  7.6513e-06,  ..., -4.8662e-05,
         -3.8901e-06, -2.6170e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2455e-08, 2.8414e-08, 3.5657e-08,  ..., 1.1744e-08, 8.8968e-08,
         1.1523e-08],
        [3.4139e-11, 1.8754e-11, 2.1561e-12,  ..., 2.0756e-11, 1.1689e-12,
         4.3909e-12],
        [2.0411e-10, 1.0789e-10, 6.5351e-12,  ..., 1.6705e-10, 3.0313e-12,
         4.4304e-11],
        [2.4068e-11, 1.1332e-11, 9.5024e-13,  ..., 1.6195e-11, 4.3156e-13,
         3.6362e-12],
        [1.2697e-10, 6.5262e-11, 5.3145e-12,  ..., 8.8684e-11, 2.7115e-12,
         1.7568e-11]], device='cuda:0')
optimizer state dict: 29.0
lr: [1.9660483627846746e-05, 1.9660483627846746e-05]
scheduler_last_epoch: 29


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[   0, 7094,  596,  ..., 4764, 9514,   14],
        [   0, 3377,   12,  ...,  333,  199,  769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4282e-06, -1.2499e-05,  9.8732e-06,  ...,  2.6272e-05,
         -1.1755e-08, -2.8948e-05],
        [-3.5167e-06, -2.4289e-06,  3.9302e-07,  ..., -2.8759e-06,
         -3.1106e-07, -1.6838e-06],
        [-3.4124e-06, -2.3693e-06,  3.8184e-07,  ..., -2.8014e-06,
         -3.0361e-07, -1.6391e-06],
        [-3.7104e-06, -2.5779e-06,  4.1910e-07,  ..., -3.0547e-06,
         -3.2783e-07, -1.7807e-06],
        [-8.3447e-06, -5.7817e-06,  9.2760e-07,  ..., -6.8247e-06,
         -7.3761e-07, -3.9935e-06]], device='cuda:0')
Loss: 1.1673489809036255


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,   266, 10726,  ..., 13973, 22191, 15913],
        [    0,   298,   301,  ...,    13, 10308,  2129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4605e-05,  1.1153e-05, -1.6312e-06,  ...,  3.5015e-05,
          5.9888e-06,  3.0599e-05],
        [-7.0184e-06, -4.8578e-06,  7.5251e-07,  ..., -5.7667e-06,
         -6.5006e-07, -3.3677e-06],
        [-6.7949e-06, -4.7088e-06,  7.2829e-07,  ..., -5.5879e-06,
         -6.2957e-07, -3.2634e-06],
        [-7.3612e-06, -5.0962e-06,  7.9535e-07,  ..., -6.0648e-06,
         -6.7987e-07, -3.5316e-06],
        [-1.6630e-05, -1.1504e-05,  1.7695e-06,  ..., -1.3649e-05,
         -1.5385e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.1758050918579102


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[    0,    20,  2637,  ..., 14044,     9,   292],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8017e-05, -4.6810e-06, -4.4078e-05,  ...,  6.3479e-06,
          5.7639e-05,  7.5368e-06],
        [-1.0505e-05, -7.3165e-06,  1.1679e-06,  ..., -8.6874e-06,
         -9.4436e-07, -5.0664e-06],
        [-1.0118e-05, -7.0482e-06,  1.1232e-06,  ..., -8.3596e-06,
         -9.0897e-07, -4.8801e-06],
        [-1.0997e-05, -7.6592e-06,  1.2312e-06,  ..., -9.1046e-06,
         -9.8534e-07, -5.2974e-06],
        [-2.4676e-05, -1.7196e-05,  2.7232e-06,  ..., -2.0385e-05,
         -2.2203e-06, -1.1861e-05]], device='cuda:0')
Loss: 1.1960432529449463


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,    67,   695,  ...,   437,   266, 44563],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6695e-05,  1.4220e-07, -3.2534e-05,  ...,  2.2785e-05,
          4.0539e-05,  1.5776e-05],
        [-1.3992e-05, -9.7156e-06,  1.5181e-06,  ..., -1.1489e-05,
         -1.2331e-06, -6.7204e-06],
        [-1.3515e-05, -9.3877e-06,  1.4640e-06,  ..., -1.1086e-05,
         -1.1884e-06, -6.4895e-06],
        [-1.4663e-05, -1.0177e-05,  1.6000e-06,  ..., -1.2055e-05,
         -1.2871e-06, -7.0333e-06],
        [-3.2961e-05, -2.2888e-05,  3.5502e-06,  ..., -2.7031e-05,
         -2.9020e-06, -1.5765e-05]], device='cuda:0')
Loss: 1.1741993427276611


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2132e-05,  1.9867e-05, -6.5692e-06,  ..., -1.1076e-05,
          5.2273e-05,  3.7848e-05],
        [-1.7554e-05, -1.2189e-05,  1.8757e-06,  ..., -1.4380e-05,
         -1.5087e-06, -8.4192e-06],
        [-1.6928e-05, -1.1757e-05,  1.8086e-06,  ..., -1.3858e-05,
         -1.4529e-06, -8.1211e-06],
        [-1.8373e-05, -1.2755e-05,  1.9744e-06,  ..., -1.5065e-05,
         -1.5739e-06, -8.8066e-06],
        [-4.1306e-05, -2.8700e-05,  4.3847e-06,  ..., -3.3796e-05,
         -3.5502e-06, -1.9759e-05]], device='cuda:0')
Loss: 1.1930264234542847


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[   0, 3665, 1419,  ...,  600,  847,  328],
        [   0,  352,  927,  ..., 1521, 3513,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0039e-05,  5.6080e-05, -3.0997e-05,  ..., -3.1982e-05,
          6.2387e-05,  6.2201e-05],
        [-2.1055e-05, -1.4588e-05,  2.2333e-06,  ..., -1.7256e-05,
         -1.7826e-06, -1.0088e-05],
        [-2.0266e-05, -1.4052e-05,  2.1495e-06,  ..., -1.6600e-05,
         -1.7136e-06, -9.7156e-06],
        [-2.2084e-05, -1.5303e-05,  2.3562e-06,  ..., -1.8105e-05,
         -1.8626e-06, -1.0572e-05],
        [-4.9472e-05, -3.4302e-05,  5.2154e-06,  ..., -4.0501e-05,
         -4.1872e-06, -2.3663e-05]], device='cuda:0')
Loss: 1.1773045063018799


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,  7555,  3908,  ...,   259,  8477,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0853e-05,  4.4881e-05, -1.1463e-05,  ..., -3.1104e-05,
          8.7153e-05,  7.4067e-05],
        [-2.4617e-05, -1.7047e-05,  2.6058e-06,  ..., -2.0146e-05,
         -2.0601e-06, -1.1779e-05],
        [-2.3693e-05, -1.6436e-05,  2.5090e-06,  ..., -1.9386e-05,
         -1.9819e-06, -1.1355e-05],
        [-2.5794e-05, -1.7881e-05,  2.7474e-06,  ..., -2.1115e-05,
         -2.1514e-06, -1.2338e-05],
        [-5.7817e-05, -4.0084e-05,  6.0834e-06,  ..., -4.7266e-05,
         -4.8392e-06, -2.7627e-05]], device='cuda:0')
Loss: 1.1660839319229126


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0, 1529, 5227,  ..., 1480,  367,  925],
        [   0, 3804,  300,  ..., 5062, 9848, 3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8615e-05,  4.4729e-05,  1.6133e-05,  ..., -1.3216e-05,
          1.1054e-04,  9.9554e-05],
        [-2.8118e-05, -1.9491e-05,  2.9616e-06,  ..., -2.3022e-05,
         -2.3469e-06, -1.3456e-05],
        [-2.7061e-05, -1.8790e-05,  2.8536e-06,  ..., -2.2158e-05,
         -2.2594e-06, -1.2971e-05],
        [-2.9460e-05, -2.0444e-05,  3.1237e-06,  ..., -2.4140e-05,
         -2.4512e-06, -1.4096e-05],
        [-6.6042e-05, -4.5836e-05,  6.9179e-06,  ..., -5.4061e-05,
         -5.5172e-06, -3.1590e-05]], device='cuda:0')
Loss: 1.1867514848709106
Graident accumulation at epoch 0, step 239, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0338],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0227,  0.0066, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0187]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5579e-04, -5.8485e-05, -2.2547e-06,  ...,  3.2016e-05,
         -7.0996e-05, -3.0762e-05],
        [-2.1058e-05, -1.4324e-05,  2.7974e-06,  ..., -1.7735e-05,
         -7.6477e-07, -9.6401e-06],
        [ 2.1028e-05,  1.7167e-05, -1.7280e-06,  ...,  2.1488e-05,
          1.1998e-06,  1.0293e-05],
        [-2.9526e-05, -1.9830e-05,  4.3819e-06,  ..., -2.4351e-05,
         -2.3288e-06, -1.2914e-05],
        [-6.0648e-05, -4.1068e-05,  7.5779e-06,  ..., -4.9202e-05,
         -4.0528e-06, -2.6712e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2420e-08, 2.8388e-08, 3.5621e-08,  ..., 1.1733e-08, 8.8891e-08,
         1.1521e-08],
        [3.4896e-11, 1.9115e-11, 2.1627e-12,  ..., 2.1266e-11, 1.1732e-12,
         4.5676e-12],
        [2.0463e-10, 1.0813e-10, 6.5367e-12,  ..., 1.6737e-10, 3.0334e-12,
         4.4428e-11],
        [2.4912e-11, 1.1738e-11, 9.5905e-13,  ..., 1.6762e-11, 4.3714e-13,
         3.8312e-12],
        [1.3121e-10, 6.7298e-11, 5.3570e-12,  ..., 9.1518e-11, 2.7392e-12,
         1.8548e-11]], device='cuda:0')
optimizer state dict: 30.0
lr: [1.9627808588917577e-05, 1.9627808588917577e-05]
scheduler_last_epoch: 30


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0, 18197,  1340,  ...,   360,   266,  1110],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6826e-05,  5.0652e-05, -2.3365e-05,  ...,  3.6158e-06,
          5.5605e-06,  1.8476e-05],
        [-3.4124e-06, -2.4140e-06,  4.2841e-07,  ..., -2.8461e-06,
         -3.6694e-07, -1.6242e-06],
        [-3.3230e-06, -2.3544e-06,  4.1910e-07,  ..., -2.7716e-06,
         -3.5577e-07, -1.5795e-06],
        [-3.5465e-06, -2.5034e-06,  4.4890e-07,  ..., -2.9504e-06,
         -3.7998e-07, -1.6838e-06],
        [-7.9274e-06, -5.6028e-06,  9.9093e-07,  ..., -6.5863e-06,
         -8.4937e-07, -3.7551e-06]], device='cuda:0')
Loss: 1.1946337223052979


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0, 37312,    12,  ...,   278,   795, 40854],
        [    0,   300, 11040,  ...,   266,  1736,  3487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4994e-05,  6.4629e-05, -3.6205e-05,  ..., -2.3510e-05,
         -3.8850e-06,  4.0358e-05],
        [-6.8694e-06, -4.8131e-06,  8.3260e-07,  ..., -5.6624e-06,
         -7.2829e-07, -3.2336e-06],
        [-6.6906e-06, -4.6939e-06,  8.1398e-07,  ..., -5.5134e-06,
         -7.0594e-07, -3.1441e-06],
        [-7.1377e-06, -5.0068e-06,  8.7172e-07,  ..., -5.8711e-06,
         -7.5251e-07, -3.3528e-06],
        [-1.5974e-05, -1.1206e-05,  1.9334e-06,  ..., -1.3143e-05,
         -1.6876e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1809028387069702


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,  1941,   437,  ..., 16539,  4129,  4156]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8612e-05,  6.5978e-05, -4.8063e-05,  ...,  1.2940e-05,
         -2.6608e-05,  4.1356e-05],
        [-1.0327e-05, -7.2420e-06,  1.2573e-06,  ..., -8.4788e-06,
         -1.0654e-06, -4.8801e-06],
        [-1.0073e-05, -7.0781e-06,  1.2312e-06,  ..., -8.2701e-06,
         -1.0356e-06, -4.7535e-06],
        [-1.0744e-05, -7.5549e-06,  1.3169e-06,  ..., -8.8066e-06,
         -1.1027e-06, -5.0664e-06],
        [-2.4080e-05, -1.6898e-05,  2.9244e-06,  ..., -1.9699e-05,
         -2.4736e-06, -1.1340e-05]], device='cuda:0')
Loss: 1.1755313873291016


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,   346,   462,  ..., 35247,  2547,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9564e-05,  2.7671e-05, -7.9427e-05,  ..., -1.1547e-05,
         -1.0692e-05,  1.3625e-05],
        [-1.3784e-05, -9.6709e-06,  1.6671e-06,  ..., -1.1310e-05,
         -1.4100e-06, -6.5267e-06],
        [-1.3456e-05, -9.4622e-06,  1.6317e-06,  ..., -1.1042e-05,
         -1.3728e-06, -6.3628e-06],
        [-1.4350e-05, -1.0088e-05,  1.7453e-06,  ..., -1.1757e-05,
         -1.4603e-06, -6.7800e-06],
        [-3.2127e-05, -2.2560e-05,  3.8780e-06,  ..., -2.6315e-05,
         -3.2783e-06, -1.5184e-05]], device='cuda:0')
Loss: 1.1693412065505981


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  923,   13,  ...,  300, 8262,   12],
        [   0, 3261, 1518,  ..., 5019,  287, 1906]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1254e-05,  5.7813e-05, -5.4704e-05,  ..., -2.9952e-05,
         -1.0692e-05,  7.7585e-05],
        [-1.7256e-05, -1.2100e-05,  2.0899e-06,  ..., -1.4141e-05,
         -1.7993e-06, -8.1658e-06],
        [-1.6868e-05, -1.1846e-05,  2.0470e-06,  ..., -1.3828e-05,
         -1.7546e-06, -7.9647e-06],
        [-1.7971e-05, -1.2621e-05,  2.1886e-06,  ..., -1.4707e-05,
         -1.8645e-06, -8.4862e-06],
        [-4.0233e-05, -2.8223e-05,  4.8541e-06,  ..., -3.2902e-05,
         -4.1872e-06, -1.8984e-05]], device='cuda:0')
Loss: 1.1811413764953613


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,   266,  1234,  ...,   908,   328, 26300],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7533e-05,  2.3624e-05, -6.0170e-05,  ..., -4.4520e-05,
         -2.9458e-05,  1.0327e-04],
        [-2.0683e-05, -1.4529e-05,  2.4978e-06,  ..., -1.6943e-05,
         -2.1476e-06, -9.7677e-06],
        [ 2.6489e-04,  1.8947e-04, -9.3455e-06,  ...,  2.7651e-04,
          1.6495e-05,  1.3754e-04],
        [-2.1547e-05, -1.5154e-05,  2.6152e-06,  ..., -1.7613e-05,
         -2.2259e-06, -1.0155e-05],
        [-4.8280e-05, -3.3915e-05,  5.8077e-06,  ..., -3.9458e-05,
         -5.0068e-06, -2.2739e-05]], device='cuda:0')
Loss: 1.166998267173767


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,  3441,   796,  ...,  7561,  1711,   857],
        [    0, 41855,     9,  ..., 33073,   401,  4528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9865e-05,  1.0332e-05, -7.3990e-05,  ..., -4.8074e-05,
         -1.7930e-05,  8.5237e-05],
        [-2.4155e-05, -1.6958e-05,  2.9150e-06,  ..., -1.9789e-05,
         -2.5108e-06, -1.1384e-05],
        [ 2.6152e-04,  1.8712e-04, -8.9395e-06,  ...,  2.7374e-04,
          1.6141e-05,  1.3597e-04],
        [-2.5153e-05, -1.7688e-05,  3.0529e-06,  ..., -2.0579e-05,
         -2.6021e-06, -1.1839e-05],
        [-5.6326e-05, -3.9577e-05,  6.7763e-06,  ..., -4.6074e-05,
         -5.8524e-06, -2.6494e-05]], device='cuda:0')
Loss: 1.1970351934432983


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0,   634, 10095,  ...,   367, 24607,   287],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3011e-05, -6.5902e-06, -7.3691e-05,  ..., -6.4691e-05,
         -3.1667e-05,  1.2093e-04],
        [-2.7627e-05, -1.9372e-05,  3.2987e-06,  ..., -2.2590e-05,
         -2.8759e-06, -1.3001e-05],
        [ 2.5809e-04,  1.8474e-04, -8.5595e-06,  ...,  2.7096e-04,
          1.5781e-05,  1.3437e-04],
        [-2.8759e-05, -2.0191e-05,  3.4533e-06,  ..., -2.3484e-05,
         -2.9802e-06, -1.3515e-05],
        [-6.4492e-05, -4.5270e-05,  7.6778e-06,  ..., -5.2661e-05,
         -6.7130e-06, -3.0294e-05]], device='cuda:0')
Loss: 1.1794111728668213
Graident accumulation at epoch 0, step 247, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0152,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0338],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4751e-04, -5.3295e-05, -9.3984e-06,  ...,  2.2345e-05,
         -6.7063e-05, -1.5593e-05],
        [-2.1715e-05, -1.4829e-05,  2.8475e-06,  ..., -1.8221e-05,
         -9.7589e-07, -9.9762e-06],
        [ 4.4735e-05,  3.3924e-05, -2.4112e-06,  ...,  4.6436e-05,
          2.6580e-06,  2.2701e-05],
        [-2.9450e-05, -1.9866e-05,  4.2891e-06,  ..., -2.4264e-05,
         -2.3939e-06, -1.2974e-05],
        [-6.1032e-05, -4.1488e-05,  7.5879e-06,  ..., -4.9548e-05,
         -4.3188e-06, -2.7070e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2383e-08, 2.8359e-08, 3.5591e-08,  ..., 1.1725e-08, 8.8803e-08,
         1.1524e-08],
        [3.5624e-11, 1.9471e-11, 2.1714e-12,  ..., 2.1755e-11, 1.1803e-12,
         4.7321e-12],
        [2.7104e-10, 1.4215e-10, 6.6035e-12,  ..., 2.4063e-10, 3.2794e-12,
         6.2440e-11],
        [2.5714e-11, 1.2134e-11, 9.7001e-13,  ..., 1.7297e-11, 4.4558e-13,
         4.0101e-12],
        [1.3523e-10, 6.9280e-11, 5.4106e-12,  ..., 9.4200e-11, 2.7816e-12,
         1.9447e-11]], device='cuda:0')
optimizer state dict: 31.0
lr: [1.95936623367937e-05, 1.95936623367937e-05]
scheduler_last_epoch: 31


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[    0,    14,  8047,  ...,  3813,     9,  8237],
        [    0,    26,   874,  ...,    12, 21591,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5730e-05,  6.6535e-06,  2.0194e-05,  ...,  4.6682e-06,
          5.6815e-06,  1.5355e-05],
        [-3.4124e-06, -2.4289e-06,  4.4703e-07,  ..., -2.8014e-06,
         -4.3213e-07, -1.5348e-06],
        [-3.3677e-06, -2.3991e-06,  4.4331e-07,  ..., -2.7716e-06,
         -4.2841e-07, -1.5199e-06],
        [-3.5465e-06, -2.5183e-06,  4.6752e-07,  ..., -2.9057e-06,
         -4.4703e-07, -1.5944e-06],
        [-7.9274e-06, -5.6326e-06,  1.0356e-06,  ..., -6.4969e-06,
         -1.0058e-06, -3.5614e-06]], device='cuda:0')
Loss: 1.1949021816253662


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[   0, 3141,  311,  ...,  328, 7818,  408],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5837e-06,  2.6536e-05,  1.6887e-05,  ...,  3.7526e-05,
         -1.4758e-05,  2.4819e-05],
        [-6.8396e-06, -4.8280e-06,  9.1828e-07,  ..., -5.5879e-06,
         -8.7731e-07, -3.0696e-06],
        [-6.7502e-06, -4.7684e-06,  9.0897e-07,  ..., -5.5283e-06,
         -8.6799e-07, -3.0324e-06],
        [-7.1079e-06, -5.0217e-06,  9.5926e-07,  ..., -5.8115e-06,
         -9.0897e-07, -3.1888e-06],
        [-1.5855e-05, -1.1206e-05,  2.1309e-06,  ..., -1.2964e-05,
         -2.0415e-06, -7.1079e-06]], device='cuda:0')
Loss: 1.1858172416687012


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   266,  2057,  ...,    88,  1801,    66],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2916e-05,  5.0262e-05, -3.2161e-06,  ...,  3.8344e-05,
          8.1885e-06,  9.4723e-05],
        [-1.0237e-05, -7.2420e-06,  1.3821e-06,  ..., -8.3596e-06,
         -1.3206e-06, -4.6045e-06],
        [-1.0148e-05, -7.1675e-06,  1.3709e-06,  ..., -8.2850e-06,
         -1.3094e-06, -4.5598e-06],
        [-1.0669e-05, -7.5549e-06,  1.4473e-06,  ..., -8.7023e-06,
         -1.3709e-06, -4.7982e-06],
        [-2.3842e-05, -1.6868e-05,  3.2187e-06,  ..., -1.9431e-05,
         -3.0771e-06, -1.0699e-05]], device='cuda:0')
Loss: 1.1755023002624512


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,  5136,   446,  ...,  1173,   300,   266],
        [    0,  9342,   600,  ...,   199, 12095,   291]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6906e-05,  9.2005e-06, -1.6102e-05,  ...,  3.0509e-05,
          1.0304e-05,  1.1233e-04],
        [-1.3679e-05, -9.6709e-06,  1.8496e-06,  ..., -1.1176e-05,
         -1.7677e-06, -6.1765e-06],
        [-1.3575e-05, -9.5814e-06,  1.8384e-06,  ..., -1.1086e-05,
         -1.7565e-06, -6.1318e-06],
        [-1.4246e-05, -1.0073e-05,  1.9353e-06,  ..., -1.1623e-05,
         -1.8347e-06, -6.4299e-06],
        [-3.1888e-05, -2.2501e-05,  4.3064e-06,  ..., -2.5988e-05,
         -4.1202e-06, -1.4365e-05]], device='cuda:0')
Loss: 1.1474974155426025


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[    0, 12182,  6294,  ...,  1042,  1070,  2228],
        [    0,  5646,    12,  ...,  1952,   287,  3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3348e-05,  1.8811e-06, -2.7128e-05,  ...,  5.2822e-05,
          1.5836e-05,  1.5020e-04],
        [-1.7121e-05, -1.2115e-05,  2.3376e-06,  ..., -1.3992e-05,
         -2.2333e-06, -7.7263e-06],
        [-1.7002e-05, -1.2025e-05,  2.3264e-06,  ..., -1.3888e-05,
         -2.2221e-06, -7.6741e-06],
        [-1.7807e-05, -1.2606e-05,  2.4457e-06,  ..., -1.4544e-05,
         -2.3190e-06, -8.0317e-06],
        [-3.9876e-05, -2.8163e-05,  5.4389e-06,  ..., -3.2514e-05,
         -5.2005e-06, -1.7956e-05]], device='cuda:0')
Loss: 1.1748642921447754


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,  3253,  1573,  ...,   298,   358,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4816e-05,  1.2932e-05,  2.8407e-06,  ...,  4.5894e-05,
         -3.1056e-07,  1.6217e-04],
        [-2.0564e-05, -1.4558e-05,  2.7884e-06,  ..., -1.6838e-05,
         -2.7325e-06, -9.2760e-06],
        [-2.0444e-05, -1.4469e-05,  2.7772e-06,  ..., -1.6734e-05,
         -2.7213e-06, -9.2238e-06],
        [-2.1324e-05, -1.5110e-05,  2.9095e-06,  ..., -1.7449e-05,
         -2.8294e-06, -9.6187e-06],
        [ 1.4123e-04,  1.0077e-04, -2.3698e-05,  ...,  1.1617e-04,
          4.6189e-05,  7.5047e-05]], device='cuda:0')
Loss: 1.197309970855713


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  257,  298,  ..., 3768,  271,  266],
        [   0,  300, 5631,  ..., 2278, 2669, 3011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3196e-05,  3.6479e-05,  1.9116e-05,  ...,  5.0280e-05,
         -3.5556e-05,  1.7564e-04],
        [-2.3976e-05, -1.7002e-05,  3.2261e-06,  ..., -1.9640e-05,
         -3.1758e-06, -1.0818e-05],
        [-2.3797e-05, -1.6883e-05,  3.2093e-06,  ..., -1.9491e-05,
         -3.1572e-06, -1.0736e-05],
        [-2.4855e-05, -1.7643e-05,  3.3658e-06,  ..., -2.0355e-05,
         -3.2876e-06, -1.1213e-05],
        [ 1.3336e-04,  9.5136e-05, -2.2692e-05,  ...,  1.0970e-04,
          4.5168e-05,  7.1500e-05]], device='cuda:0')
Loss: 1.16464364528656


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   609,    12,  ...,   409, 11041,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5191e-06,  3.3855e-05,  2.5733e-05,  ...,  4.6532e-05,
         -5.5130e-05,  1.6979e-04],
        [-2.7344e-05, -1.9386e-05,  3.6731e-06,  ..., -2.2411e-05,
         -3.6191e-06, -1.2316e-05],
        [ 5.5670e-05,  5.5598e-05, -4.3099e-06,  ...,  5.0964e-05,
          1.0756e-05,  3.3863e-05],
        [-2.8387e-05, -2.0146e-05,  3.8370e-06,  ..., -2.3261e-05,
         -3.7532e-06, -1.2785e-05],
        [ 1.2550e-04,  8.9563e-05, -2.1649e-05,  ...,  1.0320e-04,
          4.4125e-05,  6.7983e-05]], device='cuda:0')
Loss: 1.1685833930969238
Graident accumulation at epoch 0, step 255, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0049, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0015],
        [-0.0168,  0.0144, -0.0270,  ...,  0.0279, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3291e-04, -4.4580e-05, -5.8852e-06,  ...,  2.4764e-05,
         -6.5870e-05,  2.9448e-06],
        [-2.2278e-05, -1.5285e-05,  2.9301e-06,  ..., -1.8640e-05,
         -1.2402e-06, -1.0210e-05],
        [ 4.5828e-05,  3.6091e-05, -2.6010e-06,  ...,  4.6888e-05,
          3.4677e-06,  2.3817e-05],
        [-2.9343e-05, -1.9894e-05,  4.2439e-06,  ..., -2.4164e-05,
         -2.5299e-06, -1.2955e-05],
        [-4.2380e-05, -2.8383e-05,  4.6642e-06,  ..., -3.4273e-05,
          5.2556e-07, -1.7565e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2341e-08, 2.8332e-08, 3.5556e-08,  ..., 1.1716e-08, 8.8717e-08,
         1.1541e-08],
        [3.6336e-11, 1.9828e-11, 2.1827e-12,  ..., 2.2235e-11, 1.1923e-12,
         4.8790e-12],
        [2.7387e-10, 1.4510e-10, 6.6154e-12,  ..., 2.4298e-10, 3.3918e-12,
         6.3524e-11],
        [2.6494e-11, 1.2528e-11, 9.8377e-13,  ..., 1.7820e-11, 4.5922e-13,
         4.1695e-12],
        [1.5085e-10, 7.7232e-11, 5.8739e-12,  ..., 1.0476e-10, 4.7258e-12,
         2.4049e-11]], device='cuda:0')
optimizer state dict: 32.0
lr: [1.9558050089320493e-05, 1.9558050089320493e-05]
scheduler_last_epoch: 32


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,  5775,    12,  ...,    12,  1034,  9257],
        [    0,  9058,  5481,  ...,   508, 15074,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2690e-05,  8.8906e-06, -1.4277e-05,  ...,  8.7392e-06,
          1.8941e-05, -1.5180e-06],
        [-3.3677e-06, -2.4438e-06,  5.2154e-07,  ..., -2.7865e-06,
         -5.3272e-07, -1.4603e-06],
        [-3.4124e-06, -2.4736e-06,  5.2899e-07,  ..., -2.8163e-06,
         -5.4017e-07, -1.4752e-06],
        [-3.5316e-06, -2.5630e-06,  5.5134e-07,  ..., -2.9206e-06,
         -5.5507e-07, -1.5274e-06],
        [-7.8678e-06, -5.7220e-06,  1.2219e-06,  ..., -6.4969e-06,
         -1.2442e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.1734819412231445


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[   0, 1921,  843,  ..., 9420,  352,  266],
        [   0,  795, 3185,  ...,   14, 1671,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8877e-06, -1.2328e-05,  7.2246e-06,  ...,  1.3104e-05,
          3.3386e-05,  1.4407e-06],
        [-6.7055e-06, -4.8578e-06,  1.0319e-06,  ..., -5.5581e-06,
         -1.0543e-06, -2.9206e-06],
        [-6.7800e-06, -4.9174e-06,  1.0468e-06,  ..., -5.6177e-06,
         -1.0654e-06, -2.9504e-06],
        [-6.9886e-06, -5.0813e-06,  1.0841e-06,  ..., -5.8115e-06,
         -1.0952e-06, -3.0473e-06],
        [-1.5616e-05, -1.1355e-05,  2.4065e-06,  ..., -1.2964e-05,
         -2.4587e-06, -6.8098e-06]], device='cuda:0')
Loss: 1.1589224338531494


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[    0,   422,    13,  ..., 14026,   368,  4999],
        [    0,   335,   446,  ...,  5795,    12, 12433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8514e-06, -2.0645e-05,  1.4040e-06,  ..., -2.1213e-05,
          2.8521e-05,  1.5215e-05],
        [-1.0043e-05, -7.2718e-06,  1.5423e-06,  ..., -8.3447e-06,
         -1.6019e-06, -4.3958e-06],
        [-1.0177e-05, -7.3761e-06,  1.5646e-06,  ..., -8.4490e-06,
         -1.6205e-06, -4.4480e-06],
        [-1.0490e-05, -7.6145e-06,  1.6205e-06,  ..., -8.7172e-06,
         -1.6652e-06, -4.5896e-06],
        [-2.3425e-05, -1.7017e-05,  3.5986e-06,  ..., -1.9491e-05,
         -3.7402e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.1649061441421509


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[   0, 2738,  278,  ...,  292,   35, 2147],
        [   0,  199, 2834,  ..., 3988, 1049,  935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2067e-05, -4.3606e-05,  5.4446e-06,  ..., -3.1649e-05,
          4.9299e-05,  1.0794e-06],
        [-1.3411e-05, -9.7007e-06,  2.0787e-06,  ..., -1.1086e-05,
         -2.1979e-06, -5.8785e-06],
        [ 2.0694e-04,  1.3238e-04, -3.4150e-05,  ...,  1.6607e-04,
          3.6972e-05,  7.9785e-05],
        [-1.3977e-05, -1.0148e-05,  2.1793e-06,  ..., -1.1563e-05,
         -2.2836e-06, -6.1318e-06],
        [-3.1233e-05, -2.2650e-05,  4.8354e-06,  ..., -2.5868e-05,
         -5.1260e-06, -1.3709e-05]], device='cuda:0')
Loss: 1.1627345085144043


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0,   879,    27,  ...,    13,  2764,  3860],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5416e-05, -2.9139e-05,  1.9188e-05,  ..., -3.5219e-05,
          6.2540e-05,  3.3554e-05],
        [-1.6794e-05, -1.2130e-05,  2.6114e-06,  ..., -1.3903e-05,
         -2.7344e-06, -7.3612e-06],
        [ 2.0353e-04,  1.2994e-04, -3.3614e-05,  ...,  1.6323e-04,
          3.6428e-05,  7.8287e-05],
        [-1.7494e-05, -1.2681e-05,  2.7344e-06,  ..., -1.4499e-05,
         -2.8424e-06, -7.6741e-06],
        [-3.9101e-05, -2.8282e-05,  6.0648e-06,  ..., -3.2425e-05,
         -6.3777e-06, -1.7151e-05]], device='cuda:0')
Loss: 1.1765708923339844


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[    0,   508,  3282,  ...,   334,   287, 31884],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5614e-05, -2.8314e-05,  2.1080e-05,  ..., -3.6613e-05,
          6.1538e-05,  4.7317e-05],
        [-2.0191e-05, -1.4573e-05,  3.1330e-06,  ..., -1.6719e-05,
         -3.2894e-06, -8.8513e-06],
        [ 2.0009e-04,  1.2747e-04, -3.3089e-05,  ...,  1.6037e-04,
          3.5865e-05,  7.6775e-05],
        [-2.0996e-05, -1.5199e-05,  3.2708e-06,  ..., -1.7405e-05,
         -3.4124e-06, -9.2089e-06],
        [-4.7028e-05, -3.3975e-05,  7.2718e-06,  ..., -3.9011e-05,
         -7.6741e-06, -2.0623e-05]], device='cuda:0')
Loss: 1.1726465225219727


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  432,  984,  ...,  287,  496,   14],
        [   0, 1270,  413,  ...,  413,  711,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7539e-05, -3.9013e-05,  3.9587e-05,  ..., -4.5543e-05,
          5.5177e-05,  4.9703e-05],
        [-2.3589e-05, -1.7017e-05,  3.6322e-06,  ..., -1.9580e-05,
         -3.8631e-06, -1.0356e-05],
        [ 1.9667e-04,  1.2501e-04, -3.2586e-05,  ...,  1.5749e-04,
          3.5288e-05,  7.5262e-05],
        [-2.4498e-05, -1.7717e-05,  3.7886e-06,  ..., -2.0355e-05,
         -4.0010e-06, -1.0759e-05],
        [-5.4896e-05, -3.9637e-05,  8.4341e-06,  ..., -4.5657e-05,
         -9.0003e-06, -2.4110e-05]], device='cuda:0')
Loss: 1.1954370737075806


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,    12,   287,  ...,   278,  4697,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3421e-05, -4.5046e-05,  2.2169e-05,  ..., -3.1681e-05,
          6.5677e-05,  5.0138e-05],
        [-2.6971e-05, -1.9446e-05,  4.1611e-06,  ..., -2.2411e-05,
         -4.4219e-06, -1.1832e-05],
        [ 1.9325e-04,  1.2255e-04, -3.2053e-05,  ...,  1.5463e-04,
          3.4722e-05,  7.3772e-05],
        [-2.8014e-05, -2.0236e-05,  4.3400e-06,  ..., -2.3291e-05,
         -4.5821e-06, -1.2286e-05],
        [-6.2704e-05, -4.5240e-05,  9.6560e-06,  ..., -5.2184e-05,
         -1.0289e-05, -2.7508e-05]], device='cuda:0')
Loss: 1.18583083152771
Graident accumulation at epoch 0, step 263, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0339, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2796e-04, -4.4627e-05, -3.0798e-06,  ...,  1.9119e-05,
         -5.2715e-05,  7.6641e-06],
        [-2.2747e-05, -1.5701e-05,  3.0532e-06,  ..., -1.9017e-05,
         -1.5584e-06, -1.0372e-05],
        [ 6.0570e-05,  4.4737e-05, -5.5462e-06,  ...,  5.7663e-05,
          6.5931e-06,  2.8813e-05],
        [-2.9210e-05, -1.9928e-05,  4.2535e-06,  ..., -2.4077e-05,
         -2.7351e-06, -1.2888e-05],
        [-4.4412e-05, -3.0069e-05,  5.1634e-06,  ..., -3.6064e-05,
         -5.5592e-07, -1.8559e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2305e-08, 2.8306e-08, 3.5521e-08,  ..., 1.1705e-08, 8.8633e-08,
         1.1532e-08],
        [3.7027e-11, 2.0186e-11, 2.1978e-12,  ..., 2.2715e-11, 1.2106e-12,
         5.0141e-12],
        [3.1094e-10, 1.5997e-10, 7.6362e-12,  ..., 2.6665e-10, 4.5940e-12,
         6.8903e-11],
        [2.7253e-11, 1.2925e-11, 1.0016e-12,  ..., 1.8345e-11, 4.7976e-13,
         4.3163e-12],
        [1.5463e-10, 7.9202e-11, 5.9612e-12,  ..., 1.0737e-10, 4.8270e-12,
         2.4782e-11]], device='cuda:0')
optimizer state dict: 33.0
lr: [1.9520977288360243e-05, 1.9520977288360243e-05]
scheduler_last_epoch: 33


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,   292,   380,  ...,   527, 37357,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9341e-06,  3.5924e-05,  1.8128e-05,  ...,  7.3625e-06,
          2.0941e-05, -1.1930e-05],
        [-3.2783e-06, -2.3991e-06,  5.5879e-07,  ..., -2.7716e-06,
         -6.7055e-07, -1.3933e-06],
        [-3.3826e-06, -2.4736e-06,  5.7742e-07,  ..., -2.8610e-06,
         -6.9290e-07, -1.4380e-06],
        [-3.4273e-06, -2.5034e-06,  5.8860e-07,  ..., -2.8908e-06,
         -6.9663e-07, -1.4529e-06],
        [-7.7486e-06, -5.6326e-06,  1.3188e-06,  ..., -6.5267e-06,
         -1.5795e-06, -3.2783e-06]], device='cuda:0')
Loss: 1.1613693237304688


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[    0, 33119,   391,  ...,   292,  4462,  2721],
        [    0,   527,   496,  ...,    12,   795,  8296]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7536e-07,  2.3189e-05,  3.4720e-05,  ..., -1.2870e-05,
          4.0863e-05, -4.4538e-07],
        [-6.6161e-06, -4.8578e-06,  1.1139e-06,  ..., -5.5283e-06,
         -1.3411e-06, -2.7791e-06],
        [-6.8247e-06, -5.0068e-06,  1.1511e-06,  ..., -5.7071e-06,
         -1.3821e-06, -2.8685e-06],
        [-6.8843e-06, -5.0515e-06,  1.1660e-06,  ..., -5.7518e-06,
         -1.3895e-06, -2.8908e-06],
        [-1.5497e-05, -1.1355e-05,  2.6077e-06,  ..., -1.2934e-05,
         -3.1367e-06, -6.4969e-06]], device='cuda:0')
Loss: 1.1678357124328613


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,  1295,   898,  ...,   298, 38754,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4005e-05,  4.8283e-05,  2.4234e-05,  ...,  1.8941e-05,
          8.3612e-06, -2.2022e-05],
        [-9.9093e-06, -7.2569e-06,  1.7136e-06,  ..., -8.3148e-06,
         -1.9930e-06, -4.1872e-06],
        [-1.0222e-05, -7.4804e-06,  1.7695e-06,  ..., -8.5831e-06,
         -2.0526e-06, -4.3213e-06],
        [-1.0327e-05, -7.5549e-06,  1.7956e-06,  ..., -8.6576e-06,
         -2.0675e-06, -4.3586e-06],
        [-2.3186e-05, -1.6958e-05,  4.0084e-06,  ..., -1.9431e-05,
         -4.6566e-06, -9.7901e-06]], device='cuda:0')
Loss: 1.1767774820327759


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[   0,  474,  513,  ...,  221, 2951, 7773],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3176e-05,  4.6164e-05, -2.4359e-06,  ...,  2.1110e-05,
          1.8622e-05, -3.6014e-05],
        [-1.3247e-05, -9.6858e-06,  2.2948e-06,  ..., -1.1101e-05,
         -2.6524e-06, -5.6028e-06],
        [-1.3649e-05, -9.9838e-06,  2.3693e-06,  ..., -1.1459e-05,
         -2.7306e-06, -5.7817e-06],
        [-1.3784e-05, -1.0073e-05,  2.3991e-06,  ..., -1.1548e-05,
         -2.7530e-06, -5.8264e-06],
        [-3.0935e-05, -2.2590e-05,  5.3495e-06,  ..., -2.5898e-05,
         -6.1840e-06, -1.3068e-05]], device='cuda:0')
Loss: 1.1684569120407104


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,    19,    14,  ...,   278,  2588,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3176e-05,  4.6875e-05,  9.4594e-07,  ...,  4.8948e-05,
          7.2729e-06, -3.2468e-06],
        [-1.6570e-05, -1.2130e-05,  2.8722e-06,  ..., -1.3888e-05,
         -3.3304e-06, -7.0259e-06],
        [-1.7077e-05, -1.2502e-05,  2.9653e-06,  ..., -1.4335e-05,
         -3.4273e-06, -7.2494e-06],
        [-1.7226e-05, -1.2591e-05,  2.9989e-06,  ..., -1.4439e-05,
         -3.4533e-06, -7.3016e-06],
        [-3.8683e-05, -2.8253e-05,  6.6906e-06,  ..., -3.2395e-05,
         -7.7561e-06, -1.6376e-05]], device='cuda:0')
Loss: 1.1692695617675781


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   266,  1658,  ...,   278,  1083,  5993],
        [    0, 18981,    13,  ...,   365,  2714,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2231e-05,  2.6261e-05, -2.9990e-05,  ...,  1.0528e-05,
         -6.7346e-06,  4.4070e-07],
        [-1.9893e-05, -1.4558e-05,  3.4198e-06,  ..., -1.6645e-05,
         -4.0010e-06, -8.4266e-06],
        [-2.0504e-05, -1.4991e-05,  3.5316e-06,  ..., -1.7181e-05,
         -4.1164e-06, -8.6948e-06],
        [-2.0683e-05, -1.5110e-05,  3.5688e-06,  ..., -1.7300e-05,
         -4.1462e-06, -8.7544e-06],
        [-4.6432e-05, -3.3885e-05,  7.9647e-06,  ..., -3.8832e-05,
         -9.3207e-06, -1.9640e-05]], device='cuda:0')
Loss: 1.1742260456085205


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0, 28011,    12,  ...,   346,   462,   221],
        [    0,  2663,    12,  ..., 24113,   497,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1446e-05,  1.8845e-05, -3.4725e-05,  ...,  1.6725e-05,
         -7.4672e-06, -1.2228e-05],
        [-2.3171e-05, -1.6943e-05,  3.9972e-06,  ..., -1.9401e-05,
         -4.6566e-06, -9.8422e-06],
        [-2.3901e-05, -1.7464e-05,  4.1313e-06,  ..., -2.0042e-05,
         -4.7944e-06, -1.0163e-05],
        [-2.4110e-05, -1.7613e-05,  4.1761e-06,  ..., -2.0191e-05,
         -4.8317e-06, -1.0237e-05],
        [-5.4121e-05, -3.9488e-05,  9.3207e-06,  ..., -4.5300e-05,
         -1.0863e-05, -2.2963e-05]], device='cuda:0')
Loss: 1.1760936975479126


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  445,   18,  ..., 1478,  578,  494],
        [   0, 8588, 3937,  ...,  516, 1128, 2341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4242e-05, -4.7345e-06, -3.1520e-05,  ...,  3.4095e-05,
          4.0029e-06,  6.1594e-06],
        [-2.6524e-05, -1.9386e-05,  4.5709e-06,  ..., -2.2188e-05,
         -5.3421e-06, -1.1265e-05],
        [-2.7373e-05, -1.9997e-05,  4.7237e-06,  ..., -2.2918e-05,
         -5.5023e-06, -1.1638e-05],
        [-2.7597e-05, -2.0161e-05,  4.7758e-06,  ..., -2.3097e-05,
         -5.5432e-06, -1.1720e-05],
        [-6.2048e-05, -4.5240e-05,  1.0669e-05,  ..., -5.1886e-05,
         -1.2480e-05, -2.6315e-05]], device='cuda:0')
Loss: 1.1775240898132324
Graident accumulation at epoch 0, step 271, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0092,  0.0403,  ...,  0.0228,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1659e-04, -4.0638e-05, -5.9238e-06,  ...,  2.0617e-05,
         -4.7043e-05,  7.5137e-06],
        [-2.3125e-05, -1.6069e-05,  3.2049e-06,  ..., -1.9334e-05,
         -1.9368e-06, -1.0462e-05],
        [ 5.1776e-05,  3.8264e-05, -4.5192e-06,  ...,  4.9604e-05,
          5.3836e-06,  2.4768e-05],
        [-2.9049e-05, -1.9951e-05,  4.3057e-06,  ..., -2.3979e-05,
         -3.0159e-06, -1.2771e-05],
        [-4.6176e-05, -3.1586e-05,  5.7140e-06,  ..., -3.7646e-05,
         -1.7483e-06, -1.9335e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2263e-08, 2.8278e-08, 3.5487e-08,  ..., 1.1694e-08, 8.8544e-08,
         1.1521e-08],
        [3.7694e-11, 2.0542e-11, 2.2165e-12,  ..., 2.3185e-11, 1.2380e-12,
         5.1360e-12],
        [3.1138e-10, 1.6021e-10, 7.6509e-12,  ..., 2.6691e-10, 4.6197e-12,
         6.8969e-11],
        [2.7987e-11, 1.3319e-11, 1.0234e-12,  ..., 1.8860e-11, 5.1001e-13,
         4.4493e-12],
        [1.5832e-10, 8.1169e-11, 6.0691e-12,  ..., 1.0996e-10, 4.9779e-12,
         2.5450e-11]], device='cuda:0')
optimizer state dict: 34.0
lr: [1.9482449598960544e-05, 1.9482449598960544e-05]
scheduler_last_epoch: 34


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0, 3968,  446,  ...,   22,  722,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1071e-05, -5.1240e-06, -1.5599e-05,  ...,  9.6637e-07,
         -2.7878e-05, -1.3945e-05],
        [-3.2634e-06, -2.4289e-06,  5.9977e-07,  ..., -2.7716e-06,
         -7.2271e-07, -1.3188e-06],
        [-3.4124e-06, -2.5481e-06,  6.2957e-07,  ..., -2.9057e-06,
         -7.5623e-07, -1.3784e-06],
        [-3.3826e-06, -2.5183e-06,  6.2212e-07,  ..., -2.8759e-06,
         -7.4506e-07, -1.3635e-06],
        [-7.6294e-06, -5.6624e-06,  1.4007e-06,  ..., -6.4671e-06,
         -1.6838e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.1678962707519531


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 1049,   12,  ...,  292, 3963,  755],
        [   0,  278, 5798,  ...,  266,  729, 1798]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3320e-05, -2.6356e-05, -3.4893e-05,  ..., -1.8165e-05,
         -3.7878e-05, -5.0133e-05],
        [-6.5416e-06, -4.8727e-06,  1.1958e-06,  ..., -5.5283e-06,
         -1.4491e-06, -2.6822e-06],
        [-6.8247e-06, -5.0813e-06,  1.2517e-06,  ..., -5.7817e-06,
         -1.5087e-06, -2.7940e-06],
        [-6.7800e-06, -5.0515e-06,  1.2442e-06,  ..., -5.7369e-06,
         -1.4938e-06, -2.7716e-06],
        [-1.5199e-05, -1.1295e-05,  2.7791e-06,  ..., -1.2845e-05,
         -3.3602e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.1546679735183716


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,  1098,   259,  ...,  6572,  1477,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0257e-05, -2.6841e-05, -4.8687e-05,  ..., -2.2993e-05,
         -3.4381e-05, -3.2898e-05],
        [-9.8199e-06, -7.3165e-06,  1.7807e-06,  ..., -8.2999e-06,
         -2.1793e-06, -4.0531e-06],
        [-1.0237e-05, -7.6145e-06,  1.8626e-06,  ..., -8.6576e-06,
         -2.2687e-06, -4.2170e-06],
        [-1.0207e-05, -7.5996e-06,  1.8589e-06,  ..., -8.6278e-06,
         -2.2538e-06, -4.2021e-06],
        [-2.2829e-05, -1.6958e-05,  4.1351e-06,  ..., -1.9282e-05,
         -5.0589e-06, -9.3728e-06]], device='cuda:0')
Loss: 1.1693952083587646


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   729,  3430,  ...,  9715,    13, 42383],
        [    0,  4599,  9005,  ...,   809,    13,  1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4263e-05, -2.4938e-05, -5.7138e-05,  ..., -6.4751e-06,
         -5.1615e-05, -3.1808e-05],
        [-1.3083e-05, -9.7156e-06,  2.3954e-06,  ..., -1.1042e-05,
         -2.9132e-06, -5.4166e-06],
        [-1.3664e-05, -1.0148e-05,  2.5108e-06,  ..., -1.1548e-05,
         -3.0436e-06, -5.6475e-06],
        [-1.3620e-05, -1.0118e-05,  2.5034e-06,  ..., -1.1489e-05,
         -3.0212e-06, -5.6252e-06],
        [-3.0458e-05, -2.2590e-05,  5.5730e-06,  ..., -2.5690e-05,
         -6.7800e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1550865173339844


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[   0,   17,   12,  ...,   12,  461,  806],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6666e-05, -1.1068e-05, -5.5658e-05,  ...,  8.4277e-06,
         -2.8090e-05, -4.4168e-05],
        [-1.6361e-05, -1.2115e-05,  3.0138e-06,  ..., -1.3813e-05,
         -3.6582e-06, -6.7949e-06],
        [-1.7077e-05, -1.2651e-05,  3.1553e-06,  ..., -1.4439e-05,
         -3.8221e-06, -7.0855e-06],
        [-1.7032e-05, -1.2621e-05,  3.1479e-06,  ..., -1.4380e-05,
         -3.7961e-06, -7.0557e-06],
        [-3.8058e-05, -2.8163e-05,  7.0035e-06,  ..., -3.2127e-05,
         -8.5160e-06, -1.5736e-05]], device='cuda:0')
Loss: 1.1801979541778564


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  4645,  7688,  ..., 26535,   471,   287],
        [    0,   221,   380,  ...,  3990,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7245e-05, -2.4979e-05, -5.5302e-05,  ...,  2.5474e-05,
         -5.7287e-05, -3.8850e-05],
        [-1.9640e-05, -1.4573e-05,  3.6210e-06,  ..., -1.6600e-05,
         -4.4070e-06, -8.1584e-06],
        [-2.0549e-05, -1.5244e-05,  3.7961e-06,  ..., -1.7375e-05,
         -4.6119e-06, -8.5309e-06],
        [-2.0444e-05, -1.5169e-05,  3.7812e-06,  ..., -1.7270e-05,
         -4.5747e-06, -8.4788e-06],
        [-4.5657e-05, -3.3855e-05,  8.4043e-06,  ..., -3.8564e-05,
         -1.0252e-05, -1.8895e-05]], device='cuda:0')
Loss: 1.159010410308838


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,  1234,   278,  ...,  8635,   271,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3915e-05, -4.2006e-05, -6.9905e-05,  ...,  1.9983e-05,
         -5.4987e-05, -6.4425e-05],
        [-2.2888e-05, -1.6972e-05,  4.2133e-06,  ..., -1.9372e-05,
         -5.1148e-06, -9.4995e-06],
        [-2.3961e-05, -1.7762e-05,  4.4145e-06,  ..., -2.0266e-05,
         -5.3495e-06, -9.9316e-06],
        [-2.3827e-05, -1.7673e-05,  4.3996e-06,  ..., -2.0146e-05,
         -5.3085e-06, -9.8720e-06],
        [-5.3257e-05, -3.9458e-05,  9.7826e-06,  ..., -4.5002e-05,
         -1.1899e-05, -2.2009e-05]], device='cuda:0')
Loss: 1.1687698364257812


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[    0,  3504,     9,  ...,  7166, 10945,  3119],
        [    0,   287,  2997,  ...,   437,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7389e-05, -6.2455e-05, -6.8545e-05,  ...,  2.5388e-06,
         -9.0117e-05, -6.7727e-05],
        [-2.6152e-05, -1.9386e-05,  4.8168e-06,  ..., -2.2143e-05,
         -5.8636e-06, -1.0841e-05],
        [-2.7373e-05, -2.0280e-05,  5.0440e-06,  ..., -2.3156e-05,
         -6.1318e-06, -1.1332e-05],
        [-2.7239e-05, -2.0191e-05,  5.0329e-06,  ..., -2.3037e-05,
         -6.0908e-06, -1.1273e-05],
        [-6.0827e-05, -4.5061e-05,  1.1183e-05,  ..., -5.1439e-05,
         -1.3642e-05, -2.5123e-05]], device='cuda:0')
Loss: 1.1700422763824463
Graident accumulation at epoch 0, step 279, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0032,  0.0222, -0.0204],
        [ 0.0292, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0403,  ...,  0.0229,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5192e-05, -4.2819e-05, -1.2186e-05,  ...,  1.8809e-05,
         -5.1351e-05, -1.0420e-08],
        [-2.3428e-05, -1.6401e-05,  3.3661e-06,  ..., -1.9615e-05,
         -2.3294e-06, -1.0499e-05],
        [ 4.3861e-05,  3.2409e-05, -3.5629e-06,  ...,  4.2328e-05,
          4.2320e-06,  2.1158e-05],
        [-2.8868e-05, -1.9975e-05,  4.3784e-06,  ..., -2.3884e-05,
         -3.3234e-06, -1.2622e-05],
        [-4.7641e-05, -3.2933e-05,  6.2609e-06,  ..., -3.9026e-05,
         -2.9377e-06, -1.9913e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2230e-08, 2.8253e-08, 3.5456e-08,  ..., 1.1683e-08, 8.8464e-08,
         1.1514e-08],
        [3.8340e-11, 2.0897e-11, 2.2375e-12,  ..., 2.3652e-11, 1.2711e-12,
         5.2484e-12],
        [3.1182e-10, 1.6046e-10, 7.6687e-12,  ..., 2.6718e-10, 4.6527e-12,
         6.9029e-11],
        [2.8701e-11, 1.3713e-11, 1.0477e-12,  ..., 1.9372e-11, 5.4659e-13,
         4.5720e-12],
        [1.6186e-10, 8.3119e-11, 6.1881e-12,  ..., 1.1250e-10, 5.1590e-12,
         2.6055e-11]], device='cuda:0')
optimizer state dict: 35.0
lr: [1.9442472908488653e-05, 1.9442472908488653e-05]
scheduler_last_epoch: 35


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[   0,  897,  328,  ...,  908,  696,  688],
        [   0, 3860,  694,  ..., 1027,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2151e-05,  1.6399e-05, -7.1936e-06,  ...,  2.8346e-05,
         -1.6705e-05, -3.4052e-05],
        [-3.2037e-06, -2.4140e-06,  6.4820e-07,  ..., -2.7716e-06,
         -8.0839e-07, -1.3113e-06],
        [-3.3677e-06, -2.5481e-06,  6.8545e-07,  ..., -2.9355e-06,
         -8.5309e-07, -1.3858e-06],
        [-3.3528e-06, -2.5332e-06,  6.8173e-07,  ..., -2.9057e-06,
         -8.4564e-07, -1.3709e-06],
        [-7.4506e-06, -5.6326e-06,  1.5050e-06,  ..., -6.4671e-06,
         -1.8850e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.1650183200836182


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[    0,   944,   278,  ...,  2374,   699,  8867],
        [    0, 25009,   407,  ..., 13076,    13,  5226]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2025e-05,  2.1501e-05, -5.0718e-06,  ...,  3.7508e-05,
         -1.1012e-05, -3.2125e-05],
        [-6.4224e-06, -4.8280e-06,  1.2666e-06,  ..., -5.5730e-06,
         -1.6205e-06, -2.6599e-06],
        [-6.7800e-06, -5.0962e-06,  1.3374e-06,  ..., -5.9009e-06,
         -1.7099e-06, -2.8089e-06],
        [-6.6906e-06, -5.0366e-06,  1.3225e-06,  ..., -5.8115e-06,
         -1.6838e-06, -2.7642e-06],
        [-1.4931e-05, -1.1235e-05,  2.9355e-06,  ..., -1.2964e-05,
         -3.7700e-06, -6.1691e-06]], device='cuda:0')
Loss: 1.1752650737762451


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  5159,   292,  ...,   772,   271,  3728],
        [    0,  3611, 10765,  ...,   271,  4317,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0449e-05,  1.9854e-05,  2.3763e-06,  ...,  4.7495e-05,
          1.8983e-05, -2.9387e-07],
        [-9.6411e-06, -7.2271e-06,  1.8813e-06,  ..., -8.3596e-06,
         -2.4065e-06, -3.9786e-06],
        [-1.0177e-05, -7.6294e-06,  1.9856e-06,  ..., -8.8364e-06,
         -2.5406e-06, -4.1947e-06],
        [-1.0043e-05, -7.5400e-06,  1.9632e-06,  ..., -8.7172e-06,
         -2.5034e-06, -4.1351e-06],
        [-2.2441e-05, -1.6838e-05,  4.3660e-06,  ..., -1.9461e-05,
         -5.6103e-06, -9.2387e-06]], device='cuda:0')
Loss: 1.1648050546646118


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0, 2286, 1085,  ..., 1387, 1184, 1802],
        [   0, 2348,  565,  ...,   12,  709,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8701e-05,  2.4147e-05, -4.7362e-06,  ...,  4.2120e-05,
          7.1061e-06, -3.4266e-05],
        [-1.2860e-05, -9.6262e-06,  2.5071e-06,  ..., -1.1161e-05,
         -3.1851e-06, -5.2601e-06],
        [-1.3545e-05, -1.0148e-05,  2.6412e-06,  ..., -1.1772e-05,
         -3.3565e-06, -5.5358e-06],
        [-1.3381e-05, -1.0028e-05,  2.6152e-06,  ..., -1.1623e-05,
         -3.3118e-06, -5.4613e-06],
        [-2.9862e-05, -2.2382e-05,  5.8115e-06,  ..., -2.5898e-05,
         -7.4133e-06, -1.2174e-05]], device='cuda:0')
Loss: 1.1605099439620972


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,  1796,   342,  ...,   668,  2903,   518],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8108e-05,  3.7641e-05, -3.2601e-05,  ...,  5.4484e-05,
         -1.3169e-05, -2.6431e-05],
        [-1.6063e-05, -1.2025e-05,  3.1404e-06,  ..., -1.3947e-05,
         -4.0233e-06, -6.5714e-06],
        [-1.6943e-05, -1.2681e-05,  3.3155e-06,  ..., -1.4722e-05,
         -4.2468e-06, -6.9290e-06],
        [-1.6734e-05, -1.2532e-05,  3.2820e-06,  ..., -1.4544e-05,
         -4.1872e-06, -6.8322e-06],
        [-3.7372e-05, -2.7955e-05,  7.2941e-06,  ..., -3.2395e-05,
         -9.3803e-06, -1.5244e-05]], device='cuda:0')
Loss: 1.1659196615219116


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[   0,  266, 2374,  ..., 1551,  518,  638],
        [   0, 6418,  446,  ...,  413,   29,  413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6220e-05,  5.8952e-05, -1.9834e-06,  ...,  2.3442e-05,
          7.7854e-06, -3.1911e-05],
        [-1.9386e-05, -1.4484e-05,  3.7551e-06,  ..., -1.6764e-05,
         -4.9062e-06, -7.8976e-06],
        [-2.0429e-05, -1.5259e-05,  3.9600e-06,  ..., -1.7688e-05,
         -5.1744e-06, -8.3223e-06],
        [-2.0146e-05, -1.5065e-05,  3.9153e-06,  ..., -1.7434e-05,
         -5.0925e-06, -8.1882e-06],
        [-4.5002e-05, -3.3617e-05,  8.7023e-06,  ..., -3.8862e-05,
         -1.1407e-05, -1.8284e-05]], device='cuda:0')
Loss: 1.1696796417236328


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[    0,   369, 19287,  ..., 12502,  6626,   292],
        [    0,  8822,  1486,  ...,    12,   287,  6903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5858e-05,  6.8835e-05, -1.7521e-06,  ...,  3.4228e-05,
         -3.3022e-06,  2.8166e-06],
        [-2.2635e-05, -1.6928e-05,  4.3735e-06,  ..., -1.9535e-05,
         -5.7705e-06, -9.2313e-06],
        [-2.3872e-05, -1.7837e-05,  4.6156e-06,  ..., -2.0623e-05,
         -6.0908e-06, -9.7379e-06],
        [-2.3514e-05, -1.7598e-05,  4.5598e-06,  ..., -2.0310e-05,
         -5.9903e-06, -9.5740e-06],
        [-5.2482e-05, -3.9220e-05,  1.0125e-05,  ..., -4.5240e-05,
         -1.3404e-05, -2.1353e-05]], device='cuda:0')
Loss: 1.1520721912384033


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9981e-06,  5.6261e-05, -4.5967e-06,  ...,  3.1610e-05,
          8.8020e-06, -1.3424e-06],
        [-2.5868e-05, -1.9327e-05,  5.0068e-06,  ..., -2.2307e-05,
         -6.6087e-06, -1.0572e-05],
        [-2.7284e-05, -2.0370e-05,  5.2825e-06,  ..., -2.3544e-05,
         -6.9775e-06, -1.1161e-05],
        [-2.6852e-05, -2.0087e-05,  5.2154e-06,  ..., -2.3171e-05,
         -6.8545e-06, -1.0967e-05],
        [-5.9903e-05, -4.4733e-05,  1.1578e-05,  ..., -5.1588e-05,
         -1.5326e-05, -2.4438e-05]], device='cuda:0')
Loss: 1.167914628982544
Graident accumulation at epoch 0, step 287, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0222, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0014],
        [-0.0167,  0.0144, -0.0270,  ...,  0.0280, -0.0159, -0.0186]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5972e-05, -3.2911e-05, -1.1427e-05,  ...,  2.0089e-05,
         -4.5335e-05, -1.4362e-07],
        [-2.3672e-05, -1.6694e-05,  3.5302e-06,  ..., -1.9884e-05,
         -2.7574e-06, -1.0507e-05],
        [ 3.6746e-05,  2.7131e-05, -2.6784e-06,  ...,  3.5741e-05,
          3.1111e-06,  1.7926e-05],
        [-2.8666e-05, -1.9986e-05,  4.4621e-06,  ..., -2.3813e-05,
         -3.6765e-06, -1.2456e-05],
        [-4.8867e-05, -3.4113e-05,  6.7927e-06,  ..., -4.0282e-05,
         -4.1765e-06, -2.0366e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2188e-08, 2.8228e-08, 3.5420e-08,  ..., 1.1672e-08, 8.8375e-08,
         1.1502e-08],
        [3.8971e-11, 2.1250e-11, 2.2604e-12,  ..., 2.4126e-11, 1.3135e-12,
         5.3549e-12],
        [3.1225e-10, 1.6072e-10, 7.6889e-12,  ..., 2.6747e-10, 4.6967e-12,
         6.9084e-11],
        [2.9393e-11, 1.4103e-11, 1.0739e-12,  ..., 1.9890e-11, 5.9303e-13,
         4.6877e-12],
        [1.6529e-10, 8.5036e-11, 6.3160e-12,  ..., 1.1504e-10, 5.3887e-12,
         2.6627e-11]], device='cuda:0')
optimizer state dict: 36.0
lr: [1.9401053325731837e-05, 1.9401053325731837e-05]
scheduler_last_epoch: 36


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   221,   474,  ...,  1871,   271,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2059e-08,  1.7211e-05, -2.6279e-05,  ...,  3.4516e-05,
         -2.7267e-05,  2.5028e-05],
        [-3.1739e-06, -2.3991e-06,  6.5565e-07,  ..., -2.7865e-06,
         -8.5309e-07, -1.2890e-06],
        [-3.3379e-06, -2.5332e-06,  6.8918e-07,  ..., -2.9206e-06,
         -8.9779e-07, -1.3560e-06],
        [-3.3081e-06, -2.5034e-06,  6.8545e-07,  ..., -2.9057e-06,
         -8.8662e-07, -1.3486e-06],
        [-7.3314e-06, -5.5432e-06,  1.5125e-06,  ..., -6.4373e-06,
         -1.9670e-06, -2.9802e-06]], device='cuda:0')
Loss: 1.1643729209899902


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0917e-06,  4.1287e-05, -3.0223e-05,  ...,  3.9585e-05,
         -4.0521e-05,  5.2411e-05],
        [-6.3777e-06, -4.8280e-06,  1.2890e-06,  ..., -5.5581e-06,
         -1.7323e-06, -2.6003e-06],
        [-6.7204e-06, -5.0962e-06,  1.3597e-06,  ..., -5.8413e-06,
         -1.8254e-06, -2.7344e-06],
        [-6.6459e-06, -5.0217e-06,  1.3448e-06,  ..., -5.7817e-06,
         -1.7993e-06, -2.7046e-06],
        [-1.4693e-05, -1.1116e-05,  2.9653e-06,  ..., -1.2785e-05,
         -3.9935e-06, -5.9754e-06]], device='cuda:0')
Loss: 1.1567277908325195


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  7180,   266,  ...,  1805,    12,   221],
        [    0,    34,     9,  ...,    19,    14, 45576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8779e-06,  3.8401e-05, -2.1400e-05,  ...,  3.2410e-05,
         -8.2951e-05,  3.2787e-06],
        [-9.5665e-06, -7.2271e-06,  1.9036e-06,  ..., -8.3447e-06,
         -2.6077e-06, -3.9116e-06],
        [-1.0088e-05, -7.6294e-06,  2.0079e-06,  ..., -8.7917e-06,
         -2.7493e-06, -4.1202e-06],
        [-9.9540e-06, -7.5102e-06,  1.9856e-06,  ..., -8.6725e-06,
         -2.7046e-06, -4.0680e-06],
        [-2.1994e-05, -1.6600e-05,  4.3735e-06,  ..., -1.9193e-05,
         -5.9903e-06, -8.9854e-06]], device='cuda:0')
Loss: 1.1655282974243164


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[   0, 5522, 5662,  ...,  638, 1231, 1098],
        [   0,  747, 7890,  ...,  706, 8667,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1591e-05,  5.6298e-05, -2.4175e-05,  ...,  4.4047e-05,
         -8.2951e-05,  2.8066e-05],
        [-1.2800e-05, -9.6560e-06,  2.5630e-06,  ..., -1.1101e-05,
         -3.5055e-06, -5.2154e-06],
        [-1.3500e-05, -1.0192e-05,  2.7046e-06,  ..., -1.1712e-05,
         -3.6992e-06, -5.4985e-06],
        [-1.3322e-05, -1.0043e-05,  2.6748e-06,  ..., -1.1548e-05,
         -3.6396e-06, -5.4240e-06],
        [-2.9445e-05, -2.2203e-05,  5.8934e-06,  ..., -2.5541e-05,
         -8.0615e-06, -1.1995e-05]], device='cuda:0')
Loss: 1.1514304876327515


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,   292,   494,  ...,   259, 14134, 11544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9781e-05,  5.4116e-05, -3.8685e-05,  ...,  4.0495e-05,
         -7.6183e-05,  5.7226e-05],
        [-1.6063e-05, -1.2115e-05,  3.2224e-06,  ..., -1.3947e-05,
         -4.4219e-06, -6.5342e-06],
        [-1.6898e-05, -1.2755e-05,  3.3937e-06,  ..., -1.4663e-05,
         -4.6529e-06, -6.8694e-06],
        [-1.6674e-05, -1.2562e-05,  3.3528e-06,  ..., -1.4454e-05,
         -4.5747e-06, -6.7726e-06],
        [-3.6895e-05, -2.7806e-05,  7.3984e-06,  ..., -3.2008e-05,
         -1.0148e-05, -1.4991e-05]], device='cuda:0')
Loss: 1.1750774383544922


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[   0,  409,  394,  ...,  475, 5458,  328],
        [   0,  395, 4973,  ..., 5851,  409, 4370]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8676e-05,  8.3249e-05, -4.8796e-05,  ...,  7.0667e-05,
         -8.3389e-05,  2.9074e-05],
        [-1.9222e-05, -1.4514e-05,  3.8631e-06,  ..., -1.6689e-05,
         -5.2825e-06, -7.8157e-06],
        [-2.0236e-05, -1.5289e-05,  4.0717e-06,  ..., -1.7554e-05,
         -5.5581e-06, -8.2254e-06],
        [-1.9997e-05, -1.5080e-05,  4.0270e-06,  ..., -1.7330e-05,
         -5.4762e-06, -8.1137e-06],
        [-4.4227e-05, -3.3349e-05,  8.8811e-06,  ..., -3.8356e-05,
         -1.2130e-05, -1.7956e-05]], device='cuda:0')
Loss: 1.1756986379623413


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4215,  1478,  ...,   644,   409,  3803],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2274e-05,  9.4955e-05, -8.2181e-05,  ...,  6.3159e-05,
         -7.5340e-05,  5.4117e-05],
        [-2.2441e-05, -1.6928e-05,  4.4890e-06,  ..., -1.9476e-05,
         -6.1505e-06, -9.1046e-06],
        [-2.3633e-05, -1.7837e-05,  4.7348e-06,  ..., -2.0489e-05,
         -6.4708e-06, -9.5814e-06],
        [-2.3350e-05, -1.7583e-05,  4.6827e-06,  ..., -2.0221e-05,
         -6.3740e-06, -9.4548e-06],
        [-5.1647e-05, -3.8922e-05,  1.0327e-05,  ..., -4.4763e-05,
         -1.4126e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.1558184623718262


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[   0,  292,  474,  ..., 1085,  494, 2665],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5092e-05,  1.0443e-04, -1.2085e-04,  ...,  9.2191e-05,
         -9.8257e-05,  5.4117e-05],
        [-2.5645e-05, -1.9342e-05,  5.1036e-06,  ..., -2.2247e-05,
         -7.0408e-06, -1.0386e-05],
        [-2.7031e-05, -2.0385e-05,  5.3868e-06,  ..., -2.3425e-05,
         -7.4133e-06, -1.0945e-05],
        [-2.6673e-05, -2.0087e-05,  5.3234e-06,  ..., -2.3097e-05,
         -7.2978e-06, -1.0788e-05],
        [-5.9009e-05, -4.4435e-05,  1.1735e-05,  ..., -5.1111e-05,
         -1.6168e-05, -2.3872e-05]], device='cuda:0')
Loss: 1.155287265777588
Graident accumulation at epoch 0, step 295, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0222, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0159, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5884e-05, -1.9177e-05, -2.2370e-05,  ...,  2.7299e-05,
         -5.0628e-05,  5.2824e-06],
        [-2.3869e-05, -1.6958e-05,  3.6875e-06,  ..., -2.0120e-05,
         -3.1857e-06, -1.0495e-05],
        [ 3.0369e-05,  2.2380e-05, -1.8719e-06,  ...,  2.9825e-05,
          2.0587e-06,  1.5039e-05],
        [-2.8467e-05, -1.9996e-05,  4.5483e-06,  ..., -2.3741e-05,
         -4.0386e-06, -1.2289e-05],
        [-4.9881e-05, -3.5146e-05,  7.2869e-06,  ..., -4.1365e-05,
         -5.3756e-06, -2.0717e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2153e-08, 2.8211e-08, 3.5400e-08,  ..., 1.1669e-08, 8.8297e-08,
         1.1494e-08],
        [3.9589e-11, 2.1602e-11, 2.2841e-12,  ..., 2.4597e-11, 1.3618e-12,
         5.4574e-12],
        [3.1267e-10, 1.6097e-10, 7.7103e-12,  ..., 2.6775e-10, 4.7469e-12,
         6.9135e-11],
        [3.0075e-11, 1.4492e-11, 1.1011e-12,  ..., 2.0403e-11, 6.4570e-13,
         4.7994e-12],
        [1.6861e-10, 8.6926e-11, 6.4474e-12,  ..., 1.1754e-10, 5.6447e-12,
         2.7170e-11]], device='cuda:0')
optimizer state dict: 37.0
lr: [1.9358197179963892e-05, 1.9358197179963892e-05]
scheduler_last_epoch: 37


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0, 25938,   359,  ...,    36, 15859,   504],
        [    0,   382,    17,  ...,  8733,    13,  9306]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0621e-06,  7.7747e-07, -1.4076e-05,  ..., -2.3961e-06,
         -6.0715e-06, -5.6316e-05],
        [-3.1590e-06, -2.4140e-06,  6.8173e-07,  ..., -2.7716e-06,
         -8.9779e-07, -1.3188e-06],
        [-3.3677e-06, -2.5779e-06,  7.2643e-07,  ..., -2.9653e-06,
         -9.6112e-07, -1.4007e-06],
        [-3.3230e-06, -2.5332e-06,  7.1526e-07,  ..., -2.9206e-06,
         -9.3877e-07, -1.3784e-06],
        [-7.2420e-06, -5.5134e-06,  1.5572e-06,  ..., -6.3479e-06,
         -2.0564e-06, -3.0100e-06]], device='cuda:0')
Loss: 1.1524326801300049


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,   298, 49038,  ...,   288,  1690,  2736],
        [    0,   923,    13,  ...,   199,   677,  3826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3151e-05,  3.4591e-05,  2.9151e-05,  ..., -9.4002e-06,
          1.4517e-06, -4.2636e-06],
        [-6.3330e-06, -4.8280e-06,  1.3150e-06,  ..., -5.5879e-06,
         -1.8440e-06, -2.6226e-06],
        [-6.7204e-06, -5.1260e-06,  1.3970e-06,  ..., -5.9456e-06,
         -1.9595e-06, -2.7791e-06],
        [-6.6310e-06, -5.0366e-06,  1.3746e-06,  ..., -5.8562e-06,
         -1.9222e-06, -2.7344e-06],
        [-1.4544e-05, -1.1057e-05,  3.0100e-06,  ..., -1.2815e-05,
         -4.2170e-06, -5.9903e-06]], device='cuda:0')
Loss: 1.1693840026855469


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  1086,  5564,  ..., 29319, 32982,   344],
        [    0,   391,  1761,  ...,   346,    14,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7657e-05,  2.9823e-05,  4.0853e-05,  ..., -4.9692e-06,
         -4.3381e-05, -1.0106e-05],
        [-9.4771e-06, -7.2569e-06,  1.9856e-06,  ..., -8.3745e-06,
         -2.7530e-06, -3.8892e-06],
        [-1.0058e-05, -7.7039e-06,  2.1122e-06,  ..., -8.8960e-06,
         -2.9206e-06, -4.1276e-06],
        [-9.9242e-06, -7.5847e-06,  2.0824e-06,  ..., -8.7768e-06,
         -2.8722e-06, -4.0606e-06],
        [-2.1726e-05, -1.6600e-05,  4.5449e-06,  ..., -1.9163e-05,
         -6.2883e-06, -8.8811e-06]], device='cuda:0')
Loss: 1.1595276594161987


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0, 12449,    12,  ...,   292,  2178,   413],
        [    0,   287,  2269,  ..., 22413,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8232e-05,  4.0972e-05,  2.6556e-05,  ..., -1.0213e-05,
         -5.5243e-05,  3.6144e-06],
        [-1.2696e-05, -9.6709e-06,  2.6301e-06,  ..., -1.1206e-05,
         -3.6992e-06, -5.2154e-06],
        [-1.3456e-05, -1.0252e-05,  2.7940e-06,  ..., -1.1891e-05,
         -3.9265e-06, -5.5283e-06],
        [-1.3277e-05, -1.0103e-05,  2.7567e-06,  ..., -1.1727e-05,
         -3.8557e-06, -5.4389e-06],
        [-2.9027e-05, -2.2054e-05,  5.9977e-06,  ..., -2.5570e-05,
         -8.4341e-06, -1.1876e-05]], device='cuda:0')
Loss: 1.1388843059539795


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[    0,  2973, 20362,  ...,   271, 43821, 11776],
        [    0,    14,  4746,  ...,   266,  1119,  1705]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4401e-04,  6.6806e-05,  3.9541e-05,  ..., -2.1855e-05,
         -6.9521e-05,  1.5893e-05],
        [-1.5885e-05, -1.2070e-05,  3.2745e-06,  ..., -1.4007e-05,
         -4.6119e-06, -6.4895e-06],
        [ 7.1458e-05,  5.5075e-05, -2.2342e-05,  ...,  6.9147e-05,
          2.6389e-05,  4.8831e-05],
        [-1.6615e-05, -1.2621e-05,  3.4347e-06,  ..., -1.4663e-05,
         -4.8093e-06, -6.7726e-06],
        [-3.6329e-05, -2.7567e-05,  7.4729e-06,  ..., -3.2008e-05,
         -1.0520e-05, -1.4782e-05]], device='cuda:0')
Loss: 1.1711704730987549


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,  4889,  3593,  ..., 19787,   287, 22475],
        [    0,    24,    15,  ...,   221,   380,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4146e-04,  5.8619e-05, -1.4447e-05,  ..., -2.0717e-05,
         -1.0203e-04, -1.7996e-05],
        [-1.9059e-05, -1.4484e-05,  3.9265e-06,  ..., -1.6794e-05,
         -5.5172e-06, -7.8008e-06],
        [ 6.8106e-05,  5.2527e-05, -2.1652e-05,  ...,  6.6197e-05,
          2.5436e-05,  4.7453e-05],
        [-1.9938e-05, -1.5140e-05,  4.1164e-06,  ..., -1.7583e-05,
         -5.7556e-06, -8.1435e-06],
        [-4.3601e-05, -3.3110e-05,  8.9630e-06,  ..., -3.8415e-05,
         -1.2591e-05, -1.7777e-05]], device='cuda:0')
Loss: 1.1483112573623657


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[   0,  822, 5085,  ...,  293, 1608,  391],
        [   0,  677, 6499,  ..., 2738,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8911e-04,  5.8221e-05, -2.5827e-05,  ..., -5.4203e-05,
         -1.0089e-04, -3.0600e-05],
        [-2.2218e-05, -1.6883e-05,  4.5821e-06,  ..., -1.9595e-05,
         -6.4187e-06, -9.0823e-06],
        [ 6.4753e-05,  4.9978e-05, -2.0956e-05,  ...,  6.3232e-05,
          2.4482e-05,  4.6097e-05],
        [-2.3261e-05, -1.7673e-05,  4.8093e-06,  ..., -2.0534e-05,
         -6.7055e-06, -9.4846e-06],
        [-5.0873e-05, -3.8654e-05,  1.0468e-05,  ..., -4.4852e-05,
         -1.4678e-05, -2.0713e-05]], device='cuda:0')
Loss: 1.1694362163543701


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[    0,  1486,   292,  ...,  7484,    15,  5357],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1481e-04,  4.7598e-05, -3.9706e-05,  ..., -5.0964e-05,
         -9.5275e-05, -1.5254e-05],
        [-2.5421e-05, -1.9312e-05,  5.2303e-06,  ..., -2.2411e-05,
         -7.3500e-06, -1.0379e-05],
        [ 6.1370e-05,  4.7415e-05, -2.0267e-05,  ...,  6.0251e-05,
          2.3499e-05,  4.4726e-05],
        [-2.6569e-05, -2.0176e-05,  5.4836e-06,  ..., -2.3440e-05,
         -7.6666e-06, -1.0818e-05],
        [-5.8204e-05, -4.4197e-05,  1.1951e-05,  ..., -5.1290e-05,
         -1.6809e-05, -2.3678e-05]], device='cuda:0')
Loss: 1.1637306213378906
Graident accumulation at epoch 0, step 303, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0151,  0.0037,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0159, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5815e-05, -1.2499e-05, -2.4103e-05,  ...,  1.9473e-05,
         -5.5092e-05,  3.2287e-06],
        [-2.4024e-05, -1.7194e-05,  3.8418e-06,  ..., -2.0350e-05,
         -3.6021e-06, -1.0483e-05],
        [ 3.3469e-05,  2.4883e-05, -3.7113e-06,  ...,  3.2867e-05,
          4.2026e-06,  1.8007e-05],
        [-2.8277e-05, -2.0014e-05,  4.6418e-06,  ..., -2.3711e-05,
         -4.4014e-06, -1.2142e-05],
        [-5.0713e-05, -3.6051e-05,  7.7532e-06,  ..., -4.2357e-05,
         -6.5189e-06, -2.1013e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2157e-08, 2.8185e-08, 3.5366e-08,  ..., 1.1660e-08, 8.8218e-08,
         1.1483e-08],
        [4.0196e-11, 2.1954e-11, 2.3092e-12,  ..., 2.5074e-11, 1.4144e-12,
         5.5597e-12],
        [3.1612e-10, 1.6306e-10, 8.1133e-12,  ..., 2.7111e-10, 5.2944e-12,
         7.1066e-11],
        [3.0751e-11, 1.4885e-11, 1.1301e-12,  ..., 2.0932e-11, 7.0383e-13,
         4.9116e-12],
        [1.7183e-10, 8.8792e-11, 6.5837e-12,  ..., 1.2005e-10, 5.9216e-12,
         2.7703e-11]], device='cuda:0')
optimizer state dict: 38.0
lr: [1.9313911019977992e-05, 1.9313911019977992e-05]
scheduler_last_epoch: 38


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,   902, 11331,  ...,  1795,   365,   654],
        [    0,  1064,  1042,  ...,    12,   259,  4754]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8734e-05, -5.5520e-06, -7.4957e-06,  ...,  1.7913e-06,
          2.3596e-05,  1.0997e-05],
        [-3.1292e-06, -2.3991e-06,  6.8173e-07,  ..., -2.8014e-06,
         -9.2015e-07, -1.2591e-06],
        [-3.3528e-06, -2.5779e-06,  7.3388e-07,  ..., -3.0100e-06,
         -9.8348e-07, -1.3486e-06],
        [-3.2932e-06, -2.5332e-06,  7.1898e-07,  ..., -2.9504e-06,
         -9.6858e-07, -1.3262e-06],
        [-7.1824e-06, -5.5134e-06,  1.5721e-06,  ..., -6.4373e-06,
         -2.1160e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.1760932207107544


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[   0,   14,  357,  ...,   30,  287,  839],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5199e-06,  2.5700e-05, -4.3443e-06,  ..., -1.3027e-05,
          4.2725e-05,  6.5288e-05],
        [-6.2585e-06, -4.8280e-06,  1.3635e-06,  ..., -5.6177e-06,
         -1.8552e-06, -2.5481e-06],
        [-6.7055e-06, -5.1707e-06,  1.4640e-06,  ..., -6.0052e-06,
         -1.9819e-06, -2.7269e-06],
        [-6.5863e-06, -5.0962e-06,  1.4380e-06,  ..., -5.9009e-06,
         -1.9521e-06, -2.6822e-06],
        [-1.4395e-05, -1.1086e-05,  3.1367e-06,  ..., -1.2875e-05,
         -4.2617e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.1676498651504517


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,  2270,   278,  ..., 36325,  5892,  3558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0473e-05,  3.9423e-05, -6.8625e-06,  ..., -2.4657e-05,
          2.9227e-05,  8.6565e-05],
        [-9.3728e-06, -7.2420e-06,  2.0526e-06,  ..., -8.3894e-06,
         -2.7679e-06, -3.7923e-06],
        [ 1.4838e-04,  1.5099e-04, -5.5383e-05,  ...,  1.6200e-04,
          5.8088e-05,  6.2704e-05],
        [-9.8646e-06, -7.6443e-06,  2.1681e-06,  ..., -8.8066e-06,
         -2.9057e-06, -3.9861e-06],
        [-2.1547e-05, -1.6630e-05,  4.7237e-06,  ..., -1.9222e-05,
         -6.3479e-06, -8.6874e-06]], device='cuda:0')
Loss: 1.1539565324783325


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   688,  2353,  ..., 20538, 10393,    12],
        [    0, 10296,   809,  ..., 27683,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7484e-05,  3.8617e-05, -7.4136e-06,  ..., -1.3280e-05,
          6.2750e-05,  6.1112e-05],
        [-1.2487e-05, -9.6560e-06,  2.7269e-06,  ..., -1.1191e-05,
         -3.6806e-06, -5.0813e-06],
        [ 1.4507e-04,  1.4843e-04, -5.4668e-05,  ...,  1.5902e-04,
          5.7119e-05,  6.1333e-05],
        [-1.3158e-05, -1.0192e-05,  2.8796e-06,  ..., -1.1757e-05,
         -3.8669e-06, -5.3495e-06],
        [-2.8700e-05, -2.2173e-05,  6.2659e-06,  ..., -2.5660e-05,
         -8.4490e-06, -1.1638e-05]], device='cuda:0')
Loss: 1.1664867401123047


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0,   16,   14,  ...,  300, 9283,   14],
        [   0,  369,  726,  ...,   83,  409,  729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2391e-05,  4.8310e-05, -2.8313e-05,  ...,  3.1024e-06,
          6.8512e-05,  1.7366e-06],
        [-1.5602e-05, -1.2055e-05,  3.4161e-06,  ..., -1.3977e-05,
         -4.5970e-06, -6.3777e-06],
        [ 1.4177e-04,  1.4587e-04, -5.3934e-05,  ...,  1.5605e-04,
          5.6143e-05,  5.9955e-05],
        [-1.6451e-05, -1.2740e-05,  3.6098e-06,  ..., -1.4707e-05,
         -4.8354e-06, -6.7204e-06],
        [-3.5793e-05, -2.7627e-05,  7.8380e-06,  ..., -3.2008e-05,
         -1.0535e-05, -1.4588e-05]], device='cuda:0')
Loss: 1.166661024093628


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[    0,  4855, 15679,  ...,   278,   266,  1912],
        [    0,  4137,   300,  ...,  2579,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4284e-05,  6.6964e-05, -2.4392e-05,  ..., -6.3526e-06,
          3.4082e-05, -2.4518e-06],
        [-1.8701e-05, -1.4454e-05,  4.0792e-06,  ..., -1.6779e-05,
         -5.4836e-06, -7.6443e-06],
        [ 1.3844e-04,  1.4329e-04, -5.3226e-05,  ...,  1.5306e-04,
          5.5197e-05,  5.8599e-05],
        [-1.9729e-05, -1.5289e-05,  4.3102e-06,  ..., -1.7673e-05,
         -5.7705e-06, -8.0541e-06],
        [-4.2915e-05, -3.3140e-05,  9.3579e-06,  ..., -3.8445e-05,
         -1.2577e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.1639355421066284


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,  1041,    14,  ...,   360,   266, 14966],
        [    0, 20080, 11069,  ...,   300,  5768,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2669e-04,  6.5019e-05,  7.2951e-06,  ..., -4.4380e-07,
          5.2807e-06, -1.8392e-05],
        [-2.1815e-05, -1.6838e-05,  4.7982e-06,  ..., -1.9521e-05,
         -6.4000e-06, -8.9258e-06],
        [ 1.3510e-04,  1.4072e-04, -5.2451e-05,  ...,  1.5011e-04,
          5.4214e-05,  5.7220e-05],
        [-2.3022e-05, -1.7822e-05,  5.0738e-06,  ..., -2.0579e-05,
         -6.7391e-06, -9.4101e-06],
        [-5.0098e-05, -3.8654e-05,  1.1012e-05,  ..., -4.4763e-05,
         -1.4693e-05, -2.0444e-05]], device='cuda:0')
Loss: 1.1501781940460205


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[    0,  2241,  8274,  ...,   908,  1811,   278],
        [    0,  6660, 13165,  ...,   380,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2692e-04,  5.7709e-05,  2.3356e-05,  ...,  1.5071e-05,
          3.6684e-06, -3.2814e-05],
        [-2.4974e-05, -1.9252e-05,  5.5023e-06,  ..., -2.2322e-05,
         -7.3425e-06, -1.0207e-05],
        [ 1.3171e-04,  1.3812e-04, -5.1691e-05,  ...,  1.4710e-04,
          5.3200e-05,  5.5850e-05],
        [-2.6345e-05, -2.0370e-05,  5.8189e-06,  ..., -2.3514e-05,
         -7.7300e-06, -1.0751e-05],
        [-5.7340e-05, -4.4197e-05,  1.2621e-05,  ..., -5.1171e-05,
         -1.6853e-05, -2.3365e-05]], device='cuda:0')
Loss: 1.1706254482269287
Graident accumulation at epoch 0, step 311, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0050, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0076,  0.0032,  ..., -0.0094, -0.0021, -0.0339],
        [ 0.0340, -0.0091,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0167,  0.0145, -0.0271,  ...,  0.0280, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7542e-05, -5.4784e-06, -1.9357e-05,  ...,  1.9033e-05,
         -4.9216e-05, -3.7551e-07],
        [-2.4119e-05, -1.7400e-05,  4.0079e-06,  ..., -2.0547e-05,
         -3.9762e-06, -1.0456e-05],
        [ 4.3293e-05,  3.6207e-05, -8.5093e-06,  ...,  4.4290e-05,
          9.1024e-06,  2.1792e-05],
        [-2.8084e-05, -2.0050e-05,  4.7595e-06,  ..., -2.3692e-05,
         -4.7343e-06, -1.2003e-05],
        [-5.1376e-05, -3.6865e-05,  8.2400e-06,  ..., -4.3239e-05,
         -7.5523e-06, -2.1248e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2131e-08, 2.8160e-08, 3.5331e-08,  ..., 1.1648e-08, 8.8129e-08,
         1.1472e-08],
        [4.0780e-11, 2.2302e-11, 2.3372e-12,  ..., 2.5547e-11, 1.4669e-12,
         5.6583e-12],
        [3.3315e-10, 1.8197e-10, 1.0777e-11,  ..., 2.9248e-10, 8.1194e-12,
         7.4114e-11],
        [3.1414e-11, 1.5285e-11, 1.1628e-12,  ..., 2.1464e-11, 7.6288e-13,
         5.0223e-12],
        [1.7494e-10, 9.0657e-11, 6.7364e-12,  ..., 1.2255e-10, 6.1997e-12,
         2.8222e-11]], device='cuda:0')
optimizer state dict: 39.0
lr: [1.9268201613085963e-05, 1.9268201613085963e-05]
scheduler_last_epoch: 39


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   287,   358,  ...,   328,  1704,  3227],
        [    0,   474,   221,  ...,   287, 20640,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0829e-05,  1.5533e-05,  9.6961e-06,  ...,  5.5762e-06,
         -1.4627e-05,  1.9077e-05],
        [-3.0845e-06, -2.4140e-06,  6.9290e-07,  ..., -2.8163e-06,
         -8.9779e-07, -1.2666e-06],
        [-3.3230e-06, -2.6077e-06,  7.4878e-07,  ..., -3.0249e-06,
         -9.6858e-07, -1.3635e-06],
        [-3.2485e-06, -2.5481e-06,  7.3388e-07,  ..., -2.9653e-06,
         -9.4250e-07, -1.3262e-06],
        [-7.0632e-06, -5.5134e-06,  1.5870e-06,  ..., -6.4075e-06,
         -2.0564e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1679291725158691


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[   0,  365, 8790,  ..., 1172, 8806,  266],
        [   0,  278, 4191,  ...,  381, 3020,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0368e-05,  2.7379e-05, -2.2418e-05,  ...,  1.5417e-05,
         -9.0728e-06,  8.0694e-06],
        [-6.1542e-06, -4.7982e-06,  1.4119e-06,  ..., -5.6177e-06,
         -1.7881e-06, -2.5406e-06],
        [-6.6310e-06, -5.1856e-06,  1.5236e-06,  ..., -6.0499e-06,
         -1.9297e-06, -2.7344e-06],
        [-6.4969e-06, -5.0813e-06,  1.4976e-06,  ..., -5.9307e-06,
         -1.8850e-06, -2.6748e-06],
        [-1.4067e-05, -1.0967e-05,  3.2187e-06,  ..., -1.2785e-05,
         -4.0829e-06, -5.7667e-06]], device='cuda:0')
Loss: 1.1555147171020508


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[    0,  7030,   631,  ..., 34748,    12,   298],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4309e-05,  3.0979e-05, -2.1772e-05,  ...,  1.8483e-05,
         -2.8911e-05,  1.8882e-05],
        [-9.2238e-06, -7.2122e-06,  2.1309e-06,  ..., -8.3894e-06,
         -2.6971e-06, -3.8221e-06],
        [ 4.4890e-04,  3.2870e-04, -9.1787e-05,  ...,  4.3209e-04,
          1.5279e-04,  2.0754e-04],
        [-9.7603e-06, -7.6443e-06,  2.2613e-06,  ..., -8.8811e-06,
         -2.8461e-06, -4.0382e-06],
        [-2.1040e-05, -1.6451e-05,  4.8503e-06,  ..., -1.9103e-05,
         -6.1393e-06, -8.6725e-06]], device='cuda:0')
Loss: 1.1617859601974487


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[   0,  259, 6887,  ..., 1400,  292,  474],
        [   0,   12, 3518,  ..., 1580, 2573,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2955e-05,  3.7945e-05, -3.5148e-05,  ...,  3.1859e-05,
         -3.9704e-05,  1.4361e-05],
        [-1.2323e-05, -9.6262e-06,  2.8275e-06,  ..., -1.1176e-05,
         -3.6135e-06, -5.0962e-06],
        [ 4.4555e-04,  3.2609e-04, -9.1034e-05,  ...,  4.2908e-04,
          1.5180e-04,  2.0616e-04],
        [-1.3053e-05, -1.0207e-05,  3.0026e-06,  ..., -1.1832e-05,
         -3.8147e-06, -5.3942e-06],
        [-2.8163e-05, -2.2024e-05,  6.4522e-06,  ..., -2.5481e-05,
         -8.2403e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.1508692502975464


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[    0,   957,  1357,  ..., 26179,   287,  6458],
        [    0,   287,   552,  ...,  7407,  2401,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1512e-05,  2.3401e-05, -5.5567e-05,  ...,  3.1128e-05,
         -4.7534e-05,  1.4361e-05],
        [-1.5423e-05, -1.2025e-05,  3.5353e-06,  ..., -1.3947e-05,
         -4.5262e-06, -6.3702e-06],
        [ 4.4221e-04,  3.2350e-04, -9.0270e-05,  ...,  4.2610e-04,
          1.5082e-04,  2.0479e-04],
        [-1.6332e-05, -1.2755e-05,  3.7551e-06,  ..., -1.4767e-05,
         -4.7833e-06, -6.7428e-06],
        [-3.5256e-05, -2.7508e-05,  8.0764e-06,  ..., -3.1829e-05,
         -1.0341e-05, -1.4499e-05]], device='cuda:0')
Loss: 1.1641130447387695


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1848e-05,  1.6382e-05, -6.4296e-05,  ...,  2.2390e-05,
         -6.6468e-05,  1.4346e-05],
        [-1.8492e-05, -1.4424e-05,  4.2357e-06,  ..., -1.6689e-05,
         -5.4091e-06, -7.6294e-06],
        [ 4.3886e-04,  3.2087e-04, -8.9503e-05,  ...,  4.2310e-04,
          1.4985e-04,  2.0342e-04],
        [-1.9625e-05, -1.5333e-05,  4.5076e-06,  ..., -1.7717e-05,
         -5.7295e-06, -8.0913e-06],
        [-4.2349e-05, -3.3051e-05,  9.7007e-06,  ..., -3.8177e-05,
         -1.2383e-05, -1.7405e-05]], device='cuda:0')
Loss: 1.1446154117584229


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,   12,  287,  ...,   12, 5576,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2801e-05,  2.9711e-05, -7.9787e-05,  ...,  2.4427e-05,
         -5.2695e-05,  3.8658e-05],
        [-2.1592e-05, -1.6868e-05,  4.9546e-06,  ..., -1.9506e-05,
         -6.3106e-06, -8.8811e-06],
        [ 4.3555e-04,  3.1825e-04, -8.8732e-05,  ...,  4.2008e-04,
          1.4888e-04,  2.0207e-04],
        [-2.2888e-05, -1.7911e-05,  5.2676e-06,  ..., -2.0683e-05,
         -6.6794e-06, -9.4101e-06],
        [-4.9353e-05, -3.8564e-05,  1.1325e-05,  ..., -4.4525e-05,
         -1.4424e-05, -2.0221e-05]], device='cuda:0')
Loss: 1.166529893875122


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,    20,     9,  ...,    12,  2212, 24950],
        [    0,   380,  8157,  ...,   943,   352,  2278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0888e-05,  2.1690e-05, -8.5009e-05,  ...,  4.8831e-05,
         -6.0134e-05,  2.8797e-05],
        [-2.4647e-05, -1.9267e-05,  5.6773e-06,  ..., -2.2277e-05,
         -7.2084e-06, -1.0133e-05],
        [ 4.3224e-04,  3.1566e-04, -8.7953e-05,  ...,  4.1708e-04,
          1.4791e-04,  2.0072e-04],
        [-2.6152e-05, -2.0474e-05,  6.0387e-06,  ..., -2.3633e-05,
         -7.6331e-06, -1.0744e-05],
        [-5.6326e-05, -4.4048e-05,  1.2971e-05,  ..., -5.0843e-05,
         -1.6466e-05, -2.3082e-05]], device='cuda:0')
Loss: 1.160436987876892
Graident accumulation at epoch 0, step 319, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0077,  0.0032,  ..., -0.0095, -0.0021, -0.0339],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0013],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4876e-05, -2.7616e-06, -2.5923e-05,  ...,  2.2013e-05,
         -5.0308e-05,  2.5417e-06],
        [-2.4172e-05, -1.7586e-05,  4.1748e-06,  ..., -2.0720e-05,
         -4.2994e-06, -1.0423e-05],
        [ 8.2188e-05,  6.4152e-05, -1.6454e-05,  ...,  8.1570e-05,
          2.2983e-05,  3.9684e-05],
        [-2.7891e-05, -2.0092e-05,  4.8874e-06,  ..., -2.3686e-05,
         -5.0242e-06, -1.1877e-05],
        [-5.1871e-05, -3.7584e-05,  8.7132e-06,  ..., -4.3999e-05,
         -8.4437e-06, -2.1431e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2089e-08, 2.8132e-08, 3.5303e-08,  ..., 1.1639e-08, 8.8045e-08,
         1.1461e-08],
        [4.1346e-11, 2.2651e-11, 2.3671e-12,  ..., 2.6018e-11, 1.5174e-12,
         5.7554e-12],
        [5.1965e-10, 2.8143e-10, 1.8502e-11,  ..., 4.6614e-10, 2.9989e-11,
         1.1433e-10],
        [3.2067e-11, 1.5689e-11, 1.1982e-12,  ..., 2.2001e-11, 8.2038e-13,
         5.1327e-12],
        [1.7794e-10, 9.2507e-11, 6.8980e-12,  ..., 1.2502e-10, 6.4646e-12,
         2.8726e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
Epoch 0 | Batch 319/1048 | Training PPL: 10758.978594143282 | time 30.336763381958008
Saving checkpoint at epoch 0, step 319, batch 319
Epoch 0 | Validation PPL: 10.067967359066001 | Learning rate: 1.9221075944084176e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319, AFTER epoch 0, step 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  2372,  1319,  ...,  1253,   292, 34166],
        [    0,   494,   221,  ...,   298,  1062,  4923]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-3.4876e-05, -2.7616e-06, -2.5923e-05,  ...,  2.2013e-05,
         -5.0308e-05,  2.5417e-06],
        [-2.4172e-05, -1.7586e-05,  4.1748e-06,  ..., -2.0720e-05,
         -4.2994e-06, -1.0423e-05],
        [ 8.2188e-05,  6.4152e-05, -1.6454e-05,  ...,  8.1570e-05,
          2.2983e-05,  3.9684e-05],
        [-2.7891e-05, -2.0092e-05,  4.8874e-06,  ..., -2.3686e-05,
         -5.0242e-06, -1.1877e-05],
        [-5.1871e-05, -3.7584e-05,  8.7132e-06,  ..., -4.3999e-05,
         -8.4437e-06, -2.1431e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2089e-08, 2.8132e-08, 3.5303e-08,  ..., 1.1639e-08, 8.8045e-08,
         1.1461e-08],
        [4.1346e-11, 2.2651e-11, 2.3671e-12,  ..., 2.6018e-11, 1.5174e-12,
         5.7554e-12],
        [5.1965e-10, 2.8143e-10, 1.8502e-11,  ..., 4.6614e-10, 2.9989e-11,
         1.1433e-10],
        [3.2067e-11, 1.5689e-11, 1.1982e-12,  ..., 2.2001e-11, 8.2038e-13,
         5.1327e-12],
        [1.7794e-10, 9.2507e-11, 6.8980e-12,  ..., 1.2502e-10, 6.4646e-12,
         2.8726e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9426e-06,  4.0031e-05, -6.4102e-05,  ...,  1.8645e-05,
         -1.5636e-05, -3.2111e-05],
        [-3.0100e-06, -2.3991e-06,  7.3388e-07,  ..., -2.7567e-06,
         -8.9407e-07, -1.2890e-06],
        [-3.2783e-06, -2.6077e-06,  8.0094e-07,  ..., -3.0100e-06,
         -9.7603e-07, -1.4082e-06],
        [-3.2187e-06, -2.5630e-06,  7.8604e-07,  ..., -2.9504e-06,
         -9.5367e-07, -1.3784e-06],
        [-6.8545e-06, -5.4538e-06,  1.6689e-06,  ..., -6.2883e-06,
         -2.0415e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.150023341178894


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,  2734,  2703,  ...,  7851,   280,  1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0340e-05,  3.4363e-05, -4.0503e-05,  ..., -1.1180e-05,
         -2.2267e-05, -3.9125e-05],
        [-6.0797e-06, -4.7982e-06,  1.4193e-06,  ..., -5.4985e-06,
         -1.8030e-06, -2.5406e-06],
        [-6.6161e-06, -5.2303e-06,  1.5497e-06,  ..., -6.0052e-06,
         -1.9670e-06, -2.7716e-06],
        [-6.4671e-06, -5.1111e-06,  1.5162e-06,  ..., -5.8562e-06,
         -1.9148e-06, -2.7046e-06],
        [-1.3918e-05, -1.0997e-05,  3.2485e-06,  ..., -1.2606e-05,
         -4.1425e-06, -5.8264e-06]], device='cuda:0')
Loss: 1.165352702140808


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[    0,   271,  4787,  ...,   292,   494,   221],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0953e-06,  7.4649e-06, -4.8214e-05,  ..., -1.2652e-05,
         -2.0078e-05, -6.7745e-05],
        [-9.1493e-06, -7.2420e-06,  2.1234e-06,  ..., -8.2701e-06,
         -2.6673e-06, -3.7849e-06],
        [-9.9540e-06, -7.8827e-06,  2.3134e-06,  ..., -9.0152e-06,
         -2.9095e-06, -4.1276e-06],
        [-9.7156e-06, -7.7039e-06,  2.2613e-06,  ..., -8.7917e-06,
         -2.8312e-06, -4.0233e-06],
        [-2.0921e-05, -1.6570e-05,  4.8503e-06,  ..., -1.8924e-05,
         -6.1244e-06, -8.6576e-06]], device='cuda:0')
Loss: 1.1557785272598267


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,   287, 16974,  ...,   300,  2283,  4013]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6574e-07, -7.6290e-06, -4.7925e-05,  ..., -1.8508e-06,
         -2.3925e-05, -5.6646e-05],
        [-1.2204e-05, -9.6709e-06,  2.8424e-06,  ..., -1.1012e-05,
         -3.5465e-06, -5.0366e-06],
        [-1.3322e-05, -1.0550e-05,  3.1032e-06,  ..., -1.2025e-05,
         -3.8706e-06, -5.5060e-06],
        [-1.2979e-05, -1.0297e-05,  3.0287e-06,  ..., -1.1712e-05,
         -3.7663e-06, -5.3570e-06],
        [-2.7984e-05, -2.2173e-05,  6.5044e-06,  ..., -2.5243e-05,
         -8.1509e-06, -1.1548e-05]], device='cuda:0')
Loss: 1.1627811193466187


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[   0,   69,  462,  ...,  437,  266,  634],
        [   0, 1890,  278,  ...,  578,   72,  815]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8316e-06, -1.0821e-05, -1.2202e-04,  ..., -6.3317e-06,
         -2.6276e-05, -4.5142e-05],
        [-1.5244e-05, -1.2100e-05,  3.5726e-06,  ..., -1.3784e-05,
         -4.4331e-06, -6.3106e-06],
        [-1.6645e-05, -1.3217e-05,  3.9004e-06,  ..., -1.5050e-05,
         -4.8392e-06, -6.8918e-06],
        [-1.6227e-05, -1.2904e-05,  3.8110e-06,  ..., -1.4678e-05,
         -4.7125e-06, -6.7130e-06],
        [-3.4899e-05, -2.7686e-05,  8.1584e-06,  ..., -3.1531e-05,
         -1.0163e-05, -1.4424e-05]], device='cuda:0')
Loss: 1.1383451223373413


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[   0,  377,  472,  ..., 9256, 3807, 5499],
        [   0, 6203,  352,  ...,  266, 3437,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8316e-06, -9.9882e-06, -1.5343e-04,  ...,  6.4236e-06,
         -4.2174e-05, -6.2712e-05],
        [-1.8284e-05, -1.4514e-05,  4.2990e-06,  ..., -1.6555e-05,
         -5.3048e-06, -7.5772e-06],
        [-1.9968e-05, -1.5855e-05,  4.6939e-06,  ..., -1.8075e-05,
         -5.7891e-06, -8.2701e-06],
        [-1.9476e-05, -1.5482e-05,  4.5858e-06,  ..., -1.7643e-05,
         -5.6401e-06, -8.0615e-06],
        [-4.1842e-05, -3.3200e-05,  9.8124e-06,  ..., -3.7849e-05,
         -1.2159e-05, -1.7315e-05]], device='cuda:0')
Loss: 1.1594507694244385


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[    0, 47831,   266,  ...,    66,    17, 20005],
        [    0,  1016,  1387,  ..., 12156, 14838,  3550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0103e-05,  5.7777e-06, -1.4872e-04,  ..., -9.5980e-06,
         -3.8852e-05, -5.8078e-05],
        [-2.1324e-05, -1.6898e-05,  4.9993e-06,  ..., -1.9312e-05,
         -6.1616e-06, -8.8364e-06],
        [-2.3276e-05, -1.8463e-05,  5.4613e-06,  ..., -2.1085e-05,
         -6.7241e-06, -9.6485e-06],
        [-2.2724e-05, -1.8045e-05,  5.3383e-06,  ..., -2.0593e-05,
         -6.5528e-06, -9.4101e-06],
        [-4.8786e-05, -3.8683e-05,  1.1414e-05,  ..., -4.4167e-05,
         -1.4126e-05, -2.0206e-05]], device='cuda:0')
Loss: 1.1714296340942383


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  341,  298,  ...,  298, 1304,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8000e-05,  3.3076e-05, -1.2795e-04,  ..., -2.1903e-05,
         -3.6947e-05, -7.2954e-05],
        [-2.4348e-05, -1.9297e-05,  5.7332e-06,  ..., -2.2039e-05,
         -7.0408e-06, -1.0088e-05],
        [-2.6599e-05, -2.1100e-05,  6.2697e-06,  ..., -2.4095e-05,
         -7.6927e-06, -1.1019e-05],
        [-2.5943e-05, -2.0608e-05,  6.1244e-06,  ..., -2.3514e-05,
         -7.4878e-06, -1.0744e-05],
        [-5.5760e-05, -4.4227e-05,  1.3113e-05,  ..., -5.0485e-05,
         -1.6153e-05, -2.3097e-05]], device='cuda:0')
Loss: 1.1482067108154297
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0077,  0.0032,  ..., -0.0095, -0.0022, -0.0339],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0013],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4189e-05,  8.2211e-07, -3.6126e-05,  ...,  1.7621e-05,
         -4.8972e-05, -5.0079e-06],
        [-2.4190e-05, -1.7757e-05,  4.3306e-06,  ..., -2.0852e-05,
         -4.5735e-06, -1.0390e-05],
        [ 7.1309e-05,  5.5627e-05, -1.4181e-05,  ...,  7.1003e-05,
          1.9916e-05,  3.4614e-05],
        [-2.7696e-05, -2.0144e-05,  5.0111e-06,  ..., -2.3669e-05,
         -5.2705e-06, -1.1764e-05],
        [-5.2260e-05, -3.8248e-05,  9.1532e-06,  ..., -4.4648e-05,
         -9.2146e-06, -2.1598e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2048e-08, 2.8105e-08, 3.5284e-08,  ..., 1.1628e-08, 8.7958e-08,
         1.1455e-08],
        [4.1898e-11, 2.3001e-11, 2.3976e-12,  ..., 2.6478e-11, 1.5655e-12,
         5.8514e-12],
        [5.1984e-10, 2.8159e-10, 1.8523e-11,  ..., 4.6625e-10, 3.0019e-11,
         1.1434e-10],
        [3.2708e-11, 1.6098e-11, 1.2345e-12,  ..., 2.2532e-11, 8.7563e-13,
         5.2430e-12],
        [1.8087e-10, 9.4370e-11, 7.0630e-12,  ..., 1.2744e-10, 6.7191e-12,
         2.9231e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.9172541214186228e-05, 1.9172541214186228e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0,   401,   953,  ..., 10914,   554,  2360],
        [    0,  8353,  1842,  ...,    38,   643,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0236e-05,  6.4969e-06, -2.3425e-05,  ..., -2.1681e-05,
         -7.3310e-06, -2.0154e-06],
        [-2.9802e-06, -2.4289e-06,  7.1526e-07,  ..., -2.7418e-06,
         -8.3819e-07, -1.2740e-06],
        [-3.2932e-06, -2.6822e-06,  7.8604e-07,  ..., -3.0249e-06,
         -9.2387e-07, -1.4082e-06],
        [-3.1888e-06, -2.5928e-06,  7.6368e-07,  ..., -2.9206e-06,
         -8.9407e-07, -1.3635e-06],
        [-6.8247e-06, -5.5730e-06,  1.6317e-06,  ..., -6.2585e-06,
         -1.9222e-06, -2.9057e-06]], device='cuda:0')
Loss: 1.1427762508392334


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[   0,  843, 3365,  ..., 1136, 1615,  292],
        [   0, 1235,   14,  ..., 3301,  549,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8967e-05,  4.1453e-06,  5.6587e-06,  ..., -1.2405e-05,
          1.5528e-05,  1.6805e-05],
        [-6.0201e-06, -4.8578e-06,  1.4417e-06,  ..., -5.4687e-06,
         -1.6727e-06, -2.5257e-06],
        [-6.6161e-06, -5.3346e-06,  1.5795e-06,  ..., -6.0052e-06,
         -1.8366e-06, -2.7716e-06],
        [-6.4373e-06, -5.1856e-06,  1.5385e-06,  ..., -5.8413e-06,
         -1.7844e-06, -2.6971e-06],
        [-1.3769e-05, -1.1116e-05,  3.2857e-06,  ..., -1.2517e-05,
         -3.8296e-06, -5.7518e-06]], device='cuda:0')
Loss: 1.1633317470550537


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0,    12,  4567,  ...,  4154,  1799, 11883],
        [    0,  1099,   644,  ...,  5481,    14,  8782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9076e-05, -1.2212e-05,  5.5641e-06,  ..., -1.4440e-05,
          1.0618e-05,  1.7339e-05],
        [-9.0450e-06, -7.2718e-06,  2.1718e-06,  ..., -8.1807e-06,
         -2.4848e-06, -3.7625e-06],
        [-9.9093e-06, -7.9721e-06,  2.3767e-06,  ..., -8.9705e-06,
         -2.7195e-06, -4.1202e-06],
        [-9.6858e-06, -7.7784e-06,  2.3209e-06,  ..., -8.7470e-06,
         -2.6524e-06, -4.0233e-06],
        [-2.0683e-05, -1.6630e-05,  4.9546e-06,  ..., -1.8716e-05,
         -5.6848e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.1465392112731934


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   591,   688,  ...,   271,  3390,    12],
        [    0,   278, 11554,  ...,  4713,  1039, 17088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6204e-05, -2.8336e-05,  3.5506e-06,  ...,  3.0223e-05,
         -3.4897e-06,  1.2332e-05],
        [-1.2085e-05, -9.7156e-06,  2.8983e-06,  ..., -1.0908e-05,
         -3.3230e-06, -5.0142e-06],
        [-1.3247e-05, -1.0654e-05,  3.1739e-06,  ..., -1.1966e-05,
         -3.6396e-06, -5.4911e-06],
        [-1.2919e-05, -1.0371e-05,  3.0920e-06,  ..., -1.1638e-05,
         -3.5390e-06, -5.3495e-06],
        [-2.7597e-05, -2.2173e-05,  6.6012e-06,  ..., -2.4915e-05,
         -7.5847e-06, -1.1414e-05]], device='cuda:0')
Loss: 1.1538869142532349


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[   0, 1034, 5599,  ...,  259,  586, 1403],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0969e-05, -3.0623e-05, -1.7513e-05,  ...,  7.2525e-06,
         -6.2492e-07, -1.2815e-05],
        [-1.5080e-05, -1.2130e-05,  3.6210e-06,  ..., -1.3635e-05,
         -4.1239e-06, -6.2808e-06],
        [-1.6510e-05, -1.3277e-05,  3.9637e-06,  ..., -1.4946e-05,
         -4.5113e-06, -6.8694e-06],
        [-1.6153e-05, -1.2964e-05,  3.8743e-06,  ..., -1.4588e-05,
         -4.3996e-06, -6.7130e-06],
        [-3.4481e-05, -2.7686e-05,  8.2552e-06,  ..., -3.1173e-05,
         -9.4175e-06, -1.4305e-05]], device='cuda:0')
Loss: 1.160133719444275


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[   0, 1927,  287,  ..., 1027,  271,  266],
        [   0,  266, 3574,  ..., 7052, 3829,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4642e-06, -7.1099e-05, -7.2458e-06,  ..., -4.4788e-05,
         -5.4056e-05, -3.6022e-05],
        [-1.8105e-05, -1.4573e-05,  4.3139e-06,  ..., -1.6361e-05,
         -4.9211e-06, -7.5474e-06],
        [-1.9819e-05, -1.5944e-05,  4.7237e-06,  ..., -1.7926e-05,
         -5.3830e-06, -8.2552e-06],
        [-1.9372e-05, -1.5572e-05,  4.6156e-06,  ..., -1.7494e-05,
         -5.2452e-06, -8.0615e-06],
        [-4.1366e-05, -3.3259e-05,  9.8348e-06,  ..., -3.7402e-05,
         -1.1228e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.1437623500823975


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0, 1549,  824,  ..., 3609,  720,  417],
        [   0, 2700, 5221,  ...,  298,  259,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0356e-05, -6.9140e-05,  8.1686e-08,  ..., -5.0539e-05,
         -9.3217e-05, -2.0110e-05],
        [-2.1115e-05, -1.7017e-05,  5.0366e-06,  ..., -1.9088e-05,
         -5.7593e-06, -8.8289e-06],
        [-2.3097e-05, -1.8597e-05,  5.5060e-06,  ..., -2.0891e-05,
         -6.2920e-06, -9.6411e-06],
        [-2.2590e-05, -1.8179e-05,  5.3868e-06,  ..., -2.0415e-05,
         -6.1393e-06, -9.4250e-06],
        [-4.8190e-05, -3.8773e-05,  1.1466e-05,  ..., -4.3571e-05,
         -1.3128e-05, -2.0072e-05]], device='cuda:0')
Loss: 1.1526153087615967


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  2853, 21042,  ...,  4120,   607, 11176],
        [    0,    15,  2537,  ...,    14,  3544,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0356e-05, -5.5075e-05,  1.4536e-05,  ..., -5.3127e-05,
         -9.7677e-05, -2.9332e-05],
        [-2.4125e-05, -1.9431e-05,  5.7518e-06,  ..., -2.1815e-05,
         -6.5789e-06, -1.0110e-05],
        [-2.6390e-05, -2.1249e-05,  6.2920e-06,  ..., -2.3872e-05,
         -7.1898e-06, -1.1049e-05],
        [-2.5809e-05, -2.0772e-05,  6.1542e-06,  ..., -2.3320e-05,
         -7.0147e-06, -1.0796e-05],
        [-5.5075e-05, -4.4316e-05,  1.3106e-05,  ..., -4.9800e-05,
         -1.5005e-05, -2.3022e-05]], device='cuda:0')
Loss: 1.173558235168457
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9734e-05, -4.7676e-06, -3.1059e-05,  ...,  1.0546e-05,
         -5.3842e-05, -7.4403e-06],
        [-2.4183e-05, -1.7925e-05,  4.4728e-06,  ..., -2.0948e-05,
         -4.7741e-06, -1.0362e-05],
        [ 6.1539e-05,  4.7939e-05, -1.2134e-05,  ...,  6.1516e-05,
          1.7205e-05,  3.0048e-05],
        [-2.7507e-05, -2.0207e-05,  5.1254e-06,  ..., -2.3634e-05,
         -5.4450e-06, -1.1667e-05],
        [-5.2541e-05, -3.8855e-05,  9.5484e-06,  ..., -4.5163e-05,
         -9.7937e-06, -2.1740e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2006e-08, 2.8080e-08, 3.5249e-08,  ..., 1.1619e-08, 8.7880e-08,
         1.1445e-08],
        [4.2438e-11, 2.3356e-11, 2.4283e-12,  ..., 2.6927e-11, 1.6072e-12,
         5.9477e-12],
        [5.2002e-10, 2.8176e-10, 1.8544e-11,  ..., 4.6636e-10, 3.0040e-11,
         1.1434e-10],
        [3.3341e-11, 1.6513e-11, 1.2711e-12,  ..., 2.3053e-11, 9.2396e-13,
         5.3543e-12],
        [1.8372e-10, 9.6240e-11, 7.2277e-12,  ..., 1.2979e-10, 6.9375e-12,
         2.9732e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[    0,  3047,  4878,  ...,   352, 10854, 34025],
        [    0,   271,   266,  ...,  2805,   607, 10848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0763e-05, -1.4276e-05, -1.7540e-05,  ...,  1.2444e-06,
         -1.2755e-05, -2.0840e-05],
        [-2.9802e-06, -2.4140e-06,  7.0035e-07,  ..., -2.7269e-06,
         -7.3761e-07, -1.2591e-06],
        [-3.2634e-06, -2.6375e-06,  7.6368e-07,  ..., -2.9802e-06,
         -8.0466e-07, -1.3709e-06],
        [-3.2037e-06, -2.5928e-06,  7.4878e-07,  ..., -2.9206e-06,
         -7.8604e-07, -1.3486e-06],
        [-6.8545e-06, -5.5134e-06,  1.5944e-06,  ..., -6.2287e-06,
         -1.6838e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.136786699295044


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,  446, 1845,  ...,  422,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0045e-05, -2.3772e-05, -2.4643e-05,  ...,  3.8696e-06,
         -2.2888e-05, -1.0698e-05],
        [-6.0201e-06, -4.8727e-06,  1.4417e-06,  ..., -5.4836e-06,
         -1.5348e-06, -2.5332e-06],
        [-6.5565e-06, -5.3048e-06,  1.5683e-06,  ..., -5.9605e-06,
         -1.6689e-06, -2.7493e-06],
        [-6.4224e-06, -5.2005e-06,  1.5385e-06,  ..., -5.8413e-06,
         -1.6317e-06, -2.6971e-06],
        [-1.3709e-05, -1.1057e-05,  3.2634e-06,  ..., -1.2428e-05,
         -3.4869e-06, -5.7369e-06]], device='cuda:0')
Loss: 1.136494755744934


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[   0,  287, 3284,  ...,  221,  493,  221],
        [   0, 2426,  699,  ...,  221, 1551,  720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0496e-05, -3.4446e-05, -2.6795e-05,  ...,  3.5085e-05,
         -2.5596e-05,  6.2119e-06],
        [ 7.7375e-05,  5.7445e-05, -2.5385e-05,  ...,  7.6153e-05,
          2.2072e-05,  3.8478e-05],
        [-9.8795e-06, -7.9870e-06,  2.3507e-06,  ..., -9.0003e-06,
         -2.5220e-06, -4.1723e-06],
        [-9.6411e-06, -7.7933e-06,  2.2985e-06,  ..., -8.7768e-06,
         -2.4550e-06, -4.0755e-06],
        [-2.0653e-05, -1.6659e-05,  4.8950e-06,  ..., -1.8775e-05,
         -5.2676e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.158063530921936


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[    0,    14,  3609,  ...,   298,   413,    29],
        [    0,    14,   381,  ...,   278,   269, 10376]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7879e-05, -1.7735e-05,  2.3450e-07,  ...,  4.7362e-05,
         -5.1913e-05,  4.3024e-05],
        [ 7.4335e-05,  5.5001e-05, -2.4681e-05,  ...,  7.3441e-05,
          2.1293e-05,  3.7189e-05],
        [-1.3173e-05, -1.0639e-05,  3.1143e-06,  ..., -1.1951e-05,
         -3.3639e-06, -5.5656e-06],
        [-1.2860e-05, -1.0401e-05,  3.0473e-06,  ..., -1.1668e-05,
         -3.2820e-06, -5.4389e-06],
        [-2.7448e-05, -2.2143e-05,  6.4671e-06,  ..., -2.4855e-05,
         -7.0110e-06, -1.1578e-05]], device='cuda:0')
Loss: 1.140223503112793


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[   0, 7692,   12,  ...,  266, 2042,  278],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8209e-05, -8.6105e-06, -2.4185e-05,  ...,  4.5886e-05,
         -7.5426e-05,  1.8353e-05],
        [ 7.1295e-05,  5.2572e-05, -2.3962e-05,  ...,  7.0669e-05,
          2.0522e-05,  3.5907e-05],
        [-1.6481e-05, -1.3292e-05,  3.8967e-06,  ..., -1.4961e-05,
         -4.1984e-06, -6.9588e-06],
        [-1.6108e-05, -1.3009e-05,  3.8184e-06,  ..., -1.4633e-05,
         -4.1053e-06, -6.8098e-06],
        [-3.4273e-05, -2.7627e-05,  8.0839e-06,  ..., -3.1084e-05,
         -8.7395e-06, -1.4454e-05]], device='cuda:0')
Loss: 1.1479220390319824


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 3630, 2199,  ..., 4157,   27, 4765],
        [   0, 3352,  259,  ..., 3565,   12,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9463e-05, -3.0556e-05, -1.9705e-05,  ...,  6.6254e-05,
         -1.1102e-04,  1.9192e-05],
        [ 6.8270e-05,  5.0113e-05, -2.3247e-05,  ...,  6.7912e-05,
          1.9740e-05,  3.4656e-05],
        [-1.9759e-05, -1.5944e-05,  4.6752e-06,  ..., -1.7956e-05,
         -5.0440e-06, -8.3148e-06],
        [-1.9297e-05, -1.5602e-05,  4.5784e-06,  ..., -1.7539e-05,
         -4.9286e-06, -8.1286e-06],
        [-4.1097e-05, -3.3170e-05,  9.6932e-06,  ..., -3.7313e-05,
         -1.0505e-05, -1.7270e-05]], device='cuda:0')
Loss: 1.1535331010818481


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,  1266,  2257,  ..., 27146,  1141,  1196]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5000e-05, -2.8848e-05, -5.8260e-05,  ...,  4.2408e-05,
         -1.1076e-04,  7.6904e-07],
        [ 6.5215e-05,  4.7625e-05, -2.2543e-05,  ...,  6.5171e-05,
          1.8953e-05,  3.3411e-05],
        [-2.3067e-05, -1.8641e-05,  5.4389e-06,  ..., -2.0936e-05,
         -5.8971e-06, -9.6634e-06],
        [-2.2516e-05, -1.8224e-05,  5.3234e-06,  ..., -2.0444e-05,
         -5.7556e-06, -9.4399e-06],
        [-4.7922e-05, -3.8713e-05,  1.1265e-05,  ..., -4.3452e-05,
         -1.2264e-05, -2.0042e-05]], device='cuda:0')
Loss: 1.1427046060562134


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[    0, 11325,   278,  ...,   446,  1869,   642],
        [    0,  3306,  4057,  ...,   287,   266,  1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7405e-05, -5.2664e-05, -6.9374e-05,  ...,  6.3066e-05,
         -9.9826e-05,  5.6829e-05],
        [ 6.2190e-05,  4.5166e-05, -2.1820e-05,  ...,  6.2444e-05,
          1.8201e-05,  3.2182e-05],
        [-2.6360e-05, -2.1309e-05,  6.2250e-06,  ..., -2.3901e-05,
         -6.7167e-06, -1.0997e-05],
        [-2.5734e-05, -2.0832e-05,  6.0908e-06,  ..., -2.3335e-05,
         -6.5565e-06, -1.0744e-05],
        [-5.4747e-05, -4.4256e-05,  1.2890e-05,  ..., -4.9621e-05,
         -1.3970e-05, -2.2814e-05]], device='cuda:0')
Loss: 1.1566780805587769
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3501e-05, -9.5573e-06, -3.4891e-05,  ...,  1.5798e-05,
         -5.8441e-05, -1.0133e-06],
        [-1.5546e-05, -1.1616e-05,  1.8435e-06,  ..., -1.2609e-05,
         -2.4766e-06, -6.1074e-06],
        [ 5.2749e-05,  4.1014e-05, -1.0298e-05,  ...,  5.2974e-05,
          1.4813e-05,  2.5943e-05],
        [-2.7330e-05, -2.0269e-05,  5.2220e-06,  ..., -2.3604e-05,
         -5.5561e-06, -1.1575e-05],
        [-5.2762e-05, -3.9395e-05,  9.8825e-06,  ..., -4.5609e-05,
         -1.0211e-05, -2.1848e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1968e-08, 2.8055e-08, 3.5218e-08,  ..., 1.1611e-08, 8.7802e-08,
         1.1437e-08],
        [4.6263e-11, 2.5372e-11, 2.9020e-12,  ..., 3.0800e-11, 1.9369e-12,
         6.9775e-12],
        [5.2019e-10, 2.8194e-10, 1.8564e-11,  ..., 4.6646e-10, 3.0055e-11,
         1.1435e-10],
        [3.3970e-11, 1.6930e-11, 1.3069e-12,  ..., 2.3575e-11, 9.6602e-13,
         5.4644e-12],
        [1.8654e-10, 9.8102e-11, 7.3866e-12,  ..., 1.3212e-10, 7.1258e-12,
         3.0222e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.90712744520069e-05, 1.90712744520069e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   259,  2180,  ...,   638,  1615,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8010e-06, -4.5678e-06,  0.0000e+00,  ...,  2.0900e-06,
         -2.5197e-05,  1.1017e-05],
        [-3.0249e-06, -2.4587e-06,  6.9290e-07,  ..., -2.7418e-06,
         -7.1526e-07, -1.2815e-06],
        [-3.2187e-06, -2.6226e-06,  7.3761e-07,  ..., -2.9206e-06,
         -7.5996e-07, -1.3560e-06],
        [-3.1739e-06, -2.5779e-06,  7.3016e-07,  ..., -2.8759e-06,
         -7.4878e-07, -1.3337e-06],
        [-6.7949e-06, -5.5134e-06,  1.5572e-06,  ..., -6.1691e-06,
         -1.6019e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.1493808031082153


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0, 10215,   408,  ...,  6071,   360,  1317],
        [    0,  3125,   271,  ...,  1041,  1032,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6761e-05, -3.4822e-05, -2.2931e-05,  ..., -2.7492e-06,
         -3.1136e-05,  2.2907e-05],
        [-6.0797e-06, -4.9770e-06,  1.4380e-06,  ..., -5.5134e-06,
         -1.4529e-06, -2.6003e-06],
        [ 1.7033e-04,  1.4029e-04, -4.2143e-05,  ...,  1.6668e-04,
          2.5167e-05,  8.9599e-05],
        [-6.3628e-06, -5.2005e-06,  1.5087e-06,  ..., -5.7667e-06,
         -1.5162e-06, -2.7046e-06],
        [-1.3649e-05, -1.1146e-05,  3.2187e-06,  ..., -1.2368e-05,
         -3.2485e-06, -5.7966e-06]], device='cuda:0')
Loss: 1.1572803258895874


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0,  2380,  2667,  ...,    14,   381,  5621],
        [    0,    12, 32425,  ...,   389,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0742e-05, -4.5960e-05, -3.0924e-05,  ...,  1.2525e-05,
         -3.6478e-05,  8.0402e-06],
        [-9.1344e-06, -7.4506e-06,  2.1681e-06,  ..., -8.2552e-06,
         -2.1718e-06, -3.9116e-06],
        [ 1.6706e-04,  1.3765e-04, -4.1365e-05,  ...,  1.6374e-04,
          2.4395e-05,  8.8198e-05],
        [-9.5367e-06, -7.7635e-06,  2.2687e-06,  ..., -8.6129e-06,
         -2.2613e-06, -4.0606e-06],
        [-2.0474e-05, -1.6659e-05,  4.8503e-06,  ..., -1.8507e-05,
         -4.8578e-06, -8.7172e-06]], device='cuda:0')
Loss: 1.1692075729370117


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[   0,   12,  344,  ..., 2337, 1122,  408],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7314e-07, -6.1729e-05, -4.6321e-05,  ...,  3.2763e-05,
         -3.7847e-05,  2.7976e-05],
        [-1.2189e-05, -9.9093e-06,  2.9132e-06,  ..., -1.1027e-05,
         -2.9020e-06, -5.2229e-06],
        [ 1.6378e-04,  1.3501e-04, -4.0568e-05,  ...,  1.6077e-04,
          2.3617e-05,  8.6797e-05],
        [-1.2740e-05, -1.0341e-05,  3.0510e-06,  ..., -1.1504e-05,
         -3.0212e-06, -5.4240e-06],
        [-2.7329e-05, -2.2173e-05,  6.5118e-06,  ..., -2.4676e-05,
         -6.4820e-06, -1.1638e-05]], device='cuda:0')
Loss: 1.1590855121612549


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[    0,   456,    17,  ...,  1553, 29477,  2713],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8638e-06, -3.2559e-05, -1.1256e-05,  ...,  1.1801e-05,
         -3.0655e-05,  1.6026e-05],
        [-1.5274e-05, -1.2398e-05,  3.6247e-06,  ..., -1.3813e-05,
         -3.6471e-06, -6.5491e-06],
        [ 1.6051e-04,  1.3236e-04, -3.9811e-05,  ...,  1.5782e-04,
          2.2823e-05,  8.5389e-05],
        [-1.5914e-05, -1.2904e-05,  3.7849e-06,  ..., -1.4365e-05,
         -3.7886e-06, -6.7875e-06],
        [-3.4213e-05, -2.7716e-05,  8.0988e-06,  ..., -3.0875e-05,
         -8.1435e-06, -1.4588e-05]], device='cuda:0')
Loss: 1.1543818712234497


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  2310,   292,  ...,   462,   508,   586],
        [    0, 17900,   554,  ...,   266,  7912,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7735e-06, -2.3301e-05, -1.3911e-05,  ...,  6.2767e-06,
         -5.4896e-05,  2.2682e-05],
        [-1.8314e-05, -1.4871e-05,  4.3623e-06,  ..., -1.6555e-05,
         -4.3921e-06, -7.8604e-06],
        [ 1.5724e-04,  1.2971e-04, -3.9022e-05,  ...,  1.5487e-04,
          2.2026e-05,  8.3988e-05],
        [-1.9088e-05, -1.5482e-05,  4.5560e-06,  ..., -1.7226e-05,
         -4.5635e-06, -8.1509e-06],
        [-4.1038e-05, -3.3259e-05,  9.7528e-06,  ..., -3.7044e-05,
         -9.8199e-06, -1.7524e-05]], device='cuda:0')
Loss: 1.1376736164093018


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,    12,   287,  ...,  4626,    27,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9672e-05, -1.5131e-05, -3.3367e-05,  ..., -1.1384e-05,
         -5.3136e-05,  4.7893e-05],
        [-2.1383e-05, -1.7405e-05,  5.1074e-06,  ..., -1.9312e-05,
         -5.1409e-06, -9.1866e-06],
        [ 1.5398e-04,  1.2702e-04, -3.8232e-05,  ...,  1.5195e-04,
          2.1233e-05,  8.2580e-05],
        [-2.2262e-05, -1.8105e-05,  5.3272e-06,  ..., -2.0072e-05,
         -5.3346e-06, -9.5218e-06],
        [-4.7833e-05, -3.8862e-05,  1.1399e-05,  ..., -4.3154e-05,
         -1.1474e-05, -2.0459e-05]], device='cuda:0')
Loss: 1.1467034816741943


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[   0,  328, 6379,  ...,  287, 1342,    9],
        [   0,  401, 9370,  ...,    9,  287,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7306e-05, -3.9806e-05, -4.7026e-05,  ..., -2.0208e-05,
         -6.9002e-05,  6.9521e-05],
        [-2.4453e-05, -1.9878e-05,  5.8785e-06,  ..., -2.2069e-05,
         -5.8673e-06, -1.0513e-05],
        [ 1.5070e-04,  1.2439e-04, -3.7409e-05,  ...,  1.4902e-04,
          2.0458e-05,  8.1172e-05],
        [-2.5481e-05, -2.0698e-05,  6.1393e-06,  ..., -2.2948e-05,
         -6.0946e-06, -1.0908e-05],
        [-5.4687e-05, -4.4376e-05,  1.3120e-05,  ..., -4.9293e-05,
         -1.3098e-05, -2.3410e-05]], device='cuda:0')
Loss: 1.134730339050293
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0271,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4882e-05, -1.2582e-05, -3.6104e-05,  ...,  1.2197e-05,
         -5.9497e-05,  6.0401e-06],
        [-1.6437e-05, -1.2442e-05,  2.2470e-06,  ..., -1.3555e-05,
         -2.8156e-06, -6.5480e-06],
        [ 6.2544e-05,  4.9352e-05, -1.3009e-05,  ...,  6.2578e-05,
          1.5378e-05,  3.1466e-05],
        [-2.7145e-05, -2.0312e-05,  5.3137e-06,  ..., -2.3538e-05,
         -5.6100e-06, -1.1508e-05],
        [-5.2955e-05, -3.9893e-05,  1.0206e-05,  ..., -4.5977e-05,
         -1.0500e-05, -2.2004e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1929e-08, 2.8028e-08, 3.5185e-08,  ..., 1.1600e-08, 8.7719e-08,
         1.1430e-08],
        [4.6815e-11, 2.5742e-11, 2.9336e-12,  ..., 3.1256e-11, 1.9693e-12,
         7.0810e-12],
        [5.4238e-10, 2.9713e-10, 1.9945e-11,  ..., 4.8820e-10, 3.0444e-11,
         1.2082e-10],
        [3.4586e-11, 1.7342e-11, 1.3433e-12,  ..., 2.4078e-11, 1.0022e-12,
         5.5779e-12],
        [1.8934e-10, 9.9973e-11, 7.5514e-12,  ..., 1.3442e-10, 7.2902e-12,
         3.0740e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0,  1920,    19,  ...,  5232,   796,  1303],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8901e-05, -1.1550e-05, -1.7987e-05,  ...,  6.1143e-06,
          1.5805e-05,  7.6711e-06],
        [-3.0994e-06, -2.5332e-06,  7.3016e-07,  ..., -2.8014e-06,
         -7.1526e-07, -1.3635e-06],
        [-3.2485e-06, -2.6524e-06,  7.6368e-07,  ..., -2.9355e-06,
         -7.4878e-07, -1.4305e-06],
        [-3.1143e-06, -2.5481e-06,  7.3761e-07,  ..., -2.8163e-06,
         -7.1526e-07, -1.3784e-06],
        [-6.8545e-06, -5.6028e-06,  1.6168e-06,  ..., -6.1989e-06,
         -1.5795e-06, -3.0249e-06]], device='cuda:0')
Loss: 1.16524076461792


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,   600,   518,  ...,  3134,   278, 37342],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4091e-05, -3.7563e-05, -3.9500e-05,  ...,  1.1645e-05,
          2.7119e-05,  6.0831e-05],
        [-6.1244e-06, -4.9919e-06,  1.4715e-06,  ..., -5.5581e-06,
         -1.3895e-06, -2.7344e-06],
        [-6.4671e-06, -5.2601e-06,  1.5497e-06,  ..., -5.8562e-06,
         -1.4603e-06, -2.8908e-06],
        [-6.2436e-06, -5.0962e-06,  1.5050e-06,  ..., -5.6624e-06,
         -1.4082e-06, -2.7940e-06],
        [-1.3560e-05, -1.1057e-05,  3.2559e-06,  ..., -1.2308e-05,
         -3.0696e-06, -6.0648e-06]], device='cuda:0')
Loss: 1.1513824462890625


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[    0,   510,    13,  ...,  3454,   513,    13],
        [    0, 48007,   417,  ...,   944,   278,  2903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1014e-05,  4.5257e-06, -1.7630e-05,  ...,  2.2585e-05,
         -2.5130e-06,  5.7826e-05],
        [-9.2238e-06, -7.4953e-06,  2.1942e-06,  ..., -8.2701e-06,
         -2.0899e-06, -4.0978e-06],
        [-9.7454e-06, -7.9125e-06,  2.3134e-06,  ..., -8.7321e-06,
         -2.2016e-06, -4.3362e-06],
        [-9.4175e-06, -7.6592e-06,  2.2464e-06,  ..., -8.4341e-06,
         -2.1234e-06, -4.1947e-06],
        [-2.0474e-05, -1.6630e-05,  4.8652e-06,  ..., -1.8358e-05,
         -4.6268e-06, -9.1046e-06]], device='cuda:0')
Loss: 1.1356019973754883


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0,   14, 1266,  ..., 2288,  417,  199],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6932e-05, -2.8328e-05, -3.2167e-05,  ...,  2.2585e-05,
         -6.6178e-07,  5.2275e-05],
        [-1.2264e-05, -1.0014e-05,  2.9132e-06,  ..., -1.0997e-05,
         -2.7791e-06, -5.4613e-06],
        [-1.2994e-05, -1.0595e-05,  3.0808e-06,  ..., -1.1653e-05,
         -2.9355e-06, -5.7891e-06],
        [-1.2532e-05, -1.0237e-05,  2.9840e-06,  ..., -1.1221e-05,
         -2.8238e-06, -5.5879e-06],
        [-2.7269e-05, -2.2233e-05,  6.4671e-06,  ..., -2.4468e-05,
         -6.1691e-06, -1.2144e-05]], device='cuda:0')
Loss: 1.1586856842041016


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0, 14700,   717,  ..., 10570,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8847e-05,  1.2301e-05, -6.3562e-05,  ...,  1.4208e-05,
          7.8810e-06,  6.5230e-05],
        [-1.5333e-05, -1.2517e-05,  3.6471e-06,  ..., -1.3784e-05,
         -3.5055e-06, -6.8471e-06],
        [-1.6198e-05, -1.3217e-05,  3.8482e-06,  ..., -1.4558e-05,
         -3.6918e-06, -7.2345e-06],
        [-1.5646e-05, -1.2770e-05,  3.7290e-06,  ..., -1.4037e-05,
         -3.5577e-06, -6.9886e-06],
        [-3.4034e-05, -2.7746e-05,  8.0839e-06,  ..., -3.0577e-05,
         -7.7635e-06, -1.5184e-05]], device='cuda:0')
Loss: 1.1250823736190796


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0,   437,   266,  ...,   266, 16084,  1781]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6829e-05, -1.4657e-05, -7.3058e-05,  ...,  2.9526e-05,
          7.2643e-06,  6.5344e-05],
        [-1.8418e-05, -1.5020e-05,  4.3996e-06,  ..., -1.6510e-05,
         -4.1649e-06, -8.2105e-06],
        [-1.9446e-05, -1.5870e-05,  4.6454e-06,  ..., -1.7434e-05,
         -4.3884e-06, -8.6725e-06],
        [-1.8805e-05, -1.5348e-05,  4.5039e-06,  ..., -1.6838e-05,
         -4.2319e-06, -8.3894e-06],
        [-4.0859e-05, -3.3319e-05,  9.7528e-06,  ..., -3.6627e-05,
         -9.2238e-06, -1.8209e-05]], device='cuda:0')
Loss: 1.164446473121643


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,   508,   586,  ...,  6157,  3146,  7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4009e-05, -5.9680e-05, -9.0002e-05,  ...,  1.5708e-05,
          3.5879e-05,  2.7461e-05],
        [-2.1502e-05, -1.7524e-05,  5.1297e-06,  ..., -1.9252e-05,
         -4.8429e-06, -9.5516e-06],
        [-2.2709e-05, -1.8522e-05,  5.4203e-06,  ..., -2.0340e-05,
         -5.1036e-06, -1.0088e-05],
        [-2.1964e-05, -1.7926e-05,  5.2564e-06,  ..., -1.9655e-05,
         -4.9248e-06, -9.7603e-06],
        [-4.7743e-05, -3.8922e-05,  1.1384e-05,  ..., -4.2766e-05,
         -1.0736e-05, -2.1189e-05]], device='cuda:0')
Loss: 1.1473522186279297


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9373e-05, -6.5948e-05, -9.6609e-05,  ...,  2.9242e-05,
          2.0360e-05,  2.6038e-05],
        [-2.4557e-05, -2.0012e-05,  5.8822e-06,  ..., -2.2009e-05,
         -5.5283e-06, -1.0923e-05],
        [-2.5928e-05, -2.1145e-05,  6.2101e-06,  ..., -2.3246e-05,
         -5.8264e-06, -1.1533e-05],
        [-2.5094e-05, -2.0474e-05,  6.0275e-06,  ..., -2.2471e-05,
         -5.6252e-06, -1.1161e-05],
        [-5.4508e-05, -4.4405e-05,  1.3046e-05,  ..., -4.8876e-05,
         -1.2249e-05, -2.4214e-05]], device='cuda:0')
Loss: 1.1613881587982178
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4331e-05, -1.7919e-05, -4.2155e-05,  ...,  1.3902e-05,
         -5.1511e-05,  8.0399e-06],
        [-1.7249e-05, -1.3199e-05,  2.6105e-06,  ..., -1.4400e-05,
         -3.0869e-06, -6.9854e-06],
        [ 5.3697e-05,  4.2302e-05, -1.1087e-05,  ...,  5.3996e-05,
          1.3257e-05,  2.7166e-05],
        [-2.6940e-05, -2.0328e-05,  5.3851e-06,  ..., -2.3432e-05,
         -5.6115e-06, -1.1473e-05],
        [-5.3110e-05, -4.0344e-05,  1.0490e-05,  ..., -4.6267e-05,
         -1.0675e-05, -2.2225e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1888e-08, 2.8005e-08, 3.5160e-08,  ..., 1.1589e-08, 8.7631e-08,
         1.1419e-08],
        [4.7371e-11, 2.6117e-11, 2.9653e-12,  ..., 3.1709e-11, 1.9979e-12,
         7.1932e-12],
        [5.4251e-10, 2.9728e-10, 1.9964e-11,  ..., 4.8825e-10, 3.0447e-11,
         1.2084e-10],
        [3.5181e-11, 1.7744e-11, 1.3783e-12,  ..., 2.4559e-11, 1.0328e-12,
         5.6969e-12],
        [1.9212e-10, 1.0184e-10, 7.7140e-12,  ..., 1.3668e-10, 7.4329e-12,
         3.1296e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.896446322196428e-05, 1.896446322196428e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   271,   706,    13],
        [    0,    14, 15670,  ...,  2027,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0427e-05,  3.6178e-07, -1.9524e-05,  ..., -4.0187e-06,
         -8.3987e-06,  2.1196e-05],
        [-3.0547e-06, -2.5183e-06,  7.6368e-07,  ..., -2.7716e-06,
         -7.0035e-07, -1.4305e-06],
        [-3.2187e-06, -2.6524e-06,  8.0466e-07,  ..., -2.9057e-06,
         -7.3761e-07, -1.4976e-06],
        [-3.0845e-06, -2.5481e-06,  7.7486e-07,  ..., -2.8014e-06,
         -7.0408e-07, -1.4454e-06],
        [-6.7651e-06, -5.6028e-06,  1.6913e-06,  ..., -6.1393e-06,
         -1.5497e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.1291519403457642


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,  1234,   408,  ...,   292, 17323,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5114e-05,  2.8836e-06, -3.7887e-05,  ..., -2.7473e-06,
         -2.0127e-06,  5.0296e-05],
        [-6.1095e-06, -5.0217e-06,  1.5050e-06,  ..., -5.4985e-06,
         -1.3374e-06, -2.8163e-06],
        [-6.4075e-06, -5.2601e-06,  1.5795e-06,  ..., -5.7518e-06,
         -1.4044e-06, -2.9430e-06],
        [-6.2138e-06, -5.1111e-06,  1.5348e-06,  ..., -5.5879e-06,
         -1.3560e-06, -2.8610e-06],
        [-1.3590e-05, -1.1206e-05,  3.3528e-06,  ..., -1.2249e-05,
         -2.9802e-06, -6.2436e-06]], device='cuda:0')
Loss: 1.1365717649459839


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,     9,   298,  ...,    12, 24079,   287],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6923e-05, -6.5703e-07, -7.1274e-05,  ..., -2.1280e-05,
          2.5779e-06,  5.0296e-05],
        [-9.1791e-06, -7.5102e-06,  2.2501e-06,  ..., -8.2105e-06,
         -2.0228e-06, -4.2319e-06],
        [-9.6112e-06, -7.8678e-06,  2.3581e-06,  ..., -8.5831e-06,
         -2.1197e-06, -4.4182e-06],
        [-9.3281e-06, -7.6294e-06,  2.2911e-06,  ..., -8.3297e-06,
         -2.0489e-06, -4.2915e-06],
        [-2.0385e-05, -1.6719e-05,  4.9993e-06,  ..., -1.8239e-05,
         -4.5002e-06, -9.3579e-06]], device='cuda:0')
Loss: 1.1519843339920044


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[    0,    15,    83,  ...,  6030,    14, 14080],
        [    0, 29258,   765,  ...,  4196,    19,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9314e-05, -6.0743e-06, -6.2222e-05,  ..., -6.3192e-05,
         -1.5505e-05,  4.6396e-05],
        [-1.2249e-05, -1.0073e-05,  3.0398e-06,  ..., -1.0982e-05,
         -2.6822e-06, -5.6326e-06],
        [-1.2815e-05, -1.0550e-05,  3.1851e-06,  ..., -1.1474e-05,
         -2.8089e-06, -5.8860e-06],
        [-1.2413e-05, -1.0207e-05,  3.0883e-06,  ..., -1.1116e-05,
         -2.7120e-06, -5.7071e-06],
        [-2.7120e-05, -2.2322e-05,  6.7279e-06,  ..., -2.4319e-05,
         -5.9456e-06, -1.2428e-05]], device='cuda:0')
Loss: 1.1571472883224487


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[    0,  2670, 31283,  ...,    18,  9106,  1389],
        [    0,   287,   266,  ...,   333,   199,  3217]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3200e-05,  1.2895e-05, -4.5929e-05,  ..., -6.5874e-05,
         -6.3680e-06,  6.5725e-05],
        [-1.5318e-05, -1.2577e-05,  3.8110e-06,  ..., -1.3694e-05,
         -3.3826e-06, -7.0632e-06],
        [-1.6063e-05, -1.3188e-05,  4.0010e-06,  ..., -1.4350e-05,
         -3.5502e-06, -7.3984e-06],
        [-1.5512e-05, -1.2726e-05,  3.8669e-06,  ..., -1.3858e-05,
         -3.4198e-06, -7.1526e-06],
        [-3.3945e-05, -2.7835e-05,  8.4341e-06,  ..., -3.0339e-05,
         -7.5027e-06, -1.5587e-05]], device='cuda:0')
Loss: 1.1668074131011963


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  591,  953,  ..., 4118, 5750,  292],
        [   0, 4998, 1921,  ...,  968,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7800e-05,  1.7032e-05, -4.7901e-05,  ..., -5.7368e-05,
         -6.3680e-06,  6.8790e-05],
        [-1.8373e-05, -1.5080e-05,  4.5709e-06,  ..., -1.6436e-05,
         -4.0866e-06, -8.4937e-06],
        [-1.9267e-05, -1.5810e-05,  4.8019e-06,  ..., -1.7226e-05,
         -4.2878e-06, -8.9034e-06],
        [-1.8582e-05, -1.5244e-05,  4.6380e-06,  ..., -1.6615e-05,
         -4.1276e-06, -8.5905e-06],
        [-4.0650e-05, -3.3319e-05,  1.0103e-05,  ..., -3.6359e-05,
         -9.0450e-06, -1.8731e-05]], device='cuda:0')
Loss: 1.129915475845337


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,  2440,  1458,  ...,  7650,   328,  2297],
        [    0, 13576,   431,  ...,    14,   475,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1050e-05,  2.4784e-05, -5.0136e-05,  ..., -6.7093e-05,
          1.8235e-05,  8.2876e-05],
        [-2.1473e-05, -1.7613e-05,  5.3085e-06,  ..., -1.9193e-05,
         -4.7199e-06, -9.8497e-06],
        [-2.2501e-05, -1.8477e-05,  5.5768e-06,  ..., -2.0117e-05,
         -4.9509e-06, -1.0327e-05],
        [-2.1696e-05, -1.7807e-05,  5.3830e-06,  ..., -1.9386e-05,
         -4.7646e-06, -9.9614e-06],
        [-4.7535e-05, -3.8981e-05,  1.1742e-05,  ..., -4.2498e-05,
         -1.0461e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.163478970527649


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[   0,   13, 2497,  ...,  943,  259, 2646],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2344e-05,  2.3820e-05, -8.2197e-05,  ..., -7.0713e-05,
         -1.1575e-05,  9.4589e-05],
        [-2.4542e-05, -2.0146e-05,  6.0834e-06,  ..., -2.1905e-05,
         -5.4017e-06, -1.1258e-05],
        [-2.5749e-05, -2.1160e-05,  6.3963e-06,  ..., -2.2992e-05,
         -5.6699e-06, -1.1809e-05],
        [-2.4796e-05, -2.0370e-05,  6.1691e-06,  ..., -2.2128e-05,
         -5.4501e-06, -1.1377e-05],
        [-5.4359e-05, -4.4614e-05,  1.3456e-05,  ..., -4.8518e-05,
         -1.1973e-05, -2.4870e-05]], device='cuda:0')
Loss: 1.154394268989563
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1663e-05, -1.3745e-05, -4.6159e-05,  ...,  5.4404e-06,
         -4.7518e-05,  1.6695e-05],
        [-1.7978e-05, -1.3894e-05,  2.9578e-06,  ..., -1.5151e-05,
         -3.3184e-06, -7.4127e-06],
        [ 4.5752e-05,  3.5956e-05, -9.3389e-06,  ...,  4.6297e-05,
          1.1364e-05,  2.3269e-05],
        [-2.6726e-05, -2.0333e-05,  5.4635e-06,  ..., -2.3301e-05,
         -5.5953e-06, -1.1464e-05],
        [-5.3235e-05, -4.0771e-05,  1.0787e-05,  ..., -4.6492e-05,
         -1.0805e-05, -2.2489e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1854e-08, 2.7977e-08, 3.5131e-08,  ..., 1.1583e-08, 8.7544e-08,
         1.1417e-08],
        [4.7926e-11, 2.6497e-11, 2.9993e-12,  ..., 3.2157e-11, 2.0251e-12,
         7.3128e-12],
        [5.4263e-10, 2.9743e-10, 1.9985e-11,  ..., 4.8830e-10, 3.0449e-11,
         1.2086e-10],
        [3.5760e-11, 1.8141e-11, 1.4150e-12,  ..., 2.5024e-11, 1.0615e-12,
         5.8206e-12],
        [1.9489e-10, 1.0373e-10, 7.8874e-12,  ..., 1.3889e-10, 7.5689e-12,
         3.1883e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   685,   344,  ...,   680,   401,   616],
        [    0,    12,   401,  ...,  7665,  4101, 10193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5682e-05,  3.5978e-06, -8.6806e-06,  ..., -9.8161e-06,
         -2.4730e-05, -2.3323e-05],
        [-3.0547e-06, -2.5034e-06,  7.6741e-07,  ..., -2.7120e-06,
         -6.2212e-07, -1.4454e-06],
        [-3.1888e-06, -2.6226e-06,  8.0466e-07,  ..., -2.8312e-06,
         -6.4820e-07, -1.5125e-06],
        [-3.0845e-06, -2.5332e-06,  7.7859e-07,  ..., -2.7269e-06,
         -6.2585e-07, -1.4603e-06],
        [-6.7651e-06, -5.5432e-06,  1.6987e-06,  ..., -5.9903e-06,
         -1.3784e-06, -3.2037e-06]], device='cuda:0')
Loss: 1.1420022249221802


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  3773, 23452,  ..., 14393,  1121,   304],
        [    0,   221,   380,  ...,   630,  3765, 19107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2798e-05, -6.2078e-06,  4.7735e-06,  ..., -9.0435e-06,
         -2.4297e-05, -4.7847e-05],
        [-6.1095e-06, -5.0068e-06,  1.5348e-06,  ..., -5.4389e-06,
         -1.2629e-06, -2.8908e-06],
        [-6.4075e-06, -5.2601e-06,  1.6131e-06,  ..., -5.7071e-06,
         -1.3225e-06, -3.0324e-06],
        [-6.1542e-06, -5.0515e-06,  1.5497e-06,  ..., -5.4538e-06,
         -1.2666e-06, -2.9057e-06],
        [-1.3560e-05, -1.1116e-05,  3.4049e-06,  ..., -1.2040e-05,
         -2.8014e-06, -6.4075e-06]], device='cuda:0')
Loss: 1.1572355031967163


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   471,  6210,  ...,  4274,   344, 11451],
        [    0,   221,   709,  ...,  3365,  3504,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6226e-05, -3.0911e-05,  1.1310e-06,  ...,  2.2080e-05,
         -1.9081e-05, -6.5154e-05],
        [-9.1940e-06, -7.4953e-06,  2.2873e-06,  ..., -8.1360e-06,
         -1.9073e-06, -4.3437e-06],
        [-9.6411e-06, -7.8827e-06,  2.4065e-06,  ..., -8.5384e-06,
         -1.9968e-06, -4.5598e-06],
        [-9.2387e-06, -7.5549e-06,  2.3060e-06,  ..., -8.1509e-06,
         -1.9073e-06, -4.3586e-06],
        [-2.0385e-05, -1.6659e-05,  5.0738e-06,  ..., -1.8001e-05,
         -4.2319e-06, -9.6262e-06]], device='cuda:0')
Loss: 1.13746976852417


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,   287,  3121,   292],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5473e-05, -1.9186e-05, -1.4905e-05,  ..., -1.5487e-05,
         -1.3437e-05, -7.6975e-05],
        [-1.2264e-05, -1.0028e-05,  3.0659e-06,  ..., -1.0878e-05,
         -2.5779e-06, -5.8338e-06],
        [-1.2845e-05, -1.0520e-05,  3.2224e-06,  ..., -1.1399e-05,
         -2.6971e-06, -6.1169e-06],
        [-1.2293e-05, -1.0073e-05,  3.0845e-06,  ..., -1.0878e-05,
         -2.5705e-06, -5.8413e-06],
        [-2.7120e-05, -2.2203e-05,  6.7800e-06,  ..., -2.4021e-05,
         -5.6997e-06, -1.2890e-05]], device='cuda:0')
Loss: 1.1520131826400757


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   908,  1086,    12],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3206e-05, -8.8201e-06, -3.1319e-05,  ..., -1.4727e-05,
         -1.7278e-05, -5.6055e-05],
        [-1.5363e-05, -1.2547e-05,  3.8296e-06,  ..., -1.3605e-05,
         -3.2336e-06, -7.3016e-06],
        [-1.6093e-05, -1.3158e-05,  4.0196e-06,  ..., -1.4246e-05,
         -3.3826e-06, -7.6517e-06],
        [-1.5378e-05, -1.2577e-05,  3.8445e-06,  ..., -1.3590e-05,
         -3.2187e-06, -7.3016e-06],
        [-3.3945e-05, -2.7716e-05,  8.4564e-06,  ..., -3.0011e-05,
         -7.1377e-06, -1.6108e-05]], device='cuda:0')
Loss: 1.137607216835022


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    45,  6556,  ...,  1477,   352,  1611],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0145e-04,  1.1714e-05, -2.3135e-05,  ..., -3.2979e-05,
         -2.6563e-06, -5.5315e-05],
        [-1.8463e-05, -1.5095e-05,  4.5933e-06,  ..., -1.6376e-05,
         -3.8855e-06, -8.8140e-06],
        [-1.9312e-05, -1.5810e-05,  4.8131e-06,  ..., -1.7121e-05,
         -4.0568e-06, -9.2238e-06],
        [-1.8477e-05, -1.5125e-05,  4.6082e-06,  ..., -1.6347e-05,
         -3.8669e-06, -8.8140e-06],
        [-4.0770e-05, -3.3319e-05,  1.0133e-05,  ..., -3.6091e-05,
         -8.5682e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1410932540893555


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[   0,  421, 6007,  ...,  408, 2105,  843],
        [   0,  471,   12,  ...,   13, 9909, 2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0618e-04,  2.3876e-06, -3.1810e-05,  ..., -1.8644e-05,
          2.4348e-05, -4.2382e-05],
        [-2.1532e-05, -1.7628e-05,  5.3681e-06,  ..., -1.9088e-05,
         -4.5076e-06, -1.0282e-05],
        [-2.2531e-05, -1.8477e-05,  5.6326e-06,  ..., -1.9968e-05,
         -4.7125e-06, -1.0766e-05],
        [-2.1547e-05, -1.7658e-05,  5.3868e-06,  ..., -1.9059e-05,
         -4.4852e-06, -1.0274e-05],
        [-4.7505e-05, -3.8892e-05,  1.1839e-05,  ..., -4.2051e-05,
         -9.9391e-06, -2.2650e-05]], device='cuda:0')
Loss: 1.1483745574951172


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   271,   266,  ...,  8122,  1387,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0073e-04,  7.8527e-06, -7.1373e-05,  ..., -7.8012e-06,
         -6.9372e-07, -5.2596e-05],
        [-2.4572e-05, -2.0117e-05,  6.1356e-06,  ..., -2.1771e-05,
         -5.1074e-06, -1.1720e-05],
        [-2.5734e-05, -2.1100e-05,  6.4410e-06,  ..., -2.2799e-05,
         -5.3421e-06, -1.2279e-05],
        [-2.4617e-05, -2.0176e-05,  6.1616e-06,  ..., -2.1771e-05,
         -5.0887e-06, -1.1727e-05],
        [-5.4210e-05, -4.4405e-05,  1.3530e-05,  ..., -4.7982e-05,
         -1.1258e-05, -2.5824e-05]], device='cuda:0')
Loss: 1.155415415763855
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0012],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4244e-06, -1.1585e-05, -4.8680e-05,  ...,  4.1162e-06,
         -4.2835e-05,  9.7657e-06],
        [-1.8637e-05, -1.4516e-05,  3.2756e-06,  ..., -1.5813e-05,
         -3.4973e-06, -7.8434e-06],
        [ 3.8604e-05,  3.0250e-05, -7.7609e-06,  ...,  3.9387e-05,
          9.6938e-06,  1.9714e-05],
        [-2.6515e-05, -2.0317e-05,  5.5333e-06,  ..., -2.3148e-05,
         -5.5447e-06, -1.1490e-05],
        [-5.3332e-05, -4.1135e-05,  1.1061e-05,  ..., -4.6641e-05,
         -1.0850e-05, -2.2823e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1823e-08, 2.7949e-08, 3.5101e-08,  ..., 1.1571e-08, 8.7456e-08,
         1.1408e-08],
        [4.8482e-11, 2.6875e-11, 3.0340e-12,  ..., 3.2599e-11, 2.0492e-12,
         7.4428e-12],
        [5.4275e-10, 2.9758e-10, 2.0006e-11,  ..., 4.8833e-10, 3.0447e-11,
         1.2088e-10],
        [3.6331e-11, 1.8530e-11, 1.4515e-12,  ..., 2.5473e-11, 1.0863e-12,
         5.9523e-12],
        [1.9763e-10, 1.0560e-10, 8.0625e-12,  ..., 1.4106e-10, 7.6880e-12,
         3.2518e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.885217280831754e-05, 1.885217280831754e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  631, 3370,  940],
        [   0,  328, 1410,  ..., 7344,   12, 5067]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0329e-05,  9.0032e-06, -2.4776e-05,  ...,  0.0000e+00,
         -1.0549e-06,  3.5731e-05],
        [-3.0845e-06, -2.5034e-06,  7.7859e-07,  ..., -2.7120e-06,
         -6.1840e-07, -1.5125e-06],
        [-3.2037e-06, -2.6226e-06,  8.1211e-07,  ..., -2.8312e-06,
         -6.4448e-07, -1.5721e-06],
        [-3.0398e-06, -2.4885e-06,  7.6741e-07,  ..., -2.6822e-06,
         -6.0722e-07, -1.4901e-06],
        [-6.7353e-06, -5.4836e-06,  1.6987e-06,  ..., -5.9307e-06,
         -1.3486e-06, -3.2932e-06]], device='cuda:0')
Loss: 1.1242687702178955


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,  6408,   278,  1119],
        [    0, 24063,   717,  ...,  2228,  1416,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8981e-05, -2.8376e-07, -6.0777e-05,  ...,  2.2015e-05,
         -2.7247e-06,  6.4837e-05],
        [-6.1393e-06, -4.9919e-06,  1.5795e-06,  ..., -5.3942e-06,
         -1.2219e-06, -3.0324e-06],
        [-6.4224e-06, -5.2303e-06,  1.6503e-06,  ..., -5.6475e-06,
         -1.2778e-06, -3.1590e-06],
        [-6.0946e-06, -4.9770e-06,  1.5683e-06,  ..., -5.3644e-06,
         -1.2070e-06, -3.0026e-06],
        [-1.3471e-05, -1.0967e-05,  3.4571e-06,  ..., -1.1802e-05,
         -2.6748e-06, -6.6161e-06]], device='cuda:0')
Loss: 1.1363412141799927


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,   298,  2230,  ...,  2300,  3698,  4764],
        [    0,  3908,  4274,  ...,   298,  7998, 11109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0109e-04,  1.9359e-05, -6.1100e-05,  ...,  1.8608e-05,
         -6.9258e-06,  1.1296e-04],
        [-9.2238e-06, -7.5102e-06,  2.3656e-06,  ..., -8.0764e-06,
         -1.8924e-06, -4.5821e-06],
        [-9.6560e-06, -7.8529e-06,  2.4736e-06,  ..., -8.4490e-06,
         -1.9781e-06, -4.7833e-06],
        [-9.1344e-06, -7.4506e-06,  2.3432e-06,  ..., -8.0019e-06,
         -1.8664e-06, -4.5300e-06],
        [-2.0236e-05, -1.6451e-05,  5.1782e-06,  ..., -1.7643e-05,
         -4.1425e-06, -9.9987e-06]], device='cuda:0')
Loss: 1.1284170150756836


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   199,  7513,  ...,   271,   259,   957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1523e-05, -1.4284e-05, -9.0422e-05,  ...,  2.5992e-05,
          2.1388e-06,  1.2110e-04],
        [-1.2308e-05, -9.9838e-06,  3.1628e-06,  ..., -1.0759e-05,
         -2.5332e-06, -6.1244e-06],
        [-1.2875e-05, -1.0446e-05,  3.3118e-06,  ..., -1.1265e-05,
         -2.6487e-06, -6.3926e-06],
        [-1.2174e-05, -9.8944e-06,  3.1330e-06,  ..., -1.0654e-05,
         -2.4997e-06, -6.0499e-06],
        [-2.6911e-05, -2.1815e-05,  6.9067e-06,  ..., -2.3484e-05,
         -5.5358e-06, -1.3337e-05]], device='cuda:0')
Loss: 1.1422611474990845


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[    0, 23749, 27341,  ..., 34110,   342,  9672],
        [    0,  1172,   365,  ...,  1119, 15573,  3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5274e-05, -3.1736e-05, -1.0424e-04,  ...,  3.7383e-05,
          2.0471e-06,  1.1667e-04],
        [-1.5363e-05, -1.2487e-05,  3.9488e-06,  ..., -1.3471e-05,
         -3.1255e-06, -7.6219e-06],
        [-1.6063e-05, -1.3053e-05,  4.1313e-06,  ..., -1.4096e-05,
         -3.2671e-06, -7.9572e-06],
        [-1.5214e-05, -1.2383e-05,  3.9116e-06,  ..., -1.3337e-05,
         -3.0845e-06, -7.5325e-06],
        [-3.3617e-05, -2.7299e-05,  8.6278e-06,  ..., -2.9415e-05,
         -6.8322e-06, -1.6615e-05]], device='cuda:0')
Loss: 1.1555427312850952


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  259, 1329,  ...,  266,  706, 1663],
        [   0,   12, 1250,  ...,  381, 1524, 2204]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6086e-05, -3.6319e-05, -9.8262e-05,  ...,  1.8050e-05,
          2.4773e-05,  1.4687e-04],
        [-1.8492e-05, -1.5035e-05,  4.7274e-06,  ..., -1.6198e-05,
         -3.7551e-06, -9.1493e-06],
        [-1.9312e-05, -1.5706e-05,  4.9435e-06,  ..., -1.6928e-05,
         -3.9190e-06, -9.5516e-06],
        [-1.8284e-05, -1.4886e-05,  4.6752e-06,  ..., -1.6004e-05,
         -3.6992e-06, -9.0301e-06],
        [-4.0472e-05, -3.2872e-05,  1.0327e-05,  ..., -3.5375e-05,
         -8.2105e-06, -1.9968e-05]], device='cuda:0')
Loss: 1.1553860902786255


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2222e-05,  2.6390e-06, -1.1578e-04,  ...,  2.5312e-05,
          9.9312e-06,  1.4171e-04],
        [-2.1577e-05, -1.7539e-05,  5.5097e-06,  ..., -1.8880e-05,
         -4.3772e-06, -1.0677e-05],
        [-2.2545e-05, -1.8328e-05,  5.7630e-06,  ..., -1.9744e-05,
         -4.5709e-06, -1.1154e-05],
        [-2.1309e-05, -1.7345e-05,  5.4464e-06,  ..., -1.8641e-05,
         -4.3064e-06, -1.0535e-05],
        [-4.7207e-05, -3.8356e-05,  1.2040e-05,  ..., -4.1246e-05,
         -9.5665e-06, -2.3305e-05]], device='cuda:0')
Loss: 1.1285558938980103


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[   0, 3978, 2697,  ...,  461, 5955, 3792],
        [   0, 1732,  699,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3610e-05,  3.5669e-05, -1.3475e-04,  ...,  5.1406e-05,
          2.8408e-05,  1.0846e-04],
        [-2.4632e-05, -1.9997e-05,  6.2883e-06,  ..., -2.1592e-05,
         -4.9919e-06, -1.2212e-05],
        [-2.5749e-05, -2.0906e-05,  6.5826e-06,  ..., -2.2590e-05,
         -5.2117e-06, -1.2755e-05],
        [-2.4334e-05, -1.9789e-05,  6.2212e-06,  ..., -2.1338e-05,
         -4.9099e-06, -1.2055e-05],
        [-5.3942e-05, -4.3780e-05,  1.3754e-05,  ..., -4.7237e-05,
         -1.0915e-05, -2.6673e-05]], device='cuda:0')
Loss: 1.1447765827178955
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5843e-05, -6.8597e-06, -5.7288e-05,  ...,  8.8452e-06,
         -3.5711e-05,  1.9635e-05],
        [-1.9237e-05, -1.5064e-05,  3.5768e-06,  ..., -1.6391e-05,
         -3.6467e-06, -8.2802e-06],
        [ 3.2168e-05,  2.5135e-05, -6.3266e-06,  ...,  3.3190e-05,
          8.2032e-06,  1.6467e-05],
        [-2.6297e-05, -2.0264e-05,  5.6021e-06,  ..., -2.2967e-05,
         -5.4812e-06, -1.1547e-05],
        [-5.3393e-05, -4.1399e-05,  1.1330e-05,  ..., -4.6701e-05,
         -1.0857e-05, -2.3208e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1786e-08, 2.7923e-08, 3.5084e-08,  ..., 1.1562e-08, 8.7370e-08,
         1.1408e-08],
        [4.9040e-11, 2.7248e-11, 3.0705e-12,  ..., 3.3032e-11, 2.0720e-12,
         7.5845e-12],
        [5.4287e-10, 2.9771e-10, 2.0029e-11,  ..., 4.8835e-10, 3.0444e-11,
         1.2093e-10],
        [3.6886e-11, 1.8903e-11, 1.4888e-12,  ..., 2.5903e-11, 1.1094e-12,
         6.0917e-12],
        [2.0034e-10, 1.0741e-10, 8.2437e-12,  ..., 1.4315e-10, 7.7995e-12,
         3.3197e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[    0,   199,  5990,  ...,   278,   638,  5513],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4269e-06,  9.1007e-06, -1.6309e-05,  ..., -4.7186e-05,
          2.3015e-06,  4.6941e-06],
        [-3.1292e-06, -2.5332e-06,  8.4192e-07,  ..., -2.7269e-06,
         -6.0350e-07, -1.6168e-06],
        [-3.2187e-06, -2.6077e-06,  8.6799e-07,  ..., -2.8163e-06,
         -6.1840e-07, -1.6689e-06],
        [-3.0547e-06, -2.4736e-06,  8.2701e-07,  ..., -2.6673e-06,
         -5.8860e-07, -1.5795e-06],
        [-6.7353e-06, -5.4538e-06,  1.8179e-06,  ..., -5.9009e-06,
         -1.3039e-06, -3.4869e-06]], device='cuda:0')
Loss: 1.1505606174468994


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1482e-06,  1.9801e-05, -1.5759e-05,  ..., -5.4042e-05,
          1.4711e-05,  1.1935e-05],
        [-6.1989e-06, -5.0217e-06,  1.6466e-06,  ..., -5.4091e-06,
         -1.1995e-06, -3.2037e-06],
        [-6.4075e-06, -5.1856e-06,  1.7025e-06,  ..., -5.5879e-06,
         -1.2368e-06, -3.3081e-06],
        [-6.0797e-06, -4.9174e-06,  1.6205e-06,  ..., -5.3048e-06,
         -1.1735e-06, -3.1367e-06],
        [-1.3411e-05, -1.0878e-05,  3.5614e-06,  ..., -1.1712e-05,
         -2.6003e-06, -6.9290e-06]], device='cuda:0')
Loss: 1.142570972442627


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5390e-05,  2.5127e-05,  2.2106e-05,  ..., -7.7926e-05,
          1.0723e-05,  2.0380e-05],
        [-9.2536e-06, -7.4953e-06,  2.4512e-06,  ..., -8.0466e-06,
         -1.7919e-06, -4.7907e-06],
        [ 9.6300e-05,  7.6130e-05, -1.7831e-05,  ...,  5.7460e-05,
          6.2004e-06,  3.5563e-05],
        [-9.1046e-06, -7.3761e-06,  2.4177e-06,  ..., -7.9274e-06,
         -1.7583e-06, -4.7088e-06],
        [-2.0087e-05, -1.6302e-05,  5.3272e-06,  ..., -1.7494e-05,
         -3.8967e-06, -1.0401e-05]], device='cuda:0')
Loss: 1.1231986284255981


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,   342,   970,  ...,   401,  2907,  1657],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0108e-05,  1.7401e-06, -6.2542e-06,  ..., -5.8466e-05,
         -2.0524e-06,  6.5172e-05],
        [-1.2323e-05, -9.9987e-06,  3.2894e-06,  ..., -1.0714e-05,
         -2.4177e-06, -6.3628e-06],
        [ 9.3052e-05,  7.3492e-05, -1.6948e-05,  ...,  5.4644e-05,
          5.5447e-06,  3.3909e-05],
        [ 5.4908e-04,  4.4693e-04, -1.4750e-04,  ...,  4.1227e-04,
          1.4056e-04,  2.5616e-04],
        [-2.6792e-05, -2.1726e-05,  7.1451e-06,  ..., -2.3305e-05,
         -5.2601e-06, -1.3813e-05]], device='cuda:0')
Loss: 1.1314899921417236


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   870,   278,  ...,  1274, 10112,  3269],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6592e-05,  1.7732e-05, -3.5630e-05,  ..., -3.8331e-05,
          3.2485e-06,  6.6660e-05],
        [-1.5378e-05, -1.2502e-05,  4.1239e-06,  ..., -1.3441e-05,
         -3.0287e-06, -7.9349e-06],
        [ 8.9863e-05,  7.0870e-05, -1.6076e-05,  ...,  5.1813e-05,
          4.9114e-06,  3.2278e-05],
        [ 5.4609e-04,  4.4448e-04, -1.4669e-04,  ...,  4.0961e-04,
          1.3997e-04,  2.5463e-04],
        [-3.3468e-05, -2.7180e-05,  8.9630e-06,  ..., -2.9236e-05,
         -6.5863e-06, -1.7226e-05]], device='cuda:0')
Loss: 1.1545886993408203


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,  2278,   292,  ..., 12060,  1319,   292],
        [    0,   300,  3808,  ...,   496,    14,  1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1335e-05, -2.3323e-06, -3.5630e-05,  ..., -8.2023e-05,
          3.2794e-05,  6.3420e-05],
        [-1.8433e-05, -1.5020e-05,  4.9546e-06,  ..., -1.6123e-05,
         -3.6098e-06, -9.5069e-06],
        [ 8.6674e-05,  6.8247e-05, -1.5212e-05,  ...,  4.9026e-05,
          4.3079e-06,  3.0646e-05],
        [ 5.4307e-04,  4.4200e-04, -1.4586e-04,  ...,  4.0698e-04,
          1.3940e-04,  2.5308e-04],
        [-4.0144e-05, -3.2663e-05,  1.0774e-05,  ..., -3.5077e-05,
         -7.8529e-06, -2.0653e-05]], device='cuda:0')
Loss: 1.1311054229736328


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,   259,  6022,  ...,  1871,  1209,  1241]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7710e-06, -1.1685e-05, -5.6726e-05,  ..., -7.5228e-05,
         -9.8747e-07,  6.7983e-05],
        [-2.1487e-05, -1.7509e-05,  5.7667e-06,  ..., -1.8775e-05,
         -4.2059e-06, -1.1101e-05],
        [ 1.7659e-04,  1.1860e-04, -3.6896e-05,  ...,  1.3373e-04,
          1.6836e-05,  8.4725e-05],
        [ 5.4005e-04,  4.3954e-04, -1.4506e-04,  ...,  4.0434e-04,
          1.3881e-04,  2.5150e-04],
        [-4.6819e-05, -3.8087e-05,  1.2539e-05,  ..., -4.0859e-05,
         -9.1493e-06, -2.4125e-05]], device='cuda:0')
Loss: 1.1335415840148926


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[    0,   508,  2322,  ...,   968,   266, 15123],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8373e-06, -5.3407e-07, -7.7739e-05,  ..., -6.2643e-05,
         -1.2596e-05,  7.9838e-05],
        [-2.4587e-05, -1.9982e-05,  6.5789e-06,  ..., -2.1443e-05,
         -4.8019e-06, -1.2666e-05],
        [ 1.7335e-04,  1.1602e-04, -3.6046e-05,  ...,  1.3096e-04,
          1.6218e-05,  8.3101e-05],
        [ 5.3699e-04,  4.3709e-04, -1.4425e-04,  ...,  4.0172e-04,
          1.3822e-04,  2.4997e-04],
        [-5.3585e-05, -4.3511e-05,  1.4313e-05,  ..., -4.6670e-05,
         -1.0453e-05, -2.7537e-05]], device='cuda:0')
Loss: 1.1498327255249023
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4742e-05, -6.2271e-06, -5.9333e-05,  ...,  1.6964e-06,
         -3.3399e-05,  2.5655e-05],
        [-1.9772e-05, -1.5556e-05,  3.8770e-06,  ..., -1.6896e-05,
         -3.7623e-06, -8.7188e-06],
        [ 4.6287e-05,  3.4223e-05, -9.2985e-06,  ...,  4.2966e-05,
          9.0047e-06,  2.3130e-05],
        [ 3.0032e-05,  2.5472e-05, -9.3836e-06,  ...,  1.9501e-05,
          8.8893e-06,  1.4605e-05],
        [-5.3413e-05, -4.1610e-05,  1.1629e-05,  ..., -4.6698e-05,
         -1.0816e-05, -2.3641e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1744e-08, 2.7895e-08, 3.5055e-08,  ..., 1.1555e-08, 8.7283e-08,
         1.1403e-08],
        [4.9596e-11, 2.7620e-11, 3.1107e-12,  ..., 3.3459e-11, 2.0930e-12,
         7.7373e-12],
        [5.7238e-10, 3.1088e-10, 2.1309e-11,  ..., 5.0501e-10, 3.0676e-11,
         1.2771e-10],
        [3.2521e-10, 2.0993e-10, 2.2297e-11,  ..., 1.8725e-10, 2.0214e-11,
         6.8568e-11],
        [2.0301e-10, 1.0920e-10, 8.4403e-12,  ..., 1.4518e-10, 7.9009e-12,
         3.3922e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.8734471844266252e-05, 1.8734471844266252e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,    29,   413,  ...,  1527,  1503,   369],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5419e-06,  1.6590e-05,  9.7735e-06,  ..., -1.7086e-06,
          1.2764e-06, -4.8947e-06],
        [-3.1143e-06, -2.4587e-06,  8.6054e-07,  ..., -2.6524e-06,
         -6.2585e-07, -1.6689e-06],
        [-3.2485e-06, -2.5630e-06,  8.9779e-07,  ..., -2.7567e-06,
         -6.5193e-07, -1.7434e-06],
        [-3.0994e-06, -2.4438e-06,  8.6054e-07,  ..., -2.6375e-06,
         -6.1840e-07, -1.6615e-06],
        [-6.7055e-06, -5.2750e-06,  1.8552e-06,  ..., -5.7220e-06,
         -1.3486e-06, -3.6061e-06]], device='cuda:0')
Loss: 1.1306838989257812


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0, 1049,  292,  ...,  221,  380,  341],
        [   0,   13,  786,  ...,  275, 2623,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6629e-05,  1.3616e-05, -1.1337e-05,  ..., -8.5043e-06,
          3.8340e-07, -1.1953e-05],
        [-6.1691e-06, -4.9770e-06,  1.7360e-06,  ..., -5.3048e-06,
         -1.2368e-06, -3.3081e-06],
        [ 8.4705e-05,  6.1107e-05, -1.4862e-05,  ...,  8.0484e-05,
          1.6377e-05,  4.8687e-05],
        [-6.1542e-06, -4.9621e-06,  1.7360e-06,  ..., -5.2899e-06,
         -1.2256e-06, -3.2932e-06],
        [-1.3292e-05, -1.0699e-05,  3.7402e-06,  ..., -1.1444e-05,
         -2.6599e-06, -7.1377e-06]], device='cuda:0')
Loss: 1.150447130203247


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[   0, 4823,   12,  ..., 1756, 3406,  300],
        [   0,  494,  221,  ...,  437,  266, 2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0263e-04, -6.1120e-06, -4.2798e-05,  ..., -1.7549e-05,
          3.8340e-07, -2.9121e-06],
        [-9.1791e-06, -7.4208e-06,  2.5965e-06,  ..., -7.9721e-06,
         -1.8887e-06, -4.9695e-06],
        [ 3.4461e-04,  2.9194e-04, -9.1815e-05,  ...,  3.0596e-04,
          7.8909e-05,  2.2441e-04],
        [-9.1493e-06, -7.4059e-06,  2.5928e-06,  ..., -7.9423e-06,
         -1.8701e-06, -4.9472e-06],
        [-1.9819e-05, -1.6004e-05,  5.6028e-06,  ..., -1.7226e-05,
         -4.0680e-06, -1.0744e-05]], device='cuda:0')
Loss: 1.1579972505569458


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[   0,  391, 9095,  ...,  417,  199, 2038],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8780e-05, -4.9485e-05, -5.6592e-05,  ..., -6.3738e-06,
         -1.8782e-05,  7.9413e-07],
        [-1.2264e-05, -9.8795e-06,  3.4533e-06,  ..., -1.0669e-05,
         -2.4624e-06, -6.6161e-06],
        [ 3.4137e-04,  2.8936e-04, -9.0917e-05,  ...,  3.0314e-04,
          7.8309e-05,  2.2269e-04],
        [ 7.4410e-05,  6.3120e-05, -3.5402e-05,  ...,  6.4664e-05,
          1.7484e-05,  5.1117e-05],
        [-2.6494e-05, -2.1309e-05,  7.4580e-06,  ..., -2.3037e-05,
         -5.3123e-06, -1.4305e-05]], device='cuda:0')
Loss: 1.135195255279541


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[   0, 2579,  278,  ...,   56,    9,  271],
        [   0,  995,   13,  ..., 2192, 2534,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2038e-04, -8.2176e-05, -7.0418e-05,  ...,  6.7431e-07,
         -1.1703e-05, -1.2096e-05],
        [-1.5303e-05, -1.2323e-05,  4.3251e-06,  ..., -1.3337e-05,
         -3.0734e-06, -8.2403e-06],
        [ 3.3819e-04,  2.8680e-04, -9.0004e-05,  ...,  3.0036e-04,
          7.7669e-05,  2.2099e-04],
        [ 7.1370e-05,  6.0691e-05, -3.4534e-05,  ...,  6.2012e-05,
          1.6877e-05,  4.9500e-05],
        [-3.3081e-05, -2.6584e-05,  9.3356e-06,  ..., -2.8789e-05,
         -6.6310e-06, -1.7822e-05]], device='cuda:0')
Loss: 1.1441148519515991


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0, 18322,   287,  ...,   953,   271,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7288e-05, -9.2324e-05, -1.0286e-04,  ...,  1.5419e-05,
         -6.7844e-05, -3.5982e-05],
        [-1.8373e-05, -1.4767e-05,  5.1968e-06,  ..., -1.5989e-05,
         -3.6582e-06, -9.9167e-06],
        [ 3.3495e-04,  2.8422e-04, -8.9084e-05,  ...,  2.9757e-04,
          7.7050e-05,  2.1923e-04],
        [ 6.8271e-05,  5.8217e-05, -3.3651e-05,  ...,  5.9330e-05,
          1.6288e-05,  4.7801e-05],
        [-3.9697e-05, -3.1859e-05,  1.1213e-05,  ..., -3.4511e-05,
         -7.8976e-06, -2.1428e-05]], device='cuda:0')
Loss: 1.10907781124115


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,  3543,   391,  ...,  3370,  2926,  8090],
        [    0,   221,   264,  ...,  3613,  3222, 14000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7830e-05, -6.7675e-05, -1.2125e-04,  ...,  1.0706e-05,
         -8.5987e-05, -2.3168e-05],
        [-2.1443e-05, -1.7226e-05,  6.0424e-06,  ..., -1.8641e-05,
         -4.2841e-06, -1.1563e-05],
        [ 3.3175e-04,  2.8166e-04, -8.8205e-05,  ...,  2.9481e-04,
          7.6402e-05,  2.1751e-04],
        [ 6.5231e-05,  5.5773e-05, -3.2813e-05,  ...,  5.6707e-05,
          1.5674e-05,  4.6170e-05],
        [-4.6313e-05, -3.7163e-05,  1.3031e-05,  ..., -4.0233e-05,
         -9.2387e-06, -2.4974e-05]], device='cuda:0')
Loss: 1.1317639350891113


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2356,   292,  ...,    12,   287,   300],
        [    0,  1456, 32380,  ...,    12,  1172, 12557]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4511e-04, -8.1109e-05, -1.5706e-04,  ...,  1.2786e-06,
         -7.9371e-05, -6.5377e-05],
        [-2.4498e-05, -1.9670e-05,  6.8992e-06,  ..., -2.1324e-05,
         -4.9211e-06, -1.3225e-05],
        [ 3.2859e-04,  2.7912e-04, -8.7318e-05,  ...,  2.9203e-04,
          7.5746e-05,  2.1579e-04],
        [ 6.2191e-05,  5.3344e-05, -3.1959e-05,  ...,  5.4025e-05,
          1.5044e-05,  4.4523e-05],
        [-5.2899e-05, -4.2439e-05,  1.4871e-05,  ..., -4.6015e-05,
         -1.0602e-05, -2.8551e-05]], device='cuda:0')
Loss: 1.1447316408157349
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0022, -0.0340],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0165,  0.0147, -0.0272,  ...,  0.0282, -0.0157, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7779e-05, -1.3715e-05, -6.9105e-05,  ...,  1.6546e-06,
         -3.7997e-05,  1.6552e-05],
        [-2.0244e-05, -1.5967e-05,  4.1793e-06,  ..., -1.7339e-05,
         -3.8781e-06, -9.1694e-06],
        [ 7.4517e-05,  5.8713e-05, -1.7100e-05,  ...,  6.7872e-05,
          1.5679e-05,  4.2397e-05],
        [ 3.3248e-05,  2.8259e-05, -1.1641e-05,  ...,  2.2953e-05,
          9.5048e-06,  1.7596e-05],
        [-5.3361e-05, -4.1693e-05,  1.1953e-05,  ..., -4.6629e-05,
         -1.0795e-05, -2.4132e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1724e-08, 2.7873e-08, 3.5045e-08,  ..., 1.1543e-08, 8.7202e-08,
         1.1396e-08],
        [5.0146e-11, 2.7979e-11, 3.1552e-12,  ..., 3.3880e-11, 2.1152e-12,
         7.9045e-12],
        [6.7978e-10, 3.8848e-10, 2.8912e-11,  ..., 5.8978e-10, 3.6383e-11,
         1.7415e-10],
        [3.2875e-10, 2.1257e-10, 2.3296e-11,  ..., 1.8998e-10, 2.0420e-11,
         7.0482e-11],
        [2.0561e-10, 1.1089e-10, 8.6530e-12,  ..., 1.4715e-10, 8.0055e-12,
         3.4703e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 50


Running epoch 0, step 400, batch 400
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3025e-05, -2.4363e-05, -2.5808e-05,  ...,  1.6051e-05,
         -4.5727e-06, -1.1457e-05],
        [-3.0547e-06, -2.4289e-06,  9.2015e-07,  ..., -2.6673e-06,
         -6.2585e-07, -1.6987e-06],
        [-3.2037e-06, -2.5630e-06,  9.6858e-07,  ..., -2.8014e-06,
         -6.5565e-07, -1.7881e-06],
        [-3.0845e-06, -2.4736e-06,  9.3505e-07,  ..., -2.7120e-06,
         -6.2957e-07, -1.7211e-06],
        [-6.5267e-06, -5.2154e-06,  1.9670e-06,  ..., -5.6922e-06,
         -1.3337e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.1175075769424438


Running epoch 0, step 401, batch 401
Sampled inputs[:2]: tensor([[    0,  4602,  2387,  ..., 11616,    14, 18434],
        [    0,  6328,    12,  ...,   417,   199,  1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3292e-05, -4.5881e-05, -3.4656e-05,  ...,  3.7202e-05,
          6.2120e-07, -6.3885e-06],
        [-6.1244e-06, -4.8429e-06,  1.8254e-06,  ..., -5.3197e-06,
         -1.2144e-06, -3.4198e-06],
        [-6.4373e-06, -5.1111e-06,  1.9222e-06,  ..., -5.6028e-06,
         -1.2740e-06, -3.5986e-06],
        [-6.1989e-06, -4.9174e-06,  1.8589e-06,  ..., -5.4091e-06,
         -1.2256e-06, -3.4645e-06],
        [-1.3083e-05, -1.0371e-05,  3.9041e-06,  ..., -1.1384e-05,
         -2.5928e-06, -7.2867e-06]], device='cuda:0')
Loss: 1.1439810991287231


Running epoch 0, step 402, batch 402
Sampled inputs[:2]: tensor([[    0,   199,   769,  ...,   380,   560,   199],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2808e-05, -5.2720e-05, -4.4257e-05,  ...,  2.2783e-05,
          3.7691e-05, -2.1842e-06],
        [-9.1940e-06, -7.2420e-06,  2.7195e-06,  ..., -7.9572e-06,
         -1.8328e-06, -5.1409e-06],
        [-9.6709e-06, -7.6443e-06,  2.8647e-06,  ..., -8.3894e-06,
         -1.9260e-06, -5.4166e-06],
        [-9.3281e-06, -7.3612e-06,  2.7716e-06,  ..., -8.1062e-06,
         -1.8515e-06, -5.2229e-06],
        [-1.9729e-05, -1.5557e-05,  5.8413e-06,  ..., -1.7107e-05,
         -3.9265e-06, -1.1012e-05]], device='cuda:0')
Loss: 1.1406785249710083


Running epoch 0, step 403, batch 403
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,   462,   221,  ...,    29,   413,  1801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1716e-05, -4.3545e-05, -5.0618e-05,  ...,  4.8997e-05,
          1.8655e-05,  1.8509e-05],
        [-1.2279e-05, -9.6709e-06,  3.6545e-06,  ..., -1.0625e-05,
         -2.4922e-06, -6.8843e-06],
        [-1.2919e-05, -1.0207e-05,  3.8482e-06,  ..., -1.1206e-05,
         -2.6189e-06, -7.2494e-06],
        [-1.2442e-05, -9.8199e-06,  3.7178e-06,  ..., -1.0803e-05,
         -2.5108e-06, -6.9812e-06],
        [-2.6345e-05, -2.0742e-05,  7.8380e-06,  ..., -2.2799e-05,
         -5.3346e-06, -1.4737e-05]], device='cuda:0')
Loss: 1.1487088203430176


Running epoch 0, step 404, batch 404
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0,   346,   462,  ...,  2915,   275,  2565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0560e-05, -4.8013e-05, -9.2075e-05,  ...,  6.6002e-05,
          4.1296e-05, -6.3268e-06],
        [-1.5303e-05, -1.2085e-05,  4.5784e-06,  ..., -1.3277e-05,
         -3.1143e-06, -8.5980e-06],
        [-1.6123e-05, -1.2755e-05,  4.8243e-06,  ..., -1.4007e-05,
         -3.2745e-06, -9.0599e-06],
        [-1.5512e-05, -1.2264e-05,  4.6566e-06,  ..., -1.3500e-05,
         -3.1367e-06, -8.7097e-06],
        [-3.2842e-05, -2.5898e-05,  9.8050e-06,  ..., -2.8461e-05,
         -6.6608e-06, -1.8388e-05]], device='cuda:0')
Loss: 1.1443800926208496


Running epoch 0, step 405, batch 405
Sampled inputs[:2]: tensor([[    0,    13,  4831,  ...,   333,   199,  2038],
        [    0,   341, 22766,  ...,   271,   266,  1176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7093e-05, -2.9017e-05, -1.2144e-04,  ...,  4.6503e-05,
          2.3478e-05,  3.9051e-05],
        [-1.8343e-05, -1.4514e-05,  5.5060e-06,  ..., -1.5944e-05,
         -3.7663e-06, -1.0327e-05],
        [-1.9327e-05, -1.5318e-05,  5.8003e-06,  ..., -1.6823e-05,
         -3.9563e-06, -1.0878e-05],
        [-1.8567e-05, -1.4707e-05,  5.5879e-06,  ..., -1.6183e-05,
         -3.7849e-06, -1.0438e-05],
        [-3.9339e-05, -3.1114e-05,  1.1787e-05,  ..., -3.4183e-05,
         -8.0541e-06, -2.2084e-05]], device='cuda:0')
Loss: 1.1260221004486084


Running epoch 0, step 406, batch 406
Sampled inputs[:2]: tensor([[    0,   292,   263,  ...,   342,  4575,   271],
        [    0,  4347,   638,  ...,  1345,   292, 15343]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5759e-05, -1.1354e-05, -1.5563e-04,  ...,  1.2060e-05,
          1.3078e-05,  4.9704e-05],
        [-2.1398e-05, -1.6928e-05,  6.4485e-06,  ..., -1.8626e-05,
         -4.3623e-06, -1.2040e-05],
        [-2.2516e-05, -1.7822e-05,  6.7838e-06,  ..., -1.9625e-05,
         -4.5747e-06, -1.2659e-05],
        [-2.1651e-05, -1.7136e-05,  6.5416e-06,  ..., -1.8895e-05,
         -4.3809e-06, -1.2159e-05],
        [-4.5896e-05, -3.6269e-05,  1.3813e-05,  ..., -3.9935e-05,
         -9.3281e-06, -2.5749e-05]], device='cuda:0')
Loss: 1.1751528978347778


Running epoch 0, step 407, batch 407
Sampled inputs[:2]: tensor([[   0,  266, 7264,  ..., 3211,  328,  275],
        [   0,  259, 2283,  ...,  462,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2429e-04, -4.1740e-05, -1.9592e-04,  ...,  1.2005e-05,
          1.1255e-05,  3.9583e-05],
        [-2.4423e-05, -1.9342e-05,  7.3649e-06,  ..., -2.1279e-05,
         -4.9770e-06, -1.3717e-05],
        [-2.5719e-05, -2.0370e-05,  7.7523e-06,  ..., -2.2426e-05,
         -5.2191e-06, -1.4432e-05],
        [-2.4721e-05, -1.9580e-05,  7.4729e-06,  ..., -2.1592e-05,
         -4.9993e-06, -1.3866e-05],
        [-5.2363e-05, -4.1425e-05,  1.5765e-05,  ..., -4.5598e-05,
         -1.0632e-05, -2.9340e-05]], device='cuda:0')
Loss: 1.132555365562439
Graident accumulation at epoch 0, step 407, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0023, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0012],
        [-0.0165,  0.0147, -0.0273,  ...,  0.0282, -0.0157, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7430e-05, -1.6518e-05, -8.1787e-05,  ...,  2.6896e-06,
         -3.3071e-05,  1.8855e-05],
        [-2.0662e-05, -1.6305e-05,  4.4978e-06,  ..., -1.7733e-05,
         -3.9880e-06, -9.6241e-06],
        [ 6.4494e-05,  5.0805e-05, -1.4615e-05,  ...,  5.8843e-05,
          1.3589e-05,  3.6714e-05],
        [ 2.7451e-05,  2.3475e-05, -9.7298e-06,  ...,  1.8499e-05,
          8.0544e-06,  1.4450e-05],
        [-5.3261e-05, -4.1666e-05,  1.2334e-05,  ..., -4.6526e-05,
         -1.0778e-05, -2.4653e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1697e-08, 2.7847e-08, 3.5048e-08,  ..., 1.1532e-08, 8.7115e-08,
         1.1386e-08],
        [5.0692e-11, 2.8325e-11, 3.2063e-12,  ..., 3.4299e-11, 2.1378e-12,
         8.0847e-12],
        [6.7976e-10, 3.8850e-10, 2.8943e-11,  ..., 5.8970e-10, 3.6374e-11,
         1.7418e-10],
        [3.2903e-10, 2.1274e-10, 2.3328e-11,  ..., 1.9026e-10, 2.0425e-11,
         7.0604e-11],
        [2.0815e-10, 1.1250e-10, 8.8929e-12,  ..., 1.4909e-10, 8.1105e-12,
         3.5529e-11]], device='cuda:0')
optimizer state dict: 51.0
lr: [1.8611432270000978e-05, 1.8611432270000978e-05]
scheduler_last_epoch: 51


Running epoch 0, step 408, batch 408
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   680,   401,  1407],
        [    0,  1192, 11929,  ...,   266,  1551,  1860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0957e-05,  4.6667e-05,  1.1411e-06,  ...,  1.5101e-05,
          3.6489e-06,  2.3898e-05],
        [-3.0696e-06, -2.3544e-06,  9.9838e-07,  ..., -2.6226e-06,
         -6.4075e-07, -1.7509e-06],
        [-3.2634e-06, -2.5034e-06,  1.0580e-06,  ..., -2.7865e-06,
         -6.7800e-07, -1.8626e-06],
        [-3.1143e-06, -2.3991e-06,  1.0133e-06,  ..., -2.6673e-06,
         -6.4448e-07, -1.7807e-06],
        [-6.5267e-06, -5.0068e-06,  2.1309e-06,  ..., -5.5730e-06,
         -1.3560e-06, -3.7402e-06]], device='cuda:0')
Loss: 1.1254891157150269


Running epoch 0, step 409, batch 409
Sampled inputs[:2]: tensor([[    0,  7110,   437,  ...,   266,  6724,  2655],
        [    0, 14949,    12,  ...,   669, 10168,  7166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0872e-05,  6.2612e-05, -1.3779e-05,  ...,  3.3532e-05,
         -1.1793e-05,  3.0234e-05],
        [-6.1095e-06, -4.7386e-06,  2.0042e-06,  ..., -5.2899e-06,
         -1.2554e-06, -3.5092e-06],
        [-6.4820e-06, -5.0217e-06,  2.1160e-06,  ..., -5.6028e-06,
         -1.3225e-06, -3.7178e-06],
        [-6.2138e-06, -4.8280e-06,  2.0340e-06,  ..., -5.3793e-06,
         -1.2629e-06, -3.5688e-06],
        [-1.2994e-05, -1.0073e-05,  4.2617e-06,  ..., -1.1235e-05,
         -2.6524e-06, -7.4655e-06]], device='cuda:0')
Loss: 1.1380037069320679


Running epoch 0, step 410, batch 410
Sampled inputs[:2]: tensor([[   0, 3087,  401,  ..., 1875, 4122,  278],
        [   0, 1615,  328,  ...,  266, 3133,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0405e-05,  6.2395e-05,  3.0377e-06,  ...,  2.0529e-05,
         -3.0487e-05,  2.0910e-06],
        [-9.1493e-06, -7.0632e-06,  3.0026e-06,  ..., -7.9423e-06,
         -1.8291e-06, -5.2601e-06],
        [-9.6709e-06, -7.4655e-06,  3.1665e-06,  ..., -8.3894e-06,
         -1.9222e-06, -5.5581e-06],
        [-9.3728e-06, -7.2420e-06,  3.0696e-06,  ..., -8.1360e-06,
         -1.8515e-06, -5.3868e-06],
        [-1.9491e-05, -1.5050e-05,  6.3926e-06,  ..., -1.6928e-05,
         -3.8743e-06, -1.1206e-05]], device='cuda:0')
Loss: 1.1462466716766357


Running epoch 0, step 411, batch 411
Sampled inputs[:2]: tensor([[    0,    12,   358,  ...,   352,   266,   319],
        [    0,    13, 41550,  ...,    12,   546,  1996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5785e-05,  9.7077e-05, -1.0109e-06,  ...,  2.5775e-05,
         -3.7959e-05,  3.2865e-05],
        [-1.2174e-05, -9.4175e-06,  3.9786e-06,  ..., -1.0580e-05,
         -2.4550e-06, -7.0035e-06],
        [-1.2890e-05, -9.9689e-06,  4.2021e-06,  ..., -1.1191e-05,
         -2.5854e-06, -7.4133e-06],
        [-1.2487e-05, -9.6560e-06,  4.0755e-06,  ..., -1.0848e-05,
         -2.4885e-06, -7.1749e-06],
        [-2.5958e-05, -2.0087e-05,  8.4788e-06,  ..., -2.2560e-05,
         -5.2080e-06, -1.4931e-05]], device='cuda:0')
Loss: 1.1343456506729126


Running epoch 0, step 412, batch 412
Sampled inputs[:2]: tensor([[   0,  767, 9289,  ...,  494,  287, 8957],
        [   0,  857,  352,  ..., 3608,  271,  995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6448e-05,  1.0730e-04,  7.9765e-06,  ...,  4.4084e-06,
         -5.0338e-05,  1.1212e-06],
        [-1.5184e-05, -1.1787e-05,  4.9919e-06,  ..., -1.3232e-05,
         -3.0473e-06, -8.7544e-06],
        [-1.6093e-05, -1.2487e-05,  5.2750e-06,  ..., -1.4007e-05,
         -3.2112e-06, -9.2685e-06],
        [-1.5587e-05, -1.2085e-05,  5.1185e-06,  ..., -1.3575e-05,
         -3.0957e-06, -8.9705e-06],
        [-3.2425e-05, -2.5153e-05,  1.0654e-05,  ..., -2.8253e-05,
         -6.4746e-06, -1.8686e-05]], device='cuda:0')
Loss: 1.1258987188339233


Running epoch 0, step 413, batch 413
Sampled inputs[:2]: tensor([[    0, 25845,  4034,  ...,   474,   221,   474],
        [    0,   292,   380,  ...,  1725,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5190e-05,  1.7376e-04, -7.5147e-06,  ..., -1.0999e-05,
         -7.7462e-05,  3.5508e-07],
        [-1.8209e-05, -1.4126e-05,  5.9605e-06,  ..., -1.5870e-05,
         -3.6471e-06, -1.0498e-05],
        [-1.9297e-05, -1.4976e-05,  6.2957e-06,  ..., -1.6809e-05,
         -3.8445e-06, -1.1116e-05],
        [-1.8701e-05, -1.4499e-05,  6.1169e-06,  ..., -1.6302e-05,
         -3.7067e-06, -1.0766e-05],
        [-3.8862e-05, -3.0130e-05,  1.2711e-05,  ..., -3.3885e-05,
         -7.7486e-06, -2.2382e-05]], device='cuda:0')
Loss: 1.1247239112854004


Running epoch 0, step 414, batch 414
Sampled inputs[:2]: tensor([[   0, 3951,   77,  ..., 7062,  278,  600],
        [   0,  278, 1478,  ...,  266, 1607, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1370e-04,  1.4672e-04, -1.9593e-05,  ..., -1.0430e-05,
         -6.5270e-05, -1.8672e-05],
        [-2.1249e-05, -1.6510e-05,  6.9439e-06,  ..., -1.8477e-05,
         -4.2208e-06, -1.2234e-05],
        [-2.2531e-05, -1.7509e-05,  7.3463e-06,  ..., -1.9580e-05,
         -4.4554e-06, -1.2964e-05],
        [-2.1860e-05, -1.6972e-05,  7.1451e-06,  ..., -1.9014e-05,
         -4.2990e-06, -1.2562e-05],
        [-4.5389e-05, -3.5226e-05,  1.4827e-05,  ..., -3.9458e-05,
         -8.9779e-06, -2.6092e-05]], device='cuda:0')
Loss: 1.1156657934188843


Running epoch 0, step 415, batch 415
Sampled inputs[:2]: tensor([[    0, 28684,   472,  ...,   317,     9,  1926],
        [    0,    14,  2787,  ...,  9674,  2491,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0785e-04,  1.3192e-04, -8.0601e-05,  ..., -8.6562e-06,
         -7.0287e-05, -4.6129e-05],
        [-2.4289e-05, -1.8865e-05,  7.9498e-06,  ..., -2.1115e-05,
         -4.8280e-06, -1.3977e-05],
        [-2.5749e-05, -1.9997e-05,  8.4043e-06,  ..., -2.2367e-05,
         -5.0962e-06, -1.4812e-05],
        [-2.4989e-05, -1.9401e-05,  8.1807e-06,  ..., -2.1726e-05,
         -4.9211e-06, -1.4357e-05],
        [-5.1886e-05, -4.0263e-05,  1.6972e-05,  ..., -4.5061e-05,
         -1.0274e-05, -2.9817e-05]], device='cuda:0')
Loss: 1.1175179481506348
Graident accumulation at epoch 0, step 415, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0012],
        [-0.0165,  0.0147, -0.0273,  ...,  0.0282, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4472e-05, -1.6736e-06, -8.1668e-05,  ...,  1.5551e-06,
         -3.6793e-05,  1.2357e-05],
        [-2.1025e-05, -1.6561e-05,  4.8430e-06,  ..., -1.8071e-05,
         -4.0720e-06, -1.0059e-05],
        [ 5.5469e-05,  4.3724e-05, -1.2313e-05,  ...,  5.0722e-05,
          1.1721e-05,  3.1561e-05],
        [ 2.2207e-05,  1.9187e-05, -7.9387e-06,  ...,  1.4476e-05,
          6.7569e-06,  1.1570e-05],
        [-5.3124e-05, -4.1526e-05,  1.2798e-05,  ..., -4.6380e-05,
         -1.0728e-05, -2.5169e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1667e-08, 2.7837e-08, 3.5020e-08,  ..., 1.1520e-08, 8.7032e-08,
         1.1377e-08],
        [5.1232e-11, 2.8653e-11, 3.2663e-12,  ..., 3.4711e-11, 2.1590e-12,
         8.2720e-12],
        [6.7974e-10, 3.8851e-10, 2.8985e-11,  ..., 5.8961e-10, 3.6364e-11,
         1.7423e-10],
        [3.2933e-10, 2.1290e-10, 2.3372e-11,  ..., 1.9054e-10, 2.0429e-11,
         7.0739e-11],
        [2.1063e-10, 1.1400e-10, 9.1720e-12,  ..., 1.5097e-10, 8.2079e-12,
         3.6383e-11]], device='cuda:0')
optimizer state dict: 52.0
lr: [1.8547933878823103e-05, 1.8547933878823103e-05]
scheduler_last_epoch: 52


Running epoch 0, step 416, batch 416
Sampled inputs[:2]: tensor([[    0, 15912,    14,  ..., 25535,    18,  3947],
        [    0,   607, 27288,  ...,   445,  4712,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0753e-05,  2.8927e-05, -4.9117e-06,  ...,  1.3918e-06,
         -6.3440e-07,  1.4507e-05],
        [-3.0249e-06, -2.3097e-06,  1.0654e-06,  ..., -2.6524e-06,
         -5.9232e-07, -1.7956e-06],
        [-3.2037e-06, -2.4438e-06,  1.1250e-06,  ..., -2.8014e-06,
         -6.1840e-07, -1.8924e-06],
        [-3.1590e-06, -2.3991e-06,  1.1101e-06,  ..., -2.7716e-06,
         -6.0722e-07, -1.8701e-06],
        [-6.4075e-06, -4.8876e-06,  2.2501e-06,  ..., -5.6326e-06,
         -1.2442e-06, -3.7998e-06]], device='cuda:0')
Loss: 1.1303683519363403


Running epoch 0, step 417, batch 417
Sampled inputs[:2]: tensor([[   0, 3393, 3380,  ...,  292, 6502,  950],
        [   0, 1500,  367,  ...,  344, 4250,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0046e-05,  4.6085e-05, -1.3104e-06,  ..., -1.1711e-05,
         -1.3662e-05, -1.5842e-06],
        [-6.0499e-06, -4.5896e-06,  2.1160e-06,  ..., -5.2750e-06,
         -1.1809e-06, -3.5912e-06],
        [-6.3926e-06, -4.8429e-06,  2.2352e-06,  ..., -5.5581e-06,
         -1.2368e-06, -3.7849e-06],
        [-6.3032e-06, -4.7684e-06,  2.2054e-06,  ..., -5.4985e-06,
         -1.2144e-06, -3.7402e-06],
        [-1.2755e-05, -9.6858e-06,  4.4554e-06,  ..., -1.1146e-05,
         -2.4736e-06, -7.5698e-06]], device='cuda:0')
Loss: 1.1164462566375732


Running epoch 0, step 418, batch 418
Sampled inputs[:2]: tensor([[    0, 40995,  5863,  ...,    13,  9819,   609],
        [    0,    55,  2258,  ..., 32764,    75,   338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3382e-05,  7.9345e-05,  9.9231e-07,  ..., -1.6915e-06,
         -2.5132e-05,  1.6948e-05],
        [-9.0450e-06, -6.8396e-06,  3.1814e-06,  ..., -7.8678e-06,
         -1.7695e-06, -5.3793e-06],
        [-9.5963e-06, -7.2420e-06,  3.3751e-06,  ..., -8.3447e-06,
         -1.8626e-06, -5.6922e-06],
        [-9.4622e-06, -7.1377e-06,  3.3304e-06,  ..., -8.2403e-06,
         -1.8291e-06, -5.6177e-06],
        [-1.9133e-05, -1.4454e-05,  6.7204e-06,  ..., -1.6659e-05,
         -3.7178e-06, -1.1370e-05]], device='cuda:0')
Loss: 1.125869631767273


Running epoch 0, step 419, batch 419
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,  9677,   609,  ...,   199,  1919,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8325e-05,  6.7589e-05,  1.3161e-05,  ...,  1.8988e-05,
         -4.1836e-05,  4.9601e-05],
        [-1.2040e-05, -9.0897e-06,  4.2915e-06,  ..., -1.0476e-05,
         -2.3805e-06, -7.1675e-06],
        [-1.2785e-05, -9.6411e-06,  4.5598e-06,  ..., -1.1131e-05,
         -2.5108e-06, -7.5996e-06],
        [-1.2577e-05, -9.4771e-06,  4.4927e-06,  ..., -1.0967e-05,
         -2.4624e-06, -7.4804e-06],
        [-2.5511e-05, -1.9222e-05,  9.0897e-06,  ..., -2.2233e-05,
         -5.0142e-06, -1.5169e-05]], device='cuda:0')
Loss: 1.1351937055587769


Running epoch 0, step 420, batch 420
Sampled inputs[:2]: tensor([[    0,   292,    17,  ...,  5760,  1345,   578],
        [    0,   775,   721,  ...,  5650,   518, 11548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1968e-05,  9.5102e-05,  1.8588e-05,  ...,  4.0413e-05,
         -6.8806e-05,  5.3591e-05],
        [-1.5065e-05, -1.1384e-05,  5.3495e-06,  ..., -1.3098e-05,
         -3.0138e-06, -8.9705e-06],
        [-1.5989e-05, -1.2085e-05,  5.6773e-06,  ..., -1.3918e-05,
         -3.1814e-06, -9.5069e-06],
        [-1.5706e-05, -1.1876e-05,  5.5879e-06,  ..., -1.3694e-05,
         -3.1143e-06, -9.3505e-06],
        [-3.1888e-05, -2.4080e-05,  1.1310e-05,  ..., -2.7776e-05,
         -6.3479e-06, -1.8969e-05]], device='cuda:0')
Loss: 1.1169873476028442


Running epoch 0, step 421, batch 421
Sampled inputs[:2]: tensor([[    0,  6022,   644,  ..., 14834,  3554,   591],
        [    0,   287, 17044,  ...,   496,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9059e-05,  9.9949e-05,  1.6574e-05,  ...,  6.0406e-05,
         -7.3951e-05,  8.0253e-05],
        [-1.8090e-05, -1.3679e-05,  6.4224e-06,  ..., -1.5706e-05,
         -3.6769e-06, -1.0796e-05],
        [-1.9208e-05, -1.4529e-05,  6.8173e-06,  ..., -1.6704e-05,
         -3.8855e-06, -1.1444e-05],
        [-1.8850e-05, -1.4260e-05,  6.6981e-06,  ..., -1.6406e-05,
         -3.7998e-06, -1.1243e-05],
        [-3.8356e-05, -2.8968e-05,  1.3590e-05,  ..., -3.3349e-05,
         -7.7635e-06, -2.2873e-05]], device='cuda:0')
Loss: 1.1409391164779663


Running epoch 0, step 422, batch 422
Sampled inputs[:2]: tensor([[    0,   968,   266,  ...,   287,  2143, 15228],
        [    0,   504,   546,  ...,   634,   328,   630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7903e-05,  9.9916e-05,  3.7390e-05,  ...,  4.9511e-05,
         -9.3184e-05,  4.4782e-05],
        [-2.1085e-05, -1.5929e-05,  7.5325e-06,  ..., -1.8314e-05,
         -4.2766e-06, -1.2577e-05],
        [-2.2426e-05, -1.6943e-05,  8.0094e-06,  ..., -1.9506e-05,
         -4.5262e-06, -1.3351e-05],
        [-2.2024e-05, -1.6645e-05,  7.8753e-06,  ..., -1.9163e-05,
         -4.4256e-06, -1.3120e-05],
        [-4.4793e-05, -3.3796e-05,  1.5974e-05,  ..., -3.8922e-05,
         -9.0376e-06, -2.6673e-05]], device='cuda:0')
Loss: 1.136609673500061


Running epoch 0, step 423, batch 423
Sampled inputs[:2]: tensor([[   0, 6847,  437,  ...,   17,   14,   16],
        [   0,  277,  279,  ...,   12,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4026e-05,  6.8065e-05,  1.7495e-05,  ...,  5.8050e-05,
         -7.0167e-05,  6.0795e-05],
        [-2.4095e-05, -1.8194e-05,  8.5756e-06,  ..., -2.0936e-05,
         -4.9211e-06, -1.4350e-05],
        [-2.5645e-05, -1.9372e-05,  9.1270e-06,  ..., -2.2307e-05,
         -5.2154e-06, -1.5251e-05],
        [-2.5138e-05, -1.8999e-05,  8.9630e-06,  ..., -2.1890e-05,
         -5.0887e-06, -1.4961e-05],
        [-5.1230e-05, -3.8654e-05,  1.8209e-05,  ..., -4.4525e-05,
         -1.0416e-05, -3.0473e-05]], device='cuda:0')
Loss: 1.1342252492904663
Graident accumulation at epoch 0, step 423, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7427e-05,  5.3002e-06, -7.1752e-05,  ...,  7.2045e-06,
         -4.0130e-05,  1.7201e-05],
        [-2.1332e-05, -1.6724e-05,  5.2163e-06,  ..., -1.8357e-05,
         -4.1569e-06, -1.0488e-05],
        [ 4.7358e-05,  3.7415e-05, -1.0169e-05,  ...,  4.3419e-05,
          1.0027e-05,  2.6880e-05],
        [ 1.7473e-05,  1.5369e-05, -6.2485e-06,  ...,  1.0840e-05,
          5.5723e-06,  8.9165e-06],
        [-5.2934e-05, -4.1239e-05,  1.3339e-05,  ..., -4.6194e-05,
         -1.0697e-05, -2.5699e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1631e-08, 2.7814e-08, 3.4985e-08,  ..., 1.1512e-08, 8.6950e-08,
         1.1369e-08],
        [5.1761e-11, 2.8955e-11, 3.3365e-12,  ..., 3.5115e-11, 2.1810e-12,
         8.4697e-12],
        [6.7972e-10, 3.8850e-10, 2.9039e-11,  ..., 5.8952e-10, 3.6354e-11,
         1.7429e-10],
        [3.2963e-10, 2.1305e-10, 2.3429e-11,  ..., 1.9083e-10, 2.0434e-11,
         7.0892e-11],
        [2.1304e-10, 1.1538e-10, 9.4944e-12,  ..., 1.5280e-10, 8.3082e-12,
         3.7275e-11]], device='cuda:0')
optimizer state dict: 53.0
lr: [1.8483129288732575e-05, 1.8483129288732575e-05]
scheduler_last_epoch: 53


Running epoch 0, step 424, batch 424
Sampled inputs[:2]: tensor([[    0,   452,   298,  ...,   287,  1575,  7856],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1637e-06, -1.2014e-05, -1.7650e-05,  ..., -1.1496e-05,
         -1.4436e-05,  1.9739e-05],
        [-2.9504e-06, -2.1756e-06,  1.1623e-06,  ..., -2.5779e-06,
         -6.2585e-07, -1.8403e-06],
        [-3.1888e-06, -2.3544e-06,  1.2517e-06,  ..., -2.7865e-06,
         -6.7055e-07, -1.9819e-06],
        [-3.1292e-06, -2.3097e-06,  1.2293e-06,  ..., -2.7418e-06,
         -6.5565e-07, -1.9521e-06],
        [-6.3181e-06, -4.6492e-06,  2.4736e-06,  ..., -5.5134e-06,
         -1.3337e-06, -3.9041e-06]], device='cuda:0')
Loss: 1.1179313659667969


Running epoch 0, step 425, batch 425
Sampled inputs[:2]: tensor([[    0,  1869,   596,  ..., 13055, 17051,   578],
        [    0,   278,  1253,  ...,   266,  1274, 22300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7344e-06, -7.3479e-05, -6.0565e-05,  ...,  3.4035e-05,
         -4.9063e-05,  2.1732e-05],
        [-5.9307e-06, -4.3660e-06,  2.3246e-06,  ..., -5.1409e-06,
         -1.2144e-06, -3.6359e-06],
        [-6.4075e-06, -4.7237e-06,  2.5108e-06,  ..., -5.5581e-06,
         -1.3001e-06, -3.9190e-06],
        [-6.2883e-06, -4.6343e-06,  2.4661e-06,  ..., -5.4687e-06,
         -1.2740e-06, -3.8520e-06],
        [-1.2666e-05, -9.3281e-06,  4.9472e-06,  ..., -1.0997e-05,
         -2.5854e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.1219788789749146


Running epoch 0, step 426, batch 426
Sampled inputs[:2]: tensor([[    0,  7377, 30662,  ...,   287,   694, 13403],
        [    0,  1823,    12,  ...,  1874,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4343e-05, -2.9480e-05, -8.0379e-05,  ...,  3.3040e-05,
         -1.8206e-05,  7.3232e-05],
        [-8.9407e-06, -6.6012e-06,  3.4571e-06,  ..., -7.7486e-06,
         -1.8440e-06, -5.4389e-06],
        [-9.6411e-06, -7.1228e-06,  3.7253e-06,  ..., -8.3596e-06,
         -1.9707e-06, -5.8562e-06],
        [-9.4175e-06, -6.9439e-06,  3.6359e-06,  ..., -8.1658e-06,
         -1.9185e-06, -5.7146e-06],
        [-1.9103e-05, -1.4096e-05,  7.3612e-06,  ..., -1.6570e-05,
         -3.9190e-06, -1.1563e-05]], device='cuda:0')
Loss: 1.124440312385559


Running epoch 0, step 427, batch 427
Sampled inputs[:2]: tensor([[   0,  292,  380,  ..., 9636,  417,  199],
        [   0,  635,   13,  ...,   13, 4710, 1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4072e-04,  2.2966e-05, -9.0654e-05,  ...,  3.0916e-05,
          1.3215e-05,  1.3123e-04],
        [-1.1936e-05, -8.8066e-06,  4.6268e-06,  ..., -1.0401e-05,
         -2.4922e-06, -7.3016e-06],
        [-1.2815e-05, -9.4622e-06,  4.9695e-06,  ..., -1.1176e-05,
         -2.6561e-06, -7.8380e-06],
        [-1.2562e-05, -9.2536e-06,  4.8652e-06,  ..., -1.0937e-05,
         -2.5928e-06, -7.6666e-06],
        [-2.5421e-05, -1.8746e-05,  9.8199e-06,  ..., -2.2143e-05,
         -5.2750e-06, -1.5467e-05]], device='cuda:0')
Loss: 1.1415342092514038


Running epoch 0, step 428, batch 428
Sampled inputs[:2]: tensor([[    0,   471,    14,  ..., 27104,     9,   631],
        [    0,  2663,   328,  ...,   342,   266,  1163]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1210e-04,  9.2228e-06, -6.9731e-05,  ...,  2.2651e-05,
          2.0864e-05,  1.3409e-04],
        [-1.4946e-05, -1.1027e-05,  5.7593e-06,  ..., -1.3009e-05,
         -3.0808e-06, -9.1270e-06],
        [-1.6034e-05, -1.1846e-05,  6.1840e-06,  ..., -1.3962e-05,
         -3.2820e-06, -9.7901e-06],
        [-1.5736e-05, -1.1608e-05,  6.0722e-06,  ..., -1.3694e-05,
         -3.2075e-06, -9.6038e-06],
        [ 1.3823e-04,  1.1555e-04, -4.8712e-05,  ...,  1.1041e-04,
          1.9945e-05,  6.6281e-05]], device='cuda:0')
Loss: 1.1316074132919312


Running epoch 0, step 429, batch 429
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,   361,  1224,  ...,  4401,  4261,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1591e-04,  2.5885e-05, -3.7408e-05,  ..., -1.3205e-05,
          2.7679e-05,  1.2741e-04],
        [-1.7956e-05, -1.3232e-05,  6.8918e-06,  ..., -1.5631e-05,
         -3.7178e-06, -1.0967e-05],
        [-1.9252e-05, -1.4201e-05,  7.3984e-06,  ..., -1.6764e-05,
         -3.9600e-06, -1.1757e-05],
        [ 2.7372e-04,  1.7936e-04, -1.0124e-04,  ...,  2.5586e-04,
          6.5898e-05,  1.6444e-04],
        [ 1.3179e-04,  1.1084e-04, -4.6283e-05,  ...,  1.0480e-04,
          1.8589e-05,  6.2347e-05]], device='cuda:0')
Loss: 1.1464600563049316


Running epoch 0, step 430, batch 430
Sampled inputs[:2]: tensor([[   0,   21,   13,  ...,   14,  747,  806],
        [   0, 2680,  271,  ..., 4971,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4541e-04,  7.5115e-06, -5.9145e-05,  ..., -1.8101e-05,
         -3.2951e-06,  1.8163e-04],
        [-2.0936e-05, -1.5453e-05,  8.0317e-06,  ..., -1.8224e-05,
         -4.3400e-06, -1.2822e-05],
        [-2.2456e-05, -1.6585e-05,  8.6278e-06,  ..., -1.9565e-05,
         -4.6268e-06, -1.3754e-05],
        [ 2.7058e-04,  1.7702e-04, -1.0004e-04,  ...,  2.5312e-04,
          6.5246e-05,  1.6249e-04],
        [ 1.2545e-04,  1.0613e-04, -4.3854e-05,  ...,  9.9261e-05,
          1.7262e-05,  5.8413e-05]], device='cuda:0')
Loss: 1.1146223545074463


Running epoch 0, step 431, batch 431
Sampled inputs[:2]: tensor([[   0,  272,  352,  ...,  590, 4361,  446],
        [   0,   14, 1845,  ...,  806,  352,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4827e-04,  1.5169e-05, -8.7614e-05,  ..., -1.5222e-05,
         -4.7217e-06,  2.1389e-04],
        [-2.3901e-05, -1.7658e-05,  9.2089e-06,  ..., -2.0862e-05,
         -5.0031e-06, -1.4663e-05],
        [-2.5660e-05, -1.8954e-05,  9.8944e-06,  ..., -2.2411e-05,
         -5.3383e-06, -1.5736e-05],
        [ 2.6746e-04,  1.7471e-04, -9.8799e-05,  ...,  2.5036e-04,
          6.4557e-05,  1.6055e-04],
        [ 1.1916e-04,  1.0145e-04, -4.1366e-05,  ...,  9.3658e-05,
          1.5854e-05,  5.4509e-05]], device='cuda:0')
Loss: 1.1359586715698242
Graident accumulation at epoch 0, step 431, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.7512e-05,  6.2871e-06, -7.3338e-05,  ...,  4.9619e-06,
         -3.6589e-05,  3.6869e-05],
        [-2.1589e-05, -1.6818e-05,  5.6155e-06,  ..., -1.8608e-05,
         -4.2415e-06, -1.0906e-05],
        [ 4.0056e-05,  3.1778e-05, -8.1629e-06,  ...,  3.6836e-05,
          8.4904e-06,  2.2618e-05],
        [ 4.2472e-05,  3.1303e-05, -1.5504e-05,  ...,  3.4792e-05,
          1.1471e-05,  2.4080e-05],
        [-3.5725e-05, -2.6970e-05,  7.8686e-06,  ..., -3.2209e-05,
         -8.0417e-06, -1.7679e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1612e-08, 2.7786e-08, 3.4958e-08,  ..., 1.1501e-08, 8.6863e-08,
         1.1404e-08],
        [5.2281e-11, 2.9238e-11, 3.4180e-12,  ..., 3.5515e-11, 2.2039e-12,
         8.6762e-12],
        [6.7970e-10, 3.8847e-10, 2.9108e-11,  ..., 5.8943e-10, 3.6347e-11,
         1.7436e-10],
        [4.0084e-10, 2.4336e-10, 3.3167e-11,  ..., 2.5332e-10, 2.4581e-11,
         9.6597e-11],
        [2.2703e-10, 1.2556e-10, 1.1196e-11,  ..., 1.6142e-10, 8.5513e-12,
         4.0209e-11]], device='cuda:0')
optimizer state dict: 54.0
lr: [1.8417028402436446e-05, 1.8417028402436446e-05]
scheduler_last_epoch: 54


Running epoch 0, step 432, batch 432
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,   368,   729,  ...,   221,   380,  2830]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9306e-05, -2.0254e-05, -3.7595e-05,  ...,  6.3742e-06,
         -2.2669e-05,  2.5976e-05],
        [-2.9802e-06, -2.1607e-06,  1.2219e-06,  ..., -2.5779e-06,
         -6.5193e-07, -1.8924e-06],
        [-3.2783e-06, -2.3842e-06,  1.3411e-06,  ..., -2.8312e-06,
         -7.0781e-07, -2.0713e-06],
        [-3.1888e-06, -2.3246e-06,  1.3113e-06,  ..., -2.7567e-06,
         -6.8918e-07, -2.0266e-06],
        [-6.4373e-06, -4.6492e-06,  2.6375e-06,  ..., -5.5432e-06,
         -1.3933e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.1092253923416138


Running epoch 0, step 433, batch 433
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ..., 1236,  768, 4316],
        [   0,  634, 1621,  ...,  688,  586, 8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7617e-05,  3.4943e-05, -5.3623e-05,  ..., -1.5927e-05,
         -9.0203e-06,  2.0117e-05],
        [-5.9456e-06, -4.3064e-06,  2.4661e-06,  ..., -5.1856e-06,
         -1.3188e-06, -3.7849e-06],
        [-6.4969e-06, -4.7237e-06,  2.6897e-06,  ..., -5.6624e-06,
         -1.4268e-06, -4.1276e-06],
        [-6.3479e-06, -4.6194e-06,  2.6375e-06,  ..., -5.5432e-06,
         -1.3933e-06, -4.0531e-06],
        [-1.2785e-05, -9.2387e-06,  5.3048e-06,  ..., -1.1146e-05,
         -2.8163e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1372891664505005


Running epoch 0, step 434, batch 434
Sampled inputs[:2]: tensor([[    0,    13,  1529,  ..., 15682,  1355,   259],
        [    0,   300,  5201,  ...,  1997,  7423,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0500e-05,  4.4848e-05, -7.8335e-05,  ..., -1.6122e-05,
          1.2669e-05,  1.7607e-05],
        [-8.8811e-06, -6.4522e-06,  3.7029e-06,  ..., -7.7784e-06,
         -1.9521e-06, -5.7071e-06],
        [-9.6709e-06, -7.0482e-06,  4.0233e-06,  ..., -8.4639e-06,
         -2.1048e-06, -6.1989e-06],
        [-9.4622e-06, -6.8992e-06,  3.9488e-06,  ..., -8.2850e-06,
         -2.0564e-06, -6.0797e-06],
        [-1.8984e-05, -1.3769e-05,  7.9274e-06,  ..., -1.6630e-05,
         -4.1500e-06, -1.2159e-05]], device='cuda:0')
Loss: 1.139930009841919


Running epoch 0, step 435, batch 435
Sampled inputs[:2]: tensor([[    0, 20241,  1244,  ...,  6232,  1004,   300],
        [    0,   953,   328,  ...,  2245,    12,  1253]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4631e-05,  8.4604e-05, -1.2180e-04,  ..., -1.6122e-05,
         -2.8991e-05, -1.1423e-05],
        [-1.1802e-05, -8.5682e-06,  4.9174e-06,  ..., -1.0356e-05,
         -2.5667e-06, -7.5921e-06],
        [-1.2830e-05, -9.3430e-06,  5.3421e-06,  ..., -1.1250e-05,
         -2.7679e-06, -8.2403e-06],
        [-1.2577e-05, -9.1642e-06,  5.2527e-06,  ..., -1.1042e-05,
         -2.7046e-06, -8.0913e-06],
        [-2.5243e-05, -1.8328e-05,  1.0535e-05,  ..., -2.2143e-05,
         -5.4613e-06, -1.6212e-05]], device='cuda:0')
Loss: 1.1287256479263306


Running epoch 0, step 436, batch 436
Sampled inputs[:2]: tensor([[   0, 4359, 5768,  ..., 4402,  292,   69],
        [   0,  221,  825,  ...,  616, 3661, 8052]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1839e-05,  1.1551e-04, -1.3277e-04,  ...,  1.1382e-06,
         -2.6382e-05, -8.6326e-06],
        [-1.4722e-05, -1.0669e-05,  6.1169e-06,  ..., -1.2934e-05,
         -3.1628e-06, -9.4697e-06],
        [-1.6019e-05, -1.1638e-05,  6.6459e-06,  ..., -1.4067e-05,
         -3.4124e-06, -1.0282e-05],
        [-1.5721e-05, -1.1429e-05,  6.5416e-06,  ..., -1.3813e-05,
         -3.3341e-06, -1.0103e-05],
        [-3.1590e-05, -2.2888e-05,  1.3128e-05,  ..., -2.7746e-05,
         -6.7502e-06, -2.0266e-05]], device='cuda:0')
Loss: 1.12981379032135


Running epoch 0, step 437, batch 437
Sampled inputs[:2]: tensor([[    0,   694,   266,  ...,  3007,   300,  5726],
        [    0,   278, 39533,  ...,   277,  1395, 47607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9694e-05,  1.0140e-04, -1.1751e-04,  ...,  3.1106e-05,
          1.4995e-05, -1.1608e-05],
        [-1.7688e-05, -1.2845e-05,  7.3463e-06,  ..., -1.5527e-05,
         -3.8147e-06, -1.1370e-05],
        [-1.9252e-05, -1.3992e-05,  7.9870e-06,  ..., -1.6898e-05,
         -4.1164e-06, -1.2338e-05],
        [-1.8880e-05, -1.3739e-05,  7.8529e-06,  ..., -1.6585e-05,
         -4.0196e-06, -1.2115e-05],
        [-3.7968e-05, -2.7537e-05,  1.5765e-05,  ..., -3.3319e-05,
         -8.1435e-06, -2.4319e-05]], device='cuda:0')
Loss: 1.1333905458450317


Running epoch 0, step 438, batch 438
Sampled inputs[:2]: tensor([[    0,   546,   360,  ...,  9107,  2772,  4496],
        [    0, 15003, 19278,  ...,   287,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0245e-05,  1.1198e-04, -1.2653e-04,  ...,  3.3151e-05,
          1.3937e-05,  1.2256e-05],
        [-2.0653e-05, -1.5035e-05,  8.5831e-06,  ..., -1.8150e-05,
         -4.4368e-06, -1.3269e-05],
        [-2.2471e-05, -1.6376e-05,  9.3281e-06,  ..., -1.9759e-05,
         -4.7870e-06, -1.4409e-05],
        [-2.2024e-05, -1.6063e-05,  9.1642e-06,  ..., -1.9372e-05,
         -4.6715e-06, -1.4126e-05],
        [-4.4256e-05, -3.2216e-05,  1.8388e-05,  ..., -3.8892e-05,
         -9.4622e-06, -2.8342e-05]], device='cuda:0')
Loss: 1.1264958381652832


Running epoch 0, step 439, batch 439
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0, 12165,    12,  ...,  2860, 10718,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0014e-06,  1.0748e-04, -1.5678e-04,  ...,  3.1848e-05,
          5.1138e-05,  5.9966e-05],
        [-2.3618e-05, -1.7211e-05,  9.8199e-06,  ..., -2.0757e-05,
         -5.1074e-06, -1.5222e-05],
        [-2.5719e-05, -1.8761e-05,  1.0684e-05,  ..., -2.2620e-05,
         -5.5134e-06, -1.6540e-05],
        [-2.5153e-05, -1.8358e-05,  1.0468e-05,  ..., -2.2128e-05,
         -5.3681e-06, -1.6168e-05],
        [-5.0604e-05, -3.6865e-05,  2.1026e-05,  ..., -4.4465e-05,
         -1.0885e-05, -3.2485e-05]], device='cuda:0')
Loss: 1.1342047452926636
Graident accumulation at epoch 0, step 439, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0148,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1160e-05,  1.6406e-05, -8.1682e-05,  ...,  7.6505e-06,
         -2.7817e-05,  3.9179e-05],
        [-2.1792e-05, -1.6857e-05,  6.0360e-06,  ..., -1.8823e-05,
         -4.3281e-06, -1.1337e-05],
        [ 3.3479e-05,  2.6724e-05, -6.2782e-06,  ...,  3.0890e-05,
          7.0900e-06,  1.8703e-05],
        [ 3.5709e-05,  2.6337e-05, -1.2906e-05,  ...,  2.9100e-05,
          9.7869e-06,  2.0055e-05],
        [-3.7213e-05, -2.7959e-05,  9.1843e-06,  ..., -3.3435e-05,
         -8.3261e-06, -1.9159e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1570e-08, 2.7770e-08, 3.4947e-08,  ..., 1.1490e-08, 8.6779e-08,
         1.1396e-08],
        [5.2786e-11, 2.9505e-11, 3.5110e-12,  ..., 3.5910e-11, 2.2278e-12,
         8.8992e-12],
        [6.7968e-10, 3.8843e-10, 2.9193e-11,  ..., 5.8935e-10, 3.6341e-11,
         1.7446e-10],
        [4.0107e-10, 2.4346e-10, 3.3243e-11,  ..., 2.5356e-10, 2.4586e-11,
         9.6762e-11],
        [2.2936e-10, 1.2680e-10, 1.1627e-11,  ..., 1.6323e-10, 8.6612e-12,
         4.1224e-11]], device='cuda:0')
optimizer state dict: 55.0
lr: [1.8349641320727145e-05, 1.8349641320727145e-05]
scheduler_last_epoch: 55


Running epoch 0, step 440, batch 440
Sampled inputs[:2]: tensor([[    0,   494,   298,  ...,   408, 32859, 14550],
        [    0,   278,  6653,  ...,  7524,   271, 28279]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3682e-05,  4.1790e-05, -2.1217e-05,  ..., -1.1169e-06,
          2.3926e-05, -1.9096e-05],
        [-2.9951e-06, -2.1607e-06,  1.2964e-06,  ..., -2.5928e-06,
         -7.7114e-07, -2.0266e-06],
        [-3.2187e-06, -2.3097e-06,  1.3933e-06,  ..., -2.7716e-06,
         -8.2329e-07, -2.1607e-06],
        [-3.1441e-06, -2.2650e-06,  1.3635e-06,  ..., -2.7269e-06,
         -8.0094e-07, -2.1160e-06],
        [-6.3777e-06, -4.5598e-06,  2.7567e-06,  ..., -5.4836e-06,
         -1.6317e-06, -4.2915e-06]], device='cuda:0')
Loss: 1.1417206525802612


Running epoch 0, step 441, batch 441
Sampled inputs[:2]: tensor([[    0,    17,  3737,  ...,   298,   396,   221],
        [    0,   381, 13565,  ...,     9,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2545e-04,  7.3754e-05, -2.8666e-05,  ..., -7.4699e-06,
          6.1398e-05, -3.3110e-05],
        [-5.9605e-06, -4.3660e-06,  2.5779e-06,  ..., -5.2005e-06,
         -1.4827e-06, -4.0233e-06],
        [-6.4373e-06, -4.6939e-06,  2.7791e-06,  ..., -5.6028e-06,
         -1.5870e-06, -4.3213e-06],
        [-6.2734e-06, -4.5896e-06,  2.7195e-06,  ..., -5.4836e-06,
         -1.5423e-06, -4.2170e-06],
        [-1.2726e-05, -9.2685e-06,  5.4985e-06,  ..., -1.1057e-05,
         -3.1441e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.1174635887145996


Running epoch 0, step 442, batch 442
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,   328,  2097,  ...,   365,  1941,   607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6051e-04,  1.3083e-04, -2.0282e-05,  ..., -1.7463e-05,
          8.7736e-05, -1.2400e-05],
        [-8.9109e-06, -6.4969e-06,  3.8669e-06,  ..., -7.8380e-06,
         -2.2501e-06, -6.0201e-06],
        [-9.6262e-06, -7.0035e-06,  4.1723e-06,  ..., -8.4490e-06,
         -2.4140e-06, -6.4671e-06],
        [-9.4026e-06, -6.8545e-06,  4.0829e-06,  ..., -8.2701e-06,
         -2.3469e-06, -6.3181e-06],
        [-1.9073e-05, -1.3858e-05,  8.2701e-06,  ..., -1.6689e-05,
         -4.7833e-06, -1.2815e-05]], device='cuda:0')
Loss: 1.150155782699585


Running epoch 0, step 443, batch 443
Sampled inputs[:2]: tensor([[    0,  3412,  1707,  ..., 11114,    15,  1821],
        [    0,  3256,   221,  ..., 18116,   292, 47989]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6954e-04,  1.0827e-04, -2.9124e-05,  ..., -7.0567e-06,
          8.3408e-05, -2.8079e-05],
        [-1.1861e-05, -8.5831e-06,  5.1707e-06,  ..., -1.0416e-05,
         -2.9355e-06, -7.9721e-06],
        [-1.2830e-05, -9.2834e-06,  5.5879e-06,  ..., -1.1265e-05,
         -3.1553e-06, -8.5980e-06],
        [-1.2547e-05, -9.0897e-06,  5.4762e-06,  ..., -1.1027e-05,
         -3.0696e-06, -8.4043e-06],
        [-2.5392e-05, -1.8358e-05,  1.1072e-05,  ..., -2.2233e-05,
         -6.2510e-06, -1.7017e-05]], device='cuda:0')
Loss: 1.1293219327926636


Running epoch 0, step 444, batch 444
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ..., 16948,   266,  1751],
        [    0, 18787, 27117,  ...,   287, 16139,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0600e-04,  1.4492e-04, -6.8765e-05,  ...,  1.8818e-06,
          7.1392e-05, -2.7202e-05],
        [-1.4752e-05, -1.0699e-05,  6.4448e-06,  ..., -1.2979e-05,
         -3.6135e-06, -9.8720e-06],
        [-1.6004e-05, -1.1608e-05,  6.9812e-06,  ..., -1.4082e-05,
         -3.8929e-06, -1.0684e-05],
        [-1.5661e-05, -1.1370e-05,  6.8471e-06,  ..., -1.3784e-05,
         -3.7886e-06, -1.0446e-05],
        [-3.1650e-05, -2.2918e-05,  1.3813e-05,  ..., -2.7776e-05,
         -7.7114e-06, -2.1130e-05]], device='cuda:0')
Loss: 1.13041090965271


Running epoch 0, step 445, batch 445
Sampled inputs[:2]: tensor([[    0,   266, 20604,  ...,   409, 13764,  6048],
        [    0,   925,   271,  ...,   391,   721,  1576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9035e-04,  1.3061e-04, -9.2803e-05,  ..., -1.3020e-05,
          1.1199e-04,  2.5673e-06],
        [-1.7703e-05, -1.2845e-05,  7.7486e-06,  ..., -1.5557e-05,
         -4.2953e-06, -1.1794e-05],
        [-1.9208e-05, -1.3947e-05,  8.3968e-06,  ..., -1.6883e-05,
         -4.6305e-06, -1.2770e-05],
        [-1.8835e-05, -1.3679e-05,  8.2552e-06,  ..., -1.6555e-05,
         -4.5151e-06, -1.2517e-05],
        [-3.7998e-05, -2.7537e-05,  1.6615e-05,  ..., -3.3319e-05,
         -9.1717e-06, -2.5272e-05]], device='cuda:0')
Loss: 1.1372824907302856


Running epoch 0, step 446, batch 446
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0, 23487,   273,  ...,   368,   259,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8783e-04,  1.1347e-04, -8.1097e-05,  ...,  4.6269e-06,
          1.1573e-04,  4.7043e-05],
        [-2.0653e-05, -1.5005e-05,  9.0227e-06,  ..., -1.8165e-05,
         -5.0478e-06, -1.3791e-05],
        [-2.2411e-05, -1.6302e-05,  9.7826e-06,  ..., -1.9729e-05,
         -5.4464e-06, -1.4946e-05],
        [-2.1949e-05, -1.5959e-05,  9.6038e-06,  ..., -1.9312e-05,
         -5.3011e-06, -1.4633e-05],
        [-4.4346e-05, -3.2187e-05,  1.9357e-05,  ..., -3.8952e-05,
         -1.0788e-05, -2.9594e-05]], device='cuda:0')
Loss: 1.1434880495071411


Running epoch 0, step 447, batch 447
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0,    81,  1619,  ...,  2442,    13,  1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8869e-04,  1.2108e-04, -1.2914e-04,  ...,  1.1486e-05,
          9.0373e-05,  5.5670e-05],
        [-2.3589e-05, -1.7151e-05,  1.0304e-05,  ..., -2.0728e-05,
         -5.7481e-06, -1.5698e-05],
        [-2.5600e-05, -1.8641e-05,  1.1183e-05,  ..., -2.2516e-05,
         -6.2026e-06, -1.7017e-05],
        [-2.5064e-05, -1.8239e-05,  1.0967e-05,  ..., -2.2024e-05,
         -6.0350e-06, -1.6659e-05],
        [-5.0694e-05, -3.6806e-05,  2.2128e-05,  ..., -4.4495e-05,
         -1.2293e-05, -3.3706e-05]], device='cuda:0')
Loss: 1.1402418613433838
Graident accumulation at epoch 0, step 447, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0035,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4914e-05,  2.6874e-05, -8.6428e-05,  ...,  8.0341e-06,
         -1.5998e-05,  4.0828e-05],
        [-2.1971e-05, -1.6886e-05,  6.4628e-06,  ..., -1.9013e-05,
         -4.4701e-06, -1.1774e-05],
        [ 2.7571e-05,  2.2188e-05, -4.5320e-06,  ...,  2.5550e-05,
          5.7608e-06,  1.5131e-05],
        [ 2.9632e-05,  2.1879e-05, -1.0519e-05,  ...,  2.3987e-05,
          8.2047e-06,  1.6384e-05],
        [-3.8561e-05, -2.8844e-05,  1.0479e-05,  ..., -3.4541e-05,
         -8.7228e-06, -2.0614e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1564e-08, 2.7757e-08, 3.4929e-08,  ..., 1.1479e-08, 8.6700e-08,
         1.1388e-08],
        [5.3290e-11, 2.9770e-11, 3.6137e-12,  ..., 3.6304e-11, 2.2586e-12,
         9.1368e-12],
        [6.7966e-10, 3.8839e-10, 2.9289e-11,  ..., 5.8927e-10, 3.6343e-11,
         1.7458e-10],
        [4.0130e-10, 2.4355e-10, 3.3330e-11,  ..., 2.5379e-10, 2.4597e-11,
         9.6943e-11],
        [2.3170e-10, 1.2802e-10, 1.2105e-11,  ..., 1.6505e-10, 8.8037e-12,
         4.2319e-11]], device='cuda:0')
optimizer state dict: 56.0
lr: [1.828097834093899e-05, 1.828097834093899e-05]
scheduler_last_epoch: 56


Running epoch 0, step 448, batch 448
Sampled inputs[:2]: tensor([[    0,   462,  9202,  ...,    15,  3256,   271],
        [    0, 18971,   278,  ...,  1934,  1916,  2612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7327e-05, -2.0296e-05, -4.0412e-05,  ...,  4.7929e-07,
         -2.0355e-05,  1.2337e-05],
        [-2.9057e-06, -2.1011e-06,  1.2964e-06,  ..., -2.5332e-06,
         -7.4133e-07, -1.9521e-06],
        [ 8.7810e-05,  7.3552e-05, -2.7253e-05,  ...,  7.6061e-05,
          2.4646e-05,  5.2876e-05],
        [-3.1590e-06, -2.2799e-06,  1.4082e-06,  ..., -2.7567e-06,
         -7.9721e-07, -2.1160e-06],
        [-6.3181e-06, -4.5598e-06,  2.8163e-06,  ..., -5.5134e-06,
         -1.6019e-06, -4.2319e-06]], device='cuda:0')
Loss: 1.12593674659729


Running epoch 0, step 449, batch 449
Sampled inputs[:2]: tensor([[    0,   300,  1064,  ...,  6953,   944,   278],
        [    0,   850,    13,  ..., 11823,    13, 30706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0300e-05, -2.4377e-05, -3.6404e-05,  ...,  2.2142e-05,
         -6.2465e-05, -1.4425e-05],
        [-5.7966e-06, -4.1723e-06,  2.5928e-06,  ..., -5.0366e-06,
         -1.4715e-06, -3.9041e-06],
        [ 8.4607e-05,  7.1257e-05, -2.5822e-05,  ...,  7.3290e-05,
          2.3848e-05,  5.0730e-05],
        [-6.3032e-06, -4.5300e-06,  2.8163e-06,  ..., -5.4836e-06,
         -1.5758e-06, -4.2319e-06],
        [-1.2666e-05, -9.0897e-06,  5.6475e-06,  ..., -1.0997e-05,
         -3.1888e-06, -8.4937e-06]], device='cuda:0')
Loss: 1.1159770488739014


Running epoch 0, step 450, batch 450
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,    13,   635,    13],
        [    0,    14, 49045,  ...,    12,   706,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1485e-05, -1.5503e-05, -5.5592e-05,  ...,  4.8472e-05,
         -5.6214e-05, -1.3470e-05],
        [-8.7321e-06, -6.2883e-06,  3.9265e-06,  ..., -7.6145e-06,
         -2.3209e-06, -5.9158e-06],
        [ 8.1388e-05,  6.8947e-05, -2.4369e-05,  ...,  7.0473e-05,
          2.2928e-05,  4.8525e-05],
        [-9.4175e-06, -6.7651e-06,  4.2319e-06,  ..., -8.2254e-06,
         -2.4624e-06, -6.3628e-06],
        [-1.9014e-05, -1.3620e-05,  8.5235e-06,  ..., -1.6540e-05,
         -5.0068e-06, -1.2815e-05]], device='cuda:0')
Loss: 1.1167758703231812


Running epoch 0, step 451, batch 451
Sampled inputs[:2]: tensor([[    0,    12,  3367,  ..., 16917, 12221, 12138],
        [    0,  2626,    13,  ...,   300,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9195e-05, -1.2110e-05, -7.0231e-05,  ...,  8.2320e-05,
         -7.5671e-05, -1.4420e-05],
        [-1.1638e-05, -8.3745e-06,  5.2378e-06,  ..., -1.0133e-05,
         -3.0920e-06, -7.9125e-06],
        [ 7.8184e-05,  6.6652e-05, -2.2924e-05,  ...,  6.7702e-05,
          2.2083e-05,  4.6334e-05],
        [-1.2562e-05, -9.0301e-06,  5.6550e-06,  ..., -1.0952e-05,
         -3.2894e-06, -8.5086e-06],
        [-2.5362e-05, -1.8150e-05,  1.1384e-05,  ..., -2.2024e-05,
         -6.6832e-06, -1.7136e-05]], device='cuda:0')
Loss: 1.1228939294815063


Running epoch 0, step 452, batch 452
Sampled inputs[:2]: tensor([[   0,  328,  843,  ...,  298,  292,   37],
        [   0,  368, 2035,  ...,  266, 1122,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2817e-05,  4.8880e-05, -9.0953e-05,  ...,  7.8345e-05,
         -3.9216e-05, -4.3978e-05],
        [-1.4558e-05, -1.0476e-05,  6.5267e-06,  ..., -1.2696e-05,
         -3.9041e-06, -9.9540e-06],
        [ 7.4980e-05,  6.4358e-05, -2.1516e-05,  ...,  6.4885e-05,
          2.1203e-05,  4.4114e-05],
        [-1.5706e-05, -1.1295e-05,  7.0408e-06,  ..., -1.3724e-05,
         -4.1462e-06, -1.0699e-05],
        [-3.1710e-05, -2.2709e-05,  1.4186e-05,  ..., -2.7597e-05,
         -8.4341e-06, -2.1547e-05]], device='cuda:0')
Loss: 1.1259982585906982


Running epoch 0, step 453, batch 453
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0,    27,  3961,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0189e-05,  5.9603e-05, -9.3611e-05,  ...,  7.8522e-05,
         -4.5911e-05, -6.7411e-05],
        [-1.7464e-05, -1.2562e-05,  7.8231e-06,  ..., -1.5259e-05,
         -4.6603e-06, -1.1936e-05],
        [ 7.1792e-05,  6.2063e-05, -2.0093e-05,  ...,  6.2069e-05,
          2.0376e-05,  4.1938e-05],
        [-1.8835e-05, -1.3545e-05,  8.4341e-06,  ..., -1.6481e-05,
         -4.9509e-06, -1.2830e-05],
        [-3.8028e-05, -2.7239e-05,  1.6987e-05,  ..., -3.3140e-05,
         -1.0073e-05, -2.5839e-05]], device='cuda:0')
Loss: 1.127793788909912


Running epoch 0, step 454, batch 454
Sampled inputs[:2]: tensor([[    0,   431, 19346,  ...,    14,  3237, 18548],
        [    0,   266,  1624,  ...,    14,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8317e-07,  7.1109e-05, -9.4171e-05,  ...,  8.1612e-05,
         -3.6153e-05, -1.0679e-04],
        [-2.0429e-05, -1.4663e-05,  9.1344e-06,  ..., -1.7822e-05,
         -5.4687e-06, -1.3977e-05],
        [ 6.8558e-05,  5.9768e-05, -1.8662e-05,  ...,  5.9268e-05,
          1.9501e-05,  3.9718e-05],
        [-2.2009e-05, -1.5795e-05,  9.8422e-06,  ..., -1.9222e-05,
         -5.8040e-06, -1.5005e-05],
        [-4.4376e-05, -3.1710e-05,  1.9789e-05,  ..., -3.8624e-05,
         -1.1787e-05, -3.0190e-05]], device='cuda:0')
Loss: 1.112230658531189


Running epoch 0, step 455, batch 455
Sampled inputs[:2]: tensor([[   0,  275, 1911,  ..., 1371, 5151, 2813],
        [   0, 1480,  518,  ...,  445,   28,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1428e-05,  5.7232e-05, -1.0202e-04,  ...,  1.1692e-04,
         -6.2160e-05, -9.8998e-05],
        [-2.3335e-05, -1.6764e-05,  1.0423e-05,  ..., -2.0340e-05,
         -6.2026e-06, -1.5929e-05],
        [ 1.5037e-04,  1.0525e-04, -5.4858e-05,  ...,  1.3028e-04,
          5.2438e-05,  1.0035e-04],
        [-2.5183e-05, -1.8075e-05,  1.1250e-05,  ..., -2.1964e-05,
         -6.5975e-06, -1.7136e-05],
        [-5.0694e-05, -3.6269e-05,  2.2590e-05,  ..., -4.4107e-05,
         -1.3374e-05, -3.4422e-05]], device='cuda:0')
Loss: 1.1043922901153564
Graident accumulation at epoch 0, step 455, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0035,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0273,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3279e-05,  2.9909e-05, -8.7987e-05,  ...,  1.8922e-05,
         -2.0614e-05,  2.6846e-05],
        [-2.2108e-05, -1.6874e-05,  6.8588e-06,  ..., -1.9146e-05,
         -4.6434e-06, -1.2189e-05],
        [ 3.9850e-05,  3.0494e-05, -9.5647e-06,  ...,  3.6022e-05,
          1.0428e-05,  2.3652e-05],
        [ 2.4150e-05,  1.7884e-05, -8.3421e-06,  ...,  1.9392e-05,
          6.7245e-06,  1.3032e-05],
        [-3.9774e-05, -2.9586e-05,  1.1690e-05,  ..., -3.5497e-05,
         -9.1879e-06, -2.1995e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1525e-08, 2.7732e-08, 3.4904e-08,  ..., 1.1481e-08, 8.6618e-08,
         1.1386e-08],
        [5.3781e-11, 3.0021e-11, 3.7187e-12,  ..., 3.6681e-11, 2.2948e-12,
         9.3814e-12],
        [7.0159e-10, 3.9908e-10, 3.2269e-11,  ..., 6.0565e-10, 3.9056e-11,
         1.8447e-10],
        [4.0153e-10, 2.4363e-10, 3.3423e-11,  ..., 2.5402e-10, 2.4616e-11,
         9.7140e-11],
        [2.3404e-10, 1.2921e-10, 1.2603e-11,  ..., 1.6683e-10, 8.9737e-12,
         4.3462e-11]], device='cuda:0')
optimizer state dict: 57.0
lr: [1.8211049955374658e-05, 1.8211049955374658e-05]
scheduler_last_epoch: 57


Running epoch 0, step 456, batch 456
Sampled inputs[:2]: tensor([[    0,  7294, 23782,  ...,   471, 11528,  3437],
        [    0,  4494,    12,  ...,   341,  1619,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2883e-05,  2.2980e-05, -5.2022e-05,  ..., -1.7583e-06,
          2.6042e-06,  2.5461e-05],
        [-2.8610e-06, -2.0564e-06,  1.2964e-06,  ..., -2.5481e-06,
         -8.9407e-07, -2.0117e-06],
        [-3.1888e-06, -2.2948e-06,  1.4454e-06,  ..., -2.8312e-06,
         -9.9093e-07, -2.2352e-06],
        [-3.0994e-06, -2.2203e-06,  1.4007e-06,  ..., -2.7567e-06,
         -9.5367e-07, -2.1607e-06],
        [-6.1989e-06, -4.4703e-06,  2.8014e-06,  ..., -5.5134e-06,
         -1.9222e-06, -4.3511e-06]], device='cuda:0')
Loss: 1.1286141872406006


Running epoch 0, step 457, batch 457
Sampled inputs[:2]: tensor([[    0,  5260,   365,  ...,  7242,   471,   391],
        [    0, 15165,   287,  ..., 15049,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8336e-05,  4.4174e-05, -5.1138e-05,  ...,  1.6833e-05,
         -2.2690e-05,  5.3265e-05],
        [-5.7071e-06, -4.0829e-06,  2.6003e-06,  ..., -5.0664e-06,
         -1.7881e-06, -3.9935e-06],
        [-6.3777e-06, -4.5598e-06,  2.9057e-06,  ..., -5.6326e-06,
         -1.9819e-06, -4.4554e-06],
        [-6.2287e-06, -4.4405e-06,  2.8312e-06,  ..., -5.5134e-06,
         -1.9222e-06, -4.3362e-06],
        [-1.2398e-05, -8.8811e-06,  5.6326e-06,  ..., -1.0967e-05,
         -3.8594e-06, -8.6725e-06]], device='cuda:0')
Loss: 1.1266961097717285


Running epoch 0, step 458, batch 458
Sampled inputs[:2]: tensor([[    0,    26,  4044,  ...,  9531,   365,   993],
        [    0,   292, 41192,  ..., 34298,  8741,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7127e-05,  8.1689e-05, -7.3398e-05,  ...,  2.9214e-05,
          2.1173e-05,  3.6732e-05],
        [-8.6129e-06, -6.1393e-06,  3.8818e-06,  ..., -7.6443e-06,
         -2.7232e-06, -6.0350e-06],
        [-9.5665e-06, -6.8098e-06,  4.3139e-06,  ..., -8.4639e-06,
         -3.0026e-06, -6.6906e-06],
        [-9.3281e-06, -6.6310e-06,  4.2021e-06,  ..., -8.2701e-06,
         -2.9057e-06, -6.5118e-06],
        [-1.8746e-05, -1.3351e-05,  8.4192e-06,  ..., -1.6600e-05,
         -5.8860e-06, -1.3113e-05]], device='cuda:0')
Loss: 1.1540666818618774


Running epoch 0, step 459, batch 459
Sampled inputs[:2]: tensor([[    0,   320,   472,  ...,  1345,    14,  1869],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2894e-05,  8.8483e-05, -1.2627e-04,  ...,  6.1193e-05,
          6.2991e-06,  3.9091e-05],
        [-1.1474e-05, -8.1956e-06,  5.2005e-06,  ..., -1.0177e-05,
         -3.6322e-06, -8.0764e-06],
        [-1.2726e-05, -9.0748e-06,  5.7742e-06,  ..., -1.1265e-05,
         -4.0010e-06, -8.9407e-06],
        [-1.2413e-05, -8.8364e-06,  5.6252e-06,  ..., -1.0997e-05,
         -3.8743e-06, -8.7023e-06],
        [-2.4885e-05, -1.7762e-05,  1.1250e-05,  ..., -2.2054e-05,
         -7.8231e-06, -1.7464e-05]], device='cuda:0')
Loss: 1.135626196861267


Running epoch 0, step 460, batch 460
Sampled inputs[:2]: tensor([[    0, 16064, 10937,  ...,   346,   462,   221],
        [    0,    14,   496,  ...,   368,   259,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3932e-05,  1.2420e-04, -1.2442e-04,  ...,  5.9449e-05,
         -5.0467e-06,  3.9465e-05],
        [-1.4350e-05, -1.0267e-05,  6.4895e-06,  ..., -1.2681e-05,
         -4.5486e-06, -1.0073e-05],
        [-1.5959e-05, -1.1399e-05,  7.2196e-06,  ..., -1.4082e-05,
         -5.0217e-06, -1.1161e-05],
        [-1.5512e-05, -1.1072e-05,  7.0184e-06,  ..., -1.3694e-05,
         -4.8503e-06, -1.0848e-05],
        [-3.1203e-05, -2.2292e-05,  1.4067e-05,  ..., -2.7537e-05,
         -9.8199e-06, -2.1815e-05]], device='cuda:0')
Loss: 1.1294190883636475


Running epoch 0, step 461, batch 461
Sampled inputs[:2]: tensor([[    0, 23842,   342,  ...,   365,  4011, 10151],
        [    0,  2258, 10315,  ...,  4185,  9433,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2619e-05,  1.2602e-04, -1.2701e-04,  ...,  3.0018e-05,
          1.7857e-05,  2.9385e-05],
        [-1.7226e-05, -1.2323e-05,  7.7710e-06,  ..., -1.5214e-05,
         -5.4240e-06, -1.2055e-05],
        [-1.9178e-05, -1.3709e-05,  8.6576e-06,  ..., -1.6913e-05,
         -5.9977e-06, -1.3366e-05],
        [-1.8641e-05, -1.3322e-05,  8.4192e-06,  ..., -1.6451e-05,
         -5.7966e-06, -1.2994e-05],
        [-3.7491e-05, -2.6792e-05,  1.6868e-05,  ..., -3.3051e-05,
         -1.1727e-05, -2.6137e-05]], device='cuda:0')
Loss: 1.1529451608657837


Running epoch 0, step 462, batch 462
Sampled inputs[:2]: tensor([[    0, 20202,   300,  ..., 15185,   287,  6573],
        [    0,   491, 10524,  ...,  2218,  5627,  4199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3242e-05,  1.3939e-04, -1.2773e-04,  ...,  2.4328e-05,
         -5.4225e-06,  3.2366e-05],
        [-2.0117e-05, -1.4380e-05,  9.0599e-06,  ..., -1.7732e-05,
         -6.3404e-06, -1.4096e-05],
        [-2.2411e-05, -1.6004e-05,  1.0103e-05,  ..., -1.9729e-05,
         -7.0184e-06, -1.5646e-05],
        [-2.1771e-05, -1.5557e-05,  9.8199e-06,  ..., -1.9193e-05,
         -6.7800e-06, -1.5199e-05],
        [-4.3750e-05, -3.1263e-05,  1.9670e-05,  ..., -3.8534e-05,
         -1.3709e-05, -3.0547e-05]], device='cuda:0')
Loss: 1.1137731075286865


Running epoch 0, step 463, batch 463
Sampled inputs[:2]: tensor([[   0,  298,  696,  ..., 3502,  287, 1047],
        [   0,  278, 6481,  ...,   13, 8970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9400e-05,  1.7713e-04, -1.4508e-04,  ...,  1.2466e-05,
         -4.0869e-05,  5.4546e-05],
        [-2.2948e-05, -1.6406e-05,  1.0341e-05,  ..., -2.0206e-05,
         -7.1935e-06, -1.6093e-05],
        [-2.5600e-05, -1.8284e-05,  1.1548e-05,  ..., -2.2516e-05,
         -7.9721e-06, -1.7896e-05],
        [-2.4870e-05, -1.7762e-05,  1.1221e-05,  ..., -2.1890e-05,
         -7.6964e-06, -1.7375e-05],
        [-5.0008e-05, -3.5733e-05,  2.2501e-05,  ..., -4.3988e-05,
         -1.5579e-05, -3.4928e-05]], device='cuda:0')
Loss: 1.1215722560882568
Graident accumulation at epoch 0, step 463, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0035,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1892e-05,  4.4631e-05, -9.3696e-05,  ...,  1.8277e-05,
         -2.2639e-05,  2.9616e-05],
        [-2.2192e-05, -1.6827e-05,  7.2071e-06,  ..., -1.9252e-05,
         -4.8984e-06, -1.2580e-05],
        [ 3.3305e-05,  2.5616e-05, -7.4533e-06,  ...,  3.0168e-05,
          8.5884e-06,  1.9498e-05],
        [ 1.9248e-05,  1.4319e-05, -6.3858e-06,  ...,  1.5264e-05,
          5.2824e-06,  9.9909e-06],
        [-4.0798e-05, -3.0201e-05,  1.2771e-05,  ..., -3.6346e-05,
         -9.8271e-06, -2.3288e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1485e-08, 2.7736e-08, 3.4891e-08,  ..., 1.1470e-08, 8.6533e-08,
         1.1378e-08],
        [5.4254e-11, 3.0260e-11, 3.8219e-12,  ..., 3.7053e-11, 2.3442e-12,
         9.6310e-12],
        [7.0154e-10, 3.9902e-10, 3.2370e-11,  ..., 6.0555e-10, 3.9081e-11,
         1.8461e-10],
        [4.0175e-10, 2.4370e-10, 3.3516e-11,  ..., 2.5424e-10, 2.4651e-11,
         9.7344e-11],
        [2.3631e-10, 1.3036e-10, 1.3097e-11,  ..., 1.6860e-10, 9.2075e-12,
         4.4638e-11]], device='cuda:0')
optimizer state dict: 58.0
lr: [1.8139866849701876e-05, 1.8139866849701876e-05]
scheduler_last_epoch: 58


Running epoch 0, step 464, batch 464
Sampled inputs[:2]: tensor([[    0,   328,   471,  ..., 11137,   679,  6585],
        [    0,    17,  4110,  ...,   287,  7115,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3915e-05,  1.0828e-05, -7.5492e-07,  ...,  2.3878e-05,
          2.6691e-06, -6.7987e-06],
        [-2.8014e-06, -2.0415e-06,  1.2591e-06,  ..., -2.4885e-06,
         -9.8348e-07, -2.0117e-06],
        [-3.2187e-06, -2.3395e-06,  1.4454e-06,  ..., -2.8461e-06,
         -1.1176e-06, -2.2948e-06],
        [-3.0845e-06, -2.2501e-06,  1.3933e-06,  ..., -2.7269e-06,
         -1.0729e-06, -2.2054e-06],
        [-6.2585e-06, -4.5300e-06,  2.8163e-06,  ..., -5.5432e-06,
         -2.1905e-06, -4.4703e-06]], device='cuda:0')
Loss: 1.1457561254501343


Running epoch 0, step 465, batch 465
Sampled inputs[:2]: tensor([[   0, 1530,   17,  ...,  409, 1611,  895],
        [   0,   14,  417,  ..., 8821, 6845,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3499e-05,  5.9983e-05, -1.4481e-05,  ...,  3.0490e-05,
         -7.6515e-06,  1.8520e-05],
        [-5.6177e-06, -4.0829e-06,  2.5108e-06,  ..., -4.9323e-06,
         -1.9521e-06, -4.0084e-06],
        [-6.4224e-06, -4.6641e-06,  2.8759e-06,  ..., -5.6475e-06,
         -2.2128e-06, -4.5747e-06],
        [-6.1840e-06, -4.5002e-06,  2.7716e-06,  ..., -5.4240e-06,
         -2.1234e-06, -4.3958e-06],
        [-1.2517e-05, -9.0599e-06,  5.6028e-06,  ..., -1.0997e-05,
         -4.3213e-06, -8.9109e-06]], device='cuda:0')
Loss: 1.1191362142562866


Running epoch 0, step 466, batch 466
Sampled inputs[:2]: tensor([[    0,  1487,  2511,  ..., 27735,   760,   266],
        [    0,  1184,  1451,  ...,   934,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3839e-05,  1.1138e-04, -3.1630e-05,  ...,  5.0693e-05,
         -1.5985e-05, -2.7906e-06],
        [-8.4043e-06, -6.0946e-06,  3.8147e-06,  ..., -7.3314e-06,
         -2.8796e-06, -5.9754e-06],
        [-9.6560e-06, -7.0035e-06,  4.3884e-06,  ..., -8.4490e-06,
         -3.2857e-06, -6.8545e-06],
        [-9.3132e-06, -6.7651e-06,  4.2394e-06,  ..., -8.1211e-06,
         -3.1516e-06, -6.6012e-06],
        [-1.8775e-05, -1.3590e-05,  8.5384e-06,  ..., -1.6391e-05,
         -6.4075e-06, -1.3322e-05]], device='cuda:0')
Loss: 1.1222442388534546


Running epoch 0, step 467, batch 467
Sampled inputs[:2]: tensor([[    0,  1607, 26394,  ...,    19,   471,    14],
        [    0,   292, 49760,  ...,   275,  4474,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3891e-05,  9.1797e-05, -7.0909e-05,  ...,  2.9545e-05,
         -3.5451e-05, -2.1295e-05],
        [-1.1206e-05, -8.1062e-06,  5.0887e-06,  ..., -9.8050e-06,
         -3.7998e-06, -7.9423e-06],
        [-1.2875e-05, -9.2983e-06,  5.8487e-06,  ..., -1.1280e-05,
         -4.3288e-06, -9.1046e-06],
        [-1.2442e-05, -9.0003e-06,  5.6624e-06,  ..., -1.0878e-05,
         -4.1649e-06, -8.7917e-06],
        [-2.5034e-05, -1.8060e-05,  1.1370e-05,  ..., -2.1905e-05,
         -8.4490e-06, -1.7703e-05]], device='cuda:0')
Loss: 1.1132376194000244


Running epoch 0, step 468, batch 468
Sampled inputs[:2]: tensor([[    0,   278, 38717,  ...,  9945,   367,  5430],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1841e-05,  9.0114e-05, -1.5022e-04,  ...,  6.6001e-05,
         -9.5075e-07,  1.1042e-05],
        [-1.4037e-05, -1.0133e-05,  6.3628e-06,  ..., -1.2249e-05,
         -4.7982e-06, -9.9540e-06],
        [ 6.0372e-05,  4.3303e-05, -2.0139e-05,  ...,  5.7869e-05,
          1.6920e-05,  4.1869e-05],
        [-1.5527e-05, -1.1206e-05,  7.0482e-06,  ..., -1.3545e-05,
         -5.2378e-06, -1.0982e-05],
        [-3.1263e-05, -2.2531e-05,  1.4171e-05,  ..., -2.7269e-05,
         -1.0625e-05, -2.2113e-05]], device='cuda:0')
Loss: 1.1386116743087769


Running epoch 0, step 469, batch 469
Sampled inputs[:2]: tensor([[   0,   12,  722,  ...,  674,  369,  897],
        [   0,   13, 1529,  ...,  943,  266, 9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3627e-05,  7.0684e-05, -1.5375e-04,  ...,  5.2167e-05,
         -3.9329e-05, -2.5088e-05],
        [-1.6853e-05, -1.2130e-05,  7.6368e-06,  ..., -1.4707e-05,
         -5.7891e-06, -1.1951e-05],
        [ 5.7153e-05,  4.1023e-05, -1.8686e-05,  ...,  5.5052e-05,
          1.5795e-05,  3.9590e-05],
        [-1.8656e-05, -1.3426e-05,  8.4713e-06,  ..., -1.6287e-05,
         -6.3255e-06, -1.3202e-05],
        [-3.7521e-05, -2.6971e-05,  1.7002e-05,  ..., -3.2753e-05,
         -1.2815e-05, -2.6554e-05]], device='cuda:0')
Loss: 1.125436782836914


Running epoch 0, step 470, batch 470
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0071e-06,  1.3684e-04, -1.9753e-04,  ...,  5.8161e-05,
         -6.4454e-05, -7.1306e-05],
        [-1.9655e-05, -1.4111e-05,  8.9034e-06,  ..., -1.7196e-05,
         -6.8396e-06, -1.4022e-05],
        [ 5.3965e-05,  3.8758e-05, -1.7233e-05,  ...,  5.2206e-05,
          1.4603e-05,  3.7220e-05],
        [-2.1681e-05, -1.5572e-05,  9.8422e-06,  ..., -1.8984e-05,
         -7.4431e-06, -1.5438e-05],
        [-4.3690e-05, -3.1352e-05,  1.9804e-05,  ..., -3.8236e-05,
         -1.5110e-05, -3.1114e-05]], device='cuda:0')
Loss: 1.1160300970077515


Running epoch 0, step 471, batch 471
Sampled inputs[:2]: tensor([[    0,   300, 16683,  ...,  8709,    40,  9817],
        [    0,  2615,    13,  ...,   940,  3661,  6837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1551e-05,  1.1957e-04, -2.4138e-04,  ...,  8.0318e-05,
         -8.5496e-05, -8.5735e-05],
        [-2.2456e-05, -1.6123e-05,  1.0185e-05,  ..., -1.9625e-05,
         -7.7523e-06, -1.5989e-05],
        [ 5.0746e-05,  3.6449e-05, -1.5765e-05,  ...,  4.9420e-05,
          1.3560e-05,  3.4970e-05],
        [-2.4825e-05, -1.7837e-05,  1.1288e-05,  ..., -2.1726e-05,
         -8.4564e-06, -1.7643e-05],
        [-4.9859e-05, -3.5763e-05,  2.2620e-05,  ..., -4.3601e-05,
         -1.7121e-05, -3.5435e-05]], device='cuda:0')
Loss: 1.1053601503372192
Graident accumulation at epoch 0, step 471, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7547e-05,  5.2125e-05, -1.0847e-04,  ...,  2.4481e-05,
         -2.8925e-05,  1.8081e-05],
        [-2.2218e-05, -1.6757e-05,  7.5049e-06,  ..., -1.9289e-05,
         -5.1838e-06, -1.2920e-05],
        [ 3.5049e-05,  2.6700e-05, -8.2846e-06,  ...,  3.2094e-05,
          9.0856e-06,  2.1045e-05],
        [ 1.4841e-05,  1.1103e-05, -4.6185e-06,  ...,  1.1565e-05,
          3.9085e-06,  7.2275e-06],
        [-4.1704e-05, -3.0757e-05,  1.3756e-05,  ..., -3.7072e-05,
         -1.0556e-05, -2.4503e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1452e-08, 2.7722e-08, 3.4914e-08,  ..., 1.1465e-08, 8.6453e-08,
         1.1374e-08],
        [5.4704e-11, 3.0490e-11, 3.9218e-12,  ..., 3.7401e-11, 2.4020e-12,
         9.8770e-12],
        [7.0342e-10, 3.9995e-10, 3.2586e-11,  ..., 6.0739e-10, 3.9226e-11,
         1.8565e-10],
        [4.0196e-10, 2.4378e-10, 3.3610e-11,  ..., 2.5446e-10, 2.4698e-11,
         9.7558e-11],
        [2.3856e-10, 1.3151e-10, 1.3595e-11,  ..., 1.7033e-10, 9.4914e-12,
         4.5849e-11]], device='cuda:0')
optimizer state dict: 59.0
lr: [1.806743990132056e-05, 1.806743990132056e-05]
scheduler_last_epoch: 59


Running epoch 0, step 472, batch 472
Sampled inputs[:2]: tensor([[    0, 11657,   367,  ..., 31468,    26,   266],
        [    0,  1250,  1797,  ...,   266,  1417,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7442e-05, -8.4202e-06, -1.5635e-05,  ...,  1.4826e-05,
         -2.8649e-05,  1.2630e-06],
        [-2.7567e-06, -1.9521e-06,  1.2368e-06,  ..., -2.4289e-06,
         -1.0505e-06, -1.9222e-06],
        [-3.2783e-06, -2.3246e-06,  1.4752e-06,  ..., -2.8908e-06,
         -1.2368e-06, -2.2799e-06],
        [-3.0994e-06, -2.1905e-06,  1.3933e-06,  ..., -2.7269e-06,
         -1.1697e-06, -2.1458e-06],
        [-6.2585e-06, -4.4405e-06,  2.8163e-06,  ..., -5.5134e-06,
         -2.3693e-06, -4.3511e-06]], device='cuda:0')
Loss: 1.1221199035644531


Running epoch 0, step 473, batch 473
Sampled inputs[:2]: tensor([[   0,  874,  590,  ...,  300,  867,  638],
        [   0,   28, 2973,  ..., 8762, 2134,   27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8696e-05, -5.8769e-06, -3.4792e-05,  ...,  4.6235e-05,
         -4.2971e-05,  9.9815e-06],
        [-5.4985e-06, -3.9041e-06,  2.4885e-06,  ..., -4.8280e-06,
         -2.0638e-06, -3.8445e-06],
        [-6.5118e-06, -4.6343e-06,  2.9504e-06,  ..., -5.7369e-06,
         -2.4289e-06, -4.5449e-06],
        [-6.1840e-06, -4.3958e-06,  2.8014e-06,  ..., -5.4389e-06,
         -2.2948e-06, -4.3064e-06],
        [-1.2487e-05, -8.8811e-06,  5.6624e-06,  ..., -1.0967e-05,
         -4.6641e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.134204626083374


Running epoch 0, step 474, batch 474
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0,  2467, 18011,  ...,  5913,  9281,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0279e-05, -5.9458e-05, -6.6379e-05,  ...,  4.1358e-05,
         -6.5745e-05, -7.9754e-05],
        [-8.2105e-06, -5.8413e-06,  3.7476e-06,  ..., -7.2122e-06,
         -3.1069e-06, -5.7966e-06],
        [-9.6858e-06, -6.8992e-06,  4.4256e-06,  ..., -8.5384e-06,
         -3.6433e-06, -6.8247e-06],
        [-9.2387e-06, -6.5714e-06,  4.2245e-06,  ..., -8.1360e-06,
         -3.4571e-06, -6.4969e-06],
        [-1.8567e-05, -1.3232e-05,  8.4937e-06,  ..., -1.6332e-05,
         -7.0035e-06, -1.3083e-05]], device='cuda:0')
Loss: 1.1072813272476196


Running epoch 0, step 475, batch 475
Sampled inputs[:2]: tensor([[    0,  1234,   278,  ...,  1237,  1008,   417],
        [    0,    14,   747,  ..., 12545,    12, 15209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5491e-05, -3.6833e-05, -8.2470e-05,  ...,  4.2313e-05,
         -5.4244e-05, -1.1673e-04],
        [-1.0923e-05, -7.7933e-06,  4.9844e-06,  ..., -9.6112e-06,
         -4.1202e-06, -7.7188e-06],
        [-1.2875e-05, -9.2089e-06,  5.8934e-06,  ..., -1.1370e-05,
         -4.8280e-06, -9.0897e-06],
        [-1.2308e-05, -8.7917e-06,  5.6326e-06,  ..., -1.0863e-05,
         -4.5896e-06, -8.6725e-06],
        [-2.4676e-05, -1.7643e-05,  1.1295e-05,  ..., -2.1726e-05,
         -9.2685e-06, -1.7405e-05]], device='cuda:0')
Loss: 1.1060594320297241


Running epoch 0, step 476, batch 476
Sampled inputs[:2]: tensor([[    0,   292,   221,  ...,   796, 12886,   694],
        [    0,  6508,  4305,  ...,   806,  3888,  4431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0272e-04,  6.7548e-05, -1.0511e-04,  ...,  1.1401e-04,
         -9.4651e-05, -1.3241e-04],
        [-1.3754e-05, -9.7305e-06,  6.2063e-06,  ..., -1.2025e-05,
         -5.2899e-06, -9.7156e-06],
        [-1.6198e-05, -1.1474e-05,  7.3239e-06,  ..., -1.4201e-05,
         -6.1914e-06, -1.1429e-05],
        [-1.5408e-05, -1.0908e-05,  6.9737e-06,  ..., -1.3515e-05,
         -5.8562e-06, -1.0863e-05],
        [-3.1173e-05, -2.2084e-05,  1.4096e-05,  ..., -2.7269e-05,
         -1.1936e-05, -2.1994e-05]], device='cuda:0')
Loss: 1.0910048484802246


Running epoch 0, step 477, batch 477
Sampled inputs[:2]: tensor([[    0,   298,   894,  ...,   396,   298,   527],
        [    0,  1196,  3570,  ...,   722, 15816,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8689e-05,  9.4156e-05, -9.1123e-05,  ...,  1.1427e-04,
         -9.3103e-05, -1.2058e-04],
        [-1.6525e-05, -1.1742e-05,  7.4655e-06,  ..., -1.4469e-05,
         -6.4075e-06, -1.1712e-05],
        [-1.9431e-05, -1.3828e-05,  8.7917e-06,  ..., -1.7062e-05,
         -7.4878e-06, -1.3769e-05],
        [-1.8433e-05, -1.3098e-05,  8.3521e-06,  ..., -1.6183e-05,
         -7.0557e-06, -1.3039e-05],
        [-3.7432e-05, -2.6613e-05,  1.6928e-05,  ..., -3.2783e-05,
         -1.4439e-05, -2.6494e-05]], device='cuda:0')
Loss: 1.109180212020874


Running epoch 0, step 478, batch 478
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0,   259,  5112,  ...,  3520,   278,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0386e-04,  9.4157e-05, -1.1282e-04,  ...,  1.3406e-04,
         -1.0381e-04, -1.2638e-04],
        [-1.9267e-05, -1.3724e-05,  8.7097e-06,  ..., -1.6928e-05,
         -7.4580e-06, -1.3679e-05],
        [-2.2605e-05, -1.6123e-05,  1.0230e-05,  ..., -1.9908e-05,
         -8.6874e-06, -1.6049e-05],
        [-2.1487e-05, -1.5303e-05,  9.7379e-06,  ..., -1.8924e-05,
         -8.2031e-06, -1.5229e-05],
        [-4.3601e-05, -3.1054e-05,  1.9729e-05,  ..., -3.8296e-05,
         -1.6779e-05, -3.0935e-05]], device='cuda:0')
Loss: 1.1189919710159302


Running epoch 0, step 479, batch 479
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0,  2229,   352,  ...,  4988,    33,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6518e-05,  1.0148e-04, -1.2786e-04,  ...,  1.6903e-04,
         -1.5438e-04, -1.2654e-04],
        [-2.2039e-05, -1.5706e-05,  9.9987e-06,  ..., -1.9372e-05,
         -8.5905e-06, -1.5691e-05],
        [-2.5839e-05, -1.8433e-05,  1.1735e-05,  ..., -2.2754e-05,
         -9.9987e-06, -1.8403e-05],
        [-2.4527e-05, -1.7464e-05,  1.1154e-05,  ..., -2.1592e-05,
         -9.4250e-06, -1.7434e-05],
        [-4.9829e-05, -3.5495e-05,  2.2620e-05,  ..., -4.3780e-05,
         -1.9297e-05, -3.5465e-05]], device='cuda:0')
Loss: 1.1325170993804932
Graident accumulation at epoch 0, step 479, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4141e-05,  5.7060e-05, -1.1040e-04,  ...,  3.8936e-05,
         -4.1471e-05,  3.6188e-06],
        [-2.2200e-05, -1.6652e-05,  7.7543e-06,  ..., -1.9297e-05,
         -5.5245e-06, -1.3198e-05],
        [ 2.8961e-05,  2.2186e-05, -6.2826e-06,  ...,  2.6609e-05,
          7.1771e-06,  1.7100e-05],
        [ 1.0904e-05,  8.2467e-06, -3.0413e-06,  ...,  8.2494e-06,
          2.5752e-06,  4.7613e-06],
        [-4.2516e-05, -3.1231e-05,  1.4642e-05,  ..., -3.7743e-05,
         -1.1431e-05, -2.5599e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1420e-08, 2.7705e-08, 3.4895e-08,  ..., 1.1482e-08, 8.6391e-08,
         1.1378e-08],
        [5.5135e-11, 3.0706e-11, 4.0179e-12,  ..., 3.7739e-11, 2.4734e-12,
         1.0113e-11],
        [7.0338e-10, 3.9989e-10, 3.2691e-11,  ..., 6.0730e-10, 3.9286e-11,
         1.8580e-10],
        [4.0216e-10, 2.4384e-10, 3.3701e-11,  ..., 2.5467e-10, 2.4762e-11,
         9.7765e-11],
        [2.4080e-10, 1.3263e-10, 1.4094e-11,  ..., 1.7208e-10, 9.8543e-12,
         4.7061e-11]], device='cuda:0')
optimizer state dict: 60.0
lr: [1.799378017770064e-05, 1.799378017770064e-05]
scheduler_last_epoch: 60
Epoch 0 | Batch 479/1048 | Training PPL: 8605.327739698592 | time 48.624327659606934
Saving checkpoint at epoch 0, step 479, batch 479
Epoch 0 | Validation PPL: 9.30328590173348 | Learning rate: 1.799378017770064e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_479, AFTER epoch 0, step 479


Running epoch 0, step 480, batch 480
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  1878,   328,   259],
        [    0, 20291,  1990,  ...,   298,   732,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3386e-05,  4.0552e-05, -4.1652e-05,  ...,  1.1458e-05,
          5.5905e-06,  6.6470e-05],
        [-2.7269e-06, -1.9372e-06,  1.2293e-06,  ..., -2.3991e-06,
         -1.1846e-06, -1.9670e-06],
        [-3.2932e-06, -2.3395e-06,  1.4827e-06,  ..., -2.8908e-06,
         -1.4156e-06, -2.3693e-06],
        [-3.0249e-06, -2.1458e-06,  1.3635e-06,  ..., -2.6524e-06,
         -1.2964e-06, -2.1607e-06],
        [-6.2883e-06, -4.4703e-06,  2.8312e-06,  ..., -5.5134e-06,
         -2.7120e-06, -4.5002e-06]], device='cuda:0')
Loss: 1.1256093978881836


Running epoch 0, step 481, batch 481
Sampled inputs[:2]: tensor([[   0, 1064,  266,  ..., 2971,  292,  474],
        [   0,  275, 2351,  ...,   14, 4520,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8453e-05,  5.2729e-05, -1.1066e-04,  ...,  5.5572e-05,
          1.6305e-05,  3.0482e-05],
        [-5.4389e-06, -3.8594e-06,  2.4214e-06,  ..., -4.7982e-06,
         -2.3097e-06, -3.8669e-06],
        [-6.5565e-06, -4.6641e-06,  2.9132e-06,  ..., -5.7817e-06,
         -2.7567e-06, -4.6492e-06],
        [-6.0797e-06, -4.3064e-06,  2.6971e-06,  ..., -5.3495e-06,
         -2.5406e-06, -4.2915e-06],
        [-1.2547e-05, -8.9109e-06,  5.5730e-06,  ..., -1.1027e-05,
         -5.2750e-06, -8.8513e-06]], device='cuda:0')
Loss: 1.1127989292144775


Running epoch 0, step 482, batch 482
Sampled inputs[:2]: tensor([[    0,  1336,   278,  ...,   266,  3269,   278],
        [    0,  1029,  6068,  ..., 18017,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4355e-06,  2.5021e-06, -1.3219e-04,  ...,  6.1337e-05,
         -2.4293e-05,  1.3394e-05],
        [-8.1211e-06, -5.7667e-06,  3.6135e-06,  ..., -7.1526e-06,
         -3.3379e-06, -5.7295e-06],
        [-9.8348e-06, -7.0035e-06,  4.3660e-06,  ..., -8.6576e-06,
         -4.0010e-06, -6.9290e-06],
        [-9.1791e-06, -6.5118e-06,  4.0755e-06,  ..., -8.0615e-06,
         -3.7104e-06, -6.4373e-06],
        [-1.8775e-05, -1.3351e-05,  8.3297e-06,  ..., -1.6481e-05,
         -7.6443e-06, -1.3173e-05]], device='cuda:0')
Loss: 1.1096291542053223


Running epoch 0, step 483, batch 483
Sampled inputs[:2]: tensor([[   0,  259, 5918,  ...,  508, 3433, 1351],
        [   0,  300, 4402,  ..., 2013,   13, 6825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6839e-06,  3.2196e-05, -1.2430e-04,  ...,  5.7279e-05,
         -1.4695e-05,  4.6524e-06],
        [-1.0788e-05, -7.7039e-06,  4.8131e-06,  ..., -9.5069e-06,
         -4.4331e-06, -7.6368e-06],
        [-1.3098e-05, -9.3728e-06,  5.8264e-06,  ..., -1.1533e-05,
         -5.3272e-06, -9.2536e-06],
        [-1.2204e-05, -8.7172e-06,  5.4315e-06,  ..., -1.0729e-05,
         -4.9323e-06, -8.5980e-06],
        [-2.5004e-05, -1.7881e-05,  1.1131e-05,  ..., -2.1994e-05,
         -1.0192e-05, -1.7613e-05]], device='cuda:0')
Loss: 1.1108105182647705


Running epoch 0, step 484, batch 484
Sampled inputs[:2]: tensor([[    0,  1005,   292,  ...,   266, 19171,  2474],
        [    0,   607, 11059,  ...,  2081,  1194,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9849e-05, -2.5407e-05, -1.5200e-04,  ...,  3.8275e-05,
         -4.2602e-05, -1.2393e-05],
        [-1.3456e-05, -9.6411e-06,  5.9977e-06,  ..., -1.1891e-05,
         -5.4613e-06, -9.4771e-06],
        [-1.6361e-05, -1.1727e-05,  7.2792e-06,  ..., -1.4454e-05,
         -6.5714e-06, -1.1504e-05],
        [-1.5289e-05, -1.0952e-05,  6.8098e-06,  ..., -1.3486e-05,
         -6.1020e-06, -1.0714e-05],
        [-3.1203e-05, -2.2382e-05,  1.3888e-05,  ..., -2.7537e-05,
         -1.2562e-05, -2.1875e-05]], device='cuda:0')
Loss: 1.1032142639160156


Running epoch 0, step 485, batch 485
Sampled inputs[:2]: tensor([[    0,  6416,   367,  ...,   496,    14,    20],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9156e-05, -1.5466e-05, -1.3844e-04,  ...,  6.3650e-06,
         -1.6039e-05, -1.5294e-05],
        [-1.6198e-05, -1.1548e-05,  7.2122e-06,  ..., -1.4320e-05,
         -6.5491e-06, -1.1370e-05],
        [-1.9640e-05, -1.4007e-05,  8.7321e-06,  ..., -1.7375e-05,
         -7.8604e-06, -1.3769e-05],
        [-1.8358e-05, -1.3098e-05,  8.1733e-06,  ..., -1.6212e-05,
         -7.3090e-06, -1.2830e-05],
        [-3.7551e-05, -2.6792e-05,  1.6704e-05,  ..., -3.3170e-05,
         -1.5065e-05, -2.6256e-05]], device='cuda:0')
Loss: 1.1444212198257446


Running epoch 0, step 486, batch 486
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,   313,    66,  ...,   894,  2973, 25074]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4588e-05,  3.2201e-05, -1.4502e-04,  ...,  5.8655e-06,
         -5.0946e-05,  2.4274e-05],
        [-1.8910e-05, -1.3471e-05,  8.4117e-06,  ..., -1.6749e-05,
         -7.6592e-06, -1.3277e-05],
        [-2.2903e-05, -1.6317e-05,  1.0177e-05,  ..., -2.0280e-05,
         -9.1866e-06, -1.6049e-05],
        [-2.1398e-05, -1.5259e-05,  9.5218e-06,  ..., -1.8924e-05,
         -8.5309e-06, -1.4961e-05],
        [-4.3780e-05, -3.1233e-05,  1.9476e-05,  ..., -3.8743e-05,
         -1.7598e-05, -3.0637e-05]], device='cuda:0')
Loss: 1.1276544332504272


Running epoch 0, step 487, batch 487
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,    14,   747,  ...,  8271,   365,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9824e-05,  8.5392e-05, -1.2484e-04,  ...,  4.3057e-05,
         -6.5610e-05,  1.0643e-05],
        [-2.1562e-05, -1.5371e-05,  9.6038e-06,  ..., -1.9163e-05,
         -8.7470e-06, -1.5177e-05],
        [-2.6107e-05, -1.8612e-05,  1.1615e-05,  ..., -2.3201e-05,
         -1.0490e-05, -1.8343e-05],
        [-2.4438e-05, -1.7434e-05,  1.0885e-05,  ..., -2.1696e-05,
         -9.7677e-06, -1.7136e-05],
        [-4.9919e-05, -3.5644e-05,  2.2233e-05,  ..., -4.4346e-05,
         -2.0102e-05, -3.5018e-05]], device='cuda:0')
Loss: 1.1457576751708984
Graident accumulation at epoch 0, step 487, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0289, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4744e-05,  5.9894e-05, -1.1185e-04,  ...,  3.9348e-05,
         -4.3885e-05,  4.3212e-06],
        [-2.2136e-05, -1.6524e-05,  7.9392e-06,  ..., -1.9284e-05,
         -5.8467e-06, -1.3395e-05],
        [ 2.3454e-05,  1.8107e-05, -4.4928e-06,  ...,  2.1628e-05,
          5.4104e-06,  1.3556e-05],
        [ 7.3699e-06,  5.6786e-06, -1.6486e-06,  ...,  5.2548e-06,
          1.3409e-06,  2.5716e-06],
        [-4.3257e-05, -3.1672e-05,  1.5401e-05,  ..., -3.8403e-05,
         -1.2298e-05, -2.6541e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1383e-08, 2.7685e-08, 3.4876e-08,  ..., 1.1472e-08, 8.6309e-08,
         1.1367e-08],
        [5.5545e-11, 3.0912e-11, 4.1061e-12,  ..., 3.8068e-11, 2.5474e-12,
         1.0334e-11],
        [7.0336e-10, 3.9983e-10, 3.2794e-11,  ..., 6.0723e-10, 3.9357e-11,
         1.8595e-10],
        [4.0236e-10, 2.4390e-10, 3.3785e-11,  ..., 2.5489e-10, 2.4833e-11,
         9.7961e-11],
        [2.4305e-10, 1.3377e-10, 1.4574e-11,  ..., 1.7387e-10, 1.0249e-11,
         4.8240e-11]], device='cuda:0')
optimizer state dict: 61.0
lr: [1.791889893469088e-05, 1.791889893469088e-05]
scheduler_last_epoch: 61


Running epoch 0, step 488, batch 488
Sampled inputs[:2]: tensor([[    0,   654,   300,  ..., 21762,  3597, 11117],
        [    0,    12,   895,  ...,    13,  2900,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2481e-05, -9.8428e-06, -3.7712e-06,  ...,  2.5803e-05,
         -2.5784e-05, -5.7594e-09],
        [-2.6077e-06, -1.8850e-06,  1.1548e-06,  ..., -2.3395e-06,
         -1.0878e-06, -1.8105e-06],
        [-3.2634e-06, -2.3544e-06,  1.4454e-06,  ..., -2.9355e-06,
         -1.3486e-06, -2.2650e-06],
        [-3.0398e-06, -2.1905e-06,  1.3486e-06,  ..., -2.7269e-06,
         -1.2442e-06, -2.1011e-06],
        [-6.1989e-06, -4.4703e-06,  2.7418e-06,  ..., -5.5730e-06,
         -2.5630e-06, -4.2915e-06]], device='cuda:0')
Loss: 1.1164703369140625


Running epoch 0, step 489, batch 489
Sampled inputs[:2]: tensor([[    0,    13, 37178,  ...,  1692,  3287, 10652],
        [    0,  9423,   298,  ...,  5274, 37902,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2344e-05,  3.6500e-05, -3.8525e-05,  ...,  5.3708e-06,
         -3.7578e-05,  5.9683e-05],
        [-5.2899e-06, -3.7551e-06,  2.2948e-06,  ..., -4.7535e-06,
         -2.3320e-06, -3.6806e-06],
        [-6.5565e-06, -4.6492e-06,  2.8461e-06,  ..., -5.8860e-06,
         -2.8610e-06, -4.5598e-06],
        [-6.0350e-06, -4.2766e-06,  2.6301e-06,  ..., -5.4240e-06,
         -2.6152e-06, -4.1872e-06],
        [-1.2577e-05, -8.9109e-06,  5.4687e-06,  ..., -1.1295e-05,
         -5.4985e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.156143307685852


Running epoch 0, step 490, batch 490
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,  2302,   287,  ...,  1522,  1666,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3000e-05,  3.6500e-05, -8.8549e-05,  ...,  1.2863e-05,
         -8.9241e-05,  3.3157e-05],
        [-7.9125e-06, -5.6326e-06,  3.4943e-06,  ..., -7.1228e-06,
         -3.4943e-06, -5.5060e-06],
        [-9.7752e-06, -6.9588e-06,  4.3213e-06,  ..., -8.8066e-06,
         -4.2766e-06, -6.7949e-06],
        [-9.0599e-06, -6.4522e-06,  4.0233e-06,  ..., -8.1658e-06,
         -3.9339e-06, -6.2883e-06],
        [-1.8686e-05, -1.3292e-05,  8.2701e-06,  ..., -1.6838e-05,
         -8.1956e-06, -1.2994e-05]], device='cuda:0')
Loss: 1.1117613315582275


Running epoch 0, step 491, batch 491
Sampled inputs[:2]: tensor([[    0,   380, 26073,  ...,   709,   266,  2421],
        [    0,  3756,    13,  ...,  1704,   278,  5851]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3527e-06,  4.9281e-05, -9.7239e-05,  ...,  1.4535e-05,
         -1.0561e-04,  2.0875e-05],
        [-1.0565e-05, -7.5251e-06,  4.6641e-06,  ..., -9.4622e-06,
         -4.6268e-06, -7.3314e-06],
        [-1.3083e-05, -9.3281e-06,  5.7817e-06,  ..., -1.1712e-05,
         -5.6773e-06, -9.0599e-06],
        [-1.2115e-05, -8.6278e-06,  5.3719e-06,  ..., -1.0848e-05,
         -5.2154e-06, -8.3745e-06],
        [-2.5004e-05, -1.7792e-05,  1.1042e-05,  ..., -2.2382e-05,
         -1.0863e-05, -1.7315e-05]], device='cuda:0')
Loss: 1.1390575170516968


Running epoch 0, step 492, batch 492
Sampled inputs[:2]: tensor([[   0, 3351,  352,  ...,   17,  287,  357],
        [   0,  278, 4575,  ..., 1220,  278, 4575]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1142e-05,  7.8565e-05, -1.0854e-04,  ...,  1.1906e-05,
         -1.1690e-04,  5.3294e-06],
        [-1.3188e-05, -9.3877e-06,  5.8413e-06,  ..., -1.1861e-05,
         -5.8264e-06, -9.2089e-06],
        [-1.6287e-05, -1.1608e-05,  7.2271e-06,  ..., -1.4648e-05,
         -7.1302e-06, -1.1355e-05],
        [-1.5110e-05, -1.0744e-05,  6.7204e-06,  ..., -1.3575e-05,
         -6.5640e-06, -1.0505e-05],
        [-3.1143e-05, -2.2143e-05,  1.3798e-05,  ..., -2.7955e-05,
         -1.3635e-05, -2.1696e-05]], device='cuda:0')
Loss: 1.1262314319610596


Running epoch 0, step 493, batch 493
Sampled inputs[:2]: tensor([[    0,  5982,  9385,  ...,    26,   469,   446],
        [    0,   638,  2708,  ..., 28492,  1814,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9618e-07,  5.9947e-05, -1.2672e-04,  ...,  1.4872e-05,
         -1.3238e-04,  2.6581e-05],
        [-1.5825e-05, -1.1258e-05,  6.9588e-06,  ..., -1.4216e-05,
         -6.9365e-06, -1.0997e-05],
        [-1.9610e-05, -1.3977e-05,  8.6427e-06,  ..., -1.7628e-05,
         -8.5235e-06, -1.3605e-05],
        [-1.8150e-05, -1.2904e-05,  8.0094e-06,  ..., -1.6302e-05,
         -7.8306e-06, -1.2562e-05],
        [-3.7462e-05, -2.6613e-05,  1.6481e-05,  ..., -3.3617e-05,
         -1.6272e-05, -2.5958e-05]], device='cuda:0')
Loss: 1.1254348754882812


Running epoch 0, step 494, batch 494
Sampled inputs[:2]: tensor([[    0,  2280,   344,  ...,   287,   266,  3344],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3598e-05,  7.5634e-05, -1.5402e-04,  ...,  4.2066e-05,
         -1.3007e-04,  7.2671e-05],
        [-1.8477e-05, -1.3143e-05,  8.1137e-06,  ..., -1.6570e-05,
         -8.0913e-06, -1.2815e-05],
        [-2.2858e-05, -1.6287e-05,  1.0066e-05,  ..., -2.0519e-05,
         -9.9242e-06, -1.5840e-05],
        [-2.1175e-05, -1.5050e-05,  9.3281e-06,  ..., -1.8984e-05,
         -9.1195e-06, -1.4618e-05],
        [-4.3720e-05, -3.1054e-05,  1.9208e-05,  ..., -3.9190e-05,
         -1.8969e-05, -3.0220e-05]], device='cuda:0')
Loss: 1.1124070882797241


Running epoch 0, step 495, batch 495
Sampled inputs[:2]: tensor([[   0, 1853, 3373,  ..., 3020, 6695,  300],
        [   0,  472,  346,  ...,  394,  360, 5911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8580e-05,  9.0226e-05, -1.6277e-04,  ...,  4.7274e-05,
         -1.7771e-04,  5.4180e-05],
        [-2.1130e-05, -1.5050e-05,  9.2685e-06,  ..., -1.8954e-05,
         -9.2089e-06, -1.4640e-05],
        [-2.6166e-05, -1.8671e-05,  1.1511e-05,  ..., -2.3514e-05,
         -1.1310e-05, -1.8120e-05],
        [-2.4229e-05, -1.7256e-05,  1.0669e-05,  ..., -2.1756e-05,
         -1.0394e-05, -1.6719e-05],
        [-4.9949e-05, -3.5524e-05,  2.1920e-05,  ..., -4.4793e-05,
         -2.1577e-05, -3.4481e-05]], device='cuda:0')
Loss: 1.1013243198394775
Graident accumulation at epoch 0, step 495, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0289, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0148, -0.0274,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.4118e-06,  6.2927e-05, -1.1694e-04,  ...,  4.0141e-05,
         -5.7268e-05,  9.3070e-06],
        [-2.2036e-05, -1.6376e-05,  8.0721e-06,  ..., -1.9251e-05,
         -6.1829e-06, -1.3520e-05],
        [ 1.8492e-05,  1.4429e-05, -2.8924e-06,  ...,  1.7114e-05,
          3.7383e-06,  1.0388e-05],
        [ 4.2100e-06,  3.3852e-06, -4.1685e-07,  ...,  2.5538e-06,
          1.6743e-07,  6.4249e-07],
        [-4.3926e-05, -3.2057e-05,  1.6053e-05,  ..., -3.9042e-05,
         -1.3226e-05, -2.7335e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1347e-08, 2.7665e-08, 3.4868e-08,  ..., 1.1463e-08, 8.6254e-08,
         1.1359e-08],
        [5.5936e-11, 3.1107e-11, 4.1879e-12,  ..., 3.8389e-11, 2.6297e-12,
         1.0538e-11],
        [7.0334e-10, 3.9978e-10, 3.2893e-11,  ..., 6.0718e-10, 3.9446e-11,
         1.8609e-10],
        [4.0254e-10, 2.4395e-10, 3.3865e-11,  ..., 2.5511e-10, 2.4916e-11,
         9.8142e-11],
        [2.4531e-10, 1.3490e-10, 1.5040e-11,  ..., 1.7570e-10, 1.0704e-11,
         4.9381e-11]], device='cuda:0')
optimizer state dict: 62.0
lr: [1.7842807614798848e-05, 1.7842807614798848e-05]
scheduler_last_epoch: 62


Running epoch 0, step 496, batch 496
Sampled inputs[:2]: tensor([[   0, 9577, 2789,  ..., 1042, 9086,  623],
        [   0,   12,  344,  ...,  824,   12,  968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8512e-05,  1.1096e-05,  3.5696e-05,  ...,  2.6644e-05,
         -1.3190e-05,  5.5519e-05],
        [-2.6375e-06, -1.8403e-06,  1.1250e-06,  ..., -2.3693e-06,
         -1.1399e-06, -1.7360e-06],
        [ 3.7283e-04,  3.2669e-04, -1.5883e-04,  ...,  3.1783e-04,
          1.9963e-04,  2.8648e-04],
        [-3.0845e-06, -2.1458e-06,  1.3113e-06,  ..., -2.7567e-06,
         -1.3039e-06, -2.0266e-06],
        [-6.4373e-06, -4.4703e-06,  2.7269e-06,  ..., -5.7518e-06,
         -2.7567e-06, -4.2319e-06]], device='cuda:0')
Loss: 1.1277821063995361


Running epoch 0, step 497, batch 497
Sampled inputs[:2]: tensor([[   0,  287,  516,  ..., 2386, 3492, 1663],
        [   0,  271,  266,  ..., 4298, 1231,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4548e-06,  4.9301e-05,  1.3951e-05,  ...,  4.8756e-05,
         -8.3781e-05,  6.6464e-05],
        [-5.2303e-06, -3.6806e-06,  2.2724e-06,  ..., -4.7237e-06,
         -2.3022e-06, -3.4794e-06],
        [ 3.6952e-04,  3.2434e-04, -1.5736e-04,  ...,  3.1482e-04,
          1.9817e-04,  2.8426e-04],
        [-6.0946e-06, -4.2766e-06,  2.6450e-06,  ..., -5.4836e-06,
         -2.6301e-06, -4.0382e-06],
        [-1.2636e-05, -8.8811e-06,  5.4687e-06,  ..., -1.1384e-05,
         -5.5134e-06, -8.4043e-06]], device='cuda:0')
Loss: 1.110660433769226


Running epoch 0, step 498, batch 498
Sampled inputs[:2]: tensor([[    0,    14,    22,  ...,  1319,   271,   266],
        [    0,   344,  8260,  ..., 16020, 18216, 11348]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6387e-05,  9.2581e-05,  3.0246e-05,  ...,  3.8514e-05,
         -1.1026e-04,  1.0199e-04],
        [-7.8529e-06, -5.5432e-06,  3.4273e-06,  ..., -7.0632e-06,
         -3.4943e-06, -5.2378e-06],
        [ 3.6614e-04,  3.2194e-04, -1.5587e-04,  ...,  3.1181e-04,
          1.9665e-04,  2.8200e-04],
        [-9.1344e-06, -6.4373e-06,  3.9861e-06,  ..., -8.1956e-06,
         -3.9935e-06, -6.0648e-06],
        [ 7.2590e-05,  3.4910e-05, -1.4849e-05,  ...,  5.7280e-05,
          4.4481e-05,  1.7350e-05]], device='cuda:0')
Loss: 1.1185311079025269


Running epoch 0, step 499, batch 499
Sampled inputs[:2]: tensor([[    0,   825,  3066,  ...,  1184,   266,  7964],
        [    0,  4566,   300,  ...,   271,  1644, 16473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6924e-05,  1.0143e-04,  3.0246e-05,  ...,  4.0318e-05,
         -1.7063e-04,  6.0159e-05],
        [-1.0371e-05, -7.3239e-06,  4.5598e-06,  ..., -9.3579e-06,
         -4.5821e-06, -6.9216e-06],
        [ 3.6284e-04,  3.1961e-04, -1.5439e-04,  ...,  3.0883e-04,
          1.9525e-04,  2.7981e-04],
        [-1.2159e-05, -8.5682e-06,  5.3421e-06,  ..., -1.0937e-05,
         -5.2676e-06, -8.0764e-06],
        [ 6.6421e-05,  3.0559e-05, -1.2093e-05,  ...,  5.1677e-05,
          4.1844e-05,  1.3267e-05]], device='cuda:0')
Loss: 1.1252014636993408


Running epoch 0, step 500, batch 500
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,   70,   27, 5311],
        [   0,  266, 8802,  ..., 8401,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9946e-05,  4.4256e-05, -1.9446e-05,  ...,  2.5418e-05,
         -1.5846e-04,  4.8767e-05],
        [-1.2994e-05, -9.1940e-06,  5.6773e-06,  ..., -1.1727e-05,
         -5.7518e-06, -8.6576e-06],
        [ 3.5951e-04,  3.1723e-04, -1.5297e-04,  ...,  3.0581e-04,
          1.9377e-04,  2.7760e-04],
        [-1.5199e-05, -1.0729e-05,  6.6310e-06,  ..., -1.3694e-05,
         -6.6012e-06, -1.0073e-05],
        [ 6.0043e-05,  2.6029e-05, -9.3955e-06,  ...,  4.5925e-05,
          3.9028e-05,  9.0651e-06]], device='cuda:0')
Loss: 1.1203113794326782


Running epoch 0, step 501, batch 501
Sampled inputs[:2]: tensor([[    0,  7849,   278,  ...,   346,   462,   221],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4756e-06,  7.0067e-05, -6.9792e-05,  ...,  3.0215e-05,
         -1.8339e-04,  1.5899e-05],
        [-1.5572e-05, -1.1049e-05,  6.8620e-06,  ..., -1.4082e-05,
         -6.9737e-06, -1.0453e-05],
        [ 3.5624e-04,  3.1487e-04, -1.5147e-04,  ...,  3.0281e-04,
          1.9224e-04,  2.7534e-04],
        [-1.8194e-05, -1.2875e-05,  8.0019e-06,  ..., -1.6436e-05,
         -7.9945e-06, -1.2144e-05],
        [ 5.3933e-05,  2.1648e-05, -6.5941e-06,  ...,  4.0352e-05,
          3.6167e-05,  4.8331e-06]], device='cuda:0')
Loss: 1.104684829711914


Running epoch 0, step 502, batch 502
Sampled inputs[:2]: tensor([[    0,   565,  5539,  ...,    12,   516, 14426],
        [    0,   342,   726,  ...,    12,   895,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2558e-06,  7.4354e-05, -1.2355e-04,  ...,  4.8353e-06,
         -1.9968e-04,  2.3550e-05],
        [-1.8179e-05, -1.2897e-05,  8.0168e-06,  ..., -1.6481e-05,
         -8.2031e-06, -1.2249e-05],
        [ 3.5298e-04,  3.1256e-04, -1.5003e-04,  ...,  2.9980e-04,
          1.9072e-04,  2.7310e-04],
        [-2.1130e-05, -1.4961e-05,  9.3058e-06,  ..., -1.9148e-05,
         -9.3579e-06, -1.4156e-05],
        [ 4.7764e-05,  1.7297e-05, -3.8672e-06,  ...,  3.4690e-05,
          3.3291e-05,  6.0120e-07]], device='cuda:0')
Loss: 1.1279159784317017


Running epoch 0, step 503, batch 503
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0, 21178,  1952,  ..., 14930,     9,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2938e-05,  1.6983e-04, -1.3880e-04,  ...,  4.0386e-05,
         -2.4847e-04,  2.0805e-05],
        [-2.0757e-05, -1.4752e-05,  9.1568e-06,  ..., -1.8820e-05,
         -9.3952e-06, -1.4015e-05],
        [ 3.4967e-04,  3.1019e-04, -1.4857e-04,  ...,  2.9681e-04,
          1.8921e-04,  2.7085e-04],
        [-2.4080e-05, -1.7077e-05,  1.0610e-05,  ..., -2.1815e-05,
         -1.0699e-05, -1.6168e-05],
        [ 4.1565e-05,  1.2856e-05, -1.1254e-06,  ...,  2.9087e-05,
          3.0445e-05, -3.6307e-06]], device='cuda:0')
Loss: 1.0999445915222168
Graident accumulation at epoch 0, step 503, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0147,  0.0034,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0148, -0.0274,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.0644e-06,  7.3617e-05, -1.1913e-04,  ...,  4.0165e-05,
         -7.6388e-05,  1.0457e-05],
        [-2.1908e-05, -1.6214e-05,  8.1806e-06,  ..., -1.9208e-05,
         -6.5042e-06, -1.3569e-05],
        [ 5.1610e-05,  4.4005e-05, -1.7461e-05,  ...,  4.5083e-05,
          2.2286e-05,  3.6435e-05],
        [ 1.3810e-06,  1.3390e-06,  6.8580e-07,  ...,  1.1688e-07,
         -9.1922e-07, -1.0385e-06],
        [-3.5377e-05, -2.7566e-05,  1.4335e-05,  ..., -3.2229e-05,
         -8.8586e-06, -2.4964e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1305e-08, 2.7666e-08, 3.4852e-08,  ..., 1.1453e-08, 8.6229e-08,
         1.1348e-08],
        [5.6310e-11, 3.1294e-11, 4.2676e-12,  ..., 3.8705e-11, 2.7153e-12,
         1.0723e-11],
        [8.2491e-10, 4.9560e-10, 5.4935e-11,  ..., 6.9466e-10, 7.5207e-11,
         2.5927e-10],
        [4.0272e-10, 2.4400e-10, 3.3944e-11,  ..., 2.5533e-10, 2.5005e-11,
         9.8305e-11],
        [2.4679e-10, 1.3493e-10, 1.5026e-11,  ..., 1.7638e-10, 1.1620e-11,
         4.9345e-11]], device='cuda:0')
optimizer state dict: 63.0
lr: [1.7765517845442444e-05, 1.7765517845442444e-05]
scheduler_last_epoch: 63


Running epoch 0, step 504, batch 504
Sampled inputs[:2]: tensor([[    0,  2165,  1323,  ...,   199,   677,  8376],
        [    0,    12,   221,  ...,   292, 27729,  9837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7367e-05,  3.5497e-05, -7.0022e-05,  ...,  4.2050e-05,
         -7.5948e-05,  2.1785e-05],
        [-2.4885e-06, -1.7360e-06,  1.0952e-06,  ..., -2.2799e-06,
         -1.1921e-06, -1.6689e-06],
        [-3.3230e-06, -2.3097e-06,  1.4603e-06,  ..., -3.0398e-06,
         -1.5646e-06, -2.2203e-06],
        [-3.0100e-06, -2.0862e-06,  1.3188e-06,  ..., -2.7567e-06,
         -1.4082e-06, -2.0117e-06],
        [-6.1989e-06, -4.3213e-06,  2.7120e-06,  ..., -5.6624e-06,
         -2.9355e-06, -4.1425e-06]], device='cuda:0')
Loss: 1.1037051677703857


Running epoch 0, step 505, batch 505
Sampled inputs[:2]: tensor([[   0, 7203,  271,  ...,   12,  275, 3338],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1481e-05,  3.3158e-05, -9.2906e-05,  ...,  6.1223e-05,
         -4.1326e-05,  5.8928e-05],
        [-4.9919e-06, -3.5092e-06,  2.1681e-06,  ..., -4.5896e-06,
         -2.3469e-06, -3.3379e-06],
        [-6.6608e-06, -4.6790e-06,  2.8908e-06,  ..., -6.1244e-06,
         -3.0920e-06, -4.4405e-06],
        [-6.0201e-06, -4.2170e-06,  2.6077e-06,  ..., -5.5283e-06,
         -2.7716e-06, -4.0084e-06],
        [-1.2368e-05, -8.7023e-06,  5.3495e-06,  ..., -1.1355e-05,
         -5.7667e-06, -8.2254e-06]], device='cuda:0')
Loss: 1.1174802780151367


Running epoch 0, step 506, batch 506
Sampled inputs[:2]: tensor([[    0,  7712, 31756,  ...,   895,   360,   630],
        [    0,  1635,   266,  ...,   437,  3302,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5966e-05,  3.3158e-05, -7.0471e-05,  ...,  1.1211e-04,
         -5.2230e-05,  4.6799e-05],
        [-7.4953e-06, -5.2676e-06,  3.2559e-06,  ..., -6.8694e-06,
         -3.4049e-06, -4.9397e-06],
        [-1.0028e-05, -7.0482e-06,  4.3511e-06,  ..., -9.1940e-06,
         -4.4927e-06, -6.5863e-06],
        [-9.1195e-06, -6.4075e-06,  3.9563e-06,  ..., -8.3596e-06,
         -4.0531e-06, -5.9903e-06],
        [-1.8656e-05, -1.3113e-05,  8.0764e-06,  ..., -1.7077e-05,
         -8.3894e-06, -1.2249e-05]], device='cuda:0')
Loss: 1.111283779144287


Running epoch 0, step 507, batch 507
Sampled inputs[:2]: tensor([[    0, 48545,    26,  ...,  1471,   266,   319],
        [    0,   328,  5180,  ...,   344,  2356,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7069e-05,  4.5062e-05, -1.0890e-04,  ...,  1.1860e-04,
         -9.6672e-05,  2.7354e-05],
        [-1.0028e-05, -7.0482e-06,  4.3884e-06,  ..., -9.2387e-06,
         -4.6492e-06, -6.6832e-06],
        [-1.3351e-05, -9.3877e-06,  5.8338e-06,  ..., -1.2293e-05,
         -6.0946e-06, -8.8662e-06],
        [-1.2144e-05, -8.5235e-06,  5.3048e-06,  ..., -1.1176e-05,
         -5.4985e-06, -8.0615e-06],
        [-2.4796e-05, -1.7434e-05,  1.0818e-05,  ..., -2.2799e-05,
         -1.1355e-05, -1.6451e-05]], device='cuda:0')
Loss: 1.1160199642181396


Running epoch 0, step 508, batch 508
Sampled inputs[:2]: tensor([[    0,    52, 26766,  ...,  4411,  4226,   278],
        [    0,  1159,   278,  ...,     9,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1277e-04,  1.7667e-05, -8.1346e-05,  ...,  1.4904e-04,
         -1.6484e-04,  5.8417e-05],
        [-1.2577e-05, -8.8885e-06,  5.5209e-06,  ..., -1.1548e-05,
         -5.7966e-06, -8.3521e-06],
        [-1.6764e-05, -1.1846e-05,  7.3463e-06,  ..., -1.5393e-05,
         -7.6145e-06, -1.1101e-05],
        [-1.5199e-05, -1.0729e-05,  6.6534e-06,  ..., -1.3947e-05,
         -6.8396e-06, -1.0058e-05],
        [-3.1143e-05, -2.1994e-05,  1.3620e-05,  ..., -2.8551e-05,
         -1.4171e-05, -2.0593e-05]], device='cuda:0')
Loss: 1.1273818016052246


Running epoch 0, step 509, batch 509
Sampled inputs[:2]: tensor([[    0,    89,  2023,  ...,  3230,   328,   790],
        [    0,   273,    14,  ...,   271,   266, 25408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4360e-04,  7.5767e-05, -1.0759e-04,  ...,  1.4655e-04,
         -1.6302e-04,  3.8059e-06],
        [-1.5140e-05, -1.0729e-05,  6.6161e-06,  ..., -1.3903e-05,
         -7.0110e-06, -1.0073e-05],
        [-2.0117e-05, -1.4260e-05,  8.7842e-06,  ..., -1.8477e-05,
         -9.1866e-06, -1.3351e-05],
        [-1.8209e-05, -1.2904e-05,  7.9498e-06,  ..., -1.6734e-05,
         -8.2403e-06, -1.2070e-05],
        [-3.7372e-05, -2.6494e-05,  1.6302e-05,  ..., -3.4302e-05,
         -1.7092e-05, -2.4766e-05]], device='cuda:0')
Loss: 1.1175122261047363


Running epoch 0, step 510, batch 510
Sampled inputs[:2]: tensor([[    0,   271, 36770,  ...,   278,  1398,  4555],
        [    0,   266,   858,  ..., 11265,   607,  7455]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4886e-04,  6.2614e-05, -8.4579e-05,  ...,  1.3409e-04,
         -1.6808e-04,  2.4353e-05],
        [-1.7688e-05, -1.2569e-05,  7.7039e-06,  ..., -1.6242e-05,
         -8.1956e-06, -1.1757e-05],
        [-2.3484e-05, -1.6689e-05,  1.0222e-05,  ..., -2.1577e-05,
         -1.0736e-05, -1.5572e-05],
        [-2.1219e-05, -1.5080e-05,  9.2387e-06,  ..., -1.9506e-05,
         -9.6187e-06, -1.4052e-05],
        [-4.3631e-05, -3.0994e-05,  1.8969e-05,  ..., -4.0054e-05,
         -1.9982e-05, -2.8908e-05]], device='cuda:0')
Loss: 1.1272034645080566


Running epoch 0, step 511, batch 511
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ..., 5996,   71,   19],
        [   0,  471,   14,  ..., 1260, 2129,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3089e-04,  6.1744e-05, -9.0545e-05,  ...,  1.2276e-04,
         -1.6732e-04,  5.2965e-05],
        [-2.0295e-05, -1.4380e-05,  8.7991e-06,  ..., -1.8567e-05,
         -9.3505e-06, -1.3433e-05],
        [-2.6941e-05, -1.9088e-05,  1.1683e-05,  ..., -2.4661e-05,
         -1.2256e-05, -1.7792e-05],
        [-2.4348e-05, -1.7241e-05,  1.0557e-05,  ..., -2.2292e-05,
         -1.0982e-05, -1.6049e-05],
        [-5.0068e-05, -3.5435e-05,  2.1666e-05,  ..., -4.5776e-05,
         -2.2799e-05, -3.3021e-05]], device='cuda:0')
Loss: 1.1015552282333374
Graident accumulation at epoch 0, step 511, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.7313e-06,  7.2430e-05, -1.1627e-04,  ...,  4.8425e-05,
         -8.5482e-05,  1.4708e-05],
        [-2.1747e-05, -1.6030e-05,  8.2425e-06,  ..., -1.9144e-05,
         -6.7888e-06, -1.3556e-05],
        [ 4.3755e-05,  3.7696e-05, -1.4546e-05,  ...,  3.8109e-05,
          1.8831e-05,  3.1012e-05],
        [-1.1920e-06, -5.1896e-07,  1.6730e-06,  ..., -2.1240e-06,
         -1.9255e-06, -2.5395e-06],
        [-3.6846e-05, -2.8353e-05,  1.5068e-05,  ..., -3.3584e-05,
         -1.0253e-05, -2.5770e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1281e-08, 2.7642e-08, 3.4825e-08,  ..., 1.1457e-08, 8.6171e-08,
         1.1339e-08],
        [5.6666e-11, 3.1469e-11, 4.3407e-12,  ..., 3.9011e-11, 2.8000e-12,
         1.0893e-11],
        [8.2481e-10, 4.9547e-10, 5.5016e-11,  ..., 6.9458e-10, 7.5282e-11,
         2.5932e-10],
        [4.0291e-10, 2.4405e-10, 3.4022e-11,  ..., 2.5557e-10, 2.5101e-11,
         9.8465e-11],
        [2.4905e-10, 1.3605e-10, 1.5480e-11,  ..., 1.7829e-10, 1.2128e-11,
         5.0386e-11]], device='cuda:0')
optimizer state dict: 64.0
lr: [1.7687041437173095e-05, 1.7687041437173095e-05]
scheduler_last_epoch: 64


Running epoch 0, step 512, batch 512
Sampled inputs[:2]: tensor([[    0, 29883,   680,  ...,  3363,  1049,   292],
        [    0,  4882,    12,  ...,    12,  9575,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2206e-06, -2.3690e-05,  2.9970e-05,  ..., -5.3014e-05,
         -1.0297e-05,  2.1034e-05],
        [-2.4885e-06, -1.7285e-06,  1.0952e-06,  ..., -2.3097e-06,
         -1.1474e-06, -1.6242e-06],
        [-3.3677e-06, -2.3544e-06,  1.4827e-06,  ..., -3.1292e-06,
         -1.5348e-06, -2.2054e-06],
        [-3.0696e-06, -2.1458e-06,  1.3560e-06,  ..., -2.8610e-06,
         -1.3858e-06, -2.0117e-06],
        [-6.2287e-06, -4.3213e-06,  2.7269e-06,  ..., -5.7817e-06,
         -2.8312e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.1289972066879272


Running epoch 0, step 513, batch 513
Sampled inputs[:2]: tensor([[    0,  2834,   266,  ..., 39474,    12, 15441],
        [    0,  1855,    14,  ...,    12,   287, 16479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3047e-05, -1.2806e-05,  3.6792e-05,  ..., -5.9035e-05,
         -3.1177e-05, -3.9587e-05],
        [-4.9174e-06, -3.4422e-06,  2.1979e-06,  ..., -4.5747e-06,
         -2.2724e-06, -3.1963e-06],
        [-6.6459e-06, -4.6790e-06,  2.9802e-06,  ..., -6.2138e-06,
         -3.0398e-06, -4.3213e-06],
        [-6.1095e-06, -4.2915e-06,  2.7493e-06,  ..., -5.7071e-06,
         -2.7642e-06, -3.9786e-06],
        [-1.2219e-05, -8.5533e-06,  5.4687e-06,  ..., -1.1414e-05,
         -5.5879e-06, -7.9274e-06]], device='cuda:0')
Loss: 1.1024595499038696


Running epoch 0, step 514, batch 514
Sampled inputs[:2]: tensor([[    0,  1756,   271,  ...,   259, 48595, 19882],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6430e-06,  4.2059e-05, -3.4108e-06,  ..., -3.2446e-05,
         -3.0837e-05, -1.4137e-05],
        [-7.4506e-06, -5.1931e-06,  3.2932e-06,  ..., -6.9141e-06,
         -3.5614e-06, -4.8876e-06],
        [-1.0073e-05, -7.0482e-06,  4.4629e-06,  ..., -9.3877e-06,
         -4.7609e-06, -6.6012e-06],
        [-9.1940e-06, -6.4224e-06,  4.0829e-06,  ..., -8.5682e-06,
         -4.2990e-06, -6.0350e-06],
        [-1.8477e-05, -1.2875e-05,  8.1807e-06,  ..., -1.7196e-05,
         -8.7321e-06, -1.2100e-05]], device='cuda:0')
Loss: 1.1093876361846924


Running epoch 0, step 515, batch 515
Sampled inputs[:2]: tensor([[   0,   14,  292,  ..., 1385,   12,  287],
        [   0,  365, 2714,  ...,  298,  273,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1040e-05,  7.4296e-05, -5.8395e-05,  ..., -3.9075e-05,
          7.5824e-06,  5.2158e-05],
        [-9.9838e-06, -7.0110e-06,  4.4107e-06,  ..., -9.2685e-06,
         -4.8056e-06, -6.5714e-06],
        [-1.3471e-05, -9.4920e-06,  5.9679e-06,  ..., -1.2547e-05,
         -6.4149e-06, -8.8513e-06],
        [-1.2293e-05, -8.6427e-06,  5.4538e-06,  ..., -1.1444e-05,
         -5.8040e-06, -8.0913e-06],
        [-2.4617e-05, -1.7285e-05,  1.0908e-05,  ..., -2.2888e-05,
         -1.1742e-05, -1.6183e-05]], device='cuda:0')
Loss: 1.102217674255371


Running epoch 0, step 516, batch 516
Sampled inputs[:2]: tensor([[   0,  408, 1782,  ...,  271,  729, 1692],
        [   0,  591, 2036,  ...,  266, 1027,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4058e-05,  6.9274e-05, -3.4980e-05,  ..., -4.3328e-05,
          4.5302e-05,  9.3450e-05],
        [-1.2457e-05, -8.7842e-06,  5.4911e-06,  ..., -1.1563e-05,
         -5.9307e-06, -8.1286e-06],
        [-1.6838e-05, -1.1921e-05,  7.4431e-06,  ..., -1.5676e-05,
         -7.9274e-06, -1.0982e-05],
        [-1.5363e-05, -1.0848e-05,  6.7949e-06,  ..., -1.4305e-05,
         -7.1675e-06, -1.0028e-05],
        [-3.0845e-05, -2.1756e-05,  1.3620e-05,  ..., -2.8670e-05,
         -1.4544e-05, -2.0087e-05]], device='cuda:0')
Loss: 1.0954782962799072


Running epoch 0, step 517, batch 517
Sampled inputs[:2]: tensor([[   0,  266, 9823,  ...,   14, 1062, 7676],
        [   0,  271,  259,  ..., 4511,   14,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9735e-06,  6.0007e-05, -7.7895e-05,  ..., -3.4676e-05,
          3.3203e-05,  4.0541e-05],
        [-1.4946e-05, -1.0587e-05,  6.6236e-06,  ..., -1.3873e-05,
         -7.1004e-06, -9.7752e-06],
        [-2.0221e-05, -1.4365e-05,  8.9854e-06,  ..., -1.8820e-05,
         -9.4995e-06, -1.3202e-05],
        [-1.8448e-05, -1.3083e-05,  8.2031e-06,  ..., -1.7181e-05,
         -8.5905e-06, -1.2055e-05],
        [-3.6955e-05, -2.6166e-05,  1.6391e-05,  ..., -3.4332e-05,
         -1.7390e-05, -2.4110e-05]], device='cuda:0')
Loss: 1.080163598060608


Running epoch 0, step 518, batch 518
Sampled inputs[:2]: tensor([[    0,   266, 12964,  ...,   300,  3979,  4706],
        [    0, 20596,  2943,  ...,  5560,  2512,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0830e-05,  1.4353e-05, -8.9841e-05,  ..., -4.0526e-05,
          2.0083e-05, -1.1675e-05],
        [-1.7419e-05, -1.2323e-05,  7.7561e-06,  ..., -1.6198e-05,
         -8.2552e-06, -1.1384e-05],
        [-2.3544e-05, -1.6704e-05,  1.0505e-05,  ..., -2.1949e-05,
         -1.1027e-05, -1.5363e-05],
        [-2.1517e-05, -1.5244e-05,  9.6112e-06,  ..., -2.0072e-05,
         -9.9987e-06, -1.4052e-05],
        [-4.3064e-05, -3.0458e-05,  1.9193e-05,  ..., -4.0084e-05,
         -2.0221e-05, -2.8074e-05]], device='cuda:0')
Loss: 1.1056450605392456


Running epoch 0, step 519, batch 519
Sampled inputs[:2]: tensor([[   0,  257,   13,  ...,  328,  630, 1403],
        [   0, 2771, 2070,  ...,  221,  396,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0979e-05,  4.0772e-06, -1.6367e-04,  ..., -4.1417e-05,
         -6.9865e-06,  3.1904e-05],
        [-1.9923e-05, -1.4111e-05,  8.8662e-06,  ..., -1.8522e-05,
         -9.4175e-06, -1.3016e-05],
        [-2.6911e-05, -1.9118e-05,  1.2003e-05,  ..., -2.5094e-05,
         -1.2577e-05, -1.7554e-05],
        [ 4.0948e-05,  3.4264e-05, -3.1191e-05,  ...,  4.6538e-05,
          2.8023e-05,  4.8047e-05],
        [-4.9293e-05, -3.4899e-05,  2.1949e-05,  ..., -4.5866e-05,
         -2.3097e-05, -3.2127e-05]], device='cuda:0')
Loss: 1.1238600015640259
Graident accumulation at epoch 0, step 519, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.1561e-06,  6.5594e-05, -1.2101e-04,  ...,  3.9440e-05,
         -7.7632e-05,  1.6427e-05],
        [-2.1564e-05, -1.5839e-05,  8.3048e-06,  ..., -1.9082e-05,
         -7.0517e-06, -1.3502e-05],
        [ 3.6688e-05,  3.2015e-05, -1.1891e-05,  ...,  3.1788e-05,
          1.5691e-05,  2.6155e-05],
        [ 3.0221e-06,  2.9593e-06, -1.6134e-06,  ...,  2.7422e-06,
          1.0693e-06,  2.5191e-06],
        [-3.8091e-05, -2.9008e-05,  1.5756e-05,  ..., -3.4812e-05,
         -1.1537e-05, -2.6406e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1241e-08, 2.7615e-08, 3.4817e-08,  ..., 1.1447e-08, 8.6085e-08,
         1.1329e-08],
        [5.7006e-11, 3.1637e-11, 4.4150e-12,  ..., 3.9315e-11, 2.8859e-12,
         1.1052e-11],
        [8.2471e-10, 4.9534e-10, 5.5105e-11,  ..., 6.9451e-10, 7.5365e-11,
         2.5937e-10],
        [4.0418e-10, 2.4498e-10, 3.4960e-11,  ..., 2.5748e-10, 2.5861e-11,
         1.0067e-10],
        [2.5123e-10, 1.3713e-10, 1.5947e-11,  ..., 1.8022e-10, 1.2649e-11,
         5.1368e-11]], device='cuda:0')
optimizer state dict: 65.0
lr: [1.7607390381871007e-05, 1.7607390381871007e-05]
scheduler_last_epoch: 65


Running epoch 0, step 520, batch 520
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4598e-05, -2.1347e-05, -3.0283e-05,  ...,  9.9291e-06,
         -4.3334e-05, -3.5816e-05],
        [-2.4885e-06, -1.7583e-06,  1.1399e-06,  ..., -2.3544e-06,
         -1.2144e-06, -1.5944e-06],
        [-3.3677e-06, -2.3991e-06,  1.5423e-06,  ..., -3.1888e-06,
         -1.6168e-06, -2.1458e-06],
        [-3.0696e-06, -2.1756e-06,  1.4082e-06,  ..., -2.9057e-06,
         -1.4603e-06, -1.9521e-06],
        [-6.0797e-06, -4.3213e-06,  2.7865e-06,  ..., -5.7518e-06,
         -2.9355e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.109669804573059


Running epoch 0, step 521, batch 521
Sampled inputs[:2]: tensor([[   0,  271,  259,  ..., 1345,  352,  365],
        [   0, 4385,  342,  ..., 3644,  775,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8339e-05, -9.1333e-06, -4.6167e-05,  ...,  1.0788e-05,
         -6.7535e-05, -2.6948e-05],
        [-4.9174e-06, -3.5092e-06,  2.2873e-06,  ..., -4.6641e-06,
         -2.3991e-06, -3.1590e-06],
        [-6.7204e-06, -4.8131e-06,  3.1143e-06,  ..., -6.3628e-06,
         -3.2187e-06, -4.2915e-06],
        [-6.1393e-06, -4.3958e-06,  2.8461e-06,  ..., -5.8264e-06,
         -2.9132e-06, -3.9190e-06],
        [-1.2100e-05, -8.6725e-06,  5.6177e-06,  ..., -1.1474e-05,
         -5.8413e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.1011980772018433


Running epoch 0, step 522, batch 522
Sampled inputs[:2]: tensor([[   0, 7117,  278,  ...,  287,  266,  944],
        [   0,  759, 4585,  ...,  360,  300,  670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0600e-05,  1.9795e-07, -2.5266e-05,  ...,  1.3876e-05,
         -6.1254e-05, -2.3368e-05],
        [-7.3910e-06, -5.2452e-06,  3.4049e-06,  ..., -6.9737e-06,
         -3.6582e-06, -4.7535e-06],
        [-1.0103e-05, -7.1824e-06,  4.6492e-06,  ..., -9.5218e-06,
         -4.9174e-06, -6.4671e-06],
        [-9.1940e-06, -6.5267e-06,  4.2319e-06,  ..., -8.6725e-06,
         -4.4331e-06, -5.8711e-06],
        [-1.8209e-05, -1.2934e-05,  8.3894e-06,  ..., -1.7166e-05,
         -8.9109e-06, -1.1653e-05]], device='cuda:0')
Loss: 1.112504243850708


Running epoch 0, step 523, batch 523
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,  1858,   499,  ...,    14,  1032,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2685e-05,  2.8536e-05, -5.6135e-05,  ...,  5.1354e-05,
         -8.2277e-05, -4.1664e-05],
        [-9.8199e-06, -6.9737e-06,  4.5151e-06,  ..., -9.2238e-06,
         -4.8503e-06, -6.3106e-06],
        [-1.3471e-05, -9.5814e-06,  6.1914e-06,  ..., -1.2651e-05,
         -6.5416e-06, -8.6278e-06],
        [-1.2279e-05, -8.7321e-06,  5.6550e-06,  ..., -1.1548e-05,
         -5.9083e-06, -7.8529e-06],
        [-2.4289e-05, -1.7256e-05,  1.1176e-05,  ..., -2.2829e-05,
         -1.1861e-05, -1.5557e-05]], device='cuda:0')
Loss: 1.0984389781951904


Running epoch 0, step 524, batch 524
Sampled inputs[:2]: tensor([[    0,    14,    69,  ...,   287,   259,  5158],
        [    0, 21540,   527,  ...,   824,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8131e-05,  1.0672e-04, -1.1185e-04,  ...,  4.6013e-05,
         -1.4618e-04,  4.9326e-05],
        [-1.2353e-05, -8.7172e-06,  5.6773e-06,  ..., -1.1593e-05,
         -6.2361e-06, -8.0764e-06],
        [-1.6868e-05, -1.1921e-05,  7.7561e-06,  ..., -1.5825e-05,
         -8.3745e-06, -1.0982e-05],
        [-1.5303e-05, -1.0818e-05,  7.0482e-06,  ..., -1.4380e-05,
         -7.5251e-06, -9.9540e-06],
        [-3.0398e-05, -2.1458e-05,  1.3992e-05,  ..., -2.8521e-05,
         -1.5169e-05, -1.9789e-05]], device='cuda:0')
Loss: 1.0848764181137085


Running epoch 0, step 525, batch 525
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   352, 10572,   345],
        [    0,   829,   874,  ...,   292,   380,   759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6657e-05,  1.1591e-04, -1.3763e-04,  ...,  1.9766e-05,
         -1.5951e-04,  4.4357e-05],
        [-1.4797e-05, -1.0431e-05,  6.7875e-06,  ..., -1.3858e-05,
         -7.4208e-06, -9.6411e-06],
        [-2.0206e-05, -1.4275e-05,  9.2760e-06,  ..., -1.8939e-05,
         -9.9689e-06, -1.3113e-05],
        [-1.8373e-05, -1.2979e-05,  8.4490e-06,  ..., -1.7241e-05,
         -8.9705e-06, -1.1906e-05],
        [-3.6418e-05, -2.5690e-05,  1.6734e-05,  ..., -3.4153e-05,
         -1.8060e-05, -2.3633e-05]], device='cuda:0')
Loss: 1.0841703414916992


Running epoch 0, step 526, batch 526
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,  1144,  2680,  ...,   963,     9,  1184]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6657e-05,  2.3387e-04, -1.9448e-04,  ...,  3.1145e-05,
         -1.8254e-04,  7.4310e-05],
        [-1.7330e-05, -1.2226e-05,  7.9349e-06,  ..., -1.6242e-05,
         -8.7246e-06, -1.1288e-05],
        [-2.3574e-05, -1.6674e-05,  1.0811e-05,  ..., -2.2113e-05,
         -1.1683e-05, -1.5303e-05],
        [-2.1458e-05, -1.5184e-05,  9.8571e-06,  ..., -2.0161e-05,
         -1.0528e-05, -1.3918e-05],
        [-4.2528e-05, -3.0041e-05,  1.9521e-05,  ..., -3.9905e-05,
         -2.1175e-05, -2.7597e-05]], device='cuda:0')
Loss: 1.1100083589553833


Running epoch 0, step 527, batch 527
Sampled inputs[:2]: tensor([[    0,   300, 12579,  ...,  1722,   369,  5049],
        [    0,    12,  1041,  ..., 22086,  3073,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4553e-04,  2.7644e-04, -1.9760e-04,  ...,  3.4082e-05,
         -1.7099e-04,  1.0345e-04],
        [-1.9819e-05, -1.4007e-05,  9.0525e-06,  ..., -1.8582e-05,
         -9.9838e-06, -1.2912e-05],
        [-2.6941e-05, -1.9088e-05,  1.2323e-05,  ..., -2.5287e-05,
         -1.3366e-05, -1.7494e-05],
        [-2.4512e-05, -1.7375e-05,  1.1235e-05,  ..., -2.3037e-05,
         -1.2040e-05, -1.5900e-05],
        [-4.8608e-05, -3.4392e-05,  2.2247e-05,  ..., -4.5627e-05,
         -2.4214e-05, -3.1531e-05]], device='cuda:0')
Loss: 1.1194239854812622
Graident accumulation at epoch 0, step 527, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0096, -0.0024, -0.0341],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0149, -0.0275,  ...,  0.0284, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3129e-06,  8.6679e-05, -1.2867e-04,  ...,  3.8905e-05,
         -8.6968e-05,  2.5130e-05],
        [-2.1390e-05, -1.5655e-05,  8.3796e-06,  ..., -1.9032e-05,
         -7.3449e-06, -1.3443e-05],
        [ 3.0325e-05,  2.6904e-05, -9.4699e-06,  ...,  2.6081e-05,
          1.2785e-05,  2.1790e-05],
        [ 2.6861e-07,  9.2591e-07, -3.2852e-07,  ...,  1.6424e-07,
         -2.4160e-07,  6.7723e-07],
        [-3.9142e-05, -2.9546e-05,  1.6406e-05,  ..., -3.5893e-05,
         -1.2805e-05, -2.6918e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1221e-08, 2.7664e-08, 3.4822e-08,  ..., 1.1437e-08, 8.6028e-08,
         1.1328e-08],
        [5.7342e-11, 3.1801e-11, 4.4925e-12,  ..., 3.9621e-11, 2.9827e-12,
         1.1207e-11],
        [8.2461e-10, 4.9521e-10, 5.5202e-11,  ..., 6.9446e-10, 7.5468e-11,
         2.5942e-10],
        [4.0438e-10, 2.4504e-10, 3.5052e-11,  ..., 2.5775e-10, 2.5980e-11,
         1.0083e-10],
        [2.5334e-10, 1.3818e-10, 1.6426e-11,  ..., 1.8212e-10, 1.3223e-11,
         5.2310e-11]], device='cuda:0')
optimizer state dict: 66.0
lr: [1.7526576850912724e-05, 1.7526576850912724e-05]
scheduler_last_epoch: 66


Running epoch 0, step 528, batch 528
Sampled inputs[:2]: tensor([[    0,   292, 17181,  ...,   634,  5039,   266],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8766e-05, -8.4290e-06, -9.3350e-05,  ...,  2.9084e-05,
         -6.0544e-05, -2.6108e-06],
        [-2.3991e-06, -1.7360e-06,  1.1623e-06,  ..., -2.2650e-06,
         -1.1548e-06, -1.5348e-06],
        [-3.3081e-06, -2.3991e-06,  1.6093e-06,  ..., -3.1441e-06,
         -1.5646e-06, -2.1160e-06],
        [-3.0547e-06, -2.2352e-06,  1.4901e-06,  ..., -2.9057e-06,
         -1.4380e-06, -1.9521e-06],
        [-5.8413e-06, -4.2319e-06,  2.8312e-06,  ..., -5.5432e-06,
         -2.7716e-06, -3.7253e-06]], device='cuda:0')
Loss: 1.081870436668396


Running epoch 0, step 529, batch 529
Sampled inputs[:2]: tensor([[    0,    12,   344,  ..., 10482,   950, 15744],
        [    0,  1742,    14,  ...,  1684,    13,  1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6112e-05,  6.3535e-05, -1.0170e-04,  ..., -1.7982e-06,
         -5.6761e-05,  5.0692e-05],
        [-4.8876e-06, -3.4869e-06,  2.3246e-06,  ..., -4.5747e-06,
         -2.4736e-06, -3.1963e-06],
        [-6.6608e-06, -4.7684e-06,  3.1739e-06,  ..., -6.2734e-06,
         -3.3006e-06, -4.3362e-06],
        [-6.0797e-06, -4.3809e-06,  2.9057e-06,  ..., -5.7220e-06,
         -2.9802e-06, -3.9488e-06],
        [-1.1832e-05, -8.4639e-06,  5.6326e-06,  ..., -1.1116e-05,
         -5.8860e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.105779767036438


Running epoch 0, step 530, batch 530
Sampled inputs[:2]: tensor([[    0, 16847,  2027,  ...,     5,  1460,   496],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2109e-05,  4.5655e-05, -1.0170e-04,  ...,  3.3996e-05,
         -1.6370e-04,  1.1219e-05],
        [-7.2718e-06, -5.1931e-06,  3.4720e-06,  ..., -6.8098e-06,
         -3.7029e-06, -4.7982e-06],
        [-9.9540e-06, -7.1377e-06,  4.7684e-06,  ..., -9.3728e-06,
         -4.9770e-06, -6.5565e-06],
        [-9.1195e-06, -6.5714e-06,  4.3809e-06,  ..., -8.5682e-06,
         -4.5076e-06, -5.9754e-06],
        [-1.7703e-05, -1.2666e-05,  8.4639e-06,  ..., -1.6600e-05,
         -8.8811e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.0925078392028809


Running epoch 0, step 531, batch 531
Sampled inputs[:2]: tensor([[    0, 13312,  9048,  ..., 33470,  8672,  3524],
        [    0,  2025,   287,  ...,   381,  1487,  3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7057e-05,  7.4695e-05, -7.8703e-05,  ...,  1.0190e-05,
         -1.6318e-04,  8.5446e-05],
        [-9.8050e-06, -6.9588e-06,  4.6417e-06,  ..., -9.1791e-06,
         -5.0664e-06, -6.4522e-06],
        [-1.3337e-05, -9.4920e-06,  6.3255e-06,  ..., -1.2532e-05,
         -6.7651e-06, -8.7619e-06],
        [-1.2159e-05, -8.7023e-06,  5.7891e-06,  ..., -1.1429e-05,
         -6.1095e-06, -7.9572e-06],
        [-2.3842e-05, -1.6958e-05,  1.1295e-05,  ..., -2.2352e-05,
         -1.2144e-05, -1.5616e-05]], device='cuda:0')
Loss: 1.1377520561218262


Running epoch 0, step 532, batch 532
Sampled inputs[:2]: tensor([[    0,    13, 20054,  ...,    19,     9,   266],
        [    0,   266,  1527,  ...,  2525,    14, 11570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9404e-05,  7.6649e-05, -1.0807e-04,  ..., -1.9372e-05,
         -1.5399e-04,  8.5446e-05],
        [-1.2189e-05, -8.6725e-06,  5.8040e-06,  ..., -1.1414e-05,
         -6.2212e-06, -7.9647e-06],
        [-1.6630e-05, -1.1876e-05,  7.9349e-06,  ..., -1.5631e-05,
         -8.3297e-06, -1.0848e-05],
        [-1.5229e-05, -1.0923e-05,  7.2941e-06,  ..., -1.4335e-05,
         -7.5474e-06, -9.8944e-06],
        [-2.9713e-05, -2.1189e-05,  1.4171e-05,  ..., -2.7895e-05,
         -1.4961e-05, -1.9342e-05]], device='cuda:0')
Loss: 1.089799165725708


Running epoch 0, step 533, batch 533
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1371, 10516,  ...,  2456,    13,  6469]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6490e-05,  1.3072e-04, -7.5141e-05,  ..., -5.2436e-05,
         -1.2343e-04,  7.3515e-05],
        [-1.4722e-05, -1.0453e-05,  6.9439e-06,  ..., -1.3784e-05,
         -7.5400e-06, -9.5963e-06],
        [-2.0057e-05, -1.4305e-05,  9.4920e-06,  ..., -1.8850e-05,
         -1.0088e-05, -1.3053e-05],
        [-1.8358e-05, -1.3128e-05,  8.7172e-06,  ..., -1.7270e-05,
         -9.1344e-06, -1.1906e-05],
        [-3.5793e-05, -2.5481e-05,  1.6928e-05,  ..., -3.3587e-05,
         -1.8075e-05, -2.3246e-05]], device='cuda:0')
Loss: 1.1012190580368042


Running epoch 0, step 534, batch 534
Sampled inputs[:2]: tensor([[    0,   221,  1771,  ..., 14547,  1705,  1003],
        [    0,  2836,  3084,  ...,  3634,  6464,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0553e-05,  1.6850e-04, -2.8431e-05,  ..., -1.7870e-05,
         -1.5582e-04,  1.0657e-04],
        [-1.7181e-05, -1.2212e-05,  8.1137e-06,  ..., -1.6078e-05,
         -8.8215e-06, -1.1198e-05],
        [-2.3425e-05, -1.6719e-05,  1.1086e-05,  ..., -2.1994e-05,
         -1.1809e-05, -1.5229e-05],
        [-2.1413e-05, -1.5318e-05,  1.0163e-05,  ..., -2.0117e-05,
         -1.0677e-05, -1.3888e-05],
        [-4.1723e-05, -2.9743e-05,  1.9744e-05,  ..., -3.9101e-05,
         -2.1115e-05, -2.7090e-05]], device='cuda:0')
Loss: 1.0957858562469482


Running epoch 0, step 535, batch 535
Sampled inputs[:2]: tensor([[   0, 1042, 5738,  ...,   12,  287, 3643],
        [   0, 9058, 4048,  ...,   14,  759, 1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8618e-05,  2.3249e-04, -6.4994e-05,  ..., -3.9693e-05,
         -1.5721e-04,  7.0287e-05],
        [-1.9595e-05, -1.3925e-05,  9.2760e-06,  ..., -1.8433e-05,
         -1.0073e-05, -1.2778e-05],
        [-2.6703e-05, -1.9059e-05,  1.2659e-05,  ..., -2.5198e-05,
         -1.3471e-05, -1.7360e-05],
        [-2.4453e-05, -1.7479e-05,  1.1623e-05,  ..., -2.3097e-05,
         -1.2197e-05, -1.5870e-05],
        [-4.7535e-05, -3.3885e-05,  2.2531e-05,  ..., -4.4793e-05,
         -2.4065e-05, -3.0875e-05]], device='cuda:0')
Loss: 1.094415307044983
Graident accumulation at epoch 0, step 535, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0149, -0.0275,  ...,  0.0284, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0543e-05,  1.0126e-04, -1.2230e-04,  ...,  3.1045e-05,
         -9.3992e-05,  2.9645e-05],
        [-2.1210e-05, -1.5482e-05,  8.4692e-06,  ..., -1.8972e-05,
         -7.6177e-06, -1.3376e-05],
        [ 2.4622e-05,  2.2308e-05, -7.2570e-06,  ...,  2.0953e-05,
          1.0159e-05,  1.7875e-05],
        [-2.2035e-06, -9.1459e-07,  8.6663e-07,  ..., -2.1619e-06,
         -1.4371e-06, -9.7747e-07],
        [-3.9982e-05, -2.9980e-05,  1.7018e-05,  ..., -3.6783e-05,
         -1.3931e-05, -2.7314e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1182e-08, 2.7690e-08, 3.4791e-08,  ..., 1.1427e-08, 8.5967e-08,
         1.1322e-08],
        [5.7669e-11, 3.1963e-11, 4.5741e-12,  ..., 3.9921e-11, 3.0812e-12,
         1.1359e-11],
        [8.2450e-10, 4.9508e-10, 5.5307e-11,  ..., 6.9440e-10, 7.5574e-11,
         2.5946e-10],
        [4.0457e-10, 2.4510e-10, 3.5152e-11,  ..., 2.5803e-10, 2.6103e-11,
         1.0098e-10],
        [2.5535e-10, 1.3919e-10, 1.6917e-11,  ..., 1.8395e-10, 1.3789e-11,
         5.3211e-11]], device='cuda:0')
optimizer state dict: 67.0
lr: [1.7444613193311206e-05, 1.7444613193311206e-05]
scheduler_last_epoch: 67


Running epoch 0, step 536, batch 536
Sampled inputs[:2]: tensor([[    0,  7061,   437,  ...,   278,  9500,    18],
        [    0,  1040,   287,  ...,    14, 10209,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0980e-06,  4.5561e-05,  1.3600e-05,  ...,  2.4143e-05,
          5.2442e-06,  5.0740e-05],
        [-2.5779e-06, -1.7658e-06,  1.2070e-06,  ..., -2.3395e-06,
         -1.4454e-06, -1.7583e-06],
        [-3.4124e-06, -2.3395e-06,  1.6019e-06,  ..., -3.1143e-06,
         -1.8775e-06, -2.3097e-06],
        [-3.0994e-06, -2.1309e-06,  1.4603e-06,  ..., -2.8312e-06,
         -1.6838e-06, -2.1011e-06],
        [-6.0499e-06, -4.1425e-06,  2.8312e-06,  ..., -5.5134e-06,
         -3.3379e-06, -4.1127e-06]], device='cuda:0')
Loss: 1.1015827655792236


Running epoch 0, step 537, batch 537
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,   221,  4070,  ...,  1061,  3189,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2089e-05,  1.4925e-04,  1.5503e-05,  ..., -4.7065e-07,
         -3.3656e-06,  1.2832e-04],
        [-5.1260e-06, -3.5018e-06,  2.4289e-06,  ..., -4.6939e-06,
         -2.9132e-06, -3.5018e-06],
        [-6.7651e-06, -4.6343e-06,  3.2187e-06,  ..., -6.2138e-06,
         -3.7774e-06, -4.6045e-06],
        [-6.1393e-06, -4.2021e-06,  2.9281e-06,  ..., -5.6326e-06,
         -3.3826e-06, -4.1723e-06],
        [-1.2010e-05, -8.2254e-06,  5.7071e-06,  ..., -1.1027e-05,
         -6.7204e-06, -8.1658e-06]], device='cuda:0')
Loss: 1.088875651359558


Running epoch 0, step 538, batch 538
Sampled inputs[:2]: tensor([[    0,   680,   271,  ..., 12942,   271,   266],
        [    0,   445,   749,  ...,   850,  1028,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9196e-06,  7.3106e-05, -3.9633e-05,  ...,  6.2816e-06,
         -5.1358e-05,  5.3931e-05],
        [-7.5549e-06, -5.2154e-06,  3.5912e-06,  ..., -6.9290e-06,
         -4.0829e-06, -5.0440e-06],
        [-1.0118e-05, -7.0035e-06,  4.8280e-06,  ..., -9.2983e-06,
         -5.3570e-06, -6.7204e-06],
        [-9.2685e-06, -6.4224e-06,  4.4331e-06,  ..., -8.5235e-06,
         -4.8280e-06, -6.1393e-06],
        [-1.7941e-05, -1.2398e-05,  8.5384e-06,  ..., -1.6481e-05,
         -9.5218e-06, -1.1891e-05]], device='cuda:0')
Loss: 1.101517677307129


Running epoch 0, step 539, batch 539
Sampled inputs[:2]: tensor([[    0,   508, 12163,  ...,  4920,   344, 11003],
        [    0,    12,   287,  ...,  3359,  1751,  5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7856e-05, -8.2765e-06, -9.8030e-05,  ...,  4.4139e-05,
         -8.9607e-05,  7.8450e-05],
        [-9.9987e-06, -6.8992e-06,  4.7982e-06,  ..., -9.1791e-06,
         -5.3719e-06, -6.6459e-06],
        [-1.3456e-05, -9.3132e-06,  6.4895e-06,  ..., -1.2398e-05,
         -7.0855e-06, -8.9109e-06],
        [-1.2413e-05, -8.5831e-06,  5.9903e-06,  ..., -1.1429e-05,
         -6.4299e-06, -8.1956e-06],
        [-2.3842e-05, -1.6481e-05,  1.1459e-05,  ..., -2.1935e-05,
         -1.2577e-05, -1.5736e-05]], device='cuda:0')
Loss: 1.0836213827133179


Running epoch 0, step 540, batch 540
Sampled inputs[:2]: tensor([[    0,  1371,   287,  ...,   689,   278, 12774],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6601e-05, -2.5307e-05, -1.3655e-04,  ...,  2.9347e-05,
         -1.0693e-04,  7.7946e-05],
        [-1.2562e-05, -8.6799e-06,  6.0350e-06,  ..., -1.1504e-05,
         -6.7875e-06, -8.4043e-06],
        [-1.6838e-05, -1.1668e-05,  8.1211e-06,  ..., -1.5467e-05,
         -8.9034e-06, -1.1221e-05],
        [-1.5542e-05, -1.0744e-05,  7.4953e-06,  ..., -1.4260e-05,
         -8.0913e-06, -1.0312e-05],
        [-2.9832e-05, -2.0623e-05,  1.4350e-05,  ..., -2.7359e-05,
         -1.5825e-05, -1.9819e-05]], device='cuda:0')
Loss: 1.1089022159576416


Running epoch 0, step 541, batch 541
Sampled inputs[:2]: tensor([[    0, 26138,    17,  ...,   401,  1867,  4977],
        [    0,   221,   380,  ...,  5543,   768,  6375]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2844e-05,  3.7109e-05, -1.6617e-04,  ...,  7.3286e-05,
         -1.7087e-04,  6.1238e-05],
        [-1.5035e-05, -1.0356e-05,  7.2494e-06,  ..., -1.3769e-05,
         -8.0988e-06, -1.0036e-05],
        [-2.0146e-05, -1.3903e-05,  9.7528e-06,  ..., -1.8507e-05,
         -1.0617e-05, -1.3396e-05],
        [-1.8641e-05, -1.2845e-05,  9.0301e-06,  ..., -1.7121e-05,
         -9.6783e-06, -1.2353e-05],
        [-3.5763e-05, -2.4647e-05,  1.7270e-05,  ..., -3.2812e-05,
         -1.8924e-05, -2.3723e-05]], device='cuda:0')
Loss: 1.109128713607788


Running epoch 0, step 542, batch 542
Sampled inputs[:2]: tensor([[   0,    9, 1471,  ...,  741,  266, 5821],
        [   0,  870,  278,  ...,  478,  401,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3197e-05,  1.0639e-04, -2.7222e-04,  ...,  1.1371e-04,
         -1.6094e-04,  4.9859e-05],
        [-1.7568e-05, -1.2092e-05,  8.4415e-06,  ..., -1.6153e-05,
         -9.6262e-06, -1.1794e-05],
        [-2.3454e-05, -1.6168e-05,  1.1317e-05,  ..., -2.1622e-05,
         -1.2569e-05, -1.5691e-05],
        [-2.1651e-05, -1.4916e-05,  1.0453e-05,  ..., -1.9968e-05,
         -1.1437e-05, -1.4439e-05],
        [-4.1604e-05, -2.8640e-05,  2.0027e-05,  ..., -3.8296e-05,
         -2.2382e-05, -2.7746e-05]], device='cuda:0')
Loss: 1.0955907106399536


Running epoch 0, step 543, batch 543
Sampled inputs[:2]: tensor([[   0, 2085,   12,  ...,  496,   14,  747],
        [   0,    9,  342,  ...,   12,  709,  857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8761e-05,  6.4064e-05, -3.4112e-04,  ...,  1.4255e-04,
         -1.9831e-04, -3.1825e-07],
        [-2.0042e-05, -1.3776e-05,  9.6262e-06,  ..., -1.8463e-05,
         -1.0975e-05, -1.3478e-05],
        [-2.6733e-05, -1.8403e-05,  1.2890e-05,  ..., -2.4676e-05,
         -1.4313e-05, -1.7911e-05],
        [-2.4736e-05, -1.7017e-05,  1.1936e-05,  ..., -2.2858e-05,
         -1.3061e-05, -1.6525e-05],
        [-4.7445e-05, -3.2604e-05,  2.2829e-05,  ..., -4.3720e-05,
         -2.5496e-05, -3.1680e-05]], device='cuda:0')
Loss: 1.0918670892715454
Graident accumulation at epoch 0, step 543, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0149, -0.0275,  ...,  0.0284, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2365e-05,  9.7540e-05, -1.4418e-04,  ...,  4.2196e-05,
         -1.0442e-04,  2.6649e-05],
        [-2.1093e-05, -1.5312e-05,  8.5849e-06,  ..., -1.8921e-05,
         -7.9534e-06, -1.3387e-05],
        [ 1.9487e-05,  1.8237e-05, -5.2424e-06,  ...,  1.6390e-05,
          7.7122e-06,  1.4297e-05],
        [-4.4568e-06, -2.5248e-06,  1.9735e-06,  ..., -4.2315e-06,
         -2.5995e-06, -2.5323e-06],
        [-4.0728e-05, -3.0242e-05,  1.7599e-05,  ..., -3.7477e-05,
         -1.5087e-05, -2.7751e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1142e-08, 2.7666e-08, 3.4873e-08,  ..., 1.1436e-08, 8.5920e-08,
         1.1311e-08],
        [5.8013e-11, 3.2121e-11, 4.6622e-12,  ..., 4.0222e-11, 3.1986e-12,
         1.1530e-11],
        [8.2439e-10, 4.9492e-10, 5.5418e-11,  ..., 6.9431e-10, 7.5703e-11,
         2.5952e-10],
        [4.0478e-10, 2.4514e-10, 3.5259e-11,  ..., 2.5829e-10, 2.6248e-11,
         1.0115e-10],
        [2.5734e-10, 1.4011e-10, 1.7421e-11,  ..., 1.8567e-10, 1.4425e-11,
         5.4162e-11]], device='cuda:0')
optimizer state dict: 68.0
lr: [1.73615119338288e-05, 1.73615119338288e-05]
scheduler_last_epoch: 68


Running epoch 0, step 544, batch 544
Sampled inputs[:2]: tensor([[    0, 10511,  3887,  ...,  3504,   298,   422],
        [    0,  1039,   259,  ...,   221,   685,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2671e-04,  2.0749e-05, -1.1087e-04,  ...,  5.6859e-05,
         -4.3256e-05, -4.4547e-05],
        [-2.5034e-06, -1.7211e-06,  1.2517e-06,  ..., -2.3097e-06,
         -1.5199e-06, -1.8030e-06],
        [-3.2634e-06, -2.2352e-06,  1.6317e-06,  ..., -2.9951e-06,
         -1.9222e-06, -2.3246e-06],
        [-2.9802e-06, -2.0415e-06,  1.4827e-06,  ..., -2.7418e-06,
         -1.7211e-06, -2.1160e-06],
        [-5.6922e-06, -3.9041e-06,  2.8461e-06,  ..., -5.2154e-06,
         -3.3677e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.0923054218292236


Running epoch 0, step 545, batch 545
Sampled inputs[:2]: tensor([[    0, 21748,   792,  ...,   408,   266, 31879],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9580e-05,  7.4508e-05, -1.5544e-04,  ...,  7.1489e-05,
         -4.5997e-05, -6.5313e-05],
        [-5.1260e-06, -3.4124e-06,  2.4661e-06,  ..., -4.6790e-06,
         -3.0696e-06, -3.6955e-06],
        [ 7.8453e-05,  5.3298e-05, -5.4874e-05,  ...,  7.9099e-05,
          5.4266e-05,  5.3630e-05],
        [-6.1095e-06, -4.0680e-06,  2.9355e-06,  ..., -5.5879e-06,
         -3.5092e-06, -4.3660e-06],
        [ 5.1312e-05,  5.6804e-05, -6.4223e-05,  ...,  7.3092e-05,
          5.1778e-05,  6.9229e-05]], device='cuda:0')
Loss: 1.1220283508300781


Running epoch 0, step 546, batch 546
Sampled inputs[:2]: tensor([[    0,  2895,    26,  ..., 11645,  1535,  1558],
        [    0,  2173,   292,  ...,   344,  8106,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0557e-04,  1.3915e-05, -1.1196e-04,  ...,  4.9968e-05,
         -2.2369e-05, -5.6825e-05],
        [-7.7337e-06, -5.1484e-06,  3.7253e-06,  ..., -7.0333e-06,
         -4.5672e-06, -5.5209e-06],
        [ 7.5056e-05,  5.1033e-05, -5.3234e-05,  ...,  7.6029e-05,
          5.2366e-05,  5.1261e-05],
        [-9.2536e-06, -6.1542e-06,  4.4480e-06,  ..., -8.4192e-06,
         -5.2452e-06, -6.5565e-06],
        [ 4.5321e-05,  5.2811e-05, -6.1332e-05,  ...,  6.7698e-05,
          4.8395e-05,  6.5057e-05]], device='cuda:0')
Loss: 1.1117535829544067


Running epoch 0, step 547, batch 547
Sampled inputs[:2]: tensor([[   0,  741, 4933,  ...,  932,  365,  838],
        [   0, 5182,  446,  ...,  417,  199,   50]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2756e-04, -4.0088e-05, -8.5558e-05,  ...,  9.0484e-05,
         -9.8554e-05, -8.8762e-05],
        [-1.0177e-05, -6.8173e-06,  4.9472e-06,  ..., -9.3132e-06,
         -5.8487e-06, -7.1973e-06],
        [ 7.1778e-05,  4.8783e-05, -5.1595e-05,  ...,  7.2959e-05,
          5.0689e-05,  4.9026e-05],
        [-1.2353e-05, -8.2701e-06,  5.9977e-06,  ..., -1.1325e-05,
         -6.8024e-06, -8.6576e-06],
        [ 3.9510e-05,  4.8847e-05, -5.8426e-05,  ...,  6.2274e-05,
          4.5415e-05,  6.1093e-05]], device='cuda:0')
Loss: 1.108504295349121


Running epoch 0, step 548, batch 548
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7500e-04, -4.7199e-05, -6.3997e-05,  ...,  1.2442e-05,
         -1.0293e-04, -7.7086e-05],
        [-1.2711e-05, -8.4862e-06,  6.1616e-06,  ..., -1.1638e-05,
         -7.2420e-06, -8.9332e-06],
        [ 1.6135e-04,  9.4083e-05, -8.9102e-05,  ...,  1.3913e-04,
          1.1178e-04,  1.0688e-04],
        [-1.5482e-05, -1.0327e-05,  7.5027e-06,  ..., -1.4201e-05,
         -8.4639e-06, -1.0788e-05],
        [ 3.3579e-05,  4.4943e-05, -5.5580e-05,  ...,  5.6820e-05,
          4.2211e-05,  5.7070e-05]], device='cuda:0')
Loss: 1.112982988357544


Running epoch 0, step 549, batch 549
Sampled inputs[:2]: tensor([[    0, 14161,  1241,  ..., 15255,   768,  4239],
        [    0,    12,   297,  ...,  2980,  1145, 17207]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0231e-04, -1.0219e-04, -6.2860e-05,  ...,  2.6068e-05,
         -9.6708e-05, -4.8475e-07],
        [-1.5289e-05, -1.0185e-05,  7.3910e-06,  ..., -1.3947e-05,
         -8.7619e-06, -1.0744e-05],
        [ 1.5800e-04,  9.1862e-05, -8.7500e-05,  ...,  1.3612e-04,
          1.0985e-04,  1.0452e-04],
        [-1.8552e-05, -1.2353e-05,  8.9705e-06,  ..., -1.6972e-05,
         -1.0215e-05, -1.2934e-05],
        [ 2.7619e-05,  4.1009e-05, -5.2734e-05,  ...,  5.1485e-05,
          3.8769e-05,  5.2897e-05]], device='cuda:0')
Loss: 1.107089877128601


Running epoch 0, step 550, batch 550
Sampled inputs[:2]: tensor([[    0,   642,   271,  ...,  5430,  2314,  6431],
        [    0,   287,   266,  ...,   998,   342, 17709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0708e-04, -1.4431e-04, -1.6233e-04,  ...,  3.7888e-05,
         -7.3952e-05,  1.8207e-05],
        [-1.7852e-05, -1.1958e-05,  8.6352e-06,  ..., -1.6242e-05,
         -1.0200e-05, -1.2472e-05],
        [ 1.5463e-04,  8.9538e-05, -8.5868e-05,  ...,  1.3311e-04,
          1.0802e-04,  1.0227e-04],
        [-2.1726e-05, -1.4544e-05,  1.0505e-05,  ..., -1.9819e-05,
         -1.1906e-05, -1.5050e-05],
        [ 2.1688e-05,  3.6926e-05, -4.9873e-05,  ...,  4.6210e-05,
          3.5535e-05,  4.8934e-05]], device='cuda:0')
Loss: 1.0870556831359863


Running epoch 0, step 551, batch 551
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  3152,  1385,  ...,  1403,   518,  2088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8286e-04, -1.1625e-04, -1.7858e-04,  ...,  4.7857e-05,
         -9.7341e-05, -3.7691e-05],
        [-2.0415e-05, -1.3642e-05,  9.8199e-06,  ..., -1.8537e-05,
         -1.1615e-05, -1.4216e-05],
        [ 1.5126e-04,  8.7318e-05, -8.4311e-05,  ...,  1.3008e-04,
          1.0620e-04,  9.9991e-05],
        [-2.4930e-05, -1.6659e-05,  1.1988e-05,  ..., -2.2694e-05,
         -1.3612e-05, -1.7211e-05],
        [ 1.5638e-05,  3.2933e-05, -4.7072e-05,  ...,  4.0786e-05,
          3.2257e-05,  4.4851e-05]], device='cuda:0')
Loss: 1.0906795263290405
Graident accumulation at epoch 0, step 551, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0149, -0.0275,  ...,  0.0284, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7158e-05,  7.6161e-05, -1.4762e-04,  ...,  4.2762e-05,
         -1.0372e-04,  2.0215e-05],
        [-2.1026e-05, -1.5145e-05,  8.7084e-06,  ..., -1.8882e-05,
         -8.3196e-06, -1.3469e-05],
        [ 3.2664e-05,  2.5145e-05, -1.3149e-05,  ...,  2.7759e-05,
          1.7561e-05,  2.2866e-05],
        [-6.5041e-06, -3.9383e-06,  2.9750e-06,  ..., -6.0778e-06,
         -3.7008e-06, -4.0001e-06],
        [-3.5091e-05, -2.3925e-05,  1.1132e-05,  ..., -2.9651e-05,
         -1.0353e-05, -2.0490e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1181e-08, 2.7652e-08, 3.4870e-08,  ..., 1.1427e-08, 8.5844e-08,
         1.1301e-08],
        [5.8371e-11, 3.2275e-11, 4.7539e-12,  ..., 4.0526e-11, 3.3303e-12,
         1.1720e-11],
        [8.4644e-10, 5.0205e-10, 6.2471e-11,  ..., 7.1054e-10, 8.6907e-11,
         2.6926e-10],
        [4.0500e-10, 2.4518e-10, 3.5368e-11,  ..., 2.5855e-10, 2.6407e-11,
         1.0135e-10],
        [2.5733e-10, 1.4106e-10, 1.9619e-11,  ..., 1.8715e-10, 1.5451e-11,
         5.6119e-11]], device='cuda:0')
optimizer state dict: 69.0
lr: [1.7277285771063362e-05, 1.7277285771063362e-05]
scheduler_last_epoch: 69


Running epoch 0, step 552, batch 552
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,  4684, 14107,   259],
        [    0, 16765,   367,  ..., 30192,  7038,  8135]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5117e-05, -3.9536e-05, -6.7447e-05,  ...,  5.1550e-05,
         -5.7016e-05,  1.7438e-06],
        [-2.5034e-06, -1.6838e-06,  1.1921e-06,  ..., -2.2501e-06,
         -1.2293e-06, -1.6764e-06],
        [-3.4124e-06, -2.3097e-06,  1.6242e-06,  ..., -3.0845e-06,
         -1.6168e-06, -2.2799e-06],
        [-3.2187e-06, -2.1607e-06,  1.5274e-06,  ..., -2.9057e-06,
         -1.5050e-06, -2.1458e-06],
        [-5.9307e-06, -3.9637e-06,  2.8014e-06,  ..., -5.3346e-06,
         -2.8312e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.0850859880447388


Running epoch 0, step 553, batch 553
Sampled inputs[:2]: tensor([[    0,   266,  3727,  ...,  1143,   271,  5213],
        [    0,  4653, 21419,  ...,  7845,   300,   565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3673e-05, -1.0522e-04, -5.7765e-05,  ..., -1.8160e-06,
         -4.6606e-05, -7.4697e-05],
        [-5.0962e-06, -3.3677e-06,  2.4363e-06,  ..., -4.5747e-06,
         -2.7269e-06, -3.5018e-06],
        [-6.7800e-06, -4.5151e-06,  3.2410e-06,  ..., -6.1244e-06,
         -3.5167e-06, -4.6492e-06],
        [ 4.6767e-04,  2.5041e-04, -2.4608e-04,  ...,  3.9257e-04,
          3.0378e-04,  2.7101e-04],
        [-1.1891e-05, -7.8380e-06,  5.6624e-06,  ..., -1.0699e-05,
         -6.2138e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1137868165969849


Running epoch 0, step 554, batch 554
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,   221,   474,  ...,   287,   271,  2540]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8729e-06, -4.7731e-05, -5.3599e-05,  ...,  1.5668e-04,
         -1.2306e-04, -1.0892e-04],
        [-7.6890e-06, -5.0217e-06,  3.6582e-06,  ..., -6.8694e-06,
         -4.3362e-06, -5.3495e-06],
        [-1.0133e-05, -6.6608e-06,  4.8280e-06,  ..., -9.1046e-06,
         -5.5283e-06, -7.0184e-06],
        [ 4.6454e-04,  2.4840e-04, -2.4459e-04,  ...,  3.8978e-04,
          3.0192e-04,  2.6879e-04],
        [-1.7822e-05, -1.1623e-05,  8.4639e-06,  ..., -1.5944e-05,
         -9.7901e-06, -1.2279e-05]], device='cuda:0')
Loss: 1.1009382009506226


Running epoch 0, step 555, batch 555
Sampled inputs[:2]: tensor([[   0,   12,  298,  ...,  292,   36,    9],
        [   0, 1196, 2612,  ..., 2489,   14,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4649e-05, -7.9199e-05, -1.1178e-04,  ...,  2.3824e-04,
         -1.2910e-04, -1.9826e-04],
        [-1.0192e-05, -6.6757e-06,  4.8801e-06,  ..., -9.1195e-06,
         -5.8264e-06, -7.1600e-06],
        [-1.3426e-05, -8.8513e-06,  6.4299e-06,  ..., -1.2085e-05,
         -7.4133e-06, -9.3728e-06],
        [ 4.6143e-04,  2.4634e-04, -2.4308e-04,  ...,  3.8696e-04,
          3.0017e-04,  2.6657e-04],
        [-2.3603e-05, -1.5438e-05,  1.1280e-05,  ..., -2.1160e-05,
         -1.3128e-05, -1.6421e-05]], device='cuda:0')
Loss: 1.0814361572265625


Running epoch 0, step 556, batch 556
Sampled inputs[:2]: tensor([[   0,  292,   65,  ...,   12,  857,  344],
        [   0,  607,  443,  ...,  259, 2646, 1597]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0280e-06, -1.2112e-04, -1.7651e-04,  ...,  2.5981e-04,
         -1.7000e-04, -2.1908e-04],
        [-1.2740e-05, -8.3372e-06,  6.0722e-06,  ..., -1.1384e-05,
         -7.2196e-06, -8.8960e-06],
        [-1.6734e-05, -1.1027e-05,  7.9796e-06,  ..., -1.5035e-05,
         -9.1493e-06, -1.1608e-05],
        [ 4.5825e-04,  2.4426e-04, -2.4160e-04,  ...,  3.8413e-04,
          2.9854e-04,  2.6443e-04],
        [-2.9445e-05, -1.9282e-05,  1.4022e-05,  ..., -2.6375e-05,
         -1.6227e-05, -2.0385e-05]], device='cuda:0')
Loss: 1.096518874168396


Running epoch 0, step 557, batch 557
Sampled inputs[:2]: tensor([[   0,  221,  474,  ...,  266, 2025,  287],
        [   0,  461, 4182,  ..., 7461,  292, 4895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5204e-05, -4.0566e-05, -2.5854e-04,  ...,  3.5849e-04,
         -2.1358e-04, -1.7649e-04],
        [-1.5274e-05, -9.9987e-06,  7.2643e-06,  ..., -1.3709e-05,
         -8.7321e-06, -1.0714e-05],
        [-2.0042e-05, -1.3188e-05,  9.5293e-06,  ..., -1.8075e-05,
         -1.1057e-05, -1.3962e-05],
        [ 4.5514e-04,  2.4223e-04, -2.4014e-04,  ...,  3.8127e-04,
          2.9677e-04,  2.6221e-04],
        [-3.5256e-05, -2.3067e-05,  1.6749e-05,  ..., -3.1710e-05,
         -1.9595e-05, -2.4498e-05]], device='cuda:0')
Loss: 1.0874238014221191


Running epoch 0, step 558, batch 558
Sampled inputs[:2]: tensor([[    0,   287, 21212,  ...,  3123,   944,   278],
        [    0,  1197,  3025,  ...,    14,   747,  3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0984e-05, -1.3160e-04, -2.2484e-04,  ...,  3.3646e-04,
         -2.1243e-04, -1.5134e-04],
        [-1.7777e-05, -1.1638e-05,  8.4117e-06,  ..., -1.5914e-05,
         -9.9912e-06, -1.2390e-05],
        [-2.3410e-05, -1.5408e-05,  1.1072e-05,  ..., -2.1040e-05,
         -1.2688e-05, -1.6212e-05],
        [ 4.5196e-04,  2.4014e-04, -2.3869e-04,  ...,  3.7847e-04,
          2.9528e-04,  2.6010e-04],
        [-4.1217e-05, -2.6971e-05,  1.9476e-05,  ..., -3.6955e-05,
         -2.2516e-05, -2.8461e-05]], device='cuda:0')
Loss: 1.128064513206482


Running epoch 0, step 559, batch 559
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,   474, 38333,    87],
        [    0,   298, 11712,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2872e-06, -2.7263e-07, -3.3048e-04,  ...,  4.4289e-04,
         -3.0579e-04, -1.6758e-04],
        [-2.0400e-05, -1.3292e-05,  9.6485e-06,  ..., -1.8299e-05,
         -1.1742e-05, -1.4417e-05],
        [-2.6733e-05, -1.7509e-05,  1.2644e-05,  ..., -2.4065e-05,
         -1.4849e-05, -1.8761e-05],
        [ 4.4895e-04,  2.3824e-04, -2.3726e-04,  ...,  3.7573e-04,
          2.9337e-04,  2.5781e-04],
        [-4.7088e-05, -3.0667e-05,  2.2247e-05,  ..., -4.2260e-05,
         -2.6330e-05, -3.2961e-05]], device='cuda:0')
Loss: 1.0685489177703857
Graident accumulation at epoch 0, step 559, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0163,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6071e-05,  6.8518e-05, -1.6591e-04,  ...,  8.2775e-05,
         -1.2392e-04,  1.4351e-06],
        [-2.0963e-05, -1.4959e-05,  8.8024e-06,  ..., -1.8824e-05,
         -8.6619e-06, -1.3564e-05],
        [ 2.6725e-05,  2.0880e-05, -1.0570e-05,  ...,  2.2577e-05,
          1.4320e-05,  1.8704e-05],
        [ 3.9042e-05,  2.0279e-05, -2.1049e-05,  ...,  3.2103e-05,
          2.6006e-05,  2.2181e-05],
        [-3.6291e-05, -2.4599e-05,  1.2244e-05,  ..., -3.0912e-05,
         -1.1951e-05, -2.1738e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1139e-08, 2.7625e-08, 3.4944e-08,  ..., 1.1612e-08, 8.5852e-08,
         1.1317e-08],
        [5.8729e-11, 3.2420e-11, 4.8423e-12,  ..., 4.0820e-11, 3.4648e-12,
         1.1916e-11],
        [8.4631e-10, 5.0186e-10, 6.2568e-11,  ..., 7.1041e-10, 8.7040e-11,
         2.6934e-10],
        [6.0615e-10, 3.0169e-10, 9.1626e-11,  ..., 3.9946e-10, 1.1245e-10,
         1.6771e-10],
        [2.5929e-10, 1.4186e-10, 2.0095e-11,  ..., 1.8875e-10, 1.6129e-11,
         5.7150e-11]], device='cuda:0')
optimizer state dict: 70.0
lr: [1.7191947575507777e-05, 1.7191947575507777e-05]
scheduler_last_epoch: 70


Running epoch 0, step 560, batch 560
Sampled inputs[:2]: tensor([[    0,    15, 14761,  ...,   278,  3218,   287],
        [    0, 26473,  2117,  ...,    13,  3292,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3321e-05, -4.6523e-05, -9.2742e-05,  ...,  7.7098e-05,
          1.5285e-05,  9.2864e-06],
        [-2.5630e-06, -1.6615e-06,  1.2815e-06,  ..., -2.2948e-06,
         -1.5497e-06, -1.9372e-06],
        [-3.2783e-06, -2.1160e-06,  1.6391e-06,  ..., -2.9355e-06,
         -1.8850e-06, -2.4438e-06],
        [-3.1143e-06, -2.0117e-06,  1.5572e-06,  ..., -2.8014e-06,
         -1.7583e-06, -2.3246e-06],
        [-5.7220e-06, -3.6955e-06,  2.8759e-06,  ..., -5.1260e-06,
         -3.3230e-06, -4.2617e-06]], device='cuda:0')
Loss: 1.0831445455551147


Running epoch 0, step 561, batch 561
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0,   12, 9248,  ..., 2673, 4239,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0269e-06, -1.1891e-04, -5.1121e-05,  ...,  1.0790e-04,
         -1.2899e-05,  4.6213e-05],
        [-5.1409e-06, -3.3602e-06,  2.5481e-06,  ..., -4.5598e-06,
         -2.9430e-06, -3.7700e-06],
        [ 4.4537e-04,  2.5465e-04, -2.3589e-04,  ...,  3.6482e-04,
          2.1841e-04,  2.6993e-04],
        [-6.3479e-06, -4.1425e-06,  3.1367e-06,  ..., -5.6475e-06,
         -3.4124e-06, -4.6045e-06],
        [-1.1802e-05, -7.6890e-06,  5.8562e-06,  ..., -1.0461e-05,
         -6.5118e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.11159086227417


Running epoch 0, step 562, batch 562
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,   271,   768,  ..., 15555,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7457e-05, -1.9245e-04, -8.3685e-05,  ...,  6.1273e-05,
         -4.3810e-05,  9.2750e-05],
        [-7.6443e-06, -5.0142e-06,  3.7402e-06,  ..., -6.7651e-06,
         -4.0978e-06, -5.4762e-06],
        [ 5.4444e-04,  3.1448e-04, -2.6239e-04,  ...,  4.3249e-04,
          2.4986e-04,  3.2307e-04],
        [-9.6560e-06, -6.3330e-06,  4.7088e-06,  ..., -8.5682e-06,
         -4.8280e-06, -6.8396e-06],
        [-1.7792e-05, -1.1623e-05,  8.7023e-06,  ..., -1.5706e-05,
         -9.1791e-06, -1.2606e-05]], device='cuda:0')
Loss: 1.0742192268371582


Running epoch 0, step 563, batch 563
Sampled inputs[:2]: tensor([[    0,  7638,   720,  ...,  3059, 10777,   292],
        [    0,  6192,   266,  ...,  3318,  9872, 10931]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4897e-05, -2.0497e-04, -1.3668e-04,  ...,  1.1995e-04,
         -9.8144e-05, -4.5717e-05],
        [-1.0267e-05, -6.7204e-06,  4.9919e-06,  ..., -9.0748e-06,
         -5.4687e-06, -7.3090e-06],
        [ 5.4102e-04,  3.1224e-04, -2.6075e-04,  ...,  4.2947e-04,
          2.4814e-04,  3.2068e-04],
        [-1.2919e-05, -8.4490e-06,  6.2659e-06,  ..., -1.1444e-05,
         -6.4448e-06, -9.1046e-06],
        [-2.3812e-05, -1.5527e-05,  1.1578e-05,  ..., -2.1011e-05,
         -1.2219e-05, -1.6779e-05]], device='cuda:0')
Loss: 1.1088069677352905


Running epoch 0, step 564, batch 564
Sampled inputs[:2]: tensor([[    0,  2734,  2338,  ...,  3977,   970, 10537],
        [    0,   367,  1236,  ...,   344,   292,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0418e-05, -2.0051e-04, -1.3644e-04,  ...,  9.4960e-05,
         -1.0792e-04, -3.3792e-05],
        [-1.2800e-05, -8.3894e-06,  6.2361e-06,  ..., -1.1355e-05,
         -6.7651e-06, -9.0823e-06],
        [ 6.2589e-04,  3.8152e-04, -2.9123e-04,  ...,  5.1599e-04,
          2.6575e-04,  3.8892e-04],
        [-1.6138e-05, -1.0580e-05,  7.8455e-06,  ..., -1.4335e-05,
         -8.0094e-06, -1.1340e-05],
        [-2.9713e-05, -1.9401e-05,  1.4469e-05,  ..., -2.6315e-05,
         -1.5154e-05, -2.0862e-05]], device='cuda:0')
Loss: 1.0998847484588623


Running epoch 0, step 565, batch 565
Sampled inputs[:2]: tensor([[   0, 1732,  292,  ..., 3440, 4010, 1487],
        [   0,  300, 1635,  ...,  437,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0371e-05, -2.1832e-04, -2.2280e-04,  ...,  1.0068e-04,
         -1.0275e-04, -9.1460e-05],
        [-1.5348e-05, -1.0006e-05,  7.5027e-06,  ..., -1.3664e-05,
         -8.2329e-06, -1.1019e-05],
        [ 6.2259e-04,  3.7941e-04, -2.8959e-04,  ...,  5.1299e-04,
          2.6393e-04,  3.8645e-04],
        [-1.9297e-05, -1.2591e-05,  9.4101e-06,  ..., -1.7196e-05,
         -9.7230e-06, -1.3709e-05],
        [-3.5524e-05, -2.3097e-05,  1.7375e-05,  ..., -3.1590e-05,
         -1.8388e-05, -2.5213e-05]], device='cuda:0')
Loss: 1.1005431413650513


Running epoch 0, step 566, batch 566
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   586,  1944,   271],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6798e-05, -2.9170e-04, -2.5301e-04,  ...,  9.3243e-05,
         -1.0140e-04, -1.5926e-04],
        [-1.7703e-05, -1.1601e-05,  8.6501e-06,  ..., -1.5780e-05,
         -9.2536e-06, -1.2577e-05],
        [ 6.1926e-04,  3.7716e-04, -2.8797e-04,  ...,  5.1001e-04,
          2.6257e-04,  3.8427e-04],
        [-2.2545e-05, -1.4797e-05,  1.0990e-05,  ..., -2.0117e-05,
         -1.1019e-05, -1.5825e-05],
        [-4.1336e-05, -2.7031e-05,  2.0206e-05,  ..., -3.6806e-05,
         -2.0802e-05, -2.9013e-05]], device='cuda:0')
Loss: 1.0845733880996704


Running epoch 0, step 567, batch 567
Sampled inputs[:2]: tensor([[    0,  7066,  2737,  ...,  2269,   271,   927],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1157e-05, -3.2793e-04, -2.6547e-04,  ...,  1.1788e-04,
         -1.7377e-04, -1.8583e-04],
        [-2.0251e-05, -1.3269e-05,  9.9018e-06,  ..., -1.8030e-05,
         -1.0669e-05, -1.4417e-05],
        [ 6.1593e-04,  3.7497e-04, -2.8633e-04,  ...,  5.0706e-04,
          2.6079e-04,  3.8187e-04],
        [-2.5734e-05, -1.6883e-05,  1.2554e-05,  ..., -2.2933e-05,
         -1.2696e-05, -1.8120e-05],
        [-4.7207e-05, -3.0875e-05,  2.3097e-05,  ..., -4.1991e-05,
         -2.3961e-05, -3.3215e-05]], device='cuda:0')
Loss: 1.1100586652755737
Graident accumulation at epoch 0, step 567, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0146,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8579e-05,  2.8873e-05, -1.7586e-04,  ...,  8.6286e-05,
         -1.2891e-04, -1.7292e-05],
        [-2.0892e-05, -1.4790e-05,  8.9124e-06,  ..., -1.8745e-05,
         -8.8626e-06, -1.3649e-05],
        [ 8.5645e-05,  5.6289e-05, -3.8146e-05,  ...,  7.1025e-05,
          3.8967e-05,  5.5020e-05],
        [ 3.2564e-05,  1.6563e-05, -1.7689e-05,  ...,  2.6599e-05,
          2.2136e-05,  1.8151e-05],
        [-3.7383e-05, -2.5227e-05,  1.3329e-05,  ..., -3.2020e-05,
         -1.3152e-05, -2.2885e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1100e-08, 2.7705e-08, 3.4979e-08,  ..., 1.1614e-08, 8.5796e-08,
         1.1341e-08],
        [5.9081e-11, 3.2563e-11, 4.9355e-12,  ..., 4.1104e-11, 3.5752e-12,
         1.2112e-11],
        [1.2248e-09, 6.4196e-10, 1.4449e-10,  ..., 9.6681e-10, 1.5496e-10,
         4.1490e-10],
        [6.0621e-10, 3.0167e-10, 9.1692e-11,  ..., 3.9959e-10, 1.1249e-10,
         1.6787e-10],
        [2.6126e-10, 1.4267e-10, 2.0608e-11,  ..., 1.9032e-10, 1.6687e-11,
         5.8196e-11]], device='cuda:0')
optimizer state dict: 71.0
lr: [1.710551038758326e-05, 1.710551038758326e-05]
scheduler_last_epoch: 71


Running epoch 0, step 568, batch 568
Sampled inputs[:2]: tensor([[    0,  2328,   271,  ...,   706,    13,  8961],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4279e-05, -2.3542e-05, -3.6892e-05,  ..., -4.9671e-05,
          1.0085e-04,  2.5691e-05],
        [-2.5481e-06, -1.6838e-06,  1.2591e-06,  ..., -2.2054e-06,
         -1.3188e-06, -1.8105e-06],
        [-3.5018e-06, -2.3097e-06,  1.7285e-06,  ..., -3.0398e-06,
         -1.7360e-06, -2.4736e-06],
        [-3.2783e-06, -2.1607e-06,  1.6168e-06,  ..., -2.8461e-06,
         -1.5795e-06, -2.3097e-06],
        [-6.0201e-06, -3.9637e-06,  2.9802e-06,  ..., -5.2154e-06,
         -3.0100e-06, -4.2319e-06]], device='cuda:0')
Loss: 1.1048365831375122


Running epoch 0, step 569, batch 569
Sampled inputs[:2]: tensor([[    0,   596,   292,  ...,    13,  6673,   298],
        [    0,   273,   298,  ..., 23554,    12,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0805e-04,  6.2173e-05, -1.0362e-04,  ...,  2.2740e-05,
          1.2771e-04,  8.6863e-05],
        [-5.2154e-06, -3.3826e-06,  2.5481e-06,  ..., -4.5002e-06,
         -2.8610e-06, -3.7774e-06],
        [-7.0482e-06, -4.5747e-06,  3.4422e-06,  ..., -6.0946e-06,
         -3.6880e-06, -5.0515e-06],
        [-6.5267e-06, -4.2319e-06,  3.1814e-06,  ..., -5.6475e-06,
         -3.3230e-06, -4.6641e-06],
        [-1.2189e-05, -7.8976e-06,  5.9754e-06,  ..., -1.0520e-05,
         -6.4373e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.085623860359192


Running epoch 0, step 570, batch 570
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   278,   638,  ...,   278,   266,  9387]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8979e-05,  8.8141e-05, -1.2863e-04,  ...,  2.8865e-05,
          1.5266e-04,  7.0970e-05],
        [-7.6145e-06, -4.9546e-06,  3.6880e-06,  ..., -6.6310e-06,
         -3.8743e-06, -5.4389e-06],
        [-1.0431e-05, -6.7949e-06,  5.0515e-06,  ..., -9.1046e-06,
         -5.0291e-06, -7.3761e-06],
        [-9.8497e-06, -6.4075e-06,  4.7609e-06,  ..., -8.6129e-06,
         -4.6045e-06, -6.9439e-06],
        [-1.8090e-05, -1.1742e-05,  8.7917e-06,  ..., -1.5765e-05,
         -8.8066e-06, -1.2755e-05]], device='cuda:0')
Loss: 1.072270154953003


Running epoch 0, step 571, batch 571
Sampled inputs[:2]: tensor([[    0,   221,   451,  ...,   741, 25712,   950],
        [    0,    45,    17,  ...,   278,  4112,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2647e-04,  2.9397e-04, -2.5113e-04,  ...,  1.8739e-04,
         -4.9440e-05,  2.9782e-05],
        [-1.0118e-05, -6.5863e-06,  4.9248e-06,  ..., -8.9407e-06,
         -5.3868e-06, -7.4357e-06],
        [-1.3649e-05, -8.8960e-06,  6.6683e-06,  ..., -1.2070e-05,
         -6.8545e-06, -9.9093e-06],
        [-1.2979e-05, -8.4341e-06,  6.3255e-06,  ..., -1.1489e-05,
         -6.3404e-06, -9.4026e-06],
        [-2.3574e-05, -1.5318e-05,  1.1548e-05,  ..., -2.0802e-05,
         -1.1906e-05, -1.7047e-05]], device='cuda:0')
Loss: 1.071341633796692


Running epoch 0, step 572, batch 572
Sampled inputs[:2]: tensor([[    0,   344,   259,  ...,  6787, 10045,  9799],
        [    0,   221,   474,  ..., 19245,   565,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5195e-04,  3.4881e-04, -3.9312e-04,  ...,  2.4275e-04,
         -1.2706e-04, -4.7082e-05],
        [-1.2651e-05, -8.1882e-06,  6.1989e-06,  ..., -1.1250e-05,
         -6.8247e-06, -9.3132e-06],
        [-1.7032e-05, -1.1027e-05,  8.3745e-06,  ..., -1.5140e-05,
         -8.6799e-06, -1.2383e-05],
        [-1.6198e-05, -1.0461e-05,  7.9423e-06,  ..., -1.4424e-05,
         -8.0466e-06, -1.1757e-05],
        [-2.9325e-05, -1.8939e-05,  1.4454e-05,  ..., -2.6017e-05,
         -1.5035e-05, -2.1249e-05]], device='cuda:0')
Loss: 1.1059890985488892


Running epoch 0, step 573, batch 573
Sampled inputs[:2]: tensor([[    0,    13,  2549,  ...,   221,   382,   298],
        [    0, 11752,   280,  ..., 14814,  1128,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2321e-04,  4.7347e-04, -3.6651e-04,  ...,  2.1053e-04,
         -1.0314e-04, -2.4959e-05],
        [-1.5140e-05, -9.7901e-06,  7.4282e-06,  ..., -1.3471e-05,
         -8.1509e-06, -1.1154e-05],
        [-2.0355e-05, -1.3158e-05,  1.0021e-05,  ..., -1.8090e-05,
         -1.0364e-05, -1.4812e-05],
        [-1.9386e-05, -1.2502e-05,  9.5293e-06,  ..., -1.7270e-05,
         -9.6411e-06, -1.4096e-05],
        [-3.5048e-05, -2.2590e-05,  1.7300e-05,  ..., -3.1084e-05,
         -1.7941e-05, -2.5421e-05]], device='cuda:0')
Loss: 1.076064944267273


Running epoch 0, step 574, batch 574
Sampled inputs[:2]: tensor([[   0, 5041,   14,  ..., 1027, 1722, 6554],
        [   0,  659,  278,  ...,  593, 2177,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8537e-04,  4.0816e-04, -4.3263e-04,  ...,  2.1978e-04,
         -1.0314e-04, -2.3415e-05],
        [-1.7613e-05, -1.1422e-05,  8.6725e-06,  ..., -1.5646e-05,
         -9.3207e-06, -1.2912e-05],
        [-2.3752e-05, -1.5393e-05,  1.1735e-05,  ..., -2.1070e-05,
         -1.1884e-05, -1.7196e-05],
        [-2.2739e-05, -1.4707e-05,  1.1213e-05,  ..., -2.0206e-05,
         -1.1101e-05, -1.6451e-05],
        [-4.0948e-05, -2.6464e-05,  2.0266e-05,  ..., -3.6210e-05,
         -2.0593e-05, -2.9534e-05]], device='cuda:0')
Loss: 1.0909342765808105


Running epoch 0, step 575, batch 575
Sampled inputs[:2]: tensor([[    0,  4868,  1027,  ...,   409,  3047,  2953],
        [    0,    13, 26335,  ...,     5,  2570, 34403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3058e-04,  4.0947e-04, -3.3503e-04,  ...,  1.2839e-04,
          8.9126e-05,  1.1238e-05],
        [-2.0131e-05, -1.3083e-05,  9.9093e-06,  ..., -1.7822e-05,
         -1.0461e-05, -1.4603e-05],
        [-2.7239e-05, -1.7688e-05,  1.3441e-05,  ..., -2.4095e-05,
         -1.3381e-05, -1.9521e-05],
        [-2.6047e-05, -1.6883e-05,  1.2830e-05,  ..., -2.3082e-05,
         -1.2487e-05, -1.8656e-05],
        [-4.7058e-05, -3.0488e-05,  2.3261e-05,  ..., -4.1485e-05,
         -2.3246e-05, -3.3587e-05]], device='cuda:0')
Loss: 1.1143959760665894
Graident accumulation at epoch 0, step 575, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.6337e-05,  6.6932e-05, -1.9178e-04,  ...,  9.0496e-05,
         -1.0710e-04, -1.4439e-05],
        [-2.0816e-05, -1.4620e-05,  9.0121e-06,  ..., -1.8652e-05,
         -9.0224e-06, -1.3745e-05],
        [ 7.4356e-05,  4.8891e-05, -3.2987e-05,  ...,  6.1513e-05,
          3.3732e-05,  4.7566e-05],
        [ 2.6703e-05,  1.3218e-05, -1.4637e-05,  ...,  2.1631e-05,
          1.8674e-05,  1.4470e-05],
        [-3.8350e-05, -2.5753e-05,  1.4322e-05,  ..., -3.2966e-05,
         -1.4161e-05, -2.3955e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1244e-08, 2.7844e-08, 3.5057e-08,  ..., 1.1619e-08, 8.5718e-08,
         1.1329e-08],
        [5.9427e-11, 3.2702e-11, 5.0287e-12,  ..., 4.1381e-11, 3.6811e-12,
         1.2313e-11],
        [1.2243e-09, 6.4163e-10, 1.4453e-10,  ..., 9.6642e-10, 1.5499e-10,
         4.1487e-10],
        [6.0628e-10, 3.0165e-10, 9.1765e-11,  ..., 3.9972e-10, 1.1254e-10,
         1.6805e-10],
        [2.6321e-10, 1.4346e-10, 2.1128e-11,  ..., 1.9185e-10, 1.7211e-11,
         5.9265e-11]], device='cuda:0')
optimizer state dict: 72.0
lr: [1.7017987415646643e-05, 1.7017987415646643e-05]
scheduler_last_epoch: 72


Running epoch 0, step 576, batch 576
Sampled inputs[:2]: tensor([[    0,   352,  2284,  ..., 43204,    12,   709],
        [    0,   287,   955,  ...,   462,  3363,  1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4693e-05, -1.5313e-05, -6.3168e-05,  ..., -3.8870e-06,
          1.2665e-05,  1.2250e-05],
        [-2.4140e-06, -1.5721e-06,  1.2442e-06,  ..., -2.1309e-06,
         -1.0729e-06, -1.6838e-06],
        [-3.4422e-06, -2.2501e-06,  1.7881e-06,  ..., -3.0547e-06,
         -1.4380e-06, -2.3842e-06],
        [-3.3081e-06, -2.1607e-06,  1.7211e-06,  ..., -2.9504e-06,
         -1.3411e-06, -2.2948e-06],
        [-5.9009e-06, -3.8445e-06,  3.0696e-06,  ..., -5.2154e-06,
         -2.4885e-06, -4.0829e-06]], device='cuda:0')
Loss: 1.0956261157989502


Running epoch 0, step 577, batch 577
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  9472, 15004,   511],
        [    0,  1603,    12,  ...,    12,   756,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7346e-05,  1.4566e-04, -8.8180e-05,  ...,  3.2664e-05,
         -7.8349e-06, -3.7451e-05],
        [-5.0515e-06, -3.1590e-06,  2.5332e-06,  ..., -4.3511e-06,
         -2.4438e-06, -3.5316e-06],
        [-7.0781e-06, -4.4405e-06,  3.5688e-06,  ..., -6.1095e-06,
         -3.2261e-06, -4.9025e-06],
        [-6.6012e-06, -4.1425e-06,  3.3230e-06,  ..., -5.7369e-06,
         -2.9132e-06, -4.5747e-06],
        [-1.2219e-05, -7.6443e-06,  6.1691e-06,  ..., -1.0550e-05,
         -5.6177e-06, -8.4639e-06]], device='cuda:0')
Loss: 1.0890227556228638


Running epoch 0, step 578, batch 578
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  367,  300,  369],
        [   0, 1806,  319,  ..., 3427,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0746e-05,  1.0396e-04, -9.8888e-05,  ...,  3.9264e-05,
          2.4777e-05, -1.2385e-04],
        [-7.4208e-06, -4.7013e-06,  3.7104e-06,  ..., -6.4671e-06,
         -3.5092e-06, -5.1782e-06],
        [-1.0490e-05, -6.6757e-06,  5.2750e-06,  ..., -9.1642e-06,
         -4.6566e-06, -7.2569e-06],
        [-9.8795e-06, -6.2734e-06,  4.9546e-06,  ..., -8.6725e-06,
         -4.2468e-06, -6.8247e-06],
        [-1.8001e-05, -1.1399e-05,  9.0599e-06,  ..., -1.5706e-05,
         -8.0764e-06, -1.2457e-05]], device='cuda:0')
Loss: 1.0862478017807007


Running epoch 0, step 579, batch 579
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,    76,    15,  ...,    14,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9533e-05,  7.6546e-05, -1.8341e-05,  ..., -1.0510e-05,
          5.9890e-05, -4.5560e-05],
        [-9.8646e-06, -6.2808e-06,  4.9025e-06,  ..., -8.6278e-06,
         -4.5672e-06, -6.8620e-06],
        [-1.3918e-05, -8.9109e-06,  6.9514e-06,  ..., -1.2204e-05,
         -6.0573e-06, -9.6112e-06],
        [-1.3202e-05, -8.4341e-06,  6.5714e-06,  ..., -1.1623e-05,
         -5.5730e-06, -9.1046e-06],
        [-2.3961e-05, -1.5274e-05,  1.1981e-05,  ..., -2.0981e-05,
         -1.0550e-05, -1.6540e-05]], device='cuda:0')
Loss: 1.1103971004486084


Running epoch 0, step 580, batch 580
Sampled inputs[:2]: tensor([[   0,   14,  298,  ...,  333,  199,  769],
        [   0, 4263, 4865,  ..., 1878,  278, 4450]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6143e-04,  2.2519e-04, -8.1892e-05,  ...,  3.4562e-05,
          9.5509e-05, -2.5262e-05],
        [-1.2502e-05, -7.8827e-06,  6.1765e-06,  ..., -1.0982e-05,
         -6.2808e-06, -8.9481e-06],
        [-1.7270e-05, -1.0952e-05,  8.5905e-06,  ..., -1.5184e-05,
         -8.0988e-06, -1.2219e-05],
        [-1.6302e-05, -1.0304e-05,  8.0764e-06,  ..., -1.4380e-05,
         -7.4431e-06, -1.1519e-05],
        [-2.9743e-05, -1.8775e-05,  1.4812e-05,  ..., -2.6107e-05,
         -1.4082e-05, -2.1011e-05]], device='cuda:0')
Loss: 1.0714716911315918


Running epoch 0, step 581, batch 581
Sampled inputs[:2]: tensor([[    0,   342,   721,  ...,  2429,    14,   475],
        [    0,   650,    14,  ...,  3687,   278, 26952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6770e-04,  2.1118e-04, -1.1481e-04,  ...,  1.1572e-05,
          6.7645e-05, -3.4462e-05],
        [-1.4961e-05, -9.5144e-06,  7.3612e-06,  ..., -1.3143e-05,
         -7.4580e-06, -1.0721e-05],
        [-2.0683e-05, -1.3232e-05,  1.0252e-05,  ..., -1.8194e-05,
         -9.6262e-06, -1.4663e-05],
        [-1.9580e-05, -1.2495e-05,  9.6709e-06,  ..., -1.7270e-05,
         -8.8438e-06, -1.3858e-05],
        [-3.5495e-05, -2.2590e-05,  1.7613e-05,  ..., -3.1143e-05,
         -1.6689e-05, -2.5094e-05]], device='cuda:0')
Loss: 1.0607892274856567


Running epoch 0, step 582, batch 582
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   287,  2888,  4845],
        [    0,   560,   199,  ...,   292, 12605,  2096]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0702e-04,  2.6256e-04, -1.5063e-04,  ..., -3.1098e-06,
          1.5477e-04, -3.5297e-05],
        [-1.7449e-05, -1.1124e-05,  8.5682e-06,  ..., -1.5259e-05,
         -8.6278e-06, -1.2442e-05],
        [-2.4229e-05, -1.5527e-05,  1.1973e-05,  ..., -2.1219e-05,
         -1.1191e-05, -1.7092e-05],
        [-2.2963e-05, -1.4685e-05,  1.1303e-05,  ..., -2.0161e-05,
         -1.0282e-05, -1.6153e-05],
        [-4.1634e-05, -2.6554e-05,  2.0579e-05,  ..., -3.6359e-05,
         -1.9431e-05, -2.9266e-05]], device='cuda:0')
Loss: 1.0716768503189087


Running epoch 0, step 583, batch 583
Sampled inputs[:2]: tensor([[   0,  446, 1115,  ..., 1869, 4971, 1954],
        [   0, 5116, 4330,  ...,  925,  699, 1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0702e-04,  3.0188e-04, -1.2465e-04,  ..., -5.4593e-05,
          1.8960e-04,  1.4685e-06],
        [-1.9863e-05, -1.2688e-05,  9.7603e-06,  ..., -1.7375e-05,
         -9.7007e-06, -1.4119e-05],
        [-2.7686e-05, -1.7777e-05,  1.3687e-05,  ..., -2.4259e-05,
         -1.2621e-05, -1.9476e-05],
        [-2.6241e-05, -1.6816e-05,  1.2927e-05,  ..., -2.3037e-05,
         -1.1593e-05, -1.8403e-05],
        [-4.7654e-05, -3.0458e-05,  2.3559e-05,  ..., -4.1634e-05,
         -2.1979e-05, -3.3408e-05]], device='cuda:0')
Loss: 1.0835275650024414
Graident accumulation at epoch 0, step 583, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0027,  0.0228, -0.0198],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.4405e-05,  9.0427e-05, -1.8507e-04,  ...,  7.5987e-05,
         -7.7434e-05, -1.2848e-05],
        [-2.0720e-05, -1.4427e-05,  9.0869e-06,  ..., -1.8525e-05,
         -9.0902e-06, -1.3782e-05],
        [ 6.4152e-05,  4.2224e-05, -2.8320e-05,  ...,  5.2936e-05,
          2.9097e-05,  4.0862e-05],
        [ 2.1409e-05,  1.0215e-05, -1.1880e-05,  ...,  1.7164e-05,
          1.5647e-05,  1.1183e-05],
        [-3.9280e-05, -2.6223e-05,  1.5246e-05,  ..., -3.3833e-05,
         -1.4943e-05, -2.4901e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1297e-08, 2.7908e-08, 3.5037e-08,  ..., 1.1610e-08, 8.5668e-08,
         1.1318e-08],
        [5.9762e-11, 3.2830e-11, 5.1190e-12,  ..., 4.1641e-11, 3.7715e-12,
         1.2500e-11],
        [1.2239e-09, 6.4131e-10, 1.4457e-10,  ..., 9.6604e-10, 1.5499e-10,
         4.1483e-10],
        [6.0636e-10, 3.0164e-10, 9.1840e-11,  ..., 3.9985e-10, 1.1256e-10,
         1.6822e-10],
        [2.6522e-10, 1.4424e-10, 2.1662e-11,  ..., 1.9340e-10, 1.7677e-11,
         6.0322e-11]], device='cuda:0')
optimizer state dict: 73.0
lr: [1.6929392033972038e-05, 1.6929392033972038e-05]
scheduler_last_epoch: 73


Running epoch 0, step 584, batch 584
Sampled inputs[:2]: tensor([[   0,  908,   14,  ...,   19,   27,  287],
        [   0, 7303,   12,  ..., 1085,  413,  711]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7073e-05,  1.0862e-05,  7.9855e-06,  ..., -3.0349e-05,
          5.9564e-06, -2.1484e-06],
        [-2.5034e-06, -1.5348e-06,  1.1772e-06,  ..., -2.1607e-06,
         -1.1623e-06, -1.7509e-06],
        [-3.5614e-06, -2.1905e-06,  1.6764e-06,  ..., -3.0696e-06,
         -1.5348e-06, -2.4587e-06],
        [-3.3677e-06, -2.0564e-06,  1.5795e-06,  ..., -2.9057e-06,
         -1.4231e-06, -2.3246e-06],
        [-6.0797e-06, -3.7104e-06,  2.8610e-06,  ..., -5.2154e-06,
         -2.6524e-06, -4.1723e-06]], device='cuda:0')
Loss: 1.0909520387649536


Running epoch 0, step 585, batch 585
Sampled inputs[:2]: tensor([[    0,     9,   287,  ...,   369,  2968,  8347],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7753e-04,  1.3654e-04,  3.6624e-05,  ...,  2.6841e-05,
          5.4625e-05,  1.7946e-04],
        [-5.0962e-06, -3.1218e-06,  2.3693e-06,  ..., -4.3660e-06,
         -2.3916e-06, -3.5614e-06],
        [-7.2867e-06, -4.4703e-06,  3.3900e-06,  ..., -6.2436e-06,
         -3.2187e-06, -5.0515e-06],
        [-6.7949e-06, -4.1574e-06,  3.1516e-06,  ..., -5.8264e-06,
         -2.9206e-06, -4.6939e-06],
        [-1.2428e-05, -7.5847e-06,  5.7667e-06,  ..., -1.0610e-05,
         -5.5432e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.0868219137191772


Running epoch 0, step 586, batch 586
Sampled inputs[:2]: tensor([[    0,   298, 22296,  ...,   287,  6494,   644],
        [    0,    14,   221,  ...,   298,   408,  1849]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5923e-04,  2.2595e-04,  1.2636e-04,  ..., -2.1586e-05,
          2.3213e-04,  3.1515e-04],
        [-7.6592e-06, -4.7237e-06,  3.5688e-06,  ..., -6.6459e-06,
         -3.8520e-06, -5.4464e-06],
        [-1.0803e-05, -6.6608e-06,  5.0440e-06,  ..., -9.3579e-06,
         -5.1111e-06, -7.6145e-06],
        [-9.9093e-06, -6.0946e-06,  4.6045e-06,  ..., -8.5682e-06,
         -4.5523e-06, -6.9588e-06],
        [-1.8477e-05, -1.1340e-05,  8.6129e-06,  ..., -1.5944e-05,
         -8.8066e-06, -1.2934e-05]], device='cuda:0')
Loss: 1.0760310888290405


Running epoch 0, step 587, batch 587
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 14308,   278,  9452],
        [    0,     9, 25368,  ...,   271,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8325e-04,  2.5599e-04,  1.6964e-04,  ...,  4.3217e-06,
          2.8378e-04,  2.7075e-04],
        [-1.0133e-05, -6.2808e-06,  4.7162e-06,  ..., -8.8364e-06,
         -4.9993e-06, -7.1675e-06],
        [-1.4305e-05, -8.8662e-06,  6.6906e-06,  ..., -1.2457e-05,
         -6.6161e-06, -1.0014e-05],
        [-1.3232e-05, -8.1807e-06,  6.1616e-06,  ..., -1.1504e-05,
         -5.9530e-06, -9.2387e-06],
        [-2.4527e-05, -1.5140e-05,  1.1459e-05,  ..., -2.1279e-05,
         -1.1444e-05, -1.7077e-05]], device='cuda:0')
Loss: 1.097718596458435


Running epoch 0, step 588, batch 588
Sampled inputs[:2]: tensor([[    0,   266, 15258,  ...,  2366,   368,  3988],
        [    0,  3594,   950,  ...,  6517,   344, 15386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4614e-04,  1.4497e-04,  2.1495e-04,  ..., -1.9219e-05,
          3.8381e-04,  2.7905e-04],
        [-1.2562e-05, -7.8008e-06,  5.8860e-06,  ..., -1.0952e-05,
         -6.0946e-06, -8.9109e-06],
        [-1.7717e-05, -1.1012e-05,  8.3447e-06,  ..., -1.5438e-05,
         -8.0466e-06, -1.2442e-05],
        [-1.6525e-05, -1.0237e-05,  7.7486e-06,  ..., -1.4380e-05,
         -7.2941e-06, -1.1578e-05],
        [-3.0428e-05, -1.8835e-05,  1.4320e-05,  ..., -2.6405e-05,
         -1.3947e-05, -2.1249e-05]], device='cuda:0')
Loss: 1.0840342044830322


Running epoch 0, step 589, batch 589
Sampled inputs[:2]: tensor([[   0,  598,  278,  ...,  437,  266, 2388],
        [   0, 1688,  790,  ...,  546,  696,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2009e-04,  1.6117e-04,  2.1007e-04,  ..., -2.7316e-05,
          4.0553e-04,  3.1878e-04],
        [-1.4961e-05, -9.3579e-06,  7.0482e-06,  ..., -1.3053e-05,
         -7.0147e-06, -1.0543e-05],
        [ 3.2332e-04,  2.7828e-04, -1.2862e-04,  ...,  2.4658e-04,
          1.8434e-04,  1.9474e-04],
        [-1.9938e-05, -1.2442e-05,  9.4026e-06,  ..., -1.7375e-05,
         -8.4862e-06, -1.3888e-05],
        [-3.6478e-05, -2.2769e-05,  1.7256e-05,  ..., -3.1710e-05,
         -1.6153e-05, -2.5332e-05]], device='cuda:0')
Loss: 1.0755418539047241


Running epoch 0, step 590, batch 590
Sampled inputs[:2]: tensor([[   0,  365, 1110,  ..., 4130,  221,  199],
        [   0, 1197,  729,  ...,  674,  369, 8222]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0922e-04,  2.1134e-04,  2.8251e-04,  ..., -5.2826e-05,
          4.6167e-04,  3.2212e-04],
        [-1.7464e-05, -1.0923e-05,  8.2701e-06,  ..., -1.5229e-05,
         -8.1025e-06, -1.2279e-05],
        [ 9.6697e-04,  6.6680e-04, -4.7260e-04,  ...,  7.6527e-04,
          4.1412e-04,  5.4359e-04],
        [-2.3380e-05, -1.4603e-05,  1.1086e-05,  ..., -2.0370e-05,
         -9.8497e-06, -1.6242e-05],
        [-4.2677e-05, -2.6643e-05,  2.0295e-05,  ..., -3.7104e-05,
         -1.8701e-05, -2.9564e-05]], device='cuda:0')
Loss: 1.0967212915420532


Running epoch 0, step 591, batch 591
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  1067,   271,  ...,   266,   940,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1420e-04,  2.1833e-04,  2.9084e-04,  ..., -9.1026e-05,
          4.8649e-04,  3.5490e-04],
        [-1.9923e-05, -1.2524e-05,  9.4324e-06,  ..., -1.7360e-05,
         -9.0860e-06, -1.3925e-05],
        [ 9.6332e-04,  6.6441e-04, -4.7086e-04,  ...,  7.6209e-04,
          4.1277e-04,  5.4116e-04],
        [-2.6822e-05, -1.6838e-05,  1.2718e-05,  ..., -2.3350e-05,
         -1.1086e-05, -1.8522e-05],
        [-4.8876e-05, -3.0667e-05,  2.3231e-05,  ..., -4.2439e-05,
         -2.1026e-05, -3.3647e-05]], device='cuda:0')
Loss: 1.1014653444290161
Graident accumulation at epoch 0, step 591, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.0385e-05,  1.0322e-04, -1.3748e-04,  ...,  5.9286e-05,
         -2.1041e-05,  2.3927e-05],
        [-2.0641e-05, -1.4236e-05,  9.1214e-06,  ..., -1.8408e-05,
         -9.0898e-06, -1.3797e-05],
        [ 1.5407e-04,  1.0444e-04, -7.2574e-05,  ...,  1.2385e-04,
          6.7465e-05,  9.0892e-05],
        [ 1.6585e-05,  7.5096e-06, -9.4205e-06,  ...,  1.3113e-05,
          1.2974e-05,  8.2123e-06],
        [-4.0240e-05, -2.6668e-05,  1.6044e-05,  ..., -3.4693e-05,
         -1.5551e-05, -2.5775e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1355e-08, 2.7928e-08, 3.5087e-08,  ..., 1.1607e-08, 8.5819e-08,
         1.1433e-08],
        [6.0099e-11, 3.2954e-11, 5.2028e-12,  ..., 4.1901e-11, 3.8503e-12,
         1.2682e-11],
        [2.1506e-09, 1.0821e-09, 3.6613e-10,  ..., 1.5459e-09, 3.2522e-10,
         7.0727e-10],
        [6.0647e-10, 3.0162e-10, 9.1910e-11,  ..., 4.0000e-10, 1.1257e-10,
         1.6840e-10],
        [2.6734e-10, 1.4504e-10, 2.2180e-11,  ..., 1.9500e-10, 1.8101e-11,
         6.1394e-11]], device='cuda:0')
optimizer state dict: 74.0
lr: [1.6839737780707125e-05, 1.6839737780707125e-05]
scheduler_last_epoch: 74


Running epoch 0, step 592, batch 592
Sampled inputs[:2]: tensor([[    0,  3231,   271,  ...,  9279,  8231, 28871],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7768e-05,  7.4716e-05, -9.4885e-05,  ..., -6.9117e-05,
         -4.3448e-05,  2.7157e-05],
        [-2.5183e-06, -1.5944e-06,  1.1623e-06,  ..., -2.2054e-06,
         -1.2442e-06, -1.8105e-06],
        [-3.6210e-06, -2.2948e-06,  1.6838e-06,  ..., -3.1739e-06,
         -1.6838e-06, -2.5779e-06],
        [-3.2485e-06, -2.0564e-06,  1.5050e-06,  ..., -2.8461e-06,
         -1.4678e-06, -2.3097e-06],
        [-6.3181e-06, -3.9935e-06,  2.9355e-06,  ..., -5.5134e-06,
         -2.9504e-06, -4.4703e-06]], device='cuda:0')
Loss: 1.094573736190796


Running epoch 0, step 593, batch 593
Sampled inputs[:2]: tensor([[   0, 5440,   13,  ..., 1878,  342, 2060],
        [   0, 1867,  300,  ...,  259, 3095, 1842]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4845e-05,  7.0000e-05, -8.0155e-05,  ..., -6.6786e-06,
         -9.8951e-05,  2.5071e-06],
        [-5.0366e-06, -3.1441e-06,  2.2724e-06,  ..., -4.3660e-06,
         -2.3171e-06, -3.5241e-06],
        [-7.3016e-06, -4.5747e-06,  3.3230e-06,  ..., -6.3330e-06,
         -3.1367e-06, -5.0664e-06],
        [-6.6310e-06, -4.1425e-06,  3.0026e-06,  ..., -5.7518e-06,
         -2.7642e-06, -4.6045e-06],
        [-1.2636e-05, -7.8678e-06,  5.7369e-06,  ..., -1.0908e-05,
         -5.4836e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.0863646268844604


Running epoch 0, step 594, batch 594
Sampled inputs[:2]: tensor([[   0, 2162,   73,  ...,  278,  266, 1059],
        [   0, 1167,  278,  ...,  278, 1853, 1424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9176e-05, -1.9208e-05, -6.4257e-05,  ..., -8.5088e-05,
         -3.1263e-05,  2.0834e-05],
        [-7.4655e-06, -4.7088e-06,  3.3751e-06,  ..., -6.4671e-06,
         -3.2000e-06, -5.0962e-06],
        [-1.1042e-05, -6.9886e-06,  5.0217e-06,  ..., -9.5963e-06,
         -4.4182e-06, -7.4655e-06],
        [-9.9987e-06, -6.3032e-06,  4.5300e-06,  ..., -8.6874e-06,
         -3.8669e-06, -6.7651e-06],
        [-1.9044e-05, -1.1981e-05,  8.6427e-06,  ..., -1.6481e-05,
         -7.7188e-06, -1.2845e-05]], device='cuda:0')
Loss: 1.109796404838562


Running epoch 0, step 595, batch 595
Sampled inputs[:2]: tensor([[   0,  328, 6875,  ...,  369,  654,  300],
        [   0, 3019,  278,  ...,  365, 1770,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6768e-05, -9.3527e-05, -7.9468e-05,  ..., -8.6422e-05,
          2.0314e-05, -5.3209e-05],
        [-9.9391e-06, -6.2808e-06,  4.5076e-06,  ..., -8.6129e-06,
         -4.2506e-06, -6.7651e-06],
        [-1.4678e-05, -9.2983e-06,  6.7055e-06,  ..., -1.2755e-05,
         -5.8413e-06, -9.9093e-06],
        [-1.3337e-05, -8.4341e-06,  6.0722e-06,  ..., -1.1593e-05,
         -5.1335e-06, -9.0152e-06],
        [-2.5272e-05, -1.5944e-05,  1.1533e-05,  ..., -2.1875e-05,
         -1.0207e-05, -1.7017e-05]], device='cuda:0')
Loss: 1.0816341638565063


Running epoch 0, step 596, batch 596
Sampled inputs[:2]: tensor([[    0,    14,  6436,  ...,   271,  1211,  8917],
        [    0,    13, 20773,  ..., 22463,  2587,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3269e-05, -1.6898e-04, -4.1277e-05,  ..., -1.5335e-04,
          1.1262e-04, -1.3800e-06],
        [-1.2547e-05, -7.8753e-06,  5.6550e-06,  ..., -1.0878e-05,
         -5.4799e-06, -8.5607e-06],
        [-1.8433e-05, -1.1608e-05,  8.3745e-06,  ..., -1.6019e-05,
         -7.5176e-06, -1.2487e-05],
        [-1.6689e-05, -1.0490e-05,  7.5549e-06,  ..., -1.4514e-05,
         -6.5863e-06, -1.1310e-05],
        [-3.1710e-05, -1.9878e-05,  1.4380e-05,  ..., -2.7448e-05,
         -1.3098e-05, -2.1398e-05]], device='cuda:0')
Loss: 1.0959453582763672


Running epoch 0, step 597, batch 597
Sampled inputs[:2]: tensor([[   0,  790, 2816,  ...,   14, 1062,  668],
        [   0, 5332,  391,  ...,  221,  334, 1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2301e-04, -1.3843e-04, -8.5625e-05,  ..., -1.4852e-04,
          6.2771e-05, -2.7433e-05],
        [-1.5125e-05, -9.4473e-06,  6.7353e-06,  ..., -1.3143e-05,
         -6.6496e-06, -1.0289e-05],
        [-2.2143e-05, -1.3873e-05,  9.9465e-06,  ..., -1.9267e-05,
         -9.0823e-06, -1.4946e-05],
        [-2.0057e-05, -1.2547e-05,  8.9779e-06,  ..., -1.7464e-05,
         -7.9721e-06, -1.3545e-05],
        [-3.8058e-05, -2.3752e-05,  1.7062e-05,  ..., -3.2991e-05,
         -1.5810e-05, -2.5570e-05]], device='cuda:0')
Loss: 1.0959622859954834


Running epoch 0, step 598, batch 598
Sampled inputs[:2]: tensor([[   0,   19,    9,  ..., 4971,  367, 1675],
        [   0,  278,  668,  ..., 2743,  638,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7921e-04, -1.4700e-04, -1.7226e-04,  ..., -1.3414e-04,
         -2.2340e-05, -3.7451e-05],
        [-1.7747e-05, -1.1064e-05,  7.9125e-06,  ..., -1.5333e-05,
         -7.6853e-06, -1.2033e-05],
        [-2.5988e-05, -1.6257e-05,  1.1683e-05,  ..., -2.2486e-05,
         -1.0505e-05, -1.7479e-05],
        [-2.3603e-05, -1.4737e-05,  1.0565e-05,  ..., -2.0429e-05,
         -9.2387e-06, -1.5885e-05],
        [-4.4584e-05, -2.7776e-05,  1.9997e-05,  ..., -3.8445e-05,
         -1.8269e-05, -2.9862e-05]], device='cuda:0')
Loss: 1.0675933361053467


Running epoch 0, step 599, batch 599
Sampled inputs[:2]: tensor([[    0,   756,   401,  ...,  8385,  1004,   775],
        [    0,   292,   474,  ...,   446, 14932,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9089e-04, -1.4272e-04, -2.1841e-04,  ..., -7.5298e-05,
         -6.1220e-05, -5.3366e-05],
        [-2.0176e-05, -1.2599e-05,  9.0525e-06,  ..., -1.7494e-05,
         -8.6911e-06, -1.3724e-05],
        [-2.9609e-05, -1.8567e-05,  1.3404e-05,  ..., -2.5705e-05,
         -1.1891e-05, -1.9982e-05],
        [-2.6926e-05, -1.6853e-05,  1.2144e-05,  ..., -2.3395e-05,
         -1.0476e-05, -1.8194e-05],
        [-5.0724e-05, -3.1680e-05,  2.2918e-05,  ..., -4.3899e-05,
         -2.0653e-05, -3.4094e-05]], device='cuda:0')
Loss: 1.0868028402328491
Graident accumulation at epoch 0, step 599, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3257e-05,  7.8623e-05, -1.4557e-04,  ...,  4.5827e-05,
         -2.5059e-05,  1.6197e-05],
        [-2.0594e-05, -1.4073e-05,  9.1145e-06,  ..., -1.8317e-05,
         -9.0499e-06, -1.3789e-05],
        [ 1.3570e-04,  9.2142e-05, -6.3976e-05,  ...,  1.0890e-04,
          5.9529e-05,  7.9805e-05],
        [ 1.2234e-05,  5.0734e-06, -7.2640e-06,  ...,  9.4620e-06,
          1.0629e-05,  5.5716e-06],
        [-4.1288e-05, -2.7169e-05,  1.6732e-05,  ..., -3.5614e-05,
         -1.6061e-05, -2.6607e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1350e-08, 2.7920e-08, 3.5099e-08,  ..., 1.1601e-08, 8.5737e-08,
         1.1424e-08],
        [6.0446e-11, 3.3080e-11, 5.2796e-12,  ..., 4.2165e-11, 3.9220e-12,
         1.2858e-11],
        [2.1494e-09, 1.0814e-09, 3.6595e-10,  ..., 1.5450e-09, 3.2504e-10,
         7.0696e-10],
        [6.0659e-10, 3.0160e-10, 9.1966e-11,  ..., 4.0014e-10, 1.1257e-10,
         1.6856e-10],
        [2.6965e-10, 1.4589e-10, 2.2683e-11,  ..., 1.9674e-10, 1.8510e-11,
         6.2495e-11]], device='cuda:0')
optimizer state dict: 75.0
lr: [1.67490383558044e-05, 1.67490383558044e-05]
scheduler_last_epoch: 75


Running epoch 0, step 600, batch 600
Sampled inputs[:2]: tensor([[    0,    17,  2736,  ...,   352,   422,    13],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5492e-05,  3.9896e-05, -3.4180e-07,  ...,  3.4575e-05,
         -5.7240e-05, -4.6514e-05],
        [-2.5779e-06, -1.5944e-06,  1.1250e-06,  ..., -2.2352e-06,
         -1.2591e-06, -1.8030e-06],
        [-3.7253e-06, -2.3097e-06,  1.6466e-06,  ..., -3.2037e-06,
         -1.6764e-06, -2.5630e-06],
        [-3.2634e-06, -2.0117e-06,  1.4305e-06,  ..., -2.8014e-06,
         -1.4454e-06, -2.2501e-06],
        [-6.5565e-06, -4.0531e-06,  2.9057e-06,  ..., -5.6326e-06,
         -2.9802e-06, -4.5002e-06]], device='cuda:0')
Loss: 1.0698950290679932


Running epoch 0, step 601, batch 601
Sampled inputs[:2]: tensor([[    0,    12,   342,  ...,  3458,   271,   266],
        [    0,   733,   560,  ...,  1172, 22808,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7862e-05,  1.6211e-04, -1.1777e-04,  ...,  1.8332e-04,
         -2.0050e-04, -5.0337e-05],
        [-5.1558e-06, -3.1069e-06,  2.2203e-06,  ..., -4.5598e-06,
         -2.7046e-06, -3.7104e-06],
        [-7.4059e-06, -4.4703e-06,  3.2336e-06,  ..., -6.4969e-06,
         -3.5986e-06, -5.2303e-06],
        [-6.4522e-06, -3.8818e-06,  2.8089e-06,  ..., -5.6624e-06,
         -3.0771e-06, -4.5747e-06],
        [-1.2934e-05, -7.7784e-06,  5.6624e-06,  ..., -1.1325e-05,
         -6.3330e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.0824949741363525


Running epoch 0, step 602, batch 602
Sampled inputs[:2]: tensor([[    0,   266, 15794,  ...,  3128,  6479,  2626],
        [    0, 32444,    41,  ...,    14,    18,    59]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0908e-04,  8.8436e-05, -9.3595e-05,  ...,  2.2559e-04,
         -1.9843e-04, -1.2010e-04],
        [-7.7486e-06, -4.7013e-06,  3.3453e-06,  ..., -6.7949e-06,
         -3.8594e-06, -5.4389e-06],
        [-1.1370e-05, -6.8992e-06,  4.9621e-06,  ..., -9.8944e-06,
         -5.2601e-06, -7.8529e-06],
        [-9.7305e-06, -5.9083e-06,  4.2394e-06,  ..., -8.4788e-06,
         -4.4107e-06, -6.7502e-06],
        [-1.9610e-05, -1.1891e-05,  8.5831e-06,  ..., -1.7047e-05,
         -9.1642e-06, -1.3500e-05]], device='cuda:0')
Loss: 1.0878995656967163


Running epoch 0, step 603, batch 603
Sampled inputs[:2]: tensor([[    0,   266,   944,  ..., 14981,  1952,   271],
        [    0,   445,    16,  ...,  7747,  5308,  6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6652e-04,  2.3762e-05, -1.4715e-04,  ...,  1.4154e-04,
         -1.5494e-04, -2.4089e-04],
        [-1.0371e-05, -6.2734e-06,  4.4480e-06,  ..., -9.0599e-06,
         -5.1633e-06, -7.2420e-06],
        [-1.5184e-05, -9.1940e-06,  6.5938e-06,  ..., -1.3173e-05,
         -7.0110e-06, -1.0431e-05],
        [-1.2979e-05, -7.8604e-06,  5.6326e-06,  ..., -1.1280e-05,
         -5.8562e-06, -8.9407e-06],
        [-2.6166e-05, -1.5825e-05,  1.1399e-05,  ..., -2.2650e-05,
         -1.2204e-05, -1.7911e-05]], device='cuda:0')
Loss: 1.0854768753051758


Running epoch 0, step 604, batch 604
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,   292, 12522,  ...,   266,  1977,  8481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4095e-04,  4.8856e-05, -1.7444e-04,  ...,  2.4848e-04,
         -2.3663e-04, -2.4327e-04],
        [-1.2964e-05, -7.9051e-06,  5.5879e-06,  ..., -1.1340e-05,
         -6.3628e-06, -9.0078e-06],
        [-1.8999e-05, -1.1608e-05,  8.2925e-06,  ..., -1.6525e-05,
         -8.6576e-06, -1.3009e-05],
        [-1.6257e-05, -9.9167e-06,  7.0781e-06,  ..., -1.4156e-05,
         -7.2271e-06, -1.1146e-05],
        [-3.2753e-05, -1.9968e-05,  1.4320e-05,  ..., -2.8431e-05,
         -1.5065e-05, -2.2322e-05]], device='cuda:0')
Loss: 1.0828101634979248


Running epoch 0, step 605, batch 605
Sampled inputs[:2]: tensor([[    0,   342,  3001,  ...,   369, 11195,   367],
        [    0,   287, 39084,  ...,   266,  1817,  1589]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0934e-04,  8.4716e-05, -1.6071e-04,  ...,  3.6803e-04,
         -3.9976e-04, -2.2209e-04],
        [-1.5393e-05, -9.4920e-06,  6.6757e-06,  ..., -1.3545e-05,
         -7.3835e-06, -1.0654e-05],
        [-2.2724e-05, -1.4037e-05,  9.9838e-06,  ..., -1.9893e-05,
         -1.0118e-05, -1.5512e-05],
        [-1.9565e-05, -1.2062e-05,  8.5756e-06,  ..., -1.7151e-05,
         -8.4862e-06, -1.3366e-05],
        [-3.9071e-05, -2.4050e-05,  1.7181e-05,  ..., -3.4124e-05,
         -1.7568e-05, -2.6524e-05]], device='cuda:0')
Loss: 1.0817241668701172


Running epoch 0, step 606, batch 606
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,  287, 1410,  ..., 1255, 1699,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7151e-04,  2.0390e-04, -4.0552e-04,  ...,  4.9275e-04,
         -6.9312e-04, -2.8827e-04],
        [-1.7941e-05, -1.1027e-05,  7.8455e-06,  ..., -1.5751e-05,
         -8.6129e-06, -1.2472e-05],
        [-2.6479e-05, -1.6317e-05,  1.1735e-05,  ..., -2.3142e-05,
         -1.1802e-05, -1.8165e-05],
        [-2.2918e-05, -1.4089e-05,  1.0133e-05,  ..., -2.0057e-05,
         -9.9465e-06, -1.5736e-05],
        [-4.5419e-05, -2.7895e-05,  2.0146e-05,  ..., -3.9577e-05,
         -2.0429e-05, -3.0965e-05]], device='cuda:0')
Loss: 1.0767580270767212


Running epoch 0, step 607, batch 607
Sampled inputs[:2]: tensor([[   0, 1099, 2851,  ...,  518,  496,  287],
        [   0, 1765, 5370,  ..., 1711,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1620e-04,  1.6642e-04, -4.4128e-04,  ...,  4.9860e-04,
         -7.3915e-04, -3.1269e-04],
        [-2.0534e-05, -1.2621e-05,  8.9407e-06,  ..., -1.7986e-05,
         -9.7007e-06, -1.4164e-05],
        [-3.0383e-05, -1.8731e-05,  1.3396e-05,  ..., -2.6509e-05,
         -1.3359e-05, -2.0698e-05],
        [-2.6271e-05, -1.6145e-05,  1.1548e-05,  ..., -2.2948e-05,
         -1.1235e-05, -1.7911e-05],
        [-5.2184e-05, -3.2037e-05,  2.3022e-05,  ..., -4.5389e-05,
         -2.3142e-05, -3.5316e-05]], device='cuda:0')
Loss: 1.0932632684707642
Graident accumulation at epoch 0, step 607, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0150, -0.0276,  ...,  0.0285, -0.0153, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3689e-05,  8.7403e-05, -1.7514e-04,  ...,  9.1104e-05,
         -9.6468e-05, -1.6692e-05],
        [-2.0588e-05, -1.3928e-05,  9.0972e-06,  ..., -1.8284e-05,
         -9.1150e-06, -1.3827e-05],
        [ 1.1909e-04,  8.1055e-05, -5.6239e-05,  ...,  9.5355e-05,
          5.2240e-05,  6.9754e-05],
        [ 8.3838e-06,  2.9515e-06, -5.3827e-06,  ...,  6.2211e-06,
          8.4423e-06,  3.2233e-06],
        [-4.2378e-05, -2.7656e-05,  1.7361e-05,  ..., -3.6591e-05,
         -1.6769e-05, -2.7478e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1688e-08, 2.7920e-08, 3.5259e-08,  ..., 1.1838e-08, 8.6198e-08,
         1.1510e-08],
        [6.0807e-11, 3.3206e-11, 5.3542e-12,  ..., 4.2446e-11, 4.0121e-12,
         1.3045e-11],
        [2.1481e-09, 1.0806e-09, 3.6576e-10,  ..., 1.5441e-09, 3.2489e-10,
         7.0669e-10],
        [6.0668e-10, 3.0156e-10, 9.2007e-11,  ..., 4.0027e-10, 1.1258e-10,
         1.6871e-10],
        [2.7210e-10, 1.4678e-10, 2.3191e-11,  ..., 1.9860e-10, 1.9027e-11,
         6.3680e-11]], device='cuda:0')
optimizer state dict: 76.0
lr: [1.6657307618927726e-05, 1.6657307618927726e-05]
scheduler_last_epoch: 76


Running epoch 0, step 608, batch 608
Sampled inputs[:2]: tensor([[    0,    61, 22315,  ..., 36901,    17,   360],
        [    0,  5885,   271,  ...,   278,  1049,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8472e-05, -8.2534e-05,  3.4252e-05,  ..., -5.0058e-05,
         -1.7264e-05, -5.9814e-05],
        [-2.5183e-06, -1.5572e-06,  1.0207e-06,  ..., -2.1607e-06,
         -1.0431e-06, -1.6093e-06],
        [ 7.4554e-05,  4.9406e-05, -4.9152e-05,  ...,  7.1087e-05,
          6.2587e-05,  6.7226e-05],
        [-3.3081e-06, -2.0415e-06,  1.3635e-06,  ..., -2.8461e-06,
         -1.2517e-06, -2.1160e-06],
        [-6.7651e-06, -4.1723e-06,  2.7865e-06,  ..., -5.8115e-06,
         -2.6971e-06, -4.2915e-06]], device='cuda:0')
Loss: 1.1026461124420166


Running epoch 0, step 609, batch 609
Sampled inputs[:2]: tensor([[    0,  2377,   271,  ...,   395,   394,    14],
        [    0, 10565,  2677,  ...,   298,   292, 11188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2658e-04, -8.1112e-05, -6.3699e-05,  ..., -1.8624e-05,
         -8.4561e-05, -1.0444e-04],
        [-4.9025e-06, -3.0547e-06,  2.0117e-06,  ..., -4.2319e-06,
         -1.8328e-06, -3.0696e-06],
        [ 7.0620e-05,  4.6933e-05, -4.7505e-05,  ...,  6.7704e-05,
          6.1395e-05,  6.4842e-05],
        [-6.6161e-06, -4.1127e-06,  2.7493e-06,  ..., -5.7071e-06,
         -2.2054e-06, -4.1276e-06],
        [-1.3411e-05, -8.3447e-06,  5.5730e-06,  ..., -1.1533e-05,
         -4.7684e-06, -8.3148e-06]], device='cuda:0')
Loss: 1.0657320022583008


Running epoch 0, step 610, batch 610
Sampled inputs[:2]: tensor([[    0,   546,   360,  ...,    12,   461,  8753],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3934e-04, -4.4648e-05, -1.4294e-04,  ...,  6.7359e-05,
         -1.8345e-04, -1.1930e-04],
        [-7.3463e-06, -4.6045e-06,  3.0696e-06,  ..., -6.3628e-06,
         -2.7940e-06, -4.6045e-06],
        [ 6.6686e-05,  4.4444e-05, -4.5792e-05,  ...,  6.4292e-05,
          5.9934e-05,  6.2398e-05],
        [-9.8795e-06, -6.1840e-06,  4.1723e-06,  ..., -8.5682e-06,
         -3.3900e-06, -6.1691e-06],
        [-2.0146e-05, -1.2606e-05,  8.5235e-06,  ..., -1.7375e-05,
         -7.3016e-06, -1.2487e-05]], device='cuda:0')
Loss: 1.108386754989624


Running epoch 0, step 611, batch 611
Sampled inputs[:2]: tensor([[    0,   446, 21112,  ..., 22092,    22,    27],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3837e-04,  1.2266e-04, -7.1169e-05,  ...,  1.4008e-04,
         -1.8345e-04, -6.1509e-05],
        [-9.9242e-06, -6.1914e-06,  4.0308e-06,  ..., -8.5831e-06,
         -4.0382e-06, -6.2883e-06],
        [ 6.2692e-05,  4.1985e-05, -4.4264e-05,  ...,  6.0865e-05,
          5.8131e-05,  5.9820e-05],
        [-1.3098e-05, -8.1658e-06,  5.4017e-06,  ..., -1.1340e-05,
         -4.8131e-06, -8.2403e-06],
        [-2.7090e-05, -1.6868e-05,  1.1191e-05,  ..., -2.3305e-05,
         -1.0446e-05, -1.6928e-05]], device='cuda:0')
Loss: 1.0872238874435425


Running epoch 0, step 612, batch 612
Sampled inputs[:2]: tensor([[    0,   894,   496,  ...,   266,   623,   587],
        [    0,  1336,   287,  ..., 15920,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8027e-05,  1.4312e-04, -1.0396e-04,  ...,  1.6014e-04,
         -1.8564e-04, -2.9548e-05],
        [-1.2487e-05, -7.7337e-06,  5.1036e-06,  ..., -1.0759e-05,
         -5.2229e-06, -7.9870e-06],
        [ 5.8729e-05,  3.9571e-05, -4.2573e-05,  ...,  5.7497e-05,
          5.6403e-05,  5.7198e-05],
        [-1.6347e-05, -1.0133e-05,  6.7800e-06,  ..., -1.4096e-05,
         -6.1616e-06, -1.0371e-05],
        [-3.3975e-05, -2.1040e-05,  1.4111e-05,  ..., -2.9117e-05,
         -1.3471e-05, -2.1428e-05]], device='cuda:0')
Loss: 1.061795949935913


Running epoch 0, step 613, batch 613
Sampled inputs[:2]: tensor([[    0,  2548,   720,  ...,  1795,  1109, 32948],
        [    0,    14, 38914,  ...,   266,  5690,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0655e-04,  1.4811e-04, -8.1675e-05,  ...,  2.0251e-04,
         -2.7088e-04, -1.3485e-04],
        [-1.5095e-05, -9.3356e-06,  6.1318e-06,  ..., -1.2919e-05,
         -6.3851e-06, -9.6560e-06],
        [ 5.4646e-05,  3.7053e-05, -4.0927e-05,  ...,  5.4100e-05,
          5.4682e-05,  5.4590e-05],
        [-1.9684e-05, -1.2189e-05,  8.1211e-06,  ..., -1.6868e-05,
         -7.5102e-06, -1.2487e-05],
        [-4.1097e-05, -2.5392e-05,  1.6972e-05,  ..., -3.5018e-05,
         -1.6481e-05, -2.5928e-05]], device='cuda:0')
Loss: 1.070163607597351


Running epoch 0, step 614, batch 614
Sampled inputs[:2]: tensor([[    0,   346,    14,  ...,   381,   535,   505],
        [    0,    14,  3948,  ...,   571, 10097,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4814e-04,  7.9255e-05, -1.8516e-04,  ...,  1.9977e-04,
         -3.3158e-04, -2.8025e-04],
        [-1.7494e-05, -1.0863e-05,  7.1526e-06,  ..., -1.5095e-05,
         -7.3537e-06, -1.1235e-05],
        [ 5.0876e-05,  3.4639e-05, -3.9295e-05,  ...,  5.0672e-05,
          5.3266e-05,  5.2131e-05],
        [-2.2933e-05, -1.4260e-05,  9.5218e-06,  ..., -1.9819e-05,
         -8.6948e-06, -1.4618e-05],
        [-4.7684e-05, -2.9594e-05,  1.9819e-05,  ..., -4.0978e-05,
         -1.8984e-05, -3.0190e-05]], device='cuda:0')
Loss: 1.075982928276062


Running epoch 0, step 615, batch 615
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0,  474,  221,  ..., 2945,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2288e-04,  3.5235e-04, -3.0486e-04,  ...,  2.4532e-04,
         -7.4104e-04, -3.9348e-04],
        [-1.9833e-05, -1.2331e-05,  8.2254e-06,  ..., -1.7256e-05,
         -8.4415e-06, -1.2882e-05],
        [ 4.7255e-05,  3.2359e-05, -3.7596e-05,  ...,  4.7349e-05,
          5.1709e-05,  4.9613e-05],
        [-2.6092e-05, -1.6242e-05,  1.0997e-05,  ..., -2.2724e-05,
         -1.0014e-05, -1.6823e-05],
        [-5.3853e-05, -3.3468e-05,  2.2724e-05,  ..., -4.6611e-05,
         -2.1681e-05, -3.4451e-05]], device='cuda:0')
Loss: 1.069422721862793
Graident accumulation at epoch 0, step 615, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0097, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0285, -0.0153, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.4608e-05,  1.1390e-04, -1.8811e-04,  ...,  1.0653e-04,
         -1.6093e-04, -5.4371e-05],
        [-2.0513e-05, -1.3768e-05,  9.0100e-06,  ..., -1.8181e-05,
         -9.0477e-06, -1.3732e-05],
        [ 1.1191e-04,  7.6185e-05, -5.4375e-05,  ...,  9.0555e-05,
          5.2187e-05,  6.7740e-05],
        [ 4.9362e-06,  1.0321e-06, -3.7448e-06,  ...,  3.3265e-06,
          6.5967e-06,  1.2187e-06],
        [-4.3525e-05, -2.8237e-05,  1.7897e-05,  ..., -3.7593e-05,
         -1.7261e-05, -2.8175e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1751e-08, 2.8016e-08, 3.5317e-08,  ..., 1.1886e-08, 8.6661e-08,
         1.1654e-08],
        [6.1140e-11, 3.3325e-11, 5.4165e-12,  ..., 4.2702e-11, 4.0794e-12,
         1.3198e-11],
        [2.1482e-09, 1.0806e-09, 3.6681e-10,  ..., 1.5448e-09, 3.2724e-10,
         7.0844e-10],
        [6.0675e-10, 3.0152e-10, 9.2036e-11,  ..., 4.0039e-10, 1.1257e-10,
         1.6882e-10],
        [2.7473e-10, 1.4775e-10, 2.3684e-11,  ..., 2.0057e-10, 1.9478e-11,
         6.4803e-11]], device='cuda:0')
optimizer state dict: 77.0
lr: [1.656455958733442e-05, 1.656455958733442e-05]
scheduler_last_epoch: 77


Running epoch 0, step 616, batch 616
Sampled inputs[:2]: tensor([[   0, 3605, 2572,  ...,  300,  259, 1513],
        [   0,  278, 5210,  ..., 1968, 2002,  923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3002e-04,  3.9226e-04, -6.5175e-05,  ...,  1.6813e-04,
         -1.6494e-04, -7.7849e-05],
        [-2.2203e-06, -1.4156e-06,  8.9034e-07,  ..., -2.0713e-06,
         -9.0152e-07, -1.4082e-06],
        [-3.6061e-06, -2.3097e-06,  1.4976e-06,  ..., -3.3230e-06,
         -1.3039e-06, -2.2352e-06],
        [-3.0994e-06, -1.9670e-06,  1.2815e-06,  ..., -2.8759e-06,
         -1.0878e-06, -1.9372e-06],
        [-6.2287e-06, -3.9637e-06,  2.6077e-06,  ..., -5.7220e-06,
         -2.2799e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.0654406547546387


Running epoch 0, step 617, batch 617
Sampled inputs[:2]: tensor([[    0,   720,  1122,  ...,   656,   287, 14258],
        [    0,   417,   199,  ...,  8762,  4204,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3392e-04,  5.4766e-04, -2.9600e-04,  ...,  2.6208e-04,
         -3.9394e-04, -9.7857e-05],
        [-4.3958e-06, -2.8312e-06,  1.8813e-06,  ..., -4.0531e-06,
         -1.6950e-06, -2.7642e-06],
        [-7.3165e-06, -4.7088e-06,  3.2112e-06,  ..., -6.6757e-06,
         -2.5406e-06, -4.5002e-06],
        [-6.3032e-06, -4.0233e-06,  2.7567e-06,  ..., -5.7667e-06,
         -2.1160e-06, -3.9041e-06],
        [-1.2606e-05, -8.0764e-06,  5.5432e-06,  ..., -1.1444e-05,
         -4.4107e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.0623371601104736


Running epoch 0, step 618, batch 618
Sampled inputs[:2]: tensor([[   0, 5319,   14,  ..., 2372, 2356, 4093],
        [   0,   27,  417,  ...,   18,  365,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2932e-04,  6.1551e-04, -3.4851e-04,  ...,  3.1479e-04,
         -3.7075e-04, -7.9289e-05],
        [-6.7502e-06, -4.3511e-06,  2.8163e-06,  ..., -6.1095e-06,
         -2.5481e-06, -4.1500e-06],
        [-1.1370e-05, -7.3314e-06,  4.8503e-06,  ..., -1.0192e-05,
         -3.8967e-06, -6.8545e-06],
        [-9.5665e-06, -6.1393e-06,  4.0755e-06,  ..., -8.6129e-06,
         -3.1739e-06, -5.8115e-06],
        [-1.9640e-05, -1.2606e-05,  8.4043e-06,  ..., -1.7524e-05,
         -6.7949e-06, -1.1742e-05]], device='cuda:0')
Loss: 1.090285062789917


Running epoch 0, step 619, batch 619
Sampled inputs[:2]: tensor([[    0,  5862,    13,  ..., 12497,   287,  3570],
        [    0,   765,   292,  ...,   623,    12,  7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0650e-04,  8.4715e-04, -3.5595e-04,  ...,  3.5238e-04,
         -4.8714e-04, -1.1660e-04],
        [-8.9854e-06, -5.7891e-06,  3.7923e-06,  ..., -8.1509e-06,
         -3.4682e-06, -5.6177e-06],
        [-1.5110e-05, -9.7305e-06,  6.5267e-06,  ..., -1.3575e-05,
         -5.2899e-06, -9.2536e-06],
        [-1.2755e-05, -8.1807e-06,  5.5060e-06,  ..., -1.1504e-05,
         -4.3362e-06, -7.8827e-06],
        [-2.5988e-05, -1.6659e-05,  1.1280e-05,  ..., -2.3246e-05,
         -9.1940e-06, -1.5795e-05]], device='cuda:0')
Loss: 1.0771753787994385


Running epoch 0, step 620, batch 620
Sampled inputs[:2]: tensor([[    0,   374,   298,  ..., 11183,    12,   654],
        [    0,  1550,  2013,  ...,  9970,   638,  6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9932e-04,  9.6879e-04, -3.4250e-04,  ...,  3.4175e-04,
         -5.1777e-04, -8.6484e-05],
        [-1.1280e-05, -7.2271e-06,  4.7348e-06,  ..., -1.0207e-05,
         -4.5411e-06, -7.1526e-06],
        [-1.8790e-05, -1.2040e-05,  8.0913e-06,  ..., -1.6853e-05,
         -6.8471e-06, -1.1668e-05],
        [-1.5751e-05, -1.0058e-05,  6.7800e-06,  ..., -1.4186e-05,
         -5.5507e-06, -9.8497e-06],
        [-3.2365e-05, -2.0683e-05,  1.4007e-05,  ..., -2.8908e-05,
         -1.1921e-05, -1.9938e-05]], device='cuda:0')
Loss: 1.0537607669830322


Running epoch 0, step 621, batch 621
Sampled inputs[:2]: tensor([[    0,    14,   475,  ..., 44038,    12,   894],
        [    0,    12,  3570,  ...,   273,   298,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3803e-04,  1.0639e-03, -3.6130e-04,  ...,  3.8698e-04,
         -5.6140e-04, -2.1226e-05],
        [-1.3694e-05, -8.8140e-06,  5.6662e-06,  ..., -1.2308e-05,
         -5.5544e-06, -8.6725e-06],
        [-2.2754e-05, -1.4663e-05,  9.6634e-06,  ..., -2.0295e-05,
         -8.4043e-06, -1.4126e-05],
        [-1.8954e-05, -1.2159e-05,  8.0392e-06,  ..., -1.6958e-05,
         -6.7651e-06, -1.1846e-05],
        [-3.9339e-05, -2.5243e-05,  1.6764e-05,  ..., -3.4928e-05,
         -1.4678e-05, -2.4199e-05]], device='cuda:0')
Loss: 1.0730106830596924


Running epoch 0, step 622, batch 622
Sampled inputs[:2]: tensor([[   0,   12,  616,  ...,  278,  266, 2907],
        [   0,   17,   14,  ...,  650, 1711,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9349e-04,  1.2685e-03, -3.2467e-04,  ...,  4.6181e-04,
         -6.1047e-04, -5.8883e-05],
        [-1.5914e-05, -1.0319e-05,  6.5863e-06,  ..., -1.4335e-05,
         -6.3404e-06, -1.0058e-05],
        [-2.6464e-05, -1.7181e-05,  1.1235e-05,  ..., -2.3663e-05,
         -9.5963e-06, -1.6406e-05],
        [-2.2113e-05, -1.4290e-05,  9.3728e-06,  ..., -1.9833e-05,
         -7.7412e-06, -1.3798e-05],
        [-4.5806e-05, -2.9594e-05,  1.9506e-05,  ..., -4.0770e-05,
         -1.6794e-05, -2.8133e-05]], device='cuda:0')
Loss: 1.0814456939697266


Running epoch 0, step 623, batch 623
Sampled inputs[:2]: tensor([[   0, 1781,  659,  ...,   12, 1478,   14],
        [   0, 5489,   80,  ...,  221,  380,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5153e-04,  1.5262e-03, -2.9155e-04,  ...,  6.3926e-04,
         -8.1880e-04, -5.5164e-05],
        [-1.8165e-05, -1.1764e-05,  7.5288e-06,  ..., -1.6451e-05,
         -7.4953e-06, -1.1660e-05],
        [-3.0160e-05, -1.9550e-05,  1.2837e-05,  ..., -2.7075e-05,
         -1.1325e-05, -1.8969e-05],
        [-2.5094e-05, -1.6198e-05,  1.0669e-05,  ..., -2.2605e-05,
         -9.1121e-06, -1.5885e-05],
        [-5.2035e-05, -3.3557e-05,  2.2233e-05,  ..., -4.6521e-05,
         -1.9729e-05, -3.2425e-05]], device='cuda:0')
Loss: 1.0808881521224976
Graident accumulation at epoch 0, step 623, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0285, -0.0153, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5006e-05,  2.5513e-04, -1.9846e-04,  ...,  1.5980e-04,
         -2.2671e-04, -5.4450e-05],
        [-2.0278e-05, -1.3567e-05,  8.8619e-06,  ..., -1.8008e-05,
         -8.8924e-06, -1.3525e-05],
        [ 9.7702e-05,  6.6612e-05, -4.7653e-05,  ...,  7.8792e-05,
          4.5836e-05,  5.9069e-05],
        [ 1.9333e-06, -6.9086e-07, -2.3034e-06,  ...,  7.3337e-07,
          5.0259e-06, -4.9168e-07],
        [-4.4376e-05, -2.8769e-05,  1.8331e-05,  ..., -3.8486e-05,
         -1.7507e-05, -2.8600e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2274e-08, 3.0317e-08, 3.5366e-08,  ..., 1.2283e-08, 8.7245e-08,
         1.1645e-08],
        [6.1409e-11, 3.3430e-11, 5.4678e-12,  ..., 4.2930e-11, 4.1315e-12,
         1.3321e-11],
        [2.1470e-09, 1.0799e-09, 3.6661e-10,  ..., 1.5440e-09, 3.2704e-10,
         7.0809e-10],
        [6.0677e-10, 3.0148e-10, 9.2058e-11,  ..., 4.0050e-10, 1.1254e-10,
         1.6891e-10],
        [2.7716e-10, 1.4873e-10, 2.4155e-11,  ..., 2.0254e-10, 1.9848e-11,
         6.5790e-11]], device='cuda:0')
optimizer state dict: 78.0
lr: [1.6470808433733317e-05, 1.6470808433733317e-05]
scheduler_last_epoch: 78


Running epoch 0, step 624, batch 624
Sampled inputs[:2]: tensor([[   0, 1561,   14,  ..., 4433,  352, 1561],
        [   0,  600,  287,  ..., 1933,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1874e-05,  1.7088e-04,  5.7583e-05,  ..., -2.5446e-07,
          7.2680e-06,  8.4795e-05],
        [-2.1607e-06, -1.4156e-06,  8.4564e-07,  ..., -2.0415e-06,
         -1.1176e-06, -1.5274e-06],
        [-3.6508e-06, -2.3842e-06,  1.4901e-06,  ..., -3.3826e-06,
         -1.7136e-06, -2.5034e-06],
        [-2.9206e-06, -1.8999e-06,  1.1846e-06,  ..., -2.7120e-06,
         -1.3411e-06, -2.0266e-06],
        [-6.1393e-06, -3.9935e-06,  2.5183e-06,  ..., -5.6326e-06,
         -2.8908e-06, -4.1723e-06]], device='cuda:0')
Loss: 1.075290560722351


Running epoch 0, step 625, batch 625
Sampled inputs[:2]: tensor([[    0,   590,    16,  ...,    13,    35,  1151],
        [    0,   328, 16219,  ..., 14559,   351,   587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0390e-05,  1.4849e-04,  6.7965e-05,  ..., -3.8659e-05,
          9.5137e-05,  8.3944e-05],
        [-4.3660e-06, -2.8908e-06,  1.7993e-06,  ..., -4.0829e-06,
         -2.0117e-06, -2.9653e-06],
        [-7.3910e-06, -4.8876e-06,  3.1516e-06,  ..., -6.7949e-06,
         -3.0994e-06, -4.8727e-06],
        [-6.0201e-06, -3.9563e-06,  2.5555e-06,  ..., -5.5581e-06,
         -2.4661e-06, -4.0233e-06],
        [-1.2487e-05, -8.2254e-06,  5.3495e-06,  ..., -1.1444e-05,
         -5.2899e-06, -8.1658e-06]], device='cuda:0')
Loss: 1.1005945205688477


Running epoch 0, step 626, batch 626
Sampled inputs[:2]: tensor([[    0,  1145,    13,  ...,   721,  1119,  3495],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1739e-04,  1.0365e-04,  3.2968e-05,  ..., -1.6605e-06,
          5.8250e-05,  4.0438e-05],
        [-6.4969e-06, -4.2915e-06,  2.6487e-06,  ..., -6.0201e-06,
         -2.8573e-06, -4.4107e-06],
        [-1.1072e-05, -7.3016e-06,  4.6641e-06,  ..., -1.0103e-05,
         -4.3958e-06, -7.3016e-06],
        [-9.0301e-06, -5.9232e-06,  3.7849e-06,  ..., -8.2552e-06,
         -3.4869e-06, -6.0201e-06],
        [-1.8716e-05, -1.2308e-05,  7.9274e-06,  ..., -1.7017e-05,
         -7.5102e-06, -1.2219e-05]], device='cuda:0')
Loss: 1.0688666105270386


Running epoch 0, step 627, batch 627
Sampled inputs[:2]: tensor([[    0,   266,  1254,  ...,   369,  2870,   300],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5452e-05,  2.9374e-04, -1.5964e-04,  ...,  1.0094e-04,
          5.8182e-05, -1.7317e-05],
        [-8.5533e-06, -5.6550e-06,  3.5129e-06,  ..., -8.0466e-06,
         -3.8110e-06, -5.8711e-06],
        [-1.4514e-05, -9.5665e-06,  6.1616e-06,  ..., -1.3426e-05,
         -5.8040e-06, -9.6560e-06],
        [-1.1981e-05, -7.8604e-06,  5.0664e-06,  ..., -1.1131e-05,
         -4.6864e-06, -8.0764e-06],
        [-2.4676e-05, -1.6212e-05,  1.0550e-05,  ..., -2.2739e-05,
         -9.9838e-06, -1.6212e-05]], device='cuda:0')
Loss: 1.0788899660110474


Running epoch 0, step 628, batch 628
Sampled inputs[:2]: tensor([[    0,  1850,   311,  ...,  3655,  3133,  9000],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7394e-04,  1.1643e-04, -1.0546e-04,  ..., -9.2331e-05,
          2.0461e-04, -1.9599e-05],
        [-1.0759e-05, -7.0855e-06,  4.4219e-06,  ..., -1.0073e-05,
         -4.8690e-06, -7.4208e-06],
        [ 4.1231e-04,  2.1644e-04, -1.3760e-04,  ...,  2.9041e-04,
          9.0032e-05,  1.5004e-04],
        [-1.4901e-05, -9.7528e-06,  6.3181e-06,  ..., -1.3769e-05,
         -5.9083e-06, -1.0073e-05],
        [-3.0935e-05, -2.0266e-05,  1.3277e-05,  ..., -2.8372e-05,
         -1.2696e-05, -2.0385e-05]], device='cuda:0')
Loss: 1.0944162607192993


Running epoch 0, step 629, batch 629
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0, 4845, 1521,  ...,  963,  292, 6414]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4341e-04, -4.4399e-05, -9.2875e-05,  ...,  3.9422e-05,
          1.9667e-04, -4.2038e-05],
        [-1.2711e-05, -8.4117e-06,  5.2899e-06,  ..., -1.2040e-05,
         -5.8077e-06, -8.9258e-06],
        [ 4.0894e-04,  2.1416e-04, -1.3605e-04,  ...,  2.8714e-04,
          8.8662e-05,  1.4759e-04],
        [-1.7717e-05, -1.1653e-05,  7.6070e-06,  ..., -1.6525e-05,
         -7.0333e-06, -1.2144e-05],
        [-3.6448e-05, -2.3991e-05,  1.5810e-05,  ..., -3.3677e-05,
         -1.4931e-05, -2.4289e-05]], device='cuda:0')
Loss: 1.0592577457427979


Running epoch 0, step 630, batch 630
Sampled inputs[:2]: tensor([[   0,  422,   14,  ...,  271, 1360,   12],
        [   0, 1485,  271,  ..., 6359, 1799, 5442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4286e-05,  1.9226e-05, -1.1370e-04,  ..., -3.2227e-05,
          2.7482e-04,  4.4875e-05],
        [-1.4752e-05, -9.7603e-06,  6.1505e-06,  ..., -1.4096e-05,
         -6.9328e-06, -1.0431e-05],
        [ 4.0556e-04,  2.1193e-04, -1.3457e-04,  ...,  2.8383e-04,
          8.7045e-05,  1.4521e-04],
        [-2.0444e-05, -1.3448e-05,  8.7991e-06,  ..., -1.9208e-05,
         -8.2999e-06, -1.4082e-05],
        [-4.2230e-05, -2.7806e-05,  1.8373e-05,  ..., -3.9309e-05,
         -1.7717e-05, -2.8312e-05]], device='cuda:0')
Loss: 1.0720877647399902


Running epoch 0, step 631, batch 631
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0,  1445,  3597,  ...,   281,    78,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0781e-04,  9.5650e-05, -8.4665e-05,  ...,  1.9653e-05,
          2.9552e-04, -1.2288e-05],
        [-1.6719e-05, -1.1146e-05,  7.0222e-06,  ..., -1.5974e-05,
         -7.6033e-06, -1.1735e-05],
        [ 4.0209e-04,  2.0951e-04, -1.3298e-04,  ...,  2.8059e-04,
          8.6024e-05,  1.4299e-04],
        [-2.3425e-05, -1.5505e-05,  1.0170e-05,  ..., -2.1994e-05,
         -9.1493e-06, -1.6004e-05],
        [-4.8041e-05, -3.1829e-05,  2.1055e-05,  ..., -4.4703e-05,
         -1.9446e-05, -3.1963e-05]], device='cuda:0')
Loss: 1.063288927078247
Graident accumulation at epoch 0, step 631, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.2286e-05,  2.3918e-04, -1.8708e-04,  ...,  1.4578e-04,
         -1.7449e-04, -5.0234e-05],
        [-1.9922e-05, -1.3325e-05,  8.6779e-06,  ..., -1.7804e-05,
         -8.7635e-06, -1.3346e-05],
        [ 1.2814e-04,  8.0902e-05, -5.6186e-05,  ...,  9.8972e-05,
          4.9855e-05,  6.7461e-05],
        [-6.0254e-07, -2.1722e-06, -1.0560e-06,  ..., -1.5394e-06,
          3.6083e-06, -2.0429e-06],
        [-4.4743e-05, -2.9075e-05,  1.8603e-05,  ..., -3.9108e-05,
         -1.7701e-05, -2.8937e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2243e-08, 3.0296e-08, 3.5338e-08,  ..., 1.2271e-08, 8.7245e-08,
         1.1634e-08],
        [6.1627e-11, 3.3521e-11, 5.5116e-12,  ..., 4.3142e-11, 4.1852e-12,
         1.3445e-11],
        [2.3065e-09, 1.1227e-09, 3.8392e-10,  ..., 1.6212e-09, 3.3411e-10,
         7.2783e-10],
        [6.0672e-10, 3.0142e-10, 9.2069e-11,  ..., 4.0058e-10, 1.1251e-10,
         1.6900e-10],
        [2.7919e-10, 1.4959e-10, 2.4574e-11,  ..., 2.0433e-10, 2.0206e-11,
         6.6745e-11]], device='cuda:0')
optimizer state dict: 79.0
lr: [1.6376068484119055e-05, 1.6376068484119055e-05]
scheduler_last_epoch: 79


Running epoch 0, step 632, batch 632
Sampled inputs[:2]: tensor([[    0,    19,    14,  ...,    13,  6673,   298],
        [    0,  4014,    88,  ...,    14, 11961,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1191e-04, -1.2263e-03,  6.5547e-04,  ..., -6.5584e-04,
          1.0462e-03,  3.7193e-04],
        [-1.7136e-06, -1.1846e-06,  7.4133e-07,  ..., -2.0117e-06,
         -1.5423e-06, -1.6093e-06],
        [-2.9504e-06, -2.0415e-06,  1.4007e-06,  ..., -3.1888e-06,
         -2.0862e-06, -2.4438e-06],
        [-2.4289e-06, -1.6615e-06,  1.1399e-06,  ..., -2.6226e-06,
         -1.6689e-06, -2.0266e-06],
        [-4.7684e-06, -3.2783e-06,  2.2948e-06,  ..., -5.0068e-06,
         -3.1739e-06, -3.7253e-06]], device='cuda:0')
Loss: 1.0654757022857666


Running epoch 0, step 633, batch 633
Sampled inputs[:2]: tensor([[   0, 3468,  278,  ..., 2442,  292,  380],
        [   0,  965,  300,  ...,  546,  857, 4350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2091e-04, -1.3849e-03,  7.4883e-04,  ..., -6.9307e-04,
          1.1483e-03,  3.2742e-04],
        [-3.6955e-06, -2.6524e-06,  1.6317e-06,  ..., -3.8743e-06,
         -2.4736e-06, -2.9355e-06],
        [-6.4820e-06, -4.6492e-06,  3.0547e-06,  ..., -6.4820e-06,
         -3.5986e-06, -4.7535e-06],
        [-5.2452e-06, -3.7327e-06,  2.4512e-06,  ..., -5.2601e-06,
         -2.8461e-06, -3.8967e-06],
        [-1.0669e-05, -7.6294e-06,  5.0664e-06,  ..., -1.0490e-05,
         -5.7220e-06, -7.5102e-06]], device='cuda:0')
Loss: 1.0925159454345703


Running epoch 0, step 634, batch 634
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,   13, 2853,  445],
        [   0,  278, 5717,  ..., 5342, 5147,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7489e-04, -1.6872e-03,  8.7456e-04,  ..., -8.3179e-04,
          1.3595e-03,  2.6082e-04],
        [-5.5209e-06, -4.0233e-06,  2.4848e-06,  ..., -5.6103e-06,
         -3.1665e-06, -4.1723e-06],
        [-9.7901e-06, -7.1228e-06,  4.6715e-06,  ..., -9.5814e-06,
         -4.6939e-06, -6.9439e-06],
        [-8.0168e-06, -5.8040e-06,  3.8072e-06,  ..., -7.8678e-06,
         -3.7178e-06, -5.7667e-06],
        [-1.6123e-05, -1.1712e-05,  7.7486e-06,  ..., -1.5587e-05,
         -7.5847e-06, -1.1072e-05]], device='cuda:0')
Loss: 1.0619927644729614


Running epoch 0, step 635, batch 635
Sampled inputs[:2]: tensor([[    0,    14,  1075,  ..., 22182,  5948,  8401],
        [    0,  2202,   292,  ...,  2431,  2267,  3423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6256e-04, -1.7876e-03,  1.0166e-03,  ..., -9.1171e-04,
          1.4592e-03,  2.3714e-04],
        [-7.5474e-06, -5.4538e-06,  3.4198e-06,  ..., -7.5102e-06,
         -4.0829e-06, -5.4911e-06],
        [-1.3337e-05, -9.6411e-06,  6.3628e-06,  ..., -1.2875e-05,
         -6.1393e-06, -9.1940e-06],
        [-1.0923e-05, -7.8753e-06,  5.2005e-06,  ..., -1.0580e-05,
         -4.8652e-06, -7.6443e-06],
        [-2.2054e-05, -1.5914e-05,  1.0580e-05,  ..., -2.1070e-05,
         -1.0058e-05, -1.4797e-05]], device='cuda:0')
Loss: 1.0753566026687622


Running epoch 0, step 636, batch 636
Sampled inputs[:2]: tensor([[   0,  508,  927,  ..., 1390,  674,  369],
        [   0,  344, 2574,  ..., 2558, 2663,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0854e-04, -1.6943e-03,  9.6569e-04,  ..., -7.9109e-04,
          1.4057e-03,  2.0001e-04],
        [-9.4399e-06, -6.8620e-06,  4.3325e-06,  ..., -9.3430e-06,
         -5.0440e-06, -6.8471e-06],
        [-1.6659e-05, -1.2100e-05,  8.0466e-06,  ..., -1.6004e-05,
         -7.6219e-06, -1.1489e-05],
        [-1.3694e-05, -9.9167e-06,  6.6012e-06,  ..., -1.3217e-05,
         -6.0871e-06, -9.5963e-06],
        [-2.7627e-05, -2.0027e-05,  1.3411e-05,  ..., -2.6315e-05,
         -1.2577e-05, -1.8552e-05]], device='cuda:0')
Loss: 1.0773061513900757


Running epoch 0, step 637, batch 637
Sampled inputs[:2]: tensor([[    0,   401,  3408,  ...,   287, 19892,   328],
        [    0,   271,  5738,  ...,    12,    21,  9023]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6041e-04, -1.7937e-03,  9.8951e-04,  ..., -7.8447e-04,
          1.4980e-03,  2.4547e-04],
        [-1.1422e-05, -8.2776e-06,  5.2340e-06,  ..., -1.1154e-05,
         -5.7518e-06, -8.0690e-06],
        [-2.0221e-05, -1.4648e-05,  9.7081e-06,  ..., -1.9237e-05,
         -8.7768e-06, -1.3635e-05],
        [-1.6689e-05, -1.2048e-05,  8.0019e-06,  ..., -1.5959e-05,
         -7.0259e-06, -1.1429e-05],
        [-3.3736e-05, -2.4378e-05,  1.6272e-05,  ..., -3.1888e-05,
         -1.4618e-05, -2.2203e-05]], device='cuda:0')
Loss: 1.0859270095825195


Running epoch 0, step 638, batch 638
Sampled inputs[:2]: tensor([[   0, 6184, 1412,  ...,   12,  266,  944],
        [   0,  401, 3740,  ..., 5980,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5096e-04, -1.7748e-03,  8.9787e-04,  ..., -7.2595e-04,
          1.4562e-03,  2.5429e-04],
        [-1.3329e-05, -9.7081e-06,  6.1207e-06,  ..., -1.2971e-05,
         -6.5453e-06, -9.3430e-06],
        [-2.3693e-05, -1.7241e-05,  1.1377e-05,  ..., -2.2471e-05,
         -1.0066e-05, -1.5885e-05],
        [-1.9550e-05, -1.4178e-05,  9.3877e-06,  ..., -1.8656e-05,
         -8.0615e-06, -1.3322e-05],
        [-3.9488e-05, -2.8640e-05,  1.9059e-05,  ..., -3.7253e-05,
         -1.6794e-05, -2.5868e-05]], device='cuda:0')
Loss: 1.064326286315918


Running epoch 0, step 639, batch 639
Sampled inputs[:2]: tensor([[    0,  3103,  2134,  ...,  6627,   275,  1911],
        [    0, 41638,  4573,  ...,   259,   790,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0597e-03, -1.9667e-03,  1.0638e-03,  ..., -9.1689e-04,
          1.6221e-03,  3.3855e-04],
        [-1.5266e-05, -1.1064e-05,  6.9290e-06,  ..., -1.4953e-05,
         -7.8343e-06, -1.0833e-05],
        [-2.6897e-05, -1.9476e-05,  1.2815e-05,  ..., -2.5645e-05,
         -1.1966e-05, -1.8239e-05],
        [-2.2113e-05, -1.5959e-05,  1.0520e-05,  ..., -2.1204e-05,
         -9.5665e-06, -1.5244e-05],
        [-4.4882e-05, -3.2395e-05,  2.1502e-05,  ..., -4.2528e-05,
         -1.9982e-05, -2.9743e-05]], device='cuda:0')
Loss: 1.0690256357192993
Graident accumulation at epoch 0, step 639, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0145,  0.0031,  ..., -0.0026,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0150, -0.0277,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.7913e-05,  1.8590e-05, -6.1990e-05,  ...,  3.9517e-05,
          5.1667e-06, -1.1355e-05],
        [-1.9456e-05, -1.3099e-05,  8.5030e-06,  ..., -1.7519e-05,
         -8.6706e-06, -1.3095e-05],
        [ 1.1264e-04,  7.0864e-05, -4.9286e-05,  ...,  8.6510e-05,
          4.3673e-05,  5.8891e-05],
        [-2.7536e-06, -3.5509e-06,  1.0160e-07,  ..., -3.5059e-06,
          2.2908e-06, -3.3630e-06],
        [-4.4757e-05, -2.9407e-05,  1.8893e-05,  ..., -3.9450e-05,
         -1.7929e-05, -2.9017e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3324e-08, 3.4134e-08, 3.6435e-08,  ..., 1.3099e-08, 8.9789e-08,
         1.1737e-08],
        [6.1798e-11, 3.3610e-11, 5.5541e-12,  ..., 4.3322e-11, 4.2424e-12,
         1.3549e-11],
        [2.3049e-09, 1.1220e-09, 3.8370e-10,  ..., 1.6202e-09, 3.3392e-10,
         7.2743e-10],
        [6.0660e-10, 3.0137e-10, 9.2088e-11,  ..., 4.0063e-10, 1.1249e-10,
         1.6906e-10],
        [2.8093e-10, 1.5049e-10, 2.5011e-11,  ..., 2.0594e-10, 2.0585e-11,
         6.7563e-11]], device='cuda:0')
optimizer state dict: 80.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 80
Epoch 0 | Batch 639/1048 | Training PPL: 5178.160134419497 | time 66.93200063705444
Saving checkpoint at epoch 0, step 639, batch 639
Epoch 0 | Validation PPL: 8.46562011646269 | Learning rate: 1.628035421558293e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_639, AFTER epoch 0, step 639


Running epoch 0, step 640, batch 640
Sampled inputs[:2]: tensor([[   0,   14,  417,  ...,   43,  503,   67],
        [   0,  287,  271,  ..., 5090,  631, 3276]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1170e-04,  2.0297e-04, -1.7783e-04,  ...,  2.1968e-04,
         -1.8783e-04, -6.4360e-05],
        [-1.8701e-06, -1.3933e-06,  9.6112e-07,  ..., -1.7285e-06,
         -7.1898e-07, -1.0803e-06],
        [-3.4869e-06, -2.5928e-06,  1.8403e-06,  ..., -3.1739e-06,
         -1.2144e-06, -1.9521e-06],
        [-2.9653e-06, -2.2054e-06,  1.5721e-06,  ..., -2.7269e-06,
         -1.0133e-06, -1.6987e-06],
        [-5.6624e-06, -4.2021e-06,  3.0100e-06,  ..., -5.1856e-06,
         -2.0266e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0569138526916504


Running epoch 0, step 641, batch 641
Sampled inputs[:2]: tensor([[   0, 1304, 1040,  ...,  287, 1665,  741],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0679e-05,  1.7384e-04, -2.0727e-04,  ...,  3.4784e-04,
         -3.1846e-04, -6.4307e-05],
        [-3.9116e-06, -2.9728e-06,  1.9297e-06,  ..., -3.5465e-06,
         -1.6689e-06, -2.2724e-06],
        [-7.1824e-06, -5.4538e-06,  3.6582e-06,  ..., -6.4373e-06,
         -2.7865e-06, -4.0531e-06],
        [-5.9158e-06, -4.4703e-06,  3.0175e-06,  ..., -5.3495e-06,
         -2.2575e-06, -3.4049e-06],
        [-1.1623e-05, -8.7917e-06,  5.9456e-06,  ..., -1.0431e-05,
         -4.6045e-06, -6.4373e-06]], device='cuda:0')
Loss: 1.0651278495788574


Running epoch 0, step 642, batch 642
Sampled inputs[:2]: tensor([[   0,  409,  699,  ...,   12,  546,  696],
        [   0, 2422,  300,  ...,  630,  729, 3400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1054e-04,  3.6309e-04, -2.7925e-04,  ...,  4.4672e-04,
         -5.7705e-04, -2.4275e-05],
        [-5.9530e-06, -4.4703e-06,  2.8796e-06,  ..., -5.3868e-06,
         -2.6450e-06, -3.4720e-06],
        [-1.0923e-05, -8.2105e-06,  5.4687e-06,  ..., -9.7603e-06,
         -4.4256e-06, -6.1840e-06],
        [-8.8960e-06, -6.6459e-06,  4.4480e-06,  ..., -8.0019e-06,
         -3.5465e-06, -5.1335e-06],
        [-1.7732e-05, -1.3262e-05,  8.9109e-06,  ..., -1.5855e-05,
         -7.3165e-06, -9.8497e-06]], device='cuda:0')
Loss: 1.068907380104065


Running epoch 0, step 643, batch 643
Sampled inputs[:2]: tensor([[   0, 2379,   13,  ...,  287,  259, 2193],
        [   0,   34, 3881,  ..., 1027,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3389e-05,  3.8329e-04, -2.7135e-04,  ...,  4.2495e-04,
         -6.9107e-04, -5.8252e-05],
        [-7.8082e-06, -5.8711e-06,  3.7998e-06,  ..., -7.1377e-06,
         -3.5428e-06, -4.5672e-06],
        [-1.4380e-05, -1.0803e-05,  7.2569e-06,  ..., -1.2964e-05,
         -5.9307e-06, -8.1658e-06],
        [-1.1772e-05, -8.7917e-06,  5.9307e-06,  ..., -1.0684e-05,
         -4.7758e-06, -6.8098e-06],
        [-2.3395e-05, -1.7494e-05,  1.1832e-05,  ..., -2.1070e-05,
         -9.8199e-06, -1.3024e-05]], device='cuda:0')
Loss: 1.0761672258377075


Running epoch 0, step 644, batch 644
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0,  1644,  1742,  ...,   287,  1704,  2044]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1662e-05,  4.5145e-04, -3.9226e-04,  ...,  5.4342e-04,
         -7.4411e-04, -2.0352e-05],
        [-9.7305e-06, -7.3612e-06,  4.7684e-06,  ..., -8.8289e-06,
         -4.3996e-06, -5.6848e-06],
        [-1.8045e-05, -1.3635e-05,  9.1568e-06,  ..., -1.6138e-05,
         -7.4357e-06, -1.0237e-05],
        [-1.4633e-05, -1.0997e-05,  7.4133e-06,  ..., -1.3173e-05,
         -5.9232e-06, -8.4639e-06],
        [-2.9266e-05, -2.2024e-05,  1.4871e-05,  ..., -2.6166e-05,
         -1.2264e-05, -1.6287e-05]], device='cuda:0')
Loss: 1.0756583213806152


Running epoch 0, step 645, batch 645
Sampled inputs[:2]: tensor([[    0,   587,   300,  ...,  4325,   278, 12564],
        [    0,   365,  1462,  ...,   518,  6104,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1985e-05,  5.3999e-04, -5.2873e-04,  ...,  6.3254e-04,
         -8.2670e-04, -1.2176e-06],
        [-1.1697e-05, -8.8513e-06,  5.6997e-06,  ..., -1.0595e-05,
         -5.2191e-06, -6.7875e-06],
        [-2.1711e-05, -1.6376e-05,  1.0952e-05,  ..., -1.9357e-05,
         -8.8140e-06, -1.2234e-05],
        [-1.7643e-05, -1.3247e-05,  8.8885e-06,  ..., -1.5840e-05,
         -7.0333e-06, -1.0140e-05],
        [-3.5197e-05, -2.6464e-05,  1.7777e-05,  ..., -3.1352e-05,
         -1.4529e-05, -1.9461e-05]], device='cuda:0')
Loss: 1.086146354675293


Running epoch 0, step 646, batch 646
Sampled inputs[:2]: tensor([[   0,  278,  266,  ..., 5503,  259, 1036],
        [   0, 1420, 6319,  ...,  292, 4895, 4050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5136e-04,  1.8286e-04, -5.1399e-04,  ...,  5.3387e-04,
         -6.6708e-04,  2.0204e-04],
        [-1.3649e-05, -1.0207e-05,  6.5491e-06,  ..., -1.2495e-05,
         -6.5975e-06, -8.3223e-06],
        [-2.5094e-05, -1.8746e-05,  1.2524e-05,  ..., -2.2545e-05,
         -1.0945e-05, -1.4752e-05],
        [-2.0310e-05, -1.5095e-05,  1.0118e-05,  ..., -1.8373e-05,
         -8.7023e-06, -1.2182e-05],
        [-4.0710e-05, -3.0339e-05,  2.0370e-05,  ..., -3.6508e-05,
         -1.7971e-05, -2.3454e-05]], device='cuda:0')
Loss: 1.0628035068511963


Running epoch 0, step 647, batch 647
Sampled inputs[:2]: tensor([[   0,  221,  474,  ...,   12,  259, 1220],
        [   0, 6518,  681,  ...,  401, 9748,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2227e-04,  3.4450e-04, -4.6665e-04,  ...,  7.5960e-04,
         -7.4552e-04,  2.4073e-04],
        [-1.5602e-05, -1.1683e-05,  7.5623e-06,  ..., -1.4283e-05,
         -7.5959e-06, -9.5293e-06],
        [-2.8640e-05, -2.1413e-05,  1.4417e-05,  ..., -2.5749e-05,
         -1.2599e-05, -1.6883e-05],
        [-2.3171e-05, -1.7241e-05,  1.1645e-05,  ..., -2.0966e-05,
         -9.9987e-06, -1.3933e-05],
        [-4.6462e-05, -3.4630e-05,  2.3440e-05,  ..., -4.1664e-05,
         -2.0683e-05, -2.6822e-05]], device='cuda:0')
Loss: 1.078710913658142
Graident accumulation at epoch 0, step 647, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0026,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0150, -0.0277,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3349e-05,  5.1181e-05, -1.0246e-04,  ...,  1.1153e-04,
         -6.9902e-05,  1.3854e-05],
        [-1.9071e-05, -1.2958e-05,  8.4089e-06,  ..., -1.7196e-05,
         -8.5631e-06, -1.2738e-05],
        [ 9.8509e-05,  6.1636e-05, -4.2915e-05,  ...,  7.5284e-05,
          3.8046e-05,  5.1314e-05],
        [-4.7954e-06, -4.9199e-06,  1.2560e-06,  ..., -5.2519e-06,
          1.0619e-06, -4.4200e-06],
        [-4.4927e-05, -2.9929e-05,  1.9348e-05,  ..., -3.9671e-05,
         -1.8205e-05, -2.8798e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3295e-08, 3.4218e-08, 3.6616e-08,  ..., 1.3663e-08, 9.0255e-08,
         1.1783e-08],
        [6.1980e-11, 3.3713e-11, 5.6058e-12,  ..., 4.3483e-11, 4.2958e-12,
         1.3627e-11],
        [2.3034e-09, 1.1213e-09, 3.8353e-10,  ..., 1.6193e-09, 3.3375e-10,
         7.2699e-10],
        [6.0653e-10, 3.0137e-10, 9.2131e-11,  ..., 4.0067e-10, 1.1248e-10,
         1.6908e-10],
        [2.8281e-10, 1.5154e-10, 2.5536e-11,  ..., 2.0747e-10, 2.0992e-11,
         6.8215e-11]], device='cuda:0')
optimizer state dict: 81.0
lr: [1.6183680254100683e-05, 1.6183680254100683e-05]
scheduler_last_epoch: 81


Running epoch 0, step 648, batch 648
Sampled inputs[:2]: tensor([[   0, 1295,  508,  ...,  829,  772,  278],
        [   0, 6477,   12,  ..., 2931,  221,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5591e-05,  9.8166e-05, -5.7263e-05,  ...,  1.0159e-05,
         -1.0915e-05,  3.8881e-05],
        [-2.0713e-06, -1.5348e-06,  1.0803e-06,  ..., -1.7285e-06,
         -9.0152e-07, -1.0505e-06],
        [-3.9041e-06, -2.8908e-06,  2.0862e-06,  ..., -3.2187e-06,
         -1.5870e-06, -1.9222e-06],
        [ 6.8391e-05,  5.0392e-05, -5.5242e-05,  ...,  5.3374e-05,
          2.8084e-05,  3.2263e-05],
        [-6.1691e-06, -4.5300e-06,  3.2932e-06,  ..., -5.0664e-06,
         -2.5481e-06, -2.9802e-06]], device='cuda:0')
Loss: 1.074484944343567


Running epoch 0, step 649, batch 649
Sampled inputs[:2]: tensor([[   0, 6762,  689,  ..., 7061,   14,  381],
        [   0,  360,  508,  ...,  259,  554, 1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4302e-04,  3.4191e-04, -1.6745e-04,  ...,  1.4298e-04,
         -2.0811e-04,  8.4768e-05],
        [-3.9488e-06, -3.0026e-06,  2.0489e-06,  ..., -3.4571e-06,
         -1.9148e-06, -2.1979e-06],
        [-7.3612e-06, -5.5730e-06,  3.9712e-06,  ..., -6.3181e-06,
         -3.2559e-06, -3.9190e-06],
        [ 6.5590e-05,  4.8231e-05, -5.3715e-05,  ...,  5.0841e-05,
          2.6728e-05,  3.0594e-05],
        [-1.1563e-05, -8.7023e-06,  6.2436e-06,  ..., -9.8646e-06,
         -5.1707e-06, -6.0052e-06]], device='cuda:0')
Loss: 1.0484322309494019


Running epoch 0, step 650, batch 650
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,  1070,  5746,  ...,   278,   689,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4231e-04,  6.1519e-04, -1.9612e-04,  ...,  2.1424e-04,
         -3.2500e-04,  4.7877e-05],
        [-6.0499e-06, -4.5672e-06,  3.0398e-06,  ..., -5.2601e-06,
         -3.0324e-06, -3.3379e-06],
        [-1.1235e-05, -8.4341e-06,  5.8711e-06,  ..., -9.5814e-06,
         -5.1409e-06, -5.9307e-06],
        [ 6.2610e-05,  4.6026e-05, -5.2254e-05,  ...,  4.8308e-05,
          2.5298e-05,  2.9007e-05],
        [-1.7643e-05, -1.3173e-05,  9.2387e-06,  ..., -1.4991e-05,
         -8.1211e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.0428341627120972


Running epoch 0, step 651, batch 651
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,  266,  996,  ...,  709,  616, 9378]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7440e-04,  9.5665e-04, -3.8291e-04,  ...,  3.9542e-04,
         -3.1981e-04, -6.7468e-05],
        [-7.9721e-06, -6.0201e-06,  4.0904e-06,  ..., -6.9663e-06,
         -3.9376e-06, -4.4182e-06],
        [-1.4782e-05, -1.1116e-05,  7.8827e-06,  ..., -1.2651e-05,
         -6.6534e-06, -7.8231e-06],
        [ 5.9674e-05,  4.3836e-05, -5.0600e-05,  ...,  4.5745e-05,
          2.4053e-05,  2.7383e-05],
        [-2.3276e-05, -1.7405e-05,  1.2428e-05,  ..., -1.9848e-05,
         -1.0550e-05, -1.2025e-05]], device='cuda:0')
Loss: 1.0592964887619019


Running epoch 0, step 652, batch 652
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0,   300,   369,  ...,    12,   970,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6168e-04,  9.0549e-04, -3.5977e-04,  ...,  2.8162e-04,
         -2.5078e-04,  7.0586e-06],
        [-1.0043e-05, -7.5176e-06,  5.1185e-06,  ..., -8.7917e-06,
         -5.0105e-06, -5.5432e-06],
        [-1.8552e-05, -1.3843e-05,  9.8199e-06,  ..., -1.5929e-05,
         -8.4639e-06, -9.8199e-06],
        [ 5.6620e-05,  4.1630e-05, -4.9043e-05,  ...,  4.3078e-05,
          2.2593e-05,  2.5728e-05],
        [-2.9296e-05, -2.1756e-05,  1.5512e-05,  ..., -2.5064e-05,
         -1.3471e-05, -1.5154e-05]], device='cuda:0')
Loss: 1.0898876190185547


Running epoch 0, step 653, batch 653
Sampled inputs[:2]: tensor([[   0, 9611,  278,  ...,  278,  638,  600],
        [   0,  328,  266,  ...,  382,   17,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9332e-04,  9.3661e-04, -4.7149e-04,  ...,  2.6813e-04,
         -2.9745e-04,  7.9275e-05],
        [-1.1995e-05, -9.0152e-06,  6.0871e-06,  ..., -1.0602e-05,
         -6.1803e-06, -6.7353e-06],
        [-2.2128e-05, -1.6570e-05,  1.1683e-05,  ..., -1.9163e-05,
         -1.0401e-05, -1.1891e-05],
        [ 5.3803e-05,  3.9499e-05, -4.7590e-05,  ...,  4.0515e-05,
          2.1066e-05,  2.4052e-05],
        [-3.4869e-05, -2.6017e-05,  1.8418e-05,  ..., -3.0071e-05,
         -1.6496e-05, -1.8314e-05]], device='cuda:0')
Loss: 1.0728278160095215


Running epoch 0, step 654, batch 654
Sampled inputs[:2]: tensor([[   0,   18,   14,  ...,  380,  981,   12],
        [   0, 7018,   14,  ..., 8288,   12, 1250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5728e-04,  1.0930e-03, -5.6041e-04,  ...,  3.4926e-04,
         -5.0440e-04,  8.2035e-05],
        [-1.3903e-05, -1.0468e-05,  7.0855e-06,  ..., -1.2346e-05,
         -7.2084e-06, -7.9572e-06],
        [-2.5675e-05, -1.9252e-05,  1.3620e-05,  ..., -2.2292e-05,
         -1.2115e-05, -1.4037e-05],
        [ 5.0942e-05,  3.7339e-05, -4.6026e-05,  ...,  3.7952e-05,
          1.9687e-05,  2.2257e-05],
        [-4.0382e-05, -3.0160e-05,  2.1443e-05,  ..., -3.4928e-05,
         -1.9163e-05, -2.1562e-05]], device='cuda:0')
Loss: 1.0513755083084106


Running epoch 0, step 655, batch 655
Sampled inputs[:2]: tensor([[    0,  2732,   413,  ...,   287,   266,  3668],
        [    0,    12,   696,  ..., 14275,  2661,  6129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8943e-04,  1.3726e-03, -7.1343e-04,  ...,  5.4879e-04,
         -5.7326e-04,  8.2035e-05],
        [-1.5855e-05, -1.1966e-05,  8.0764e-06,  ..., -1.4134e-05,
         -8.2739e-06, -9.1195e-06],
        [-2.9281e-05, -2.1994e-05,  1.5557e-05,  ..., -2.5466e-05,
         -1.3828e-05, -1.6049e-05],
        [ 4.8081e-05,  3.5163e-05, -4.4491e-05,  ...,  3.5389e-05,
          1.8309e-05,  2.0588e-05],
        [-4.6074e-05, -3.4451e-05,  2.4512e-05,  ..., -3.9905e-05,
         -2.1860e-05, -2.4647e-05]], device='cuda:0')
Loss: 1.0653934478759766
Graident accumulation at epoch 0, step 655, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0152, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7071e-05,  1.8332e-04, -1.6355e-04,  ...,  1.5525e-04,
         -1.2024e-04,  2.0672e-05],
        [-1.8749e-05, -1.2858e-05,  8.3757e-06,  ..., -1.6889e-05,
         -8.5342e-06, -1.2376e-05],
        [ 8.5730e-05,  5.3273e-05, -3.7068e-05,  ...,  6.5209e-05,
          3.2858e-05,  4.4577e-05],
        [ 4.9228e-07, -9.1160e-07, -3.3187e-06,  ..., -1.1878e-06,
          2.7866e-06, -1.9192e-06],
        [-4.5042e-05, -3.0382e-05,  1.9864e-05,  ..., -3.9695e-05,
         -1.8570e-05, -2.8383e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3492e-08, 3.6068e-08, 3.7088e-08,  ..., 1.3951e-08, 9.0493e-08,
         1.1778e-08],
        [6.2169e-11, 3.3822e-11, 5.6654e-12,  ..., 4.3639e-11, 4.3600e-12,
         1.3696e-11],
        [2.3020e-09, 1.1207e-09, 3.8339e-10,  ..., 1.6183e-09, 3.3360e-10,
         7.2652e-10],
        [6.0823e-10, 3.0230e-10, 9.4019e-11,  ..., 4.0152e-10, 1.1270e-10,
         1.6934e-10],
        [2.8465e-10, 1.5258e-10, 2.6111e-11,  ..., 2.0885e-10, 2.1449e-11,
         6.8754e-11]], device='cuda:0')
optimizer state dict: 82.0
lr: [1.6086061372297498e-05, 1.6086061372297498e-05]
scheduler_last_epoch: 82


Running epoch 0, step 656, batch 656
Sampled inputs[:2]: tensor([[   0, 1730, 2068,  ...,  445, 2704,  445],
        [   0,   12,  401,  ...,  504,  565,  590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7266e-04,  1.5441e-04, -1.0765e-04,  ...,  2.1781e-04,
         -2.2488e-04,  9.0271e-05],
        [-1.9968e-06, -1.6168e-06,  1.0878e-06,  ..., -1.6987e-06,
         -1.0431e-06, -1.0505e-06],
        [-3.7998e-06, -3.0398e-06,  2.1309e-06,  ..., -3.1441e-06,
         -1.8105e-06, -1.8850e-06],
        [-2.9951e-06, -2.3991e-06,  1.6838e-06,  ..., -2.5034e-06,
         -1.4231e-06, -1.5348e-06],
        [-5.6922e-06, -4.5300e-06,  3.2037e-06,  ..., -4.6790e-06,
         -2.7120e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0797169208526611


Running epoch 0, step 657, batch 657
Sampled inputs[:2]: tensor([[    0,    14,  3228,  ..., 13747,   287, 20295],
        [    0,   278,  6318,  ...,   458,    17,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6641e-04,  3.1723e-04, -1.9506e-04,  ...,  4.3005e-04,
         -3.3786e-04,  3.6680e-05],
        [-3.9637e-06, -3.2336e-06,  2.1979e-06,  ..., -3.3677e-06,
         -1.9521e-06, -2.0862e-06],
        [ 2.4121e-04,  2.3496e-04, -1.7289e-04,  ...,  2.5963e-04,
          1.4345e-04,  6.8729e-05],
        [-5.9903e-06, -4.8578e-06,  3.4422e-06,  ..., -5.0366e-06,
         -2.7046e-06, -3.0920e-06],
        [-1.1444e-05, -9.2387e-06,  6.5714e-06,  ..., -9.5069e-06,
         -5.2154e-06, -5.5432e-06]], device='cuda:0')
Loss: 1.0694011449813843


Running epoch 0, step 658, batch 658
Sampled inputs[:2]: tensor([[   0,  278, 9939,  ..., 1238,   14,  445],
        [   0, 7111,  409,  ..., 1908, 1260,  883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2511e-04,  4.5431e-04, -1.9350e-04,  ...,  4.4231e-04,
         -3.8659e-04,  1.0364e-04],
        [-5.9903e-06, -4.7982e-06,  3.3081e-06,  ..., -5.0962e-06,
         -2.9951e-06, -3.1665e-06],
        [ 2.3730e-04,  2.3196e-04, -1.7068e-04,  ...,  2.5638e-04,
          1.4163e-04,  6.6747e-05],
        [-9.0748e-06, -7.2271e-06,  5.1931e-06,  ..., -7.6443e-06,
         -4.1500e-06, -4.7088e-06],
        [-1.7315e-05, -1.3739e-05,  9.9242e-06,  ..., -1.4395e-05,
         -7.9721e-06, -8.4490e-06]], device='cuda:0')
Loss: 1.090345859527588


Running epoch 0, step 659, batch 659
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   508,  1853,    14],
        [    0,  1136,   944,  ...,   401, 13771,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9791e-04,  5.4718e-04, -1.7705e-04,  ...,  5.5731e-04,
         -2.7629e-04,  2.4369e-04],
        [-7.8082e-06, -6.3181e-06,  4.2915e-06,  ..., -6.7949e-06,
         -4.0904e-06, -4.2841e-06],
        [ 2.3385e-04,  2.2915e-04, -1.6865e-04,  ...,  2.5334e-04,
          1.3988e-04,  6.4862e-05],
        [-1.1846e-05, -9.4771e-06,  6.7949e-06,  ..., -1.0133e-05,
         -5.5879e-06, -6.3106e-06],
        [-2.2590e-05, -1.8001e-05,  1.3039e-05,  ..., -1.8984e-05,
         -1.0639e-05, -1.1206e-05]], device='cuda:0')
Loss: 1.0538568496704102


Running epoch 0, step 660, batch 660
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0, 10064,   768,  ...,   266,  2816,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8979e-04,  5.2957e-04, -2.5414e-04,  ...,  5.7607e-04,
         -3.7456e-04,  2.1373e-04],
        [-9.6187e-06, -7.8529e-06,  5.3123e-06,  ..., -8.5086e-06,
         -5.0962e-06, -5.3868e-06],
        [ 2.3049e-04,  2.2636e-04, -1.6664e-04,  ...,  2.5033e-04,
          1.3832e-04,  6.3022e-05],
        [-1.4618e-05, -1.1787e-05,  8.4341e-06,  ..., -1.2666e-05,
         -6.9067e-06, -7.9200e-06],
        [-2.7537e-05, -2.2084e-05,  1.6019e-05,  ..., -2.3395e-05,
         -1.2949e-05, -1.3813e-05]], device='cuda:0')
Loss: 1.0706417560577393


Running epoch 0, step 661, batch 661
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,   360,   259,  ...,    12,   358,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2590e-04,  5.7551e-04, -2.7850e-04,  ...,  6.8930e-04,
         -5.1731e-04,  3.1271e-04],
        [-1.1660e-05, -9.4324e-06,  6.4149e-06,  ..., -1.0237e-05,
         -5.8748e-06, -6.3479e-06],
        [ 2.2668e-04,  2.2338e-04, -1.6450e-04,  ...,  2.4708e-04,
          1.3692e-04,  6.1286e-05],
        [-1.7732e-05, -1.4201e-05,  1.0192e-05,  ..., -1.5318e-05,
         -8.0019e-06, -9.3877e-06],
        [-3.3498e-05, -2.6733e-05,  1.9386e-05,  ..., -2.8431e-05,
         -1.5214e-05, -1.6436e-05]], device='cuda:0')
Loss: 1.0801030397415161


Running epoch 0, step 662, batch 662
Sampled inputs[:2]: tensor([[   0,   11,  360,  ..., 4524, 1553,  401],
        [   0,  726, 3979,  ...,   27, 2085,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4637e-04,  5.9759e-04, -2.9805e-04,  ...,  6.2856e-04,
         -6.4372e-04,  4.1308e-04],
        [-1.3657e-05, -1.1124e-05,  7.5772e-06,  ..., -1.1921e-05,
         -6.7316e-06, -7.3239e-06],
        [ 2.2284e-04,  2.2015e-04, -1.6220e-04,  ...,  2.4390e-04,
          1.3541e-04,  5.9483e-05],
        [-2.0757e-05, -1.6749e-05,  1.2003e-05,  ..., -1.7852e-05,
         -9.1791e-06, -1.0863e-05],
        [-3.9250e-05, -3.1561e-05,  2.2829e-05,  ..., -3.3170e-05,
         -1.7494e-05, -1.9073e-05]], device='cuda:0')
Loss: 1.073149561882019


Running epoch 0, step 663, batch 663
Sampled inputs[:2]: tensor([[    0,   352,   721,  ...,   634, 17642,   278],
        [    0,   292, 23950,  ...,  9305,   287,  4401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0797e-04,  4.9478e-04, -3.4230e-04,  ...,  6.1001e-04,
         -5.8421e-04,  4.5049e-04],
        [-1.5624e-05, -1.2711e-05,  8.6948e-06,  ..., -1.3605e-05,
         -7.4953e-06, -8.3223e-06],
        [ 2.1899e-04,  2.1705e-04, -1.5995e-04,  ...,  2.4066e-04,
          1.3406e-04,  5.7620e-05],
        [-2.3901e-05, -1.9267e-05,  1.3843e-05,  ..., -2.0519e-05,
         -1.0274e-05, -1.2435e-05],
        [-4.5121e-05, -3.6269e-05,  2.6271e-05,  ..., -3.8087e-05,
         -1.9580e-05, -2.1830e-05]], device='cuda:0')
Loss: 1.0615270137786865
Graident accumulation at epoch 0, step 663, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0152, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5433e-05,  2.1447e-04, -1.8143e-04,  ...,  2.0073e-04,
         -1.6664e-04,  6.3654e-05],
        [-1.8437e-05, -1.2844e-05,  8.4076e-06,  ..., -1.6561e-05,
         -8.4303e-06, -1.1971e-05],
        [ 9.9056e-05,  6.9651e-05, -4.9357e-05,  ...,  8.2755e-05,
          4.2979e-05,  4.5882e-05],
        [-1.9471e-06, -2.7472e-06, -1.6025e-06,  ..., -3.1209e-06,
          1.4805e-06, -2.9708e-06],
        [-4.5050e-05, -3.0970e-05,  2.0505e-05,  ..., -3.9534e-05,
         -1.8671e-05, -2.7727e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3818e-08, 3.6277e-08, 3.7168e-08,  ..., 1.4309e-08, 9.0744e-08,
         1.1969e-08],
        [6.2351e-11, 3.3950e-11, 5.7353e-12,  ..., 4.3781e-11, 4.4118e-12,
         1.3752e-11],
        [2.3477e-09, 1.1667e-09, 4.0859e-10,  ..., 1.6746e-09, 3.5124e-10,
         7.2912e-10],
        [6.0820e-10, 3.0237e-10, 9.4116e-11,  ..., 4.0154e-10, 1.1269e-10,
         1.6932e-10],
        [2.8640e-10, 1.5374e-10, 2.6775e-11,  ..., 2.1009e-10, 2.1811e-11,
         6.9162e-11]], device='cuda:0')
optimizer state dict: 83.0
lr: [1.5987512487190642e-05, 1.5987512487190642e-05]
scheduler_last_epoch: 83


Running epoch 0, step 664, batch 664
Sampled inputs[:2]: tensor([[    0,   287,  4170,  ...,    27, 12612,    13],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2341e-05,  1.8947e-04,  6.4978e-05,  ...,  1.4410e-04,
         -2.6929e-04,  4.7730e-05],
        [-2.0266e-06, -1.6540e-06,  1.0952e-06,  ..., -1.7807e-06,
         -1.0580e-06, -1.1474e-06],
        [-3.7700e-06, -3.0845e-06,  2.1309e-06,  ..., -3.2187e-06,
         -1.8105e-06, -1.9968e-06],
        [-3.0547e-06, -2.4885e-06,  1.7211e-06,  ..., -2.6524e-06,
         -1.4752e-06, -1.6913e-06],
        [-5.3942e-06, -4.3809e-06,  3.0398e-06,  ..., -4.5896e-06,
         -2.5928e-06, -2.7567e-06]], device='cuda:0')
Loss: 1.0755568742752075


Running epoch 0, step 665, batch 665
Sampled inputs[:2]: tensor([[   0, 7432,  287,  ...,   12,  461, 2652],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1390e-04,  2.2781e-04,  1.4696e-04,  ...,  1.3152e-04,
         -1.0253e-04,  2.0084e-04],
        [-4.1574e-06, -3.4049e-06,  2.2203e-06,  ..., -3.5912e-06,
         -2.0862e-06, -2.2724e-06],
        [-7.7039e-06, -6.3181e-06,  4.2915e-06,  ..., -6.4969e-06,
         -3.5465e-06, -3.9637e-06],
        [-6.1989e-06, -5.0515e-06,  3.4422e-06,  ..., -5.2899e-06,
         -2.8685e-06, -3.3230e-06],
        [-1.1086e-05, -9.0301e-06,  6.1691e-06,  ..., -9.2983e-06,
         -5.1260e-06, -5.4836e-06]], device='cuda:0')
Loss: 1.0967392921447754


Running epoch 0, step 666, batch 666
Sampled inputs[:2]: tensor([[    0,   587,   292,  ...,    12,   287,  2261],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0975e-05,  2.9445e-04,  2.1054e-04,  ...,  6.8589e-05,
         -9.8793e-05,  3.1750e-04],
        [-6.0946e-06, -5.0664e-06,  3.2708e-06,  ..., -5.3197e-06,
         -3.1069e-06, -3.4869e-06],
        [-1.1280e-05, -9.3281e-06,  6.3479e-06,  ..., -9.5218e-06,
         -5.1782e-06, -5.9754e-06],
        [-9.1046e-06, -7.4953e-06,  5.1111e-06,  ..., -7.7933e-06,
         -4.2245e-06, -5.0664e-06],
        [-1.6063e-05, -1.3202e-05,  9.0450e-06,  ..., -1.3500e-05,
         -7.4208e-06, -8.1509e-06]], device='cuda:0')
Loss: 1.0305204391479492


Running epoch 0, step 667, batch 667
Sampled inputs[:2]: tensor([[    0,   508, 22318,  ...,    13,  1107,  4093],
        [    0,  8450,   292,  ...,   352,   722, 37719]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0975e-05,  2.1374e-04,  1.3124e-04,  ..., -9.7799e-06,
         -6.6656e-05,  4.1169e-04],
        [-7.9423e-06, -6.6012e-06,  4.3735e-06,  ..., -6.9737e-06,
         -4.0308e-06, -4.6790e-06],
        [-1.4722e-05, -1.2159e-05,  8.4937e-06,  ..., -1.2502e-05,
         -6.6981e-06, -8.0168e-06],
        [-1.2025e-05, -9.8944e-06,  6.9365e-06,  ..., -1.0341e-05,
         -5.5060e-06, -6.8694e-06],
        [-2.0951e-05, -1.7196e-05,  1.2070e-05,  ..., -1.7673e-05,
         -9.5665e-06, -1.0908e-05]], device='cuda:0')
Loss: 1.0567468404769897


Running epoch 0, step 668, batch 668
Sampled inputs[:2]: tensor([[    0, 12919,   292,  ...,   221,   273,   298],
        [    0,     5,  4413,  ...,  9205, 16744,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3768e-05, -1.0731e-04,  3.2603e-04,  ..., -4.8931e-06,
          4.8921e-04,  6.6910e-04],
        [-9.9391e-06, -8.2776e-06,  5.4389e-06,  ..., -8.6501e-06,
         -5.0440e-06, -5.8413e-06],
        [-1.8373e-05, -1.5184e-05,  1.0565e-05,  ..., -1.5453e-05,
         -8.3297e-06, -9.9689e-06],
        [-1.4946e-05, -1.2323e-05,  8.5905e-06,  ..., -1.2740e-05,
         -6.8247e-06, -8.5086e-06],
        [-2.6226e-05, -2.1547e-05,  1.5080e-05,  ..., -2.1935e-05,
         -1.1921e-05, -1.3605e-05]], device='cuda:0')
Loss: 1.080239176750183


Running epoch 0, step 669, batch 669
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0,    13,  1320,  ...,  8686,  6851,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0215e-04, -1.0884e-04,  3.1329e-04,  ...,  2.8697e-05,
          4.8921e-04,  6.1218e-04],
        [-1.1787e-05, -9.8348e-06,  6.4597e-06,  ..., -1.0364e-05,
         -5.9865e-06, -6.9663e-06],
        [-2.1785e-05, -1.8030e-05,  1.2577e-05,  ..., -1.8448e-05,
         -9.8124e-06, -1.1824e-05],
        [-1.7852e-05, -1.4737e-05,  1.0297e-05,  ..., -1.5363e-05,
         -8.1435e-06, -1.0200e-05],
        [-3.1173e-05, -2.5630e-05,  1.7986e-05,  ..., -2.6226e-05,
         -1.4052e-05, -1.6168e-05]], device='cuda:0')
Loss: 1.0663312673568726


Running epoch 0, step 670, batch 670
Sampled inputs[:2]: tensor([[    0, 10288,   300,  ...,  5365,    12,  3539],
        [    0,  3829,   278,  ..., 11978,     9,   968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2087e-04, -1.1089e-04,  3.2749e-04,  ...,  1.6937e-04,
          3.8410e-04,  6.0664e-04],
        [-1.3843e-05, -1.1526e-05,  7.5996e-06,  ..., -1.2070e-05,
         -6.7204e-06, -8.0019e-06],
        [-2.5630e-05, -2.1175e-05,  1.4782e-05,  ..., -2.1577e-05,
         -1.1057e-05, -1.3657e-05],
        [-2.1055e-05, -1.7330e-05,  1.2144e-05,  ..., -1.7986e-05,
         -9.1717e-06, -1.1794e-05],
        [-3.6627e-05, -3.0041e-05,  2.1100e-05,  ..., -3.0637e-05,
         -1.5862e-05, -1.8656e-05]], device='cuda:0')
Loss: 1.0632680654525757


Running epoch 0, step 671, batch 671
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0,  6584,   278,  ...,  1039,   965,  1410]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4559e-04, -2.6291e-04,  3.1363e-04,  ...,  1.4869e-04,
          4.3904e-04,  7.2967e-04],
        [-1.5743e-05, -1.3053e-05,  8.7172e-06,  ..., -1.3776e-05,
         -7.7337e-06, -9.3207e-06],
        [-2.9057e-05, -2.3901e-05,  1.6898e-05,  ..., -2.4512e-05,
         -1.2621e-05, -1.5803e-05],
        [-2.4006e-05, -1.9670e-05,  1.3977e-05,  ..., -2.0549e-05,
         -1.0535e-05, -1.3746e-05],
        [-4.1574e-05, -3.3975e-05,  2.4170e-05,  ..., -3.4839e-05,
         -1.8112e-05, -2.1622e-05]], device='cuda:0')
Loss: 1.0582847595214844
Graident accumulation at epoch 0, step 671, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.6331e-05,  1.6673e-04, -1.3192e-04,  ...,  1.9552e-04,
         -1.0607e-04,  1.3026e-04],
        [-1.8167e-05, -1.2865e-05,  8.4386e-06,  ..., -1.6283e-05,
         -8.3606e-06, -1.1706e-05],
        [ 8.6245e-05,  6.0295e-05, -4.2731e-05,  ...,  7.2028e-05,
          3.7419e-05,  3.9713e-05],
        [-4.1530e-06, -4.4394e-06, -4.4549e-08,  ..., -4.8637e-06,
          2.7895e-07, -4.0483e-06],
        [-4.4702e-05, -3.1271e-05,  2.0871e-05,  ..., -3.9064e-05,
         -1.8615e-05, -2.7117e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3795e-08, 3.6310e-08, 3.7229e-08,  ..., 1.4317e-08, 9.0846e-08,
         1.2489e-08],
        [6.2537e-11, 3.4086e-11, 5.8056e-12,  ..., 4.3927e-11, 4.4672e-12,
         1.3825e-11],
        [2.3462e-09, 1.1661e-09, 4.0846e-10,  ..., 1.6735e-09, 3.5105e-10,
         7.2864e-10],
        [6.0816e-10, 3.0246e-10, 9.4218e-11,  ..., 4.0156e-10, 1.1269e-10,
         1.6934e-10],
        [2.8784e-10, 1.5474e-10, 2.7333e-11,  ..., 2.1110e-10, 2.2117e-11,
         6.9561e-11]], device='cuda:0')
optimizer state dict: 84.0
lr: [1.5888048657910018e-05, 1.5888048657910018e-05]
scheduler_last_epoch: 84


Running epoch 0, step 672, batch 672
Sampled inputs[:2]: tensor([[    0,   806,   300,  ...,   360,  4918,  1106],
        [    0, 39004,   266,  ...,   287, 21972,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3980e-04,  7.0623e-06,  3.0686e-05,  ...,  1.4699e-04,
         -3.4689e-04, -2.1697e-04],
        [-2.1011e-06, -1.7360e-06,  1.1474e-06,  ..., -1.8030e-06,
         -9.6858e-07, -1.1474e-06],
        [-3.7253e-06, -3.0547e-06,  2.1160e-06,  ..., -3.0994e-06,
         -1.5497e-06, -1.8626e-06],
        [-3.1888e-06, -2.6077e-06,  1.8179e-06,  ..., -2.6971e-06,
         -1.3486e-06, -1.6987e-06],
        [-5.2452e-06, -4.2915e-06,  2.9653e-06,  ..., -4.3213e-06,
         -2.1905e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.0609891414642334


Running epoch 0, step 673, batch 673
Sampled inputs[:2]: tensor([[    0,   292,    40,  ..., 26995,   278,   717],
        [    0,   591, 18622,  ...,   955,  6118,  9191]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6563e-04, -1.9022e-04,  2.3049e-04,  ..., -8.9901e-05,
          1.5389e-04, -8.4577e-05],
        [-4.1574e-06, -3.3602e-06,  2.2203e-06,  ..., -3.5614e-06,
         -1.9185e-06, -2.4140e-06],
        [-7.4953e-06, -5.9903e-06,  4.2021e-06,  ..., -6.1542e-06,
         -3.0324e-06, -3.9339e-06],
        [-6.3032e-06, -5.0366e-06,  3.5390e-06,  ..., -5.2750e-06,
         -2.6003e-06, -3.5241e-06],
        [-1.0431e-05, -8.3148e-06,  5.8264e-06,  ..., -8.4937e-06,
         -4.2617e-06, -5.2601e-06]], device='cuda:0')
Loss: 1.064445972442627


Running epoch 0, step 674, batch 674
Sampled inputs[:2]: tensor([[    0,  7110,   278,  ...,    66,    13,  9070],
        [    0,   266, 28695,  ...,   278,   266,  6087]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9829e-04, -1.2303e-04,  2.1905e-04,  ..., -6.3230e-05,
          1.7515e-04, -1.5152e-04],
        [-6.3479e-06, -5.1111e-06,  3.3453e-06,  ..., -5.3048e-06,
         -2.7306e-06, -3.4347e-06],
        [-1.1489e-05, -9.1493e-06,  6.3479e-06,  ..., -9.2536e-06,
         -4.3660e-06, -5.6550e-06],
        [-9.5814e-06, -7.6443e-06,  5.3048e-06,  ..., -7.8529e-06,
         -3.6806e-06, -5.0217e-06],
        [-1.6153e-05, -1.2845e-05,  8.9109e-06,  ..., -1.2934e-05,
         -6.2138e-06, -7.6592e-06]], device='cuda:0')
Loss: 1.0650230646133423


Running epoch 0, step 675, batch 675
Sampled inputs[:2]: tensor([[    0,  3658,   271,  ...,   278,   970,    12],
        [    0, 47354,  5923,  ...,   266, 14679,  8137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3562e-04,  1.3931e-05,  4.2786e-04,  ...,  7.1350e-05,
          2.0910e-04,  1.0032e-05],
        [-8.4788e-06, -6.8322e-06,  4.4256e-06,  ..., -7.0482e-06,
         -3.7067e-06, -4.6939e-06],
        [-1.5423e-05, -1.2293e-05,  8.4490e-06,  ..., -1.2353e-05,
         -5.9828e-06, -7.8157e-06],
        [-1.2770e-05, -1.0177e-05,  7.0035e-06,  ..., -1.0401e-05,
         -4.9844e-06, -6.8545e-06],
        [-2.1577e-05, -1.7136e-05,  1.1787e-05,  ..., -1.7166e-05,
         -8.4639e-06, -1.0505e-05]], device='cuda:0')
Loss: 1.0565563440322876


Running epoch 0, step 676, batch 676
Sampled inputs[:2]: tensor([[   0, 3087,  342,  ...,   14,  381, 1416],
        [   0,  391, 1351,  ...,   13,   40,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0506e-04,  4.5711e-05,  3.6229e-04,  ...,  1.0573e-04,
          5.5249e-05, -2.7878e-05],
        [-1.0714e-05, -8.5905e-06,  5.5954e-06,  ..., -8.8140e-06,
         -4.5374e-06, -5.7891e-06],
        [-1.9535e-05, -1.5527e-05,  1.0654e-05,  ..., -1.5557e-05,
         -7.4133e-06, -9.7230e-06],
        [-1.6168e-05, -1.2845e-05,  8.8364e-06,  ..., -1.3068e-05,
         -6.1393e-06, -8.5086e-06],
        [-2.7388e-05, -2.1696e-05,  1.4901e-05,  ..., -2.1696e-05,
         -1.0535e-05, -1.3113e-05]], device='cuda:0')
Loss: 1.0684092044830322


Running epoch 0, step 677, batch 677
Sampled inputs[:2]: tensor([[    0,    30,  1869,  ...,  4998, 44266,    12],
        [    0,  2939,    14,  ...,  1702,  1481,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6108e-04, -7.9384e-05,  4.9914e-04,  ..., -6.8184e-05,
          2.0830e-04,  1.4371e-04],
        [-1.2800e-05, -1.0349e-05,  6.6757e-06,  ..., -1.0595e-05,
         -5.5134e-06, -6.9812e-06],
        [-2.3320e-05, -1.8626e-05,  1.2755e-05,  ..., -1.8612e-05,
         -8.9705e-06, -1.1660e-05],
        [-1.9342e-05, -1.5453e-05,  1.0580e-05,  ..., -1.5676e-05,
         -7.4580e-06, -1.0230e-05],
        [-3.2634e-05, -2.5988e-05,  1.7822e-05,  ..., -2.5928e-05,
         -1.2696e-05, -1.5676e-05]], device='cuda:0')
Loss: 1.0691407918930054


Running epoch 0, step 678, batch 678
Sampled inputs[:2]: tensor([[   0, 7963,   17,  ...,   50,   13,   18],
        [   0,   18,  998,  ..., 5322,  504,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4401e-04,  5.0380e-05,  5.1301e-04,  ...,  5.2748e-05,
          1.1255e-04,  2.4071e-04],
        [-1.4976e-05, -1.2100e-05,  7.8529e-06,  ..., -1.2398e-05,
         -6.4224e-06, -8.0466e-06],
        [-2.7284e-05, -2.1830e-05,  1.4976e-05,  ..., -2.1860e-05,
         -1.0513e-05, -1.3493e-05],
        [-2.2650e-05, -1.8120e-05,  1.2442e-05,  ..., -1.8403e-05,
         -8.7246e-06, -1.1839e-05],
        [-3.8177e-05, -3.0488e-05,  2.0921e-05,  ..., -3.0458e-05,
         -1.4886e-05, -1.8165e-05]], device='cuda:0')
Loss: 1.073960542678833


Running epoch 0, step 679, batch 679
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0,   266,  1176,  ...,   199, 17791,  3662]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4449e-04,  2.0880e-04,  5.1301e-04,  ...,  5.8523e-05,
          2.9332e-04,  3.0291e-04],
        [-1.7151e-05, -1.3880e-05,  9.0301e-06,  ..., -1.4134e-05,
         -7.4059e-06, -9.3579e-06],
        [-3.1367e-05, -2.5094e-05,  1.7300e-05,  ..., -2.4974e-05,
         -1.2137e-05, -1.5728e-05],
        [-2.5898e-05, -2.0742e-05,  1.4298e-05,  ..., -2.0921e-05,
         -1.0028e-05, -1.3709e-05],
        [-4.3631e-05, -3.4809e-05,  2.4036e-05,  ..., -3.4600e-05,
         -1.7077e-05, -2.1026e-05]], device='cuda:0')
Loss: 1.0227420330047607
Graident accumulation at epoch 0, step 679, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0287, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0151, -0.0278,  ...,  0.0286, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2496e-06,  1.7094e-04, -6.7429e-05,  ...,  1.8182e-04,
         -6.6129e-05,  1.4752e-04],
        [-1.8066e-05, -1.2966e-05,  8.4977e-06,  ..., -1.6068e-05,
         -8.2652e-06, -1.1471e-05],
        [ 7.4484e-05,  5.1756e-05, -3.6728e-05,  ...,  6.2328e-05,
          3.2463e-05,  3.4169e-05],
        [-6.3275e-06, -6.0697e-06,  1.3897e-06,  ..., -6.4695e-06,
         -7.5179e-07, -5.0144e-06],
        [-4.4595e-05, -3.1625e-05,  2.1188e-05,  ..., -3.8618e-05,
         -1.8461e-05, -2.6508e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3772e-08, 3.6317e-08, 3.7455e-08,  ..., 1.4306e-08, 9.0841e-08,
         1.2569e-08],
        [6.2768e-11, 3.4245e-11, 5.8813e-12,  ..., 4.4083e-11, 4.5176e-12,
         1.3898e-11],
        [2.3448e-09, 1.1655e-09, 4.0835e-10,  ..., 1.6725e-09, 3.5085e-10,
         7.2816e-10],
        [6.0823e-10, 3.0259e-10, 9.4328e-11,  ..., 4.0160e-10, 1.1268e-10,
         1.6936e-10],
        [2.8946e-10, 1.5580e-10, 2.7883e-11,  ..., 2.1208e-10, 2.2387e-11,
         6.9933e-11]], device='cuda:0')
optimizer state dict: 85.0
lr: [1.5787685083396957e-05, 1.5787685083396957e-05]
scheduler_last_epoch: 85


Running epoch 0, step 680, batch 680
Sampled inputs[:2]: tensor([[   0,   12, 1197,  ...,  516, 1136, 9774],
        [   0, 6795, 1728,  ...,  578,   19,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9431e-05,  1.0724e-04,  5.4807e-05,  ..., -2.6987e-06,
         -1.0287e-04, -8.9348e-05],
        [-2.2054e-06, -1.6242e-06,  1.1176e-06,  ..., -1.7956e-06,
         -9.3877e-07, -1.2293e-06],
        [-3.7849e-06, -2.8014e-06,  2.0117e-06,  ..., -2.9802e-06,
         -1.4529e-06, -1.9372e-06],
        [-3.4273e-06, -2.5183e-06,  1.8254e-06,  ..., -2.7418e-06,
         -1.3337e-06, -1.8701e-06],
        [-5.2750e-06, -3.9041e-06,  2.8163e-06,  ..., -4.1425e-06,
         -2.0415e-06, -2.6226e-06]], device='cuda:0')
Loss: 1.0637547969818115


Running epoch 0, step 681, batch 681
Sampled inputs[:2]: tensor([[    0,  4601,   328,  ..., 10258,  2282,    12],
        [    0,   409,  3669,  ...,    12,   374,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4091e-04,  7.1762e-05,  6.0883e-05,  ..., -1.1513e-04,
         -7.2070e-05, -4.5515e-05],
        [-4.5002e-06, -3.4198e-06,  2.2650e-06,  ..., -3.6135e-06,
         -1.8626e-06, -2.4736e-06],
        [-7.7784e-06, -5.9158e-06,  4.0829e-06,  ..., -6.0648e-06,
         -2.9579e-06, -3.9488e-06],
        [-6.7651e-06, -5.1111e-06,  3.5614e-06,  ..., -5.3495e-06,
         -2.5779e-06, -3.6508e-06],
        [-1.0759e-05, -8.1956e-06,  5.6624e-06,  ..., -8.3745e-06,
         -4.1425e-06, -5.2601e-06]], device='cuda:0')
Loss: 1.0505070686340332


Running epoch 0, step 682, batch 682
Sampled inputs[:2]: tensor([[   0, 2192, 3182,  ..., 1445, 1531,  300],
        [   0, 3448,  278,  ...,  380,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5337e-04,  3.4552e-05,  4.7707e-05,  ..., -6.9998e-05,
          6.9395e-05, -5.4059e-05],
        [-6.8247e-06, -5.1856e-06,  3.4720e-06,  ..., -5.4166e-06,
         -2.6636e-06, -3.5241e-06],
        [-1.1861e-05, -9.0152e-06,  6.2436e-06,  ..., -9.1791e-06,
         -4.2915e-06, -5.7071e-06],
        [-1.0222e-05, -7.7188e-06,  5.4166e-06,  ..., -8.0168e-06,
         -3.6806e-06, -5.2080e-06],
        [-1.6481e-05, -1.2517e-05,  8.6874e-06,  ..., -1.2755e-05,
         -6.0797e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.0714144706726074


Running epoch 0, step 683, batch 683
Sampled inputs[:2]: tensor([[    0, 10251,   278,  ...,   278,   319,    13],
        [    0,   221,   527,  ...,   298,   335,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2952e-04, -6.1740e-05,  1.3800e-04,  ..., -2.6948e-04,
          1.3051e-04, -5.1786e-05],
        [-9.0152e-06, -6.8769e-06,  4.5970e-06,  ..., -7.2196e-06,
         -3.6843e-06, -4.7237e-06],
        [-1.5676e-05, -1.1951e-05,  8.2999e-06,  ..., -1.2219e-05,
         -5.9307e-06, -7.6294e-06],
        [-1.3486e-05, -1.0207e-05,  7.1749e-06,  ..., -1.0639e-05,
         -5.0813e-06, -6.9514e-06],
        [-2.1666e-05, -1.6510e-05,  1.1474e-05,  ..., -1.6868e-05,
         -8.3148e-06, -1.0118e-05]], device='cuda:0')
Loss: 1.059516191482544


Running epoch 0, step 684, batch 684
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   271,   266,  ...,   365,  2463,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2592e-04, -4.1523e-06,  2.3471e-04,  ..., -2.0779e-04,
          5.2226e-05, -1.8744e-04],
        [-1.1399e-05, -8.6799e-06,  5.6773e-06,  ..., -9.0823e-06,
         -4.7423e-06, -6.0797e-06],
        [-1.9878e-05, -1.5125e-05,  1.0312e-05,  ..., -1.5423e-05,
         -7.6890e-06, -9.8646e-06],
        [-1.6809e-05, -1.2696e-05,  8.7619e-06,  ..., -1.3188e-05,
         -6.4448e-06, -8.7917e-06],
        [-2.7299e-05, -2.0742e-05,  1.4186e-05,  ..., -2.1130e-05,
         -1.0669e-05, -1.2994e-05]], device='cuda:0')
Loss: 1.0460387468338013


Running epoch 0, step 685, batch 685
Sampled inputs[:2]: tensor([[   0, 4485,  741,  ...,  292,  221,  341],
        [   0,  527, 2811,  ...,  287, 1288,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4134e-04, -2.5929e-04,  4.7576e-04,  ..., -4.3318e-04,
          7.8359e-05, -3.8751e-05],
        [-1.3754e-05, -1.0371e-05,  6.7949e-06,  ..., -1.0990e-05,
         -5.8375e-06, -7.3835e-06],
        [-2.3812e-05, -1.7986e-05,  1.2279e-05,  ..., -1.8537e-05,
         -9.3803e-06, -1.1891e-05],
        [-2.0206e-05, -1.5140e-05,  1.0461e-05,  ..., -1.5914e-05,
         -7.9200e-06, -1.0639e-05],
        [-3.2812e-05, -2.4766e-05,  1.6943e-05,  ..., -2.5481e-05,
         -1.3068e-05, -1.5721e-05]], device='cuda:0')
Loss: 1.0651129484176636


Running epoch 0, step 686, batch 686
Sampled inputs[:2]: tensor([[    0,  1590,  2140,  ...,   287,  5342,  1319],
        [    0,   287,  6761,  ...,  1918, 33351,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8798e-04, -1.9295e-04,  3.0565e-04,  ..., -3.7010e-04,
          3.2815e-05, -5.0741e-05],
        [-1.6049e-05, -1.2144e-05,  7.9423e-06,  ..., -1.2912e-05,
         -6.9328e-06, -8.6278e-06],
        [-2.7746e-05, -2.1011e-05,  1.4320e-05,  ..., -2.1711e-05,
         -1.1101e-05, -1.3858e-05],
        [-2.3544e-05, -1.7703e-05,  1.2204e-05,  ..., -1.8656e-05,
         -9.3877e-06, -1.2413e-05],
        [-3.8356e-05, -2.9027e-05,  1.9819e-05,  ..., -2.9922e-05,
         -1.5512e-05, -1.8388e-05]], device='cuda:0')
Loss: 1.074975609779358


Running epoch 0, step 687, batch 687
Sampled inputs[:2]: tensor([[   0, 1057,   14,  ...,   14, 4735,   13],
        [   0,  266, 2604,  ...,  278, 4035, 4165]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3846e-04, -3.0097e-04,  4.4814e-04,  ..., -5.0827e-04,
          5.8318e-05, -1.8134e-05],
        [-1.8388e-05, -1.3918e-05,  9.1344e-06,  ..., -1.4752e-05,
         -7.9982e-06, -9.8646e-06],
        [-3.1829e-05, -2.4110e-05,  1.6451e-05,  ..., -2.4885e-05,
         -1.2867e-05, -1.5944e-05],
        [-2.6897e-05, -2.0236e-05,  1.3962e-05,  ..., -2.1264e-05,
         -1.0811e-05, -1.4186e-05],
        [-4.3988e-05, -3.3289e-05,  2.2754e-05,  ..., -3.4273e-05,
         -1.7971e-05, -2.1130e-05]], device='cuda:0')
Loss: 1.0822052955627441
Graident accumulation at epoch 0, step 687, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0287, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0151, -0.0278,  ...,  0.0286, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.5521e-05,  1.2375e-04, -1.5872e-05,  ...,  1.1281e-04,
         -5.3685e-05,  1.3096e-04],
        [-1.8098e-05, -1.3061e-05,  8.5614e-06,  ..., -1.5936e-05,
         -8.2385e-06, -1.1310e-05],
        [ 6.3852e-05,  4.4170e-05, -3.1410e-05,  ...,  5.3606e-05,
          2.7930e-05,  2.9158e-05],
        [-8.3844e-06, -7.4863e-06,  2.6469e-06,  ..., -7.9489e-06,
         -1.7577e-06, -5.9316e-06],
        [-4.4534e-05, -3.1791e-05,  2.1344e-05,  ..., -3.8184e-05,
         -1.8412e-05, -2.5970e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4136e-08, 3.6371e-08, 3.7619e-08,  ..., 1.4550e-08, 9.0753e-08,
         1.2557e-08],
        [6.3044e-11, 3.4404e-11, 5.9589e-12,  ..., 4.4256e-11, 4.5770e-12,
         1.3982e-11],
        [2.3435e-09, 1.1650e-09, 4.0822e-10,  ..., 1.6714e-09, 3.5066e-10,
         7.2768e-10],
        [6.0834e-10, 3.0269e-10, 9.4428e-11,  ..., 4.0165e-10, 1.1268e-10,
         1.6939e-10],
        [2.9110e-10, 1.5675e-10, 2.8373e-11,  ..., 2.1305e-10, 2.2687e-11,
         7.0310e-11]], device='cuda:0')
optimizer state dict: 86.0
lr: [1.5686437100081734e-05, 1.5686437100081734e-05]
scheduler_last_epoch: 86


Running epoch 0, step 688, batch 688
Sampled inputs[:2]: tensor([[    0,   266,  1336,  ...,  1841,  9705,  1219],
        [    0, 24414,  4865,  ...,  8720,   344,  1566]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7904e-04,  2.5488e-04, -2.5915e-04,  ...,  2.2183e-04,
         -2.1017e-04, -1.6153e-04],
        [-2.1458e-06, -1.6242e-06,  1.2368e-06,  ..., -1.7509e-06,
         -8.6427e-07, -1.2442e-06],
        [-3.6657e-06, -2.7418e-06,  2.1756e-06,  ..., -2.8312e-06,
         -1.3039e-06, -1.8999e-06],
        [-3.3379e-06, -2.4885e-06,  1.9968e-06,  ..., -2.6375e-06,
         -1.1995e-06, -1.8850e-06],
        [-5.0664e-06, -3.7849e-06,  2.9802e-06,  ..., -3.9041e-06,
         -1.8254e-06, -2.5034e-06]], device='cuda:0')
Loss: 1.0254658460617065


Running epoch 0, step 689, batch 689
Sampled inputs[:2]: tensor([[    0,  4441,  1821,  ...,   642,  2310,    14],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5372e-05,  3.2648e-04, -4.3063e-04,  ...,  2.6152e-04,
         -3.6739e-04, -3.0797e-04],
        [-4.4852e-06, -3.4198e-06,  2.4661e-06,  ..., -3.5986e-06,
         -1.6727e-06, -2.3916e-06],
        [-7.8976e-06, -5.9754e-06,  4.4554e-06,  ..., -6.1095e-06,
         -2.6971e-06, -3.8818e-06],
        [-6.8247e-06, -5.1558e-06,  3.8967e-06,  ..., -5.3644e-06,
         -2.3171e-06, -3.5912e-06],
        [-1.0818e-05, -8.1956e-06,  6.0797e-06,  ..., -8.3745e-06,
         -3.7625e-06, -5.1111e-06]], device='cuda:0')
Loss: 1.0538371801376343


Running epoch 0, step 690, batch 690
Sampled inputs[:2]: tensor([[    0,  2086, 10663,  ...,   271,   266,  6927],
        [    0,  2645,    12,  ...,     5,  1239,  7200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0696e-05,  4.0957e-04, -3.7382e-04,  ...,  3.1242e-04,
         -4.9207e-04, -2.0864e-04],
        [-6.9737e-06, -5.3048e-06,  3.7178e-06,  ..., -5.5358e-06,
         -2.6934e-06, -3.6210e-06],
        [ 7.5365e-05,  7.0423e-05, -5.0110e-05,  ...,  5.8319e-05,
          6.2889e-05,  9.9291e-06],
        [-1.0356e-05, -7.8380e-06,  5.7369e-06,  ..., -8.1062e-06,
         -3.6955e-06, -5.3272e-06],
        [-1.6689e-05, -1.2726e-05,  9.0897e-06,  ..., -1.2964e-05,
         -6.1765e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.0717154741287231


Running epoch 0, step 691, batch 691
Sampled inputs[:2]: tensor([[   0, 3615,   16,  ..., 2140, 1098,  352],
        [   0,  475,  266,  ...,  843,  287, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1800e-04,  3.5682e-04, -5.4409e-04,  ...,  3.5529e-04,
         -5.6802e-04, -2.8024e-04],
        [-9.4026e-06, -7.1451e-06,  4.9025e-06,  ..., -7.4357e-06,
         -3.7216e-06, -4.8280e-06],
        [ 7.1192e-05,  6.7264e-05, -4.7994e-05,  ...,  5.5131e-05,
          6.1242e-05,  7.9920e-06],
        [-1.3798e-05, -1.0431e-05,  7.4804e-06,  ..., -1.0759e-05,
         -5.0515e-06, -7.0259e-06],
        [-2.2531e-05, -1.7136e-05,  1.2040e-05,  ..., -1.7405e-05,
         -8.5011e-06, -1.0416e-05]], device='cuda:0')
Loss: 1.0716158151626587


Running epoch 0, step 692, batch 692
Sampled inputs[:2]: tensor([[    0,   266,  1211,  ...,  1336,   694,   516],
        [    0, 49141,    14,  ...,   342,   259,  1943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8382e-05,  5.0360e-04, -6.1708e-04,  ...,  5.4746e-04,
         -9.1301e-04, -5.7854e-04],
        [-1.1787e-05, -8.9109e-06,  6.1467e-06,  ..., -9.3877e-06,
         -4.8317e-06, -6.1393e-06],
        [ 6.7258e-05,  6.4343e-05, -4.5863e-05,  ...,  5.2001e-05,
          5.9536e-05,  6.0101e-06],
        [-1.7241e-05, -1.2964e-05,  9.3505e-06,  ..., -1.3545e-05,
         -6.5714e-06, -8.8960e-06],
        [-2.8044e-05, -2.1249e-05,  1.5005e-05,  ..., -2.1756e-05,
         -1.0915e-05, -1.3068e-05]], device='cuda:0')
Loss: 1.0769673585891724


Running epoch 0, step 693, batch 693
Sampled inputs[:2]: tensor([[   0,    9,  870,  ..., 2671,  965, 3229],
        [   0,  367, 2063,  ..., 3022,  221,  733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9193e-05,  6.1203e-04, -6.2189e-04,  ...,  6.4824e-04,
         -9.4858e-04, -5.7690e-04],
        [-1.4305e-05, -1.0796e-05,  7.3612e-06,  ..., -1.1258e-05,
         -5.8003e-06, -7.3761e-06],
        [ 6.2967e-05,  6.1110e-05, -4.3717e-05,  ...,  4.8842e-05,
          5.7957e-05,  3.9984e-06],
        [-2.0757e-05, -1.5587e-05,  1.1101e-05,  ..., -1.6138e-05,
         -7.8380e-06, -1.0617e-05],
        [-3.4124e-05, -2.5809e-05,  1.8016e-05,  ..., -2.6226e-05,
         -1.3180e-05, -1.5825e-05]], device='cuda:0')
Loss: 1.06279456615448


Running epoch 0, step 694, batch 694
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,  7428,  1566,  ...,   199,  1726,  5647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6644e-05,  8.0173e-04, -8.0097e-04,  ...,  7.8828e-04,
         -1.1767e-03, -5.8855e-04],
        [-1.6794e-05, -1.2673e-05,  8.6799e-06,  ..., -1.3165e-05,
         -6.7353e-06, -8.5011e-06],
        [ 5.8645e-05,  5.7861e-05, -4.1363e-05,  ...,  4.5594e-05,
          5.6422e-05,  2.1507e-06],
        [-2.4334e-05, -1.8254e-05,  1.3053e-05,  ..., -1.8835e-05,
         -9.0823e-06, -1.2234e-05],
        [-4.0025e-05, -3.0249e-05,  2.1219e-05,  ..., -3.0637e-05,
         -1.5311e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.052899956703186


Running epoch 0, step 695, batch 695
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,  1640,  1103,  ...,   685,  1478,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5196e-04,  9.0526e-04, -9.7653e-04,  ...,  7.7439e-04,
         -1.3206e-03, -7.2773e-04],
        [-1.9193e-05, -1.4462e-05,  9.8869e-06,  ..., -1.5087e-05,
         -7.7933e-06, -9.7901e-06],
        [ 5.4592e-05,  5.4807e-05, -3.9232e-05,  ...,  4.2450e-05,
          5.4738e-05,  1.5394e-07],
        [-2.7657e-05, -2.0728e-05,  1.4804e-05,  ..., -2.1458e-05,
         -1.0461e-05, -1.3985e-05],
        [-4.5717e-05, -3.4511e-05,  2.4199e-05,  ..., -3.5018e-05,
         -1.7695e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.0558257102966309
Graident accumulation at epoch 0, step 695, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0230, -0.0195],
        [ 0.0287, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0151, -0.0278,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.4774e-05,  2.0190e-04, -1.1194e-04,  ...,  1.7897e-04,
         -1.8038e-04,  4.5087e-05],
        [-1.8207e-05, -1.3201e-05,  8.6939e-06,  ..., -1.5851e-05,
         -8.1939e-06, -1.1158e-05],
        [ 6.2926e-05,  4.5233e-05, -3.2192e-05,  ...,  5.2491e-05,
          3.0611e-05,  2.6257e-05],
        [-1.0312e-05, -8.8104e-06,  3.8627e-06,  ..., -9.2998e-06,
         -2.6280e-06, -6.7369e-06],
        [-4.4653e-05, -3.2063e-05,  2.1630e-05,  ..., -3.7867e-05,
         -1.8341e-05, -2.5465e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4115e-08, 3.7154e-08, 3.8535e-08,  ..., 1.5135e-08, 9.2407e-08,
         1.3074e-08],
        [6.3349e-11, 3.4579e-11, 6.0507e-12,  ..., 4.4440e-11, 4.6332e-12,
         1.4064e-11],
        [2.3441e-09, 1.1668e-09, 4.0935e-10,  ..., 1.6716e-09, 3.5331e-10,
         7.2695e-10],
        [6.0850e-10, 3.0282e-10, 9.4553e-11,  ..., 4.0171e-10, 1.1268e-10,
         1.6942e-10],
        [2.9290e-10, 1.5778e-10, 2.8930e-11,  ..., 2.1406e-10, 2.2978e-11,
         7.0677e-11]], device='cuda:0')
optimizer state dict: 87.0
lr: [1.5584320179540008e-05, 1.5584320179540008e-05]
scheduler_last_epoch: 87


Running epoch 0, step 696, batch 696
Sampled inputs[:2]: tensor([[    0,  2827,  5744,  ...,   365,   513,    13],
        [    0, 10893, 10997,  ...,   367,   616,  7903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5344e-04,  2.6879e-04, -5.1536e-04,  ...,  2.0557e-04,
         -3.5946e-04, -3.4562e-04],
        [-2.2799e-06, -1.6987e-06,  1.3039e-06,  ..., -1.9670e-06,
         -1.1921e-06, -1.4454e-06],
        [-3.7849e-06, -2.8312e-06,  2.2203e-06,  ..., -3.1292e-06,
         -1.8179e-06, -2.1905e-06],
        [-3.3081e-06, -2.4438e-06,  1.9968e-06,  ..., -2.8312e-06,
         -1.6242e-06, -2.1011e-06],
        [-5.3048e-06, -3.9935e-06,  3.0696e-06,  ..., -4.3511e-06,
         -2.5779e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.0488626956939697


Running epoch 0, step 697, batch 697
Sampled inputs[:2]: tensor([[    0, 17561,    12,  ...,   741,   496,    14],
        [    0,  3529,   271,  ...,  1553,   365,  2714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0781e-04,  3.5375e-04, -6.8402e-04,  ...,  2.6202e-04,
         -5.9793e-04, -3.2809e-04],
        [-4.7237e-06, -3.5167e-06,  2.6003e-06,  ..., -3.9637e-06,
         -2.2799e-06, -2.7269e-06],
        [-7.8678e-06, -5.9009e-06,  4.4405e-06,  ..., -6.4075e-06,
         -3.5688e-06, -4.2021e-06],
        [-6.6608e-06, -4.9323e-06,  3.8296e-06,  ..., -5.5432e-06,
         -3.0473e-06, -3.8594e-06],
        [-1.1027e-05, -8.2850e-06,  6.1542e-06,  ..., -8.9407e-06,
         -5.0813e-06, -5.6326e-06]], device='cuda:0')
Loss: 1.0575381517410278


Running epoch 0, step 698, batch 698
Sampled inputs[:2]: tensor([[   0, 9430,  287,  ..., 1141, 2280,  408],
        [   0,   35, 3815,  ...,  278, 7097, 4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7384e-04,  4.4927e-04, -6.3590e-04,  ...,  3.6925e-04,
         -7.5329e-04, -3.6136e-04],
        [-7.1079e-06, -5.1856e-06,  3.8669e-06,  ..., -5.9307e-06,
         -3.3975e-06, -4.0457e-06],
        [-1.1832e-05, -8.7321e-06,  6.6012e-06,  ..., -9.5963e-06,
         -5.3421e-06, -6.2436e-06],
        [-9.9689e-06, -7.2569e-06,  5.6550e-06,  ..., -8.2701e-06,
         -4.5151e-06, -5.6922e-06],
        [-1.6570e-05, -1.2279e-05,  9.1642e-06,  ..., -1.3411e-05,
         -7.6145e-06, -8.3596e-06]], device='cuda:0')
Loss: 1.0546232461929321


Running epoch 0, step 699, batch 699
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,   292,    41,  ...,   271,  9536,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7093e-04,  8.4950e-04, -5.1623e-04,  ...,  4.1146e-04,
         -8.1060e-04, -3.1928e-04],
        [-9.7603e-06, -7.0781e-06,  5.1335e-06,  ..., -7.9572e-06,
         -4.6566e-06, -5.5730e-06],
        [-1.6361e-05, -1.1981e-05,  8.8364e-06,  ..., -1.2979e-05,
         -7.3835e-06, -8.7172e-06],
        [-1.3515e-05, -9.7752e-06,  7.4059e-06,  ..., -1.0937e-05,
         -6.1095e-06, -7.7337e-06],
        [-2.2888e-05, -1.6838e-05,  1.2279e-05,  ..., -1.8120e-05,
         -1.0490e-05, -1.1697e-05]], device='cuda:0')
Loss: 1.0554393529891968


Running epoch 0, step 700, batch 700
Sampled inputs[:2]: tensor([[   0, 9582, 3645,  ..., 1027,   12,  461],
        [   0,   12, 1197,  ...,  352, 2513,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7391e-04,  8.5429e-04, -6.6529e-04,  ...,  3.7866e-04,
         -8.7474e-04, -4.0464e-04],
        [-1.2204e-05, -8.9034e-06,  6.4895e-06,  ..., -9.9093e-06,
         -5.7071e-06, -6.8322e-06],
        [-2.0415e-05, -1.5020e-05,  1.1131e-05,  ..., -1.6153e-05,
         -9.0450e-06, -1.0669e-05],
        [-1.6928e-05, -1.2308e-05,  9.3579e-06,  ..., -1.3649e-05,
         -7.4804e-06, -9.4995e-06],
        [-2.8640e-05, -2.1160e-05,  1.5512e-05,  ..., -2.2620e-05,
         -1.2904e-05, -1.4350e-05]], device='cuda:0')
Loss: 1.0498977899551392


Running epoch 0, step 701, batch 701
Sampled inputs[:2]: tensor([[    0,   287,  1070,  ...,   292,   221,   374],
        [    0,   367,   925,  ..., 25491,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3069e-04,  8.2298e-04, -5.8751e-04,  ...,  3.5524e-04,
         -8.0980e-04, -2.7595e-04],
        [-1.4514e-05, -1.0580e-05,  7.7188e-06,  ..., -1.1817e-05,
         -6.8620e-06, -8.1062e-06],
        [-2.4319e-05, -1.7852e-05,  1.3262e-05,  ..., -1.9252e-05,
         -1.0826e-05, -1.2636e-05],
        [-2.0191e-05, -1.4663e-05,  1.1154e-05,  ..., -1.6302e-05,
         -9.0003e-06, -1.1280e-05],
        [-3.4183e-05, -2.5183e-05,  1.8507e-05,  ..., -2.7001e-05,
         -1.5453e-05, -1.7017e-05]], device='cuda:0')
Loss: 1.0640170574188232


Running epoch 0, step 702, batch 702
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,    21,   292,  ...,    13,  1861,  4254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5847e-04,  1.0631e-03, -4.5912e-04,  ...,  4.6708e-04,
         -8.5704e-04, -1.7904e-04],
        [-1.7032e-05, -1.2405e-05,  8.9407e-06,  ..., -1.3739e-05,
         -7.9647e-06, -9.5367e-06],
        [-2.8670e-05, -2.1040e-05,  1.5453e-05,  ..., -2.2501e-05,
         -1.2636e-05, -1.4961e-05],
        [-2.3663e-05, -1.7181e-05,  1.2897e-05,  ..., -1.8910e-05,
         -1.0423e-05, -1.3232e-05],
        [-4.0144e-05, -2.9564e-05,  2.1502e-05,  ..., -3.1471e-05,
         -1.7956e-05, -2.0087e-05]], device='cuda:0')
Loss: 1.04466712474823


Running epoch 0, step 703, batch 703
Sampled inputs[:2]: tensor([[    0,  3984, 13077,  ...,   287,   650,   413],
        [    0,  1481,   278,  ...,  3940,  4938,     5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9517e-04,  1.2597e-03, -4.9043e-04,  ...,  5.7224e-04,
         -9.1713e-04, -1.4081e-04],
        [-1.9446e-05, -1.4178e-05,  1.0207e-05,  ..., -1.5676e-05,
         -9.0450e-06, -1.0878e-05],
        [-3.2693e-05, -2.4021e-05,  1.7643e-05,  ..., -2.5660e-05,
         -1.4335e-05, -1.7032e-05],
        [-2.7031e-05, -1.9640e-05,  1.4745e-05,  ..., -2.1592e-05,
         -1.1839e-05, -1.5102e-05],
        [-4.5896e-05, -3.3826e-05,  2.4602e-05,  ..., -3.5971e-05,
         -2.0429e-05, -2.2918e-05]], device='cuda:0')
Loss: 1.0664972066879272
Graident accumulation at epoch 0, step 703, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0029,  ..., -0.0025,  0.0230, -0.0195],
        [ 0.0287, -0.0080,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0151, -0.0279,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1779e-05,  3.0767e-04, -1.4979e-04,  ...,  2.1830e-04,
         -2.5405e-04,  2.6498e-05],
        [-1.8331e-05, -1.3299e-05,  8.8453e-06,  ..., -1.5834e-05,
         -8.2791e-06, -1.1130e-05],
        [ 5.3364e-05,  3.8308e-05, -2.7209e-05,  ...,  4.4676e-05,
          2.6116e-05,  2.1928e-05],
        [-1.1984e-05, -9.8934e-06,  4.9509e-06,  ..., -1.0529e-05,
         -3.5491e-06, -7.5734e-06],
        [-4.4777e-05, -3.2239e-05,  2.1927e-05,  ..., -3.7677e-05,
         -1.8550e-05, -2.5210e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4109e-08, 3.8704e-08, 3.8737e-08,  ..., 1.5447e-08, 9.3155e-08,
         1.3080e-08],
        [6.3664e-11, 3.4746e-11, 6.1488e-12,  ..., 4.4641e-11, 4.7104e-12,
         1.4168e-11],
        [2.3428e-09, 1.1662e-09, 4.0925e-10,  ..., 1.6706e-09, 3.5316e-10,
         7.2652e-10],
        [6.0862e-10, 3.0290e-10, 9.4676e-11,  ..., 4.0177e-10, 1.1271e-10,
         1.6948e-10],
        [2.9471e-10, 1.5877e-10, 2.9506e-11,  ..., 2.1514e-10, 2.3372e-11,
         7.1132e-11]], device='cuda:0')
optimizer state dict: 88.0
lr: [1.5481349926128634e-05, 1.5481349926128634e-05]
scheduler_last_epoch: 88


Running epoch 0, step 704, batch 704
Sampled inputs[:2]: tensor([[    0, 42306,   278,  ...,  1110,  3427,  4224],
        [    0,    20,    13,  ...,   496,    14,  1032]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8309e-05,  2.7177e-04,  6.4523e-05,  ..., -2.4758e-05,
         -3.1221e-05,  6.5737e-05],
        [-2.2054e-06, -1.6466e-06,  1.2219e-06,  ..., -1.9372e-06,
         -1.2890e-06, -1.4976e-06],
        [-3.7104e-06, -2.7716e-06,  2.1458e-06,  ..., -3.1292e-06,
         -2.0117e-06, -2.2799e-06],
        [-2.9802e-06, -2.2054e-06,  1.7285e-06,  ..., -2.5630e-06,
         -1.6391e-06, -1.9968e-06],
        [-5.3644e-06, -4.0233e-06,  3.0696e-06,  ..., -4.4703e-06,
         -2.9206e-06, -3.0845e-06]], device='cuda:0')
Loss: 1.0628892183303833


Running epoch 0, step 705, batch 705
Sampled inputs[:2]: tensor([[    0,  1265,   328,  ...,  2282, 35414,    13],
        [    0,    76,   472,  ..., 21215,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5841e-05,  6.6804e-05, -1.0790e-05,  ...,  3.0829e-05,
          1.2332e-04,  3.4639e-04],
        [-4.2170e-06, -3.1218e-06,  2.4363e-06,  ..., -3.8594e-06,
         -2.4885e-06, -3.0547e-06],
        [-6.9737e-06, -5.2005e-06,  4.2021e-06,  ..., -5.9754e-06,
         -3.7402e-06, -4.3809e-06],
        [-5.8711e-06, -4.3064e-06,  3.5837e-06,  ..., -5.2452e-06,
         -3.2559e-06, -4.1723e-06],
        [-1.0192e-05, -7.6294e-06,  6.0648e-06,  ..., -8.6129e-06,
         -5.4985e-06, -5.9605e-06]], device='cuda:0')
Loss: 1.0427579879760742


Running epoch 0, step 706, batch 706
Sampled inputs[:2]: tensor([[    0,   721,  1717,  ...,   278, 26029,    12],
        [    0,   266,   298,  ...,   266,   818,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4173e-05,  4.1691e-04,  1.7633e-04,  ...,  1.1011e-04,
         -1.5223e-04,  3.4639e-04],
        [-6.6608e-06, -4.8056e-06,  3.6880e-06,  ..., -5.8711e-06,
         -3.7327e-06, -4.6790e-06],
        [-1.1116e-05, -8.0913e-06,  6.4075e-06,  ..., -9.2983e-06,
         -5.7667e-06, -6.9439e-06],
        [-9.1046e-06, -6.5267e-06,  5.3048e-06,  ..., -7.8678e-06,
         -4.8131e-06, -6.2883e-06],
        [-1.6153e-05, -1.1802e-05,  9.1940e-06,  ..., -1.3351e-05,
         -8.4192e-06, -9.4324e-06]], device='cuda:0')
Loss: 1.0486443042755127


Running epoch 0, step 707, batch 707
Sampled inputs[:2]: tensor([[    0, 43788,    12,  ...,    12,  6288,   391],
        [    0,   266,  1144,  ..., 21458,    12, 15890]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8517e-05,  2.6683e-04,  2.3069e-04,  ...,  1.9110e-05,
         -1.3583e-04,  4.7446e-04],
        [-8.8960e-06, -6.4149e-06,  4.9993e-06,  ..., -7.7784e-06,
         -4.9323e-06, -6.3181e-06],
        [-1.4737e-05, -1.0729e-05,  8.5831e-06,  ..., -1.2249e-05,
         -7.5623e-06, -9.2983e-06],
        [-1.2264e-05, -8.7768e-06,  7.2569e-06,  ..., -1.0505e-05,
         -6.3926e-06, -8.5682e-06],
        [-2.1547e-05, -1.5765e-05,  1.2413e-05,  ..., -1.7732e-05,
         -1.1161e-05, -1.2726e-05]], device='cuda:0')
Loss: 1.0361571311950684


Running epoch 0, step 708, batch 708
Sampled inputs[:2]: tensor([[    0,   266,   997,  ...,  2670,     5,   278],
        [    0,   300,  6263,  ..., 18488,  1665,  1640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2953e-05,  1.0513e-04,  1.1893e-04,  ...,  5.1508e-05,
         -1.0663e-04,  5.1264e-04],
        [-1.1101e-05, -8.0541e-06,  6.3181e-06,  ..., -9.7603e-06,
         -6.0648e-06, -7.7337e-06],
        [-1.8239e-05, -1.3351e-05,  1.0729e-05,  ..., -1.5259e-05,
         -9.2536e-06, -1.1310e-05],
        [-1.5348e-05, -1.1042e-05,  9.1791e-06,  ..., -1.3217e-05,
         -7.8827e-06, -1.0535e-05],
        [-2.6643e-05, -1.9610e-05,  1.5482e-05,  ..., -2.2113e-05,
         -1.3694e-05, -1.5482e-05]], device='cuda:0')
Loss: 1.05592679977417


Running epoch 0, step 709, batch 709
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,   342, 22510,  ..., 49108,   278, 25904]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4250e-04,  2.1194e-04,  5.7860e-05,  ...,  2.4115e-04,
         -3.1757e-04,  5.8598e-04],
        [-1.3053e-05, -9.4995e-06,  7.6145e-06,  ..., -1.1623e-05,
         -7.2792e-06, -9.4101e-06],
        [-2.1532e-05, -1.5795e-05,  1.2949e-05,  ..., -1.8179e-05,
         -1.1086e-05, -1.3739e-05],
        [-1.8209e-05, -1.3128e-05,  1.1176e-05,  ..., -1.5870e-05,
         -9.5293e-06, -1.2934e-05],
        [-3.1322e-05, -2.3082e-05,  1.8567e-05,  ..., -2.6166e-05,
         -1.6302e-05, -1.8686e-05]], device='cuda:0')
Loss: 1.0344252586364746


Running epoch 0, step 710, batch 710
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0,  344, 2183,  ...,   14,  759,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6335e-05,  2.7563e-04,  4.5438e-05,  ...,  2.2074e-04,
         -4.4199e-04,  5.2022e-04],
        [-1.5289e-05, -1.1168e-05,  8.8960e-06,  ..., -1.3590e-05,
         -8.4564e-06, -1.0982e-05],
        [-2.5243e-05, -1.8612e-05,  1.5140e-05,  ..., -2.1353e-05,
         -1.2949e-05, -1.6138e-05],
        [-2.1279e-05, -1.5423e-05,  1.3024e-05,  ..., -1.8552e-05,
         -1.1072e-05, -1.5080e-05],
        [-3.6687e-05, -2.7165e-05,  2.1711e-05,  ..., -3.0756e-05,
         -1.9073e-05, -2.1994e-05]], device='cuda:0')
Loss: 1.0430673360824585


Running epoch 0, step 711, batch 711
Sampled inputs[:2]: tensor([[    0,   266,  4616,  ...,  1906,  7256,   287],
        [    0, 26074,   486,  ...,  2314,   266,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8448e-04, -1.5538e-04, -4.3486e-05,  ..., -2.7543e-06,
         -3.0448e-04,  6.3234e-04],
        [-1.7554e-05, -1.2852e-05,  1.0245e-05,  ..., -1.5445e-05,
         -9.4846e-06, -1.2323e-05],
        [-2.8923e-05, -2.1398e-05,  1.7375e-05,  ..., -2.4289e-05,
         -1.4529e-05, -1.8135e-05],
        [-2.4498e-05, -1.7807e-05,  1.5020e-05,  ..., -2.1160e-05,
         -1.2435e-05, -1.7002e-05],
        [-4.2111e-05, -3.1278e-05,  2.4974e-05,  ..., -3.5077e-05,
         -2.1473e-05, -2.4766e-05]], device='cuda:0')
Loss: 1.0450730323791504
Graident accumulation at epoch 0, step 711, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0056, -0.0145,  0.0029,  ..., -0.0025,  0.0231, -0.0195],
        [ 0.0287, -0.0080,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0279,  ...,  0.0287, -0.0151, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.9049e-05,  2.6137e-04, -1.3916e-04,  ...,  1.9619e-04,
         -2.5909e-04,  8.7082e-05],
        [-1.8254e-05, -1.3254e-05,  8.9852e-06,  ..., -1.5795e-05,
         -8.3996e-06, -1.1250e-05],
        [ 4.5136e-05,  3.2337e-05, -2.2750e-05,  ...,  3.7779e-05,
          2.2052e-05,  1.7922e-05],
        [-1.3235e-05, -1.0685e-05,  5.9578e-06,  ..., -1.1592e-05,
         -4.4377e-06, -8.5163e-06],
        [-4.4510e-05, -3.2143e-05,  2.2232e-05,  ..., -3.7417e-05,
         -1.8842e-05, -2.5166e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4146e-08, 3.8689e-08, 3.8700e-08,  ..., 1.5432e-08, 9.3155e-08,
         1.3467e-08],
        [6.3908e-11, 3.4876e-11, 6.2476e-12,  ..., 4.4835e-11, 4.7956e-12,
         1.4306e-11],
        [2.3413e-09, 1.1655e-09, 4.0914e-10,  ..., 1.6695e-09, 3.5302e-10,
         7.2612e-10],
        [6.0861e-10, 3.0292e-10, 9.4807e-11,  ..., 4.0182e-10, 1.1275e-10,
         1.6960e-10],
        [2.9619e-10, 1.5959e-10, 3.0101e-11,  ..., 2.1615e-10, 2.3810e-11,
         7.1674e-11]], device='cuda:0')
optimizer state dict: 89.0
lr: [1.537754207460116e-05, 1.537754207460116e-05]
scheduler_last_epoch: 89


Running epoch 0, step 712, batch 712
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,   759,  1128,  ...,   221,   474,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7270e-04, -3.2629e-04,  1.5904e-04,  ..., -2.1485e-04,
          2.3835e-04, -4.4286e-05],
        [-2.2799e-06, -1.7583e-06,  1.4305e-06,  ..., -1.9222e-06,
         -1.2517e-06, -1.5423e-06],
        [-3.8445e-06, -3.0100e-06,  2.4736e-06,  ..., -3.1590e-06,
         -2.0266e-06, -2.4140e-06],
        [-3.1739e-06, -2.4438e-06,  2.1011e-06,  ..., -2.6375e-06,
         -1.6391e-06, -2.1756e-06],
        [-5.8115e-06, -4.5896e-06,  3.6657e-06,  ..., -4.7684e-06,
         -3.1441e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.0559611320495605


Running epoch 0, step 713, batch 713
Sampled inputs[:2]: tensor([[   0,  669,  292,  ..., 4032,  271, 4442],
        [   0, 1871,  518,  ...,  271,  259, 1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2940e-04, -5.0741e-04,  1.9751e-04,  ..., -2.6000e-04,
          5.3419e-04,  2.4302e-04],
        [-4.3809e-06, -3.3081e-06,  2.7716e-06,  ..., -3.8743e-06,
         -2.6152e-06, -3.2187e-06],
        [-7.2569e-06, -5.6028e-06,  4.7088e-06,  ..., -6.1691e-06,
         -4.0680e-06, -4.7982e-06],
        [-6.0648e-06, -4.5896e-06,  4.0531e-06,  ..., -5.3048e-06,
         -3.4273e-06, -4.5151e-06],
        [-1.1057e-05, -8.6129e-06,  7.0482e-06,  ..., -9.3579e-06,
         -6.3330e-06, -6.8247e-06]], device='cuda:0')
Loss: 1.0490314960479736


Running epoch 0, step 714, batch 714
Sampled inputs[:2]: tensor([[   0, 2728, 3139,  ..., 2254,  221,  380],
        [   0,  280, 5656,  ..., 7369, 2276,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4606e-04, -6.9820e-04,  3.1337e-04,  ..., -3.8866e-04,
          7.1182e-04,  3.4997e-04],
        [-6.8843e-06, -5.1260e-06,  4.2021e-06,  ..., -5.8711e-06,
         -3.8072e-06, -4.8205e-06],
        [-1.1340e-05, -8.6576e-06,  7.1228e-06,  ..., -9.4026e-06,
         -6.0052e-06, -7.2122e-06],
        [-9.3430e-06, -6.9886e-06,  6.0201e-06,  ..., -7.9274e-06,
         -4.9248e-06, -6.6310e-06],
        [-1.7196e-05, -1.3262e-05,  1.0639e-05,  ..., -1.4275e-05,
         -9.3728e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.0545798540115356


Running epoch 0, step 715, batch 715
Sampled inputs[:2]: tensor([[   0, 4108,   85,  ...,   40,   12, 1530],
        [   0, 1802, 4165,  ...,  298,  445,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1665e-04, -8.9906e-04,  4.8446e-04,  ..., -6.4867e-04,
          1.1202e-03,  5.2623e-04],
        [-9.2238e-06, -6.9439e-06,  5.5805e-06,  ..., -7.8678e-06,
         -5.0813e-06, -6.4000e-06],
        [-1.5154e-05, -1.1668e-05,  9.4324e-06,  ..., -1.2577e-05,
         -8.0019e-06, -9.5516e-06],
        [-1.2398e-05, -9.3728e-06,  7.9125e-06,  ..., -1.0520e-05,
         -6.5193e-06, -8.7023e-06],
        [-2.3037e-05, -1.7911e-05,  1.4141e-05,  ..., -1.9133e-05,
         -1.2547e-05, -1.3620e-05]], device='cuda:0')
Loss: 1.0875557661056519


Running epoch 0, step 716, batch 716
Sampled inputs[:2]: tensor([[   0,  677, 9606,  ..., 9468, 9268,  328],
        [   0,  706, 6989,  ..., 6914,   15, 2537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0577e-03, -1.1642e-03,  7.5497e-04,  ..., -9.4508e-04,
          1.5750e-03,  7.3062e-04],
        [-1.1489e-05, -8.5607e-06,  7.0632e-06,  ..., -9.7528e-06,
         -6.2883e-06, -7.9796e-06],
        [-1.8865e-05, -1.4395e-05,  1.1891e-05,  ..., -1.5587e-05,
         -9.8944e-06, -1.1891e-05],
        [-1.5512e-05, -1.1593e-05,  1.0043e-05,  ..., -1.3098e-05,
         -8.1062e-06, -1.0893e-05],
        [-2.8610e-05, -2.2084e-05,  1.7792e-05,  ..., -2.3693e-05,
         -1.5512e-05, -1.6913e-05]], device='cuda:0')
Loss: 1.0534934997558594


Running epoch 0, step 717, batch 717
Sampled inputs[:2]: tensor([[    0, 21410, 13160,  ...,   292,    69,    14],
        [    0,   494,   825,  ...,   897,   328,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2428e-03, -1.2800e-03,  8.3436e-04,  ..., -1.0859e-03,
          1.7189e-03,  9.4530e-04],
        [-1.3947e-05, -1.0401e-05,  8.5086e-06,  ..., -1.1824e-05,
         -7.6443e-06, -9.5144e-06],
        [-2.2858e-05, -1.7449e-05,  1.4290e-05,  ..., -1.8910e-05,
         -1.2025e-05, -1.4216e-05],
        [-1.8761e-05, -1.4022e-05,  1.2025e-05,  ..., -1.5825e-05,
         -9.8273e-06, -1.2949e-05],
        [-3.4750e-05, -2.6822e-05,  2.1443e-05,  ..., -2.8819e-05,
         -1.8895e-05, -2.0295e-05]], device='cuda:0')
Loss: 1.088955283164978


Running epoch 0, step 718, batch 718
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0,  3398,   271,  ...,    13,  1581, 13600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4888e-03, -1.6625e-03,  7.6675e-04,  ..., -1.3682e-03,
          2.0000e-03,  1.0858e-03],
        [-1.6153e-05, -1.2174e-05,  1.0066e-05,  ..., -1.3761e-05,
         -8.8289e-06, -1.1116e-05],
        [-2.6435e-05, -2.0370e-05,  1.6823e-05,  ..., -2.1964e-05,
         -1.3866e-05, -1.6585e-05],
        [-2.1800e-05, -1.6466e-05,  1.4260e-05,  ..., -1.8463e-05,
         -1.1377e-05, -1.5184e-05],
        [-4.0233e-05, -3.1352e-05,  2.5243e-05,  ..., -3.3498e-05,
         -2.1815e-05, -2.3678e-05]], device='cuda:0')
Loss: 1.0445382595062256


Running epoch 0, step 719, batch 719
Sampled inputs[:2]: tensor([[    0,   278,   565,  ...,  1125,  5222,   287],
        [    0,  1067,   292,  ..., 10792, 11280,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5327e-03, -1.5933e-03,  7.9232e-04,  ..., -1.3464e-03,
          2.0640e-03,  1.2533e-03],
        [-1.8716e-05, -1.3873e-05,  1.1452e-05,  ..., -1.5758e-05,
         -1.0230e-05, -1.3098e-05],
        [-3.0637e-05, -2.3261e-05,  1.9178e-05,  ..., -2.5198e-05,
         -1.6116e-05, -1.9580e-05],
        [-2.5049e-05, -1.8626e-05,  1.6093e-05,  ..., -2.0996e-05,
         -1.3113e-05, -1.7703e-05],
        [-4.6253e-05, -3.5584e-05,  2.8595e-05,  ..., -3.8177e-05,
         -2.5123e-05, -2.7731e-05]], device='cuda:0')
Loss: 1.005647897720337
Graident accumulation at epoch 0, step 719, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0025,  0.0231, -0.0195],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0279,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8841e-04,  7.5903e-05, -4.6009e-05,  ...,  4.1932e-05,
         -2.6783e-05,  2.0370e-04],
        [-1.8300e-05, -1.3316e-05,  9.2318e-06,  ..., -1.5791e-05,
         -8.5826e-06, -1.1434e-05],
        [ 3.7558e-05,  2.6778e-05, -1.8558e-05,  ...,  3.1481e-05,
          1.8235e-05,  1.4172e-05],
        [-1.4416e-05, -1.1479e-05,  6.9714e-06,  ..., -1.2532e-05,
         -5.3052e-06, -9.4349e-06],
        [-4.4685e-05, -3.2487e-05,  2.2868e-05,  ..., -3.7493e-05,
         -1.9470e-05, -2.5422e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6451e-08, 4.1189e-08, 3.9289e-08,  ..., 1.7229e-08, 9.7322e-08,
         1.5024e-08],
        [6.4195e-11, 3.5034e-11, 6.3725e-12,  ..., 4.5038e-11, 4.8955e-12,
         1.4463e-11],
        [2.3399e-09, 1.1649e-09, 4.0910e-10,  ..., 1.6684e-09, 3.5293e-10,
         7.2578e-10],
        [6.0863e-10, 3.0296e-10, 9.4971e-11,  ..., 4.0186e-10, 1.1281e-10,
         1.6974e-10],
        [2.9804e-10, 1.6069e-10, 3.0888e-11,  ..., 2.1740e-10, 2.4417e-11,
         7.2371e-11]], device='cuda:0')
optimizer state dict: 90.0
lr: [1.5272912487703465e-05, 1.5272912487703465e-05]
scheduler_last_epoch: 90


Running epoch 0, step 720, batch 720
Sampled inputs[:2]: tensor([[    0, 13642, 14635,  ...,   367,  1040,  8580],
        [    0,   298,  2587,  ...,   298,   894,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4418e-05,  1.3128e-04,  2.1536e-04,  ..., -1.5844e-04,
         -2.7058e-04,  1.3416e-04],
        [-2.4140e-06, -1.7956e-06,  1.3933e-06,  ..., -1.9222e-06,
         -1.0878e-06, -1.4901e-06],
        [-4.2915e-06, -3.2485e-06,  2.5481e-06,  ..., -3.3677e-06,
         -1.9073e-06, -2.4885e-06],
        [-3.2187e-06, -2.3842e-06,  1.9372e-06,  ..., -2.5332e-06,
         -1.3709e-06, -2.0266e-06],
        [-6.4969e-06, -4.9472e-06,  3.7998e-06,  ..., -5.1260e-06,
         -2.9951e-06, -3.5167e-06]], device='cuda:0')
Loss: 1.0436238050460815


Running epoch 0, step 721, batch 721
Sampled inputs[:2]: tensor([[   0,   14, 4494,  ..., 4830,  368,  266],
        [   0,  729, 3084,  ...,  381, 1445,  642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2550e-05, -5.2126e-05,  4.6927e-04,  ..., -2.1317e-04,
         -1.0604e-04,  4.0224e-04],
        [-4.7088e-06, -3.5986e-06,  2.8759e-06,  ..., -3.8892e-06,
         -2.2352e-06, -2.8685e-06],
        [ 1.9227e-04,  1.7309e-04, -8.0240e-05,  ...,  1.2260e-04,
          8.1635e-05,  5.2384e-05],
        [-6.3330e-06, -4.7982e-06,  4.0382e-06,  ..., -5.1856e-06,
         -2.8461e-06, -3.9637e-06],
        [-1.2487e-05, -9.7454e-06,  7.6443e-06,  ..., -1.0163e-05,
         -6.0350e-06, -6.6310e-06]], device='cuda:0')
Loss: 1.0868374109268188


Running epoch 0, step 722, batch 722
Sampled inputs[:2]: tensor([[    0,  4154, 14296,  ...,   516,  1796, 18233],
        [    0,   685,  3482,  ..., 23113,    12,  6481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4012e-04, -4.0828e-04,  7.8031e-04,  ..., -5.1770e-04,
          5.1698e-04,  6.8977e-04],
        [-6.9588e-06, -5.1856e-06,  4.2170e-06,  ..., -5.8413e-06,
         -3.4943e-06, -4.4629e-06],
        [ 1.8827e-04,  1.7023e-04, -7.7826e-05,  ...,  1.1926e-04,
          7.9519e-05,  4.9850e-05],
        [-9.4324e-06, -6.9588e-06,  5.9754e-06,  ..., -7.8529e-06,
         -4.5076e-06, -6.1989e-06],
        [-1.8567e-05, -1.4186e-05,  1.1280e-05,  ..., -1.5259e-05,
         -9.3579e-06, -1.0282e-05]], device='cuda:0')
Loss: 1.0478453636169434


Running epoch 0, step 723, batch 723
Sampled inputs[:2]: tensor([[    0,  4890,  1528,  ...,   847,   328,  1703],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2973e-04, -6.2855e-04,  1.0024e-03,  ..., -5.8837e-04,
          6.5095e-04,  8.6325e-04],
        [-8.8960e-06, -6.6683e-06,  5.4911e-06,  ..., -7.7337e-06,
         -4.5523e-06, -5.8189e-06],
        [ 1.8480e-04,  1.6758e-04, -7.5501e-05,  ...,  1.1610e-04,
          7.7791e-05,  4.7764e-05],
        [-1.2264e-05, -9.1046e-06,  7.9423e-06,  ..., -1.0580e-05,
         -5.9456e-06, -8.2105e-06],
        [-2.3782e-05, -1.8209e-05,  1.4722e-05,  ..., -1.9968e-05,
         -1.2040e-05, -1.3188e-05]], device='cuda:0')
Loss: 1.0536185503005981


Running epoch 0, step 724, batch 724
Sampled inputs[:2]: tensor([[    0,    40,   568,  ...,  3750,   300,  3421],
        [    0,  1188,    12,  ...,   292, 23032,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7790e-04, -7.4850e-04,  1.1833e-03,  ..., -6.3074e-04,
          8.9751e-04,  1.1838e-03],
        [-1.1057e-05, -8.3372e-06,  6.8396e-06,  ..., -9.6262e-06,
         -5.6624e-06, -7.0706e-06],
        [ 1.8109e-04,  1.6466e-04, -7.3147e-05,  ...,  1.1292e-04,
          7.5943e-05,  4.5782e-05],
        [-1.5289e-05, -1.1429e-05,  9.9242e-06,  ..., -1.3232e-05,
         -7.4282e-06, -1.0028e-05],
        [-2.9534e-05, -2.2769e-05,  1.8328e-05,  ..., -2.4945e-05,
         -1.5005e-05, -1.6049e-05]], device='cuda:0')
Loss: 1.0624895095825195


Running epoch 0, step 725, batch 725
Sampled inputs[:2]: tensor([[    0,  4258,   717,  ...,    34,   609,  1169],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3526e-04, -1.1513e-03,  1.3370e-03,  ..., -8.5321e-04,
          1.3172e-03,  1.4314e-03],
        [-1.3113e-05, -1.0014e-05,  8.1882e-06,  ..., -1.1578e-05,
         -6.7875e-06, -8.5607e-06],
        [ 1.7762e-04,  1.6178e-04, -7.0822e-05,  ...,  1.0977e-04,
          7.4132e-05,  4.3577e-05],
        [-1.8120e-05, -1.3739e-05,  1.1891e-05,  ..., -1.5885e-05,
         -8.8885e-06, -1.2100e-05],
        [-3.4809e-05, -2.7150e-05,  2.1800e-05,  ..., -2.9713e-05,
         -1.7852e-05, -1.9148e-05]], device='cuda:0')
Loss: 1.0404349565505981


Running epoch 0, step 726, batch 726
Sampled inputs[:2]: tensor([[    0,   365,   984,  ..., 18562,  4237, 31813],
        [    0,  1552,   300,  ...,  1085,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4635e-04, -1.4356e-03,  1.9253e-03,  ..., -9.0802e-04,
          2.2958e-03,  1.9650e-03],
        [-1.5035e-05, -1.1504e-05,  9.4250e-06,  ..., -1.3418e-05,
         -7.8753e-06, -1.0066e-05],
        [ 4.5306e-04,  3.7344e-04, -3.0170e-04,  ...,  2.9569e-04,
          2.4154e-04,  2.4248e-05],
        [-2.0862e-05, -1.5840e-05,  1.3746e-05,  ..., -1.8433e-05,
         -1.0334e-05, -1.4231e-05],
        [-4.0114e-05, -3.1322e-05,  2.5228e-05,  ..., -3.4392e-05,
         -2.0683e-05, -2.2382e-05]], device='cuda:0')
Loss: 1.0214521884918213


Running epoch 0, step 727, batch 727
Sampled inputs[:2]: tensor([[    0,  1795,   365,  ...,   266, 46932,   293],
        [    0,    12,  1631,  ...,  1143,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5108e-04, -1.2340e-03,  2.0552e-03,  ..., -9.4807e-04,
          2.5392e-03,  2.2225e-03],
        [-1.7345e-05, -1.3165e-05,  1.0684e-05,  ..., -1.5341e-05,
         -9.0897e-06, -1.1563e-05],
        [ 4.4892e-04,  3.7043e-04, -2.9940e-04,  ...,  2.9236e-04,
          2.3948e-04,  2.1805e-05],
        [-2.4036e-05, -1.8090e-05,  1.5534e-05,  ..., -2.1040e-05,
         -1.1906e-05, -1.6302e-05],
        [-4.6462e-05, -3.5942e-05,  2.8744e-05,  ..., -3.9488e-05,
         -2.3931e-05, -2.5928e-05]], device='cuda:0')
Loss: 1.0432021617889404
Graident accumulation at epoch 0, step 727, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0279,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.5468e-04, -5.5085e-05,  1.6411e-04,  ..., -5.7068e-05,
          2.2982e-04,  4.0559e-04],
        [-1.8204e-05, -1.3301e-05,  9.3771e-06,  ..., -1.5746e-05,
         -8.6333e-06, -1.1447e-05],
        [ 7.8694e-05,  6.1143e-05, -4.6641e-05,  ...,  5.7570e-05,
          4.0360e-05,  1.4935e-05],
        [-1.5378e-05, -1.2140e-05,  7.8277e-06,  ..., -1.3383e-05,
         -5.9653e-06, -1.0122e-05],
        [-4.4862e-05, -3.2833e-05,  2.3456e-05,  ..., -3.7693e-05,
         -1.9916e-05, -2.5473e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7129e-08, 4.2671e-08, 4.3474e-08,  ..., 1.8111e-08, 1.0367e-07,
         1.9949e-08],
        [6.4431e-11, 3.5172e-11, 6.4803e-12,  ..., 4.5229e-11, 4.9732e-12,
         1.4582e-11],
        [2.5391e-09, 1.3009e-09, 4.9833e-10,  ..., 1.7523e-09, 4.0992e-10,
         7.2553e-10],
        [6.0860e-10, 3.0299e-10, 9.5117e-11,  ..., 4.0190e-10, 1.1284e-10,
         1.6984e-10],
        [2.9990e-10, 1.6183e-10, 3.1684e-11,  ..., 2.1874e-10, 2.4965e-11,
         7.2971e-11]], device='cuda:0')
optimizer state dict: 91.0
lr: [1.5167477153749745e-05, 1.5167477153749745e-05]
scheduler_last_epoch: 91


Running epoch 0, step 728, batch 728
Sampled inputs[:2]: tensor([[    0,   508,   586,  ...,   445,    29,   445],
        [    0,  2286,    29,  ...,   518,  1307, 16881]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 0.0000e+00,  2.4653e-04,  9.5384e-05,  ...,  4.2452e-05,
         -5.4011e-05,  1.9513e-05],
        [-2.5183e-06, -1.7881e-06,  1.2517e-06,  ..., -2.0415e-06,
         -1.1027e-06, -1.2591e-06],
        [-4.5896e-06, -3.3081e-06,  2.3395e-06,  ..., -3.6806e-06,
         -1.9521e-06, -2.1905e-06],
        [-3.4124e-06, -2.4140e-06,  1.7434e-06,  ..., -2.7567e-06,
         -1.4380e-06, -1.7732e-06],
        [-6.9141e-06, -4.9770e-06,  3.4869e-06,  ..., -5.5134e-06,
         -2.9802e-06, -3.1590e-06]], device='cuda:0')
Loss: 1.0458570718765259


Running epoch 0, step 729, batch 729
Sampled inputs[:2]: tensor([[   0, 3592,  417,  ..., 4893,  328,  266],
        [   0,  531,   20,  ...,   12, 1644,  680]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4487e-04,  3.7105e-04, -1.5024e-04,  ...,  5.8214e-05,
         -5.1106e-04, -3.0389e-04],
        [-4.9025e-06, -3.4943e-06,  2.6226e-06,  ..., -3.9637e-06,
         -2.1011e-06, -2.4214e-06],
        [-8.8811e-06, -6.4075e-06,  4.8429e-06,  ..., -7.1228e-06,
         -3.7178e-06, -4.1723e-06],
        [-6.7353e-06, -4.7684e-06,  3.7104e-06,  ..., -5.4240e-06,
         -2.7567e-06, -3.4422e-06],
        [-1.3262e-05, -9.5963e-06,  7.1526e-06,  ..., -1.0610e-05,
         -5.6773e-06, -5.9605e-06]], device='cuda:0')
Loss: 1.0345710515975952


Running epoch 0, step 730, batch 730
Sampled inputs[:2]: tensor([[    0,   301,   298,  ...,   806,   352, 22105],
        [    0,   266,  2653,  ...,    29,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8421e-04,  4.4053e-04, -2.8031e-04,  ..., -4.4904e-05,
         -7.0829e-04, -5.0419e-04],
        [-7.3016e-06, -5.2005e-06,  3.9488e-06,  ..., -5.9307e-06,
         -3.1292e-06, -3.6731e-06],
        [-1.3322e-05, -9.5814e-06,  7.3612e-06,  ..., -1.0699e-05,
         -5.5730e-06, -6.3330e-06],
        [-1.0014e-05, -7.0781e-06,  5.5879e-06,  ..., -8.1062e-06,
         -4.0978e-06, -5.2005e-06],
        [-1.9908e-05, -1.4365e-05,  1.0893e-05,  ..., -1.5974e-05,
         -8.5235e-06, -9.0450e-06]], device='cuda:0')
Loss: 1.0303304195404053


Running epoch 0, step 731, batch 731
Sampled inputs[:2]: tensor([[    0,   287,   298,  ..., 14121,  3121,   409],
        [    0,   768,  3227,  ...,  3487,    13, 31431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9301e-04,  6.5887e-04, -5.5600e-04,  ...,  1.6817e-04,
         -8.8073e-04, -6.1156e-04],
        [-9.7454e-06, -6.8620e-06,  5.2527e-06,  ..., -7.8976e-06,
         -4.2096e-06, -4.9770e-06],
        [-1.7762e-05, -1.2636e-05,  9.7752e-06,  ..., -1.4260e-05,
         -7.5251e-06, -8.5980e-06],
        [-1.3381e-05, -9.3430e-06,  7.4655e-06,  ..., -1.0833e-05,
         -5.5283e-06, -7.0706e-06],
        [-2.6464e-05, -1.8924e-05,  1.4409e-05,  ..., -2.1219e-05,
         -1.1489e-05, -1.2219e-05]], device='cuda:0')
Loss: 1.047695279121399


Running epoch 0, step 732, batch 732
Sampled inputs[:2]: tensor([[    0,    12,  3067,  ...,  1381,   278,  5011],
        [    0,   271, 10474,  ...,   298,  2286,    29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0382e-04,  8.3333e-04, -7.0764e-04,  ...,  2.5364e-04,
         -1.1293e-03, -6.8643e-04],
        [-1.2130e-05, -8.5309e-06,  6.5863e-06,  ..., -9.9391e-06,
         -5.3048e-06, -6.1020e-06],
        [-2.1935e-05, -1.5616e-05,  1.2144e-05,  ..., -1.7807e-05,
         -9.4250e-06, -1.0490e-05],
        [-1.6674e-05, -1.1653e-05,  9.3654e-06,  ..., -1.3664e-05,
         -6.9961e-06, -8.7097e-06],
        [-3.2842e-05, -2.3514e-05,  1.7986e-05,  ..., -2.6643e-05,
         -1.4469e-05, -1.4961e-05]], device='cuda:0')
Loss: 1.0541023015975952


Running epoch 0, step 733, batch 733
Sampled inputs[:2]: tensor([[    0,  6693,  1235,  ..., 10814,  1810,   367],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9286e-04,  8.6458e-04, -6.8902e-04,  ...,  2.7192e-04,
         -1.1378e-03, -7.3354e-04],
        [-1.4499e-05, -1.0245e-05,  7.8604e-06,  ..., -1.1906e-05,
         -6.3330e-06, -7.2420e-06],
        [-2.6166e-05, -1.8716e-05,  1.4469e-05,  ..., -2.1294e-05,
         -1.1243e-05, -1.2428e-05],
        [-1.9953e-05, -1.4007e-05,  1.1191e-05,  ..., -1.6376e-05,
         -8.3521e-06, -1.0341e-05],
        [-3.9369e-05, -2.8312e-05,  2.1532e-05,  ..., -3.2037e-05,
         -1.7360e-05, -1.7792e-05]], device='cuda:0')
Loss: 1.0606855154037476


Running epoch 0, step 734, batch 734
Sampled inputs[:2]: tensor([[   0,   21, 1304,  ..., 3577,   13, 2497],
        [   0, 7230,   13,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0625e-04,  1.2547e-03, -7.9898e-04,  ...,  3.9803e-04,
         -1.5281e-03, -7.6801e-04],
        [-1.7032e-05, -1.2070e-05,  9.1493e-06,  ..., -1.3903e-05,
         -7.5549e-06, -8.5086e-06],
        [-3.0726e-05, -2.2054e-05,  1.6809e-05,  ..., -2.4855e-05,
         -1.3418e-05, -1.4618e-05],
        [-2.3365e-05, -1.6451e-05,  1.2979e-05,  ..., -1.9073e-05,
         -9.9540e-06, -1.2115e-05],
        [-4.6223e-05, -3.3319e-05,  2.5004e-05,  ..., -3.7402e-05,
         -2.0683e-05, -2.0936e-05]], device='cuda:0')
Loss: 1.0392746925354004


Running epoch 0, step 735, batch 735
Sampled inputs[:2]: tensor([[   0,  767, 1345,  ...,  276,  327,  328],
        [   0,  278, 5492,  ...,  328,  995,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8953e-04,  1.3008e-03, -7.8423e-04,  ...,  5.1916e-04,
         -1.6235e-03, -7.8203e-04],
        [-1.9506e-05, -1.3821e-05,  1.0394e-05,  ..., -1.5900e-05,
         -8.7395e-06, -9.7230e-06],
        [-3.5286e-05, -2.5302e-05,  1.9133e-05,  ..., -2.8491e-05,
         -1.5549e-05, -1.6749e-05],
        [-2.6762e-05, -1.8835e-05,  1.4737e-05,  ..., -2.1800e-05,
         -1.1511e-05, -1.3836e-05],
        [-5.3078e-05, -3.8236e-05,  2.8476e-05,  ..., -4.2856e-05,
         -2.3961e-05, -2.4006e-05]], device='cuda:0')
Loss: 1.0592679977416992
Graident accumulation at epoch 0, step 735, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0279,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7026e-04,  8.0504e-05,  6.9276e-05,  ...,  5.5416e-07,
          4.4482e-05,  2.8682e-04],
        [-1.8334e-05, -1.3353e-05,  9.4787e-06,  ..., -1.5761e-05,
         -8.6439e-06, -1.1275e-05],
        [ 6.7296e-05,  5.2499e-05, -4.0064e-05,  ...,  4.8964e-05,
          3.4769e-05,  1.1767e-05],
        [-1.6517e-05, -1.2810e-05,  8.5186e-06,  ..., -1.4225e-05,
         -6.5199e-06, -1.0493e-05],
        [-4.5684e-05, -3.3373e-05,  2.3958e-05,  ..., -3.8209e-05,
         -2.0321e-05, -2.5326e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7429e-08, 4.4320e-08, 4.4045e-08,  ..., 1.8362e-08, 1.0620e-07,
         2.0541e-08],
        [6.4747e-11, 3.5328e-11, 6.5818e-12,  ..., 4.5436e-11, 5.0446e-12,
         1.4662e-11],
        [2.5378e-09, 1.3003e-09, 4.9820e-10,  ..., 1.7513e-09, 4.0976e-10,
         7.2508e-10],
        [6.0871e-10, 3.0304e-10, 9.5239e-11,  ..., 4.0197e-10, 1.1286e-10,
         1.6986e-10],
        [3.0241e-10, 1.6313e-10, 3.2463e-11,  ..., 2.2036e-10, 2.5515e-11,
         7.3474e-11]], device='cuda:0')
optimizer state dict: 92.0
lr: [1.5061252184179384e-05, 1.5061252184179384e-05]
scheduler_last_epoch: 92


Running epoch 0, step 736, batch 736
Sampled inputs[:2]: tensor([[    0,   367,  3704,  ...,  1746,    14,   759],
        [    0,  4878,   607,  ...,    14, 17331,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0076e-04, -1.8787e-05, -6.4081e-05,  ...,  6.4782e-06,
         -7.5209e-06, -2.4973e-05],
        [-2.5034e-06, -1.7807e-06,  1.2815e-06,  ..., -2.0713e-06,
         -1.0654e-06, -1.1921e-06],
        [-4.5002e-06, -3.2187e-06,  2.3246e-06,  ..., -3.6955e-06,
         -1.8924e-06, -2.0564e-06],
        [-3.3677e-06, -2.3693e-06,  1.7658e-06,  ..., -2.7716e-06,
         -1.3784e-06, -1.6689e-06],
        [-6.7055e-06, -4.7982e-06,  3.4273e-06,  ..., -5.4836e-06,
         -2.8908e-06, -2.9206e-06]], device='cuda:0')
Loss: 1.0657936334609985


Running epoch 0, step 737, batch 737
Sampled inputs[:2]: tensor([[   0, 1067,  408,  ..., 4657, 1016,  271],
        [   0,  221,  334,  ...,  271,  266, 7246]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4768e-04,  1.1930e-04, -1.2677e-04,  ..., -4.6167e-06,
         -2.6977e-04, -1.5225e-04],
        [-4.9919e-06, -3.3677e-06,  2.5034e-06,  ..., -4.1574e-06,
         -2.2873e-06, -2.5257e-06],
        [-8.9109e-06, -6.0946e-06,  4.5300e-06,  ..., -7.3612e-06,
         -4.0382e-06, -4.2915e-06],
        [-6.7204e-06, -4.5002e-06,  3.4496e-06,  ..., -5.5879e-06,
         -2.9877e-06, -3.5167e-06],
        [-1.3292e-05, -9.1195e-06,  6.7204e-06,  ..., -1.0937e-05,
         -6.1542e-06, -6.1244e-06]], device='cuda:0')
Loss: 1.058737874031067


Running epoch 0, step 738, batch 738
Sampled inputs[:2]: tensor([[   0, 9829,  292,  ..., 2928, 1029,  271],
        [   0,   13, 4363,  ...,  271, 2462,  709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4704e-04,  2.8951e-04, -3.1038e-04,  ...,  8.9073e-05,
         -4.0797e-04, -3.1513e-04],
        [-7.5102e-06, -5.1185e-06,  3.7476e-06,  ..., -6.2883e-06,
         -3.4347e-06, -3.7849e-06],
        [-1.3292e-05, -9.1791e-06,  6.7502e-06,  ..., -1.1027e-05,
         -6.0052e-06, -6.3777e-06],
        [-1.0088e-05, -6.8247e-06,  5.1558e-06,  ..., -8.4341e-06,
         -4.4778e-06, -5.2601e-06],
        [-1.9908e-05, -1.3769e-05,  1.0028e-05,  ..., -1.6451e-05,
         -9.1791e-06, -9.1344e-06]], device='cuda:0')
Loss: 1.0543040037155151


Running epoch 0, step 739, batch 739
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  292,   86,   16],
        [   0,   73,   14,  ...,  650,   13, 3658]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8578e-04,  3.8929e-04, -3.1131e-04,  ...,  9.9867e-05,
         -4.5938e-04, -3.6808e-04],
        [-1.0133e-05, -6.8396e-06,  5.0515e-06,  ..., -8.3894e-06,
         -4.5225e-06, -5.0142e-06],
        [-1.8001e-05, -1.2323e-05,  9.1195e-06,  ..., -1.4782e-05,
         -7.9721e-06, -8.4937e-06],
        [-1.3515e-05, -9.0599e-06,  6.9067e-06,  ..., -1.1176e-05,
         -5.8562e-06, -6.9067e-06],
        [-2.6971e-05, -1.8507e-05,  1.3560e-05,  ..., -2.2113e-05,
         -1.2234e-05, -1.2159e-05]], device='cuda:0')
Loss: 1.0605391263961792


Running epoch 0, step 740, batch 740
Sampled inputs[:2]: tensor([[   0,  292,  380,  ..., 6156,  278,  266],
        [   0, 1042, 2548,  ...,  328,  259, 2771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5220e-04,  5.1012e-04, -7.5000e-04,  ...,  1.7668e-04,
         -5.0414e-04, -5.9339e-04],
        [-1.2621e-05, -8.4937e-06,  6.2659e-06,  ..., -1.0476e-05,
         -5.7891e-06, -6.2734e-06],
        [-2.2441e-05, -1.5348e-05,  1.1355e-05,  ..., -1.8448e-05,
         -1.0177e-05, -1.0610e-05],
        [-1.6823e-05, -1.1235e-05,  8.5607e-06,  ..., -1.3962e-05,
         -7.4953e-06, -8.6352e-06],
        [-3.3587e-05, -2.3037e-05,  1.6883e-05,  ..., -2.7567e-05,
         -1.5557e-05, -1.5169e-05]], device='cuda:0')
Loss: 1.0698353052139282


Running epoch 0, step 741, batch 741
Sampled inputs[:2]: tensor([[    0,  2344,   271,  ...,  5415,    14,  1075],
        [    0, 14026,  4137,  ..., 12292,  1553,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7480e-04,  5.5782e-04, -1.2413e-03,  ...,  2.8535e-04,
         -9.3496e-04, -6.6778e-04],
        [-1.4871e-05, -9.9763e-06,  7.4804e-06,  ..., -1.2428e-05,
         -6.9290e-06, -7.5698e-06],
        [-2.6554e-05, -1.8105e-05,  1.3620e-05,  ..., -2.1890e-05,
         -1.2159e-05, -1.2785e-05],
        [-2.0027e-05, -1.3307e-05,  1.0364e-05,  ..., -1.6734e-05,
         -9.0599e-06, -1.0572e-05],
        [-3.9786e-05, -2.7239e-05,  2.0266e-05,  ..., -3.2723e-05,
         -1.8582e-05, -1.8239e-05]], device='cuda:0')
Loss: 1.019938588142395


Running epoch 0, step 742, batch 742
Sampled inputs[:2]: tensor([[    0,   292, 44809,  ...,   642,   437,  9038],
        [    0,   949, 11135,  ...,   278,   772,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9809e-04,  6.4030e-04, -1.4770e-03,  ...,  2.9572e-04,
         -1.0160e-03, -8.6840e-04],
        [-1.7241e-05, -1.1601e-05,  8.7842e-06,  ..., -1.4439e-05,
         -8.0541e-06, -8.7842e-06],
        [-3.0726e-05, -2.1040e-05,  1.5974e-05,  ..., -2.5421e-05,
         -1.4126e-05, -1.4827e-05],
        [-2.3410e-05, -1.5616e-05,  1.2286e-05,  ..., -1.9610e-05,
         -1.0625e-05, -1.2361e-05],
        [-4.6015e-05, -3.1620e-05,  2.3738e-05,  ..., -3.7998e-05,
         -2.1592e-05, -2.1130e-05]], device='cuda:0')
Loss: 1.0499659776687622


Running epoch 0, step 743, batch 743
Sampled inputs[:2]: tensor([[   0, 1471,  266,  ...,  525, 5202,  292],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7121e-04,  8.0379e-04, -1.8619e-03,  ...,  3.9714e-04,
         -1.1485e-03, -1.1270e-03],
        [-1.9655e-05, -1.3217e-05,  1.0043e-05,  ..., -1.6451e-05,
         -9.0897e-06, -1.0028e-05],
        [-3.4958e-05, -2.3916e-05,  1.8239e-05,  ..., -2.8893e-05,
         -1.5929e-05, -1.6868e-05],
        [-2.6673e-05, -1.7792e-05,  1.4052e-05,  ..., -2.2322e-05,
         -1.1981e-05, -1.4089e-05],
        [-5.2422e-05, -3.6001e-05,  2.7135e-05,  ..., -4.3243e-05,
         -2.4393e-05, -2.4065e-05]], device='cuda:0')
Loss: 1.0161049365997314
Graident accumulation at epoch 0, step 743, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.6113e-05,  1.5283e-04, -1.2384e-04,  ...,  4.0213e-05,
         -7.4814e-05,  1.4545e-04],
        [-1.8466e-05, -1.3340e-05,  9.5352e-06,  ..., -1.5830e-05,
         -8.6885e-06, -1.1150e-05],
        [ 5.7071e-05,  4.4857e-05, -3.4234e-05,  ...,  4.1178e-05,
          2.9699e-05,  8.9033e-06],
        [-1.7532e-05, -1.3308e-05,  9.0720e-06,  ..., -1.5035e-05,
         -7.0659e-06, -1.0853e-05],
        [-4.6358e-05, -3.3636e-05,  2.4276e-05,  ..., -3.8712e-05,
         -2.0728e-05, -2.5200e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7977e-08, 4.4922e-08, 4.7468e-08,  ..., 1.8502e-08, 1.0742e-07,
         2.1790e-08],
        [6.5069e-11, 3.5467e-11, 6.6761e-12,  ..., 4.5661e-11, 5.1222e-12,
         1.4748e-11],
        [2.5365e-09, 1.2995e-09, 4.9803e-10,  ..., 1.7504e-09, 4.0960e-10,
         7.2464e-10],
        [6.0881e-10, 3.0305e-10, 9.5342e-11,  ..., 4.0207e-10, 1.1289e-10,
         1.6989e-10],
        [3.0486e-10, 1.6426e-10, 3.3167e-11,  ..., 2.2201e-10, 2.6084e-11,
         7.3980e-11]], device='cuda:0')
optimizer state dict: 93.0
lr: [1.4954253811094988e-05, 1.4954253811094988e-05]
scheduler_last_epoch: 93


Running epoch 0, step 744, batch 744
Sampled inputs[:2]: tensor([[   0, 4995,  287,  ...,  300, 4531, 4729],
        [   0,  396,  298,  ...,   52, 5065,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3385e-05,  1.8311e-04, -1.3740e-04,  ...,  1.3050e-04,
         -3.2232e-04, -3.2682e-05],
        [-2.4736e-06, -1.5646e-06,  1.2070e-06,  ..., -1.9819e-06,
         -1.2070e-06, -1.4678e-06],
        [-4.4703e-06, -2.8759e-06,  2.2501e-06,  ..., -3.5316e-06,
         -2.1309e-06, -2.5034e-06],
        [-3.3677e-06, -2.1011e-06,  1.6987e-06,  ..., -2.6971e-06,
         -1.5870e-06, -2.0713e-06],
        [-6.4671e-06, -4.2021e-06,  3.2336e-06,  ..., -5.0962e-06,
         -3.1441e-06, -3.4422e-06]], device='cuda:0')
Loss: 1.025110125541687


Running epoch 0, step 745, batch 745
Sampled inputs[:2]: tensor([[    0,    83,   292,  ...,   445,    11, 16109],
        [    0,   365,  1941,  ..., 38029,  1790, 44066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0538e-04,  1.6743e-04, -3.0148e-04,  ...,  5.2091e-05,
         -3.6568e-04, -1.9761e-04],
        [-4.8280e-06, -3.0920e-06,  2.4289e-06,  ..., -3.9637e-06,
         -2.2948e-06, -2.7791e-06],
        [-8.5831e-06, -5.5879e-06,  4.4405e-06,  ..., -6.9439e-06,
         -3.9861e-06, -4.6343e-06],
        [-6.5863e-06, -4.1425e-06,  3.4124e-06,  ..., -5.4091e-06,
         -3.0324e-06, -3.9339e-06],
        [-1.2547e-05, -8.2552e-06,  6.4373e-06,  ..., -1.0133e-05,
         -5.9605e-06, -6.4224e-06]], device='cuda:0')
Loss: 1.0491927862167358


Running epoch 0, step 746, batch 746
Sampled inputs[:2]: tensor([[    0,   474,   706,  ...,    83, 38084,   475],
        [    0,   598,   696,  ...,  4048,  1795,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9638e-04,  3.2237e-04, -3.3850e-04,  ...,  3.3252e-05,
         -5.9930e-04, -1.6147e-04],
        [-7.2867e-06, -4.5747e-06,  3.6508e-06,  ..., -5.9456e-06,
         -3.4571e-06, -4.2766e-06],
        [-1.2845e-05, -8.2403e-06,  6.6459e-06,  ..., -1.0371e-05,
         -6.0275e-06, -7.0930e-06],
        [-9.9838e-06, -6.1840e-06,  5.1782e-06,  ..., -8.1509e-06,
         -4.6119e-06, -6.0499e-06],
        [-1.8775e-05, -1.2189e-05,  9.6560e-06,  ..., -1.5140e-05,
         -9.0301e-06, -9.8497e-06]], device='cuda:0')
Loss: 1.0123255252838135


Running epoch 0, step 747, batch 747
Sampled inputs[:2]: tensor([[    0,  6491,  3667,  ...,  5042,    14,  2152],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9513e-04,  3.1717e-04, -4.0802e-04,  ...,  4.9632e-05,
         -5.6551e-04, -2.2623e-04],
        [-9.7007e-06, -6.1095e-06,  4.9397e-06,  ..., -7.9274e-06,
         -4.6268e-06, -5.5954e-06],
        [-1.7285e-05, -1.1116e-05,  9.0450e-06,  ..., -1.3977e-05,
         -8.1584e-06, -9.4175e-06],
        [-1.3396e-05, -8.3297e-06,  7.0482e-06,  ..., -1.0937e-05,
         -6.2063e-06, -7.9870e-06],
        [-2.5153e-05, -1.6332e-05,  1.3068e-05,  ..., -2.0325e-05,
         -1.2174e-05, -1.3039e-05]], device='cuda:0')
Loss: 1.0778908729553223


Running epoch 0, step 748, batch 748
Sampled inputs[:2]: tensor([[    0,   409, 22809,  ...,   342,   720,    14],
        [    0,  1431,   221,  ...,   756,   409,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9629e-04,  2.8864e-04, -5.5019e-04,  ...,  1.1834e-04,
         -7.6353e-04, -3.6126e-04],
        [-1.1966e-05, -7.5549e-06,  6.2510e-06,  ..., -9.8273e-06,
         -5.6550e-06, -6.8620e-06],
        [-2.1368e-05, -1.3769e-05,  1.1429e-05,  ..., -1.7360e-05,
         -1.0006e-05, -1.1563e-05],
        [-1.6645e-05, -1.0371e-05,  8.9854e-06,  ..., -1.3664e-05,
         -7.6368e-06, -9.8720e-06],
        [-3.0965e-05, -2.0176e-05,  1.6436e-05,  ..., -2.5183e-05,
         -1.4916e-05, -1.5929e-05]], device='cuda:0')
Loss: 1.0451830625534058


Running epoch 0, step 749, batch 749
Sampled inputs[:2]: tensor([[    0,   689,    13,  ...,   756,   271, 31773],
        [    0,  2720,    14,  ...,   300, 15867,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3269e-04,  6.7333e-04, -1.2029e-03,  ...,  1.8062e-04,
         -1.4221e-03, -1.1682e-03],
        [-1.4171e-05, -8.9407e-06,  7.4729e-06,  ..., -1.1735e-05,
         -6.7279e-06, -8.1509e-06],
        [-2.5272e-05, -1.6272e-05,  1.3635e-05,  ..., -2.0668e-05,
         -1.1876e-05, -1.3694e-05],
        [-1.9923e-05, -1.2368e-05,  1.0870e-05,  ..., -1.6510e-05,
         -9.1791e-06, -1.1884e-05],
        [-3.6627e-05, -2.3887e-05,  1.9595e-05,  ..., -2.9981e-05,
         -1.7703e-05, -1.8835e-05]], device='cuda:0')
Loss: 1.044248342514038


Running epoch 0, step 750, batch 750
Sampled inputs[:2]: tensor([[   0,  278, 1041,  ..., 2098, 1837,   12],
        [   0,  271, 4219,  ...,  644,   14, 3607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6083e-04,  7.5545e-04, -1.8697e-03,  ...,  2.5837e-04,
         -2.1582e-03, -1.6967e-03],
        [-1.6302e-05, -1.0341e-05,  8.6874e-06,  ..., -1.3657e-05,
         -7.8902e-06, -9.5442e-06],
        [-2.8983e-05, -1.8805e-05,  1.5780e-05,  ..., -2.3916e-05,
         -1.3843e-05, -1.5914e-05],
        [-2.2992e-05, -1.4335e-05,  1.2733e-05,  ..., -1.9327e-05,
         -1.0826e-05, -1.4044e-05],
        [-4.2051e-05, -2.7657e-05,  2.2665e-05,  ..., -3.4690e-05,
         -2.0668e-05, -2.1860e-05]], device='cuda:0')
Loss: 1.0149061679840088


Running epoch 0, step 751, batch 751
Sampled inputs[:2]: tensor([[    0,   266, 12080,  ...,   674,   369, 10956],
        [    0,   365,  2849,  ...,     9,  3365,  5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1323e-04,  8.1673e-04, -2.0851e-03,  ...,  2.7459e-04,
         -2.3509e-03, -1.9216e-03],
        [-1.8716e-05, -1.1913e-05,  9.9465e-06,  ..., -1.5698e-05,
         -8.9854e-06, -1.0788e-05],
        [-3.3185e-05, -2.1607e-05,  1.8001e-05,  ..., -2.7433e-05,
         -1.5743e-05, -1.7956e-05],
        [-2.6301e-05, -1.6466e-05,  1.4506e-05,  ..., -2.2128e-05,
         -1.2293e-05, -1.5818e-05],
        [-4.8280e-05, -3.1859e-05,  2.5958e-05,  ..., -3.9935e-05,
         -2.3559e-05, -2.4781e-05]], device='cuda:0')
Loss: 1.0645828247070312
Graident accumulation at epoch 0, step 751, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0152, -0.0280,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.7178e-05,  2.1922e-04, -3.1997e-04,  ...,  6.3651e-05,
         -3.0242e-04, -6.1259e-05],
        [-1.8491e-05, -1.3197e-05,  9.5763e-06,  ..., -1.5817e-05,
         -8.7182e-06, -1.1114e-05],
        [ 4.8045e-05,  3.8211e-05, -2.9010e-05,  ...,  3.4317e-05,
          2.5155e-05,  6.2174e-06],
        [-1.8409e-05, -1.3624e-05,  9.6154e-06,  ..., -1.5744e-05,
         -7.5887e-06, -1.1349e-05],
        [-4.6550e-05, -3.3458e-05,  2.4444e-05,  ..., -3.8835e-05,
         -2.1011e-05, -2.5158e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8099e-08, 4.5544e-08, 5.1768e-08,  ..., 1.8559e-08, 1.1284e-07,
         2.5461e-08],
        [6.5354e-11, 3.5574e-11, 6.7684e-12,  ..., 4.5862e-11, 5.1978e-12,
         1.4850e-11],
        [2.5351e-09, 1.2987e-09, 4.9786e-10,  ..., 1.7494e-09, 4.0944e-10,
         7.2424e-10],
        [6.0889e-10, 3.0302e-10, 9.5457e-11,  ..., 4.0215e-10, 1.1293e-10,
         1.6997e-10],
        [3.0688e-10, 1.6511e-10, 3.3807e-11,  ..., 2.2338e-10, 2.6613e-11,
         7.4520e-11]], device='cuda:0')
optimizer state dict: 94.0
lr: [1.4846498384781962e-05, 1.4846498384781962e-05]
scheduler_last_epoch: 94


Running epoch 0, step 752, batch 752
Sampled inputs[:2]: tensor([[   0,  380,  759,  ..., 1420, 1804,  490],
        [   0, 1943, 1837,  ...,  870,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1971e-05, -1.9183e-05,  2.2223e-04,  ...,  4.1533e-05,
          2.1198e-04,  2.3921e-04],
        [-2.2054e-06, -1.4529e-06,  1.1474e-06,  ..., -1.8179e-06,
         -1.1474e-06, -1.4603e-06],
        [-3.8743e-06, -2.6375e-06,  2.0713e-06,  ..., -3.1590e-06,
         -2.0117e-06, -2.4140e-06],
        [-3.2187e-06, -2.1011e-06,  1.7434e-06,  ..., -2.6524e-06,
         -1.6168e-06, -2.2352e-06],
        [-5.5730e-06, -3.8743e-06,  2.9504e-06,  ..., -4.5598e-06,
         -2.9951e-06, -3.2336e-06]], device='cuda:0')
Loss: 1.0542218685150146


Running epoch 0, step 753, batch 753
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0,  1760,   446,  ...,   329,  1405,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2459e-04, -4.2610e-05,  3.0460e-04,  ...,  6.6183e-05,
          2.3895e-04,  2.2116e-04],
        [-4.4703e-06, -2.8163e-06,  2.4512e-06,  ..., -3.6135e-06,
         -2.2054e-06, -3.0249e-06],
        [-7.7784e-06, -5.0962e-06,  4.3660e-06,  ..., -6.2436e-06,
         -3.8370e-06, -4.9919e-06],
        [-6.4820e-06, -4.0233e-06,  3.6955e-06,  ..., -5.2303e-06,
         -3.0771e-06, -4.6045e-06],
        [-1.1265e-05, -7.5251e-06,  6.2138e-06,  ..., -9.0599e-06,
         -5.7817e-06, -6.7502e-06]], device='cuda:0')
Loss: 1.0364047288894653


Running epoch 0, step 754, batch 754
Sampled inputs[:2]: tensor([[    0,  2341,  7956,  ...,  2355,   413,    72],
        [    0,   607,  2697,  ...,   391, 14410, 14997]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7282e-04, -7.0573e-05,  2.6311e-04,  ...,  1.5002e-05,
          3.5545e-04,  2.0872e-04],
        [ 7.7014e-05,  8.0630e-05, -5.9114e-05,  ...,  3.8199e-05,
          6.6642e-05,  5.5971e-05],
        [-1.1742e-05, -7.6443e-06,  6.6161e-06,  ..., -9.5665e-06,
         -5.9977e-06, -7.1824e-06],
        [ 1.1369e-04,  8.0543e-05, -1.9339e-05,  ...,  6.4718e-05,
          4.8221e-05,  2.0536e-05],
        [-1.7136e-05, -1.1340e-05,  9.5069e-06,  ..., -1.3977e-05,
         -9.0897e-06, -9.8199e-06]], device='cuda:0')
Loss: 1.0778082609176636


Running epoch 0, step 755, batch 755
Sampled inputs[:2]: tensor([[    0,  1615,   292,  ...,  4824,   292,  9936],
        [    0,    15, 43895,  ...,   292,   380, 16795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7090e-04, -1.0182e-04,  4.0366e-04,  ...,  7.4610e-05,
          4.1876e-04,  1.9337e-04],
        [ 7.4690e-05,  7.9185e-05, -5.7862e-05,  ...,  3.6322e-05,
          6.5458e-05,  5.4406e-05],
        [-1.5855e-05, -1.0282e-05,  8.8811e-06,  ..., -1.2860e-05,
         -8.0839e-06, -9.8050e-06],
        [ 1.1039e-04,  7.8532e-05, -1.7484e-05,  ...,  6.2051e-05,
          4.6596e-05,  1.8182e-05],
        [-2.3037e-05, -1.5184e-05,  1.2711e-05,  ..., -1.8716e-05,
         -1.2204e-05, -1.3351e-05]], device='cuda:0')
Loss: 1.043086290359497


Running epoch 0, step 756, batch 756
Sampled inputs[:2]: tensor([[   0,  221,  381,  ...,  360, 8978,   14],
        [   0,  516,  689,  ...,  278,  516, 6137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6650e-04, -2.4048e-04,  2.8603e-04,  ...,  7.1959e-05,
          5.9502e-04,  1.4891e-04],
        [ 7.2767e-05,  7.8075e-05, -5.6625e-05,  ...,  3.4586e-05,
          6.4467e-05,  5.2901e-05],
        [-1.9073e-05, -1.2279e-05,  1.0982e-05,  ..., -1.5676e-05,
         -9.7379e-06, -1.2055e-05],
        [ 1.0752e-04,  7.6967e-05, -1.5472e-05,  ...,  5.9398e-05,
          4.5136e-05,  1.5723e-05],
        [-2.7716e-05, -1.8194e-05,  1.5721e-05,  ..., -2.2829e-05,
         -1.4752e-05, -1.6347e-05]], device='cuda:0')
Loss: 1.031413197517395


Running epoch 0, step 757, batch 757
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 1034, 1928,   15],
        [   0, 1737,  278,  ..., 2604,  367, 2002]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0301e-04, -2.9554e-04,  2.3515e-04,  ...,  8.4222e-05,
          5.7568e-04,  1.3230e-05],
        [ 7.0488e-05,  7.6540e-05, -5.5299e-05,  ...,  3.2746e-05,
          6.3386e-05,  5.1635e-05],
        [ 7.3910e-05,  2.7791e-05, -4.1058e-05,  ...,  8.6829e-05,
          5.0334e-05,  1.6561e-05],
        [ 1.0422e-04,  7.4777e-05, -1.3490e-05,  ...,  5.6731e-05,
          4.3646e-05,  1.3786e-05],
        [-3.3587e-05, -2.2277e-05,  1.9088e-05,  ..., -2.7597e-05,
         -1.7658e-05, -1.9282e-05]], device='cuda:0')
Loss: 1.0477162599563599


Running epoch 0, step 758, batch 758
Sampled inputs[:2]: tensor([[   0,   15,   19,  ...,   12,  287, 7897],
        [   0, 6957,  271,  ..., 9094,  266, 4320]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9491e-04, -2.7671e-04,  2.1793e-04,  ...,  1.4639e-04,
          5.0851e-04, -6.8920e-07],
        [ 6.8237e-05,  7.5161e-05, -5.4003e-05,  ...,  3.0928e-05,
          6.2276e-05,  5.0234e-05],
        [ 7.0006e-05,  2.5317e-05, -3.8778e-05,  ...,  8.3700e-05,
          4.8382e-05,  1.4281e-05],
        [ 1.0098e-04,  7.2825e-05, -1.1553e-05,  ...,  5.4123e-05,
          4.2096e-05,  1.1685e-05],
        [-3.9160e-05, -2.5898e-05,  2.2307e-05,  ..., -3.2127e-05,
         -2.0564e-05, -2.2322e-05]], device='cuda:0')
Loss: 1.053592324256897


Running epoch 0, step 759, batch 759
Sampled inputs[:2]: tensor([[   0, 5841,  328,  ..., 2051,  266,  756],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9209e-04, -2.7001e-04,  2.3838e-05,  ...,  2.8611e-04,
          1.7949e-04, -3.1501e-04],
        [ 6.6300e-05,  7.3917e-05, -5.2758e-05,  ...,  2.9184e-05,
          6.1278e-05,  4.8841e-05],
        [ 6.6698e-05,  2.3097e-05, -3.6587e-05,  ...,  8.0809e-05,
          4.6713e-05,  1.2120e-05],
        [ 9.8033e-05,  7.0977e-05, -9.5115e-06,  ...,  5.1426e-05,
          4.0621e-05,  9.3901e-06],
        [-4.3929e-05, -2.9147e-05,  2.5377e-05,  ..., -3.6269e-05,
         -2.3082e-05, -2.5168e-05]], device='cuda:0')
Loss: 1.0170629024505615
Graident accumulation at epoch 0, step 759, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0152, -0.0280,  ...,  0.0288, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.3669e-05,  1.7030e-04, -2.8559e-04,  ...,  8.5897e-05,
         -2.5423e-04, -8.6634e-05],
        [-1.0012e-05, -4.4855e-06,  3.3428e-06,  ..., -1.1317e-05,
         -1.7186e-06, -5.1186e-06],
        [ 4.9911e-05,  3.6699e-05, -2.9768e-05,  ...,  3.8966e-05,
          2.7311e-05,  6.8076e-06],
        [-6.7649e-06, -5.1635e-06,  7.7027e-06,  ..., -9.0270e-06,
         -2.7677e-06, -9.2752e-06],
        [-4.6288e-05, -3.3027e-05,  2.4537e-05,  ..., -3.8578e-05,
         -2.1218e-05, -2.5159e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8530e-08, 4.5571e-08, 5.1717e-08,  ..., 1.8622e-08, 1.1276e-07,
         2.5535e-08],
        [6.9684e-11, 4.1002e-11, 9.5450e-12,  ..., 4.6668e-11, 8.9476e-12,
         1.7220e-11],
        [2.5370e-09, 1.2979e-09, 4.9870e-10,  ..., 1.7542e-09, 4.1121e-10,
         7.2366e-10],
        [6.1789e-10, 3.0775e-10, 9.5452e-11,  ..., 4.0440e-10, 1.1446e-10,
         1.6989e-10],
        [3.0851e-10, 1.6579e-10, 3.4417e-11,  ..., 2.2447e-10, 2.7119e-11,
         7.5079e-11]], device='cuda:0')
optimizer state dict: 95.0
lr: [1.4738002371210062e-05, 1.4738002371210062e-05]
scheduler_last_epoch: 95


Running epoch 0, step 760, batch 760
Sampled inputs[:2]: tensor([[   0,  287, 9430,  ..., 3121,  352,  360],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3896e-04, -2.4015e-04,  4.1212e-04,  ..., -6.1664e-05,
          2.9752e-04, -8.7974e-05],
        [-1.8701e-06, -1.2368e-06,  1.2666e-06,  ..., -1.7285e-06,
         -9.6858e-07, -1.3635e-06],
        [-3.1739e-06, -2.1905e-06,  2.1607e-06,  ..., -2.8461e-06,
         -1.6317e-06, -2.1011e-06],
        [-2.7716e-06, -1.7881e-06,  2.0266e-06,  ..., -2.5928e-06,
         -1.3858e-06, -2.2203e-06],
        [-4.7088e-06, -3.3379e-06,  3.1292e-06,  ..., -4.2319e-06,
         -2.5779e-06, -2.8312e-06]], device='cuda:0')
Loss: 1.0332579612731934


Running epoch 0, step 761, batch 761
Sampled inputs[:2]: tensor([[    0, 15033,   278,  ...,   266,  2937,    14],
        [    0,  2314,   516,  ...,  1871,    13,  1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7947e-04, -7.1685e-04,  6.8458e-04,  ...,  2.3164e-04,
          1.1734e-03,  7.1779e-06],
        [-3.5241e-06, -2.0973e-06,  2.5555e-06,  ..., -3.3453e-06,
         -1.9744e-06, -2.8312e-06],
        [-6.0201e-06, -3.8147e-06,  4.4107e-06,  ..., -5.5432e-06,
         -3.3304e-06, -4.3809e-06],
        [-5.4687e-06, -3.0845e-06,  4.3362e-06,  ..., -5.2899e-06,
         -2.9653e-06, -4.8578e-06],
        [-8.7917e-06, -5.8264e-06,  6.2287e-06,  ..., -8.0764e-06,
         -5.1558e-06, -5.7071e-06]], device='cuda:0')
Loss: 1.0149128437042236


Running epoch 0, step 762, batch 762
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0, 1231,  352,  ..., 8524,   14,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8689e-04, -7.6981e-04,  8.5297e-04,  ...,  1.6895e-04,
          1.4304e-03,  1.3875e-04],
        [-5.4240e-06, -3.1777e-06,  3.8221e-06,  ..., -5.1260e-06,
         -3.0920e-06, -4.2915e-06],
        [-9.2089e-06, -5.7667e-06,  6.5863e-06,  ..., -8.4490e-06,
         -5.1931e-06, -6.6161e-06],
        [-8.2701e-06, -4.6045e-06,  6.3479e-06,  ..., -7.9423e-06,
         -4.5598e-06, -7.1973e-06],
        [-1.3560e-05, -8.8513e-06,  9.3877e-06,  ..., -1.2428e-05,
         -8.0764e-06, -8.7172e-06]], device='cuda:0')
Loss: 1.0196391344070435


Running epoch 0, step 763, batch 763
Sampled inputs[:2]: tensor([[    0, 23809, 27646,  ...,   266,  3373,   554],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1803e-04, -1.0030e-03,  1.0899e-03,  ...,  1.2856e-04,
          1.7515e-03,  5.0541e-04],
        [-7.1973e-06, -4.2729e-06,  5.1558e-06,  ..., -6.7949e-06,
         -4.0270e-06, -5.6326e-06],
        [-1.2100e-05, -7.7039e-06,  8.7917e-06,  ..., -1.1116e-05,
         -6.7279e-06, -8.5682e-06],
        [-1.0952e-05, -6.2287e-06,  8.5384e-06,  ..., -1.0505e-05,
         -5.9307e-06, -9.3877e-06],
        [-1.7911e-05, -1.1876e-05,  1.2606e-05,  ..., -1.6421e-05,
         -1.0565e-05, -1.1295e-05]], device='cuda:0')
Loss: 1.0126068592071533


Running epoch 0, step 764, batch 764
Sampled inputs[:2]: tensor([[    0, 14867,   278,  ...,   674,   369,  4127],
        [    0,  1477,    12,  ..., 31038,   408,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5603e-04, -1.0815e-03,  1.3095e-03,  ...,  1.6992e-04,
          2.0358e-03,  5.9317e-04],
        [-9.1791e-06, -5.5619e-06,  6.4299e-06,  ..., -8.5682e-06,
         -5.1074e-06, -6.9961e-06],
        [-1.5467e-05, -1.0014e-05,  1.1027e-05,  ..., -1.4096e-05,
         -8.5831e-06, -1.0699e-05],
        [-1.3888e-05, -8.1211e-06,  1.0565e-05,  ..., -1.3188e-05,
         -7.5102e-06, -1.1548e-05],
        [-2.2858e-05, -1.5363e-05,  1.5810e-05,  ..., -2.0802e-05,
         -1.3441e-05, -1.4156e-05]], device='cuda:0')
Loss: 1.0481383800506592


Running epoch 0, step 765, batch 765
Sampled inputs[:2]: tensor([[    0,  4665,   909,  ...,  3607,   259,  1108],
        [    0,    14, 30840,  ...,   287,   932,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6958e-04, -1.1808e-03,  1.5454e-03,  ...,  2.0257e-04,
          2.2832e-03,  7.1376e-04],
        [-1.1250e-05, -6.9328e-06,  7.7784e-06,  ..., -1.0304e-05,
         -6.1579e-06, -8.2254e-06],
        [-1.8984e-05, -1.2428e-05,  1.3366e-05,  ..., -1.7062e-05,
         -1.0401e-05, -1.2696e-05],
        [-1.6898e-05, -1.0103e-05,  1.2636e-05,  ..., -1.5706e-05,
         -8.9705e-06, -1.3448e-05],
        [-2.8104e-05, -1.9044e-05,  1.9237e-05,  ..., -2.5272e-05,
         -1.6332e-05, -1.6898e-05]], device='cuda:0')
Loss: 1.0727779865264893


Running epoch 0, step 766, batch 766
Sampled inputs[:2]: tensor([[    0, 13649,  7841,  ...,   287,  4713,    14],
        [    0,   352,   266,  ...,  2416,   287,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1143e-03, -1.3770e-03,  1.8463e-03,  ...,  7.7698e-05,
          2.6430e-03,  1.0710e-03],
        [-1.3158e-05, -8.1472e-06,  9.0227e-06,  ..., -1.2018e-05,
         -7.1488e-06, -9.5293e-06],
        [-2.2173e-05, -1.4558e-05,  1.5467e-05,  ..., -1.9863e-05,
         -1.2070e-05, -1.4693e-05],
        [-1.9714e-05, -1.1839e-05,  1.4573e-05,  ..., -1.8239e-05,
         -1.0364e-05, -1.5505e-05],
        [-3.2902e-05, -2.2352e-05,  2.2337e-05,  ..., -2.9504e-05,
         -1.9014e-05, -1.9625e-05]], device='cuda:0')
Loss: 1.0396156311035156


Running epoch 0, step 767, batch 767
Sampled inputs[:2]: tensor([[    0,   824,   278,  ...,   266, 10997,   863],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3297e-03, -2.1226e-03,  2.2082e-03,  ...,  9.3733e-07,
          4.1781e-03,  1.8059e-03],
        [-1.4633e-05, -8.9481e-06,  1.0140e-05,  ..., -1.3657e-05,
         -8.2143e-06, -1.1146e-05],
        [-2.4647e-05, -1.6063e-05,  1.7360e-05,  ..., -2.2352e-05,
         -1.3739e-05, -1.6928e-05],
        [-2.1875e-05, -1.2890e-05,  1.6481e-05,  ..., -2.0757e-05,
         -1.1943e-05, -1.8172e-05],
        [-3.6672e-05, -2.4796e-05,  2.5079e-05,  ..., -3.3200e-05,
         -2.1681e-05, -2.2531e-05]], device='cuda:0')
Loss: 0.9921359419822693
Graident accumulation at epoch 0, step 767, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0152, -0.0280,  ...,  0.0288, -0.0149, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1727e-04, -5.8995e-05, -3.6207e-05,  ...,  7.7401e-05,
          1.8900e-04,  1.0262e-04],
        [-1.0474e-05, -4.9318e-06,  4.0226e-06,  ..., -1.1551e-05,
         -2.3682e-06, -5.7213e-06],
        [ 4.2455e-05,  3.1423e-05, -2.5055e-05,  ...,  3.2834e-05,
          2.3206e-05,  4.4341e-06],
        [-8.2759e-06, -5.9361e-06,  8.5805e-06,  ..., -1.0200e-05,
         -3.6853e-06, -1.0165e-05],
        [-4.5326e-05, -3.2204e-05,  2.4591e-05,  ..., -3.8040e-05,
         -2.1264e-05, -2.4896e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0250e-08, 5.0032e-08, 5.6541e-08,  ..., 1.8603e-08, 1.3010e-07,
         2.8771e-08],
        [6.9829e-11, 4.1041e-11, 9.6383e-12,  ..., 4.6808e-11, 9.0061e-12,
         1.7327e-11],
        [2.5350e-09, 1.2969e-09, 4.9850e-10,  ..., 1.7529e-09, 4.1099e-10,
         7.2322e-10],
        [6.1776e-10, 3.0761e-10, 9.5628e-11,  ..., 4.0442e-10, 1.1449e-10,
         1.7005e-10],
        [3.0954e-10, 1.6624e-10, 3.5012e-11,  ..., 2.2535e-10, 2.7562e-11,
         7.5512e-11]], device='cuda:0')
optimizer state dict: 96.0
lr: [1.4628782349517233e-05, 1.4628782349517233e-05]
scheduler_last_epoch: 96


Running epoch 0, step 768, batch 768
Sampled inputs[:2]: tensor([[   0, 1416,  367,  ...,  555,  764,  367],
        [   0,  437, 1916,  ...,   13, 1303, 2708]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6524e-05,  1.7855e-04,  2.0272e-05,  ...,  5.9829e-05,
         -1.4207e-04,  1.4094e-04],
        [-2.0862e-06, -1.4380e-06,  1.2144e-06,  ..., -1.7360e-06,
         -1.0729e-06, -1.0878e-06],
        [-3.7253e-06, -2.6375e-06,  2.2054e-06,  ..., -3.0845e-06,
         -1.9073e-06, -1.8775e-06],
        [-2.9206e-06, -1.9968e-06,  1.7658e-06,  ..., -2.4438e-06,
         -1.4529e-06, -1.6168e-06],
        [-5.4836e-06, -3.9339e-06,  3.1888e-06,  ..., -4.5896e-06,
         -2.9504e-06, -2.5630e-06]], device='cuda:0')
Loss: 1.0244797468185425


Running epoch 0, step 769, batch 769
Sampled inputs[:2]: tensor([[   0, 9116,  278,  ..., 6997, 3244, 1192],
        [   0,  894,   73,  ..., 2323,  909, 4103]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4789e-04,  3.0045e-04, -2.6767e-04,  ...,  5.9829e-05,
         -2.2387e-04,  2.7333e-05],
        [-4.1425e-06, -2.7865e-06,  2.3618e-06,  ..., -3.5465e-06,
         -2.1234e-06, -2.2426e-06],
        [ 1.0245e-04,  8.2617e-05, -4.9779e-07,  ...,  9.6098e-05,
          9.5583e-05,  2.5237e-05],
        [-5.8115e-06, -3.8669e-06,  3.4645e-06,  ..., -5.0217e-06,
         -2.8908e-06, -3.3751e-06],
        [-1.0818e-05, -7.6294e-06,  6.1840e-06,  ..., -9.2685e-06,
         -5.8115e-06, -5.2005e-06]], device='cuda:0')
Loss: 1.0237497091293335


Running epoch 0, step 770, batch 770
Sampled inputs[:2]: tensor([[    0,  9657,   300,  ...,    12,   271,   266],
        [    0,   199, 11296,  ...,   266, 10463,  8256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8249e-04,  2.2296e-04, -3.9675e-04,  ...,  9.8920e-05,
         -1.7254e-04,  2.4196e-05],
        [-6.2138e-06, -4.1276e-06,  3.4645e-06,  ..., -5.3570e-06,
         -3.2336e-06, -3.3528e-06],
        [ 9.8769e-05,  8.0158e-05,  1.5139e-06,  ...,  9.2909e-05,
          9.3616e-05,  2.3352e-05],
        [-8.7768e-06, -5.7518e-06,  5.1111e-06,  ..., -7.6145e-06,
         -4.4256e-06, -5.0664e-06],
        [-1.6302e-05, -1.1370e-05,  9.1642e-06,  ..., -1.4037e-05,
         -8.8662e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.038400411605835


Running epoch 0, step 771, batch 771
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,    14,   381,  ...,  7106,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6349e-04,  2.3033e-04, -1.9591e-04,  ...,  5.3933e-05,
         -2.2149e-05,  8.4264e-05],
        [-8.2701e-06, -5.5432e-06,  4.6417e-06,  ..., -7.2047e-06,
         -4.3064e-06, -4.4033e-06],
        [ 9.5133e-05,  7.7595e-05,  3.6447e-06,  ...,  8.9660e-05,
          9.1716e-05,  2.1571e-05],
        [-1.1697e-05, -7.7188e-06,  6.8471e-06,  ..., -1.0237e-05,
         -5.9083e-06, -6.6608e-06],
        [-2.1785e-05, -1.5303e-05,  1.2338e-05,  ..., -1.8954e-05,
         -1.1846e-05, -1.0327e-05]], device='cuda:0')
Loss: 1.084344506263733


Running epoch 0, step 772, batch 772
Sampled inputs[:2]: tensor([[   0, 6809,  344,  ...,   14, 1266,  795],
        [   0,   15, 4291,  ..., 1685,  278, 2101]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6922e-04,  2.3033e-04, -2.5853e-04,  ...,  1.6533e-04,
         -6.9511e-05,  1.9542e-04],
        [-1.0282e-05, -6.9290e-06,  5.7518e-06,  ..., -8.9854e-06,
         -5.4166e-06, -5.5879e-06],
        [ 9.1512e-05,  7.5062e-05,  5.7011e-06,  ...,  8.6531e-05,
          8.9779e-05,  1.9560e-05],
        [-1.4588e-05, -9.6709e-06,  8.5160e-06,  ..., -1.2785e-05,
         -7.4282e-06, -8.4564e-06],
        [-2.7061e-05, -1.9059e-05,  1.5303e-05,  ..., -2.3514e-05,
         -1.4782e-05, -1.3039e-05]], device='cuda:0')
Loss: 1.0233862400054932


Running epoch 0, step 773, batch 773
Sampled inputs[:2]: tensor([[    0,   508,  1548,  ...,   494, 10792,     9],
        [    0,  3235,   471,  ...,  1967,  4273,  2738]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8466e-04,  3.5242e-04, -3.6508e-04,  ...,  1.0241e-04,
         -1.1969e-04,  4.0078e-04],
        [-1.2144e-05, -8.1509e-06,  6.6645e-06,  ..., -1.0788e-05,
         -6.5491e-06, -6.7726e-06],
        [ 8.8159e-05,  7.2812e-05,  7.4669e-06,  ...,  8.3491e-05,
          8.7879e-05,  1.7667e-05],
        [-1.7241e-05, -1.1355e-05,  9.8720e-06,  ..., -1.5363e-05,
         -9.0152e-06, -1.0237e-05],
        [-3.2067e-05, -2.2441e-05,  1.7971e-05,  ..., -2.7984e-05,
         -1.7628e-05, -1.5616e-05]], device='cuda:0')
Loss: 1.0223655700683594


Running epoch 0, step 774, batch 774
Sampled inputs[:2]: tensor([[    0,   287,  7763,  ...,   689,  2409,   699],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0398e-04,  4.6228e-04, -3.8452e-04,  ...,  5.2745e-05,
         -1.1120e-04,  5.9883e-04],
        [-1.4111e-05, -9.4846e-06,  7.7821e-06,  ..., -1.2577e-05,
         -7.6517e-06, -7.9274e-06],
        [ 8.4583e-05,  7.0338e-05,  9.5381e-06,  ...,  8.0317e-05,
          8.5927e-05,  1.5685e-05],
        [-2.0117e-05, -1.3255e-05,  1.1571e-05,  ..., -1.7986e-05,
         -1.0565e-05, -1.2040e-05],
        [-3.7283e-05, -2.6107e-05,  2.0936e-05,  ..., -3.2604e-05,
         -2.0549e-05, -1.8299e-05]], device='cuda:0')
Loss: 1.0458590984344482


Running epoch 0, step 775, batch 775
Sampled inputs[:2]: tensor([[   0, 4213, 1921,  ..., 1340, 1049,  292],
        [   0,  969,  258,  ...,  726, 5303, 6512]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0121e-04,  3.9929e-04, -3.1349e-04,  ...,  2.5873e-05,
         -1.0114e-04,  5.8599e-04],
        [-1.6212e-05, -1.0990e-05,  8.9668e-06,  ..., -1.4417e-05,
         -8.7991e-06, -8.9705e-06],
        [ 8.0768e-05,  6.7567e-05,  1.1729e-05,  ...,  7.6964e-05,
          8.3855e-05,  1.3815e-05],
        [-2.3112e-05, -1.5371e-05,  1.3322e-05,  ..., -2.0593e-05,
         -1.2122e-05, -1.3612e-05],
        [-4.2975e-05, -3.0279e-05,  2.4155e-05,  ..., -3.7611e-05,
         -2.3752e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.0926834344863892
Graident accumulation at epoch 0, step 775, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0280,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3543e-04, -1.3166e-05, -6.3935e-05,  ...,  7.2248e-05,
          1.5998e-04,  1.5096e-04],
        [-1.1048e-05, -5.5375e-06,  4.5170e-06,  ..., -1.1838e-05,
         -3.0113e-06, -6.0463e-06],
        [ 4.6286e-05,  3.5038e-05, -2.1377e-05,  ...,  3.7247e-05,
          2.9271e-05,  5.3722e-06],
        [-9.7595e-06, -6.8795e-06,  9.0546e-06,  ..., -1.1239e-05,
         -4.5290e-06, -1.0510e-05],
        [-4.5091e-05, -3.2011e-05,  2.4548e-05,  ..., -3.7997e-05,
         -2.1513e-05, -2.4499e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0561e-08, 5.0141e-08, 5.6583e-08,  ..., 1.8585e-08, 1.2998e-07,
         2.9085e-08],
        [7.0022e-11, 4.1121e-11, 9.7091e-12,  ..., 4.6969e-11, 9.0745e-12,
         1.7390e-11],
        [2.5390e-09, 1.3002e-09, 4.9814e-10,  ..., 1.7571e-09, 4.1761e-10,
         7.2269e-10],
        [6.1767e-10, 3.0754e-10, 9.5710e-11,  ..., 4.0444e-10, 1.1452e-10,
         1.7006e-10],
        [3.1108e-10, 1.6699e-10, 3.5560e-11,  ..., 2.2654e-10, 2.8099e-11,
         7.5874e-11]], device='cuda:0')
optimizer state dict: 97.0
lr: [1.4518855009476186e-05, 1.4518855009476186e-05]
scheduler_last_epoch: 97


Running epoch 0, step 776, batch 776
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0,   792,   287,  ...,   706,  9751,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0796e-05,  1.0831e-04, -1.3085e-04,  ...,  5.4995e-05,
         -1.2297e-04,  1.0284e-05],
        [-2.0564e-06, -1.4380e-06,  1.1250e-06,  ..., -1.8403e-06,
         -1.1623e-06, -1.0431e-06],
        [-3.7104e-06, -2.6375e-06,  2.0862e-06,  ..., -3.3230e-06,
         -2.0713e-06, -1.8477e-06],
        [-2.9057e-06, -2.0117e-06,  1.6540e-06,  ..., -2.6077e-06,
         -1.5870e-06, -1.5721e-06],
        [-5.3346e-06, -3.8445e-06,  2.9653e-06,  ..., -4.7982e-06,
         -3.0994e-06, -2.5034e-06]], device='cuda:0')
Loss: 1.0462132692337036


Running epoch 0, step 777, batch 777
Sampled inputs[:2]: tensor([[    0,   531,  9804,  ...,  1027,   360,  1576],
        [    0,   266,  5232,  ...,  2719,    13, 25385]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4965e-05,  9.0639e-05, -3.7976e-04,  ...,  7.4875e-06,
         -1.1013e-04, -3.6890e-05],
        [-4.0084e-06, -2.7493e-06,  2.1681e-06,  ..., -3.5763e-06,
         -2.2724e-06, -2.0564e-06],
        [-7.3910e-06, -5.1409e-06,  4.0978e-06,  ..., -6.5416e-06,
         -4.1276e-06, -3.6731e-06],
        [-5.7369e-06, -3.8669e-06,  3.2037e-06,  ..., -5.1260e-06,
         -3.1665e-06, -3.1367e-06],
        [-1.0788e-05, -7.6145e-06,  5.9307e-06,  ..., -9.5963e-06,
         -6.2287e-06, -5.0515e-06]], device='cuda:0')
Loss: 1.0307114124298096


Running epoch 0, step 778, batch 778
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0,  689, 3953,  ...,  461,  943,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7099e-05,  2.0696e-05, -4.0815e-04,  ...,  2.2465e-05,
         -1.4367e-04, -7.5519e-05],
        [-6.0499e-06, -4.2245e-06,  3.3006e-06,  ..., -5.4017e-06,
         -3.4124e-06, -3.0324e-06],
        [-1.1161e-05, -7.8976e-06,  6.2287e-06,  ..., -9.8944e-06,
         -6.2138e-06, -5.4538e-06],
        [-8.6427e-06, -5.9530e-06,  4.8727e-06,  ..., -7.7188e-06,
         -4.7237e-06, -4.6268e-06],
        [-1.6332e-05, -1.1727e-05,  9.0301e-06,  ..., -1.4544e-05,
         -9.4175e-06, -7.5251e-06]], device='cuda:0')
Loss: 1.0423749685287476


Running epoch 0, step 779, batch 779
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   274, 30228,   287],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2627e-05, -2.1699e-05, -4.8427e-04,  ...,  2.4845e-05,
         -2.2823e-04, -5.3629e-05],
        [-8.0913e-06, -5.6028e-06,  4.3437e-06,  ..., -7.1675e-06,
         -4.5002e-06, -4.0978e-06],
        [-1.4976e-05, -1.0520e-05,  8.2403e-06,  ..., -1.3158e-05,
         -8.1956e-06, -7.3761e-06],
        [-1.1548e-05, -7.8753e-06,  6.4000e-06,  ..., -1.0252e-05,
         -6.2212e-06, -6.2585e-06],
        [-2.1875e-05, -1.5602e-05,  1.1936e-05,  ..., -1.9312e-05,
         -1.2368e-05, -1.0148e-05]], device='cuda:0')
Loss: 1.0489827394485474


Running epoch 0, step 780, batch 780
Sampled inputs[:2]: tensor([[    0,  3380,  1197,  ...,   631,   369,  3123],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4993e-05,  1.2264e-04, -4.6007e-04,  ...,  5.2838e-05,
         -2.9899e-04, -5.2161e-05],
        [-1.0341e-05, -7.1675e-06,  5.4240e-06,  ..., -9.0078e-06,
         -5.6997e-06, -5.2378e-06],
        [-1.9237e-05, -1.3545e-05,  1.0341e-05,  ..., -1.6645e-05,
         -1.0476e-05, -9.5069e-06],
        [-1.4603e-05, -9.9763e-06,  7.9051e-06,  ..., -1.2740e-05,
         -7.8008e-06, -7.8678e-06],
        [-2.8253e-05, -2.0161e-05,  1.5050e-05,  ..., -2.4527e-05,
         -1.5855e-05, -1.3173e-05]], device='cuda:0')
Loss: 1.0359251499176025


Running epoch 0, step 781, batch 781
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1881e-05,  8.2943e-05, -4.7301e-04,  ...,  1.1664e-04,
         -2.7496e-04, -1.3274e-04],
        [-1.2517e-05, -8.7097e-06,  6.5118e-06,  ..., -1.0855e-05,
         -6.8173e-06, -6.2138e-06],
        [-2.3291e-05, -1.6466e-05,  1.2398e-05,  ..., -2.0087e-05,
         -1.2562e-05, -1.1288e-05],
        [-1.7598e-05, -1.2077e-05,  9.4324e-06,  ..., -1.5274e-05,
         -9.2909e-06, -9.2760e-06],
        [-3.4183e-05, -2.4483e-05,  1.8060e-05,  ..., -2.9624e-05,
         -1.9029e-05, -1.5646e-05]], device='cuda:0')
Loss: 1.0373306274414062


Running epoch 0, step 782, batch 782
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0,  1340,   800,  ...,   259, 13583,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4219e-06,  8.8166e-05, -5.5616e-04,  ...,  1.8469e-04,
         -3.6986e-04, -2.6163e-04],
        [-1.4514e-05, -1.0110e-05,  7.6070e-06,  ..., -1.2636e-05,
         -7.8976e-06, -7.2643e-06],
        [-2.7001e-05, -1.9118e-05,  1.4484e-05,  ..., -2.3350e-05,
         -1.4514e-05, -1.3158e-05],
        [-2.0459e-05, -1.4059e-05,  1.1057e-05,  ..., -1.7852e-05,
         -1.0811e-05, -1.0885e-05],
        [-3.9607e-05, -2.8387e-05,  2.1100e-05,  ..., -3.4392e-05,
         -2.1964e-05, -1.8209e-05]], device='cuda:0')
Loss: 1.0238759517669678


Running epoch 0, step 783, batch 783
Sampled inputs[:2]: tensor([[   0,  437,  638,  ..., 4514,   14,  333],
        [   0, 5902,  518,  ..., 3126,   12,  497]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6107e-05,  2.2846e-04, -6.8749e-04,  ...,  2.2486e-04,
         -3.4147e-04, -2.1569e-04],
        [-1.6451e-05, -1.1422e-05,  8.6576e-06,  ..., -1.4395e-05,
         -9.0227e-06, -8.2925e-06],
        [-3.0532e-05, -2.1547e-05,  1.6466e-05,  ..., -2.6479e-05,
         -1.6496e-05, -1.4924e-05],
        [-2.3216e-05, -1.5877e-05,  1.2629e-05,  ..., -2.0370e-05,
         -1.2368e-05, -1.2465e-05],
        [-4.4763e-05, -3.1993e-05,  2.3976e-05,  ..., -3.8952e-05,
         -2.4945e-05, -2.0608e-05]], device='cuda:0')
Loss: 1.0349547863006592
Graident accumulation at epoch 0, step 783, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0341, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0280,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2349e-04,  1.0997e-05, -1.2629e-04,  ...,  8.7509e-05,
          1.0984e-04,  1.1430e-04],
        [-1.1588e-05, -6.1260e-06,  4.9311e-06,  ..., -1.2093e-05,
         -3.6124e-06, -6.2709e-06],
        [ 3.8604e-05,  2.9379e-05, -1.7592e-05,  ...,  3.0875e-05,
          2.4694e-05,  3.3426e-06],
        [-1.1105e-05, -7.7793e-06,  9.4120e-06,  ..., -1.2152e-05,
         -5.3129e-06, -1.0705e-05],
        [-4.5058e-05, -3.2010e-05,  2.4490e-05,  ..., -3.8093e-05,
         -2.1856e-05, -2.4110e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0511e-08, 5.0143e-08, 5.6999e-08,  ..., 1.8617e-08, 1.2997e-07,
         2.9103e-08],
        [7.0222e-11, 4.1210e-11, 9.7743e-12,  ..., 4.7129e-11, 9.1468e-12,
         1.7442e-11],
        [2.5374e-09, 1.2993e-09, 4.9791e-10,  ..., 1.7560e-09, 4.1746e-10,
         7.2219e-10],
        [6.1759e-10, 3.0748e-10, 9.5774e-11,  ..., 4.0445e-10, 1.1456e-10,
         1.7005e-10],
        [3.1277e-10, 1.6785e-10, 3.6100e-11,  ..., 2.2783e-10, 2.8693e-11,
         7.6223e-11]], device='cuda:0')
optimizer state dict: 98.0
lr: [1.4408237148944047e-05, 1.4408237148944047e-05]
scheduler_last_epoch: 98


Running epoch 0, step 784, batch 784
Sampled inputs[:2]: tensor([[    0,   287,  4599,  ..., 11812,   266,  1036],
        [    0,  2255, 21868,  ...,   591,  5902,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6628e-06,  1.3938e-04, -5.9255e-05,  ..., -1.2377e-04,
         -6.5862e-05,  2.9977e-05],
        [-2.0713e-06, -1.4156e-06,  1.0505e-06,  ..., -1.8328e-06,
         -1.1027e-06, -1.0580e-06],
        [-3.9041e-06, -2.7269e-06,  2.0415e-06,  ..., -3.4571e-06,
         -2.0564e-06, -1.9372e-06],
        [-2.9206e-06, -1.9670e-06,  1.5274e-06,  ..., -2.6077e-06,
         -1.5348e-06, -1.5721e-06],
        [-5.5134e-06, -3.9041e-06,  2.8759e-06,  ..., -4.8876e-06,
         -3.0100e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.0459363460540771


Running epoch 0, step 785, batch 785
Sampled inputs[:2]: tensor([[   0,  694, 2326,  ...,  278, 1781, 9660],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5537e-06,  1.2346e-04, -7.2404e-06,  ..., -1.4933e-04,
         -1.0752e-04,  1.2071e-04],
        [-4.0680e-06, -2.8089e-06,  2.1011e-06,  ..., -3.6284e-06,
         -2.1681e-06, -2.0713e-06],
        [-7.7188e-06, -5.4240e-06,  4.1127e-06,  ..., -6.8545e-06,
         -4.0531e-06, -3.8221e-06],
        [-5.7518e-06, -3.9190e-06,  3.0696e-06,  ..., -5.1707e-06,
         -3.0100e-06, -3.0994e-06],
        [-1.0848e-05, -7.7188e-06,  5.7667e-06,  ..., -9.6262e-06,
         -5.8562e-06, -5.0962e-06]], device='cuda:0')
Loss: 1.0360356569290161


Running epoch 0, step 786, batch 786
Sampled inputs[:2]: tensor([[    0,    13, 23904,  ...,   560,  8840,    26],
        [    0,   275,  4452,  ...,    12,  3516,  5227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5963e-05,  1.4381e-04, -3.8543e-05,  ..., -1.4153e-04,
         -2.4525e-04,  1.7332e-04],
        [-6.0499e-06, -4.1798e-06,  3.1739e-06,  ..., -5.3421e-06,
         -3.1367e-06, -3.1218e-06],
        [-1.1563e-05, -8.1211e-06,  6.2436e-06,  ..., -1.0133e-05,
         -5.9009e-06, -5.8189e-06],
        [-8.6278e-06, -5.8860e-06,  4.6715e-06,  ..., -7.6592e-06,
         -4.3660e-06, -4.7088e-06],
        [-1.6212e-05, -1.1533e-05,  8.7172e-06,  ..., -1.4246e-05,
         -8.5384e-06, -7.7486e-06]], device='cuda:0')
Loss: 1.0366624593734741


Running epoch 0, step 787, batch 787
Sampled inputs[:2]: tensor([[   0, 4448,   12,  ..., 3183,  328, 9559],
        [   0,  756,  401,  ...,  271, 7272, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0659e-04,  3.1228e-04, -3.5338e-04,  ..., -2.2207e-04,
         -8.1393e-04,  2.0731e-04],
        [-7.9349e-06, -5.3719e-06,  4.1798e-06,  ..., -7.0259e-06,
         -4.2990e-06, -4.4629e-06],
        [-1.5229e-05, -1.0520e-05,  8.2701e-06,  ..., -1.3292e-05,
         -8.0615e-06, -8.2627e-06],
        [-1.1370e-05, -7.5176e-06,  6.2585e-06,  ..., -1.0163e-05,
         -6.0499e-06, -6.8694e-06],
        [-2.1398e-05, -1.4991e-05,  1.1548e-05,  ..., -1.8686e-05,
         -1.1653e-05, -1.0967e-05]], device='cuda:0')
Loss: 1.0175390243530273


Running epoch 0, step 788, batch 788
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   593,   300,  ...,   278,  4694,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9081e-04,  3.0145e-04, -5.4008e-04,  ..., -1.6311e-04,
         -9.3527e-04,  2.9452e-04],
        [-1.0021e-05, -6.8396e-06,  5.2229e-06,  ..., -8.8438e-06,
         -5.3942e-06, -5.4613e-06],
        [-1.9163e-05, -1.3351e-05,  1.0297e-05,  ..., -1.6734e-05,
         -1.0118e-05, -1.0133e-05],
        [-1.4216e-05, -9.4995e-06,  7.7263e-06,  ..., -1.2666e-05,
         -7.5102e-06, -8.3223e-06],
        [-2.7061e-05, -1.9103e-05,  1.4439e-05,  ..., -2.3663e-05,
         -1.4707e-05, -1.3515e-05]], device='cuda:0')
Loss: 1.0538848638534546


Running epoch 0, step 789, batch 789
Sampled inputs[:2]: tensor([[    0, 21448,   344,  ...,   365,  1501,   271],
        [    0,   565,    27,  ...,    88,  4451,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5895e-04,  2.9760e-04, -6.7550e-04,  ..., -1.4174e-04,
         -1.0614e-03,  2.0954e-04],
        [-1.2033e-05, -8.2031e-06,  6.2138e-06,  ..., -1.0610e-05,
         -6.4969e-06, -6.5863e-06],
        [-2.3007e-05, -1.5989e-05,  1.2249e-05,  ..., -2.0072e-05,
         -1.2174e-05, -1.2234e-05],
        [-1.7047e-05, -1.1384e-05,  9.1642e-06,  ..., -1.5154e-05,
         -8.9929e-06, -9.9987e-06],
        [-3.2514e-05, -2.2888e-05,  1.7211e-05,  ..., -2.8402e-05,
         -1.7717e-05, -1.6332e-05]], device='cuda:0')
Loss: 1.0302386283874512


Running epoch 0, step 790, batch 790
Sampled inputs[:2]: tensor([[    0,  1978, 20360,  ...,   898,   699, 10262],
        [    0, 13081,   278,  ...,   368,   266,  1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7512e-04,  3.0317e-04, -7.6935e-04,  ..., -3.0128e-04,
         -1.0236e-03,  1.5197e-04],
        [-1.4044e-05, -9.6112e-06,  7.2718e-06,  ..., -1.2442e-05,
         -7.6219e-06, -7.6294e-06],
        [-2.6792e-05, -1.8671e-05,  1.4260e-05,  ..., -2.3484e-05,
         -1.4246e-05, -1.4156e-05],
        [-1.9848e-05, -1.3307e-05,  1.0677e-05,  ..., -1.7717e-05,
         -1.0513e-05, -1.1548e-05],
        [-3.7968e-05, -2.6792e-05,  2.0087e-05,  ..., -3.3349e-05,
         -2.0802e-05, -1.8969e-05]], device='cuda:0')
Loss: 1.0479602813720703


Running epoch 0, step 791, batch 791
Sampled inputs[:2]: tensor([[   0,  320, 4886,  ...,   14,  333,  199],
        [   0, 3159,  278,  ...,  266, 2545,  863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7667e-04,  3.4380e-04, -6.8740e-04,  ..., -3.0199e-04,
         -1.1099e-03,  3.0415e-04],
        [-1.6026e-05, -1.0997e-05,  8.3074e-06,  ..., -1.4208e-05,
         -8.6799e-06, -8.6799e-06],
        [-3.0577e-05, -2.1353e-05,  1.6287e-05,  ..., -2.6822e-05,
         -1.6242e-05, -1.6108e-05],
        [-2.2650e-05, -1.5229e-05,  1.2174e-05,  ..., -2.0206e-05,
         -1.1966e-05, -1.3106e-05],
        [-4.3422e-05, -3.0696e-05,  2.2992e-05,  ..., -3.8177e-05,
         -2.3752e-05, -2.1607e-05]], device='cuda:0')
Loss: 1.0427353382110596
Graident accumulation at epoch 0, step 791, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0165],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.3477e-05,  4.4277e-05, -1.8240e-04,  ...,  4.8559e-05,
         -1.2136e-05,  1.3328e-04],
        [-1.2032e-05, -6.6131e-06,  5.2687e-06,  ..., -1.2305e-05,
         -4.1192e-06, -6.5118e-06],
        [ 3.1686e-05,  2.4306e-05, -1.4205e-05,  ...,  2.5105e-05,
          2.0600e-05,  1.3976e-06],
        [-1.2260e-05, -8.5243e-06,  9.6882e-06,  ..., -1.2958e-05,
         -5.9781e-06, -1.0945e-05],
        [-4.4895e-05, -3.1878e-05,  2.4341e-05,  ..., -3.8101e-05,
         -2.2046e-05, -2.3859e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0793e-08, 5.0211e-08, 5.7414e-08,  ..., 1.8690e-08, 1.3107e-07,
         2.9166e-08],
        [7.0409e-11, 4.1290e-11, 9.8336e-12,  ..., 4.7284e-11, 9.2130e-12,
         1.7500e-11],
        [2.5358e-09, 1.2985e-09, 4.9768e-10,  ..., 1.7550e-09, 4.1731e-10,
         7.2173e-10],
        [6.1749e-10, 3.0741e-10, 9.5826e-11,  ..., 4.0446e-10, 1.1459e-10,
         1.7005e-10],
        [3.1435e-10, 1.6862e-10, 3.6592e-11,  ..., 2.2906e-10, 2.9228e-11,
         7.6613e-11]], device='cuda:0')
optimizer state dict: 99.0
lr: [1.4296945671295503e-05, 1.4296945671295503e-05]
scheduler_last_epoch: 99


Running epoch 0, step 792, batch 792
Sampled inputs[:2]: tensor([[   0,  341, 2802,  ..., 1798,   12,  266],
        [   0, 3406,  300,  ..., 1726, 3521, 4481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6116e-05,  2.1730e-04,  1.7014e-05,  ..., -6.5778e-06,
         -1.7454e-04, -1.8535e-05],
        [-1.7807e-06, -1.2740e-06,  9.5367e-07,  ..., -1.6466e-06,
         -9.3132e-07, -1.0803e-06],
        [-3.5465e-06, -2.5630e-06,  1.9670e-06,  ..., -3.1888e-06,
         -1.7807e-06, -2.0564e-06],
        [-2.5928e-06, -1.8179e-06,  1.4380e-06,  ..., -2.3991e-06,
         -1.3039e-06, -1.6838e-06],
        [-4.7684e-06, -3.5018e-06,  2.6524e-06,  ..., -4.2915e-06,
         -2.4885e-06, -2.5779e-06]], device='cuda:0')
Loss: 1.014296531677246


Running epoch 0, step 793, batch 793
Sampled inputs[:2]: tensor([[    0,    12,   546,  ..., 24994, 31107,   266],
        [    0,  4868,  3106,  ...,  2637,   278,   521]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5475e-05,  2.1730e-04, -3.2071e-04,  ...,  6.2093e-05,
         -3.2991e-04, -6.6432e-05],
        [-3.5241e-06, -2.4959e-06,  1.9893e-06,  ..., -3.2485e-06,
         -1.7993e-06, -2.1458e-06],
        [-6.8843e-06, -4.9323e-06,  3.9935e-06,  ..., -6.1840e-06,
         -3.3900e-06, -3.9935e-06],
        [-5.2452e-06, -3.5986e-06,  3.1143e-06,  ..., -4.8578e-06,
         -2.5705e-06, -3.4794e-06],
        [-9.3281e-06, -6.8098e-06,  5.4091e-06,  ..., -8.4043e-06,
         -4.7982e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.0194666385650635


Running epoch 0, step 794, batch 794
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,   287, 10086,   300],
        [    0,   790, 43134,  ...,   446,   381,  1034]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2539e-05,  3.2482e-04, -2.9804e-04,  ..., -1.2399e-05,
         -1.2842e-04,  9.5497e-05],
        [-5.4762e-06, -3.8669e-06,  2.9951e-06,  ..., -4.9993e-06,
         -2.8796e-06, -3.3230e-06],
        [-1.0788e-05, -7.7039e-06,  6.0946e-06,  ..., -9.6262e-06,
         -5.5209e-06, -6.2585e-06],
        [-8.0913e-06, -5.5507e-06,  4.6268e-06,  ..., -7.4059e-06,
         -4.1127e-06, -5.2676e-06],
        [-1.4633e-05, -1.0625e-05,  8.2701e-06,  ..., -1.3083e-05,
         -7.7784e-06, -7.9721e-06]], device='cuda:0')
Loss: 1.0563642978668213


Running epoch 0, step 795, batch 795
Sampled inputs[:2]: tensor([[    0,   858,    13,  ...,  2253,   847,   300],
        [    0,    14,  8058,  ..., 10316,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5728e-05,  4.6044e-04, -3.2093e-04,  ...,  5.2757e-05,
         -2.6912e-04,  1.6648e-04],
        [-7.3761e-06, -5.2378e-06,  4.0233e-06,  ..., -6.7055e-06,
         -3.9078e-06, -4.5076e-06],
        [-1.4499e-05, -1.0401e-05,  8.1658e-06,  ..., -1.2890e-05,
         -7.4878e-06, -8.4937e-06],
        [-1.0878e-05, -7.5027e-06,  6.1914e-06,  ..., -9.8944e-06,
         -5.5656e-06, -7.1079e-06],
        [-1.9729e-05, -1.4380e-05,  1.1101e-05,  ..., -1.7583e-05,
         -1.0580e-05, -1.0863e-05]], device='cuda:0')
Loss: 1.035063624382019


Running epoch 0, step 796, batch 796
Sampled inputs[:2]: tensor([[   0, 4929, 4214,  ..., 1172,  591, 4422],
        [   0,  669,   14,  ...,  596,  292,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9628e-04,  5.3313e-04, -3.9139e-04,  ...,  1.4842e-04,
         -5.1592e-04,  2.2057e-04],
        [-9.2760e-06, -6.5193e-06,  5.0366e-06,  ..., -8.3447e-06,
         -4.8615e-06, -5.7369e-06],
        [-1.8433e-05, -1.3098e-05,  1.0327e-05,  ..., -1.6227e-05,
         -9.3952e-06, -1.0952e-05],
        [-1.3724e-05, -9.3579e-06,  7.7486e-06,  ..., -1.2353e-05,
         -6.9290e-06, -9.0748e-06],
        [-2.4915e-05, -1.8001e-05,  1.3918e-05,  ..., -2.1994e-05,
         -1.3217e-05, -1.3903e-05]], device='cuda:0')
Loss: 1.013648509979248


Running epoch 0, step 797, batch 797
Sampled inputs[:2]: tensor([[    0,  4988, 36842,  ...,  7630, 18362,    13],
        [    0,  3353,    17,  ...,   596,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3863e-04,  5.6619e-04, -3.9283e-04,  ...,  1.1237e-04,
         -6.1042e-04,  1.9884e-04],
        [-1.1243e-05, -7.9498e-06,  6.1393e-06,  ..., -1.0043e-05,
         -5.8971e-06, -6.8024e-06],
        [ 6.8727e-05,  5.5995e-05, -7.5078e-06,  ...,  3.0347e-05,
          3.7499e-05,  2.5187e-05],
        [-1.6615e-05, -1.1429e-05,  9.4101e-06,  ..., -1.4842e-05,
         -8.3894e-06, -1.0729e-05],
        [-3.0160e-05, -2.1905e-05,  1.6898e-05,  ..., -2.6554e-05,
         -1.6063e-05, -1.6585e-05]], device='cuda:0')
Loss: 1.0812638998031616


Running epoch 0, step 798, batch 798
Sampled inputs[:2]: tensor([[   0, 4191,  368,  ...,  367, 4182,   14],
        [   0,  266, 6737,  ..., 2409,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1915e-04,  6.6696e-04, -4.6227e-04,  ...,  1.0410e-04,
         -5.9873e-04,  3.9515e-04],
        [-1.3165e-05, -9.3430e-06,  7.1451e-06,  ..., -1.1712e-05,
         -6.8732e-06, -8.0243e-06],
        [ 6.4852e-05,  5.3148e-05, -5.4366e-06,  ...,  2.7024e-05,
          3.5576e-05,  2.2802e-05],
        [-1.9416e-05, -1.3411e-05,  1.0915e-05,  ..., -1.7256e-05,
         -9.7603e-06, -1.2614e-05],
        [-3.5375e-05, -2.5809e-05,  1.9670e-05,  ..., -3.1054e-05,
         -1.8746e-05, -1.9610e-05]], device='cuda:0')
Loss: 1.0306142568588257


Running epoch 0, step 799, batch 799
Sampled inputs[:2]: tensor([[    0,    14,   759,  ..., 15790,   278,   706],
        [    0,   360,  5323,  ..., 29974,    25,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2977e-04,  6.7489e-04, -3.9913e-04,  ...,  1.7912e-04,
         -6.5005e-04,  3.2367e-04],
        [-1.5087e-05, -1.0729e-05,  8.2031e-06,  ..., -1.3389e-05,
         -7.8231e-06, -9.1270e-06],
        [ 6.0978e-05,  5.0317e-05, -3.2610e-06,  ...,  2.3657e-05,
          3.3691e-05,  2.0612e-05],
        [-2.2262e-05, -1.5438e-05,  1.2524e-05,  ..., -1.9744e-05,
         -1.1124e-05, -1.4365e-05],
        [-4.0531e-05, -2.9624e-05,  2.2560e-05,  ..., -3.5584e-05,
         -2.1383e-05, -2.2367e-05]], device='cuda:0')
Loss: 1.0440095663070679
Graident accumulation at epoch 0, step 799, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.5153e-05,  1.0734e-04, -2.0407e-04,  ...,  6.1616e-05,
         -7.5928e-05,  1.5232e-04],
        [-1.2338e-05, -7.0247e-06,  5.5621e-06,  ..., -1.2413e-05,
         -4.4895e-06, -6.7733e-06],
        [ 3.4615e-05,  2.6907e-05, -1.3110e-05,  ...,  2.4960e-05,
          2.1910e-05,  3.3190e-06],
        [-1.3260e-05, -9.2156e-06,  9.9719e-06,  ..., -1.3636e-05,
         -6.4927e-06, -1.1287e-05],
        [-4.4458e-05, -3.1653e-05,  2.4163e-05,  ..., -3.7849e-05,
         -2.1980e-05, -2.3710e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0795e-08, 5.0616e-08, 5.7516e-08,  ..., 1.8703e-08, 1.3136e-07,
         2.9242e-08],
        [7.0566e-11, 4.1363e-11, 9.8910e-12,  ..., 4.7416e-11, 9.2650e-12,
         1.7566e-11],
        [2.5370e-09, 1.2997e-09, 4.9719e-10,  ..., 1.7538e-09, 4.1803e-10,
         7.2143e-10],
        [6.1737e-10, 3.0734e-10, 9.5887e-11,  ..., 4.0444e-10, 1.1460e-10,
         1.7009e-10],
        [3.1568e-10, 1.6933e-10, 3.7065e-11,  ..., 2.3009e-10, 2.9656e-11,
         7.7037e-11]], device='cuda:0')
optimizer state dict: 100.0
lr: [1.418499758283982e-05, 1.418499758283982e-05]
scheduler_last_epoch: 100
Epoch 0 | Batch 799/1048 | Training PPL: 4238.973968821384 | time 85.26517367362976
Saving checkpoint at epoch 0, step 799, batch 799
Epoch 0 | Validation PPL: 7.784432692690202 | Learning rate: 1.418499758283982e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_799, AFTER epoch 0, step 799


Running epoch 0, step 800, batch 800
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,   680,   993,  ...,   699, 11426,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2123e-05,  6.6434e-05,  5.2496e-05,  ...,  1.6359e-05,
         -1.3522e-04,  8.8510e-05],
        [-1.9073e-06, -1.3337e-06,  1.0729e-06,  ..., -1.6093e-06,
         -9.3877e-07, -1.4827e-06],
        [-3.9935e-06, -2.8312e-06,  2.2799e-06,  ..., -3.2932e-06,
         -1.9222e-06, -3.0100e-06],
        [-2.8610e-06, -1.9372e-06,  1.6913e-06,  ..., -2.3991e-06,
         -1.3262e-06, -2.3842e-06],
        [-5.0366e-06, -3.6359e-06,  2.8461e-06,  ..., -4.2021e-06,
         -2.5630e-06, -3.5763e-06]], device='cuda:0')
Loss: 1.022029161453247


Running epoch 0, step 801, batch 801
Sampled inputs[:2]: tensor([[    0,    13,  1581,  ...,    13, 11628, 14876],
        [    0,    14,   475,  ...,  6895,  5842,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8526e-05,  1.1432e-04, -5.5651e-05,  ...,  1.3502e-05,
         -2.6995e-04, -9.6835e-06],
        [-3.5539e-06, -2.5034e-06,  2.0787e-06,  ..., -3.1739e-06,
         -1.7881e-06, -2.6226e-06],
        [-7.3463e-06, -5.2601e-06,  4.3958e-06,  ..., -6.3926e-06,
         -3.5837e-06, -5.2303e-06],
        [-5.4240e-06, -3.6806e-06,  3.3602e-06,  ..., -4.8578e-06,
         -2.5779e-06, -4.3660e-06],
        [-9.3579e-06, -6.8247e-06,  5.5283e-06,  ..., -8.1956e-06,
         -4.8280e-06, -6.2287e-06]], device='cuda:0')
Loss: 1.0430973768234253


Running epoch 0, step 802, batch 802
Sampled inputs[:2]: tensor([[    0,  7120,   344,  ...,  6273,    52, 22639],
        [    0,  1874,   300,  ...,    14,  5372,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3973e-04,  3.7458e-04,  5.8730e-05,  ..., -1.7368e-05,
         -2.0776e-04,  1.4717e-04],
        [-5.4091e-06, -3.7774e-06,  3.0473e-06,  ..., -4.7684e-06,
         -2.7195e-06, -3.9712e-06],
        [-1.1131e-05, -7.9125e-06,  6.4671e-06,  ..., -9.5814e-06,
         -5.4464e-06, -7.8678e-06],
        [-8.1509e-06, -5.5134e-06,  4.8429e-06,  ..., -7.2122e-06,
         -3.9041e-06, -6.4522e-06],
        [-1.4275e-05, -1.0341e-05,  8.2403e-06,  ..., -1.2368e-05,
         -7.3463e-06, -9.4473e-06]], device='cuda:0')
Loss: 1.0159025192260742


Running epoch 0, step 803, batch 803
Sampled inputs[:2]: tensor([[    0,  1934,  2413,  ..., 19697,    13, 16325],
        [    0,   266, 10262,  ...,   271,  3437,  4392]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5129e-04,  3.7458e-04,  2.8375e-04,  ...,  2.8486e-05,
          2.0949e-07,  3.4374e-04],
        [-7.3016e-06, -5.0217e-06,  3.9935e-06,  ..., -6.3851e-06,
         -3.7625e-06, -5.3719e-06],
        [-1.5125e-05, -1.0595e-05,  8.5533e-06,  ..., -1.2919e-05,
         -7.5921e-06, -1.0714e-05],
        [-1.0878e-05, -7.2643e-06,  6.2585e-06,  ..., -9.5367e-06,
         -5.3495e-06, -8.5980e-06],
        [-1.9372e-05, -1.3813e-05,  1.0893e-05,  ..., -1.6630e-05,
         -1.0177e-05, -1.2845e-05]], device='cuda:0')
Loss: 1.0226401090621948


Running epoch 0, step 804, batch 804
Sampled inputs[:2]: tensor([[    0,   445,     8,  ...,    13, 25386,    17],
        [    0,   380,   341,  ...,   955,   644,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9794e-04,  2.8517e-04,  3.8833e-04,  ...,  1.8151e-04,
         -9.5687e-05,  5.4414e-04],
        [-9.0972e-06, -6.3330e-06,  5.0887e-06,  ..., -7.9200e-06,
         -4.6305e-06, -6.6459e-06],
        [ 3.8043e-05,  2.6160e-05, -1.2567e-05,  ...,  7.1944e-05,
          3.9772e-05,  2.5478e-05],
        [-1.3590e-05, -9.2015e-06,  7.9647e-06,  ..., -1.1846e-05,
         -6.6087e-06, -1.0654e-05],
        [-2.3931e-05, -1.7285e-05,  1.3709e-05,  ..., -2.0534e-05,
         -1.2502e-05, -1.5810e-05]], device='cuda:0')
Loss: 1.0177772045135498


Running epoch 0, step 805, batch 805
Sampled inputs[:2]: tensor([[    0,    13,   711,  ...,   591,   953,   352],
        [    0,    12,  6426,  ...,  2629, 13422,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5049e-04,  5.1815e-04,  3.3723e-04,  ...,  9.7089e-05,
         -2.3279e-04,  5.7560e-04],
        [-1.0751e-05, -7.5474e-06,  6.1095e-06,  ..., -9.4548e-06,
         -5.5842e-06, -7.8678e-06],
        [ 3.4691e-05,  2.3642e-05, -1.0436e-05,  ...,  6.8919e-05,
          3.7902e-05,  2.3184e-05],
        [-1.6093e-05, -1.0952e-05,  9.6411e-06,  ..., -1.4216e-05,
         -8.0243e-06, -1.2726e-05],
        [-2.8282e-05, -2.0608e-05,  1.6451e-05,  ..., -2.4468e-05,
         -1.5020e-05, -1.8567e-05]], device='cuda:0')
Loss: 0.9977407455444336


Running epoch 0, step 806, batch 806
Sampled inputs[:2]: tensor([[    0,  1268,   278,  ...,   461,   925,   630],
        [    0, 39200,  1828,  ...,   300,  3067,  4443]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7431e-04,  6.0361e-04,  3.0897e-04,  ...,  3.4266e-04,
          3.3558e-05,  5.3934e-04],
        [-1.2346e-05, -8.6203e-06,  7.1600e-06,  ..., -1.0990e-05,
         -6.4038e-06, -9.1121e-06],
        [ 3.1442e-05,  2.1377e-05, -8.2008e-06,  ...,  6.5894e-05,
          3.6293e-05,  2.0814e-05],
        [-1.8612e-05, -1.2584e-05,  1.1459e-05,  ..., -1.6689e-05,
         -9.2760e-06, -1.4916e-05],
        [-3.2395e-05, -2.3574e-05,  1.9237e-05,  ..., -2.8282e-05,
         -1.7196e-05, -2.1279e-05]], device='cuda:0')
Loss: 1.0092368125915527


Running epoch 0, step 807, batch 807
Sampled inputs[:2]: tensor([[    0, 49570,   644,  ...,   461,   800,   266],
        [    0,  4994,  8429,  ...,    12,   795,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5373e-04,  6.2928e-04,  2.2699e-04,  ...,  3.8644e-04,
          1.4759e-04,  5.6754e-04],
        [-1.4037e-05, -9.7975e-06,  8.2478e-06,  ..., -1.2562e-05,
         -7.1712e-06, -1.0297e-05],
        [ 2.8060e-05,  1.8963e-05, -5.9657e-06,  ...,  6.2780e-05,
          3.4795e-05,  1.8505e-05],
        [-2.1249e-05, -1.4357e-05,  1.3232e-05,  ..., -1.9148e-05,
         -1.0394e-05, -1.6913e-05],
        [-3.6657e-05, -2.6673e-05,  2.2009e-05,  ..., -3.2216e-05,
         -1.9222e-05, -2.3991e-05]], device='cuda:0')
Loss: 1.0223349332809448
Graident accumulation at epoch 0, step 807, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0026, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2735e-05,  1.5953e-04, -1.6097e-04,  ...,  9.4098e-05,
         -5.3576e-05,  1.9384e-04],
        [-1.2508e-05, -7.3019e-06,  5.8307e-06,  ..., -1.2428e-05,
         -4.7577e-06, -7.1256e-06],
        [ 3.3960e-05,  2.6113e-05, -1.2396e-05,  ...,  2.8742e-05,
          2.3198e-05,  4.8376e-06],
        [-1.4059e-05, -9.7298e-06,  1.0298e-05,  ..., -1.4188e-05,
         -6.8828e-06, -1.1850e-05],
        [-4.3678e-05, -3.1155e-05,  2.3947e-05,  ..., -3.7286e-05,
         -2.1704e-05, -2.3738e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1312e-08, 5.0962e-08, 5.7510e-08,  ..., 1.8834e-08, 1.3125e-07,
         2.9534e-08],
        [7.0693e-11, 4.1418e-11, 9.9492e-12,  ..., 4.7526e-11, 9.3072e-12,
         1.7654e-11],
        [2.5353e-09, 1.2988e-09, 4.9673e-10,  ..., 1.7560e-09, 4.1882e-10,
         7.2105e-10],
        [6.1720e-10, 3.0724e-10, 9.5966e-11,  ..., 4.0440e-10, 1.1459e-10,
         1.7020e-10],
        [3.1670e-10, 1.6988e-10, 3.7512e-11,  ..., 2.3090e-10, 2.9996e-11,
         7.7535e-11]], device='cuda:0')
optimizer state dict: 101.0
lr: [1.4072409990222116e-05, 1.4072409990222116e-05]
scheduler_last_epoch: 101


Running epoch 0, step 808, batch 808
Sampled inputs[:2]: tensor([[    0,   271, 21394,  ...,  1487,   287,   292],
        [    0,   221,   374,  ...,  2296,   365,  4579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8207e-05, -5.6434e-05,  1.2119e-04,  ...,  2.2114e-04,
          5.1225e-04,  1.7348e-04],
        [-1.8701e-06, -1.4305e-06,  9.9838e-07,  ..., -1.5721e-06,
         -1.1101e-06, -1.4976e-06],
        [-3.8743e-06, -3.0249e-06,  2.1756e-06,  ..., -3.1888e-06,
         -2.2203e-06, -2.9802e-06],
        [-2.7567e-06, -2.0564e-06,  1.5274e-06,  ..., -2.2948e-06,
         -1.5497e-06, -2.3693e-06],
        [-4.7684e-06, -3.7849e-06,  2.6673e-06,  ..., -3.9637e-06,
         -2.8461e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.0390279293060303


Running epoch 0, step 809, batch 809
Sampled inputs[:2]: tensor([[    0,  1235,   368,  ..., 12152,  8498,   287],
        [    0,  6532,  6984,  ...,   271,  8212, 14409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0372e-04,  4.4500e-05,  2.6696e-04,  ...,  3.2941e-04,
          7.0973e-04,  2.1219e-04],
        [-3.5465e-06, -2.5928e-06,  2.0340e-06,  ..., -3.0547e-06,
         -2.0266e-06, -2.9504e-06],
        [-7.3314e-06, -5.4836e-06,  4.3958e-06,  ..., -6.1840e-06,
         -4.0308e-06, -5.8413e-06],
        [-5.3793e-06, -3.7774e-06,  3.2336e-06,  ..., -4.5747e-06,
         -2.8536e-06, -4.7982e-06],
        [-8.9705e-06, -6.8694e-06,  5.3346e-06,  ..., -7.6294e-06,
         -5.1856e-06, -6.6161e-06]], device='cuda:0')
Loss: 1.0657516717910767


Running epoch 0, step 810, batch 810
Sampled inputs[:2]: tensor([[    0,  7935,  6521,  ..., 41312,   365,   806],
        [    0,    12,   638,  ...,   380,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0775e-05,  7.1995e-05,  3.1363e-04,  ...,  3.6345e-04,
          8.6525e-04,  3.3564e-04],
        [-5.2527e-06, -3.8370e-06,  3.0845e-06,  ..., -4.5747e-06,
         -3.0175e-06, -4.4554e-06],
        [-1.0774e-05, -8.0913e-06,  6.6012e-06,  ..., -9.1791e-06,
         -5.9977e-06, -8.6725e-06],
        [-7.9274e-06, -5.5581e-06,  4.9695e-06,  ..., -6.8843e-06,
         -4.2990e-06, -7.2718e-06],
        [-1.3292e-05, -1.0237e-05,  8.0764e-06,  ..., -1.1399e-05,
         -7.8082e-06, -9.8348e-06]], device='cuda:0')
Loss: 1.0155842304229736


Running epoch 0, step 811, batch 811
Sampled inputs[:2]: tensor([[    0, 11541,  4784,  ...,  2837, 38541,    12],
        [    0,   300,  2607,  ...,  1279,   368,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4034e-05,  1.8867e-04,  6.8934e-04,  ...,  6.0806e-04,
          1.3027e-03,  5.6263e-04],
        [-7.0184e-06, -5.1931e-06,  4.1053e-06,  ..., -6.1616e-06,
         -4.0531e-06, -5.9009e-06],
        [-1.4365e-05, -1.0863e-05,  8.7768e-06,  ..., -1.2293e-05,
         -8.0094e-06, -1.1444e-05],
        [-1.0580e-05, -7.5251e-06,  6.5714e-06,  ..., -9.2536e-06,
         -5.7742e-06, -9.5814e-06],
        [-1.7673e-05, -1.3694e-05,  1.0744e-05,  ..., -1.5214e-05,
         -1.0356e-05, -1.2964e-05]], device='cuda:0')
Loss: 1.0605241060256958


Running epoch 0, step 812, batch 812
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,  2785,  1061,  ...,  1194,   692,  4339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1249e-05,  1.1916e-04,  8.6443e-04,  ...,  7.2265e-04,
          1.8243e-03,  6.1196e-04],
        [-8.6874e-06, -6.3404e-06,  5.1111e-06,  ..., -7.6070e-06,
         -4.9360e-06, -7.4133e-06],
        [-1.7896e-05, -1.3322e-05,  1.0982e-05,  ..., -1.5259e-05,
         -9.7752e-06, -1.4514e-05],
        [-1.3277e-05, -9.2685e-06,  8.2776e-06,  ..., -1.1563e-05,
         -7.0930e-06, -1.2219e-05],
        [-2.1905e-05, -1.6749e-05,  1.3366e-05,  ..., -1.8790e-05,
         -1.2591e-05, -1.6332e-05]], device='cuda:0')
Loss: 1.0374152660369873


Running epoch 0, step 813, batch 813
Sampled inputs[:2]: tensor([[    0,  2612,   271,  ...,   369,  9862,   287],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9142e-05,  1.4001e-04,  1.2279e-03,  ...,  7.3653e-04,
          2.0879e-03,  4.9458e-04],
        [-1.0379e-05, -7.5176e-06,  6.1393e-06,  ..., -9.1344e-06,
         -5.8748e-06, -8.8736e-06],
        [-2.1309e-05, -1.5780e-05,  1.3128e-05,  ..., -1.8239e-05,
         -1.1571e-05, -1.7285e-05],
        [-1.5885e-05, -1.0997e-05,  9.9465e-06,  ..., -1.3918e-05,
         -8.4639e-06, -1.4678e-05],
        [-2.6077e-05, -1.9863e-05,  1.5944e-05,  ..., -2.2441e-05,
         -1.4931e-05, -1.9431e-05]], device='cuda:0')
Loss: 1.0070933103561401


Running epoch 0, step 814, batch 814
Sampled inputs[:2]: tensor([[    0,  5635,   328,  ...,   287, 27260,   271],
        [    0,  4120,   278,  ...,   298,   273,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1979e-06,  1.6888e-04,  1.3344e-03,  ...,  6.9638e-04,
          2.3871e-03,  6.1070e-04],
        [-1.2137e-05, -8.8364e-06,  7.1749e-06,  ..., -1.0625e-05,
         -6.8806e-06, -1.0327e-05],
        [-2.4840e-05, -1.8463e-05,  1.5289e-05,  ..., -2.1160e-05,
         -1.3523e-05, -2.0117e-05],
        [-1.8626e-05, -1.2979e-05,  1.1638e-05,  ..., -1.6212e-05,
         -9.9316e-06, -1.7121e-05],
        [-3.0398e-05, -2.3231e-05,  1.8582e-05,  ..., -2.6062e-05,
         -1.7479e-05, -2.2635e-05]], device='cuda:0')
Loss: 1.0465463399887085


Running epoch 0, step 815, batch 815
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0,  767, 1811,  ..., 1441, 1428,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1276e-05,  2.6424e-04,  1.4268e-03,  ...,  7.1828e-04,
          2.4404e-03,  6.6036e-04],
        [-1.3836e-05, -1.0096e-05,  8.2478e-06,  ..., -1.2144e-05,
         -7.7933e-06, -1.1779e-05],
        [-2.8208e-05, -2.1026e-05,  1.7524e-05,  ..., -2.4110e-05,
         -1.5289e-05, -2.2843e-05],
        [-2.1219e-05, -1.4827e-05,  1.3374e-05,  ..., -1.8522e-05,
         -1.1250e-05, -1.9506e-05],
        [-3.4511e-05, -2.6464e-05,  2.1294e-05,  ..., -2.9698e-05,
         -1.9804e-05, -2.5675e-05]], device='cuda:0')
Loss: 1.0157963037490845
Graident accumulation at epoch 0, step 815, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0057, -0.0144,  0.0028,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8589e-05,  1.7000e-04, -2.1863e-06,  ...,  1.5652e-04,
          1.9583e-04,  2.4049e-04],
        [-1.2640e-05, -7.5813e-06,  6.0724e-06,  ..., -1.2400e-05,
         -5.0613e-06, -7.5910e-06],
        [ 2.7743e-05,  2.1399e-05, -9.4038e-06,  ...,  2.3457e-05,
          1.9349e-05,  2.0695e-06],
        [-1.4775e-05, -1.0239e-05,  1.0605e-05,  ..., -1.4621e-05,
         -7.3195e-06, -1.2615e-05],
        [-4.2761e-05, -3.0686e-05,  2.3682e-05,  ..., -3.6527e-05,
         -2.1514e-05, -2.3932e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1261e-08, 5.0981e-08, 5.9489e-08,  ..., 1.9331e-08, 1.3707e-07,
         2.9941e-08],
        [7.0814e-11, 4.1479e-11, 1.0007e-11,  ..., 4.7626e-11, 9.3586e-12,
         1.7775e-11],
        [2.5335e-09, 1.2979e-09, 4.9654e-10,  ..., 1.7548e-09, 4.1864e-10,
         7.2085e-10],
        [6.1703e-10, 3.0715e-10, 9.6049e-11,  ..., 4.0434e-10, 1.1460e-10,
         1.7041e-10],
        [3.1758e-10, 1.7041e-10, 3.7928e-11,  ..., 2.3155e-10, 3.0358e-11,
         7.8117e-11]], device='cuda:0')
optimizer state dict: 102.0
lr: [1.3959200097809337e-05, 1.3959200097809337e-05]
scheduler_last_epoch: 102


Running epoch 0, step 816, batch 816
Sampled inputs[:2]: tensor([[    0,  3227,   300,  ...,  1817,  5709,   300],
        [    0, 16763,  1538,  ...,   631,  3299,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0912e-05, -5.6454e-05, -3.0247e-04,  ...,  3.8930e-05,
         -2.2735e-05, -2.3500e-05],
        [-1.6168e-06, -1.1623e-06,  1.0654e-06,  ..., -1.4380e-06,
         -9.4622e-07, -1.4678e-06],
        [-3.3528e-06, -2.4736e-06,  2.2799e-06,  ..., -2.9206e-06,
         -1.9073e-06, -2.9057e-06],
        [-2.5481e-06, -1.7360e-06,  1.7956e-06,  ..., -2.2501e-06,
         -1.4082e-06, -2.5332e-06],
        [-3.8743e-06, -2.9802e-06,  2.5779e-06,  ..., -3.4124e-06,
         -2.3842e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.0367411375045776


Running epoch 0, step 817, batch 817
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,    14, 45192,  ..., 24171,   292,  3620]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7082e-05,  7.9830e-05, -2.2823e-04,  ...,  6.4962e-05,
          2.8429e-04,  2.1139e-04],
        [-3.5688e-06, -2.6450e-06,  2.0787e-06,  ..., -3.0845e-06,
         -2.1160e-06, -3.1292e-06],
        [-7.2569e-06, -5.5581e-06,  4.4405e-06,  ..., -6.1989e-06,
         -4.2319e-06, -6.1244e-06],
        [-5.3495e-06, -3.8072e-06,  3.3379e-06,  ..., -4.6045e-06,
         -3.0324e-06, -5.0962e-06],
        [-8.5533e-06, -6.8247e-06,  5.1707e-06,  ..., -7.4059e-06,
         -5.3644e-06, -6.6310e-06]], device='cuda:0')
Loss: 1.0377191305160522


Running epoch 0, step 818, batch 818
Sampled inputs[:2]: tensor([[    0,   368,   275,  ...,  6389,  9102,    12],
        [    0,  1278,    69,  ...,    15,  7377, 20524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1380e-05,  4.6666e-05, -1.9354e-04,  ...,  9.7810e-05,
          3.3258e-04,  3.2671e-04],
        [-5.3346e-06, -3.9861e-06,  3.0696e-06,  ..., -4.6194e-06,
         -3.1665e-06, -4.5821e-06],
        [-1.0818e-05, -8.3297e-06,  6.5416e-06,  ..., -9.2387e-06,
         -6.2734e-06, -8.9407e-06],
        [-7.9870e-06, -5.7444e-06,  4.8801e-06,  ..., -6.8843e-06,
         -4.5300e-06, -7.4506e-06],
        [-1.2755e-05, -1.0177e-05,  7.6294e-06,  ..., -1.1012e-05,
         -7.9125e-06, -9.6858e-06]], device='cuda:0')
Loss: 1.028253197669983


Running epoch 0, step 819, batch 819
Sampled inputs[:2]: tensor([[    0,   792,    83,  ..., 29085, 15914,   365],
        [    0,  5301,   792,  ..., 27135, 34090,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2709e-04, -9.5420e-05, -1.1932e-04,  ...,  9.6221e-05,
          4.4231e-04,  2.8602e-04],
        [-7.1749e-06, -5.3644e-06,  4.0755e-06,  ..., -6.1989e-06,
         -4.2319e-06, -6.1020e-06],
        [-1.4558e-05, -1.1221e-05,  8.6874e-06,  ..., -1.2428e-05,
         -8.4192e-06, -1.1951e-05],
        [-1.0788e-05, -7.7859e-06,  6.4895e-06,  ..., -9.2834e-06,
         -6.0797e-06, -9.9242e-06],
        [-1.7196e-05, -1.3709e-05,  1.0148e-05,  ..., -1.4827e-05,
         -1.0639e-05, -1.2994e-05]], device='cuda:0')
Loss: 1.0545545816421509


Running epoch 0, step 820, batch 820
Sampled inputs[:2]: tensor([[   0,   12, 2418,  ...,  446,  381, 2204],
        [   0,  300, 5864,  ...,   12, 3667,  796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3091e-04, -2.0012e-04, -1.1161e-06,  ...,  1.7998e-04,
          6.8547e-04,  1.9037e-04],
        [-9.0450e-06, -6.7428e-06,  5.1036e-06,  ..., -7.7486e-06,
         -5.2825e-06, -7.6592e-06],
        [-1.8492e-05, -1.4186e-05,  1.0937e-05,  ..., -1.5646e-05,
         -1.0595e-05, -1.5140e-05],
        [-1.3590e-05, -9.7677e-06,  8.0988e-06,  ..., -1.1563e-05,
         -7.5549e-06, -1.2413e-05],
        [-2.1726e-05, -1.7256e-05,  1.2740e-05,  ..., -1.8597e-05,
         -1.3322e-05, -1.6391e-05]], device='cuda:0')
Loss: 1.0459518432617188


Running epoch 0, step 821, batch 821
Sampled inputs[:2]: tensor([[    0,  4294,   278,  ...,    13,  2759,  5160],
        [    0,    12,   266,  ...,   278,   266, 10995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7926e-04, -3.0139e-04, -1.1679e-04,  ...,  1.8440e-04,
          8.5965e-04,  2.0183e-04],
        [-1.0721e-05, -7.9796e-06,  6.1616e-06,  ..., -9.1866e-06,
         -6.1840e-06, -9.0078e-06],
        [-2.1964e-05, -1.6823e-05,  1.3202e-05,  ..., -1.8612e-05,
         -1.2442e-05, -1.7852e-05],
        [-1.6212e-05, -1.1638e-05,  9.8422e-06,  ..., -1.3813e-05,
         -8.8811e-06, -1.4707e-05],
        [-2.5630e-05, -2.0325e-05,  1.5259e-05,  ..., -2.1979e-05,
         -1.5587e-05, -1.9178e-05]], device='cuda:0')
Loss: 1.0164586305618286


Running epoch 0, step 822, batch 822
Sampled inputs[:2]: tensor([[    0,  2042,  2909,  ...,    14, 15061,  5742],
        [    0, 17442,  2416,  ...,  7244,    66, 16907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1539e-04, -4.5090e-04,  1.2760e-04,  ...,  1.8923e-04,
          9.0294e-04,  1.9362e-04],
        [-1.2644e-05, -9.4771e-06,  7.2867e-06,  ..., -1.0692e-05,
         -7.2941e-06, -1.0476e-05],
        [-2.5928e-05, -1.9982e-05,  1.5587e-05,  ..., -2.1741e-05,
         -1.4737e-05, -2.0877e-05],
        [-1.9103e-05, -1.3843e-05,  1.1601e-05,  ..., -1.6063e-05,
         -1.0476e-05, -1.7062e-05],
        [-3.0220e-05, -2.4080e-05,  1.8001e-05,  ..., -2.5690e-05,
         -1.8403e-05, -2.2456e-05]], device='cuda:0')
Loss: 1.0509616136550903


Running epoch 0, step 823, batch 823
Sampled inputs[:2]: tensor([[   0, 4323, 8213,  ..., 1153,  278, 4258],
        [   0,   12, 1471,  ..., 1356,  600,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0578e-04, -4.5169e-04, -1.3098e-04,  ...,  2.1074e-04,
          9.2545e-04,  3.0268e-04],
        [-1.4260e-05, -1.0587e-05,  8.2180e-06,  ..., -1.2100e-05,
         -8.1994e-06, -1.1943e-05],
        [-2.9162e-05, -2.2292e-05,  1.7583e-05,  ..., -2.4483e-05,
         -1.6473e-05, -2.3618e-05],
        [-2.1562e-05, -1.5408e-05,  1.3128e-05,  ..., -1.8194e-05,
         -1.1779e-05, -1.9506e-05],
        [-3.3960e-05, -2.6867e-05,  2.0280e-05,  ..., -2.8893e-05,
         -2.0564e-05, -2.5317e-05]], device='cuda:0')
Loss: 0.9717889428138733
Graident accumulation at epoch 0, step 823, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0144,  0.0028,  ..., -0.0023,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0153, -0.0281,  ...,  0.0288, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3152e-05,  1.0783e-04, -1.5065e-05,  ...,  1.6194e-04,
          2.6879e-04,  2.4671e-04],
        [-1.2802e-05, -7.8819e-06,  6.2870e-06,  ..., -1.2370e-05,
         -5.3751e-06, -8.0262e-06],
        [ 2.2053e-05,  1.7030e-05, -6.7051e-06,  ...,  1.8663e-05,
          1.5767e-05, -4.9932e-07],
        [-1.5454e-05, -1.0756e-05,  1.0858e-05,  ..., -1.4978e-05,
         -7.7655e-06, -1.3304e-05],
        [-4.1881e-05, -3.0304e-05,  2.3342e-05,  ..., -3.5764e-05,
         -2.1419e-05, -2.4070e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1252e-08, 5.1134e-08, 5.9446e-08,  ..., 1.9356e-08, 1.3779e-07,
         3.0003e-08],
        [7.0946e-11, 4.1549e-11, 1.0065e-11,  ..., 4.7725e-11, 9.4165e-12,
         1.7900e-11],
        [2.5318e-09, 1.2971e-09, 4.9636e-10,  ..., 1.7537e-09, 4.1849e-10,
         7.2069e-10],
        [6.1688e-10, 3.0708e-10, 9.6125e-11,  ..., 4.0427e-10, 1.1463e-10,
         1.7062e-10],
        [3.1841e-10, 1.7096e-10, 3.8301e-11,  ..., 2.3216e-10, 3.0751e-11,
         7.8680e-11]], device='cuda:0')
optimizer state dict: 103.0
lr: [1.3845385205061268e-05, 1.3845385205061268e-05]
scheduler_last_epoch: 103


Running epoch 0, step 824, batch 824
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8774e-04,  2.6325e-05, -1.7716e-04,  ..., -8.9660e-06,
         -1.4650e-04, -1.0726e-04],
        [-1.7732e-06, -1.2740e-06,  9.5367e-07,  ..., -1.6019e-06,
         -1.2740e-06, -1.7136e-06],
        [-3.5912e-06, -2.6822e-06,  2.0564e-06,  ..., -3.1888e-06,
         -2.4885e-06, -3.2783e-06],
        [-2.6524e-06, -1.8254e-06,  1.5050e-06,  ..., -2.4140e-06,
         -1.8850e-06, -2.7567e-06],
        [-4.2021e-06, -3.2336e-06,  2.3991e-06,  ..., -3.7104e-06,
         -2.9951e-06, -3.5465e-06]], device='cuda:0')
Loss: 1.0212061405181885


Running epoch 0, step 825, batch 825
Sampled inputs[:2]: tensor([[    0,   721,  1119,  ...,   600,   328,  3363],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6589e-04, -4.0354e-05, -3.8940e-04,  ..., -1.1610e-06,
         -8.4303e-05, -6.1206e-05],
        [-3.6135e-06, -2.8387e-06,  2.0191e-06,  ..., -3.1888e-06,
         -2.4140e-06, -3.0398e-06],
        [ 7.7081e-05,  4.0793e-05, -2.7618e-05,  ...,  5.3818e-05,
          9.7714e-05,  1.0008e-05],
        [-5.3942e-06, -4.0904e-06,  3.1292e-06,  ..., -4.7535e-06,
         -3.4869e-06, -4.8727e-06],
        [-8.6427e-06, -7.1079e-06,  4.9919e-06,  ..., -7.5847e-06,
         -5.8264e-06, -6.5416e-06]], device='cuda:0')
Loss: 1.0663944482803345


Running epoch 0, step 826, batch 826
Sampled inputs[:2]: tensor([[   0,  437,  266,  ..., 5512,  822,   89],
        [   0,  996, 2226,  ...,  516, 3470,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6696e-04, -1.4495e-04, -5.0941e-04,  ...,  1.6707e-05,
          3.5388e-05,  2.0298e-06],
        [-5.5507e-06, -4.3511e-06,  3.0175e-06,  ..., -4.8727e-06,
         -3.7104e-06, -4.5896e-06],
        [ 7.3118e-05,  3.7589e-05, -2.5457e-05,  ...,  5.0361e-05,
          9.5047e-05,  6.8787e-06],
        [-8.1956e-06, -6.2510e-06,  4.6417e-06,  ..., -7.1675e-06,
         -5.3123e-06, -7.2569e-06],
        [-1.3202e-05, -1.0878e-05,  7.4655e-06,  ..., -1.1608e-05,
         -9.0450e-06, -9.9242e-06]], device='cuda:0')
Loss: 1.0266740322113037


Running epoch 0, step 827, batch 827
Sampled inputs[:2]: tensor([[   0,  271, 4728,  ...,  344,  259, 1774],
        [   0,   12,  287,  ...,  266, 2105, 3925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5221e-04, -1.8312e-04, -5.6088e-04,  ...,  7.0370e-05,
         -1.7996e-05, -3.6145e-05],
        [-7.2941e-06, -5.7518e-06,  4.0084e-06,  ..., -6.4075e-06,
         -4.8131e-06, -6.0797e-06],
        [ 6.9333e-05,  3.4504e-05, -2.3237e-05,  ...,  4.7098e-05,
          9.2722e-05,  3.7643e-06],
        [-1.0848e-05, -8.2925e-06,  6.2585e-06,  ..., -9.4771e-06,
         -6.9067e-06, -9.7156e-06],
        [-1.7405e-05, -1.4409e-05,  9.9093e-06,  ..., -1.5289e-05,
         -1.1772e-05, -1.3128e-05]], device='cuda:0')
Loss: 1.0113404989242554


Running epoch 0, step 828, batch 828
Sampled inputs[:2]: tensor([[    0,   273,   298,  ...,  7437,  2767,   518],
        [    0, 10676,   328,  ...,     9,   360,  2583]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6222e-04, -2.3463e-04, -6.1590e-04,  ...,  1.5367e-04,
          4.0201e-05,  7.2588e-06],
        [-9.1940e-06, -7.3239e-06,  5.0142e-06,  ..., -8.0317e-06,
         -5.9828e-06, -7.5474e-06],
        [ 6.5160e-05,  3.1003e-05, -2.0972e-05,  ...,  4.3566e-05,
          9.0204e-05,  6.0528e-07],
        [-1.3635e-05, -1.0528e-05,  7.7784e-06,  ..., -1.1832e-05,
         -8.5309e-06, -1.2010e-05],
        [-2.2084e-05, -1.8433e-05,  1.2457e-05,  ..., -1.9312e-05,
         -1.4767e-05, -1.6496e-05]], device='cuda:0')
Loss: 1.0341222286224365


Running epoch 0, step 829, batch 829
Sampled inputs[:2]: tensor([[   0, 4304, 7406,  ...,  957, 7366,  328],
        [   0,    9,  278,  ...,  278,  298,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4350e-04, -1.4994e-04, -5.8902e-04,  ...,  2.5014e-04,
          1.0311e-04,  7.4818e-05],
        [-1.1101e-05, -8.7619e-06,  5.9754e-06,  ..., -9.6709e-06,
         -7.1451e-06, -9.0823e-06],
        [ 6.1167e-05,  2.7933e-05, -1.8856e-05,  ...,  4.0183e-05,
          8.7850e-05, -2.5091e-06],
        [-1.6510e-05, -1.2629e-05,  9.2685e-06,  ..., -1.4275e-05,
         -1.0222e-05, -1.4469e-05],
        [-2.6643e-05, -2.2054e-05,  1.4886e-05,  ..., -2.3216e-05,
         -1.7613e-05, -1.9833e-05]], device='cuda:0')
Loss: 1.039273738861084


Running epoch 0, step 830, batch 830
Sampled inputs[:2]: tensor([[    0,   409,  4146,  ...,     9,   360,   259],
        [    0,  3473,   278,  ..., 11743,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1611e-04, -2.0663e-04, -6.7858e-04,  ...,  2.8628e-04,
          1.0785e-04,  3.1716e-05],
        [-1.2927e-05, -1.0170e-05,  6.9812e-06,  ..., -1.1243e-05,
         -8.2850e-06, -1.0513e-05],
        [ 5.7382e-05,  2.4953e-05, -1.6665e-05,  ...,  3.6935e-05,
          8.5525e-05, -5.4148e-06],
        [-1.9297e-05, -1.4700e-05,  1.0885e-05,  ..., -1.6659e-05,
         -1.1884e-05, -1.6809e-05],
        [-3.0965e-05, -2.5541e-05,  1.7375e-05,  ..., -2.6956e-05,
         -2.0415e-05, -2.2933e-05]], device='cuda:0')
Loss: 1.0329805612564087


Running epoch 0, step 831, batch 831
Sampled inputs[:2]: tensor([[    0,   374,  5195,  ...,   266,  5555,    14],
        [    0,   677, 35427,  ..., 30465,  2783,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1869e-04, -2.0829e-04, -6.9291e-04,  ...,  2.9289e-04,
          1.0149e-04, -9.3826e-06],
        [-1.4663e-05, -1.1526e-05,  7.9423e-06,  ..., -1.2808e-05,
         -9.4771e-06, -1.1951e-05],
        [ 5.3791e-05,  2.2107e-05, -1.4609e-05,  ...,  3.3791e-05,
          8.3186e-05, -8.2460e-06],
        [-2.1949e-05, -1.6682e-05,  1.2428e-05,  ..., -1.9044e-05,
         -1.3635e-05, -1.9193e-05],
        [-3.5197e-05, -2.8968e-05,  1.9789e-05,  ..., -3.0667e-05,
         -2.3276e-05, -2.6017e-05]], device='cuda:0')
Loss: 1.0271317958831787
Graident accumulation at epoch 0, step 831, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0144,  0.0028,  ..., -0.0023,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0153, -0.0281,  ...,  0.0288, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.9684e-06,  7.6221e-05, -8.2850e-05,  ...,  1.7503e-04,
          2.5206e-04,  2.2110e-04],
        [-1.2988e-05, -8.2463e-06,  6.4525e-06,  ..., -1.2413e-05,
         -5.7853e-06, -8.4187e-06],
        [ 2.5226e-05,  1.7537e-05, -7.4955e-06,  ...,  2.0176e-05,
          2.2509e-05, -1.2740e-06],
        [-1.6103e-05, -1.1349e-05,  1.1015e-05,  ..., -1.5385e-05,
         -8.3524e-06, -1.3893e-05],
        [-4.1213e-05, -3.0170e-05,  2.2986e-05,  ..., -3.5254e-05,
         -2.1605e-05, -2.4265e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1215e-08, 5.1126e-08, 5.9867e-08,  ..., 1.9423e-08, 1.3767e-07,
         2.9973e-08],
        [7.1090e-11, 4.1640e-11, 1.0118e-11,  ..., 4.7841e-11, 9.4969e-12,
         1.8025e-11],
        [2.5322e-09, 1.2963e-09, 4.9607e-10,  ..., 1.7531e-09, 4.2499e-10,
         7.2004e-10],
        [6.1675e-10, 3.0705e-10, 9.6184e-11,  ..., 4.0423e-10, 1.1470e-10,
         1.7082e-10],
        [3.1933e-10, 1.7163e-10, 3.8655e-11,  ..., 2.3287e-10, 3.1262e-11,
         7.9278e-11]], device='cuda:0')
optimizer state dict: 104.0
lr: [1.3730982703887026e-05, 1.3730982703887026e-05]
scheduler_last_epoch: 104


Running epoch 0, step 832, batch 832
Sampled inputs[:2]: tensor([[   0,  199,  769,  ...,  685, 1423,   13],
        [   0,   14,  475,  ..., 4103,  278, 4190]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3197e-05, -4.0354e-05, -2.5517e-04,  ..., -1.1189e-05,
         -1.5869e-04, -2.4744e-04],
        [-1.8477e-06, -1.4901e-06,  9.0897e-07,  ..., -1.6019e-06,
         -1.1474e-06, -1.3262e-06],
        [-4.0531e-06, -3.2932e-06,  2.0862e-06,  ..., -3.4422e-06,
         -2.4289e-06, -2.7865e-06],
        [-2.7567e-06, -2.1607e-06,  1.4380e-06,  ..., -2.4140e-06,
         -1.6764e-06, -2.1458e-06],
        [-4.5896e-06, -3.7700e-06,  2.3395e-06,  ..., -3.9041e-06,
         -2.8312e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.002111554145813


Running epoch 0, step 833, batch 833
Sampled inputs[:2]: tensor([[    0,   266, 30368,  ...,   950,   266,  1868],
        [    0,  3037,  4511,  ...,  1711,    12,  2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6612e-05, -1.8616e-04, -4.8930e-04,  ..., -6.2269e-05,
         -3.9093e-04, -3.8398e-04],
        [-3.6582e-06, -2.8908e-06,  1.7434e-06,  ..., -3.2336e-06,
         -2.3097e-06, -2.7344e-06],
        [-7.9870e-06, -6.3777e-06,  3.9935e-06,  ..., -6.9290e-06,
         -4.8727e-06, -5.7518e-06],
        [-5.4687e-06, -4.1723e-06,  2.7195e-06,  ..., -4.8280e-06,
         -3.3379e-06, -4.3660e-06],
        [-9.0003e-06, -7.2867e-06,  4.4852e-06,  ..., -7.8380e-06,
         -5.6624e-06, -6.0499e-06]], device='cuda:0')
Loss: 1.0093967914581299


Running epoch 0, step 834, batch 834
Sampled inputs[:2]: tensor([[    0,    12, 30621,  ...,   578,  3126,    14],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8857e-05, -2.3553e-04, -5.6875e-04,  ..., -1.5543e-04,
         -4.7345e-04, -4.9123e-04],
        [-5.4911e-06, -4.4182e-06,  2.6152e-06,  ..., -4.8056e-06,
         -3.5092e-06, -4.0159e-06],
        [ 5.8984e-05,  6.5726e-05, -7.6055e-06,  ...,  6.1955e-05,
          5.0894e-05,  3.1972e-05],
        [-8.2254e-06, -6.4224e-06,  4.0606e-06,  ..., -7.1824e-06,
         -5.1036e-06, -6.4075e-06],
        [-1.3590e-05, -1.1191e-05,  6.7949e-06,  ..., -1.1742e-05,
         -8.6576e-06, -8.9854e-06]], device='cuda:0')
Loss: 1.042694330215454


Running epoch 0, step 835, batch 835
Sampled inputs[:2]: tensor([[    0,   287,  1790,  ..., 11367,  9476,  2545],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0627e-05, -2.5629e-04, -5.4743e-04,  ..., -2.2141e-04,
         -5.9336e-04, -4.4646e-04],
        [-7.4431e-06, -6.0946e-06,  3.5837e-06,  ..., -6.5416e-06,
         -4.8652e-06, -5.3868e-06],
        [ 5.4782e-05,  6.2075e-05, -5.4299e-06,  ...,  5.8245e-05,
          4.8048e-05,  2.9081e-05],
        [-1.1027e-05, -8.7768e-06,  5.4762e-06,  ..., -9.6709e-06,
         -7.0259e-06, -8.4937e-06],
        [-1.8328e-05, -1.5393e-05,  9.2685e-06,  ..., -1.5944e-05,
         -1.1951e-05, -1.2070e-05]], device='cuda:0')
Loss: 1.0459420680999756


Running epoch 0, step 836, batch 836
Sampled inputs[:2]: tensor([[    0,   391,  4356,  ...,   287, 32873,  5362],
        [    0,  1842,   360,  ..., 10251,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3116e-04, -3.3112e-04, -8.8755e-04,  ..., -2.4003e-04,
         -7.4182e-04, -5.2814e-04],
        [-9.2611e-06, -7.5847e-06,  4.5225e-06,  ..., -8.1509e-06,
         -6.0201e-06, -6.7130e-06],
        [ 5.0759e-05,  5.8752e-05, -3.2543e-06,  ...,  5.4743e-05,
          4.5574e-05,  2.6265e-05],
        [-1.3724e-05, -1.0908e-05,  6.9588e-06,  ..., -1.2055e-05,
         -8.7097e-06, -1.0639e-05],
        [-2.2799e-05, -1.9148e-05,  1.1683e-05,  ..., -1.9848e-05,
         -1.4782e-05, -1.4991e-05]], device='cuda:0')
Loss: 1.008577585220337


Running epoch 0, step 837, batch 837
Sampled inputs[:2]: tensor([[    0,   287, 19777,  ...,   266,  5061,   278],
        [    0,  5129,  1245,  ...,   292, 24298,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3779e-04, -1.8354e-04, -7.8679e-04,  ..., -2.9558e-04,
         -8.6870e-04, -5.2635e-04],
        [-1.1228e-05, -9.0301e-06,  5.3383e-06,  ..., -9.7677e-06,
         -7.3165e-06, -8.3223e-06],
        [ 4.6258e-05,  5.5370e-05, -1.2725e-06,  ...,  5.1077e-05,
          4.2684e-05,  2.2674e-05],
        [-1.6600e-05, -1.2964e-05,  8.1956e-06,  ..., -1.4409e-05,
         -1.0543e-05, -1.3098e-05],
        [-2.7806e-05, -2.2992e-05,  1.3903e-05,  ..., -2.3931e-05,
         -1.8090e-05, -1.8775e-05]], device='cuda:0')
Loss: 1.0281059741973877


Running epoch 0, step 838, batch 838
Sampled inputs[:2]: tensor([[   0, 2992,  352,  ...,  259, 2063, 6088],
        [   0, 2270, 3279,  ...,  380,  475,  768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9601e-04, -1.8684e-04, -9.9514e-04,  ..., -4.2874e-04,
         -1.0506e-03, -6.2574e-04],
        [-1.2994e-05, -1.0461e-05,  6.1542e-06,  ..., -1.1414e-05,
         -8.5607e-06, -9.6932e-06],
        [ 4.2414e-05,  5.2240e-05,  6.3489e-07,  ...,  4.7591e-05,
          4.0136e-05,  1.9828e-05],
        [-1.9237e-05, -1.5005e-05,  9.4622e-06,  ..., -1.6868e-05,
         -1.2353e-05, -1.5303e-05],
        [-3.2127e-05, -2.6584e-05,  1.6078e-05,  ..., -2.7865e-05,
         -2.1026e-05, -2.1741e-05]], device='cuda:0')
Loss: 1.0135749578475952


Running epoch 0, step 839, batch 839
Sampled inputs[:2]: tensor([[    0,   376,   283,  ..., 29188,   292,  7627],
        [    0, 21413,  1735,  ..., 10789, 12523,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9337e-04, -1.8344e-04, -9.9456e-04,  ..., -4.7337e-04,
         -1.0232e-03, -6.1706e-04],
        [-1.4991e-05, -1.2077e-05,  7.0669e-06,  ..., -1.3098e-05,
         -9.8795e-06, -1.1161e-05],
        [ 3.8003e-05,  4.8634e-05,  2.7509e-06,  ...,  4.3910e-05,
          3.7289e-05,  1.6654e-05],
        [-2.2128e-05, -1.7285e-05,  1.0826e-05,  ..., -1.9282e-05,
         -1.4201e-05, -1.7554e-05],
        [-3.7134e-05, -3.0756e-05,  1.8507e-05,  ..., -3.2097e-05,
         -2.4363e-05, -2.5153e-05]], device='cuda:0')
Loss: 1.054229497909546
Graident accumulation at epoch 0, step 839, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0144,  0.0028,  ..., -0.0023,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0153, -0.0282,  ...,  0.0289, -0.0148, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1265e-05,  5.0256e-05, -1.7402e-04,  ...,  1.1019e-04,
          1.2453e-04,  1.3729e-04],
        [-1.3189e-05, -8.6294e-06,  6.5139e-06,  ..., -1.2482e-05,
         -6.1947e-06, -8.6929e-06],
        [ 2.6504e-05,  2.0647e-05, -6.4708e-06,  ...,  2.2549e-05,
          2.3987e-05,  5.1878e-07],
        [-1.6706e-05, -1.1943e-05,  1.0996e-05,  ..., -1.5775e-05,
         -8.9373e-06, -1.4259e-05],
        [-4.0805e-05, -3.0229e-05,  2.2539e-05,  ..., -3.4938e-05,
         -2.1880e-05, -2.4354e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1318e-08, 5.1108e-08, 6.0796e-08,  ..., 1.9627e-08, 1.3858e-07,
         3.0324e-08],
        [7.1244e-11, 4.1745e-11, 1.0158e-11,  ..., 4.7965e-11, 9.5850e-12,
         1.8131e-11],
        [2.5311e-09, 1.2974e-09, 4.9558e-10,  ..., 1.7532e-09, 4.2596e-10,
         7.1960e-10],
        [6.1662e-10, 3.0704e-10, 9.6205e-11,  ..., 4.0420e-10, 1.1479e-10,
         1.7096e-10],
        [3.2039e-10, 1.7240e-10, 3.8958e-11,  ..., 2.3366e-10, 3.1824e-11,
         7.9832e-11]], device='cuda:0')
optimizer state dict: 105.0
lr: [1.3616010075987416e-05, 1.3616010075987416e-05]
scheduler_last_epoch: 105


Running epoch 0, step 840, batch 840
Sampled inputs[:2]: tensor([[    0, 17301,   300,  ...,   278,   546,  1576],
        [    0,   278, 30377,  ...,    13,    83,  2908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5982e-05, -2.6356e-05,  1.5949e-04,  ...,  2.4688e-04,
          9.1543e-05,  2.0613e-04],
        [-1.9968e-06, -1.5348e-06,  8.2329e-07,  ..., -1.7136e-06,
         -1.3113e-06, -1.4678e-06],
        [-4.5002e-06, -3.5018e-06,  1.9670e-06,  ..., -3.8147e-06,
         -2.8759e-06, -3.2187e-06],
        [-2.9057e-06, -2.1607e-06,  1.2144e-06,  ..., -2.4736e-06,
         -1.8626e-06, -2.2501e-06],
        [-5.1558e-06, -4.0829e-06,  2.2650e-06,  ..., -4.3809e-06,
         -3.3677e-06, -3.4869e-06]], device='cuda:0')
Loss: 1.0739001035690308


Running epoch 0, step 841, batch 841
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2342e-05,  1.3283e-04,  2.8578e-04,  ...,  3.1178e-04,
          7.1881e-05,  4.0363e-04],
        [-4.0382e-06, -3.0771e-06,  1.6205e-06,  ..., -3.4198e-06,
         -2.6450e-06, -3.0845e-06],
        [-9.0301e-06, -7.0035e-06,  3.8520e-06,  ..., -7.5549e-06,
         -5.7667e-06, -6.7055e-06],
        [-5.7966e-06, -4.3064e-06,  2.3693e-06,  ..., -4.8727e-06,
         -3.7029e-06, -4.6194e-06],
        [-1.0431e-05, -8.2254e-06,  4.4852e-06,  ..., -8.7619e-06,
         -6.8098e-06, -7.3612e-06]], device='cuda:0')
Loss: 1.0119493007659912


Running epoch 0, step 842, batch 842
Sampled inputs[:2]: tensor([[   0,  328,  490,  ..., 6280, 4283, 4582],
        [   0, 2790,  266,  ...,  401, 1496,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5047e-05,  4.7985e-05,  1.0644e-04,  ...,  3.4485e-04,
         -1.8015e-04,  3.2925e-04],
        [-5.9009e-06, -4.5747e-06,  2.4661e-06,  ..., -5.0142e-06,
         -3.7402e-06, -4.3586e-06],
        [-1.3202e-05, -1.0431e-05,  5.8785e-06,  ..., -1.1101e-05,
         -8.1807e-06, -9.4771e-06],
        [-8.5533e-06, -6.4671e-06,  3.6806e-06,  ..., -7.2420e-06,
         -5.2750e-06, -6.6459e-06],
        [-1.5169e-05, -1.2189e-05,  6.7949e-06,  ..., -1.2815e-05,
         -9.6858e-06, -1.0297e-05]], device='cuda:0')
Loss: 1.0025546550750732


Running epoch 0, step 843, batch 843
Sampled inputs[:2]: tensor([[    0,  1487,   409,  ...,  6979,  1273,   496],
        [    0,   389, 18984,  ...,   287,   768,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0745e-05,  4.0253e-05,  1.4705e-05,  ...,  4.8965e-04,
         -1.6242e-04,  2.8844e-04],
        [-7.8976e-06, -6.1691e-06,  3.3453e-06,  ..., -6.7279e-06,
         -5.1036e-06, -5.8189e-06],
        [-1.7703e-05, -1.4067e-05,  7.9796e-06,  ..., -1.4916e-05,
         -1.1161e-05, -1.2636e-05],
        [-1.1429e-05, -8.6874e-06,  4.9844e-06,  ..., -9.7007e-06,
         -7.1973e-06, -8.8811e-06],
        [-2.0355e-05, -1.6481e-05,  9.2387e-06,  ..., -1.7226e-05,
         -1.3217e-05, -1.3739e-05]], device='cuda:0')
Loss: 1.049665927886963


Running epoch 0, step 844, batch 844
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0,   287,  4579,  ...,   909,    12,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5364e-05, -4.2015e-05, -1.1142e-04,  ...,  5.4877e-04,
         -3.9137e-04,  3.0424e-04],
        [-9.7379e-06, -7.6815e-06,  4.2394e-06,  ..., -8.3596e-06,
         -6.2436e-06, -7.1004e-06],
        [-2.1935e-05, -1.7524e-05,  1.0140e-05,  ..., -1.8582e-05,
         -1.3664e-05, -1.5482e-05],
        [-1.4201e-05, -1.0893e-05,  6.3851e-06,  ..., -1.2130e-05,
         -8.8215e-06, -1.0908e-05],
        [-2.5034e-05, -2.0385e-05,  1.1638e-05,  ..., -2.1309e-05,
         -1.6108e-05, -1.6689e-05]], device='cuda:0')
Loss: 1.0418635606765747


Running epoch 0, step 845, batch 845
Sampled inputs[:2]: tensor([[    0,  1451, 14349,  ...,   741,  2945,  7257],
        [    0, 15372, 10123,  ...,  1782,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3882e-04, -9.5447e-05, -1.1695e-04,  ...,  5.3461e-04,
         -3.0456e-04,  3.9382e-04],
        [-1.1720e-05, -9.3654e-06,  5.0887e-06,  ..., -1.0021e-05,
         -7.5549e-06, -8.4117e-06],
        [-2.6345e-05, -2.1324e-05,  1.2107e-05,  ..., -2.2262e-05,
         -1.6555e-05, -1.8388e-05],
        [ 1.3576e-04,  1.8316e-04, -7.0616e-05,  ...,  1.1689e-04,
          1.1098e-04,  7.5549e-05],
        [-3.0190e-05, -2.4885e-05,  1.3947e-05,  ..., -2.5660e-05,
         -1.9580e-05, -1.9908e-05]], device='cuda:0')
Loss: 1.022754192352295


Running epoch 0, step 846, batch 846
Sampled inputs[:2]: tensor([[    0,    12, 12774,  ...,  1231,   278,   266],
        [    0, 34525,  2008,  ...,  1194,   300, 11120]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9779e-04, -8.4386e-05, -4.3627e-04,  ...,  4.4501e-04,
         -6.0535e-04,  4.0612e-04],
        [-1.3605e-05, -1.0893e-05,  5.9791e-06,  ..., -1.1630e-05,
         -8.8364e-06, -9.8422e-06],
        [-3.0547e-05, -2.4766e-05,  1.4193e-05,  ..., -2.5779e-05,
         -1.9312e-05, -2.1458e-05],
        [ 1.3298e-04,  1.8098e-04, -6.9223e-05,  ...,  1.1452e-04,
          1.0914e-04,  7.3270e-05],
        [-3.5107e-05, -2.8998e-05,  1.6406e-05,  ..., -2.9802e-05,
         -2.2918e-05, -2.3261e-05]], device='cuda:0')
Loss: 0.9954736232757568


Running epoch 0, step 847, batch 847
Sampled inputs[:2]: tensor([[    0, 12923,  2489,  ...,   474,  3301,    54],
        [    0,   259,   587,  ...,    14,    71,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5417e-05, -1.8296e-04, -5.6410e-04,  ...,  4.1726e-04,
         -6.3732e-04,  1.4559e-04],
        [-1.5453e-05, -1.2420e-05,  6.8322e-06,  ..., -1.3247e-05,
         -9.8869e-06, -1.1049e-05],
        [-3.4660e-05, -2.8193e-05,  1.6205e-05,  ..., -2.9340e-05,
         -2.1592e-05, -2.4080e-05],
        [ 1.3026e-04,  1.7881e-04, -6.7919e-05,  ...,  1.1217e-04,
          1.0766e-04,  7.1392e-05],
        [-3.9726e-05, -3.2932e-05,  1.8671e-05,  ..., -3.3826e-05,
         -2.5600e-05, -2.6017e-05]], device='cuda:0')
Loss: 1.002776861190796
Graident accumulation at epoch 0, step 847, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0159,  0.0153, -0.0282,  ...,  0.0289, -0.0148, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.2597e-05,  2.6934e-05, -2.1303e-04,  ...,  1.4090e-04,
          4.8346e-05,  1.3812e-04],
        [-1.3415e-05, -9.0085e-06,  6.5458e-06,  ..., -1.2558e-05,
         -6.5639e-06, -8.9286e-06],
        [ 2.0388e-05,  1.5763e-05, -4.2033e-06,  ...,  1.7360e-05,
          1.9429e-05, -1.9411e-06],
        [-2.0087e-06,  7.1323e-06,  3.1043e-06,  ..., -2.9801e-06,
          2.7226e-06, -5.6941e-06],
        [-4.0697e-05, -3.0499e-05,  2.2152e-05,  ..., -3.4827e-05,
         -2.2252e-05, -2.4520e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1270e-08, 5.1091e-08, 6.1054e-08,  ..., 1.9782e-08, 1.3884e-07,
         3.0314e-08],
        [7.1411e-11, 4.1857e-11, 1.0194e-11,  ..., 4.8092e-11, 9.6732e-12,
         1.8235e-11],
        [2.5298e-09, 1.2969e-09, 4.9535e-10,  ..., 1.7523e-09, 4.2600e-10,
         7.1946e-10],
        [6.3297e-10, 3.3871e-10, 1.0072e-10,  ..., 4.1637e-10, 1.2626e-10,
         1.7588e-10],
        [3.2165e-10, 1.7331e-10, 3.9268e-11,  ..., 2.3457e-10, 3.2448e-11,
         8.0429e-11]], device='cuda:0')
optimizer state dict: 106.0
lr: [1.3500484890183603e-05, 1.3500484890183603e-05]
scheduler_last_epoch: 106


Running epoch 0, step 848, batch 848
Sampled inputs[:2]: tensor([[    0,   278,  2354,  ...,  4974,  7757,   472],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6961e-05, -5.8220e-05, -1.9771e-04,  ...,  1.3720e-04,
         -2.0278e-04,  2.2206e-05],
        [-1.7658e-06, -1.4603e-06,  8.7917e-07,  ..., -1.5572e-06,
         -1.0207e-06, -1.1623e-06],
        [-4.0829e-06, -3.3826e-06,  2.1756e-06,  ..., -3.5018e-06,
         -2.2650e-06, -2.5630e-06],
        [-2.6971e-06, -2.1309e-06,  1.4305e-06,  ..., -2.3544e-06,
         -1.5050e-06, -1.9073e-06],
        [-4.5896e-06, -3.8743e-06,  2.4438e-06,  ..., -3.9637e-06,
         -2.6524e-06, -2.6822e-06]], device='cuda:0')
Loss: 0.9922645688056946


Running epoch 0, step 849, batch 849
Sampled inputs[:2]: tensor([[    0,  1549,  7052,  ...,  2529,  3958,    37],
        [    0,  3703,   278,  ...,  9807,    14, 10365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0474e-04, -1.4662e-04, -2.8034e-04,  ...,  1.6153e-04,
         -3.4605e-04, -2.7837e-05],
        [-3.6359e-06, -3.0249e-06,  1.8477e-06,  ..., -3.1367e-06,
         -2.0936e-06, -2.3097e-06],
        [-8.4043e-06, -7.0333e-06,  4.4852e-06,  ..., -7.1079e-06,
         -4.6641e-06, -5.1856e-06],
        [-5.4985e-06, -4.4256e-06,  2.9206e-06,  ..., -4.7088e-06,
         -3.0398e-06, -3.7625e-06],
        [-9.4175e-06, -8.0466e-06,  5.0366e-06,  ..., -8.0764e-06,
         -5.4836e-06, -5.4538e-06]], device='cuda:0')
Loss: 1.0636036396026611


Running epoch 0, step 850, batch 850
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 37683,    14,  1500],
        [    0,   369, 17432,  ...,   874,  2577,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0474e-04, -1.7704e-04, -3.2451e-04,  ...,  1.8334e-04,
         -7.9318e-04, -2.7837e-05],
        [-5.3197e-06, -4.3884e-06,  2.7306e-06,  ..., -4.6566e-06,
         -3.1590e-06, -3.4943e-06],
        [-1.2398e-05, -1.0297e-05,  6.6757e-06,  ..., -1.0625e-05,
         -7.0930e-06, -7.9125e-06],
        [-8.1211e-06, -6.4820e-06,  4.3511e-06,  ..., -7.0781e-06,
         -4.6492e-06, -5.7444e-06],
        [-1.3918e-05, -1.1802e-05,  7.4953e-06,  ..., -1.2040e-05,
         -8.3148e-06, -8.2850e-06]], device='cuda:0')
Loss: 0.9929178357124329


Running epoch 0, step 851, batch 851
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,   266,   298,  ...,   654,   271,  4483]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8702e-05, -1.5622e-04, -3.5563e-04,  ..., -5.3261e-05,
         -8.7909e-04, -1.4565e-04],
        [-7.2196e-06, -5.9083e-06,  3.6322e-06,  ..., -6.2883e-06,
         -4.3809e-06, -4.7684e-06],
        [-1.6779e-05, -1.3828e-05,  8.8662e-06,  ..., -1.4320e-05,
         -9.7901e-06, -1.0759e-05],
        [-1.0923e-05, -8.6576e-06,  5.7146e-06,  ..., -9.4771e-06,
         -6.4000e-06, -7.7412e-06],
        [-1.8984e-05, -1.5974e-05,  1.0043e-05,  ..., -1.6332e-05,
         -1.1519e-05, -1.1399e-05]], device='cuda:0')
Loss: 1.0289305448532104


Running epoch 0, step 852, batch 852
Sampled inputs[:2]: tensor([[   0,  278,  634,  ...,  598, 1722,  591],
        [   0,  365, 5911,  ...,  925,  408,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6902e-05, -1.0206e-04, -3.8929e-04,  ...,  2.6968e-05,
         -8.0728e-04, -7.6824e-05],
        [-9.0450e-06, -7.4208e-06,  4.4554e-06,  ..., -7.8753e-06,
         -5.5209e-06, -5.9828e-06],
        [-2.1160e-05, -1.7449e-05,  1.0982e-05,  ..., -1.8045e-05,
         -1.2413e-05, -1.3560e-05],
        [-1.3709e-05, -1.0893e-05,  7.0184e-06,  ..., -1.1861e-05,
         -8.0615e-06, -9.6783e-06],
        [-2.3901e-05, -2.0087e-05,  1.2457e-05,  ..., -2.0534e-05,
         -1.4544e-05, -1.4350e-05]], device='cuda:0')
Loss: 1.0150694847106934


Running epoch 0, step 853, batch 853
Sampled inputs[:2]: tensor([[    0,  2546,   300,  ...,    14,  1075,   756],
        [    0,    14, 21687,  ...,   943,  2153,  4089]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0332e-04, -3.8223e-05, -2.7501e-04,  ...,  1.7780e-04,
         -7.3376e-04,  1.1329e-06],
        [-1.0885e-05, -9.0003e-06,  5.3458e-06,  ..., -9.5218e-06,
         -6.6757e-06, -7.2047e-06],
        [-2.5451e-05, -2.1130e-05,  1.3158e-05,  ..., -2.1830e-05,
         -1.5005e-05, -1.6347e-05],
        [-1.6451e-05, -1.3188e-05,  8.3745e-06,  ..., -1.4305e-05,
         -9.7081e-06, -1.1615e-05],
        [-2.8729e-05, -2.4289e-05,  1.4931e-05,  ..., -2.4825e-05,
         -1.7583e-05, -1.7285e-05]], device='cuda:0')
Loss: 1.0475513935089111


Running epoch 0, step 854, batch 854
Sampled inputs[:2]: tensor([[   0, 2544,  394,  ...,   14, 1062,  516],
        [   0,  344, 8133,  ...,  278, 1603,  674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1777e-04, -2.8948e-05, -3.5642e-04,  ...,  2.2406e-04,
         -7.2023e-04,  1.6855e-05],
        [-1.2793e-05, -1.0602e-05,  6.2063e-06,  ..., -1.1183e-05,
         -7.9125e-06, -8.4564e-06],
        [-3.0011e-05, -2.4974e-05,  1.5303e-05,  ..., -2.5734e-05,
         -1.7852e-05, -1.9267e-05],
        [-1.9297e-05, -1.5527e-05,  9.6783e-06,  ..., -1.6749e-05,
         -1.1474e-05, -1.3568e-05],
        [-3.3915e-05, -2.8729e-05,  1.7390e-05,  ..., -2.9325e-05,
         -2.0921e-05, -2.0474e-05]], device='cuda:0')
Loss: 1.0416744947433472


Running epoch 0, step 855, batch 855
Sampled inputs[:2]: tensor([[    0, 38460,     9,  ...,   829,   870,    12],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4657e-04, -7.9124e-05, -4.1881e-04,  ...,  9.3389e-05,
         -6.2764e-04, -1.2294e-04],
        [-1.4633e-05, -1.2137e-05,  7.1079e-06,  ..., -1.2808e-05,
         -9.1493e-06, -9.7826e-06],
        [-3.4273e-05, -2.8566e-05,  1.7509e-05,  ..., -2.9430e-05,
         -2.0638e-05, -2.2247e-05],
        [-2.2009e-05, -1.7717e-05,  1.1109e-05,  ..., -1.9148e-05,
         -1.3269e-05, -1.5713e-05],
        [-3.8832e-05, -3.2991e-05,  1.9923e-05,  ..., -3.3647e-05,
         -2.4244e-05, -2.3678e-05]], device='cuda:0')
Loss: 1.015206217765808
Graident accumulation at epoch 0, step 855, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0194],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.6802e-06,  1.6328e-05, -2.3361e-04,  ...,  1.3615e-04,
         -1.9252e-05,  1.1201e-04],
        [-1.3537e-05, -9.3213e-06,  6.6020e-06,  ..., -1.2583e-05,
         -6.8225e-06, -9.0140e-06],
        [ 1.4922e-05,  1.1330e-05, -2.0320e-06,  ...,  1.2681e-05,
          1.5422e-05, -3.9718e-06],
        [-4.0087e-06,  4.6474e-06,  3.9048e-06,  ..., -4.5969e-06,
          1.1234e-06, -6.6960e-06],
        [-4.0511e-05, -3.0748e-05,  2.1929e-05,  ..., -3.4709e-05,
         -2.2452e-05, -2.4436e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1240e-08, 5.1046e-08, 6.1168e-08,  ..., 1.9771e-08, 1.3910e-07,
         3.0299e-08],
        [7.1554e-11, 4.1963e-11, 1.0234e-11,  ..., 4.8208e-11, 9.7472e-12,
         1.8313e-11],
        [2.5284e-09, 1.2964e-09, 4.9516e-10,  ..., 1.7514e-09, 4.2600e-10,
         7.1923e-10],
        [6.3282e-10, 3.3868e-10, 1.0074e-10,  ..., 4.1632e-10, 1.2631e-10,
         1.7595e-10],
        [3.2284e-10, 1.7423e-10, 3.9626e-11,  ..., 2.3547e-10, 3.3003e-11,
         8.0909e-11]], device='cuda:0')
optimizer state dict: 107.0
lr: [1.3384424799732402e-05, 1.3384424799732402e-05]
scheduler_last_epoch: 107


Running epoch 0, step 856, batch 856
Sampled inputs[:2]: tensor([[   0, 8125, 5241,  ...,  328, 3227,  278],
        [   0, 8416,  669,  ...,  298,  894,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1964e-05,  1.1336e-04,  6.1917e-06,  ..., -6.6040e-05,
         -8.9745e-05,  1.4973e-04],
        [-1.7434e-06, -1.4380e-06,  8.1211e-07,  ..., -1.5348e-06,
         -1.0878e-06, -1.1548e-06],
        [-4.1723e-06, -3.4422e-06,  2.0862e-06,  ..., -3.5763e-06,
         -2.4736e-06, -2.6524e-06],
        [-2.6673e-06, -2.1011e-06,  1.2964e-06,  ..., -2.3097e-06,
         -1.5944e-06, -1.8552e-06],
        [-4.7386e-06, -3.9935e-06,  2.3991e-06,  ..., -4.0829e-06,
         -2.9057e-06, -2.7865e-06]], device='cuda:0')
Loss: 1.0153625011444092


Running epoch 0, step 857, batch 857
Sampled inputs[:2]: tensor([[   0,  266, 2967,  ...,  287, 4432,   13],
        [   0,  266, 3634,  ...,  694,  266, 1784]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9375e-05,  4.9707e-05, -1.5303e-04,  ..., -1.3459e-04,
         -3.3037e-05, -3.2672e-05],
        [-3.5316e-06, -2.9132e-06,  1.7732e-06,  ..., -3.0696e-06,
         -2.0415e-06, -2.1607e-06],
        [ 6.5100e-05,  5.0857e-05, -4.6197e-05,  ...,  6.6413e-05,
          3.1737e-05,  3.6265e-05],
        [-5.4389e-06, -4.3362e-06,  2.8387e-06,  ..., -4.6790e-06,
         -2.9951e-06, -3.5092e-06],
        [-9.5069e-06, -8.0466e-06,  5.0217e-06,  ..., -8.1956e-06,
         -5.5581e-06, -5.3048e-06]], device='cuda:0')
Loss: 1.009110450744629


Running epoch 0, step 858, batch 858
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,   298,   669,  ...,   287, 19731,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8119e-05,  5.8623e-05,  5.7975e-05,  ..., -8.6683e-05,
          1.8359e-04,  2.0550e-04],
        [-5.3644e-06, -4.3511e-06,  2.6301e-06,  ..., -4.6417e-06,
         -3.2410e-06, -3.4347e-06],
        [ 6.0689e-05,  4.7355e-05, -4.3992e-05,  ...,  6.2702e-05,
          2.8951e-05,  3.3300e-05],
        [-8.2105e-06, -6.4522e-06,  4.1723e-06,  ..., -7.0333e-06,
         -4.7535e-06, -5.5358e-06],
        [-1.4663e-05, -1.2219e-05,  7.6145e-06,  ..., -1.2577e-05,
         -8.9109e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.0350459814071655


Running epoch 0, step 859, batch 859
Sampled inputs[:2]: tensor([[    0,    14,  1147,  ...,    19,    14, 42301],
        [    0,   292,    48,  ...,   199, 19047,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7089e-05,  5.5868e-05,  1.3423e-04,  ..., -8.7569e-05,
          1.1823e-04,  2.1505e-04],
        [-7.1526e-06, -5.8785e-06,  3.5614e-06,  ..., -6.1989e-06,
         -4.2692e-06, -4.5672e-06],
        [ 5.6397e-05,  4.3645e-05, -4.1667e-05,  ...,  5.8992e-05,
          2.6507e-05,  3.0618e-05],
        [-1.0923e-05, -8.7321e-06,  5.6475e-06,  ..., -9.3877e-06,
         -6.2734e-06, -7.3537e-06],
        [-1.9550e-05, -1.6510e-05,  1.0267e-05,  ..., -1.6838e-05,
         -1.1802e-05, -1.1414e-05]], device='cuda:0')
Loss: 1.0054504871368408


Running epoch 0, step 860, batch 860
Sampled inputs[:2]: tensor([[    0, 28926,   266,  ...,  1061,  2615,    13],
        [    0,    14,  6707,  ..., 17771,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4414e-05,  1.7827e-04,  2.5828e-04,  ..., -1.2072e-04,
          1.3542e-04,  1.8897e-04],
        [-8.8960e-06, -7.2718e-06,  4.3996e-06,  ..., -7.7710e-06,
         -5.3048e-06, -5.7071e-06],
        [ 5.2255e-05,  4.0277e-05, -3.9521e-05,  ...,  5.5341e-05,
          2.4152e-05,  2.8025e-05],
        [-1.3605e-05, -1.0818e-05,  6.9886e-06,  ..., -1.1787e-05,
         -7.8157e-06, -9.2089e-06],
        [-2.4348e-05, -2.0474e-05,  1.2770e-05,  ..., -2.1070e-05,
         -1.4618e-05, -1.4201e-05]], device='cuda:0')
Loss: 1.0450888872146606


Running epoch 0, step 861, batch 861
Sampled inputs[:2]: tensor([[   0, 1075,  940,  ..., 3780,   13, 4467],
        [   0,  266,  452,  ..., 1725, 2200,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5553e-05,  1.0866e-04,  1.4965e-04,  ..., -1.6309e-04,
          1.1972e-04,  8.0238e-05],
        [-1.0610e-05, -8.6948e-06,  5.3346e-06,  ..., -9.2909e-06,
         -6.2473e-06, -6.6832e-06],
        [ 4.8232e-05,  3.6939e-05, -3.7212e-05,  ...,  5.1839e-05,
          2.2022e-05,  2.5790e-05],
        [-1.6347e-05, -1.3024e-05,  8.5458e-06,  ..., -1.4201e-05,
         -9.2462e-06, -1.0885e-05],
        [-2.8878e-05, -2.4348e-05,  1.5393e-05,  ..., -2.5064e-05,
         -1.7181e-05, -1.6540e-05]], device='cuda:0')
Loss: 1.0398625135421753


Running epoch 0, step 862, batch 862
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  6354, 12230,     9],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2568e-04,  9.0704e-05,  8.3938e-05,  ..., -1.5073e-04,
          6.6343e-05,  4.7861e-05],
        [-1.2465e-05, -1.0245e-05,  6.2659e-06,  ..., -1.0855e-05,
         -7.3276e-06, -7.8455e-06],
        [ 4.3880e-05,  3.3229e-05, -3.4932e-05,  ...,  4.8174e-05,
          1.9533e-05,  2.3093e-05],
        [-1.9178e-05, -1.5348e-05,  1.0006e-05,  ..., -1.6585e-05,
         -1.0833e-05, -1.2763e-05],
        [-3.3915e-05, -2.8729e-05,  1.8030e-05,  ..., -2.9355e-05,
         -2.0206e-05, -1.9476e-05]], device='cuda:0')
Loss: 1.0153605937957764


Running epoch 0, step 863, batch 863
Sampled inputs[:2]: tensor([[    0,   494,   360,  ...,   391, 24104, 35211],
        [    0,  4209,   278,  ...,   287,  9971,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7132e-04,  2.4827e-04, -2.7177e-05,  ..., -1.3965e-04,
         -8.9092e-05,  2.2565e-04],
        [-1.4462e-05, -1.1556e-05,  7.0818e-06,  ..., -1.2353e-05,
         -8.4452e-06, -9.5367e-06],
        [ 3.9082e-05,  2.9951e-05, -3.2816e-05,  ...,  4.4568e-05,
          1.6851e-05,  1.9099e-05],
        [-2.2069e-05, -1.7203e-05,  1.1265e-05,  ..., -1.8731e-05,
         -1.2405e-05, -1.5311e-05],
        [-3.9488e-05, -3.2634e-05,  2.0504e-05,  ..., -3.3587e-05,
         -2.3440e-05, -2.3827e-05]], device='cuda:0')
Loss: 0.9850402474403381
Graident accumulation at epoch 0, step 863, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2020e-05,  3.9522e-05, -2.1296e-04,  ...,  1.0857e-04,
         -2.6236e-05,  1.2338e-04],
        [-1.3629e-05, -9.5448e-06,  6.6500e-06,  ..., -1.2560e-05,
         -6.9847e-06, -9.0662e-06],
        [ 1.7338e-05,  1.3192e-05, -5.1104e-06,  ...,  1.5870e-05,
          1.5565e-05, -1.6647e-06],
        [-5.8147e-06,  2.4623e-06,  4.6408e-06,  ..., -6.0103e-06,
         -2.2947e-07, -7.5575e-06],
        [-4.0408e-05, -3.0937e-05,  2.1786e-05,  ..., -3.4597e-05,
         -2.2550e-05, -2.4375e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1218e-08, 5.1056e-08, 6.1108e-08,  ..., 1.9770e-08, 1.3897e-07,
         3.0320e-08],
        [7.1692e-11, 4.2054e-11, 1.0274e-11,  ..., 4.8313e-11, 9.8088e-12,
         1.8385e-11],
        [2.5274e-09, 1.2960e-09, 4.9574e-10,  ..., 1.7517e-09, 4.2585e-10,
         7.1888e-10],
        [6.3268e-10, 3.3864e-10, 1.0077e-10,  ..., 4.1626e-10, 1.2634e-10,
         1.7601e-10],
        [3.2407e-10, 1.7512e-10, 4.0007e-11,  ..., 2.3636e-10, 3.3519e-11,
         8.1396e-11]], device='cuda:0')
optimizer state dict: 108.0
lr: [1.3267847539628745e-05, 1.3267847539628745e-05]
scheduler_last_epoch: 108


Running epoch 0, step 864, batch 864
Sampled inputs[:2]: tensor([[    0, 13964,    13,  ...,    14,   560,   199],
        [    0,   292, 15087,  ...,  2675,  1663,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7580e-05, -1.4108e-05,  1.2907e-04,  ...,  1.2027e-04,
          2.3859e-04, -1.7660e-04],
        [-1.5795e-06, -1.2517e-06,  9.7603e-07,  ..., -1.4156e-06,
         -9.2387e-07, -9.8348e-07],
        [-3.7998e-06, -3.0696e-06,  2.4438e-06,  ..., -3.3528e-06,
         -2.1458e-06, -2.2650e-06],
        [-2.5183e-06, -1.9073e-06,  1.6913e-06,  ..., -2.2650e-06,
         -1.4231e-06, -1.7285e-06],
        [-4.3511e-06, -3.6210e-06,  2.7567e-06,  ..., -3.8445e-06,
         -2.5928e-06, -2.3693e-06]], device='cuda:0')
Loss: 0.9814117550849915


Running epoch 0, step 865, batch 865
Sampled inputs[:2]: tensor([[    0,    12,  9328,  ...,    20,   408,   790],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9972e-04,  2.1134e-04,  3.5606e-04,  ...,  1.8254e-04,
          5.3294e-04,  1.2512e-04],
        [-3.2932e-06, -2.5854e-06,  1.8068e-06,  ..., -2.8461e-06,
         -2.0117e-06, -2.3767e-06],
        [-7.9125e-06, -6.2883e-06,  4.6343e-06,  ..., -6.6757e-06,
         -4.6492e-06, -5.3942e-06],
        [-5.1558e-06, -3.8743e-06,  3.0547e-06,  ..., -4.4107e-06,
         -3.0324e-06, -3.9488e-06],
        [-9.3281e-06, -7.5847e-06,  5.4389e-06,  ..., -7.8976e-06,
         -5.7220e-06, -5.8711e-06]], device='cuda:0')
Loss: 1.0114226341247559


Running epoch 0, step 866, batch 866
Sampled inputs[:2]: tensor([[    0,    13,  2615,  ..., 31594, 15867,  3484],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6812e-04,  1.7092e-04,  4.4250e-04,  ...,  2.9595e-04,
          6.2190e-04,  2.5986e-04],
        [-5.0589e-06, -4.0904e-06,  2.7753e-06,  ..., -4.3362e-06,
         -3.0249e-06, -3.4720e-06],
        [-1.2293e-05, -1.0043e-05,  7.1079e-06,  ..., -1.0341e-05,
         -7.1079e-06, -8.0913e-06],
        [-7.9870e-06, -6.2138e-06,  4.6492e-06,  ..., -6.7651e-06,
         -4.5672e-06, -5.8189e-06],
        [-1.4395e-05, -1.2025e-05,  8.2999e-06,  ..., -1.2189e-05,
         -8.7321e-06, -8.7768e-06]], device='cuda:0')
Loss: 1.0454894304275513


Running epoch 0, step 867, batch 867
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  275, 2576, 3588],
        [   0, 1531,   14,  ..., 6169,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9391e-04,  2.0541e-04,  5.8841e-04,  ...,  4.3404e-04,
          8.8572e-04,  2.6296e-04],
        [-6.7726e-06, -5.5283e-06,  3.7141e-06,  ..., -5.7966e-06,
         -3.9786e-06, -4.4480e-06],
        [-1.6317e-05, -1.3471e-05,  9.4026e-06,  ..., -1.3724e-05,
         -9.2834e-06, -1.0341e-05],
        [-1.0639e-05, -8.4043e-06,  6.1765e-06,  ..., -9.0152e-06,
         -5.9903e-06, -7.4506e-06],
        [-1.9163e-05, -1.6198e-05,  1.1042e-05,  ..., -1.6302e-05,
         -1.1504e-05, -1.1265e-05]], device='cuda:0')
Loss: 1.0042976140975952


Running epoch 0, step 868, batch 868
Sampled inputs[:2]: tensor([[    0,   257,   221,  ...,  1474,  2044,   300],
        [    0, 23230,    12,  ...,  5092,   741,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0116e-04,  3.0260e-04,  5.8841e-04,  ...,  7.7764e-04,
          1.0281e-03,  5.1304e-04],
        [-8.4639e-06, -6.6981e-06,  4.5709e-06,  ..., -7.2643e-06,
         -4.9695e-06, -5.8264e-06],
        [-2.0340e-05, -1.6317e-05,  1.1638e-05,  ..., -1.7107e-05,
         -1.1519e-05, -1.3426e-05],
        [-1.3322e-05, -1.0170e-05,  7.6294e-06,  ..., -1.1310e-05,
         -7.4878e-06, -9.7305e-06],
        [-2.3842e-05, -1.9610e-05,  1.3664e-05,  ..., -2.0236e-05,
         -1.4201e-05, -1.4603e-05]], device='cuda:0')
Loss: 0.9857637286186218


Running epoch 0, step 869, batch 869
Sampled inputs[:2]: tensor([[    0,   287, 14752,  ...,   910, 26097,  1477],
        [    0,   259,  1143,  ...,   593,   360,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5222e-04,  2.8395e-04,  5.1606e-04,  ...,  8.2003e-04,
          1.0633e-03,  3.6793e-04],
        [-1.0312e-05, -8.2403e-06,  5.5842e-06,  ..., -8.7917e-06,
         -6.0499e-06, -6.8620e-06],
        [ 6.0927e-05,  7.0944e-05, -3.9768e-06,  ...,  9.5315e-05,
          7.3662e-05,  1.7100e-05],
        [-1.6212e-05, -1.2539e-05,  9.2536e-06,  ..., -1.3679e-05,
         -9.1121e-06, -1.1444e-05],
        [-2.8998e-05, -2.4140e-05,  1.6615e-05,  ..., -2.4617e-05,
         -1.7375e-05, -1.7345e-05]], device='cuda:0')
Loss: 1.0386269092559814


Running epoch 0, step 870, batch 870
Sampled inputs[:2]: tensor([[    0, 12440,   578,  ..., 25918,   287,   996],
        [    0,   221, 18844,  ...,   199, 10174,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9879e-04,  2.9553e-04,  5.6079e-04,  ...,  8.6385e-04,
          1.1224e-03,  5.2806e-04],
        [-1.2018e-05, -9.4548e-06,  6.5751e-06,  ..., -1.0177e-05,
         -6.9365e-06, -8.0094e-06],
        [ 5.6785e-05,  6.7949e-05, -1.4734e-06,  ...,  9.1992e-05,
          7.1576e-05,  1.4388e-05],
        [-1.9044e-05, -1.4462e-05,  1.0975e-05,  ..., -1.5929e-05,
         -1.0483e-05, -1.3456e-05],
        [-3.3796e-05, -2.7701e-05,  1.9506e-05,  ..., -2.8521e-05,
         -1.9938e-05, -2.0280e-05]], device='cuda:0')
Loss: 1.0239193439483643


Running epoch 0, step 871, batch 871
Sampled inputs[:2]: tensor([[    0,  1241,  2098,  ...,  1862,   631,   369],
        [    0,   292, 21215,  ...,   266,   818,  1527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4924e-04,  3.1946e-04,  6.9376e-04,  ...,  8.6391e-04,
          1.2807e-03,  5.7004e-04],
        [-1.3798e-05, -1.0937e-05,  7.5586e-06,  ..., -1.1653e-05,
         -7.8715e-06, -9.0376e-06],
        [ 5.2493e-05,  6.4343e-05,  1.0002e-06,  ...,  8.8445e-05,
          6.9341e-05,  1.1929e-05],
        [-2.1920e-05, -1.6786e-05,  1.2614e-05,  ..., -1.8284e-05,
         -1.1906e-05, -1.5207e-05],
        [-3.8683e-05, -3.1903e-05,  2.2337e-05,  ..., -3.2604e-05,
         -2.2635e-05, -2.2888e-05]], device='cuda:0')
Loss: 1.0311570167541504
Graident accumulation at epoch 0, step 871, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5742e-05,  6.7516e-05, -1.2229e-04,  ...,  1.8410e-04,
          1.0446e-04,  1.6804e-04],
        [-1.3646e-05, -9.6841e-06,  6.7408e-06,  ..., -1.2470e-05,
         -7.0734e-06, -9.0634e-06],
        [ 2.0853e-05,  1.8307e-05, -4.4994e-06,  ...,  2.3127e-05,
          2.0943e-05, -3.0530e-07],
        [-7.4252e-06,  5.3745e-07,  5.4381e-06,  ..., -7.2376e-06,
         -1.3971e-06, -8.3224e-06],
        [-4.0236e-05, -3.1033e-05,  2.1841e-05,  ..., -3.4398e-05,
         -2.2559e-05, -2.4226e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1289e-08, 5.1107e-08, 6.1528e-08,  ..., 2.0497e-08, 1.4047e-07,
         3.0614e-08],
        [7.1810e-11, 4.2132e-11, 1.0321e-11,  ..., 4.8400e-11, 9.8609e-12,
         1.8449e-11],
        [2.5276e-09, 1.2988e-09, 4.9525e-10,  ..., 1.7578e-09, 4.3024e-10,
         7.1830e-10],
        [6.3253e-10, 3.3858e-10, 1.0083e-10,  ..., 4.1618e-10, 1.2636e-10,
         1.7607e-10],
        [3.2525e-10, 1.7596e-10, 4.0465e-11,  ..., 2.3719e-10, 3.3998e-11,
         8.1838e-11]], device='cuda:0')
optimizer state dict: 109.0
lr: [1.3150770923895586e-05, 1.3150770923895586e-05]
scheduler_last_epoch: 109


Running epoch 0, step 872, batch 872
Sampled inputs[:2]: tensor([[    0, 16028,   669,  ...,   292,  6502,  7050],
        [    0,  1008,   266,  ...,  1941,   437,  1626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6532e-06,  1.0702e-04,  7.3382e-05,  ..., -4.3845e-05,
         -6.1690e-06,  1.2882e-04],
        [-1.8477e-06, -1.3262e-06,  9.0152e-07,  ..., -1.4156e-06,
         -1.0133e-06, -1.2890e-06],
        [-4.4107e-06, -3.3081e-06,  2.3097e-06,  ..., -3.3677e-06,
         -2.4289e-06, -2.9802e-06],
        [-2.9206e-06, -2.0564e-06,  1.5050e-06,  ..., -2.2203e-06,
         -1.5572e-06, -2.1011e-06],
        [-5.2452e-06, -4.0233e-06,  2.7418e-06,  ..., -4.0531e-06,
         -3.0249e-06, -3.3528e-06]], device='cuda:0')
Loss: 0.9592065215110779


Running epoch 0, step 873, batch 873
Sampled inputs[:2]: tensor([[    0,    13, 15578,  ...,   221,   494,   221],
        [    0,  6978,  2285,  ...,  4477,   271,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4917e-05,  5.6548e-05,  2.2676e-04,  ...,  2.4717e-05,
          3.9366e-04,  2.0330e-04],
        [-3.5614e-06, -2.7791e-06,  1.8030e-06,  ..., -2.8536e-06,
         -2.0787e-06, -2.3469e-06],
        [-8.5831e-06, -6.8247e-06,  4.6343e-06,  ..., -6.7949e-06,
         -4.8876e-06, -5.4687e-06],
        [-5.6773e-06, -4.2915e-06,  3.0026e-06,  ..., -4.5002e-06,
         -3.1963e-06, -3.9041e-06],
        [-1.0163e-05, -8.3148e-06,  5.5283e-06,  ..., -8.1658e-06,
         -6.0648e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.0494824647903442


Running epoch 0, step 874, batch 874
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  2117,  2792, 12848],
        [    0,  1706,  8554,  ...,  9742,   221, 14082]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9398e-04,  2.1545e-04,  3.9722e-04,  ...,  1.3023e-04,
          4.8888e-04,  2.7644e-04],
        [-5.2899e-06, -4.2468e-06,  2.7120e-06,  ..., -4.3437e-06,
         -3.2261e-06, -3.5018e-06],
        [-1.2755e-05, -1.0386e-05,  6.9886e-06,  ..., -1.0327e-05,
         -7.5251e-06, -8.1658e-06],
        [-8.4490e-06, -6.5416e-06,  4.5151e-06,  ..., -6.8396e-06,
         -4.9472e-06, -5.8562e-06],
        [-1.5110e-05, -1.2636e-05,  8.3297e-06,  ..., -1.2398e-05,
         -9.3281e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.0252025127410889


Running epoch 0, step 875, batch 875
Sampled inputs[:2]: tensor([[   0,   13, 4596,  ...,  408,  689,  298],
        [   0,  472,  346,  ...,  298,  527,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6500e-05, -1.0505e-04,  1.1362e-03,  ...,  5.4765e-04,
          2.1378e-03,  7.6375e-04],
        [-6.8694e-06, -5.4166e-06,  3.6396e-06,  ..., -5.7667e-06,
         -4.2543e-06, -4.6417e-06],
        [-1.6466e-05, -1.3173e-05,  9.3430e-06,  ..., -1.3500e-05,
         -9.7603e-06, -1.0610e-05],
        [-1.1027e-05, -8.3297e-06,  6.1765e-06,  ..., -9.1344e-06,
         -6.5491e-06, -7.8231e-06],
        [-1.9580e-05, -1.6093e-05,  1.1161e-05,  ..., -1.6212e-05,
         -1.2144e-05, -1.1772e-05]], device='cuda:0')
Loss: 0.9869657754898071


Running epoch 0, step 876, batch 876
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,   29,  445,   16],
        [   0,  879,   27,  ..., 3958, 2875,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2361e-05, -1.3786e-05,  1.1813e-03,  ...,  5.0361e-04,
          2.3030e-03,  8.9745e-04],
        [-8.7768e-06, -6.8843e-06,  4.4927e-06,  ..., -7.2643e-06,
         -5.4017e-06, -5.9530e-06],
        [-2.1055e-05, -1.6794e-05,  1.1548e-05,  ..., -1.7062e-05,
         -1.2457e-05, -1.3694e-05],
        [-1.4022e-05, -1.0580e-05,  7.5623e-06,  ..., -1.1444e-05,
         -8.2850e-06, -9.9838e-06],
        [-2.4945e-05, -2.0444e-05,  1.3784e-05,  ..., -2.0474e-05,
         -1.5482e-05, -1.5169e-05]], device='cuda:0')
Loss: 1.007261037826538


Running epoch 0, step 877, batch 877
Sampled inputs[:2]: tensor([[   0,  496,   14,  ...,  266,  596,   13],
        [   0, 1927,  863,  ..., 1163,   13, 1888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3401e-04,  2.2232e-04,  1.2131e-03,  ...,  5.0361e-04,
          2.1773e-03,  8.1996e-04],
        [-1.0617e-05, -8.2925e-06,  5.4277e-06,  ..., -8.7395e-06,
         -6.4149e-06, -7.1973e-06],
        [-2.5436e-05, -2.0236e-05,  1.3903e-05,  ..., -2.0579e-05,
         -1.4856e-05, -1.6600e-05],
        [-1.6913e-05, -1.2755e-05,  9.1121e-06,  ..., -1.3739e-05,
         -9.7975e-06, -1.1995e-05],
        [-3.0071e-05, -2.4557e-05,  1.6555e-05,  ..., -2.4647e-05,
         -1.8463e-05, -1.8373e-05]], device='cuda:0')
Loss: 0.9853706955909729


Running epoch 0, step 878, batch 878
Sampled inputs[:2]: tensor([[    0,   669,  1528,  ..., 21826,   259,  5024],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8460e-04,  2.2232e-04,  1.3000e-03,  ...,  5.3755e-04,
          2.1526e-03,  7.9863e-04],
        [-1.2383e-05, -9.6709e-06,  6.4112e-06,  ..., -1.0215e-05,
         -7.3835e-06, -8.2925e-06],
        [-2.9549e-05, -2.3484e-05,  1.6332e-05,  ..., -2.3976e-05,
         -1.7062e-05, -1.9088e-05],
        [-1.9759e-05, -1.4901e-05,  1.0759e-05,  ..., -1.6078e-05,
         -1.1288e-05, -1.3851e-05],
        [-3.4899e-05, -2.8461e-05,  1.9416e-05,  ..., -2.8670e-05,
         -2.1204e-05, -2.1085e-05]], device='cuda:0')
Loss: 1.0310888290405273


Running epoch 0, step 879, batch 879
Sampled inputs[:2]: tensor([[    0,   445,    29,  ..., 20247,   272,   298],
        [    0, 35449,   824,  ...,   278, 30449,  3659]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6320e-04,  3.3473e-04,  1.1160e-03,  ...,  6.5885e-04,
          1.8538e-03,  1.0490e-03],
        [-1.4186e-05, -1.0967e-05,  7.3351e-06,  ..., -1.1645e-05,
         -8.3521e-06, -9.5442e-06],
        [-3.3930e-05, -2.6703e-05,  1.8716e-05,  ..., -2.7403e-05,
         -1.9342e-05, -2.2039e-05],
        [-2.2665e-05, -1.6928e-05,  1.2308e-05,  ..., -1.8343e-05,
         -1.2770e-05, -1.5937e-05],
        [-3.9876e-05, -3.2201e-05,  2.2143e-05,  ..., -3.2604e-05,
         -2.3931e-05, -2.4229e-05]], device='cuda:0')
Loss: 0.989413857460022
Graident accumulation at epoch 0, step 879, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.7488e-05,  9.4237e-05,  1.5370e-06,  ...,  2.3158e-04,
          2.7939e-04,  2.5614e-04],
        [-1.3700e-05, -9.8124e-06,  6.8002e-06,  ..., -1.2387e-05,
         -7.2013e-06, -9.1114e-06],
        [ 1.5375e-05,  1.3806e-05, -2.1778e-06,  ...,  1.8074e-05,
          1.6914e-05, -2.4787e-06],
        [-8.9492e-06, -1.2091e-06,  6.1251e-06,  ..., -8.3482e-06,
         -2.5344e-06, -9.0838e-06],
        [-4.0200e-05, -3.1150e-05,  2.1872e-05,  ..., -3.4218e-05,
         -2.2696e-05, -2.4227e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1370e-08, 5.1168e-08, 6.2712e-08,  ..., 2.0911e-08, 1.4376e-07,
         3.1684e-08],
        [7.1940e-11, 4.2210e-11, 1.0365e-11,  ..., 4.8487e-11, 9.9208e-12,
         1.8521e-11],
        [2.5263e-09, 1.2983e-09, 4.9510e-10,  ..., 1.7567e-09, 4.3018e-10,
         7.1807e-10],
        [6.3241e-10, 3.3853e-10, 1.0088e-10,  ..., 4.1610e-10, 1.2639e-10,
         1.7615e-10],
        [3.2651e-10, 1.7682e-10, 4.0915e-11,  ..., 2.3802e-10, 3.4537e-11,
         8.2343e-11]], device='cuda:0')
optimizer state dict: 110.0
lr: [1.3033212842861785e-05, 1.3033212842861785e-05]
scheduler_last_epoch: 110


Running epoch 0, step 880, batch 880
Sampled inputs[:2]: tensor([[    0,    17,   590,  ...,  1412,    35,  5015],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4332e-06, -2.3401e-05,  1.2944e-04,  ..., -4.1229e-05,
          9.8012e-05,  7.1102e-05],
        [-1.7583e-06, -1.4007e-06,  1.0356e-06,  ..., -1.4976e-06,
         -1.0431e-06, -1.0505e-06],
        [-4.1127e-06, -3.2932e-06,  2.5183e-06,  ..., -3.4571e-06,
         -2.3842e-06, -2.4289e-06],
        [-2.8461e-06, -2.1905e-06,  1.7360e-06,  ..., -2.3842e-06,
         -1.6019e-06, -1.7881e-06],
        [-4.8280e-06, -3.9935e-06,  2.9653e-06,  ..., -4.1127e-06,
         -2.9504e-06, -2.6822e-06]], device='cuda:0')
Loss: 1.0251963138580322


Running epoch 0, step 881, batch 881
Sampled inputs[:2]: tensor([[   0,  380,  560,  ...,  287, 6769,  806],
        [   0, 9792, 3239,  ...,  699, 3636, 1761]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0912e-05, -1.1048e-04,  1.8713e-04,  ..., -1.5980e-05,
         -1.2517e-05,  1.0122e-04],
        [-3.5018e-06, -2.7269e-06,  2.0415e-06,  ..., -2.9579e-06,
         -2.0638e-06, -2.1160e-06],
        [-8.2552e-06, -6.4671e-06,  5.0068e-06,  ..., -6.8694e-06,
         -4.7237e-06, -4.8876e-06],
        [-5.7667e-06, -4.3362e-06,  3.4869e-06,  ..., -4.7833e-06,
         -3.2336e-06, -3.6582e-06],
        [-9.6262e-06, -7.8082e-06,  5.8711e-06,  ..., -8.1360e-06,
         -5.8264e-06, -5.3644e-06]], device='cuda:0')
Loss: 1.035838007926941


Running epoch 0, step 882, batch 882
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0,  7779,    12,  ...,  1380, 10199,  1086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0477e-04, -2.4801e-04,  1.7309e-04,  ...,  1.4384e-04,
         -1.4751e-05,  1.0324e-04],
        [-5.2154e-06, -4.0978e-06,  3.1367e-06,  ..., -4.3660e-06,
         -2.9989e-06, -3.1218e-06],
        [-1.2279e-05, -9.7305e-06,  7.6592e-06,  ..., -1.0177e-05,
         -6.8992e-06, -7.2271e-06],
        [-8.6576e-06, -6.5863e-06,  5.4389e-06,  ..., -7.1526e-06,
         -4.7460e-06, -5.4911e-06],
        [-1.4275e-05, -1.1653e-05,  8.8960e-06,  ..., -1.1981e-05,
         -8.4639e-06, -7.8827e-06]], device='cuda:0')
Loss: 1.0357218980789185


Running epoch 0, step 883, batch 883
Sampled inputs[:2]: tensor([[   0,  352,  644,  ..., 2928,  590, 3040],
        [   0,  992,  409,  ..., 5843,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5519e-05, -9.8616e-05,  1.0816e-04,  ...,  2.3846e-04,
         -1.8735e-04,  2.0354e-04],
        [-6.9737e-06, -5.3346e-06,  4.1872e-06,  ..., -5.7518e-06,
         -3.9041e-06, -4.2096e-06],
        [-1.6421e-05, -1.2681e-05,  1.0222e-05,  ..., -1.3411e-05,
         -9.0003e-06, -9.7454e-06],
        [-1.1578e-05, -8.5384e-06,  7.2494e-06,  ..., -9.4175e-06,
         -6.1691e-06, -7.3910e-06],
        [-1.9133e-05, -1.5259e-05,  1.1906e-05,  ..., -1.5855e-05,
         -1.1101e-05, -1.0639e-05]], device='cuda:0')
Loss: 0.9963201284408569


Running epoch 0, step 884, batch 884
Sampled inputs[:2]: tensor([[    0,  3767,  2337,  ...,   950,   847,   300],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5849e-04, -7.8800e-05,  1.4157e-04,  ...,  3.4598e-04,
         -4.9762e-05,  3.8678e-04],
        [-8.8811e-06, -6.7800e-06,  5.1707e-06,  ..., -7.2718e-06,
         -5.0515e-06, -5.5134e-06],
        [ 8.7119e-05,  9.5025e-05, -1.6976e-05,  ...,  3.5622e-05,
          4.3277e-05,  2.3097e-05],
        [-1.4588e-05, -1.0759e-05,  8.8662e-06,  ..., -1.1787e-05,
         -7.8902e-06, -9.5218e-06],
        [-2.4498e-05, -1.9521e-05,  1.4782e-05,  ..., -2.0176e-05,
         -1.4454e-05, -1.4096e-05]], device='cuda:0')
Loss: 1.0338921546936035


Running epoch 0, step 885, batch 885
Sampled inputs[:2]: tensor([[   0, 5775,  292,  ..., 8671, 1339,  642],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0769e-04, -1.2019e-04,  2.7494e-05,  ...,  4.3834e-04,
         -1.7396e-04,  3.6651e-04],
        [-1.0647e-05, -8.1286e-06,  6.1914e-06,  ..., -8.7395e-06,
         -6.0126e-06, -6.5938e-06],
        [ 8.2858e-05,  9.1717e-05, -1.4413e-05,  ...,  3.2090e-05,
          4.0998e-05,  2.0474e-05],
        [-1.7568e-05, -1.2964e-05,  1.0647e-05,  ..., -1.4231e-05,
         -9.4324e-06, -1.1459e-05],
        [-2.9355e-05, -2.3425e-05,  1.7688e-05,  ..., -2.4289e-05,
         -1.7256e-05, -1.6913e-05]], device='cuda:0')
Loss: 1.0293885469436646


Running epoch 0, step 886, batch 886
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,  1125,   609,   292],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5622e-04, -2.0258e-04, -1.0597e-04,  ...,  4.3834e-04,
         -4.7509e-04,  2.2348e-04],
        [-1.2375e-05, -9.4250e-06,  7.2047e-06,  ..., -1.0170e-05,
         -6.9961e-06, -7.6443e-06],
        [ 7.8834e-05,  8.8662e-05, -1.1924e-05,  ...,  2.8782e-05,
          3.8792e-05,  1.8090e-05],
        [-2.0429e-05, -1.5020e-05,  1.2390e-05,  ..., -1.6585e-05,
         -1.0997e-05, -1.3322e-05],
        [-3.4124e-05, -2.7195e-05,  2.0638e-05,  ..., -2.8282e-05,
         -2.0042e-05, -1.9550e-05]], device='cuda:0')
Loss: 0.9976837635040283


Running epoch 0, step 887, batch 887
Sampled inputs[:2]: tensor([[    0,    15,    72,  ...,   380, 22463,  2587],
        [    0,  5506,   696,  ...,   607, 11129,   276]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2826e-04, -2.4461e-04, -1.5429e-04,  ...,  5.1966e-04,
         -4.2328e-04,  2.6712e-04],
        [-1.4089e-05, -1.0833e-05,  8.1211e-06,  ..., -1.1608e-05,
         -8.0615e-06, -8.7842e-06],
        [ 7.4781e-05,  8.5339e-05, -9.5997e-06,  ...,  2.5489e-05,
          3.6438e-05,  1.5542e-05],
        [-2.3171e-05, -1.7181e-05,  1.3910e-05,  ..., -1.8820e-05,
         -1.2614e-05, -1.5207e-05],
        [-3.8981e-05, -3.1278e-05,  2.3454e-05,  ..., -3.2276e-05,
         -2.3007e-05, -2.2382e-05]], device='cuda:0')
Loss: 1.0168663263320923
Graident accumulation at epoch 0, step 887, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0257e-04,  6.0353e-05, -1.4046e-05,  ...,  2.6039e-04,
          2.0912e-04,  2.5723e-04],
        [-1.3739e-05, -9.9145e-06,  6.9323e-06,  ..., -1.2309e-05,
         -7.2873e-06, -9.0787e-06],
        [ 2.1316e-05,  2.0960e-05, -2.9200e-06,  ...,  1.8816e-05,
          1.8867e-05, -6.7659e-07],
        [-1.0371e-05, -2.8063e-06,  6.9037e-06,  ..., -9.3954e-06,
         -3.5424e-06, -9.6961e-06],
        [-4.0078e-05, -3.1163e-05,  2.2030e-05,  ..., -3.4024e-05,
         -2.2727e-05, -2.4042e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1426e-08, 5.1177e-08, 6.2673e-08,  ..., 2.1160e-08, 1.4380e-07,
         3.1724e-08],
        [7.2066e-11, 4.2285e-11, 1.0420e-11,  ..., 4.8574e-11, 9.9759e-12,
         1.8580e-11],
        [2.5293e-09, 1.3042e-09, 4.9470e-10,  ..., 1.7556e-09, 4.3108e-10,
         7.1759e-10],
        [6.3231e-10, 3.3849e-10, 1.0097e-10,  ..., 4.1603e-10, 1.2643e-10,
         1.7620e-10],
        [3.2770e-10, 1.7762e-10, 4.1424e-11,  ..., 2.3882e-10, 3.5032e-11,
         8.2762e-11]], device='cuda:0')
optimizer state dict: 111.0
lr: [1.2915191260428308e-05, 1.2915191260428308e-05]
scheduler_last_epoch: 111


Running epoch 0, step 888, batch 888
Sampled inputs[:2]: tensor([[   0,  857,  344,  ..., 1529, 9106, 1447],
        [   0, 2973,   30,  ...,  408,  259, 1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0751e-05, -1.6024e-04, -1.3108e-04,  ...,  1.0556e-05,
         -1.4110e-04, -1.3470e-04],
        [-1.8701e-06, -1.4454e-06,  1.0878e-06,  ..., -1.5050e-06,
         -1.0654e-06, -1.1027e-06],
        [-4.2915e-06, -3.3826e-06,  2.5928e-06,  ..., -3.4571e-06,
         -2.4140e-06, -2.5332e-06],
        [-2.9802e-06, -2.2352e-06,  1.7807e-06,  ..., -2.3693e-06,
         -1.6242e-06, -1.8552e-06],
        [-5.0962e-06, -4.1425e-06,  3.0547e-06,  ..., -4.1723e-06,
         -3.0398e-06, -2.8163e-06]], device='cuda:0')
Loss: 1.0229206085205078


Running epoch 0, step 889, batch 889
Sampled inputs[:2]: tensor([[   0,  278, 3358,  ...,   12,  287, 9612],
        [   0,  472,  346,  ..., 9161,  300, 4460]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1244e-04,  5.8872e-05,  1.1020e-05,  ...,  1.0508e-04,
          3.0833e-05, -1.6980e-04],
        [-3.7104e-06, -2.7195e-06,  2.1532e-06,  ..., -3.0398e-06,
         -2.1607e-06, -2.2873e-06],
        [-8.4937e-06, -6.3628e-06,  5.1260e-06,  ..., -6.9141e-06,
         -4.8578e-06, -5.1707e-06],
        [-5.9307e-06, -4.1723e-06,  3.5688e-06,  ..., -4.7982e-06,
         -3.3155e-06, -3.8818e-06],
        [-1.0073e-05, -7.8380e-06,  6.0797e-06,  ..., -8.3447e-06,
         -6.0946e-06, -5.7518e-06]], device='cuda:0')
Loss: 1.0052763223648071


Running epoch 0, step 890, batch 890
Sampled inputs[:2]: tensor([[    0,  2853,   590,  ...,  1351,  2927,    12],
        [    0,  1254,  1773,  ..., 19459,  2447,  2613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1529e-04, -3.7129e-05,  1.2353e-04,  ...,  7.8809e-05,
          1.2084e-04, -2.0423e-04],
        [-5.5730e-06, -4.1574e-06,  3.2559e-06,  ..., -4.5598e-06,
         -3.2336e-06, -3.4496e-06],
        [-1.2815e-05, -9.7901e-06,  7.7784e-06,  ..., -1.0446e-05,
         -7.3612e-06, -7.8976e-06],
        [-8.9258e-06, -6.4373e-06,  5.3942e-06,  ..., -7.2122e-06,
         -4.9919e-06, -5.8487e-06],
        [-1.5140e-05, -1.1951e-05,  9.1940e-06,  ..., -1.2547e-05,
         -9.1642e-06, -8.7768e-06]], device='cuda:0')
Loss: 1.0364478826522827


Running epoch 0, step 891, batch 891
Sampled inputs[:2]: tensor([[    0,  2518,   437,  ...,    12,  1041,   283],
        [    0, 15152,  1106,  ...,   607,   266,  2529]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9558e-04, -8.7457e-05,  1.3332e-04,  ...,  9.1273e-05,
          5.5531e-05, -2.2319e-04],
        [-7.3761e-06, -5.4613e-06,  4.2617e-06,  ..., -6.0052e-06,
         -4.2245e-06, -4.6194e-06],
        [-1.7107e-05, -1.2919e-05,  1.0297e-05,  ..., -1.3843e-05,
         -9.6560e-06, -1.0639e-05],
        [-1.1906e-05, -8.4937e-06,  7.1228e-06,  ..., -9.5665e-06,
         -6.5491e-06, -7.8902e-06],
        [-2.0057e-05, -1.5676e-05,  1.2085e-05,  ..., -1.6510e-05,
         -1.1966e-05, -1.1697e-05]], device='cuda:0')
Loss: 1.007741093635559


Running epoch 0, step 892, batch 892
Sampled inputs[:2]: tensor([[    0,   287,  6932,  ...,  1549,  1480,   518],
        [    0,    12, 20722,  ...,   266,  1916,  5341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7452e-04, -1.9250e-04, -1.2556e-04,  ...,  1.9211e-05,
         -1.6460e-04, -3.7275e-04],
        [-9.2313e-06, -6.8396e-06,  5.3644e-06,  ..., -7.4878e-06,
         -5.2154e-06, -5.7444e-06],
        [ 8.5317e-05,  7.3354e-05, -3.2043e-05,  ...,  5.6459e-05,
          4.3904e-05,  1.0923e-05],
        [-1.4946e-05, -1.0684e-05,  9.0003e-06,  ..., -1.1951e-05,
         -8.1062e-06, -9.8422e-06],
        [-2.5094e-05, -1.9640e-05,  1.5199e-05,  ..., -2.0593e-05,
         -1.4827e-05, -1.4544e-05]], device='cuda:0')
Loss: 1.0306698083877563


Running epoch 0, step 893, batch 893
Sampled inputs[:2]: tensor([[    0,   895,  4110,  ...,  1578,  1245,    13],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0538e-04, -2.9341e-04, -1.4613e-04,  ...,  6.1649e-05,
         -1.0706e-04, -2.6382e-04],
        [-1.1168e-05, -8.2105e-06,  6.3851e-06,  ..., -9.0003e-06,
         -6.3628e-06, -7.1079e-06],
        [ 8.0727e-05,  7.0031e-05, -2.9540e-05,  ...,  5.2897e-05,
          4.1222e-05,  7.7189e-06],
        [-1.8075e-05, -1.2845e-05,  1.0699e-05,  ..., -1.4350e-05,
         -9.8944e-06, -1.2122e-05],
        [-3.0518e-05, -2.3723e-05,  1.8165e-05,  ..., -2.4885e-05,
         -1.8179e-05, -1.8135e-05]], device='cuda:0')
Loss: 1.0322237014770508


Running epoch 0, step 894, batch 894
Sampled inputs[:2]: tensor([[    0,  6640,    13,  ...,   292,   221,   273],
        [    0,  1527, 21622,  ..., 14406,    13,  6182]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8335e-04, -2.9058e-04, -2.0091e-04,  ...,  7.5785e-05,
         -1.4991e-04, -3.1113e-04],
        [-1.3024e-05, -9.5367e-06,  7.3686e-06,  ..., -1.0498e-05,
         -7.5027e-06, -8.5086e-06],
        [ 7.6465e-05,  6.6901e-05, -2.7171e-05,  ...,  4.9515e-05,
          3.8689e-05,  4.5896e-06],
        [-2.0981e-05, -1.4871e-05,  1.2286e-05,  ..., -1.6659e-05,
         -1.1608e-05, -1.4402e-05],
        [-3.5584e-05, -2.7537e-05,  2.0981e-05,  ..., -2.8968e-05,
         -2.1338e-05, -2.1636e-05]], device='cuda:0')
Loss: 0.99127197265625


Running epoch 0, step 895, batch 895
Sampled inputs[:2]: tensor([[    0,  2140,    12,  ...,   696,   688,  1998],
        [    0,   271,   266,  ..., 46357, 11101, 10621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4235e-04, -3.3650e-04, -3.0898e-04,  ...,  1.4035e-05,
         -1.4991e-04, -4.1627e-04],
        [-1.4879e-05, -1.0848e-05,  8.4192e-06,  ..., -1.1981e-05,
         -8.4788e-06, -9.6038e-06],
        [ 7.2263e-05,  6.3862e-05, -2.4682e-05,  ...,  4.6177e-05,
          3.6513e-05,  2.1607e-06],
        [-2.3916e-05, -1.6883e-05,  1.4000e-05,  ..., -1.8984e-05,
         -1.3098e-05, -1.6227e-05],
        [-4.0442e-05, -3.1188e-05,  2.3857e-05,  ..., -3.2872e-05,
         -2.4006e-05, -2.4274e-05]], device='cuda:0')
Loss: 1.0023304224014282
Graident accumulation at epoch 0, step 895, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.0735e-06,  2.0668e-05, -4.3539e-05,  ...,  2.3575e-04,
          1.7322e-04,  1.8988e-04],
        [-1.3853e-05, -1.0008e-05,  7.0810e-06,  ..., -1.2276e-05,
         -7.4065e-06, -9.1312e-06],
        [ 2.6410e-05,  2.5250e-05, -5.0963e-06,  ...,  2.1552e-05,
          2.0631e-05, -3.9286e-07],
        [-1.1726e-05, -4.2139e-06,  7.6133e-06,  ..., -1.0354e-05,
         -4.4980e-06, -1.0349e-05],
        [-4.0114e-05, -3.1166e-05,  2.2213e-05,  ..., -3.3909e-05,
         -2.2855e-05, -2.4065e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2084e-08, 5.1239e-08, 6.2706e-08,  ..., 2.1139e-08, 1.4368e-07,
         3.1865e-08],
        [7.2216e-11, 4.2360e-11, 1.0481e-11,  ..., 4.8669e-11, 1.0038e-11,
         1.8654e-11],
        [2.5320e-09, 1.3070e-09, 4.9482e-10,  ..., 1.7560e-09, 4.3198e-10,
         7.1688e-10],
        [6.3225e-10, 3.3844e-10, 1.0107e-10,  ..., 4.1598e-10, 1.2647e-10,
         1.7629e-10],
        [3.2901e-10, 1.7842e-10, 4.1952e-11,  ..., 2.3966e-10, 3.5573e-11,
         8.3268e-11]], device='cuda:0')
optimizer state dict: 112.0
lr: [1.2796724211323173e-05, 1.2796724211323173e-05]
scheduler_last_epoch: 112


Running epoch 0, step 896, batch 896
Sampled inputs[:2]: tensor([[    0,   634,   631,  ...,  3431,   287, 27947],
        [    0,  1529,   354,  ...,   709,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5986e-05,  7.8937e-05,  1.7130e-04,  ..., -1.0134e-04,
         -4.5959e-06,  3.6395e-05],
        [-1.8552e-06, -1.3039e-06,  1.0505e-06,  ..., -1.5497e-06,
         -1.0878e-06, -1.2890e-06],
        [-4.2617e-06, -3.0547e-06,  2.5332e-06,  ..., -3.5167e-06,
         -2.4289e-06, -2.8759e-06],
        [-2.9504e-06, -1.9968e-06,  1.7509e-06,  ..., -2.4438e-06,
         -1.6689e-06, -2.1458e-06],
        [-5.1260e-06, -3.7998e-06,  3.0547e-06,  ..., -4.2915e-06,
         -3.0696e-06, -3.2783e-06]], device='cuda:0')
Loss: 1.0067073106765747


Running epoch 0, step 897, batch 897
Sampled inputs[:2]: tensor([[   0,  292,   46,  ..., 1217,   17,  292],
        [   0, 4371, 4806,  ...,  685,  461,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1216e-05,  3.2104e-05,  8.5961e-05,  ..., -1.1493e-04,
          2.4373e-05,  3.6952e-05],
        [-3.8072e-06, -2.7865e-06,  2.1085e-06,  ..., -3.2112e-06,
         -2.2501e-06, -2.5257e-06],
        [-8.6129e-06, -6.4224e-06,  4.9919e-06,  ..., -7.1824e-06,
         -4.9770e-06, -5.6028e-06],
        [-6.0201e-06, -4.2766e-06,  3.4720e-06,  ..., -5.0366e-06,
         -3.4422e-06, -4.1872e-06],
        [-1.0222e-05, -7.8529e-06,  5.9456e-06,  ..., -8.6725e-06,
         -6.2138e-06, -6.3330e-06]], device='cuda:0')
Loss: 1.0224114656448364


Running epoch 0, step 898, batch 898
Sampled inputs[:2]: tensor([[   0, 1356,   18,  ...,   31,  333,  199],
        [   0,  394,  292,  ..., 1711,  365,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8639e-05, -6.2107e-05,  6.6279e-05,  ..., -1.8645e-04,
         -4.9062e-05, -5.6513e-05],
        [-5.7593e-06, -4.2021e-06,  3.2559e-06,  ..., -4.7907e-06,
         -3.2857e-06, -3.7253e-06],
        [-1.3053e-05, -9.7156e-06,  7.6890e-06,  ..., -1.0774e-05,
         -7.3016e-06, -8.3148e-06],
        [ 1.6124e-04,  1.6643e-04, -1.2802e-04,  ...,  1.0556e-04,
          1.2863e-04,  9.5911e-05],
        [-1.5527e-05, -1.1906e-05,  9.1791e-06,  ..., -1.3024e-05,
         -9.1642e-06, -9.4473e-06]], device='cuda:0')
Loss: 1.0234795808792114


Running epoch 0, step 899, batch 899
Sampled inputs[:2]: tensor([[    0, 18905,  2311,  ..., 10213,   908,   694],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9249e-05, -8.6219e-05, -1.0637e-04,  ..., -1.8645e-04,
         -1.1633e-04,  8.3534e-07],
        [-7.6815e-06, -5.6401e-06,  4.3660e-06,  ..., -6.3479e-06,
         -4.3809e-06, -4.9174e-06],
        [-1.7524e-05, -1.3113e-05,  1.0356e-05,  ..., -1.4380e-05,
         -9.8348e-06, -1.1072e-05],
        [ 1.5813e-04,  1.6417e-04, -1.2619e-04,  ...,  1.0308e-04,
          1.2693e-04,  9.3899e-05],
        [-2.0713e-05, -1.5959e-05,  1.2293e-05,  ..., -1.7285e-05,
         -1.2264e-05, -1.2487e-05]], device='cuda:0')
Loss: 1.0462509393692017


Running epoch 0, step 900, batch 900
Sampled inputs[:2]: tensor([[    0,   474,   221,  ..., 32291,   360,  2458],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3510e-06, -1.9370e-04, -2.0598e-04,  ..., -1.6034e-04,
         -1.7115e-04, -1.5316e-04],
        [-9.5665e-06, -6.9737e-06,  5.4911e-06,  ..., -7.9051e-06,
         -5.4464e-06, -6.1244e-06],
        [-2.1815e-05, -1.6198e-05,  1.3024e-05,  ..., -1.7896e-05,
         -1.2219e-05, -1.3784e-05],
        [ 1.5500e-04,  1.6202e-04, -1.2424e-04,  ...,  1.0053e-04,
          1.2522e-04,  9.1783e-05],
        [-2.5690e-05, -1.9684e-05,  1.5393e-05,  ..., -2.1458e-05,
         -1.5214e-05, -1.5467e-05]], device='cuda:0')
Loss: 1.010822057723999


Running epoch 0, step 901, batch 901
Sampled inputs[:2]: tensor([[    0, 17694,    12,  ..., 12452,   446,   475],
        [    0,  1726,  3775,  ...,   300,   266,  1686]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5047e-05, -2.0569e-04, -2.4907e-04,  ..., -2.3850e-04,
         -3.1125e-04, -3.2060e-04],
        [-1.1444e-05, -8.3148e-06,  6.5789e-06,  ..., -9.5293e-06,
         -6.5863e-06, -7.3165e-06],
        [-2.5928e-05, -1.9208e-05,  1.5527e-05,  ..., -2.1413e-05,
         -1.4633e-05, -1.6332e-05],
        [ 1.5202e-04,  1.5996e-04, -1.2246e-04,  ...,  9.7979e-05,
          1.2348e-04,  8.9816e-05],
        [-3.0696e-05, -2.3454e-05,  1.8433e-05,  ..., -2.5749e-05,
         -1.8284e-05, -1.8403e-05]], device='cuda:0')
Loss: 1.0321781635284424


Running epoch 0, step 902, batch 902
Sampled inputs[:2]: tensor([[   0, 6088, 1172,  ...,  546,  401,  925],
        [   0,  221,  422,  ..., 2693,  733,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7350e-05, -1.2078e-04, -1.4600e-04,  ..., -1.6212e-04,
         -3.1891e-04, -1.2639e-04],
        [-1.3426e-05, -9.6485e-06,  7.6219e-06,  ..., -1.1146e-05,
         -7.7710e-06, -8.6799e-06],
        [-3.0518e-05, -2.2352e-05,  1.8030e-05,  ..., -2.5123e-05,
         -1.7360e-05, -1.9446e-05],
        [ 1.4887e-04,  1.5791e-04, -1.2076e-04,  ...,  9.5446e-05,
          1.2165e-04,  8.7611e-05],
        [-3.6061e-05, -2.7239e-05,  2.1383e-05,  ..., -3.0130e-05,
         -2.1622e-05, -2.1920e-05]], device='cuda:0')
Loss: 1.0177314281463623


Running epoch 0, step 903, batch 903
Sampled inputs[:2]: tensor([[    0,   714,    14,  ...,  1501, 11397, 31940],
        [    0,   586,   940,  ...,  1471,  2612,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3471e-05, -1.0616e-04, -4.1591e-04,  ..., -1.7892e-04,
         -4.6947e-04, -2.9290e-04],
        [-1.5289e-05, -1.0900e-05,  8.7172e-06,  ..., -1.2629e-05,
         -8.7321e-06, -9.8497e-06],
        [-3.4750e-05, -2.5272e-05,  2.0638e-05,  ..., -2.8506e-05,
         -1.9535e-05, -2.2084e-05],
        [ 1.4586e-04,  1.5595e-04, -1.1889e-04,  ...,  9.3062e-05,
          1.2015e-04,  8.5599e-05],
        [-4.0919e-05, -3.0711e-05,  2.4363e-05,  ..., -3.4064e-05,
         -2.4289e-05, -2.4766e-05]], device='cuda:0')
Loss: 1.0011073350906372
Graident accumulation at epoch 0, step 903, batch 903
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0154, -0.0283,  ...,  0.0289, -0.0146, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.6132e-06,  7.9851e-06, -8.0776e-05,  ...,  1.9428e-04,
          1.0895e-04,  1.4161e-04],
        [-1.3997e-05, -1.0097e-05,  7.2446e-06,  ..., -1.2312e-05,
         -7.5390e-06, -9.2031e-06],
        [ 2.0294e-05,  2.0198e-05, -2.5228e-06,  ...,  1.6546e-05,
          1.6615e-05, -2.5619e-06],
        [ 4.0332e-06,  1.1803e-05, -5.0368e-06,  ..., -1.2653e-08,
          7.9673e-06, -7.5438e-07],
        [-4.0195e-05, -3.1120e-05,  2.2428e-05,  ..., -3.3924e-05,
         -2.2998e-05, -2.4135e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2033e-08, 5.1199e-08, 6.2816e-08,  ..., 2.1150e-08, 1.4376e-07,
         3.1919e-08],
        [7.2377e-11, 4.2437e-11, 1.0546e-11,  ..., 4.8780e-11, 1.0104e-11,
         1.8732e-11],
        [2.5307e-09, 1.3063e-09, 4.9475e-10,  ..., 1.7551e-09, 4.3193e-10,
         7.1665e-10],
        [6.5289e-10, 3.6242e-10, 1.1510e-10,  ..., 4.2422e-10, 1.4078e-10,
         1.8344e-10],
        [3.3036e-10, 1.7918e-10, 4.2504e-11,  ..., 2.4058e-10, 3.6127e-11,
         8.3798e-11]], device='cuda:0')
optimizer state dict: 113.0
lr: [1.2677829798345599e-05, 1.2677829798345599e-05]
scheduler_last_epoch: 113


Running epoch 0, step 904, batch 904
Sampled inputs[:2]: tensor([[    0, 13466,    14,  ..., 11227,  1966,  4039],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2943e-06,  7.7108e-05,  1.4226e-04,  ...,  6.8833e-05,
          2.8836e-04,  8.2469e-05],
        [-1.9819e-06, -1.2517e-06,  1.0580e-06,  ..., -1.6317e-06,
         -1.0356e-06, -1.4231e-06],
        [-4.3511e-06, -2.8163e-06,  2.4289e-06,  ..., -3.5316e-06,
         -2.2203e-06, -3.0547e-06],
        [-3.1441e-06, -1.9073e-06,  1.7285e-06,  ..., -2.5481e-06,
         -1.5646e-06, -2.3097e-06],
        [-5.0962e-06, -3.4273e-06,  2.8610e-06,  ..., -4.2021e-06,
         -2.7567e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.0157753229141235


Running epoch 0, step 905, batch 905
Sampled inputs[:2]: tensor([[    0,   266,  2555,  ...,   587,    14, 14947],
        [    0,  3761,    12,  ...,    14,    22,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5890e-05,  1.1706e-04,  1.6519e-04,  ...,  1.5004e-04,
          3.1091e-04,  7.7976e-05],
        [-3.9637e-06, -2.4885e-06,  2.0713e-06,  ..., -3.2261e-06,
         -2.1234e-06, -2.8461e-06],
        [-8.7023e-06, -5.6028e-06,  4.7833e-06,  ..., -6.9737e-06,
         -4.5300e-06, -6.0797e-06],
        [-6.2585e-06, -3.7700e-06,  3.3751e-06,  ..., -5.0068e-06,
         -3.1888e-06, -4.6045e-06],
        [-1.0252e-05, -6.8396e-06,  5.6624e-06,  ..., -8.3447e-06,
         -5.6326e-06, -6.8247e-06]], device='cuda:0')
Loss: 0.9984188675880432


Running epoch 0, step 906, batch 906
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,  4998,  4756,  5139],
        [    0,   271,  8130,  ...,   609, 28676,   965]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7169e-04, -1.2377e-04,  3.3725e-04,  ...,  2.6672e-04,
          8.7222e-04,  9.1226e-05],
        [-5.7817e-06, -3.8370e-06,  3.0994e-06,  ..., -4.8056e-06,
         -3.1367e-06, -4.0531e-06],
        [-1.2785e-05, -8.6874e-06,  7.2420e-06,  ..., -1.0461e-05,
         -6.7353e-06, -8.7470e-06],
        [-9.1940e-06, -5.8711e-06,  5.1633e-06,  ..., -7.5251e-06,
         -4.7535e-06, -6.6906e-06],
        [-1.4871e-05, -1.0431e-05,  8.4341e-06,  ..., -1.2338e-05,
         -8.2701e-06, -9.6262e-06]], device='cuda:0')
Loss: 0.9991636276245117


Running epoch 0, step 907, batch 907
Sampled inputs[:2]: tensor([[   0, 2220, 1110,  ...,  382,   18,   13],
        [   0,  472,  346,  ...,  266,  720,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6957e-04, -2.6934e-04,  8.3573e-05,  ...,  2.6425e-04,
          3.1824e-04, -9.8437e-06],
        [-7.7039e-06, -5.0664e-06,  4.2468e-06,  ..., -6.3926e-06,
         -4.0308e-06, -5.2601e-06],
        [-1.7017e-05, -1.1489e-05,  9.8795e-06,  ..., -1.3947e-05,
         -8.6874e-06, -1.1355e-05],
        [-1.2383e-05, -7.8380e-06,  7.1451e-06,  ..., -1.0133e-05,
         -6.1765e-06, -8.8066e-06],
        [-1.9521e-05, -1.3664e-05,  1.1325e-05,  ..., -1.6242e-05,
         -1.0595e-05, -1.2353e-05]], device='cuda:0')
Loss: 1.003740668296814


Running epoch 0, step 908, batch 908
Sampled inputs[:2]: tensor([[   0,  278, 7524,  ..., 1288,  669,  352],
        [   0,  271, 3403,  ..., 6168,  300, 2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8218e-04, -3.2582e-04,  1.0795e-04,  ...,  3.7554e-04,
          3.6406e-04, -4.9678e-05],
        [-9.5665e-06, -6.2734e-06,  5.3495e-06,  ..., -7.9945e-06,
         -5.0664e-06, -6.5118e-06],
        [-2.1130e-05, -1.4201e-05,  1.2413e-05,  ..., -1.7434e-05,
         -1.0908e-05, -1.4037e-05],
        [-1.5467e-05, -9.7454e-06,  9.0227e-06,  ..., -1.2740e-05,
         -7.8157e-06, -1.0952e-05],
        [-2.4289e-05, -1.6928e-05,  1.4290e-05,  ..., -2.0325e-05,
         -1.3307e-05, -1.5289e-05]], device='cuda:0')
Loss: 1.0197381973266602


Running epoch 0, step 909, batch 909
Sampled inputs[:2]: tensor([[    0, 49018,   292,  ...,  8774,   642,   365],
        [    0,  1596,  2700,  ...,   943,   266,  4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0943e-04, -3.6602e-04,  9.5133e-05,  ...,  2.9979e-04,
          3.4605e-04, -2.3782e-04],
        [-1.1623e-05, -7.6964e-06,  6.4746e-06,  ..., -9.6411e-06,
         -6.1989e-06, -7.8455e-06],
        [-2.5660e-05, -1.7419e-05,  1.5005e-05,  ..., -2.1055e-05,
         -1.3396e-05, -1.6972e-05],
        [-1.8686e-05, -1.1906e-05,  1.0841e-05,  ..., -1.5289e-05,
         -9.5218e-06, -1.3143e-05],
        [-2.9713e-05, -2.0891e-05,  1.7390e-05,  ..., -2.4706e-05,
         -1.6436e-05, -1.8626e-05]], device='cuda:0')
Loss: 1.0345515012741089


Running epoch 0, step 910, batch 910
Sampled inputs[:2]: tensor([[   0,  278, 1295,  ..., 4337,  271, 1268],
        [   0,    9,  287,  ...,  259, 8244, 1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8261e-05, -3.5453e-04,  1.4230e-05,  ...,  3.7911e-04,
          1.6838e-04, -2.0522e-04],
        [-1.3560e-05, -9.0003e-06,  7.6517e-06,  ..., -1.1213e-05,
         -7.1526e-06, -9.0152e-06],
        [-2.9951e-05, -2.0370e-05,  1.7703e-05,  ..., -2.4527e-05,
         -1.5482e-05, -1.9535e-05],
        [-2.1875e-05, -1.3992e-05,  1.2852e-05,  ..., -1.7852e-05,
         -1.1027e-05, -1.5154e-05],
        [-3.4511e-05, -2.4334e-05,  2.0415e-05,  ..., -2.8670e-05,
         -1.8939e-05, -2.1368e-05]], device='cuda:0')
Loss: 1.0084562301635742


Running epoch 0, step 911, batch 911
Sampled inputs[:2]: tensor([[    0,  1682,   271,  ...,   367,  3210,   271],
        [    0,  1665,  6306,  ...,   300, 10204,   582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4685e-04, -4.3824e-04, -4.0036e-05,  ...,  3.1753e-04,
          1.7493e-04, -2.4699e-04],
        [-1.5557e-05, -1.0394e-05,  8.8364e-06,  ..., -1.2860e-05,
         -8.2552e-06, -1.0230e-05],
        [-3.4362e-05, -2.3499e-05,  2.0385e-05,  ..., -2.8133e-05,
         -1.7866e-05, -2.2188e-05],
        [-2.5094e-05, -1.6153e-05,  1.4819e-05,  ..., -2.0459e-05,
         -1.2718e-05, -1.7211e-05],
        [-3.9697e-05, -2.8148e-05,  2.3574e-05,  ..., -3.2991e-05,
         -2.1905e-05, -2.4334e-05]], device='cuda:0')
Loss: 1.0426198244094849
Graident accumulation at epoch 0, step 911, batch 911
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0154, -0.0283,  ...,  0.0289, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3337e-05, -3.6637e-05, -7.6702e-05,  ...,  2.0661e-04,
          1.1555e-04,  1.0275e-04],
        [-1.4153e-05, -1.0127e-05,  7.4038e-06,  ..., -1.2366e-05,
         -7.6106e-06, -9.3057e-06],
        [ 1.4829e-05,  1.5828e-05, -2.3206e-07,  ...,  1.2078e-05,
          1.3167e-05, -4.5245e-06],
        [ 1.1205e-06,  9.0073e-06, -3.0512e-06,  ..., -2.0573e-06,
          5.8988e-06, -2.4000e-06],
        [-4.0145e-05, -3.0823e-05,  2.2542e-05,  ..., -3.3831e-05,
         -2.2889e-05, -2.4155e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2002e-08, 5.1340e-08, 6.2755e-08,  ..., 2.1229e-08, 1.4364e-07,
         3.1948e-08],
        [7.2547e-11, 4.2503e-11, 1.0614e-11,  ..., 4.8896e-11, 1.0162e-11,
         1.8818e-11],
        [2.5294e-09, 1.3056e-09, 4.9467e-10,  ..., 1.7541e-09, 4.3182e-10,
         7.1642e-10],
        [6.5287e-10, 3.6232e-10, 1.1520e-10,  ..., 4.2422e-10, 1.4080e-10,
         1.8355e-10],
        [3.3160e-10, 1.7980e-10, 4.3017e-11,  ..., 2.4143e-10, 3.6571e-11,
         8.4307e-11]], device='cuda:0')
optimizer state dict: 114.0
lr: [1.255852618959973e-05, 1.255852618959973e-05]
scheduler_last_epoch: 114


Running epoch 0, step 912, batch 912
Sampled inputs[:2]: tensor([[   0, 1911,  679,  ...,   19, 3737,  609],
        [   0, 1103,  271,  ...,  957,  756,  368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2862e-06, -1.7734e-06,  8.1891e-05,  ..., -9.8931e-05,
          4.4797e-05, -3.3063e-05],
        [-2.0415e-06, -1.4380e-06,  1.1846e-06,  ..., -1.7211e-06,
         -1.0133e-06, -1.1548e-06],
        [-4.4405e-06, -3.1888e-06,  2.6673e-06,  ..., -3.7402e-06,
         -2.1905e-06, -2.5034e-06],
        [-3.2932e-06, -2.2650e-06,  1.9521e-06,  ..., -2.7567e-06,
         -1.5721e-06, -1.9521e-06],
        [-5.0068e-06, -3.7253e-06,  3.0398e-06,  ..., -4.2915e-06,
         -2.6375e-06, -2.7120e-06]], device='cuda:0')
Loss: 1.02665114402771


Running epoch 0, step 913, batch 913
Sampled inputs[:2]: tensor([[    0, 24086,   266,  ..., 18814,    19,   292],
        [    0,   266,  4908,  ...,  1209,   328,  1603]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8209e-06, -1.4235e-05, -1.5195e-04,  ..., -2.5493e-04,
          1.8948e-05, -4.8224e-05],
        [-3.9488e-06, -2.5555e-06,  2.3395e-06,  ..., -3.2857e-06,
         -1.9260e-06, -2.3693e-06],
        [-8.6129e-06, -5.7071e-06,  5.2899e-06,  ..., -7.1377e-06,
         -4.1723e-06, -5.1260e-06],
        [-6.5118e-06, -4.0606e-06,  3.9637e-06,  ..., -5.3644e-06,
         -3.0398e-06, -4.0829e-06],
        [-9.6262e-06, -6.6459e-06,  5.9456e-06,  ..., -8.1062e-06,
         -4.9770e-06, -5.4538e-06]], device='cuda:0')
Loss: 0.9898453950881958


Running epoch 0, step 914, batch 914
Sampled inputs[:2]: tensor([[   0, 1106,  259,  ...,  271,  679,  382],
        [   0,   18,   66,  ...,   65,   17,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9493e-05, -2.6194e-05, -5.4913e-05,  ..., -3.2667e-04,
          5.8117e-05, -3.0487e-05],
        [-6.0499e-06, -4.1202e-06,  3.5390e-06,  ..., -5.0291e-06,
         -3.1032e-06, -3.5986e-06],
        [-1.3173e-05, -9.1642e-06,  7.9721e-06,  ..., -1.0908e-05,
         -6.6906e-06, -7.7635e-06],
        [ 3.6455e-04,  4.1005e-04, -2.9844e-04,  ...,  3.1101e-04,
          3.4946e-04,  2.5415e-04],
        [-1.4931e-05, -1.0788e-05,  9.0748e-06,  ..., -1.2547e-05,
         -8.0466e-06, -8.4192e-06]], device='cuda:0')
Loss: 1.0495703220367432


Running epoch 0, step 915, batch 915
Sampled inputs[:2]: tensor([[    0,  2165,  9311,  ..., 10570,   437,   266],
        [    0,   266,  6071,  ...,  1061,  1107,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1994e-04,  1.9873e-05, -7.9592e-05,  ..., -5.2067e-04,
         -4.3108e-05, -1.4202e-04],
        [-7.9572e-06, -5.3421e-06,  4.6343e-06,  ..., -6.6310e-06,
         -4.0308e-06, -4.7982e-06],
        [-1.7405e-05, -1.1921e-05,  1.0520e-05,  ..., -1.4424e-05,
         -8.7172e-06, -1.0356e-05],
        [ 3.6134e-04,  4.0810e-04, -2.9650e-04,  ...,  3.0835e-04,
          3.4796e-04,  2.5205e-04],
        [-1.9610e-05, -1.3962e-05,  1.1906e-05,  ..., -1.6481e-05,
         -1.0431e-05, -1.1131e-05]], device='cuda:0')
Loss: 0.9940098524093628


Running epoch 0, step 916, batch 916
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0, 13509,   472,  ...,  1805,    13, 27816]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7483e-06,  1.6523e-04, -2.6433e-04,  ..., -1.0188e-03,
         -5.9423e-04, -3.7894e-04],
        [-9.7901e-06, -6.4373e-06,  5.6773e-06,  ..., -8.2105e-06,
         -5.2229e-06, -6.2659e-06],
        [-2.1368e-05, -1.4380e-05,  1.2919e-05,  ..., -1.7762e-05,
         -1.1235e-05, -1.3366e-05],
        [ 3.5842e-04,  4.0649e-04, -2.9463e-04,  ...,  3.0580e-04,
          3.4604e-04,  2.4956e-04],
        [-2.4468e-05, -1.7092e-05,  1.4856e-05,  ..., -2.0593e-05,
         -1.3590e-05, -1.4588e-05]], device='cuda:0')
Loss: 0.9388662576675415


Running epoch 0, step 917, batch 917
Sampled inputs[:2]: tensor([[   0,   13,   41,  ...,    5,  271, 2936],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1068e-05,  1.4880e-04, -2.2560e-04,  ..., -1.0055e-03,
         -6.4436e-04, -4.2482e-04],
        [-1.1772e-05, -7.6964e-06,  6.8545e-06,  ..., -9.8497e-06,
         -6.2436e-06, -7.5027e-06],
        [-2.5630e-05, -1.7151e-05,  1.5542e-05,  ..., -2.1264e-05,
         -1.3396e-05, -1.5989e-05],
        [ 3.5519e-04,  4.0453e-04, -2.9266e-04,  ...,  3.0316e-04,
          3.4444e-04,  2.4746e-04],
        [-2.9296e-05, -2.0340e-05,  1.7822e-05,  ..., -2.4617e-05,
         -1.6168e-05, -1.7405e-05]], device='cuda:0')
Loss: 1.027549147605896


Running epoch 0, step 918, batch 918
Sampled inputs[:2]: tensor([[    0,   834,    89,  ...,  4030,    12,  6528],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4244e-04,  1.7123e-04, -4.3071e-04,  ..., -1.0377e-03,
         -1.1070e-03, -5.3898e-04],
        [-1.3679e-05, -8.9407e-06,  7.9870e-06,  ..., -1.1481e-05,
         -7.1563e-06, -8.6203e-06],
        [-2.9713e-05, -1.9878e-05,  1.8060e-05,  ..., -2.4736e-05,
         -1.5348e-05, -1.8358e-05],
        [ 3.5202e-04,  4.0254e-04, -2.9068e-04,  ...,  3.0045e-04,
          3.4297e-04,  2.4548e-04],
        [-3.3826e-05, -2.3469e-05,  2.0608e-05,  ..., -2.8521e-05,
         -1.8463e-05, -1.9893e-05]], device='cuda:0')
Loss: 1.000331997871399


Running epoch 0, step 919, batch 919
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,  2326,  3595,  6887],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2166e-04,  1.4808e-04, -3.8580e-04,  ..., -8.9301e-04,
         -1.1167e-03, -4.3321e-04],
        [-1.5579e-05, -1.0207e-05,  9.1121e-06,  ..., -1.3098e-05,
         -8.0653e-06, -9.7528e-06],
        [-3.3796e-05, -2.2620e-05,  2.0579e-05,  ..., -2.8163e-05,
         -1.7270e-05, -2.0757e-05],
        [ 3.4887e-04,  4.0053e-04, -2.8875e-04,  ...,  2.9781e-04,
          3.4154e-04,  2.4353e-04],
        [-3.8445e-05, -2.6688e-05,  2.3469e-05,  ..., -3.2455e-05,
         -2.0787e-05, -2.2441e-05]], device='cuda:0')
Loss: 1.028497338294983
Graident accumulation at epoch 0, step 919, batch 919
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0154, -0.0283,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.8376e-06, -1.8165e-05, -1.0761e-04,  ...,  9.6646e-05,
         -7.6733e-06,  4.9150e-05],
        [-1.4295e-05, -1.0135e-05,  7.5746e-06,  ..., -1.2440e-05,
         -7.6561e-06, -9.3504e-06],
        [ 9.9662e-06,  1.1983e-05,  1.8490e-06,  ...,  8.0540e-06,
          1.0123e-05, -6.1478e-06],
        [ 3.5896e-05,  4.8160e-05, -3.1622e-05,  ...,  2.7930e-05,
          3.9463e-05,  2.2193e-05],
        [-3.9975e-05, -3.0409e-05,  2.2635e-05,  ..., -3.3693e-05,
         -2.2679e-05, -2.3984e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1965e-08, 5.1311e-08, 6.2841e-08,  ..., 2.2005e-08, 1.4475e-07,
         3.2104e-08],
        [7.2717e-11, 4.2564e-11, 1.0686e-11,  ..., 4.9019e-11, 1.0217e-11,
         1.8894e-11],
        [2.5280e-09, 1.3048e-09, 4.9460e-10,  ..., 1.7531e-09, 4.3168e-10,
         7.1614e-10],
        [7.7393e-10, 5.2238e-10, 1.9847e-10,  ..., 5.1249e-10, 2.5731e-10,
         2.4267e-10],
        [3.3275e-10, 1.8033e-10, 4.3525e-11,  ..., 2.4224e-10, 3.6967e-11,
         8.4726e-11]], device='cuda:0')
optimizer state dict: 115.0
lr: [1.24388316157184e-05, 1.24388316157184e-05]
scheduler_last_epoch: 115


Running epoch 0, step 920, batch 920
Sampled inputs[:2]: tensor([[    0, 25778,  3804,  ...,  2354,    12,   554],
        [    0,  3408,   300,  ...,  3868,   300,  2932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9368e-05,  2.3293e-05,  4.7605e-07,  ..., -3.4300e-05,
          2.7772e-05, -6.2596e-05],
        [-2.0713e-06, -1.3188e-06,  1.1623e-06,  ..., -1.7434e-06,
         -9.9093e-07, -1.1548e-06],
        [-4.3213e-06, -2.8461e-06,  2.5332e-06,  ..., -3.6359e-06,
         -2.0713e-06, -2.3842e-06],
        [-3.3230e-06, -2.0564e-06,  1.9222e-06,  ..., -2.7716e-06,
         -1.5348e-06, -1.9222e-06],
        [-4.7684e-06, -3.2485e-06,  2.8014e-06,  ..., -4.0829e-06,
         -2.4140e-06, -2.5034e-06]], device='cuda:0')
Loss: 1.0377898216247559


Running epoch 0, step 921, batch 921
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,    13,  2920,   609],
        [    0, 30229,    12,  ...,   518,   717,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3587e-05,  7.0279e-05, -9.8162e-05,  ..., -5.7413e-05,
         -1.1387e-04, -1.1812e-04],
        [-3.9935e-06, -2.4959e-06,  2.3618e-06,  ..., -3.3826e-06,
         -1.8701e-06, -2.2724e-06],
        [-8.3148e-06, -5.3644e-06,  5.1409e-06,  ..., -6.9886e-06,
         -3.8594e-06, -4.6343e-06],
        [-6.4820e-06, -3.9041e-06,  4.0233e-06,  ..., -5.4538e-06,
         -2.9132e-06, -3.8594e-06],
        [-9.0301e-06, -6.0350e-06,  5.5581e-06,  ..., -7.7039e-06,
         -4.4554e-06, -4.7535e-06]], device='cuda:0')
Loss: 0.9968456029891968


Running epoch 0, step 922, batch 922
Sampled inputs[:2]: tensor([[   0,  259, 3022,  ...,  437, 5100, 1782],
        [   0, 9818,  347,  ...,  413, 7359,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7071e-05,  1.2897e-04, -1.7371e-05,  ..., -2.1387e-04,
          1.5218e-06, -1.7323e-04],
        [-6.0350e-06, -3.7923e-06,  3.4124e-06,  ..., -5.1558e-06,
         -2.9728e-06, -3.6880e-06],
        [-1.2606e-05, -8.1509e-06,  7.4804e-06,  ..., -1.0654e-05,
         -6.1095e-06, -7.5102e-06],
        [-9.6411e-06, -5.8264e-06,  5.7369e-06,  ..., -8.1658e-06,
         -4.5598e-06, -6.1393e-06],
        [-1.3918e-05, -9.3132e-06,  8.2403e-06,  ..., -1.1906e-05,
         -7.1377e-06, -7.8529e-06]], device='cuda:0')
Loss: 1.0165427923202515


Running epoch 0, step 923, batch 923
Sampled inputs[:2]: tensor([[    0,   259,  2561,  ...,    77,  4830,   292],
        [    0,  6904,  6069,  ..., 17196,   471,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2517e-04,  1.5477e-04,  4.8861e-05,  ..., -2.9401e-04,
          1.7311e-04, -2.9758e-04],
        [-8.0168e-06, -5.0589e-06,  4.5374e-06,  ..., -6.8545e-06,
         -3.9563e-06, -4.9099e-06],
        [-1.6749e-05, -1.0848e-05,  9.9391e-06,  ..., -1.4126e-05,
         -8.0913e-06, -9.9987e-06],
        [-1.2815e-05, -7.7635e-06,  7.5847e-06,  ..., -1.0848e-05,
         -6.0797e-06, -8.1807e-06],
        [-1.8448e-05, -1.2383e-05,  1.0952e-05,  ..., -1.5780e-05,
         -9.4473e-06, -1.0416e-05]], device='cuda:0')
Loss: 1.0132769346237183


Running epoch 0, step 924, batch 924
Sampled inputs[:2]: tensor([[    0,  7011,   650,  ..., 28839, 11610,  3222],
        [    0,    12,   689,  ...,  1110,  1712,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3612e-05,  7.3303e-05,  8.6486e-05,  ..., -2.8378e-04,
          1.7851e-04, -2.8826e-04],
        [-1.0028e-05, -6.4299e-06,  5.6848e-06,  ..., -8.5235e-06,
         -4.9174e-06, -6.0201e-06],
        [-2.1011e-05, -1.3828e-05,  1.2472e-05,  ..., -1.7658e-05,
         -1.0118e-05, -1.2338e-05],
        [-1.6078e-05, -9.9391e-06,  9.5069e-06,  ..., -1.3530e-05,
         -7.5847e-06, -1.0058e-05],
        [-2.3097e-05, -1.5736e-05,  1.3709e-05,  ..., -1.9684e-05,
         -1.1802e-05, -1.2845e-05]], device='cuda:0')
Loss: 1.0282500982284546


Running epoch 0, step 925, batch 925
Sampled inputs[:2]: tensor([[   0, 3036,  471,  ...,  287, 1906,   12],
        [   0, 1428,  266,  ..., 3169, 3058,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8436e-05,  6.6119e-05,  9.7719e-05,  ..., -4.1885e-04,
          8.6143e-05, -3.4105e-04],
        [-1.2025e-05, -7.8306e-06,  6.8322e-06,  ..., -1.0177e-05,
         -5.9456e-06, -7.1824e-06],
        [-2.5362e-05, -1.6943e-05,  1.5050e-05,  ..., -2.1249e-05,
         -1.2338e-05, -1.4886e-05],
        [-1.9267e-05, -1.2130e-05,  1.1399e-05,  ..., -1.6153e-05,
         -9.1717e-06, -1.2010e-05],
        [-2.7806e-05, -1.9193e-05,  1.6496e-05,  ..., -2.3648e-05,
         -1.4320e-05, -1.5467e-05]], device='cuda:0')
Loss: 1.012340784072876


Running epoch 0, step 926, batch 926
Sampled inputs[:2]: tensor([[    0,   266,  1586,  ...,  1888,  2117,   328],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9435e-05,  7.0958e-05, -1.3996e-04,  ..., -5.2180e-04,
          5.3624e-07, -3.6640e-04],
        [-1.4111e-05, -9.1046e-06,  8.0466e-06,  ..., -1.1854e-05,
         -6.9663e-06, -8.4192e-06],
        [-2.9802e-05, -1.9744e-05,  1.7747e-05,  ..., -2.4810e-05,
         -1.4484e-05, -1.7509e-05],
        [-2.2620e-05, -1.4111e-05,  1.3411e-05,  ..., -1.8820e-05,
         -1.0744e-05, -1.4082e-05],
        [-3.2753e-05, -2.2426e-05,  1.9491e-05,  ..., -2.7701e-05,
         -1.6898e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.0066388845443726


Running epoch 0, step 927, batch 927
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0, 38136,    12,  ...,   367, 12851,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9110e-04,  5.1896e-05, -1.7399e-04,  ..., -5.2071e-04,
          1.3260e-04, -2.8914e-04],
        [-1.6212e-05, -1.0490e-05,  9.2462e-06,  ..., -1.3553e-05,
         -7.9162e-06, -9.4920e-06],
        [-3.4213e-05, -2.2739e-05,  2.0370e-05,  ..., -2.8387e-05,
         -1.6481e-05, -1.9759e-05],
        [-2.5958e-05, -1.6257e-05,  1.5378e-05,  ..., -2.1502e-05,
         -1.2189e-05, -1.5862e-05],
        [-3.7581e-05, -2.5824e-05,  2.2367e-05,  ..., -3.1695e-05,
         -1.9237e-05, -2.0593e-05]], device='cuda:0')
Loss: 1.0219553709030151
Graident accumulation at epoch 0, step 927, batch 927
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0022,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0158,  0.0154, -0.0283,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1156e-05, -1.1159e-05, -1.1425e-04,  ...,  3.4910e-05,
          6.3543e-06,  1.5321e-05],
        [-1.4487e-05, -1.0170e-05,  7.7418e-06,  ..., -1.2551e-05,
         -7.6821e-06, -9.3646e-06],
        [ 5.5483e-06,  8.5109e-06,  3.7011e-06,  ...,  4.4100e-06,
          7.4625e-06, -7.5089e-06],
        [ 2.9710e-05,  4.1718e-05, -2.6922e-05,  ...,  2.2986e-05,
          3.4298e-05,  1.8387e-05],
        [-3.9736e-05, -2.9951e-05,  2.2608e-05,  ..., -3.3494e-05,
         -2.2335e-05, -2.3645e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1950e-08, 5.1262e-08, 6.2808e-08,  ..., 2.2255e-08, 1.4462e-07,
         3.2156e-08],
        [7.2907e-11, 4.2632e-11, 1.0761e-11,  ..., 4.9153e-11, 1.0269e-11,
         1.8965e-11],
        [2.5266e-09, 1.3040e-09, 4.9452e-10,  ..., 1.7522e-09, 4.3152e-10,
         7.1581e-10],
        [7.7383e-10, 5.2212e-10, 1.9851e-10,  ..., 5.1244e-10, 2.5720e-10,
         2.4268e-10],
        [3.3383e-10, 1.8082e-10, 4.3982e-11,  ..., 2.4300e-10, 3.7300e-11,
         8.5065e-11]], device='cuda:0')
optimizer state dict: 116.0
lr: [1.2318764367077325e-05, 1.2318764367077325e-05]
scheduler_last_epoch: 116


Running epoch 0, step 928, batch 928
Sampled inputs[:2]: tensor([[    0,  1712,    12,  ...,  1255,  1688,   266],
        [    0,    80, 10802,  ...,   287, 28533, 25359]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4980e-06, -5.0280e-05, -1.3588e-05,  ...,  0.0000e+00,
          1.5306e-05, -3.6734e-06],
        [-1.8626e-06, -1.1697e-06,  1.1176e-06,  ..., -1.6317e-06,
         -1.0207e-06, -1.1921e-06],
        [-4.0531e-06, -2.5630e-06,  2.5630e-06,  ..., -3.4571e-06,
         -2.1309e-06, -2.5034e-06],
        [-3.1739e-06, -1.8924e-06,  1.9819e-06,  ..., -2.7269e-06,
         -1.6540e-06, -2.0862e-06],
        [-4.3511e-06, -2.8312e-06,  2.7716e-06,  ..., -3.7700e-06,
         -2.3991e-06, -2.5332e-06]], device='cuda:0')
Loss: 1.0186656713485718


Running epoch 0, step 929, batch 929
Sampled inputs[:2]: tensor([[    0,  5160,   278,  ...,   496,    14, 46919],
        [    0,   285,    53,  ...,   259,  5012,  3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1669e-04, -6.5359e-05, -3.9110e-05,  ...,  1.1361e-05,
          1.8482e-06, -1.7536e-05],
        [-3.9637e-06, -2.6077e-06,  2.3767e-06,  ..., -3.4049e-06,
         -2.0489e-06, -2.3022e-06],
        [-8.4937e-06, -5.6326e-06,  5.2899e-06,  ..., -7.1824e-06,
         -4.2915e-06, -4.8280e-06],
        [-6.6161e-06, -4.1872e-06,  4.0829e-06,  ..., -5.6028e-06,
         -3.2783e-06, -3.9712e-06],
        [-9.1791e-06, -6.2883e-06,  5.7518e-06,  ..., -7.9125e-06,
         -4.8876e-06, -4.9621e-06]], device='cuda:0')
Loss: 1.0531854629516602


Running epoch 0, step 930, batch 930
Sampled inputs[:2]: tensor([[    0,  4852,   266,  ...,  2523,  2080,  2632],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5761e-05, -1.8658e-05, -1.8683e-04,  ..., -1.5142e-04,
         -2.2413e-04,  5.0761e-07],
        [-5.9307e-06, -3.8892e-06,  3.6061e-06,  ..., -5.1260e-06,
         -2.9877e-06, -3.3975e-06],
        [-1.2666e-05, -8.4043e-06,  8.0019e-06,  ..., -1.0803e-05,
         -6.2585e-06, -7.1228e-06],
        [-9.8646e-06, -6.2138e-06,  6.1840e-06,  ..., -8.4043e-06,
         -4.7684e-06, -5.8562e-06],
        [-1.3620e-05, -9.3430e-06,  8.6427e-06,  ..., -1.1817e-05,
         -7.1228e-06, -7.2718e-06]], device='cuda:0')
Loss: 1.0074214935302734


Running epoch 0, step 931, batch 931
Sampled inputs[:2]: tensor([[   0, 2736, 2523,  ..., 4086, 4798, 7701],
        [   0, 7219,  591,  ...,  278,  266, 5908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4304e-05, -2.2343e-05, -1.2449e-04,  ..., -1.4657e-04,
         -8.9886e-05, -4.1758e-05],
        [-7.9870e-06, -5.3197e-06,  4.8578e-06,  ..., -6.8992e-06,
         -4.0531e-06, -4.4852e-06],
        [-1.6987e-05, -1.1444e-05,  1.0714e-05,  ..., -1.4514e-05,
         -8.4639e-06, -9.4026e-06],
        [ 1.6853e-04,  1.1360e-04, -1.8953e-04,  ...,  2.8347e-04,
          1.5522e-04,  9.4023e-05],
        [-1.8328e-05, -1.2770e-05,  1.1593e-05,  ..., -1.5929e-05,
         -9.6858e-06, -9.6411e-06]], device='cuda:0')
Loss: 1.0617672204971313


Running epoch 0, step 932, batch 932
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2483e-04,  1.1516e-04, -1.5718e-04,  ..., -1.7095e-04,
         -3.0521e-04, -2.4913e-04],
        [-9.7975e-06, -6.4820e-06,  5.9828e-06,  ..., -8.5086e-06,
         -4.9919e-06, -5.5656e-06],
        [-2.0787e-05, -1.3947e-05,  1.3217e-05,  ..., -1.7807e-05,
         -1.0349e-05, -1.1578e-05],
        [ 1.6552e-04,  1.1180e-04, -1.8749e-04,  ...,  2.8082e-04,
          1.5372e-04,  9.2086e-05],
        [-2.2560e-05, -1.5661e-05,  1.4350e-05,  ..., -1.9625e-05,
         -1.1906e-05, -1.1876e-05]], device='cuda:0')
Loss: 0.9476807713508606


Running epoch 0, step 933, batch 933
Sampled inputs[:2]: tensor([[    0,  2923,   266,  ...,  7763,   360,  1255],
        [    0, 25241,   717,  ...,   413,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3100e-04,  2.0866e-04,  9.8199e-06,  ..., -1.7700e-04,
         -2.9106e-04, -1.9139e-04],
        [-1.1809e-05, -7.8976e-06,  7.0930e-06,  ..., -1.0237e-05,
         -6.0275e-06, -6.7055e-06],
        [-2.5079e-05, -1.7032e-05,  1.5676e-05,  ..., -2.1458e-05,
         -1.2524e-05, -1.3977e-05],
        [ 1.6216e-04,  1.0950e-04, -1.8561e-04,  ...,  2.7799e-04,
          1.5207e-04,  9.0119e-05],
        [-2.7239e-05, -1.9103e-05,  1.7047e-05,  ..., -2.3648e-05,
         -1.4409e-05, -1.4365e-05]], device='cuda:0')
Loss: 1.03965163230896


Running epoch 0, step 934, batch 934
Sampled inputs[:2]: tensor([[    0,   287,  5724,  ...,   298,   591,  2609],
        [    0,    12,  1790,  ..., 11026,   292,  2116]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9743e-04,  2.5617e-04,  1.9972e-04,  ..., -1.5242e-04,
         -2.0366e-04, -1.3178e-04],
        [-1.3851e-05, -9.3132e-06,  8.2925e-06,  ..., -1.1966e-05,
         -7.0184e-06, -7.7263e-06],
        [-2.9340e-05, -2.0042e-05,  1.8284e-05,  ..., -2.5064e-05,
         -1.4611e-05, -1.6108e-05],
        [ 1.5890e-04,  1.0730e-04, -1.8364e-04,  ...,  2.7524e-04,
          1.5053e-04,  8.8413e-05],
        [-3.1918e-05, -2.2531e-05,  1.9938e-05,  ..., -2.7701e-05,
         -1.6838e-05, -1.6585e-05]], device='cuda:0')
Loss: 1.028131365776062


Running epoch 0, step 935, batch 935
Sampled inputs[:2]: tensor([[    0,  2523, 10780,  ...,  1041,    26, 13745],
        [    0,  1979,   352,  ...,   292,  1591,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8561e-04,  2.6543e-04,  3.5791e-04,  ..., -1.7659e-04,
         -8.1958e-05, -1.3484e-04],
        [-1.5683e-05, -1.0438e-05,  9.4846e-06,  ..., -1.3679e-05,
         -8.1062e-06, -8.8960e-06],
        [-3.3215e-05, -2.2471e-05,  2.0906e-05,  ..., -2.8580e-05,
         -1.6831e-05, -1.8477e-05],
        [ 1.5592e-04,  1.0559e-04, -1.8155e-04,  ...,  2.7249e-04,
          1.4880e-04,  8.6386e-05],
        [-3.6240e-05, -2.5377e-05,  2.2858e-05,  ..., -3.1665e-05,
         -1.9431e-05, -1.9073e-05]], device='cuda:0')
Loss: 0.985875129699707
Graident accumulation at epoch 0, step 935, batch 935
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0026,  ..., -0.0022,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0157,  0.0154, -0.0283,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8520e-05,  1.6500e-05, -6.7034e-05,  ...,  1.3760e-05,
         -2.4769e-06,  3.0493e-07],
        [-1.4607e-05, -1.0197e-05,  7.9161e-06,  ..., -1.2664e-05,
         -7.7245e-06, -9.3177e-06],
        [ 1.6720e-06,  5.4127e-06,  5.4216e-06,  ...,  1.1109e-06,
          5.0332e-06, -8.6058e-06],
        [ 4.2331e-05,  4.8105e-05, -4.2385e-05,  ...,  4.7936e-05,
          4.5748e-05,  2.5187e-05],
        [-3.9386e-05, -2.9493e-05,  2.2633e-05,  ..., -3.3311e-05,
         -2.2044e-05, -2.3188e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1979e-08, 5.1281e-08, 6.2874e-08,  ..., 2.2264e-08, 1.4448e-07,
         3.2142e-08],
        [7.3080e-11, 4.2698e-11, 1.0840e-11,  ..., 4.9291e-11, 1.0325e-11,
         1.9026e-11],
        [2.5252e-09, 1.3032e-09, 4.9446e-10,  ..., 1.7513e-09, 4.3137e-10,
         7.1544e-10],
        [7.9737e-10, 5.3275e-10, 2.3127e-10,  ..., 5.8617e-10, 2.7909e-10,
         2.4990e-10],
        [3.3481e-10, 1.8128e-10, 4.4460e-11,  ..., 2.4376e-10, 3.7640e-11,
         8.5344e-11]], device='cuda:0')
optimizer state dict: 117.0
lr: [1.219834279100018e-05, 1.219834279100018e-05]
scheduler_last_epoch: 117


Running epoch 0, step 936, batch 936
Sampled inputs[:2]: tensor([[   0,  401,  266,  ...,  266, 2236, 1458],
        [   0,  749,    9,  ..., 2756,   14, 1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8767e-05, -4.0892e-05, -5.3254e-06,  ...,  5.9311e-05,
          5.6450e-05, -4.4755e-06],
        [-2.0266e-06, -1.4380e-06,  1.3411e-06,  ..., -1.7285e-06,
         -9.9093e-07, -9.9093e-07],
        [-4.2915e-06, -3.0994e-06,  2.9057e-06,  ..., -3.6508e-06,
         -2.0862e-06, -2.1160e-06],
        [-3.3379e-06, -2.3246e-06,  2.2501e-06,  ..., -2.8163e-06,
         -1.5646e-06, -1.7062e-06],
        [-4.5300e-06, -3.3677e-06,  3.0547e-06,  ..., -3.9041e-06,
         -2.3395e-06, -2.1160e-06]], device='cuda:0')
Loss: 1.0584218502044678


Running epoch 0, step 937, batch 937
Sampled inputs[:2]: tensor([[    0,   266,  4505,  ...,    12,   461,   806],
        [    0,   874,   445,  ...,    14, 16205,  8510]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5537e-06,  3.2907e-05, -9.3797e-05,  ..., -4.4083e-05,
          1.0729e-04,  1.3651e-04],
        [-4.0382e-06, -2.7344e-06,  2.5481e-06,  ..., -3.4943e-06,
         -2.0638e-06, -2.1830e-06],
        [-8.4937e-06, -5.9009e-06,  5.5581e-06,  ..., -7.3016e-06,
         -4.3064e-06, -4.5598e-06],
        [-6.6906e-06, -4.4405e-06,  4.3213e-06,  ..., -5.7220e-06,
         -3.3006e-06, -3.7625e-06],
        [-9.0897e-06, -6.5118e-06,  5.9456e-06,  ..., -7.9274e-06,
         -4.8876e-06, -4.6194e-06]], device='cuda:0')
Loss: 1.0090949535369873


Running epoch 0, step 938, batch 938
Sampled inputs[:2]: tensor([[    0,  6481,   298,  ...,  6145, 16858,   824],
        [    0,  7527,    15,  ...,  2677,   292, 30654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6750e-05,  7.3816e-05, -1.9917e-04,  ..., -1.5132e-04,
         -7.9376e-06,  1.2283e-04],
        [-5.9009e-06, -3.9190e-06,  3.7551e-06,  ..., -5.1036e-06,
         -2.9579e-06, -3.2783e-06],
        [-1.2457e-05, -8.4937e-06,  8.2552e-06,  ..., -1.0699e-05,
         -6.1840e-06, -6.8545e-06],
        [-9.8497e-06, -6.3479e-06,  6.5118e-06,  ..., -8.4192e-06,
         -4.7460e-06, -5.7295e-06],
        [-1.3232e-05, -9.3132e-06,  8.7470e-06,  ..., -1.1533e-05,
         -7.0035e-06, -6.8545e-06]], device='cuda:0')
Loss: 0.9715620279312134


Running epoch 0, step 939, batch 939
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   287,  2421,  ...,  6612,   352,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8147e-04,  2.1258e-04, -2.7067e-04,  ..., -2.9716e-04,
          1.0767e-04,  3.3402e-04],
        [-7.8827e-06, -5.1409e-06,  4.8280e-06,  ..., -6.8396e-06,
         -4.0159e-06, -4.6119e-06],
        [-1.6749e-05, -1.1161e-05,  1.0639e-05,  ..., -1.4380e-05,
         -8.4043e-06, -9.6709e-06],
        [ 1.2535e-04,  8.2483e-05, -8.6916e-05,  ...,  2.3840e-04,
          1.3188e-04,  8.4242e-05],
        [-1.7881e-05, -1.2323e-05,  1.1355e-05,  ..., -1.5587e-05,
         -9.5665e-06, -9.8050e-06]], device='cuda:0')
Loss: 1.0038764476776123


Running epoch 0, step 940, batch 940
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,    16,    52,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0423e-04,  4.4509e-04, -2.4469e-04,  ..., -3.8401e-04,
          3.3315e-05,  3.6084e-04],
        [-9.9093e-06, -6.4746e-06,  5.9456e-06,  ..., -8.6352e-06,
         -5.0515e-06, -5.8487e-06],
        [-2.0951e-05, -1.4037e-05,  1.3083e-05,  ..., -1.8090e-05,
         -1.0565e-05, -1.2219e-05],
        [ 1.2206e-04,  8.0352e-05, -8.5031e-05,  ...,  2.3551e-04,
          1.3023e-04,  8.2171e-05],
        [-2.2501e-05, -1.5572e-05,  1.4037e-05,  ..., -1.9729e-05,
         -1.2070e-05, -1.2472e-05]], device='cuda:0')
Loss: 1.0048246383666992


Running epoch 0, step 941, batch 941
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   292,    48,  1792],
        [    0, 10026,   992,  ...,   273,  2831,  8716]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1355e-04,  6.4386e-04, -1.0673e-04,  ..., -4.5293e-04,
          4.3783e-05,  7.1651e-04],
        [-1.1981e-05, -7.6815e-06,  7.0110e-06,  ..., -1.0423e-05,
         -6.1542e-06, -7.2643e-06],
        [-2.5272e-05, -1.6674e-05,  1.5438e-05,  ..., -2.1771e-05,
         -1.2845e-05, -1.5080e-05],
        [ 1.1875e-04,  7.8475e-05, -8.3272e-05,  ...,  2.3268e-04,
          1.2850e-04,  7.9846e-05],
        [-2.7180e-05, -1.8552e-05,  1.6645e-05,  ..., -2.3782e-05,
         -1.4663e-05, -1.5423e-05]], device='cuda:0')
Loss: 1.005078911781311


Running epoch 0, step 942, batch 942
Sampled inputs[:2]: tensor([[   0, 1477, 3205,  ..., 6441, 9363,  271],
        [   0,  792,   83,  ...,  300,  768,  932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7660e-04,  6.2696e-04, -2.1753e-04,  ..., -4.9212e-04,
         -1.6598e-04,  6.9577e-04],
        [-1.4007e-05, -9.0152e-06,  8.3148e-06,  ..., -1.2107e-05,
         -7.1377e-06, -8.2776e-06],
        [-2.9534e-05, -1.9550e-05,  1.8284e-05,  ..., -2.5332e-05,
         -1.4916e-05, -1.7226e-05],
        [ 1.1535e-04,  7.6299e-05, -8.1022e-05,  ...,  2.2989e-04,
          1.2692e-04,  7.8073e-05],
        [-3.1680e-05, -2.1711e-05,  1.9640e-05,  ..., -2.7597e-05,
         -1.7002e-05, -1.7554e-05]], device='cuda:0')
Loss: 1.023922324180603


Running epoch 0, step 943, batch 943
Sampled inputs[:2]: tensor([[   0,   14, 7870,  ...,  284,  830,  292],
        [   0,  443,   40,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6543e-04,  6.7616e-04, -3.3280e-04,  ..., -4.9151e-04,
         -5.8239e-04,  6.3826e-04],
        [-1.5825e-05, -1.0125e-05,  9.4101e-06,  ..., -1.3739e-05,
         -8.0764e-06, -9.4101e-06],
        [-3.3379e-05, -2.1935e-05,  2.0728e-05,  ..., -2.8685e-05,
         -1.6823e-05, -1.9506e-05],
        [ 1.1227e-04,  7.4563e-05, -7.9040e-05,  ...,  2.2718e-04,
          1.2540e-04,  7.6046e-05],
        [-3.5822e-05, -2.4393e-05,  2.2277e-05,  ..., -3.1248e-05,
         -1.9178e-05, -1.9848e-05]], device='cuda:0')
Loss: 0.9614737629890442
Graident accumulation at epoch 0, step 943, batch 943
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0143,  0.0026,  ..., -0.0022,  0.0233, -0.0193],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.8741e-06,  8.2465e-05, -9.3611e-05,  ..., -3.6767e-05,
         -6.0468e-05,  6.4101e-05],
        [-1.4728e-05, -1.0190e-05,  8.0655e-06,  ..., -1.2771e-05,
         -7.7597e-06, -9.3270e-06],
        [-1.8331e-06,  2.6780e-06,  6.9522e-06,  ..., -1.8686e-06,
          2.8475e-06, -9.6957e-06],
        [ 4.9325e-05,  5.0751e-05, -4.6050e-05,  ...,  6.5861e-05,
          5.3714e-05,  3.0273e-05],
        [-3.9030e-05, -2.8983e-05,  2.2598e-05,  ..., -3.3104e-05,
         -2.1758e-05, -2.2854e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1998e-08, 5.1687e-08, 6.2921e-08,  ..., 2.2483e-08, 1.4468e-07,
         3.2517e-08],
        [7.3257e-11, 4.2758e-11, 1.0918e-11,  ..., 4.9431e-11, 1.0380e-11,
         1.9095e-11],
        [2.5238e-09, 1.3024e-09, 4.9439e-10,  ..., 1.7503e-09, 4.3123e-10,
         7.1510e-10],
        [8.0917e-10, 5.3778e-10, 2.3729e-10,  ..., 6.3720e-10, 2.9454e-10,
         2.5544e-10],
        [3.3576e-10, 1.8169e-10, 4.4912e-11,  ..., 2.4450e-10, 3.7970e-11,
         8.5653e-11]], device='cuda:0')
optimizer state dict: 118.0
lr: [1.2077585288954968e-05, 1.2077585288954968e-05]
scheduler_last_epoch: 118


Running epoch 0, step 944, batch 944
Sampled inputs[:2]: tensor([[    0,     9,  9925,  ...,   527, 23286,  6062],
        [    0,  9029,   634,  ...,  1424,  6872,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7196e-06,  1.9762e-04, -1.0610e-04,  ..., -2.5012e-05,
         -1.2605e-05,  1.5947e-04],
        [-1.9819e-06, -1.1101e-06,  1.0878e-06,  ..., -1.7360e-06,
         -1.0654e-06, -1.4976e-06],
        [-4.1723e-06, -2.4289e-06,  2.4438e-06,  ..., -3.5614e-06,
         -2.1756e-06, -3.0249e-06],
        [-3.2783e-06, -1.7732e-06,  1.8775e-06,  ..., -2.8163e-06,
         -1.7062e-06, -2.5183e-06],
        [-4.6194e-06, -2.8014e-06,  2.7418e-06,  ..., -3.9935e-06,
         -2.5481e-06, -3.1739e-06]], device='cuda:0')
Loss: 0.9816341996192932


Running epoch 0, step 945, batch 945
Sampled inputs[:2]: tensor([[    0,  3502,   527,  ..., 21301, 22248,  1773],
        [    0,  2823,   287,  ...,  3504,     9, 13910]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3730e-05,  3.1053e-04,  3.1707e-05,  ..., -2.0354e-06,
          9.8182e-05,  2.3534e-04],
        [-3.9339e-06, -2.3842e-06,  2.1979e-06,  ..., -3.4943e-06,
         -2.1234e-06, -2.7493e-06],
        [-8.1658e-06, -5.1409e-06,  4.8727e-06,  ..., -7.1079e-06,
         -4.3213e-06, -5.4985e-06],
        [-6.4820e-06, -3.7998e-06,  3.7774e-06,  ..., -5.6773e-06,
         -3.4124e-06, -4.6343e-06],
        [-9.0897e-06, -5.9307e-06,  5.4836e-06,  ..., -7.9870e-06,
         -5.0515e-06, -5.7817e-06]], device='cuda:0')
Loss: 1.0119925737380981


Running epoch 0, step 946, batch 946
Sampled inputs[:2]: tensor([[   0, 3308,  259,  ...,   14, 6349, 1389],
        [   0, 2698,  221,  ..., 8352, 5680,  782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2738e-05,  4.5145e-04,  1.3867e-04,  ...,  2.1680e-05,
          3.3744e-05,  3.5365e-04],
        [-5.9605e-06, -3.7029e-06,  3.3528e-06,  ..., -5.2750e-06,
         -3.2634e-06, -4.0904e-06],
        [-1.2308e-05, -7.9572e-06,  7.3761e-06,  ..., -1.0744e-05,
         -6.6757e-06, -8.1956e-06],
        [-9.7752e-06, -5.8860e-06,  5.7444e-06,  ..., -8.5533e-06,
         -5.2378e-06, -6.8992e-06],
        [-1.3828e-05, -9.2685e-06,  8.3447e-06,  ..., -1.2189e-05,
         -7.8827e-06, -8.7023e-06]], device='cuda:0')
Loss: 1.0041649341583252


Running epoch 0, step 947, batch 947
Sampled inputs[:2]: tensor([[    0,   360,  3285,  ...,   423,  3579,   468],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1024e-05,  4.8891e-04,  9.8579e-05,  ..., -7.6596e-06,
          2.2194e-05,  3.9313e-04],
        [-7.8827e-06, -4.9695e-06,  4.5523e-06,  ..., -6.9514e-06,
         -4.2245e-06, -5.2080e-06],
        [ 3.3404e-05,  9.0189e-05, -6.9106e-06,  ...,  6.7783e-05,
          5.7740e-05, -3.8743e-06],
        [-1.3068e-05, -7.9572e-06,  7.8753e-06,  ..., -1.1370e-05,
         -6.7949e-06, -8.9109e-06],
        [-1.8120e-05, -1.2308e-05,  1.1176e-05,  ..., -1.5959e-05,
         -1.0133e-05, -1.0982e-05]], device='cuda:0')
Loss: 1.008623480796814


Running epoch 0, step 948, batch 948
Sampled inputs[:2]: tensor([[    0,    13, 23070,  ...,   266,   319,    13],
        [    0, 11853,  1611,  ...,  4413,  4240,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9086e-05,  6.0767e-04, -6.0446e-05,  ...,  8.8180e-05,
         -4.3177e-05,  4.6524e-04],
        [-9.7305e-06, -6.1393e-06,  5.6997e-06,  ..., -8.5756e-06,
         -5.1297e-06, -6.3106e-06],
        [ 2.9500e-05,  8.7656e-05, -4.3476e-06,  ...,  6.4401e-05,
          5.5870e-05, -6.1244e-06],
        [-1.6272e-05, -9.9093e-06,  9.9316e-06,  ..., -1.4141e-05,
         -8.3074e-06, -1.0893e-05],
        [-2.2292e-05, -1.5110e-05,  1.3918e-05,  ..., -1.9610e-05,
         -1.2249e-05, -1.3232e-05]], device='cuda:0')
Loss: 1.0054268836975098


Running epoch 0, step 949, batch 949
Sampled inputs[:2]: tensor([[    0, 22390,   292,  ...,  3552,   278,   317],
        [    0,   413,    29,  ...,   818,   278,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6027e-04,  5.5434e-04, -2.2493e-05,  ...,  1.4556e-04,
          1.0953e-04,  3.8080e-04],
        [-1.1846e-05, -7.5921e-06,  6.7875e-06,  ..., -1.0446e-05,
         -6.3740e-06, -7.7263e-06],
        [ 2.4970e-05,  8.4482e-05, -1.8889e-06,  ...,  6.0437e-05,
          5.3233e-05, -9.1344e-06],
        [-1.9595e-05, -1.2144e-05,  1.1705e-05,  ..., -1.7047e-05,
         -1.0200e-05, -1.3188e-05],
        [-2.7418e-05, -1.8820e-05,  1.6719e-05,  ..., -2.4170e-05,
         -1.5393e-05, -1.6496e-05]], device='cuda:0')
Loss: 1.0094213485717773


Running epoch 0, step 950, batch 950
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  1142,    87,  ...,  2273,   287,   829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6310e-04,  6.7737e-04,  1.7132e-04,  ...,  2.8680e-04,
          9.0746e-05,  4.8053e-04],
        [-1.3739e-05, -8.8885e-06,  7.8902e-06,  ..., -1.2174e-05,
         -7.4692e-06, -8.9183e-06],
        [ 2.0947e-05,  8.1636e-05,  5.8465e-07,  ...,  5.6786e-05,
          5.0893e-05, -1.1638e-05],
        [-2.2814e-05, -1.4260e-05,  1.3657e-05,  ..., -1.9968e-05,
         -1.2025e-05, -1.5318e-05],
        [-3.1829e-05, -2.2069e-05,  1.9446e-05,  ..., -2.8253e-05,
         -1.8090e-05, -1.9088e-05]], device='cuda:0')
Loss: 1.0095727443695068


Running epoch 0, step 951, batch 951
Sampled inputs[:2]: tensor([[    0,   292, 17190,  ...,  3078,     9,   287],
        [    0,   266,  3536,  ...,   266,  1883,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4261e-04,  7.3465e-04,  1.6965e-04,  ...,  2.6893e-04,
          3.3807e-05,  4.6374e-04],
        [-1.5736e-05, -1.0103e-05,  9.0376e-06,  ..., -1.3918e-05,
         -8.5197e-06, -1.0259e-05],
        [ 1.6745e-05,  7.8998e-05,  3.1178e-06,  ...,  5.3135e-05,
          4.8703e-05, -1.4424e-05],
        [-2.6122e-05, -1.6212e-05,  1.5624e-05,  ..., -2.2829e-05,
         -1.3702e-05, -1.7613e-05],
        [-3.6448e-05, -2.5108e-05,  2.2233e-05,  ..., -3.2365e-05,
         -2.0668e-05, -2.2024e-05]], device='cuda:0')
Loss: 1.018183708190918
Graident accumulation at epoch 0, step 951, batch 951
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0143,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.5375e-05,  1.4768e-04, -6.7284e-05,  ..., -6.1975e-06,
         -5.1040e-05,  1.0406e-04],
        [-1.4829e-05, -1.0181e-05,  8.1627e-06,  ..., -1.2886e-05,
         -7.8357e-06, -9.4202e-06],
        [ 2.4734e-08,  1.0310e-05,  6.5688e-06,  ...,  3.6318e-06,
          7.4330e-06, -1.0169e-05],
        [ 4.1780e-05,  4.4055e-05, -3.9883e-05,  ...,  5.6992e-05,
          4.6972e-05,  2.5485e-05],
        [-3.8771e-05, -2.8596e-05,  2.2561e-05,  ..., -3.3030e-05,
         -2.1649e-05, -2.2771e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2063e-08, 5.2175e-08, 6.2887e-08,  ..., 2.2533e-08, 1.4453e-07,
         3.2699e-08],
        [7.3432e-11, 4.2817e-11, 1.0989e-11,  ..., 4.9575e-11, 1.0442e-11,
         1.9181e-11],
        [2.5215e-09, 1.3073e-09, 4.9391e-10,  ..., 1.7514e-09, 4.3317e-10,
         7.1460e-10],
        [8.0905e-10, 5.3750e-10, 2.3729e-10,  ..., 6.3708e-10, 2.9443e-10,
         2.5549e-10],
        [3.3675e-10, 1.8214e-10, 4.5361e-11,  ..., 2.4530e-10, 3.8359e-11,
         8.6052e-11]], device='cuda:0')
optimizer state dict: 119.0
lr: [1.1956510313742102e-05, 1.1956510313742102e-05]
scheduler_last_epoch: 119


Running epoch 0, step 952, batch 952
Sampled inputs[:2]: tensor([[    0,   266,  2086,  ...,  4283,   720,    14],
        [    0, 15931,    14,  ...,  2645,   699,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2272e-05, -1.3784e-04, -1.8923e-04,  ...,  2.1525e-06,
         -1.6925e-05, -2.0920e-04],
        [-1.8552e-06, -1.3113e-06,  1.2666e-06,  ..., -1.6540e-06,
         -1.0431e-06, -1.1325e-06],
        [-3.9935e-06, -2.8908e-06,  2.8312e-06,  ..., -3.5763e-06,
         -2.2352e-06, -2.4587e-06],
        [-3.2037e-06, -2.1607e-06,  2.2650e-06,  ..., -2.8163e-06,
         -1.7360e-06, -2.0713e-06],
        [-4.3213e-06, -3.2932e-06,  3.0547e-06,  ..., -3.9637e-06,
         -2.6077e-06, -2.4885e-06]], device='cuda:0')
Loss: 1.022263526916504


Running epoch 0, step 953, batch 953
Sampled inputs[:2]: tensor([[   0, 2974,  278,  ...,  365, 8758,  271],
        [   0,   12,  496,  ...,  437,  266, 3767]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6638e-05, -1.9376e-04, -1.6498e-04,  ...,  7.8755e-05,
         -9.7387e-05, -2.1100e-04],
        [-3.7253e-06, -2.5705e-06,  2.4736e-06,  ..., -3.2485e-06,
         -1.9632e-06, -2.2352e-06],
        [-8.0466e-06, -5.7071e-06,  5.5879e-06,  ..., -7.0333e-06,
         -4.2319e-06, -4.8429e-06],
        [-6.4522e-06, -4.2766e-06,  4.4554e-06,  ..., -5.5581e-06,
         -3.2783e-06, -4.0829e-06],
        [-8.6725e-06, -6.4671e-06,  6.0052e-06,  ..., -7.7784e-06,
         -4.9323e-06, -4.8876e-06]], device='cuda:0')
Loss: 1.0075123310089111


Running epoch 0, step 954, batch 954
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,   446, 28686,  ...,    35,  2706, 19712]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6853e-04, -2.2681e-04, -1.9092e-04,  ...,  1.2878e-04,
          1.9676e-04, -2.2664e-04],
        [-5.6103e-06, -3.8743e-06,  3.5912e-06,  ..., -4.9397e-06,
         -3.0212e-06, -3.4198e-06],
        [-1.2040e-05, -8.5533e-06,  8.1211e-06,  ..., -1.0595e-05,
         -6.4224e-06, -7.3165e-06],
        [-9.5516e-06, -6.3479e-06,  6.3777e-06,  ..., -8.3297e-06,
         -4.9695e-06, -6.1393e-06],
        [-1.3113e-05, -9.7901e-06,  8.8513e-06,  ..., -1.1832e-05,
         -7.5549e-06, -7.4655e-06]], device='cuda:0')
Loss: 1.0377110242843628


Running epoch 0, step 955, batch 955
Sampled inputs[:2]: tensor([[    0,  6673,   298,  ...,  4391,   292,   221],
        [    0,  6976, 16084,  ...,    19,  9955,  3854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1138e-05, -2.7208e-04, -1.7222e-04,  ..., -2.7569e-06,
          4.5670e-05, -5.5995e-04],
        [-7.4953e-06, -5.1036e-06,  4.8429e-06,  ..., -6.5938e-06,
         -4.0270e-06, -4.6417e-06],
        [-1.6123e-05, -1.1310e-05,  1.0997e-05,  ..., -1.4156e-05,
         -8.5980e-06, -9.9540e-06],
        [-1.2800e-05, -8.4043e-06,  8.6576e-06,  ..., -1.1146e-05,
         -6.6534e-06, -8.3447e-06],
        [-1.7524e-05, -1.2904e-05,  1.1966e-05,  ..., -1.5765e-05,
         -1.0088e-05, -1.0148e-05]], device='cuda:0')
Loss: 1.0042282342910767


Running epoch 0, step 956, batch 956
Sampled inputs[:2]: tensor([[    0,   631,   516,  ..., 13374,   898,   266],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5225e-04, -3.1045e-04, -1.3826e-04,  ...,  7.5332e-06,
          1.4457e-04, -5.4657e-04],
        [-9.4324e-06, -6.4149e-06,  6.0573e-06,  ..., -8.2478e-06,
         -4.9956e-06, -5.7817e-06],
        [ 5.3358e-05,  3.8319e-05, -3.8469e-05,  ...,  5.2141e-05,
          7.0334e-05,  2.1040e-05],
        [-1.6093e-05, -1.0595e-05,  1.0803e-05,  ..., -1.3947e-05,
         -8.2552e-06, -1.0356e-05],
        [-2.1875e-05, -1.6093e-05,  1.4842e-05,  ..., -1.9565e-05,
         -1.2442e-05, -1.2547e-05]], device='cuda:0')
Loss: 1.0280159711837769


Running epoch 0, step 957, batch 957
Sampled inputs[:2]: tensor([[    0, 25409,   287,  ...,  1005,   344,  3493],
        [    0,  5685,   565,  ..., 23968,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5625e-05, -3.4934e-04, -7.4861e-05,  ...,  1.5198e-04,
          2.9501e-04, -5.4155e-04],
        [-1.1355e-05, -7.6741e-06,  7.1973e-06,  ..., -9.9763e-06,
         -6.0685e-06, -7.0333e-06],
        [ 4.9365e-05,  3.5607e-05, -3.5951e-05,  ...,  4.8565e-05,
          6.8113e-05,  1.8477e-05],
        [-1.9327e-05, -1.2666e-05,  1.2800e-05,  ..., -1.6838e-05,
         -1.0028e-05, -1.2547e-05],
        [-2.6226e-05, -1.9163e-05,  1.7598e-05,  ..., -2.3499e-05,
         -1.5005e-05, -1.5169e-05]], device='cuda:0')
Loss: 1.0170741081237793


Running epoch 0, step 958, batch 958
Sampled inputs[:2]: tensor([[   0, 9017,  600,  ..., 6133, 1098,  352],
        [   0,  957, 1231,  ...,  800,  342, 1398]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2979e-04, -2.7019e-04, -4.5358e-05,  ...,  2.1407e-04,
          3.5713e-04, -4.8227e-04],
        [-1.3292e-05, -8.8885e-06,  8.3223e-06,  ..., -1.1697e-05,
         -7.0967e-06, -8.4117e-06],
        [ 4.5312e-05,  3.2954e-05, -3.3433e-05,  ...,  4.4989e-05,
          6.5983e-05,  1.5646e-05],
        [-2.2590e-05, -1.4663e-05,  1.4782e-05,  ..., -1.9699e-05,
         -1.1697e-05, -1.4931e-05],
        [-3.0726e-05, -2.2218e-05,  2.0400e-05,  ..., -2.7552e-05,
         -1.7539e-05, -1.8150e-05]], device='cuda:0')
Loss: 1.016876220703125


Running epoch 0, step 959, batch 959
Sampled inputs[:2]: tensor([[    0,    14,   560,  ...,  1248,  1398,  1268],
        [    0,    14,  5551,  ...,   668, 11988,  2538]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5118e-04, -3.6633e-04, -2.0900e-04,  ...,  2.8268e-04,
          4.4483e-04, -7.3233e-04],
        [-1.5154e-05, -1.0163e-05,  9.4399e-06,  ..., -1.3314e-05,
         -8.0951e-06, -9.6187e-06],
        [ 4.1229e-05,  3.0093e-05, -3.0825e-05,  ...,  4.1487e-05,
          6.3822e-05,  1.3053e-05],
        [-2.5779e-05, -1.6764e-05,  1.6809e-05,  ..., -2.2426e-05,
         -1.3359e-05, -1.7077e-05],
        [-3.5226e-05, -2.5496e-05,  2.3291e-05,  ..., -3.1486e-05,
         -2.0072e-05, -2.0817e-05]], device='cuda:0')
Loss: 1.008270502090454
Graident accumulation at epoch 0, step 959, batch 959
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0143,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7955e-05,  9.6283e-05, -8.1456e-05,  ...,  2.2690e-05,
         -1.4539e-06,  2.0425e-05],
        [-1.4862e-05, -1.0179e-05,  8.2904e-06,  ..., -1.2929e-05,
         -7.8617e-06, -9.4401e-06],
        [ 4.1452e-06,  1.2288e-05,  2.8294e-06,  ...,  7.4173e-06,
          1.3072e-05, -7.8464e-06],
        [ 3.5024e-05,  3.7973e-05, -3.4214e-05,  ...,  4.9050e-05,
          4.0939e-05,  2.1228e-05],
        [-3.8417e-05, -2.8286e-05,  2.2634e-05,  ..., -3.2876e-05,
         -2.1491e-05, -2.2575e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2034e-08, 5.2257e-08, 6.2868e-08,  ..., 2.2590e-08, 1.4459e-07,
         3.3203e-08],
        [7.3588e-11, 4.2878e-11, 1.1067e-11,  ..., 4.9703e-11, 1.0497e-11,
         1.9255e-11],
        [2.5207e-09, 1.3069e-09, 4.9437e-10,  ..., 1.7514e-09, 4.3681e-10,
         7.1405e-10],
        [8.0890e-10, 5.3725e-10, 2.3734e-10,  ..., 6.3695e-10, 2.9431e-10,
         2.5553e-10],
        [3.3765e-10, 1.8261e-10, 4.5858e-11,  ..., 2.4605e-10, 3.8724e-11,
         8.6399e-11]], device='cuda:0')
optimizer state dict: 120.0
lr: [1.1835136366674677e-05, 1.1835136366674677e-05]
scheduler_last_epoch: 120
Epoch 0 | Batch 959/1048 | Training PPL: 3184.861292185294 | time 103.50848603248596
Saving checkpoint at epoch 0, step 959, batch 959
Epoch 0 | Validation PPL: 7.4012270457077305 | Learning rate: 1.1835136366674677e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_959, AFTER epoch 0, step 959


Running epoch 0, step 960, batch 960
Sampled inputs[:2]: tensor([[    0,  4337,  2057,  ...,  3020,  1722,   369],
        [    0,   342,   408,  ...,  5162, 25842,  4855]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7665e-05,  1.7407e-05, -1.7584e-04,  ..., -1.3215e-05,
         -1.0225e-04, -2.4757e-04],
        [-1.7360e-06, -1.1176e-06,  1.1325e-06,  ..., -1.6093e-06,
         -1.0282e-06, -1.1697e-06],
        [-3.6806e-06, -2.4587e-06,  2.5481e-06,  ..., -3.3677e-06,
         -2.1458e-06, -2.4140e-06],
        [-2.9802e-06, -1.8328e-06,  2.0862e-06,  ..., -2.7418e-06,
         -1.7285e-06, -2.1160e-06],
        [-4.1425e-06, -2.9206e-06,  2.8610e-06,  ..., -3.8743e-06,
         -2.5779e-06, -2.5481e-06]], device='cuda:0')
Loss: 0.9900699853897095


Running epoch 0, step 961, batch 961
Sampled inputs[:2]: tensor([[   0,  266, 6449,  ...,  474,  221,  474],
        [   0,  369,  726,  ...,  292,  221,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1514e-04, -1.5738e-04, -2.7609e-04,  ...,  4.9781e-05,
          4.9935e-05, -2.6265e-04],
        [-3.5316e-06, -2.1756e-06,  2.2575e-06,  ..., -3.1888e-06,
         -1.9781e-06, -2.4065e-06],
        [-7.5549e-06, -4.8131e-06,  5.1111e-06,  ..., -6.7204e-06,
         -4.1574e-06, -5.0217e-06],
        [-6.1244e-06, -3.6061e-06,  4.1276e-06,  ..., -5.4687e-06,
         -3.3379e-06, -4.3660e-06],
        [-8.3148e-06, -5.6177e-06,  5.6177e-06,  ..., -7.5698e-06,
         -4.9025e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.0068999528884888


Running epoch 0, step 962, batch 962
Sampled inputs[:2]: tensor([[    0,   413,    28,  ...,   328, 37605,  6499],
        [    0,  1575,  4384,  ...,   328,   722,  6124]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1972e-04, -1.9788e-04, -1.9120e-04,  ...,  9.2923e-05,
          3.6434e-04, -3.4521e-04],
        [-5.4687e-06, -3.4347e-06,  3.2634e-06,  ..., -4.8801e-06,
         -3.1553e-06, -3.8520e-06],
        [-1.1787e-05, -7.7188e-06,  7.4953e-06,  ..., -1.0416e-05,
         -6.7502e-06, -8.1509e-06],
        [-9.2536e-06, -5.6177e-06,  5.8487e-06,  ..., -8.1956e-06,
         -5.2154e-06, -6.7949e-06],
        [-1.3143e-05, -9.0748e-06,  8.3447e-06,  ..., -1.1891e-05,
         -8.0466e-06, -8.5980e-06]], device='cuda:0')
Loss: 1.0061708688735962


Running epoch 0, step 963, batch 963
Sampled inputs[:2]: tensor([[    0,  2715,  1478,  ...,  1171,  4697, 41847],
        [    0,    13,  1311,  ...,   271,   795,   957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4187e-04, -2.9915e-04, -1.9466e-04,  ...,  1.1781e-04,
          6.7057e-04, -3.3496e-04],
        [-7.2345e-06, -4.6864e-06,  4.3660e-06,  ..., -6.5193e-06,
         -4.2059e-06, -5.0962e-06],
        [-1.5572e-05, -1.0505e-05,  1.0014e-05,  ..., -1.3903e-05,
         -8.9854e-06, -1.0788e-05],
        [-1.2204e-05, -7.6443e-06,  7.8008e-06,  ..., -1.0923e-05,
         -6.9588e-06, -9.0003e-06],
        [-1.7345e-05, -1.2323e-05,  1.1131e-05,  ..., -1.5855e-05,
         -1.0714e-05, -1.1325e-05]], device='cuda:0')
Loss: 0.9934422373771667


Running epoch 0, step 964, batch 964
Sampled inputs[:2]: tensor([[    0,    20, 13016,  ...,    14,  2743,   516],
        [    0, 10446,    14,  ...,   266,  1164,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3714e-04, -1.9156e-04, -1.9035e-04,  ...,  1.7723e-04,
          7.4115e-04, -2.1080e-04],
        [-9.0748e-06, -5.8487e-06,  5.4389e-06,  ..., -8.2031e-06,
         -5.2266e-06, -6.3628e-06],
        [-1.9476e-05, -1.3039e-05,  1.2457e-05,  ..., -1.7419e-05,
         -1.1131e-05, -1.3381e-05],
        [-1.5348e-05, -9.5665e-06,  9.7230e-06,  ..., -1.3769e-05,
         -8.6725e-06, -1.1235e-05],
        [-2.1756e-05, -1.5303e-05,  1.3903e-05,  ..., -1.9878e-05,
         -1.3277e-05, -1.4096e-05]], device='cuda:0')
Loss: 1.021559238433838


Running epoch 0, step 965, batch 965
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9232e-04, -2.6979e-04, -2.1756e-04,  ...,  2.1933e-04,
          7.0932e-04, -2.4034e-04],
        [-1.0915e-05, -7.0706e-06,  6.6981e-06,  ..., -9.7677e-06,
         -6.1318e-06, -7.4357e-06],
        [-2.3380e-05, -1.5751e-05,  1.5274e-05,  ..., -2.0772e-05,
         -1.3068e-05, -1.5661e-05],
        [-1.8522e-05, -1.1653e-05,  1.2003e-05,  ..., -1.6466e-05,
         -1.0200e-05, -1.3188e-05],
        [-2.5868e-05, -1.8328e-05,  1.6853e-05,  ..., -2.3499e-05,
         -1.5512e-05, -1.6347e-05]], device='cuda:0')
Loss: 0.9985932111740112


Running epoch 0, step 966, batch 966
Sampled inputs[:2]: tensor([[   0, 1871,  401,  ...,   14, 4797,   12],
        [   0,  409,  729,  ...,  391,  266,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3797e-04, -3.9657e-04, -2.7833e-04,  ...,  1.0001e-04,
          6.5655e-04, -5.3780e-04],
        [-1.2673e-05, -8.2403e-06,  7.9051e-06,  ..., -1.1325e-05,
         -7.0818e-06, -8.5980e-06],
        [-2.7254e-05, -1.8403e-05,  1.8060e-05,  ..., -2.4170e-05,
         -1.5125e-05, -1.8194e-05],
        [-2.1622e-05, -1.3635e-05,  1.4238e-05,  ..., -1.9178e-05,
         -1.1809e-05, -1.5348e-05],
        [-3.0100e-05, -2.1383e-05,  1.9878e-05,  ..., -2.7314e-05,
         -1.7956e-05, -1.8939e-05]], device='cuda:0')
Loss: 0.9920105338096619


Running epoch 0, step 967, batch 967
Sampled inputs[:2]: tensor([[    0, 15666,   609,  ...,   527,  4486,     9],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5964e-04, -4.7491e-04, -4.7368e-04,  ...,  2.6939e-04,
          9.1378e-04, -5.5015e-04],
        [-1.4380e-05, -9.2387e-06,  8.9109e-06,  ..., -1.2904e-05,
         -8.0243e-06, -9.9391e-06],
        [-3.0920e-05, -2.0608e-05,  2.0415e-05,  ..., -2.7463e-05,
         -1.7077e-05, -2.0921e-05],
        [-2.4602e-05, -1.5281e-05,  1.6116e-05,  ..., -2.1875e-05,
         -1.3396e-05, -1.7747e-05],
        [-3.4124e-05, -2.3946e-05,  2.2486e-05,  ..., -3.0980e-05,
         -2.0266e-05, -2.1711e-05]], device='cuda:0')
Loss: 0.9857997298240662
Graident accumulation at epoch 0, step 967, batch 967
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0143,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.0123e-05,  3.9164e-05, -1.2068e-04,  ...,  4.7360e-05,
          9.0069e-05, -3.6632e-05],
        [-1.4814e-05, -1.0085e-05,  8.3524e-06,  ..., -1.2926e-05,
         -7.8779e-06, -9.4900e-06],
        [ 6.3865e-07,  8.9987e-06,  4.5879e-06,  ...,  3.9293e-06,
          1.0057e-05, -9.1539e-06],
        [ 2.9062e-05,  3.2647e-05, -2.9181e-05,  ...,  4.1958e-05,
          3.5506e-05,  1.7331e-05],
        [-3.7988e-05, -2.7852e-05,  2.2619e-05,  ..., -3.2686e-05,
         -2.1368e-05, -2.2489e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2193e-08, 5.2430e-08, 6.3030e-08,  ..., 2.2640e-08, 1.4528e-07,
         3.3473e-08],
        [7.3721e-11, 4.2920e-11, 1.1135e-11,  ..., 4.9820e-11, 1.0551e-11,
         1.9334e-11],
        [2.5191e-09, 1.3060e-09, 4.9429e-10,  ..., 1.7504e-09, 4.3666e-10,
         7.1378e-10],
        [8.0870e-10, 5.3694e-10, 2.3736e-10,  ..., 6.3679e-10, 2.9420e-10,
         2.5559e-10],
        [3.3848e-10, 1.8300e-10, 4.6318e-11,  ..., 2.4676e-10, 3.9096e-11,
         8.6784e-11]], device='cuda:0')
optimizer state dict: 121.0
lr: [1.1713481994751294e-05, 1.1713481994751294e-05]
scheduler_last_epoch: 121


Running epoch 0, step 968, batch 968
Sampled inputs[:2]: tensor([[    0,    21,    66,  ...,  1377,   278,  1634],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7477e-05, -1.4509e-04, -5.3479e-05,  ..., -1.0003e-05,
          3.5995e-06,  6.6314e-06],
        [-1.8552e-06, -1.0580e-06,  1.0207e-06,  ..., -1.5944e-06,
         -9.8348e-07, -1.4380e-06],
        [-4.2915e-06, -2.5332e-06,  2.4885e-06,  ..., -3.6657e-06,
         -2.2352e-06, -3.2783e-06],
        [-3.2037e-06, -1.7732e-06,  1.8477e-06,  ..., -2.7269e-06,
         -1.6168e-06, -2.5332e-06],
        [-4.7684e-06, -2.9057e-06,  2.7269e-06,  ..., -4.1425e-06,
         -2.6524e-06, -3.5018e-06]], device='cuda:0')
Loss: 1.0139113664627075


Running epoch 0, step 969, batch 969
Sampled inputs[:2]: tensor([[    0,    19,     9,  ..., 11504,   446,   381],
        [    0, 18717,  2837,  ...,    48,    18,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5585e-05, -1.1669e-04,  1.0259e-04,  ..., -4.2795e-05,
          4.6449e-05,  1.8558e-04],
        [-3.7029e-06, -2.2352e-06,  2.0191e-06,  ..., -3.2559e-06,
         -2.0042e-06, -2.8014e-06],
        [-8.4043e-06, -5.2452e-06,  4.8727e-06,  ..., -7.3165e-06,
         -4.5002e-06, -6.2585e-06],
        [-6.3628e-06, -3.7253e-06,  3.6284e-06,  ..., -5.5432e-06,
         -3.3304e-06, -4.9621e-06],
        [-9.2983e-06, -6.0350e-06,  5.3495e-06,  ..., -8.2552e-06,
         -5.3048e-06, -6.6310e-06]], device='cuda:0')
Loss: 1.03168523311615


Running epoch 0, step 970, batch 970
Sampled inputs[:2]: tensor([[    0,   221,   467,  ..., 21991,   630,  3990],
        [    0,  3440,  5745,  ...,   360,  4998,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2301e-04, -2.6599e-04,  1.0895e-04,  ..., -6.0292e-05,
         -1.6791e-05,  2.0630e-04],
        [-5.4613e-06, -3.3602e-06,  3.1963e-06,  ..., -4.7833e-06,
         -2.8424e-06, -3.9488e-06],
        [-1.2249e-05, -7.8082e-06,  7.5847e-06,  ..., -1.0669e-05,
         -6.3628e-06, -8.7470e-06],
        [-9.4771e-06, -5.6773e-06,  5.8189e-06,  ..., -8.2403e-06,
         -4.7982e-06, -7.0930e-06],
        [-1.3381e-05, -8.8811e-06,  8.2105e-06,  ..., -1.1876e-05,
         -7.4506e-06, -9.1195e-06]], device='cuda:0')
Loss: 1.0021700859069824


Running epoch 0, step 971, batch 971
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  298, 9855,  278],
        [   0,  266, 1784,  ..., 1119, 1276,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1379e-05, -3.7185e-04, -4.7879e-05,  ..., -4.3781e-05,
          1.7005e-04,  2.1053e-04],
        [-7.1973e-06, -4.3809e-06,  4.2319e-06,  ..., -6.3553e-06,
         -3.8110e-06, -5.2080e-06],
        [-1.6123e-05, -1.0133e-05,  1.0043e-05,  ..., -1.4111e-05,
         -8.4490e-06, -1.1474e-05],
        [-1.2457e-05, -7.3314e-06,  7.6741e-06,  ..., -1.0908e-05,
         -6.4000e-06, -9.3281e-06],
        [-1.7613e-05, -1.1578e-05,  1.0908e-05,  ..., -1.5691e-05,
         -9.8944e-06, -1.1906e-05]], device='cuda:0')
Loss: 1.000157117843628


Running epoch 0, step 972, batch 972
Sampled inputs[:2]: tensor([[   0,  342, 4781,  ...,  630,  940,  271],
        [   0,  417,  199,  ..., 1853,   12,  709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2240e-05, -3.0281e-04, -3.3737e-04,  ..., -2.2919e-04,
          6.6881e-05, -1.5644e-04],
        [-8.9481e-06, -5.5283e-06,  5.1856e-06,  ..., -8.0019e-06,
         -4.8690e-06, -6.4299e-06],
        [-1.9968e-05, -1.2711e-05,  1.2323e-05,  ..., -1.7673e-05,
         -1.0729e-05, -1.4067e-05],
        [-1.5467e-05, -9.2238e-06,  9.3952e-06,  ..., -1.3709e-05,
         -8.1807e-06, -1.1519e-05],
        [-2.1935e-05, -1.4633e-05,  1.3500e-05,  ..., -1.9774e-05,
         -1.2636e-05, -1.4633e-05]], device='cuda:0')
Loss: 0.9930065274238586


Running epoch 0, step 973, batch 973
Sampled inputs[:2]: tensor([[   0,    9,  292,  ...,  944,  278, 1758],
        [   0, 3115, 1640,  ...,  300,  266, 5453]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2693e-05, -3.3244e-04, -5.8110e-04,  ..., -2.2854e-04,
         -2.1867e-04, -5.3577e-04],
        [-1.0625e-05, -6.6310e-06,  6.2734e-06,  ..., -9.5144e-06,
         -5.7891e-06, -7.4729e-06],
        [-2.3574e-05, -1.5154e-05,  1.4827e-05,  ..., -2.0891e-05,
         -1.2681e-05, -1.6242e-05],
        [-1.8463e-05, -1.1094e-05,  1.1481e-05,  ..., -1.6391e-05,
         -9.7826e-06, -1.3500e-05],
        [-2.5928e-05, -1.7464e-05,  1.6257e-05,  ..., -2.3395e-05,
         -1.4946e-05, -1.6868e-05]], device='cuda:0')
Loss: 0.9924317002296448


Running epoch 0, step 974, batch 974
Sampled inputs[:2]: tensor([[    0,   299,   292,  ...,   266,  2474,   360],
        [    0,   221,   474,  ...,    14, 10961,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3506e-05, -2.9344e-04, -6.8791e-04,  ..., -1.7139e-04,
         -2.1867e-04, -7.2436e-04],
        [-1.2457e-05, -7.8082e-06,  7.3835e-06,  ..., -1.1154e-05,
         -6.7502e-06, -8.6650e-06],
        [-2.7508e-05, -1.7792e-05,  1.7360e-05,  ..., -2.4453e-05,
         -1.4782e-05, -1.8820e-05],
        [-2.1562e-05, -1.3046e-05,  1.3448e-05,  ..., -1.9163e-05,
         -1.1392e-05, -1.5616e-05],
        [-3.0190e-05, -2.0444e-05,  1.8969e-05,  ..., -2.7299e-05,
         -1.7390e-05, -1.9491e-05]], device='cuda:0')
Loss: 0.9934266805648804


Running epoch 0, step 975, batch 975
Sampled inputs[:2]: tensor([[    0,   278, 19142,  ...,   271,   266,   298],
        [    0,   266,  7407,  ...,   287,   365,  4371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3249e-05, -3.4514e-04, -8.5980e-04,  ..., -2.2059e-04,
         -1.5125e-04, -7.6241e-04],
        [-1.4350e-05, -8.9332e-06,  8.4937e-06,  ..., -1.2793e-05,
         -7.7337e-06, -9.9316e-06],
        [-3.1531e-05, -2.0355e-05,  1.9893e-05,  ..., -2.7969e-05,
         -1.6928e-05, -2.1487e-05],
        [ 2.3912e-04,  3.2612e-04, -1.6484e-04,  ...,  3.0271e-04,
          2.8033e-04,  1.6454e-04],
        [-3.4630e-05, -2.3425e-05,  2.1771e-05,  ..., -3.1263e-05,
         -1.9938e-05, -2.2262e-05]], device='cuda:0')
Loss: 1.013474702835083
Graident accumulation at epoch 0, step 975, batch 975
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5786e-05,  7.3344e-07, -1.9459e-04,  ...,  2.0566e-05,
          6.5938e-05, -1.0921e-04],
        [-1.4767e-05, -9.9701e-06,  8.3666e-06,  ..., -1.2913e-05,
         -7.8635e-06, -9.5341e-06],
        [-2.5783e-06,  6.0633e-06,  6.1184e-06,  ...,  7.3940e-07,
          7.3586e-06, -1.0387e-05],
        [ 5.0068e-05,  6.1995e-05, -4.2746e-05,  ...,  6.8032e-05,
          5.9988e-05,  3.2052e-05],
        [-3.7652e-05, -2.7409e-05,  2.2534e-05,  ..., -3.2544e-05,
         -2.1225e-05, -2.2466e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2145e-08, 5.2497e-08, 6.3706e-08,  ..., 2.2666e-08, 1.4515e-07,
         3.4020e-08],
        [7.3853e-11, 4.2957e-11, 1.1196e-11,  ..., 4.9933e-11, 1.0600e-11,
         1.9414e-11],
        [2.5176e-09, 1.3052e-09, 4.9419e-10,  ..., 1.7494e-09, 4.3651e-10,
         7.1352e-10],
        [8.6507e-10, 6.4276e-10, 2.6429e-10,  ..., 7.2778e-10, 3.7249e-10,
         2.8240e-10],
        [3.3934e-10, 1.8337e-10, 4.6746e-11,  ..., 2.4749e-10, 3.9454e-11,
         8.7193e-11]], device='cuda:0')
optimizer state dict: 122.0
lr: [1.1591565787821919e-05, 1.1591565787821919e-05]
scheduler_last_epoch: 122


Running epoch 0, step 976, batch 976
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  5750,   642,  ...,   221, 15441,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6195e-05,  1.2122e-04,  2.7161e-04,  ..., -1.1841e-04,
          1.2648e-04,  8.4688e-05],
        [-1.7509e-06, -1.2740e-06,  9.3877e-07,  ..., -1.5721e-06,
         -1.1399e-06, -1.1548e-06],
        [-3.9339e-06, -2.9206e-06,  2.2203e-06,  ..., -3.5316e-06,
         -2.5481e-06, -2.5779e-06],
        [-3.1292e-06, -2.2203e-06,  1.7285e-06,  ..., -2.8014e-06,
         -2.0117e-06, -2.1458e-06],
        [-4.4703e-06, -3.3975e-06,  2.5183e-06,  ..., -4.0829e-06,
         -3.0100e-06, -2.8163e-06]], device='cuda:0')
Loss: 1.0225764513015747


Running epoch 0, step 977, batch 977
Sampled inputs[:2]: tensor([[    0,   342,   516,  ...,    12,   729,  3701],
        [    0,  2261,     9,  ..., 15008,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4757e-06,  1.2377e-04,  2.6403e-04,  ..., -1.7027e-04,
         -3.8840e-06,  2.1595e-05],
        [-3.4720e-06, -2.4587e-06,  2.0117e-06,  ..., -3.0994e-06,
         -2.0824e-06, -2.1830e-06],
        [-7.7188e-06, -5.6177e-06,  4.7088e-06,  ..., -6.8992e-06,
         -4.6492e-06, -4.8280e-06],
        [-6.1691e-06, -4.2766e-06,  3.7104e-06,  ..., -5.4836e-06,
         -3.6582e-06, -4.0308e-06],
        [-8.4639e-06, -6.3628e-06,  5.1409e-06,  ..., -7.7188e-06,
         -5.3942e-06, -5.0515e-06]], device='cuda:0')
Loss: 0.9892123341560364


Running epoch 0, step 978, batch 978
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ...,  278, 6607,   13],
        [   0, 1746,   14,  ..., 3134, 5968,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0049e-05,  1.5494e-04,  3.3587e-04,  ..., -2.3536e-04,
          5.5138e-05, -1.5879e-04],
        [-5.2303e-06, -3.6359e-06,  3.0771e-06,  ..., -4.6492e-06,
         -3.0734e-06, -3.2708e-06],
        [-1.1519e-05, -8.2701e-06,  7.1675e-06,  ..., -1.0267e-05,
         -6.8247e-06, -7.1675e-06],
        [-9.2685e-06, -6.3181e-06,  5.6773e-06,  ..., -8.2105e-06,
         -5.4017e-06, -6.0424e-06],
        [-1.2636e-05, -9.4026e-06,  7.8380e-06,  ..., -1.1474e-05,
         -7.9423e-06, -7.4506e-06]], device='cuda:0')
Loss: 0.9985615015029907


Running epoch 0, step 979, batch 979
Sampled inputs[:2]: tensor([[    0,  5998,   591,  ...,  3126,    12,   358],
        [    0,   266, 15324,  ...,   943,  1613,  7178]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3797e-05,  1.1719e-04,  2.1438e-04,  ..., -4.3082e-04,
          2.6387e-05, -3.3297e-04],
        [-6.8992e-06, -4.7013e-06,  4.1053e-06,  ..., -6.1840e-06,
         -4.0159e-06, -4.3511e-06],
        [-1.5169e-05, -1.0669e-05,  9.5665e-06,  ..., -1.3575e-05,
         -8.8513e-06, -9.4473e-06],
        [-1.2249e-05, -8.1435e-06,  7.6443e-06,  ..., -1.0937e-05,
         -7.0632e-06, -8.0541e-06],
        [-1.6719e-05, -1.2189e-05,  1.0520e-05,  ..., -1.5214e-05,
         -1.0356e-05, -9.8199e-06]], device='cuda:0')
Loss: 0.9986940026283264


Running epoch 0, step 980, batch 980
Sampled inputs[:2]: tensor([[    0,   516,  1424,  ...,  3473,   278,  2442],
        [    0,   368, 46614,  ...,  1070,   278,  1028]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8841e-05,  4.1626e-05, -1.6667e-05,  ..., -4.1040e-04,
          6.3534e-06, -5.6861e-04],
        [-8.6576e-06, -5.9009e-06,  5.1111e-06,  ..., -7.7710e-06,
         -5.0217e-06, -5.4613e-06],
        [-1.9163e-05, -1.3411e-05,  1.1981e-05,  ..., -1.7107e-05,
         -1.1086e-05, -1.1906e-05],
        [-1.5378e-05, -1.0185e-05,  9.4995e-06,  ..., -1.3709e-05,
         -8.7842e-06, -1.0096e-05],
        [-2.1040e-05, -1.5274e-05,  1.3143e-05,  ..., -1.9118e-05,
         -1.2919e-05, -1.2323e-05]], device='cuda:0')
Loss: 0.9937775731086731


Running epoch 0, step 981, batch 981
Sampled inputs[:2]: tensor([[    0,   298, 21144,  ...,  7825, 19426,  3709],
        [    0,  1477,   591,  ...,  4111, 18012, 11991]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4193e-06, -7.3697e-06,  1.2435e-05,  ..., -4.7214e-04,
          8.7434e-05, -4.7734e-04],
        [-1.0520e-05, -7.0781e-06,  6.1393e-06,  ..., -9.4101e-06,
         -6.1169e-06, -6.7726e-06],
        [-2.3335e-05, -1.6138e-05,  1.4409e-05,  ..., -2.0772e-05,
         -1.3545e-05, -1.4812e-05],
        [-1.8522e-05, -1.2152e-05,  1.1340e-05,  ..., -1.6466e-05,
         -1.0602e-05, -1.2331e-05],
        [-2.5839e-05, -1.8507e-05,  1.5929e-05,  ..., -2.3410e-05,
         -1.5914e-05, -1.5557e-05]], device='cuda:0')
Loss: 1.0383069515228271


Running epoch 0, step 982, batch 982
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,  9088,  7217,  ...,   199, 17822,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3964e-04, -1.1095e-04, -1.2120e-05,  ..., -5.5050e-04,
          9.4265e-05, -5.1213e-04],
        [-1.2301e-05, -8.3223e-06,  7.2047e-06,  ..., -1.1012e-05,
         -7.1302e-06, -7.8753e-06],
        [-2.7299e-05, -1.9014e-05,  1.6913e-05,  ..., -2.4363e-05,
         -1.5825e-05, -1.7270e-05],
        [ 5.3971e-05,  7.9544e-05, -1.3823e-06,  ...,  4.7359e-05,
          5.9654e-05,  7.7916e-06],
        [-3.0249e-05, -2.1830e-05,  1.8716e-05,  ..., -2.7493e-05,
         -1.8626e-05, -1.8165e-05]], device='cuda:0')
Loss: 1.0287182331085205


Running epoch 0, step 983, batch 983
Sampled inputs[:2]: tensor([[    0,   278,   266,  ..., 10639,   292,  4723],
        [    0,  2663,   328,  ...,   266,  1040,  1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0671e-05, -1.1458e-04, -1.6832e-05,  ..., -5.6581e-04,
          4.7868e-05, -5.1957e-04],
        [-1.4119e-05, -9.4995e-06,  8.2776e-06,  ..., -1.2577e-05,
         -8.0839e-06, -9.1046e-06],
        [-3.1263e-05, -2.1711e-05,  1.9416e-05,  ..., -2.7791e-05,
         -1.7956e-05, -1.9938e-05],
        [ 5.0872e-05,  7.7577e-05,  5.3991e-07,  ...,  4.4722e-05,
          5.8067e-05,  5.6310e-06],
        [-3.4600e-05, -2.4915e-05,  2.1458e-05,  ..., -3.1337e-05,
         -2.1145e-05, -2.0921e-05]], device='cuda:0')
Loss: 0.9800919890403748
Graident accumulation at epoch 0, step 983, batch 983
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0140e-05, -1.0797e-05, -1.7682e-04,  ..., -3.8072e-05,
          6.4131e-05, -1.5025e-04],
        [-1.4702e-05, -9.9230e-06,  8.3577e-06,  ..., -1.2879e-05,
         -7.8855e-06, -9.4912e-06],
        [-5.4467e-06,  3.2859e-06,  7.4482e-06,  ..., -2.1136e-06,
          4.8271e-06, -1.1342e-05],
        [ 5.0148e-05,  6.3553e-05, -3.8418e-05,  ...,  6.5701e-05,
          5.9796e-05,  2.9409e-05],
        [-3.7347e-05, -2.7160e-05,  2.2427e-05,  ..., -3.2423e-05,
         -2.1217e-05, -2.2312e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2101e-08, 5.2458e-08, 6.3642e-08,  ..., 2.2964e-08, 1.4501e-07,
         3.4256e-08],
        [7.3979e-11, 4.3004e-11, 1.1253e-11,  ..., 5.0042e-11, 1.0655e-11,
         1.9477e-11],
        [2.5161e-09, 1.3043e-09, 4.9407e-10,  ..., 1.7484e-09, 4.3640e-10,
         7.1321e-10],
        [8.6679e-10, 6.4814e-10, 2.6403e-10,  ..., 7.2905e-10, 3.7549e-10,
         2.8215e-10],
        [3.4020e-10, 1.8380e-10, 4.7159e-11,  ..., 2.4822e-10, 3.9862e-11,
         8.7544e-11]], device='cuda:0')
optimizer state dict: 123.0
lr: [1.1469406375747185e-05, 1.1469406375747185e-05]
scheduler_last_epoch: 123


Running epoch 0, step 984, batch 984
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  1412,    35,  ...,  6077,   298,  1826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6443e-06,  1.3887e-04,  1.3500e-04,  ..., -9.4653e-05,
         -2.6212e-05,  1.6486e-04],
        [-1.6913e-06, -1.2070e-06,  8.7544e-07,  ..., -1.6019e-06,
         -1.1697e-06, -1.1623e-06],
        [-3.7551e-06, -2.7716e-06,  2.1160e-06,  ..., -3.5167e-06,
         -2.5630e-06, -2.5034e-06],
        [-2.9802e-06, -2.1011e-06,  1.6093e-06,  ..., -2.8312e-06,
         -2.0713e-06, -2.1160e-06],
        [-4.2617e-06, -3.2037e-06,  2.4140e-06,  ..., -4.0233e-06,
         -2.9951e-06, -2.6822e-06]], device='cuda:0')
Loss: 0.9800246357917786


Running epoch 0, step 985, batch 985
Sampled inputs[:2]: tensor([[    0,   518,  9048,  ...,  1354,   352,   266],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4745e-05,  4.6767e-05,  1.4085e-04,  ..., -9.8805e-05,
          2.7522e-04,  2.7583e-04],
        [-3.2783e-06, -2.3022e-06,  1.9185e-06,  ..., -3.0398e-06,
         -2.1197e-06, -2.1681e-06],
        [ 7.3079e-05,  5.6118e-05, -4.4477e-05,  ...,  6.4282e-05,
          9.9621e-05,  2.0344e-05],
        [-5.9456e-06, -4.0978e-06,  3.6657e-06,  ..., -5.5134e-06,
         -3.8296e-06, -4.1127e-06],
        [-8.1062e-06, -6.0797e-06,  5.0813e-06,  ..., -7.5549e-06,
         -5.4389e-06, -4.9472e-06]], device='cuda:0')
Loss: 0.9927248358726501


Running epoch 0, step 986, batch 986
Sampled inputs[:2]: tensor([[    0,  1340,  1049,  ...,  1441,  1211,  4165],
        [    0,   452,    13,  ...,   358,    13, 12347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5353e-04,  9.0663e-05,  1.8838e-04,  ..., -1.2481e-04,
          2.5103e-04,  2.1314e-04],
        [-4.9546e-06, -3.4645e-06,  2.9542e-06,  ..., -4.5672e-06,
         -3.1628e-06, -3.1963e-06],
        [ 6.9338e-05,  5.3450e-05, -4.2033e-05,  ...,  6.0900e-05,
          9.7326e-05,  1.8094e-05],
        [-8.9854e-06, -6.1542e-06,  5.6475e-06,  ..., -8.2552e-06,
         -5.6922e-06, -6.0648e-06],
        [-1.2219e-05, -9.1195e-06,  7.7635e-06,  ..., -1.1340e-05,
         -8.1062e-06, -7.2867e-06]], device='cuda:0')
Loss: 1.009139895439148


Running epoch 0, step 987, batch 987
Sampled inputs[:2]: tensor([[    0, 16803,   965,  ..., 36064,    12, 13769],
        [    0,   809,   367,  ...,   717,   287,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6614e-05,  1.3619e-04,  1.5411e-04,  ..., -1.7563e-04,
          1.5789e-04,  1.3699e-04],
        [-6.7949e-06, -4.6939e-06,  3.9153e-06,  ..., -6.1765e-06,
         -4.2282e-06, -4.4703e-06],
        [ 6.5196e-05,  5.0530e-05, -3.9693e-05,  ...,  5.7249e-05,
          9.4882e-05,  1.5248e-05],
        [-1.2204e-05, -8.2999e-06,  7.4357e-06,  ..., -1.1086e-05,
         -7.5623e-06, -8.3745e-06],
        [-1.6868e-05, -1.2502e-05,  1.0401e-05,  ..., -1.5512e-05,
         -1.1012e-05, -1.0341e-05]], device='cuda:0')
Loss: 0.9772940874099731


Running epoch 0, step 988, batch 988
Sampled inputs[:2]: tensor([[    0,  4110,   271,  ...,   944,   278,  3230],
        [    0,  4710,    12,  ...,  3969,     9, 11692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9773e-05,  2.6024e-04,  2.5470e-04,  ..., -1.0744e-04,
          3.1923e-04,  5.5412e-05],
        [-8.4639e-06, -5.9158e-06,  4.9062e-06,  ..., -7.7114e-06,
         -5.2787e-06, -5.4762e-06],
        [ 1.2236e-04,  1.0966e-04, -9.4660e-05,  ...,  1.4660e-04,
          1.6669e-04,  4.2905e-05],
        [-1.5214e-05, -1.0476e-05,  9.3058e-06,  ..., -1.3858e-05,
         -9.4473e-06, -1.0259e-05],
        [-2.0862e-05, -1.5602e-05,  1.2949e-05,  ..., -1.9237e-05,
         -1.3664e-05, -1.2591e-05]], device='cuda:0')
Loss: 1.0297085046768188


Running epoch 0, step 989, batch 989
Sampled inputs[:2]: tensor([[    0,  1119,   943,  ...,   759,   920,  8874],
        [    0,   923,  2583,  ..., 11385,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0194e-04,  2.0731e-04,  1.9018e-04,  ..., -1.6569e-04,
          2.9826e-04,  4.7732e-05],
        [-1.0177e-05, -7.0482e-06,  5.8897e-06,  ..., -9.1940e-06,
         -6.3069e-06, -6.5938e-06],
        [ 1.1856e-04,  1.0704e-04, -9.2320e-05,  ...,  1.4333e-04,
          1.6441e-04,  4.0461e-05],
        [-1.8328e-05, -1.2487e-05,  1.1176e-05,  ..., -1.6540e-05,
         -1.1303e-05, -1.2375e-05],
        [-2.5094e-05, -1.8641e-05,  1.5557e-05,  ..., -2.2978e-05,
         -1.6361e-05, -1.5169e-05]], device='cuda:0')
Loss: 0.979699432849884


Running epoch 0, step 990, batch 990
Sampled inputs[:2]: tensor([[   0,  278, 2097,  ..., 1754,  287,  631],
        [   0,  271,  266,  ..., 3795,  908,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3684e-06,  4.3930e-04,  2.0698e-04,  ..., -1.2873e-04,
          3.6494e-05,  3.8926e-04],
        [-1.1973e-05, -8.1211e-06,  6.8136e-06,  ..., -1.0788e-05,
         -7.3873e-06, -7.9125e-06],
        [ 1.1453e-04,  1.0453e-04, -9.0130e-05,  ...,  1.3976e-04,
          1.6200e-04,  3.7540e-05],
        [-2.1428e-05, -1.4298e-05,  1.2860e-05,  ..., -1.9282e-05,
         -1.3158e-05, -1.4715e-05],
        [-2.9624e-05, -2.1547e-05,  1.8030e-05,  ..., -2.7031e-05,
         -1.9193e-05, -1.8328e-05]], device='cuda:0')
Loss: 0.9467033743858337


Running epoch 0, step 991, batch 991
Sampled inputs[:2]: tensor([[    0,  3933,  6394,  ...,  1364,   950,   847],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3258e-05,  4.3992e-04,  1.3571e-04,  ..., -1.3108e-04,
         -1.1029e-04,  2.3727e-04],
        [-1.3649e-05, -9.2760e-06,  7.8343e-06,  ..., -1.2286e-05,
         -8.4080e-06, -8.9034e-06],
        [ 2.2742e-04,  2.6285e-04, -2.3185e-04,  ...,  3.0856e-04,
          3.0331e-04,  8.3751e-05],
        [-2.4527e-05, -1.6384e-05,  1.4812e-05,  ..., -2.2039e-05,
         -1.5028e-05, -1.6622e-05],
        [-3.3796e-05, -2.4602e-05,  2.0683e-05,  ..., -3.0801e-05,
         -2.1845e-05, -2.0653e-05]], device='cuda:0')
Loss: 1.0210663080215454
Graident accumulation at epoch 0, step 991, batch 991
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0284,  ...,  0.0290, -0.0145, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0800e-05,  3.4275e-05, -1.4556e-04,  ..., -4.7372e-05,
          4.6689e-05, -1.1149e-04],
        [-1.4597e-05, -9.8583e-06,  8.3053e-06,  ..., -1.2820e-05,
         -7.9378e-06, -9.4324e-06],
        [ 1.7840e-05,  2.9242e-05, -1.6482e-05,  ...,  2.8954e-05,
          3.4676e-05, -1.8329e-06],
        [ 4.2680e-05,  5.5559e-05, -3.3095e-05,  ...,  5.6927e-05,
          5.2314e-05,  2.4806e-05],
        [-3.6992e-05, -2.6904e-05,  2.2252e-05,  ..., -3.2261e-05,
         -2.1280e-05, -2.2146e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2051e-08, 5.2599e-08, 6.3597e-08,  ..., 2.2958e-08, 1.4488e-07,
         3.4278e-08],
        [7.4091e-11, 4.3047e-11, 1.1304e-11,  ..., 5.0143e-11, 1.0715e-11,
         1.9537e-11],
        [2.5653e-09, 1.3721e-09, 5.4734e-10,  ..., 1.8419e-09, 5.2796e-10,
         7.1951e-10],
        [8.6653e-10, 6.4776e-10, 2.6399e-10,  ..., 7.2881e-10, 3.7534e-10,
         2.8215e-10],
        [3.4100e-10, 1.8422e-10, 4.7540e-11,  ..., 2.4892e-10, 4.0299e-11,
         8.7883e-11]], device='cuda:0')
optimizer state dict: 124.0
lr: [1.1347022425551613e-05, 1.1347022425551613e-05]
scheduler_last_epoch: 124


Running epoch 0, step 992, batch 992
Sampled inputs[:2]: tensor([[    0,   266,  5528,  ...,   685,   266,  1231],
        [    0, 21801, 13084,  ...,  1738,  2946,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6548e-05,  2.3402e-04,  3.9409e-05,  ...,  5.5469e-06,
          7.8788e-05,  6.2262e-06],
        [-1.8626e-06, -1.1697e-06,  9.0897e-07,  ..., -1.6093e-06,
         -1.1697e-06, -1.3486e-06],
        [-4.2617e-06, -2.7865e-06,  2.1756e-06,  ..., -3.6955e-06,
         -2.6971e-06, -3.0845e-06],
        [-3.2634e-06, -2.0415e-06,  1.6764e-06,  ..., -2.8312e-06,
         -2.0415e-06, -2.4140e-06],
        [-4.8578e-06, -3.2336e-06,  2.4140e-06,  ..., -4.2617e-06,
         -3.2037e-06, -3.4273e-06]], device='cuda:0')
Loss: 0.9721639156341553


Running epoch 0, step 993, batch 993
Sampled inputs[:2]: tensor([[   0,  278, 1620,  ...,  360, 1758,  278],
        [   0, 2652,  271,  ...,  634, 1921,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9598e-05,  3.9969e-04,  1.2197e-04,  ...,  1.2903e-05,
         -1.3043e-04,  7.5327e-06],
        [-3.5539e-06, -2.2650e-06,  1.9073e-06,  ..., -3.0920e-06,
         -2.2575e-06, -2.5257e-06],
        [-8.0764e-06, -5.4240e-06,  4.5896e-06,  ..., -7.0632e-06,
         -5.1856e-06, -5.7220e-06],
        [-6.3032e-06, -3.9935e-06,  3.5614e-06,  ..., -5.4985e-06,
         -4.0084e-06, -4.5896e-06],
        [-9.0599e-06, -6.1989e-06,  5.0664e-06,  ..., -8.0168e-06,
         -6.0648e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9632030725479126


Running epoch 0, step 994, batch 994
Sampled inputs[:2]: tensor([[    0,  2018,  4798,  ...,   292,  1919,   221],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3958e-04,  3.9555e-04,  5.5083e-06,  ..., -1.6277e-05,
         -1.4647e-04, -1.0237e-04],
        [-5.1185e-06, -3.3751e-06,  2.9728e-06,  ..., -4.5449e-06,
         -3.1963e-06, -3.4496e-06],
        [ 1.2265e-04,  1.2915e-04, -1.4237e-04,  ...,  1.0937e-04,
          8.6341e-05,  2.8893e-05],
        [-9.2536e-06, -6.0350e-06,  5.6773e-06,  ..., -8.2403e-06,
         -5.7667e-06, -6.4299e-06],
        [-1.2964e-05, -9.1642e-06,  7.8529e-06,  ..., -1.1697e-05,
         -8.5533e-06, -8.2999e-06]], device='cuda:0')
Loss: 1.0005425214767456


Running epoch 0, step 995, batch 995
Sampled inputs[:2]: tensor([[    0,  1883,  1090,  ...,   365,  1943,   298],
        [    0,   259, 19567,  ...,   266,  3899,  2123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4931e-04,  3.8212e-04, -1.3156e-04,  ..., -4.7719e-05,
         -2.0779e-04, -2.1436e-04],
        [-6.7577e-06, -4.5374e-06,  4.1127e-06,  ..., -6.0573e-06,
         -4.1649e-06, -4.3400e-06],
        [ 1.1897e-04,  1.2644e-04, -1.3969e-04,  ...,  1.0594e-04,
          8.4136e-05,  2.6897e-05],
        [-1.2368e-05, -8.2105e-06,  7.9423e-06,  ..., -1.1101e-05,
         -7.5847e-06, -8.1956e-06],
        [-1.6928e-05, -1.2219e-05,  1.0714e-05,  ..., -1.5467e-05,
         -1.1101e-05, -1.0327e-05]], device='cuda:0')
Loss: 1.0365614891052246


Running epoch 0, step 996, batch 996
Sampled inputs[:2]: tensor([[   0, 4538,  271,  ..., 1603,  591,  688],
        [   0,   73,   30,  ..., 4112,   12, 9416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8684e-04,  4.6966e-04, -1.9952e-04,  ..., -6.5096e-05,
         -2.0542e-04, -1.0890e-04],
        [-8.4341e-06, -5.7742e-06,  5.2303e-06,  ..., -7.5921e-06,
         -5.2303e-06, -5.3011e-06],
        [ 1.8815e-04,  1.5766e-04, -1.7279e-04,  ...,  1.8864e-04,
          9.1430e-05,  8.2548e-05],
        [-1.5453e-05, -1.0446e-05,  1.0058e-05,  ..., -1.3918e-05,
         -9.5367e-06, -1.0058e-05],
        [-2.1100e-05, -1.5512e-05,  1.3605e-05,  ..., -1.9372e-05,
         -1.3903e-05, -1.2577e-05]], device='cuda:0')
Loss: 1.011764645576477


Running epoch 0, step 997, batch 997
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  2057,   342, 11927],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8684e-04,  4.8231e-04, -1.9875e-04,  ..., -7.5929e-05,
         -4.3779e-04, -2.7436e-04],
        [-9.9540e-06, -6.7726e-06,  6.2734e-06,  ..., -8.9630e-06,
         -6.1132e-06, -6.2250e-06],
        [ 1.8463e-04,  1.5530e-04, -1.7024e-04,  ...,  1.8551e-04,
          8.9418e-05,  8.0462e-05],
        [-1.8418e-05, -1.2301e-05,  1.2234e-05,  ..., -1.6555e-05,
         -1.1221e-05, -1.1966e-05],
        [-2.4796e-05, -1.8105e-05,  1.6272e-05,  ..., -2.2739e-05,
         -1.6153e-05, -1.4633e-05]], device='cuda:0')
Loss: 0.9687359929084778


Running epoch 0, step 998, batch 998
Sampled inputs[:2]: tensor([[    0,  1932,   278,  ...,   609,   271,   266],
        [    0, 33315,   266,  ...,    12,  1126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3546e-04,  4.6301e-04, -1.7691e-04,  ..., -1.4281e-05,
         -4.3485e-04, -3.0726e-04],
        [-1.1586e-05, -7.8827e-06,  7.2569e-06,  ..., -1.0483e-05,
         -7.1786e-06, -7.3276e-06],
        [ 1.8086e-04,  1.5265e-04, -1.6783e-04,  ...,  1.8204e-04,
          8.6989e-05,  7.7973e-05],
        [-2.1383e-05, -1.4268e-05,  1.4089e-05,  ..., -1.9297e-05,
         -1.3143e-05, -1.4052e-05],
        [-2.8968e-05, -2.1145e-05,  1.8924e-05,  ..., -2.6643e-05,
         -1.8984e-05, -1.7241e-05]], device='cuda:0')
Loss: 1.0119984149932861


Running epoch 0, step 999, batch 999
Sampled inputs[:2]: tensor([[   0,  927,  259,  ...,  328, 9430, 2330],
        [   0,   12,  287,  ...,  381, 3513, 1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8179e-04,  4.6972e-04, -2.7122e-04,  ..., -1.1159e-04,
         -5.3446e-04, -4.6328e-04],
        [-1.3150e-05, -8.9630e-06,  8.3074e-06,  ..., -1.1951e-05,
         -8.1137e-06, -8.2739e-06],
        [ 9.7569e-04,  7.8403e-04, -6.8459e-04,  ...,  9.2512e-04,
          5.4939e-04,  4.4001e-04],
        [-2.4378e-05, -1.6235e-05,  1.6205e-05,  ..., -2.2084e-05,
         -1.4894e-05, -1.5944e-05],
        [-3.2872e-05, -2.4036e-05,  2.1726e-05,  ..., -3.0324e-05,
         -2.1458e-05, -1.9401e-05]], device='cuda:0')
Loss: 1.002394676208496
Graident accumulation at epoch 0, step 999, batch 999
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0286, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0284,  ...,  0.0290, -0.0145, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8541e-05,  7.7820e-05, -1.5813e-04,  ..., -5.3794e-05,
         -1.1426e-05, -1.4667e-04],
        [-1.4452e-05, -9.7688e-06,  8.3055e-06,  ..., -1.2733e-05,
         -7.9554e-06, -9.3166e-06],
        [ 1.1362e-04,  1.0472e-04, -8.3293e-05,  ...,  1.1857e-04,
          8.6147e-05,  4.2352e-05],
        [ 3.5975e-05,  4.8380e-05, -2.8165e-05,  ...,  4.9026e-05,
          4.5593e-05,  2.0731e-05],
        [-3.6580e-05, -2.6617e-05,  2.2200e-05,  ..., -3.2067e-05,
         -2.1298e-05, -2.1871e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2032e-08, 5.2767e-08, 6.3607e-08,  ..., 2.2947e-08, 1.4502e-07,
         3.4459e-08],
        [7.4190e-11, 4.3085e-11, 1.1361e-11,  ..., 5.0235e-11, 1.0770e-11,
         1.9586e-11],
        [3.5147e-09, 1.9854e-09, 1.0155e-09,  ..., 2.6959e-09, 8.2926e-10,
         9.1240e-10],
        [8.6625e-10, 6.4737e-10, 2.6398e-10,  ..., 7.2857e-10, 3.7519e-10,
         2.8212e-10],
        [3.4174e-10, 1.8462e-10, 4.7965e-11,  ..., 2.4960e-10, 4.0719e-11,
         8.8171e-11]], device='cuda:0')
optimizer state dict: 125.0
lr: [1.1224432638571088e-05, 1.1224432638571088e-05]
scheduler_last_epoch: 125


Running epoch 0, step 1000, batch 1000
Sampled inputs[:2]: tensor([[   0, 7926, 6750,  ...,  259, 1524, 6257],
        [   0,   47,   12,  ..., 4367,  278,  471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4046e-05,  1.5598e-06, -3.0141e-05,  ..., -4.4439e-05,
         -4.0879e-05, -6.9536e-05],
        [-1.5274e-06, -1.1474e-06,  1.0282e-06,  ..., -1.4752e-06,
         -1.0654e-06, -9.2387e-07],
        [-3.6508e-06, -2.7865e-06,  2.5779e-06,  ..., -3.4720e-06,
         -2.5034e-06, -2.1756e-06],
        [-2.9355e-06, -2.1607e-06,  2.0415e-06,  ..., -2.8163e-06,
         -2.0117e-06, -1.8552e-06],
        [-3.9637e-06, -3.1590e-06,  2.7865e-06,  ..., -3.8743e-06,
         -2.8759e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.018755316734314


Running epoch 0, step 1001, batch 1001
Sampled inputs[:2]: tensor([[    0,   292, 15156,  ...,    35,  3815,  1422],
        [    0,   461,   654,  ...,  6548,  7171,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6800e-05, -1.0433e-04, -4.1244e-05,  ..., -7.2935e-05,
         -1.2965e-04, -4.9123e-06],
        [-3.0920e-06, -2.3171e-06,  2.0340e-06,  ..., -2.9579e-06,
         -2.2128e-06, -1.9222e-06],
        [-7.4506e-06, -5.7071e-06,  5.1558e-06,  ..., -7.0333e-06,
         -5.2303e-06, -4.5598e-06],
        [-5.8711e-06, -4.3064e-06,  3.9935e-06,  ..., -5.5730e-06,
         -4.1127e-06, -3.7923e-06],
        [-8.1360e-06, -6.4373e-06,  5.5730e-06,  ..., -7.8380e-06,
         -5.9903e-06, -4.7088e-06]], device='cuda:0')
Loss: 1.0054341554641724


Running epoch 0, step 1002, batch 1002
Sampled inputs[:2]: tensor([[   0,  638, 1862,  ...,   14, 7869,   14],
        [   0, 2555,  984,  ..., 5900, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7360e-05,  5.5430e-05, -1.8172e-05,  ..., -2.4675e-04,
         -2.3835e-04,  8.2855e-05],
        [-4.6864e-06, -3.4273e-06,  3.0324e-06,  ..., -4.4405e-06,
         -3.2857e-06, -2.9206e-06],
        [-1.1086e-05, -8.3745e-06,  7.6145e-06,  ..., -1.0431e-05,
         -7.7188e-06, -6.7949e-06],
        [-8.7768e-06, -6.3330e-06,  5.9456e-06,  ..., -8.2850e-06,
         -6.0946e-06, -5.6922e-06],
        [-1.2040e-05, -9.4026e-06,  8.2105e-06,  ..., -1.1563e-05,
         -8.8215e-06, -6.9886e-06]], device='cuda:0')
Loss: 0.9779109954833984


Running epoch 0, step 1003, batch 1003
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,   266,  3382,  ...,   759,   631,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6042e-04,  1.0511e-05, -3.3610e-05,  ..., -2.6832e-04,
         -2.7985e-04,  5.5939e-05],
        [-6.2361e-06, -4.5449e-06,  4.0904e-06,  ..., -5.8413e-06,
         -4.2468e-06, -3.7961e-06],
        [-1.4707e-05, -1.1027e-05,  1.0252e-05,  ..., -1.3664e-05,
         -9.9242e-06, -8.8066e-06],
        [-1.1727e-05, -8.3745e-06,  8.0913e-06,  ..., -1.0923e-05,
         -7.8753e-06, -7.4357e-06],
        [-1.5795e-05, -1.2279e-05,  1.0923e-05,  ..., -1.5005e-05,
         -1.1280e-05, -8.9258e-06]], device='cuda:0')
Loss: 0.9823163747787476


Running epoch 0, step 1004, batch 1004
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,   677, 25912,  ...,  2337,   292,  4462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8625e-04,  1.8580e-04,  1.8745e-04,  ..., -3.7974e-04,
         -9.8096e-05,  9.2781e-05],
        [-7.8455e-06, -5.7518e-06,  4.8988e-06,  ..., -7.3910e-06,
         -5.5656e-06, -4.8988e-06],
        [-1.8403e-05, -1.3992e-05,  1.2308e-05,  ..., -1.7226e-05,
         -1.2979e-05, -1.1280e-05],
        [-1.4558e-05, -1.0476e-05,  9.5740e-06,  ..., -1.3679e-05,
         -1.0274e-05, -9.4771e-06],
        [-2.0117e-05, -1.5840e-05,  1.3322e-05,  ..., -1.9237e-05,
         -1.4961e-05, -1.1668e-05]], device='cuda:0')
Loss: 1.0123258829116821


Running epoch 0, step 1005, batch 1005
Sampled inputs[:2]: tensor([[   0, 1101,  300,  ..., 6104,  367,  993],
        [   0, 5007, 7551,  ...,    9, 2095,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9027e-04,  2.4201e-04,  1.3250e-04,  ..., -3.6144e-04,
         -1.8674e-04,  1.3604e-04],
        [-9.4399e-06, -6.8843e-06,  5.9120e-06,  ..., -8.8736e-06,
         -6.6459e-06, -5.8599e-06],
        [-2.2009e-05, -1.6674e-05,  1.4767e-05,  ..., -2.0579e-05,
         -1.5453e-05, -1.3411e-05],
        [-1.7494e-05, -1.2502e-05,  1.1526e-05,  ..., -1.6406e-05,
         -1.2286e-05, -1.1340e-05],
        [-2.4050e-05, -1.8880e-05,  1.6004e-05,  ..., -2.2948e-05,
         -1.7777e-05, -1.3843e-05]], device='cuda:0')
Loss: 1.0090699195861816


Running epoch 0, step 1006, batch 1006
Sampled inputs[:2]: tensor([[    0,  1171,   341,  ...,   278, 14713,    18],
        [    0,   271, 16217,  ...,  6352,  4546,  2558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1897e-04,  2.7342e-04,  3.3015e-04,  ..., -3.5669e-04,
         -1.1396e-04,  2.3485e-04],
        [-1.1012e-05, -8.1360e-06,  6.9104e-06,  ..., -1.0364e-05,
         -7.7710e-06, -6.8098e-06],
        [-2.5749e-05, -1.9744e-05,  1.7285e-05,  ..., -2.4110e-05,
         -1.8120e-05, -1.5661e-05],
        [-2.0489e-05, -1.4842e-05,  1.3508e-05,  ..., -1.9237e-05,
         -1.4417e-05, -1.3232e-05],
        [-2.8163e-05, -2.2352e-05,  1.8746e-05,  ..., -2.6911e-05,
         -2.0877e-05, -1.6198e-05]], device='cuda:0')
Loss: 1.0438549518585205


Running epoch 0, step 1007, batch 1007
Sampled inputs[:2]: tensor([[    0,  2906, 46441,  ..., 39156,   287, 11452],
        [    0,  5332,   266,  ...,   300,   259, 15369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1525e-04,  2.6335e-04,  2.8662e-04,  ..., -4.1791e-04,
         -6.6222e-05,  2.1383e-04],
        [-1.2659e-05, -9.3579e-06,  7.9237e-06,  ..., -1.1891e-05,
         -8.9109e-06, -7.8082e-06],
        [-2.9624e-05, -2.2709e-05,  1.9804e-05,  ..., -2.7716e-05,
         -2.0832e-05, -1.7986e-05],
        [-2.3469e-05, -1.7032e-05,  1.5408e-05,  ..., -2.2009e-05,
         -1.6503e-05, -1.5110e-05],
        [-3.2485e-05, -2.5764e-05,  2.1547e-05,  ..., -3.0965e-05,
         -2.4036e-05, -1.8656e-05]], device='cuda:0')
Loss: 1.031416893005371
Graident accumulation at epoch 0, step 1007, batch 1007
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0286, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0290, -0.0145, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1616e-06,  9.6373e-05, -1.1365e-04,  ..., -9.0206e-05,
         -1.6906e-05, -1.1062e-04],
        [-1.4273e-05, -9.7277e-06,  8.2674e-06,  ..., -1.2649e-05,
         -8.0509e-06, -9.1657e-06],
        [ 9.9300e-05,  9.1978e-05, -7.2984e-05,  ...,  1.0394e-04,
          7.5449e-05,  3.6318e-05],
        [ 3.0030e-05,  4.1839e-05, -2.3808e-05,  ...,  4.1923e-05,
          3.9383e-05,  1.7147e-05],
        [-3.6170e-05, -2.6532e-05,  2.2134e-05,  ..., -3.1957e-05,
         -2.1572e-05, -2.1550e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1993e-08, 5.2783e-08, 6.3626e-08,  ..., 2.3099e-08, 1.4488e-07,
         3.4470e-08],
        [7.4276e-11, 4.3129e-11, 1.1413e-11,  ..., 5.0326e-11, 1.0839e-11,
         1.9627e-11],
        [3.5121e-09, 1.9840e-09, 1.0148e-09,  ..., 2.6940e-09, 8.2886e-10,
         9.1181e-10],
        [8.6594e-10, 6.4702e-10, 2.6396e-10,  ..., 7.2833e-10, 3.7508e-10,
         2.8207e-10],
        [3.4245e-10, 1.8510e-10, 4.8381e-11,  ..., 2.5030e-10, 4.1256e-11,
         8.8431e-11]], device='cuda:0')
optimizer state dict: 126.0
lr: [1.1101655747595168e-05, 1.1101655747595168e-05]
scheduler_last_epoch: 126


Running epoch 0, step 1008, batch 1008
Sampled inputs[:2]: tensor([[   0, 1795,  650,  ...,  516, 2793, 1109],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2948e-05,  1.2583e-05, -1.2456e-05,  ...,  2.6597e-05,
         -4.4693e-05,  1.6238e-04],
        [-1.5497e-06, -1.1846e-06,  1.1101e-06,  ..., -1.4082e-06,
         -1.0505e-06, -9.3505e-07],
        [ 9.6040e-05,  1.5278e-04, -6.6865e-05,  ...,  1.4135e-04,
          1.4423e-04,  9.1344e-05],
        [-2.9355e-06, -2.2203e-06,  2.1905e-06,  ..., -2.6673e-06,
         -1.9670e-06, -1.8552e-06],
        [-4.0829e-06, -3.3379e-06,  3.0249e-06,  ..., -3.8147e-06,
         -2.9653e-06, -2.3246e-06]], device='cuda:0')
Loss: 1.0300490856170654


Running epoch 0, step 1009, batch 1009
Sampled inputs[:2]: tensor([[    0, 24674,   513,  ...,  6099,    12,  4863],
        [    0,    14,  3445,  ...,   298,   527,  2732]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6536e-05, -8.4517e-05, -1.4452e-04,  ...,  6.0077e-05,
         -8.3840e-05,  1.0545e-04],
        [-3.0845e-06, -2.3991e-06,  2.0787e-06,  ..., -2.8759e-06,
         -2.1532e-06, -1.8850e-06],
        [ 9.2314e-05,  1.4977e-04, -6.4347e-05,  ...,  1.3782e-04,
          1.4160e-04,  8.9064e-05],
        [-5.8115e-06, -4.4405e-06,  4.0904e-06,  ..., -5.4091e-06,
         -4.0084e-06, -3.7327e-06],
        [-8.0764e-06, -6.6608e-06,  5.6922e-06,  ..., -7.6592e-06,
         -5.9307e-06, -4.6194e-06]], device='cuda:0')
Loss: 0.9935612678527832


Running epoch 0, step 1010, batch 1010
Sampled inputs[:2]: tensor([[    0, 28107,    14,  ...,   864,   298,   413],
        [    0,   391,  7750,  ...,  4133,   271,   668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5940e-05,  2.3148e-04,  1.4058e-04,  ..., -3.7368e-05,
         -8.1437e-05,  9.8071e-05],
        [-4.7609e-06, -3.5986e-06,  2.8834e-06,  ..., -4.4107e-06,
         -3.5167e-06, -3.1516e-06],
        [ 8.8202e-05,  1.4668e-04, -6.2186e-05,  ...,  1.3408e-04,
          1.3824e-04,  8.6024e-05],
        [-8.8513e-06, -6.5714e-06,  5.6177e-06,  ..., -8.1658e-06,
         -6.4820e-06, -6.0871e-06],
        [-1.2726e-05, -1.0237e-05,  8.1211e-06,  ..., -1.1951e-05,
         -9.8646e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.960800290107727


Running epoch 0, step 1011, batch 1011
Sampled inputs[:2]: tensor([[   0,  298,  894,  ...,  266, 2904, 1679],
        [   0,  287,  768,  ...,  221,  474,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1465e-04,  3.3945e-04,  3.5498e-04,  ..., -3.7368e-05,
          1.1864e-04,  1.1917e-04],
        [-6.5267e-06, -4.7609e-06,  3.8184e-06,  ..., -6.0126e-06,
         -4.7609e-06, -4.5076e-06],
        [ 8.4000e-05,  1.4376e-04, -5.9772e-05,  ...,  1.3026e-04,
          1.3523e-04,  8.2820e-05],
        [-1.1891e-05, -8.5831e-06,  7.3612e-06,  ..., -1.0923e-05,
         -8.6278e-06, -8.4564e-06],
        [-1.7554e-05, -1.3664e-05,  1.0848e-05,  ..., -1.6421e-05,
         -1.3500e-05, -1.1459e-05]], device='cuda:0')
Loss: 0.9765405654907227


Running epoch 0, step 1012, batch 1012
Sampled inputs[:2]: tensor([[   0,  292, 3030,  ..., 1231, 2156,  266],
        [   0,  271,  266,  ...,   14,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6030e-04,  3.3223e-04,  9.3423e-05,  ..., -2.5400e-05,
         -1.2356e-04,  2.3774e-06],
        [-8.0690e-06, -5.8711e-06,  4.8466e-06,  ..., -7.4506e-06,
         -5.8562e-06, -5.4501e-06],
        [ 8.0274e-05,  1.4096e-04, -5.7120e-05,  ...,  1.2679e-04,
          1.3258e-04,  8.0570e-05],
        [-1.4797e-05, -1.0639e-05,  9.4026e-06,  ..., -1.3635e-05,
         -1.0684e-05, -1.0327e-05],
        [-2.1547e-05, -1.6794e-05,  1.3694e-05,  ..., -2.0236e-05,
         -1.6525e-05, -1.3739e-05]], device='cuda:0')
Loss: 1.0045967102050781


Running epoch 0, step 1013, batch 1013
Sampled inputs[:2]: tensor([[   0, 1716,  271,  ...,  292,   78, 1365],
        [   0,  292, 2860,  ...,  266, 7000, 7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9284e-04,  2.8701e-04, -9.5538e-05,  ..., -7.5948e-05,
         -1.6547e-04, -1.1681e-04],
        [-9.6634e-06, -7.1675e-06,  5.9344e-06,  ..., -8.9556e-06,
         -7.0184e-06, -6.3553e-06],
        [ 7.6430e-05,  1.3774e-04, -5.4363e-05,  ...,  1.2314e-04,
          1.2975e-04,  7.8365e-05],
        [-1.7747e-05, -1.2994e-05,  1.1489e-05,  ..., -1.6406e-05,
         -1.2815e-05, -1.2077e-05],
        [-2.5690e-05, -2.0370e-05,  1.6630e-05,  ..., -2.4259e-05,
         -1.9744e-05, -1.5974e-05]], device='cuda:0')
Loss: 1.0230809450149536


Running epoch 0, step 1014, batch 1014
Sampled inputs[:2]: tensor([[    0,   380,  2114,  ...,   456, 28979,   472],
        [    0,   396,   221,  ...,  1279,   720,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6402e-04,  3.9360e-04, -5.1303e-05,  ..., -9.8546e-05,
         -1.5036e-04, -1.8692e-04],
        [-1.1288e-05, -8.3148e-06,  6.8843e-06,  ..., -1.0476e-05,
         -8.1956e-06, -7.5996e-06],
        [ 7.2585e-05,  1.3492e-04, -5.1919e-05,  ...,  1.1958e-04,
          1.2698e-04,  7.5534e-05],
        [-2.0638e-05, -1.4976e-05,  1.3322e-05,  ..., -1.9118e-05,
         -1.4931e-05, -1.4357e-05],
        [-3.0100e-05, -2.3663e-05,  1.9416e-05,  ..., -2.8372e-05,
         -2.3037e-05, -1.9044e-05]], device='cuda:0')
Loss: 0.9445963501930237


Running epoch 0, step 1015, batch 1015
Sampled inputs[:2]: tensor([[    0, 10348,  2994,  ...,   266, 24089, 10607],
        [    0,  1231,   278,  ...,    12,  2606,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2566e-04,  3.3088e-04,  8.3774e-05,  ..., -6.6551e-05,
         -1.1656e-04, -1.6725e-04],
        [-1.2875e-05, -9.5293e-06,  7.9945e-06,  ..., -1.1913e-05,
         -9.2238e-06, -8.4750e-06],
        [ 6.8786e-05,  1.3188e-04, -4.9103e-05,  ...,  1.1611e-04,
          1.2448e-04,  7.3433e-05],
        [-2.3559e-05, -1.7181e-05,  1.5467e-05,  ..., -2.1756e-05,
         -1.6809e-05, -1.6056e-05],
        [-3.4004e-05, -2.6911e-05,  2.2277e-05,  ..., -3.2037e-05,
         -2.5809e-05, -2.1070e-05]], device='cuda:0')
Loss: 1.0170023441314697
Graident accumulation at epoch 0, step 1015, batch 1015
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7921e-05,  1.1982e-04, -9.3911e-05,  ..., -8.7840e-05,
         -2.6872e-05, -1.1629e-04],
        [-1.4133e-05, -9.7079e-06,  8.2401e-06,  ..., -1.2575e-05,
         -8.1682e-06, -9.0967e-06],
        [ 9.6249e-05,  9.5968e-05, -7.0596e-05,  ...,  1.0516e-04,
          8.0352e-05,  4.0029e-05],
        [ 2.4671e-05,  3.5937e-05, -1.9880e-05,  ...,  3.5555e-05,
          3.3764e-05,  1.3827e-05],
        [-3.5954e-05, -2.6570e-05,  2.2149e-05,  ..., -3.1965e-05,
         -2.1995e-05, -2.1502e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1992e-08, 5.2840e-08, 6.3569e-08,  ..., 2.3080e-08, 1.4475e-07,
         3.4463e-08],
        [7.4368e-11, 4.3177e-11, 1.1465e-11,  ..., 5.0418e-11, 1.0913e-11,
         1.9679e-11],
        [3.5133e-09, 1.9994e-09, 1.0162e-09,  ..., 2.7048e-09, 8.4353e-10,
         9.1629e-10],
        [8.6563e-10, 6.4666e-10, 2.6393e-10,  ..., 7.2807e-10, 3.7499e-10,
         2.8204e-10],
        [3.4327e-10, 1.8564e-10, 4.8829e-11,  ..., 2.5108e-10, 4.1881e-11,
         8.8787e-11]], device='cuda:0')
optimizer state dict: 127.0
lr: [1.0978710514004527e-05, 1.0978710514004527e-05]
scheduler_last_epoch: 127


Running epoch 0, step 1016, batch 1016
Sampled inputs[:2]: tensor([[    0,   266,  2109,  ...,  6730, 11558,   287],
        [    0,   775,   266,  ...,   409,   328,  5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5895e-05, -9.7059e-05, -5.3518e-05,  ...,  3.9971e-06,
          3.5952e-05, -3.6144e-05],
        [-1.5199e-06, -1.1325e-06,  9.3132e-07,  ..., -1.4305e-06,
         -1.0803e-06, -9.5367e-07],
        [-3.7700e-06, -2.8908e-06,  2.5034e-06,  ..., -3.5167e-06,
         -2.6375e-06, -2.3246e-06],
        [-2.7865e-06, -2.0266e-06,  1.8179e-06,  ..., -2.6077e-06,
         -1.9521e-06, -1.8254e-06],
        [-3.9637e-06, -3.1590e-06,  2.6077e-06,  ..., -3.7700e-06,
         -2.9355e-06, -2.2799e-06]], device='cuda:0')
Loss: 1.0186474323272705


Running epoch 0, step 1017, batch 1017
Sampled inputs[:2]: tensor([[   0, 2771,   13,  ..., 1412,   35,   15],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7962e-04, -8.4177e-05,  2.1145e-06,  ...,  2.2889e-04,
          2.4531e-04,  5.7935e-06],
        [-3.0026e-06, -2.2799e-06,  1.8552e-06,  ..., -2.8759e-06,
         -2.2054e-06, -1.9148e-06],
        [-7.4804e-06, -5.7817e-06,  4.9919e-06,  ..., -7.0482e-06,
         -5.3346e-06, -4.6641e-06],
        [-5.6177e-06, -4.1276e-06,  3.6657e-06,  ..., -5.3197e-06,
         -4.0382e-06, -3.7476e-06],
        [-7.9274e-06, -6.3330e-06,  5.2601e-06,  ..., -7.5847e-06,
         -5.9456e-06, -4.6045e-06]], device='cuda:0')
Loss: 1.0438793897628784


Running epoch 0, step 1018, batch 1018
Sampled inputs[:2]: tensor([[   0, 1074, 1593,  ...,  992, 1810,  300],
        [   0, 2706,  292,  ...,   13, 8954,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6289e-04, -8.6609e-05,  1.6041e-04,  ...,  2.0891e-04,
          2.6952e-04,  3.3142e-05],
        [-4.4927e-06, -3.4496e-06,  2.7232e-06,  ..., -4.3735e-06,
         -3.4869e-06, -2.9504e-06],
        [-1.1146e-05, -8.7619e-06,  7.3165e-06,  ..., -1.0669e-05,
         -8.4341e-06, -7.1228e-06],
        [ 1.3761e-04,  1.6117e-04, -1.0718e-04,  ...,  1.2308e-04,
          1.6177e-04,  5.9348e-05],
        [-1.1981e-05, -9.7305e-06,  7.8231e-06,  ..., -1.1638e-05,
         -9.5069e-06, -7.1526e-06]], device='cuda:0')
Loss: 1.0199260711669922


Running epoch 0, step 1019, batch 1019
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  374,  221,  527],
        [   0,  623,   12,  ..., 4792, 6572,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4986e-04,  9.9814e-06,  1.6964e-04,  ...,  1.9242e-04,
          2.4474e-04,  3.6422e-05],
        [-6.0350e-06, -4.5896e-06,  3.6992e-06,  ..., -5.7966e-06,
         -4.5449e-06, -3.9488e-06],
        [-1.4886e-05, -1.1623e-05,  9.8646e-06,  ..., -1.4126e-05,
         -1.1027e-05, -9.5218e-06],
        [ 1.3474e-04,  1.5908e-04, -1.0525e-04,  ...,  1.2042e-04,
          1.5982e-04,  5.7395e-05],
        [-1.5914e-05, -1.2845e-05,  1.0490e-05,  ..., -1.5348e-05,
         -1.2398e-05, -9.5218e-06]], device='cuda:0')
Loss: 0.977418065071106


Running epoch 0, step 1020, batch 1020
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  6124,  1209,  ...,  1176,  3164,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0379e-05,  9.9814e-06,  1.5388e-04,  ...,  1.5854e-04,
          7.8418e-06, -1.7352e-04],
        [-7.5027e-06, -5.6475e-06,  4.6752e-06,  ..., -7.2345e-06,
         -5.5879e-06, -4.8317e-06],
        [-1.8388e-05, -1.4231e-05,  1.2368e-05,  ..., -1.7494e-05,
         -1.3441e-05, -1.1563e-05],
        [ 1.3198e-04,  1.5718e-04, -1.0332e-04,  ...,  1.1777e-04,
          1.5792e-04,  5.5652e-05],
        [-1.9640e-05, -1.5721e-05,  1.3113e-05,  ..., -1.9014e-05,
         -1.5154e-05, -1.1533e-05]], device='cuda:0')
Loss: 1.0053582191467285


Running epoch 0, step 1021, batch 1021
Sampled inputs[:2]: tensor([[   0, 6574, 1707,  ...,   14, 5077,   12],
        [   0, 5583,  598,  ...,  199,  395, 6551]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2931e-05,  4.2835e-05,  8.7758e-05,  ...,  2.9854e-04,
         -1.1396e-04, -1.5925e-04],
        [-9.0972e-06, -6.9067e-06,  5.6885e-06,  ..., -8.6501e-06,
         -6.6981e-06, -5.7928e-06],
        [-2.2352e-05, -1.7449e-05,  1.5035e-05,  ..., -2.1011e-05,
         -1.6168e-05, -1.3962e-05],
        [ 1.2892e-04,  1.5481e-04, -1.0130e-04,  ...,  1.1507e-04,
          1.5581e-04,  5.3715e-05],
        [-2.3752e-05, -1.9193e-05,  1.5855e-05,  ..., -2.2754e-05,
         -1.8179e-05, -1.3873e-05]], device='cuda:0')
Loss: 1.0062249898910522


Running epoch 0, step 1022, batch 1022
Sampled inputs[:2]: tensor([[    0,  7036,   278,  ...,   221,   290,   446],
        [    0,   935,   508,  ...,   287, 41582,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2885e-04,  4.0861e-05,  2.9040e-04,  ...,  4.0678e-04,
          3.2915e-05, -3.6493e-05],
        [-1.0490e-05, -8.0168e-06,  6.5751e-06,  ..., -1.0028e-05,
         -7.8306e-06, -6.7540e-06],
        [-2.5764e-05, -2.0236e-05,  1.7405e-05,  ..., -2.4304e-05,
         -1.8865e-05, -1.6198e-05],
        [ 1.2632e-04,  1.5280e-04, -9.9516e-05,  ...,  1.1251e-04,
          1.5368e-04,  5.1815e-05],
        [-2.7493e-05, -2.2352e-05,  1.8463e-05,  ..., -2.6405e-05,
         -2.1249e-05, -1.6153e-05]], device='cuda:0')
Loss: 0.9704193472862244


Running epoch 0, step 1023, batch 1023
Sampled inputs[:2]: tensor([[    0,   266,  1513,  ...,   367,  1941,   344],
        [    0,    12, 17340,  ...,   408,  1550,  2415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3711e-04, -4.8089e-05,  2.1851e-04,  ...,  2.8890e-04,
          2.2520e-04,  2.6647e-06],
        [-1.2025e-05, -9.3430e-06,  7.5586e-06,  ..., -1.1526e-05,
         -9.0450e-06, -7.6629e-06],
        [-2.9474e-05, -2.3544e-05,  1.9938e-05,  ..., -2.7910e-05,
         -2.1785e-05, -1.8388e-05],
        [ 1.2350e-04,  1.5040e-04, -9.7616e-05,  ...,  1.0977e-04,
          1.5145e-04,  5.0049e-05],
        [-3.1576e-05, -2.6122e-05,  2.1234e-05,  ..., -3.0458e-05,
         -2.4647e-05, -1.8418e-05]], device='cuda:0')
Loss: 1.0167139768600464
Graident accumulation at epoch 0, step 1023, batch 1023
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0022,  0.0234, -0.0192],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4178e-06,  1.0303e-04, -6.2670e-05,  ..., -5.0166e-05,
         -1.6641e-06, -1.0439e-04],
        [-1.3922e-05, -9.6714e-06,  8.1719e-06,  ..., -1.2470e-05,
         -8.2559e-06, -8.9533e-06],
        [ 8.3676e-05,  8.4017e-05, -6.1542e-05,  ...,  9.1852e-05,
          7.0138e-05,  3.4188e-05],
        [ 3.4554e-05,  4.7383e-05, -2.7654e-05,  ...,  4.2976e-05,
          4.5533e-05,  1.7449e-05],
        [-3.5516e-05, -2.6525e-05,  2.2057e-05,  ..., -3.1814e-05,
         -2.2260e-05, -2.1194e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1959e-08, 5.2790e-08, 6.3553e-08,  ..., 2.3141e-08, 1.4465e-07,
         3.4429e-08],
        [7.4438e-11, 4.3221e-11, 1.1511e-11,  ..., 5.0501e-11, 1.0984e-11,
         1.9718e-11],
        [3.5106e-09, 1.9979e-09, 1.0156e-09,  ..., 2.7028e-09, 8.4316e-10,
         9.1571e-10],
        [8.8002e-10, 6.6864e-10, 2.7320e-10,  ..., 7.3939e-10, 3.9755e-10,
         2.8426e-10],
        [3.4392e-10, 1.8613e-10, 4.9231e-11,  ..., 2.5176e-10, 4.2447e-11,
         8.9037e-11]], device='cuda:0')
optimizer state dict: 128.0
lr: [1.085561572490406e-05, 1.085561572490406e-05]
scheduler_last_epoch: 128


Running epoch 0, step 1024, batch 1024
Sampled inputs[:2]: tensor([[    0,  4175,   437,  ...,  1700,    14,   381],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2945e-05, -5.5303e-05, -6.9429e-05,  ...,  6.2960e-06,
         -1.6321e-05, -7.9363e-05],
        [-1.5572e-06, -1.2368e-06,  1.0654e-06,  ..., -1.4007e-06,
         -1.0133e-06, -8.7544e-07],
        [-3.9041e-06, -3.2336e-06,  2.8312e-06,  ..., -3.5316e-06,
         -2.5779e-06, -2.2203e-06],
        [-2.9355e-06, -2.3097e-06,  2.1309e-06,  ..., -2.6226e-06,
         -1.8850e-06, -1.7509e-06],
        [-3.8445e-06, -3.3528e-06,  2.7716e-06,  ..., -3.6210e-06,
         -2.7567e-06, -2.0713e-06]], device='cuda:0')
Loss: 1.0235363245010376


Running epoch 0, step 1025, batch 1025
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0,    14, 13078,  ...,  1994,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6000e-05, -7.1529e-05,  4.3494e-05,  ...,  3.8478e-05,
          3.0852e-04,  2.4983e-05],
        [-3.2037e-06, -2.3916e-06,  1.9893e-06,  ..., -2.8983e-06,
         -2.1830e-06, -1.9707e-06],
        [-7.8678e-06, -6.1840e-06,  5.2899e-06,  ..., -7.1377e-06,
         -5.4091e-06, -4.7982e-06],
        [-5.7966e-06, -4.2915e-06,  3.8445e-06,  ..., -5.2303e-06,
         -3.9414e-06, -3.7327e-06],
        [-8.1360e-06, -6.6459e-06,  5.3942e-06,  ..., -7.5847e-06,
         -5.9754e-06, -4.6939e-06]], device='cuda:0')
Loss: 0.9896639585494995


Running epoch 0, step 1026, batch 1026
Sampled inputs[:2]: tensor([[    0,    14,  3080,  ..., 14737,    13, 17982],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4695e-04, -4.8147e-05,  1.8155e-04,  ...,  1.6575e-04,
          5.6304e-04,  2.2207e-04],
        [-4.8056e-06, -3.6135e-06,  2.8871e-06,  ..., -4.4256e-06,
         -3.3602e-06, -3.1106e-06],
        [-1.1921e-05, -9.4473e-06,  7.7784e-06,  ..., -1.1012e-05,
         -8.4043e-06, -7.6443e-06],
        [-8.6278e-06, -6.4224e-06,  5.5283e-06,  ..., -7.9125e-06,
         -6.0126e-06, -5.8040e-06],
        [-1.2487e-05, -1.0207e-05,  8.0317e-06,  ..., -1.1817e-05,
         -9.3430e-06, -7.6145e-06]], device='cuda:0')
Loss: 1.011980652809143


Running epoch 0, step 1027, batch 1027
Sampled inputs[:2]: tensor([[    0,  3445,   328,  ...,   278, 12323,   554],
        [    0,   344,   259,  ..., 47553,   287, 28978]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8144e-04, -6.8986e-05,  2.5019e-04,  ...,  3.5941e-04,
          6.8677e-04,  1.2944e-04],
        [-6.2287e-06, -4.6864e-06,  3.8408e-06,  ..., -5.7444e-06,
         -4.3660e-06, -3.9972e-06],
        [-1.5393e-05, -1.2144e-05,  1.0312e-05,  ..., -1.4186e-05,
         -1.0818e-05, -9.7007e-06],
        [-1.1310e-05, -8.3745e-06,  7.4655e-06,  ..., -1.0371e-05,
         -7.8976e-06, -7.5698e-06],
        [-1.6183e-05, -1.3202e-05,  1.0699e-05,  ..., -1.5259e-05,
         -1.2085e-05, -9.6112e-06]], device='cuda:0')
Loss: 0.9827576279640198


Running epoch 0, step 1028, batch 1028
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,   806,  1255,  ...,   474,   221,   380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5334e-04, -8.6027e-05,  2.8007e-04,  ...,  4.0315e-04,
          7.2977e-04,  1.0130e-05],
        [-7.7412e-06, -5.8264e-06,  4.8093e-06,  ..., -7.1004e-06,
         -5.3644e-06, -4.8243e-06],
        [-1.9088e-05, -1.5065e-05,  1.2890e-05,  ..., -1.7494e-05,
         -1.3277e-05, -1.1697e-05],
        [-1.4111e-05, -1.0476e-05,  9.3728e-06,  ..., -1.2875e-05,
         -9.7528e-06, -9.1791e-06],
        [-1.9923e-05, -1.6302e-05,  1.3292e-05,  ..., -1.8686e-05,
         -1.4752e-05, -1.1474e-05]], device='cuda:0')
Loss: 0.9986618161201477


Running epoch 0, step 1029, batch 1029
Sampled inputs[:2]: tensor([[    0,  5105,   271,  ...,   308,  3056,  3640],
        [    0, 27366,   504,  ...,  1358,   365,  6883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8839e-04, -1.9164e-04,  2.2494e-04,  ...,  5.6435e-04,
          6.5426e-04, -9.6019e-05],
        [-9.2685e-06, -6.9663e-06,  5.7705e-06,  ..., -8.5235e-06,
         -6.4597e-06, -5.8599e-06],
        [-2.2903e-05, -1.8016e-05,  1.5482e-05,  ..., -2.1011e-05,
         -1.5974e-05, -1.4201e-05],
        [-1.6928e-05, -1.2502e-05,  1.1265e-05,  ..., -1.5467e-05,
         -1.1735e-05, -1.1161e-05],
        [-2.4006e-05, -1.9580e-05,  1.6034e-05,  ..., -2.2531e-05,
         -1.7822e-05, -1.3977e-05]], device='cuda:0')
Loss: 0.9979729652404785


Running epoch 0, step 1030, batch 1030
Sampled inputs[:2]: tensor([[    0,  2355,  2728,  ...,   554,  9025,   368],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2356e-04, -3.7704e-04,  1.9017e-04,  ...,  8.8703e-04,
          7.8523e-04, -1.7558e-07],
        [-1.0863e-05, -8.2329e-06,  6.7167e-06,  ..., -9.9912e-06,
         -7.6219e-06, -6.8657e-06],
        [-2.6897e-05, -2.1249e-05,  1.8016e-05,  ..., -2.4661e-05,
         -1.8880e-05, -1.6674e-05],
        [-1.9774e-05, -1.4722e-05,  1.3046e-05,  ..., -1.8075e-05,
         -1.3806e-05, -1.3039e-05],
        [-2.8208e-05, -2.3127e-05,  1.8701e-05,  ..., -2.6464e-05,
         -2.1040e-05, -1.6436e-05]], device='cuda:0')
Loss: 1.0246036052703857


Running epoch 0, step 1031, batch 1031
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   199,  3289,  ...,  2269,  6476,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7229e-04, -3.5189e-04,  1.0050e-04,  ...,  9.1948e-04,
          8.5813e-04, -3.6327e-05],
        [-1.2659e-05, -9.0897e-06,  7.4469e-06,  ..., -1.1556e-05,
         -8.7842e-06, -8.4899e-06],
        [-3.1278e-05, -2.3529e-05,  2.0027e-05,  ..., -2.8431e-05,
         -2.1726e-05, -2.0489e-05],
        [-2.2829e-05, -1.6138e-05,  1.4357e-05,  ..., -2.0713e-05,
         -1.5788e-05, -1.5840e-05],
        [-3.3155e-05, -2.5764e-05,  2.0996e-05,  ..., -3.0786e-05,
         -2.4348e-05, -2.0579e-05]], device='cuda:0')
Loss: 0.9626230597496033
Graident accumulation at epoch 0, step 1031, batch 1031
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0022,  0.0235, -0.0192],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5053e-05,  5.7540e-05, -4.6352e-05,  ...,  4.6799e-05,
          8.4315e-05, -9.7584e-05],
        [-1.3796e-05, -9.6132e-06,  8.0994e-06,  ..., -1.2379e-05,
         -8.3087e-06, -8.9069e-06],
        [ 7.2181e-05,  7.3263e-05, -5.3385e-05,  ...,  7.9823e-05,
          6.0952e-05,  2.8720e-05],
        [ 2.8816e-05,  4.1031e-05, -2.3453e-05,  ...,  3.6607e-05,
          3.9401e-05,  1.4120e-05],
        [-3.5280e-05, -2.6449e-05,  2.1951e-05,  ..., -3.1712e-05,
         -2.2469e-05, -2.1132e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1937e-08, 5.2861e-08, 6.3500e-08,  ..., 2.3963e-08, 1.4524e-07,
         3.4396e-08],
        [7.4524e-11, 4.3260e-11, 1.1555e-11,  ..., 5.0584e-11, 1.1050e-11,
         1.9771e-11],
        [3.5081e-09, 1.9965e-09, 1.0150e-09,  ..., 2.7009e-09, 8.4279e-10,
         9.1522e-10],
        [8.7966e-10, 6.6823e-10, 2.7313e-10,  ..., 7.3908e-10, 3.9740e-10,
         2.8423e-10],
        [3.4468e-10, 1.8661e-10, 4.9622e-11,  ..., 2.5245e-10, 4.2997e-11,
         8.9372e-11]], device='cuda:0')
optimizer state dict: 129.0
lr: [1.0732390190252052e-05, 1.0732390190252052e-05]
scheduler_last_epoch: 129


Running epoch 0, step 1032, batch 1032
Sampled inputs[:2]: tensor([[    0, 36122,  1085,  ...,  6231,     9,  7794],
        [    0,   278,  2088,  ...,    69,    14,    71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7967e-05, -3.1885e-06,  1.5890e-04,  ...,  1.9033e-04,
         -1.0022e-04,  4.7086e-05],
        [-1.5572e-06, -1.0580e-06,  1.0282e-06,  ..., -1.3635e-06,
         -8.9779e-07, -8.9034e-07],
        [-3.9041e-06, -2.7865e-06,  2.7567e-06,  ..., -3.3975e-06,
         -2.2501e-06, -2.1607e-06],
        [-2.8461e-06, -1.9073e-06,  1.9819e-06,  ..., -2.5034e-06,
         -1.6466e-06, -1.7062e-06],
        [-3.9041e-06, -2.8908e-06,  2.7418e-06,  ..., -3.4869e-06,
         -2.4140e-06, -2.0266e-06]], device='cuda:0')
Loss: 1.0063495635986328


Running epoch 0, step 1033, batch 1033
Sampled inputs[:2]: tensor([[   0,  616, 4935,  ...,   89, 4448,  271],
        [   0, 5896,  352,  ..., 1168,  767, 1390]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6203e-05, -7.6312e-05,  1.6995e-04,  ...,  2.9531e-04,
          2.9413e-04,  1.2112e-04],
        [-3.1665e-06, -2.1756e-06,  2.0191e-06,  ..., -2.7642e-06,
         -2.0079e-06, -1.9111e-06],
        [-7.9274e-06, -5.6922e-06,  5.4240e-06,  ..., -6.8694e-06,
         -5.0217e-06, -4.6492e-06],
        [-5.7071e-06, -3.8594e-06,  3.8520e-06,  ..., -4.9770e-06,
         -3.6135e-06, -3.5986e-06],
        [-8.1360e-06, -6.0797e-06,  5.5134e-06,  ..., -7.2420e-06,
         -5.4985e-06, -4.5002e-06]], device='cuda:0')
Loss: 1.0375300645828247


Running epoch 0, step 1034, batch 1034
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0, 1774, 1781,  ..., 4685,  409, 4614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5205e-04,  2.9356e-05,  2.8363e-04,  ...,  2.9265e-04,
          4.2969e-04,  2.0174e-04],
        [-4.7013e-06, -3.2932e-06,  3.0249e-06,  ..., -4.1500e-06,
         -2.9691e-06, -2.8200e-06],
        [-1.1742e-05, -8.5831e-06,  8.1062e-06,  ..., -1.0297e-05,
         -7.4059e-06, -6.8694e-06],
        [-8.5980e-06, -5.9009e-06,  5.8487e-06,  ..., -7.5549e-06,
         -5.3942e-06, -5.3793e-06],
        [-1.1981e-05, -9.0897e-06,  8.1807e-06,  ..., -1.0759e-05,
         -8.0466e-06, -6.5863e-06]], device='cuda:0')
Loss: 1.003379464149475


Running epoch 0, step 1035, batch 1035
Sampled inputs[:2]: tensor([[    0,    12,   344,  ...,    14,  2295,   516],
        [    0,    13,  2497,  ..., 27714,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6469e-04,  8.9215e-05,  4.2683e-04,  ...,  3.5431e-04,
          7.8460e-04,  3.1419e-04],
        [-6.2585e-06, -4.4182e-06,  4.0680e-06,  ..., -5.5283e-06,
         -3.9525e-06, -3.6918e-06],
        [-1.5616e-05, -1.1504e-05,  1.0893e-05,  ..., -1.3739e-05,
         -9.8497e-06, -9.0450e-06],
        [-1.1444e-05, -7.9125e-06,  7.8753e-06,  ..., -1.0073e-05,
         -7.1749e-06, -7.0706e-06],
        [-1.5855e-05, -1.2159e-05,  1.0923e-05,  ..., -1.4305e-05,
         -1.0699e-05, -8.6278e-06]], device='cuda:0')
Loss: 1.001121997833252


Running epoch 0, step 1036, batch 1036
Sampled inputs[:2]: tensor([[   0,   12, 2085,  ...,  287,  593, 4137],
        [   0, 2577,  995,  ..., 6104,   14, 2032]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5028e-05,  8.8745e-05,  4.4547e-04,  ...,  3.5596e-04,
          7.1940e-04,  3.0495e-04],
        [-7.8231e-06, -5.5209e-06,  5.1633e-06,  ..., -6.9067e-06,
         -4.8503e-06, -4.6231e-06],
        [-1.9491e-05, -1.4395e-05,  1.3798e-05,  ..., -1.7196e-05,
         -1.2144e-05, -1.1384e-05],
        [-1.4335e-05, -9.9391e-06,  1.0036e-05,  ..., -1.2621e-05,
         -8.8289e-06, -8.8811e-06],
        [-1.9670e-05, -1.5140e-05,  1.3709e-05,  ..., -1.7822e-05,
         -1.3158e-05, -1.0788e-05]], device='cuda:0')
Loss: 0.9895782470703125


Running epoch 0, step 1037, batch 1037
Sampled inputs[:2]: tensor([[    0, 13706,  1862,  ...,   275,  1036, 42948],
        [    0,   756,   943,  ...,  4016,    12,   627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9709e-05,  1.1141e-04,  5.6980e-04,  ...,  3.7270e-04,
          1.0604e-03,  4.0348e-04],
        [-9.2983e-06, -6.4634e-06,  6.1318e-06,  ..., -8.1658e-06,
         -5.7444e-06, -5.5768e-06],
        [-2.3142e-05, -1.6853e-05,  1.6436e-05,  ..., -2.0251e-05,
         -1.4335e-05, -1.3620e-05],
        [-1.7121e-05, -1.1668e-05,  1.2003e-05,  ..., -1.4976e-05,
         -1.0513e-05, -1.0736e-05],
        [-2.3335e-05, -1.7703e-05,  1.6302e-05,  ..., -2.0981e-05,
         -1.5512e-05, -1.2890e-05]], device='cuda:0')
Loss: 0.9934329390525818


Running epoch 0, step 1038, batch 1038
Sampled inputs[:2]: tensor([[   0,  944,  278,  ..., 5755,  292,  221],
        [   0, 8290,  391,  ...,  298, 1253,    7]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6144e-05,  1.0326e-05,  7.0211e-04,  ...,  3.3009e-04,
          1.1541e-03,  3.6282e-04],
        [-1.0900e-05, -7.5139e-06,  7.0557e-06,  ..., -9.5889e-06,
         -6.7800e-06, -6.6794e-06],
        [-2.7165e-05, -1.9610e-05,  1.8954e-05,  ..., -2.3782e-05,
         -1.6913e-05, -1.6332e-05],
        [-1.9923e-05, -1.3478e-05,  1.3724e-05,  ..., -1.7434e-05,
         -1.2293e-05, -1.2733e-05],
        [-2.7478e-05, -2.0608e-05,  1.8865e-05,  ..., -2.4676e-05,
         -1.8314e-05, -1.5512e-05]], device='cuda:0')
Loss: 0.9923440217971802


Running epoch 0, step 1039, batch 1039
Sampled inputs[:2]: tensor([[    0,   642,   287,  ...,   800,    12,  3338],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3357e-05, -1.3030e-05,  6.7263e-04,  ...,  3.6522e-04,
          1.1073e-03,  2.9764e-04],
        [-1.2428e-05, -8.5495e-06,  8.1062e-06,  ..., -1.0915e-05,
         -7.6070e-06, -7.4990e-06],
        [-3.0965e-05, -2.2292e-05,  2.1741e-05,  ..., -2.7090e-05,
         -1.8984e-05, -1.8358e-05],
        [-2.2814e-05, -1.5385e-05,  1.5810e-05,  ..., -1.9938e-05,
         -1.3851e-05, -1.4372e-05],
        [-3.1233e-05, -2.3410e-05,  2.1577e-05,  ..., -2.8029e-05,
         -2.0534e-05, -1.7375e-05]], device='cuda:0')
Loss: 1.0083370208740234
Graident accumulation at epoch 0, step 1039, batch 1039
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0022,  0.0235, -0.0192],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0156,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.2116e-06,  5.0483e-05,  2.5546e-05,  ...,  7.8641e-05,
          1.8661e-04, -5.8061e-05],
        [-1.3659e-05, -9.5069e-06,  8.1001e-06,  ..., -1.2233e-05,
         -8.2386e-06, -8.7661e-06],
        [ 6.1866e-05,  6.3707e-05, -4.5873e-05,  ...,  6.9132e-05,
          5.2958e-05,  2.4012e-05],
        [ 2.3653e-05,  3.5389e-05, -1.9526e-05,  ...,  3.0953e-05,
          3.4075e-05,  1.1271e-05],
        [-3.4875e-05, -2.6145e-05,  2.1914e-05,  ..., -3.1343e-05,
         -2.2276e-05, -2.0756e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1889e-08, 5.2808e-08, 6.3889e-08,  ..., 2.4072e-08, 1.4632e-07,
         3.4450e-08],
        [7.4604e-11, 4.3290e-11, 1.1609e-11,  ..., 5.0652e-11, 1.1097e-11,
         1.9807e-11],
        [3.5056e-09, 1.9950e-09, 1.0145e-09,  ..., 2.6990e-09, 8.4231e-10,
         9.1464e-10],
        [8.7930e-10, 6.6780e-10, 2.7311e-10,  ..., 7.3874e-10, 3.9720e-10,
         2.8415e-10],
        [3.4531e-10, 1.8697e-10, 5.0038e-11,  ..., 2.5299e-10, 4.3376e-11,
         8.9584e-11]], device='cuda:0')
optimizer state dict: 130.0
lr: [1.060905273998585e-05, 1.060905273998585e-05]
scheduler_last_epoch: 130


Running epoch 0, step 1040, batch 1040
Sampled inputs[:2]: tensor([[    0,   221,   380,  ..., 10022,    12,   461],
        [    0,  3825,  1626,  ...,  5096,  3775,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1981e-04,  6.4809e-06, -8.7469e-05,  ...,  1.1317e-04,
         -2.0532e-04,  3.7768e-06],
        [-1.5944e-06, -9.8348e-07,  1.1250e-06,  ..., -1.3039e-06,
         -7.9349e-07, -8.7172e-07],
        [-4.0233e-06, -2.5928e-06,  3.0100e-06,  ..., -3.2932e-06,
         -2.0117e-06, -2.1756e-06],
        [-2.9802e-06, -1.7956e-06,  2.2054e-06,  ..., -2.4140e-06,
         -1.4603e-06, -1.7062e-06],
        [-3.7551e-06, -2.5481e-06,  2.7716e-06,  ..., -3.1590e-06,
         -2.0415e-06, -1.8924e-06]], device='cuda:0')
Loss: 0.9771875143051147


Running epoch 0, step 1041, batch 1041
Sampled inputs[:2]: tensor([[    0,  1603,   694,  ...,    36,    18,   298],
        [    0,   352, 13159,  ...,  3111,   394,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7573e-05, -1.3734e-05, -7.8854e-05,  ...,  2.1728e-04,
         -2.8639e-04,  1.8555e-05],
        [-3.1590e-06, -1.9744e-06,  2.0303e-06,  ..., -2.6822e-06,
         -1.7248e-06, -1.9744e-06],
        [-7.8976e-06, -5.1409e-06,  5.4687e-06,  ..., -6.6161e-06,
         -4.2468e-06, -4.7535e-06],
        [-5.7966e-06, -3.5241e-06,  3.9712e-06,  ..., -4.8429e-06,
         -3.0771e-06, -3.7029e-06],
        [-7.7486e-06, -5.2452e-06,  5.2899e-06,  ..., -6.6459e-06,
         -4.5002e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9813314080238342


Running epoch 0, step 1042, batch 1042
Sampled inputs[:2]: tensor([[   0, 3532,  300,  ...,   12,  461,  806],
        [   0,  271, 3421,  ...,  306,  472,  346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6923e-05, -2.2156e-05, -3.5021e-05,  ...,  3.4167e-04,
          3.0355e-05,  1.7887e-04],
        [-4.7088e-06, -2.9169e-06,  2.9430e-06,  ..., -4.0084e-06,
         -2.5518e-06, -2.9877e-06],
        [-1.1832e-05, -7.6592e-06,  8.0168e-06,  ..., -9.9093e-06,
         -6.3032e-06, -7.2122e-06],
        [-8.6576e-06, -5.1931e-06,  5.7667e-06,  ..., -7.2569e-06,
         -4.5747e-06, -5.6550e-06],
        [-1.1683e-05, -7.8529e-06,  7.8082e-06,  ..., -9.9987e-06,
         -6.7055e-06, -6.6906e-06]], device='cuda:0')
Loss: 0.9649131298065186


Running epoch 0, step 1043, batch 1043
Sampled inputs[:2]: tensor([[   0,  271, 3616,  ...,   12, 1348, 5037],
        [   0,  437,  266,  ...,  630,  586,  824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4356e-04,  2.8015e-05, -4.9403e-05,  ...,  2.7640e-04,
          1.7210e-04,  1.7887e-04],
        [-6.3106e-06, -4.0345e-06,  3.8855e-06,  ..., -5.3570e-06,
         -3.4831e-06, -3.9227e-06],
        [-1.5825e-05, -1.0535e-05,  1.0550e-05,  ..., -1.3247e-05,
         -8.5980e-06, -9.5069e-06],
        [-1.1578e-05, -7.2047e-06,  7.5847e-06,  ..., -9.7007e-06,
         -6.2287e-06, -7.4208e-06],
        [-1.5616e-05, -1.0803e-05,  1.0267e-05,  ..., -1.3381e-05,
         -9.1344e-06, -8.8215e-06]], device='cuda:0')
Loss: 0.9934015274047852


Running epoch 0, step 1044, batch 1044
Sampled inputs[:2]: tensor([[   0,  380, 1075,  ...,  298,  365, 4920],
        [   0,  365,  925,  ...,  909,  598,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1981e-04,  4.8155e-05, -8.3489e-05,  ...,  2.4205e-04,
          1.9021e-04,  1.4019e-04],
        [-8.0913e-06, -5.1595e-06,  4.7758e-06,  ..., -6.8322e-06,
         -4.4741e-06, -5.0776e-06],
        [-2.0176e-05, -1.3441e-05,  1.2979e-05,  ..., -1.6823e-05,
         -1.1042e-05, -1.2219e-05],
        [-1.4529e-05, -9.0525e-06,  9.1717e-06,  ..., -1.2159e-05,
         -7.8902e-06, -9.3877e-06],
        [-2.0176e-05, -1.3933e-05,  1.2815e-05,  ..., -1.7196e-05,
         -1.1846e-05, -1.1504e-05]], device='cuda:0')
Loss: 0.9663382768630981


Running epoch 0, step 1045, batch 1045
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  1751,   287,  ...,  6079,  1059,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6448e-04,  4.5015e-05, -1.1981e-05,  ...,  1.7115e-04,
          2.7732e-04,  1.1772e-04],
        [-9.6709e-06, -6.3293e-06,  5.7891e-06,  ..., -8.1733e-06,
         -5.4277e-06, -5.9269e-06],
        [-2.4140e-05, -1.6510e-05,  1.5706e-05,  ..., -2.0221e-05,
         -1.3456e-05, -1.4350e-05],
        [-1.7390e-05, -1.1154e-05,  1.1094e-05,  ..., -1.4588e-05,
         -9.6038e-06, -1.0997e-05],
        [-2.4170e-05, -1.7136e-05,  1.5527e-05,  ..., -2.0698e-05,
         -1.4454e-05, -1.3530e-05]], device='cuda:0')
Loss: 1.0295627117156982


Running epoch 0, step 1046, batch 1046
Sampled inputs[:2]: tensor([[    0,  1176,    13,  ...,  1919,   221,   380],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0370e-04,  5.5482e-05,  7.0621e-05,  ...,  1.1124e-04,
          2.5827e-04,  6.9531e-05],
        [-1.1332e-05, -7.4916e-06,  6.7651e-06,  ..., -9.6112e-06,
         -6.4410e-06, -6.9104e-06],
        [-2.8163e-05, -1.9461e-05,  1.8254e-05,  ..., -2.3693e-05,
         -1.5929e-05, -1.6689e-05],
        [-2.0280e-05, -1.3165e-05,  1.2882e-05,  ..., -1.7092e-05,
         -1.1377e-05, -1.2778e-05],
        [-2.8342e-05, -2.0280e-05,  1.8135e-05,  ..., -2.4378e-05,
         -1.7166e-05, -1.5825e-05]], device='cuda:0')
Loss: 0.9840993285179138


Running epoch 0, step 1047, batch 1047
Sampled inputs[:2]: tensor([[   0,  266, 4411,  ...,  368, 6388, 3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4043e-05, -1.8598e-04, -3.1997e-05,  ...,  1.1250e-04,
          1.7357e-04, -1.0533e-04],
        [-1.2875e-05, -8.3447e-06,  7.7263e-06,  ..., -1.0908e-05,
         -7.2420e-06, -7.9013e-06],
        [-3.1978e-05, -2.1681e-05,  2.0802e-05,  ..., -2.6867e-05,
         -1.7896e-05, -1.9044e-05],
        [-2.3022e-05, -1.4618e-05,  1.4700e-05,  ..., -1.9372e-05,
         -1.2770e-05, -1.4611e-05],
        [-3.2142e-05, -2.2590e-05,  2.0638e-05,  ..., -2.7597e-05,
         -1.9267e-05, -1.8001e-05]], device='cuda:0')
Loss: 0.966343343257904
Graident accumulation at epoch 0, step 1047, batch 1047
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0861e-06,  2.6837e-05,  1.9792e-05,  ...,  8.2027e-05,
          1.8531e-04, -6.2789e-05],
        [-1.3581e-05, -9.3906e-06,  8.0627e-06,  ..., -1.2100e-05,
         -8.1389e-06, -8.6797e-06],
        [ 5.2482e-05,  5.5168e-05, -3.9205e-05,  ...,  5.9532e-05,
          4.5873e-05,  1.9707e-05],
        [ 1.8985e-05,  3.0388e-05, -1.6104e-05,  ...,  2.5920e-05,
          2.9391e-05,  8.6828e-06],
        [-3.4602e-05, -2.5789e-05,  2.1786e-05,  ..., -3.0969e-05,
         -2.1975e-05, -2.0481e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1839e-08, 5.2790e-08, 6.3826e-08,  ..., 2.4061e-08, 1.4621e-07,
         3.4427e-08],
        [7.4695e-11, 4.3316e-11, 1.1657e-11,  ..., 5.0720e-11, 1.1138e-11,
         1.9850e-11],
        [3.5031e-09, 1.9935e-09, 1.0139e-09,  ..., 2.6970e-09, 8.4179e-10,
         9.1409e-10],
        [8.7895e-10, 6.6734e-10, 2.7305e-10,  ..., 7.3838e-10, 3.9696e-10,
         2.8408e-10],
        [3.4600e-10, 1.8730e-10, 5.0414e-11,  ..., 2.5350e-10, 4.3704e-11,
         8.9819e-11]], device='cuda:0')
optimizer state dict: 131.0
lr: [1.0485622221144482e-05, 1.0485622221144482e-05]
scheduler_last_epoch: 131
End of epoch 0 | Validation PPL: 7.236249797152205 | Learning rate: 1.0485622221144482e-05
[2025-03-25 13:47:48,486] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
End of epoch checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/end_of_epoch_checkpoint.0, AFTER epoch 0


Running epoch 1, step 1048, batch 0
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,    13, 23904,  ...,   560,  8840,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0527e-05, -5.4738e-05,  4.7567e-05,  ...,  2.0448e-05,
         -4.3968e-05, -6.9241e-05],
        [-1.6615e-06, -1.0878e-06,  1.0282e-06,  ..., -1.3560e-06,
         -9.2387e-07, -1.0133e-06],
        [-4.1723e-06, -2.8759e-06,  2.7567e-06,  ..., -3.4273e-06,
         -2.3544e-06, -2.5183e-06],
        [-2.9206e-06, -1.9073e-06,  1.8999e-06,  ..., -2.3991e-06,
         -1.6466e-06, -1.8552e-06],
        [-4.1723e-06, -2.9653e-06,  2.7418e-06,  ..., -3.5018e-06,
         -2.5034e-06, -2.3991e-06]], device='cuda:0')
Loss: 0.9973124265670776


Running epoch 1, step 1049, batch 1
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,  1086,  5564,  ..., 29319, 32982,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4860e-05,  9.6576e-05,  3.0412e-04,  ...,  7.9070e-05,
          2.0971e-05,  4.1117e-05],
        [-3.3304e-06, -2.2501e-06,  1.9781e-06,  ..., -2.7269e-06,
         -1.8626e-06, -2.1383e-06],
        [-8.5235e-06, -6.0946e-06,  5.4240e-06,  ..., -7.0333e-06,
         -4.8578e-06, -5.4091e-06],
        [-5.9009e-06, -3.9637e-06,  3.6880e-06,  ..., -4.8280e-06,
         -3.3081e-06, -3.9265e-06],
        [-8.5533e-06, -6.2734e-06,  5.3793e-06,  ..., -7.1824e-06,
         -5.1409e-06, -5.1558e-06]], device='cuda:0')
Loss: 0.9933308959007263


Running epoch 1, step 1050, batch 2
Sampled inputs[:2]: tensor([[   0, 9818,  347,  ...,  413, 7359,   15],
        [   0, 1690, 2558,  ..., 2025,   12,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6589e-05,  1.3402e-04,  5.4127e-04,  ...,  5.2153e-05,
          9.8615e-05,  1.3905e-04],
        [-5.0217e-06, -3.4273e-06,  2.8424e-06,  ..., -4.1574e-06,
         -2.9206e-06, -3.2783e-06],
        [-1.2875e-05, -9.2685e-06,  7.8380e-06,  ..., -1.0714e-05,
         -7.5996e-06, -8.3148e-06],
        [-8.8662e-06, -6.0052e-06,  5.2899e-06,  ..., -7.3165e-06,
         -5.1484e-06, -5.9977e-06],
        [-1.2964e-05, -9.5367e-06,  7.7933e-06,  ..., -1.0967e-05,
         -8.0466e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.0235569477081299


Running epoch 1, step 1051, batch 3
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,    12,   342,  ...,  3458,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7136e-06,  4.2352e-04,  6.5193e-04,  ..., -6.3362e-05,
          6.8638e-05,  1.0635e-04],
        [-6.6534e-06, -4.3809e-06,  3.7402e-06,  ..., -5.5656e-06,
         -3.8706e-06, -4.3064e-06],
        [-1.6809e-05, -1.1727e-05,  1.0192e-05,  ..., -1.4111e-05,
         -9.9093e-06, -1.0744e-05],
        [-1.1697e-05, -7.6145e-06,  6.9290e-06,  ..., -9.7603e-06,
         -6.7949e-06, -7.8455e-06],
        [-1.7047e-05, -1.2144e-05,  1.0192e-05,  ..., -1.4544e-05,
         -1.0550e-05, -1.0341e-05]], device='cuda:0')
Loss: 0.9607057571411133


Running epoch 1, step 1052, batch 4
Sampled inputs[:2]: tensor([[   0, 4337, 2057,  ..., 3020, 1722,  369],
        [   0, 9657,  300,  ...,   12,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3456e-05,  4.9167e-04,  5.8156e-04,  ..., -1.0851e-04,
         -2.5134e-04,  3.3650e-05],
        [-8.0913e-06, -5.1968e-06,  4.7982e-06,  ..., -6.7949e-06,
         -4.6305e-06, -5.1409e-06],
        [-2.0444e-05, -1.3888e-05,  1.3053e-05,  ..., -1.7181e-05,
         -1.1802e-05, -1.2800e-05],
        [-1.4469e-05, -9.1121e-06,  9.1046e-06,  ..., -1.2085e-05,
         -8.2180e-06, -9.5218e-06],
        [-2.0534e-05, -1.4320e-05,  1.2904e-05,  ..., -1.7554e-05,
         -1.2502e-05, -1.2167e-05]], device='cuda:0')
Loss: 0.9554687738418579


Running epoch 1, step 1053, batch 5
Sampled inputs[:2]: tensor([[    0,  2328,   271,  ...,   706,    13,  8961],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0438e-04,  4.8843e-04,  4.9966e-04,  ..., -1.6577e-04,
         -3.2611e-04,  1.2447e-04],
        [-9.7230e-06, -6.2995e-06,  5.8934e-06,  ..., -8.0988e-06,
         -5.5544e-06, -6.1095e-06],
        [-2.4617e-05, -1.6823e-05,  1.6019e-05,  ..., -2.0519e-05,
         -1.4171e-05, -1.5259e-05],
        [-1.7434e-05, -1.1079e-05,  1.1191e-05,  ..., -1.4439e-05,
         -9.8795e-06, -1.1362e-05],
        [-2.4557e-05, -1.7270e-05,  1.5706e-05,  ..., -2.0862e-05,
         -1.4946e-05, -1.4417e-05]], device='cuda:0')
Loss: 1.0067451000213623


Running epoch 1, step 1054, batch 6
Sampled inputs[:2]: tensor([[   0,  531, 9804,  ..., 1027,  360, 1576],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3582e-05,  4.2603e-04,  3.8824e-04,  ..., -2.0505e-04,
         -5.2889e-04,  8.9547e-05],
        [-1.1235e-05, -7.2308e-06,  6.9067e-06,  ..., -9.3952e-06,
         -6.4038e-06, -7.0632e-06],
        [-2.8461e-05, -1.9282e-05,  1.8790e-05,  ..., -2.3752e-05,
         -1.6287e-05, -1.7583e-05],
        [-2.0236e-05, -1.2763e-05,  1.3188e-05,  ..., -1.6823e-05,
         -1.1429e-05, -1.3202e-05],
        [-2.8372e-05, -1.9804e-05,  1.8433e-05,  ..., -2.4140e-05,
         -1.7196e-05, -1.6592e-05]], device='cuda:0')
Loss: 0.9735954999923706


Running epoch 1, step 1055, batch 7
Sampled inputs[:2]: tensor([[    0, 13649,  7841,  ...,   287,  4713,    14],
        [    0,   259,  5112,  ...,  3520,   278,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2484e-05,  4.8171e-04,  3.1332e-04,  ..., -2.7571e-04,
         -6.3184e-04,  1.5815e-04],
        [-1.2882e-05, -8.3782e-06,  7.8492e-06,  ..., -1.0788e-05,
         -7.4171e-06, -8.0839e-06],
        [-3.2574e-05, -2.2277e-05,  2.1338e-05,  ..., -2.7224e-05,
         -1.8820e-05, -2.0087e-05],
        [-2.3142e-05, -1.4745e-05,  1.4931e-05,  ..., -1.9267e-05,
         -1.3210e-05, -1.5080e-05],
        [-3.2514e-05, -2.2918e-05,  2.0981e-05,  ..., -2.7701e-05,
         -1.9893e-05, -1.8962e-05]], device='cuda:0')
Loss: 0.9955131411552429
Graident accumulation at epoch 1, step 1055, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3709e-06,  7.2324e-05,  4.9144e-05,  ...,  4.6253e-05,
          1.0359e-04, -4.0695e-05],
        [-1.3511e-05, -9.2894e-06,  8.0414e-06,  ..., -1.1969e-05,
         -8.0667e-06, -8.6201e-06],
        [ 4.3976e-05,  4.7424e-05, -3.3151e-05,  ...,  5.0856e-05,
          3.9403e-05,  1.5727e-05],
        [ 1.4773e-05,  2.5875e-05, -1.3000e-05,  ...,  2.1402e-05,
          2.5131e-05,  6.3065e-06],
        [-3.4393e-05, -2.5502e-05,  2.1706e-05,  ..., -3.0642e-05,
         -2.1767e-05, -2.0329e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1795e-08, 5.2969e-08, 6.3860e-08,  ..., 2.4113e-08, 1.4646e-07,
         3.4417e-08],
        [7.4786e-11, 4.3343e-11, 1.1707e-11,  ..., 5.0786e-11, 1.1182e-11,
         1.9895e-11],
        [3.5006e-09, 1.9920e-09, 1.0133e-09,  ..., 2.6950e-09, 8.4130e-10,
         9.1358e-10],
        [8.7860e-10, 6.6689e-10, 2.7300e-10,  ..., 7.3801e-10, 3.9674e-10,
         2.8403e-10],
        [3.4671e-10, 1.8763e-10, 5.0804e-11,  ..., 2.5401e-10, 4.4056e-11,
         9.0088e-11]], device='cuda:0')
optimizer state dict: 132.0
lr: [1.0362117494988668e-05, 1.0362117494988668e-05]
scheduler_last_epoch: 132


Running epoch 1, step 1056, batch 8
Sampled inputs[:2]: tensor([[    0,   341,   298,  ...,   298,  1304,   292],
        [    0,  1265,  1545,  ...,   292, 36667, 36197]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5598e-05,  7.5270e-06, -6.9730e-06,  ..., -4.4825e-05,
         -2.6769e-04, -5.9223e-05],
        [-1.7434e-06, -1.0878e-06,  8.9779e-07,  ..., -1.4380e-06,
         -1.0282e-06, -1.2442e-06],
        [-4.4107e-06, -2.9057e-06,  2.4587e-06,  ..., -3.6508e-06,
         -2.6375e-06, -3.0994e-06],
        [-2.8759e-06, -1.7881e-06,  1.5721e-06,  ..., -2.3842e-06,
         -1.7062e-06, -2.0862e-06],
        [-4.5598e-06, -3.0249e-06,  2.4736e-06,  ..., -3.8147e-06,
         -2.8014e-06, -3.0845e-06]], device='cuda:0')
Loss: 0.9755340218544006


Running epoch 1, step 1057, batch 9
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2208,   12, 1901],
        [   0,  275, 2101,  ..., 1145,  590, 1619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2954e-04, -8.6209e-06, -1.4498e-04,  ..., -3.4518e-05,
         -2.1887e-04, -1.5891e-04],
        [-3.3155e-06, -2.0713e-06,  1.9483e-06,  ..., -2.7493e-06,
         -1.8142e-06, -2.1569e-06],
        [-8.2850e-06, -5.4687e-06,  5.2750e-06,  ..., -6.8694e-06,
         -4.6045e-06, -5.2601e-06],
        [-5.7369e-06, -3.5390e-06,  3.6135e-06,  ..., -4.7833e-06,
         -3.1516e-06, -3.8445e-06],
        [-8.2254e-06, -5.5730e-06,  5.1111e-06,  ..., -6.9290e-06,
         -4.8131e-06, -4.9844e-06]], device='cuda:0')
Loss: 0.9825185537338257


Running epoch 1, step 1058, batch 10
Sampled inputs[:2]: tensor([[    0,    13,  4467,  ...,  2390, 47857,   287],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7044e-04,  1.6728e-05, -1.3405e-05,  ..., -1.3923e-04,
         -2.3370e-04, -1.3360e-04],
        [-4.9993e-06, -3.2261e-06,  2.9095e-06,  ..., -4.1425e-06,
         -2.8722e-06, -3.3043e-06],
        [-1.2547e-05, -8.5235e-06,  7.8976e-06,  ..., -1.0386e-05,
         -7.2867e-06, -8.0913e-06],
        [-8.7768e-06, -5.5954e-06,  5.4240e-06,  ..., -7.2867e-06,
         -5.0440e-06, -5.9754e-06],
        [-1.2547e-05, -8.7321e-06,  7.7188e-06,  ..., -1.0550e-05,
         -7.6741e-06, -7.7561e-06]], device='cuda:0')
Loss: 1.0100868940353394


Running epoch 1, step 1059, batch 11
Sampled inputs[:2]: tensor([[   0,  344, 8133,  ...,  368, 1119, 5539],
        [   0,  221,  527,  ...,  298,  335,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9083e-04,  3.5850e-05, -4.8981e-05,  ..., -1.3807e-04,
         -3.4974e-04, -1.6440e-04],
        [-6.7055e-06, -4.2021e-06,  3.7588e-06,  ..., -5.5730e-06,
         -3.8780e-06, -4.6678e-06],
        [-1.6958e-05, -1.1086e-05,  1.0297e-05,  ..., -1.3977e-05,
         -9.8050e-06, -1.1474e-05],
        [-1.1697e-05, -7.2047e-06,  6.9588e-06,  ..., -9.7007e-06,
         -6.7204e-06, -8.3447e-06],
        [-1.7196e-05, -1.1504e-05,  1.0237e-05,  ..., -1.4395e-05,
         -1.0461e-05, -1.1168e-05]], device='cuda:0')
Loss: 0.9642345905303955


Running epoch 1, step 1060, batch 12
Sampled inputs[:2]: tensor([[   0,  266, 5528,  ...,  685,  266, 1231],
        [   0,  806, 1255,  ...,  474,  221,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1075e-04, -3.7023e-05, -1.2301e-04,  ..., -2.4879e-04,
         -3.5360e-04, -2.4456e-04],
        [-8.3148e-06, -5.2899e-06,  4.8690e-06,  ..., -6.8545e-06,
         -4.7348e-06, -5.5358e-06],
        [-2.1040e-05, -1.3947e-05,  1.3247e-05,  ..., -1.7211e-05,
         -1.1951e-05, -1.3664e-05],
        [-1.4663e-05, -9.1866e-06,  9.0897e-06,  ..., -1.2040e-05,
         -8.2776e-06, -1.0021e-05],
        [-2.0921e-05, -1.4216e-05,  1.2904e-05,  ..., -1.7419e-05,
         -1.2562e-05, -1.3046e-05]], device='cuda:0')
Loss: 0.9842023253440857


Running epoch 1, step 1061, batch 13
Sampled inputs[:2]: tensor([[    0,  1039,   259,  ...,   221,   685,   546],
        [    0, 10205,   342,  ...,  6354, 12230,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9254e-04, -6.2823e-05, -2.8726e-04,  ..., -2.6031e-04,
         -6.5846e-04, -2.4280e-04],
        [-1.0021e-05, -6.3032e-06,  5.9418e-06,  ..., -8.2031e-06,
         -5.5432e-06, -6.6534e-06],
        [-2.5451e-05, -1.6734e-05,  1.6212e-05,  ..., -2.0713e-05,
         -1.4096e-05, -1.6525e-05],
        [-1.7717e-05, -1.0990e-05,  1.1116e-05,  ..., -1.4454e-05,
         -9.7230e-06, -1.2092e-05],
        [-2.5064e-05, -1.6928e-05,  1.5661e-05,  ..., -2.0817e-05,
         -1.4752e-05, -1.5624e-05]], device='cuda:0')
Loss: 0.9856160879135132


Running epoch 1, step 1062, batch 14
Sampled inputs[:2]: tensor([[    0,    20, 13016,  ...,    14,  2743,   516],
        [    0,  1823,    12,  ...,  1874,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8385e-05,  2.5262e-05, -3.8217e-04,  ..., -3.0004e-04,
         -7.2118e-04, -1.9616e-04],
        [-1.1638e-05, -7.3835e-06,  6.9477e-06,  ..., -9.5591e-06,
         -6.5044e-06, -7.6517e-06],
        [-2.9594e-05, -1.9565e-05,  1.8954e-05,  ..., -2.4140e-05,
         -1.6496e-05, -1.9029e-05],
        [-2.0623e-05, -1.2875e-05,  1.2994e-05,  ..., -1.6853e-05,
         -1.1384e-05, -1.3947e-05],
        [-2.9236e-05, -1.9863e-05,  1.8388e-05,  ..., -2.4319e-05,
         -1.7315e-05, -1.8008e-05]], device='cuda:0')
Loss: 0.9999416470527649


Running epoch 1, step 1063, batch 15
Sampled inputs[:2]: tensor([[    0,  5750,   642,  ...,   221, 15441,   644],
        [    0, 32444,    41,  ...,    14,    18,    59]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9948e-06, -6.1338e-05, -9.4919e-05,  ..., -2.3291e-04,
         -5.4642e-04, -1.9466e-05],
        [-1.3225e-05, -8.6054e-06,  7.9386e-06,  ..., -1.0915e-05,
         -7.4953e-06, -8.6278e-06],
        [-3.3826e-05, -2.2903e-05,  2.1726e-05,  ..., -2.7761e-05,
         -1.9133e-05, -2.1636e-05],
        [-2.3469e-05, -1.5035e-05,  1.4834e-05,  ..., -1.9282e-05,
         -1.3143e-05, -1.5765e-05],
        [-3.3319e-05, -2.3127e-05,  2.1040e-05,  ..., -2.7835e-05,
         -1.9938e-05, -2.0392e-05]], device='cuda:0')
Loss: 1.0312716960906982
Graident accumulation at epoch 1, step 1063, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.1343e-06,  5.8957e-05,  3.4738e-05,  ...,  1.8337e-05,
          3.8592e-05, -3.8572e-05],
        [-1.3482e-05, -9.2210e-06,  8.0311e-06,  ..., -1.1864e-05,
         -8.0096e-06, -8.6209e-06],
        [ 3.6196e-05,  4.0391e-05, -2.7663e-05,  ...,  4.2995e-05,
          3.3550e-05,  1.1991e-05],
        [ 1.0949e-05,  2.1784e-05, -1.0217e-05,  ...,  1.7333e-05,
          2.1303e-05,  4.0993e-06],
        [-3.4286e-05, -2.5265e-05,  2.1639e-05,  ..., -3.0361e-05,
         -2.1584e-05, -2.0335e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1744e-08, 5.2920e-08, 6.3805e-08,  ..., 2.4143e-08, 1.4661e-07,
         3.4383e-08],
        [7.4886e-11, 4.3374e-11, 1.1758e-11,  ..., 5.0854e-11, 1.1227e-11,
         1.9950e-11],
        [3.4983e-09, 1.9905e-09, 1.0128e-09,  ..., 2.6931e-09, 8.4082e-10,
         9.1313e-10],
        [8.7828e-10, 6.6645e-10, 2.7295e-10,  ..., 7.3764e-10, 3.9652e-10,
         2.8399e-10],
        [3.4747e-10, 1.8798e-10, 5.1196e-11,  ..., 2.5453e-10, 4.4409e-11,
         9.0414e-11]], device='cuda:0')
optimizer state dict: 133.0
lr: [1.023855743411865e-05, 1.023855743411865e-05]
scheduler_last_epoch: 133


Running epoch 1, step 1064, batch 16
Sampled inputs[:2]: tensor([[    0,  3159,   278,  ...,   266,  2545,   863],
        [    0,  5129,  1245,  ...,   292, 24298,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3416e-05,  2.0751e-05,  1.2331e-04,  ..., -2.8199e-05,
         -4.0989e-05,  8.2983e-06],
        [-1.5423e-06, -9.9093e-07,  1.0431e-06,  ..., -1.2666e-06,
         -8.7172e-07, -9.6112e-07],
        [-4.0829e-06, -2.7418e-06,  2.9355e-06,  ..., -3.3230e-06,
         -2.2948e-06, -2.4736e-06],
        [-2.8461e-06, -1.7807e-06,  2.0117e-06,  ..., -2.3097e-06,
         -1.5870e-06, -1.8403e-06],
        [-3.9041e-06, -2.6971e-06,  2.7567e-06,  ..., -3.2336e-06,
         -2.3097e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.0110023021697998


Running epoch 1, step 1065, batch 17
Sampled inputs[:2]: tensor([[    0,   925,   271,  ...,   391,   721,  1576],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9727e-05,  5.4989e-05,  1.2331e-04,  ..., -1.2863e-04,
         -9.6986e-05,  1.6872e-05],
        [-3.1367e-06, -2.1458e-06,  2.1160e-06,  ..., -2.5779e-06,
         -1.8179e-06, -1.8328e-06],
        [-8.1658e-06, -5.8413e-06,  5.8562e-06,  ..., -6.6906e-06,
         -4.7386e-06, -4.7088e-06],
        [-5.8115e-06, -3.8967e-06,  4.0978e-06,  ..., -4.7237e-06,
         -3.3304e-06, -3.5390e-06],
        [-7.8082e-06, -5.7966e-06,  5.5134e-06,  ..., -6.5416e-06,
         -4.8131e-06, -4.2617e-06]], device='cuda:0')
Loss: 1.0155651569366455


Running epoch 1, step 1066, batch 18
Sampled inputs[:2]: tensor([[   0, 8588, 3937,  ...,  516, 1128, 2341],
        [   0, 6275,   12,  ..., 2027, 2887,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8930e-05,  5.2580e-05,  2.2743e-04,  ..., -1.3397e-04,
          1.7366e-05,  4.1138e-05],
        [-4.6715e-06, -3.1739e-06,  3.1069e-06,  ..., -3.8445e-06,
         -2.6673e-06, -2.8163e-06],
        [-1.2159e-05, -8.6129e-06,  8.6278e-06,  ..., -9.9391e-06,
         -6.8992e-06, -7.1973e-06],
        [-8.6874e-06, -5.7742e-06,  6.0648e-06,  ..., -7.0930e-06,
         -4.8950e-06, -5.4762e-06],
        [-1.1712e-05, -8.5831e-06,  8.1658e-06,  ..., -9.7752e-06,
         -7.0632e-06, -6.5416e-06]], device='cuda:0')
Loss: 0.9855971932411194


Running epoch 1, step 1067, batch 19
Sampled inputs[:2]: tensor([[    0,   494,   298,  ...,   408, 32859, 14550],
        [    0,   278,   266,  ...,   352, 10572,   345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0270e-05, -9.3529e-05,  1.4069e-04,  ..., -1.4320e-04,
         -8.3801e-05,  6.4670e-05],
        [-6.3702e-06, -4.1798e-06,  4.0159e-06,  ..., -5.2750e-06,
         -3.7029e-06, -4.1351e-06],
        [-1.6481e-05, -1.1265e-05,  1.1146e-05,  ..., -1.3500e-05,
         -9.4771e-06, -1.0401e-05],
        [-1.1608e-05, -7.4357e-06,  7.7188e-06,  ..., -9.5069e-06,
         -6.6310e-06, -7.7710e-06],
        [-1.6153e-05, -1.1370e-05,  1.0714e-05,  ..., -1.3486e-05,
         -9.8199e-06, -9.6709e-06]], device='cuda:0')
Loss: 0.9645098447799683


Running epoch 1, step 1068, batch 20
Sampled inputs[:2]: tensor([[    0,   902, 11331,  ...,  1795,   365,   654],
        [    0,   292,   474,  ...,  1085,   494,  2665]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9294e-04, -1.0796e-04,  2.6405e-05,  ..., -1.9730e-04,
         -8.3801e-05,  9.1934e-05],
        [-7.9423e-06, -5.2601e-06,  5.0142e-06,  ..., -6.5863e-06,
         -4.6082e-06, -5.0813e-06],
        [-2.0593e-05, -1.4201e-05,  1.3918e-05,  ..., -1.6928e-05,
         -1.1861e-05, -1.2845e-05],
        [-1.4573e-05, -9.4324e-06,  9.6709e-06,  ..., -1.1981e-05,
         -8.3372e-06, -9.6336e-06],
        [-2.0057e-05, -1.4246e-05,  1.3322e-05,  ..., -1.6794e-05,
         -1.2189e-05, -1.1876e-05]], device='cuda:0')
Loss: 0.9947993159294128


Running epoch 1, step 1069, batch 21
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,  1231,   278,  ...,    12,  2606,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6867e-04, -2.4051e-05, -3.5012e-05,  ..., -1.3357e-04,
         -2.1505e-04,  1.6722e-04],
        [-9.4920e-06, -6.3553e-06,  6.2063e-06,  ..., -7.8231e-06,
         -5.4426e-06, -5.9158e-06],
        [-2.4557e-05, -1.7166e-05,  1.7166e-05,  ..., -2.0146e-05,
         -1.4052e-05, -1.4976e-05],
        [-1.7479e-05, -1.1489e-05,  1.2025e-05,  ..., -1.4305e-05,
         -9.8944e-06, -1.1280e-05],
        [-2.3708e-05, -1.7092e-05,  1.6257e-05,  ..., -1.9819e-05,
         -1.4365e-05, -1.3709e-05]], device='cuda:0')
Loss: 1.013502836227417


Running epoch 1, step 1070, batch 22
Sampled inputs[:2]: tensor([[    0,  1640,  1103,  ...,   685,  1478,    14],
        [    0, 26700,  5475,  ...,  5707,    65,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8889e-04,  5.1102e-05,  1.7614e-05,  ..., -1.7879e-04,
         -2.2057e-04,  2.8265e-04],
        [-1.1146e-05, -7.3537e-06,  7.1600e-06,  ..., -9.2313e-06,
         -6.4038e-06, -7.0855e-06],
        [-2.8640e-05, -1.9789e-05,  1.9759e-05,  ..., -2.3574e-05,
         -1.6421e-05, -1.7747e-05],
        [-2.0415e-05, -1.3217e-05,  1.3813e-05,  ..., -1.6809e-05,
         -1.1601e-05, -1.3426e-05],
        [-2.7850e-05, -1.9789e-05,  1.8880e-05,  ..., -2.3350e-05,
         -1.6868e-05, -1.6361e-05]], device='cuda:0')
Loss: 0.9633296132087708


Running epoch 1, step 1071, batch 23
Sampled inputs[:2]: tensor([[    0,   221,   825,  ...,   616,  3661,  8052],
        [    0, 33315,   266,  ...,    12,  1126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8042e-04,  5.9293e-05, -9.9166e-05,  ..., -1.4226e-04,
         -4.3157e-04,  2.9192e-04],
        [-1.2659e-05, -8.4341e-06,  8.2776e-06,  ..., -1.0476e-05,
         -7.2680e-06, -7.8715e-06],
        [-3.2604e-05, -2.2724e-05,  2.2829e-05,  ..., -2.6837e-05,
         -1.8671e-05, -1.9819e-05],
        [-2.3291e-05, -1.5259e-05,  1.6034e-05,  ..., -1.9178e-05,
         -1.3232e-05, -1.5005e-05],
        [-3.1501e-05, -2.2590e-05,  2.1666e-05,  ..., -2.6420e-05,
         -1.9073e-05, -1.8150e-05]], device='cuda:0')
Loss: 1.0132652521133423
Graident accumulation at epoch 1, step 1071, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0285,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2521e-05,  5.8991e-05,  2.1347e-05,  ...,  2.2770e-06,
         -8.4239e-06, -5.5230e-06],
        [-1.3400e-05, -9.1423e-06,  8.0557e-06,  ..., -1.1725e-05,
         -7.9354e-06, -8.5459e-06],
        [ 2.9316e-05,  3.4080e-05, -2.2614e-05,  ...,  3.6012e-05,
          2.8328e-05,  8.8099e-06],
        [ 7.5246e-06,  1.8080e-05, -7.5917e-06,  ...,  1.3682e-05,
          1.7850e-05,  2.1888e-06],
        [-3.4007e-05, -2.4997e-05,  2.1642e-05,  ..., -2.9967e-05,
         -2.1333e-05, -2.0117e-05]], device='cuda:0')
optimizer state dict: tensor([[5.1771e-08, 5.2870e-08, 6.3751e-08,  ..., 2.4139e-08, 1.4665e-07,
         3.4434e-08],
        [7.4971e-11, 4.3402e-11, 1.1815e-11,  ..., 5.0913e-11, 1.1269e-11,
         1.9992e-11],
        [3.4958e-09, 1.9890e-09, 1.0123e-09,  ..., 2.6911e-09, 8.4033e-10,
         9.1261e-10],
        [8.7794e-10, 6.6602e-10, 2.7293e-10,  ..., 7.3727e-10, 3.9630e-10,
         2.8393e-10],
        [3.4811e-10, 1.8830e-10, 5.1614e-11,  ..., 2.5497e-10, 4.4729e-11,
         9.0653e-11]], device='cuda:0')
optimizer state dict: 134.0
lr: [1.01149609195903e-05, 1.01149609195903e-05]
scheduler_last_epoch: 134


Running epoch 1, step 1072, batch 24
Sampled inputs[:2]: tensor([[   0,   24,   15,  ...,  221,  380,  417],
        [   0, 1167, 2667,  ..., 4769,   13, 5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9147e-05,  1.2748e-04, -1.5764e-06,  ..., -1.5360e-04,
         -3.8354e-05, -1.9603e-05],
        [-1.6838e-06, -9.9838e-07,  9.6112e-07,  ..., -1.3113e-06,
         -9.2387e-07, -1.2219e-06],
        [-4.2021e-06, -2.7120e-06,  2.6226e-06,  ..., -3.3081e-06,
         -2.3544e-06, -3.0100e-06],
        [-2.9653e-06, -1.7658e-06,  1.8105e-06,  ..., -2.3097e-06,
         -1.6317e-06, -2.1905e-06],
        [-4.2021e-06, -2.7120e-06,  2.5183e-06,  ..., -3.3528e-06,
         -2.4587e-06, -2.9206e-06]], device='cuda:0')
Loss: 0.959389865398407


Running epoch 1, step 1073, batch 25
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,   271, 21394,  ...,  1487,   287,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5195e-04,  1.5835e-04,  3.5151e-05,  ..., -2.2029e-04,
         -3.3169e-05, -1.0862e-04],
        [-3.1963e-06, -2.1905e-06,  1.9595e-06,  ..., -2.5779e-06,
         -1.8999e-06, -2.1160e-06],
        [-8.1360e-06, -5.9158e-06,  5.3495e-06,  ..., -6.6012e-06,
         -4.8727e-06, -5.3197e-06],
        [-5.8711e-06, -4.0308e-06,  3.7923e-06,  ..., -4.7237e-06,
         -3.4943e-06, -3.9935e-06],
        [-8.0466e-06, -5.9307e-06,  5.1409e-06,  ..., -6.6310e-06,
         -5.0515e-06, -5.0664e-06]], device='cuda:0')
Loss: 1.0166535377502441


Running epoch 1, step 1074, batch 26
Sampled inputs[:2]: tensor([[    0,   287,  3609,  ...,  3661,  5944,   838],
        [    0,   287,  1790,  ..., 11367,  9476,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1351e-04,  3.3067e-04,  1.9620e-04,  ..., -1.2992e-04,
          6.2543e-05,  1.0574e-04],
        [-4.7833e-06, -3.4049e-06,  3.0175e-06,  ..., -3.8743e-06,
         -2.8498e-06, -3.0249e-06],
        [-1.2279e-05, -9.2089e-06,  8.2999e-06,  ..., -9.9689e-06,
         -7.3463e-06, -7.6592e-06],
        [-8.8066e-06, -6.2510e-06,  5.8338e-06,  ..., -7.1079e-06,
         -5.2452e-06, -5.7444e-06],
        [-1.1951e-05, -9.1046e-06,  7.8678e-06,  ..., -9.8795e-06,
         -7.5102e-06, -7.1377e-06]], device='cuda:0')
Loss: 1.005212664604187


Running epoch 1, step 1075, batch 27
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ...,  278, 6607,   13],
        [   0, 1128, 3231,  ..., 8375,  199, 2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3726e-04,  3.4435e-04,  1.4092e-04,  ..., -1.8541e-04,
          3.6723e-05,  6.2962e-05],
        [-6.3479e-06, -4.5523e-06,  4.1574e-06,  ..., -5.1633e-06,
         -3.7104e-06, -3.8631e-06],
        [-1.6212e-05, -1.2204e-05,  1.1340e-05,  ..., -1.3217e-05,
         -9.5367e-06, -9.7603e-06],
        [-1.1712e-05, -8.3670e-06,  8.0541e-06,  ..., -9.4920e-06,
         -6.8322e-06, -7.3761e-06],
        [-1.5616e-05, -1.2010e-05,  1.0669e-05,  ..., -1.2994e-05,
         -9.7007e-06, -8.9779e-06]], device='cuda:0')
Loss: 1.0230042934417725


Running epoch 1, step 1076, batch 28
Sampled inputs[:2]: tensor([[   0, 5024, 3846,  ..., 5880, 1377,   12],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7807e-04,  3.7285e-04,  1.4048e-04,  ..., -2.9562e-04,
          6.2466e-05,  1.9452e-04],
        [-7.8902e-06, -5.6624e-06,  5.2899e-06,  ..., -6.4000e-06,
         -4.5858e-06, -4.6976e-06],
        [-2.0266e-05, -1.5274e-05,  1.4484e-05,  ..., -1.6496e-05,
         -1.1876e-05, -1.1936e-05],
        [-1.4693e-05, -1.0498e-05,  1.0364e-05,  ..., -1.1891e-05,
         -8.5235e-06, -9.0748e-06],
        [-1.9297e-05, -1.4916e-05,  1.3500e-05,  ..., -1.6063e-05,
         -1.1981e-05, -1.0841e-05]], device='cuda:0')
Loss: 1.006820559501648


Running epoch 1, step 1077, batch 29
Sampled inputs[:2]: tensor([[    0,  1597,   278,  ...,    20,    38,   446],
        [    0,     5,  4413,  ...,  9205, 16744,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5653e-04,  4.9808e-04,  2.4833e-04,  ..., -2.0579e-04,
          3.1808e-04,  4.9047e-04],
        [-9.3952e-06, -6.7577e-06,  6.1765e-06,  ..., -7.6890e-06,
         -5.5470e-06, -5.7854e-06],
        [-2.4289e-05, -1.8299e-05,  1.7047e-05,  ..., -1.9878e-05,
         -1.4395e-05, -1.4737e-05],
        [-1.7554e-05, -1.2524e-05,  1.2122e-05,  ..., -1.4320e-05,
         -1.0334e-05, -1.1221e-05],
        [-2.3201e-05, -1.7926e-05,  1.5989e-05,  ..., -1.9401e-05,
         -1.4544e-05, -1.3404e-05]], device='cuda:0')
Loss: 0.9823031425476074


Running epoch 1, step 1078, batch 30
Sampled inputs[:2]: tensor([[   0, 1732,  699,  ...,  417,  199, 1726],
        [   0, 1270,  413,  ...,  413,  711,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4988e-04,  6.0503e-04,  2.9368e-04,  ..., -3.9568e-04,
          2.4930e-04,  5.4758e-04],
        [-1.0923e-05, -7.8157e-06,  7.2271e-06,  ..., -8.9332e-06,
         -6.3516e-06, -6.6683e-06],
        [-2.8402e-05, -2.1279e-05,  2.0042e-05,  ..., -2.3246e-05,
         -1.6600e-05, -1.7092e-05],
        [-2.0519e-05, -1.4521e-05,  1.4268e-05,  ..., -1.6719e-05,
         -1.1861e-05, -1.3009e-05],
        [-2.6926e-05, -2.0713e-05,  1.8671e-05,  ..., -2.2501e-05,
         -1.6674e-05, -1.5415e-05]], device='cuda:0')
Loss: 0.9799809455871582


Running epoch 1, step 1079, batch 31
Sampled inputs[:2]: tensor([[    0,   278,  7914,  ...,  1194,   300,  4419],
        [    0, 25938,   359,  ...,    36, 15859,   504]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4409e-04,  6.9150e-04,  1.6450e-04,  ..., -3.7576e-04,
          1.9944e-04,  4.0004e-04],
        [-1.2413e-05, -8.7842e-06,  8.3372e-06,  ..., -1.0125e-05,
         -7.1228e-06, -7.4841e-06],
        [-3.2142e-05, -2.3827e-05,  2.3052e-05,  ..., -2.6211e-05,
         -1.8507e-05, -1.9073e-05],
        [-2.3410e-05, -1.6339e-05,  1.6563e-05,  ..., -1.9014e-05,
         -1.3337e-05, -1.4678e-05],
        [-3.0458e-05, -2.3201e-05,  2.1458e-05,  ..., -2.5362e-05,
         -1.8612e-05, -1.7151e-05]], device='cuda:0')
Loss: 0.9743265509605408
Graident accumulation at epoch 1, step 1079, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4678e-05,  1.2224e-04,  3.5663e-05,  ..., -3.5527e-05,
          1.2362e-05,  3.5033e-05],
        [-1.3301e-05, -9.1065e-06,  8.0839e-06,  ..., -1.1565e-05,
         -7.8541e-06, -8.4397e-06],
        [ 2.3170e-05,  2.8289e-05, -1.8047e-05,  ...,  2.9789e-05,
          2.3644e-05,  6.0215e-06],
        [ 4.4312e-06,  1.4638e-05, -5.1763e-06,  ...,  1.0413e-05,
          1.4731e-05,  5.0218e-07],
        [-3.3652e-05, -2.4818e-05,  2.1623e-05,  ..., -2.9507e-05,
         -2.1061e-05, -1.9820e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2134e-08, 5.3296e-08, 6.3715e-08,  ..., 2.4256e-08, 1.4655e-07,
         3.4560e-08],
        [7.5051e-11, 4.3436e-11, 1.1873e-11,  ..., 5.0965e-11, 1.1308e-11,
         2.0028e-11],
        [3.4934e-09, 1.9876e-09, 1.0118e-09,  ..., 2.6891e-09, 8.3983e-10,
         9.1206e-10],
        [8.7761e-10, 6.6562e-10, 2.7293e-10,  ..., 7.3690e-10, 3.9608e-10,
         2.8386e-10],
        [3.4869e-10, 1.8865e-10, 5.2023e-11,  ..., 2.5536e-10, 4.5030e-11,
         9.0857e-11]], device='cuda:0')
optimizer state dict: 135.0
lr: [9.99134683802993e-06, 9.99134683802993e-06]
scheduler_last_epoch: 135


Running epoch 1, step 1080, batch 32
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,   546,   360,  ...,    12,   461,  8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9694e-05,  7.9700e-05, -8.7027e-05,  ..., -2.6289e-05,
         -7.1315e-05, -3.4673e-05],
        [-1.4827e-06, -9.7603e-07,  1.1697e-06,  ..., -1.1697e-06,
         -7.0408e-07, -8.2701e-07],
        [-3.8147e-06, -2.5928e-06,  3.1888e-06,  ..., -2.9802e-06,
         -1.8179e-06, -2.0862e-06],
        [-2.9355e-06, -1.8701e-06,  2.4289e-06,  ..., -2.2799e-06,
         -1.3560e-06, -1.6987e-06],
        [-3.4124e-06, -2.4140e-06,  2.8163e-06,  ..., -2.7269e-06,
         -1.7658e-06, -1.7360e-06]], device='cuda:0')
Loss: 0.991174042224884


Running epoch 1, step 1081, batch 33
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,  6508,  4305,  ...,   806,  3888,  4431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2741e-04,  7.4418e-05, -1.3787e-04,  ...,  3.9728e-05,
          1.7072e-05,  7.2381e-05],
        [-3.2261e-06, -1.8962e-06,  1.9334e-06,  ..., -2.5481e-06,
         -1.7025e-06, -2.3320e-06],
        [-8.3745e-06, -5.1111e-06,  5.3942e-06,  ..., -6.5267e-06,
         -4.4107e-06, -5.9009e-06],
        [-5.9307e-06, -3.4198e-06,  3.8445e-06,  ..., -4.6343e-06,
         -3.0473e-06, -4.3064e-06],
        [-8.3596e-06, -5.1409e-06,  5.0962e-06,  ..., -6.6310e-06,
         -4.7162e-06, -5.7891e-06]], device='cuda:0')
Loss: 0.9438420534133911


Running epoch 1, step 1082, batch 34
Sampled inputs[:2]: tensor([[    0,   292,  3030,  ...,  1231,  2156,   266],
        [    0,   266, 28695,  ...,   278,   266,  6087]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4576e-04,  2.5408e-06, -2.0738e-04,  ..., -8.6543e-05,
          2.3225e-05,  2.1964e-05],
        [-4.7684e-06, -3.0957e-06,  3.0734e-06,  ..., -3.7998e-06,
         -2.5928e-06, -3.1777e-06],
        [-1.2457e-05, -8.4192e-06,  8.5831e-06,  ..., -9.8795e-06,
         -6.8098e-06, -8.1658e-06],
        [-8.8960e-06, -5.6997e-06,  6.1393e-06,  ..., -7.0333e-06,
         -4.7386e-06, -6.0126e-06],
        [-1.2130e-05, -8.2701e-06,  7.9721e-06,  ..., -9.7752e-06,
         -7.0557e-06, -7.7561e-06]], device='cuda:0')
Loss: 1.0309598445892334


Running epoch 1, step 1083, batch 35
Sampled inputs[:2]: tensor([[   0, 1358,  367,  ..., 1758, 2921,   12],
        [   0, 3634, 3444,  ...,  642, 2156,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7973e-04, -8.1205e-05, -1.5687e-04,  ..., -7.8359e-05,
          6.8269e-05,  1.1742e-04],
        [-6.4075e-06, -4.1761e-06,  4.1984e-06,  ..., -5.0515e-06,
         -3.4608e-06, -4.2357e-06],
        [-1.6928e-05, -1.1474e-05,  1.1846e-05,  ..., -1.3262e-05,
         -9.1791e-06, -1.1027e-05],
        [-1.2010e-05, -7.7114e-06,  8.4043e-06,  ..., -9.3579e-06,
         -6.3255e-06, -8.0690e-06],
        [-1.6212e-05, -1.1161e-05,  1.0878e-05,  ..., -1.2964e-05,
         -9.4101e-06, -1.0245e-05]], device='cuda:0')
Loss: 1.0039293766021729


Running epoch 1, step 1084, batch 36
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2715e-04, -9.2053e-05, -9.1572e-05,  ..., -8.3187e-05,
          2.5639e-04,  1.0708e-04],
        [-8.0168e-06, -5.3160e-06,  5.2340e-06,  ..., -6.3777e-06,
         -4.4294e-06, -5.2787e-06],
        [-2.1100e-05, -1.4573e-05,  1.4722e-05,  ..., -1.6674e-05,
         -1.1712e-05, -1.3694e-05],
        [-1.4976e-05, -9.7975e-06,  1.0416e-05,  ..., -1.1787e-05,
         -8.0988e-06, -1.0021e-05],
        [-2.0325e-05, -1.4290e-05,  1.3664e-05,  ..., -1.6406e-05,
         -1.2048e-05, -1.2748e-05]], device='cuda:0')
Loss: 1.0028314590454102


Running epoch 1, step 1085, batch 37
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,    14,  8058,  ..., 10316,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3742e-04, -1.1058e-04, -1.6675e-04,  ..., -2.1592e-05,
          2.4742e-04,  1.6174e-04],
        [-9.5442e-06, -6.4038e-06,  6.2846e-06,  ..., -7.6219e-06,
         -5.3123e-06, -6.2622e-06],
        [-2.5123e-05, -1.7583e-05,  1.7673e-05,  ..., -1.9938e-05,
         -1.4052e-05, -1.6242e-05],
        [-1.7852e-05, -1.1794e-05,  1.2502e-05,  ..., -1.4111e-05,
         -9.7379e-06, -1.1943e-05],
        [-2.4125e-05, -1.7226e-05,  1.6406e-05,  ..., -1.9550e-05,
         -1.4402e-05, -1.5028e-05]], device='cuda:0')
Loss: 0.9805930256843567


Running epoch 1, step 1086, batch 38
Sampled inputs[:2]: tensor([[   0, 1145,   13,  ...,  721, 1119, 3495],
        [   0,    9,  870,  ..., 2671,  965, 3229]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2343e-04, -7.2623e-05, -1.3649e-04,  ..., -2.4857e-05,
          3.3109e-04,  2.7383e-04],
        [-1.1124e-05, -7.5512e-06,  7.2978e-06,  ..., -8.8513e-06,
         -6.2101e-06, -7.2531e-06],
        [-2.9266e-05, -2.0757e-05,  2.0549e-05,  ..., -2.3186e-05,
         -1.6451e-05, -1.8820e-05],
        [-2.0862e-05, -1.3970e-05,  1.4544e-05,  ..., -1.6451e-05,
         -1.1444e-05, -1.3910e-05],
        [-2.7999e-05, -2.0310e-05,  1.9059e-05,  ..., -2.2694e-05,
         -1.6831e-05, -1.7323e-05]], device='cuda:0')
Loss: 0.991872251033783


Running epoch 1, step 1087, batch 39
Sampled inputs[:2]: tensor([[    0,    14, 38914,  ...,   266,  5690,   278],
        [    0,   266,  1254,  ...,   369,  2870,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8329e-04, -1.0133e-04, -1.3646e-04,  ..., -1.7142e-04,
          4.7993e-04,  1.7734e-04],
        [-1.2748e-05, -8.6240e-06,  8.2515e-06,  ..., -1.0133e-05,
         -7.1153e-06, -8.3409e-06],
        [-3.3587e-05, -2.3797e-05,  2.3291e-05,  ..., -2.6613e-05,
         -1.8939e-05, -2.1696e-05],
        [-2.3916e-05, -1.5952e-05,  1.6481e-05,  ..., -1.8850e-05,
         -1.3128e-05, -1.5996e-05],
        [-3.2201e-05, -2.3350e-05,  2.1651e-05,  ..., -2.6107e-05,
         -1.9409e-05, -2.0005e-05]], device='cuda:0')
Loss: 0.9713412523269653
Graident accumulation at epoch 1, step 1087, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1454e-04,  9.9885e-05,  1.8450e-05,  ..., -4.9116e-05,
          5.9118e-05,  4.9264e-05],
        [-1.3246e-05, -9.0582e-06,  8.1006e-06,  ..., -1.1422e-05,
         -7.7803e-06, -8.4299e-06],
        [ 1.7495e-05,  2.3080e-05, -1.3914e-05,  ...,  2.4149e-05,
          1.9386e-05,  3.2498e-06],
        [ 1.5964e-06,  1.1579e-05, -3.0106e-06,  ...,  7.4863e-06,
          1.1945e-05, -1.1477e-06],
        [-3.3507e-05, -2.4671e-05,  2.1626e-05,  ..., -2.9167e-05,
         -2.0895e-05, -1.9839e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2228e-08, 5.3253e-08, 6.3670e-08,  ..., 2.4261e-08, 1.4663e-07,
         3.4557e-08],
        [7.5138e-11, 4.3467e-11, 1.1929e-11,  ..., 5.1017e-11, 1.1347e-11,
         2.0077e-11],
        [3.4910e-09, 1.9862e-09, 1.0113e-09,  ..., 2.6872e-09, 8.3935e-10,
         9.1162e-10],
        [8.7731e-10, 6.6521e-10, 2.7293e-10,  ..., 7.3652e-10, 3.9585e-10,
         2.8384e-10],
        [3.4938e-10, 1.8901e-10, 5.2440e-11,  ..., 2.5579e-10, 4.5362e-11,
         9.1166e-11]], device='cuda:0')
optimizer state dict: 136.0
lr: [9.867734078748245e-06, 9.867734078748245e-06]
scheduler_last_epoch: 136


Running epoch 1, step 1088, batch 40
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,   437,  1916,  ...,    13,  1303,  2708]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0920e-06,  5.5429e-05, -1.1911e-04,  ...,  3.7009e-05,
         -2.1626e-05, -2.8689e-05],
        [-1.5050e-06, -1.1250e-06,  1.0282e-06,  ..., -1.2442e-06,
         -8.9407e-07, -9.0152e-07],
        [-3.9339e-06, -3.0398e-06,  2.9206e-06,  ..., -3.2037e-06,
         -2.2650e-06, -2.2948e-06],
        [-2.8759e-06, -2.1160e-06,  2.0713e-06,  ..., -2.3544e-06,
         -1.6764e-06, -1.8105e-06],
        [-3.7700e-06, -2.9951e-06,  2.7418e-06,  ..., -3.1441e-06,
         -2.3246e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9680395126342773


Running epoch 1, step 1089, batch 41
Sampled inputs[:2]: tensor([[   0, 8840,   26,  ...,   28,   16,   14],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3484e-05,  6.6189e-05, -1.6276e-04,  ...,  6.5620e-05,
          5.1238e-05, -2.0665e-04],
        [-2.9951e-06, -2.2650e-06,  2.0862e-06,  ..., -2.4587e-06,
         -1.7583e-06, -1.8105e-06],
        [-7.9572e-06, -6.2287e-06,  5.9754e-06,  ..., -6.4820e-06,
         -4.6045e-06, -4.7088e-06],
        [-5.7667e-06, -4.2915e-06,  4.2319e-06,  ..., -4.7088e-06,
         -3.3304e-06, -3.6359e-06],
        [-7.4804e-06, -6.0201e-06,  5.4985e-06,  ..., -6.2138e-06,
         -4.6343e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.014939308166504


Running epoch 1, step 1090, batch 42
Sampled inputs[:2]: tensor([[   0, 4304, 7406,  ...,  957, 7366,  328],
        [   0, 1549, 7052,  ..., 2529, 3958,   37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2419e-05,  1.5048e-04, -2.6409e-04,  ...,  7.5131e-05,
          4.5873e-06, -1.8286e-04],
        [-4.5076e-06, -3.3751e-06,  3.1590e-06,  ..., -3.6806e-06,
         -2.5742e-06, -2.7195e-06],
        [-1.2040e-05, -9.3579e-06,  9.0450e-06,  ..., -9.7603e-06,
         -6.7949e-06, -7.1526e-06],
        [-8.7470e-06, -6.4224e-06,  6.4373e-06,  ..., -7.0930e-06,
         -4.9025e-06, -5.5283e-06],
        [-1.1191e-05, -8.9556e-06,  8.2403e-06,  ..., -9.2685e-06,
         -6.7800e-06, -6.2883e-06]], device='cuda:0')
Loss: 1.0119740962982178


Running epoch 1, step 1091, batch 43
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,  527,  496,  ...,   12,  795, 8296]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6174e-05,  2.7909e-04, -3.4673e-04,  ...,  5.0177e-05,
         -6.5816e-05, -1.6224e-04],
        [-5.9679e-06, -4.5374e-06,  4.1127e-06,  ..., -4.9174e-06,
         -3.4980e-06, -3.6396e-06],
        [-1.6034e-05, -1.2651e-05,  1.1846e-05,  ..., -1.3068e-05,
         -9.2983e-06, -9.6112e-06],
        [-1.1519e-05, -8.5682e-06,  8.3745e-06,  ..., -9.4026e-06,
         -6.6310e-06, -7.3537e-06],
        [-1.4976e-05, -1.2159e-05,  1.0848e-05,  ..., -1.2472e-05,
         -9.2834e-06, -8.4788e-06]], device='cuda:0')
Loss: 0.9502571821212769


Running epoch 1, step 1092, batch 44
Sampled inputs[:2]: tensor([[    0,  2548,   720,  ...,  1795,  1109, 32948],
        [    0, 11054,    12,  ...,   560,   199,   677]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9300e-05,  3.0863e-04, -3.0482e-04,  ...,  9.1475e-06,
          1.3735e-04, -1.5044e-04],
        [-7.4580e-06, -5.7295e-06,  5.0962e-06,  ..., -6.1840e-06,
         -4.3772e-06, -4.4890e-06],
        [-1.9997e-05, -1.5914e-05,  1.4663e-05,  ..., -1.6421e-05,
         -1.1608e-05, -1.1876e-05],
        [-1.4439e-05, -1.0848e-05,  1.0416e-05,  ..., -1.1861e-05,
         -8.3074e-06, -9.1270e-06],
        [-1.8716e-05, -1.5333e-05,  1.3456e-05,  ..., -1.5706e-05,
         -1.1623e-05, -1.0490e-05]], device='cuda:0')
Loss: 0.9811347126960754


Running epoch 1, step 1093, batch 45
Sampled inputs[:2]: tensor([[    0,  1304,  1040,  ...,   287,  1665,   741],
        [    0,  8405,  4142,  ..., 18796,     9,   699]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6021e-05,  3.0986e-04, -1.3499e-04,  ..., -6.3023e-05,
          2.7063e-04, -7.6795e-05],
        [-8.9779e-06, -6.9886e-06,  6.1765e-06,  ..., -7.4282e-06,
         -5.2936e-06, -5.3756e-06],
        [-2.3991e-05, -1.9372e-05,  1.7703e-05,  ..., -1.9699e-05,
         -1.4052e-05, -1.4201e-05],
        [-1.7345e-05, -1.3247e-05,  1.2606e-05,  ..., -1.4246e-05,
         -1.0058e-05, -1.0915e-05],
        [-2.2367e-05, -1.8612e-05,  1.6198e-05,  ..., -1.8790e-05,
         -1.4037e-05, -1.2487e-05]], device='cuda:0')
Loss: 1.0143980979919434


Running epoch 1, step 1094, batch 46
Sampled inputs[:2]: tensor([[    0,   278, 14971,  ...,  2341,   266,   717],
        [    0,   508,  2322,  ...,   968,   266, 15123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3423e-05,  4.6698e-04, -1.7964e-04,  ..., -1.1087e-04,
          3.0167e-04, -1.5106e-04],
        [-1.0550e-05, -8.1062e-06,  7.1116e-06,  ..., -8.6948e-06,
         -6.2697e-06, -6.4112e-06],
        [-2.8133e-05, -2.2396e-05,  2.0370e-05,  ..., -2.2978e-05,
         -1.6585e-05, -1.6853e-05],
        [-2.0310e-05, -1.5289e-05,  1.4454e-05,  ..., -1.6585e-05,
         -1.1861e-05, -1.2942e-05],
        [-2.6360e-05, -2.1592e-05,  1.8761e-05,  ..., -2.2024e-05,
         -1.6630e-05, -1.4901e-05]], device='cuda:0')
Loss: 0.9602920413017273


Running epoch 1, step 1095, batch 47
Sampled inputs[:2]: tensor([[    0,   199, 14973,  ...,   638,  1119,  1329],
        [    0,   328, 27958,  ...,   417,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4455e-04,  4.5132e-04, -2.2686e-04,  ..., -2.0416e-04,
          3.1951e-04, -2.3404e-04],
        [-1.2085e-05, -9.2387e-06,  8.2068e-06,  ..., -9.9093e-06,
         -7.0743e-06, -7.2978e-06],
        [-3.2306e-05, -2.5570e-05,  2.3514e-05,  ..., -2.6271e-05,
         -1.8761e-05, -1.9267e-05],
        [-2.3350e-05, -1.7479e-05,  1.6749e-05,  ..., -1.8969e-05,
         -1.3411e-05, -1.4797e-05],
        [-3.0130e-05, -2.4542e-05,  2.1547e-05,  ..., -2.5079e-05,
         -1.8746e-05, -1.6943e-05]], device='cuda:0')
Loss: 0.9790427088737488
Graident accumulation at epoch 1, step 1095, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.8631e-05,  1.3503e-04, -6.0808e-06,  ..., -6.4620e-05,
          8.5157e-05,  2.0934e-05],
        [-1.3130e-05, -9.0763e-06,  8.1113e-06,  ..., -1.1270e-05,
         -7.7097e-06, -8.3167e-06],
        [ 1.2515e-05,  1.8215e-05, -1.0171e-05,  ...,  1.9107e-05,
          1.5571e-05,  9.9808e-07],
        [-8.9823e-07,  8.6731e-06, -1.0346e-06,  ...,  4.8408e-06,
          9.4097e-06, -2.5126e-06],
        [-3.3169e-05, -2.4658e-05,  2.1618e-05,  ..., -2.8758e-05,
         -2.0680e-05, -1.9549e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2197e-08, 5.3403e-08, 6.3657e-08,  ..., 2.4279e-08, 1.4659e-07,
         3.4577e-08],
        [7.5209e-11, 4.3508e-11, 1.1984e-11,  ..., 5.1064e-11, 1.1386e-11,
         2.0111e-11],
        [3.4886e-09, 1.9848e-09, 1.0109e-09,  ..., 2.6852e-09, 8.3886e-10,
         9.1108e-10],
        [8.7697e-10, 6.6485e-10, 2.7294e-10,  ..., 7.3614e-10, 3.9564e-10,
         2.8377e-10],
        [3.4994e-10, 1.8942e-10, 5.2852e-11,  ..., 2.5616e-10, 4.5668e-11,
         9.1362e-11]], device='cuda:0')
optimizer state dict: 137.0
lr: [9.744141530853894e-06, 9.744141530853894e-06]
scheduler_last_epoch: 137


Running epoch 1, step 1096, batch 48
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1732e-06,  8.4724e-05, -7.5691e-05,  ...,  1.0887e-05,
         -1.2036e-04,  4.7483e-05],
        [-1.5125e-06, -1.0952e-06,  1.0803e-06,  ..., -1.1921e-06,
         -7.6741e-07, -7.8231e-07],
        [-3.9935e-06, -3.0249e-06,  3.0547e-06,  ..., -3.1292e-06,
         -2.0415e-06, -1.9968e-06],
        [-3.0845e-06, -2.1756e-06,  2.3097e-06,  ..., -2.4140e-06,
         -1.5646e-06, -1.6913e-06],
        [-3.4869e-06, -2.7418e-06,  2.6524e-06,  ..., -2.7865e-06,
         -1.9222e-06, -1.6168e-06]], device='cuda:0')
Loss: 0.9798404574394226


Running epoch 1, step 1097, batch 49
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0393e-04, -3.6069e-05, -2.2310e-04,  ...,  2.9532e-05,
         -1.9671e-04, -9.3750e-05],
        [-2.9951e-06, -2.1234e-06,  2.1309e-06,  ..., -2.3246e-06,
         -1.5125e-06, -1.6242e-06],
        [-7.9572e-06, -5.8860e-06,  6.0648e-06,  ..., -6.1393e-06,
         -4.0084e-06, -4.2021e-06],
        [-6.0648e-06, -4.1872e-06,  4.5300e-06,  ..., -4.6790e-06,
         -3.0324e-06, -3.4645e-06],
        [-7.0333e-06, -5.3793e-06,  5.3197e-06,  ..., -5.5283e-06,
         -3.8221e-06, -3.4571e-06]], device='cuda:0')
Loss: 0.9711260199546814


Running epoch 1, step 1098, batch 50
Sampled inputs[:2]: tensor([[    0, 25241,   717,  ...,   413,    16,    14],
        [    0,   389, 18984,  ...,   287,   768,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1417e-04,  1.5047e-04, -1.4055e-04,  ...,  2.4177e-04,
         -1.2083e-04,  2.4645e-05],
        [-4.5523e-06, -3.2336e-06,  3.0920e-06,  ..., -3.5763e-06,
         -2.4475e-06, -2.6003e-06],
        [-1.2100e-05, -9.0152e-06,  8.8364e-06,  ..., -9.4622e-06,
         -6.5267e-06, -6.7353e-06],
        [-9.1046e-06, -6.3330e-06,  6.4969e-06,  ..., -7.1228e-06,
         -4.8652e-06, -5.4464e-06],
        [-1.0937e-05, -8.4043e-06,  7.8976e-06,  ..., -8.7321e-06,
         -6.3553e-06, -5.6922e-06]], device='cuda:0')
Loss: 1.0073094367980957


Running epoch 1, step 1099, batch 51
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 6945, 2360,  ...,   30,  413,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8859e-06,  1.5047e-04,  1.3902e-04,  ...,  1.7597e-04,
          2.3604e-05,  3.2265e-04],
        [-6.1542e-06, -4.3437e-06,  3.8929e-06,  ..., -4.9025e-06,
         -3.3937e-06, -3.8147e-06],
        [-1.6361e-05, -1.2115e-05,  1.1176e-05,  ..., -1.2949e-05,
         -9.0301e-06, -9.8646e-06],
        [-1.2130e-05, -8.3894e-06,  8.0690e-06,  ..., -9.6262e-06,
         -6.6459e-06, -7.8306e-06],
        [-1.5020e-05, -1.1444e-05,  1.0133e-05,  ..., -1.2130e-05,
         -8.8736e-06, -8.5533e-06]], device='cuda:0')
Loss: 0.9756981134414673


Running epoch 1, step 1100, batch 52
Sampled inputs[:2]: tensor([[    0,  1615,   328,  ...,   266,  3133,   963],
        [    0,   616,  2002,  ..., 19763,   642,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0175e-05,  1.2504e-04,  6.1833e-05,  ...,  1.8758e-04,
         -1.2632e-05,  1.8546e-04],
        [-7.6368e-06, -5.3048e-06,  4.9211e-06,  ..., -6.1020e-06,
         -4.1202e-06, -4.7572e-06],
        [-2.0295e-05, -1.4737e-05,  1.4141e-05,  ..., -1.6063e-05,
         -1.0923e-05, -1.2264e-05],
        [-1.5140e-05, -1.0274e-05,  1.0274e-05,  ..., -1.2010e-05,
         -8.0839e-06, -9.7975e-06],
        [-1.8567e-05, -1.3843e-05,  1.2755e-05,  ..., -1.4976e-05,
         -1.0699e-05, -1.0565e-05]], device='cuda:0')
Loss: 0.9754065275192261


Running epoch 1, step 1101, batch 53
Sampled inputs[:2]: tensor([[    0,  3256,   221,  ..., 18116,   292, 47989],
        [    0,   365,  1110,  ...,  4130,   221,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8344e-05,  1.9021e-04,  7.7280e-05,  ...,  1.2152e-04,
          1.2960e-04,  3.0470e-04],
        [-9.2015e-06, -6.5118e-06,  5.9195e-06,  ..., -7.4357e-06,
         -5.0254e-06, -5.6997e-06],
        [-2.4408e-05, -1.8016e-05,  1.6987e-05,  ..., -1.9550e-05,
         -1.3322e-05, -1.4707e-05],
        [-1.8090e-05, -1.2524e-05,  1.2256e-05,  ..., -1.4514e-05,
         -9.7826e-06, -1.1668e-05],
        [-2.2441e-05, -1.7017e-05,  1.5423e-05,  ..., -1.8328e-05,
         -1.3098e-05, -1.2740e-05]], device='cuda:0')
Loss: 0.9868714213371277


Running epoch 1, step 1102, batch 54
Sampled inputs[:2]: tensor([[    0,  1304,   292,  ...,  2101,   292,   474],
        [    0,    15,    83,  ...,  6030,    14, 14080]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5127e-04,  2.4119e-04, -3.7150e-06,  ...,  1.6358e-05,
          9.2942e-06,  2.0073e-04],
        [-1.0721e-05, -7.6964e-06,  6.9253e-06,  ..., -8.6725e-06,
         -5.9530e-06, -6.6757e-06],
        [-2.8610e-05, -2.1383e-05,  1.9968e-05,  ..., -2.2948e-05,
         -1.5825e-05, -1.7360e-05],
        [-2.1085e-05, -1.4804e-05,  1.4342e-05,  ..., -1.6928e-05,
         -1.1556e-05, -1.3679e-05],
        [-2.6241e-05, -2.0131e-05,  1.8060e-05,  ..., -2.1458e-05,
         -1.5512e-05, -1.5005e-05]], device='cuda:0')
Loss: 0.9954900741577148


Running epoch 1, step 1103, batch 55
Sampled inputs[:2]: tensor([[    0,   380,  8157,  ...,   943,   352,  2278],
        [    0,    13, 23070,  ...,   266,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2460e-05,  2.6092e-04,  6.6292e-05,  ...,  6.8539e-05,
          6.3357e-05,  2.0499e-04],
        [-1.2092e-05, -8.6874e-06,  7.9237e-06,  ..., -9.8199e-06,
         -6.7018e-06, -7.5139e-06],
        [-3.2261e-05, -2.4065e-05,  2.2873e-05,  ..., -2.5913e-05,
         -1.7762e-05, -1.9461e-05],
        [-2.3827e-05, -1.6727e-05,  1.6473e-05,  ..., -1.9208e-05,
         -1.3039e-05, -1.5438e-05],
        [-2.9519e-05, -2.2590e-05,  2.0638e-05,  ..., -2.4155e-05,
         -1.7367e-05, -1.6741e-05]], device='cuda:0')
Loss: 0.9771074652671814
Graident accumulation at epoch 1, step 1103, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0025,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.2522e-05,  1.4762e-04,  1.1566e-06,  ..., -5.1304e-05,
          8.2977e-05,  3.9339e-05],
        [-1.3026e-05, -9.0374e-06,  8.0925e-06,  ..., -1.1125e-05,
         -7.6089e-06, -8.2364e-06],
        [ 8.0370e-06,  1.3987e-05, -6.8664e-06,  ...,  1.4605e-05,
          1.2238e-05, -1.0478e-06],
        [-3.1911e-06,  6.1331e-06,  7.1615e-07,  ...,  2.4359e-06,
          7.1648e-06, -3.8051e-06],
        [-3.2804e-05, -2.4451e-05,  2.1520e-05,  ..., -2.8298e-05,
         -2.0349e-05, -1.9268e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2150e-08, 5.3418e-08, 6.3598e-08,  ..., 2.4259e-08, 1.4644e-07,
         3.4584e-08],
        [7.5280e-11, 4.3540e-11, 1.2035e-11,  ..., 5.1109e-11, 1.1420e-11,
         2.0147e-11],
        [3.4861e-09, 1.9834e-09, 1.0104e-09,  ..., 2.6832e-09, 8.3834e-10,
         9.1055e-10],
        [8.7666e-10, 6.6446e-10, 2.7294e-10,  ..., 7.3577e-10, 3.9541e-10,
         2.8373e-10],
        [3.5046e-10, 1.8974e-10, 5.3225e-11,  ..., 2.5649e-10, 4.5924e-11,
         9.1551e-11]], device='cuda:0')
optimizer state dict: 138.0
lr: [9.620588080367043e-06, 9.620588080367043e-06]
scheduler_last_epoch: 138


Running epoch 1, step 1104, batch 56
Sampled inputs[:2]: tensor([[   0,  328, 6379,  ...,  287, 1342,    9],
        [   0,   12,  689,  ..., 1110, 1712, 2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8363e-05, -1.1050e-05, -1.0010e-05,  ...,  2.2857e-05,
         -2.6399e-05,  4.6745e-05],
        [-1.4678e-06, -1.1474e-06,  1.0431e-06,  ..., -1.1846e-06,
         -7.9349e-07, -8.2329e-07],
        [-3.9339e-06, -3.1590e-06,  2.9504e-06,  ..., -3.1739e-06,
         -2.1160e-06, -2.1905e-06],
        [-3.0398e-06, -2.3097e-06,  2.2501e-06,  ..., -2.4140e-06,
         -1.5795e-06, -1.7732e-06],
        [-3.4720e-06, -2.8461e-06,  2.5630e-06,  ..., -2.8461e-06,
         -2.0117e-06, -1.8105e-06]], device='cuda:0')
Loss: 1.0001847743988037


Running epoch 1, step 1105, batch 57
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,   401,  1576,   271],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4328e-05,  8.8726e-05, -1.3936e-04,  ...,  1.3054e-05,
         -1.2200e-04, -4.2599e-05],
        [-2.8387e-06, -2.1309e-06,  2.0638e-06,  ..., -2.3246e-06,
         -1.5050e-06, -1.6652e-06],
        [-7.5996e-06, -5.8264e-06,  5.8860e-06,  ..., -6.1542e-06,
         -3.9563e-06, -4.3809e-06],
        [-5.8860e-06, -4.2617e-06,  4.5151e-06,  ..., -4.7386e-06,
         -2.9951e-06, -3.6210e-06],
        [-6.7651e-06, -5.3495e-06,  5.1707e-06,  ..., -5.5879e-06,
         -3.8296e-06, -3.6210e-06]], device='cuda:0')
Loss: 0.9630308151245117


Running epoch 1, step 1106, batch 58
Sampled inputs[:2]: tensor([[   0,  271,  957,  ..., 1597, 1276,  292],
        [   0, 6909,  352,  ..., 1075,  706, 6909]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5689e-04,  2.3567e-04, -1.6563e-04,  ...,  6.8171e-05,
         -1.0053e-04, -3.4443e-05],
        [-4.3809e-06, -3.1218e-06,  2.9840e-06,  ..., -3.6359e-06,
         -2.3209e-06, -2.6636e-06],
        [-1.1563e-05, -8.5235e-06,  8.5086e-06,  ..., -9.5069e-06,
         -6.0722e-06, -6.8694e-06],
        [-8.8513e-06, -6.1244e-06,  6.3926e-06,  ..., -7.2569e-06,
         -4.5598e-06, -5.6326e-06],
        [-1.0431e-05, -7.9125e-06,  7.5549e-06,  ..., -8.7321e-06,
         -5.9158e-06, -5.7817e-06]], device='cuda:0')
Loss: 0.9769548177719116


Running epoch 1, step 1107, batch 59
Sampled inputs[:2]: tensor([[    0,  2379,    13,  ...,   287,   259,  2193],
        [    0, 12305,  1179,  ...,  6321,   600,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0566e-04,  3.4326e-04, -1.2962e-04,  ...,  9.4795e-05,
         -1.5997e-04,  7.7701e-06],
        [-5.8934e-06, -4.2170e-06,  4.0121e-06,  ..., -4.8801e-06,
         -3.2112e-06, -3.5278e-06],
        [-1.5616e-05, -1.1563e-05,  1.1459e-05,  ..., -1.2815e-05,
         -8.4564e-06, -9.1642e-06],
        [-1.1832e-05, -8.2403e-06,  8.5235e-06,  ..., -9.7007e-06,
         -6.2957e-06, -7.4357e-06],
        [-1.4067e-05, -1.0744e-05,  1.0177e-05,  ..., -1.1772e-05,
         -8.2254e-06, -7.7188e-06]], device='cuda:0')
Loss: 0.9999511241912842


Running epoch 1, step 1108, batch 60
Sampled inputs[:2]: tensor([[   0, 8822, 1486,  ...,   12,  287, 6903],
        [   0, 1716, 1773,  ..., 5014,   12,  847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3052e-04,  3.5515e-04, -2.0198e-04,  ...,  8.5511e-05,
         -1.0809e-04,  2.6959e-05],
        [-7.5474e-06, -5.4389e-06,  5.0478e-06,  ..., -6.1914e-06,
         -4.1462e-06, -4.5486e-06],
        [-1.9938e-05, -1.4931e-05,  1.4365e-05,  ..., -1.6272e-05,
         -1.0975e-05, -1.1832e-05],
        [-1.4856e-05, -1.0476e-05,  1.0505e-05,  ..., -1.2115e-05,
         -8.0168e-06, -9.3877e-06],
        [-1.8060e-05, -1.3918e-05,  1.2830e-05,  ..., -1.5035e-05,
         -1.0699e-05, -1.0058e-05]], device='cuda:0')
Loss: 0.9769765734672546


Running epoch 1, step 1109, batch 61
Sampled inputs[:2]: tensor([[    0,  1101,   300,  ...,  6104,   367,   993],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4659e-04,  2.7936e-04, -2.4535e-04,  ...,  1.9071e-04,
         -4.6049e-05,  1.7495e-04],
        [-9.0823e-06, -6.5044e-06,  6.0387e-06,  ..., -7.4506e-06,
         -5.0180e-06, -5.5023e-06],
        [-2.3991e-05, -1.7837e-05,  1.7166e-05,  ..., -1.9580e-05,
         -1.3269e-05, -1.4305e-05],
        [-1.7896e-05, -1.2532e-05,  1.2562e-05,  ..., -1.4603e-05,
         -9.7230e-06, -1.1355e-05],
        [-2.1756e-05, -1.6645e-05,  1.5363e-05,  ..., -1.8105e-05,
         -1.2919e-05, -1.2174e-05]], device='cuda:0')
Loss: 1.0150022506713867


Running epoch 1, step 1110, batch 62
Sampled inputs[:2]: tensor([[   0, 3544,  417,  ...,  380,  381, 3794],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7490e-04,  3.3220e-04, -4.2748e-04,  ...,  1.5529e-04,
         -1.4234e-04, -6.7977e-05],
        [-1.0401e-05, -7.4729e-06,  6.9626e-06,  ..., -8.6352e-06,
         -5.8487e-06, -6.3702e-06],
        [-2.7522e-05, -2.0459e-05,  1.9848e-05,  ..., -2.2635e-05,
         -1.5371e-05, -1.6496e-05],
        [-2.0623e-05, -1.4417e-05,  1.4603e-05,  ..., -1.7002e-05,
         -1.1377e-05, -1.3240e-05],
        [-2.5064e-05, -1.9163e-05,  1.7837e-05,  ..., -2.0996e-05,
         -1.4991e-05, -1.4059e-05]], device='cuda:0')
Loss: 0.9832507371902466


Running epoch 1, step 1111, batch 63
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6224e-04,  3.0885e-04, -6.1513e-04,  ...,  1.8092e-04,
         -1.9550e-04, -1.0522e-04],
        [-1.1966e-05, -8.5905e-06,  7.9982e-06,  ..., -9.8720e-06,
         -6.6720e-06, -7.3612e-06],
        [-3.1784e-05, -2.3618e-05,  2.2888e-05,  ..., -2.6003e-05,
         -1.7621e-05, -1.9133e-05],
        [-2.3693e-05, -1.6563e-05,  1.6749e-05,  ..., -1.9416e-05,
         -1.2957e-05, -1.5251e-05],
        [-2.8908e-05, -2.2069e-05,  2.0504e-05,  ..., -2.4080e-05,
         -1.7151e-05, -1.6309e-05]], device='cuda:0')
Loss: 0.9589175581932068
Graident accumulation at epoch 1, step 1111, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0024,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0149e-04,  1.6374e-04, -6.0472e-05,  ..., -2.8082e-05,
          5.5130e-05,  2.4883e-05],
        [-1.2920e-05, -8.9927e-06,  8.0831e-06,  ..., -1.1000e-05,
         -7.5152e-06, -8.1489e-06],
        [ 4.0549e-06,  1.0227e-05, -3.8909e-06,  ...,  1.0544e-05,
          9.2520e-06, -2.8563e-06],
        [-5.2413e-06,  3.8636e-06,  2.3194e-06,  ...,  2.5072e-07,
          5.1527e-06, -4.9497e-06],
        [-3.2415e-05, -2.4213e-05,  2.1419e-05,  ..., -2.7876e-05,
         -2.0029e-05, -1.8972e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2229e-08, 5.3460e-08, 6.3913e-08,  ..., 2.4268e-08, 1.4634e-07,
         3.4561e-08],
        [7.5348e-11, 4.3571e-11, 1.2087e-11,  ..., 5.1156e-11, 1.1453e-11,
         2.0181e-11],
        [3.4836e-09, 1.9820e-09, 1.0099e-09,  ..., 2.6811e-09, 8.3781e-10,
         9.1000e-10],
        [8.7635e-10, 6.6407e-10, 2.7295e-10,  ..., 7.3541e-10, 3.9519e-10,
         2.8367e-10],
        [3.5095e-10, 1.9004e-10, 5.3592e-11,  ..., 2.5681e-10, 4.6172e-11,
         9.1725e-11]], device='cuda:0')
optimizer state dict: 139.0
lr: [9.497092607333449e-06, 9.497092607333449e-06]
scheduler_last_epoch: 139


Running epoch 1, step 1112, batch 64
Sampled inputs[:2]: tensor([[    0,  4347,   638,  ...,  1345,   292, 15343],
        [    0,  2355,  2728,  ...,   554,  9025,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1730e-04, -7.6807e-05,  1.9864e-05,  ..., -9.1598e-06,
          7.5875e-05,  2.6992e-05],
        [-1.4678e-06, -1.1101e-06,  1.0133e-06,  ..., -1.2219e-06,
         -8.7544e-07, -8.6799e-07],
        [-3.9041e-06, -3.0100e-06,  2.8461e-06,  ..., -3.2037e-06,
         -2.2799e-06, -2.2501e-06],
        [-2.9653e-06, -2.1309e-06,  2.1458e-06,  ..., -2.4289e-06,
         -1.6913e-06, -1.8328e-06],
        [-3.5465e-06, -2.8163e-06,  2.5481e-06,  ..., -2.9802e-06,
         -2.2054e-06, -1.9222e-06]], device='cuda:0')
Loss: 1.0056264400482178


Running epoch 1, step 1113, batch 65
Sampled inputs[:2]: tensor([[   0,  437,  266,  ..., 5512,  822,   89],
        [   0, 9430,  287,  ..., 1141, 2280,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0438e-04,  7.6034e-05,  7.7239e-05,  ..., -2.1582e-05,
          1.1962e-04,  6.1889e-05],
        [-2.9653e-06, -2.1979e-06,  2.0117e-06,  ..., -2.4736e-06,
         -1.7621e-06, -1.7434e-06],
        [-7.8380e-06, -5.9903e-06,  5.6624e-06,  ..., -6.4820e-06,
         -4.6194e-06, -4.5151e-06],
        [-5.9456e-06, -4.2468e-06,  4.2170e-06,  ..., -4.9323e-06,
         -3.4422e-06, -3.6806e-06],
        [-7.0781e-06, -5.5730e-06,  5.0366e-06,  ..., -5.9754e-06,
         -4.4554e-06, -3.8296e-06]], device='cuda:0')
Loss: 0.9935582280158997


Running epoch 1, step 1114, batch 66
Sampled inputs[:2]: tensor([[    0,    13, 10036,  ...,   328,  2347, 12801],
        [    0,    21,   292,  ...,    13,  1861,  4254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6149e-04,  5.9916e-05,  1.9488e-04,  ...,  1.6933e-04,
         -1.3484e-04,  8.4676e-05],
        [-4.5821e-06, -3.3006e-06,  2.8759e-06,  ..., -3.7923e-06,
         -2.7232e-06, -2.8610e-06],
        [ 1.2756e-04,  1.5731e-04, -1.1352e-04,  ...,  6.2729e-05,
          7.8585e-05,  2.5008e-05],
        [-9.0897e-06, -6.3628e-06,  5.9903e-06,  ..., -7.4804e-06,
         -5.2899e-06, -5.9009e-06],
        [-1.1161e-05, -8.4937e-06,  7.3463e-06,  ..., -9.3281e-06,
         -7.0184e-06, -6.4820e-06]], device='cuda:0')
Loss: 0.9728149771690369


Running epoch 1, step 1115, batch 67
Sampled inputs[:2]: tensor([[    0,  2372,  1319,  ...,  1253,   292, 34166],
        [    0,  1234,   278,  ...,  8635,   271,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0753e-04,  1.3277e-04,  8.6278e-05,  ...,  8.9027e-05,
         -1.8089e-04,  5.8092e-05],
        [-6.1169e-06, -4.3958e-06,  3.9488e-06,  ..., -5.0068e-06,
         -3.4906e-06, -3.6992e-06],
        [ 1.2357e-04,  1.5433e-04, -1.1054e-04,  ...,  5.9570e-05,
          7.6573e-05,  2.2877e-05],
        [-1.2144e-05, -8.5086e-06,  8.2254e-06,  ..., -9.8795e-06,
         -6.7875e-06, -7.6443e-06],
        [-1.4648e-05, -1.1191e-05,  9.9242e-06,  ..., -1.2159e-05,
         -8.9407e-06, -8.2180e-06]], device='cuda:0')
Loss: 0.9885769486427307


Running epoch 1, step 1116, batch 68
Sampled inputs[:2]: tensor([[    0,  4902,   518,  ...,  5493,  3227,   278],
        [    0,   609,   271,  ...,   287, 15506, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5350e-04,  1.4124e-04,  1.0361e-04,  ...,  2.0047e-04,
         -9.1298e-05,  5.8092e-05],
        [-7.7412e-06, -5.4389e-06,  4.9844e-06,  ..., -6.2659e-06,
         -4.3511e-06, -4.7125e-06],
        [ 1.1925e-04,  1.5139e-04, -1.0759e-04,  ...,  5.6232e-05,
          7.4263e-05,  2.0225e-05],
        [-1.5318e-05, -1.0535e-05,  1.0356e-05,  ..., -1.2323e-05,
         -8.4639e-06, -9.7007e-06],
        [-1.8552e-05, -1.3947e-05,  1.2562e-05,  ..., -1.5259e-05,
         -1.1191e-05, -1.0498e-05]], device='cuda:0')
Loss: 1.0009961128234863


Running epoch 1, step 1117, batch 69
Sampled inputs[:2]: tensor([[    0,  6574,  1707,  ...,    14,  5077,    12],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3721e-04,  1.5019e-04,  9.3564e-05,  ...,  3.3073e-04,
         -1.7581e-04,  2.8049e-04],
        [-9.2611e-06, -6.4671e-06,  5.8524e-06,  ..., -7.5176e-06,
         -5.3123e-06, -5.8077e-06],
        [ 1.1516e-04,  1.4854e-04, -1.0503e-04,  ...,  5.2984e-05,
          7.1790e-05,  1.7438e-05],
        [-1.8373e-05, -1.2532e-05,  1.2189e-05,  ..., -1.4797e-05,
         -1.0349e-05, -1.1951e-05],
        [-2.2396e-05, -1.6645e-05,  1.4961e-05,  ..., -1.8358e-05,
         -1.3620e-05, -1.2971e-05]], device='cuda:0')
Loss: 1.0052947998046875


Running epoch 1, step 1118, batch 70
Sampled inputs[:2]: tensor([[    0,   360,  5323,  ..., 29974,    25,    27],
        [    0,  9058,  4048,  ...,    14,   759,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6359e-04,  6.8357e-05,  4.8435e-05,  ...,  3.4927e-04,
         -1.0851e-04,  2.0965e-04],
        [-1.0803e-05, -7.5698e-06,  7.0296e-06,  ..., -8.7395e-06,
         -6.0946e-06, -6.6347e-06],
        [ 1.1099e-04,  1.4546e-04, -1.0169e-04,  ...,  4.9661e-05,
          6.9659e-05,  1.5158e-05],
        [-2.1547e-05, -1.4767e-05,  1.4707e-05,  ..., -1.7285e-05,
         -1.1906e-05, -1.3739e-05],
        [-2.5928e-05, -1.9357e-05,  1.7747e-05,  ..., -2.1234e-05,
         -1.5587e-05, -1.4789e-05]], device='cuda:0')
Loss: 1.0078920125961304


Running epoch 1, step 1119, batch 71
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5861e-04,  1.7463e-05,  9.0987e-05,  ...,  3.1822e-04,
         -2.3979e-04,  1.2873e-04],
        [-1.2256e-05, -8.6427e-06,  8.0504e-06,  ..., -9.9614e-06,
         -6.9179e-06, -7.4692e-06],
        [ 1.0718e-04,  1.4255e-04, -9.8842e-05,  ...,  4.6502e-05,
          6.7558e-05,  1.3027e-05],
        [-2.4527e-05, -1.6898e-05,  1.6883e-05,  ..., -1.9759e-05,
         -1.3538e-05, -1.5534e-05],
        [-2.9281e-05, -2.1994e-05,  2.0206e-05,  ..., -2.4080e-05,
         -1.7583e-05, -1.6563e-05]], device='cuda:0')
Loss: 0.9912958145141602
Graident accumulation at epoch 1, step 1119, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0024,  ..., -0.0021,  0.0235, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1721e-04,  1.4911e-04, -4.5326e-05,  ...,  6.5482e-06,
          2.5638e-05,  3.5267e-05],
        [-1.2854e-05, -8.9577e-06,  8.0798e-06,  ..., -1.0896e-05,
         -7.4555e-06, -8.0809e-06],
        [ 1.4367e-05,  2.3459e-05, -1.3386e-05,  ...,  1.4140e-05,
          1.5083e-05, -1.2680e-06],
        [-7.1699e-06,  1.7874e-06,  3.7758e-06,  ..., -1.7502e-06,
          3.2837e-06, -6.0082e-06],
        [-3.2101e-05, -2.3991e-05,  2.1297e-05,  ..., -2.7496e-05,
         -1.9785e-05, -1.8731e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2244e-08, 5.3407e-08, 6.3857e-08,  ..., 2.4345e-08, 1.4625e-07,
         3.4543e-08],
        [7.5423e-11, 4.3602e-11, 1.2140e-11,  ..., 5.1204e-11, 1.1489e-11,
         2.0217e-11],
        [3.4916e-09, 2.0004e-09, 1.0187e-09,  ..., 2.6806e-09, 8.4154e-10,
         9.0926e-10],
        [8.7607e-10, 6.6369e-10, 2.7296e-10,  ..., 7.3507e-10, 3.9497e-10,
         2.8363e-10],
        [3.5145e-10, 1.9033e-10, 5.3947e-11,  ..., 2.5713e-10, 4.6435e-11,
         9.1908e-11]], device='cuda:0')
optimizer state dict: 140.0
lr: [9.373673982939395e-06, 9.373673982939395e-06]
scheduler_last_epoch: 140


Running epoch 1, step 1120, batch 72
Sampled inputs[:2]: tensor([[    0,  5489,    80,  ...,   221,   380,   333],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4417e-05,  4.8740e-05, -9.0203e-05,  ...,  5.7905e-05,
         -1.2926e-04,  6.5826e-05],
        [-1.4305e-06, -9.9093e-07,  9.7603e-07,  ..., -1.2591e-06,
         -9.0897e-07, -8.9779e-07],
        [-3.6955e-06, -2.5928e-06,  2.7418e-06,  ..., -3.1590e-06,
         -2.2799e-06, -2.2054e-06],
        [-2.8610e-06, -1.9073e-06,  2.0713e-06,  ..., -2.5034e-06,
         -1.7881e-06, -1.8775e-06],
        [-3.3230e-06, -2.4140e-06,  2.4438e-06,  ..., -2.8908e-06,
         -2.1756e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.9669848084449768


Running epoch 1, step 1121, batch 73
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,  1796,   342,  ...,   668,  2903,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8228e-05,  4.8740e-05, -3.5978e-05,  ...,  3.8346e-05,
         -2.4182e-05,  1.8363e-04],
        [-2.9579e-06, -2.1011e-06,  1.9819e-06,  ..., -2.4959e-06,
         -1.7807e-06, -1.7695e-06],
        [-7.7486e-06, -5.6326e-06,  5.6028e-06,  ..., -6.4224e-06,
         -4.6045e-06, -4.4852e-06],
        [-5.9456e-06, -4.1127e-06,  4.2021e-06,  ..., -4.9919e-06,
         -3.5241e-06, -3.7253e-06],
        [-6.8098e-06, -5.1260e-06,  4.8727e-06,  ..., -5.7667e-06,
         -4.3213e-06, -3.6880e-06]], device='cuda:0')
Loss: 0.9955875873565674


Running epoch 1, step 1122, batch 74
Sampled inputs[:2]: tensor([[    0,   328,   471,  ..., 11137,   679,  6585],
        [    0,  3235,   471,  ...,  1967,  4273,  2738]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3222e-04,  1.1185e-04, -2.1176e-05,  ...,  1.2733e-04,
          1.3910e-04,  1.8363e-04],
        [-4.4629e-06, -3.2336e-06,  2.8126e-06,  ..., -3.8370e-06,
         -2.7940e-06, -2.7306e-06],
        [-1.1653e-05, -8.6427e-06,  8.0019e-06,  ..., -9.8348e-06,
         -7.1973e-06, -6.8843e-06],
        [-9.0003e-06, -6.3777e-06,  5.9754e-06,  ..., -7.7039e-06,
         -5.5805e-06, -5.7369e-06],
        [-1.0401e-05, -7.9572e-06,  7.0930e-06,  ..., -8.9407e-06,
         -6.7949e-06, -5.7593e-06]], device='cuda:0')
Loss: 1.0107288360595703


Running epoch 1, step 1123, batch 75
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0, 23487,   273,  ...,   368,   259,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6236e-04,  1.3845e-04,  3.0118e-05,  ...,  9.3856e-05,
          4.7000e-04,  3.2834e-04],
        [-6.0573e-06, -4.3884e-06,  3.8482e-06,  ..., -5.1260e-06,
         -3.7700e-06, -3.7067e-06],
        [-1.5914e-05, -1.1787e-05,  1.0937e-05,  ..., -1.3232e-05,
         -9.7603e-06, -9.4771e-06],
        [-1.2189e-05, -8.5980e-06,  8.1360e-06,  ..., -1.0222e-05,
         -7.4506e-06, -7.7486e-06],
        [-1.4201e-05, -1.0878e-05,  9.6709e-06,  ..., -1.2070e-05,
         -9.2685e-06, -7.9647e-06]], device='cuda:0')
Loss: 1.0125356912612915


Running epoch 1, step 1124, batch 76
Sampled inputs[:2]: tensor([[    0,    14, 13078,  ...,  1994,    12,   287],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7244e-04,  1.9879e-04,  2.0813e-04,  ...,  1.1149e-04,
          7.4700e-04,  4.4834e-04],
        [-7.6890e-06, -5.4985e-06,  4.7833e-06,  ..., -6.4522e-06,
         -4.7311e-06, -4.7572e-06],
        [-2.0117e-05, -1.4782e-05,  1.3560e-05,  ..., -1.6615e-05,
         -1.2219e-05, -1.2115e-05],
        [-1.5363e-05, -1.0714e-05,  1.0036e-05,  ..., -1.2770e-05,
         -9.2834e-06, -9.8646e-06],
        [-1.8001e-05, -1.3649e-05,  1.2010e-05,  ..., -1.5184e-05,
         -1.1638e-05, -1.0215e-05]], device='cuda:0')
Loss: 0.9861274361610413


Running epoch 1, step 1125, batch 77
Sampled inputs[:2]: tensor([[   0,  292,   17,  ..., 5760, 1345,  578],
        [   0, 1086,   26,  ...,  298,  527,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5999e-04,  3.6119e-04,  1.3852e-04,  ...,  7.3421e-05,
          5.4742e-04,  5.0698e-04],
        [-9.3430e-06, -6.6012e-06,  5.6960e-06,  ..., -7.8157e-06,
         -5.6997e-06, -5.8822e-06],
        [-2.4229e-05, -1.7613e-05,  1.6093e-05,  ..., -1.9953e-05,
         -1.4603e-05, -1.4827e-05],
        [-1.8537e-05, -1.2785e-05,  1.1899e-05,  ..., -1.5363e-05,
         -1.1124e-05, -1.2100e-05],
        [-2.1964e-05, -1.6421e-05,  1.4409e-05,  ..., -1.8448e-05,
         -1.4067e-05, -1.2673e-05]], device='cuda:0')
Loss: 0.9412949085235596


Running epoch 1, step 1126, batch 78
Sampled inputs[:2]: tensor([[    0,  2348,   565,  ...,    12,   709,   266],
        [    0,   680,   401,  ...,  2872,   292, 23535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7957e-04,  3.2841e-04,  6.2456e-06,  ...,  5.7848e-05,
          2.8902e-04,  2.1286e-04],
        [-1.0699e-05, -7.4431e-06,  6.6310e-06,  ..., -8.9556e-06,
         -6.4224e-06, -6.7651e-06],
        [-2.7746e-05, -1.9804e-05,  1.8775e-05,  ..., -2.2784e-05,
         -1.6391e-05, -1.6943e-05],
        [-2.1309e-05, -1.4320e-05,  1.3955e-05,  ..., -1.7628e-05,
         -1.2524e-05, -1.3970e-05],
        [-2.5123e-05, -1.8507e-05,  1.6779e-05,  ..., -2.1040e-05,
         -1.5818e-05, -1.4402e-05]], device='cuda:0')
Loss: 0.9093751311302185


Running epoch 1, step 1127, batch 79
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0,  199,  769,  ...,  685, 1423,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5025e-04,  1.6230e-04, -1.6686e-04,  ...,  1.7504e-04,
          3.4340e-04,  1.7197e-04],
        [-1.2212e-05, -8.3856e-06,  7.5214e-06,  ..., -1.0245e-05,
         -7.3090e-06, -7.8455e-06],
        [-3.1680e-05, -2.2277e-05,  2.1353e-05,  ..., -2.5988e-05,
         -1.8582e-05, -1.9521e-05],
        [-2.4319e-05, -1.6093e-05,  1.5862e-05,  ..., -2.0131e-05,
         -1.4223e-05, -1.6131e-05],
        [-2.8804e-05, -2.0832e-05,  1.9163e-05,  ..., -2.4065e-05,
         -1.7978e-05, -1.6682e-05]], device='cuda:0')
Loss: 0.9368202090263367
Graident accumulation at epoch 1, step 1127, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5051e-04,  1.5043e-04, -5.7480e-05,  ...,  2.3397e-05,
          5.7414e-05,  4.8937e-05],
        [-1.2789e-05, -8.9005e-06,  8.0240e-06,  ..., -1.0831e-05,
         -7.4408e-06, -8.0574e-06],
        [ 9.7623e-06,  1.8886e-05, -9.9121e-06,  ...,  1.0127e-05,
          1.1716e-05, -3.0932e-06],
        [-8.8848e-06, -6.5497e-10,  4.9844e-06,  ..., -3.5884e-06,
          1.5330e-06, -7.0204e-06],
        [-3.1772e-05, -2.3675e-05,  2.1084e-05,  ..., -2.7153e-05,
         -1.9604e-05, -1.8526e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2394e-08, 5.3380e-08, 6.3821e-08,  ..., 2.4351e-08, 1.4622e-07,
         3.4538e-08],
        [7.5496e-11, 4.3628e-11, 1.2184e-11,  ..., 5.1257e-11, 1.1531e-11,
         2.0258e-11],
        [3.4892e-09, 1.9989e-09, 1.0181e-09,  ..., 2.6786e-09, 8.4104e-10,
         9.0874e-10],
        [8.7579e-10, 6.6329e-10, 2.7294e-10,  ..., 7.3474e-10, 3.9478e-10,
         2.8361e-10],
        [3.5193e-10, 1.9058e-10, 5.4260e-11,  ..., 2.5746e-10, 4.6712e-11,
         9.2094e-11]], device='cuda:0')
optimizer state dict: 141.0
lr: [9.250351066628025e-06, 9.250351066628025e-06]
scheduler_last_epoch: 141


Running epoch 1, step 1128, batch 80
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,  1250,  1797,  ...,   266,  1417,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3925e-05, -7.0419e-05,  3.9642e-06,  ...,  3.5052e-05,
          9.4649e-05,  8.3254e-05],
        [-1.5721e-06, -9.6858e-07,  1.0058e-06,  ..., -1.2591e-06,
         -9.4250e-07, -9.6112e-07],
        [-4.0233e-06, -2.6375e-06,  2.7716e-06,  ..., -3.2037e-06,
         -2.4289e-06, -2.4289e-06],
        [-3.1441e-06, -1.8850e-06,  2.1011e-06,  ..., -2.5034e-06,
         -1.8775e-06, -2.0266e-06],
        [-3.5614e-06, -2.4289e-06,  2.4140e-06,  ..., -2.8908e-06,
         -2.2799e-06, -2.0117e-06]], device='cuda:0')
Loss: 0.9907975792884827


Running epoch 1, step 1129, batch 81
Sampled inputs[:2]: tensor([[    0,  3829,   278,  ..., 11978,     9,   968],
        [    0,  6518,   681,  ...,   401,  9748,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7812e-06, -8.9044e-05,  1.0822e-04,  ...,  4.5597e-05,
          3.7722e-04,  1.0496e-04],
        [-3.2112e-06, -1.9968e-06,  2.0266e-06,  ..., -2.6077e-06,
         -1.8850e-06, -2.0117e-06],
        [-8.1658e-06, -5.3644e-06,  5.5730e-06,  ..., -6.5863e-06,
         -4.8131e-06, -5.0366e-06],
        [-6.3479e-06, -3.8370e-06,  4.2021e-06,  ..., -5.1260e-06,
         -3.6880e-06, -4.1574e-06],
        [-7.3612e-06, -5.0068e-06,  4.9174e-06,  ..., -6.0499e-06,
         -4.6194e-06, -4.2766e-06]], device='cuda:0')
Loss: 1.0088307857513428


Running epoch 1, step 1130, batch 82
Sampled inputs[:2]: tensor([[    0,  3388,   278,  ...,  7203,   271,  1746],
        [    0, 35449,   824,  ...,   278, 30449,  3659]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2924e-05, -1.8498e-04,  3.5904e-05,  ..., -3.5301e-06,
          3.2020e-04,  4.2517e-05],
        [-4.7758e-06, -3.0026e-06,  3.0920e-06,  ..., -3.8743e-06,
         -2.7828e-06, -2.9393e-06],
        [-1.2219e-05, -8.0615e-06,  8.5384e-06,  ..., -9.8050e-06,
         -7.0930e-06, -7.3761e-06],
        [-9.5069e-06, -5.7891e-06,  6.4522e-06,  ..., -7.6294e-06,
         -5.4315e-06, -6.1095e-06],
        [-1.0893e-05, -7.4506e-06,  7.4804e-06,  ..., -8.9258e-06,
         -6.7651e-06, -6.1765e-06]], device='cuda:0')
Loss: 0.9889285564422607


Running epoch 1, step 1131, batch 83
Sampled inputs[:2]: tensor([[   0, 2377,  271,  ...,  395,  394,   14],
        [   0,  908,   14,  ...,   19,   27,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2924e-05, -2.3740e-04,  2.0661e-05,  ...,  8.8802e-05,
          3.7086e-04,  1.2263e-05],
        [-6.4746e-06, -4.0978e-06,  4.2841e-06,  ..., -5.1484e-06,
         -3.6582e-06, -3.8408e-06],
        [-1.6570e-05, -1.0982e-05,  1.1742e-05,  ..., -1.3113e-05,
         -9.4026e-06, -9.7007e-06],
        [-1.2860e-05, -7.9051e-06,  8.9258e-06,  ..., -1.0133e-05,
         -7.1302e-06, -7.9796e-06],
        [-1.4529e-05, -1.0014e-05,  1.0133e-05,  ..., -1.1757e-05,
         -8.8513e-06, -8.0317e-06]], device='cuda:0')
Loss: 0.990726113319397


Running epoch 1, step 1132, batch 84
Sampled inputs[:2]: tensor([[    0, 13576,   431,  ...,    14,   475,   298],
        [    0,   221,   422,  ...,  2693,   733,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6681e-05, -1.0318e-04,  1.3479e-04,  ...,  1.8331e-04,
          4.0451e-04,  1.8262e-04],
        [-8.1509e-06, -5.1335e-06,  5.2117e-06,  ..., -6.4671e-06,
         -4.6790e-06, -4.9882e-06],
        [-2.0921e-05, -1.3754e-05,  1.4320e-05,  ..., -1.6481e-05,
         -1.2010e-05, -1.2606e-05],
        [-1.6108e-05, -9.8571e-06,  1.0811e-05,  ..., -1.2666e-05,
         -9.0376e-06, -1.0259e-05],
        [-1.8641e-05, -1.2696e-05,  1.2547e-05,  ..., -1.5005e-05,
         -1.1444e-05, -1.0669e-05]], device='cuda:0')
Loss: 0.9928770661354065


Running epoch 1, step 1133, batch 85
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1500e-05, -9.9657e-05,  5.3335e-05,  ...,  1.4777e-04,
          3.3789e-04,  2.5873e-04],
        [-9.8050e-06, -6.2361e-06,  6.2026e-06,  ..., -7.7710e-06,
         -5.6624e-06, -6.0089e-06],
        [-2.5243e-05, -1.6779e-05,  1.7092e-05,  ..., -1.9893e-05,
         -1.4588e-05, -1.5259e-05],
        [-1.9476e-05, -1.2033e-05,  1.2912e-05,  ..., -1.5289e-05,
         -1.0990e-05, -1.2435e-05],
        [-2.2545e-05, -1.5512e-05,  1.4991e-05,  ..., -1.8150e-05,
         -1.3918e-05, -1.2949e-05]], device='cuda:0')
Loss: 0.9884681105613708


Running epoch 1, step 1134, batch 86
Sampled inputs[:2]: tensor([[    0,   266,  1784,  ...,  1119,  1276,   292],
        [    0,   368, 46614,  ...,  1070,   278,  1028]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1348e-05, -2.0487e-05,  4.8837e-05,  ...,  3.1344e-04,
          6.1309e-04,  4.4419e-04],
        [-1.1407e-05, -7.2718e-06,  7.1079e-06,  ..., -9.1121e-06,
         -6.6459e-06, -7.1488e-06],
        [-2.9385e-05, -1.9535e-05,  1.9655e-05,  ..., -2.3291e-05,
         -1.7077e-05, -1.8075e-05],
        [-2.2605e-05, -1.3985e-05,  1.4760e-05,  ..., -1.7881e-05,
         -1.2875e-05, -1.4745e-05],
        [-2.6330e-05, -1.8105e-05,  1.7315e-05,  ..., -2.1294e-05,
         -1.6317e-05, -1.5363e-05]], device='cuda:0')
Loss: 0.9819837808609009


Running epoch 1, step 1135, batch 87
Sampled inputs[:2]: tensor([[   0,  380,  333,  ...,  333,  199, 2038],
        [   0, 4073, 1548,  ...,  292,  221,  301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4881e-05, -2.2913e-06, -1.2184e-05,  ...,  3.1399e-04,
          5.2788e-04,  3.5469e-04],
        [-1.3001e-05, -8.3297e-06,  8.1286e-06,  ..., -1.0394e-05,
         -7.5623e-06, -8.0876e-06],
        [-3.3438e-05, -2.2337e-05,  2.2426e-05,  ..., -2.6539e-05,
         -1.9401e-05, -2.0444e-05],
        [-2.5824e-05, -1.6041e-05,  1.6905e-05,  ..., -2.0444e-05,
         -1.4685e-05, -1.6727e-05],
        [-2.9892e-05, -2.0653e-05,  1.9714e-05,  ..., -2.4214e-05,
         -1.8522e-05, -1.7345e-05]], device='cuda:0')
Loss: 0.9702083468437195
Graident accumulation at epoch 1, step 1135, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2897e-04,  1.3516e-04, -5.2950e-05,  ...,  5.2457e-05,
          1.0446e-04,  7.9513e-05],
        [-1.2811e-05, -8.8434e-06,  8.0344e-06,  ..., -1.0787e-05,
         -7.4530e-06, -8.0604e-06],
        [ 5.4422e-06,  1.4763e-05, -6.6782e-06,  ...,  6.4606e-06,
          8.6044e-06, -4.8284e-06],
        [-1.0579e-05, -1.6047e-06,  6.1765e-06,  ..., -5.2740e-06,
         -8.8826e-08, -7.9910e-06],
        [-3.1584e-05, -2.3373e-05,  2.0947e-05,  ..., -2.6859e-05,
         -1.9496e-05, -1.8408e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2346e-08, 5.3326e-08, 6.3758e-08,  ..., 2.4425e-08, 1.4635e-07,
         3.4629e-08],
        [7.5590e-11, 4.3654e-11, 1.2238e-11,  ..., 5.1314e-11, 1.1577e-11,
         2.0303e-11],
        [3.4868e-09, 1.9974e-09, 1.0176e-09,  ..., 2.6766e-09, 8.4058e-10,
         9.0824e-10],
        [8.7558e-10, 6.6288e-10, 2.7295e-10,  ..., 7.3442e-10, 3.9460e-10,
         2.8360e-10],
        [3.5247e-10, 1.9081e-10, 5.4594e-11,  ..., 2.5779e-10, 4.7008e-11,
         9.2303e-11]], device='cuda:0')
optimizer state dict: 142.0
lr: [9.12714270321745e-06, 9.12714270321745e-06]
scheduler_last_epoch: 142


Running epoch 1, step 1136, batch 88
Sampled inputs[:2]: tensor([[    0, 28926,   266,  ...,  1061,  2615,    13],
        [    0,  4672,   278,  ...,  7523,  2305,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0962e-04, -1.9470e-05, -5.2623e-06,  ..., -5.5270e-05,
          8.8843e-05, -2.4420e-05],
        [-1.5795e-06, -9.6858e-07,  1.0356e-06,  ..., -1.3262e-06,
         -9.6858e-07, -1.0133e-06],
        [-3.9339e-06, -2.4885e-06,  2.7567e-06,  ..., -3.2485e-06,
         -2.3842e-06, -2.4587e-06],
        [-3.1590e-06, -1.8552e-06,  2.1607e-06,  ..., -2.6226e-06,
         -1.9073e-06, -2.1160e-06],
        [ 1.4380e-04,  1.2493e-04, -1.1265e-04,  ...,  1.1841e-04,
          1.2166e-04,  5.6300e-05]], device='cuda:0')
Loss: 1.0149952173233032


Running epoch 1, step 1137, batch 89
Sampled inputs[:2]: tensor([[    0,  7849,   278,  ...,   346,   462,   221],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1679e-05, -1.3680e-04, -1.0442e-05,  ..., -2.9349e-05,
          2.8886e-04, -1.7368e-05],
        [-3.0324e-06, -1.8775e-06,  1.9744e-06,  ..., -2.5854e-06,
         -1.9222e-06, -2.0713e-06],
        [-7.4655e-06, -4.8280e-06,  5.2750e-06,  ..., -6.2138e-06,
         -4.6492e-06, -4.8429e-06],
        [-6.0201e-06, -3.5465e-06,  4.1425e-06,  ..., -5.0664e-06,
         -3.7700e-06, -4.2766e-06],
        [ 1.4049e-04,  1.2270e-04, -1.1033e-04,  ...,  1.1561e-04,
          1.1944e-04,  5.4214e-05]], device='cuda:0')
Loss: 0.9415315985679626


Running epoch 1, step 1138, batch 90
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,  4599,  9005,  ...,   809,    13,  1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5206e-04, -1.7459e-04, -1.1100e-05,  ..., -4.9766e-05,
          2.3554e-04, -1.1424e-04],
        [-4.6492e-06, -2.9430e-06,  3.0696e-06,  ..., -3.8743e-06,
         -2.8573e-06, -3.0845e-06],
        [-1.1668e-05, -7.6890e-06,  8.2999e-06,  ..., -9.5516e-06,
         -7.0781e-06, -7.4655e-06],
        [-9.3281e-06, -5.6624e-06,  6.4820e-06,  ..., -7.6741e-06,
         -5.6326e-06, -6.4522e-06],
        [ 1.3681e-04,  1.2006e-04, -1.0770e-04,  ...,  1.1260e-04,
          1.1713e-04,  5.2053e-05]], device='cuda:0')
Loss: 1.0159912109375


Running epoch 1, step 1139, batch 91
Sampled inputs[:2]: tensor([[   0, 1603,  694,  ...,   36,   18,  298],
        [   0,  587,  292,  ...,   12,  287, 2261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8239e-05, -1.5943e-04, -1.1100e-05,  ..., -9.0335e-05,
          2.9703e-04,  7.4149e-05],
        [-6.3404e-06, -3.8967e-06,  3.9376e-06,  ..., -5.3048e-06,
         -3.9078e-06, -4.3735e-06],
        [-1.5661e-05, -1.0058e-05,  1.0610e-05,  ..., -1.2815e-05,
         -9.5069e-06, -1.0341e-05],
        [-1.2472e-05, -7.3612e-06,  8.1882e-06,  ..., -1.0312e-05,
         -7.5698e-06, -8.9109e-06],
        [ 1.3299e-04,  1.1777e-04, -1.0548e-04,  ...,  1.0944e-04,
          1.1470e-04,  4.9461e-05]], device='cuda:0')
Loss: 0.9354516267776489


Running epoch 1, step 1140, batch 92
Sampled inputs[:2]: tensor([[    0,   775,   721,  ...,  5650,   518, 11548],
        [    0,   992,   409,  ...,  5843,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0859e-04, -2.4424e-04, -6.7650e-05,  ..., -3.8572e-05,
          2.9703e-04,  1.4395e-04],
        [-8.0168e-06, -4.8354e-06,  5.0478e-06,  ..., -6.5565e-06,
         -4.7721e-06, -5.4911e-06],
        [-2.0042e-05, -1.2636e-05,  1.3664e-05,  ..., -1.6108e-05,
         -1.1787e-05, -1.3262e-05],
        [-1.5780e-05, -9.1642e-06,  1.0468e-05,  ..., -1.2755e-05,
         -9.2313e-06, -1.1191e-05],
        [ 1.2927e-04,  1.1549e-04, -1.0294e-04,  ...,  1.0658e-04,
          1.1260e-04,  4.7106e-05]], device='cuda:0')
Loss: 0.9846892356872559


Running epoch 1, step 1141, batch 93
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,    12,  4856,  ...,   342,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6498e-05, -3.4043e-04, -4.8211e-05,  ...,  9.1972e-06,
          3.4226e-04,  2.8009e-04],
        [-9.6112e-06, -5.7966e-06,  6.0908e-06,  ..., -7.8455e-06,
         -5.7481e-06, -6.5789e-06],
        [ 6.0418e-05,  6.1274e-05, -3.5241e-05,  ...,  4.2066e-05,
          4.4957e-05, -1.1211e-05],
        [-1.8999e-05, -1.1019e-05,  1.2659e-05,  ..., -1.5318e-05,
         -1.1154e-05, -1.3471e-05],
        [ 1.2560e-04,  1.1312e-04, -1.0042e-04,  ...,  1.0359e-04,
          1.1025e-04,  4.4796e-05]], device='cuda:0')
Loss: 0.9910189509391785


Running epoch 1, step 1142, batch 94
Sampled inputs[:2]: tensor([[    0,  5896,   352,  ...,  1168,   767,  1390],
        [    0,   508,  3282,  ...,   334,   287, 31884]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6441e-05, -3.5283e-04, -5.6320e-05,  ..., -2.2255e-05,
          5.5080e-04,  1.6279e-04],
        [-1.1332e-05, -6.7241e-06,  7.1414e-06,  ..., -9.1642e-06,
         -6.7018e-06, -7.7263e-06],
        [ 5.6157e-05,  5.8830e-05, -3.2410e-05,  ...,  3.8803e-05,
          4.2543e-05, -1.3997e-05],
        [-2.2203e-05, -1.2703e-05,  1.4745e-05,  ..., -1.7762e-05,
         -1.2919e-05, -1.5676e-05],
        [ 1.2173e-04,  1.1084e-04, -9.7885e-05,  ...,  1.0058e-04,
          1.0792e-04,  4.2412e-05]], device='cuda:0')
Loss: 0.9919712543487549


Running epoch 1, step 1143, batch 95
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2233e-04, -3.3505e-04, -5.3750e-05,  ..., -6.2197e-05,
          5.7453e-04,  1.9010e-04],
        [-1.2979e-05, -7.7300e-06,  8.1770e-06,  ..., -1.0394e-05,
         -7.6778e-06, -8.8587e-06],
        [ 5.1895e-05,  5.6073e-05, -2.9534e-05,  ...,  3.5629e-05,
          3.9995e-05, -1.6873e-05],
        [-2.5585e-05, -1.4715e-05,  1.6965e-05,  ..., -2.0266e-05,
         -1.4901e-05, -1.8090e-05],
        [ 1.1785e-04,  1.0829e-04, -9.5307e-05,  ...,  9.7656e-05,
          1.0551e-04,  3.9954e-05]], device='cuda:0')
Loss: 0.9876293540000916
Graident accumulation at epoch 1, step 1143, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.3842e-05,  8.8139e-05, -5.3030e-05,  ...,  4.0991e-05,
          1.5147e-04,  9.0572e-05],
        [-1.2827e-05, -8.7321e-06,  8.0487e-06,  ..., -1.0748e-05,
         -7.4755e-06, -8.1402e-06],
        [ 1.0087e-05,  1.8894e-05, -8.9638e-06,  ...,  9.3774e-06,
          1.1743e-05, -6.0328e-06],
        [-1.2079e-05, -2.9157e-06,  7.2554e-06,  ..., -6.7731e-06,
         -1.5701e-06, -9.0009e-06],
        [-1.6640e-05, -1.0207e-05,  9.3215e-06,  ..., -1.4408e-05,
         -6.9954e-06, -1.2572e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2343e-08, 5.3385e-08, 6.3697e-08,  ..., 2.4405e-08, 1.4653e-07,
         3.4631e-08],
        [7.5683e-11, 4.3670e-11, 1.2293e-11,  ..., 5.1371e-11, 1.1624e-11,
         2.0361e-11],
        [3.4860e-09, 1.9985e-09, 1.0174e-09,  ..., 2.6752e-09, 8.4134e-10,
         9.0762e-10],
        [8.7536e-10, 6.6244e-10, 2.7296e-10,  ..., 7.3410e-10, 3.9443e-10,
         2.8365e-10],
        [3.6601e-10, 2.0235e-10, 6.3623e-11,  ..., 2.6706e-10, 5.8094e-11,
         9.3807e-11]], device='cuda:0')
optimizer state dict: 143.0
lr: [9.004067720021106e-06, 9.004067720021106e-06]
scheduler_last_epoch: 143


Running epoch 1, step 1144, batch 96
Sampled inputs[:2]: tensor([[    0, 19191,   266,  ...,   287,   843,  1528],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0327e-04, -1.6742e-04,  3.9207e-05,  ..., -2.4349e-05,
          1.0011e-04, -1.1806e-04],
        [-1.6317e-06, -9.9838e-07,  1.0952e-06,  ..., -1.2740e-06,
         -9.9838e-07, -1.0058e-06],
        [-4.2021e-06, -2.6971e-06,  3.0100e-06,  ..., -3.2783e-06,
         -2.5779e-06, -2.5779e-06],
        [-3.3379e-06, -1.9968e-06,  2.3395e-06,  ..., -2.5779e-06,
         -2.0266e-06, -2.1607e-06],
        [-3.7402e-06, -2.5183e-06,  2.6375e-06,  ..., -2.9802e-06,
         -2.4438e-06, -2.1756e-06]], device='cuda:0')
Loss: 1.0103545188903809


Running epoch 1, step 1145, batch 97
Sampled inputs[:2]: tensor([[    0,   287,  5724,  ...,   298,   591,  2609],
        [    0, 16765,   367,  ..., 30192,  7038,  8135]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3329e-04, -1.9082e-04,  8.2953e-05,  ..., -7.4055e-05,
         -6.1979e-05, -9.9000e-05],
        [-3.2261e-06, -2.0713e-06,  2.1979e-06,  ..., -2.5257e-06,
         -1.9521e-06, -2.0340e-06],
        [-8.5235e-06, -5.7071e-06,  6.1542e-06,  ..., -6.6608e-06,
         -5.1558e-06, -5.3495e-06],
        [-6.6310e-06, -4.1574e-06,  4.7088e-06,  ..., -5.1409e-06,
         -3.9637e-06, -4.3809e-06],
        [-7.4208e-06, -5.1856e-06,  5.2899e-06,  ..., -5.9307e-06,
         -4.7833e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9934345483779907


Running epoch 1, step 1146, batch 98
Sampled inputs[:2]: tensor([[    0, 17471,  4778,  ...,  2177,   271,   266],
        [    0, 29073,   916,  ...,    12,   287,   850]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1568e-04, -2.5863e-04,  9.3952e-05,  ...,  8.4983e-05,
          1.5500e-04, -8.0098e-05],
        [-4.8578e-06, -3.0696e-06,  3.2410e-06,  ..., -3.8072e-06,
         -2.9281e-06, -3.1218e-06],
        [-1.2696e-05, -8.3745e-06,  9.0152e-06,  ..., -9.9093e-06,
         -7.6294e-06, -8.0615e-06],
        [-9.8944e-06, -6.1095e-06,  6.8992e-06,  ..., -7.6890e-06,
         -5.8860e-06, -6.6310e-06],
        [-1.1161e-05, -7.6592e-06,  7.8380e-06,  ..., -8.8960e-06,
         -7.1377e-06, -6.7055e-06]], device='cuda:0')
Loss: 0.9904455542564392


Running epoch 1, step 1147, batch 99
Sampled inputs[:2]: tensor([[    0,  3101,   275,  ...,  2345,   609,   287],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6742e-04, -2.9779e-04,  1.1676e-04,  ...,  4.4042e-05,
          7.7151e-05, -1.5352e-04],
        [-6.4149e-06, -3.9116e-06,  4.2468e-06,  ..., -5.0738e-06,
         -3.8184e-06, -4.2096e-06],
        [-1.6630e-05, -1.0639e-05,  1.1802e-05,  ..., -1.3039e-05,
         -9.8348e-06, -1.0684e-05],
        [-1.3113e-05, -7.7784e-06,  9.1046e-06,  ..., -1.0252e-05,
         -7.6666e-06, -8.9109e-06],
        [-1.4663e-05, -9.7305e-06,  1.0267e-05,  ..., -1.1742e-05,
         -9.2387e-06, -8.8960e-06]], device='cuda:0')
Loss: 0.9595270752906799


Running epoch 1, step 1148, batch 100
Sampled inputs[:2]: tensor([[    0,   266,  6449,  ...,   474,   221,   474],
        [    0,   417,   199,  ...,  9472, 15004,   511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1671e-04, -5.0401e-04, -1.9345e-04,  ..., -8.1437e-05,
          1.9943e-04, -3.5002e-04],
        [-8.0541e-06, -4.9174e-06,  5.2676e-06,  ..., -6.4000e-06,
         -4.8168e-06, -5.2601e-06],
        [-2.0832e-05, -1.3351e-05,  1.4633e-05,  ..., -1.6421e-05,
         -1.2398e-05, -1.3337e-05],
        [-1.6436e-05, -9.7752e-06,  1.1280e-05,  ..., -1.2904e-05,
         -9.6485e-06, -1.1131e-05],
        [-1.8463e-05, -1.2293e-05,  1.2785e-05,  ..., -1.4856e-05,
         -1.1712e-05, -1.1176e-05]], device='cuda:0')
Loss: 0.9900081157684326


Running epoch 1, step 1149, batch 101
Sampled inputs[:2]: tensor([[   0,  328,  957,  ...,  298,  275, 8570],
        [   0,  271, 3403,  ..., 6168,  300, 2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2420e-04, -5.2845e-04, -1.4617e-04,  ..., -1.8207e-04,
          2.6398e-04, -2.9887e-04],
        [-9.6485e-06, -5.9903e-06,  6.3702e-06,  ..., -7.6964e-06,
         -5.8450e-06, -6.2883e-06],
        [-2.4945e-05, -1.6242e-05,  1.7688e-05,  ..., -1.9774e-05,
         -1.5080e-05, -1.5974e-05],
        [-1.9655e-05, -1.1891e-05,  1.3620e-05,  ..., -1.5512e-05,
         -1.1705e-05, -1.3292e-05],
        [-2.2143e-05, -1.4976e-05,  1.5482e-05,  ..., -1.7911e-05,
         -1.4260e-05, -1.3411e-05]], device='cuda:0')
Loss: 1.0045467615127563


Running epoch 1, step 1150, batch 102
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0,  4350,    14,  ...,   266,  9479,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3936e-04, -5.3823e-04, -9.0791e-05,  ..., -4.6444e-05,
          2.2256e-04, -1.3186e-04],
        [-1.1258e-05, -7.0259e-06,  7.3314e-06,  ..., -9.0078e-06,
         -6.8210e-06, -7.3761e-06],
        [-2.9147e-05, -1.9103e-05,  2.0415e-05,  ..., -2.3201e-05,
         -1.7658e-05, -1.8775e-05],
        [-2.2903e-05, -1.3947e-05,  1.5676e-05,  ..., -1.8150e-05,
         -1.3672e-05, -1.5587e-05],
        [-2.5883e-05, -1.7613e-05,  1.7881e-05,  ..., -2.1011e-05,
         -1.6689e-05, -1.5780e-05]], device='cuda:0')
Loss: 0.9706927537918091


Running epoch 1, step 1151, batch 103
Sampled inputs[:2]: tensor([[   0,  352,  266,  ..., 2416,  287,  300],
        [   0,  298, 8761,  ...,  271,  266,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5388e-04, -5.9352e-04, -1.2087e-04,  ..., -3.2365e-05,
          5.1258e-05, -1.1924e-04],
        [-1.3016e-05, -7.7561e-06,  8.3074e-06,  ..., -1.0356e-05,
         -7.7002e-06, -8.7768e-06],
        [-3.3587e-05, -2.1070e-05,  2.3067e-05,  ..., -2.6554e-05,
         -1.9863e-05, -2.2218e-05],
        [-2.6181e-05, -1.5289e-05,  1.7613e-05,  ..., -2.0638e-05,
         -1.5266e-05, -1.8224e-05],
        [-3.0085e-05, -1.9498e-05,  2.0310e-05,  ..., -2.4244e-05,
         -1.8880e-05, -1.8954e-05]], device='cuda:0')
Loss: 0.9613834619522095
Graident accumulation at epoch 1, step 1151, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9069e-05,  1.9973e-05, -5.9814e-05,  ...,  3.3656e-05,
          1.4145e-04,  6.9591e-05],
        [-1.2846e-05, -8.6345e-06,  8.0746e-06,  ..., -1.0709e-05,
         -7.4979e-06, -8.2039e-06],
        [ 5.7200e-06,  1.4898e-05, -5.7608e-06,  ...,  5.7843e-06,
          8.5828e-06, -7.6513e-06],
        [-1.3490e-05, -4.1530e-06,  8.2912e-06,  ..., -8.1596e-06,
         -2.9397e-06, -9.9233e-06],
        [-1.7984e-05, -1.1136e-05,  1.0420e-05,  ..., -1.5391e-05,
         -8.1838e-06, -1.3210e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2497e-08, 5.3684e-08, 6.3648e-08,  ..., 2.4381e-08, 1.4639e-07,
         3.4610e-08],
        [7.5777e-11, 4.3687e-11, 1.2350e-11,  ..., 5.1427e-11, 1.1672e-11,
         2.0418e-11],
        [3.4836e-09, 1.9969e-09, 1.0170e-09,  ..., 2.6733e-09, 8.4089e-10,
         9.0721e-10],
        [8.7517e-10, 6.6201e-10, 2.7300e-10,  ..., 7.3379e-10, 3.9427e-10,
         2.8370e-10],
        [3.6655e-10, 2.0253e-10, 6.3972e-11,  ..., 2.6739e-10, 5.8392e-11,
         9.4072e-11]], device='cuda:0')
optimizer state dict: 144.0
lr: [8.881144923970756e-06, 8.881144923970756e-06]
scheduler_last_epoch: 144


Running epoch 1, step 1152, batch 104
Sampled inputs[:2]: tensor([[    0,    19,    14,  ...,   278,  2588,   944],
        [    0,  3398,   271,  ...,    13,  1581, 13600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8643e-05,  5.2863e-05,  2.9109e-05,  ..., -2.0599e-04,
          6.8366e-05,  2.8921e-05],
        [-1.6540e-06, -1.0803e-06,  1.0952e-06,  ..., -1.3262e-06,
         -1.0282e-06, -1.1027e-06],
        [-4.4107e-06, -3.0398e-06,  3.0547e-06,  ..., -3.5316e-06,
         -2.7716e-06, -2.9206e-06],
        [-3.4273e-06, -2.2054e-06,  2.3544e-06,  ..., -2.7269e-06,
         -2.1160e-06, -2.3842e-06],
        [-3.8743e-06, -2.7865e-06,  2.6822e-06,  ..., -3.1888e-06,
         -2.5928e-06, -2.4736e-06]], device='cuda:0')
Loss: 0.9863989949226379


Running epoch 1, step 1153, batch 105
Sampled inputs[:2]: tensor([[    0,    12,   546,  ..., 24994, 31107,   266],
        [    0,   328,   490,  ...,  6280,  4283,  4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0849e-05,  2.6075e-06, -3.7666e-05,  ..., -1.5758e-04,
          9.4249e-06,  2.8921e-05],
        [-3.2634e-06, -2.0191e-06,  2.2128e-06,  ..., -2.4885e-06,
         -1.8291e-06, -2.1309e-06],
        [-8.7023e-06, -5.6773e-06,  6.2287e-06,  ..., -6.6310e-06,
         -4.9323e-06, -5.6326e-06],
        [-6.8545e-06, -4.1425e-06,  4.8727e-06,  ..., -5.1707e-06,
         -3.7849e-06, -4.6790e-06],
        [-7.5251e-06, -5.1409e-06,  5.3495e-06,  ..., -5.8860e-06,
         -4.6045e-06, -4.6492e-06]], device='cuda:0')
Loss: 0.9734496474266052


Running epoch 1, step 1154, batch 106
Sampled inputs[:2]: tensor([[    0,   287,   221,  ...,  1871,  1482,    12],
        [    0,     9,   300,  ...,  6838,   328, 18619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1005e-04, -2.4993e-05,  6.6853e-05,  ..., -1.2516e-04,
          1.5712e-04,  9.2992e-05],
        [-4.9248e-06, -3.0249e-06,  3.2187e-06,  ..., -3.8072e-06,
         -2.8424e-06, -3.2410e-06],
        [-1.2964e-05, -8.4192e-06,  9.0152e-06,  ..., -1.0014e-05,
         -7.5698e-06, -8.4341e-06],
        [-1.0207e-05, -6.1393e-06,  7.0035e-06,  ..., -7.8231e-06,
         -5.8413e-06, -7.0333e-06],
        [-1.1325e-05, -7.6741e-06,  7.8082e-06,  ..., -8.9556e-06,
         -7.0930e-06, -7.0184e-06]], device='cuda:0')
Loss: 0.9786641597747803


Running epoch 1, step 1155, batch 107
Sampled inputs[:2]: tensor([[   0, 2790,  266,  ...,  401, 1496,   14],
        [   0,  221,  474,  ..., 6451,  292,   34]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1005e-04, -3.8328e-05,  4.2008e-06,  ..., -1.1900e-04,
          1.5631e-04, -4.2965e-05],
        [-6.4820e-06, -3.8780e-06,  4.2170e-06,  ..., -5.0813e-06,
         -3.7774e-06, -4.4033e-06],
        [-1.6898e-05, -1.0684e-05,  1.1787e-05,  ..., -1.3158e-05,
         -9.9093e-06, -1.1221e-05],
        [-1.3351e-05, -7.7635e-06,  9.1493e-06,  ..., -1.0371e-05,
         -7.7188e-06, -9.4622e-06],
        [-1.4916e-05, -9.8348e-06,  1.0312e-05,  ..., -1.1876e-05,
         -9.3579e-06, -9.4175e-06]], device='cuda:0')
Loss: 0.9398656487464905


Running epoch 1, step 1156, batch 108
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,   635,    13,  ...,   292,    20,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0294e-04, -1.0240e-04, -4.6170e-05,  ..., -8.7981e-05,
          1.4739e-04,  2.2546e-05],
        [-8.1360e-06, -4.8615e-06,  5.2899e-06,  ..., -6.3777e-06,
         -4.7386e-06, -5.5283e-06],
        [-2.1368e-05, -1.3456e-05,  1.4842e-05,  ..., -1.6674e-05,
         -1.2517e-05, -1.4260e-05],
        [-1.6689e-05, -9.7007e-06,  1.1399e-05,  ..., -1.2964e-05,
         -9.6262e-06, -1.1832e-05],
        [-1.8790e-05, -1.2338e-05,  1.2919e-05,  ..., -1.4976e-05,
         -1.1757e-05, -1.1906e-05]], device='cuda:0')
Loss: 0.9703704118728638


Running epoch 1, step 1157, batch 109
Sampled inputs[:2]: tensor([[    0,   475,   668,  ..., 17680,   368,  1351],
        [    0,  9419,   221,  ...,    15, 22168,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7468e-05, -1.3370e-04,  1.4230e-04,  ..., -1.1216e-04,
          2.8625e-04,  1.6209e-04],
        [-9.8422e-06, -5.8822e-06,  6.3404e-06,  ..., -7.7188e-06,
         -5.7817e-06, -6.7204e-06],
        [-2.5988e-05, -1.6361e-05,  1.7881e-05,  ..., -2.0310e-05,
         -1.5363e-05, -1.7464e-05],
        [-2.0117e-05, -1.1727e-05,  1.3590e-05,  ..., -1.5646e-05,
         -1.1712e-05, -1.4305e-05],
        [-2.2873e-05, -1.4991e-05,  1.5572e-05,  ..., -1.8239e-05,
         -1.4409e-05, -1.4603e-05]], device='cuda:0')
Loss: 1.0096735954284668


Running epoch 1, step 1158, batch 110
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 46357, 11101, 10621],
        [    0,  5159,   292,  ...,   772,   271,  3728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7400e-04, -1.2340e-04,  1.0035e-04,  ..., -1.0785e-04,
          1.9439e-04,  1.3375e-04],
        [-1.1481e-05, -6.7875e-06,  7.4133e-06,  ..., -8.9854e-06,
         -6.6310e-06, -7.7933e-06],
        [-3.0279e-05, -1.8835e-05,  2.0847e-05,  ..., -2.3618e-05,
         -1.7598e-05, -2.0221e-05],
        [-2.3499e-05, -1.3530e-05,  1.5885e-05,  ..., -1.8239e-05,
         -1.3426e-05, -1.6600e-05],
        [-2.6643e-05, -1.7270e-05,  1.8165e-05,  ..., -2.1204e-05,
         -1.6525e-05, -1.6913e-05]], device='cuda:0')
Loss: 0.9910172820091248


Running epoch 1, step 1159, batch 111
Sampled inputs[:2]: tensor([[    0,  1875,  2117,  ...,  1422,  1059,   963],
        [    0,    12,   287,  ...,  2381, 12046,  2231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1142e-04, -1.0654e-04,  5.3768e-05,  ..., -1.3884e-04,
          6.0383e-05,  2.1352e-04],
        [-1.3068e-05, -7.6555e-06,  8.4490e-06,  ..., -1.0230e-05,
         -7.5437e-06, -8.8960e-06],
        [-3.4451e-05, -2.1234e-05,  2.3767e-05,  ..., -2.6822e-05,
         -1.9982e-05, -2.3022e-05],
        [-2.6792e-05, -1.5244e-05,  1.8150e-05,  ..., -2.0772e-05,
         -1.5289e-05, -1.8969e-05],
        [-3.0354e-05, -1.9506e-05,  2.0728e-05,  ..., -2.4110e-05,
         -1.8790e-05, -1.9252e-05]], device='cuda:0')
Loss: 0.9470945000648499
Graident accumulation at epoch 1, step 1159, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4020e-05,  7.3217e-06, -4.8456e-05,  ...,  1.6406e-05,
          1.3334e-04,  8.3984e-05],
        [-1.2868e-05, -8.5366e-06,  8.1120e-06,  ..., -1.0661e-05,
         -7.5025e-06, -8.2731e-06],
        [ 1.7029e-06,  1.1285e-05, -2.8079e-06,  ...,  2.5236e-06,
          5.7262e-06, -9.1884e-06],
        [-1.4820e-05, -5.2621e-06,  9.2770e-06,  ..., -9.4209e-06,
         -4.1746e-06, -1.0828e-05],
        [-1.9221e-05, -1.1973e-05,  1.1451e-05,  ..., -1.6263e-05,
         -9.2445e-06, -1.3814e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2489e-08, 5.3642e-08, 6.3587e-08,  ..., 2.4376e-08, 1.4625e-07,
         3.4621e-08],
        [7.5872e-11, 4.3702e-11, 1.2409e-11,  ..., 5.1480e-11, 1.1717e-11,
         2.0477e-11],
        [3.4813e-09, 1.9954e-09, 1.0165e-09,  ..., 2.6713e-09, 8.4045e-10,
         9.0683e-10],
        [8.7501e-10, 6.6158e-10, 2.7306e-10,  ..., 7.3349e-10, 3.9411e-10,
         2.8377e-10],
        [3.6710e-10, 2.0271e-10, 6.4338e-11,  ..., 2.6770e-10, 5.8687e-11,
         9.4349e-11]], device='cuda:0')
optimizer state dict: 145.0
lr: [8.758393098742647e-06, 8.758393098742647e-06]
scheduler_last_epoch: 145


Running epoch 1, step 1160, batch 112
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   287,  2888,  4845],
        [    0,   668,  1837,  ...,  4381,    14, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6549e-04, -3.0829e-08,  4.9705e-05,  ...,  6.8078e-05,
          8.4443e-05,  3.5484e-05],
        [-1.6168e-06, -9.4622e-07,  1.0505e-06,  ..., -1.2293e-06,
         -8.7917e-07, -1.0878e-06],
        [-4.4107e-06, -2.7567e-06,  3.0547e-06,  ..., -3.3379e-06,
         -2.4140e-06, -2.8908e-06],
        [-3.5316e-06, -2.0266e-06,  2.3991e-06,  ..., -2.6673e-06,
         -1.8999e-06, -2.4438e-06],
        [-3.7104e-06, -2.3991e-06,  2.5481e-06,  ..., -2.8759e-06,
         -2.1607e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9737911820411682


Running epoch 1, step 1161, batch 113
Sampled inputs[:2]: tensor([[    0,   292,    40,  ..., 26995,   278,   717],
        [    0,    12,  2735,  ...,    12,   344,  1496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2731e-04,  1.2135e-04,  1.9830e-04,  ...,  1.9999e-04,
          2.5324e-04,  6.0259e-05],
        [-3.2410e-06, -1.9222e-06,  1.9968e-06,  ..., -2.6003e-06,
         -1.8999e-06, -2.3544e-06],
        [-8.7321e-06, -5.4240e-06,  5.7966e-06,  ..., -6.9141e-06,
         -5.0813e-06, -6.1691e-06],
        [-6.8396e-06, -3.9488e-06,  4.4107e-06,  ..., -5.4240e-06,
         -3.9414e-06, -5.0664e-06],
        [-7.7039e-06, -4.9025e-06,  5.0664e-06,  ..., -6.2138e-06,
         -4.7386e-06, -5.1856e-06]], device='cuda:0')
Loss: 1.0042370557785034


Running epoch 1, step 1162, batch 114
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ..., 16948,   266,  1751],
        [    0,   706,  1005,  ...,   278,   266,  5590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9403e-04,  8.5697e-05,  1.7418e-04,  ...,  9.6801e-05,
          4.8275e-04, -2.3392e-05],
        [-4.7982e-06, -2.9877e-06,  3.0324e-06,  ..., -3.8669e-06,
         -2.7977e-06, -3.3826e-06],
        [-1.3024e-05, -8.4341e-06,  8.8513e-06,  ..., -1.0371e-05,
         -7.5251e-06, -9.0152e-06],
        [-1.0103e-05, -6.1393e-06,  6.6906e-06,  ..., -8.0615e-06,
         -5.7667e-06, -7.3165e-06],
        [-1.1355e-05, -7.5549e-06,  7.6145e-06,  ..., -9.2238e-06,
         -6.9886e-06, -7.4506e-06]], device='cuda:0')
Loss: 1.0053801536560059


Running epoch 1, step 1163, batch 115
Sampled inputs[:2]: tensor([[    0,   298, 49038,  ...,   288,  1690,  2736],
        [    0,  1485,   271,  ...,  6359,  1799,  5442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0518e-04,  2.9514e-05,  3.3112e-04,  ..., -6.6635e-05,
          9.2886e-04, -3.6904e-05],
        [-6.4000e-06, -4.1425e-06,  4.0755e-06,  ..., -5.1931e-06,
         -3.8259e-06, -4.4629e-06],
        [-1.7345e-05, -1.1697e-05,  1.1832e-05,  ..., -1.3947e-05,
         -1.0327e-05, -1.1906e-05],
        [-1.3366e-05, -8.4788e-06,  8.9109e-06,  ..., -1.0744e-05,
         -7.8380e-06, -9.6112e-06],
        [-1.5154e-05, -1.0505e-05,  1.0177e-05,  ..., -1.2428e-05,
         -9.5963e-06, -9.8795e-06]], device='cuda:0')
Loss: 1.0032564401626587


Running epoch 1, step 1164, batch 116
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,  2386,  4012,  ...,   300, 15480,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0701e-04,  1.1674e-04,  3.3434e-04,  ..., -1.1287e-04,
          9.9196e-04, -9.8074e-05],
        [-8.0243e-06, -5.1260e-06,  5.0813e-06,  ..., -6.4224e-06,
         -4.7423e-06, -5.6326e-06],
        [-2.1577e-05, -1.4409e-05,  1.4707e-05,  ..., -1.7136e-05,
         -1.2785e-05, -1.4842e-05],
        [-1.6659e-05, -1.0446e-05,  1.1086e-05,  ..., -1.3232e-05,
         -9.7230e-06, -1.2040e-05],
        [-1.8910e-05, -1.2964e-05,  1.2711e-05,  ..., -1.5303e-05,
         -1.1906e-05, -1.2338e-05]], device='cuda:0')
Loss: 0.9575743079185486


Running epoch 1, step 1165, batch 117
Sampled inputs[:2]: tensor([[   0, 6673,  298,  ..., 4391,  292,  221],
        [   0,  266, 3727,  ..., 1143,  271, 5213]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7418e-04,  4.8041e-05,  3.5420e-04,  ..., -2.7216e-04,
          9.9344e-04, -2.1563e-04],
        [-9.6932e-06, -6.0797e-06,  6.0648e-06,  ..., -7.7561e-06,
         -5.7109e-06, -6.8620e-06],
        [-2.6166e-05, -1.7136e-05,  1.7628e-05,  ..., -2.0757e-05,
         -1.5423e-05, -1.8135e-05],
        [ 8.8753e-05,  7.5252e-05, -3.5342e-05,  ...,  4.6939e-05,
          7.0146e-05,  2.2769e-05],
        [-2.3142e-05, -1.5527e-05,  1.5333e-05,  ..., -1.8686e-05,
         -1.4454e-05, -1.5229e-05]], device='cuda:0')
Loss: 0.9765597581863403


Running epoch 1, step 1166, batch 118
Sampled inputs[:2]: tensor([[   0,  969,  258,  ...,  726, 5303, 6512],
        [   0,  365, 5911,  ...,  925,  408,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8508e-04,  1.0044e-04,  4.8571e-04,  ..., -2.7907e-04,
          1.1217e-03, -1.6236e-04],
        [-1.1250e-05, -7.1600e-06,  6.9961e-06,  ..., -9.0078e-06,
         -6.7316e-06, -7.9796e-06],
        [-3.0518e-05, -2.0221e-05,  2.0415e-05,  ..., -2.4229e-05,
         -1.8269e-05, -2.1204e-05],
        [ 8.5489e-05,  7.3062e-05, -3.3301e-05,  ...,  4.4331e-05,
          6.8045e-05,  2.0355e-05],
        [-2.6956e-05, -1.8314e-05,  1.7762e-05,  ..., -2.1785e-05,
         -1.7077e-05, -1.7777e-05]], device='cuda:0')
Loss: 1.0135067701339722


Running epoch 1, step 1167, batch 119
Sampled inputs[:2]: tensor([[    0,    13, 20054,  ...,    19,     9,   266],
        [    0,  1197,  3025,  ...,    14,   747,  3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0712e-04,  7.3766e-06,  4.9457e-04,  ..., -1.9744e-04,
          1.0206e-03, -2.0454e-04],
        [-1.2882e-05, -8.1435e-06,  8.0690e-06,  ..., -1.0252e-05,
         -7.6629e-06, -9.0748e-06],
        [-3.4988e-05, -2.3007e-05,  2.3514e-05,  ..., -2.7612e-05,
         -2.0802e-05, -2.4170e-05],
        [ 8.2122e-05,  7.1095e-05, -3.0991e-05,  ...,  4.1798e-05,
          6.6190e-05,  1.8016e-05],
        [-3.0920e-05, -2.0862e-05,  2.0474e-05,  ..., -2.4855e-05,
         -1.9476e-05, -2.0280e-05]], device='cuda:0')
Loss: 1.0157463550567627
Graident accumulation at epoch 1, step 1167, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0191],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0286,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.8094e-05,  7.3272e-06,  5.8463e-06,  ..., -4.9790e-06,
          2.2207e-04,  5.5132e-05],
        [-1.2870e-05, -8.4973e-06,  8.1077e-06,  ..., -1.0620e-05,
         -7.5185e-06, -8.3533e-06],
        [-1.9662e-06,  7.8555e-06, -1.7575e-07,  ..., -4.8990e-07,
          3.0734e-06, -1.0687e-05],
        [-5.1256e-06,  2.3736e-06,  5.2502e-06,  ..., -4.2990e-06,
          2.8619e-06, -7.9435e-06],
        [-2.0391e-05, -1.2862e-05,  1.2353e-05,  ..., -1.7122e-05,
         -1.0268e-05, -1.4461e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2805e-08, 5.3588e-08, 6.3768e-08,  ..., 2.4391e-08, 1.4714e-07,
         3.4628e-08],
        [7.5962e-11, 4.3724e-11, 1.2461e-11,  ..., 5.1534e-11, 1.1764e-11,
         2.0539e-11],
        [3.4791e-09, 1.9939e-09, 1.0160e-09,  ..., 2.6694e-09, 8.4004e-10,
         9.0651e-10],
        [8.8088e-10, 6.6597e-10, 2.7374e-10,  ..., 7.3450e-10, 3.9809e-10,
         2.8381e-10],
        [3.6769e-10, 2.0294e-10, 6.4692e-11,  ..., 2.6805e-10, 5.9007e-11,
         9.4666e-11]], device='cuda:0')
optimizer state dict: 146.0
lr: [8.635831001887192e-06, 8.635831001887192e-06]
scheduler_last_epoch: 146


Running epoch 1, step 1168, batch 120
Sampled inputs[:2]: tensor([[   0, 1477, 3205,  ..., 6441, 9363,  271],
        [   0, 3353,   17,  ...,  596,   12,  461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6982e-05, -4.8936e-05, -1.8099e-04,  ..., -1.0912e-04,
          8.6655e-07, -7.8872e-05],
        [-1.6019e-06, -1.1846e-06,  1.1474e-06,  ..., -1.2591e-06,
         -9.9093e-07, -9.9093e-07],
        [ 8.1382e-05,  7.1016e-05, -1.9716e-05,  ...,  6.3896e-05,
          4.7777e-05,  5.2740e-05],
        [-3.5018e-06, -2.5481e-06,  2.5779e-06,  ..., -2.7418e-06,
         -2.1160e-06, -2.2650e-06],
        [-3.8743e-06, -3.0547e-06,  2.8610e-06,  ..., -3.1292e-06,
         -2.5481e-06, -2.3097e-06]], device='cuda:0')
Loss: 1.0099623203277588


Running epoch 1, step 1169, batch 121
Sampled inputs[:2]: tensor([[   0,  292,  380,  ..., 6156,  278,  266],
        [   0,  369,  726,  ...,  292,  221,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8303e-05, -2.0716e-04, -3.6971e-04,  ..., -7.5224e-05,
          2.0067e-04, -7.3070e-05],
        [-3.0994e-06, -2.1756e-06,  2.0303e-06,  ..., -2.5034e-06,
         -1.9893e-06, -2.1234e-06],
        [ 7.7269e-05,  6.8245e-05, -1.7079e-05,  ...,  6.0573e-05,
          4.5124e-05,  4.9760e-05],
        [-6.7800e-06, -4.6343e-06,  4.5747e-06,  ..., -5.4389e-06,
         -4.2766e-06, -4.8280e-06],
        [-7.5698e-06, -5.6177e-06,  5.2303e-06,  ..., -6.1393e-06,
         -5.0366e-06, -4.8280e-06]], device='cuda:0')
Loss: 1.0040339231491089


Running epoch 1, step 1170, batch 122
Sampled inputs[:2]: tensor([[    0, 25778,  3804,  ...,  2354,    12,   554],
        [    0, 17442,  2416,  ...,  7244,    66, 16907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3753e-05, -2.1529e-04, -3.7772e-04,  ..., -8.8587e-05,
          1.1536e-04, -8.3845e-05],
        [-4.7088e-06, -3.3677e-06,  3.1479e-06,  ..., -3.7476e-06,
         -2.9393e-06, -3.1292e-06],
        [ 7.2710e-05,  6.4743e-05, -1.3726e-05,  ...,  5.7027e-05,
          4.2383e-05,  4.6884e-05],
        [-1.0267e-05, -7.1824e-06,  7.1079e-06,  ..., -8.1211e-06,
         -6.3181e-06, -7.1079e-06],
        [-1.1384e-05, -8.6576e-06,  8.0168e-06,  ..., -9.1791e-06,
         -7.4804e-06, -7.1228e-06]], device='cuda:0')
Loss: 1.009177565574646


Running epoch 1, step 1171, batch 123
Sampled inputs[:2]: tensor([[   0, 1552,  271,  ...,   13,  287,  995],
        [   0, 9466,   36,  ..., 1795,  437,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2916e-04, -2.5120e-04, -2.0170e-04,  ..., -6.2359e-05,
          2.9819e-04, -2.6830e-05],
        [-6.2287e-06, -4.4927e-06,  4.1984e-06,  ..., -4.9695e-06,
         -3.8631e-06, -4.1723e-06],
        [ 6.8239e-05,  6.1360e-05, -1.0492e-05,  ...,  5.3465e-05,
          3.9671e-05,  4.3859e-05],
        [-1.3664e-05, -9.6411e-06,  9.5218e-06,  ..., -1.0818e-05,
         -8.3447e-06, -9.5218e-06],
        [-1.5184e-05, -1.1608e-05,  1.0759e-05,  ..., -1.2264e-05,
         -9.9093e-06, -9.5814e-06]], device='cuda:0')
Loss: 0.9994378089904785


Running epoch 1, step 1172, batch 124
Sampled inputs[:2]: tensor([[    0, 48705,   292,  ...,   266,  2548,  2697],
        [    0,   292,   474,  ...,   446, 14932,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7424e-04, -2.9841e-04, -4.1928e-04,  ..., -7.0331e-05,
          1.3063e-04, -1.4313e-04],
        [-7.8157e-06, -5.5358e-06,  5.2489e-06,  ..., -6.2361e-06,
         -4.7758e-06, -5.2154e-06],
        [ 6.3769e-05,  5.8306e-05, -7.3632e-06,  ...,  4.9904e-05,
          3.7122e-05,  4.0938e-05],
        [-1.7107e-05, -1.1846e-05,  1.1891e-05,  ..., -1.3545e-05,
         -1.0282e-05, -1.1876e-05],
        [-1.8999e-05, -1.4305e-05,  1.3396e-05,  ..., -1.5363e-05,
         -1.2219e-05, -1.1966e-05]], device='cuda:0')
Loss: 0.9731788635253906


Running epoch 1, step 1173, batch 125
Sampled inputs[:2]: tensor([[   0, 4110,  271,  ...,  944,  278, 3230],
        [   0, 6538, 1805,  ...,  298,  271,  721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8793e-04, -1.8491e-04, -4.8275e-04,  ..., -1.2226e-06,
          7.5445e-05, -1.5348e-04],
        [-9.4101e-06, -6.6161e-06,  6.2026e-06,  ..., -7.5400e-06,
         -5.7742e-06, -6.4075e-06],
        [ 1.0238e-04,  1.5488e-04, -4.0464e-05,  ...,  1.1922e-04,
          1.0126e-04,  8.1946e-05],
        [-2.0549e-05, -1.4126e-05,  1.4052e-05,  ..., -1.6332e-05,
         -1.2398e-05, -1.4514e-05],
        [-2.3112e-05, -1.7211e-05,  1.5974e-05,  ..., -1.8746e-05,
         -1.4886e-05, -1.4886e-05]], device='cuda:0')
Loss: 1.0133824348449707


Running epoch 1, step 1174, batch 126
Sampled inputs[:2]: tensor([[   0,  292,  960,  ...,  271, 1356,   14],
        [   0,  320, 4886,  ...,   14,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4049e-04, -1.6572e-04, -4.3676e-04,  ..., -3.7001e-05,
         -7.4308e-05, -2.7056e-04],
        [-1.0937e-05, -7.6964e-06,  7.2531e-06,  ..., -8.8066e-06,
         -6.7130e-06, -7.4282e-06],
        [ 9.8090e-05,  1.5171e-04, -3.7320e-05,  ...,  1.1564e-04,
          9.8596e-05,  7.9070e-05],
        [-2.3842e-05, -1.6421e-05,  1.6421e-05,  ..., -1.9044e-05,
         -1.4395e-05, -1.6809e-05],
        [-2.6807e-05, -2.0042e-05,  1.8656e-05,  ..., -2.1890e-05,
         -1.7330e-05, -1.7241e-05]], device='cuda:0')
Loss: 0.9773479700088501


Running epoch 1, step 1175, batch 127
Sampled inputs[:2]: tensor([[   0,  721, 1119,  ...,  600,  328, 3363],
        [   0, 3408,  300,  ...,   14, 5870,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3223e-04, -2.4703e-04, -5.8589e-04,  ..., -1.5183e-04,
         -2.2085e-04, -3.8008e-04],
        [-1.2524e-05, -8.7991e-06,  8.4303e-06,  ..., -9.9987e-06,
         -7.6257e-06, -8.4266e-06],
        [ 9.3649e-05,  1.4853e-04, -3.3893e-05,  ...,  1.1232e-04,
          9.6048e-05,  7.6269e-05],
        [-2.7329e-05, -1.8790e-05,  1.9103e-05,  ..., -2.1636e-05,
         -1.6347e-05, -1.9088e-05],
        [-3.0577e-05, -2.2843e-05,  2.1547e-05,  ..., -2.4781e-05,
         -1.9640e-05, -1.9506e-05]], device='cuda:0')
Loss: 0.9867309927940369
Graident accumulation at epoch 1, step 1175, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0062e-05, -1.8108e-05, -5.3327e-05,  ..., -1.9664e-05,
          1.7778e-04,  1.1611e-05],
        [-1.2835e-05, -8.5275e-06,  8.1400e-06,  ..., -1.0558e-05,
         -7.5293e-06, -8.3606e-06],
        [ 7.5954e-06,  2.1923e-05, -3.5475e-06,  ...,  1.0791e-05,
          1.2371e-05, -1.9910e-06],
        [-7.3460e-06,  2.5722e-07,  6.6355e-06,  ..., -6.0328e-06,
          9.4105e-07, -9.0580e-06],
        [-2.1410e-05, -1.3860e-05,  1.3273e-05,  ..., -1.7888e-05,
         -1.1205e-05, -1.4966e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2863e-08, 5.3596e-08, 6.4047e-08,  ..., 2.4389e-08, 1.4704e-07,
         3.4738e-08],
        [7.6043e-11, 4.3758e-11, 1.2520e-11,  ..., 5.1582e-11, 1.1810e-11,
         2.0589e-11],
        [3.4844e-09, 2.0140e-09, 1.0162e-09,  ..., 2.6794e-09, 8.4843e-10,
         9.1142e-10],
        [8.8075e-10, 6.6566e-10, 2.7384e-10,  ..., 7.3423e-10, 3.9796e-10,
         2.8389e-10],
        [3.6826e-10, 2.0326e-10, 6.5092e-11,  ..., 2.6840e-10, 5.9334e-11,
         9.4952e-11]], device='cuda:0')
optimizer state dict: 147.0
lr: [8.513477361962645e-06, 8.513477361962645e-06]
scheduler_last_epoch: 147


Running epoch 1, step 1176, batch 128
Sampled inputs[:2]: tensor([[    0,   475,  2985,  ...,   292,  5273,     9],
        [    0,  7994,    12,  ..., 13800,   278,   795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1856e-04, -7.5565e-08, -8.5173e-05,  ..., -6.9575e-05,
         -2.3446e-04, -1.6593e-04],
        [ 5.1395e-05,  9.2523e-05, -8.5478e-05,  ...,  5.5055e-05,
          6.6577e-05,  4.1257e-05],
        [-3.7849e-06, -2.7269e-06,  2.8759e-06,  ..., -3.1590e-06,
         -2.5928e-06, -2.5779e-06],
        [-2.9951e-06, -2.0266e-06,  2.2501e-06,  ..., -2.5481e-06,
         -2.1309e-06, -2.2799e-06],
        [-3.3975e-06, -2.4885e-06,  2.5630e-06,  ..., -2.8461e-06,
         -2.4140e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9383542537689209


Running epoch 1, step 1177, batch 129
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,  1927,   863,  ...,  1163,    13,  1888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3066e-05,  1.1218e-04,  9.1236e-07,  ..., -4.4033e-05,
         -2.4114e-04, -1.6749e-04],
        [ 4.9934e-05,  9.1420e-05, -8.4420e-05,  ...,  5.3863e-05,
          6.5675e-05,  4.0281e-05],
        [-8.1658e-06, -6.1244e-06,  6.1840e-06,  ..., -6.7204e-06,
         -5.2452e-06, -5.4985e-06],
        [-6.4075e-06, -4.5449e-06,  4.7982e-06,  ..., -5.3048e-06,
         -4.1872e-06, -4.6641e-06],
        [-6.9886e-06, -5.3346e-06,  5.2303e-06,  ..., -5.8115e-06,
         -4.7237e-06, -4.4405e-06]], device='cuda:0')
Loss: 0.9648613929748535


Running epoch 1, step 1178, batch 130
Sampled inputs[:2]: tensor([[    0,  1978, 20360,  ...,   898,   699, 10262],
        [    0,  5319,    14,  ...,  2372,  2356,  4093]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4808e-05,  6.4524e-05,  7.8423e-05,  ..., -3.7907e-05,
         -3.0898e-04, -2.3611e-04],
        [ 4.8422e-05,  9.0206e-05, -8.3347e-05,  ...,  5.2618e-05,
          6.4677e-05,  3.9290e-05],
        [-1.2547e-05, -9.7007e-06,  9.4175e-06,  ..., -1.0297e-05,
         -8.1062e-06, -8.3297e-06],
        [-9.7156e-06, -7.1377e-06,  7.2122e-06,  ..., -7.9870e-06,
         -6.3181e-06, -6.9141e-06],
        [-1.0744e-05, -8.4788e-06,  7.9870e-06,  ..., -8.9407e-06,
         -7.3165e-06, -6.7651e-06]], device='cuda:0')
Loss: 1.0095221996307373


Running epoch 1, step 1179, batch 131
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   266,  2025,   287],
        [    0, 11541,  4784,  ...,  2837, 38541,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5031e-05,  2.4410e-04,  1.8429e-04,  ...,  3.4934e-05,
         -5.0123e-04, -4.0798e-04],
        [ 4.6932e-05,  8.9148e-05, -8.2334e-05,  ...,  5.1344e-05,
          6.3735e-05,  3.8269e-05],
        [-1.6749e-05, -1.2770e-05,  1.2442e-05,  ..., -1.3858e-05,
         -1.0744e-05, -1.1161e-05],
        [-1.2994e-05, -9.4026e-06,  9.5218e-06,  ..., -1.0759e-05,
         -8.3596e-06, -9.2387e-06],
        [-1.4439e-05, -1.1250e-05,  1.0625e-05,  ..., -1.2100e-05,
         -9.7454e-06, -9.1195e-06]], device='cuda:0')
Loss: 0.9885452389717102


Running epoch 1, step 1180, batch 132
Sampled inputs[:2]: tensor([[    0,    25,     5,  ...,  3935,    14,    16],
        [    0,   689,    13,  ...,   756,   271, 31773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6367e-05,  2.9324e-04,  1.5301e-04,  ..., -2.3009e-05,
         -7.4117e-04, -4.8492e-04],
        [ 4.5509e-05,  8.8082e-05, -8.1343e-05,  ...,  5.0152e-05,
          6.2803e-05,  3.7308e-05],
        [-2.0742e-05, -1.5855e-05,  1.5423e-05,  ..., -1.7181e-05,
         -1.3337e-05, -1.3813e-05],
        [-1.6168e-05, -1.1727e-05,  1.1846e-05,  ..., -1.3411e-05,
         -1.0431e-05, -1.1504e-05],
        [-1.7956e-05, -1.4037e-05,  1.3217e-05,  ..., -1.5065e-05,
         -1.2159e-05, -1.1310e-05]], device='cuda:0')
Loss: 0.9744278192520142


Running epoch 1, step 1181, batch 133
Sampled inputs[:2]: tensor([[    0,   271,   768,  ..., 15555,   278,   266],
        [    0,  1064,  1042,  ...,    12,   259,  4754]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8752e-05,  3.6038e-04,  1.4502e-04,  ..., -5.8127e-05,
         -7.8885e-04, -3.6551e-04],
        [ 4.3922e-05,  8.6890e-05, -8.0263e-05,  ...,  4.8915e-05,
          6.1853e-05,  3.6280e-05],
        [-2.5451e-05, -1.9491e-05,  1.8775e-05,  ..., -2.0862e-05,
         -1.6168e-05, -1.6868e-05],
        [-1.9699e-05, -1.4335e-05,  1.4320e-05,  ..., -1.6153e-05,
         -1.2517e-05, -1.3873e-05],
        [-2.1890e-05, -1.7166e-05,  1.6004e-05,  ..., -1.8194e-05,
         -1.4663e-05, -1.3754e-05]], device='cuda:0')
Loss: 1.000606656074524


Running epoch 1, step 1182, batch 134
Sampled inputs[:2]: tensor([[   0, 1527,  292,  ..., 2122,  278, 1911],
        [   0,  299,  292,  ...,  266, 2474,  360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2670e-05,  6.1065e-04,  1.9819e-04,  ..., -1.2035e-04,
         -7.6784e-04, -3.7737e-04],
        [ 4.2409e-05,  8.5750e-05, -7.9264e-05,  ...,  4.7679e-05,
          6.0855e-05,  3.5200e-05],
        [-2.9862e-05, -2.2888e-05,  2.1830e-05,  ..., -2.4423e-05,
         -1.9044e-05, -1.9997e-05],
        [-2.3067e-05, -1.6809e-05,  1.6630e-05,  ..., -1.8880e-05,
         -1.4707e-05, -1.6391e-05],
        [-2.5585e-05, -2.0102e-05,  1.8552e-05,  ..., -2.1249e-05,
         -1.7226e-05, -1.6257e-05]], device='cuda:0')
Loss: 0.9647228717803955


Running epoch 1, step 1183, batch 135
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2354e-05,  6.8487e-04,  3.3101e-04,  ..., -1.8813e-04,
         -8.5056e-04, -3.2305e-04],
        [ 4.0852e-05,  8.4640e-05, -7.8303e-05,  ...,  4.6390e-05,
          5.9819e-05,  3.4067e-05],
        [-3.4243e-05, -2.6122e-05,  2.4736e-05,  ..., -2.8029e-05,
         -2.1964e-05, -2.3156e-05],
        [-2.6464e-05, -1.9193e-05,  1.8820e-05,  ..., -2.1681e-05,
         -1.6958e-05, -1.8969e-05],
        [-2.9460e-05, -2.3037e-05,  2.1100e-05,  ..., -2.4483e-05,
         -1.9923e-05, -1.8924e-05]], device='cuda:0')
Loss: 0.9889845252037048
Graident accumulation at epoch 1, step 1183, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5291e-05,  5.2189e-05, -1.4893e-05,  ..., -3.6511e-05,
          7.4942e-05, -2.1855e-05],
        [-7.4665e-06,  7.8929e-07, -5.0437e-07,  ..., -4.8630e-06,
         -7.9440e-07, -4.1178e-06],
        [ 3.4115e-06,  1.7119e-05, -7.1913e-07,  ...,  6.9091e-06,
          8.9373e-06, -4.1075e-06],
        [-9.2578e-06, -1.6878e-06,  7.8540e-06,  ..., -7.5976e-06,
         -8.4880e-07, -1.0049e-05],
        [-2.2215e-05, -1.4778e-05,  1.4056e-05,  ..., -1.8548e-05,
         -1.2077e-05, -1.5361e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2814e-08, 5.4011e-08, 6.4093e-08,  ..., 2.4400e-08, 1.4762e-07,
         3.4808e-08],
        [7.7635e-11, 5.0878e-11, 1.8639e-11,  ..., 5.3682e-11, 1.5377e-11,
         2.1729e-11],
        [3.4821e-09, 2.0127e-09, 1.0158e-09,  ..., 2.6775e-09, 8.4806e-10,
         9.1104e-10],
        [8.8057e-10, 6.6536e-10, 2.7392e-10,  ..., 7.3397e-10, 3.9785e-10,
         2.8397e-10],
        [3.6876e-10, 2.0358e-10, 6.5472e-11,  ..., 2.6873e-10, 5.9672e-11,
         9.5215e-11]], device='cuda:0')
optimizer state dict: 148.0
lr: [8.391350875673234e-06, 8.391350875673234e-06]
scheduler_last_epoch: 148


Running epoch 1, step 1184, batch 136
Sampled inputs[:2]: tensor([[   0,  298,  894,  ..., 7605, 3220,  259],
        [   0,  266, 7407,  ...,  287,  365, 4371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0520e-05,  3.0286e-05, -4.1077e-05,  ..., -6.4616e-05,
         -2.8612e-05,  4.8391e-05],
        [-1.5423e-06, -1.1548e-06,  1.0356e-06,  ..., -1.2890e-06,
         -1.0282e-06, -1.1399e-06],
        [-4.4107e-06, -3.4422e-06,  3.1441e-06,  ..., -3.6955e-06,
         -2.9504e-06, -3.2336e-06],
        [ 2.2263e-04,  3.8301e-04, -2.0586e-04,  ...,  2.5286e-04,
          3.3145e-04,  2.0327e-04],
        [-4.0531e-06, -3.1888e-06,  2.8461e-06,  ..., -3.4273e-06,
         -2.8014e-06, -2.8312e-06]], device='cuda:0')
Loss: 0.9794284701347351


Running epoch 1, step 1185, batch 137
Sampled inputs[:2]: tensor([[    0,   271, 28279,  ...,   367,   806,   271],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3792e-05,  1.3330e-04,  2.7392e-05,  ..., -1.7615e-04,
          4.9486e-05, -5.2996e-05],
        [-3.0994e-06, -2.2650e-06,  1.9893e-06,  ..., -2.5854e-06,
         -2.1085e-06, -2.2873e-06],
        [-8.8513e-06, -6.7502e-06,  6.0797e-06,  ..., -7.3761e-06,
         -6.0499e-06, -6.4671e-06],
        [ 2.1932e-04,  3.8068e-04, -2.0374e-04,  ...,  2.5011e-04,
          3.2915e-04,  2.0075e-04],
        [-8.1360e-06, -6.3032e-06,  5.5283e-06,  ..., -6.8694e-06,
         -5.7817e-06, -5.6922e-06]], device='cuda:0')
Loss: 0.9847064018249512


Running epoch 1, step 1186, batch 138
Sampled inputs[:2]: tensor([[   0,  381, 1795,  ...,   12,  344,  593],
        [   0, 1074, 1593,  ...,  992, 1810,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7987e-05,  1.9586e-04, -9.4372e-05,  ..., -2.8119e-04,
         -1.3510e-04, -6.3606e-05],
        [-4.5300e-06, -3.3677e-06,  3.0845e-06,  ..., -3.7774e-06,
         -3.0473e-06, -3.2708e-06],
        [-1.3024e-05, -1.0028e-05,  9.4324e-06,  ..., -1.0818e-05,
         -8.7470e-06, -9.2834e-06],
        [ 2.1603e-04,  3.7824e-04, -2.0114e-04,  ...,  2.4742e-04,
          3.2707e-04,  1.9840e-04],
        [-1.1668e-05, -9.1642e-06,  8.3447e-06,  ..., -9.8348e-06,
         -8.1956e-06, -7.9274e-06]], device='cuda:0')
Loss: 1.0071083307266235


Running epoch 1, step 1187, batch 139
Sampled inputs[:2]: tensor([[   0,  494,  221,  ...,  298, 1062, 4923],
        [   0, 1057,   14,  ...,   14, 4735,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5231e-05,  2.9880e-04, -1.0328e-07,  ..., -1.0127e-04,
         -2.7883e-04, -2.1742e-04],
        [-6.1467e-06, -4.3660e-06,  4.0978e-06,  ..., -5.1335e-06,
         -4.0904e-06, -4.5300e-06],
        [-1.7732e-05, -1.3053e-05,  1.2517e-05,  ..., -1.4693e-05,
         -1.1727e-05, -1.2845e-05],
        [ 2.1257e-04,  3.7618e-04, -1.9893e-04,  ...,  2.4454e-04,
          3.2489e-04,  1.9564e-04],
        [-1.5959e-05, -1.1981e-05,  1.1146e-05,  ..., -1.3396e-05,
         -1.1012e-05, -1.1042e-05]], device='cuda:0')
Loss: 0.9975566267967224


Running epoch 1, step 1188, batch 140
Sampled inputs[:2]: tensor([[    0,  5136,   446,  ...,  1173,   300,   266],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8915e-05,  2.8183e-04,  1.0687e-05,  ..., -1.6613e-04,
         -3.6566e-04, -2.9033e-04],
        [-7.7114e-06, -5.5209e-06,  5.1484e-06,  ..., -6.4000e-06,
         -5.0440e-06, -5.6028e-06],
        [-2.2233e-05, -1.6510e-05,  1.5691e-05,  ..., -1.8343e-05,
         -1.4499e-05, -1.5944e-05],
        [ 2.0921e-04,  3.7372e-04, -1.9659e-04,  ...,  2.4183e-04,
          3.2285e-04,  1.9324e-04],
        [-1.9893e-05, -1.5110e-05,  1.3888e-05,  ..., -1.6659e-05,
         -1.3590e-05, -1.3649e-05]], device='cuda:0')
Loss: 0.9886799454689026


Running epoch 1, step 1189, batch 141
Sampled inputs[:2]: tensor([[   0, 7180,  266,  ..., 1805,   12,  221],
        [   0, 7692,   12,  ...,  266, 2042,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0250e-04,  4.2634e-04, -9.5501e-05,  ..., -2.0535e-04,
         -5.9730e-04, -4.8923e-04],
        [-9.1791e-06, -6.4000e-06,  6.1840e-06,  ..., -7.5921e-06,
         -5.8673e-06, -6.7204e-06],
        [-2.6375e-05, -1.9088e-05,  1.8835e-05,  ..., -2.1651e-05,
         -1.6794e-05, -1.8984e-05],
        [ 2.0594e-04,  3.7185e-04, -1.9415e-04,  ...,  2.3922e-04,
          3.2106e-04,  1.9071e-04],
        [-2.3484e-05, -1.7375e-05,  1.6570e-05,  ..., -1.9550e-05,
         -1.5691e-05, -1.6123e-05]], device='cuda:0')
Loss: 0.9346723556518555


Running epoch 1, step 1190, batch 142
Sampled inputs[:2]: tensor([[   0, 4882,   12,  ...,   12, 9575,  287],
        [   0,  806,  300,  ...,  360, 4918, 1106]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9670e-04,  5.3374e-04, -2.4797e-05,  ..., -1.0865e-04,
         -7.8816e-04, -5.2525e-04],
        [-1.0602e-05, -7.5400e-06,  7.2792e-06,  ..., -8.8066e-06,
         -6.8136e-06, -7.6890e-06],
        [-3.0369e-05, -2.2307e-05,  2.2069e-05,  ..., -2.5004e-05,
         -1.9386e-05, -2.1651e-05],
        [ 2.0283e-04,  3.6944e-04, -1.9164e-04,  ...,  2.3660e-04,
          3.1905e-04,  1.8851e-04],
        [-2.6986e-05, -2.0280e-05,  1.9372e-05,  ..., -2.2531e-05,
         -1.8105e-05, -1.8314e-05]], device='cuda:0')
Loss: 0.9951735138893127


Running epoch 1, step 1191, batch 143
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2915,  275, 2565],
        [   0,  199, 3289,  ..., 2269, 6476,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3489e-04,  6.6230e-04, -1.2956e-04,  ..., -1.4226e-04,
         -8.6486e-04, -9.1319e-04],
        [-1.2167e-05, -8.4005e-06,  8.2403e-06,  ..., -1.0058e-05,
         -7.7188e-06, -8.9332e-06],
        [-3.4750e-05, -2.4840e-05,  2.4974e-05,  ..., -2.8446e-05,
         -2.1920e-05, -2.5004e-05],
        [ 1.9954e-04,  3.6768e-04, -1.8951e-04,  ...,  2.3399e-04,
          3.1715e-04,  1.8585e-04],
        [-3.0950e-05, -2.2590e-05,  2.1949e-05,  ..., -2.5675e-05,
         -2.0489e-05, -2.1204e-05]], device='cuda:0')
Loss: 0.9336439371109009
Graident accumulation at epoch 1, step 1191, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.7251e-05,  1.1320e-04, -2.6360e-05,  ..., -4.7086e-05,
         -1.9038e-05, -1.1099e-04],
        [-7.9366e-06, -1.2969e-07,  3.7010e-07,  ..., -5.3826e-06,
         -1.4868e-06, -4.5993e-06],
        [-4.0457e-07,  1.2923e-05,  1.8502e-06,  ...,  3.3736e-06,
          5.8516e-06, -6.1972e-06],
        [ 1.1622e-05,  3.5249e-05, -1.1883e-05,  ...,  1.6561e-05,
          3.0951e-05,  9.5411e-06],
        [-2.3088e-05, -1.5559e-05,  1.4845e-05,  ..., -1.9260e-05,
         -1.2918e-05, -1.5946e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2950e-08, 5.4396e-08, 6.4046e-08,  ..., 2.4396e-08, 1.4822e-07,
         3.5607e-08],
        [7.7706e-11, 5.0898e-11, 1.8688e-11,  ..., 5.3730e-11, 1.5421e-11,
         2.1787e-11],
        [3.4798e-09, 2.0113e-09, 1.0154e-09,  ..., 2.6756e-09, 8.4769e-10,
         9.1076e-10],
        [9.1950e-10, 7.9988e-10, 3.0956e-10,  ..., 7.8799e-10, 4.9804e-10,
         3.1823e-10],
        [3.6935e-10, 2.0389e-10, 6.5888e-11,  ..., 2.6912e-10, 6.0032e-11,
         9.5569e-11]], device='cuda:0')
optimizer state dict: 149.0
lr: [8.269470205012111e-06, 8.269470205012111e-06]
scheduler_last_epoch: 149


Running epoch 1, step 1192, batch 144
Sampled inputs[:2]: tensor([[    0,   287,   768,  ...,   221,   474,   221],
        [    0,   445,     8,  ...,    13, 25386,    17]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4542e-06,  3.7068e-05,  4.8025e-05,  ...,  1.7935e-04,
         -2.0749e-04,  4.3239e-06],
        [-1.4827e-06, -1.1325e-06,  1.1176e-06,  ..., -1.2070e-06,
         -8.9779e-07, -1.0282e-06],
        [-4.2617e-06, -3.3975e-06,  3.4124e-06,  ..., -3.4869e-06,
         -2.6226e-06, -2.9653e-06],
        [-3.2932e-06, -2.4885e-06,  2.5928e-06,  ..., -2.6673e-06,
         -1.9819e-06, -2.3842e-06],
        [-3.6210e-06, -2.9504e-06,  2.8461e-06,  ..., -2.9951e-06,
         -2.3693e-06, -2.3693e-06]], device='cuda:0')
Loss: 0.9695969820022583


Running epoch 1, step 1193, batch 145
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,   259,  2283,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4529e-04,  3.1638e-04,  3.3189e-05,  ...,  1.4670e-04,
         -3.5685e-04, -4.9419e-05],
        [-2.8536e-06, -2.1979e-06,  2.0340e-06,  ..., -2.3842e-06,
         -1.8515e-06, -2.1309e-06],
        [-8.1062e-06, -6.4671e-06,  6.1989e-06,  ..., -6.7353e-06,
         -5.2899e-06, -5.9158e-06],
        [-6.1840e-06, -4.6641e-06,  4.6641e-06,  ..., -5.1409e-06,
         -3.9935e-06, -4.7833e-06],
        [-7.1526e-06, -5.7966e-06,  5.3495e-06,  ..., -6.0052e-06,
         -4.9323e-06, -4.9323e-06]], device='cuda:0')
Loss: 0.9025197625160217


Running epoch 1, step 1194, batch 146
Sampled inputs[:2]: tensor([[   0, 1726, 3775,  ...,  300,  266, 1686],
        [   0,   12,  722,  ...,  674,  369,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0052e-04,  3.7379e-04, -1.2653e-06,  ...,  1.5063e-04,
         -3.9560e-04, -1.0908e-04],
        [-4.3064e-06, -3.3230e-06,  3.1665e-06,  ..., -3.6210e-06,
         -2.8126e-06, -3.0622e-06],
        [-1.2189e-05, -9.7305e-06,  9.5665e-06,  ..., -1.0192e-05,
         -8.0019e-06, -8.5086e-06],
        [-9.4026e-06, -7.0930e-06,  7.2867e-06,  ..., -7.8529e-06,
         -6.0946e-06, -6.9290e-06],
        [-1.0729e-05, -8.7321e-06,  8.2552e-06,  ..., -9.0748e-06,
         -7.4506e-06, -7.0781e-06]], device='cuda:0')
Loss: 0.9864562749862671


Running epoch 1, step 1195, batch 147
Sampled inputs[:2]: tensor([[   0, 5699,   20,  ..., 3502, 2051,  266],
        [   0, 2793,  271,  ...,  374,  298,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0481e-04,  3.4550e-04, -1.4890e-04,  ...,  2.3184e-04,
         -2.6760e-04, -2.5659e-05],
        [-5.7146e-06, -4.4107e-06,  4.2319e-06,  ..., -4.8354e-06,
         -3.7663e-06, -4.0382e-06],
        [-1.6212e-05, -1.2890e-05,  1.2815e-05,  ..., -1.3605e-05,
         -1.0669e-05, -1.1206e-05],
        [-1.2577e-05, -9.4622e-06,  9.7752e-06,  ..., -1.0550e-05,
         -8.1956e-06, -9.2089e-06],
        [-1.4305e-05, -1.1563e-05,  1.1101e-05,  ..., -1.2130e-05,
         -9.9242e-06, -9.3281e-06]], device='cuda:0')
Loss: 0.993906557559967


Running epoch 1, step 1196, batch 148
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   680,   271,  ..., 12942,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9685e-05,  3.3408e-04, -2.4766e-04,  ...,  1.7773e-04,
         -5.0679e-04, -3.1561e-04],
        [-7.1004e-06, -5.4985e-06,  5.3644e-06,  ..., -6.0052e-06,
         -4.6082e-06, -4.9286e-06],
        [-2.0266e-05, -1.6123e-05,  1.6242e-05,  ..., -1.6987e-05,
         -1.3098e-05, -1.3754e-05],
        [-1.5765e-05, -1.1906e-05,  1.2472e-05,  ..., -1.3202e-05,
         -1.0088e-05, -1.1355e-05],
        [-1.7717e-05, -1.4380e-05,  1.3977e-05,  ..., -1.5035e-05,
         -1.2115e-05, -1.1355e-05]], device='cuda:0')
Loss: 0.9912400245666504


Running epoch 1, step 1197, batch 149
Sampled inputs[:2]: tensor([[    0,  3393,  3380,  ...,   292,  6502,   950],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4822e-06,  4.6124e-04, -3.2200e-04,  ...,  2.1054e-04,
         -5.3932e-04, -2.7210e-04],
        [-8.4192e-06, -6.5491e-06,  6.3702e-06,  ..., -7.1600e-06,
         -5.5023e-06, -5.9269e-06],
        [-2.4006e-05, -1.9118e-05,  1.9282e-05,  ..., -2.0161e-05,
         -1.5542e-05, -1.6421e-05],
        [-1.8701e-05, -1.4126e-05,  1.4827e-05,  ..., -1.5736e-05,
         -1.2040e-05, -1.3694e-05],
        [-2.1011e-05, -1.7077e-05,  1.6630e-05,  ..., -1.7866e-05,
         -1.4380e-05, -1.3530e-05]], device='cuda:0')
Loss: 0.9462035894393921


Running epoch 1, step 1198, batch 150
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,   221,   380,  ...,  5543,   768,  6375]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8318e-05,  5.5265e-04, -4.2143e-04,  ...,  4.1657e-04,
         -9.2324e-04, -1.6517e-04],
        [-9.8050e-06, -7.6294e-06,  7.4878e-06,  ..., -8.3223e-06,
         -6.3144e-06, -6.8471e-06],
        [-2.7999e-05, -2.2307e-05,  2.2680e-05,  ..., -2.3469e-05,
         -1.7852e-05, -1.9029e-05],
        [-2.1845e-05, -1.6510e-05,  1.7479e-05,  ..., -1.8343e-05,
         -1.3828e-05, -1.5900e-05],
        [-2.4334e-05, -1.9819e-05,  1.9431e-05,  ..., -2.0683e-05,
         -1.6451e-05, -1.5587e-05]], device='cuda:0')
Loss: 0.9634019732475281


Running epoch 1, step 1199, batch 151
Sampled inputs[:2]: tensor([[    0,  2346, 17886,  ...,   287,  6769,   806],
        [    0,   995,    13,  ...,  2192,  2534,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8451e-04,  5.5466e-04, -5.6188e-04,  ...,  3.5002e-04,
         -1.0072e-03, -3.8083e-04],
        [-1.1206e-05, -8.7619e-06,  8.5458e-06,  ..., -9.5442e-06,
         -7.2904e-06, -7.7747e-06],
        [-3.1963e-05, -2.5615e-05,  2.5868e-05,  ..., -2.6867e-05,
         -2.0593e-05, -2.1622e-05],
        [-2.4885e-05, -1.8924e-05,  1.9893e-05,  ..., -2.0966e-05,
         -1.5929e-05, -1.8016e-05],
        [-2.7806e-05, -2.2814e-05,  2.2188e-05,  ..., -2.3723e-05,
         -1.9014e-05, -1.7732e-05]], device='cuda:0')
Loss: 0.9908639192581177
Graident accumulation at epoch 1, step 1199, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0021,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.9977e-05,  1.5735e-04, -7.9911e-05,  ..., -7.3751e-06,
         -1.1785e-04, -1.3797e-04],
        [-8.2635e-06, -9.9291e-07,  1.1877e-06,  ..., -5.7987e-06,
         -2.0672e-06, -4.9169e-06],
        [-3.5604e-06,  9.0690e-06,  4.2520e-06,  ...,  3.4955e-07,
          3.2071e-06, -7.7396e-06],
        [ 7.9709e-06,  2.9832e-05, -8.7051e-06,  ...,  1.2808e-05,
          2.6263e-05,  6.7855e-06],
        [-2.3560e-05, -1.6284e-05,  1.5579e-05,  ..., -1.9707e-05,
         -1.3527e-05, -1.6124e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2931e-08, 5.4649e-08, 6.4297e-08,  ..., 2.4494e-08, 1.4909e-07,
         3.5716e-08],
        [7.7754e-11, 5.0924e-11, 1.8742e-11,  ..., 5.3767e-11, 1.5459e-11,
         2.1826e-11],
        [3.4773e-09, 2.0099e-09, 1.0150e-09,  ..., 2.6736e-09, 8.4727e-10,
         9.1031e-10],
        [9.1920e-10, 7.9944e-10, 3.0964e-10,  ..., 7.8764e-10, 4.9779e-10,
         3.1823e-10],
        [3.6975e-10, 2.0421e-10, 6.6315e-11,  ..., 2.6941e-10, 6.0333e-11,
         9.5788e-11]], device='cuda:0')
optimizer state dict: 150.0
lr: [8.147853974409676e-06, 8.147853974409676e-06]
scheduler_last_epoch: 150


Running epoch 1, step 1200, batch 152
Sampled inputs[:2]: tensor([[   0,  292,   65,  ...,   12,  857,  344],
        [   0,  677, 9606,  ..., 9468, 9268,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9289e-05,  1.1156e-04, -1.0517e-04,  ...,  1.7275e-05,
         -3.1926e-05, -5.3587e-05],
        [-1.4827e-06, -1.0878e-06,  1.1101e-06,  ..., -1.1921e-06,
         -9.2015e-07, -1.0356e-06],
        [-4.1425e-06, -3.1888e-06,  3.2783e-06,  ..., -3.3677e-06,
         -2.5928e-06, -2.8908e-06],
        [-3.1590e-06, -2.2948e-06,  2.4736e-06,  ..., -2.5481e-06,
         -1.9521e-06, -2.3246e-06],
        [-3.6806e-06, -2.9057e-06,  2.8610e-06,  ..., -3.0398e-06,
         -2.4438e-06, -2.4140e-06]], device='cuda:0')
Loss: 0.9714568853378296


Running epoch 1, step 1201, batch 153
Sampled inputs[:2]: tensor([[    0,   271, 16217,  ...,  6352,  4546,  2558],
        [    0,    16,    14,  ...,   300,  9283,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7947e-05,  1.4870e-04, -3.2842e-04,  ..., -2.2704e-05,
         -5.5371e-05, -2.6428e-04],
        [-2.8685e-06, -2.2799e-06,  2.0042e-06,  ..., -2.4363e-06,
         -1.9707e-06, -2.0713e-06],
        [-8.0168e-06, -6.6161e-06,  6.0052e-06,  ..., -6.7949e-06,
         -5.4687e-06, -5.7220e-06],
        [-6.1244e-06, -4.8131e-06,  4.4703e-06,  ..., -5.2154e-06,
         -4.2021e-06, -4.6641e-06],
        [-7.2420e-06, -6.0946e-06,  5.3346e-06,  ..., -6.2138e-06,
         -5.1856e-06, -4.8429e-06]], device='cuda:0')
Loss: 0.9905964136123657


Running epoch 1, step 1202, batch 154
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  546,  360,  ..., 9107, 2772, 4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8499e-05,  1.2917e-04, -1.3407e-04,  ...,  4.1108e-05,
          1.3492e-04, -1.6558e-04],
        [-4.1649e-06, -3.4496e-06,  3.0622e-06,  ..., -3.6061e-06,
         -2.8796e-06, -3.0249e-06],
        [-1.1832e-05, -1.0043e-05,  9.2685e-06,  ..., -1.0177e-05,
         -8.0615e-06, -8.4788e-06],
        [-9.0748e-06, -7.3910e-06,  6.9588e-06,  ..., -7.8380e-06,
         -6.2138e-06, -6.9290e-06],
        [-1.0535e-05, -9.1195e-06,  8.1211e-06,  ..., -9.1791e-06,
         -7.5698e-06, -7.0781e-06]], device='cuda:0')
Loss: 0.9960756897926331


Running epoch 1, step 1203, batch 155
Sampled inputs[:2]: tensor([[    0,  1241,  2098,  ...,  1862,   631,   369],
        [    0,    19,     9,  ..., 11504,   446,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2059e-04,  1.2046e-04, -1.1524e-04,  ..., -1.9030e-05,
          1.8314e-04, -2.5812e-04],
        [-5.5879e-06, -4.5747e-06,  4.2170e-06,  ..., -4.7982e-06,
         -3.7439e-06, -4.0382e-06],
        [-1.5944e-05, -1.3396e-05,  1.2785e-05,  ..., -1.3620e-05,
         -1.0565e-05, -1.1399e-05],
        [-1.2234e-05, -9.8497e-06,  9.6262e-06,  ..., -1.0461e-05,
         -8.0913e-06, -9.2536e-06],
        [-1.4111e-05, -1.2115e-05,  1.1101e-05,  ..., -1.2219e-05,
         -9.8944e-06, -9.4771e-06]], device='cuda:0')
Loss: 0.985002875328064


Running epoch 1, step 1204, batch 156
Sampled inputs[:2]: tensor([[   0, 9116,  278,  ..., 6997, 3244, 1192],
        [   0,   15, 4291,  ..., 1685,  278, 2101]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6842e-04,  2.6000e-04, -1.8939e-04,  ..., -8.6650e-05,
          2.3573e-04, -3.3724e-04],
        [-6.9588e-06, -5.7369e-06,  5.2676e-06,  ..., -6.0052e-06,
         -4.6752e-06, -4.9770e-06],
        [-1.9789e-05, -1.6734e-05,  1.5885e-05,  ..., -1.6958e-05,
         -1.3143e-05, -1.3992e-05],
        [-1.5184e-05, -1.2308e-05,  1.1995e-05,  ..., -1.3039e-05,
         -1.0058e-05, -1.1384e-05],
        [-1.7524e-05, -1.5154e-05,  1.3813e-05,  ..., -1.5259e-05,
         -1.2323e-05, -1.1653e-05]], device='cuda:0')
Loss: 0.9671201109886169


Running epoch 1, step 1205, batch 157
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0,   271,  8130,  ...,   609, 28676,   965]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3259e-04,  3.3224e-04, -1.6725e-04,  ..., -2.2186e-05,
          2.9216e-04, -2.8288e-04],
        [-8.2776e-06, -6.8545e-06,  6.3255e-06,  ..., -7.1749e-06,
         -5.5246e-06, -5.9158e-06],
        [-2.3633e-05, -2.0057e-05,  1.9133e-05,  ..., -2.0325e-05,
         -1.5587e-05, -1.6704e-05],
        [-1.8060e-05, -1.4678e-05,  1.4409e-05,  ..., -1.5572e-05,
         -1.1861e-05, -1.3545e-05],
        [-2.0817e-05, -1.8105e-05,  1.6555e-05,  ..., -1.8209e-05,
         -1.4588e-05, -1.3843e-05]], device='cuda:0')
Loss: 0.974717915058136


Running epoch 1, step 1206, batch 158
Sampled inputs[:2]: tensor([[    0,  1967,  6851,  ...,  1151,   809,   360],
        [    0, 13706,  1862,  ...,   275,  1036, 42948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9613e-04,  3.2137e-04, -1.4162e-04,  ..., -6.6949e-05,
          3.0988e-04, -3.2506e-04],
        [-9.7007e-06, -7.9498e-06,  7.4655e-06,  ..., -8.3297e-06,
         -6.3814e-06, -6.9067e-06],
        [-2.7716e-05, -2.3305e-05,  2.2575e-05,  ..., -2.3633e-05,
         -1.8060e-05, -1.9550e-05],
        [-2.1234e-05, -1.7077e-05,  1.7077e-05,  ..., -1.8120e-05,
         -1.3746e-05, -1.5870e-05],
        [-2.4244e-05, -2.0936e-05,  1.9416e-05,  ..., -2.1055e-05,
         -1.6823e-05, -1.6123e-05]], device='cuda:0')
Loss: 0.9734746217727661


Running epoch 1, step 1207, batch 159
Sampled inputs[:2]: tensor([[    0,   508, 12163,  ...,  4920,   344, 11003],
        [    0,   300, 16683,  ...,  8709,    40,  9817]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6688e-04,  3.5825e-04, -2.2334e-04,  ..., -7.8478e-05,
          4.1910e-04, -2.6523e-04],
        [-1.1131e-05, -9.0152e-06,  8.6054e-06,  ..., -9.5293e-06,
         -7.2345e-06, -7.8604e-06],
        [-3.1799e-05, -2.6450e-05,  2.6003e-05,  ..., -2.7031e-05,
         -2.0504e-05, -2.2262e-05],
        [-2.4393e-05, -1.9372e-05,  1.9699e-05,  ..., -2.0742e-05,
         -1.5594e-05, -1.8075e-05],
        [-2.7671e-05, -2.3648e-05,  2.2247e-05,  ..., -2.3976e-05,
         -1.9014e-05, -1.8269e-05]], device='cuda:0')
Loss: 0.9633569121360779
Graident accumulation at epoch 1, step 1207, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.6292e-05,  1.7744e-04, -9.4254e-05,  ..., -1.4485e-05,
         -6.4157e-05, -1.5070e-04],
        [-8.5502e-06, -1.7951e-06,  1.9294e-06,  ..., -6.1718e-06,
         -2.5839e-06, -5.2112e-06],
        [-6.3843e-06,  5.5171e-06,  6.4271e-06,  ..., -2.3885e-06,
          8.3602e-07, -9.1919e-06],
        [ 4.7345e-06,  2.4911e-05, -5.8646e-06,  ...,  9.4534e-06,
          2.2077e-05,  4.2994e-06],
        [-2.3971e-05, -1.7021e-05,  1.6246e-05,  ..., -2.0134e-05,
         -1.4076e-05, -1.6339e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2906e-08, 5.4723e-08, 6.4283e-08,  ..., 2.4476e-08, 1.4911e-07,
         3.5751e-08],
        [7.7800e-11, 5.0954e-11, 1.8798e-11,  ..., 5.3804e-11, 1.5496e-11,
         2.1866e-11],
        [3.4749e-09, 2.0086e-09, 1.0147e-09,  ..., 2.6717e-09, 8.4684e-10,
         9.0990e-10],
        [9.1888e-10, 7.9902e-10, 3.0972e-10,  ..., 7.8728e-10, 4.9754e-10,
         3.1824e-10],
        [3.7015e-10, 2.0456e-10, 6.6744e-11,  ..., 2.6972e-10, 6.0634e-11,
         9.6026e-11]], device='cuda:0')
optimizer state dict: 151.0
lr: [8.026520767887557e-06, 8.026520767887557e-06]
scheduler_last_epoch: 151
Epoch 1 | Batch 159/1048 | Training PPL: 2223.5388829910335 | time 11.487561225891113
Saving checkpoint at epoch 1, step 1207, batch 159
Epoch 1 | Validation PPL: 7.036611228416232 | Learning rate: 8.026520767887557e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1207, AFTER epoch 1, step 1207


Running epoch 1, step 1208, batch 160
Sampled inputs[:2]: tensor([[    0, 14161,  1241,  ..., 15255,   768,  4239],
        [    0,    19,    14,  ...,    13,  6673,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7593e-04, -3.7488e-05,  3.7692e-05,  ...,  1.6297e-04,
         -7.3259e-05,  1.4164e-04],
        [-1.5050e-06, -9.6858e-07,  8.8289e-07,  ..., -1.3337e-06,
         -9.9838e-07, -1.3933e-06],
        [-4.1723e-06, -2.7865e-06,  2.6822e-06,  ..., -3.5912e-06,
         -2.6822e-06, -3.6657e-06],
        [-2.9653e-06, -1.8477e-06,  1.8179e-06,  ..., -2.6077e-06,
         -1.9372e-06, -2.7716e-06],
        [-4.1127e-06, -2.7120e-06,  2.6375e-06,  ..., -3.5167e-06,
         -2.7120e-06, -3.3826e-06]], device='cuda:0')
Loss: 0.9559807181358337


Running epoch 1, step 1209, batch 161
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  1746,    14,  ...,  3134,  5968,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0934e-04,  6.3398e-05,  5.8433e-05,  ...,  1.5885e-04,
          8.0404e-05,  1.5013e-04],
        [-2.9206e-06, -2.0564e-06,  1.9185e-06,  ..., -2.5779e-06,
         -1.9521e-06, -2.4140e-06],
        [-8.0764e-06, -5.9009e-06,  5.7667e-06,  ..., -7.0035e-06,
         -5.3048e-06, -6.4522e-06],
        [-5.9158e-06, -4.0531e-06,  4.0829e-06,  ..., -5.2005e-06,
         -3.9041e-06, -5.0068e-06],
        [-7.5847e-06, -5.5581e-06,  5.3346e-06,  ..., -6.5863e-06,
         -5.1707e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9610204696655273


Running epoch 1, step 1210, batch 162
Sampled inputs[:2]: tensor([[    0,   446, 28686,  ...,    35,  2706, 19712],
        [    0,   271,  8429,  ...,  9404,   963,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9890e-04,  5.8138e-06, -5.1673e-05,  ...,  1.8894e-04,
          1.1309e-04,  1.5013e-04],
        [-4.2021e-06, -3.0994e-06,  2.8610e-06,  ..., -3.7774e-06,
         -2.9355e-06, -3.4422e-06],
        [-1.1668e-05, -8.8364e-06,  8.6129e-06,  ..., -1.0252e-05,
         -7.9274e-06, -9.1940e-06],
        [-8.6725e-06, -6.1840e-06,  6.1989e-06,  ..., -7.7337e-06,
         -5.9605e-06, -7.3165e-06],
        [-1.0893e-05, -8.3297e-06,  7.9274e-06,  ..., -9.6112e-06,
         -7.7039e-06, -8.0168e-06]], device='cuda:0')
Loss: 0.9778989553451538


Running epoch 1, step 1211, batch 163
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  380,  560,  199],
        [   0,  271,  266,  ...,  365, 2463,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6458e-04,  3.6842e-05, -6.8612e-05,  ...,  1.3318e-04,
          8.8681e-05,  1.2098e-04],
        [-5.5209e-06, -4.2617e-06,  3.9637e-06,  ..., -4.9397e-06,
         -3.8706e-06, -4.3735e-06],
        [-1.5348e-05, -1.2159e-05,  1.1861e-05,  ..., -1.3471e-05,
         -1.0535e-05, -1.1742e-05],
        [-1.1489e-05, -8.5980e-06,  8.6874e-06,  ..., -1.0192e-05,
         -7.9423e-06, -9.4026e-06],
        [-1.4082e-05, -1.1295e-05,  1.0699e-05,  ..., -1.2457e-05,
         -1.0118e-05, -1.0073e-05]], device='cuda:0')
Loss: 0.9729995727539062


Running epoch 1, step 1212, batch 164
Sampled inputs[:2]: tensor([[    0,  7185,   328,  ...,  1427,  1477,  1061],
        [    0,   391,  4356,  ...,   287, 32873,  5362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6139e-04, -8.7581e-06, -1.7178e-04,  ...,  9.7735e-05,
          1.4652e-04,  3.8770e-05],
        [-6.8173e-06, -5.3495e-06,  5.0515e-06,  ..., -6.0797e-06,
         -4.7237e-06, -5.2862e-06],
        [-1.9014e-05, -1.5289e-05,  1.5095e-05,  ..., -1.6645e-05,
         -1.2919e-05, -1.4260e-05],
        [-1.4305e-05, -1.0893e-05,  1.1176e-05,  ..., -1.2636e-05,
         -9.7603e-06, -1.1489e-05],
        [-1.7285e-05, -1.4126e-05,  1.3486e-05,  ..., -1.5303e-05,
         -1.2353e-05, -1.2130e-05]], device='cuda:0')
Loss: 0.972664475440979


Running epoch 1, step 1213, batch 165
Sampled inputs[:2]: tensor([[    0, 49018,   292,  ...,  8774,   642,   365],
        [    0,   360,   259,  ...,    14,   381,  1371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5947e-04,  1.1142e-04, -6.9253e-05,  ...,  1.2708e-05,
          8.1680e-05, -1.1907e-04],
        [-8.1882e-06, -6.5491e-06,  6.0722e-06,  ..., -7.2792e-06,
         -5.6922e-06, -6.2175e-06],
        [-2.2814e-05, -1.8716e-05,  1.8105e-05,  ..., -1.9953e-05,
         -1.5572e-05, -1.6883e-05],
        [-1.7226e-05, -1.3411e-05,  1.3471e-05,  ..., -1.5169e-05,
         -1.1772e-05, -1.3605e-05],
        [-2.0728e-05, -1.7315e-05,  1.6153e-05,  ..., -1.8373e-05,
         -1.4916e-05, -1.4365e-05]], device='cuda:0')
Loss: 0.9967513084411621


Running epoch 1, step 1214, batch 166
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,   287, 20640,   292],
        [    0, 13466,    14,  ..., 11227,  1966,  4039]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1747e-04,  1.6686e-04, -8.3967e-06,  ...,  5.6710e-05,
          1.1574e-04, -3.2764e-04],
        [-9.6932e-06, -7.6070e-06,  7.1004e-06,  ..., -8.6278e-06,
         -6.6608e-06, -7.3873e-06],
        [-2.6926e-05, -2.1681e-05,  2.1085e-05,  ..., -2.3529e-05,
         -1.8165e-05, -1.9997e-05],
        [-2.0295e-05, -1.5482e-05,  1.5646e-05,  ..., -1.7881e-05,
         -1.3694e-05, -1.6049e-05],
        [-2.4632e-05, -2.0176e-05,  1.8939e-05,  ..., -2.1815e-05,
         -1.7524e-05, -1.7166e-05]], device='cuda:0')
Loss: 0.968197762966156


Running epoch 1, step 1215, batch 167
Sampled inputs[:2]: tensor([[    0,   257,    13,  ...,   328,   630,  1403],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1818e-04,  2.2284e-04,  1.7659e-04,  ..., -5.8257e-05,
          1.1574e-04, -3.0625e-04],
        [-1.1072e-05, -8.7842e-06,  8.1286e-06,  ..., -9.8646e-06,
         -7.6443e-06, -8.3633e-06],
        [-3.0741e-05, -2.5064e-05,  2.4095e-05,  ..., -2.6941e-05,
         -2.0847e-05, -2.2694e-05],
        [-2.3201e-05, -1.7926e-05,  1.7896e-05,  ..., -2.0474e-05,
         -1.5736e-05, -1.8239e-05],
        [-2.8089e-05, -2.3320e-05,  2.1607e-05,  ..., -2.4945e-05,
         -2.0087e-05, -1.9446e-05]], device='cuda:0')
Loss: 1.0003676414489746
Graident accumulation at epoch 1, step 1215, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9845e-05,  1.8198e-04, -6.7170e-05,  ..., -1.8863e-05,
         -4.6167e-05, -1.6625e-04],
        [-8.8024e-06, -2.4940e-06,  2.5494e-06,  ..., -6.5411e-06,
         -3.0900e-06, -5.5264e-06],
        [-8.8200e-06,  2.4590e-06,  8.1939e-06,  ..., -4.8438e-06,
         -1.3323e-06, -1.0542e-05],
        [ 1.9410e-06,  2.0628e-05, -3.4885e-06,  ...,  6.4606e-06,
          1.8296e-05,  2.0456e-06],
        [-2.4383e-05, -1.7651e-05,  1.6782e-05,  ..., -2.0615e-05,
         -1.4677e-05, -1.6650e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2867e-08, 5.4717e-08, 6.4250e-08,  ..., 2.4455e-08, 1.4898e-07,
         3.5809e-08],
        [7.7845e-11, 5.0980e-11, 1.8845e-11,  ..., 5.3848e-11, 1.5539e-11,
         2.1914e-11],
        [3.4723e-09, 2.0072e-09, 1.0143e-09,  ..., 2.6697e-09, 8.4643e-10,
         9.0950e-10],
        [9.1850e-10, 7.9854e-10, 3.0973e-10,  ..., 7.8691e-10, 4.9729e-10,
         3.1826e-10],
        [3.7057e-10, 2.0490e-10, 6.7144e-11,  ..., 2.7007e-10, 6.0977e-11,
         9.6308e-11]], device='cuda:0')
optimizer state dict: 152.0
lr: [7.905489126218852e-06, 7.905489126218852e-06]
scheduler_last_epoch: 152


Running epoch 1, step 1216, batch 168
Sampled inputs[:2]: tensor([[    0, 18905,  2311,  ..., 10213,   908,   694],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5873e-05, -1.5116e-05,  3.4006e-05,  ..., -2.0691e-05,
         -9.1953e-05,  9.1168e-05],
        [-1.3262e-06, -1.0505e-06,  1.1176e-06,  ..., -1.1697e-06,
         -9.3132e-07, -9.2387e-07],
        [-3.7700e-06, -3.0547e-06,  3.3230e-06,  ..., -3.3081e-06,
         -2.6226e-06, -2.6226e-06],
        [-2.8163e-06, -2.1756e-06,  2.4736e-06,  ..., -2.4587e-06,
         -1.9372e-06, -2.0564e-06],
        [-3.3677e-06, -2.8312e-06,  2.9504e-06,  ..., -3.0249e-06,
         -2.5183e-06, -2.2054e-06]], device='cuda:0')
Loss: 0.9898673295974731


Running epoch 1, step 1217, batch 169
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1760,    9,  ..., 5996,   71,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9879e-05, -5.4524e-05, -6.7475e-05,  ..., -5.9022e-05,
         -7.3723e-05,  9.3086e-05],
        [-2.7791e-06, -2.2203e-06,  2.2650e-06,  ..., -2.3544e-06,
         -1.8217e-06, -1.8924e-06],
        [-7.7933e-06, -6.4224e-06,  6.6608e-06,  ..., -6.6310e-06,
         -5.1558e-06, -5.3197e-06],
        [-5.9456e-06, -4.6492e-06,  5.0664e-06,  ..., -5.0068e-06,
         -3.8445e-06, -4.2468e-06],
        [-6.9290e-06, -5.9009e-06,  5.8711e-06,  ..., -6.0201e-06,
         -4.9174e-06, -4.4703e-06]], device='cuda:0')
Loss: 0.994256854057312


Running epoch 1, step 1218, batch 170
Sampled inputs[:2]: tensor([[    0,    14,  6707,  ..., 17771,   300,   259],
        [    0,    17,  2736,  ...,   352,   422,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5353e-04,  1.4725e-05,  1.0249e-04,  ..., -2.4176e-05,
          1.6972e-04,  2.2712e-05],
        [-4.1276e-06, -3.2410e-06,  3.1590e-06,  ..., -3.6806e-06,
         -2.8424e-06, -3.0026e-06],
        [-1.1310e-05, -9.1791e-06,  9.2238e-06,  ..., -9.9987e-06,
         -7.7188e-06, -8.0913e-06],
        [-8.6427e-06, -6.6459e-06,  6.9290e-06,  ..., -7.6741e-06,
         -5.8711e-06, -6.5863e-06],
        [-1.0476e-05, -8.7470e-06,  8.4490e-06,  ..., -9.4324e-06,
         -7.6145e-06, -7.0632e-06]], device='cuda:0')
Loss: 0.991719126701355


Running epoch 1, step 1219, batch 171
Sampled inputs[:2]: tensor([[    0,  1197, 10640,  ...,  2405,   437,  5880],
        [    0,  2853, 21042,  ...,  4120,   607, 11176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2433e-04,  5.7697e-05,  1.8666e-04,  ..., -5.6503e-05,
          2.5986e-04,  7.7201e-05],
        [-5.5358e-06, -4.4405e-06,  4.2096e-06,  ..., -4.9397e-06,
         -3.8631e-06, -3.9786e-06],
        [-1.5125e-05, -1.2577e-05,  1.2264e-05,  ..., -1.3441e-05,
         -1.0535e-05, -1.0714e-05],
        [-1.1593e-05, -9.1493e-06,  9.2387e-06,  ..., -1.0327e-05,
         -8.0168e-06, -8.7321e-06],
        [-1.3962e-05, -1.1936e-05,  1.1191e-05,  ..., -1.2621e-05,
         -1.0341e-05, -9.3430e-06]], device='cuda:0')
Loss: 1.0217562913894653


Running epoch 1, step 1220, batch 172
Sampled inputs[:2]: tensor([[    0,   445,    29,  ..., 20247,   272,   298],
        [    0,  2380,  2667,  ...,    14,   381,  5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6562e-04, -1.2453e-05, -1.0054e-04,  ...,  3.5018e-04,
          9.3009e-05,  3.5947e-04],
        [-6.8992e-06, -5.4166e-06,  5.0552e-06,  ..., -6.2287e-06,
         -4.7684e-06, -5.1782e-06],
        [-1.8969e-05, -1.5453e-05,  1.4842e-05,  ..., -1.7032e-05,
         -1.3098e-05, -1.4037e-05],
        [-1.4454e-05, -1.1161e-05,  1.1086e-05,  ..., -1.3039e-05,
         -9.9391e-06, -1.1325e-05],
        [-1.7598e-05, -1.4648e-05,  1.3590e-05,  ..., -1.6004e-05,
         -1.2815e-05, -1.2279e-05]], device='cuda:0')
Loss: 0.9687694907188416


Running epoch 1, step 1221, batch 173
Sampled inputs[:2]: tensor([[   0,  278, 6046,  ..., 1671,  199,  395],
        [   0, 5041,   14,  ..., 1027, 1722, 6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4574e-04,  4.9621e-05, -1.8538e-04,  ...,  4.5609e-04,
          4.0689e-04,  6.1011e-04],
        [-8.2925e-06, -6.4149e-06,  6.0610e-06,  ..., -7.4431e-06,
         -5.6773e-06, -6.3106e-06],
        [-2.2843e-05, -1.8358e-05,  1.7837e-05,  ..., -2.0370e-05,
         -1.5616e-05, -1.7107e-05],
        [-1.7375e-05, -1.3202e-05,  1.3277e-05,  ..., -1.5572e-05,
         -1.1832e-05, -1.3813e-05],
        [-2.1026e-05, -1.7285e-05,  1.6183e-05,  ..., -1.8999e-05,
         -1.5184e-05, -1.4827e-05]], device='cuda:0')
Loss: 0.9721555113792419


Running epoch 1, step 1222, batch 174
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   287,   552,  ...,  7407,  2401,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1158e-04, -9.1420e-05, -1.7147e-04,  ...,  3.5419e-04,
          5.8408e-04,  5.0330e-04],
        [-9.6932e-06, -7.5251e-06,  7.1488e-06,  ..., -8.6874e-06,
         -6.6273e-06, -7.2718e-06],
        [-2.6643e-05, -2.1502e-05,  2.0951e-05,  ..., -2.3723e-05,
         -1.8194e-05, -1.9670e-05],
        [-2.0280e-05, -1.5467e-05,  1.5631e-05,  ..., -1.8135e-05,
         -1.3784e-05, -1.5900e-05],
        [-2.4483e-05, -2.0251e-05,  1.8984e-05,  ..., -2.2098e-05,
         -1.7673e-05, -1.7017e-05]], device='cuda:0')
Loss: 1.0278899669647217


Running epoch 1, step 1223, batch 175
Sampled inputs[:2]: tensor([[   0,  199,  769,  ...,  380,  560,  199],
        [   0,   12,  287,  ..., 4626,   27,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0498e-04, -7.8799e-05, -2.7249e-04,  ...,  2.4573e-04,
          6.4048e-04,  4.1332e-04],
        [-1.1072e-05, -8.5458e-06,  8.1323e-06,  ..., -9.8720e-06,
         -7.5400e-06, -8.3223e-06],
        [-3.0443e-05, -2.4378e-05,  2.3872e-05,  ..., -2.6911e-05,
         -2.0653e-05, -2.2501e-05],
        [-2.3142e-05, -1.7509e-05,  1.7792e-05,  ..., -2.0564e-05,
         -1.5646e-05, -1.8194e-05],
        [-2.7999e-05, -2.2992e-05,  2.1636e-05,  ..., -2.5094e-05,
         -2.0087e-05, -1.9461e-05]], device='cuda:0')
Loss: 0.9683258533477783
Graident accumulation at epoch 1, step 1223, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6379e-06,  1.5590e-04, -8.7702e-05,  ...,  7.5967e-06,
          2.2498e-05, -1.0830e-04],
        [-9.0293e-06, -3.0992e-06,  3.1077e-06,  ..., -6.8742e-06,
         -3.5350e-06, -5.8060e-06],
        [-1.0982e-05, -2.2469e-07,  9.7617e-06,  ..., -7.0505e-06,
         -3.2643e-06, -1.1738e-05],
        [-5.6729e-07,  1.6814e-05, -1.3605e-06,  ...,  3.7582e-06,
          1.4902e-05,  2.1572e-08],
        [-2.4745e-05, -1.8185e-05,  1.7268e-05,  ..., -2.1063e-05,
         -1.5218e-05, -1.6931e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2907e-08, 5.4669e-08, 6.4260e-08,  ..., 2.4491e-08, 1.4924e-07,
         3.5944e-08],
        [7.7889e-11, 5.1002e-11, 1.8892e-11,  ..., 5.3891e-11, 1.5580e-11,
         2.1961e-11],
        [3.4698e-09, 2.0058e-09, 1.0138e-09,  ..., 2.6678e-09, 8.4601e-10,
         9.0910e-10],
        [9.1811e-10, 7.9805e-10, 3.0974e-10,  ..., 7.8655e-10, 4.9704e-10,
         3.1827e-10],
        [3.7098e-10, 2.0523e-10, 6.7545e-11,  ..., 2.7043e-10, 6.1320e-11,
         9.6591e-11]], device='cuda:0')
optimizer state dict: 153.0
lr: [7.784777544094901e-06, 7.784777544094901e-06]
scheduler_last_epoch: 153


Running epoch 1, step 1224, batch 176
Sampled inputs[:2]: tensor([[    0,  2261,     9,  ..., 15008,    14,   333],
        [    0,   368,  2418,  ...,  3275,  1116,  5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6005e-05, -2.7921e-05,  8.9559e-05,  ..., -5.1520e-05,
          1.3293e-04,  4.1129e-05],
        [-1.3188e-06, -1.1176e-06,  9.8348e-07,  ..., -1.1697e-06,
         -9.3505e-07, -9.9838e-07],
        [-3.6359e-06, -3.1739e-06,  2.8908e-06,  ..., -3.2187e-06,
         -2.6077e-06, -2.7269e-06],
        [-2.7716e-06, -2.2948e-06,  2.1607e-06,  ..., -2.4438e-06,
         -1.9372e-06, -2.1905e-06],
        [-3.1590e-06, -2.8461e-06,  2.4736e-06,  ..., -2.8610e-06,
         -2.4289e-06, -2.2501e-06]], device='cuda:0')
Loss: 0.9794393181800842


Running epoch 1, step 1225, batch 177
Sampled inputs[:2]: tensor([[    0, 17471,  7279,  ...,   328,  6179,   287],
        [    0, 27342,    17,  ...,  5125,  3244,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0574e-05, -7.9033e-06,  9.1162e-05,  ..., -1.1497e-04,
          2.6835e-04,  1.3752e-04],
        [-2.7865e-06, -2.2203e-06,  1.8738e-06,  ..., -2.5108e-06,
         -2.0675e-06, -2.2277e-06],
        [-7.6294e-06, -6.2585e-06,  5.4836e-06,  ..., -6.8396e-06,
         -5.6326e-06, -6.0499e-06],
        [-5.6028e-06, -4.3958e-06,  3.9488e-06,  ..., -5.0217e-06,
         -4.0978e-06, -4.6641e-06],
        [-7.1526e-06, -5.9903e-06,  5.0366e-06,  ..., -6.5267e-06,
         -5.5879e-06, -5.4091e-06]], device='cuda:0')
Loss: 0.9956433176994324


Running epoch 1, step 1226, batch 178
Sampled inputs[:2]: tensor([[    0,   843,    14,  ...,   659,   271, 10511],
        [    0,   456,    17,  ...,  1553, 29477,  2713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2605e-04, -7.5578e-05,  3.6858e-04,  ..., -2.6044e-04,
          5.2178e-04,  1.5176e-04],
        [-4.2468e-06, -3.3230e-06,  2.8275e-06,  ..., -3.8296e-06,
         -3.1255e-06, -3.4198e-06],
        [-1.1653e-05, -9.3877e-06,  8.2701e-06,  ..., -1.0431e-05,
         -8.5235e-06, -9.2685e-06],
        [-8.5086e-06, -6.5416e-06,  5.9158e-06,  ..., -7.6443e-06,
         -6.1989e-06, -7.1377e-06],
        [-1.0997e-05, -9.0301e-06,  7.6592e-06,  ..., -1.0014e-05,
         -8.4937e-06, -8.3447e-06]], device='cuda:0')
Loss: 0.9926459193229675


Running epoch 1, step 1227, batch 179
Sampled inputs[:2]: tensor([[    0,   199,   769,  ..., 12038, 15317,   342],
        [    0,  4845,  1521,  ...,   963,   292,  6414]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8871e-04, -5.5128e-05,  3.0720e-04,  ..., -3.1207e-04,
          4.3850e-04,  1.6174e-04],
        [-5.5954e-06, -4.3437e-06,  3.7961e-06,  ..., -5.0366e-06,
         -4.0084e-06, -4.3735e-06],
        [-1.5274e-05, -1.2204e-05,  1.1072e-05,  ..., -1.3620e-05,
         -1.0848e-05, -1.1787e-05],
        [-1.1295e-05, -8.5980e-06,  8.0168e-06,  ..., -1.0133e-05,
         -8.0094e-06, -9.2238e-06],
        [-1.4246e-05, -1.1638e-05,  1.0118e-05,  ..., -1.2934e-05,
         -1.0759e-05, -1.0431e-05]], device='cuda:0')
Loss: 0.9680511951446533


Running epoch 1, step 1228, batch 180
Sampled inputs[:2]: tensor([[   0,   14, 8383,  ...,  266, 1717,   13],
        [   0, 1486,  292,  ..., 7484,   15, 5357]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0850e-04,  5.3352e-06,  4.7994e-04,  ..., -3.3164e-04,
          7.3046e-04,  1.5954e-04],
        [-7.0110e-06, -5.5134e-06,  4.8019e-06,  ..., -6.2734e-06,
         -4.9621e-06, -5.3495e-06],
        [-1.9237e-05, -1.5616e-05,  1.4052e-05,  ..., -1.7077e-05,
         -1.3530e-05, -1.4544e-05],
        [-1.4156e-05, -1.0937e-05,  1.0148e-05,  ..., -1.2621e-05,
         -9.9093e-06, -1.1310e-05],
        [-1.7762e-05, -1.4752e-05,  1.2726e-05,  ..., -1.6063e-05,
         -1.3307e-05, -1.2740e-05]], device='cuda:0')
Loss: 0.9796969890594482


Running epoch 1, step 1229, batch 181
Sampled inputs[:2]: tensor([[    0,  8023,  1309,  ...,  3370,   266, 14988],
        [    0,  7428,  1566,  ...,   199,  1726,  5647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7474e-04,  6.3422e-05,  4.4956e-04,  ..., -3.5903e-04,
          8.2342e-04,  1.9846e-04],
        [-8.3297e-06, -6.6161e-06,  5.8897e-06,  ..., -7.4431e-06,
         -5.8077e-06, -6.2548e-06],
        [-2.2992e-05, -1.8775e-05,  1.7300e-05,  ..., -2.0370e-05,
         -1.5870e-05, -1.7092e-05],
        [-1.6928e-05, -1.3143e-05,  1.2532e-05,  ..., -1.5035e-05,
         -1.1615e-05, -1.3322e-05],
        [-2.1055e-05, -1.7598e-05,  1.5527e-05,  ..., -1.8999e-05,
         -1.5512e-05, -1.4797e-05]], device='cuda:0')
Loss: 0.9768009185791016


Running epoch 1, step 1230, batch 182
Sampled inputs[:2]: tensor([[    0,  9058,  5481,  ...,   508, 15074,   300],
        [    0,    61, 22315,  ..., 36901,    17,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5970e-04,  8.8793e-05,  7.7335e-04,  ..., -4.0723e-04,
          1.2392e-03,  3.3708e-04],
        [-9.6783e-06, -7.8008e-06,  6.8098e-06,  ..., -8.7246e-06,
         -6.8434e-06, -7.3127e-06],
        [ 3.1217e-05,  3.7865e-05, -3.6582e-05,  ...,  5.5447e-05,
          8.4897e-05,  2.2814e-05],
        [-1.9684e-05, -1.5542e-05,  1.4499e-05,  ..., -1.7643e-05,
         -1.3717e-05, -1.5587e-05],
        [-2.4542e-05, -2.0802e-05,  1.8045e-05,  ..., -2.2307e-05,
         -1.8284e-05, -1.7330e-05]], device='cuda:0')
Loss: 1.0064505338668823


Running epoch 1, step 1231, batch 183
Sampled inputs[:2]: tensor([[    0,   266,  2057,  ...,    88,  1801,    66],
        [    0,   221,   474,  ...,    14, 10961,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9127e-04,  2.6149e-04,  8.0657e-04,  ..., -3.7061e-04,
          1.0796e-03,  2.7963e-04],
        [-1.1101e-05, -8.9332e-06,  7.8753e-06,  ..., -9.9614e-06,
         -7.8194e-06, -8.3335e-06],
        [ 2.7372e-05,  3.4676e-05, -3.3557e-05,  ...,  5.2080e-05,
          8.2245e-05,  2.0042e-05],
        [-2.2635e-05, -1.7852e-05,  1.6779e-05,  ..., -2.0206e-05,
         -1.5713e-05, -1.7807e-05],
        [-2.8059e-05, -2.3797e-05,  2.0757e-05,  ..., -2.5436e-05,
         -2.0862e-05, -1.9729e-05]], device='cuda:0')
Loss: 0.9942837953567505
Graident accumulation at epoch 1, step 1231, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2402e-05,  1.6646e-04,  1.7253e-06,  ..., -3.0224e-05,
          1.2821e-04, -6.9503e-05],
        [-9.2365e-06, -3.6826e-06,  3.5844e-06,  ..., -7.1829e-06,
         -3.9634e-06, -6.0588e-06],
        [-7.1468e-06,  3.2654e-06,  5.4298e-06,  ..., -1.1375e-06,
          5.2866e-06, -8.5600e-06],
        [-2.7740e-06,  1.3347e-05,  4.5343e-07,  ...,  1.3618e-06,
          1.1840e-05, -1.7613e-06],
        [-2.5076e-05, -1.8746e-05,  1.7617e-05,  ..., -2.1500e-05,
         -1.5782e-05, -1.7211e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3204e-08, 5.4683e-08, 6.4846e-08,  ..., 2.4604e-08, 1.5026e-07,
         3.5986e-08],
        [7.7935e-11, 5.1031e-11, 1.8935e-11,  ..., 5.3937e-11, 1.5626e-11,
         2.2009e-11],
        [3.4671e-09, 2.0050e-09, 1.0139e-09,  ..., 2.6678e-09, 8.5193e-10,
         9.0859e-10],
        [9.1771e-10, 7.9757e-10, 3.0971e-10,  ..., 7.8617e-10, 4.9679e-10,
         3.1827e-10],
        [3.7140e-10, 2.0559e-10, 6.7908e-11,  ..., 2.7080e-10, 6.1694e-11,
         9.6883e-11]], device='cuda:0')
optimizer state dict: 154.0
lr: [7.664404467299166e-06, 7.664404467299166e-06]
scheduler_last_epoch: 154


Running epoch 1, step 1232, batch 184
Sampled inputs[:2]: tensor([[    0,   409,  3669,  ...,    12,   374,    20],
        [    0,  7294, 23782,  ...,   471, 11528,  3437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2734e-04,  3.2697e-05,  1.3330e-04,  ...,  2.3021e-06,
          1.7729e-04,  1.0939e-04],
        [-1.3858e-06, -1.1176e-06,  9.9838e-07,  ..., -1.2442e-06,
         -9.3877e-07, -1.0356e-06],
        [-3.8743e-06, -3.2336e-06,  2.9653e-06,  ..., -3.5018e-06,
         -2.6673e-06, -2.8908e-06],
        [-2.8312e-06, -2.2203e-06,  2.1309e-06,  ..., -2.5332e-06,
         -1.8999e-06, -2.2352e-06],
        [-3.5167e-06, -3.0249e-06,  2.6524e-06,  ..., -3.2336e-06,
         -2.5779e-06, -2.4587e-06]], device='cuda:0')
Loss: 0.9861399531364441


Running epoch 1, step 1233, batch 185
Sampled inputs[:2]: tensor([[    0, 11325,   278,  ...,   446,  1869,   642],
        [    0,    12,  1250,  ...,   381,  1524,  2204]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5632e-04, -1.8801e-05,  1.4479e-04,  ...,  2.1651e-05,
          2.7845e-04,  2.2290e-04],
        [-2.7865e-06, -2.2724e-06,  2.0564e-06,  ..., -2.4661e-06,
         -1.8850e-06, -1.9558e-06],
        [-7.7784e-06, -6.5714e-06,  6.0797e-06,  ..., -6.9290e-06,
         -5.3048e-06, -5.4836e-06],
        [-5.6922e-06, -4.5449e-06,  4.3958e-06,  ..., -5.0068e-06,
         -3.7923e-06, -4.2319e-06],
        [-7.0184e-06, -6.1244e-06,  5.3942e-06,  ..., -6.3777e-06,
         -5.1111e-06, -4.6492e-06]], device='cuda:0')
Loss: 1.0244088172912598


Running epoch 1, step 1234, batch 186
Sampled inputs[:2]: tensor([[    0,  1171,   341,  ...,   278, 14713,    18],
        [    0,   300, 13523,  ..., 42438,   786,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8943e-05, -4.7366e-05,  4.3885e-05,  ...,  3.4374e-05,
          2.0500e-04,  2.6972e-04],
        [-4.1723e-06, -3.3751e-06,  3.0994e-06,  ..., -3.6880e-06,
         -2.8275e-06, -2.8796e-06],
        [-1.1593e-05, -9.7156e-06,  9.1046e-06,  ..., -1.0297e-05,
         -7.9125e-06, -8.0466e-06],
        [-8.5086e-06, -6.7502e-06,  6.6161e-06,  ..., -7.4804e-06,
         -5.6848e-06, -6.2287e-06],
        [-1.0461e-05, -9.1046e-06,  8.0913e-06,  ..., -9.4920e-06,
         -7.6592e-06, -6.8098e-06]], device='cuda:0')
Loss: 1.008687138557434


Running epoch 1, step 1235, batch 187
Sampled inputs[:2]: tensor([[    0,  7111,   409,  ...,  1908,  1260,   883],
        [    0,   431, 19346,  ...,    14,  3237, 18548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8943e-05, -1.3482e-04,  8.2212e-05,  ...,  3.3602e-05,
          3.9881e-04,  3.3420e-04],
        [-5.5358e-06, -4.4703e-06,  4.0047e-06,  ..., -4.9621e-06,
         -3.7961e-06, -3.9078e-06],
        [-1.5259e-05, -1.2755e-05,  1.1817e-05,  ..., -1.3664e-05,
         -1.0505e-05, -1.0714e-05],
        [-1.1265e-05, -8.9556e-06,  8.5831e-06,  ..., -1.0058e-05,
         -7.6517e-06, -8.4043e-06],
        [-1.3873e-05, -1.1995e-05,  1.0580e-05,  ..., -1.2666e-05,
         -1.0237e-05, -9.1046e-06]], device='cuda:0')
Loss: 0.9996426701545715


Running epoch 1, step 1236, batch 188
Sampled inputs[:2]: tensor([[    0,   857,   344,  ...,  1529,  9106,  1447],
        [    0, 28684,   472,  ...,   317,     9,  1926]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4452e-04, -1.2618e-04, -1.2541e-04,  ...,  5.1875e-05,
          4.4050e-04,  3.1920e-04],
        [-6.8992e-06, -5.5209e-06,  5.0180e-06,  ..., -6.1616e-06,
         -4.7162e-06, -4.8690e-06],
        [-1.8775e-05, -1.5616e-05,  1.4663e-05,  ..., -1.6764e-05,
         -1.2904e-05, -1.3158e-05],
        [-1.3933e-05, -1.0982e-05,  1.0699e-05,  ..., -1.2413e-05,
         -9.4622e-06, -1.0416e-05],
        [-1.7092e-05, -1.4722e-05,  1.3128e-05,  ..., -1.5542e-05,
         -1.2591e-05, -1.1161e-05]], device='cuda:0')
Loss: 0.9516633152961731


Running epoch 1, step 1237, batch 189
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,  3988,  1049,   935],
        [    0, 10084,    12,  ..., 24717,   365,  1616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5159e-04, -1.3721e-04, -1.2465e-04,  ...,  6.4825e-05,
          5.6283e-04,  3.4069e-04],
        [-8.2701e-06, -6.5938e-06,  5.9940e-06,  ..., -7.3761e-06,
         -5.6624e-06, -5.9195e-06],
        [-2.2471e-05, -1.8641e-05,  1.7509e-05,  ..., -2.0057e-05,
         -1.5497e-05, -1.5974e-05],
        [-1.6689e-05, -1.3113e-05,  1.2755e-05,  ..., -1.4856e-05,
         -1.1355e-05, -1.2621e-05],
        [-2.0459e-05, -1.7554e-05,  1.5661e-05,  ..., -1.8597e-05,
         -1.5095e-05, -1.3545e-05]], device='cuda:0')
Loss: 0.9676244854927063


Running epoch 1, step 1238, batch 190
Sampled inputs[:2]: tensor([[    0,   285,    53,  ...,   259,  5012,  3037],
        [    0, 48545,    26,  ...,  1471,   266,   319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5887e-05, -2.3264e-04, -2.3461e-04,  ...,  6.3549e-05,
          4.5413e-04,  3.4362e-04],
        [-9.6336e-06, -7.5623e-06,  6.9700e-06,  ..., -8.6278e-06,
         -6.6161e-06, -7.0073e-06],
        [-2.6226e-05, -2.1368e-05,  2.0340e-05,  ..., -2.3425e-05,
         -1.8045e-05, -1.8880e-05],
        [-1.9521e-05, -1.5035e-05,  1.4856e-05,  ..., -1.7434e-05,
         -1.3292e-05, -1.4991e-05],
        [-2.3976e-05, -2.0191e-05,  1.8284e-05,  ..., -2.1815e-05,
         -1.7643e-05, -1.6123e-05]], device='cuda:0')
Loss: 0.981881320476532


Running epoch 1, step 1239, batch 191
Sampled inputs[:2]: tensor([[    0,   759,  1184,  ...,   472,   346,    14],
        [    0,   369, 17432,  ...,   874,  2577,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8137e-05, -2.7161e-04, -2.3461e-04,  ...,  2.7652e-05,
          2.0037e-04,  3.4158e-04],
        [-1.0945e-05, -8.5682e-06,  8.0131e-06,  ..., -9.7975e-06,
         -7.4878e-06, -7.9162e-06],
        [-2.9773e-05, -2.4170e-05,  2.3335e-05,  ..., -2.6539e-05,
         -2.0370e-05, -2.1309e-05],
        [-2.2262e-05, -1.7092e-05,  1.7151e-05,  ..., -1.9863e-05,
         -1.5080e-05, -1.7002e-05],
        [-2.7150e-05, -2.2799e-05,  2.0906e-05,  ..., -2.4661e-05,
         -1.9893e-05, -1.8135e-05]], device='cuda:0')
Loss: 0.9661110043525696
Graident accumulation at epoch 1, step 1239, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3348e-05,  1.2265e-04, -2.1909e-05,  ..., -2.4436e-05,
          1.3542e-04, -2.8395e-05],
        [-9.4073e-06, -4.1712e-06,  4.0273e-06,  ..., -7.4443e-06,
         -4.3159e-06, -6.2445e-06],
        [-9.4094e-06,  5.2190e-07,  7.2203e-06,  ..., -3.6777e-06,
          2.7209e-06, -9.8349e-06],
        [-4.7229e-06,  1.0303e-05,  2.1232e-06,  ..., -7.6073e-07,
          9.1483e-06, -3.2854e-06],
        [-2.5283e-05, -1.9151e-05,  1.7945e-05,  ..., -2.1816e-05,
         -1.6194e-05, -1.7303e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3152e-08, 5.4702e-08, 6.4836e-08,  ..., 2.4580e-08, 1.5015e-07,
         3.6067e-08],
        [7.7977e-11, 5.1054e-11, 1.8981e-11,  ..., 5.3979e-11, 1.5666e-11,
         2.2049e-11],
        [3.4645e-09, 2.0036e-09, 1.0135e-09,  ..., 2.6659e-09, 8.5149e-10,
         9.0814e-10],
        [9.1728e-10, 7.9706e-10, 3.0970e-10,  ..., 7.8578e-10, 4.9652e-10,
         3.1824e-10],
        [3.7176e-10, 2.0590e-10, 6.8277e-11,  ..., 2.7114e-10, 6.2028e-11,
         9.7115e-11]], device='cuda:0')
optimizer state dict: 155.0
lr: [7.544388289888527e-06, 7.544388289888527e-06]
scheduler_last_epoch: 155


Running epoch 1, step 1240, batch 192
Sampled inputs[:2]: tensor([[    0,  1211, 11131,  ..., 31480,   565,   446],
        [    0,  3412,  1707,  ..., 11114,    15,  1821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8558e-05, -4.1293e-05, -1.2508e-04,  ...,  4.6395e-05,
         -1.3987e-04, -6.5535e-05],
        [-1.4156e-06, -9.7603e-07,  1.0654e-06,  ..., -1.2666e-06,
         -8.6799e-07, -9.5367e-07],
        [-3.7253e-06, -2.6822e-06,  2.9504e-06,  ..., -3.3230e-06,
         -2.2948e-06, -2.4736e-06],
        [-2.8610e-06, -1.9222e-06,  2.2352e-06,  ..., -2.5481e-06,
         -1.7285e-06, -2.0266e-06],
        [-3.3677e-06, -2.5332e-06,  2.6375e-06,  ..., -3.0696e-06,
         -2.2352e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9717079997062683


Running epoch 1, step 1241, batch 193
Sampled inputs[:2]: tensor([[    0, 11657,   367,  ..., 31468,    26,   266],
        [    0,    26,   874,  ...,    12, 21591,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9311e-05,  2.1166e-05,  3.0394e-05,  ...,  1.0595e-04,
         -8.1058e-05,  4.5533e-05],
        [-2.8387e-06, -2.0713e-06,  2.0489e-06,  ..., -2.5332e-06,
         -1.8366e-06, -1.9372e-06],
        [-7.6294e-06, -5.7966e-06,  5.8115e-06,  ..., -6.7949e-06,
         -4.9770e-06, -5.1558e-06],
        [-5.7220e-06, -4.1127e-06,  4.3213e-06,  ..., -5.0813e-06,
         -3.6657e-06, -4.0978e-06],
        [-6.8545e-06, -5.4091e-06,  5.1409e-06,  ..., -6.2436e-06,
         -4.7982e-06, -4.3362e-06]], device='cuda:0')
Loss: 1.0123354196548462


Running epoch 1, step 1242, batch 194
Sampled inputs[:2]: tensor([[    0,    29,   413,  ...,  1527,  1503,   369],
        [    0,   287,  2503,  ...,   496,    14, 37791]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3986e-04,  2.8027e-04,  9.7507e-05,  ...,  4.0697e-05,
         -3.2451e-04,  2.9947e-04],
        [-4.3735e-06, -3.2261e-06,  2.9020e-06,  ..., -3.8818e-06,
         -2.9244e-06, -3.1218e-06],
        [-1.1742e-05, -9.0301e-06,  8.2999e-06,  ..., -1.0386e-05,
         -7.9274e-06, -8.2850e-06],
        [-8.6725e-06, -6.3032e-06,  6.0275e-06,  ..., -7.6592e-06,
         -5.7518e-06, -6.4671e-06],
        [-1.0818e-05, -8.5682e-06,  7.4953e-06,  ..., -9.7454e-06,
         -7.7784e-06, -7.1973e-06]], device='cuda:0')
Loss: 0.9376934170722961


Running epoch 1, step 1243, batch 195
Sampled inputs[:2]: tensor([[   0,  452,  298,  ...,  287, 1575, 7856],
        [   0,  320,  472,  ..., 1345,   14, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8207e-05,  2.4745e-04,  1.1356e-04,  ...,  2.9276e-04,
         -3.8206e-04,  4.2838e-04],
        [-5.7891e-06, -4.2170e-06,  3.8855e-06,  ..., -5.1782e-06,
         -3.9004e-06, -4.1276e-06],
        [-1.5497e-05, -1.1757e-05,  1.1086e-05,  ..., -1.3769e-05,
         -1.0490e-05, -1.0937e-05],
        [-1.1489e-05, -8.2031e-06,  8.0690e-06,  ..., -1.0207e-05,
         -7.6592e-06, -8.5831e-06],
        [-1.4305e-05, -1.1176e-05,  1.0028e-05,  ..., -1.2934e-05,
         -1.0282e-05, -9.4771e-06]], device='cuda:0')
Loss: 0.9639549255371094


Running epoch 1, step 1244, batch 196
Sampled inputs[:2]: tensor([[   0, 3605, 2572,  ...,  300,  259, 1513],
        [   0,  266, 2653,  ...,   29,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6492e-05,  1.0149e-04, -2.0771e-05,  ...,  2.3082e-04,
         -3.8803e-04,  3.1281e-04],
        [-7.0930e-06, -5.1409e-06,  4.8056e-06,  ..., -6.3777e-06,
         -4.7237e-06, -5.1633e-06],
        [-1.9088e-05, -1.4335e-05,  1.3813e-05,  ..., -1.6987e-05,
         -1.2681e-05, -1.3694e-05],
        [-1.4201e-05, -1.0028e-05,  1.0066e-05,  ..., -1.2681e-05,
         -9.3281e-06, -1.0848e-05],
        [-1.7568e-05, -1.3590e-05,  1.2487e-05,  ..., -1.5885e-05,
         -1.2413e-05, -1.1787e-05]], device='cuda:0')
Loss: 0.9526011347770691


Running epoch 1, step 1245, batch 197
Sampled inputs[:2]: tensor([[    0, 44175,   744,  ..., 16394, 26528,    12],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5529e-04,  9.5518e-05, -1.7691e-04,  ...,  1.4377e-04,
         -4.9275e-04,  2.0443e-04],
        [-8.2552e-06, -5.9269e-06,  5.6848e-06,  ..., -7.5027e-06,
         -5.5805e-06, -6.1132e-06],
        [-2.2054e-05, -1.6361e-05,  1.6272e-05,  ..., -1.9714e-05,
         -1.4752e-05, -1.5900e-05],
        [-1.6525e-05, -1.1474e-05,  1.1966e-05,  ..., -1.4886e-05,
         -1.0990e-05, -1.2815e-05],
        [-2.0429e-05, -1.5631e-05,  1.4812e-05,  ..., -1.8522e-05,
         -1.4544e-05, -1.3709e-05]], device='cuda:0')
Loss: 0.8852625489234924


Running epoch 1, step 1246, batch 198
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0,  2229,   352,  ...,  4988,    33,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5322e-04,  9.2585e-05, -1.9603e-04,  ...,  2.3039e-04,
         -3.1585e-04,  2.8482e-04],
        [-9.6411e-06, -6.9998e-06,  6.6683e-06,  ..., -8.7842e-06,
         -6.5640e-06, -7.0892e-06],
        [-2.5675e-05, -1.9297e-05,  1.9014e-05,  ..., -2.3037e-05,
         -1.7315e-05, -1.8403e-05],
        [-1.9267e-05, -1.3560e-05,  1.3992e-05,  ..., -1.7405e-05,
         -1.2912e-05, -1.4842e-05],
        [-2.3901e-05, -1.8522e-05,  1.7405e-05,  ..., -2.1756e-05,
         -1.7136e-05, -1.5944e-05]], device='cuda:0')
Loss: 0.9896125793457031


Running epoch 1, step 1247, batch 199
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0779e-04,  1.9742e-04, -2.9407e-04,  ...,  2.4501e-04,
         -6.1991e-04,  2.4090e-04],
        [-1.0975e-05, -7.9274e-06,  7.6964e-06,  ..., -1.0014e-05,
         -7.4543e-06, -8.0727e-06],
        [-2.9162e-05, -2.1771e-05,  2.1875e-05,  ..., -2.6166e-05,
         -1.9580e-05, -2.0847e-05],
        [-2.1920e-05, -1.5326e-05,  1.6138e-05,  ..., -1.9819e-05,
         -1.4655e-05, -1.6883e-05],
        [-2.7090e-05, -2.0891e-05,  1.9982e-05,  ..., -2.4647e-05,
         -1.9357e-05, -1.8016e-05]], device='cuda:0')
Loss: 0.9484938383102417
Graident accumulation at epoch 1, step 1247, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7234e-05,  1.3013e-04, -4.9125e-05,  ...,  2.5082e-06,
          5.9888e-05, -1.4655e-06],
        [-9.5641e-06, -4.5468e-06,  4.3942e-06,  ..., -7.7013e-06,
         -4.6297e-06, -6.4273e-06],
        [-1.1385e-05, -1.7074e-06,  8.6858e-06,  ..., -5.9265e-06,
          4.9081e-07, -1.0936e-05],
        [-6.4425e-06,  7.7405e-06,  3.5247e-06,  ..., -2.6665e-06,
          6.7679e-06, -4.6451e-06],
        [-2.5464e-05, -1.9325e-05,  1.8149e-05,  ..., -2.2099e-05,
         -1.6510e-05, -1.7374e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3142e-08, 5.4686e-08, 6.4858e-08,  ..., 2.4615e-08, 1.5038e-07,
         3.6089e-08],
        [7.8019e-11, 5.1065e-11, 1.9021e-11,  ..., 5.4025e-11, 1.5706e-11,
         2.2092e-11],
        [3.4619e-09, 2.0021e-09, 1.0129e-09,  ..., 2.6639e-09, 8.5102e-10,
         9.0767e-10],
        [9.1685e-10, 7.9650e-10, 3.0965e-10,  ..., 7.8539e-10, 4.9624e-10,
         3.1821e-10],
        [3.7213e-10, 2.0613e-10, 6.8608e-11,  ..., 2.7148e-10, 6.2340e-11,
         9.7343e-11]], device='cuda:0')
optimizer state dict: 156.0
lr: [7.424747351382533e-06, 7.424747351382533e-06]
scheduler_last_epoch: 156


Running epoch 1, step 1248, batch 200
Sampled inputs[:2]: tensor([[    0,  3767,  2337,  ...,   950,   847,   300],
        [    0,  8450,   292,  ...,   352,   722, 37719]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4922e-05,  8.9927e-05, -1.5445e-04,  ..., -2.7154e-05,
          4.8039e-05,  2.7906e-04],
        [-1.4752e-06, -9.0897e-07,  7.7486e-07,  ..., -1.3784e-06,
         -1.0431e-06, -1.3933e-06],
        [-3.9935e-06, -2.5928e-06,  2.2799e-06,  ..., -3.6657e-06,
         -2.8312e-06, -3.6806e-06],
        [-2.8461e-06, -1.7062e-06,  1.5497e-06,  ..., -2.6375e-06,
         -2.0117e-06, -2.7567e-06],
        [-4.0531e-06, -2.6375e-06,  2.2799e-06,  ..., -3.7253e-06,
         -2.9653e-06, -3.5316e-06]], device='cuda:0')
Loss: 0.9557324051856995


Running epoch 1, step 1249, batch 201
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  3125,   271,  ...,  1041,  1032,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5078e-04,  1.6166e-04, -1.5150e-04,  ..., -1.6096e-05,
          5.4755e-05,  3.9137e-04],
        [-2.8238e-06, -1.9968e-06,  1.6615e-06,  ..., -2.6524e-06,
         -1.9893e-06, -2.3991e-06],
        [-7.7635e-06, -5.7071e-06,  4.9323e-06,  ..., -7.1675e-06,
         -5.4240e-06, -6.4373e-06],
        [-5.6475e-06, -3.8967e-06,  3.4571e-06,  ..., -5.2601e-06,
         -3.9488e-06, -4.9621e-06],
        [-7.4506e-06, -5.4985e-06,  4.6343e-06,  ..., -6.9290e-06,
         -5.4389e-06, -5.8562e-06]], device='cuda:0')
Loss: 0.9561074376106262


Running epoch 1, step 1250, batch 202
Sampled inputs[:2]: tensor([[   0,  342,  516,  ...,   12,  729, 3701],
        [   0,  278,  266,  ..., 5503,  259, 1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0564e-05,  1.6183e-04, -1.9179e-04,  ...,  2.7901e-05,
         -2.3011e-05,  4.8207e-04],
        [-4.3735e-06, -3.0100e-06,  2.6450e-06,  ..., -3.9935e-06,
         -2.9579e-06, -3.6135e-06],
        [-1.1906e-05, -8.5682e-06,  7.7486e-06,  ..., -1.0744e-05,
         -8.0466e-06, -9.6560e-06],
        [-8.5831e-06, -5.8040e-06,  5.4240e-06,  ..., -7.7784e-06,
         -5.7593e-06, -7.3165e-06],
        [-1.1414e-05, -8.2701e-06,  7.2718e-06,  ..., -1.0401e-05,
         -8.0764e-06, -8.8066e-06]], device='cuda:0')
Loss: 0.9655326008796692


Running epoch 1, step 1251, batch 203
Sampled inputs[:2]: tensor([[   0,    9,  342,  ...,   12,  709,  857],
        [   0,  285,  590,  ...,  199,  395, 3523]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1361e-04,  1.5044e-04, -3.0127e-04,  ...,  1.3527e-04,
          9.0492e-05,  6.3474e-04],
        [-5.8338e-06, -3.8557e-06,  3.3937e-06,  ..., -5.3644e-06,
         -3.9414e-06, -4.9695e-06],
        [-1.5482e-05, -1.0833e-05,  9.8497e-06,  ..., -1.4052e-05,
         -1.0476e-05, -1.2800e-05],
        [-1.1384e-05, -7.4059e-06,  6.9514e-06,  ..., -1.0416e-05,
         -7.6666e-06, -9.9838e-06],
        [-1.5140e-05, -1.0565e-05,  9.4324e-06,  ..., -1.3828e-05,
         -1.0669e-05, -1.1891e-05]], device='cuda:0')
Loss: 0.9159232974052429


Running epoch 1, step 1252, batch 204
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,  6584,   278,  ...,  1039,   965,  1410]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6740e-04,  2.5174e-04, -3.0879e-04,  ...,  8.4461e-05,
          1.4146e-04,  7.0469e-04],
        [-7.3314e-06, -4.8243e-06,  4.1351e-06,  ..., -6.7651e-06,
         -5.0738e-06, -6.2734e-06],
        [-1.9446e-05, -1.3515e-05,  1.1995e-05,  ..., -1.7688e-05,
         -1.3426e-05, -1.6123e-05],
        [-1.4335e-05, -9.2536e-06,  8.4415e-06,  ..., -1.3173e-05,
         -9.9167e-06, -1.2651e-05],
        [-1.9222e-05, -1.3366e-05,  1.1653e-05,  ..., -1.7613e-05,
         -1.3798e-05, -1.5125e-05]], device='cuda:0')
Loss: 0.9704598188400269


Running epoch 1, step 1253, batch 205
Sampled inputs[:2]: tensor([[   0, 3253, 1573,  ...,  298,  358,   14],
        [   0,   12,  287,  ...,  365, 1943,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8203e-04,  3.6423e-04, -9.3741e-06,  ...,  1.0931e-05,
          2.2203e-04,  8.2308e-04],
        [-8.7768e-06, -5.9716e-06,  4.9770e-06,  ..., -8.1211e-06,
         -6.2212e-06, -7.4133e-06],
        [-2.3440e-05, -1.6794e-05,  1.4484e-05,  ..., -2.1428e-05,
         -1.6600e-05, -1.9297e-05],
        [-1.7211e-05, -1.1489e-05,  1.0177e-05,  ..., -1.5855e-05,
         -1.2182e-05, -1.5005e-05],
        [ 1.4746e-04,  1.1196e-04, -5.7631e-05,  ...,  1.1892e-04,
          1.2603e-04,  9.6699e-05]], device='cuda:0')
Loss: 0.9972277283668518


Running epoch 1, step 1254, batch 206
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,  7935,  6521,  ..., 41312,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5984e-04,  4.4620e-04, -9.4644e-05,  ..., -1.8823e-06,
          2.8933e-04,  9.2990e-04],
        [-1.0185e-05, -7.0818e-06,  5.8077e-06,  ..., -9.4175e-06,
         -7.2271e-06, -8.4490e-06],
        [-2.7284e-05, -1.9908e-05,  1.6928e-05,  ..., -2.4930e-05,
         -1.9342e-05, -2.2069e-05],
        [-2.0012e-05, -1.3664e-05,  1.1899e-05,  ..., -1.8433e-05,
         -1.4208e-05, -1.7181e-05],
        [ 1.4388e-04,  1.0896e-04, -5.5351e-05,  ...,  1.1559e-04,
          1.2332e-04,  9.4270e-05]], device='cuda:0')
Loss: 0.9922467470169067


Running epoch 1, step 1255, batch 207
Sampled inputs[:2]: tensor([[   0,   22, 2577,  ..., 4970,    9, 3868],
        [   0, 3860,  694,  ..., 1027,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5657e-04,  3.8575e-04, -2.4338e-04,  ...,  9.3610e-05,
          3.4227e-04,  1.0638e-03],
        [-1.1541e-05, -8.0317e-06,  6.6608e-06,  ..., -1.0751e-05,
         -8.2329e-06, -9.5814e-06],
        [-3.0890e-05, -2.2531e-05,  1.9386e-05,  ..., -2.8402e-05,
         -2.1964e-05, -2.4945e-05],
        [-2.2724e-05, -1.5490e-05,  1.3664e-05,  ..., -2.1085e-05,
         -1.6190e-05, -1.9535e-05],
        [ 1.4046e-04,  1.0645e-04, -5.3027e-05,  ...,  1.1230e-04,
          1.2076e-04,  9.1722e-05]], device='cuda:0')
Loss: 0.9878578782081604
Graident accumulation at epoch 1, step 1255, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.0168e-05,  1.5569e-04, -6.8551e-05,  ...,  1.1618e-05,
          8.8127e-05,  1.0506e-04],
        [-9.7618e-06, -4.8953e-06,  4.6209e-06,  ..., -8.0063e-06,
         -4.9900e-06, -6.7427e-06],
        [-1.3335e-05, -3.7897e-06,  9.7558e-06,  ..., -8.1740e-06,
         -1.7547e-06, -1.2337e-05],
        [-8.0707e-06,  5.4175e-06,  4.5387e-06,  ..., -4.5084e-06,
          4.4721e-06, -6.1342e-06],
        [-8.8720e-06, -6.7484e-06,  1.1032e-05,  ..., -8.6593e-06,
         -2.7831e-06, -6.4646e-06]], device='cuda:0')
optimizer state dict: tensor([[5.3520e-08, 5.4780e-08, 6.4852e-08,  ..., 2.4600e-08, 1.5035e-07,
         3.7185e-08],
        [7.8074e-11, 5.1079e-11, 1.9046e-11,  ..., 5.4087e-11, 1.5758e-11,
         2.2162e-11],
        [3.4594e-09, 2.0006e-09, 1.0123e-09,  ..., 2.6620e-09, 8.5065e-10,
         9.0738e-10],
        [9.1645e-10, 7.9595e-10, 3.0952e-10,  ..., 7.8505e-10, 4.9600e-10,
         3.1827e-10],
        [3.9148e-10, 2.1726e-10, 7.1351e-11,  ..., 2.8382e-10, 7.6860e-11,
         1.0566e-10]], device='cuda:0')
optimizer state dict: 157.0
lr: [7.305499933960942e-06, 7.305499933960942e-06]
scheduler_last_epoch: 157


Running epoch 1, step 1256, batch 208
Sampled inputs[:2]: tensor([[   0,  266, 1586,  ..., 1888, 2117,  328],
        [   0,   12,  638,  ...,  374,  221,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1993e-05, -1.0144e-05, -3.0978e-04,  ..., -4.5355e-05,
         -2.7994e-04, -8.9003e-05],
        [-1.4007e-06, -9.8348e-07,  9.2015e-07,  ..., -1.2591e-06,
         -8.9407e-07, -1.0356e-06],
        [-3.8743e-06, -2.8014e-06,  2.7269e-06,  ..., -3.4422e-06,
         -2.4438e-06, -2.8014e-06],
        [-2.8610e-06, -1.9521e-06,  1.9670e-06,  ..., -2.5630e-06,
         -1.8030e-06, -2.2352e-06],
        [-3.4124e-06, -2.5481e-06,  2.3842e-06,  ..., -3.0696e-06,
         -2.2948e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9555658102035522


Running epoch 1, step 1257, batch 209
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1213e-05, -3.7010e-05, -3.6030e-04,  ..., -2.6422e-05,
         -3.5587e-04, -2.0506e-04],
        [-2.7791e-06, -2.0415e-06,  1.8291e-06,  ..., -2.5034e-06,
         -1.8477e-06, -1.9856e-06],
        [-7.5847e-06, -5.7518e-06,  5.3644e-06,  ..., -6.7651e-06,
         -4.9770e-06, -5.3048e-06],
        [-5.7071e-06, -4.0829e-06,  3.9339e-06,  ..., -5.1260e-06,
         -3.7402e-06, -4.3064e-06],
        [-6.7651e-06, -5.3048e-06,  4.7386e-06,  ..., -6.1244e-06,
         -4.7535e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9832054972648621


Running epoch 1, step 1258, batch 210
Sampled inputs[:2]: tensor([[    0,   259,  1380,  ...,   287, 10221,   280],
        [    0,   413,    29,  ...,   818,   278,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5940e-07,  1.8392e-04, -3.4226e-04,  ..., -3.9573e-05,
         -3.6440e-04, -3.6669e-04],
        [-4.1947e-06, -3.1888e-06,  2.5406e-06,  ..., -3.8743e-06,
         -2.9430e-06, -3.1181e-06],
        [-1.1429e-05, -8.9407e-06,  7.5102e-06,  ..., -1.0431e-05,
         -7.9423e-06, -8.3148e-06],
        [-8.4937e-06, -6.3032e-06,  5.4017e-06,  ..., -7.8231e-06,
         -5.9009e-06, -6.6161e-06],
        [-1.0401e-05, -8.3596e-06,  6.7353e-06,  ..., -9.6262e-06,
         -7.6890e-06, -7.0781e-06]], device='cuda:0')
Loss: 0.9409632086753845


Running epoch 1, step 1259, batch 211
Sampled inputs[:2]: tensor([[   0,   14, 3449,  ...,   12, 2665,    5],
        [   0,  474,  513,  ...,  221, 2951, 7773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9818e-05,  1.7741e-04, -3.2541e-04,  ...,  7.3669e-05,
         -2.4623e-04, -3.9133e-04],
        [-5.5730e-06, -4.2021e-06,  3.2708e-06,  ..., -5.1558e-06,
         -3.9563e-06, -4.2357e-06],
        [-1.5110e-05, -1.1757e-05,  9.6858e-06,  ..., -1.3798e-05,
         -1.0654e-05, -1.1221e-05],
        [-1.1221e-05, -8.2701e-06,  6.9141e-06,  ..., -1.0371e-05,
         -7.9274e-06, -8.9258e-06],
        [-1.3933e-05, -1.1086e-05,  8.8066e-06,  ..., -1.2890e-05,
         -1.0371e-05, -9.6560e-06]], device='cuda:0')
Loss: 0.941966712474823


Running epoch 1, step 1260, batch 212
Sampled inputs[:2]: tensor([[    0,  1607, 26394,  ...,    19,   471,    14],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5453e-05,  1.5227e-04, -3.8152e-04,  ...,  1.6308e-06,
         -7.0862e-05, -3.8340e-04],
        [-7.0110e-06, -5.1856e-06,  4.2692e-06,  ..., -6.4597e-06,
         -4.8578e-06, -5.3309e-06],
        [-1.8954e-05, -1.4514e-05,  1.2532e-05,  ..., -1.7226e-05,
         -1.3068e-05, -1.4052e-05],
        [-1.4096e-05, -1.0207e-05,  9.0003e-06,  ..., -1.2964e-05,
         -9.7156e-06, -1.1176e-05],
        [-1.7300e-05, -1.3560e-05,  1.1265e-05,  ..., -1.5929e-05,
         -1.2606e-05, -1.1966e-05]], device='cuda:0')
Loss: 0.960906445980072


Running epoch 1, step 1261, batch 213
Sampled inputs[:2]: tensor([[    0,    17,  3978,  ...,  3988,   598,    12],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5578e-04,  8.5982e-05, -5.7226e-04,  ...,  7.6587e-06,
         -2.3551e-04, -4.8604e-04],
        [-8.4192e-06, -6.2659e-06,  5.1819e-06,  ..., -7.7412e-06,
         -5.8115e-06, -6.3144e-06],
        [-2.2724e-05, -1.7479e-05,  1.5154e-05,  ..., -2.0623e-05,
         -1.5587e-05, -1.6645e-05],
        [-1.6943e-05, -1.2338e-05,  1.0923e-05,  ..., -1.5557e-05,
         -1.1608e-05, -1.3277e-05],
        [-2.0742e-05, -1.6347e-05,  1.3620e-05,  ..., -1.9073e-05,
         -1.5050e-05, -1.4186e-05]], device='cuda:0')
Loss: 0.9813442230224609


Running epoch 1, step 1262, batch 214
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,  4684, 14107,   259],
        [    0,   659,   278,  ...,  4032,  1109,   721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2349e-04,  7.2695e-05, -5.7226e-04,  ..., -1.3828e-05,
         -2.2356e-04, -4.2246e-04],
        [-9.8422e-06, -7.3463e-06,  6.1356e-06,  ..., -9.0301e-06,
         -6.7502e-06, -7.3127e-06],
        [-2.6688e-05, -2.0579e-05,  1.7956e-05,  ..., -2.4199e-05,
         -1.8179e-05, -1.9401e-05],
        [-1.9863e-05, -1.4469e-05,  1.2949e-05,  ..., -1.8179e-05,
         -1.3493e-05, -1.5438e-05],
        [-2.4199e-05, -1.9148e-05,  1.6019e-05,  ..., -2.2247e-05,
         -1.7464e-05, -1.6451e-05]], device='cuda:0')
Loss: 0.9897642731666565


Running epoch 1, step 1263, batch 215
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,  271, 4111, 1188],
        [   0, 9010,   17,  ..., 3813, 1147,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0766e-05,  1.3516e-04, -4.4902e-04,  ..., -1.0592e-04,
          6.8127e-05, -3.9298e-04],
        [-1.1310e-05, -8.5533e-06,  7.0408e-06,  ..., -1.0416e-05,
         -7.8380e-06, -8.3633e-06],
        [-3.0681e-05, -2.3976e-05,  2.0579e-05,  ..., -2.7969e-05,
         -2.1130e-05, -2.2262e-05],
        [-2.2814e-05, -1.6868e-05,  1.4842e-05,  ..., -2.0981e-05,
         -1.5669e-05, -1.7688e-05],
        [-2.7880e-05, -2.2382e-05,  1.8403e-05,  ..., -2.5779e-05,
         -2.0340e-05, -1.8954e-05]], device='cuda:0')
Loss: 1.0279048681259155
Graident accumulation at epoch 1, step 1263, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.3074e-05,  1.5364e-04, -1.0660e-04,  ..., -1.3504e-07,
          8.6127e-05,  5.5259e-05],
        [-9.9166e-06, -5.2611e-06,  4.8629e-06,  ..., -8.2472e-06,
         -5.2748e-06, -6.9048e-06],
        [-1.5070e-05, -5.8083e-06,  1.0838e-05,  ..., -1.0154e-05,
         -3.6922e-06, -1.3329e-05],
        [-9.5450e-06,  3.1889e-06,  5.5689e-06,  ..., -6.1556e-06,
          2.4580e-06, -7.2895e-06],
        [-1.0773e-05, -8.3117e-06,  1.1769e-05,  ..., -1.0371e-05,
         -4.5388e-06, -7.7136e-06]], device='cuda:0')
optimizer state dict: tensor([[5.3473e-08, 5.4744e-08, 6.4989e-08,  ..., 2.4586e-08, 1.5020e-07,
         3.7302e-08],
        [7.8124e-11, 5.1101e-11, 1.9077e-11,  ..., 5.4141e-11, 1.5804e-11,
         2.2210e-11],
        [3.4568e-09, 1.9991e-09, 1.0117e-09,  ..., 2.6602e-09, 8.5025e-10,
         9.0697e-10],
        [9.1605e-10, 7.9543e-10, 3.0943e-10,  ..., 7.8470e-10, 4.9575e-10,
         3.1826e-10],
        [3.9187e-10, 2.1754e-10, 7.1619e-11,  ..., 2.8420e-10, 7.7197e-11,
         1.0591e-10]], device='cuda:0')
optimizer state dict: 158.0
lr: [7.186664259670068e-06, 7.186664259670068e-06]
scheduler_last_epoch: 158


Running epoch 1, step 1264, batch 216
Sampled inputs[:2]: tensor([[   0,   12,  328,  ...,  578,   19,   40],
        [   0,  266, 3634,  ...,  694,  266, 1784]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5713e-05, -1.1012e-04,  7.5474e-05,  ..., -3.4223e-05,
          9.8902e-05, -1.7362e-05],
        [-1.4603e-06, -1.2070e-06,  9.1642e-07,  ..., -1.3262e-06,
         -1.0431e-06, -9.7603e-07],
        [-3.9935e-06, -3.4422e-06,  2.6524e-06,  ..., -3.6657e-06,
         -2.8610e-06, -2.6822e-06],
        [-2.9802e-06, -2.4289e-06,  1.9372e-06,  ..., -2.7120e-06,
         -2.1160e-06, -2.1160e-06],
        [-3.6061e-06, -3.1739e-06,  2.3544e-06,  ..., -3.3379e-06,
         -2.7269e-06, -2.2650e-06]], device='cuda:0')
Loss: 1.0168242454528809


Running epoch 1, step 1265, batch 217
Sampled inputs[:2]: tensor([[    0,   278,  5798,  ...,   266,   729,  1798],
        [    0, 17561,    12,  ...,   741,   496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1658e-05, -8.4965e-05,  1.7712e-05,  ..., -7.2118e-05,
          8.9866e-05, -3.3779e-05],
        [-2.8908e-06, -2.2724e-06,  1.9073e-06,  ..., -2.5779e-06,
         -1.8738e-06, -1.9372e-06],
        [-7.9572e-06, -6.5118e-06,  5.5581e-06,  ..., -7.1079e-06,
         -5.1707e-06, -5.3048e-06],
        [-5.8562e-06, -4.5300e-06,  4.0233e-06,  ..., -5.2154e-06,
         -3.7625e-06, -4.1574e-06],
        [-6.9290e-06, -5.8264e-06,  4.7684e-06,  ..., -6.2734e-06,
         -4.8280e-06, -4.2915e-06]], device='cuda:0')
Loss: 0.9519296884536743


Running epoch 1, step 1266, batch 218
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0,  1737,   278,  ...,  2604,   367,  2002]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1595e-05, -9.7785e-05,  1.3780e-04,  ..., -1.2567e-04,
          1.3794e-04, -3.0787e-05],
        [-4.2990e-06, -3.4645e-06,  2.8349e-06,  ..., -3.8520e-06,
         -2.8796e-06, -2.9132e-06],
        [-1.1921e-05, -9.9689e-06,  8.3297e-06,  ..., -1.0699e-05,
         -7.9870e-06, -8.0764e-06],
        [-8.7768e-06, -6.9439e-06,  6.0350e-06,  ..., -7.8380e-06,
         -5.8189e-06, -6.3181e-06],
        [-1.0371e-05, -8.9109e-06,  7.1228e-06,  ..., -9.4473e-06,
         -7.4208e-06, -6.5416e-06]], device='cuda:0')
Loss: 0.9915350079536438


Running epoch 1, step 1267, batch 219
Sampled inputs[:2]: tensor([[   0,  259, 2561,  ...,   77, 4830,  292],
        [   0, 1217,    9,  ..., 1821,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4358e-05, -6.3366e-05,  3.1553e-05,  ..., -3.4550e-04,
          2.6939e-04, -2.2012e-04],
        [-5.6699e-06, -4.4852e-06,  3.6694e-06,  ..., -5.1931e-06,
         -3.8482e-06, -3.9116e-06],
        [-1.5557e-05, -1.2755e-05,  1.0744e-05,  ..., -1.4171e-05,
         -1.0490e-05, -1.0654e-05],
        [-1.1548e-05, -8.9407e-06,  7.7859e-06,  ..., -1.0535e-05,
         -7.7412e-06, -8.4341e-06],
        [-1.3679e-05, -1.1533e-05,  9.2685e-06,  ..., -1.2651e-05,
         -9.8348e-06, -8.7023e-06]], device='cuda:0')
Loss: 0.9842879176139832


Running epoch 1, step 1268, batch 220
Sampled inputs[:2]: tensor([[    0, 29883,   680,  ...,  3363,  1049,   292],
        [    0,    15,    19,  ...,   266,  6391,  1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1085e-04, -7.4459e-05, -3.7305e-05,  ..., -4.1872e-04,
          2.8950e-04, -2.4768e-04],
        [-7.0855e-06, -5.6699e-06,  4.6454e-06,  ..., -6.4671e-06,
         -4.8317e-06, -4.7982e-06],
        [-1.9461e-05, -1.6108e-05,  1.3590e-05,  ..., -1.7703e-05,
         -1.3232e-05, -1.3128e-05],
        [-1.4454e-05, -1.1310e-05,  9.8720e-06,  ..., -1.3143e-05,
         -9.7081e-06, -1.0356e-05],
        [-1.7062e-05, -1.4529e-05,  1.1683e-05,  ..., -1.5751e-05,
         -1.2353e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9961992502212524


Running epoch 1, step 1269, batch 221
Sampled inputs[:2]: tensor([[    0,   935,  2613,  ...,   623,  4289,  6803],
        [    0,   474,   706,  ...,    83, 38084,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6943e-05, -8.1489e-05,  4.3328e-06,  ..., -4.8741e-04,
          2.3351e-04, -2.6553e-04],
        [-8.5831e-06, -6.5528e-06,  5.4128e-06,  ..., -7.8306e-06,
         -5.8524e-06, -6.0275e-06],
        [-2.3216e-05, -1.8477e-05,  1.5765e-05,  ..., -2.1070e-05,
         -1.5795e-05, -1.6078e-05],
        [-1.7330e-05, -1.2971e-05,  1.1437e-05,  ..., -1.5780e-05,
         -1.1675e-05, -1.2785e-05],
        [-2.0802e-05, -1.6883e-05,  1.3784e-05,  ..., -1.9133e-05,
         -1.5005e-05, -1.3486e-05]], device='cuda:0')
Loss: 0.935045063495636


Running epoch 1, step 1270, batch 222
Sampled inputs[:2]: tensor([[   0, 4014,   88,  ..., 1103,   14, 1771],
        [   0,  475,  266,  ...,  843,  287, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3334e-05, -6.4751e-05, -1.9092e-04,  ..., -4.6610e-04,
          2.2132e-04, -1.6437e-04],
        [-1.0006e-05, -7.7151e-06,  6.3144e-06,  ..., -9.1195e-06,
         -6.8508e-06, -7.0333e-06],
        [ 4.5774e-05,  7.8315e-05, -1.1149e-05,  ...,  3.7728e-05,
          8.3115e-05,  3.4196e-05],
        [-2.0325e-05, -1.5385e-05,  1.3389e-05,  ..., -1.8477e-05,
         -1.3761e-05, -1.5005e-05],
        [-2.4334e-05, -1.9908e-05,  1.6138e-05,  ..., -2.2352e-05,
         -1.7613e-05, -1.5795e-05]], device='cuda:0')
Loss: 1.0034785270690918


Running epoch 1, step 1271, batch 223
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,  516, 1424,  ..., 3473,  278, 2442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4496e-05, -2.5054e-04, -5.6296e-04,  ..., -5.3545e-04,
          8.8326e-05, -3.2990e-04],
        [-1.1399e-05, -8.7731e-06,  7.3127e-06,  ..., -1.0334e-05,
         -7.7337e-06, -7.9609e-06],
        [ 4.1870e-05,  7.5335e-05, -8.1985e-06,  ...,  3.4375e-05,
          8.0716e-05,  3.1603e-05],
        [-2.3261e-05, -1.7531e-05,  1.5594e-05,  ..., -2.0996e-05,
         -1.5542e-05, -1.7047e-05],
        [-2.7686e-05, -2.2560e-05,  1.8626e-05,  ..., -2.5287e-05,
         -1.9863e-05, -1.7852e-05]], device='cuda:0')
Loss: 0.9709042906761169
Graident accumulation at epoch 1, step 1271, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.7317e-05,  1.1322e-04, -1.5223e-04,  ..., -5.3667e-05,
          8.6347e-05,  1.6743e-05],
        [-1.0065e-05, -5.6123e-06,  5.1078e-06,  ..., -8.4559e-06,
         -5.5207e-06, -7.0104e-06],
        [-9.3759e-06,  2.3060e-06,  8.9344e-06,  ..., -5.7007e-06,
          4.7486e-06, -8.8362e-06],
        [-1.0917e-05,  1.1169e-06,  6.5715e-06,  ..., -7.6396e-06,
          6.5804e-07, -8.2653e-06],
        [-1.2464e-05, -9.7365e-06,  1.2454e-05,  ..., -1.1863e-05,
         -6.0712e-06, -8.7274e-06]], device='cuda:0')
optimizer state dict: tensor([[5.3426e-08, 5.4752e-08, 6.5241e-08,  ..., 2.4848e-08, 1.5006e-07,
         3.7373e-08],
        [7.8176e-11, 5.1127e-11, 1.9111e-11,  ..., 5.4194e-11, 1.5848e-11,
         2.2251e-11],
        [3.4551e-09, 2.0028e-09, 1.0107e-09,  ..., 2.6587e-09, 8.5592e-10,
         9.0706e-10],
        [9.1568e-10, 7.9495e-10, 3.0937e-10,  ..., 7.8436e-10, 4.9550e-10,
         3.1824e-10],
        [3.9224e-10, 2.1783e-10, 7.1894e-11,  ..., 2.8455e-10, 7.7515e-11,
         1.0612e-10]], device='cuda:0')
optimizer state dict: 159.0
lr: [7.068258487638268e-06, 7.068258487638268e-06]
scheduler_last_epoch: 159


Running epoch 1, step 1272, batch 224
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0,   382,    17,  ...,  8733,    13,  9306]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7331e-06, -1.2962e-04, -1.4642e-04,  ..., -1.5910e-05,
         -1.0519e-04, -1.4174e-04],
        [-1.4007e-06, -1.1027e-06,  9.1270e-07,  ..., -1.2740e-06,
         -1.0058e-06, -9.9838e-07],
        [-3.9041e-06, -3.2037e-06,  2.6822e-06,  ..., -3.5614e-06,
         -2.8163e-06, -2.7865e-06],
        [-2.9057e-06, -2.2352e-06,  1.9670e-06,  ..., -2.6226e-06,
         -2.0564e-06, -2.1905e-06],
        [-3.3975e-06, -2.8759e-06,  2.3097e-06,  ..., -3.1590e-06,
         -2.6077e-06, -2.2650e-06]], device='cuda:0')
Loss: 1.001444935798645


Running epoch 1, step 1273, batch 225
Sampled inputs[:2]: tensor([[    0,  6702, 18279,  ...,    14, 47571,    12],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5967e-05, -4.1683e-05, -9.3195e-05,  ...,  1.1079e-06,
         -4.7460e-05, -1.4375e-04],
        [-2.8461e-06, -2.0564e-06,  1.5683e-06,  ..., -2.5928e-06,
         -1.9893e-06, -2.2426e-06],
        [-7.7188e-06, -5.7966e-06,  4.6790e-06,  ..., -6.9588e-06,
         -5.4091e-06, -5.9456e-06],
        [-5.8115e-06, -4.0978e-06,  3.3453e-06,  ..., -5.2601e-06,
         -4.0531e-06, -4.7535e-06],
        [-7.0184e-06, -5.3644e-06,  4.1798e-06,  ..., -6.4224e-06,
         -5.2005e-06, -5.0366e-06]], device='cuda:0')
Loss: 0.91594398021698


Running epoch 1, step 1274, batch 226
Sampled inputs[:2]: tensor([[    0,  1529,   354,  ...,   709,   271,   266],
        [    0, 15003, 19278,  ...,   287,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2754e-04, -8.6063e-06, -1.2080e-04,  ..., -1.0840e-04,
         -1.2288e-04, -1.5294e-04],
        [-4.2692e-06, -3.1888e-06,  2.3954e-06,  ..., -3.9265e-06,
         -3.0249e-06, -3.3230e-06],
        [-1.1653e-05, -9.0152e-06,  7.1377e-06,  ..., -1.0595e-05,
         -8.2254e-06, -8.8811e-06],
        [-8.6427e-06, -6.3032e-06,  5.0589e-06,  ..., -7.9125e-06,
         -6.0946e-06, -7.0184e-06],
        [-1.0610e-05, -8.3894e-06,  6.4000e-06,  ..., -9.8050e-06,
         -7.9125e-06, -7.5698e-06]], device='cuda:0')
Loss: 0.9645078182220459


Running epoch 1, step 1275, batch 227
Sampled inputs[:2]: tensor([[    0, 38460,     9,  ...,   829,   870,    12],
        [    0,  2771,    13,  ...,  1412,    35,    15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3424e-04, -5.9862e-05, -2.5055e-04,  ..., -9.0195e-05,
         -1.3809e-05, -1.2128e-05],
        [-5.6773e-06, -4.3362e-06,  3.2149e-06,  ..., -5.2378e-06,
         -4.0680e-06, -4.3139e-06],
        [-1.5497e-05, -1.2234e-05,  9.5665e-06,  ..., -1.4156e-05,
         -1.1042e-05, -1.1563e-05],
        [-1.1548e-05, -8.6278e-06,  6.8098e-06,  ..., -1.0610e-05,
         -8.2254e-06, -9.1791e-06],
        [-1.4022e-05, -1.1310e-05,  8.5309e-06,  ..., -1.2994e-05,
         -1.0535e-05, -9.7752e-06]], device='cuda:0')
Loss: 0.9943464398384094


Running epoch 1, step 1276, batch 228
Sampled inputs[:2]: tensor([[   0,  292,  685,  ...,  278, 3281,  298],
        [   0,  493,  221,  ...,  259,  726, 2786]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2727e-04,  2.9105e-06, -3.0830e-04,  ..., -2.2272e-04,
         -1.3883e-04, -4.4433e-05],
        [-7.1004e-06, -5.3048e-06,  4.2208e-06,  ..., -6.5044e-06,
         -4.9435e-06, -5.3421e-06],
        [-1.9491e-05, -1.5065e-05,  1.2547e-05,  ..., -1.7673e-05,
         -1.3471e-05, -1.4380e-05],
        [-1.4573e-05, -1.0625e-05,  9.0450e-06,  ..., -1.3277e-05,
         -1.0036e-05, -1.1459e-05],
        [-1.7464e-05, -1.3828e-05,  1.1064e-05,  ..., -1.6063e-05,
         -1.2770e-05, -1.2040e-05]], device='cuda:0')
Loss: 0.9704895615577698


Running epoch 1, step 1277, batch 229
Sampled inputs[:2]: tensor([[    0,   221,  1771,  ..., 14547,  1705,  1003],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4747e-04, -4.8358e-05, -2.5131e-04,  ..., -1.3184e-04,
          3.4408e-05,  1.4124e-04],
        [-8.5533e-06, -6.4671e-06,  5.0291e-06,  ..., -7.8306e-06,
         -6.0089e-06, -6.4522e-06],
        [-2.3454e-05, -1.8388e-05,  1.4916e-05,  ..., -2.1294e-05,
         -1.6421e-05, -1.7405e-05],
        [-1.7494e-05, -1.2934e-05,  1.0714e-05,  ..., -1.5944e-05,
         -1.2182e-05, -1.3798e-05],
        [-2.1070e-05, -1.6898e-05,  1.3210e-05,  ..., -1.9372e-05,
         -1.5557e-05, -1.4618e-05]], device='cuda:0')
Loss: 0.9866499304771423


Running epoch 1, step 1278, batch 230
Sampled inputs[:2]: tensor([[   0, 1356,  634,  ..., 6604,  634,   14],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9240e-04, -1.3384e-04, -3.0699e-04,  ..., -1.3184e-04,
          3.4408e-05,  7.0091e-05],
        [-9.8944e-06, -7.5474e-06,  5.9195e-06,  ..., -9.0525e-06,
         -6.9290e-06, -7.4133e-06],
        [-2.7165e-05, -2.1428e-05,  1.7539e-05,  ..., -2.4617e-05,
         -1.8924e-05, -2.0012e-05],
        [-2.0295e-05, -1.5110e-05,  1.2666e-05,  ..., -1.8463e-05,
         -1.4067e-05, -1.5914e-05],
        [-2.4304e-05, -1.9640e-05,  1.5445e-05,  ..., -2.2322e-05,
         -1.7866e-05, -1.6719e-05]], device='cuda:0')
Loss: 0.9409030079841614


Running epoch 1, step 1279, batch 231
Sampled inputs[:2]: tensor([[   0, 4120,  278,  ...,  298,  273,  221],
        [   0,  380,  981,  ...,  567, 5407,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5418e-04, -8.4091e-05, -2.4729e-04,  ..., -1.8576e-04,
          2.6618e-05,  9.4339e-05],
        [-1.1258e-05, -8.6650e-06,  6.7875e-06,  ..., -1.0274e-05,
         -7.8455e-06, -8.3819e-06],
        [-3.0950e-05, -2.4572e-05,  2.0117e-05,  ..., -2.7969e-05,
         -2.1413e-05, -2.2665e-05],
        [-2.3142e-05, -1.7390e-05,  1.4558e-05,  ..., -2.1011e-05,
         -1.5944e-05, -1.8045e-05],
        [-2.7567e-05, -2.2426e-05,  1.7636e-05,  ..., -2.5257e-05,
         -2.0146e-05, -1.8835e-05]], device='cuda:0')
Loss: 0.956051230430603
Graident accumulation at epoch 1, step 1279, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6168e-05,  9.3489e-05, -1.6174e-04,  ..., -6.6877e-05,
          8.0374e-05,  2.4503e-05],
        [-1.0184e-05, -5.9176e-06,  5.2758e-06,  ..., -8.6377e-06,
         -5.7532e-06, -7.1476e-06],
        [-1.1533e-05, -3.8177e-07,  1.0053e-05,  ..., -7.9276e-06,
          2.1324e-06, -1.0219e-05],
        [-1.2139e-05, -7.3373e-07,  7.3702e-06,  ..., -8.9767e-06,
         -1.0022e-06, -9.2433e-06],
        [-1.3974e-05, -1.1006e-05,  1.2973e-05,  ..., -1.3202e-05,
         -7.4787e-06, -9.7382e-06]], device='cuda:0')
optimizer state dict: tensor([[5.3397e-08, 5.4704e-08, 6.5237e-08,  ..., 2.4858e-08, 1.4991e-07,
         3.7345e-08],
        [7.8224e-11, 5.1151e-11, 1.9138e-11,  ..., 5.4245e-11, 1.5893e-11,
         2.2299e-11],
        [3.4526e-09, 2.0014e-09, 1.0101e-09,  ..., 2.6568e-09, 8.5552e-10,
         9.0667e-10],
        [9.1530e-10, 7.9445e-10, 3.0927e-10,  ..., 7.8402e-10, 4.9526e-10,
         3.1824e-10],
        [3.9261e-10, 2.1812e-10, 7.2133e-11,  ..., 2.8491e-10, 7.7843e-11,
         1.0637e-10]], device='cuda:0')
optimizer state dict: 160.0
lr: [6.950300711301095e-06, 6.950300711301095e-06]
scheduler_last_epoch: 160


Running epoch 1, step 1280, batch 232
Sampled inputs[:2]: tensor([[    0, 49570,   644,  ...,   461,   800,   266],
        [    0, 11661,    12,  ...,  1707,   394,   264]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8460e-05,  1.2195e-05, -9.5336e-06,  ...,  1.5463e-04,
         -9.6498e-05, -5.3825e-06],
        [-1.3560e-06, -1.0878e-06,  9.9838e-07,  ..., -1.1921e-06,
         -8.6799e-07, -9.1270e-07],
        [-3.8445e-06, -3.1739e-06,  2.9504e-06,  ..., -3.3677e-06,
         -2.4587e-06, -2.5630e-06],
        [-2.9057e-06, -2.2650e-06,  2.2203e-06,  ..., -2.5332e-06,
         -1.8254e-06, -2.0564e-06],
        [-3.1590e-06, -2.6971e-06,  2.3842e-06,  ..., -2.8163e-06,
         -2.1756e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.9667320847511292


Running epoch 1, step 1281, batch 233
Sampled inputs[:2]: tensor([[    0, 13751,    12,  ...,  1264,  5676,   367],
        [    0,   726,  3979,  ...,    27,  2085,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2653e-04, -1.6310e-05, -1.4929e-04,  ...,  1.6732e-04,
         -1.0869e-04,  1.5133e-04],
        [-2.7567e-06, -2.1309e-06,  1.9409e-06,  ..., -2.4512e-06,
         -1.7732e-06, -1.9632e-06],
        [-7.6592e-06, -6.0946e-06,  5.6922e-06,  ..., -6.7502e-06,
         -4.8727e-06, -5.3942e-06],
        [-5.8264e-06, -4.3511e-06,  4.2915e-06,  ..., -5.1260e-06,
         -3.6582e-06, -4.3511e-06],
        [-6.4820e-06, -5.3495e-06,  4.7088e-06,  ..., -5.8115e-06,
         -4.4554e-06, -4.2468e-06]], device='cuda:0')
Loss: 0.9565209150314331


Running epoch 1, step 1282, batch 234
Sampled inputs[:2]: tensor([[    0,  6904,  6069,  ..., 17196,   471,   221],
        [    0,  1500,   367,  ...,   344,  4250,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6063e-05, -3.7389e-05, -1.1760e-04,  ...,  1.2822e-04,
         -1.6430e-04,  6.2981e-05],
        [-4.1574e-06, -3.2037e-06,  2.7940e-06,  ..., -3.7476e-06,
         -2.8089e-06, -2.9765e-06],
        [-1.1474e-05, -9.1046e-06,  8.1956e-06,  ..., -1.0252e-05,
         -7.6890e-06, -8.1211e-06],
        [-8.6874e-06, -6.4820e-06,  6.1169e-06,  ..., -7.7486e-06,
         -5.7593e-06, -6.5118e-06],
        [-9.8497e-06, -8.1062e-06,  6.8992e-06,  ..., -8.9407e-06,
         -7.0930e-06, -6.4820e-06]], device='cuda:0')
Loss: 0.9724579453468323


Running epoch 1, step 1283, batch 235
Sampled inputs[:2]: tensor([[    0,  2336,    26,  ...,  2564,   271,  1422],
        [    0,  4653, 21419,  ...,  7845,   300,   565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9804e-04,  3.4891e-05, -1.3674e-04,  ...,  6.7298e-05,
         -2.4608e-04,  4.4721e-05],
        [-5.6624e-06, -4.3362e-06,  3.6731e-06,  ..., -5.1185e-06,
         -3.8594e-06, -4.1015e-06],
        [-1.5587e-05, -1.2368e-05,  1.0744e-05,  ..., -1.4007e-05,
         -1.0595e-05, -1.1191e-05],
        [ 2.8617e-04,  3.7212e-04, -1.7153e-04,  ...,  2.1788e-04,
          2.7911e-04,  1.5866e-04],
        [-1.3649e-05, -1.1206e-05,  9.2387e-06,  ..., -1.2457e-05,
         -9.9242e-06, -9.1642e-06]], device='cuda:0')
Loss: 0.9679006338119507


Running epoch 1, step 1284, batch 236
Sampled inputs[:2]: tensor([[   0,  471,   12,  ...,   13, 9909, 2673],
        [   0,   13, 6913,  ...,  278, 1317, 4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8804e-04,  5.8993e-05,  5.6110e-06,  ...,  6.7022e-05,
         -2.9186e-04,  5.9059e-05],
        [-6.9961e-06, -5.4166e-06,  4.5411e-06,  ..., -6.3702e-06,
         -4.7944e-06, -5.0254e-06],
        [-1.9312e-05, -1.5467e-05,  1.3337e-05,  ..., -1.7464e-05,
         -1.3202e-05, -1.3724e-05],
        [ 2.8337e-04,  3.6990e-04, -1.6963e-04,  ...,  2.1526e-04,
          2.7715e-04,  1.5662e-04],
        [-1.6853e-05, -1.3977e-05,  1.1444e-05,  ..., -1.5482e-05,
         -1.2323e-05, -1.1191e-05]], device='cuda:0')
Loss: 0.997148334980011


Running epoch 1, step 1285, batch 237
Sampled inputs[:2]: tensor([[   0, 1503, 1785,  ...,  221,  380, 1869],
        [   0,  271, 5738,  ...,   12,   21, 9023]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5394e-04, -1.1540e-04, -3.5636e-05,  ...,  7.3045e-05,
         -3.1378e-04, -3.0490e-05],
        [-8.4341e-06, -6.5640e-06,  5.4874e-06,  ..., -7.6219e-06,
         -5.7295e-06, -5.9567e-06],
        [-2.3305e-05, -1.8761e-05,  1.6153e-05,  ..., -2.0936e-05,
         -1.5825e-05, -1.6287e-05],
        [ 2.8036e-04,  3.6754e-04, -1.6755e-04,  ...,  2.1265e-04,
          2.7522e-04,  1.5458e-04],
        [-2.0280e-05, -1.6868e-05,  1.3828e-05,  ..., -1.8507e-05,
         -1.4722e-05, -1.3232e-05]], device='cuda:0')
Loss: 0.9918866753578186


Running epoch 1, step 1286, batch 238
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,    12,  3518,  ...,  1580,  2573,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1371e-05,  2.0274e-06, -3.6741e-05,  ...,  1.4031e-05,
         -3.5612e-04,  1.4901e-04],
        [-9.9614e-06, -7.4394e-06,  6.2771e-06,  ..., -8.9630e-06,
         -6.6981e-06, -7.2978e-06],
        [-2.7478e-05, -2.1353e-05,  1.8507e-05,  ..., -2.4557e-05,
         -1.8507e-05, -1.9878e-05],
        [ 2.7723e-04,  3.6578e-04, -1.6586e-04,  ...,  2.0989e-04,
          2.7322e-04,  1.5174e-04],
        [-2.4125e-05, -1.9282e-05,  1.5944e-05,  ..., -2.1890e-05,
         -1.7300e-05, -1.6361e-05]], device='cuda:0')
Loss: 0.9225906133651733


Running epoch 1, step 1287, batch 239
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0343e-05,  2.0274e-06, -2.2902e-05,  ..., -1.8516e-06,
         -3.0117e-04, -1.5072e-05],
        [-1.1303e-05, -8.5197e-06,  7.1451e-06,  ..., -1.0245e-05,
         -7.7486e-06, -8.2888e-06],
        [-3.1278e-05, -2.4498e-05,  2.1130e-05,  ..., -2.8178e-05,
         -2.1458e-05, -2.2650e-05],
        [ 2.7446e-04,  3.6361e-04, -1.6399e-04,  ...,  2.0724e-04,
          2.7104e-04,  1.4958e-04],
        [-2.7478e-05, -2.2143e-05,  1.8194e-05,  ..., -2.5108e-05,
         -2.0027e-05, -1.8626e-05]], device='cuda:0')
Loss: 1.0063457489013672
Graident accumulation at epoch 1, step 1287, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.5517e-05,  8.4343e-05, -1.4786e-04,  ..., -6.0374e-05,
          4.2219e-05,  2.0545e-05],
        [-1.0296e-05, -6.1778e-06,  5.4627e-06,  ..., -8.7984e-06,
         -5.9527e-06, -7.2617e-06],
        [-1.3508e-05, -2.7933e-06,  1.1160e-05,  ..., -9.9526e-06,
         -2.2656e-07, -1.1462e-05],
        [ 1.6521e-05,  3.5700e-05, -9.7662e-06,  ...,  1.2645e-05,
          2.6202e-05,  6.6395e-06],
        [-1.5325e-05, -1.2119e-05,  1.3495e-05,  ..., -1.4393e-05,
         -8.7336e-06, -1.0627e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3348e-08, 5.4649e-08, 6.5172e-08,  ..., 2.4833e-08, 1.4985e-07,
         3.7308e-08],
        [7.8274e-11, 5.1172e-11, 1.9170e-11,  ..., 5.4296e-11, 1.5938e-11,
         2.2345e-11],
        [3.4502e-09, 2.0000e-09, 1.0096e-09,  ..., 2.6549e-09, 8.5512e-10,
         9.0627e-10],
        [9.8971e-10, 9.2587e-10, 3.3585e-10,  ..., 8.2618e-10, 5.6823e-10,
         3.4030e-10],
        [3.9297e-10, 2.1839e-10, 7.2392e-11,  ..., 2.8525e-10, 7.8166e-11,
         1.0661e-10]], device='cuda:0')
optimizer state dict: 161.0
lr: [6.8328089556364305e-06, 6.8328089556364305e-06]
scheduler_last_epoch: 161


Running epoch 1, step 1288, batch 240
Sampled inputs[:2]: tensor([[    0,  2992,   352,  ...,   259,  2063,  6088],
        [    0,  4889,  3593,  ..., 19787,   287, 22475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4003e-04, -1.2636e-04, -6.5043e-05,  ...,  5.6145e-05,
         -1.4006e-04, -1.0065e-04],
        [-1.3337e-06, -1.0207e-06,  9.6112e-07,  ..., -1.1772e-06,
         -8.4937e-07, -9.3877e-07],
        [-3.7551e-06, -2.9355e-06,  2.8908e-06,  ..., -3.2634e-06,
         -2.3544e-06, -2.5779e-06],
        [-2.8908e-06, -2.1160e-06,  2.1905e-06,  ..., -2.5332e-06,
         -1.7956e-06, -2.1309e-06],
        [-3.1292e-06, -2.5332e-06,  2.3842e-06,  ..., -2.7567e-06,
         -2.1160e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.9728183150291443


Running epoch 1, step 1289, batch 241
Sampled inputs[:2]: tensor([[    0,  1853,  3373,  ...,  3020,  6695,   300],
        [    0,  7527,    15,  ...,  2677,   292, 30654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5147e-04, -1.7837e-04, -3.2855e-04,  ..., -2.8257e-05,
         -1.4717e-04, -2.2520e-04],
        [-2.6450e-06, -2.0564e-06,  1.9521e-06,  ..., -2.3693e-06,
         -1.6913e-06, -1.8738e-06],
        [-7.5996e-06, -6.0350e-06,  5.9605e-06,  ..., -6.7055e-06,
         -4.7982e-06, -5.2601e-06],
        [-5.8115e-06, -4.3362e-06,  4.5151e-06,  ..., -5.1409e-06,
         -3.6284e-06, -4.3064e-06],
        [-6.1989e-06, -5.0962e-06,  4.7833e-06,  ..., -5.5432e-06,
         -4.2170e-06, -3.9488e-06]], device='cuda:0')
Loss: 0.9457253813743591


Running epoch 1, step 1290, batch 242
Sampled inputs[:2]: tensor([[    0,   965,   300,  ...,   546,   857,  4350],
        [    0,    12, 17340,  ...,   408,  1550,  2415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2691e-04, -1.1770e-04, -2.6590e-04,  ..., -1.4386e-04,
          1.8645e-05, -1.6214e-04],
        [-4.0010e-06, -3.1739e-06,  2.8796e-06,  ..., -3.6210e-06,
         -2.6524e-06, -2.8573e-06],
        [-1.1533e-05, -9.4026e-06,  8.8066e-06,  ..., -1.0356e-05,
         -7.5996e-06, -8.1062e-06],
        [-8.7321e-06, -6.7204e-06,  6.6161e-06,  ..., -7.8529e-06,
         -5.6848e-06, -6.5267e-06],
        [-9.5218e-06, -8.0466e-06,  7.1526e-06,  ..., -8.6874e-06,
         -6.7502e-06, -6.1989e-06]], device='cuda:0')
Loss: 0.983237087726593


Running epoch 1, step 1291, batch 243
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  3359,  1751,  5048],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6324e-05, -1.3572e-04, -3.6407e-04,  ..., -1.6381e-04,
          4.7929e-05, -2.2831e-04],
        [-5.4613e-06, -4.2319e-06,  3.8408e-06,  ..., -4.8354e-06,
         -3.6024e-06, -3.9227e-06],
        [-1.5616e-05, -1.2487e-05,  1.1668e-05,  ..., -1.3754e-05,
         -1.0267e-05, -1.1086e-05],
        [-1.1891e-05, -8.9556e-06,  8.8066e-06,  ..., -1.0476e-05,
         -7.7114e-06, -8.9258e-06],
        [-1.3009e-05, -1.0774e-05,  9.5516e-06,  ..., -1.1653e-05,
         -9.1940e-06, -8.5831e-06]], device='cuda:0')
Loss: 0.9552505612373352


Running epoch 1, step 1292, batch 244
Sampled inputs[:2]: tensor([[   0,   12,  344,  ...,   14, 2295,  516],
        [   0,  287, 6015,  ...,   14,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2292e-06, -1.2957e-04, -3.2355e-04,  ..., -7.3703e-05,
          2.3045e-04, -2.0301e-04],
        [-6.8247e-06, -5.3123e-06,  4.7572e-06,  ..., -6.1169e-06,
         -4.5411e-06, -4.8764e-06],
        [-1.9461e-05, -1.5691e-05,  1.4439e-05,  ..., -1.7405e-05,
         -1.2934e-05, -1.3769e-05],
        [-1.4842e-05, -1.1265e-05,  1.0893e-05,  ..., -1.3262e-05,
         -9.7230e-06, -1.1101e-05],
        [-1.6257e-05, -1.3545e-05,  1.1846e-05,  ..., -1.4767e-05,
         -1.1563e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9954033493995667


Running epoch 1, step 1293, batch 245
Sampled inputs[:2]: tensor([[    0,    14,   496,  ...,   368,   259,   490],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4935e-05, -1.6433e-04, -1.8253e-04,  ..., -1.6617e-04,
          4.1864e-04, -3.1948e-04],
        [-8.2180e-06, -6.4373e-06,  5.5805e-06,  ..., -7.4357e-06,
         -5.5768e-06, -5.9567e-06],
        [-2.3454e-05, -1.9029e-05,  1.6972e-05,  ..., -2.1145e-05,
         -1.5900e-05, -1.6838e-05],
        [-1.7807e-05, -1.3590e-05,  1.2703e-05,  ..., -1.6063e-05,
         -1.1913e-05, -1.3515e-05],
        [-1.9833e-05, -1.6585e-05,  1.4082e-05,  ..., -1.8150e-05,
         -1.4335e-05, -1.3262e-05]], device='cuda:0')
Loss: 0.9976293444633484


Running epoch 1, step 1294, batch 246
Sampled inputs[:2]: tensor([[    0,   344,  8260,  ..., 16020, 18216, 11348],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9184e-05, -2.3456e-04, -2.4966e-04,  ..., -2.9876e-04,
          5.4226e-04, -3.8482e-04],
        [-9.5889e-06, -7.4506e-06,  6.5491e-06,  ..., -8.6501e-06,
         -6.4336e-06, -6.9402e-06],
        [-2.7254e-05, -2.1949e-05,  1.9848e-05,  ..., -2.4468e-05,
         -1.8284e-05, -1.9550e-05],
        [-2.0757e-05, -1.5721e-05,  1.4909e-05,  ..., -1.8641e-05,
         -1.3724e-05, -1.5736e-05],
        [ 3.3713e-05,  6.7815e-05, -4.8771e-05,  ...,  4.8924e-05,
          4.4173e-05, -2.3754e-05]], device='cuda:0')
Loss: 0.9954907894134521


Running epoch 1, step 1295, batch 247
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,   729,  3430,  ...,  9715,    13, 42383]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0343e-05, -1.8456e-04, -3.0337e-04,  ..., -1.5452e-04,
          8.6127e-04, -2.4611e-04],
        [-1.0975e-05, -8.4639e-06,  7.4320e-06,  ..., -9.9167e-06,
         -7.3537e-06, -8.0578e-06],
        [-3.1218e-05, -2.4930e-05,  2.2560e-05,  ..., -2.8029e-05,
         -2.0877e-05, -2.2665e-05],
        [-2.3723e-05, -1.7807e-05,  1.6890e-05,  ..., -2.1324e-05,
         -1.5646e-05, -1.8224e-05],
        [ 3.0256e-05,  6.5148e-05, -4.6446e-05,  ...,  4.5794e-05,
          4.1774e-05, -2.6287e-05]], device='cuda:0')
Loss: 0.9679985046386719
Graident accumulation at epoch 1, step 1295, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1000e-05,  5.7453e-05, -1.6341e-04,  ..., -6.9789e-05,
          1.2412e-04, -6.1198e-06],
        [-1.0364e-05, -6.4064e-06,  5.6597e-06,  ..., -8.9103e-06,
         -6.0928e-06, -7.3413e-06],
        [-1.5279e-05, -5.0070e-06,  1.2300e-05,  ..., -1.1760e-05,
         -2.2916e-06, -1.2582e-05],
        [ 1.2496e-05,  3.0350e-05, -7.1005e-06,  ...,  9.2482e-06,
          2.2018e-05,  4.1531e-06],
        [-1.0767e-05, -4.3926e-06,  7.5006e-06,  ..., -8.3742e-06,
         -3.6828e-06, -1.2193e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3301e-08, 5.4629e-08, 6.5199e-08,  ..., 2.4832e-08, 1.5044e-07,
         3.7331e-08],
        [7.8316e-11, 5.1193e-11, 1.9206e-11,  ..., 5.4340e-11, 1.5976e-11,
         2.2388e-11],
        [3.4477e-09, 1.9986e-09, 1.0091e-09,  ..., 2.6531e-09, 8.5470e-10,
         9.0588e-10],
        [9.8928e-10, 9.2526e-10, 3.3580e-10,  ..., 8.2581e-10, 5.6790e-10,
         3.4029e-10],
        [3.9350e-10, 2.2241e-10, 7.4477e-11,  ..., 2.8706e-10, 7.9833e-11,
         1.0720e-10]], device='cuda:0')
optimizer state dict: 162.0
lr: [6.715801174410152e-06, 6.715801174410152e-06]
scheduler_last_epoch: 162


Running epoch 1, step 1296, batch 248
Sampled inputs[:2]: tensor([[    0,   266,  6079,  ...,   437,   266, 44526],
        [    0,   259,  2180,  ...,   638,  1615,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3077e-05, -8.8627e-05, -1.5303e-04,  ..., -5.0270e-05,
         -3.6761e-06, -1.0664e-04],
        [-1.4156e-06, -1.0431e-06,  9.8348e-07,  ..., -1.2890e-06,
         -9.5367e-07, -9.8348e-07],
        [-3.8743e-06, -2.9355e-06,  2.8610e-06,  ..., -3.5018e-06,
         -2.5928e-06, -2.6673e-06],
        [-2.9802e-06, -2.1309e-06,  2.1756e-06,  ..., -2.6971e-06,
         -1.9670e-06, -2.1756e-06],
        [-3.3379e-06, -2.6077e-06,  2.4140e-06,  ..., -3.0398e-06,
         -2.3693e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9838040471076965


Running epoch 1, step 1297, batch 249
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  367, 3675,  ...,   22, 3180,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2196e-05, -8.8587e-05, -1.0042e-04,  ...,  4.6933e-05,
         -1.9766e-05, -1.7254e-04],
        [-2.7940e-06, -2.1309e-06,  2.0117e-06,  ..., -2.5034e-06,
         -1.8664e-06, -1.8589e-06],
        [-7.8082e-06, -6.1840e-06,  5.9456e-06,  ..., -6.9737e-06,
         -5.2005e-06, -5.1409e-06],
        [-6.0648e-06, -4.5151e-06,  4.5747e-06,  ..., -5.4091e-06,
         -3.9935e-06, -4.2468e-06],
        [-6.6012e-06, -5.4091e-06,  4.9323e-06,  ..., -5.9754e-06,
         -4.6641e-06, -4.0233e-06]], device='cuda:0')
Loss: 1.0102449655532837


Running epoch 1, step 1298, batch 250
Sampled inputs[:2]: tensor([[    0, 17734,    12,  ...,   278,  2421,   940],
        [    0, 10511,  3887,  ...,  3504,   298,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2532e-04, -1.3091e-04, -1.3912e-04,  ...,  1.3444e-04,
         -1.3449e-04, -1.2537e-04],
        [-4.1351e-06, -3.1888e-06,  2.9467e-06,  ..., -3.7104e-06,
         -2.8200e-06, -2.8871e-06],
        [-1.1683e-05, -9.3728e-06,  8.8364e-06,  ..., -1.0431e-05,
         -7.9125e-06, -8.0615e-06],
        [-8.9407e-06, -6.7353e-06,  6.6757e-06,  ..., -7.9870e-06,
         -6.0052e-06, -6.5565e-06],
        [-9.8944e-06, -8.1658e-06,  7.3314e-06,  ..., -8.9556e-06,
         -7.0930e-06, -6.3330e-06]], device='cuda:0')
Loss: 0.9781079292297363


Running epoch 1, step 1299, batch 251
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   631,  2820,   344],
        [    0, 40624,   266,  ..., 12236,   292,    41]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1096e-04, -9.9206e-05, -4.7572e-05,  ...,  8.9658e-05,
          5.7549e-05, -1.3113e-04],
        [-5.5209e-06, -4.2766e-06,  3.8929e-06,  ..., -4.9770e-06,
         -3.7961e-06, -3.8631e-06],
        [-1.5467e-05, -1.2502e-05,  1.1623e-05,  ..., -1.3888e-05,
         -1.0625e-05, -1.0699e-05],
        [-1.1846e-05, -9.0003e-06,  8.7619e-06,  ..., -1.0669e-05,
         -8.0764e-06, -8.7321e-06],
        [-1.3143e-05, -1.0952e-05,  9.6858e-06,  ..., -1.1981e-05,
         -9.5665e-06, -8.4341e-06]], device='cuda:0')
Loss: 0.975906252861023


Running epoch 1, step 1300, batch 252
Sampled inputs[:2]: tensor([[   0, 4485,  741,  ...,  292,  221,  341],
        [   0, 3804,  300,  ..., 5062, 9848, 3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6941e-05, -2.0192e-04,  2.4846e-05,  ...,  9.4035e-05,
          2.6716e-04, -8.9121e-05],
        [-6.9216e-06, -5.2899e-06,  4.8131e-06,  ..., -6.2510e-06,
         -4.7237e-06, -4.8913e-06],
        [-1.9372e-05, -1.5467e-05,  1.4350e-05,  ..., -1.7405e-05,
         -1.3173e-05, -1.3515e-05],
        [-1.4856e-05, -1.1116e-05,  1.0818e-05,  ..., -1.3381e-05,
         -1.0028e-05, -1.1042e-05],
        [-1.6481e-05, -1.3575e-05,  1.1981e-05,  ..., -1.5035e-05,
         -1.1891e-05, -1.0669e-05]], device='cuda:0')
Loss: 0.9903340339660645


Running epoch 1, step 1301, batch 253
Sampled inputs[:2]: tensor([[    0, 22390,   292,  ...,  3552,   278,   317],
        [    0,   271, 10474,  ...,   298,  2286,    29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7221e-05, -2.1799e-04,  8.5275e-05,  ...,  1.4161e-04,
          3.3136e-04, -8.4127e-05],
        [-8.3521e-06, -6.3926e-06,  5.8115e-06,  ..., -7.5623e-06,
         -5.6997e-06, -5.8450e-06],
        [-2.3454e-05, -1.8656e-05,  1.7330e-05,  ..., -2.1100e-05,
         -1.5900e-05, -1.6257e-05],
        [-1.7896e-05, -1.3396e-05,  1.3024e-05,  ..., -1.6138e-05,
         -1.2040e-05, -1.3188e-05],
        [-1.9938e-05, -1.6391e-05,  1.4469e-05,  ..., -1.8254e-05,
         -1.4380e-05, -1.2830e-05]], device='cuda:0')
Loss: 0.9929884672164917


Running epoch 1, step 1302, batch 254
Sampled inputs[:2]: tensor([[    0,  3047,  4878,  ...,   352, 10854, 34025],
        [    0,  1596,  2700,  ...,   943,   266,  4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6660e-05, -2.7063e-04, -3.4034e-05,  ...,  4.0888e-05,
          2.8243e-04, -2.1424e-04],
        [-9.7379e-06, -7.3686e-06,  6.7092e-06,  ..., -8.8364e-06,
         -6.6683e-06, -6.9030e-06],
        [-2.7299e-05, -2.1473e-05,  2.0042e-05,  ..., -2.4572e-05,
         -1.8537e-05, -1.9118e-05],
        [-2.0817e-05, -1.5348e-05,  1.5005e-05,  ..., -1.8790e-05,
         -1.4037e-05, -1.5527e-05],
        [-2.3395e-05, -1.9014e-05,  1.6853e-05,  ..., -2.1413e-05,
         -1.6913e-05, -1.5199e-05]], device='cuda:0')
Loss: 0.9631572961807251


Running epoch 1, step 1303, batch 255
Sampled inputs[:2]: tensor([[   0, 2027,  365,  ...,  368, 1782,  394],
        [   0, 1712,   12,  ..., 1255, 1688,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1082e-06, -1.1392e-04, -9.0862e-05,  ..., -1.6032e-04,
          1.4395e-04, -1.5011e-04],
        [-1.1198e-05, -8.3223e-06,  7.5027e-06,  ..., -1.0118e-05,
         -7.6890e-06, -8.1770e-06],
        [-3.1441e-05, -2.4363e-05,  2.2501e-05,  ..., -2.8208e-05,
         -2.1487e-05, -2.2665e-05],
        [-2.3887e-05, -1.7315e-05,  1.6786e-05,  ..., -2.1487e-05,
         -1.6198e-05, -1.8269e-05],
        [-2.7105e-05, -2.1651e-05,  1.8954e-05,  ..., -2.4691e-05,
         -1.9699e-05, -1.8194e-05]], device='cuda:0')
Loss: 0.9145874381065369
Graident accumulation at epoch 1, step 1303, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7089e-05,  4.0316e-05, -1.5615e-04,  ..., -7.8842e-05,
          1.2611e-04, -2.0519e-05],
        [-1.0447e-05, -6.5980e-06,  5.8440e-06,  ..., -9.0310e-06,
         -6.2524e-06, -7.4249e-06],
        [-1.6895e-05, -6.9426e-06,  1.3320e-05,  ..., -1.3405e-05,
         -4.2112e-06, -1.3591e-05],
        [ 8.8579e-06,  2.5583e-05, -4.7119e-06,  ...,  6.1746e-06,
          1.8196e-05,  1.9109e-06],
        [-1.2401e-05, -6.1184e-06,  8.6460e-06,  ..., -1.0006e-05,
         -5.2845e-06, -1.2793e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3248e-08, 5.4587e-08, 6.5142e-08,  ..., 2.4833e-08, 1.5031e-07,
         3.7316e-08],
        [7.8363e-11, 5.1211e-11, 1.9243e-11,  ..., 5.4388e-11, 1.6019e-11,
         2.2432e-11],
        [3.4452e-09, 1.9972e-09, 1.0086e-09,  ..., 2.6512e-09, 8.5431e-10,
         9.0549e-10],
        [9.8886e-10, 9.2464e-10, 3.3575e-10,  ..., 8.2544e-10, 5.6760e-10,
         3.4029e-10],
        [3.9384e-10, 2.2266e-10, 7.4762e-11,  ..., 2.8739e-10, 8.0141e-11,
         1.0742e-10]], device='cuda:0')
optimizer state dict: 163.0
lr: [6.599295247432596e-06, 6.599295247432596e-06]
scheduler_last_epoch: 163


Running epoch 1, step 1304, batch 256
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7076e-05, -4.6020e-05, -7.6637e-05,  ..., -1.6938e-04,
         -4.5725e-05,  2.1444e-05],
        [-1.4231e-06, -1.0431e-06,  1.0356e-06,  ..., -1.2219e-06,
         -9.2760e-07, -9.5367e-07],
        [-3.9339e-06, -3.0398e-06,  3.0398e-06,  ..., -3.3975e-06,
         -2.6077e-06, -2.6673e-06],
        [-3.0696e-06, -2.2352e-06,  2.3544e-06,  ..., -2.6524e-06,
         -1.9968e-06, -2.1905e-06],
        [-3.3081e-06, -2.6822e-06,  2.5183e-06,  ..., -2.9206e-06,
         -2.3693e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9822831749916077


Running epoch 1, step 1305, batch 257
Sampled inputs[:2]: tensor([[    0,    14,  6436,  ...,   271,  1211,  8917],
        [    0,  5332,   266,  ...,   300,   259, 15369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5378e-05, -1.4192e-05, -4.4173e-05,  ..., -1.7947e-04,
          2.0778e-05, -2.4606e-05],
        [-2.8163e-06, -2.1979e-06,  2.0042e-06,  ..., -2.5034e-06,
         -1.9334e-06, -1.8962e-06],
        [-7.9274e-06, -6.4820e-06,  5.9456e-06,  ..., -7.0781e-06,
         -5.4985e-06, -5.3793e-06],
        [-6.0201e-06, -4.6492e-06,  4.4703e-06,  ..., -5.3495e-06,
         -4.0978e-06, -4.2766e-06],
        [-6.7055e-06, -5.7220e-06,  4.9472e-06,  ..., -6.1244e-06,
         -5.0068e-06, -4.2468e-06]], device='cuda:0')
Loss: 1.0081636905670166


Running epoch 1, step 1306, batch 258
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0, 13964,    13,  ...,    14,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9970e-05, -6.8736e-05,  7.4296e-06,  ..., -1.6404e-04,
          5.6763e-05, -1.2972e-04],
        [-4.1872e-06, -3.2037e-06,  3.0845e-06,  ..., -3.7029e-06,
         -2.7642e-06, -2.8163e-06],
        [-1.1772e-05, -9.4473e-06,  9.1344e-06,  ..., -1.0461e-05,
         -7.8529e-06, -7.9870e-06],
        [-9.0003e-06, -6.7949e-06,  6.9290e-06,  ..., -7.9572e-06,
         -5.8860e-06, -6.3926e-06],
        [-9.8050e-06, -8.2105e-06,  7.4506e-06,  ..., -8.9109e-06,
         -7.0632e-06, -6.1840e-06]], device='cuda:0')
Loss: 0.9611168503761292


Running epoch 1, step 1307, batch 259
Sampled inputs[:2]: tensor([[    0, 10766,  8311,  ...,   328,   957,  1231],
        [    0,  6203,   352,  ...,   266,  3437,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6013e-04, -6.8736e-05, -3.4652e-05,  ..., -1.9379e-04,
         -8.2643e-05, -1.0731e-04],
        [-5.5581e-06, -4.1462e-06,  4.0904e-06,  ..., -4.9397e-06,
         -3.6322e-06, -3.8445e-06],
        [-1.5616e-05, -1.2249e-05,  1.2159e-05,  ..., -1.3933e-05,
         -1.0312e-05, -1.0788e-05],
        [-1.1981e-05, -8.7917e-06,  9.2238e-06,  ..., -1.0639e-05,
         -7.7635e-06, -8.7172e-06],
        [-1.3024e-05, -1.0639e-05,  9.9391e-06,  ..., -1.1846e-05,
         -9.2685e-06, -8.3596e-06]], device='cuda:0')
Loss: 0.9536563754081726


Running epoch 1, step 1308, batch 260
Sampled inputs[:2]: tensor([[    0,   266,   944,  ..., 14981,  1952,   271],
        [    0,  3119,   278,  ...,   352,   674,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0691e-06, -1.6467e-04, -4.7675e-05,  ..., -2.3145e-04,
          2.9749e-04, -7.7485e-05],
        [-7.0781e-06, -5.0925e-06,  5.0403e-06,  ..., -6.2808e-06,
         -4.6082e-06, -4.9844e-06],
        [-1.9729e-05, -1.4991e-05,  1.4901e-05,  ..., -1.7539e-05,
         -1.2964e-05, -1.3828e-05],
        [-1.5154e-05, -1.0729e-05,  1.1310e-05,  ..., -1.3426e-05,
         -9.7752e-06, -1.1176e-05],
        [-1.6734e-05, -1.3173e-05,  1.2323e-05,  ..., -1.5169e-05,
         -1.1832e-05, -1.0982e-05]], device='cuda:0')
Loss: 0.9989896416664124


Running epoch 1, step 1309, batch 261
Sampled inputs[:2]: tensor([[    0,  4988, 36842,  ...,  7630, 18362,    13],
        [    0,  1342,    14,  ...,  1236, 15667, 12931]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1732e-05, -1.5514e-04, -4.7675e-05,  ..., -2.6687e-04,
          2.1219e-04, -1.0388e-04],
        [-8.4415e-06, -6.2026e-06,  6.1281e-06,  ..., -7.5251e-06,
         -5.5172e-06, -5.8636e-06],
        [-2.3752e-05, -1.8373e-05,  1.8239e-05,  ..., -2.1219e-05,
         -1.5646e-05, -1.6436e-05],
        [-1.8209e-05, -1.3173e-05,  1.3843e-05,  ..., -1.6198e-05,
         -1.1772e-05, -1.3232e-05],
        [-1.9997e-05, -1.6004e-05,  1.4991e-05,  ..., -1.8209e-05,
         -1.4156e-05, -1.2964e-05]], device='cuda:0')
Loss: 1.0315382480621338


Running epoch 1, step 1310, batch 262
Sampled inputs[:2]: tensor([[    0, 23809, 27646,  ...,   266,  3373,   554],
        [    0,  1580,   271,  ...,   656,   943,  1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2475e-05, -1.5093e-04, -8.0290e-05,  ..., -3.3594e-04,
          1.1415e-04, -7.4554e-05],
        [-9.8646e-06, -7.2755e-06,  7.1786e-06,  ..., -8.7619e-06,
         -6.4112e-06, -6.8247e-06],
        [-2.7776e-05, -2.1577e-05,  2.1428e-05,  ..., -2.4706e-05,
         -1.8179e-05, -1.9148e-05],
        [-2.1249e-05, -1.5453e-05,  1.6227e-05,  ..., -1.8850e-05,
         -1.3694e-05, -1.5423e-05],
        [-2.3231e-05, -1.8671e-05,  1.7494e-05,  ..., -2.1070e-05,
         -1.6347e-05, -1.4976e-05]], device='cuda:0')
Loss: 0.9507618546485901


Running epoch 1, step 1311, batch 263
Sampled inputs[:2]: tensor([[    0,   259, 19567,  ...,   266,  3899,  2123],
        [    0,  6976, 16084,  ...,    19,  9955,  3854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2468e-05, -1.4532e-04, -2.1145e-05,  ..., -4.3686e-04,
         -8.8443e-05, -1.8033e-04],
        [-1.1280e-05, -8.4229e-06,  8.3037e-06,  ..., -1.0036e-05,
         -7.3649e-06, -7.7337e-06],
        [-3.1799e-05, -2.5004e-05,  2.4781e-05,  ..., -2.8372e-05,
         -2.0966e-05, -2.1771e-05],
        [-2.4363e-05, -1.7971e-05,  1.8805e-05,  ..., -2.1666e-05,
         -1.5810e-05, -1.7524e-05],
        [-2.6494e-05, -2.1562e-05,  2.0161e-05,  ..., -2.4110e-05,
         -1.8761e-05, -1.6958e-05]], device='cuda:0')
Loss: 1.0205187797546387
Graident accumulation at epoch 1, step 1311, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5133e-05,  2.1752e-05, -1.4265e-04,  ..., -1.1464e-04,
          1.0465e-04, -3.6500e-05],
        [-1.0531e-05, -6.7805e-06,  6.0899e-06,  ..., -9.1315e-06,
         -6.3637e-06, -7.4557e-06],
        [-1.8385e-05, -8.7488e-06,  1.4466e-05,  ..., -1.4902e-05,
         -5.8866e-06, -1.4409e-05],
        [ 5.5358e-06,  2.1228e-05, -2.3602e-06,  ...,  3.3905e-06,
          1.4795e-05, -3.2563e-08],
        [-1.3810e-05, -7.6628e-06,  9.7975e-06,  ..., -1.1416e-05,
         -6.6321e-06, -1.3210e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3203e-08, 5.4554e-08, 6.5077e-08,  ..., 2.4999e-08, 1.5017e-07,
         3.7311e-08],
        [7.8412e-11, 5.1230e-11, 1.9293e-11,  ..., 5.4434e-11, 1.6057e-11,
         2.2470e-11],
        [3.4428e-09, 1.9959e-09, 1.0082e-09,  ..., 2.6494e-09, 8.5390e-10,
         9.0506e-10],
        [9.8847e-10, 9.2403e-10, 3.3577e-10,  ..., 8.2509e-10, 5.6728e-10,
         3.4025e-10],
        [3.9414e-10, 2.2290e-10, 7.5093e-11,  ..., 2.8768e-10, 8.0413e-11,
         1.0760e-10]], device='cuda:0')
optimizer state dict: 164.0
lr: [6.4833089778264036e-06, 6.4833089778264036e-06]
scheduler_last_epoch: 164


Running epoch 1, step 1312, batch 264
Sampled inputs[:2]: tensor([[    0,   259,  1143,  ...,   593,   360,   278],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0544e-04, -8.6301e-05, -8.5394e-05,  ..., -1.9422e-04,
          2.7093e-05, -2.6602e-04],
        [-1.5125e-06, -1.0356e-06,  1.0133e-06,  ..., -1.3188e-06,
         -1.0058e-06, -1.1399e-06],
        [-4.2021e-06, -3.0547e-06,  3.0249e-06,  ..., -3.6806e-06,
         -2.8163e-06, -3.1441e-06],
        [-3.1143e-06, -2.1160e-06,  2.2054e-06,  ..., -2.7269e-06,
         -2.0713e-06, -2.4289e-06],
        [-3.7253e-06, -2.7716e-06,  2.6226e-06,  ..., -3.3081e-06,
         -2.6524e-06, -2.6226e-06]], device='cuda:0')
Loss: 0.9598264694213867


Running epoch 1, step 1313, batch 265
Sampled inputs[:2]: tensor([[   0,  413,   16,  ...,  680,  401, 1407],
        [   0,  287, 7763,  ...,  689, 2409,  699]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5859e-05, -4.5845e-05,  9.7749e-05,  ..., -1.9982e-04,
          5.3698e-05, -1.9947e-04],
        [-2.9951e-06, -2.0862e-06,  1.9670e-06,  ..., -2.6822e-06,
         -2.0489e-06, -2.3097e-06],
        [-8.3447e-06, -6.1095e-06,  5.8711e-06,  ..., -7.4357e-06,
         -5.6922e-06, -6.3628e-06],
        [-6.1989e-06, -4.2319e-06,  4.2617e-06,  ..., -5.5432e-06,
         -4.2021e-06, -4.9621e-06],
        [-7.4655e-06, -5.5581e-06,  5.1409e-06,  ..., -6.7353e-06,
         -5.3644e-06, -5.3197e-06]], device='cuda:0')
Loss: 0.9542244672775269


Running epoch 1, step 1314, batch 266
Sampled inputs[:2]: tensor([[   0,  270,  472,  ...,  292,   73,   14],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2777e-04, -8.7644e-05,  7.0676e-05,  ..., -1.5379e-04,
         -7.6556e-05, -1.1040e-04],
        [-4.4107e-06, -3.1888e-06,  2.9653e-06,  ..., -3.9786e-06,
         -3.0473e-06, -3.3006e-06],
        [-1.2368e-05, -9.3728e-06,  8.8513e-06,  ..., -1.1086e-05,
         -8.4788e-06, -9.1642e-06],
        [-9.2387e-06, -6.5565e-06,  6.4820e-06,  ..., -8.3297e-06,
         -6.3330e-06, -7.2271e-06],
        [-1.0863e-05, -8.4043e-06,  7.6145e-06,  ..., -9.8646e-06,
         -7.8529e-06, -7.5400e-06]], device='cuda:0')
Loss: 0.9848339557647705


Running epoch 1, step 1315, batch 267
Sampled inputs[:2]: tensor([[    0,  2720,    14,  ...,   300, 15867,   368],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3300e-04, -7.7251e-05,  1.3502e-04,  ..., -1.5714e-04,
         -5.9167e-05, -1.1854e-04],
        [-5.7742e-06, -4.1649e-06,  3.9935e-06,  ..., -5.1931e-06,
         -3.9265e-06, -4.2394e-06],
        [-1.6242e-05, -1.2264e-05,  1.1966e-05,  ..., -1.4499e-05,
         -1.0937e-05, -1.1772e-05],
        [-1.2159e-05, -8.5980e-06,  8.8215e-06,  ..., -1.0908e-05,
         -8.1882e-06, -9.3281e-06],
        [-1.4096e-05, -1.0908e-05,  1.0177e-05,  ..., -1.2770e-05,
         -1.0058e-05, -9.5516e-06]], device='cuda:0')
Loss: 0.9617670774459839


Running epoch 1, step 1316, batch 268
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,  7712, 31756,  ...,   895,   360,   630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3936e-04, -4.0493e-05,  1.7801e-04,  ..., -1.6404e-04,
         -1.4255e-04, -1.6957e-04],
        [-7.3165e-06, -5.1856e-06,  5.0440e-06,  ..., -6.5342e-06,
         -4.8466e-06, -5.3346e-06],
        [-2.0444e-05, -1.5244e-05,  1.5035e-05,  ..., -1.8179e-05,
         -1.3486e-05, -1.4737e-05],
        [-1.5348e-05, -1.0699e-05,  1.1131e-05,  ..., -1.3679e-05,
         -1.0073e-05, -1.1668e-05],
        [-1.7777e-05, -1.3605e-05,  1.2770e-05,  ..., -1.6078e-05,
         -1.2487e-05, -1.2025e-05]], device='cuda:0')
Loss: 0.9733331799507141


Running epoch 1, step 1317, batch 269
Sampled inputs[:2]: tensor([[    0,  1811,   278,  ...,   278,   259,  4617],
        [    0,  9041,  8375,  ...,   221,   474, 43112]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8915e-04, -1.9939e-04,  1.5761e-04,  ..., -1.3739e-04,
          7.7089e-05, -2.6838e-04],
        [-8.7619e-06, -6.1616e-06,  6.0126e-06,  ..., -7.8380e-06,
         -5.7817e-06, -6.4224e-06],
        [-2.4289e-05, -1.8001e-05,  1.7837e-05,  ..., -2.1622e-05,
         -1.6004e-05, -1.7539e-05],
        [-1.8358e-05, -1.2711e-05,  1.3262e-05,  ..., -1.6406e-05,
         -1.2040e-05, -1.4007e-05],
        [-2.1175e-05, -1.6078e-05,  1.5199e-05,  ..., -1.9148e-05,
         -1.4827e-05, -1.4335e-05]], device='cuda:0')
Loss: 0.9482724666595459


Running epoch 1, step 1318, batch 270
Sampled inputs[:2]: tensor([[    0, 17900,   554,  ...,   266,  7912,    26],
        [    0,    12,   616,  ...,   278,   266,  2907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9483e-04, -2.2717e-04,  2.0901e-04,  ..., -8.0214e-05,
          1.3515e-04, -2.4966e-04],
        [-1.0230e-05, -7.1675e-06,  7.1749e-06,  ..., -9.0674e-06,
         -6.6347e-06, -7.3686e-06],
        [-2.8402e-05, -2.0981e-05,  2.1249e-05,  ..., -2.5079e-05,
         -1.8418e-05, -2.0176e-05],
        [-2.1562e-05, -1.4886e-05,  1.5914e-05,  ..., -1.9088e-05,
         -1.3888e-05, -1.6168e-05],
        [-2.4512e-05, -1.8612e-05,  1.7926e-05,  ..., -2.2009e-05,
         -1.6943e-05, -1.6347e-05]], device='cuda:0')
Loss: 0.9663877487182617


Running epoch 1, step 1319, batch 271
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0,    14,    23,  ...,   278,   266,  1462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1003e-04, -1.3064e-04,  3.9518e-04,  ..., -9.4204e-05,
          3.7614e-04, -1.7707e-04],
        [-1.1660e-05, -8.4341e-06,  8.1584e-06,  ..., -1.0416e-05,
         -7.7672e-06, -8.3447e-06],
        [-3.2336e-05, -2.4632e-05,  2.4110e-05,  ..., -2.8819e-05,
         -2.1562e-05, -2.2858e-05],
        [-2.4557e-05, -1.7524e-05,  1.8045e-05,  ..., -2.1920e-05,
         -1.6272e-05, -1.8314e-05],
        [-2.7955e-05, -2.1875e-05,  2.0385e-05,  ..., -2.5317e-05,
         -1.9819e-05, -1.8537e-05]], device='cuda:0')
Loss: 1.005921721458435
Graident accumulation at epoch 1, step 1319, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0236, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.7383e-05,  6.5132e-06, -8.8869e-05,  ..., -1.1260e-04,
          1.3180e-04, -5.0557e-05],
        [-1.0644e-05, -6.9458e-06,  6.2968e-06,  ..., -9.2599e-06,
         -6.5040e-06, -7.5446e-06],
        [-1.9780e-05, -1.0337e-05,  1.5431e-05,  ..., -1.6293e-05,
         -7.4542e-06, -1.5254e-05],
        [ 2.5265e-06,  1.7353e-05, -3.1961e-07,  ...,  8.5949e-07,
          1.1689e-05, -1.8607e-06],
        [-1.5224e-05, -9.0840e-06,  1.0856e-05,  ..., -1.2806e-05,
         -7.9507e-06, -1.3742e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3318e-08, 5.4516e-08, 6.5169e-08,  ..., 2.4983e-08, 1.5016e-07,
         3.7305e-08],
        [7.8470e-11, 5.1250e-11, 1.9340e-11,  ..., 5.4488e-11, 1.6101e-11,
         2.2517e-11],
        [3.4404e-09, 1.9945e-09, 1.0078e-09,  ..., 2.6476e-09, 8.5351e-10,
         9.0467e-10],
        [9.8808e-10, 9.2342e-10, 3.3576e-10,  ..., 8.2474e-10, 5.6698e-10,
         3.4025e-10],
        [3.9453e-10, 2.2316e-10, 7.5434e-11,  ..., 2.8803e-10, 8.0726e-11,
         1.0784e-10]], device='cuda:0')
optimizer state dict: 165.0
lr: [6.367860089306028e-06, 6.367860089306028e-06]
scheduler_last_epoch: 165


Running epoch 1, step 1320, batch 272
Sampled inputs[:2]: tensor([[   0,  689, 2149,  ..., 4263,   14,  292],
        [   0,  596,  292,  ...,   13, 6673,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4812e-05,  1.4953e-04,  1.7342e-05,  ..., -9.0956e-05,
          9.6419e-05,  2.4512e-05],
        [-1.5497e-06, -1.0133e-06,  1.0058e-06,  ..., -1.3411e-06,
         -9.9093e-07, -1.1548e-06],
        [-4.2617e-06, -2.9355e-06,  3.0100e-06,  ..., -3.6508e-06,
         -2.7269e-06, -3.1143e-06],
        [-3.1888e-06, -2.0415e-06,  2.1756e-06,  ..., -2.7567e-06,
         -2.0415e-06, -2.4736e-06],
        [-3.7104e-06, -2.6226e-06,  2.5481e-06,  ..., -3.2336e-06,
         -2.5034e-06, -2.5034e-06]], device='cuda:0')
Loss: 0.9415667653083801


Running epoch 1, step 1321, batch 273
Sampled inputs[:2]: tensor([[   0, 1871,  401,  ...,   14, 4797,   12],
        [   0,  471,   14,  ..., 1260, 2129,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6742e-05,  2.4683e-04,  1.0916e-04,  ..., -1.3373e-04,
         -3.6350e-05, -1.8155e-04],
        [-2.9132e-06, -2.0340e-06,  2.0564e-06,  ..., -2.5779e-06,
         -1.9036e-06, -2.1234e-06],
        [-8.1062e-06, -5.9158e-06,  6.1393e-06,  ..., -7.0930e-06,
         -5.2452e-06, -5.8115e-06],
        [-6.1840e-06, -4.2170e-06,  4.5896e-06,  ..., -5.4538e-06,
         -4.0084e-06, -4.7088e-06],
        [-6.9439e-06, -5.2303e-06,  5.1260e-06,  ..., -6.1840e-06,
         -4.7833e-06, -4.6045e-06]], device='cuda:0')
Loss: 0.9333781003952026


Running epoch 1, step 1322, batch 274
Sampled inputs[:2]: tensor([[    0,    20,    13,  ...,   496,    14,  1032],
        [    0,     9,   287,  ..., 14761,  9700,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5044e-05,  4.9453e-04,  2.6511e-04,  ..., -1.7639e-04,
         -7.3211e-05, -1.4259e-04],
        [-4.4182e-06, -2.9691e-06,  2.9728e-06,  ..., -3.9712e-06,
         -3.0138e-06, -3.4049e-06],
        [-1.2070e-05, -8.5533e-06,  8.7470e-06,  ..., -1.0714e-05,
         -8.1658e-06, -9.1195e-06],
        [-9.1642e-06, -6.0275e-06,  6.4597e-06,  ..., -8.2254e-06,
         -6.1989e-06, -7.3165e-06],
        [-1.0729e-05, -7.7486e-06,  7.5400e-06,  ..., -9.6709e-06,
         -7.6443e-06, -7.5549e-06]], device='cuda:0')
Loss: 0.9478837847709656


Running epoch 1, step 1323, batch 275
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,  2280,   344,  ...,   287,   266,  3344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4695e-05,  4.3406e-04,  2.1521e-04,  ..., -7.5486e-05,
         -2.7408e-04, -1.6131e-04],
        [-5.8711e-06, -3.9972e-06,  4.1202e-06,  ..., -5.2303e-06,
         -3.9004e-06, -4.3884e-06],
        [-1.6093e-05, -1.1519e-05,  1.2070e-05,  ..., -1.4201e-05,
         -1.0625e-05, -1.1817e-05],
        [-1.2204e-05, -8.1286e-06,  8.9630e-06,  ..., -1.0848e-05,
         -8.0317e-06, -9.4622e-06],
        [-1.4186e-05, -1.0371e-05,  1.0327e-05,  ..., -1.2696e-05,
         -9.8944e-06, -9.7156e-06]], device='cuda:0')
Loss: 0.9586717486381531


Running epoch 1, step 1324, batch 276
Sampled inputs[:2]: tensor([[    0,  2974,   278,  ...,   365,  8758,   271],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8061e-05,  4.5835e-04, -6.9172e-05,  ...,  1.3320e-05,
         -4.3491e-04, -1.7507e-04],
        [-7.2718e-06, -4.9807e-06,  5.2229e-06,  ..., -6.4671e-06,
         -4.7572e-06, -5.3309e-06],
        [-2.0117e-05, -1.4454e-05,  1.5408e-05,  ..., -1.7717e-05,
         -1.3053e-05, -1.4469e-05],
        [-1.5244e-05, -1.0200e-05,  1.1452e-05,  ..., -1.3530e-05,
         -9.8795e-06, -1.1623e-05],
        [-1.7479e-05, -1.2845e-05,  1.3009e-05,  ..., -1.5587e-05,
         -1.2010e-05, -1.1712e-05]], device='cuda:0')
Loss: 0.9637489914894104


Running epoch 1, step 1325, batch 277
Sampled inputs[:2]: tensor([[    0,   565,  5539,  ...,    12,   516, 14426],
        [    0,   287,   271,  ...,  1039,  4186,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9016e-04,  4.9221e-04,  1.0252e-04,  ...,  4.5864e-05,
         -3.6435e-04, -8.8337e-05],
        [-8.7097e-06, -6.1281e-06,  6.3255e-06,  ..., -7.7486e-06,
         -5.7705e-06, -6.3069e-06],
        [-2.4199e-05, -1.7792e-05,  1.8671e-05,  ..., -2.1368e-05,
         -1.5929e-05, -1.7256e-05],
        [-1.8328e-05, -1.2599e-05,  1.3910e-05,  ..., -1.6272e-05,
         -1.2010e-05, -1.3798e-05],
        [-2.0936e-05, -1.5751e-05,  1.5721e-05,  ..., -1.8716e-05,
         -1.4573e-05, -1.3918e-05]], device='cuda:0')
Loss: 1.0207133293151855


Running epoch 1, step 1326, batch 278
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0, 2159,  271,  ..., 1268,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6370e-05,  5.5102e-04,  1.9207e-04,  ..., -7.9276e-06,
         -2.6660e-04, -4.4964e-05],
        [-1.0304e-05, -7.1563e-06,  7.4282e-06,  ..., -9.1493e-06,
         -6.7689e-06, -7.4245e-06],
        [-2.8491e-05, -2.0728e-05,  2.1830e-05,  ..., -2.5138e-05,
         -1.8656e-05, -2.0236e-05],
        [-2.1502e-05, -1.4640e-05,  1.6235e-05,  ..., -1.9059e-05,
         -1.3992e-05, -1.6093e-05],
        [-2.4840e-05, -1.8463e-05,  1.8507e-05,  ..., -2.2173e-05,
         -1.7196e-05, -1.6481e-05]], device='cuda:0')
Loss: 0.9603073000907898


Running epoch 1, step 1327, batch 279
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0,  445,   16,  ..., 7747, 5308, 6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7697e-05,  6.8974e-04,  2.0324e-04,  ..., -4.8901e-05,
         -2.5884e-04,  1.4063e-05],
        [-1.1735e-05, -8.2292e-06,  8.5235e-06,  ..., -1.0446e-05,
         -7.6629e-06, -8.4154e-06],
        [-3.2514e-05, -2.3901e-05,  2.5094e-05,  ..., -2.8774e-05,
         -2.1189e-05, -2.2992e-05],
        [-2.4542e-05, -1.6890e-05,  1.8679e-05,  ..., -2.1815e-05,
         -1.5877e-05, -1.8299e-05],
        [-2.8133e-05, -2.1130e-05,  2.1130e-05,  ..., -2.5183e-05,
         -1.9416e-05, -1.8567e-05]], device='cuda:0')
Loss: 0.9625715613365173
Graident accumulation at epoch 1, step 1327, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1414e-05,  7.4836e-05, -5.9658e-05,  ..., -1.0623e-04,
          9.2736e-05, -4.4095e-05],
        [-1.0753e-05, -7.0742e-06,  6.5195e-06,  ..., -9.3785e-06,
         -6.6199e-06, -7.6317e-06],
        [-2.1054e-05, -1.1693e-05,  1.6397e-05,  ..., -1.7541e-05,
         -8.8277e-06, -1.6027e-05],
        [-1.8037e-07,  1.3928e-05,  1.5802e-06,  ..., -1.4080e-06,
          8.9321e-06, -3.5045e-06],
        [-1.6515e-05, -1.0289e-05,  1.1884e-05,  ..., -1.4044e-05,
         -9.0973e-06, -1.4225e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3270e-08, 5.4937e-08, 6.5145e-08,  ..., 2.4960e-08, 1.5008e-07,
         3.7268e-08],
        [7.8529e-11, 5.1267e-11, 1.9394e-11,  ..., 5.4543e-11, 1.6144e-11,
         2.2565e-11],
        [3.4380e-09, 1.9930e-09, 1.0074e-09,  ..., 2.6457e-09, 8.5310e-10,
         9.0430e-10],
        [9.8770e-10, 9.2278e-10, 3.3577e-10,  ..., 8.2439e-10, 5.6666e-10,
         3.4024e-10],
        [3.9493e-10, 2.2338e-10, 7.5805e-11,  ..., 2.8838e-10, 8.1022e-11,
         1.0808e-10]], device='cuda:0')
optimizer state dict: 166.0
lr: [6.252966223469409e-06, 6.252966223469409e-06]
scheduler_last_epoch: 166


Running epoch 1, step 1328, batch 280
Sampled inputs[:2]: tensor([[    0,    80, 10802,  ...,   287, 28533, 25359],
        [    0,  2973,    30,  ...,   408,   259,  1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3170e-05, -1.9490e-04,  2.3840e-05,  ...,  6.6775e-06,
         -9.2105e-05,  2.7346e-05],
        [-1.4231e-06, -1.0952e-06,  1.1027e-06,  ..., -1.2890e-06,
         -9.9093e-07, -1.0058e-06],
        [-4.0531e-06, -3.1441e-06,  3.3081e-06,  ..., -3.5912e-06,
         -2.7567e-06, -2.8163e-06],
        [-3.0249e-06, -2.2799e-06,  2.4289e-06,  ..., -2.7120e-06,
         -2.0564e-06, -2.2203e-06],
        [-3.4869e-06, -2.7567e-06,  2.8163e-06,  ..., -3.1441e-06,
         -2.5034e-06, -2.2501e-06]], device='cuda:0')
Loss: 1.0198071002960205


Running epoch 1, step 1329, batch 281
Sampled inputs[:2]: tensor([[    0,  2544,   394,  ...,    14,  1062,   516],
        [    0, 36122,  1085,  ...,  6231,     9,  7794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2664e-07, -1.9306e-04,  1.9811e-04,  ...,  1.0407e-04,
         -1.3117e-04,  1.3804e-04],
        [-2.7940e-06, -2.1681e-06,  2.2054e-06,  ..., -2.6152e-06,
         -2.0191e-06, -1.9595e-06],
        [-7.8678e-06, -6.2436e-06,  6.5118e-06,  ..., -7.2718e-06,
         -5.6326e-06, -5.4389e-06],
        [-5.9605e-06, -4.5598e-06,  4.8876e-06,  ..., -5.5730e-06,
         -4.2766e-06, -4.3809e-06],
        [-6.8247e-06, -5.5432e-06,  5.5879e-06,  ..., -6.3926e-06,
         -5.1260e-06, -4.3958e-06]], device='cuda:0')
Loss: 1.0182572603225708


Running epoch 1, step 1330, batch 282
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4227e-05, -2.3871e-04,  2.9802e-04,  ..., -3.1149e-05,
         -1.4211e-04,  1.6058e-04],
        [-4.1723e-06, -3.2559e-06,  3.3081e-06,  ..., -3.8669e-06,
         -2.8983e-06, -2.9057e-06],
        [-1.1742e-05, -9.4175e-06,  9.8050e-06,  ..., -1.0788e-05,
         -8.1062e-06, -8.0764e-06],
        [-8.9556e-06, -6.8694e-06,  7.3910e-06,  ..., -8.2999e-06,
         -6.1691e-06, -6.5565e-06],
        [-1.0088e-05, -8.2701e-06,  8.2999e-06,  ..., -9.3877e-06,
         -7.3314e-06, -6.4373e-06]], device='cuda:0')
Loss: 0.9643876552581787


Running epoch 1, step 1331, batch 283
Sampled inputs[:2]: tensor([[    0,    12,   221,  ...,   292, 27729,  9837],
        [    0,  7219,   591,  ...,   278,   266,  5908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0964e-05, -1.9306e-04,  2.3224e-04,  ...,  7.8294e-05,
         -4.0836e-04,  1.2718e-05],
        [-5.5879e-06, -4.2915e-06,  4.5151e-06,  ..., -5.1335e-06,
         -3.8259e-06, -3.8147e-06],
        [-1.5527e-05, -1.2293e-05,  1.3217e-05,  ..., -1.4156e-05,
         -1.0595e-05, -1.0461e-05],
        [-1.1921e-05, -8.9854e-06,  1.0058e-05,  ..., -1.0952e-05,
         -8.1062e-06, -8.5533e-06],
        [-1.3307e-05, -1.0803e-05,  1.1131e-05,  ..., -1.2279e-05,
         -9.5814e-06, -8.3074e-06]], device='cuda:0')
Loss: 0.9674182534217834


Running epoch 1, step 1332, batch 284
Sampled inputs[:2]: tensor([[   0, 4258,  717,  ...,   34,  609, 1169],
        [   0, 5998,  591,  ..., 3126,   12,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6492e-06, -1.8892e-04,  2.3585e-04,  ...,  9.0130e-05,
         -3.1759e-04, -1.0226e-04],
        [-6.9439e-06, -5.2676e-06,  5.5581e-06,  ..., -6.4075e-06,
         -4.7572e-06, -4.8131e-06],
        [-1.9148e-05, -1.4961e-05,  1.6198e-05,  ..., -1.7449e-05,
         -1.3009e-05, -1.2919e-05],
        [-1.4722e-05, -1.0923e-05,  1.2338e-05,  ..., -1.3575e-05,
         -1.0028e-05, -1.0684e-05],
        [-1.6496e-05, -1.3188e-05,  1.3709e-05,  ..., -1.5184e-05,
         -1.1802e-05, -1.0289e-05]], device='cuda:0')
Loss: 0.9482228755950928


Running epoch 1, step 1333, batch 285
Sampled inputs[:2]: tensor([[    0,  6481,   298,  ...,  6145, 16858,   824],
        [    0,   706,  6989,  ...,  6914,    15,  2537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8857e-05, -2.3774e-04,  3.4041e-04,  ...,  1.3395e-04,
         -3.7861e-04, -1.2238e-04],
        [-8.3670e-06, -6.2287e-06,  6.6757e-06,  ..., -7.6592e-06,
         -5.6103e-06, -5.7556e-06],
        [-2.3112e-05, -1.7777e-05,  1.9491e-05,  ..., -2.0951e-05,
         -1.5408e-05, -1.5527e-05],
        [-1.7717e-05, -1.2904e-05,  1.4812e-05,  ..., -1.6212e-05,
         -1.1824e-05, -1.2770e-05],
        [-1.9833e-05, -1.5661e-05,  1.6421e-05,  ..., -1.8179e-05,
         -1.3977e-05, -1.2331e-05]], device='cuda:0')
Loss: 0.9726065993309021


Running epoch 1, step 1334, batch 286
Sampled inputs[:2]: tensor([[    0,  2626,    13,  ...,   300,   369,   259],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2445e-04, -1.5522e-04,  3.7533e-04,  ...,  1.6067e-04,
         -3.7120e-04, -7.8520e-05],
        [-9.8050e-06, -7.3388e-06,  7.7710e-06,  ..., -8.9481e-06,
         -6.5640e-06, -6.7391e-06],
        [-2.7046e-05, -2.0966e-05,  2.2680e-05,  ..., -2.4468e-05,
         -1.8060e-05, -1.8165e-05],
        [-2.0787e-05, -1.5244e-05,  1.7256e-05,  ..., -1.8969e-05,
         -1.3880e-05, -1.4976e-05],
        [-2.3171e-05, -1.8433e-05,  1.9059e-05,  ..., -2.1204e-05,
         -1.6361e-05, -1.4402e-05]], device='cuda:0')
Loss: 0.9798847436904907


Running epoch 1, step 1335, batch 287
Sampled inputs[:2]: tensor([[    0, 33792,   352,  ...,   278,   546, 30495],
        [    0,  1034,  5599,  ...,   259,   586,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3375e-05, -1.8049e-04,  4.3078e-04,  ...,  1.4151e-04,
         -4.3008e-04, -2.4198e-06],
        [-1.1235e-05, -8.3968e-06,  8.9705e-06,  ..., -1.0177e-05,
         -7.4059e-06, -7.5847e-06],
        [-3.1099e-05, -2.4065e-05,  2.6196e-05,  ..., -2.7969e-05,
         -2.0444e-05, -2.0593e-05],
        [-2.3901e-05, -1.7509e-05,  1.9968e-05,  ..., -2.1636e-05,
         -1.5676e-05, -1.6928e-05],
        [-2.6494e-05, -2.1055e-05,  2.1905e-05,  ..., -2.4125e-05,
         -1.8463e-05, -1.6265e-05]], device='cuda:0')
Loss: 0.9934086203575134
Graident accumulation at epoch 1, step 1335, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9935e-05,  4.9304e-05, -1.0614e-05,  ..., -8.1457e-05,
          4.0454e-05, -3.9927e-05],
        [-1.0801e-05, -7.2064e-06,  6.7646e-06,  ..., -9.4584e-06,
         -6.6985e-06, -7.6270e-06],
        [-2.2058e-05, -1.2931e-05,  1.7377e-05,  ..., -1.8584e-05,
         -9.9894e-06, -1.6484e-05],
        [-2.5525e-06,  1.0785e-05,  3.4189e-06,  ..., -3.4308e-06,
          6.4713e-06, -4.8468e-06],
        [-1.7513e-05, -1.1365e-05,  1.2886e-05,  ..., -1.5052e-05,
         -1.0034e-05, -1.4429e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3223e-08, 5.4915e-08, 6.5265e-08,  ..., 2.4955e-08, 1.5011e-07,
         3.7231e-08],
        [7.8577e-11, 5.1286e-11, 1.9455e-11,  ..., 5.4592e-11, 1.6183e-11,
         2.2600e-11],
        [3.4355e-09, 1.9916e-09, 1.0071e-09,  ..., 2.6439e-09, 8.5267e-10,
         9.0382e-10],
        [9.8728e-10, 9.2216e-10, 3.3583e-10,  ..., 8.2404e-10, 5.6634e-10,
         3.4019e-10],
        [3.9524e-10, 2.2360e-10, 7.6209e-11,  ..., 2.8867e-10, 8.1282e-11,
         1.0823e-10]], device='cuda:0')
optimizer state dict: 167.0
lr: [6.138644937102163e-06, 6.138644937102163e-06]
scheduler_last_epoch: 167


Running epoch 1, step 1336, batch 288
Sampled inputs[:2]: tensor([[    0, 12324,  7368,  ...,   365,   726,  3595],
        [    0,    16,    52,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1021e-05,  4.5727e-05, -2.5936e-05,  ...,  1.7047e-04,
          2.7777e-05, -2.7114e-05],
        [-1.4156e-06, -1.0952e-06,  1.1250e-06,  ..., -1.2964e-06,
         -9.4250e-07, -9.1642e-07],
        [ 5.2910e-05,  8.0548e-05, -3.9482e-05,  ...,  5.5916e-05,
          7.7934e-05, -1.4826e-06],
        [-3.0100e-06, -2.2948e-06,  2.4885e-06,  ..., -2.7716e-06,
         -1.9968e-06, -2.0713e-06],
        [-3.3379e-06, -2.7865e-06,  2.7418e-06,  ..., -3.0994e-06,
         -2.3544e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9935733675956726


Running epoch 1, step 1337, batch 289
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,   271,  3421,  ...,   306,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2264e-05,  3.3514e-05, -3.3827e-05,  ...,  2.5739e-04,
          2.2209e-04, -2.3624e-05],
        [-2.8461e-06, -2.2128e-06,  2.1607e-06,  ..., -2.6152e-06,
         -2.0005e-06, -1.9148e-06],
        [ 4.9006e-05,  7.7344e-05, -3.6413e-05,  ...,  5.2325e-05,
          7.5013e-05, -4.1350e-06],
        [-5.9903e-06, -4.6045e-06,  4.7684e-06,  ..., -5.5432e-06,
         -4.2319e-06, -4.2766e-06],
        [-6.7949e-06, -5.6773e-06,  5.4091e-06,  ..., -6.3032e-06,
         -5.0515e-06, -4.1425e-06]], device='cuda:0')
Loss: 0.9793941378593445


Running epoch 1, step 1338, batch 290
Sampled inputs[:2]: tensor([[    0,  3806,    13,  ..., 11786,  2254,   221],
        [    0,   281,   221,  ...,  2236, 15064,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1757e-05,  2.6157e-05, -1.3286e-04,  ...,  9.7679e-05,
          2.7246e-04, -3.4181e-05],
        [-4.3660e-06, -3.0994e-06,  3.2708e-06,  ..., -4.0010e-06,
         -2.9691e-06, -3.0622e-06],
        [ 4.5042e-05,  7.4870e-05, -3.3313e-05,  ...,  4.8764e-05,
          7.2525e-05, -7.0110e-06],
        [-9.1046e-06, -6.3404e-06,  7.1228e-06,  ..., -8.3745e-06,
         -6.1989e-06, -6.7353e-06],
        [-1.0505e-05, -8.0019e-06,  8.2254e-06,  ..., -9.6411e-06,
         -7.4953e-06, -6.6757e-06]], device='cuda:0')
Loss: 0.9577286243438721


Running epoch 1, step 1339, batch 291
Sampled inputs[:2]: tensor([[    0,    34,     9,  ...,    19,    14, 45576],
        [    0,   257,   298,  ...,  3768,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5308e-05, -2.7736e-05, -4.4005e-05,  ...,  2.6676e-04,
          3.4345e-04,  5.9543e-05],
        [-5.8785e-06, -3.9935e-06,  4.2468e-06,  ..., -5.4017e-06,
         -3.9302e-06, -4.3958e-06],
        [ 4.0929e-05,  7.2322e-05, -3.0408e-05,  ...,  4.5083e-05,
          6.9947e-05, -1.0423e-05],
        [-1.2100e-05, -8.0690e-06,  9.1344e-06,  ..., -1.1146e-05,
         -8.1062e-06, -9.4175e-06],
        [-1.4290e-05, -1.0327e-05,  1.0833e-05,  ..., -1.3024e-05,
         -9.9242e-06, -9.5963e-06]], device='cuda:0')
Loss: 0.9519385695457458


Running epoch 1, step 1340, batch 292
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0191e-04, -5.1066e-05, -1.3222e-04,  ...,  4.5089e-04,
          4.4194e-04,  1.3376e-04],
        [-7.3388e-06, -4.9621e-06,  5.2601e-06,  ..., -6.7279e-06,
         -4.9286e-06, -5.5209e-06],
        [ 3.7115e-05,  6.9655e-05, -2.7546e-05,  ...,  4.1686e-05,
          6.7354e-05, -1.3255e-05],
        [-1.5065e-05, -9.9912e-06,  1.1265e-05,  ..., -1.3828e-05,
         -1.0133e-05, -1.1772e-05],
        [-1.7896e-05, -1.2860e-05,  1.3471e-05,  ..., -1.6242e-05,
         -1.2487e-05, -1.2085e-05]], device='cuda:0')
Loss: 0.9650530815124512


Running epoch 1, step 1341, batch 293
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,  8122,  1387,   616],
        [    0,   271,   266,  ...,  2805,   607, 10848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8617e-04, -8.0879e-05, -1.6694e-04,  ...,  3.4443e-04,
          5.1523e-04,  1.5109e-04],
        [-8.8140e-06, -5.9977e-06,  6.4820e-06,  ..., -7.9870e-06,
         -5.7966e-06, -6.4820e-06],
        [ 3.3211e-05,  6.6734e-05, -2.4104e-05,  ...,  3.8333e-05,
          6.5030e-05, -1.5803e-05],
        [-1.8135e-05, -1.2107e-05,  1.3947e-05,  ..., -1.6451e-05,
         -1.1913e-05, -1.3873e-05],
        [-2.1279e-05, -1.5482e-05,  1.6361e-05,  ..., -1.9208e-05,
         -1.4678e-05, -1.4141e-05]], device='cuda:0')
Loss: 0.9615854620933533


Running epoch 1, step 1342, batch 294
Sampled inputs[:2]: tensor([[    0, 15411,  4286,  ...,  3337,   300,  2257],
        [    0,  9509, 21000,  ...,  1953,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3069e-04, -2.7358e-05, -2.6753e-04,  ...,  4.7525e-04,
          2.8198e-04,  1.8136e-04],
        [-1.0237e-05, -7.0855e-06,  7.6890e-06,  ..., -9.2685e-06,
         -6.7651e-06, -7.4245e-06],
        [ 2.9217e-05,  6.3605e-05, -2.0543e-05,  ...,  3.4772e-05,
          6.2347e-05, -1.8425e-05],
        [-2.1160e-05, -1.4357e-05,  1.6630e-05,  ..., -1.9148e-05,
         -1.3940e-05, -1.5974e-05],
        [-2.4542e-05, -1.8120e-05,  1.9208e-05,  ..., -2.2158e-05,
         -1.7017e-05, -1.6093e-05]], device='cuda:0')
Loss: 0.9676576852798462


Running epoch 1, step 1343, batch 295
Sampled inputs[:2]: tensor([[    0,   278,   264,  ..., 21836,   344,   259],
        [    0,    20,  2637,  ..., 14044,     9,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7000e-04,  1.0849e-04, -1.6585e-04,  ...,  6.9178e-04,
          3.5173e-04,  4.1819e-04],
        [-1.1690e-05, -8.1956e-06,  8.8736e-06,  ..., -1.0587e-05,
         -7.7486e-06, -8.4303e-06],
        [ 2.5253e-05,  6.0446e-05, -1.7145e-05,  ...,  3.1180e-05,
          5.9650e-05, -2.1152e-05],
        [-2.4244e-05, -1.6697e-05,  1.9252e-05,  ..., -2.1949e-05,
         -1.6026e-05, -1.8209e-05],
        [-2.7895e-05, -2.0862e-05,  2.2009e-05,  ..., -2.5243e-05,
         -1.9416e-05, -1.8254e-05]], device='cuda:0')
Loss: 0.9464451670646667
Graident accumulation at epoch 1, step 1343, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.0577e-06,  5.5222e-05, -2.6137e-05,  ..., -4.1327e-06,
          7.1582e-05,  5.8847e-06],
        [-1.0890e-05, -7.3053e-06,  6.9755e-06,  ..., -9.5713e-06,
         -6.8035e-06, -7.7073e-06],
        [-1.7327e-05, -5.5930e-06,  1.3925e-05,  ..., -1.3608e-05,
         -3.0254e-06, -1.6951e-05],
        [-4.7217e-06,  8.0365e-06,  5.0023e-06,  ..., -5.2827e-06,
          4.2216e-06, -6.1830e-06],
        [-1.8551e-05, -1.2315e-05,  1.3798e-05,  ..., -1.6071e-05,
         -1.0972e-05, -1.4811e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3243e-08, 5.4872e-08, 6.5227e-08,  ..., 2.5409e-08, 1.5009e-07,
         3.7369e-08],
        [7.8635e-11, 5.1302e-11, 1.9514e-11,  ..., 5.4649e-11, 1.6227e-11,
         2.2649e-11],
        [3.4327e-09, 1.9933e-09, 1.0063e-09,  ..., 2.6422e-09, 8.5537e-10,
         9.0336e-10],
        [9.8688e-10, 9.2152e-10, 3.3587e-10,  ..., 8.2370e-10, 5.6603e-10,
         3.4018e-10],
        [3.9562e-10, 2.2381e-10, 7.6617e-11,  ..., 2.8902e-10, 8.1577e-11,
         1.0846e-10]], device='cuda:0')
optimizer state dict: 168.0
lr: [6.0249136994947676e-06, 6.0249136994947676e-06]
scheduler_last_epoch: 168


Running epoch 1, step 1344, batch 296
Sampled inputs[:2]: tensor([[    0,   531,    20,  ...,    12,  1644,   680],
        [    0,  1869,   596,  ..., 13055, 17051,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0057e-05, -8.9241e-05, -1.0956e-04,  ...,  2.2140e-05,
         -2.4165e-04,  1.6882e-05],
        [-1.4752e-06, -1.0803e-06,  1.2740e-06,  ..., -1.2442e-06,
         -9.3505e-07, -8.7917e-07],
        [-4.0233e-06, -3.0696e-06,  3.6508e-06,  ..., -3.3975e-06,
         -2.5630e-06, -2.3693e-06],
        [-3.0845e-06, -2.2054e-06,  2.7716e-06,  ..., -2.5928e-06,
         -1.9222e-06, -1.9222e-06],
        [-3.3379e-06, -2.6375e-06,  2.9653e-06,  ..., -2.8610e-06,
         -2.2650e-06, -1.8105e-06]], device='cuda:0')
Loss: 0.9541959762573242


Running epoch 1, step 1345, batch 297
Sampled inputs[:2]: tensor([[    0,    14,  3228,  ..., 13747,   287, 20295],
        [    0,   221,   474,  ...,   287,   271,  2540]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4624e-04,  9.3248e-05, -6.5909e-05,  ...,  8.3520e-05,
         -6.3419e-04,  1.0151e-04],
        [-2.9281e-06, -2.1085e-06,  2.4810e-06,  ..., -2.4661e-06,
         -1.8850e-06, -1.7881e-06],
        [ 1.9277e-04,  2.8057e-04, -1.7591e-04,  ...,  2.4173e-04,
          2.1733e-04,  1.0797e-04],
        [-6.1691e-06, -4.3213e-06,  5.4389e-06,  ..., -5.1707e-06,
         -3.9041e-06, -3.9339e-06],
        [-6.7353e-06, -5.2154e-06,  5.8562e-06,  ..., -5.7369e-06,
         -4.5896e-06, -3.7327e-06]], device='cuda:0')
Loss: 0.9619227051734924


Running epoch 1, step 1346, batch 298
Sampled inputs[:2]: tensor([[    0,   494,   360,  ...,   391, 24104, 35211],
        [    0,    12, 32425,  ...,   389,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4747e-04,  1.8945e-04, -1.4777e-04,  ..., -8.3447e-06,
         -6.5919e-04,  4.1059e-05],
        [-4.5896e-06, -3.1441e-06,  3.5539e-06,  ..., -3.9265e-06,
         -2.8983e-06, -3.0249e-06],
        [ 1.8851e-04,  2.7774e-04, -1.7295e-04,  ...,  2.3801e-04,
          2.1466e-04,  1.0484e-04],
        [-9.3132e-06, -6.2734e-06,  7.5549e-06,  ..., -7.9274e-06,
         -5.8413e-06, -6.3330e-06],
        [-1.0699e-05, -7.8976e-06,  8.5682e-06,  ..., -9.2387e-06,
         -7.2122e-06, -6.4448e-06]], device='cuda:0')
Loss: 0.9452206492424011


Running epoch 1, step 1347, batch 299
Sampled inputs[:2]: tensor([[    0,  6184,  1412,  ...,    12,   266,   944],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1087e-04,  2.4988e-04, -6.5815e-05,  ...,  8.2187e-05,
         -6.5409e-04,  1.0179e-04],
        [-6.0350e-06, -4.2841e-06,  4.6566e-06,  ..., -5.2378e-06,
         -3.9637e-06, -4.0159e-06],
        [ 1.8451e-04,  2.7449e-04, -1.6974e-04,  ...,  2.3440e-04,
          2.1177e-04,  1.0213e-04],
        [-1.2368e-05, -8.6278e-06,  9.9689e-06,  ..., -1.0699e-05,
         -8.0615e-06, -8.5384e-06],
        [-1.4201e-05, -1.0803e-05,  1.1340e-05,  ..., -1.2442e-05,
         -9.8795e-06, -8.6650e-06]], device='cuda:0')
Loss: 0.9760224223136902


Running epoch 1, step 1348, batch 300
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8246e-04,  2.4882e-04, -1.2314e-04,  ...,  2.3999e-04,
         -7.1133e-04,  2.1154e-04],
        [-7.4431e-06, -5.3570e-06,  5.8934e-06,  ..., -6.4895e-06,
         -4.8876e-06, -4.9248e-06],
        [ 5.1534e-04,  4.9922e-04, -4.7460e-04,  ...,  6.2186e-04,
          5.4824e-04,  3.3788e-04],
        [-1.5378e-05, -1.0878e-05,  1.2740e-05,  ..., -1.3381e-05,
         -1.0028e-05, -1.0595e-05],
        [-1.7494e-05, -1.3486e-05,  1.4290e-05,  ..., -1.5408e-05,
         -1.2189e-05, -1.0632e-05]], device='cuda:0')
Loss: 0.9776230454444885


Running epoch 1, step 1349, batch 301
Sampled inputs[:2]: tensor([[   0,  271, 2862,  ...,  287, 5699,   18],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6678e-04,  1.8912e-04, -2.0633e-05,  ...,  2.1305e-04,
         -8.3690e-04,  1.5631e-04],
        [-8.9705e-06, -6.5118e-06,  7.0557e-06,  ..., -7.8604e-06,
         -5.9903e-06, -5.8860e-06],
        [ 5.1104e-04,  4.9584e-04, -4.7118e-04,  ...,  6.1798e-04,
          5.4510e-04,  3.3516e-04],
        [-1.8552e-05, -1.3262e-05,  1.5244e-05,  ..., -1.6227e-05,
         -1.2323e-05, -1.2681e-05],
        [-2.1279e-05, -1.6555e-05,  1.7270e-05,  ..., -1.8880e-05,
         -1.5110e-05, -1.2897e-05]], device='cuda:0')
Loss: 0.986463725566864


Running epoch 1, step 1350, batch 302
Sampled inputs[:2]: tensor([[    0, 20241,  1244,  ...,  6232,  1004,   300],
        [    0,  7377, 30662,  ...,   287,   694, 13403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7638e-04,  3.0094e-04,  6.8512e-05,  ...,  2.2329e-04,
         -7.6461e-04,  1.9663e-04],
        [-1.0394e-05, -7.4245e-06,  8.0764e-06,  ..., -9.1493e-06,
         -6.8992e-06, -6.8992e-06],
        [ 5.0723e-04,  4.9328e-04, -4.6823e-04,  ...,  6.1459e-04,
          5.4268e-04,  3.3252e-04],
        [-2.1502e-05, -1.5080e-05,  1.7479e-05,  ..., -1.8910e-05,
         -1.4216e-05, -1.4871e-05],
        [-2.4751e-05, -1.8954e-05,  1.9893e-05,  ..., -2.1964e-05,
         -1.7419e-05, -1.5102e-05]], device='cuda:0')
Loss: 0.9504150152206421


Running epoch 1, step 1351, batch 303
Sampled inputs[:2]: tensor([[   0, 3352,  259,  ..., 3565,   12,  409],
        [   0, 5105,  271,  ...,  308, 3056, 3640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3602e-04,  2.9043e-04, -9.6743e-05,  ...,  2.2107e-04,
         -8.9524e-04,  2.7147e-06],
        [-1.1861e-05, -8.5197e-06,  9.2983e-06,  ..., -1.0453e-05,
         -7.8417e-06, -7.9051e-06],
        [ 5.0318e-04,  4.9013e-04, -4.6467e-04,  ...,  6.1098e-04,
          5.4008e-04,  3.2979e-04],
        [-2.4512e-05, -1.7270e-05,  2.0087e-05,  ..., -2.1562e-05,
         -1.6108e-05, -1.7002e-05],
        [-2.8238e-05, -2.1726e-05,  2.2873e-05,  ..., -2.5108e-05,
         -1.9819e-05, -1.7278e-05]], device='cuda:0')
Loss: 0.9652458429336548
Graident accumulation at epoch 1, step 1351, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.5451e-05,  7.8743e-05, -3.3198e-05,  ...,  1.8387e-05,
         -2.5100e-05,  5.5677e-06],
        [-1.0987e-05, -7.4268e-06,  7.2077e-06,  ..., -9.6595e-06,
         -6.9074e-06, -7.7271e-06],
        [ 3.4723e-05,  4.3980e-05, -3.3935e-05,  ...,  4.8851e-05,
          5.1285e-05,  1.7723e-05],
        [-6.7007e-06,  5.5058e-06,  6.5107e-06,  ..., -6.9106e-06,
          2.1886e-06, -7.2649e-06],
        [-1.9520e-05, -1.3256e-05,  1.4706e-05,  ..., -1.6975e-05,
         -1.1857e-05, -1.5058e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3303e-08, 5.4901e-08, 6.5171e-08,  ..., 2.5432e-08, 1.5074e-07,
         3.7331e-08],
        [7.8697e-11, 5.1323e-11, 1.9581e-11,  ..., 5.4704e-11, 1.6272e-11,
         2.2689e-11],
        [3.6825e-09, 2.2315e-09, 1.2213e-09,  ..., 3.0129e-09, 1.1462e-09,
         1.0112e-09],
        [9.8649e-10, 9.2090e-10, 3.3594e-10,  ..., 8.2334e-10, 5.6573e-10,
         3.4013e-10],
        [3.9602e-10, 2.2406e-10, 7.7064e-11,  ..., 2.8936e-10, 8.1889e-11,
         1.0865e-10]], device='cuda:0')
optimizer state dict: 169.0
lr: [5.9117898897731115e-06, 5.9117898897731115e-06]
scheduler_last_epoch: 169


Running epoch 1, step 1352, batch 304
Sampled inputs[:2]: tensor([[    0, 11853,  1611,  ...,  4413,  4240,   278],
        [    0,   278, 30377,  ...,    13,    83,  2908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2883e-05, -7.5901e-05,  6.7555e-05,  ...,  1.6387e-04,
          7.1959e-05, -1.4173e-05],
        [-1.5199e-06, -1.0431e-06,  1.1474e-06,  ..., -1.2815e-06,
         -9.6112e-07, -9.3505e-07],
        [-4.1425e-06, -2.9504e-06,  3.2634e-06,  ..., -3.4720e-06,
         -2.5928e-06, -2.5034e-06],
        [-3.1590e-06, -2.1011e-06,  2.4736e-06,  ..., -2.6375e-06,
         -1.9521e-06, -2.0415e-06],
        [-3.6061e-06, -2.6524e-06,  2.8014e-06,  ..., -3.0696e-06,
         -2.3991e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9785276651382446


Running epoch 1, step 1353, batch 305
Sampled inputs[:2]: tensor([[    0,  6957,   271,  ...,  9094,   266,  4320],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2594e-05, -1.1212e-04,  9.9321e-05,  ...,  2.9347e-04,
         -4.5654e-05,  2.0133e-05],
        [-2.9802e-06, -2.1234e-06,  2.4140e-06,  ..., -2.5406e-06,
         -1.8515e-06, -1.8589e-06],
        [-8.1360e-06, -5.9903e-06,  6.8843e-06,  ..., -6.9290e-06,
         -5.0366e-06, -5.0366e-06],
        [-6.1691e-06, -4.2766e-06,  5.1707e-06,  ..., -5.2303e-06,
         -3.7551e-06, -4.0233e-06],
        [-7.0333e-06, -5.3495e-06,  5.8264e-06,  ..., -6.0797e-06,
         -4.6492e-06, -4.0233e-06]], device='cuda:0')
Loss: 0.99913489818573


Running epoch 1, step 1354, batch 306
Sampled inputs[:2]: tensor([[    0,  1927,   287,  ...,  1027,   271,   266],
        [    0,  2823,   287,  ...,  3504,     9, 13910]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5804e-05, -1.7578e-04,  1.7533e-04,  ...,  3.0731e-04,
         -9.8929e-05, -1.0569e-04],
        [-4.4182e-06, -3.1441e-06,  3.5688e-06,  ..., -3.8669e-06,
         -2.8573e-06, -2.8275e-06],
        [-1.1981e-05, -8.8215e-06,  1.0148e-05,  ..., -1.0416e-05,
         -7.6443e-06, -7.5698e-06],
        [-9.1344e-06, -6.3479e-06,  7.6592e-06,  ..., -7.9721e-06,
         -5.8115e-06, -6.1393e-06],
        [-1.0431e-05, -7.8976e-06,  8.6427e-06,  ..., -9.1791e-06,
         -7.0781e-06, -6.0797e-06]], device='cuda:0')
Loss: 0.9723834991455078


Running epoch 1, step 1355, batch 307
Sampled inputs[:2]: tensor([[   0, 3933, 6394,  ..., 1364,  950,  847],
        [   0,   14,  759,  ..., 2540, 1323,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2155e-05, -1.6784e-04,  2.9081e-04,  ...,  3.0103e-04,
         -7.3368e-05, -9.5722e-05],
        [-5.9530e-06, -4.2468e-06,  4.7833e-06,  ..., -5.2154e-06,
         -3.8855e-06, -3.7588e-06],
        [ 1.2139e-04,  1.5751e-04, -1.3048e-04,  ...,  1.4826e-04,
          1.1425e-04,  5.1889e-05],
        [-1.2293e-05, -8.6129e-06,  1.0267e-05,  ..., -1.0744e-05,
         -7.9274e-06, -8.1658e-06],
        [-1.3992e-05, -1.0729e-05,  1.1563e-05,  ..., -1.2398e-05,
         -9.6709e-06, -8.1360e-06]], device='cuda:0')
Loss: 0.9782682657241821


Running epoch 1, step 1356, batch 308
Sampled inputs[:2]: tensor([[    0, 10446,    14,  ...,   266,  1164,   287],
        [    0,   809,   367,  ...,   717,   287,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2360e-04, -6.3915e-05,  3.0320e-04,  ...,  3.4755e-04,
          8.7016e-05, -9.9912e-05],
        [-7.5027e-06, -5.2676e-06,  5.8115e-06,  ..., -6.6087e-06,
         -4.8690e-06, -4.8690e-06],
        [ 1.1743e-04,  1.5470e-04, -1.2759e-04,  ...,  1.4472e-04,
          1.1166e-04,  4.9103e-05],
        [-1.5408e-05, -1.0654e-05,  1.2457e-05,  ..., -1.3560e-05,
         -9.9391e-06, -1.0490e-05],
        [-1.7628e-05, -1.3322e-05,  1.4156e-05,  ..., -1.5676e-05,
         -1.2130e-05, -1.0505e-05]], device='cuda:0')
Loss: 0.939315140247345


Running epoch 1, step 1357, batch 309
Sampled inputs[:2]: tensor([[  0, 360, 259,  ...,  12, 358,  19],
        [  0, 504, 546,  ..., 634, 328, 630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0581e-04, -5.2316e-06,  3.4983e-04,  ...,  4.3995e-04,
         -2.2103e-05, -9.9076e-05],
        [-9.0376e-06, -6.4522e-06,  7.1004e-06,  ..., -7.9647e-06,
         -5.8226e-06, -5.7630e-06],
        [ 1.1334e-04,  1.5136e-04, -1.2396e-04,  ...,  1.4105e-04,
          1.0900e-04,  4.6689e-05],
        [-1.8552e-05, -1.3083e-05,  1.5244e-05,  ..., -1.6361e-05,
         -1.1921e-05, -1.2428e-05],
        [-2.1070e-05, -1.6257e-05,  1.7151e-05,  ..., -1.8820e-05,
         -1.4544e-05, -1.2383e-05]], device='cuda:0')
Loss: 1.0066946744918823


Running epoch 1, step 1358, batch 310
Sampled inputs[:2]: tensor([[    0,    14,    71,  ...,   278, 14258, 12440],
        [    0,   894,   496,  ...,   266,   623,   587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2567e-04, -8.2860e-05,  3.7204e-04,  ...,  3.4208e-04,
          2.5947e-06, -9.9076e-05],
        [-1.0684e-05, -7.4878e-06,  8.2701e-06,  ..., -9.3430e-06,
         -6.8061e-06, -6.9179e-06],
        [ 1.0893e-04,  1.4838e-04, -1.2062e-04,  ...,  1.3731e-04,
          1.0629e-04,  4.3545e-05],
        [-2.1815e-05, -1.5154e-05,  1.7703e-05,  ..., -1.9118e-05,
         -1.3873e-05, -1.4782e-05],
        [-2.5094e-05, -1.9014e-05,  2.0072e-05,  ..., -2.2277e-05,
         -1.7166e-05, -1.5125e-05]], device='cuda:0')
Loss: 0.9452416300773621


Running epoch 1, step 1359, batch 311
Sampled inputs[:2]: tensor([[   0, 2734, 2703,  ..., 7851,  280, 1713],
        [   0, 2341, 7956,  ..., 2355,  413,   72]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8941e-04, -1.6963e-04,  3.5570e-04,  ...,  3.6247e-04,
          5.8561e-05, -4.4864e-05],
        [-1.2219e-05, -8.6054e-06,  9.4473e-06,  ..., -1.0662e-05,
         -7.8492e-06, -7.8529e-06],
        [ 1.0470e-04,  1.4518e-04, -1.1719e-04,  ...,  1.3366e-04,
          1.0340e-04,  4.0952e-05],
        [ 6.4981e-05,  6.5347e-05, -3.5193e-05,  ...,  6.1159e-05,
          6.4329e-05, -1.5575e-05],
        [-2.8759e-05, -2.1890e-05,  2.2992e-05,  ..., -2.5481e-05,
         -1.9819e-05, -1.7226e-05]], device='cuda:0')
Loss: 0.9964084625244141
Graident accumulation at epoch 1, step 1359, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.1846e-05,  5.3906e-05,  5.6921e-06,  ...,  5.2796e-05,
         -1.6734e-05,  5.2456e-07],
        [-1.1110e-05, -7.5446e-06,  7.4317e-06,  ..., -9.7597e-06,
         -7.0015e-06, -7.7397e-06],
        [ 4.1721e-05,  5.4099e-05, -4.2261e-05,  ...,  5.7332e-05,
          5.6497e-05,  2.0046e-05],
        [ 4.6742e-07,  1.1490e-05,  2.3403e-06,  ..., -1.0366e-07,
          8.4026e-06, -8.0960e-06],
        [-2.0444e-05, -1.4119e-05,  1.5534e-05,  ..., -1.7826e-05,
         -1.2653e-05, -1.5275e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3489e-08, 5.4875e-08, 6.5233e-08,  ..., 2.5538e-08, 1.5059e-07,
         3.7296e-08],
        [7.8767e-11, 5.1346e-11, 1.9651e-11,  ..., 5.4763e-11, 1.6317e-11,
         2.2728e-11],
        [3.6898e-09, 2.2504e-09, 1.2338e-09,  ..., 3.0277e-09, 1.1557e-09,
         1.0119e-09],
        [9.8973e-10, 9.2425e-10, 3.3684e-10,  ..., 8.2625e-10, 5.6930e-10,
         3.4003e-10],
        [3.9645e-10, 2.2432e-10, 7.7515e-11,  ..., 2.8972e-10, 8.2200e-11,
         1.0883e-10]], device='cuda:0')
optimizer state dict: 170.0
lr: [5.799290794242787e-06, 5.799290794242787e-06]
scheduler_last_epoch: 170


Running epoch 1, step 1360, batch 312
Sampled inputs[:2]: tensor([[    0, 23842,   342,  ...,   365,  4011, 10151],
        [    0,   790, 43134,  ...,   446,   381,  1034]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4568e-06, -4.5135e-06,  5.2741e-05,  ..., -2.5121e-05,
          0.0000e+00,  8.1259e-05],
        [-1.4380e-06, -1.1027e-06,  1.1250e-06,  ..., -1.2964e-06,
         -1.0356e-06, -9.3132e-07],
        [-3.9041e-06, -3.1292e-06,  3.2485e-06,  ..., -3.5167e-06,
         -2.8312e-06, -2.5183e-06],
        [-2.9653e-06, -2.2650e-06,  2.4289e-06,  ..., -2.6822e-06,
         -2.1458e-06, -2.0415e-06],
        [-3.4422e-06, -2.8312e-06,  2.8163e-06,  ..., -3.1441e-06,
         -2.6226e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9979743361473083


Running epoch 1, step 1361, batch 313
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,   300,   259,  ...,   352, 12080,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2343e-04,  1.9303e-05,  1.3289e-04,  ...,  5.8508e-05,
          7.3578e-05,  1.7307e-04],
        [-2.9206e-06, -2.1681e-06,  2.3022e-06,  ..., -2.6301e-06,
         -2.0042e-06, -1.9297e-06],
        [-7.8380e-06, -6.0946e-06,  6.5714e-06,  ..., -7.0482e-06,
         -5.4389e-06, -5.1260e-06],
        [-6.0350e-06, -4.4405e-06,  4.9770e-06,  ..., -5.4389e-06,
         -4.1574e-06, -4.1872e-06],
        [-6.8694e-06, -5.4985e-06,  5.6624e-06,  ..., -6.2585e-06,
         -5.0366e-06, -4.1574e-06]], device='cuda:0')
Loss: 0.9713543653488159


Running epoch 1, step 1362, batch 314
Sampled inputs[:2]: tensor([[    0,   650,    14,  ...,  3687,   278, 26952],
        [    0,   668,  2474,  ...,   668,  4599,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1121e-05, -1.6420e-05,  2.6831e-04,  ...,  2.5046e-05,
          4.4498e-05,  1.5845e-04],
        [-4.4405e-06, -3.2261e-06,  3.4571e-06,  ..., -3.9563e-06,
         -2.9467e-06, -2.9579e-06],
        [-1.1981e-05, -9.1195e-06,  9.9093e-06,  ..., -1.0654e-05,
         -8.0317e-06, -7.9125e-06],
        [-9.1195e-06, -6.5565e-06,  7.4357e-06,  ..., -8.1360e-06,
         -6.0499e-06, -6.3628e-06],
        [-1.0431e-05, -8.1509e-06,  8.4341e-06,  ..., -9.4026e-06,
         -7.4059e-06, -6.3926e-06]], device='cuda:0')
Loss: 0.9534623622894287


Running epoch 1, step 1363, batch 315
Sampled inputs[:2]: tensor([[    0,  7333,   342,  ...,    13,  1818,  6183],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0171e-05,  2.9705e-05,  2.3760e-04,  ..., -7.7668e-05,
         -9.3935e-05,  1.7164e-04],
        [-5.8562e-06, -4.2245e-06,  4.6715e-06,  ..., -5.2154e-06,
         -3.8557e-06, -3.8408e-06],
        [-1.5885e-05, -1.1951e-05,  1.3441e-05,  ..., -1.4096e-05,
         -1.0535e-05, -1.0267e-05],
        [-1.2159e-05, -8.6129e-06,  1.0163e-05,  ..., -1.0818e-05,
         -7.9721e-06, -8.3447e-06],
        [-1.3679e-05, -1.0565e-05,  1.1310e-05,  ..., -1.2279e-05,
         -9.5963e-06, -8.1882e-06]], device='cuda:0')
Loss: 0.9634928703308105


Running epoch 1, step 1364, batch 316
Sampled inputs[:2]: tensor([[   0,  471,  590,  ..., 5007,   13, 2920],
        [   0,  199, 7513,  ...,  271,  259,  957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1583e-05,  1.2249e-04,  3.4942e-04,  ..., -2.5513e-04,
          1.0522e-04,  3.9111e-04],
        [-7.5400e-06, -5.2378e-06,  5.7146e-06,  ..., -6.6012e-06,
         -4.8839e-06, -5.0403e-06],
        [-2.0266e-05, -1.4886e-05,  1.6421e-05,  ..., -1.7762e-05,
         -1.3366e-05, -1.3351e-05],
        [-1.5467e-05, -1.0610e-05,  1.2323e-05,  ..., -1.3575e-05,
         -1.0058e-05, -1.0774e-05],
        [-1.7643e-05, -1.3247e-05,  1.3903e-05,  ..., -1.5631e-05,
         -1.2279e-05, -1.0811e-05]], device='cuda:0')
Loss: 0.9174475073814392


Running epoch 1, step 1365, batch 317
Sampled inputs[:2]: tensor([[    0,  2270,   278,  ..., 36325,  5892,  3558],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1611e-06,  1.3142e-04,  2.6312e-04,  ..., -2.5088e-04,
          2.7012e-04,  5.0210e-04],
        [-9.0227e-06, -6.3181e-06,  6.9663e-06,  ..., -7.8827e-06,
         -5.8115e-06, -5.9269e-06],
        [-2.4259e-05, -1.7926e-05,  1.9982e-05,  ..., -2.1219e-05,
         -1.5900e-05, -1.5721e-05],
        [-1.8507e-05, -1.2800e-05,  1.5020e-05,  ..., -1.6212e-05,
         -1.1958e-05, -1.2696e-05],
        [-2.1085e-05, -1.5959e-05,  1.6898e-05,  ..., -1.8656e-05,
         -1.4603e-05, -1.2688e-05]], device='cuda:0')
Loss: 0.984116792678833


Running epoch 1, step 1366, batch 318
Sampled inputs[:2]: tensor([[    0, 17694,    12,  ..., 12452,   446,   475],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1681e-05,  1.9048e-04,  2.3052e-04,  ..., -3.1383e-04,
          8.6447e-05,  4.7592e-04],
        [-1.0453e-05, -7.3016e-06,  8.1733e-06,  ..., -9.1866e-06,
         -6.7800e-06, -6.8173e-06],
        [-2.7895e-05, -2.0564e-05,  2.3246e-05,  ..., -2.4512e-05,
         -1.8373e-05, -1.7911e-05],
        [-2.1398e-05, -1.4752e-05,  1.7598e-05,  ..., -1.8865e-05,
         -1.3925e-05, -1.4596e-05],
        [-2.4304e-05, -1.8388e-05,  1.9714e-05,  ..., -2.1607e-05,
         -1.6928e-05, -1.4462e-05]], device='cuda:0')
Loss: 0.9577931761741638


Running epoch 1, step 1367, batch 319
Sampled inputs[:2]: tensor([[   0,  894,   16,  ...,  892,  300,  722],
        [   0, 2827, 5744,  ...,  365,  513,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4240e-04,  1.7407e-04,  3.4703e-04,  ..., -2.2981e-04,
          2.1469e-04,  6.3680e-04],
        [-1.1943e-05, -8.3521e-06,  9.2387e-06,  ..., -1.0528e-05,
         -7.8157e-06, -7.9200e-06],
        [-3.2008e-05, -2.3589e-05,  2.6390e-05,  ..., -2.8178e-05,
         -2.1249e-05, -2.0862e-05],
        [-2.4483e-05, -1.6898e-05,  1.9908e-05,  ..., -2.1636e-05,
         -1.6086e-05, -1.6950e-05],
        [-2.7999e-05, -2.1115e-05,  2.2486e-05,  ..., -2.4915e-05,
         -1.9610e-05, -1.6920e-05]], device='cuda:0')
Loss: 0.9596637487411499
Graident accumulation at epoch 1, step 1367, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.0421e-05,  6.5923e-05,  3.9826e-05,  ...,  2.4535e-05,
          6.4089e-06,  6.4152e-05],
        [-1.1193e-05, -7.6254e-06,  7.6124e-06,  ..., -9.8365e-06,
         -7.0829e-06, -7.7577e-06],
        [ 3.4348e-05,  4.6330e-05, -3.5396e-05,  ...,  4.8781e-05,
          4.8722e-05,  1.5956e-05],
        [-2.0276e-06,  8.6512e-06,  4.0971e-06,  ..., -2.2569e-06,
          5.9538e-06, -8.9814e-06],
        [-2.1199e-05, -1.4819e-05,  1.6229e-05,  ..., -1.8534e-05,
         -1.3349e-05, -1.5439e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3494e-08, 5.4851e-08, 6.5288e-08,  ..., 2.5566e-08, 1.5049e-07,
         3.7664e-08],
        [7.8831e-11, 5.1364e-11, 1.9716e-11,  ..., 5.4819e-11, 1.6362e-11,
         2.2768e-11],
        [3.6871e-09, 2.2487e-09, 1.2332e-09,  ..., 3.0255e-09, 1.1550e-09,
         1.0113e-09],
        [9.8934e-10, 9.2361e-10, 3.3690e-10,  ..., 8.2590e-10, 5.6899e-10,
         3.3998e-10],
        [3.9684e-10, 2.2454e-10, 7.7943e-11,  ..., 2.9006e-10, 8.2502e-11,
         1.0901e-10]], device='cuda:0')
optimizer state dict: 171.0
lr: [5.687433603747612e-06, 5.687433603747612e-06]
scheduler_last_epoch: 171
Epoch 1 | Batch 319/1048 | Training PPL: 2158.8047476051756 | time 29.875121355056763
Saving checkpoint at epoch 1, step 1367, batch 319
Epoch 1 | Validation PPL: 6.899443188453456 | Learning rate: 5.687433603747612e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1367, AFTER epoch 1, step 1367


Running epoch 1, step 1368, batch 320
Sampled inputs[:2]: tensor([[   0,  767, 1615,  ..., 2952, 1760,    9],
        [   0,  996, 2226,  ...,  516, 3470,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2235e-05, -2.3065e-05, -6.3181e-06,  ..., -6.1453e-05,
          1.1462e-04,  8.5510e-05],
        [-1.4976e-06, -1.1623e-06,  1.3188e-06,  ..., -1.3188e-06,
         -9.6858e-07, -8.6054e-07],
        [-4.0829e-06, -3.3081e-06,  3.7551e-06,  ..., -3.6210e-06,
         -2.7120e-06, -2.3693e-06],
        [-3.1441e-06, -2.4140e-06,  2.8908e-06,  ..., -2.7716e-06,
         -2.0415e-06, -1.8924e-06],
        [-3.3975e-06, -2.8759e-06,  3.0696e-06,  ..., -3.0696e-06,
         -2.4289e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.9823768734931946


Running epoch 1, step 1369, batch 321
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,  2025,   287,  ...,   381,  1487,  3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3437e-05, -7.2309e-05,  1.2661e-04,  ..., -9.1007e-06,
          2.0565e-04,  1.0949e-04],
        [-2.8461e-06, -2.3097e-06,  2.4363e-06,  ..., -2.6077e-06,
         -1.9819e-06, -1.6987e-06],
        [-7.8976e-06, -6.6757e-06,  7.0930e-06,  ..., -7.2569e-06,
         -5.5730e-06, -4.7535e-06],
        [-6.0201e-06, -4.8131e-06,  5.3793e-06,  ..., -5.4985e-06,
         -4.1723e-06, -3.7998e-06],
        [-6.6459e-06, -5.8264e-06,  5.8413e-06,  ..., -6.2138e-06,
         -5.0217e-06, -3.7178e-06]], device='cuda:0')
Loss: 1.003962755203247


Running epoch 1, step 1370, batch 322
Sampled inputs[:2]: tensor([[   0, 1176,   13,  ..., 1919,  221,  380],
        [   0,   15, 2537,  ...,   14, 3544,  417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3030e-04, -4.7587e-05,  2.9713e-04,  ..., -8.7694e-05,
          2.5819e-04,  1.1309e-04],
        [-4.3362e-06, -3.4124e-06,  3.6210e-06,  ..., -3.9265e-06,
         -3.0175e-06, -2.6673e-06],
        [-1.1802e-05, -9.7305e-06,  1.0416e-05,  ..., -1.0744e-05,
         -8.3596e-06, -7.2867e-06],
        [-9.0152e-06, -7.0184e-06,  7.8976e-06,  ..., -8.1807e-06,
         -6.2883e-06, -5.8562e-06],
        [-1.0148e-05, -8.6427e-06,  8.7619e-06,  ..., -9.3877e-06,
         -7.6592e-06, -5.8338e-06]], device='cuda:0')
Loss: 0.9502729177474976


Running epoch 1, step 1371, batch 323
Sampled inputs[:2]: tensor([[    0,    14, 21687,  ...,   943,  2153,  4089],
        [    0,   598,   278,  ...,   437,   266,  2388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7272e-05,  1.3364e-04,  2.7976e-04,  ...,  1.7412e-05,
          2.6738e-04,  2.6590e-04],
        [-5.7742e-06, -4.4554e-06,  4.8578e-06,  ..., -5.2080e-06,
         -3.9265e-06, -3.6098e-06],
        [-1.5795e-05, -1.2785e-05,  1.4022e-05,  ..., -1.4320e-05,
         -1.0923e-05, -9.9093e-06],
        [-1.2025e-05, -9.1642e-06,  1.0595e-05,  ..., -1.0863e-05,
         -8.1807e-06, -7.9274e-06],
        [-1.3426e-05, -1.1235e-05,  1.1653e-05,  ..., -1.2383e-05,
         -9.9093e-06, -7.8306e-06]], device='cuda:0')
Loss: 0.9800031781196594


Running epoch 1, step 1372, batch 324
Sampled inputs[:2]: tensor([[   0,  334,  287,  ..., 1348, 6139,  342],
        [   0, 3377,   12,  ...,  333,  199,  769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7312e-06,  2.2959e-04,  3.4163e-04,  ..., -4.0532e-05,
          8.8544e-05,  7.2990e-05],
        [-7.2718e-06, -5.3197e-06,  6.0201e-06,  ..., -6.5044e-06,
         -4.8280e-06, -4.6231e-06],
        [-1.9670e-05, -1.5229e-05,  1.7256e-05,  ..., -1.7658e-05,
         -1.3292e-05, -1.2472e-05],
        [-1.5035e-05, -1.0893e-05,  1.3068e-05,  ..., -1.3486e-05,
         -1.0021e-05, -1.0028e-05],
        [-1.6913e-05, -1.3471e-05,  1.4469e-05,  ..., -1.5423e-05,
         -1.2174e-05, -9.9465e-06]], device='cuda:0')
Loss: 0.912827730178833


Running epoch 1, step 1373, batch 325
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    70,    27,  5311],
        [    0, 21930,    12,  ...,  2849,   863,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6322e-05,  2.7927e-04,  2.5071e-04,  ..., -1.8511e-05,
          9.0642e-05,  1.8516e-04],
        [-8.7097e-06, -6.4597e-06,  7.3165e-06,  ..., -7.7635e-06,
         -5.7891e-06, -5.4576e-06],
        [-2.3752e-05, -1.8597e-05,  2.1100e-05,  ..., -2.1249e-05,
         -1.6019e-05, -1.4901e-05],
        [-1.8179e-05, -1.3351e-05,  1.6034e-05,  ..., -1.6212e-05,
         -1.2077e-05, -1.1966e-05],
        [-2.0266e-05, -1.6347e-05,  1.7568e-05,  ..., -1.8433e-05,
         -1.4588e-05, -1.1787e-05]], device='cuda:0')
Loss: 1.002841830253601


Running epoch 1, step 1374, batch 326
Sampled inputs[:2]: tensor([[    0,    12, 12774,  ...,  1231,   278,   266],
        [    0,   298,  2230,  ...,  2300,  3698,  4764]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8291e-05,  3.2339e-04,  3.3854e-04,  ..., -1.5023e-04,
          1.1773e-04,  3.3855e-04],
        [-1.0215e-05, -7.5474e-06,  8.3223e-06,  ..., -9.1270e-06,
         -6.9067e-06, -6.5975e-06],
        [-2.7835e-05, -2.1711e-05,  2.4036e-05,  ..., -2.4945e-05,
         -1.9059e-05, -1.7956e-05],
        [-2.1249e-05, -1.5557e-05,  1.8150e-05,  ..., -1.8999e-05,
         -1.4372e-05, -1.4365e-05],
        [-2.4080e-05, -1.9282e-05,  2.0236e-05,  ..., -2.1890e-05,
         -1.7539e-05, -1.4439e-05]], device='cuda:0')
Loss: 0.9397827982902527


Running epoch 1, step 1375, batch 327
Sampled inputs[:2]: tensor([[   0,  825, 3066,  ..., 1184,  266, 7964],
        [   0, 2587,   27,  ...,  259, 2462, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3998e-05,  3.0569e-04,  1.7334e-04,  ..., -1.0411e-04,
          8.9978e-05,  3.6008e-04],
        [-1.1712e-05, -8.5086e-06,  9.4473e-06,  ..., -1.0476e-05,
         -7.8604e-06, -7.7076e-06],
        [-3.1948e-05, -2.4468e-05,  2.7329e-05,  ..., -2.8580e-05,
         -2.1651e-05, -2.0936e-05],
        [-2.4393e-05, -1.7524e-05,  2.0608e-05,  ..., -2.1815e-05,
         -1.6339e-05, -1.6764e-05],
        [-2.7686e-05, -2.1741e-05,  2.3052e-05,  ..., -2.5123e-05,
         -1.9953e-05, -1.6883e-05]], device='cuda:0')
Loss: 0.9814152717590332
Graident accumulation at epoch 1, step 1375, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.5779e-05,  8.9900e-05,  5.3177e-05,  ...,  1.1670e-05,
          1.4766e-05,  9.3746e-05],
        [-1.1245e-05, -7.7137e-06,  7.7959e-06,  ..., -9.9004e-06,
         -7.1607e-06, -7.7527e-06],
        [ 2.7719e-05,  3.9251e-05, -2.9123e-05,  ...,  4.1045e-05,
          4.1685e-05,  1.2266e-05],
        [-4.2641e-06,  6.0337e-06,  5.7482e-06,  ..., -4.2128e-06,
          3.7245e-06, -9.7596e-06],
        [-2.1848e-05, -1.5511e-05,  1.6912e-05,  ..., -1.9193e-05,
         -1.4009e-05, -1.5584e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3449e-08, 5.4889e-08, 6.5253e-08,  ..., 2.5551e-08, 1.5034e-07,
         3.7756e-08],
        [7.8890e-11, 5.1385e-11, 1.9786e-11,  ..., 5.4874e-11, 1.6407e-11,
         2.2804e-11],
        [3.6845e-09, 2.2470e-09, 1.2327e-09,  ..., 3.0233e-09, 1.1544e-09,
         1.0107e-09],
        [9.8895e-10, 9.2299e-10, 3.3699e-10,  ..., 8.2555e-10, 5.6869e-10,
         3.3992e-10],
        [3.9721e-10, 2.2479e-10, 7.8397e-11,  ..., 2.9040e-10, 8.2817e-11,
         1.0919e-10]], device='cuda:0')
optimizer state dict: 172.0
lr: [5.576235411042707e-06, 5.576235411042707e-06]
scheduler_last_epoch: 172


Running epoch 1, step 1376, batch 328
Sampled inputs[:2]: tensor([[   0,  616, 4935,  ...,   89, 4448,  271],
        [   0,  328, 6875,  ...,  369,  654,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5746e-05, -1.2287e-05,  8.6050e-05,  ..., -2.2979e-05,
          1.3838e-04, -2.1431e-06],
        [-1.4305e-06, -1.1474e-06,  1.1250e-06,  ..., -1.3113e-06,
         -1.1250e-06, -9.0525e-07],
        [-3.9339e-06, -3.2634e-06,  3.3081e-06,  ..., -3.6061e-06,
         -3.1143e-06, -2.4885e-06],
        [-2.9504e-06, -2.3395e-06,  2.4438e-06,  ..., -2.7120e-06,
         -2.3246e-06, -1.9670e-06],
        [-3.4571e-06, -2.9653e-06,  2.8461e-06,  ..., -3.2037e-06,
         -2.8908e-06, -2.0117e-06]], device='cuda:0')
Loss: 1.0022368431091309


Running epoch 1, step 1377, batch 329
Sampled inputs[:2]: tensor([[    0,  1234,   408,  ...,   292, 17323,   221],
        [    0,   380,   333,  ...,  8127,   504,   679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3119e-04, -1.0911e-04, -1.1570e-05,  ...,  1.9341e-05,
          1.3838e-04,  6.9838e-05],
        [-2.8834e-06, -2.1756e-06,  2.3171e-06,  ..., -2.6301e-06,
         -2.1085e-06, -1.8068e-06],
        [-7.8380e-06, -6.1989e-06,  6.6906e-06,  ..., -7.1526e-06,
         -5.7817e-06, -4.9025e-06],
        [-6.0350e-06, -4.5002e-06,  5.0813e-06,  ..., -5.4985e-06,
         -4.3958e-06, -3.9935e-06],
        [-6.8545e-06, -5.6177e-06,  5.7369e-06,  ..., -6.3330e-06,
         -5.3644e-06, -3.9637e-06]], device='cuda:0')
Loss: 0.9624561071395874


Running epoch 1, step 1378, batch 330
Sampled inputs[:2]: tensor([[   0,  874,  590,  ...,  300,  867,  638],
        [   0, 1550, 2013,  ..., 9970,  638, 6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3784e-04, -1.4313e-04, -7.1067e-05,  ...,  2.1802e-06,
          2.1123e-04,  1.5544e-04],
        [-4.2766e-06, -3.0287e-06,  3.5614e-06,  ..., -3.8445e-06,
         -2.9877e-06, -2.7195e-06],
        [-1.1578e-05, -8.6576e-06,  1.0267e-05,  ..., -1.0416e-05,
         -8.1360e-06, -7.3165e-06],
        [-8.9407e-06, -6.2361e-06,  7.8380e-06,  ..., -8.0466e-06,
         -6.2212e-06, -6.0201e-06],
        [-1.0043e-05, -7.8082e-06,  8.7023e-06,  ..., -9.1493e-06,
         -7.5251e-06, -5.8487e-06]], device='cuda:0')
Loss: 0.9408188462257385


Running epoch 1, step 1379, batch 331
Sampled inputs[:2]: tensor([[   0, 1561,   14,  ..., 4433,  352, 1561],
        [   0,  300,  266,  ...,   13, 2920,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5890e-04, -1.2275e-05, -6.9932e-05,  ..., -3.7679e-05,
          4.1913e-06,  1.5544e-04],
        [-5.7369e-06, -3.9414e-06,  4.8131e-06,  ..., -5.1484e-06,
         -3.8669e-06, -3.7104e-06],
        [-1.5691e-05, -1.1325e-05,  1.3947e-05,  ..., -1.4052e-05,
         -1.0595e-05, -1.0058e-05],
        [-1.2025e-05, -8.1286e-06,  1.0595e-05,  ..., -1.0803e-05,
         -8.0615e-06, -8.1956e-06],
        [-1.3426e-05, -1.0073e-05,  1.1683e-05,  ..., -1.2189e-05,
         -9.7007e-06, -7.9498e-06]], device='cuda:0')
Loss: 0.9571109414100647


Running epoch 1, step 1380, batch 332
Sampled inputs[:2]: tensor([[   0,  756,  401,  ...,  271, 7272, 1663],
        [   0, 1008,  266,  ..., 1941,  437, 1626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7446e-04, -5.7471e-05, -1.5181e-04,  ..., -6.9638e-05,
          1.9921e-04,  2.3230e-05],
        [-7.1228e-06, -4.7944e-06,  5.9828e-06,  ..., -6.2883e-06,
         -4.7348e-06, -4.6529e-06],
        [-1.9535e-05, -1.3858e-05,  1.7419e-05,  ..., -1.7196e-05,
         -1.3009e-05, -1.2591e-05],
        [-1.5020e-05, -9.9242e-06,  1.3262e-05,  ..., -1.3247e-05,
         -9.9167e-06, -1.0297e-05],
        [-1.6659e-05, -1.2279e-05,  1.4544e-05,  ..., -1.4871e-05,
         -1.1876e-05, -9.8869e-06]], device='cuda:0')
Loss: 0.9261946678161621


Running epoch 1, step 1381, batch 333
Sampled inputs[:2]: tensor([[   0,   12,  630,  ..., 5049,   14, 2371],
        [   0, 2319,   30,  ...,  508, 6703,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5779e-05, -2.7702e-05, -8.7116e-05,  ..., -4.6102e-05,
          2.4219e-04,  2.8349e-05],
        [-8.5682e-06, -5.8673e-06,  7.2494e-06,  ..., -7.5921e-06,
         -5.7183e-06, -5.5470e-06],
        [ 3.6493e-05,  8.0240e-05, -4.8384e-05,  ...,  5.3744e-05,
          7.9762e-05,  2.4451e-05],
        [-1.8165e-05, -1.2219e-05,  1.6123e-05,  ..., -1.6093e-05,
         -1.2033e-05, -1.2338e-05],
        [-2.0087e-05, -1.5020e-05,  1.7613e-05,  ..., -1.8030e-05,
         -1.4380e-05, -1.1884e-05]], device='cuda:0')
Loss: 1.0141189098358154


Running epoch 1, step 1382, batch 334
Sampled inputs[:2]: tensor([[    0,    11,   360,  ...,  4524,  1553,   401],
        [    0,   452,    13,  ...,   358,    13, 12347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0157e-05,  1.9700e-05, -2.2223e-04,  ..., -8.0321e-05,
          1.0676e-04, -2.1949e-05],
        [-1.0006e-05, -6.8881e-06,  8.3745e-06,  ..., -8.9183e-06,
         -6.6943e-06, -6.4932e-06],
        [ 3.2648e-05,  7.7350e-05, -4.5180e-05,  ...,  5.0228e-05,
          7.7154e-05,  2.1978e-05],
        [-2.1130e-05, -1.4290e-05,  1.8552e-05,  ..., -1.8835e-05,
         -1.4044e-05, -1.4395e-05],
        [-2.3469e-05, -1.7643e-05,  2.0370e-05,  ..., -2.1160e-05,
         -1.6809e-05, -1.3895e-05]], device='cuda:0')
Loss: 0.9694477319717407


Running epoch 1, step 1383, batch 335
Sampled inputs[:2]: tensor([[    0,   266,   452,  ...,  1725,  2200,   342],
        [    0,  1624,   391,  ...,   391, 36249,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9650e-05,  9.0031e-05, -2.1282e-04,  ..., -1.5129e-04,
          2.1371e-04,  2.3840e-05],
        [-1.1303e-05, -7.9162e-06,  9.4920e-06,  ..., -1.0148e-05,
         -7.6294e-06, -7.3612e-06],
        [ 2.9042e-05,  7.4369e-05, -4.1887e-05,  ...,  4.6830e-05,
          7.4561e-05,  1.9608e-05],
        [-2.3901e-05, -1.6451e-05,  2.1055e-05,  ..., -2.1473e-05,
         -1.6056e-05, -1.6376e-05],
        [-2.6464e-05, -2.0221e-05,  2.3052e-05,  ..., -2.4021e-05,
         -1.9103e-05, -1.5698e-05]], device='cuda:0')
Loss: 0.9825800657272339
Graident accumulation at epoch 1, step 1383, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.3236e-05,  8.9913e-05,  2.6578e-05,  ..., -4.6258e-06,
          3.4660e-05,  8.6755e-05],
        [-1.1251e-05, -7.7340e-06,  7.9655e-06,  ..., -9.9251e-06,
         -7.2076e-06, -7.7136e-06],
        [ 2.7851e-05,  4.2762e-05, -3.0400e-05,  ...,  4.1623e-05,
          4.4972e-05,  1.3001e-05],
        [-6.2279e-06,  3.7852e-06,  7.2789e-06,  ..., -5.9388e-06,
          1.7464e-06, -1.0421e-05],
        [-2.2310e-05, -1.5982e-05,  1.7526e-05,  ..., -1.9676e-05,
         -1.4518e-05, -1.5595e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3402e-08, 5.4842e-08, 6.5233e-08,  ..., 2.5548e-08, 1.5024e-07,
         3.7719e-08],
        [7.8938e-11, 5.1397e-11, 1.9856e-11,  ..., 5.4922e-11, 1.6449e-11,
         2.2836e-11],
        [3.6816e-09, 2.2503e-09, 1.2333e-09,  ..., 3.0224e-09, 1.1588e-09,
         1.0101e-09],
        [9.8853e-10, 9.2234e-10, 3.3709e-10,  ..., 8.2518e-10, 5.6837e-10,
         3.3985e-10],
        [3.9751e-10, 2.2497e-10, 7.8850e-11,  ..., 2.9068e-10, 8.3100e-11,
         1.0933e-10]], device='cuda:0')
optimizer state dict: 173.0
lr: [5.465713208182582e-06, 5.465713208182582e-06]
scheduler_last_epoch: 173


Running epoch 1, step 1384, batch 336
Sampled inputs[:2]: tensor([[    0, 29258,   765,  ...,  4196,    19,    12],
        [    0,   642,   287,  ...,   800,    12,  3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4032e-04, -3.8988e-05,  3.1879e-05,  ..., -6.6096e-05,
         -1.4548e-04, -1.9605e-04],
        [-1.4082e-06, -1.0431e-06,  1.2890e-06,  ..., -1.2517e-06,
         -8.8662e-07, -8.9034e-07],
        [-3.9339e-06, -3.0398e-06,  3.7700e-06,  ..., -3.5018e-06,
         -2.4885e-06, -2.5332e-06],
        [-2.9802e-06, -2.1756e-06,  2.8461e-06,  ..., -2.6375e-06,
         -1.8477e-06, -1.9968e-06],
        [-3.2187e-06, -2.5928e-06,  3.0249e-06,  ..., -2.9206e-06,
         -2.2054e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.9651585221290588


Running epoch 1, step 1385, batch 337
Sampled inputs[:2]: tensor([[    0,    14,   292,  ...,  1385,    12,   287],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7239e-05,  1.4881e-04,  3.2233e-05,  ..., -8.7468e-05,
         -7.0858e-05, -1.2766e-04],
        [-2.8312e-06, -2.0191e-06,  2.3991e-06,  ..., -2.5481e-06,
         -1.8924e-06, -1.8515e-06],
        [-8.0466e-06, -6.0797e-06,  7.2122e-06,  ..., -7.2420e-06,
         -5.4389e-06, -5.2750e-06],
        [-6.0201e-06, -4.2170e-06,  5.3197e-06,  ..., -5.4091e-06,
         -4.0084e-06, -4.1574e-06],
        [-6.6906e-06, -5.1856e-06,  5.8413e-06,  ..., -6.0946e-06,
         -4.7982e-06, -4.0531e-06]], device='cuda:0')
Loss: 0.991014301776886


Running epoch 1, step 1386, batch 338
Sampled inputs[:2]: tensor([[    0, 20202,   300,  ..., 15185,   287,  6573],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5778e-05,  1.9847e-04,  4.9507e-05,  ..., -1.2523e-04,
         -1.4276e-04, -1.0859e-04],
        [-4.2170e-06, -2.9728e-06,  3.5390e-06,  ..., -3.7998e-06,
         -2.8238e-06, -2.7940e-06],
        [-1.1951e-05, -8.8662e-06,  1.0639e-05,  ..., -1.0729e-05,
         -8.0466e-06, -7.8678e-06],
        [-9.0003e-06, -6.1989e-06,  7.8976e-06,  ..., -8.1062e-06,
         -5.9903e-06, -6.2734e-06],
        [-9.9093e-06, -7.5400e-06,  8.5831e-06,  ..., -9.0003e-06,
         -7.0781e-06, -6.0201e-06]], device='cuda:0')
Loss: 0.9457849264144897


Running epoch 1, step 1387, batch 339
Sampled inputs[:2]: tensor([[    0,   266,  3574,  ...,  7052,  3829,   292],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2720e-04,  1.8237e-04,  2.3760e-05,  ..., -2.5829e-04,
         -2.5499e-05, -8.3679e-05],
        [-5.5656e-06, -3.9786e-06,  4.6417e-06,  ..., -5.0589e-06,
         -3.7923e-06, -3.7551e-06],
        [-1.5706e-05, -1.1817e-05,  1.3933e-05,  ..., -1.4216e-05,
         -1.0744e-05, -1.0535e-05],
        [-1.1787e-05, -8.2552e-06,  1.0282e-05,  ..., -1.0699e-05,
         -7.9870e-06, -8.3447e-06],
        [-1.3053e-05, -1.0073e-05,  1.1280e-05,  ..., -1.1966e-05,
         -9.4771e-06, -8.0615e-06]], device='cuda:0')
Loss: 0.9563625454902649


Running epoch 1, step 1388, batch 340
Sampled inputs[:2]: tensor([[    0,    14,  4746,  ...,   266,  1119,  1705],
        [    0,    15,    72,  ...,   380, 22463,  2587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7461e-04,  2.3087e-04,  8.4865e-05,  ..., -2.1941e-04,
          2.1556e-04, -8.3679e-05],
        [-6.8992e-06, -4.9770e-06,  5.6103e-06,  ..., -6.3255e-06,
         -4.7982e-06, -4.7311e-06],
        [-1.9431e-05, -1.4737e-05,  1.6868e-05,  ..., -1.7658e-05,
         -1.3471e-05, -1.3158e-05],
        [-1.4633e-05, -1.0356e-05,  1.2457e-05,  ..., -1.3411e-05,
         -1.0133e-05, -1.0535e-05],
        [-1.6257e-05, -1.2606e-05,  1.3739e-05,  ..., -1.4961e-05,
         -1.1951e-05, -1.0118e-05]], device='cuda:0')
Loss: 0.9613856077194214


Running epoch 1, step 1389, batch 341
Sampled inputs[:2]: tensor([[    0,   292,   221,  ...,   796, 12886,   694],
        [    0,  3001,  3325,  ..., 16332,  2661,  1200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3997e-04,  3.0023e-04,  1.8925e-05,  ..., -2.8636e-04,
          9.7154e-05, -1.1698e-04],
        [-8.3596e-06, -5.8524e-06,  6.6981e-06,  ..., -7.6294e-06,
         -5.7295e-06, -5.8115e-06],
        [-2.3544e-05, -1.7360e-05,  2.0161e-05,  ..., -2.1294e-05,
         -1.6093e-05, -1.6153e-05],
        [-1.7717e-05, -1.2159e-05,  1.4871e-05,  ..., -1.6168e-05,
         -1.2100e-05, -1.2904e-05],
        [-1.9848e-05, -1.4931e-05,  1.6496e-05,  ..., -1.8179e-05,
         -1.4380e-05, -1.2562e-05]], device='cuda:0')
Loss: 0.9415486454963684


Running epoch 1, step 1390, batch 342
Sampled inputs[:2]: tensor([[    0,    13, 11273,  ...,   292,  1057,    14],
        [    0,  4323,  2377,  ...,  3878,  4044,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4974e-04,  4.7885e-04,  8.9883e-05,  ..., -3.6955e-04,
          8.6494e-05, -1.1098e-04],
        [-9.7007e-06, -6.8136e-06,  7.6741e-06,  ..., -8.8960e-06,
         -6.7577e-06, -6.8247e-06],
        [-2.7254e-05, -2.0176e-05,  2.3067e-05,  ..., -2.4796e-05,
         -1.8969e-05, -1.8969e-05],
        [-2.0564e-05, -1.4141e-05,  1.7032e-05,  ..., -1.8865e-05,
         -1.4275e-05, -1.5154e-05],
        [-2.3127e-05, -1.7494e-05,  1.8999e-05,  ..., -2.1309e-05,
         -1.7047e-05, -1.4886e-05]], device='cuda:0')
Loss: 0.9282665848731995


Running epoch 1, step 1391, batch 343
Sampled inputs[:2]: tensor([[    0,  1682,   271,  ...,   300,   266, 10935],
        [    0,   278,   266,  ...,    13,  2853,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6823e-04,  5.2960e-04,  1.6312e-04,  ..., -3.6627e-04,
          8.6494e-05, -8.5262e-05],
        [-1.0967e-05, -7.7523e-06,  8.8289e-06,  ..., -1.0096e-05,
         -7.6480e-06, -7.7076e-06],
        [-3.0890e-05, -2.2963e-05,  2.6554e-05,  ..., -2.8193e-05,
         -2.1458e-05, -2.1487e-05],
        [-2.3410e-05, -1.6183e-05,  1.9759e-05,  ..., -2.1547e-05,
         -1.6227e-05, -1.7270e-05],
        [-2.6062e-05, -1.9848e-05,  2.1741e-05,  ..., -2.4095e-05,
         -1.9222e-05, -1.6734e-05]], device='cuda:0')
Loss: 0.9680454134941101
Graident accumulation at epoch 1, step 1391, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9101e-06,  1.3388e-04,  4.0232e-05,  ..., -4.0790e-05,
          3.9843e-05,  6.9553e-05],
        [-1.1223e-05, -7.7358e-06,  8.0519e-06,  ..., -9.9422e-06,
         -7.2516e-06, -7.7130e-06],
        [ 2.1977e-05,  3.6190e-05, -2.4704e-05,  ...,  3.4642e-05,
          3.8329e-05,  9.5517e-06],
        [-7.9461e-06,  1.7884e-06,  8.5269e-06,  ..., -7.4996e-06,
         -5.0937e-08, -1.1106e-05],
        [-2.2685e-05, -1.6369e-05,  1.7947e-05,  ..., -2.0118e-05,
         -1.4989e-05, -1.5709e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3485e-08, 5.5068e-08, 6.5194e-08,  ..., 2.5657e-08, 1.5010e-07,
         3.7689e-08],
        [7.8980e-11, 5.1405e-11, 1.9914e-11,  ..., 5.4969e-11, 1.6491e-11,
         2.2872e-11],
        [3.6789e-09, 2.2486e-09, 1.2327e-09,  ..., 3.0202e-09, 1.1581e-09,
         1.0096e-09],
        [9.8809e-10, 9.2168e-10, 3.3715e-10,  ..., 8.2482e-10, 5.6807e-10,
         3.3981e-10],
        [3.9779e-10, 2.2514e-10, 7.9244e-11,  ..., 2.9097e-10, 8.3386e-11,
         1.0950e-10]], device='cuda:0')
optimizer state dict: 174.0
lr: [5.355883883924591e-06, 5.355883883924591e-06]
scheduler_last_epoch: 174


Running epoch 1, step 1392, batch 344
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,   292, 15087,  ...,  2675,  1663,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1774e-05,  1.2404e-05, -9.5634e-05,  ..., -1.1997e-04,
         -6.5791e-05,  6.2082e-05],
        [-1.3709e-06, -9.8348e-07,  1.1921e-06,  ..., -1.1995e-06,
         -8.7172e-07, -8.4564e-07],
        [ 1.4253e-04,  2.2991e-04, -8.5344e-05,  ...,  1.3357e-04,
          1.4516e-04,  3.3280e-05],
        [-2.9504e-06, -2.0713e-06,  2.7120e-06,  ..., -2.5779e-06,
         -1.8701e-06, -1.9222e-06],
        [-3.1441e-06, -2.4736e-06,  2.8312e-06,  ..., -2.8014e-06,
         -2.1905e-06, -1.7583e-06]], device='cuda:0')
Loss: 0.9488200545310974


Running epoch 1, step 1393, batch 345
Sampled inputs[:2]: tensor([[   0, 3532,  300,  ...,   12,  461,  806],
        [   0,  365,  925,  ...,  909,  598,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6554e-05, -6.9619e-06, -1.6264e-04,  ..., -2.3119e-04,
         -1.9786e-04,  6.4341e-05],
        [-2.9206e-06, -1.9595e-06,  2.3171e-06,  ..., -2.5257e-06,
         -1.7583e-06, -1.9483e-06],
        [ 1.3815e-04,  2.2697e-04, -8.1932e-05,  ...,  1.2983e-04,
          1.4262e-04,  3.0211e-05],
        [-6.1095e-06, -4.0233e-06,  5.1260e-06,  ..., -5.2750e-06,
         -3.6657e-06, -4.2617e-06],
        [-6.8843e-06, -5.0515e-06,  5.6624e-06,  ..., -6.0350e-06,
         -4.5002e-06, -4.2021e-06]], device='cuda:0')
Loss: 0.9319466352462769


Running epoch 1, step 1394, batch 346
Sampled inputs[:2]: tensor([[    0,   342,   721,  ...,  2429,    14,   475],
        [    0,    71,    14,  ...,  1770,   391, 39516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5663e-04,  6.0564e-05, -6.5897e-05,  ..., -1.5407e-04,
         -1.6968e-04,  2.0333e-04],
        [-4.3213e-06, -2.9206e-06,  3.5092e-06,  ..., -3.7774e-06,
         -2.6301e-06, -2.8647e-06],
        [ 1.3425e-04,  2.2414e-04, -7.8430e-05,  ...,  1.2638e-04,
          1.4020e-04,  2.7677e-05],
        [-9.0897e-06, -6.0350e-06,  7.7784e-06,  ..., -7.9423e-06,
         -5.5060e-06, -6.3330e-06],
        [-1.0103e-05, -7.4804e-06,  8.4937e-06,  ..., -8.9258e-06,
         -6.6608e-06, -6.1244e-06]], device='cuda:0')
Loss: 0.9604460000991821


Running epoch 1, step 1395, batch 347
Sampled inputs[:2]: tensor([[    0,   591, 36195,  ...,  3359,   717,    12],
        [    0,   413,    28,  ...,   328, 37605,  6499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2435e-05, -4.9270e-05, -3.5993e-05,  ..., -8.0450e-05,
         -1.2167e-04,  2.4667e-04],
        [-5.7667e-06, -4.0606e-06,  4.6417e-06,  ..., -5.0813e-06,
         -3.6806e-06, -3.7551e-06],
        [ 1.3013e-04,  2.2073e-04, -7.4988e-05,  ...,  1.2262e-04,
          1.3714e-04,  2.5085e-05],
        [-1.2115e-05, -8.4192e-06,  1.0282e-05,  ..., -1.0684e-05,
         -7.6964e-06, -8.3148e-06],
        [-1.3545e-05, -1.0446e-05,  1.1295e-05,  ..., -1.2115e-05,
         -9.4026e-06, -8.1360e-06]], device='cuda:0')
Loss: 0.9890615344047546


Running epoch 1, step 1396, batch 348
Sampled inputs[:2]: tensor([[    0,  1029,  6068,  ..., 18017,   300,   259],
        [    0,    30,  1869,  ...,  4998, 44266,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4434e-04, -6.0414e-05, -3.2707e-05,  ..., -8.7329e-05,
         -2.3282e-04,  3.2936e-04],
        [-7.2047e-06, -5.1782e-06,  5.8562e-06,  ..., -6.3404e-06,
         -4.6156e-06, -4.6305e-06],
        [ 1.2599e-04,  2.1735e-04, -7.1308e-05,  ...,  1.1896e-04,
          1.3435e-04,  2.2551e-05],
        [-1.5274e-05, -1.0848e-05,  1.3053e-05,  ..., -1.3456e-05,
         -9.7677e-06, -1.0341e-05],
        [-1.6853e-05, -1.3247e-05,  1.4171e-05,  ..., -1.5095e-05,
         -1.1757e-05, -1.0014e-05]], device='cuda:0')
Loss: 0.9987353682518005


Running epoch 1, step 1397, batch 349
Sampled inputs[:2]: tensor([[   0, 1765, 5370,  ..., 1711,  292,  380],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9683e-04, -6.4049e-05, -1.6087e-04,  ..., -7.5433e-05,
         -2.0834e-04,  3.0746e-04],
        [-8.5533e-06, -6.1914e-06,  7.0110e-06,  ..., -7.5325e-06,
         -5.5134e-06, -5.5209e-06],
        [ 1.2215e-04,  2.1431e-04, -6.7776e-05,  ...,  1.1557e-04,
          1.3179e-04,  2.0048e-05],
        [-1.8194e-05, -1.3009e-05,  1.5706e-05,  ..., -1.6049e-05,
         -1.1705e-05, -1.2398e-05],
        [-1.9997e-05, -1.5825e-05,  1.6972e-05,  ..., -1.7896e-05,
         -1.3992e-05, -1.1869e-05]], device='cuda:0')
Loss: 0.9748108983039856


Running epoch 1, step 1398, batch 350
Sampled inputs[:2]: tensor([[   0, 1196, 2612,  ..., 2489,   14,  333],
        [   0, 5775,   12,  ...,   12, 1034, 9257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6359e-04, -2.3265e-05, -3.7476e-04,  ..., -1.0065e-04,
         -3.3866e-04,  2.4078e-04],
        [-9.8795e-06, -7.0706e-06,  8.1956e-06,  ..., -8.6948e-06,
         -6.3702e-06, -6.3591e-06],
        [ 1.1845e-04,  2.1174e-04, -6.4259e-05,  ...,  1.1239e-04,
          1.2942e-04,  1.7768e-05],
        [-2.1115e-05, -1.4864e-05,  1.8477e-05,  ..., -1.8582e-05,
         -1.3560e-05, -1.4350e-05],
        [-2.3037e-05, -1.8045e-05,  1.9789e-05,  ..., -2.0564e-05,
         -1.6123e-05, -1.3575e-05]], device='cuda:0')
Loss: 0.911487340927124


Running epoch 1, step 1399, batch 351
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,  7120,   344,  ...,  6273,    52, 22639]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5822e-05, -3.3338e-05, -3.0283e-04,  ..., -1.5700e-04,
         -3.5197e-04,  1.6686e-04],
        [-1.1280e-05, -8.0764e-06,  9.4101e-06,  ..., -9.9018e-06,
         -7.2904e-06, -7.2755e-06],
        [ 1.1440e-04,  2.0872e-04, -6.0579e-05,  ...,  1.0888e-04,
          1.2674e-04,  1.5131e-05],
        [-2.4155e-05, -1.6995e-05,  2.1219e-05,  ..., -2.1204e-05,
         -1.5542e-05, -1.6436e-05],
        [-2.6375e-05, -2.0638e-05,  2.2754e-05,  ..., -2.3484e-05,
         -1.8492e-05, -1.5572e-05]], device='cuda:0')
Loss: 0.9940000176429749
Graident accumulation at epoch 1, step 1399, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3631e-06,  1.1716e-04,  5.9259e-06,  ..., -5.2412e-05,
          6.6252e-07,  7.9284e-05],
        [-1.1228e-05, -7.7699e-06,  8.1877e-06,  ..., -9.9381e-06,
         -7.2555e-06, -7.6692e-06],
        [ 3.1219e-05,  5.3443e-05, -2.8292e-05,  ...,  4.2066e-05,
          4.7170e-05,  1.0110e-05],
        [-9.5669e-06, -8.9885e-08,  9.7962e-06,  ..., -8.8701e-06,
         -1.6000e-06, -1.1639e-05],
        [-2.3054e-05, -1.6796e-05,  1.8428e-05,  ..., -2.0455e-05,
         -1.5339e-05, -1.5695e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3437e-08, 5.5014e-08, 6.5221e-08,  ..., 2.5656e-08, 1.5007e-07,
         3.7679e-08],
        [7.9028e-11, 5.1419e-11, 1.9983e-11,  ..., 5.5012e-11, 1.6528e-11,
         2.2902e-11],
        [3.6883e-09, 2.2899e-09, 1.2352e-09,  ..., 3.0290e-09, 1.1730e-09,
         1.0088e-09],
        [9.8768e-10, 9.2104e-10, 3.3726e-10,  ..., 8.2445e-10, 5.6774e-10,
         3.3974e-10],
        [3.9809e-10, 2.2534e-10, 7.9682e-11,  ..., 2.9123e-10, 8.3645e-11,
         1.0963e-10]], device='cuda:0')
optimizer state dict: 175.0
lr: [5.246764221148206e-06, 5.246764221148206e-06]
scheduler_last_epoch: 175


Running epoch 1, step 1400, batch 352
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0, 2018, 4798,  ...,  292, 1919,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0523e-04,  4.6767e-06, -1.4327e-05,  ..., -4.6402e-05,
         -6.5850e-05, -1.9880e-04],
        [-1.3486e-06, -9.9838e-07,  1.1623e-06,  ..., -1.1995e-06,
         -8.7172e-07, -8.8289e-07],
        [-3.7700e-06, -2.9802e-06,  3.4720e-06,  ..., -3.3528e-06,
         -2.4885e-06, -2.4289e-06],
        [-2.9504e-06, -2.1607e-06,  2.6822e-06,  ..., -2.6375e-06,
         -1.9222e-06, -2.0564e-06],
        [-3.0845e-06, -2.5183e-06,  2.7716e-06,  ..., -2.7865e-06,
         -2.1905e-06, -1.8030e-06]], device='cuda:0')
Loss: 0.946163535118103


Running epoch 1, step 1401, batch 353
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0885e-04,  5.5920e-05,  3.8445e-06,  ..., -6.4680e-05,
         -8.7139e-05, -3.3679e-04],
        [-2.8014e-06, -1.9968e-06,  2.2873e-06,  ..., -2.4736e-06,
         -1.8403e-06, -1.8440e-06],
        [-7.9721e-06, -6.0797e-06,  6.9439e-06,  ..., -7.0632e-06,
         -5.3495e-06, -5.2154e-06],
        [-6.1244e-06, -4.3213e-06,  5.2601e-06,  ..., -5.4389e-06,
         -4.0382e-06, -4.2766e-06],
        [-6.5416e-06, -5.1558e-06,  5.5581e-06,  ..., -5.8860e-06,
         -4.7088e-06, -3.9339e-06]], device='cuda:0')
Loss: 0.9714570641517639


Running epoch 1, step 1402, batch 354
Sampled inputs[:2]: tensor([[   0,  729, 3084,  ...,  381, 1445,  642],
        [   0,  278, 6318,  ...,  458,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8270e-04,  3.6859e-05, -1.4116e-04,  ...,  5.9876e-05,
         -3.6190e-05, -3.1134e-04],
        [-4.2170e-06, -3.0324e-06,  3.4645e-06,  ..., -3.6880e-06,
         -2.7642e-06, -2.6934e-06],
        [ 1.2175e-04,  1.3871e-04, -1.0963e-04,  ...,  8.6696e-05,
          1.2403e-04,  3.1376e-05],
        [-9.1493e-06, -6.5267e-06,  7.9125e-06,  ..., -8.0615e-06,
         -6.0350e-06, -6.1989e-06],
        [-9.7603e-06, -7.8082e-06,  8.3745e-06,  ..., -8.7470e-06,
         -7.0632e-06, -5.7146e-06]], device='cuda:0')
Loss: 0.9965173602104187


Running epoch 1, step 1403, batch 355
Sampled inputs[:2]: tensor([[    0,    14,  3948,  ...,   571, 10097,    12],
        [    0,  4113,   709,  ..., 22407,  3231,  1130]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4244e-04, -1.2750e-05, -1.5646e-04,  ...,  7.7434e-05,
         -1.3553e-04, -3.3770e-04],
        [-5.5507e-06, -4.0159e-06,  4.6492e-06,  ..., -4.9099e-06,
         -3.6471e-06, -3.5390e-06],
        [ 1.1785e-04,  1.3571e-04, -1.0598e-04,  ...,  8.3105e-05,
          1.2142e-04,  2.8888e-05],
        [-1.2100e-05, -8.6725e-06,  1.0654e-05,  ..., -1.0774e-05,
         -7.9721e-06, -8.1807e-06],
        [-1.2830e-05, -1.0267e-05,  1.1176e-05,  ..., -1.1623e-05,
         -9.2685e-06, -7.5176e-06]], device='cuda:0')
Loss: 0.9870032668113708


Running epoch 1, step 1404, batch 356
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0860e-04,  6.9726e-05, -2.0080e-04,  ...,  9.3438e-05,
         -1.2380e-04, -3.2658e-04],
        [-6.9737e-06, -4.9844e-06,  5.8785e-06,  ..., -6.1616e-06,
         -4.4815e-06, -4.4443e-06],
        [ 1.1377e-04,  1.3279e-04, -1.0228e-04,  ...,  7.9514e-05,
          1.1902e-04,  2.6295e-05],
        [-1.5244e-05, -1.0759e-05,  1.3471e-05,  ..., -1.3530e-05,
         -9.7826e-06, -1.0297e-05],
        [-1.6049e-05, -1.2696e-05,  1.4022e-05,  ..., -1.4514e-05,
         -1.1325e-05, -9.4101e-06]], device='cuda:0')
Loss: 0.9706324934959412


Running epoch 1, step 1405, batch 357
Sampled inputs[:2]: tensor([[    0,   266, 12964,  ...,   300,  3979,  4706],
        [    0, 16028,   669,  ...,   292,  6502,  7050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6287e-04, -9.7387e-07, -1.8455e-04,  ..., -7.5059e-06,
         -1.3779e-04, -2.7936e-04],
        [-8.4415e-06, -5.8636e-06,  6.8620e-06,  ..., -7.4506e-06,
         -5.4017e-06, -5.5395e-06],
        [ 1.0977e-04,  1.3014e-04, -9.9349e-05,  ...,  7.5982e-05,
          1.1642e-04,  2.3360e-05],
        [-1.8269e-05, -1.2562e-05,  1.5631e-05,  ..., -1.6212e-05,
         -1.1720e-05, -1.2621e-05],
        [-1.9699e-05, -1.5125e-05,  1.6570e-05,  ..., -1.7777e-05,
         -1.3828e-05, -1.1928e-05]], device='cuda:0')
Loss: 0.8930427432060242


Running epoch 1, step 1406, batch 358
Sampled inputs[:2]: tensor([[    0,    34,  3881,  ...,  1027,   271,   266],
        [    0,  1110, 26330,  ...,  1558,   674,  2351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1876e-04, -1.0094e-04, -2.2890e-04,  ..., -1.0538e-04,
         -1.9348e-04, -4.6183e-04],
        [-9.7975e-06, -6.7577e-06,  7.9647e-06,  ..., -8.6650e-06,
         -6.2920e-06, -6.4448e-06],
        [ 1.0584e-04,  1.2741e-04, -9.5922e-05,  ...,  7.2481e-05,
          1.1385e-04,  2.0707e-05],
        [-2.1294e-05, -1.4499e-05,  1.8224e-05,  ..., -1.8895e-05,
         -1.3642e-05, -1.4752e-05],
        [-2.2903e-05, -1.7434e-05,  1.9267e-05,  ..., -2.0683e-05,
         -1.6093e-05, -1.3910e-05]], device='cuda:0')
Loss: 0.9626149535179138


Running epoch 1, step 1407, batch 359
Sampled inputs[:2]: tensor([[    0,   266, 15324,  ...,   943,  1613,  7178],
        [    0,   560,   199,  ...,  6408,   278,  1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6472e-04, -1.0025e-04, -4.6232e-04,  ..., -1.6883e-04,
         -2.9254e-04, -4.6183e-04],
        [-1.1250e-05, -7.6927e-06,  9.1121e-06,  ..., -9.9316e-06,
         -7.1898e-06, -7.4208e-06],
        [ 1.0170e-04,  1.2461e-04, -9.2450e-05,  ...,  6.8904e-05,
          1.1130e-04,  1.7936e-05],
        [-2.4542e-05, -1.6525e-05,  2.0921e-05,  ..., -2.1696e-05,
         -1.5609e-05, -1.7062e-05],
        [-2.6345e-05, -1.9863e-05,  2.2098e-05,  ..., -2.3723e-05,
         -1.8388e-05, -1.6056e-05]], device='cuda:0')
Loss: 0.9867096543312073
Graident accumulation at epoch 1, step 1407, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.7699e-05,  9.5418e-05, -4.0898e-05,  ..., -6.4054e-05,
         -2.8657e-05,  2.5173e-05],
        [-1.1231e-05, -7.7621e-06,  8.2801e-06,  ..., -9.9375e-06,
         -7.2489e-06, -7.6444e-06],
        [ 3.8267e-05,  6.0560e-05, -3.4707e-05,  ...,  4.4750e-05,
          5.3584e-05,  1.0892e-05],
        [-1.1064e-05, -1.7334e-06,  1.0909e-05,  ..., -1.0153e-05,
         -3.0009e-06, -1.2181e-05],
        [-2.3383e-05, -1.7102e-05,  1.8795e-05,  ..., -2.0781e-05,
         -1.5644e-05, -1.5731e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3825e-08, 5.4969e-08, 6.5369e-08,  ..., 2.5659e-08, 1.5001e-07,
         3.7854e-08],
        [7.9076e-11, 5.1427e-11, 2.0046e-11,  ..., 5.5056e-11, 1.6563e-11,
         2.2934e-11],
        [3.6949e-09, 2.3031e-09, 1.2425e-09,  ..., 3.0308e-09, 1.1842e-09,
         1.0081e-09],
        [9.8730e-10, 9.2040e-10, 3.3736e-10,  ..., 8.2409e-10, 5.6742e-10,
         3.3969e-10],
        [3.9839e-10, 2.2551e-10, 8.0091e-11,  ..., 2.9150e-10, 8.3899e-11,
         1.0978e-10]], device='cuda:0')
optimizer state dict: 176.0
lr: [5.1383708942904056e-06, 5.1383708942904056e-06]
scheduler_last_epoch: 176


Running epoch 1, step 1408, batch 360
Sampled inputs[:2]: tensor([[    0,   367,  3704,  ...,  1746,    14,   759],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5154e-05, -4.1760e-05,  5.5312e-05,  ..., -1.3106e-05,
          9.9814e-05, -6.7176e-05],
        [-1.3933e-06, -1.0505e-06,  1.1250e-06,  ..., -1.1921e-06,
         -9.5367e-07, -9.0525e-07],
        [-4.0829e-06, -3.2336e-06,  3.5018e-06,  ..., -3.5167e-06,
         -2.8610e-06, -2.6822e-06],
        [-3.0845e-06, -2.2799e-06,  2.5928e-06,  ..., -2.6226e-06,
         -2.1011e-06, -2.1160e-06],
        [-3.2634e-06, -2.6971e-06,  2.7418e-06,  ..., -2.8759e-06,
         -2.4587e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9949719309806824


Running epoch 1, step 1409, batch 361
Sampled inputs[:2]: tensor([[    0,   266,   858,  ..., 11265,   607,  7455],
        [    0,   346,   462,  ..., 35247,  2547,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4865e-05, -1.0413e-04,  7.9914e-05,  ...,  6.6656e-05,
          2.0481e-04, -2.2279e-04],
        [-2.7418e-06, -2.0564e-06,  2.1830e-06,  ..., -2.4438e-06,
         -1.9819e-06, -1.7881e-06],
        [-7.9274e-06, -6.2287e-06,  6.6906e-06,  ..., -7.0333e-06,
         -5.7220e-06, -5.1409e-06],
        [-6.0052e-06, -4.3958e-06,  4.9770e-06,  ..., -5.3346e-06,
         -4.3064e-06, -4.1574e-06],
        [-6.4969e-06, -5.3048e-06,  5.3942e-06,  ..., -5.8860e-06,
         -5.0068e-06, -3.9041e-06]], device='cuda:0')
Loss: 0.9723154306411743


Running epoch 1, step 1410, batch 362
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,   266,   923,  ...,    14,   298, 12230]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2594e-04, -1.3857e-04,  2.8517e-05,  ...,  1.3205e-04,
          2.1430e-04, -5.6866e-05],
        [-4.1127e-06, -3.0100e-06,  3.3453e-06,  ..., -3.6284e-06,
         -2.8424e-06, -2.6636e-06],
        [ 7.1015e-05,  1.0119e-04, -2.6917e-05,  ...,  5.1161e-05,
          4.7123e-05,  2.5055e-05],
        [-9.0748e-06, -6.4820e-06,  7.7039e-06,  ..., -7.9721e-06,
         -6.2138e-06, -6.2436e-06],
        [-9.6709e-06, -7.7635e-06,  8.1658e-06,  ..., -8.6725e-06,
         -7.1675e-06, -5.7667e-06]], device='cuda:0')
Loss: 0.9749298691749573


Running epoch 1, step 1411, batch 363
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,  3990,   717,    12],
        [    0,  2352,  4275,  ..., 10518,   342,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2541e-04, -1.6663e-04, -1.2112e-04,  ...,  1.7143e-04,
          8.3440e-05, -9.4239e-05],
        [-5.5507e-06, -3.9861e-06,  4.4107e-06,  ..., -4.9248e-06,
         -3.7849e-06, -3.6545e-06],
        [ 6.6873e-05,  9.8267e-05, -2.3669e-05,  ...,  4.7480e-05,
          4.4426e-05,  2.2254e-05],
        [-1.2115e-05, -8.4937e-06,  1.0058e-05,  ..., -1.0714e-05,
         -8.1956e-06, -8.4490e-06],
        [-1.3158e-05, -1.0312e-05,  1.0848e-05,  ..., -1.1817e-05,
         -9.5814e-06, -7.9721e-06]], device='cuda:0')
Loss: 0.9539690017700195


Running epoch 1, step 1412, batch 364
Sampled inputs[:2]: tensor([[   0, 3406,  300,  ..., 1726, 3521, 4481],
        [   0, 3058,  292,  ..., 1387, 1236,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4992e-05, -1.1853e-04, -1.5407e-04,  ..., -3.5502e-05,
         -1.6631e-04, -1.4203e-04],
        [-6.9886e-06, -4.8988e-06,  5.4166e-06,  ..., -6.2734e-06,
         -4.7386e-06, -4.7125e-06],
        [ 6.2849e-05,  9.5540e-05, -2.0599e-05,  ...,  4.3755e-05,
          4.1759e-05,  1.9378e-05],
        [-1.5020e-05, -1.0282e-05,  1.2204e-05,  ..., -1.3426e-05,
         -1.0096e-05, -1.0654e-05],
        [-1.6809e-05, -1.2815e-05,  1.3530e-05,  ..., -1.5229e-05,
         -1.2130e-05, -1.0416e-05]], device='cuda:0')
Loss: 0.932905375957489


Running epoch 1, step 1413, batch 365
Sampled inputs[:2]: tensor([[   0,   13, 4831,  ...,  333,  199, 2038],
        [   0, 5221, 7166,  ..., 4309,  342,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8013e-05, -3.1378e-04, -1.9102e-04,  ..., -7.8981e-05,
          6.5067e-05, -1.9718e-04],
        [-8.4192e-06, -5.8413e-06,  6.4969e-06,  ..., -7.5251e-06,
         -5.6550e-06, -5.7332e-06],
        [ 5.8647e-05,  9.2605e-05, -1.7201e-05,  ...,  4.0119e-05,
          3.9047e-05,  1.6413e-05],
        [-1.8045e-05, -1.2234e-05,  1.4603e-05,  ..., -1.6063e-05,
         -1.2018e-05, -1.2919e-05],
        [-2.0325e-05, -1.5333e-05,  1.6302e-05,  ..., -1.8314e-05,
         -1.4558e-05, -1.2726e-05]], device='cuda:0')
Loss: 0.9493547677993774


Running epoch 1, step 1414, batch 366
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0,    14,   475,  ...,  6895,  5842,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0888e-04, -3.9836e-04, -4.4950e-04,  ...,  2.0067e-05,
          1.0280e-04, -2.8826e-04],
        [-9.6709e-06, -6.7838e-06,  7.5251e-06,  ..., -8.6725e-06,
         -6.5528e-06, -6.5230e-06],
        [ 5.5026e-05,  8.9803e-05, -1.3983e-05,  ...,  3.6856e-05,
          3.6484e-05,  1.4222e-05],
        [-2.0847e-05, -1.4305e-05,  1.7047e-05,  ..., -1.8641e-05,
         -1.4044e-05, -1.4775e-05],
        [-2.3231e-05, -1.7658e-05,  1.8820e-05,  ..., -2.0951e-05,
         -1.6749e-05, -1.4335e-05]], device='cuda:0')
Loss: 0.9866425395011902


Running epoch 1, step 1415, batch 367
Sampled inputs[:2]: tensor([[    0,    89,  2023,  ...,   271,    13,   704],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1638e-05, -3.8161e-04, -4.3112e-04,  ..., -7.9646e-05,
          6.6808e-05, -3.2979e-04],
        [-1.0975e-05, -7.7598e-06,  8.5905e-06,  ..., -9.8720e-06,
         -7.4469e-06, -7.4059e-06],
        [ 5.1092e-05,  8.6719e-05, -1.0585e-05,  ...,  3.3235e-05,
          3.3802e-05,  1.1555e-05],
        [-2.3812e-05, -1.6481e-05,  1.9565e-05,  ..., -2.1368e-05,
         -1.6041e-05, -1.6890e-05],
        [ 4.2017e-05,  5.1648e-05, -3.6432e-05,  ...,  5.4280e-05,
          5.9189e-05,  2.7967e-05]], device='cuda:0')
Loss: 0.9899469614028931
Graident accumulation at epoch 1, step 1415, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.1765e-05,  4.7715e-05, -7.9920e-05,  ..., -6.5613e-05,
         -1.9111e-05, -1.0323e-05],
        [-1.1205e-05, -7.7619e-06,  8.3112e-06,  ..., -9.9309e-06,
         -7.2687e-06, -7.6205e-06],
        [ 3.9549e-05,  6.3176e-05, -3.2295e-05,  ...,  4.3598e-05,
          5.1606e-05,  1.0958e-05],
        [-1.2339e-05, -3.2082e-06,  1.1774e-05,  ..., -1.1274e-05,
         -4.3049e-06, -1.2652e-05],
        [-1.6843e-05, -1.0227e-05,  1.3272e-05,  ..., -1.3275e-05,
         -8.1607e-06, -1.1361e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3780e-08, 5.5060e-08, 6.5490e-08,  ..., 2.5639e-08, 1.4986e-07,
         3.7925e-08],
        [7.9117e-11, 5.1436e-11, 2.0100e-11,  ..., 5.5098e-11, 1.6602e-11,
         2.2966e-11],
        [3.6939e-09, 2.3084e-09, 1.2414e-09,  ..., 3.0288e-09, 1.1841e-09,
         1.0072e-09],
        [9.8688e-10, 9.1975e-10, 3.3741e-10,  ..., 8.2372e-10, 5.6711e-10,
         3.3963e-10],
        [3.9975e-10, 2.2795e-10, 8.1338e-11,  ..., 2.9416e-10, 8.7319e-11,
         1.1045e-10]], device='cuda:0')
optimizer state dict: 177.0
lr: [5.030720466797722e-06, 5.030720466797722e-06]
scheduler_last_epoch: 177


Running epoch 1, step 1416, batch 368
Sampled inputs[:2]: tensor([[    0,   623,    12,  ...,  4792,  6572,   300],
        [    0,   508,  1548,  ...,   494, 10792,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4591e-05, -7.8623e-05, -6.9699e-05,  ..., -1.2026e-04,
          1.4926e-04, -1.1645e-04],
        [-1.2517e-06, -8.8662e-07,  8.9407e-07,  ..., -1.2219e-06,
         -9.3505e-07, -9.8348e-07],
        [-3.6359e-06, -2.6375e-06,  2.8610e-06,  ..., -3.3826e-06,
         -2.5779e-06, -2.6375e-06],
        [-2.7120e-06, -1.8477e-06,  2.0415e-06,  ..., -2.6226e-06,
         -1.9819e-06, -2.2054e-06],
        [-3.0845e-06, -2.2650e-06,  2.3842e-06,  ..., -2.8908e-06,
         -2.3097e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9374983906745911


Running epoch 1, step 1417, batch 369
Sampled inputs[:2]: tensor([[    0,  5862,    13,  ..., 12497,   287,  3570],
        [    0,  2615,    13,  ...,   940,  3661,  6837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4933e-05, -1.3178e-04, -7.1432e-05,  ..., -1.4642e-04,
          9.0628e-05, -1.7736e-04],
        [-2.4885e-06, -1.8291e-06,  1.9521e-06,  ..., -2.3767e-06,
         -1.7956e-06, -1.8552e-06],
        [-7.3314e-06, -5.4836e-06,  6.1989e-06,  ..., -6.7502e-06,
         -5.0515e-06, -5.1111e-06],
        [-5.5879e-06, -3.9339e-06,  4.6045e-06,  ..., -5.2899e-06,
         -3.9339e-06, -4.3213e-06],
        [-6.0201e-06, -4.5896e-06,  4.9770e-06,  ..., -5.5879e-06,
         -4.4107e-06, -3.8445e-06]], device='cuda:0')
Loss: 0.9531998038291931


Running epoch 1, step 1418, batch 370
Sampled inputs[:2]: tensor([[    0,  1128,   292,  ...,  1485,   287, 11833],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7538e-06, -1.0380e-04,  5.5436e-05,  ..., -1.9329e-04,
          2.1874e-04, -1.1619e-04],
        [-3.8520e-06, -2.8424e-06,  2.8908e-06,  ..., -3.6284e-06,
         -2.8238e-06, -2.8387e-06],
        [-1.1444e-05, -8.6874e-06,  9.2089e-06,  ..., -1.0520e-05,
         -8.1509e-06, -8.0615e-06],
        [-8.6725e-06, -6.1542e-06,  6.7949e-06,  ..., -8.1062e-06,
         -6.2436e-06, -6.6608e-06],
        [-9.4622e-06, -7.3612e-06,  7.4506e-06,  ..., -8.7917e-06,
         -7.1377e-06, -6.1542e-06]], device='cuda:0')
Loss: 0.9814574718475342


Running epoch 1, step 1419, batch 371
Sampled inputs[:2]: tensor([[    0, 16187,   565,  ...,   586,  3196,   271],
        [    0,   508,   927,  ...,  1390,   674,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2080e-05, -9.6315e-05,  1.4112e-04,  ..., -2.1901e-04,
          1.2866e-04, -1.8921e-04],
        [-5.1931e-06, -3.8184e-06,  4.0084e-06,  ..., -4.8056e-06,
         -3.7290e-06, -3.7141e-06],
        [-1.5378e-05, -1.1697e-05,  1.2726e-05,  ..., -1.3962e-05,
         -1.0848e-05, -1.0610e-05],
        [-1.1608e-05, -8.2403e-06,  9.3877e-06,  ..., -1.0669e-05,
         -8.2254e-06, -8.7023e-06],
        [-1.2621e-05, -9.8497e-06,  1.0177e-05,  ..., -1.1578e-05,
         -9.4473e-06, -8.0243e-06]], device='cuda:0')
Loss: 0.9145129919052124


Running epoch 1, step 1420, batch 372
Sampled inputs[:2]: tensor([[    0,  1487,  2511,  ..., 27735,   760,   266],
        [    0,     9,   278,  ...,   278,   298,   452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8912e-05, -2.7445e-04,  1.2428e-04,  ..., -9.3556e-05,
          1.4384e-04, -4.3564e-05],
        [-6.5640e-06, -4.7721e-06,  4.9174e-06,  ..., -6.1095e-06,
         -4.7348e-06, -4.7199e-06],
        [-1.9580e-05, -1.4782e-05,  1.5706e-05,  ..., -1.7926e-05,
         -1.3873e-05, -1.3649e-05],
        [-1.4767e-05, -1.0371e-05,  1.1548e-05,  ..., -1.3694e-05,
         -1.0535e-05, -1.1161e-05],
        [-1.6168e-05, -1.2517e-05,  1.2651e-05,  ..., -1.4931e-05,
         -1.2130e-05, -1.0394e-05]], device='cuda:0')
Loss: 0.9854093790054321


Running epoch 1, step 1421, batch 373
Sampled inputs[:2]: tensor([[   0, 1064,  266,  ..., 2971,  292,  474],
        [   0,   14, 3445,  ...,  298,  527, 2732]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9338e-05, -2.8141e-04,  5.5451e-05,  ..., -1.4945e-04,
          2.4303e-04, -2.4592e-05],
        [-7.8604e-06, -5.7779e-06,  5.8934e-06,  ..., -7.3463e-06,
         -5.7183e-06, -5.6066e-06],
        [-2.3350e-05, -1.7822e-05,  1.8775e-05,  ..., -2.1473e-05,
         -1.6704e-05, -1.6168e-05],
        [-1.7554e-05, -1.2472e-05,  1.3754e-05,  ..., -1.6361e-05,
         -1.2651e-05, -1.3188e-05],
        [-1.9357e-05, -1.5154e-05,  1.5199e-05,  ..., -1.7956e-05,
         -1.4633e-05, -1.2346e-05]], device='cuda:0')
Loss: 0.9753623008728027


Running epoch 1, step 1422, batch 374
Sampled inputs[:2]: tensor([[    0,   328,  5180,  ...,   344,  2356,   409],
        [    0, 38717,  1679,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5673e-04, -3.2241e-04, -6.2985e-05,  ..., -1.8831e-04,
          1.3881e-04, -2.3825e-05],
        [-9.1642e-06, -6.6943e-06,  6.9663e-06,  ..., -8.5011e-06,
         -6.5975e-06, -6.4746e-06],
        [-2.7135e-05, -2.0593e-05,  2.2113e-05,  ..., -2.4781e-05,
         -1.9222e-05, -1.8612e-05],
        [-2.0489e-05, -1.4469e-05,  1.6302e-05,  ..., -1.8954e-05,
         -1.4603e-05, -1.5229e-05],
        [-2.2396e-05, -1.7479e-05,  1.7807e-05,  ..., -2.0668e-05,
         -1.6823e-05, -1.4141e-05]], device='cuda:0')
Loss: 0.9680541157722473


Running epoch 1, step 1423, batch 375
Sampled inputs[:2]: tensor([[    0,    14,  5551,  ...,   668, 11988,  2538],
        [    0,  4215,  1478,  ...,   644,   409,  3803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8130e-04, -3.2699e-04,  2.0315e-05,  ..., -2.4129e-04,
          4.5705e-04, -1.6341e-05],
        [-1.0513e-05, -7.7076e-06,  7.8827e-06,  ..., -9.7826e-06,
         -7.5959e-06, -7.4506e-06],
        [-3.1158e-05, -2.3723e-05,  2.5064e-05,  ..., -2.8536e-05,
         -2.2158e-05, -2.1443e-05],
        [-2.3484e-05, -1.6659e-05,  1.8418e-05,  ..., -2.1800e-05,
         -1.6794e-05, -1.7494e-05],
        [-2.5898e-05, -2.0236e-05,  2.0340e-05,  ..., -2.3946e-05,
         -1.9491e-05, -1.6421e-05]], device='cuda:0')
Loss: 0.9843920469284058
Graident accumulation at epoch 1, step 1423, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8459e-05,  1.0244e-05, -6.9897e-05,  ..., -8.3181e-05,
          2.8505e-05, -1.0925e-05],
        [-1.1136e-05, -7.7565e-06,  8.2683e-06,  ..., -9.9161e-06,
         -7.3014e-06, -7.6035e-06],
        [ 3.2479e-05,  5.4486e-05, -2.6559e-05,  ...,  3.6385e-05,
          4.4229e-05,  7.7183e-06],
        [-1.3454e-05, -4.5533e-06,  1.2439e-05,  ..., -1.2327e-05,
         -5.5538e-06, -1.3137e-05],
        [-1.7749e-05, -1.1228e-05,  1.3979e-05,  ..., -1.4342e-05,
         -9.2937e-06, -1.1867e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3805e-08, 5.5112e-08, 6.5425e-08,  ..., 2.5672e-08, 1.4992e-07,
         3.7888e-08],
        [7.9148e-11, 5.1444e-11, 2.0142e-11,  ..., 5.5139e-11, 1.6643e-11,
         2.2999e-11],
        [3.6911e-09, 2.3066e-09, 1.2407e-09,  ..., 3.0266e-09, 1.1834e-09,
         1.0067e-09],
        [9.8644e-10, 9.1911e-10, 3.3741e-10,  ..., 8.2338e-10, 5.6682e-10,
         3.3960e-10],
        [4.0003e-10, 2.2813e-10, 8.1670e-11,  ..., 2.9444e-10, 8.7611e-11,
         1.1061e-10]], device='cuda:0')
optimizer state dict: 178.0
lr: [4.9238293885951606e-06, 4.9238293885951606e-06]
scheduler_last_epoch: 178


Running epoch 1, step 1424, batch 376
Sampled inputs[:2]: tensor([[    0,  2220,  1110,  ...,   382,    18,    13],
        [    0,  9088,  7217,  ...,   199, 17822,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1917e-04,  1.3427e-05, -4.3633e-05,  ..., -5.1353e-05,
         -1.0339e-05,  6.7533e-05],
        [-1.3039e-06, -9.8348e-07,  1.0952e-06,  ..., -1.1697e-06,
         -8.0094e-07, -8.0466e-07],
        [-4.0233e-06, -3.1888e-06,  3.5763e-06,  ..., -3.6359e-06,
         -2.4885e-06, -2.5034e-06],
        [-2.9951e-06, -2.2054e-06,  2.6673e-06,  ..., -2.6971e-06,
         -1.8105e-06, -1.9670e-06],
        [-3.1143e-06, -2.5779e-06,  2.6673e-06,  ..., -2.8610e-06,
         -2.1011e-06, -1.7881e-06]], device='cuda:0')
Loss: 0.974746584892273


Running epoch 1, step 1425, batch 377
Sampled inputs[:2]: tensor([[    0,   300,   369,  ...,    12,   970,    12],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8758e-04,  3.9009e-05, -5.1995e-05,  ..., -1.8485e-04,
          5.7809e-06,  5.0276e-05],
        [-2.6226e-06, -1.9670e-06,  2.1607e-06,  ..., -2.3916e-06,
         -1.7695e-06, -1.6242e-06],
        [-8.0764e-06, -6.3777e-06,  7.0333e-06,  ..., -7.4059e-06,
         -5.4687e-06, -5.0217e-06],
        [-6.0797e-06, -4.4256e-06,  5.2601e-06,  ..., -5.5283e-06,
         -4.0457e-06, -4.0084e-06],
        [-6.2585e-06, -5.1409e-06,  5.2899e-06,  ..., -5.8264e-06,
         -4.5449e-06, -3.6061e-06]], device='cuda:0')
Loss: 0.9886969923973083


Running epoch 1, step 1426, batch 378
Sampled inputs[:2]: tensor([[    0,    14,  2787,  ...,  9674,  2491,    12],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0457e-04,  3.4576e-05, -6.0817e-05,  ..., -2.1513e-04,
          5.3764e-05,  5.8084e-05],
        [-4.0606e-06, -2.9802e-06,  3.2783e-06,  ..., -3.6135e-06,
         -2.6599e-06, -2.4959e-06],
        [-1.2398e-05, -9.5665e-06,  1.0580e-05,  ..., -1.1086e-05,
         -8.1956e-06, -7.6443e-06],
        [-9.2387e-06, -6.6012e-06,  7.8231e-06,  ..., -8.2105e-06,
         -5.9828e-06, -6.0201e-06],
        [-9.6560e-06, -7.7337e-06,  8.0317e-06,  ..., -8.7768e-06,
         -6.8396e-06, -5.5432e-06]], device='cuda:0')
Loss: 0.9666871428489685


Running epoch 1, step 1427, batch 379
Sampled inputs[:2]: tensor([[    0,   278,  4452,  ...,    14,    18,  3046],
        [    0, 47684,   292,  ...,   287, 49958, 22022]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4093e-05, -3.8052e-05, -1.0173e-04,  ..., -3.0103e-04,
          2.7392e-05,  2.7253e-05],
        [-5.4166e-06, -3.9153e-06,  4.4107e-06,  ..., -4.7833e-06,
         -3.5316e-06, -3.2894e-06],
        [-1.6361e-05, -1.2442e-05,  1.4082e-05,  ..., -1.4484e-05,
         -1.0759e-05, -9.8944e-06],
        [-1.2338e-05, -8.6874e-06,  1.0520e-05,  ..., -1.0863e-05,
         -7.9498e-06, -7.9200e-06],
        [-1.2875e-05, -1.0163e-05,  1.0833e-05,  ..., -1.1563e-05,
         -9.0599e-06, -7.2271e-06]], device='cuda:0')
Loss: 0.9691025018692017


Running epoch 1, step 1428, batch 380
Sampled inputs[:2]: tensor([[   0,  898, 1427,  ...,  508, 1860,  266],
        [   0, 1412,   35,  ..., 6077,  298, 1826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7827e-06,  1.1983e-04, -3.1893e-05,  ..., -3.8851e-04,
         -1.5602e-04,  2.7126e-05],
        [-6.7502e-06, -4.8690e-06,  5.3644e-06,  ..., -5.9828e-06,
         -4.5896e-06, -4.2021e-06],
        [-2.0206e-05, -1.5378e-05,  1.7062e-05,  ..., -1.7926e-05,
         -1.3813e-05, -1.2472e-05],
        [-1.5363e-05, -1.0803e-05,  1.2770e-05,  ..., -1.3605e-05,
         -1.0379e-05, -1.0081e-05],
        [-1.6138e-05, -1.2681e-05,  1.3277e-05,  ..., -1.4514e-05,
         -1.1757e-05, -9.2536e-06]], device='cuda:0')
Loss: 0.955624520778656


Running epoch 1, step 1429, batch 381
Sampled inputs[:2]: tensor([[    0,   409,  4146,  ...,     9,   360,   259],
        [    0,   342, 22510,  ..., 49108,   278, 25904]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2388e-04,  1.0689e-04, -1.2778e-05,  ..., -4.1032e-04,
         -3.6174e-04,  3.9919e-05],
        [-8.0615e-06, -5.7369e-06,  6.4448e-06,  ..., -7.1600e-06,
         -5.4725e-06, -5.0887e-06],
        [-2.3991e-05, -1.8045e-05,  2.0415e-05,  ..., -2.1309e-05,
         -1.6406e-05, -1.4946e-05],
        [-1.8284e-05, -1.2703e-05,  1.5333e-05,  ..., -1.6242e-05,
         -1.2361e-05, -1.2152e-05],
        [-1.9193e-05, -1.4916e-05,  1.5929e-05,  ..., -1.7270e-05,
         -1.3992e-05, -1.1094e-05]], device='cuda:0')
Loss: 0.9522956609725952


Running epoch 1, step 1430, batch 382
Sampled inputs[:2]: tensor([[    0,  3761,    12,  ...,  3476, 20966,   391],
        [    0,   344,  2183,  ...,    14,   759,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5985e-04,  2.5265e-05,  1.2901e-04,  ..., -3.2095e-04,
         -2.6964e-04,  2.2025e-04],
        [-9.4771e-06, -6.6236e-06,  7.3239e-06,  ..., -8.4788e-06,
         -6.4634e-06, -6.1616e-06],
        [-2.8074e-05, -2.0772e-05,  2.3171e-05,  ..., -2.5019e-05,
         -1.9252e-05, -1.7911e-05],
        [-2.1264e-05, -1.4544e-05,  1.7241e-05,  ..., -1.9029e-05,
         -1.4476e-05, -1.4506e-05],
        [-2.2829e-05, -1.7360e-05,  1.8358e-05,  ..., -2.0593e-05,
         -1.6600e-05, -1.3553e-05]], device='cuda:0')
Loss: 0.9736316204071045


Running epoch 1, step 1431, batch 383
Sampled inputs[:2]: tensor([[   0,  591, 2036,  ...,  266, 1027,  278],
        [   0, 3308,  259,  ...,   14, 6349, 1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2004e-04,  2.5265e-05,  3.5873e-04,  ..., -4.1585e-04,
         -2.9177e-04,  2.7628e-04],
        [-1.0863e-05, -7.6517e-06,  8.3074e-06,  ..., -9.7305e-06,
         -7.4133e-06, -7.0743e-06],
        [-3.2127e-05, -2.3961e-05,  2.6271e-05,  ..., -2.8700e-05,
         -2.2098e-05, -2.0579e-05],
        [-2.4378e-05, -1.6809e-05,  1.9550e-05,  ..., -2.1860e-05,
         -1.6607e-05, -1.6652e-05],
        [-2.6211e-05, -2.0087e-05,  2.0862e-05,  ..., -2.3708e-05,
         -1.9118e-05, -1.5639e-05]], device='cuda:0')
Loss: 0.9569643139839172
Graident accumulation at epoch 1, step 1431, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.8617e-05,  1.1746e-05, -2.7034e-05,  ..., -1.1645e-04,
         -3.5221e-06,  1.7796e-05],
        [-1.1109e-05, -7.7460e-06,  8.2722e-06,  ..., -9.8975e-06,
         -7.3126e-06, -7.5506e-06],
        [ 2.6018e-05,  4.6641e-05, -2.1276e-05,  ...,  2.9876e-05,
          3.7596e-05,  4.8887e-06],
        [-1.4546e-05, -5.7788e-06,  1.3150e-05,  ..., -1.3280e-05,
         -6.6592e-06, -1.3488e-05],
        [-1.8595e-05, -1.2114e-05,  1.4667e-05,  ..., -1.5279e-05,
         -1.0276e-05, -1.2245e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3854e-08, 5.5057e-08, 6.5488e-08,  ..., 2.5819e-08, 1.4985e-07,
         3.7926e-08],
        [7.9187e-11, 5.1451e-11, 2.0191e-11,  ..., 5.5178e-11, 1.6681e-11,
         2.3026e-11],
        [3.6885e-09, 2.3049e-09, 1.2402e-09,  ..., 3.0244e-09, 1.1828e-09,
         1.0061e-09],
        [9.8605e-10, 9.1847e-10, 3.3745e-10,  ..., 8.2303e-10, 5.6653e-10,
         3.3954e-10],
        [4.0031e-10, 2.2831e-10, 8.2024e-11,  ..., 2.9471e-10, 8.7889e-11,
         1.1074e-10]], device='cuda:0')
optimizer state dict: 179.0
lr: [4.817713993572543e-06, 4.817713993572543e-06]
scheduler_last_epoch: 179


Running epoch 1, step 1432, batch 384
Sampled inputs[:2]: tensor([[    0,   271,  8278,  ...,   271,  8278,  3560],
        [    0,    14, 49045,  ...,    12,   706,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8611e-06,  7.0771e-05, -1.1601e-04,  ..., -1.2418e-04,
          3.2022e-05, -6.7262e-05],
        [-1.3635e-06, -1.0282e-06,  1.0878e-06,  ..., -1.1921e-06,
         -9.6112e-07, -8.2701e-07],
        [-3.9935e-06, -3.1441e-06,  3.3975e-06,  ..., -3.5018e-06,
         -2.8163e-06, -2.3991e-06],
        [-3.0696e-06, -2.2948e-06,  2.5630e-06,  ..., -2.6822e-06,
         -2.1458e-06, -1.9521e-06],
        [-3.2187e-06, -2.6226e-06,  2.6971e-06,  ..., -2.8759e-06,
         -2.4587e-06, -1.7881e-06]], device='cuda:0')
Loss: 0.963321328163147


Running epoch 1, step 1433, batch 385
Sampled inputs[:2]: tensor([[   0, 5182,  446,  ...,  417,  199,   50],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2055e-04,  4.5055e-05, -9.6127e-05,  ..., -1.6046e-04,
         -2.7702e-05, -1.6533e-04],
        [-2.7344e-06, -1.9893e-06,  2.2128e-06,  ..., -2.3767e-06,
         -1.7993e-06, -1.7062e-06],
        [-8.1956e-06, -6.2287e-06,  7.0184e-06,  ..., -7.1228e-06,
         -5.3942e-06, -5.1111e-06],
        [-6.2138e-06, -4.4405e-06,  5.2452e-06,  ..., -5.3793e-06,
         -4.0308e-06, -4.0829e-06],
        [-6.4522e-06, -5.1111e-06,  5.4389e-06,  ..., -5.7220e-06,
         -4.6045e-06, -3.7253e-06]], device='cuda:0')
Loss: 0.9801875948905945


Running epoch 1, step 1434, batch 386
Sampled inputs[:2]: tensor([[    0,   491, 10524,  ...,  2218,  5627,  4199],
        [    0,  2914,   352,  ...,   897,   328,  1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0046e-04,  1.0498e-04, -1.9288e-04,  ..., -1.0360e-04,
         -2.0983e-04, -2.0968e-04],
        [-4.1574e-06, -2.9504e-06,  3.3304e-06,  ..., -3.6135e-06,
         -2.7344e-06, -2.5854e-06],
        [-1.2428e-05, -9.2983e-06,  1.0520e-05,  ..., -1.0818e-05,
         -8.2701e-06, -7.7188e-06],
        [-9.4324e-06, -6.5863e-06,  7.8678e-06,  ..., -8.1807e-06,
         -6.1616e-06, -6.1840e-06],
        [-9.8348e-06, -7.6741e-06,  8.1807e-06,  ..., -8.7470e-06,
         -7.0632e-06, -5.6922e-06]], device='cuda:0')
Loss: 0.9947304725646973


Running epoch 1, step 1435, batch 387
Sampled inputs[:2]: tensor([[    0,    13,  1924,  ...,  2117,   300, 26473],
        [    0, 13509,   472,  ...,  1805,    13, 27816]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6052e-05,  1.3230e-04, -1.3975e-04,  ..., -2.3749e-04,
         -5.3003e-04, -4.1983e-04],
        [-5.4091e-06, -3.7737e-06,  4.3213e-06,  ..., -4.7535e-06,
         -3.7029e-06, -3.4720e-06],
        [-1.5974e-05, -1.1742e-05,  1.3515e-05,  ..., -1.3947e-05,
         -1.0937e-05, -1.0058e-05],
        [-1.2204e-05, -8.3148e-06,  1.0177e-05,  ..., -1.0699e-05,
         -8.2925e-06, -8.2254e-06],
        [-1.2934e-05, -9.8795e-06,  1.0744e-05,  ..., -1.1519e-05,
         -9.5218e-06, -7.5698e-06]], device='cuda:0')
Loss: 0.8991788625717163


Running epoch 1, step 1436, batch 388
Sampled inputs[:2]: tensor([[   0,  271, 3616,  ...,   12, 1348, 5037],
        [   0,  957,  680,  ..., 2573,  669,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3972e-05, -1.7695e-05, -2.9208e-04,  ..., -1.9943e-04,
         -4.7727e-04, -5.9461e-04],
        [-6.6608e-06, -4.7162e-06,  5.3421e-06,  ..., -5.8711e-06,
         -4.5821e-06, -4.2282e-06],
        [-1.9684e-05, -1.4648e-05,  1.6749e-05,  ..., -1.7241e-05,
         -1.3545e-05, -1.2264e-05],
        [-1.5125e-05, -1.0490e-05,  1.2681e-05,  ..., -1.3322e-05,
         -1.0334e-05, -1.0103e-05],
        [-1.5885e-05, -1.2293e-05,  1.3277e-05,  ..., -1.4171e-05,
         -1.1727e-05, -9.1568e-06]], device='cuda:0')
Loss: 0.9943044185638428


Running epoch 1, step 1437, batch 389
Sampled inputs[:2]: tensor([[    0,  1184,  1451,  ...,   934,   352,   266],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3726e-05, -3.2040e-05, -3.5429e-04,  ..., -1.9848e-04,
         -4.0104e-04, -6.1681e-04],
        [-7.9796e-06, -5.6699e-06,  6.3926e-06,  ..., -7.0557e-06,
         -5.4762e-06, -5.1260e-06],
        [-2.3648e-05, -1.7613e-05,  2.0102e-05,  ..., -2.0757e-05,
         -1.6212e-05, -1.4931e-05],
        [-1.8179e-05, -1.2621e-05,  1.5214e-05,  ..., -1.6049e-05,
         -1.2361e-05, -1.2293e-05],
        [-1.9059e-05, -1.4737e-05,  1.5885e-05,  ..., -1.7017e-05,
         -1.3992e-05, -1.1124e-05]], device='cuda:0')
Loss: 0.980877697467804


Running epoch 1, step 1438, batch 390
Sampled inputs[:2]: tensor([[   0,   14, 7870,  ...,  284,  830,  292],
        [   0,  298,  374,  ...,  298,  413,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4700e-05,  3.4519e-04, -6.3632e-05,  ..., -2.7734e-04,
         -4.8151e-04, -4.2531e-04],
        [-9.2909e-06, -6.5081e-06,  7.2420e-06,  ..., -8.2850e-06,
         -6.4000e-06, -6.1095e-06],
        [-2.7493e-05, -2.0176e-05,  2.2829e-05,  ..., -2.4244e-05,
         -1.8910e-05, -1.7643e-05],
        [-2.1130e-05, -1.4402e-05,  1.7211e-05,  ..., -1.8790e-05,
         -1.4417e-05, -1.4544e-05],
        [-2.2367e-05, -1.6987e-05,  1.8179e-05,  ..., -2.0027e-05,
         -1.6421e-05, -1.3255e-05]], device='cuda:0')
Loss: 0.9020073413848877


Running epoch 1, step 1439, batch 391
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,  2326,  3595,  6887],
        [    0,  1016,  1387,  ..., 12156, 14838,  3550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0894e-05,  4.3419e-04, -8.9064e-05,  ..., -1.6433e-04,
         -4.7053e-04, -5.7335e-04],
        [-1.0610e-05, -7.4618e-06,  8.3074e-06,  ..., -9.4622e-06,
         -7.2829e-06, -6.9141e-06],
        [-3.1427e-05, -2.3112e-05,  2.6241e-05,  ..., -2.7731e-05,
         -2.1532e-05, -2.0012e-05],
        [-2.4155e-05, -1.6533e-05,  1.9759e-05,  ..., -2.1487e-05,
         -1.6414e-05, -1.6481e-05],
        [-2.5511e-05, -1.9416e-05,  2.0847e-05,  ..., -2.2843e-05,
         -1.8671e-05, -1.4976e-05]], device='cuda:0')
Loss: 0.9889901280403137
Graident accumulation at epoch 1, step 1439, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0110, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.2666e-05,  5.3991e-05, -3.3237e-05,  ..., -1.2124e-04,
         -5.0223e-05, -4.1318e-05],
        [-1.1059e-05, -7.7176e-06,  8.2757e-06,  ..., -9.8540e-06,
         -7.3096e-06, -7.4870e-06],
        [ 2.0274e-05,  3.9666e-05, -1.6525e-05,  ...,  2.4116e-05,
          3.1684e-05,  2.3986e-06],
        [-1.5507e-05, -6.8542e-06,  1.3811e-05,  ..., -1.4101e-05,
         -7.6346e-06, -1.3787e-05],
        [-1.9286e-05, -1.2844e-05,  1.5285e-05,  ..., -1.6035e-05,
         -1.1116e-05, -1.2518e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3800e-08, 5.5191e-08, 6.5430e-08,  ..., 2.5820e-08, 1.4993e-07,
         3.8217e-08],
        [7.9221e-11, 5.1455e-11, 2.0239e-11,  ..., 5.5213e-11, 1.6718e-11,
         2.3051e-11],
        [3.6858e-09, 2.3031e-09, 1.2396e-09,  ..., 3.0222e-09, 1.1820e-09,
         1.0055e-09],
        [9.8565e-10, 9.1782e-10, 3.3750e-10,  ..., 8.2267e-10, 5.6624e-10,
         3.3947e-10],
        [4.0056e-10, 2.2846e-10, 8.2376e-11,  ..., 2.9493e-10, 8.8150e-11,
         1.1086e-10]], device='cuda:0')
optimizer state dict: 180.0
lr: [4.712390497088522e-06, 4.712390497088522e-06]
scheduler_last_epoch: 180


Running epoch 1, step 1440, batch 392
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0,    17,  4110,  ...,   287,  7115,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9315e-05,  5.6820e-05, -3.7644e-05,  ..., -5.4688e-05,
         -1.8096e-04, -5.5080e-05],
        [-1.2890e-06, -9.8348e-07,  1.1101e-06,  ..., -1.1548e-06,
         -8.7172e-07, -7.5996e-07],
        [-3.9041e-06, -3.0696e-06,  3.5465e-06,  ..., -3.4571e-06,
         -2.6077e-06, -2.2650e-06],
        [-2.9653e-06, -2.2054e-06,  2.6673e-06,  ..., -2.6375e-06,
         -1.9670e-06, -1.8552e-06],
        [-3.0249e-06, -2.5034e-06,  2.7120e-06,  ..., -2.7418e-06,
         -2.1905e-06, -1.6168e-06]], device='cuda:0')
Loss: 0.984296977519989


Running epoch 1, step 1441, batch 393
Sampled inputs[:2]: tensor([[    0,   688,  2353,  ..., 20538, 10393,    12],
        [    0,   600,   518,  ...,  3134,   278, 37342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5965e-05,  9.4604e-06, -1.9266e-05,  ..., -1.2888e-04,
         -1.9157e-04, -2.9520e-05],
        [-2.5928e-06, -1.9334e-06,  2.1830e-06,  ..., -2.2948e-06,
         -1.7285e-06, -1.5460e-06],
        [-7.8082e-06, -6.0648e-06,  6.9290e-06,  ..., -6.9141e-06,
         -5.2303e-06, -4.6641e-06],
        [-6.0350e-06, -4.3809e-06,  5.3048e-06,  ..., -5.3197e-06,
         -3.9637e-06, -3.8221e-06],
        [-6.1244e-06, -5.0068e-06,  5.3495e-06,  ..., -5.5432e-06,
         -4.4256e-06, -3.3826e-06]], device='cuda:0')
Loss: 0.9782185554504395


Running epoch 1, step 1442, batch 394
Sampled inputs[:2]: tensor([[    0,    45,    17,  ...,   278,  4112,    14],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9990e-05,  9.3056e-05,  1.2460e-06,  ..., -1.7564e-04,
         -3.6842e-04, -4.5121e-05],
        [-3.9041e-06, -2.8424e-06,  3.2187e-06,  ..., -3.4422e-06,
         -2.5406e-06, -2.3656e-06],
        [ 7.1541e-05,  6.5328e-05, -5.8030e-05,  ...,  5.4167e-05,
          8.3013e-05,  1.2014e-05],
        [-9.0748e-06, -6.4075e-06,  7.8082e-06,  ..., -7.9721e-06,
         -5.8115e-06, -5.8338e-06],
        [-9.2685e-06, -7.3910e-06,  7.9274e-06,  ..., -8.3297e-06,
         -6.5267e-06, -5.1782e-06]], device='cuda:0')
Loss: 0.9585025310516357


Running epoch 1, step 1443, batch 395
Sampled inputs[:2]: tensor([[   0,  342, 4014,  ...,  368,  408, 2105],
        [   0, 3227,  300,  ..., 1817, 5709,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1276e-04,  1.1576e-04,  1.0349e-04,  ..., -1.5892e-04,
         -3.1955e-04,  4.1435e-06],
        [-5.1707e-06, -3.8184e-06,  4.2543e-06,  ..., -4.5449e-06,
         -3.3788e-06, -3.1404e-06],
        [ 6.7548e-05,  6.2184e-05, -5.4617e-05,  ...,  5.0725e-05,
          8.0421e-05,  9.6153e-06],
        [-1.2040e-05, -8.6278e-06,  1.0327e-05,  ..., -1.0550e-05,
         -7.7486e-06, -7.7710e-06],
        [-1.2308e-05, -9.8795e-06,  1.0476e-05,  ..., -1.0997e-05,
         -8.6427e-06, -6.8620e-06]], device='cuda:0')
Loss: 0.9813868403434753


Running epoch 1, step 1444, batch 396
Sampled inputs[:2]: tensor([[    0, 48598,  3313,  ...,  3482,    12,  1099],
        [    0,  2652,   271,  ...,   634,  1921,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0426e-04,  1.3139e-04,  4.2666e-05,  ..., -2.1411e-04,
         -5.3710e-04,  1.9881e-05],
        [-6.4969e-06, -4.7795e-06,  5.3123e-06,  ..., -5.6624e-06,
         -4.2506e-06, -3.9861e-06],
        [ 6.3584e-05,  5.9159e-05, -5.1235e-05,  ...,  4.7402e-05,
          7.7798e-05,  7.1268e-06],
        [-1.5080e-05, -1.0759e-05,  1.2875e-05,  ..., -1.3098e-05,
         -9.7305e-06, -9.8124e-06],
        [-1.5393e-05, -1.2323e-05,  1.3053e-05,  ..., -1.3620e-05,
         -1.0818e-05, -8.6427e-06]], device='cuda:0')
Loss: 0.949826180934906


Running epoch 1, step 1445, batch 397
Sampled inputs[:2]: tensor([[    0,   278,  1059,  ...,   300,  1877,    13],
        [    0,   352,   721,  ...,   634, 17642,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0122e-04,  1.4321e-04, -9.5430e-05,  ..., -2.4351e-04,
         -6.1920e-04,  2.6470e-06],
        [-7.7635e-06, -5.6662e-06,  6.3553e-06,  ..., -6.7949e-06,
         -5.0515e-06, -4.7870e-06],
        [ 5.9784e-05,  5.6417e-05, -4.7897e-05,  ...,  4.4049e-05,
          7.5399e-05,  4.8022e-06],
        [-1.8060e-05, -1.2755e-05,  1.5453e-05,  ..., -1.5751e-05,
         -1.1593e-05, -1.1794e-05],
        [-1.8373e-05, -1.4558e-05,  1.5602e-05,  ..., -1.6272e-05,
         -1.2845e-05, -1.0304e-05]], device='cuda:0')
Loss: 0.9432430267333984


Running epoch 1, step 1446, batch 398
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4708e-04,  3.0257e-04, -8.9011e-05,  ..., -3.2302e-04,
         -6.6464e-04, -2.0357e-05],
        [-9.0599e-06, -6.5528e-06,  7.4655e-06,  ..., -7.9274e-06,
         -5.9120e-06, -5.5395e-06],
        [ 5.5970e-05,  5.3631e-05, -4.4425e-05,  ...,  4.0681e-05,
          7.2776e-05,  2.5968e-06],
        [-2.1100e-05, -1.4797e-05,  1.8165e-05,  ..., -1.8418e-05,
         -1.3620e-05, -1.3642e-05],
        [-2.1368e-05, -1.6838e-05,  1.8269e-05,  ..., -1.8954e-05,
         -1.5035e-05, -1.1906e-05]], device='cuda:0')
Loss: 0.981696367263794


Running epoch 1, step 1447, batch 399
Sampled inputs[:2]: tensor([[   0, 4137,  300,  ..., 2579,  278,  266],
        [   0, 6408,  391,  ...,  870,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8448e-04,  3.0669e-04, -1.5267e-04,  ..., -3.0945e-04,
         -6.7220e-04,  3.1351e-05],
        [-1.0408e-05, -7.5661e-06,  8.5011e-06,  ..., -9.1121e-06,
         -6.8173e-06, -6.3330e-06],
        [ 5.1916e-05,  5.0397e-05, -4.1117e-05,  ...,  3.7120e-05,
          7.0005e-05,  2.1265e-07],
        [-2.4185e-05, -1.7092e-05,  2.0653e-05,  ..., -2.1145e-05,
         -1.5691e-05, -1.5579e-05],
        [-2.4557e-05, -1.9506e-05,  2.0832e-05,  ..., -2.1815e-05,
         -1.7390e-05, -1.3649e-05]], device='cuda:0')
Loss: 0.9631648063659668
Graident accumulation at epoch 1, step 1447, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.6847e-05,  7.9261e-05, -4.5181e-05,  ..., -1.4006e-04,
         -1.1242e-04, -3.4051e-05],
        [-1.0994e-05, -7.7024e-06,  8.2983e-06,  ..., -9.7798e-06,
         -7.2604e-06, -7.3716e-06],
        [ 2.3438e-05,  4.0739e-05, -1.8984e-05,  ...,  2.5416e-05,
          3.5516e-05,  2.1800e-06],
        [-1.6375e-05, -7.8780e-06,  1.4495e-05,  ..., -1.4805e-05,
         -8.4402e-06, -1.3967e-05],
        [-1.9814e-05, -1.3510e-05,  1.5840e-05,  ..., -1.6613e-05,
         -1.1743e-05, -1.2631e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3827e-08, 5.5230e-08, 6.5388e-08,  ..., 2.5890e-08, 1.5023e-07,
         3.8180e-08],
        [7.9250e-11, 5.1461e-11, 2.0291e-11,  ..., 5.5241e-11, 1.6747e-11,
         2.3068e-11],
        [3.6848e-09, 2.3034e-09, 1.2401e-09,  ..., 3.0205e-09, 1.1858e-09,
         1.0045e-09],
        [9.8525e-10, 9.1720e-10, 3.3759e-10,  ..., 8.2229e-10, 5.6592e-10,
         3.3937e-10],
        [4.0077e-10, 2.2861e-10, 8.2728e-11,  ..., 2.9511e-10, 8.8364e-11,
         1.1093e-10]], device='cuda:0')
optimizer state dict: 181.0
lr: [4.6078749934927426e-06, 4.6078749934927426e-06]
scheduler_last_epoch: 181


Running epoch 1, step 1448, batch 400
Sampled inputs[:2]: tensor([[    0,    67,   695,  ...,   437,   266, 44563],
        [    0,  2816,   292,  ...,  3662,   461,  2723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9215e-05,  5.7253e-05, -2.9239e-04,  ..., -9.5772e-05,
         -1.6171e-04, -2.6001e-04],
        [-1.2666e-06, -8.6799e-07,  1.0803e-06,  ..., -1.0878e-06,
         -8.1584e-07, -7.7114e-07],
        [-3.8445e-06, -2.7418e-06,  3.4720e-06,  ..., -3.2932e-06,
         -2.4587e-06, -2.3097e-06],
        [-3.0249e-06, -1.9968e-06,  2.7120e-06,  ..., -2.5779e-06,
         -1.9073e-06, -1.9372e-06],
        [-2.9951e-06, -2.2352e-06,  2.6375e-06,  ..., -2.6077e-06,
         -2.0564e-06, -1.6615e-06]], device='cuda:0')
Loss: 0.9642850756645203


Running epoch 1, step 1449, batch 401
Sampled inputs[:2]: tensor([[    0,    12,  4567,  ...,  4154,  1799, 11883],
        [    0, 15689,   278,  ..., 12016,   271,  4353]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8686e-04,  1.1079e-04, -4.7094e-04,  ..., -8.3264e-05,
         -2.8356e-04, -1.6433e-04],
        [-2.5779e-06, -1.8179e-06,  2.2352e-06,  ..., -2.1756e-06,
         -1.6354e-06, -1.5423e-06],
        [-7.8976e-06, -5.7667e-06,  7.1824e-06,  ..., -6.6608e-06,
         -4.9621e-06, -4.7237e-06],
        [-6.2138e-06, -4.2170e-06,  5.6326e-06,  ..., -5.2005e-06,
         -3.8445e-06, -3.9190e-06],
        [-6.0499e-06, -4.6194e-06,  5.3793e-06,  ..., -5.1856e-06,
         -4.1127e-06, -3.3453e-06]], device='cuda:0')
Loss: 0.9866974949836731


Running epoch 1, step 1450, batch 402
Sampled inputs[:2]: tensor([[    0,  5685,   565,  ..., 23968,    14,   381],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8345e-05,  1.0247e-04, -4.1144e-04,  ...,  3.0266e-05,
         -1.3829e-04, -1.5354e-04],
        [-3.9488e-06, -2.6971e-06,  3.1888e-06,  ..., -3.4049e-06,
         -2.5779e-06, -2.4959e-06],
        [-1.1951e-05, -8.4788e-06,  1.0222e-05,  ..., -1.0207e-05,
         -7.6890e-06, -7.4208e-06],
        [-9.2834e-06, -6.1169e-06,  7.8529e-06,  ..., -7.9423e-06,
         -5.9307e-06, -6.1244e-06],
        [-9.4473e-06, -6.9588e-06,  7.8678e-06,  ..., -8.2105e-06,
         -6.5267e-06, -5.4464e-06]], device='cuda:0')
Loss: 0.965663731098175


Running epoch 1, step 1451, batch 403
Sampled inputs[:2]: tensor([[    0, 23530,  6713,  ...,  2813,   518,   266],
        [    0,  2344,   271,  ...,  5415,    14,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7960e-05,  1.3544e-04, -5.2607e-04,  ..., -7.6050e-05,
         -1.6602e-04, -2.8547e-04],
        [-5.2378e-06, -3.5241e-06,  4.1723e-06,  ..., -4.5076e-06,
         -3.4161e-06, -3.3639e-06],
        [-1.5885e-05, -1.1101e-05,  1.3426e-05,  ..., -1.3515e-05,
         -1.0177e-05, -9.9689e-06],
        [-1.2323e-05, -7.9796e-06,  1.0297e-05,  ..., -1.0520e-05,
         -7.8678e-06, -8.2701e-06],
        [-1.2577e-05, -9.1344e-06,  1.0371e-05,  ..., -1.0893e-05,
         -8.6427e-06, -7.3165e-06]], device='cuda:0')
Loss: 0.9488507509231567


Running epoch 1, step 1452, batch 404
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0,  1842,   360,  ..., 10251,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4327e-05,  1.3628e-04, -6.0320e-04,  ..., -6.5696e-05,
         -1.4412e-04, -2.5268e-04],
        [-6.5938e-06, -4.4778e-06,  5.2527e-06,  ..., -5.6401e-06,
         -4.2692e-06, -4.1574e-06],
        [-1.9908e-05, -1.4082e-05,  1.6853e-05,  ..., -1.6883e-05,
         -1.2770e-05, -1.2338e-05],
        [-1.5423e-05, -1.0125e-05,  1.2890e-05,  ..., -1.3098e-05,
         -9.8199e-06, -1.0192e-05],
        [-1.5736e-05, -1.1593e-05,  1.3009e-05,  ..., -1.3590e-05,
         -1.0848e-05, -9.0301e-06]], device='cuda:0')
Loss: 0.9700663089752197


Running epoch 1, step 1453, batch 405
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0478e-04,  2.1558e-04, -6.2461e-04,  ...,  1.0002e-04,
         -6.2896e-05, -3.1665e-04],
        [-7.9870e-06, -5.2378e-06,  6.2287e-06,  ..., -6.7949e-06,
         -5.0887e-06, -5.1633e-06],
        [-2.4050e-05, -1.6525e-05,  1.9982e-05,  ..., -2.0280e-05,
         -1.5229e-05, -1.5318e-05],
        [-1.8567e-05, -1.1794e-05,  1.5184e-05,  ..., -1.5706e-05,
         -1.1653e-05, -1.2547e-05],
        [-1.9059e-05, -1.3605e-05,  1.5453e-05,  ..., -1.6391e-05,
         -1.2979e-05, -1.1265e-05]], device='cuda:0')
Loss: 0.9495270252227783


Running epoch 1, step 1454, batch 406
Sampled inputs[:2]: tensor([[    0,  4566,   300,  ...,   271,  1644, 16473],
        [    0,  1445,  3597,  ...,   281,    78,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3516e-04,  2.9805e-04, -5.2725e-04,  ..., -4.1231e-05,
         -3.4722e-04, -2.7083e-04],
        [-9.3058e-06, -6.0759e-06,  7.3016e-06,  ..., -7.9721e-06,
         -5.9120e-06, -5.9716e-06],
        [-2.7955e-05, -1.9148e-05,  2.3335e-05,  ..., -2.3738e-05,
         -1.7673e-05, -1.7673e-05],
        [-2.1622e-05, -1.3657e-05,  1.7762e-05,  ..., -1.8418e-05,
         -1.3530e-05, -1.4529e-05],
        [-2.2128e-05, -1.5751e-05,  1.8030e-05,  ..., -1.9148e-05,
         -1.5035e-05, -1.2971e-05]], device='cuda:0')
Loss: 0.9800921678543091


Running epoch 1, step 1455, batch 407
Sampled inputs[:2]: tensor([[    0, 49141,    14,  ...,   342,   259,  1943],
        [    0,  3703,   278,  ...,  9807,    14, 10365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4287e-04,  3.4651e-04, -4.5936e-04,  ..., -1.5800e-07,
         -4.4421e-04, -2.2310e-04],
        [-1.0550e-05, -7.0222e-06,  8.3745e-06,  ..., -9.0823e-06,
         -6.7800e-06, -6.7614e-06],
        [-3.1725e-05, -2.2098e-05,  2.6762e-05,  ..., -2.7061e-05,
         -2.0266e-05, -2.0012e-05],
        [-2.4557e-05, -1.5803e-05,  2.0415e-05,  ..., -2.1026e-05,
         -1.5542e-05, -1.6510e-05],
        [-2.5034e-05, -1.8120e-05,  2.0623e-05,  ..., -2.1756e-05,
         -1.7166e-05, -1.4625e-05]], device='cuda:0')
Loss: 0.995985209941864
Graident accumulation at epoch 1, step 1455, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.4449e-05,  1.0599e-04, -8.6598e-05,  ..., -1.2607e-04,
         -1.4560e-04, -5.2956e-05],
        [-1.0949e-05, -7.6344e-06,  8.3059e-06,  ..., -9.7101e-06,
         -7.2124e-06, -7.3105e-06],
        [ 1.7922e-05,  3.4455e-05, -1.4409e-05,  ...,  2.0168e-05,
          2.9938e-05, -3.9254e-08],
        [-1.7193e-05, -8.6704e-06,  1.5087e-05,  ..., -1.5427e-05,
         -9.1504e-06, -1.4221e-05],
        [-2.0336e-05, -1.3971e-05,  1.6318e-05,  ..., -1.7128e-05,
         -1.2285e-05, -1.2830e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3891e-08, 5.5294e-08, 6.5534e-08,  ..., 2.5865e-08, 1.5027e-07,
         3.8191e-08],
        [7.9282e-11, 5.1459e-11, 2.0341e-11,  ..., 5.5268e-11, 1.6777e-11,
         2.3090e-11],
        [3.6821e-09, 2.3015e-09, 1.2396e-09,  ..., 3.0182e-09, 1.1850e-09,
         1.0039e-09],
        [9.8486e-10, 9.1653e-10, 3.3767e-10,  ..., 8.2191e-10, 5.6559e-10,
         3.3931e-10],
        [4.0099e-10, 2.2871e-10, 8.3071e-11,  ..., 2.9529e-10, 8.8570e-11,
         1.1103e-10]], device='cuda:0')
optimizer state dict: 182.0
lr: [4.504183453666481e-06, 4.504183453666481e-06]
scheduler_last_epoch: 182


Running epoch 1, step 1456, batch 408
Sampled inputs[:2]: tensor([[    0,  1635,   266,  ...,   437,  3302,   287],
        [    0,   677, 25912,  ...,  2337,   292,  4462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4275e-05,  8.0595e-05,  1.9405e-04,  ...,  9.5318e-05,
          7.4779e-06, -1.5788e-05],
        [-1.3337e-06, -9.2387e-07,  8.9779e-07,  ..., -1.1474e-06,
         -9.0525e-07, -8.4564e-07],
        [-3.9041e-06, -2.9504e-06,  2.9057e-06,  ..., -3.3826e-06,
         -2.7418e-06, -2.4885e-06],
        [-3.0398e-06, -2.0862e-06,  2.1458e-06,  ..., -2.6375e-06,
         -2.1011e-06, -2.0117e-06],
        [-3.1888e-06, -2.4885e-06,  2.2948e-06,  ..., -2.8163e-06,
         -2.4140e-06, -1.8701e-06]], device='cuda:0')
Loss: 0.9480189681053162


Running epoch 1, step 1457, batch 409
Sampled inputs[:2]: tensor([[    0,    14, 15670,  ...,  2027,   417,   199],
        [    0,  6477,    12,  ...,  2931,   221,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1197e-04,  1.5200e-04,  2.8772e-04,  ...,  4.5685e-05,
          2.0869e-04, -5.4301e-05],
        [-2.6301e-06, -1.8924e-06,  1.9036e-06,  ..., -2.2873e-06,
         -1.8440e-06, -1.6801e-06],
        [-7.8380e-06, -6.0946e-06,  6.1691e-06,  ..., -6.8694e-06,
         -5.6475e-06, -5.0217e-06],
        [ 3.8090e-05,  8.0785e-05, -4.9353e-05,  ...,  3.8453e-05,
          6.8510e-05,  1.1221e-05],
        [-6.3479e-06, -5.0962e-06,  4.8578e-06,  ..., -5.6475e-06,
         -4.8876e-06, -3.7551e-06]], device='cuda:0')
Loss: 0.9803940057754517


Running epoch 1, step 1458, batch 410
Sampled inputs[:2]: tensor([[   0,  586, 1016,  ..., 7151, 8280,  300],
        [   0, 1751,  287,  ..., 6079, 1059,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2953e-05,  2.2154e-04,  3.0018e-04,  ..., -2.5721e-05,
          6.0188e-04, -2.8643e-06],
        [-3.9414e-06, -2.8610e-06,  2.9244e-06,  ..., -3.4347e-06,
         -2.7716e-06, -2.5332e-06],
        [-1.1683e-05, -9.1195e-06,  9.3877e-06,  ..., -1.0267e-05,
         -8.4490e-06, -7.5400e-06],
        [ 3.5154e-05,  7.8639e-05, -4.6954e-05,  ...,  3.5875e-05,
          6.6423e-05,  9.2089e-06],
        [-9.5218e-06, -7.6890e-06,  7.4655e-06,  ..., -8.5086e-06,
         -7.3761e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9660235047340393


Running epoch 1, step 1459, batch 411
Sampled inputs[:2]: tensor([[   0, 8158, 1416,  ...,  413,   29,  413],
        [   0,   14, 3609,  ...,  298,  413,   29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7681e-05,  3.6166e-04,  3.6812e-04,  ...,  6.1689e-06,
          6.8974e-04,  4.6209e-05],
        [-5.2303e-06, -3.7998e-06,  3.7923e-06,  ..., -4.6045e-06,
         -3.7402e-06, -3.3639e-06],
        [-1.5587e-05, -1.2115e-05,  1.2234e-05,  ..., -1.3798e-05,
         -1.1384e-05, -1.0014e-05],
        [ 3.2144e-05,  7.6538e-05, -4.4867e-05,  ...,  3.3163e-05,
          6.4188e-05,  7.1973e-06],
        [-1.2770e-05, -1.0237e-05,  9.7603e-06,  ..., -1.1474e-05,
         -9.9391e-06, -7.5847e-06]], device='cuda:0')
Loss: 0.9712498784065247


Running epoch 1, step 1460, batch 412
Sampled inputs[:2]: tensor([[    0,   494,   825,  ...,   897,   328,   275],
        [    0,  2973, 20362,  ...,   271, 43821, 11776]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5207e-04,  3.8893e-04,  4.6587e-04,  ..., -4.1670e-05,
          6.7883e-04,  1.1225e-04],
        [-6.5863e-06, -4.7982e-06,  4.8131e-06,  ..., -5.7593e-06,
         -4.7013e-06, -4.1984e-06],
        [ 9.1311e-05,  5.2523e-05, -3.1352e-05,  ...,  8.1392e-05,
          3.2113e-05,  3.8316e-05],
        [ 2.8925e-05,  7.4213e-05, -4.2349e-05,  ...,  3.0421e-05,
          6.1908e-05,  5.0962e-06],
        [-1.6049e-05, -1.2904e-05,  1.2308e-05,  ..., -1.4335e-05,
         -1.2442e-05, -9.4622e-06]], device='cuda:0')
Loss: 0.9995947480201721


Running epoch 1, step 1461, batch 413
Sampled inputs[:2]: tensor([[    0,   981,    12,  ...,   266, 12907,  6670],
        [    0,  1549,   824,  ...,  3609,   720,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0456e-04,  4.4775e-04,  5.4578e-04,  ..., -1.1845e-04,
          6.4786e-04,  2.1607e-04],
        [-7.8231e-06, -5.6624e-06,  5.7966e-06,  ..., -6.8396e-06,
         -5.5581e-06, -5.0142e-06],
        [ 8.7541e-05,  4.9721e-05, -2.8119e-05,  ...,  7.8129e-05,
          2.9535e-05,  3.5857e-05],
        [ 2.6005e-05,  7.2246e-05, -3.9905e-05,  ...,  2.7888e-05,
          5.9912e-05,  3.0398e-06],
        [-1.8984e-05, -1.5184e-05,  1.4767e-05,  ..., -1.6928e-05,
         -1.4603e-05, -1.1221e-05]], device='cuda:0')
Loss: 0.9426475763320923


Running epoch 1, step 1462, batch 414
Sampled inputs[:2]: tensor([[    0, 39224,    34,  ...,   401,  1716,   271],
        [    0,  1049,   292,  ...,   221,   380,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7533e-05,  7.2530e-04,  6.4190e-04,  ..., -2.3749e-05,
          6.7822e-04,  1.0180e-04],
        [-9.1270e-06, -6.5230e-06,  6.6496e-06,  ..., -7.9721e-06,
         -6.4522e-06, -5.9977e-06],
        [ 8.3726e-05,  4.7024e-05, -2.5362e-05,  ...,  7.4850e-05,
          2.6928e-05,  3.3071e-05],
        [ 2.3054e-05,  7.0369e-05, -3.7879e-05,  ...,  2.5310e-05,
          5.7870e-05,  7.1526e-07],
        [-2.2277e-05, -1.7524e-05,  1.7032e-05,  ..., -1.9789e-05,
         -1.6958e-05, -1.3456e-05]], device='cuda:0')
Loss: 0.9286741614341736


Running epoch 1, step 1463, batch 415
Sampled inputs[:2]: tensor([[    0, 18981,    13,  ...,   365,  2714,   408],
        [    0,   368,   729,  ...,   221,   380,  2830]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1603e-04,  7.1314e-04,  6.1056e-04,  ..., -4.3495e-05,
          7.1137e-04,  7.1718e-06],
        [-1.0431e-05, -7.5139e-06,  7.6331e-06,  ..., -9.0897e-06,
         -7.3463e-06, -6.7540e-06],
        [ 7.9762e-05,  4.3865e-05, -2.2143e-05,  ...,  7.1453e-05,
          2.4245e-05,  3.0746e-05],
        [ 2.0074e-05,  6.8134e-05, -3.5509e-05,  ...,  2.2762e-05,
          5.5858e-05, -1.1474e-06],
        [-2.5332e-05, -2.0057e-05,  1.9461e-05,  ..., -2.2456e-05,
         -1.9208e-05, -1.5080e-05]], device='cuda:0')
Loss: 0.9574096202850342
Graident accumulation at epoch 1, step 1463, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.6607e-05,  1.6670e-04, -1.6883e-05,  ..., -1.1781e-04,
         -5.9902e-05, -4.6943e-05],
        [-1.0897e-05, -7.6224e-06,  8.2386e-06,  ..., -9.6480e-06,
         -7.2258e-06, -7.2549e-06],
        [ 2.4106e-05,  3.5396e-05, -1.5183e-05,  ...,  2.5297e-05,
          2.9368e-05,  3.0393e-06],
        [-1.3466e-05, -9.9003e-07,  1.0027e-05,  ..., -1.1608e-05,
         -2.6495e-06, -1.2914e-05],
        [-2.0835e-05, -1.4580e-05,  1.6633e-05,  ..., -1.7660e-05,
         -1.2978e-05, -1.3055e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3850e-08, 5.5748e-08, 6.5841e-08,  ..., 2.5841e-08, 1.5063e-07,
         3.8153e-08],
        [7.9311e-11, 5.1464e-11, 2.0379e-11,  ..., 5.5295e-11, 1.6814e-11,
         2.3113e-11],
        [3.6848e-09, 2.3012e-09, 1.2388e-09,  ..., 3.0203e-09, 1.1844e-09,
         1.0038e-09],
        [9.8428e-10, 9.2026e-10, 3.3860e-10,  ..., 8.2161e-10, 5.6815e-10,
         3.3897e-10],
        [4.0123e-10, 2.2888e-10, 8.3366e-11,  ..., 2.9550e-10, 8.8851e-11,
         1.1115e-10]], device='cuda:0')
optimizer state dict: 183.0
lr: [4.401331722582158e-06, 4.401331722582158e-06]
scheduler_last_epoch: 183


Running epoch 1, step 1464, batch 416
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  1295,  1178,  ...,  4808,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2446e-04,  1.1708e-04, -1.1307e-04,  ..., -1.4595e-05,
         -1.5166e-05,  4.3449e-05],
        [-1.2890e-06, -9.7603e-07,  8.9779e-07,  ..., -1.2144e-06,
         -1.0058e-06, -8.5682e-07],
        [-3.8743e-06, -3.1441e-06,  2.9355e-06,  ..., -3.6657e-06,
         -3.1441e-06, -2.6077e-06],
        [-3.0249e-06, -2.2650e-06,  2.1905e-06,  ..., -2.8610e-06,
         -2.3991e-06, -2.1160e-06],
        [-3.2187e-06, -2.6822e-06,  2.3693e-06,  ..., -3.1143e-06,
         -2.7567e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9572696089744568


Running epoch 1, step 1465, batch 417
Sampled inputs[:2]: tensor([[    0,  2162,    73,  ...,   278,   266,  1059],
        [    0,  2950,    13,  ..., 16513,   300,  2205]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4295e-04,  1.3249e-04,  3.7935e-05,  ..., -1.3495e-04,
          2.6675e-04, -2.5900e-05],
        [-2.6375e-06, -2.0638e-06,  1.9856e-06,  ..., -2.3842e-06,
         -1.9409e-06, -1.6838e-06],
        [-8.0168e-06, -6.6161e-06,  6.4373e-06,  ..., -7.3016e-06,
         -6.0648e-06, -5.2005e-06],
        [-6.1095e-06, -4.6939e-06,  4.7684e-06,  ..., -5.5134e-06,
         -4.5002e-06, -4.0978e-06],
        [-6.4820e-06, -5.5283e-06,  5.0664e-06,  ..., -6.0201e-06,
         -5.2154e-06, -3.9488e-06]], device='cuda:0')
Loss: 0.9963496923446655


Running epoch 1, step 1466, batch 418
Sampled inputs[:2]: tensor([[    0,  1188,   278,  ...,   271,  8368,   292],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6017e-04,  2.1684e-04,  9.7527e-05,  ..., -1.3912e-04,
          3.6845e-04, -1.7108e-05],
        [-4.2692e-06, -2.9206e-06,  2.8908e-06,  ..., -3.7178e-06,
         -2.9393e-06, -2.9206e-06],
        [-1.2755e-05, -9.3132e-06,  9.2834e-06,  ..., -1.1146e-05,
         -8.9854e-06, -8.7470e-06],
        [-9.4324e-06, -6.4149e-06,  6.6906e-06,  ..., -8.2105e-06,
         -6.4969e-06, -6.6459e-06],
        [-1.1042e-05, -8.1062e-06,  7.6592e-06,  ..., -9.7901e-06,
         -8.1807e-06, -7.2420e-06]], device='cuda:0')
Loss: 0.9586885571479797


Running epoch 1, step 1467, batch 419
Sampled inputs[:2]: tensor([[   0, 8125, 5241,  ...,  328, 3227,  278],
        [   0,  443,   40,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0019e-04,  3.1516e-04,  9.9578e-05,  ..., -2.8539e-04,
          7.6888e-05, -1.8774e-04],
        [-5.5581e-06, -3.7812e-06,  3.8296e-06,  ..., -4.8429e-06,
         -3.8818e-06, -3.8147e-06],
        [-1.6659e-05, -1.1981e-05,  1.2353e-05,  ..., -1.4454e-05,
         -1.1757e-05, -1.1310e-05],
        [-1.2338e-05, -8.2552e-06,  8.9258e-06,  ..., -1.0729e-05,
         -8.5980e-06, -8.7470e-06],
        [-1.4201e-05, -1.0371e-05,  1.0103e-05,  ..., -1.2502e-05,
         -1.0550e-05, -9.1270e-06]], device='cuda:0')
Loss: 0.944339394569397


Running epoch 1, step 1468, batch 420
Sampled inputs[:2]: tensor([[   0,  287, 3284,  ...,  221,  493,  221],
        [   0, 2939,   14,  ..., 1702, 1481,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3143e-04,  3.0010e-04,  1.3929e-05,  ..., -2.3088e-04,
          1.6490e-04, -3.3642e-04],
        [-6.7726e-06, -4.6678e-06,  4.7162e-06,  ..., -5.9903e-06,
         -4.7907e-06, -4.7572e-06],
        [-2.0444e-05, -1.4767e-05,  1.5393e-05,  ..., -1.7866e-05,
         -1.4454e-05, -1.4037e-05],
        [-1.5259e-05, -1.0267e-05,  1.1176e-05,  ..., -1.3426e-05,
         -1.0714e-05, -1.1072e-05],
        [-1.7256e-05, -1.2666e-05,  1.2502e-05,  ..., -1.5274e-05,
         -1.2845e-05, -1.1139e-05]], device='cuda:0')
Loss: 0.9578810930252075


Running epoch 1, step 1469, batch 421
Sampled inputs[:2]: tensor([[    0,   767,  4478,  ...,   278,   266, 19201],
        [    0, 26138,    17,  ...,   401,  1867,  4977]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5990e-04,  2.5028e-04,  1.5156e-04,  ..., -2.9448e-04,
          1.5672e-04, -3.3621e-04],
        [-8.0541e-06, -5.6177e-06,  5.6922e-06,  ..., -7.1302e-06,
         -5.7220e-06, -5.6326e-06],
        [-2.4319e-05, -1.7822e-05,  1.8567e-05,  ..., -2.1338e-05,
         -1.7330e-05, -1.6630e-05],
        [-1.8269e-05, -1.2472e-05,  1.3605e-05,  ..., -1.6123e-05,
         -1.2934e-05, -1.3232e-05],
        [-2.0444e-05, -1.5244e-05,  1.5080e-05,  ..., -1.8150e-05,
         -1.5333e-05, -1.3120e-05]], device='cuda:0')
Loss: 0.9812596440315247


Running epoch 1, step 1470, batch 422
Sampled inputs[:2]: tensor([[    0,  1356,    18,  ...,    31,   333,   199],
        [    0,     9,  9925,  ...,   527, 23286,  6062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4674e-04,  3.4270e-04,  2.6825e-04,  ..., -2.4653e-04,
          7.9912e-05, -3.4210e-04],
        [-9.4399e-06, -6.5602e-06,  6.6534e-06,  ..., -8.3297e-06,
         -6.6683e-06, -6.5863e-06],
        [-2.8431e-05, -2.0802e-05,  2.1636e-05,  ..., -2.4855e-05,
         -2.0131e-05, -1.9416e-05],
        [ 7.7973e-05,  2.0105e-04, -1.0007e-04,  ...,  9.0481e-05,
          1.4241e-04,  3.4325e-05],
        [-2.3901e-05, -1.7807e-05,  1.7598e-05,  ..., -2.1145e-05,
         -1.7852e-05, -1.5311e-05]], device='cuda:0')
Loss: 0.9547551274299622


Running epoch 1, step 1471, batch 423
Sampled inputs[:2]: tensor([[    0,  2278,   292,  ..., 12060,  1319,   292],
        [    0,    12,  5820,  ...,     5,  2122,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2494e-04,  3.9701e-04,  2.0919e-04,  ..., -3.3035e-04,
          1.6704e-04, -2.1809e-04],
        [-1.0818e-05, -7.4990e-06,  7.6219e-06,  ..., -9.5367e-06,
         -7.6666e-06, -7.5623e-06],
        [-3.2485e-05, -2.3797e-05,  2.4706e-05,  ..., -2.8431e-05,
         -2.3097e-05, -2.2307e-05],
        [ 7.4858e-05,  1.9896e-04, -9.7786e-05,  ...,  8.7754e-05,
          1.4016e-04,  3.1985e-05],
        [-2.7433e-05, -2.0474e-05,  2.0191e-05,  ..., -2.4304e-05,
         -2.0593e-05, -1.7680e-05]], device='cuda:0')
Loss: 0.9704742431640625
Graident accumulation at epoch 1, step 1471, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0028, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.4453e-05,  1.8973e-04,  5.7247e-06,  ..., -1.3906e-04,
         -3.7208e-05, -6.4058e-05],
        [-1.0890e-05, -7.6100e-06,  8.1769e-06,  ..., -9.6369e-06,
         -7.2699e-06, -7.2856e-06],
        [ 1.8447e-05,  2.9477e-05, -1.1194e-05,  ...,  1.9924e-05,
          2.4122e-05,  5.0466e-07],
        [-4.6339e-06,  1.9005e-05, -7.5408e-07,  ..., -1.6721e-06,
          1.1632e-05, -8.4237e-06],
        [-2.1495e-05, -1.5169e-05,  1.6988e-05,  ..., -1.8325e-05,
         -1.3739e-05, -1.3518e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4072e-08, 5.5849e-08, 6.5819e-08,  ..., 2.5924e-08, 1.5051e-07,
         3.8163e-08],
        [7.9349e-11, 5.1468e-11, 2.0417e-11,  ..., 5.5331e-11, 1.6856e-11,
         2.3147e-11],
        [3.6822e-09, 2.2994e-09, 1.2382e-09,  ..., 3.0181e-09, 1.1837e-09,
         1.0033e-09],
        [9.8890e-10, 9.5892e-10, 3.4782e-10,  ..., 8.2849e-10, 5.8722e-10,
         3.3965e-10],
        [4.0158e-10, 2.2907e-10, 8.3691e-11,  ..., 2.9580e-10, 8.9186e-11,
         1.1135e-10]], device='cuda:0')
optimizer state dict: 184.0
lr: [4.299335516882092e-06, 4.299335516882092e-06]
scheduler_last_epoch: 184


Running epoch 1, step 1472, batch 424
Sampled inputs[:2]: tensor([[    0,  7011,   650,  ..., 28839, 11610,  3222],
        [    0,    12,   221,  ...,   593,   360,   726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1264e-04, -1.1121e-04, -6.4623e-05,  ...,  1.4603e-04,
          1.8378e-04,  5.6346e-05],
        [-1.3113e-06, -9.1270e-07,  9.7603e-07,  ..., -1.0952e-06,
         -9.2015e-07, -8.6427e-07],
        [-4.0829e-06, -2.9951e-06,  3.2485e-06,  ..., -3.3826e-06,
         -2.8461e-06, -2.6673e-06],
        [-3.0696e-06, -2.0713e-06,  2.3991e-06,  ..., -2.5481e-06,
         -2.1309e-06, -2.1458e-06],
        [-3.2485e-06, -2.5034e-06,  2.5183e-06,  ..., -2.7418e-06,
         -2.4289e-06, -1.9521e-06]], device='cuda:0')
Loss: 0.9806801676750183


Running epoch 1, step 1473, batch 425
Sampled inputs[:2]: tensor([[   0, 1253,  287,  ..., 2988,   14,  417],
        [   0,    9,  391,  ...,  300, 2646, 1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5979e-04, -1.6392e-04, -1.2028e-04,  ...,  1.6769e-04,
          2.9885e-04, -2.7373e-05],
        [-2.6450e-06, -1.8589e-06,  1.9297e-06,  ..., -2.2277e-06,
         -1.9185e-06, -1.7844e-06],
        [-8.0168e-06, -6.0350e-06,  6.3628e-06,  ..., -6.7502e-06,
         -5.9158e-06, -5.3942e-06],
        [-6.1244e-06, -4.2319e-06,  4.7237e-06,  ..., -5.1558e-06,
         -4.4852e-06, -4.3958e-06],
        [-6.4820e-06, -5.0962e-06,  5.0217e-06,  ..., -5.5432e-06,
         -5.0962e-06, -4.0084e-06]], device='cuda:0')
Loss: 0.9769723415374756


Running epoch 1, step 1474, batch 426
Sampled inputs[:2]: tensor([[   0, 4868, 1027,  ...,  409, 3047, 2953],
        [   0,  300, 1635,  ...,  437,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2377e-04, -2.5077e-04, -1.3622e-04,  ...,  2.4042e-04,
          3.7017e-04, -1.6769e-05],
        [-4.0904e-06, -2.7455e-06,  3.0026e-06,  ..., -3.4422e-06,
         -2.7977e-06, -2.8424e-06],
        [-1.2219e-05, -8.8066e-06,  9.7305e-06,  ..., -1.0252e-05,
         -8.5235e-06, -8.4043e-06],
        [-9.2834e-06, -6.1393e-06,  7.1824e-06,  ..., -7.8082e-06,
         -6.4075e-06, -6.7949e-06],
        [-1.0028e-05, -7.4804e-06,  7.7784e-06,  ..., -8.5533e-06,
         -7.4357e-06, -6.4075e-06]], device='cuda:0')
Loss: 0.9716612100601196


Running epoch 1, step 1475, batch 427
Sampled inputs[:2]: tensor([[   0,  287,  516,  ..., 2386, 3492, 1663],
        [   0, 7110,  437,  ...,  266, 6724, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5940e-05, -2.7556e-04, -7.2898e-05,  ...,  2.5834e-04,
          1.6686e-04, -7.5917e-05],
        [-5.4091e-06, -3.7067e-06,  4.0382e-06,  ..., -4.6194e-06,
         -3.7067e-06, -3.6992e-06],
        [-1.6242e-05, -1.1891e-05,  1.3083e-05,  ..., -1.3873e-05,
         -1.1325e-05, -1.1042e-05],
        [-1.2338e-05, -8.3148e-06,  9.6858e-06,  ..., -1.0550e-05,
         -8.4937e-06, -8.8960e-06],
        [-1.3322e-05, -1.0118e-05,  1.0446e-05,  ..., -1.1563e-05,
         -9.8795e-06, -8.4341e-06]], device='cuda:0')
Loss: 0.9755279421806335


Running epoch 1, step 1476, batch 428
Sampled inputs[:2]: tensor([[    0,    12,   895,  ...,    13,  2900,    14],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3813e-05, -2.3923e-04, -1.1839e-04,  ...,  3.1133e-04,
          1.2136e-04, -1.0274e-04],
        [-6.6310e-06, -4.5300e-06,  5.1111e-06,  ..., -5.6848e-06,
         -4.4145e-06, -4.4815e-06],
        [-2.0057e-05, -1.4573e-05,  1.6645e-05,  ..., -1.7181e-05,
         -1.3530e-05, -1.3441e-05],
        [-1.5259e-05, -1.0215e-05,  1.2398e-05,  ..., -1.3113e-05,
         -1.0170e-05, -1.0893e-05],
        [-1.6198e-05, -1.2264e-05,  1.3068e-05,  ..., -1.4096e-05,
         -1.1705e-05, -1.0081e-05]], device='cuda:0')
Loss: 0.9694342017173767


Running epoch 1, step 1477, batch 429
Sampled inputs[:2]: tensor([[   0, 1083,  287,  ...,   12,  287, 2098],
        [   0, 3658,  271,  ...,  278,  970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8632e-04, -2.7804e-04, -3.8223e-05,  ...,  3.1108e-04,
          2.0503e-04, -6.8401e-07],
        [-8.0988e-06, -5.4911e-06,  6.0946e-06,  ..., -6.8992e-06,
         -5.3681e-06, -5.5172e-06],
        [-2.4438e-05, -1.7568e-05,  1.9789e-05,  ..., -2.0757e-05,
         -1.6391e-05, -1.6510e-05],
        [-1.8448e-05, -1.2271e-05,  1.4618e-05,  ..., -1.5736e-05,
         -1.2241e-05, -1.3232e-05],
        [-1.9908e-05, -1.4856e-05,  1.5646e-05,  ..., -1.7196e-05,
         -1.4268e-05, -1.2524e-05]], device='cuda:0')
Loss: 0.9627377390861511


Running epoch 1, step 1478, batch 430
Sampled inputs[:2]: tensor([[   0, 1824,   13,  ...,  266, 5940,   19],
        [   0,   18,   66,  ...,   65,   17,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8273e-05, -2.9670e-04,  1.9002e-05,  ...,  2.8707e-04,
          1.2652e-04,  3.9687e-05],
        [-9.4399e-06, -6.5342e-06,  7.1004e-06,  ..., -8.0541e-06,
         -6.3442e-06, -6.3665e-06],
        [-2.8551e-05, -2.0921e-05,  2.3022e-05,  ..., -2.4304e-05,
         -1.9386e-05, -1.9133e-05],
        [ 3.1963e-04,  4.5778e-04, -2.5112e-04,  ...,  3.0980e-04,
          3.5961e-04,  2.1760e-04],
        [-2.3231e-05, -1.7673e-05,  1.8209e-05,  ..., -2.0117e-05,
         -1.6846e-05, -1.4506e-05]], device='cuda:0')
Loss: 0.9877310395240784


Running epoch 1, step 1479, batch 431
Sampled inputs[:2]: tensor([[    0, 26473,  2117,  ...,    13,  3292,   950],
        [    0,  2202,   292,  ...,  2431,  2267,  3423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0430e-04, -3.8254e-04, -1.2914e-04,  ...,  2.0775e-04,
          3.6432e-04,  6.7137e-05],
        [-1.0699e-05, -7.4729e-06,  7.9833e-06,  ..., -9.2015e-06,
         -7.3127e-06, -7.2606e-06],
        [-3.2261e-05, -2.3857e-05,  2.5868e-05,  ..., -2.7657e-05,
         -2.2233e-05, -2.1696e-05],
        [ 3.1680e-04,  4.5571e-04, -2.4904e-04,  ...,  3.0722e-04,
          3.5744e-04,  2.1548e-04],
        [-2.6435e-05, -2.0310e-05,  2.0608e-05,  ..., -2.3052e-05,
         -1.9453e-05, -1.6548e-05]], device='cuda:0')
Loss: 0.9570940136909485
Graident accumulation at epoch 1, step 1479, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0020,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0577e-05,  1.3250e-04, -7.7622e-06,  ..., -1.0438e-04,
          2.9449e-06, -5.0939e-05],
        [-1.0870e-05, -7.5963e-06,  8.1576e-06,  ..., -9.5934e-06,
         -7.2741e-06, -7.2831e-06],
        [ 1.3376e-05,  2.4144e-05, -7.4875e-06,  ...,  1.5166e-05,
          1.9486e-05, -1.7154e-06],
        [ 2.7510e-05,  6.2676e-05, -2.5583e-05,  ...,  2.9217e-05,
          4.6212e-05,  1.3967e-05],
        [-2.1989e-05, -1.5683e-05,  1.7350e-05,  ..., -1.8797e-05,
         -1.4311e-05, -1.3821e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4060e-08, 5.5940e-08, 6.5770e-08,  ..., 2.5941e-08, 1.5049e-07,
         3.8129e-08],
        [7.9384e-11, 5.1473e-11, 2.0460e-11,  ..., 5.5360e-11, 1.6892e-11,
         2.3177e-11],
        [3.6795e-09, 2.2977e-09, 1.2376e-09,  ..., 3.0159e-09, 1.1830e-09,
         1.0028e-09],
        [1.0883e-09, 1.1656e-09, 4.0949e-10,  ..., 9.2205e-10, 7.1440e-10,
         3.8575e-10],
        [4.0188e-10, 2.2926e-10, 8.4032e-11,  ..., 2.9603e-10, 8.9475e-11,
         1.1152e-10]], device='cuda:0')
optimizer state dict: 185.0
lr: [4.19821042247685e-06, 4.19821042247685e-06]
scheduler_last_epoch: 185


Running epoch 1, step 1480, batch 432
Sampled inputs[:2]: tensor([[   0,   12, 3067,  ..., 1381,  278, 5011],
        [   0,  518, 9048,  ..., 1354,  352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5805e-05, -1.1481e-04, -1.1166e-04,  ...,  0.0000e+00,
          1.6904e-04,  7.4470e-05],
        [-1.1772e-06, -8.2701e-07,  9.3132e-07,  ..., -1.0431e-06,
         -8.8289e-07, -9.0525e-07],
        [-3.6061e-06, -2.6375e-06,  3.0547e-06,  ..., -3.1292e-06,
         -2.6226e-06, -2.6524e-06],
        [-2.8014e-06, -1.8850e-06,  2.3246e-06,  ..., -2.4736e-06,
         -2.1011e-06, -2.2799e-06],
        [-2.9206e-06, -2.2501e-06,  2.4289e-06,  ..., -2.5928e-06,
         -2.2799e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.950087308883667


Running epoch 1, step 1481, batch 433
Sampled inputs[:2]: tensor([[    0, 18971,   278,  ...,  1934,  1916,  2612],
        [    0,   328,  1410,  ...,  7344,    12,  5067]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9345e-04, -2.7099e-04, -2.5780e-04,  ...,  2.9291e-05,
          5.2254e-04,  3.2418e-05],
        [-2.4438e-06, -1.7807e-06,  1.9819e-06,  ..., -2.1383e-06,
         -1.7211e-06, -1.7546e-06],
        [ 7.9811e-05,  1.2116e-04, -1.5222e-05,  ...,  5.6071e-05,
          7.5032e-05,  3.5152e-05],
        [-5.7518e-06, -4.0457e-06,  4.9174e-06,  ..., -5.0068e-06,
         -4.0382e-06, -4.3511e-06],
        [-5.9605e-06, -4.7535e-06,  5.0664e-06,  ..., -5.2601e-06,
         -4.4703e-06, -3.8072e-06]], device='cuda:0')
Loss: 0.9751054048538208


Running epoch 1, step 1482, batch 434
Sampled inputs[:2]: tensor([[   0, 1075,  940,  ..., 3780,   13, 4467],
        [   0,  266, 7264,  ..., 3211,  328,  275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8090e-04, -4.1229e-04, -4.5812e-04,  ..., -5.2308e-05,
          5.7930e-04, -1.3401e-05],
        [-3.8072e-06, -2.7157e-06,  3.0771e-06,  ..., -3.2336e-06,
         -2.5518e-06, -2.6189e-06],
        [ 7.5608e-05,  1.1814e-04, -1.1646e-05,  ...,  5.2689e-05,
          7.2469e-05,  3.2484e-05],
        [-8.9407e-06, -6.1616e-06,  7.5847e-06,  ..., -7.5400e-06,
         -5.9307e-06, -6.4671e-06],
        [-9.1791e-06, -7.2122e-06,  7.7337e-06,  ..., -7.9274e-06,
         -6.6161e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9944798946380615


Running epoch 1, step 1483, batch 435
Sampled inputs[:2]: tensor([[   0, 2836, 3084,  ..., 3634, 6464,  271],
        [   0,  328, 2097,  ...,  365, 1941,  607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6996e-04, -3.1190e-04, -3.1483e-04,  ..., -1.9219e-04,
          7.8040e-04,  6.3193e-05],
        [-5.1036e-06, -3.6694e-06,  4.0382e-06,  ..., -4.3660e-06,
         -3.4831e-06, -3.4980e-06],
        [ 7.1645e-05,  1.1501e-04, -8.4718e-06,  ...,  4.9187e-05,
          6.9579e-05,  2.9772e-05],
        [-1.1966e-05, -8.3521e-06,  9.9391e-06,  ..., -1.0207e-05,
         -8.1211e-06, -8.6576e-06],
        [-1.2502e-05, -9.9242e-06,  1.0297e-05,  ..., -1.0893e-05,
         -9.1940e-06, -7.7784e-06]], device='cuda:0')
Loss: 0.9663337469100952


Running epoch 1, step 1484, batch 436
Sampled inputs[:2]: tensor([[   0, 8754,   14,  ..., 6125,  394,  927],
        [   0,  344, 3693,  ..., 1782, 3679,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2254e-04, -2.7661e-04, -3.8540e-04,  ..., -3.2517e-04,
          8.6275e-04, -8.0692e-05],
        [-6.4448e-06, -4.6231e-06,  5.1335e-06,  ..., -5.4836e-06,
         -4.3176e-06, -4.3660e-06],
        [ 6.7383e-05,  1.1176e-04, -4.8062e-06,  ...,  4.5611e-05,
          6.6867e-05,  2.7001e-05],
        [-1.5110e-05, -1.0557e-05,  1.2621e-05,  ..., -1.2815e-05,
         -1.0043e-05, -1.0803e-05],
        [-1.5751e-05, -1.2532e-05,  1.3039e-05,  ..., -1.3709e-05,
         -1.1444e-05, -9.7603e-06]], device='cuda:0')
Loss: 1.0003973245620728


Running epoch 1, step 1485, batch 437
Sampled inputs[:2]: tensor([[    0,  8290,   391,  ...,   298,  1253,     7],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2968e-04, -2.7896e-04, -2.7444e-04,  ..., -3.0541e-04,
          1.0810e-03, -1.2405e-04],
        [-7.8008e-06, -5.4464e-06,  6.2063e-06,  ..., -6.6161e-06,
         -5.1372e-06, -5.3644e-06],
        [ 6.3270e-05,  1.0909e-04, -1.3044e-06,  ...,  4.2198e-05,
          6.4348e-05,  2.4020e-05],
        [-1.8135e-05, -1.2338e-05,  1.5154e-05,  ..., -1.5333e-05,
         -1.1854e-05, -1.3113e-05],
        [-1.9059e-05, -1.4752e-05,  1.5751e-05,  ..., -1.6510e-05,
         -1.3620e-05, -1.1981e-05]], device='cuda:0')
Loss: 0.9722369909286499


Running epoch 1, step 1486, batch 438
Sampled inputs[:2]: tensor([[    0,  5625,  2558,  ...,   680,   292,   494],
        [    0,   221,   259,  ...,   199, 13800,  9254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7890e-04, -4.8185e-04, -3.6183e-04,  ..., -3.5162e-04,
          1.5515e-03, -2.5836e-05],
        [-9.1642e-06, -6.2473e-06,  7.1451e-06,  ..., -7.8157e-06,
         -6.0052e-06, -6.4671e-06],
        [ 5.9217e-05,  1.0650e-04,  1.7652e-06,  ...,  3.8711e-05,
          6.1741e-05,  2.0876e-05],
        [-2.1189e-05, -1.4119e-05,  1.7390e-05,  ..., -1.8016e-05,
         -1.3806e-05, -1.5646e-05],
        [-2.2531e-05, -1.6987e-05,  1.8284e-05,  ..., -1.9565e-05,
         -1.5959e-05, -1.4529e-05]], device='cuda:0')
Loss: 0.944793701171875


Running epoch 1, step 1487, batch 439
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0, 27366,   504,  ...,  1358,   365,  6883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2141e-04, -5.1257e-04, -4.9325e-04,  ..., -3.4232e-04,
          1.7317e-03, -6.7414e-05],
        [-1.0453e-05, -7.1786e-06,  8.1509e-06,  ..., -8.9258e-06,
         -6.9104e-06, -7.2904e-06],
        [ 5.5254e-05,  1.0351e-04,  5.0584e-06,  ...,  3.5329e-05,
          5.8969e-05,  1.8373e-05],
        [-2.4229e-05, -1.6250e-05,  1.9863e-05,  ..., -2.0608e-05,
         -1.5922e-05, -1.7688e-05],
        [-2.5675e-05, -1.9491e-05,  2.0847e-05,  ..., -2.2292e-05,
         -1.8328e-05, -1.6354e-05]], device='cuda:0')
Loss: 0.9948946833610535
Graident accumulation at epoch 1, step 1487, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2621e-05,  6.7997e-05, -5.6311e-05,  ..., -1.2818e-04,
          1.7582e-04, -5.2586e-05],
        [-1.0829e-05, -7.5545e-06,  8.1569e-06,  ..., -9.5266e-06,
         -7.2378e-06, -7.2839e-06],
        [ 1.7564e-05,  3.2080e-05, -6.2329e-06,  ...,  1.7182e-05,
          2.3435e-05,  2.9341e-07],
        [ 2.2336e-05,  5.4783e-05, -2.1038e-05,  ...,  2.4235e-05,
          3.9999e-05,  1.0801e-05],
        [-2.2358e-05, -1.6064e-05,  1.7700e-05,  ..., -1.9147e-05,
         -1.4712e-05, -1.4074e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4109e-08, 5.6147e-08, 6.5947e-08,  ..., 2.6032e-08, 1.5334e-07,
         3.8095e-08],
        [7.9414e-11, 5.1473e-11, 2.0506e-11,  ..., 5.5384e-11, 1.6923e-11,
         2.3207e-11],
        [3.6789e-09, 2.3061e-09, 1.2364e-09,  ..., 3.0141e-09, 1.1853e-09,
         1.0021e-09],
        [1.0878e-09, 1.1647e-09, 4.0948e-10,  ..., 9.2155e-10, 7.1394e-10,
         3.8567e-10],
        [4.0214e-10, 2.2941e-10, 8.4382e-11,  ..., 2.9623e-10, 8.9722e-11,
         1.1167e-10]], device='cuda:0')
optimizer state dict: 186.0
lr: [4.097971892163585e-06, 4.097971892163585e-06]
scheduler_last_epoch: 186


Running epoch 1, step 1488, batch 440
Sampled inputs[:2]: tensor([[    0,  1480,   518,  ...,   445,    28,   445],
        [    0, 24674,   513,  ...,  6099,    12,  4863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9123e-05, -7.8452e-05, -4.5388e-05,  ..., -1.0190e-04,
         -9.9596e-05,  8.8906e-05],
        [-1.3113e-06, -9.8348e-07,  1.0356e-06,  ..., -1.0803e-06,
         -8.4937e-07, -8.4937e-07],
        [ 6.9863e-05,  9.6485e-05, -3.5217e-05,  ...,  7.4052e-05,
          7.0034e-05,  2.7862e-05],
        [-3.0696e-06, -2.2799e-06,  2.5332e-06,  ..., -2.5332e-06,
         -1.9819e-06, -2.1011e-06],
        [-3.1292e-06, -2.5630e-06,  2.5779e-06,  ..., -2.6375e-06,
         -2.2352e-06, -1.8999e-06]], device='cuda:0')
Loss: 0.9545530676841736


Running epoch 1, step 1489, batch 441
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    12,   259,  1220],
        [    0,   352, 13159,  ...,  3111,   394,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8740e-04, -8.0867e-05,  5.0622e-05,  ..., -3.9321e-05,
         -2.3191e-04, -4.0570e-05],
        [-2.6524e-06, -1.8775e-06,  2.1309e-06,  ..., -2.1681e-06,
         -1.6466e-06, -1.7099e-06],
        [ 6.5751e-05,  9.3610e-05, -3.1670e-05,  ...,  7.0729e-05,
          6.7560e-05,  2.5240e-05],
        [-6.2436e-06, -4.3511e-06,  5.2601e-06,  ..., -5.0962e-06,
         -3.8445e-06, -4.2617e-06],
        [-6.3032e-06, -4.9174e-06,  5.2601e-06,  ..., -5.2601e-06,
         -4.3064e-06, -3.7774e-06]], device='cuda:0')
Loss: 0.9745073318481445


Running epoch 1, step 1490, batch 442
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   546, 28676,  ...,   271,  1267,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0668e-04, -8.0352e-05, -1.4206e-05,  ..., -5.7120e-05,
         -2.3506e-04, -1.9768e-05],
        [-3.9041e-06, -2.7604e-06,  3.1143e-06,  ..., -3.2410e-06,
         -2.4624e-06, -2.6226e-06],
        [ 6.1846e-05,  9.0793e-05, -2.8362e-05,  ...,  6.7480e-05,
          6.5057e-05,  2.2543e-05],
        [-9.1493e-06, -6.3032e-06,  7.6592e-06,  ..., -7.5400e-06,
         -5.6922e-06, -6.4373e-06],
        [-9.4324e-06, -7.2867e-06,  7.8380e-06,  ..., -7.9125e-06,
         -6.4671e-06, -5.7593e-06]], device='cuda:0')
Loss: 0.9568583369255066


Running epoch 1, step 1491, batch 443
Sampled inputs[:2]: tensor([[    0,   278,  7524,  ...,  1288,   669,   352],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6528e-04, -8.8961e-05,  1.7086e-05,  ..., -2.0260e-05,
         -2.7634e-04,  8.6400e-05],
        [-5.1782e-06, -3.6098e-06,  4.0457e-06,  ..., -4.3958e-06,
         -3.3863e-06, -3.5688e-06],
        [ 5.7972e-05,  8.8141e-05, -2.5352e-05,  ...,  6.4068e-05,
          6.2345e-05,  1.9786e-05],
        [-1.2100e-05, -8.1807e-06,  9.9093e-06,  ..., -1.0192e-05,
         -7.7784e-06, -8.7172e-06],
        [-1.2621e-05, -9.5665e-06,  1.0282e-05,  ..., -1.0774e-05,
         -8.8513e-06, -7.8604e-06]], device='cuda:0')
Loss: 0.9749614000320435


Running epoch 1, step 1492, batch 444
Sampled inputs[:2]: tensor([[   0,  957, 1231,  ...,  800,  342, 1398],
        [   0, 3543,  391,  ..., 3370, 2926, 8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4357e-04, -2.7846e-05,  6.5661e-05,  ...,  5.1122e-05,
         -1.8455e-04,  5.8153e-05],
        [-6.5938e-06, -4.4890e-06,  4.9658e-06,  ..., -5.5954e-06,
         -4.3549e-06, -4.6492e-06],
        [ 5.3740e-05,  8.5429e-05, -2.2417e-05,  ...,  6.0566e-05,
          5.9484e-05,  1.6642e-05],
        [-1.5244e-05, -1.0058e-05,  1.2025e-05,  ..., -1.2815e-05,
         -9.8944e-06, -1.1176e-05],
        [-1.6347e-05, -1.2025e-05,  1.2800e-05,  ..., -1.3918e-05,
         -1.1504e-05, -1.0468e-05]], device='cuda:0')
Loss: 0.9830026030540466


Running epoch 1, step 1493, batch 445
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,  1477,    12,  ..., 31038,   408,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6754e-04, -1.3560e-05,  1.3210e-05,  ...,  1.0439e-04,
         -1.1622e-04,  1.1047e-04],
        [-7.9274e-06, -5.4576e-06,  5.8711e-06,  ..., -6.7651e-06,
         -5.3085e-06, -5.6699e-06],
        [ 4.9598e-05,  8.2270e-05, -1.9347e-05,  ...,  5.6975e-05,
          5.6504e-05,  1.3528e-05],
        [-1.8269e-05, -1.2204e-05,  1.4186e-05,  ..., -1.5453e-05,
         -1.2070e-05, -1.3590e-05],
        [-1.9819e-05, -1.4722e-05,  1.5289e-05,  ..., -1.6958e-05,
         -1.4126e-05, -1.2882e-05]], device='cuda:0')
Loss: 0.9502171277999878


Running epoch 1, step 1494, batch 446
Sampled inputs[:2]: tensor([[   0,   13, 3105,  ...,  496,   14,  879],
        [   0, 2663,  328,  ...,  292,   86,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1158e-05, -3.9770e-05,  4.4437e-05,  ...,  7.0621e-05,
          7.5951e-05,  1.3506e-04],
        [-9.3058e-06, -6.5081e-06,  6.8098e-06,  ..., -7.9572e-06,
         -6.3069e-06, -6.6385e-06],
        [ 4.5425e-05,  7.8902e-05, -1.6262e-05,  ...,  5.3354e-05,
          5.3419e-05,  1.0607e-05],
        [-2.1353e-05, -1.4544e-05,  1.6391e-05,  ..., -1.8135e-05,
         -1.4320e-05, -1.5870e-05],
        [-2.3291e-05, -1.7628e-05,  1.7822e-05,  ..., -2.0042e-05,
         -1.6868e-05, -1.5147e-05]], device='cuda:0')
Loss: 0.9846948981285095


Running epoch 1, step 1495, batch 447
Sampled inputs[:2]: tensor([[    0,    12,   696,  ..., 14275,  2661,  6129],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4985e-06,  1.4488e-05, -1.5365e-05,  ...,  1.0815e-04,
          2.5309e-04,  1.1677e-04],
        [-1.0647e-05, -7.5288e-06,  7.6964e-06,  ..., -9.1493e-06,
         -7.3276e-06, -7.5698e-06],
        [ 4.1313e-05,  7.5579e-05, -1.3297e-05,  ...,  4.9718e-05,
          5.0335e-05,  7.7906e-06],
        [-2.4393e-05, -1.6823e-05,  1.8477e-05,  ..., -2.0847e-05,
         -1.6630e-05, -1.8105e-05],
        [-2.6792e-05, -2.0489e-05,  2.0266e-05,  ..., -2.3156e-05,
         -1.9610e-05, -1.7352e-05]], device='cuda:0')
Loss: 0.9784189462661743
Graident accumulation at epoch 1, step 1495, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0009e-05,  6.2646e-05, -5.2216e-05,  ..., -1.0454e-04,
          1.8355e-04, -3.5651e-05],
        [-1.0811e-05, -7.5520e-06,  8.1109e-06,  ..., -9.4889e-06,
         -7.2468e-06, -7.3124e-06],
        [ 1.9939e-05,  3.6430e-05, -6.9394e-06,  ...,  2.0436e-05,
          2.6125e-05,  1.0431e-06],
        [ 1.7663e-05,  4.7623e-05, -1.7086e-05,  ...,  1.9727e-05,
          3.4336e-05,  7.9107e-06],
        [-2.2801e-05, -1.6507e-05,  1.7957e-05,  ..., -1.9548e-05,
         -1.5202e-05, -1.4402e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4055e-08, 5.6091e-08, 6.5882e-08,  ..., 2.6018e-08, 1.5325e-07,
         3.8071e-08],
        [7.9448e-11, 5.1478e-11, 2.0545e-11,  ..., 5.5413e-11, 1.6960e-11,
         2.3241e-11],
        [3.6769e-09, 2.3095e-09, 1.2353e-09,  ..., 3.0135e-09, 1.1867e-09,
         1.0012e-09],
        [1.0873e-09, 1.1639e-09, 4.0941e-10,  ..., 9.2106e-10, 7.1350e-10,
         3.8561e-10],
        [4.0245e-10, 2.2960e-10, 8.4708e-11,  ..., 2.9647e-10, 9.0016e-11,
         1.1186e-10]], device='cuda:0')
optimizer state dict: 187.0
lr: [3.998635243264737e-06, 3.998635243264737e-06]
scheduler_last_epoch: 187


Running epoch 1, step 1496, batch 448
Sampled inputs[:2]: tensor([[   0, 1530,   17,  ...,  409, 1611,  895],
        [   0,   13,   41,  ...,    5,  271, 2936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9961e-05,  4.8507e-06,  6.6258e-06,  ..., -1.0104e-04,
          1.3212e-04, -1.2526e-05],
        [-1.3337e-06, -9.9093e-07,  1.0505e-06,  ..., -1.0803e-06,
         -8.7917e-07, -9.1270e-07],
        [-4.0531e-06, -3.1143e-06,  3.3677e-06,  ..., -3.2932e-06,
         -2.6822e-06, -2.7865e-06],
        [-3.1143e-06, -2.2501e-06,  2.5630e-06,  ..., -2.5183e-06,
         -2.0117e-06, -2.2352e-06],
        [-3.3081e-06, -2.6375e-06,  2.6822e-06,  ..., -2.7418e-06,
         -2.3395e-06, -2.1160e-06]], device='cuda:0')
Loss: 1.0015157461166382


Running epoch 1, step 1497, batch 449
Sampled inputs[:2]: tensor([[    0,   273,    14,  ...,   271,   266, 25408],
        [    0,   560,   199,  ...,    29,   445,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9395e-05, -3.4402e-06, -3.5727e-05,  ...,  1.4465e-05,
          1.3212e-04, -1.5649e-04],
        [-2.7567e-06, -1.9595e-06,  1.8738e-06,  ..., -2.2873e-06,
         -1.8552e-06, -1.9781e-06],
        [-8.4043e-06, -6.3181e-06,  6.1840e-06,  ..., -6.9886e-06,
         -5.7518e-06, -6.0052e-06],
        [-6.2883e-06, -4.3958e-06,  4.4852e-06,  ..., -5.2452e-06,
         -4.2468e-06, -4.7237e-06],
        [-6.9439e-06, -5.3644e-06,  4.9621e-06,  ..., -5.8711e-06,
         -5.0217e-06, -4.6045e-06]], device='cuda:0')
Loss: 0.9596930742263794


Running epoch 1, step 1498, batch 450
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   300,   275,  1184],
        [    0,   555,   764,  ...,   932,   709, 18731]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7391e-05, -1.3955e-05, -2.4472e-05,  ...,  2.6870e-05,
          1.5133e-04, -1.6516e-04],
        [-4.0755e-06, -2.9802e-06,  2.8349e-06,  ..., -3.4273e-06,
         -2.7530e-06, -2.9318e-06],
        [-1.2577e-05, -9.6858e-06,  9.4026e-06,  ..., -1.0595e-05,
         -8.5980e-06, -9.0152e-06],
        [-9.3877e-06, -6.7502e-06,  6.8396e-06,  ..., -7.9125e-06,
         -6.3479e-06, -7.0781e-06],
        [-1.0252e-05, -8.1360e-06,  7.4655e-06,  ..., -8.7768e-06,
         -7.4357e-06, -6.8396e-06]], device='cuda:0')
Loss: 0.9648443460464478


Running epoch 1, step 1499, batch 451
Sampled inputs[:2]: tensor([[   0,  445,   18,  ..., 1478,  578,  494],
        [   0,  346,   14,  ...,  381,  535,  505]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3769e-05, -8.6202e-05,  5.9947e-05,  ..., -2.1338e-05,
          2.3831e-04, -2.8556e-05],
        [-5.5134e-06, -3.9004e-06,  3.7104e-06,  ..., -4.6045e-06,
         -3.6992e-06, -3.9749e-06],
        [-1.7017e-05, -1.2755e-05,  1.2308e-05,  ..., -1.4231e-05,
         -1.1563e-05, -1.2234e-05],
        [-1.2636e-05, -8.8066e-06,  8.9109e-06,  ..., -1.0595e-05,
         -8.4937e-06, -9.5367e-06],
        [-1.4007e-05, -1.0774e-05,  9.7901e-06,  ..., -1.1921e-05,
         -1.0103e-05, -9.4324e-06]], device='cuda:0')
Loss: 0.9471196532249451


Running epoch 1, step 1500, batch 452
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0,   825,  1243,  ...,    15,    22,    42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2920e-05, -2.5642e-05,  3.9352e-05,  ..., -1.2715e-04,
          1.8526e-04,  4.9666e-05],
        [-6.8620e-06, -4.7609e-06,  4.7758e-06,  ..., -5.6922e-06,
         -4.5635e-06, -4.8876e-06],
        [-2.1219e-05, -1.5587e-05,  1.5825e-05,  ..., -1.7628e-05,
         -1.4260e-05, -1.5065e-05],
        [-1.5795e-05, -1.0774e-05,  1.1533e-05,  ..., -1.3128e-05,
         -1.0490e-05, -1.1787e-05],
        [-1.7285e-05, -1.3098e-05,  1.2472e-05,  ..., -1.4633e-05,
         -1.2383e-05, -1.1474e-05]], device='cuda:0')
Loss: 0.9588404893875122


Running epoch 1, step 1501, batch 453
Sampled inputs[:2]: tensor([[    0,   278,  1295,  ...,  4337,   271,  1268],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5656e-05, -1.2682e-04,  7.6528e-05,  ..., -4.6724e-05,
          3.1821e-04,  8.3264e-05],
        [-8.1882e-06, -5.7295e-06,  5.8636e-06,  ..., -6.7800e-06,
         -5.4315e-06, -5.7444e-06],
        [-2.5272e-05, -1.8701e-05,  1.9342e-05,  ..., -2.0996e-05,
         -1.6943e-05, -1.7747e-05],
        [-1.8939e-05, -1.3039e-05,  1.4216e-05,  ..., -1.5691e-05,
         -1.2517e-05, -1.3933e-05],
        [-2.0489e-05, -1.5676e-05,  1.5169e-05,  ..., -1.7345e-05,
         -1.4678e-05, -1.3441e-05]], device='cuda:0')
Loss: 0.9908660650253296


Running epoch 1, step 1502, batch 454
Sampled inputs[:2]: tensor([[   0, 1682,  271,  ...,  367, 3210,  271],
        [   0,   14, 1845,  ...,  806,  352,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0197e-04,  4.3391e-05,  2.8570e-05,  ..., -8.4192e-05,
          4.0801e-04,  8.4757e-05],
        [-9.6038e-06, -6.7949e-06,  6.8769e-06,  ..., -8.0094e-06,
         -6.4522e-06, -6.7055e-06],
        [-2.9594e-05, -2.2158e-05,  2.2635e-05,  ..., -2.4781e-05,
         -2.0146e-05, -2.0713e-05],
        [-2.2084e-05, -1.5408e-05,  1.6570e-05,  ..., -1.8433e-05,
         -1.4797e-05, -1.6153e-05],
        [-2.4080e-05, -1.8656e-05,  1.7852e-05,  ..., -2.0564e-05,
         -1.7509e-05, -1.5765e-05]], device='cuda:0')
Loss: 0.9895914196968079


Running epoch 1, step 1503, batch 455
Sampled inputs[:2]: tensor([[    0,    14,   560,  ...,  1248,  1398,  1268],
        [    0, 38232,   446,  ...,   287,  2456, 29919]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6540e-04,  1.1552e-04, -3.2569e-05,  ..., -1.5886e-04,
          3.6991e-04, -1.1997e-04],
        [-1.0915e-05, -7.7486e-06,  7.7598e-06,  ..., -9.1195e-06,
         -7.4506e-06, -7.6555e-06],
        [-3.3706e-05, -2.5317e-05,  2.5675e-05,  ..., -2.8223e-05,
         -2.3246e-05, -2.3633e-05],
        [-2.5138e-05, -1.7554e-05,  1.8731e-05,  ..., -2.1011e-05,
         -1.7121e-05, -1.8477e-05],
        [-2.7522e-05, -2.1383e-05,  2.0310e-05,  ..., -2.3499e-05,
         -2.0266e-05, -1.8045e-05]], device='cuda:0')
Loss: 0.9792680740356445
Graident accumulation at epoch 1, step 1503, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4549e-05,  6.7933e-05, -5.0251e-05,  ..., -1.0998e-04,
          2.0218e-04, -4.4083e-05],
        [-1.0821e-05, -7.5716e-06,  8.0758e-06,  ..., -9.4519e-06,
         -7.2671e-06, -7.3468e-06],
        [ 1.4574e-05,  3.0255e-05, -3.6779e-06,  ...,  1.5570e-05,
          2.1188e-05, -1.4245e-06],
        [ 1.3383e-05,  4.1105e-05, -1.3505e-05,  ...,  1.5653e-05,
          2.9190e-05,  5.2719e-06],
        [-2.3273e-05, -1.6994e-05,  1.8192e-05,  ..., -1.9943e-05,
         -1.5708e-05, -1.4766e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4028e-08, 5.6048e-08, 6.5817e-08,  ..., 2.6017e-08, 1.5323e-07,
         3.8047e-08],
        [7.9488e-11, 5.1487e-11, 2.0585e-11,  ..., 5.5441e-11, 1.6999e-11,
         2.3276e-11],
        [3.6744e-09, 2.3078e-09, 1.2348e-09,  ..., 3.0113e-09, 1.1860e-09,
         1.0007e-09],
        [1.0868e-09, 1.1630e-09, 4.0935e-10,  ..., 9.2058e-10, 7.1308e-10,
         3.8557e-10],
        [4.0281e-10, 2.2982e-10, 8.5036e-11,  ..., 2.9673e-10, 9.0337e-11,
         1.1207e-10]], device='cuda:0')
optimizer state dict: 188.0
lr: [3.900215655287364e-06, 3.900215655287364e-06]
scheduler_last_epoch: 188


Running epoch 1, step 1504, batch 456
Sampled inputs[:2]: tensor([[    0,  1041,    14,  ...,   360,   266, 14966],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5353e-04,  4.3977e-05,  1.2820e-04,  ...,  1.3244e-04,
          9.4189e-05,  1.1105e-04],
        [-1.3113e-06, -9.2387e-07,  8.4192e-07,  ..., -1.1325e-06,
         -9.8348e-07, -1.0133e-06],
        [ 7.9417e-05,  7.3686e-05, -5.1260e-05,  ...,  5.9292e-05,
          5.5511e-05,  5.0203e-05],
        [-3.0696e-06, -2.0862e-06,  2.0564e-06,  ..., -2.6375e-06,
         -2.2948e-06, -2.4736e-06],
        [-3.4273e-06, -2.5928e-06,  2.3544e-06,  ..., -2.9951e-06,
         -2.7120e-06, -2.4140e-06]], device='cuda:0')
Loss: 0.9782264232635498


Running epoch 1, step 1505, batch 457
Sampled inputs[:2]: tensor([[    0,   298,   894,  ...,   396,   298,   527],
        [    0, 15666,   609,  ...,   527,  4486,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0573e-04,  1.6261e-04,  7.0375e-05,  ...,  1.0532e-04,
         -1.7037e-04,  2.6184e-04],
        [-2.7567e-06, -1.8142e-06,  1.7397e-06,  ..., -2.3544e-06,
         -1.9297e-06, -2.1458e-06],
        [ 7.5095e-05,  7.0870e-05, -4.8354e-05,  ...,  5.5671e-05,
          5.2665e-05,  4.6865e-05],
        [-6.1989e-06, -3.9786e-06,  4.0829e-06,  ..., -5.2899e-06,
         -4.3660e-06, -4.9919e-06],
        [-7.2271e-06, -5.1260e-06,  4.8131e-06,  ..., -6.2436e-06,
         -5.3495e-06, -5.1707e-06]], device='cuda:0')
Loss: 0.9147790670394897


Running epoch 1, step 1506, batch 458
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0, 14409, 45007,  ...,  1197,   266,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2568e-04,  3.0409e-04,  3.2421e-05,  ..., -9.9939e-06,
         -1.7879e-04,  2.6992e-04],
        [-4.2394e-06, -2.6934e-06,  2.6152e-06,  ..., -3.5912e-06,
         -2.8573e-06, -3.2559e-06],
        [ 7.0774e-05,  6.8083e-05, -4.5553e-05,  ...,  5.2065e-05,
          4.9893e-05,  4.3602e-05],
        [-9.4473e-06, -5.8860e-06,  6.1095e-06,  ..., -8.0019e-06,
         -6.4075e-06, -7.5102e-06],
        [-1.1027e-05, -7.5996e-06,  7.1675e-06,  ..., -9.4771e-06,
         -7.9274e-06, -7.8827e-06]], device='cuda:0')
Loss: 0.9346163868904114


Running epoch 1, step 1507, batch 459
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5169e-04,  3.0926e-04, -2.7199e-04,  ...,  1.1833e-04,
         -2.3841e-04,  2.7761e-04],
        [-5.5805e-06, -3.4831e-06,  3.5167e-06,  ..., -4.7013e-06,
         -3.6545e-06, -4.2990e-06],
        [ 6.6512e-05,  6.5505e-05, -4.2424e-05,  ...,  4.8638e-05,
          4.7450e-05,  4.0458e-05],
        [-1.2636e-05, -7.6964e-06,  8.3745e-06,  ..., -1.0625e-05,
         -8.2850e-06, -1.0058e-05],
        [-1.4395e-05, -9.6858e-06,  9.5963e-06,  ..., -1.2204e-05,
         -9.9689e-06, -1.0177e-05]], device='cuda:0')
Loss: 0.9342537522315979


Running epoch 1, step 1508, batch 460
Sampled inputs[:2]: tensor([[   0,  271,  259,  ..., 1345,  352,  365],
        [   0,  266, 1658,  ...,  278, 1083, 5993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8884e-04,  4.4519e-04, -2.6655e-04,  ...,  6.6503e-05,
         -4.0051e-04,  2.7103e-04],
        [-7.0408e-06, -4.4182e-06,  4.3996e-06,  ..., -5.9232e-06,
         -4.5858e-06, -5.4240e-06],
        [ 6.2042e-05,  6.2480e-05, -3.9473e-05,  ...,  4.4942e-05,
          4.4618e-05,  3.7075e-05],
        [-1.5795e-05, -9.6634e-06,  1.0356e-05,  ..., -1.3277e-05,
         -1.0297e-05, -1.2577e-05],
        [-1.8194e-05, -1.2293e-05,  1.2025e-05,  ..., -1.5393e-05,
         -1.2502e-05, -1.2845e-05]], device='cuda:0')
Loss: 0.9166034460067749


Running epoch 1, step 1509, batch 461
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,   984,    14,   759],
        [    0,  9458,   278,  ...,    15,  5251, 27858]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5138e-04,  4.5037e-04, -3.1831e-04,  ...,  5.6342e-05,
         -5.2041e-04,  1.4384e-04],
        [-8.3894e-06, -5.4240e-06,  5.4874e-06,  ..., -6.9737e-06,
         -5.4091e-06, -6.2995e-06],
        [ 5.7661e-05,  5.9098e-05, -3.5763e-05,  ...,  4.1500e-05,
          4.1906e-05,  3.4214e-05],
        [-1.8984e-05, -1.1973e-05,  1.3024e-05,  ..., -1.5751e-05,
         -1.2204e-05, -1.4752e-05],
        [-2.1502e-05, -1.4961e-05,  1.4767e-05,  ..., -1.8045e-05,
         -1.4693e-05, -1.4856e-05]], device='cuda:0')
Loss: 0.9880464673042297


Running epoch 1, step 1510, batch 462
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,  5308,   266, 14679],
        [    0,  1862,   674,  ...,   391,   266,  7688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5834e-04,  3.9048e-04, -5.2232e-04,  ...,  2.9984e-05,
         -3.4800e-04,  2.4278e-04],
        [-9.7305e-06, -6.3628e-06,  6.5677e-06,  ..., -8.0392e-06,
         -6.2026e-06, -7.1935e-06],
        [ 5.3459e-05,  5.6028e-05, -3.2172e-05,  ...,  3.8162e-05,
          3.9403e-05,  3.1368e-05],
        [-2.2158e-05, -1.4149e-05,  1.5721e-05,  ..., -1.8254e-05,
         -1.4059e-05, -1.6987e-05],
        [-2.4691e-05, -1.7405e-05,  1.7434e-05,  ..., -2.0638e-05,
         -1.6749e-05, -1.6868e-05]], device='cuda:0')
Loss: 0.9564979672431946


Running epoch 1, step 1511, batch 463
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   360,  2374,  ...,   221,   474,   357]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6081e-04,  2.5875e-04, -4.1769e-04,  ...,  5.2177e-05,
         -1.5329e-04,  7.7843e-05],
        [-1.1072e-05, -7.2755e-06,  7.5437e-06,  ..., -9.1568e-06,
         -7.1228e-06, -8.1472e-06],
        [ 4.9316e-05,  5.3107e-05, -2.8953e-05,  ...,  3.4735e-05,
          3.6572e-05,  2.8477e-05],
        [-2.5287e-05, -1.6220e-05,  1.8120e-05,  ..., -2.0832e-05,
         -1.6175e-05, -1.9297e-05],
        [-2.8074e-05, -1.9878e-05,  1.9997e-05,  ..., -2.3484e-05,
         -1.9222e-05, -1.9073e-05]], device='cuda:0')
Loss: 0.9695669412612915
Graident accumulation at epoch 1, step 1511, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.7175e-05,  8.7015e-05, -8.6995e-05,  ..., -9.3761e-05,
          1.6664e-04, -3.1890e-05],
        [-1.0846e-05, -7.5420e-06,  8.0226e-06,  ..., -9.4224e-06,
         -7.2527e-06, -7.4268e-06],
        [ 1.8048e-05,  3.2540e-05, -6.2055e-06,  ...,  1.7486e-05,
          2.2726e-05,  1.5657e-06],
        [ 9.5159e-06,  3.5372e-05, -1.0342e-05,  ...,  1.2004e-05,
          2.4654e-05,  2.8150e-06],
        [-2.3753e-05, -1.7283e-05,  1.8372e-05,  ..., -2.0297e-05,
         -1.6060e-05, -1.5197e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4411e-08, 5.6059e-08, 6.5926e-08,  ..., 2.5994e-08, 1.5310e-07,
         3.8015e-08],
        [7.9531e-11, 5.1488e-11, 2.0621e-11,  ..., 5.5469e-11, 1.7032e-11,
         2.3319e-11],
        [3.6731e-09, 2.3084e-09, 1.2344e-09,  ..., 3.0095e-09, 1.1862e-09,
         1.0005e-09],
        [1.0864e-09, 1.1621e-09, 4.0927e-10,  ..., 9.2010e-10, 7.1263e-10,
         3.8556e-10],
        [4.0319e-10, 2.2999e-10, 8.5351e-11,  ..., 2.9698e-10, 9.0616e-11,
         1.1233e-10]], device='cuda:0')
optimizer state dict: 189.0
lr: [3.80272816760364e-06, 3.80272816760364e-06]
scheduler_last_epoch: 189


Running epoch 1, step 1512, batch 464
Sampled inputs[:2]: tensor([[    0,    33,    12,  ...,  1110,   467, 17467],
        [    0,  4448,    12,  ...,  3183,   328,  9559]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1651e-05, -8.1016e-05,  1.0088e-05,  ..., -3.6950e-05,
          2.0112e-04,  1.6716e-04],
        [-1.3486e-06, -1.0133e-06,  8.0839e-07,  ..., -1.2144e-06,
         -9.9838e-07, -1.0580e-06],
        [-4.1425e-06, -3.2932e-06,  2.6971e-06,  ..., -3.7253e-06,
         -3.1143e-06, -3.2336e-06],
        [-3.1292e-06, -2.3395e-06,  1.9521e-06,  ..., -2.8461e-06,
         -2.3693e-06, -2.5779e-06],
        [-3.5912e-06, -2.8908e-06,  2.2650e-06,  ..., -3.2634e-06,
         -2.8014e-06, -2.6375e-06]], device='cuda:0')
Loss: 0.956499457359314


Running epoch 1, step 1513, batch 465
Sampled inputs[:2]: tensor([[    0,  3084,   278,  ..., 10981,  3589,    12],
        [    0,   278,  2097,  ...,  1754,   287,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2971e-06, -6.8344e-05, -3.2809e-05,  ...,  4.9850e-05,
          2.0944e-04,  1.7808e-04],
        [-2.7195e-06, -2.0117e-06,  1.8515e-06,  ..., -2.3395e-06,
         -1.8887e-06, -1.9632e-06],
        [-8.3745e-06, -6.4820e-06,  6.0797e-06,  ..., -7.1973e-06,
         -5.8413e-06, -6.0350e-06],
        [-6.3032e-06, -4.5747e-06,  4.4554e-06,  ..., -5.4389e-06,
         -4.3809e-06, -4.7982e-06],
        [-7.0333e-06, -5.5879e-06,  4.9472e-06,  ..., -6.1393e-06,
         -5.1707e-06, -4.7386e-06]], device='cuda:0')
Loss: 0.9716734886169434


Running epoch 1, step 1514, batch 466
Sampled inputs[:2]: tensor([[   0, 9677,  609,  ...,  199, 1919,  298],
        [   0,  278, 1041,  ..., 2098, 1837,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7003e-05, -1.6314e-04,  4.0578e-05,  ...,  8.7587e-05,
          2.1404e-04,  2.7889e-04],
        [-4.0233e-06, -2.9951e-06,  2.7716e-06,  ..., -3.4422e-06,
         -2.8089e-06, -2.8946e-06],
        [-1.2547e-05, -9.7901e-06,  9.2387e-06,  ..., -1.0744e-05,
         -8.8215e-06, -9.0450e-06],
        [-9.3430e-06, -6.8545e-06,  6.7055e-06,  ..., -8.0168e-06,
         -6.5267e-06, -7.0930e-06],
        [-1.0312e-05, -8.2850e-06,  7.3761e-06,  ..., -8.9705e-06,
         -7.6592e-06, -6.9588e-06]], device='cuda:0')
Loss: 0.9639279246330261


Running epoch 1, step 1515, batch 467
Sampled inputs[:2]: tensor([[   0,  408, 1782,  ...,  271,  729, 1692],
        [   0, 1603,   12,  ...,   12,  756,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0401e-05, -3.1616e-04, -1.8561e-06,  ...,  4.2618e-04,
          2.0325e-04,  7.0656e-04],
        [-5.4911e-06, -3.9078e-06,  3.5577e-06,  ..., -4.6939e-06,
         -3.7365e-06, -4.0196e-06],
        [-1.7107e-05, -1.2830e-05,  1.1906e-05,  ..., -1.4618e-05,
         -1.1787e-05, -1.2532e-05],
        [-1.2577e-05, -8.8662e-06,  8.5235e-06,  ..., -1.0803e-05,
         -8.6129e-06, -9.6560e-06],
        [-1.4275e-05, -1.0952e-05,  9.6112e-06,  ..., -1.2383e-05,
         -1.0356e-05, -9.8497e-06]], device='cuda:0')
Loss: 0.932095468044281


Running epoch 1, step 1516, batch 468
Sampled inputs[:2]: tensor([[   0, 3103, 2134,  ..., 6627,  275, 1911],
        [   0,   14, 3080,  ...,  910,  266, 5275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0401e-05, -2.5767e-04,  1.4228e-05,  ...,  4.0078e-04,
         -1.3130e-04,  7.6617e-04],
        [-6.9737e-06, -4.8913e-06,  4.4368e-06,  ..., -5.9083e-06,
         -4.6976e-06, -5.1819e-06],
        [-2.1785e-05, -1.6108e-05,  1.4886e-05,  ..., -1.8463e-05,
         -1.4901e-05, -1.6168e-05],
        [-1.5840e-05, -1.1012e-05,  1.0535e-05,  ..., -1.3486e-05,
         -1.0759e-05, -1.2293e-05],
        [-1.8328e-05, -1.3843e-05,  1.2115e-05,  ..., -1.5751e-05,
         -1.3173e-05, -1.2845e-05]], device='cuda:0')
Loss: 0.9400091767311096


Running epoch 1, step 1517, batch 469
Sampled inputs[:2]: tensor([[    0,  4710,    12,  ...,  3969,     9, 11692],
        [    0,   278,  1099,  ...,   496,    14,   879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1344e-04, -2.9799e-04,  2.7888e-05,  ...,  5.3984e-04,
          8.4066e-05,  7.5854e-04],
        [-8.3596e-06, -5.8264e-06,  5.2862e-06,  ..., -7.0781e-06,
         -5.6587e-06, -6.2473e-06],
        [-2.5928e-05, -1.9059e-05,  1.7673e-05,  ..., -2.1905e-05,
         -1.7792e-05, -1.9252e-05],
        [-1.8880e-05, -1.3068e-05,  1.2457e-05,  ..., -1.6078e-05,
         -1.2919e-05, -1.4722e-05],
        [-2.1845e-05, -1.6376e-05,  1.4454e-05,  ..., -1.8686e-05,
         -1.5721e-05, -1.5274e-05]], device='cuda:0')
Loss: 0.956061840057373


Running epoch 1, step 1518, batch 470
Sampled inputs[:2]: tensor([[    0,  2734,  2338,  ...,  3977,   970, 10537],
        [    0,  4994,  8429,  ...,    12,   795,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0102e-04, -2.7463e-04, -1.6407e-04,  ...,  4.6222e-04,
          8.2753e-05,  7.4348e-04],
        [-9.6709e-06, -6.7949e-06,  6.3144e-06,  ..., -8.2254e-06,
         -6.5379e-06, -7.1488e-06],
        [-2.9922e-05, -2.2113e-05,  2.1011e-05,  ..., -2.5377e-05,
         -2.0444e-05, -2.1979e-05],
        [-2.1935e-05, -1.5259e-05,  1.4961e-05,  ..., -1.8746e-05,
         -1.4946e-05, -1.6928e-05],
        [-2.5004e-05, -1.8910e-05,  1.7032e-05,  ..., -2.1473e-05,
         -1.7986e-05, -1.7270e-05]], device='cuda:0')
Loss: 0.9741716980934143


Running epoch 1, step 1519, batch 471
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,  5583,   598,  ...,   199,   395,  6551]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1972e-04, -2.0245e-04,  1.0210e-04,  ...,  5.6654e-04,
          8.4342e-05,  6.6532e-04],
        [-1.0982e-05, -7.7859e-06,  7.3351e-06,  ..., -9.3058e-06,
         -7.3947e-06, -7.9945e-06],
        [-3.4183e-05, -2.5451e-05,  2.4527e-05,  ..., -2.8878e-05,
         -2.3201e-05, -2.4781e-05],
        [-2.5123e-05, -1.7613e-05,  1.7568e-05,  ..., -2.1338e-05,
         -1.6972e-05, -1.9103e-05],
        [-2.8297e-05, -2.1577e-05,  1.9655e-05,  ..., -2.4229e-05,
         -2.0266e-05, -1.9297e-05]], device='cuda:0')
Loss: 0.9926509857177734
Graident accumulation at epoch 1, step 1519, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.9430e-05,  5.8068e-05, -6.8085e-05,  ..., -2.7730e-05,
          1.5841e-04,  3.7831e-05],
        [-1.0860e-05, -7.5664e-06,  7.9538e-06,  ..., -9.4108e-06,
         -7.2669e-06, -7.4836e-06],
        [ 1.2825e-05,  2.6741e-05, -3.1322e-06,  ...,  1.2850e-05,
          1.8133e-05, -1.0690e-06],
        [ 6.0519e-06,  3.0074e-05, -7.5512e-06,  ...,  8.6701e-06,
          2.0491e-05,  6.2317e-07],
        [-2.4208e-05, -1.7712e-05,  1.8501e-05,  ..., -2.0690e-05,
         -1.6480e-05, -1.5607e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4371e-08, 5.6044e-08, 6.5870e-08,  ..., 2.6289e-08, 1.5296e-07,
         3.8420e-08],
        [7.9572e-11, 5.1497e-11, 2.0654e-11,  ..., 5.5500e-11, 1.7070e-11,
         2.3360e-11],
        [3.6706e-09, 2.3067e-09, 1.2337e-09,  ..., 3.0073e-09, 1.1855e-09,
         1.0002e-09],
        [1.0859e-09, 1.1612e-09, 4.0917e-10,  ..., 9.1963e-10, 7.1220e-10,
         3.8554e-10],
        [4.0359e-10, 2.3023e-10, 8.5652e-11,  ..., 2.9727e-10, 9.0936e-11,
         1.1259e-10]], device='cuda:0')
optimizer state dict: 190.0
lr: [3.7061876771526483e-06, 3.7061876771526483e-06]
scheduler_last_epoch: 190


Running epoch 1, step 1520, batch 472
Sampled inputs[:2]: tensor([[   0, 1890,  278,  ..., 1400,  367, 1874],
        [   0,  870,  278,  ...,  478,  401,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2473e-04, -7.6958e-05, -1.3148e-04,  ..., -8.0382e-06,
          6.0890e-05, -2.8626e-04],
        [-1.2144e-06, -8.3819e-07,  9.6858e-07,  ..., -1.0356e-06,
         -7.8976e-07, -9.3132e-07],
        [-3.7700e-06, -2.6077e-06,  3.2634e-06,  ..., -3.1292e-06,
         -2.3693e-06, -2.7865e-06],
        [-2.8610e-06, -1.8701e-06,  2.4289e-06,  ..., -2.4140e-06,
         -1.8254e-06, -2.2799e-06],
        [-2.9802e-06, -2.1607e-06,  2.5034e-06,  ..., -2.5183e-06,
         -2.0117e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9524759650230408


Running epoch 1, step 1521, batch 473
Sampled inputs[:2]: tensor([[    0,  1420,  6319,  ...,   292,  4895,  4050],
        [    0,   292, 12522,  ...,   266,  1977,  8481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5740e-05,  1.7514e-05, -6.0651e-05,  ...,  3.5143e-05,
          9.9838e-05, -3.9072e-05],
        [-2.6003e-06, -1.8440e-06,  1.8068e-06,  ..., -2.2724e-06,
         -1.7434e-06, -1.9893e-06],
        [-8.0615e-06, -5.8860e-06,  6.0797e-06,  ..., -6.9141e-06,
         -5.3197e-06, -5.9903e-06],
        [-5.9903e-06, -4.1053e-06,  4.3958e-06,  ..., -5.2303e-06,
         -4.0159e-06, -4.7833e-06],
        [-6.6012e-06, -4.9770e-06,  4.8280e-06,  ..., -5.7369e-06,
         -4.6045e-06, -4.5598e-06]], device='cuda:0')
Loss: 0.9463387727737427


Running epoch 1, step 1522, batch 474
Sampled inputs[:2]: tensor([[    0,  4359,  5768,  ...,  4402,   292,    69],
        [    0,   292, 29800,  ...,  4144,   278,  1243]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1095e-04,  7.6602e-05, -2.2887e-04,  ...,  8.7733e-05,
          2.3502e-05, -7.3898e-05],
        [-3.9712e-06, -2.7120e-06,  2.8349e-06,  ..., -3.4049e-06,
         -2.6040e-06, -2.9802e-06],
        [-1.2085e-05, -8.6278e-06,  9.3728e-06,  ..., -1.0252e-05,
         -7.8827e-06, -8.8960e-06],
        [-9.1046e-06, -6.0275e-06,  6.8843e-06,  ..., -7.7933e-06,
         -5.9679e-06, -7.1526e-06],
        [-9.9242e-06, -7.3463e-06,  7.4655e-06,  ..., -8.5533e-06,
         -6.8843e-06, -6.7800e-06]], device='cuda:0')
Loss: 0.9330062866210938


Running epoch 1, step 1523, batch 475
Sampled inputs[:2]: tensor([[    0,    73,    30,  ...,  4112,    12,  9416],
        [    0,   301,   298,  ..., 10030,   300,  3780]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6007e-05,  1.6318e-04, -2.1469e-04,  ..., -1.0978e-04,
         -8.4109e-05,  6.7838e-05],
        [-5.3942e-06, -3.7625e-06,  3.7588e-06,  ..., -4.6045e-06,
         -3.6545e-06, -4.0308e-06],
        [ 6.1983e-05,  7.5951e-06, -5.0526e-05,  ...,  4.1940e-05,
          3.6887e-05,  2.8092e-05],
        [-1.2323e-05, -8.3670e-06,  9.0599e-06,  ..., -1.0490e-05,
         -8.3223e-06, -9.6411e-06],
        [-1.3635e-05, -1.0282e-05,  9.9391e-06,  ..., -1.1742e-05,
         -9.8050e-06, -9.3877e-06]], device='cuda:0')
Loss: 0.9876243472099304


Running epoch 1, step 1524, batch 476
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,  2195,   278,   266],
        [    0,   287, 21212,  ...,  3123,   944,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9660e-05,  1.5569e-04, -3.8474e-04,  ..., -1.1657e-04,
         -1.9917e-04,  1.1012e-05],
        [-6.7502e-06, -4.7237e-06,  4.7870e-06,  ..., -5.6699e-06,
         -4.4890e-06, -4.9658e-06],
        [ 5.7780e-05,  4.5254e-06, -4.7143e-05,  ...,  3.8632e-05,
          3.4294e-05,  2.5201e-05],
        [-1.5467e-05, -1.0513e-05,  1.1563e-05,  ..., -1.2949e-05,
         -1.0215e-05, -1.1921e-05],
        [-1.6943e-05, -1.2800e-05,  1.2532e-05,  ..., -1.4395e-05,
         -1.1981e-05, -1.1504e-05]], device='cuda:0')
Loss: 0.9736830592155457


Running epoch 1, step 1525, batch 477
Sampled inputs[:2]: tensor([[   0,  273,  298,  ..., 7437, 2767,  518],
        [   0,  381, 1659,  ..., 1403,  271, 6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2750e-05,  1.9379e-04, -5.3563e-04,  ..., -1.7684e-04,
         -4.1220e-04, -3.6706e-06],
        [-8.0988e-06, -5.6513e-06,  5.7556e-06,  ..., -6.7800e-06,
         -5.3048e-06, -5.9493e-06],
        [ 5.3549e-05,  1.4558e-06, -4.3865e-05,  ...,  3.5145e-05,
          3.1687e-05,  2.2101e-05],
        [-1.8626e-05, -1.2644e-05,  1.3933e-05,  ..., -1.5542e-05,
         -1.2122e-05, -1.4335e-05],
        [-2.0325e-05, -1.5333e-05,  1.5080e-05,  ..., -1.7226e-05,
         -1.4201e-05, -1.3828e-05]], device='cuda:0')
Loss: 0.9456983804702759


Running epoch 1, step 1526, batch 478
Sampled inputs[:2]: tensor([[    0,   266, 27347,  ...,   368,  3367,    13],
        [    0,  1070,  5746,  ...,   278,   689,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1665e-04,  1.3454e-04, -6.1724e-04,  ..., -1.7684e-04,
         -5.7044e-04, -6.3796e-05],
        [ 6.1596e-05,  4.3606e-05, -4.8999e-05,  ...,  5.1000e-05,
          6.2548e-05,  1.1294e-05],
        [ 4.9138e-05, -1.3754e-06, -4.0825e-05,  ...,  3.1658e-05,
          2.9019e-05,  1.8868e-05],
        [-2.1920e-05, -1.4566e-05,  1.6138e-05,  ..., -1.8150e-05,
         -1.4089e-05, -1.6853e-05],
        [-2.3991e-05, -1.7732e-05,  1.7479e-05,  ..., -2.0176e-05,
         -1.6570e-05, -1.6332e-05]], device='cuda:0')
Loss: 0.9059440493583679


Running epoch 1, step 1527, batch 479
Sampled inputs[:2]: tensor([[    0,  4108,    85,  ...,    40,    12,  1530],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3248e-04,  1.3028e-04, -5.0367e-04,  ..., -2.5329e-04,
         -5.1300e-04, -1.3861e-04],
        [ 6.0270e-05,  4.2593e-05, -4.8022e-05,  ...,  4.9883e-05,
          6.1621e-05,  1.0381e-05],
        [ 4.5025e-05, -4.6388e-06, -3.7621e-05,  ...,  2.8186e-05,
          2.6143e-05,  1.6007e-05],
        [-2.5034e-05, -1.6890e-05,  1.8507e-05,  ..., -2.0757e-05,
         -1.6235e-05, -1.9118e-05],
        [-2.7359e-05, -2.0504e-05,  2.0042e-05,  ..., -2.3067e-05,
         -1.9088e-05, -1.8522e-05]], device='cuda:0')
Loss: 0.9892092347145081
Graident accumulation at epoch 1, step 1527, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0343],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.6239e-05,  6.5289e-05, -1.1164e-04,  ..., -5.0286e-05,
          9.1265e-05,  2.0187e-05],
        [-3.7467e-06, -2.5504e-06,  2.3562e-06,  ..., -3.4814e-06,
         -3.7813e-07, -5.6971e-06],
        [ 1.6045e-05,  2.3603e-05, -6.5811e-06,  ...,  1.4384e-05,
          1.8934e-05,  6.3859e-07],
        [ 2.9434e-06,  2.5377e-05, -4.9453e-06,  ...,  5.7274e-06,
          1.6819e-05, -1.3510e-06],
        [-2.4523e-05, -1.7991e-05,  1.8655e-05,  ..., -2.0928e-05,
         -1.6741e-05, -1.5899e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4334e-08, 5.6005e-08, 6.6058e-08,  ..., 2.6327e-08, 1.5307e-07,
         3.8401e-08],
        [8.3125e-11, 5.3260e-11, 2.2940e-11,  ..., 5.7933e-11, 2.0850e-11,
         2.3444e-11],
        [3.6690e-09, 2.3044e-09, 1.2339e-09,  ..., 3.0051e-09, 1.1850e-09,
         9.9941e-10],
        [1.0855e-09, 1.1604e-09, 4.0910e-10,  ..., 9.1914e-10, 7.1175e-10,
         3.8552e-10],
        [4.0394e-10, 2.3042e-10, 8.5968e-11,  ..., 2.9751e-10, 9.1210e-11,
         1.1282e-10]], device='cuda:0')
optimizer state dict: 191.0
lr: [3.6106089361640563e-06, 3.6106089361640563e-06]
scheduler_last_epoch: 191
Epoch 1 | Batch 479/1048 | Training PPL: 2734.4179523649273 | time 48.17525053024292
Saving checkpoint at epoch 1, step 1527, batch 479
Epoch 1 | Validation PPL: 6.808398893747563 | Learning rate: 3.6106089361640563e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1527, AFTER epoch 1, step 1527


Running epoch 1, step 1528, batch 480
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0,   287, 17044,  ...,   496,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8768e-05, -7.5698e-06,  1.3701e-04,  ...,  6.0489e-05,
          1.5326e-04, -3.7081e-05],
        [-1.3262e-06, -1.0952e-06,  9.8348e-07,  ..., -1.1176e-06,
         -9.9093e-07, -9.6112e-07],
        [-4.1723e-06, -3.5763e-06,  3.2783e-06,  ..., -3.5316e-06,
         -3.1590e-06, -3.0249e-06],
        [-3.1441e-06, -2.5779e-06,  2.4140e-06,  ..., -2.6524e-06,
         -2.3693e-06, -2.3991e-06],
        [-3.4124e-06, -3.0100e-06,  2.6375e-06,  ..., -2.9206e-06,
         -2.7120e-06, -2.3246e-06]], device='cuda:0')
Loss: 1.0153535604476929


Running epoch 1, step 1529, batch 481
Sampled inputs[:2]: tensor([[   0, 2241, 8274,  ...,  908, 1811,  278],
        [   0,   12, 9248,  ..., 2673, 4239,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1495e-05, -2.6738e-05, -3.5149e-07,  ...,  2.0113e-04,
          1.0691e-05,  9.1406e-05],
        [-2.6897e-06, -2.1085e-06,  1.9595e-06,  ..., -2.2054e-06,
         -1.9073e-06, -1.8664e-06],
        [ 3.5761e-04,  2.8915e-04, -3.1457e-04,  ...,  3.3648e-04,
          3.2883e-04,  3.0061e-04],
        [-6.3479e-06, -4.9472e-06,  4.8131e-06,  ..., -5.2005e-06,
         -4.5151e-06, -4.6343e-06],
        [-6.7949e-06, -5.7518e-06,  5.1558e-06,  ..., -5.6922e-06,
         -5.1409e-06, -4.4405e-06]], device='cuda:0')
Loss: 0.9836555123329163


Running epoch 1, step 1530, batch 482
Sampled inputs[:2]: tensor([[   0,  275, 4452,  ...,   12, 3516, 5227],
        [   0, 4995,  287,  ...,  300, 4531, 4729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1131e-05,  3.2939e-05,  8.9851e-06,  ...,  1.7641e-04,
         -2.1524e-04,  1.0557e-04],
        [-4.0382e-06, -3.0920e-06,  2.9951e-06,  ..., -3.2783e-06,
         -2.7418e-06, -2.8051e-06],
        [ 3.5326e-04,  2.8588e-04, -3.1100e-04,  ...,  3.3304e-04,
          3.2614e-04,  2.9760e-04],
        [-9.5218e-06, -7.2122e-06,  7.3910e-06,  ..., -7.7039e-06,
         -6.4522e-06, -6.9439e-06],
        [-1.0103e-05, -8.3447e-06,  7.7933e-06,  ..., -8.3745e-06,
         -7.3612e-06, -6.6012e-06]], device='cuda:0')
Loss: 0.9726458787918091


Running epoch 1, step 1531, batch 483
Sampled inputs[:2]: tensor([[    0,  2302,   287,  ...,  1522,  1666,   300],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7916e-04,  1.7769e-05, -7.4263e-05,  ...,  2.1829e-04,
         -4.0899e-04,  3.5008e-05],
        [-5.3793e-06, -4.0345e-06,  4.0308e-06,  ..., -4.3586e-06,
         -3.5837e-06, -3.7327e-06],
        [ 3.4912e-04,  2.8281e-04, -3.0757e-04,  ...,  3.2967e-04,
          3.2348e-04,  2.9473e-04],
        [-1.2666e-05, -9.3728e-06,  9.9242e-06,  ..., -1.0237e-05,
         -8.4192e-06, -9.2387e-06],
        [-1.3337e-05, -1.0878e-05,  1.0401e-05,  ..., -1.1072e-05,
         -9.6112e-06, -8.7023e-06]], device='cuda:0')
Loss: 0.9764692783355713


Running epoch 1, step 1532, batch 484
Sampled inputs[:2]: tensor([[    0,  1034,   287,  ...,  9677,    13,  6687],
        [    0,   266, 30368,  ...,   950,   266,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3919e-04, -5.7465e-06,  2.6407e-05,  ...,  1.7908e-04,
         -3.9627e-04, -1.6038e-04],
        [-6.7130e-06, -4.8652e-06,  4.9993e-06,  ..., -5.4389e-06,
         -4.4033e-06, -4.7162e-06],
        [ 3.4495e-04,  2.8009e-04, -3.0434e-04,  ...,  3.2635e-04,
          3.2094e-04,  2.9170e-04],
        [-1.5721e-05, -1.1213e-05,  1.2234e-05,  ..., -1.2696e-05,
         -1.0282e-05, -1.1578e-05],
        [-1.6704e-05, -1.3128e-05,  1.2919e-05,  ..., -1.3798e-05,
         -1.1772e-05, -1.0952e-05]], device='cuda:0')
Loss: 0.9283609390258789


Running epoch 1, step 1533, batch 485
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,   14,  747,  ..., 2039,  287, 8053]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7552e-04,  2.5032e-05, -2.3812e-05,  ...,  1.1744e-04,
         -5.1477e-04, -1.5758e-04],
        [-8.1509e-06, -5.8413e-06,  5.9679e-06,  ..., -6.6161e-06,
         -5.3011e-06, -5.7593e-06],
        [ 3.4051e-04,  2.7691e-04, -3.0113e-04,  ...,  3.2270e-04,
          3.1813e-04,  2.8847e-04],
        [-1.8969e-05, -1.3404e-05,  1.4514e-05,  ..., -1.5378e-05,
         -1.2308e-05, -1.4022e-05],
        [-2.0459e-05, -1.5885e-05,  1.5557e-05,  ..., -1.6943e-05,
         -1.4305e-05, -1.3545e-05]], device='cuda:0')
Loss: 0.9384337663650513


Running epoch 1, step 1534, batch 486
Sampled inputs[:2]: tensor([[   0, 4175,  437,  ..., 1700,   14,  381],
        [   0,   28, 2973,  ..., 8762, 2134,   27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3064e-04, -4.8753e-05, -6.4290e-05,  ...,  9.4099e-05,
         -6.1863e-04, -1.7206e-04],
        [-9.4995e-06, -6.8694e-06,  7.0482e-06,  ..., -7.6964e-06,
         -6.1914e-06, -6.6645e-06],
        [ 3.3610e-04,  2.7342e-04, -2.9744e-04,  ...,  3.1912e-04,
          3.1518e-04,  2.8544e-04],
        [-2.2158e-05, -1.5788e-05,  1.7166e-05,  ..., -1.7926e-05,
         -1.4380e-05, -1.6257e-05],
        [-2.3827e-05, -1.8641e-05,  1.8314e-05,  ..., -1.9729e-05,
         -1.6704e-05, -1.5706e-05]], device='cuda:0')
Loss: 1.0181171894073486


Running epoch 1, step 1535, batch 487
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2140e-04, -2.3227e-05, -7.5227e-05,  ...,  8.8625e-05,
         -6.3248e-04, -1.2520e-04],
        [-1.0855e-05, -7.8380e-06,  8.1286e-06,  ..., -8.7693e-06,
         -6.9961e-06, -7.5996e-06],
        [ 4.0349e-04,  3.3970e-04, -3.5270e-04,  ...,  3.9715e-04,
          3.7026e-04,  3.1041e-04],
        [-2.5421e-05, -1.8083e-05,  1.9863e-05,  ..., -2.0504e-05,
         -1.6280e-05, -1.8612e-05],
        [-2.7299e-05, -2.1294e-05,  2.1160e-05,  ..., -2.2531e-05,
         -1.8939e-05, -1.7956e-05]], device='cuda:0')
Loss: 0.9834874272346497
Graident accumulation at epoch 1, step 1535, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6475e-05,  5.6438e-05, -1.0800e-04,  ..., -3.6395e-05,
          1.8891e-05,  5.6477e-06],
        [-4.4576e-06, -3.0792e-06,  2.9334e-06,  ..., -4.0102e-06,
         -1.0399e-06, -5.8873e-06],
        [ 5.4790e-05,  5.5213e-05, -4.1193e-05,  ...,  5.2660e-05,
          5.4066e-05,  3.1616e-05],
        [ 1.0689e-07,  2.1031e-05, -2.4645e-06,  ...,  3.1043e-06,
          1.3509e-05, -3.0770e-06],
        [-2.4800e-05, -1.8322e-05,  1.8905e-05,  ..., -2.1088e-05,
         -1.6961e-05, -1.6104e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4383e-08, 5.5949e-08, 6.5997e-08,  ..., 2.6308e-08, 1.5331e-07,
         3.8378e-08],
        [8.3159e-11, 5.3268e-11, 2.2983e-11,  ..., 5.7952e-11, 2.0878e-11,
         2.3478e-11],
        [3.8281e-09, 2.4175e-09, 1.3571e-09,  ..., 3.1599e-09, 1.3209e-09,
         1.0948e-09],
        [1.0850e-09, 1.1595e-09, 4.0909e-10,  ..., 9.1864e-10, 7.1131e-10,
         3.8548e-10],
        [4.0428e-10, 2.3064e-10, 8.6330e-11,  ..., 2.9772e-10, 9.1477e-11,
         1.1303e-10]], device='cuda:0')
optimizer state dict: 192.0
lr: [3.5160065499038043e-06, 3.5160065499038043e-06]
scheduler_last_epoch: 192


Running epoch 1, step 1536, batch 488
Sampled inputs[:2]: tensor([[    0,  1756,   271,  ...,   259, 48595, 19882],
        [    0,   461,  1169,  ..., 14135,  2771,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9244e-05, -6.8488e-05, -4.7663e-05,  ..., -5.3768e-05,
         -2.4668e-04, -3.8548e-05],
        [-1.3337e-06, -9.1642e-07,  1.0654e-06,  ..., -1.0580e-06,
         -7.8976e-07, -8.7917e-07],
        [-4.1723e-06, -3.0249e-06,  3.5614e-06,  ..., -3.3230e-06,
         -2.5034e-06, -2.7716e-06],
        [-3.1739e-06, -2.1607e-06,  2.6673e-06,  ..., -2.5183e-06,
         -1.8626e-06, -2.2054e-06],
        [-3.2187e-06, -2.4289e-06,  2.6822e-06,  ..., -2.6226e-06,
         -2.0862e-06, -1.9819e-06]], device='cuda:0')
Loss: 0.9476696848869324


Running epoch 1, step 1537, batch 489
Sampled inputs[:2]: tensor([[    0,   768,  3227,  ...,  3487,    13, 31431],
        [    0,    89,  2023,  ...,  3230,   328,   790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0626e-05,  4.9225e-05, -1.5775e-05,  ..., -1.2748e-04,
         -3.9959e-04, -1.9928e-04],
        [-2.5705e-06, -1.7732e-06,  2.0154e-06,  ..., -2.1011e-06,
         -1.5646e-06, -1.7583e-06],
        [-8.1062e-06, -5.7966e-06,  6.8098e-06,  ..., -6.5565e-06,
         -4.8876e-06, -5.4836e-06],
        [-6.1840e-06, -4.1574e-06,  5.0962e-06,  ..., -5.0217e-06,
         -3.6880e-06, -4.4405e-06],
        [-6.2585e-06, -4.6641e-06,  5.1260e-06,  ..., -5.1558e-06,
         -4.0829e-06, -3.8892e-06]], device='cuda:0')
Loss: 0.9664924740791321


Running epoch 1, step 1538, batch 490
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0729e-05,  8.7731e-05,  5.3579e-06,  ..., -1.0561e-04,
         -3.5834e-04, -1.3592e-04],
        [-3.8147e-06, -2.6673e-06,  2.9430e-06,  ..., -3.1739e-06,
         -2.4028e-06, -2.6971e-06],
        [-1.1921e-05, -8.6427e-06,  9.9242e-06,  ..., -9.7752e-06,
         -7.4059e-06, -8.2701e-06],
        [-9.1344e-06, -6.2138e-06,  7.4357e-06,  ..., -7.5549e-06,
         -5.6550e-06, -6.7651e-06],
        [-9.3430e-06, -7.0035e-06,  7.5549e-06,  ..., -7.7784e-06,
         -6.2436e-06, -5.9456e-06]], device='cuda:0')
Loss: 0.9422619938850403


Running epoch 1, step 1539, batch 491
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,   944,   278,  ...,  5755,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7365e-04,  2.3827e-04,  1.5072e-04,  ..., -1.1708e-04,
         -6.8269e-04, -1.5028e-04],
        [-5.1260e-06, -3.6508e-06,  3.9190e-06,  ..., -4.2692e-06,
         -3.2969e-06, -3.6098e-06],
        [-1.6093e-05, -1.1891e-05,  1.3232e-05,  ..., -1.3277e-05,
         -1.0282e-05, -1.1191e-05],
        [-1.2264e-05, -8.5086e-06,  9.8795e-06,  ..., -1.0177e-05,
         -7.7859e-06, -9.0748e-06],
        [-1.2636e-05, -9.6560e-06,  1.0103e-05,  ..., -1.0595e-05,
         -8.6576e-06, -8.1062e-06]], device='cuda:0')
Loss: 0.9826276898384094


Running epoch 1, step 1540, batch 492
Sampled inputs[:2]: tensor([[   0, 2383, 9843,  ...,  401, 3959,  300],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0787e-04,  3.3133e-04,  2.3036e-04,  ..., -1.1708e-04,
         -8.0565e-04, -1.0372e-04],
        [-6.4597e-06, -4.5933e-06,  4.9993e-06,  ..., -5.3346e-06,
         -4.0829e-06, -4.5076e-06],
        [-2.0444e-05, -1.5095e-05,  1.6928e-05,  ..., -1.6779e-05,
         -1.2860e-05, -1.4171e-05],
        [-1.5512e-05, -1.0759e-05,  1.2621e-05,  ..., -1.2770e-05,
         -9.6783e-06, -1.1384e-05],
        [-1.5855e-05, -1.2144e-05,  1.2785e-05,  ..., -1.3232e-05,
         -1.0714e-05, -1.0163e-05]], device='cuda:0')
Loss: 0.9652355909347534


Running epoch 1, step 1541, batch 493
Sampled inputs[:2]: tensor([[   0,   18,  271,  ..., 4868,  963,  271],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0036e-04,  3.6706e-04,  2.0065e-04,  ..., -4.3004e-05,
         -9.6761e-04, -1.7322e-04],
        [-7.7710e-06, -5.5470e-06,  6.0275e-06,  ..., -6.4448e-06,
         -4.9472e-06, -5.4203e-06],
        [-2.4527e-05, -1.8165e-05,  2.0295e-05,  ..., -2.0221e-05,
         -1.5542e-05, -1.6972e-05],
        [-1.8582e-05, -1.2919e-05,  1.5125e-05,  ..., -1.5363e-05,
         -1.1690e-05, -1.3620e-05],
        [-1.9073e-05, -1.4663e-05,  1.5378e-05,  ..., -1.5989e-05,
         -1.2964e-05, -1.2204e-05]], device='cuda:0')
Loss: 0.9638528823852539


Running epoch 1, step 1542, batch 494
Sampled inputs[:2]: tensor([[    0, 21540,   527,  ...,   824,    14,   381],
        [    0,   292, 17190,  ...,  3078,     9,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0458e-04,  3.7663e-04,  2.3072e-04,  ...,  7.8599e-06,
         -9.6149e-04, -1.4170e-04],
        [-9.1940e-06, -6.4112e-06,  6.7987e-06,  ..., -7.7039e-06,
         -5.8860e-06, -6.6049e-06],
        [-2.9057e-05, -2.1070e-05,  2.2918e-05,  ..., -2.4185e-05,
         -1.8537e-05, -2.0668e-05],
        [-2.1845e-05, -1.4856e-05,  1.6898e-05,  ..., -1.8269e-05,
         -1.3866e-05, -1.6421e-05],
        [-2.3067e-05, -1.7270e-05,  1.7688e-05,  ..., -1.9521e-05,
         -1.5691e-05, -1.5333e-05]], device='cuda:0')
Loss: 0.9522014260292053


Running epoch 1, step 1543, batch 495
Sampled inputs[:2]: tensor([[    0,  1575,  4384,  ...,   328,   722,  6124],
        [    0,   266, 15794,  ...,  3128,  6479,  2626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4674e-04,  4.0932e-04,  4.3761e-04,  ...,  1.1549e-05,
         -8.8841e-04, -1.4667e-04],
        [-1.0543e-05, -7.2792e-06,  7.7188e-06,  ..., -8.8438e-06,
         -6.7391e-06, -7.6480e-06],
        [-3.3289e-05, -2.3901e-05,  2.6017e-05,  ..., -2.7716e-05,
         -2.1189e-05, -2.3916e-05],
        [-2.4945e-05, -1.6779e-05,  1.9088e-05,  ..., -2.0877e-05,
         -1.5788e-05, -1.8895e-05],
        [-2.6569e-05, -1.9670e-05,  2.0176e-05,  ..., -2.2501e-05,
         -1.8030e-05, -1.7866e-05]], device='cuda:0')
Loss: 0.9444774389266968
Graident accumulation at epoch 1, step 1543, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8154e-05,  9.1726e-05, -5.3440e-05,  ..., -3.1600e-05,
         -7.1839e-05, -9.5844e-06],
        [-5.0661e-06, -3.4992e-06,  3.4120e-06,  ..., -4.4936e-06,
         -1.6098e-06, -6.0634e-06],
        [ 4.5982e-05,  4.7301e-05, -3.4472e-05,  ...,  4.4623e-05,
          4.6541e-05,  2.6063e-05],
        [-2.3983e-06,  1.7250e-05, -3.0918e-07,  ...,  7.0618e-07,
          1.0579e-05, -4.6588e-06],
        [-2.4977e-05, -1.8456e-05,  1.9032e-05,  ..., -2.1230e-05,
         -1.7068e-05, -1.6280e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4350e-08, 5.6061e-08, 6.6123e-08,  ..., 2.6282e-08, 1.5395e-07,
         3.8361e-08],
        [8.3187e-11, 5.3268e-11, 2.3019e-11,  ..., 5.7972e-11, 2.0903e-11,
         2.3513e-11],
        [3.8254e-09, 2.4157e-09, 1.3564e-09,  ..., 3.1575e-09, 1.3201e-09,
         1.0942e-09],
        [1.0846e-09, 1.1587e-09, 4.0904e-10,  ..., 9.1816e-10, 7.1085e-10,
         3.8545e-10],
        [4.0458e-10, 2.3079e-10, 8.6651e-11,  ..., 2.9793e-10, 9.1711e-11,
         1.1323e-10]], device='cuda:0')
optimizer state dict: 193.0
lr: [3.422394974442298e-06, 3.422394974442298e-06]
scheduler_last_epoch: 193


Running epoch 1, step 1544, batch 496
Sampled inputs[:2]: tensor([[    0,  7264, 14450,  ...,   367,   654,   300],
        [    0,  1550,   685,  ...,   943,  1239,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1047e-04, -8.9018e-05, -1.0697e-04,  ..., -5.7851e-05,
         -2.8970e-05,  2.9206e-05],
        [-1.3635e-06, -9.8348e-07,  1.0282e-06,  ..., -1.0803e-06,
         -8.2701e-07, -9.3132e-07],
        [-4.2915e-06, -3.2037e-06,  3.4571e-06,  ..., -3.3975e-06,
         -2.6375e-06, -2.9057e-06],
        [-3.1888e-06, -2.2650e-06,  2.5183e-06,  ..., -2.5183e-06,
         -1.9222e-06, -2.2501e-06],
        [-3.3379e-06, -2.5779e-06,  2.6375e-06,  ..., -2.6971e-06,
         -2.2203e-06, -2.1309e-06]], device='cuda:0')
Loss: 0.9630137085914612


Running epoch 1, step 1545, batch 497
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,  4672,   278,  ...,    13,   265, 49987]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6519e-04, -1.1947e-04, -1.5835e-04,  ...,  1.8228e-05,
          2.8173e-05,  1.3818e-04],
        [-2.6971e-06, -1.9185e-06,  2.0638e-06,  ..., -2.1532e-06,
         -1.6764e-06, -1.8552e-06],
        [-8.6129e-06, -6.3479e-06,  6.9737e-06,  ..., -6.8843e-06,
         -5.4091e-06, -5.9158e-06],
        [-6.3926e-06, -4.4703e-06,  5.0962e-06,  ..., -5.0962e-06,
         -3.9339e-06, -4.5747e-06],
        [-6.7353e-06, -5.1409e-06,  5.3495e-06,  ..., -5.4687e-06,
         -4.5300e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.9814581871032715


Running epoch 1, step 1546, batch 498
Sampled inputs[:2]: tensor([[    0,  2706,   292,  ...,    13,  8954,    13],
        [    0,   634, 10095,  ...,   367, 24607,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6418e-04,  4.4018e-05, -5.6798e-05,  ..., -6.8616e-05,
          5.5760e-05,  2.2542e-04],
        [-4.0382e-06, -2.8946e-06,  2.9802e-06,  ..., -3.3081e-06,
         -2.6003e-06, -2.8908e-06],
        [-1.2815e-05, -9.5516e-06,  1.0058e-05,  ..., -1.0490e-05,
         -8.3148e-06, -9.1344e-06],
        [ 1.2470e-04,  1.8076e-04, -1.1154e-04,  ...,  1.1273e-04,
          1.3458e-04,  7.7805e-05],
        [-1.0207e-05, -7.8827e-06,  7.8231e-06,  ..., -8.5086e-06,
         -7.0930e-06, -6.8545e-06]], device='cuda:0')
Loss: 0.959831714630127


Running epoch 1, step 1547, batch 499
Sampled inputs[:2]: tensor([[    0,  4823,    12,  ...,  1756,  3406,   300],
        [    0,    47,  1838,  ...,   792,    83, 42612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3459e-04,  8.2761e-05,  5.3863e-05,  ..., -1.4156e-04,
          1.4368e-04,  2.6292e-04],
        [-5.3570e-06, -3.8296e-06,  4.0084e-06,  ..., -4.3809e-06,
         -3.4757e-06, -3.7923e-06],
        [ 1.7404e-04,  1.9753e-04, -1.8387e-04,  ...,  1.3945e-04,
          2.0584e-04,  4.7428e-05],
        [ 1.2160e-04,  1.7862e-04, -1.0902e-04,  ...,  1.1022e-04,
          1.3257e-04,  7.5569e-05],
        [-1.3486e-05, -1.0401e-05,  1.0461e-05,  ..., -1.1235e-05,
         -9.4026e-06, -8.9854e-06]], device='cuda:0')
Loss: 0.9858168363571167


Running epoch 1, step 1548, batch 500
Sampled inputs[:2]: tensor([[   0,  271, 4728,  ...,  344,  259, 1774],
        [   0, 1874,  300,  ...,   14, 5372,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0047e-04,  2.7712e-04,  7.8749e-05,  ..., -2.1438e-04,
          5.2477e-05,  2.7134e-04],
        [-6.7949e-06, -4.6901e-06,  4.9509e-06,  ..., -5.5134e-06,
         -4.3586e-06, -4.8950e-06],
        [ 1.6962e-04,  1.9468e-04, -1.8069e-04,  ...,  1.3599e-04,
          2.0308e-04,  4.4090e-05],
        [ 1.1838e-04,  1.7673e-04, -1.0680e-04,  ...,  1.0767e-04,
          1.3058e-04,  7.3021e-05],
        [-1.7092e-05, -1.2770e-05,  1.2964e-05,  ..., -1.4111e-05,
         -1.1787e-05, -1.1533e-05]], device='cuda:0')
Loss: 0.8916754722595215


Running epoch 1, step 1549, batch 501
Sampled inputs[:2]: tensor([[    0,   278,  4575,  ...,  1220,   278,  4575],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6046e-04,  3.4073e-04,  1.3472e-04,  ..., -2.3553e-04,
         -2.4461e-04,  2.1462e-04],
        [-8.0988e-06, -5.6028e-06,  5.8971e-06,  ..., -6.6236e-06,
         -5.2266e-06, -5.9158e-06],
        [ 1.6569e-04,  1.9186e-04, -1.7759e-04,  ...,  1.3271e-04,
          2.0053e-04,  4.1109e-05],
        [ 1.1533e-04,  1.7467e-04, -1.0449e-04,  ...,  1.0510e-04,
          1.2858e-04,  7.0548e-05],
        [-2.0385e-05, -1.5199e-05,  1.5482e-05,  ..., -1.6883e-05,
         -1.4037e-05, -1.3843e-05]], device='cuda:0')
Loss: 0.9504584074020386


Running epoch 1, step 1550, batch 502
Sampled inputs[:2]: tensor([[   0, 9855,  278,  ...,  266, 3134,  278],
        [   0,  298, 2587,  ...,  298,  894,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9314e-04,  4.8213e-04,  2.6956e-04,  ..., -3.2854e-04,
         -4.6379e-04,  2.1074e-04],
        [-9.5218e-06, -6.5528e-06,  6.7838e-06,  ..., -7.7784e-06,
         -6.1281e-06, -7.0184e-06],
        [ 1.6107e-04,  1.8863e-04, -1.7449e-04,  ...,  1.2897e-04,
          1.9760e-04,  3.7548e-05],
        [ 1.1211e-04,  1.7257e-04, -1.0240e-04,  ...,  1.0250e-04,
          1.2655e-04,  6.7985e-05],
        [-2.4110e-05, -1.7852e-05,  1.7911e-05,  ..., -1.9938e-05,
         -1.6540e-05, -1.6540e-05]], device='cuda:0')
Loss: 0.9224151968955994


Running epoch 1, step 1551, batch 503
Sampled inputs[:2]: tensor([[    0,  2496, 10545,  ...,   287, 13978,   408],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1051e-04,  7.0759e-04,  4.3097e-04,  ..., -3.2993e-04,
         -7.4191e-04,  2.6208e-04],
        [-1.0870e-05, -7.5735e-06,  7.7225e-06,  ..., -8.9109e-06,
         -7.1265e-06, -8.0392e-06],
        [ 1.5681e-04,  1.8526e-04, -1.7137e-04,  ...,  1.2540e-04,
          1.9442e-04,  3.4300e-05],
        [ 1.0904e-04,  1.7029e-04, -1.0017e-04,  ...,  9.9925e-05,
          1.2429e-04,  6.5541e-05],
        [-2.7657e-05, -2.0757e-05,  2.0459e-05,  ..., -2.2978e-05,
         -1.9342e-05, -1.9103e-05]], device='cuda:0')
Loss: 0.9548665285110474
Graident accumulation at epoch 1, step 1551, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.7389e-05,  1.5331e-04, -4.9996e-06,  ..., -6.1433e-05,
         -1.3885e-04,  1.7582e-05],
        [-5.6465e-06, -3.9066e-06,  3.8430e-06,  ..., -4.9353e-06,
         -2.1615e-06, -6.2610e-06],
        [ 5.7065e-05,  6.1097e-05, -4.8162e-05,  ...,  5.2700e-05,
          6.1329e-05,  2.6886e-05],
        [ 8.7455e-06,  3.2554e-05, -1.0295e-05,  ...,  1.0628e-05,
          2.1950e-05,  2.3612e-06],
        [-2.5245e-05, -1.8686e-05,  1.9175e-05,  ..., -2.1404e-05,
         -1.7295e-05, -1.6563e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4340e-08, 5.6506e-08, 6.6243e-08,  ..., 2.6365e-08, 1.5435e-07,
         3.8391e-08],
        [8.3222e-11, 5.3272e-11, 2.3056e-11,  ..., 5.7994e-11, 2.0933e-11,
         2.3555e-11],
        [3.8462e-09, 2.4476e-09, 1.3844e-09,  ..., 3.1700e-09, 1.3566e-09,
         1.0943e-09],
        [1.0954e-09, 1.1865e-09, 4.1867e-10,  ..., 9.2723e-10, 7.2558e-10,
         3.8936e-10],
        [4.0494e-10, 2.3099e-10, 8.6983e-11,  ..., 2.9816e-10, 9.1993e-11,
         1.1348e-10]], device='cuda:0')
optimizer state dict: 194.0
lr: [3.32978851444543e-06, 3.32978851444543e-06]
scheduler_last_epoch: 194


Running epoch 1, step 1552, batch 504
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,  1698,    13, 24137],
        [    0,  3592,   417,  ...,  4893,   328,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3375e-04, -1.0495e-04, -1.3037e-04,  ...,  1.7028e-04,
         -2.6542e-04, -4.1163e-05],
        [-1.3188e-06, -8.4192e-07,  1.0505e-06,  ..., -1.0282e-06,
         -7.8231e-07, -9.6858e-07],
        [-4.3809e-06, -2.9206e-06,  3.6806e-06,  ..., -3.3975e-06,
         -2.6077e-06, -3.2037e-06],
        [-3.2485e-06, -2.0117e-06,  2.6822e-06,  ..., -2.5183e-06,
         -1.9073e-06, -2.5034e-06],
        [-3.3379e-06, -2.3246e-06,  2.7418e-06,  ..., -2.6524e-06,
         -2.1309e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9642964005470276


Running epoch 1, step 1553, batch 505
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,  5150,  1030,  ...,    14,   475,  1763]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2383e-04, -2.4240e-05, -2.1901e-04,  ...,  1.6237e-04,
         -2.6693e-04,  5.2917e-05],
        [-2.6226e-06, -1.7136e-06,  2.0415e-06,  ..., -2.0713e-06,
         -1.6093e-06, -1.8850e-06],
        [-8.4341e-06, -5.7518e-06,  7.0333e-06,  ..., -6.6459e-06,
         -5.1856e-06, -6.0052e-06],
        [-6.3181e-06, -3.9935e-06,  5.1558e-06,  ..., -4.9770e-06,
         -3.8445e-06, -4.7684e-06],
        [-6.5714e-06, -4.6790e-06,  5.3346e-06,  ..., -5.2750e-06,
         -4.3213e-06, -4.3660e-06]], device='cuda:0')
Loss: 0.9732994437217712


Running epoch 1, step 1554, batch 506
Sampled inputs[:2]: tensor([[   0, 1380,  342,  ..., 3904,  259,  624],
        [   0,  669,   14,  ...,  596,  292,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3769e-04, -1.0797e-04, -2.5411e-04,  ...,  1.9947e-04,
         -3.4390e-04,  5.2917e-05],
        [-4.0308e-06, -2.5928e-06,  3.0473e-06,  ..., -3.2187e-06,
         -2.4140e-06, -2.9132e-06],
        [-1.2904e-05, -8.6427e-06,  1.0431e-05,  ..., -1.0237e-05,
         -7.7188e-06, -9.1940e-06],
        [-9.5218e-06, -5.9456e-06,  7.5400e-06,  ..., -7.5847e-06,
         -5.6475e-06, -7.1973e-06],
        [-1.0222e-05, -7.1228e-06,  8.0466e-06,  ..., -8.2701e-06,
         -6.5565e-06, -6.8247e-06]], device='cuda:0')
Loss: 0.9536612629890442


Running epoch 1, step 1555, batch 507
Sampled inputs[:2]: tensor([[   0,  767, 1811,  ..., 1441, 1428,  278],
        [   0,  857,  352,  ..., 3608,  271,  995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7422e-04, -1.5611e-05, -3.4555e-04,  ...,  8.9967e-05,
         -5.4056e-04,  4.7616e-05],
        [-5.3197e-06, -3.5092e-06,  4.0680e-06,  ..., -4.2766e-06,
         -3.2745e-06, -3.8445e-06],
        [-1.6958e-05, -1.1608e-05,  1.3843e-05,  ..., -1.3515e-05,
         -1.0386e-05, -1.2085e-05],
        [-1.2547e-05, -8.0317e-06,  1.0043e-05,  ..., -1.0043e-05,
         -7.6443e-06, -9.4771e-06],
        [-1.3426e-05, -9.5814e-06,  1.0684e-05,  ..., -1.0923e-05,
         -8.8364e-06, -8.9407e-06]], device='cuda:0')
Loss: 0.9263343214988708


Running epoch 1, step 1556, batch 508
Sampled inputs[:2]: tensor([[   0,   12,  287,  ..., 2336,  221,  334],
        [   0, 4929, 4214,  ..., 1172,  591, 4422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9047e-06, -8.4616e-05, -2.8023e-04,  ...,  1.4303e-04,
         -5.6296e-04,  9.5245e-05],
        [-6.6236e-06, -4.4703e-06,  4.9211e-06,  ..., -5.4017e-06,
         -4.2357e-06, -4.8727e-06],
        [-2.1070e-05, -1.4707e-05,  1.6779e-05,  ..., -1.6972e-05,
         -1.3337e-05, -1.5214e-05],
        [-1.5557e-05, -1.0192e-05,  1.2100e-05,  ..., -1.2636e-05,
         -9.8497e-06, -1.1966e-05],
        [-1.6809e-05, -1.2204e-05,  1.3053e-05,  ..., -1.3813e-05,
         -1.1384e-05, -1.1340e-05]], device='cuda:0')
Loss: 0.9549311399459839


Running epoch 1, step 1557, batch 509
Sampled inputs[:2]: tensor([[   0, 1644, 1742,  ...,  287, 1704, 2044],
        [   0, 2366, 5036,  ..., 1477,  352,  631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1951e-05, -1.2507e-04, -2.8612e-04,  ...,  1.9873e-04,
         -5.5427e-04,  6.8614e-05],
        [-8.0317e-06, -5.2974e-06,  5.8711e-06,  ..., -6.4969e-06,
         -5.0217e-06, -6.0126e-06],
        [-2.5570e-05, -1.7479e-05,  2.0042e-05,  ..., -2.0429e-05,
         -1.5840e-05, -1.8790e-05],
        [-1.8761e-05, -1.2033e-05,  1.4335e-05,  ..., -1.5125e-05,
         -1.1638e-05, -1.4633e-05],
        [-2.0474e-05, -1.4484e-05,  1.5616e-05,  ..., -1.6674e-05,
         -1.3530e-05, -1.4067e-05]], device='cuda:0')
Loss: 0.936498761177063


Running epoch 1, step 1558, batch 510
Sampled inputs[:2]: tensor([[   0, 2296,  446,  ..., 2937,  287, 2795],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3504e-05, -1.9574e-04, -3.9172e-04,  ...,  2.0101e-04,
         -6.2753e-04,  7.5132e-05],
        [-9.3430e-06, -6.2473e-06,  6.8992e-06,  ..., -7.5698e-06,
         -5.8860e-06, -6.9663e-06],
        [-2.9653e-05, -2.0579e-05,  2.3440e-05,  ..., -2.3782e-05,
         -1.8582e-05, -2.1800e-05],
        [-2.1800e-05, -1.4208e-05,  1.6838e-05,  ..., -1.7598e-05,
         -1.3635e-05, -1.6958e-05],
        [-2.3797e-05, -1.7121e-05,  1.8328e-05,  ..., -1.9476e-05,
         -1.5929e-05, -1.6376e-05]], device='cuda:0')
Loss: 0.9546390771865845


Running epoch 1, step 1559, batch 511
Sampled inputs[:2]: tensor([[   0,   14,  417,  ...,   43,  503,   67],
        [   0,   27, 3961,  ...,  462,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2340e-05, -2.6338e-04, -3.5590e-04,  ...,  3.1567e-04,
         -8.8327e-04, -1.4794e-04],
        [-1.0632e-05, -7.1265e-06,  7.9945e-06,  ..., -8.6352e-06,
         -6.6720e-06, -7.9200e-06],
        [-3.3677e-05, -2.3440e-05,  2.7061e-05,  ..., -2.7120e-05,
         -2.1070e-05, -2.4781e-05],
        [-2.4885e-05, -1.6250e-05,  1.9595e-05,  ..., -2.0146e-05,
         -1.5497e-05, -1.9357e-05],
        [-2.6926e-05, -1.9461e-05,  2.1055e-05,  ..., -2.2098e-05,
         -1.8016e-05, -1.8537e-05]], device='cuda:0')
Loss: 0.9705153703689575
Graident accumulation at epoch 1, step 1559, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1416e-05,  1.1164e-04, -4.0090e-05,  ..., -2.3722e-05,
         -2.1329e-04,  1.0300e-06],
        [-6.1450e-06, -4.2286e-06,  4.2582e-06,  ..., -5.3053e-06,
         -2.6126e-06, -6.4269e-06],
        [ 4.7990e-05,  5.2644e-05, -4.0639e-05,  ...,  4.4718e-05,
          5.3089e-05,  2.1720e-05],
        [ 5.3824e-06,  2.7674e-05, -7.3061e-06,  ...,  7.5506e-06,
          1.8205e-05,  1.8941e-07],
        [-2.5413e-05, -1.8764e-05,  1.9363e-05,  ..., -2.1474e-05,
         -1.7367e-05, -1.6760e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4286e-08, 5.6518e-08, 6.6303e-08,  ..., 2.6438e-08, 1.5497e-07,
         3.8375e-08],
        [8.3252e-11, 5.3269e-11, 2.3097e-11,  ..., 5.8010e-11, 2.0956e-11,
         2.3594e-11],
        [3.8435e-09, 2.4457e-09, 1.3838e-09,  ..., 3.1676e-09, 1.3556e-09,
         1.0938e-09],
        [1.0949e-09, 1.1856e-09, 4.1863e-10,  ..., 9.2671e-10, 7.2510e-10,
         3.8934e-10],
        [4.0526e-10, 2.3114e-10, 8.7339e-11,  ..., 2.9835e-10, 9.2226e-11,
         1.1371e-10]], device='cuda:0')
optimizer state dict: 195.0
lr: [3.2382013209886466e-06, 3.2382013209886466e-06]
scheduler_last_epoch: 195


Running epoch 1, step 1560, batch 512
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,   360,   259,  ...,  5710,   278,  2433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4114e-04,  1.0697e-04, -2.1862e-06,  ...,  1.0761e-04,
         -2.5738e-04, -1.5104e-04],
        [-1.2517e-06, -7.1898e-07,  9.5367e-07,  ..., -1.0431e-06,
         -7.6368e-07, -9.9838e-07],
        [-3.7551e-06, -2.2203e-06,  3.1143e-06,  ..., -3.0249e-06,
         -2.2501e-06, -2.8312e-06],
        [-2.8908e-06, -1.5646e-06,  2.3246e-06,  ..., -2.3991e-06,
         -1.7658e-06, -2.3842e-06],
        [-3.1292e-06, -1.9222e-06,  2.5034e-06,  ..., -2.5630e-06,
         -2.0117e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.8883062601089478


Running epoch 1, step 1561, batch 513
Sampled inputs[:2]: tensor([[   0, 1197,  729,  ...,  674,  369, 8222],
        [   0,  266, 8802,  ..., 8401,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4901e-05,  1.1992e-04, -3.3995e-05,  ...,  1.0611e-05,
         -8.8679e-05, -1.8313e-04],
        [-2.6152e-06, -1.7546e-06,  1.9893e-06,  ..., -2.1458e-06,
         -1.6615e-06, -1.9260e-06],
        [ 5.2939e-04,  4.0791e-04, -5.0550e-04,  ...,  4.0892e-04,
          4.1685e-04,  1.4615e-04],
        [-5.9903e-06, -3.9041e-06,  4.7982e-06,  ..., -4.9174e-06,
         -3.8370e-06, -4.6194e-06],
        [-6.5118e-06, -4.7684e-06,  5.2154e-06,  ..., -5.3793e-06,
         -4.4703e-06, -4.3213e-06]], device='cuda:0')
Loss: 1.0042023658752441


Running epoch 1, step 1562, batch 514
Sampled inputs[:2]: tensor([[    0,   298, 22296,  ...,   287,  6494,   644],
        [    0,   257,   221,  ...,  1474,  2044,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4661e-04,  1.5132e-04, -1.0749e-04,  ..., -6.3719e-05,
          9.8754e-06, -1.1198e-04],
        [-4.1127e-06, -2.5257e-06,  2.8126e-06,  ..., -3.4645e-06,
         -2.5518e-06, -3.2373e-06],
        [ 5.2498e-04,  4.0546e-04, -5.0279e-04,  ...,  4.0515e-04,
          4.1423e-04,  1.4245e-04],
        [-9.0748e-06, -5.4389e-06,  6.5491e-06,  ..., -7.6443e-06,
         -5.6848e-06, -7.4059e-06],
        [-1.0654e-05, -7.0781e-06,  7.7039e-06,  ..., -8.9407e-06,
         -7.0482e-06, -7.5847e-06]], device='cuda:0')
Loss: 0.9094265699386597


Running epoch 1, step 1563, batch 515
Sampled inputs[:2]: tensor([[    0,   271, 36770,  ...,   278,  1398,  4555],
        [    0,   638,  2708,  ..., 28492,  1814,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5088e-04,  1.6311e-04, -1.8208e-04,  ..., -2.2155e-04,
          3.0702e-05, -1.9963e-04],
        [-5.3942e-06, -3.5390e-06,  3.9004e-06,  ..., -4.5076e-06,
         -3.4086e-06, -4.1462e-06],
        [ 5.2080e-04,  4.0202e-04, -4.9906e-04,  ...,  4.0171e-04,
          4.1142e-04,  1.3941e-04],
        [-1.2159e-05, -7.8529e-06,  9.2760e-06,  ..., -1.0163e-05,
         -7.7263e-06, -9.7305e-06],
        [-1.3903e-05, -9.8944e-06,  1.0535e-05,  ..., -1.1697e-05,
         -9.4175e-06, -9.7901e-06]], device='cuda:0')
Loss: 1.0068590641021729


Running epoch 1, step 1564, batch 516
Sampled inputs[:2]: tensor([[   0,   13, 4596,  ...,  408,  689,  298],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2677e-04,  2.1621e-04,  3.0050e-05,  ..., -2.7518e-04,
         -2.2533e-04, -3.1342e-04],
        [-6.7651e-06, -4.4331e-06,  4.4890e-06,  ..., -5.6997e-06,
         -4.4443e-06, -5.2564e-06],
        [ 5.1663e-04,  3.9911e-04, -4.9698e-04,  ...,  3.9813e-04,
          4.0822e-04,  1.3613e-04],
        [-1.5259e-05, -9.8199e-06,  1.0677e-05,  ..., -1.2875e-05,
         -1.0110e-05, -1.2293e-05],
        [-1.7568e-05, -1.2442e-05,  1.2286e-05,  ..., -1.4871e-05,
         -1.2323e-05, -1.2457e-05]], device='cuda:0')
Loss: 0.8791845440864563


Running epoch 1, step 1565, batch 517
Sampled inputs[:2]: tensor([[    0,  1854,   292,  ...,   328,  1360,    14],
        [    0,   266, 12080,  ...,   674,   369, 10956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2677e-04,  1.6958e-04, -6.4136e-05,  ..., -2.6316e-04,
         -2.9740e-05, -3.7905e-04],
        [-8.0839e-06, -5.2750e-06,  5.4315e-06,  ..., -6.8173e-06,
         -5.2527e-06, -6.3367e-06],
        [ 5.1258e-04,  3.9643e-04, -4.9383e-04,  ...,  3.9478e-04,
          4.0578e-04,  1.3294e-04],
        [-1.8254e-05, -1.1660e-05,  1.2912e-05,  ..., -1.5378e-05,
         -1.1906e-05, -1.4797e-05],
        [-2.0996e-05, -1.4767e-05,  1.4849e-05,  ..., -1.7747e-05,
         -1.4529e-05, -1.4946e-05]], device='cuda:0')
Loss: 0.9686618447303772


Running epoch 1, step 1566, batch 518
Sampled inputs[:2]: tensor([[    0,   344,  2574,  ...,  2558,  2663,   328],
        [    0,   300, 12579,  ...,  1722,   369,  5049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0980e-05,  1.5482e-04, -1.0692e-04,  ..., -2.6938e-04,
          1.3584e-05, -5.3845e-04],
        [-9.3207e-06, -6.1989e-06,  6.4000e-06,  ..., -7.8529e-06,
         -6.1244e-06, -7.2718e-06],
        [ 5.0864e-04,  3.9345e-04, -4.9054e-04,  ...,  3.9156e-04,
          4.0308e-04,  1.3002e-04],
        [-2.1175e-05, -1.3776e-05,  1.5311e-05,  ..., -1.7792e-05,
         -1.3903e-05, -1.7092e-05],
        [-2.4199e-05, -1.7285e-05,  1.7442e-05,  ..., -2.0429e-05,
         -1.6868e-05, -1.7136e-05]], device='cuda:0')
Loss: 0.9978498816490173


Running epoch 1, step 1567, batch 519
Sampled inputs[:2]: tensor([[    0,   474,   221,  ..., 32291,   360,  2458],
        [    0,  2426,   699,  ...,   221,  1551,   720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7011e-05,  2.1493e-04, -6.5062e-05,  ..., -8.9396e-05,
         -1.0002e-05, -5.3346e-04],
        [ 8.1282e-05,  4.4060e-05, -4.8953e-05,  ...,  5.4930e-05,
          7.1618e-05,  1.0199e-04],
        [ 5.0420e-04,  3.9059e-04, -4.8719e-04,  ...,  3.8807e-04,
          4.0049e-04,  1.2673e-04],
        [-2.4468e-05, -1.5743e-05,  1.7740e-05,  ..., -2.0370e-05,
         -1.5780e-05, -1.9610e-05],
        [-2.7925e-05, -1.9789e-05,  2.0154e-05,  ..., -2.3425e-05,
         -1.9222e-05, -1.9744e-05]], device='cuda:0')
Loss: 0.9518764615058899
Graident accumulation at epoch 1, step 1567, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3976e-05,  1.2197e-04, -4.2587e-05,  ..., -3.0290e-05,
         -1.9296e-04, -5.2419e-05],
        [ 2.5977e-06,  6.0029e-07, -1.0630e-06,  ...,  7.1829e-07,
          4.8105e-06,  4.4149e-06],
        [ 9.3612e-05,  8.6438e-05, -8.5294e-05,  ...,  7.9054e-05,
          8.7829e-05,  3.2220e-05],
        [ 2.3974e-06,  2.3332e-05, -4.8015e-06,  ...,  4.7586e-06,
          1.4807e-05, -1.7905e-06],
        [-2.5664e-05, -1.8866e-05,  1.9442e-05,  ..., -2.1669e-05,
         -1.7553e-05, -1.7059e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4235e-08, 5.6508e-08, 6.6241e-08,  ..., 2.6420e-08, 1.5482e-07,
         3.8621e-08],
        [8.9776e-11, 5.5157e-11, 2.5470e-11,  ..., 6.0969e-11, 2.6064e-11,
         3.3972e-11],
        [4.0938e-09, 2.5958e-09, 1.6197e-09,  ..., 3.3150e-09, 1.5147e-09,
         1.1088e-09],
        [1.0944e-09, 1.1846e-09, 4.1853e-10,  ..., 9.2619e-10, 7.2462e-10,
         3.8934e-10],
        [4.0563e-10, 2.3130e-10, 8.7658e-11,  ..., 2.9860e-10, 9.2503e-11,
         1.1399e-10]], device='cuda:0')
optimizer state dict: 196.0
lr: [3.1476473893945937e-06, 3.1476473893945937e-06]
scheduler_last_epoch: 196


Running epoch 1, step 1568, batch 520
Sampled inputs[:2]: tensor([[    0,   269,    12,  ..., 45645,    14,   298],
        [    0,  3978,  2697,  ...,   461,  5955,  3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2881e-04,  5.5629e-05, -2.0254e-04,  ...,  1.0548e-05,
          2.5800e-04,  1.4417e-04],
        [-1.5497e-06, -8.7917e-07,  7.8604e-07,  ..., -1.3188e-06,
         -1.0654e-06, -1.3411e-06],
        [-4.4405e-06, -2.6375e-06,  2.5183e-06,  ..., -3.6955e-06,
         -3.0547e-06, -3.6806e-06],
        [-3.1590e-06, -1.7434e-06,  1.6466e-06,  ..., -2.6971e-06,
         -2.2054e-06, -2.7716e-06],
        [-4.4107e-06, -2.6375e-06,  2.4587e-06,  ..., -3.6955e-06,
         -3.1441e-06, -3.4422e-06]], device='cuda:0')
Loss: 0.9574397802352905


Running epoch 1, step 1569, batch 521
Sampled inputs[:2]: tensor([[    0, 16803,   965,  ..., 36064,    12, 13769],
        [    0,  7230,    13,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9596e-05,  1.2328e-04, -2.5785e-04,  ..., -4.5223e-05,
          6.7619e-05,  4.6151e-05],
        [-2.8759e-06, -1.8328e-06,  1.8515e-06,  ..., -2.3842e-06,
         -1.8924e-06, -2.2911e-06],
        [-8.6129e-06, -5.8115e-06,  6.0797e-06,  ..., -7.0632e-06,
         -5.6922e-06, -6.6906e-06],
        [-6.2883e-06, -3.9786e-06,  4.2990e-06,  ..., -5.2005e-06,
         -4.1425e-06, -5.1558e-06],
        [-7.6741e-06, -5.2601e-06,  5.1856e-06,  ..., -6.4075e-06,
         -5.4091e-06, -5.6326e-06]], device='cuda:0')
Loss: 0.9612594842910767


Running epoch 1, step 1570, batch 522
Sampled inputs[:2]: tensor([[   0,    9, 1471,  ...,  741,  266, 5821],
        [   0,  462,  221,  ...,   29,  413, 1801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9495e-05,  1.2609e-04, -3.1703e-04,  ...,  1.0557e-04,
          1.4455e-04,  6.8957e-05],
        [-4.2841e-06, -2.6859e-06,  2.8200e-06,  ..., -3.5465e-06,
         -2.7716e-06, -3.3267e-06],
        [-1.2815e-05, -8.4937e-06,  9.2238e-06,  ..., -1.0446e-05,
         -8.3297e-06, -9.6858e-06],
        [-9.4175e-06, -5.8189e-06,  6.5640e-06,  ..., -7.7784e-06,
         -6.1244e-06, -7.5400e-06],
        [-1.1206e-05, -7.6294e-06,  7.7635e-06,  ..., -9.3132e-06,
         -7.7784e-06, -7.9721e-06]], device='cuda:0')
Loss: 0.9347103834152222


Running epoch 1, step 1571, batch 523
Sampled inputs[:2]: tensor([[    0,    13,  4363,  ...,   271,  2462,   709],
        [    0, 43071,   278,  ...,   266, 21576,  5936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7148e-04, -3.3376e-05, -3.7334e-04,  ...,  3.1598e-04,
          5.4430e-04,  4.3902e-05],
        [-5.7295e-06, -3.4757e-06,  3.6731e-06,  ..., -4.8056e-06,
         -3.6955e-06, -4.5560e-06],
        [-1.7136e-05, -1.0923e-05,  1.2010e-05,  ..., -1.4096e-05,
         -1.1012e-05, -1.3217e-05],
        [-1.2547e-05, -7.4506e-06,  8.4564e-06,  ..., -1.0490e-05,
         -8.0913e-06, -1.0252e-05],
        [-1.5080e-05, -9.8348e-06,  1.0207e-05,  ..., -1.2621e-05,
         -1.0297e-05, -1.0937e-05]], device='cuda:0')
Loss: 0.9464479088783264


Running epoch 1, step 1572, batch 524
Sampled inputs[:2]: tensor([[    0,  3231,   271,  ...,  9279,  8231, 28871],
        [    0,  5116,  4330,  ...,   925,   699,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4255e-05,  7.0179e-05, -4.3102e-04,  ...,  1.5195e-04,
          3.6737e-04,  2.6518e-05],
        [-7.0706e-06, -4.3511e-06,  4.6082e-06,  ..., -5.9083e-06,
         -4.5896e-06, -5.6662e-06],
        [-2.1279e-05, -1.3724e-05,  1.5140e-05,  ..., -1.7434e-05,
         -1.3709e-05, -1.6570e-05],
        [-1.5482e-05, -9.2909e-06,  1.0617e-05,  ..., -1.2875e-05,
         -1.0014e-05, -1.2755e-05],
        [-1.8671e-05, -1.2338e-05,  1.2845e-05,  ..., -1.5572e-05,
         -1.2815e-05, -1.3649e-05]], device='cuda:0')
Loss: 0.9197297692298889


Running epoch 1, step 1573, batch 525
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  275, 2576, 3588],
        [   0,  925,  271,  ...,  631, 3370,  940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0570e-04,  3.7838e-05, -3.7024e-04,  ...,  2.0384e-04,
          8.2555e-04, -1.0133e-05],
        [-8.4117e-06, -5.2042e-06,  5.6066e-06,  ..., -6.9812e-06,
         -5.3793e-06, -6.6720e-06],
        [-2.5332e-05, -1.6540e-05,  1.8463e-05,  ..., -2.0713e-05,
         -1.6153e-05, -1.9610e-05],
        [-1.8477e-05, -1.1198e-05,  1.3001e-05,  ..., -1.5303e-05,
         -1.1802e-05, -1.5125e-05],
        [-2.2009e-05, -1.4737e-05,  1.5482e-05,  ..., -1.8328e-05,
         -1.5020e-05, -1.5974e-05]], device='cuda:0')
Loss: 0.93797767162323


Running epoch 1, step 1574, batch 526
Sampled inputs[:2]: tensor([[   0, 1106,  259,  ...,  271,  679,  382],
        [   0,  221,  334,  ...,  271,  266, 7246]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0900e-04,  5.7452e-05, -2.0927e-04,  ...,  1.7561e-04,
          1.1144e-03, -7.4081e-05],
        [-9.8124e-06, -6.2026e-06,  6.5193e-06,  ..., -8.2105e-06,
         -6.4522e-06, -7.7598e-06],
        [-2.9504e-05, -1.9699e-05,  2.1398e-05,  ..., -2.4348e-05,
         -1.9357e-05, -2.2814e-05],
        [-2.1517e-05, -1.3344e-05,  1.5058e-05,  ..., -1.7971e-05,
         -1.4141e-05, -1.7598e-05],
        [-2.5764e-05, -1.7643e-05,  1.8090e-05,  ..., -2.1636e-05,
         -1.8030e-05, -1.8701e-05]], device='cuda:0')
Loss: 1.020459771156311


Running epoch 1, step 1575, batch 527
Sampled inputs[:2]: tensor([[    0, 10565,  2677,  ...,   298,   292, 11188],
        [    0,   266,   997,  ...,  2670,     5,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1940e-04,  8.6784e-05, -3.5601e-04,  ...,  1.4931e-04,
          1.1258e-03, -6.1938e-05],
        [-1.1109e-05, -7.0482e-06,  7.5102e-06,  ..., -9.2760e-06,
         -7.2680e-06, -8.7135e-06],
        [-3.3617e-05, -2.2486e-05,  2.4736e-05,  ..., -2.7686e-05,
         -2.1875e-05, -2.5794e-05],
        [-2.4542e-05, -1.5244e-05,  1.7472e-05,  ..., -2.0444e-05,
         -1.5989e-05, -1.9938e-05],
        [-2.8953e-05, -1.9893e-05,  2.0608e-05,  ..., -2.4274e-05,
         -2.0131e-05, -2.0832e-05]], device='cuda:0')
Loss: 0.9508134126663208
Graident accumulation at epoch 1, step 1575, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0288,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2518e-05,  1.1845e-04, -7.3930e-05,  ..., -1.2330e-05,
         -6.1080e-05, -5.3371e-05],
        [ 1.2270e-06, -1.6457e-07, -2.0565e-07,  ..., -2.8114e-07,
          3.6027e-06,  3.1021e-06],
        [ 8.0889e-05,  7.5546e-05, -7.4291e-05,  ...,  6.8380e-05,
          7.6859e-05,  2.6419e-05],
        [-2.9654e-07,  1.9474e-05, -2.5742e-06,  ...,  2.2383e-06,
          1.1727e-05, -3.6052e-06],
        [-2.5993e-05, -1.8969e-05,  1.9559e-05,  ..., -2.1929e-05,
         -1.7811e-05, -1.7436e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4283e-08, 5.6459e-08, 6.6301e-08,  ..., 2.6415e-08, 1.5593e-07,
         3.8586e-08],
        [8.9809e-11, 5.5152e-11, 2.5501e-11,  ..., 6.0995e-11, 2.6091e-11,
         3.4014e-11],
        [4.0909e-09, 2.5937e-09, 1.6187e-09,  ..., 3.3125e-09, 1.5136e-09,
         1.1084e-09],
        [1.0939e-09, 1.1837e-09, 4.1842e-10,  ..., 9.2569e-10, 7.2415e-10,
         3.8935e-10],
        [4.0607e-10, 2.3147e-10, 8.7995e-11,  ..., 2.9889e-10, 9.2816e-11,
         1.1431e-10]], device='cuda:0')
optimizer state dict: 197.0
lr: [3.058140557094472e-06, 3.058140557094472e-06]
scheduler_last_epoch: 197


Running epoch 1, step 1576, batch 528
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0,  4385,   342,  ...,  3644,   775,   874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6314e-05,  3.6512e-05,  1.1684e-04,  ...,  7.8430e-05,
          8.1733e-05,  2.1753e-05],
        [-1.2517e-06, -9.6858e-07,  1.0058e-06,  ..., -1.0580e-06,
         -9.1270e-07, -9.1270e-07],
        [-3.9935e-06, -3.2037e-06,  3.3975e-06,  ..., -3.3677e-06,
         -2.8908e-06, -2.9504e-06],
        [-3.0100e-06, -2.2650e-06,  2.5183e-06,  ..., -2.5183e-06,
         -2.1756e-06, -2.3395e-06],
        [-3.2336e-06, -2.7269e-06,  2.6971e-06,  ..., -2.8163e-06,
         -2.5332e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.004167079925537


Running epoch 1, step 1577, batch 529
Sampled inputs[:2]: tensor([[   0,   13,  786,  ...,  275, 2623,   13],
        [   0,  600, 9092,  ...,  554, 1485,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0712e-05,  6.1202e-05,  7.2788e-05,  ...,  1.1202e-04,
          2.3543e-06, -6.3315e-05],
        [-2.6003e-06, -1.9297e-06,  2.1309e-06,  ..., -2.1532e-06,
         -1.7770e-06, -1.8440e-06],
        [ 5.4138e-05,  5.9449e-05, -5.3872e-06,  ...,  7.3433e-05,
          5.6884e-05,  2.2792e-05],
        [-6.0499e-06, -4.3809e-06,  5.1558e-06,  ..., -4.9770e-06,
         -4.0755e-06, -4.5598e-06],
        [-6.5565e-06, -5.3048e-06,  5.5432e-06,  ..., -5.5730e-06,
         -4.7982e-06, -4.3958e-06]], device='cuda:0')
Loss: 0.9945470690727234


Running epoch 1, step 1578, batch 530
Sampled inputs[:2]: tensor([[   0,   14,  221,  ...,  298,  408, 1849],
        [   0,  894,   73,  ..., 2323,  909, 4103]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1537e-04,  7.7237e-05,  9.0865e-05,  ...,  2.3522e-04,
          6.0434e-04, -1.7505e-05],
        [-4.0531e-06, -2.8200e-06,  3.0808e-06,  ..., -3.3230e-06,
         -2.6710e-06, -2.9616e-06],
        [ 1.2072e-04,  1.6765e-04, -8.5993e-06,  ...,  1.2159e-04,
          1.6482e-04,  6.0752e-05],
        [-9.2685e-06, -6.2883e-06,  7.3314e-06,  ..., -7.5698e-06,
         -6.0424e-06, -7.1079e-06],
        [-1.0297e-05, -7.7784e-06,  8.0317e-06,  ..., -8.6427e-06,
         -7.2718e-06, -7.0930e-06]], device='cuda:0')
Loss: 0.968096911907196


Running epoch 1, step 1579, batch 531
Sampled inputs[:2]: tensor([[   0, 5902,  518,  ..., 3126,   12,  497],
        [   0, 2555,  984,  ..., 5900, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6648e-04,  1.5447e-04,  8.5527e-05,  ...,  1.5627e-04,
          6.0906e-04,  2.6404e-05],
        [-5.3346e-06, -3.6620e-06,  4.0941e-06,  ..., -4.3809e-06,
         -3.5055e-06, -3.9116e-06],
        [ 1.1688e-04,  1.6503e-04, -5.3061e-06,  ...,  1.1846e-04,
          1.6231e-04,  5.7980e-05],
        [-1.2159e-05, -8.1211e-06,  9.7603e-06,  ..., -9.9391e-06,
         -7.9125e-06, -9.3430e-06],
        [-1.3396e-05, -1.0014e-05,  1.0610e-05,  ..., -1.1221e-05,
         -9.4324e-06, -9.1493e-06]], device='cuda:0')
Loss: 0.9638755321502686


Running epoch 1, step 1580, batch 532
Sampled inputs[:2]: tensor([[    0,    13,  1107,  ...,   287, 25185,    14],
        [    0,   685,   344,  ...,   680,   401,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2991e-04, -3.5179e-05,  1.3552e-05,  ...,  2.2339e-04,
          6.0314e-04, -1.7592e-04],
        [-6.6608e-06, -4.5076e-06,  5.1893e-06,  ..., -5.4538e-06,
         -4.3139e-06, -4.8727e-06],
        [ 1.1279e-04,  1.6232e-04, -1.7000e-06,  ...,  1.1518e-04,
          1.5984e-04,  5.5015e-05],
        [-1.5229e-05, -1.0006e-05,  1.2428e-05,  ..., -1.2383e-05,
         -9.7305e-06, -1.1653e-05],
        [-1.6659e-05, -1.2279e-05,  1.3411e-05,  ..., -1.3903e-05,
         -1.1578e-05, -1.1340e-05]], device='cuda:0')
Loss: 0.9539873600006104


Running epoch 1, step 1581, batch 533
Sampled inputs[:2]: tensor([[    0,   437,   638,  ...,  4514,    14,   333],
        [    0, 18322,   287,  ...,   953,   271,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6957e-04, -1.4371e-05, -1.6997e-04,  ...,  2.5092e-04,
          6.2911e-04, -2.1482e-04],
        [-8.0392e-06, -5.3495e-06,  6.2324e-06,  ..., -6.5118e-06,
         -5.1074e-06, -5.9158e-06],
        [ 1.0850e-04,  1.5959e-04,  1.7719e-06,  ...,  1.1192e-04,
          1.5741e-04,  5.1811e-05],
        [-1.8448e-05, -1.1913e-05,  1.4961e-05,  ..., -1.4856e-05,
         -1.1556e-05, -1.4201e-05],
        [-2.0027e-05, -1.4529e-05,  1.6078e-05,  ..., -1.6525e-05,
         -1.3664e-05, -1.3664e-05]], device='cuda:0')
Loss: 0.9478970170021057


Running epoch 1, step 1582, batch 534
Sampled inputs[:2]: tensor([[    0, 37312,    12,  ...,   278,   795, 40854],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4697e-04,  1.5301e-05, -2.8766e-04,  ...,  2.6938e-04,
          4.8714e-04, -3.2235e-04],
        [-9.3132e-06, -6.2659e-06,  7.2382e-06,  ..., -7.5772e-06,
         -6.0201e-06, -6.8843e-06],
        [ 1.0457e-04,  1.5664e-04,  5.1098e-06,  ...,  1.0867e-04,
          1.5464e-04,  4.8801e-05],
        [-2.1428e-05, -1.3985e-05,  1.7419e-05,  ..., -1.7330e-05,
         -1.3657e-05, -1.6600e-05],
        [-2.3171e-05, -1.7017e-05,  1.8656e-05,  ..., -1.9193e-05,
         -1.6049e-05, -1.5870e-05]], device='cuda:0')
Loss: 0.9689632058143616


Running epoch 1, step 1583, batch 535
Sampled inputs[:2]: tensor([[   0, 2013,   13,  ...,  271,  266,  908],
        [   0,  298,  894,  ...,  266, 2904, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9879e-05, -8.7812e-06, -1.7110e-04,  ..., -3.7096e-06,
          9.2870e-04, -3.6446e-04],
        [-1.0759e-05, -7.1898e-06,  8.1770e-06,  ..., -8.8215e-06,
         -6.9514e-06, -8.0466e-06],
        [ 1.0037e-04,  1.5370e-04,  8.1198e-06,  ...,  1.0507e-04,
          1.5188e-04,  4.5433e-05],
        [-2.4393e-05, -1.5907e-05,  1.9476e-05,  ..., -1.9893e-05,
         -1.5594e-05, -1.9044e-05],
        [-2.6986e-05, -1.9670e-05,  2.1204e-05,  ..., -2.2516e-05,
         -1.8686e-05, -1.8775e-05]], device='cuda:0')
Loss: 0.926278829574585
Graident accumulation at epoch 1, step 1583, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3278e-05,  1.0573e-04, -8.3646e-05,  ..., -1.1468e-05,
          3.7898e-05, -8.4480e-05],
        [ 2.8443e-08, -8.6709e-07,  6.3261e-07,  ..., -1.1352e-06,
          2.5473e-06,  1.9872e-06],
        [ 8.2837e-05,  8.3362e-05, -6.6050e-05,  ...,  7.2048e-05,
          8.4361e-05,  2.8320e-05],
        [-2.7062e-06,  1.5936e-05, -3.6921e-07,  ...,  2.5155e-08,
          8.9950e-06, -5.1491e-06],
        [-2.6093e-05, -1.9039e-05,  1.9723e-05,  ..., -2.1988e-05,
         -1.7898e-05, -1.7570e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4230e-08, 5.6403e-08, 6.6264e-08,  ..., 2.6389e-08, 1.5664e-07,
         3.8681e-08],
        [8.9835e-11, 5.5149e-11, 2.5542e-11,  ..., 6.1011e-11, 2.6113e-11,
         3.4045e-11],
        [4.0969e-09, 2.6147e-09, 1.6172e-09,  ..., 3.3202e-09, 1.5352e-09,
         1.1093e-09],
        [1.0934e-09, 1.1828e-09, 4.1838e-10,  ..., 9.2516e-10, 7.2367e-10,
         3.8932e-10],
        [4.0639e-10, 2.3162e-10, 8.8356e-11,  ..., 2.9910e-10, 9.3072e-11,
         1.1455e-10]], device='cuda:0')
optimizer state dict: 198.0
lr: [2.969694501513574e-06, 2.969694501513574e-06]
scheduler_last_epoch: 198


Running epoch 1, step 1584, batch 536
Sampled inputs[:2]: tensor([[   0,   12,  401,  ...,  504,  565,  590],
        [   0,  593, 1387,  ...,  508, 8222, 1415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3321e-06, -9.7136e-05, -3.5577e-05,  ...,  1.5865e-04,
          1.1419e-04, -9.2387e-05],
        [-1.3188e-06, -8.3819e-07,  1.1176e-06,  ..., -1.0654e-06,
         -7.7486e-07, -9.6858e-07],
        [-3.9637e-06, -2.6524e-06,  3.5912e-06,  ..., -3.1739e-06,
         -2.3544e-06, -2.8610e-06],
        [-3.0398e-06, -1.8924e-06,  2.7418e-06,  ..., -2.4438e-06,
         -1.7807e-06, -2.3246e-06],
        [-3.1143e-06, -2.2054e-06,  2.7418e-06,  ..., -2.5630e-06,
         -2.0266e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9757289886474609


Running epoch 1, step 1585, batch 537
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4659e-06, -1.5183e-04, -2.8773e-04,  ..., -5.1851e-05,
         -9.5297e-05, -4.4969e-04],
        [-2.6152e-06, -1.6466e-06,  2.1234e-06,  ..., -2.1011e-06,
         -1.5870e-06, -1.9446e-06],
        [-7.8678e-06, -5.2005e-06,  6.8843e-06,  ..., -6.2287e-06,
         -4.7833e-06, -5.6773e-06],
        [-6.0350e-06, -3.6657e-06,  5.2154e-06,  ..., -4.8131e-06,
         -3.6359e-06, -4.6343e-06],
        [-6.3181e-06, -4.4107e-06,  5.3793e-06,  ..., -5.1111e-06,
         -4.1872e-06, -4.2021e-06]], device='cuda:0')
Loss: 0.9163770079612732


Running epoch 1, step 1586, batch 538
Sampled inputs[:2]: tensor([[    0,   367,  3399,  ..., 13481,   408,  6944],
        [    0,    14,   747,  ...,  8271,   365,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2817e-05, -1.5941e-04, -3.0893e-04,  ...,  3.6359e-06,
          2.1677e-05, -5.1503e-04],
        [-3.8072e-06, -2.5630e-06,  3.1069e-06,  ..., -3.1590e-06,
         -2.4289e-06, -2.8871e-06],
        [-1.1563e-05, -8.1658e-06,  1.0177e-05,  ..., -9.4771e-06,
         -7.3910e-06, -8.5384e-06],
        [-8.7470e-06, -5.7220e-06,  7.6145e-06,  ..., -7.2420e-06,
         -5.5581e-06, -6.9141e-06],
        [-9.2685e-06, -6.8843e-06,  7.9274e-06,  ..., -7.7635e-06,
         -6.4224e-06, -6.3032e-06]], device='cuda:0')
Loss: 0.9511584043502808


Running epoch 1, step 1587, batch 539
Sampled inputs[:2]: tensor([[   0, 9577, 2789,  ..., 1042, 9086,  623],
        [   0,  292,   46,  ..., 1217,   17,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0106e-04, -9.2107e-05, -1.7772e-04,  ...,  4.8739e-05,
          1.7608e-04, -3.7232e-04],
        [-5.0813e-06, -3.5465e-06,  4.0606e-06,  ..., -4.2245e-06,
         -3.3639e-06, -3.8333e-06],
        [ 2.8526e-04,  4.0260e-04, -2.1818e-04,  ...,  2.0080e-04,
          3.1920e-04,  1.6456e-04],
        [-1.1727e-05, -8.0019e-06,  9.9689e-06,  ..., -9.7454e-06,
         -7.7635e-06, -9.2685e-06],
        [-1.2457e-05, -9.5963e-06,  1.0446e-05,  ..., -1.0520e-05,
         -8.9705e-06, -8.5384e-06]], device='cuda:0')
Loss: 0.9808968901634216


Running epoch 1, step 1588, batch 540
Sampled inputs[:2]: tensor([[   0,  634, 1621,  ...,  688,  586, 8477],
        [   0,  259, 6022,  ..., 1871, 1209, 1241]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8848e-05, -2.1592e-04, -2.7360e-04,  ..., -1.4540e-05,
          8.5965e-05, -4.1582e-04],
        [-6.3926e-06, -4.4703e-06,  5.2527e-06,  ..., -5.2229e-06,
         -4.1425e-06, -4.7348e-06],
        [ 3.4530e-04,  4.6554e-04, -2.7564e-04,  ...,  3.0669e-04,
          3.7671e-04,  2.3520e-04],
        [-1.4856e-05, -1.0177e-05,  1.2949e-05,  ..., -1.2115e-05,
         -9.6112e-06, -1.1548e-05],
        [-1.5572e-05, -1.2055e-05,  1.3366e-05,  ..., -1.2994e-05,
         -1.1057e-05, -1.0565e-05]], device='cuda:0')
Loss: 0.9987338185310364


Running epoch 1, step 1589, batch 541
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,  1235,   368,  ..., 12152,  8498,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6990e-05, -2.3525e-04, -4.0142e-04,  ..., -2.9700e-05,
          3.4998e-05, -3.9561e-04],
        [-7.6741e-06, -5.3644e-06,  6.1207e-06,  ..., -6.3255e-06,
         -5.1036e-06, -5.7779e-06],
        [ 3.4131e-04,  4.6269e-04, -2.7266e-04,  ...,  3.0335e-04,
          3.7382e-04,  2.3209e-04],
        [-1.7837e-05, -1.2159e-05,  1.5065e-05,  ..., -1.4648e-05,
         -1.1817e-05, -1.4037e-05],
        [-1.8895e-05, -1.4499e-05,  1.5765e-05,  ..., -1.5825e-05,
         -1.3635e-05, -1.2964e-05]], device='cuda:0')
Loss: 0.9720628261566162


Running epoch 1, step 1590, batch 542
Sampled inputs[:2]: tensor([[   0, 1943, 1837,  ...,  870,  287,  266],
        [   0,  259,  587,  ...,   14,   71,  462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2075e-05, -3.2274e-04, -4.1273e-04,  ..., -9.3971e-06,
          1.0340e-04, -3.5771e-04],
        [-8.9779e-06, -6.2324e-06,  7.1935e-06,  ..., -7.3612e-06,
         -5.8971e-06, -6.7689e-06],
        [ 3.3723e-04,  4.5985e-04, -2.6907e-04,  ...,  3.0014e-04,
          3.7133e-04,  2.2900e-04],
        [-2.0862e-05, -1.4141e-05,  1.7688e-05,  ..., -1.7032e-05,
         -1.3657e-05, -1.6451e-05],
        [-2.2009e-05, -1.6779e-05,  1.8433e-05,  ..., -1.8343e-05,
         -1.5706e-05, -1.5154e-05]], device='cuda:0')
Loss: 0.9571570158004761


Running epoch 1, step 1591, batch 543
Sampled inputs[:2]: tensor([[    0, 47354,  5923,  ...,   266, 14679,  8137],
        [    0, 40995,  5863,  ...,    13,  9819,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4575e-05, -3.0533e-04, -4.4032e-04,  ..., -9.1780e-06,
         -6.0563e-05, -3.5060e-04],
        [-1.0245e-05, -7.0892e-06,  8.1994e-06,  ..., -8.4117e-06,
         -6.7092e-06, -7.7747e-06],
        [ 3.3332e-04,  4.5705e-04, -2.6569e-04,  ...,  2.9694e-04,
          3.6880e-04,  2.2599e-04],
        [-2.3827e-05, -1.6093e-05,  2.0161e-05,  ..., -1.9491e-05,
         -1.5564e-05, -1.8954e-05],
        [-2.5138e-05, -1.9118e-05,  2.1070e-05,  ..., -2.0951e-05,
         -1.7896e-05, -1.7360e-05]], device='cuda:0')
Loss: 0.9593392610549927
Graident accumulation at epoch 1, step 1591, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0493e-05,  6.4624e-05, -1.1931e-04,  ..., -1.1239e-05,
          2.8052e-05, -1.1109e-04],
        [-9.9886e-07, -1.4893e-06,  1.3893e-06,  ..., -1.8628e-06,
          1.6216e-06,  1.0110e-06],
        [ 1.0789e-04,  1.2073e-04, -8.6014e-05,  ...,  9.4537e-05,
          1.1280e-04,  4.8088e-05],
        [-4.8183e-06,  1.2733e-05,  1.6838e-06,  ..., -1.9264e-06,
          6.5391e-06, -6.5296e-06],
        [-2.5997e-05, -1.9047e-05,  1.9858e-05,  ..., -2.1884e-05,
         -1.7898e-05, -1.7549e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4181e-08, 5.6440e-08, 6.6392e-08,  ..., 2.6363e-08, 1.5648e-07,
         3.8765e-08],
        [8.9850e-11, 5.5144e-11, 2.5584e-11,  ..., 6.1021e-11, 2.6132e-11,
         3.4071e-11],
        [4.2039e-09, 2.8210e-09, 1.6861e-09,  ..., 3.4051e-09, 1.6697e-09,
         1.1593e-09],
        [1.0929e-09, 1.1818e-09, 4.1836e-10,  ..., 9.2461e-10, 7.2319e-10,
         3.8929e-10],
        [4.0661e-10, 2.3176e-10, 8.8712e-11,  ..., 2.9924e-10, 9.3299e-11,
         1.1474e-10]], device='cuda:0')
optimizer state dict: 199.0
lr: [2.882322737981248e-06, 2.882322737981248e-06]
scheduler_last_epoch: 199


Running epoch 1, step 1592, batch 544
Sampled inputs[:2]: tensor([[    0,   266,  2109,  ...,  6730, 11558,   287],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1988e-04, -8.1701e-05, -4.3145e-04,  ..., -9.7646e-05,
         -1.6449e-04, -1.9818e-04],
        [-1.2293e-06, -8.3447e-07,  1.0580e-06,  ..., -1.0058e-06,
         -7.7114e-07, -9.0897e-07],
        [-3.7849e-06, -2.6375e-06,  3.4869e-06,  ..., -3.0100e-06,
         -2.3097e-06, -2.6673e-06],
        [-2.8610e-06, -1.8552e-06,  2.6077e-06,  ..., -2.2948e-06,
         -1.7583e-06, -2.2054e-06],
        [-3.0100e-06, -2.2203e-06,  2.6822e-06,  ..., -2.4438e-06,
         -1.9968e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9591467976570129


Running epoch 1, step 1593, batch 545
Sampled inputs[:2]: tensor([[   0, 7061,  437,  ...,  278, 9500,   18],
        [   0,  409,  394,  ...,  475, 5458,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7025e-05,  7.9274e-05, -2.8405e-04,  ...,  6.5242e-05,
          9.1449e-06, -1.5277e-04],
        [-2.5183e-06, -1.7434e-06,  1.8328e-06,  ..., -2.1458e-06,
         -1.7472e-06, -2.0266e-06],
        [-7.6890e-06, -5.4836e-06,  6.1244e-06,  ..., -6.3628e-06,
         -5.1856e-06, -5.9009e-06],
        [-5.7817e-06, -3.8669e-06,  4.4331e-06,  ..., -4.8876e-06,
         -3.9786e-06, -4.8429e-06],
        [-6.4969e-06, -4.7684e-06,  4.9621e-06,  ..., -5.4538e-06,
         -4.6641e-06, -4.5896e-06]], device='cuda:0')
Loss: 0.9721492528915405


Running epoch 1, step 1594, batch 546
Sampled inputs[:2]: tensor([[    0,  3941,   257,  ...,    50,   699, 13374],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2859e-05,  1.1739e-04, -2.1367e-04,  ...,  2.0680e-06,
          1.0675e-04, -1.9585e-04],
        [-3.7998e-06, -2.6487e-06,  2.8461e-06,  ..., -3.2112e-06,
         -2.6114e-06, -2.9951e-06],
        [-1.1623e-05, -8.3894e-06,  9.4920e-06,  ..., -9.5963e-06,
         -7.8231e-06, -8.8513e-06],
        [-8.6874e-06, -5.8487e-06,  6.8471e-06,  ..., -7.2867e-06,
         -5.9158e-06, -7.1377e-06],
        [-9.6411e-06, -7.1824e-06,  7.5698e-06,  ..., -8.1062e-06,
         -6.9290e-06, -6.7651e-06]], device='cuda:0')
Loss: 0.9591004252433777


Running epoch 1, step 1595, batch 547
Sampled inputs[:2]: tensor([[   0, 5281, 4452,  ...,   14, 3391,   12],
        [   0, 1883, 1090,  ...,  365, 1943,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0866e-04,  1.3936e-04, -3.1661e-04,  ..., -1.0448e-04,
         -3.5698e-05, -2.9876e-04],
        [-5.1260e-06, -3.5614e-06,  3.9861e-06,  ..., -4.2692e-06,
         -3.4459e-06, -3.9451e-06],
        [-1.5676e-05, -1.1370e-05,  1.3202e-05,  ..., -1.2860e-05,
         -1.0446e-05, -1.1772e-05],
        [-1.1787e-05, -7.9498e-06,  9.6485e-06,  ..., -9.7603e-06,
         -7.8827e-06, -9.4920e-06],
        [-1.2815e-05, -9.6560e-06,  1.0416e-05,  ..., -1.0729e-05,
         -9.1493e-06, -8.8811e-06]], device='cuda:0')
Loss: 0.9675809741020203


Running epoch 1, step 1596, batch 548
Sampled inputs[:2]: tensor([[   0,  271,  259,  ..., 4511,   14,  333],
        [   0,  792,  287,  ...,  706, 9751,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2120e-04,  2.2821e-04, -4.1012e-04,  ..., -1.3792e-04,
          2.2273e-04, -1.3597e-04],
        [-6.4820e-06, -4.5225e-06,  5.0217e-06,  ..., -5.3644e-06,
         -4.3251e-06, -4.9062e-06],
        [-1.9759e-05, -1.4439e-05,  1.6630e-05,  ..., -1.6168e-05,
         -1.3128e-05, -1.4663e-05],
        [-1.4797e-05, -1.0051e-05,  1.2092e-05,  ..., -1.2189e-05,
         -9.8497e-06, -1.1727e-05],
        [-1.6078e-05, -1.2174e-05,  1.3039e-05,  ..., -1.3411e-05,
         -1.1459e-05, -1.1012e-05]], device='cuda:0')
Loss: 0.9396724700927734


Running epoch 1, step 1597, batch 549
Sampled inputs[:2]: tensor([[    0,    12,   496,  ...,   437,   266,  3767],
        [    0,    12,   461,  ...,  2525,   278, 23762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3778e-04,  2.7478e-04, -2.9269e-04,  ..., -2.7923e-05,
          2.2273e-04,  9.4408e-06],
        [-7.7784e-06, -5.4687e-06,  6.0797e-06,  ..., -6.4299e-06,
         -5.1446e-06, -5.8338e-06],
        [-2.3842e-05, -1.7613e-05,  2.0206e-05,  ..., -1.9580e-05,
         -1.5795e-05, -1.7643e-05],
        [-1.7792e-05, -1.2226e-05,  1.4685e-05,  ..., -1.4678e-05,
         -1.1757e-05, -1.4022e-05],
        [-1.9237e-05, -1.4737e-05,  1.5721e-05,  ..., -1.6108e-05,
         -1.3694e-05, -1.3143e-05]], device='cuda:0')
Loss: 0.9680083990097046


Running epoch 1, step 1598, batch 550
Sampled inputs[:2]: tensor([[    0,  2042,  2909,  ...,    14, 15061,  5742],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3568e-04,  2.5952e-04, -2.8710e-04,  ..., -5.2802e-05,
          6.3945e-04,  2.0299e-04],
        [-9.0972e-06, -6.5193e-06,  7.1079e-06,  ..., -7.5251e-06,
         -6.1281e-06, -6.7838e-06],
        [-2.8044e-05, -2.1115e-05,  2.3663e-05,  ..., -2.3127e-05,
         -1.8969e-05, -2.0728e-05],
        [-2.0906e-05, -1.4670e-05,  1.7218e-05,  ..., -1.7270e-05,
         -1.4067e-05, -1.6406e-05],
        [-2.2605e-05, -1.7643e-05,  1.8418e-05,  ..., -1.8984e-05,
         -1.6376e-05, -1.5453e-05]], device='cuda:0')
Loss: 1.009523868560791


Running epoch 1, step 1599, batch 551
Sampled inputs[:2]: tensor([[    0,   298, 11712,  ...,   221,   273,   298],
        [    0,  4323,  8213,  ...,  1153,   278,  4258]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3170e-04,  2.9442e-04, -5.6114e-04,  ..., -1.4870e-04,
          6.6280e-04,  3.5935e-04],
        [-1.0669e-05, -7.2867e-06,  7.9982e-06,  ..., -8.8066e-06,
         -6.9849e-06, -8.0727e-06],
        [-3.2574e-05, -2.3529e-05,  2.6479e-05,  ..., -2.6822e-05,
         -2.1517e-05, -2.4453e-05],
        [-2.3976e-05, -1.6160e-05,  1.9066e-05,  ..., -1.9789e-05,
         -1.5758e-05, -1.8969e-05],
        [-2.6926e-05, -1.9908e-05,  2.0891e-05,  ..., -2.2575e-05,
         -1.8939e-05, -1.8835e-05]], device='cuda:0')
Loss: 0.8688417673110962
Graident accumulation at epoch 1, step 1599, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2739e-06,  8.7604e-05, -1.6350e-04,  ..., -2.4985e-05,
          9.1527e-05, -6.4048e-05],
        [-1.9659e-06, -2.0690e-06,  2.0502e-06,  ..., -2.5572e-06,
          7.6097e-07,  1.0264e-07],
        [ 9.3839e-05,  1.0630e-04, -7.4765e-05,  ...,  8.2402e-05,
          9.9372e-05,  4.0833e-05],
        [-6.7341e-06,  9.8440e-06,  3.4221e-06,  ..., -3.7127e-06,
          4.3094e-06, -7.7736e-06],
        [-2.6090e-05, -1.9133e-05,  1.9961e-05,  ..., -2.1953e-05,
         -1.8002e-05, -1.7677e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4237e-08, 5.6470e-08, 6.6640e-08,  ..., 2.6358e-08, 1.5677e-07,
         3.8855e-08],
        [8.9874e-11, 5.5142e-11, 2.5622e-11,  ..., 6.1038e-11, 2.6155e-11,
         3.4102e-11],
        [4.2007e-09, 2.8187e-09, 1.6852e-09,  ..., 3.4024e-09, 1.6685e-09,
         1.1587e-09],
        [1.0924e-09, 1.1809e-09, 4.1831e-10,  ..., 9.2408e-10, 7.2271e-10,
         3.8926e-10],
        [4.0693e-10, 2.3192e-10, 8.9060e-11,  ..., 2.9945e-10, 9.3565e-11,
         1.1498e-10]], device='cuda:0')
optimizer state dict: 200.0
lr: [2.796038617665642e-06, 2.796038617665642e-06]
scheduler_last_epoch: 200


Running epoch 1, step 1600, batch 552
Sampled inputs[:2]: tensor([[   0, 2314,  266,  ...,  342, 7299, 1099],
        [   0, 4834,  278,  ...,   13, 8382,  669]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7977e-05, -1.6623e-06, -7.0590e-05,  ...,  3.3447e-05,
         -1.3258e-04,  2.2033e-04],
        [-1.4082e-06, -9.9838e-07,  1.0729e-06,  ..., -1.1176e-06,
         -9.5367e-07, -9.6858e-07],
        [ 1.3120e-04,  9.5111e-05, -1.2472e-04,  ...,  1.2404e-04,
          1.0278e-04,  7.5730e-05],
        [-3.0696e-06, -2.1607e-06,  2.4736e-06,  ..., -2.4587e-06,
         -2.1160e-06, -2.2352e-06],
        [-3.4124e-06, -2.6971e-06,  2.7120e-06,  ..., -2.8014e-06,
         -2.5630e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9607068300247192


Running epoch 1, step 1601, batch 553
Sampled inputs[:2]: tensor([[    0,  2771,  2070,  ...,   221,   396,   298],
        [    0,  2715, 10929,  ...,  4978,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7092e-04, -4.0009e-06, -9.6770e-05,  ...,  3.9132e-05,
         -1.0945e-04,  3.4903e-04],
        [-2.7567e-06, -1.9446e-06,  2.0191e-06,  ..., -2.2501e-06,
         -1.9968e-06, -2.0117e-06],
        [ 1.2718e-04,  9.2116e-05, -1.2161e-04,  ...,  1.2065e-04,
          9.9640e-05,  7.2631e-05],
        [ 7.8027e-05,  6.5612e-05, -2.8549e-05,  ...,  6.1583e-05,
          5.0754e-05,  6.6420e-05],
        [-6.7502e-06, -5.2750e-06,  5.2154e-06,  ..., -5.6624e-06,
         -5.3346e-06, -4.6194e-06]], device='cuda:0')
Loss: 0.939241886138916


Running epoch 1, step 1602, batch 554
Sampled inputs[:2]: tensor([[    0,     9, 25368,  ...,   271,   266,  1136],
        [    0,   300,  5631,  ...,  2278,  2669,  3011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1768e-04, -1.1944e-04, -1.2118e-04,  ...,  2.1605e-05,
          3.8518e-05,  1.9198e-04],
        [-4.0829e-06, -2.7828e-06,  3.0547e-06,  ..., -3.3304e-06,
         -2.8349e-06, -3.0324e-06],
        [ 1.2321e-04,  8.9523e-05, -1.1822e-04,  ...,  1.1751e-04,
          9.7167e-05,  6.9636e-05],
        [ 7.4957e-05,  6.3734e-05, -2.6016e-05,  ...,  5.9109e-05,
          4.8831e-05,  6.3946e-05],
        [-9.9987e-06, -7.5102e-06,  7.8976e-06,  ..., -8.3297e-06,
         -7.5698e-06, -6.8843e-06]], device='cuda:0')
Loss: 0.9461522698402405


Running epoch 1, step 1603, batch 555
Sampled inputs[:2]: tensor([[    0, 38136,    12,  ...,   367, 12851,  1040],
        [    0,   266, 20604,  ...,   409, 13764,  6048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8368e-04, -1.0435e-04, -3.9071e-04,  ...,  8.2033e-05,
          9.8162e-05,  3.7063e-04],
        [-5.4389e-06, -3.7588e-06,  4.1425e-06,  ..., -4.3884e-06,
         -3.6806e-06, -3.9414e-06],
        [ 1.1919e-04,  8.6439e-05, -1.1474e-04,  ...,  1.1432e-04,
          9.4574e-05,  6.6894e-05],
        [ 7.1902e-05,  6.1544e-05, -2.3423e-05,  ...,  5.6710e-05,
          4.6894e-05,  6.1771e-05],
        [-1.3202e-05, -1.0088e-05,  1.0625e-05,  ..., -1.0937e-05,
         -9.8199e-06, -8.9109e-06]], device='cuda:0')
Loss: 0.9913871884346008


Running epoch 1, step 1604, batch 556
Sampled inputs[:2]: tensor([[    0,   591,  1545,  ...,    71,   462,   221],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6365e-04, -1.2441e-04, -5.1759e-04,  ...,  1.2879e-04,
          5.4232e-05,  4.3753e-04],
        [-6.7428e-06, -4.6417e-06,  5.1484e-06,  ..., -5.4389e-06,
         -4.5151e-06, -4.9248e-06],
        [ 1.1516e-04,  8.3623e-05, -1.1142e-04,  ...,  1.1113e-04,
          9.2071e-05,  6.3943e-05],
        [ 6.8877e-05,  5.9562e-05, -2.1009e-05,  ...,  5.4281e-05,
          4.4972e-05,  5.9386e-05],
        [-1.6391e-05, -1.2428e-05,  1.3188e-05,  ..., -1.3515e-05,
         -1.1966e-05, -1.1072e-05]], device='cuda:0')
Loss: 0.9493751525878906


Running epoch 1, step 1605, batch 557
Sampled inputs[:2]: tensor([[    0,   221,   709,  ...,  3365,  3504,   278],
        [    0,  1188,    12,  ...,   292, 23032,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0636e-04, -7.2528e-05, -4.7294e-04,  ...,  1.8875e-04,
         -1.3843e-05,  3.3406e-04],
        [-8.0466e-06, -5.5358e-06,  6.2808e-06,  ..., -6.5118e-06,
         -5.3644e-06, -5.8375e-06],
        [ 1.1117e-04,  8.0776e-05, -1.0776e-04,  ...,  1.0787e-04,
          8.9478e-05,  6.1157e-05],
        [ 6.5912e-05,  5.7595e-05, -1.8312e-05,  ...,  5.1867e-05,
          4.3072e-05,  5.7196e-05],
        [-1.9506e-05, -1.4782e-05,  1.5974e-05,  ..., -1.6138e-05,
         -1.4156e-05, -1.3083e-05]], device='cuda:0')
Loss: 0.987189769744873


Running epoch 1, step 1606, batch 558
Sampled inputs[:2]: tensor([[   0,  790, 2816,  ...,   14, 1062,  668],
        [   0,  292,  380,  ..., 9636,  417,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3527e-04, -5.0599e-05, -6.5618e-04,  ...,  1.5873e-04,
         -2.1083e-04,  3.1272e-04],
        [-9.3579e-06, -6.4075e-06,  7.3612e-06,  ..., -7.6145e-06,
         -6.2175e-06, -6.7689e-06],
        [ 1.0721e-04,  7.8005e-05, -1.0428e-04,  ...,  1.0456e-04,
          8.6915e-05,  5.8415e-05],
        [ 6.2932e-05,  5.5673e-05, -1.5749e-05,  ...,  4.9364e-05,
          4.1135e-05,  5.4976e-05],
        [-2.2680e-05, -1.7121e-05,  1.8671e-05,  ..., -1.8820e-05,
         -1.6347e-05, -1.5080e-05]], device='cuda:0')
Loss: 0.9907169342041016


Running epoch 1, step 1607, batch 559
Sampled inputs[:2]: tensor([[   0,  266, 1790,  ...,  292,   78,  527],
        [   0, 2736, 2523,  ..., 4086, 4798, 7701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8508e-04, -2.0875e-04, -6.4550e-04,  ...,  2.3226e-04,
          5.2333e-05,  3.3947e-04],
        [-1.0662e-05, -7.3500e-06,  8.4043e-06,  ..., -8.7097e-06,
         -7.1786e-06, -7.7821e-06],
        [ 1.0318e-04,  7.4950e-05, -1.0082e-04,  ...,  1.0119e-04,
          8.3964e-05,  5.5286e-05],
        [ 1.9160e-04,  1.7420e-04, -2.2011e-04,  ...,  2.6245e-04,
          2.1542e-04,  1.8529e-04],
        [-2.5973e-05, -1.9744e-05,  2.1413e-05,  ..., -2.1651e-05,
         -1.8924e-05, -1.7449e-05]], device='cuda:0')
Loss: 1.0112930536270142
Graident accumulation at epoch 1, step 1607, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1454e-05,  5.7969e-05, -2.1170e-04,  ...,  7.4013e-07,
          8.7607e-05, -2.3696e-05],
        [-2.8355e-06, -2.5971e-06,  2.6856e-06,  ..., -3.1725e-06,
         -3.2992e-08, -6.8583e-07],
        [ 9.4774e-05,  1.0317e-04, -7.7370e-05,  ...,  8.4281e-05,
          9.7831e-05,  4.2279e-05],
        [ 1.3099e-05,  2.6280e-05, -1.8931e-05,  ...,  2.2904e-05,
          2.5421e-05,  1.1533e-05],
        [-2.6078e-05, -1.9194e-05,  2.0107e-05,  ..., -2.1923e-05,
         -1.8094e-05, -1.7655e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4264e-08, 5.6457e-08, 6.6991e-08,  ..., 2.6386e-08, 1.5661e-07,
         3.8932e-08],
        [8.9898e-11, 5.5141e-11, 2.5667e-11,  ..., 6.1052e-11, 2.6180e-11,
         3.4129e-11],
        [4.2072e-09, 2.8215e-09, 1.6936e-09,  ..., 3.4092e-09, 1.6738e-09,
         1.1606e-09],
        [1.1280e-09, 1.2101e-09, 4.6634e-10,  ..., 9.9204e-10, 7.6840e-10,
         4.2321e-10],
        [4.0720e-10, 2.3208e-10, 8.9429e-11,  ..., 2.9962e-10, 9.3829e-11,
         1.1516e-10]], device='cuda:0')
optimizer state dict: 201.0
lr: [2.7108553255335225e-06, 2.7108553255335225e-06]
scheduler_last_epoch: 201


Running epoch 1, step 1608, batch 560
Sampled inputs[:2]: tensor([[    0,    52, 26766,  ...,  4411,  4226,   278],
        [    0,  1921,   843,  ...,  9420,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5398e-04, -2.0143e-05,  1.0328e-04,  ...,  1.4621e-04,
         -3.1659e-05,  1.6396e-06],
        [-1.2815e-06, -9.3505e-07,  1.1101e-06,  ..., -1.0580e-06,
         -8.4937e-07, -9.0152e-07],
        [-4.1127e-06, -3.1441e-06,  3.7700e-06,  ..., -3.4273e-06,
         -2.7418e-06, -2.9504e-06],
        [-2.9653e-06, -2.1458e-06,  2.6822e-06,  ..., -2.4587e-06,
         -1.9670e-06, -2.2054e-06],
        [-3.1292e-06, -2.4885e-06,  2.8014e-06,  ..., -2.6524e-06,
         -2.2203e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9755655527114868


Running epoch 1, step 1609, batch 561
Sampled inputs[:2]: tensor([[    0,   968,   266,  ...,   287,  2143, 15228],
        [    0,  1371,   287,  ...,   689,   278, 12774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8506e-04, -5.6058e-05,  5.0974e-07,  ...,  4.1821e-06,
         -1.6038e-04, -1.1110e-04],
        [-2.6226e-06, -1.8366e-06,  2.1532e-06,  ..., -2.1234e-06,
         -1.7695e-06, -1.8775e-06],
        [-8.1658e-06, -5.9903e-06,  7.1675e-06,  ..., -6.6012e-06,
         -5.4687e-06, -5.8860e-06],
        [-5.9754e-06, -4.0978e-06,  5.1260e-06,  ..., -4.8280e-06,
         -4.0084e-06, -4.5002e-06],
        [-6.5416e-06, -5.0068e-06,  5.5581e-06,  ..., -5.3942e-06,
         -4.6939e-06, -4.3511e-06]], device='cuda:0')
Loss: 0.9494520425796509


Running epoch 1, step 1610, batch 562
Sampled inputs[:2]: tensor([[   0, 4154,   12,  ...,   14,  560,  199],
        [   0,  266, 2086,  ..., 4283,  720,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4535e-04, -9.8061e-05, -2.3861e-05,  ..., -3.5048e-05,
         -5.4642e-06, -1.8399e-04],
        [-3.9190e-06, -2.7977e-06,  3.2261e-06,  ..., -3.2037e-06,
         -2.6673e-06, -2.8461e-06],
        [-1.2100e-05, -9.0003e-06,  1.0654e-05,  ..., -9.8646e-06,
         -8.2105e-06, -8.8215e-06],
        [-8.8662e-06, -6.1989e-06,  7.6443e-06,  ..., -7.2271e-06,
         -6.0052e-06, -6.7800e-06],
        [-9.7603e-06, -7.5847e-06,  8.3447e-06,  ..., -8.1211e-06,
         -7.1079e-06, -6.5714e-06]], device='cuda:0')
Loss: 0.9920859336853027


Running epoch 1, step 1611, batch 563
Sampled inputs[:2]: tensor([[   0,   40,  568,  ..., 3750,  300, 3421],
        [   0, 4998, 1921,  ...,  968,  266, 1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5387e-04, -1.6258e-04, -1.1020e-04,  ..., -3.5851e-05,
          8.0886e-06, -2.0256e-04],
        [-5.1335e-06, -3.6694e-06,  4.2096e-06,  ..., -4.2692e-06,
         -3.5502e-06, -3.8072e-06],
        [-1.5855e-05, -1.1683e-05,  1.3947e-05,  ..., -1.3039e-05,
         -1.0863e-05, -1.1608e-05],
        [-1.1623e-05, -8.0988e-06,  9.9838e-06,  ..., -9.6262e-06,
         -8.0019e-06, -9.0450e-06],
        [-1.2785e-05, -9.8348e-06,  1.0952e-05,  ..., -1.0729e-05,
         -9.3728e-06, -8.6129e-06]], device='cuda:0')
Loss: 0.9487738013267517


Running epoch 1, step 1612, batch 564
Sampled inputs[:2]: tensor([[   0,  328,  266,  ...,   14, 3352,  266],
        [   0, 2029,   13,  ...,   12, 4536,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5698e-04, -1.5215e-04, -1.5079e-04,  ...,  2.1706e-05,
          2.4031e-04, -1.9722e-04],
        [-6.4448e-06, -4.5374e-06,  5.2527e-06,  ..., -5.3495e-06,
         -4.3772e-06, -4.7088e-06],
        [-1.9729e-05, -1.4424e-05,  1.7285e-05,  ..., -1.6227e-05,
         -1.3351e-05, -1.4246e-05],
        [-1.4603e-05, -1.0051e-05,  1.2502e-05,  ..., -1.2100e-05,
         -9.9018e-06, -1.1221e-05],
        [-1.5914e-05, -1.2144e-05,  1.3590e-05,  ..., -1.3366e-05,
         -1.1533e-05, -1.0595e-05]], device='cuda:0')
Loss: 0.9830045700073242


Running epoch 1, step 1613, batch 565
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8419e-04, -1.7281e-04, -2.3606e-04,  ...,  2.2353e-04,
          1.9945e-06, -9.4071e-05],
        [-7.7859e-06, -5.3458e-06,  6.2734e-06,  ..., -6.4820e-06,
         -5.2117e-06, -5.7966e-06],
        [-2.3633e-05, -1.6913e-05,  2.0519e-05,  ..., -1.9461e-05,
         -1.5795e-05, -1.7315e-05],
        [-1.7479e-05, -1.1757e-05,  1.4812e-05,  ..., -1.4544e-05,
         -1.1720e-05, -1.3664e-05],
        [-1.9237e-05, -1.4335e-05,  1.6272e-05,  ..., -1.6168e-05,
         -1.3739e-05, -1.3009e-05]], device='cuda:0')
Loss: 0.931422233581543


Running epoch 1, step 1614, batch 566
Sampled inputs[:2]: tensor([[   0, 6762,  689,  ..., 7061,   14,  381],
        [   0, 2698,  221,  ..., 8352, 5680,  782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5623e-05, -1.8903e-04, -1.0691e-04,  ...,  2.7515e-04,
          1.7921e-04, -7.3407e-06],
        [-9.1419e-06, -6.1467e-06,  7.3314e-06,  ..., -7.5921e-06,
         -6.0536e-06, -6.8694e-06],
        [-2.7627e-05, -1.9401e-05,  2.3916e-05,  ..., -2.2680e-05,
         -1.8284e-05, -2.0385e-05],
        [-2.0474e-05, -1.3478e-05,  1.7256e-05,  ..., -1.7002e-05,
         -1.3590e-05, -1.6138e-05],
        [-2.2620e-05, -1.6510e-05,  1.9044e-05,  ..., -1.8939e-05,
         -1.5989e-05, -1.5408e-05]], device='cuda:0')
Loss: 0.9457290768623352


Running epoch 1, step 1615, batch 567
Sampled inputs[:2]: tensor([[    0,   365,  1941,  ..., 38029,  1790, 44066],
        [    0,   344, 14017,  ...,    65,   298,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8480e-04, -1.9180e-04, -1.0066e-05,  ...,  2.1920e-04,
          4.9272e-04,  7.0786e-06],
        [-1.0423e-05, -7.0594e-06,  8.3596e-06,  ..., -8.6725e-06,
         -6.9551e-06, -7.7970e-06],
        [-3.1427e-05, -2.2218e-05,  2.7224e-05,  ..., -2.5854e-05,
         -2.0891e-05, -2.3082e-05],
        [-2.3305e-05, -1.5430e-05,  1.9640e-05,  ..., -1.9386e-05,
         -1.5557e-05, -1.8314e-05],
        [-2.5824e-05, -1.8999e-05,  2.1741e-05,  ..., -2.1666e-05,
         -1.8358e-05, -1.7494e-05]], device='cuda:0')
Loss: 0.9742113351821899
Graident accumulation at epoch 1, step 1615, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6789e-05,  3.2992e-05, -1.9153e-04,  ...,  2.2586e-05,
          1.2812e-04, -2.0619e-05],
        [-3.5943e-06, -3.0434e-06,  3.2530e-06,  ..., -3.7225e-06,
         -7.2520e-07, -1.3970e-06],
        [ 8.2154e-05,  9.0630e-05, -6.6911e-05,  ...,  7.3267e-05,
          8.5959e-05,  3.5743e-05],
        [ 9.4585e-06,  2.2109e-05, -1.5074e-05,  ...,  1.8675e-05,
          2.1323e-05,  8.5486e-06],
        [-2.6053e-05, -1.9175e-05,  2.0270e-05,  ..., -2.1897e-05,
         -1.8121e-05, -1.7639e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4244e-08, 5.6437e-08, 6.6924e-08,  ..., 2.6408e-08, 1.5670e-07,
         3.8893e-08],
        [8.9917e-11, 5.5135e-11, 2.5712e-11,  ..., 6.1067e-11, 2.6202e-11,
         3.4156e-11],
        [4.2039e-09, 2.8192e-09, 1.6927e-09,  ..., 3.4065e-09, 1.6726e-09,
         1.1600e-09],
        [1.1274e-09, 1.2091e-09, 4.6626e-10,  ..., 9.9142e-10, 7.6787e-10,
         4.2312e-10],
        [4.0746e-10, 2.3221e-10, 8.9812e-11,  ..., 2.9979e-10, 9.4073e-11,
         1.1536e-10]], device='cuda:0')
optimizer state dict: 202.0
lr: [2.626785878335505e-06, 2.626785878335505e-06]
scheduler_last_epoch: 202


Running epoch 1, step 1616, batch 568
Sampled inputs[:2]: tensor([[    0,   432,   984,  ...,   287,   496,    14],
        [    0,    15, 14761,  ...,   278,  3218,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0131e-04, -1.3616e-04, -2.2649e-04,  ...,  1.0191e-04,
          3.6459e-04, -2.1404e-04],
        [-1.4082e-06, -9.6112e-07,  8.8289e-07,  ..., -1.2666e-06,
         -1.0505e-06, -1.1250e-06],
        [-4.0531e-06, -2.9057e-06,  2.8014e-06,  ..., -3.5763e-06,
         -2.9802e-06, -3.2187e-06],
        [-3.0100e-06, -2.0117e-06,  1.9372e-06,  ..., -2.7120e-06,
         -2.2650e-06, -2.5332e-06],
        [-3.7104e-06, -2.7567e-06,  2.4736e-06,  ..., -3.3528e-06,
         -2.9057e-06, -2.7418e-06]], device='cuda:0')
Loss: 0.9913226366043091


Running epoch 1, step 1617, batch 569
Sampled inputs[:2]: tensor([[   0, 2728, 3139,  ..., 2254,  221,  380],
        [   0,   18,   14,  ...,  380,  981,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6178e-04, -6.8173e-05, -2.4173e-04,  ...,  1.6171e-04,
          3.9926e-04, -1.8020e-04],
        [-2.8461e-06, -1.8850e-06,  1.8030e-06,  ..., -2.4810e-06,
         -2.0191e-06, -2.2128e-06],
        [-8.2850e-06, -5.7817e-06,  5.7817e-06,  ..., -7.1079e-06,
         -5.8860e-06, -6.3628e-06],
        [-6.0946e-06, -3.9637e-06,  4.0084e-06,  ..., -5.3346e-06,
         -4.3958e-06, -4.9919e-06],
        [-7.3612e-06, -5.3495e-06,  4.9919e-06,  ..., -6.4671e-06,
         -5.6028e-06, -5.2601e-06]], device='cuda:0')
Loss: 0.9507101774215698


Running epoch 1, step 1618, batch 570
Sampled inputs[:2]: tensor([[    0,  3440,  5745,  ...,   360,  4998,   654],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2499e-04, -2.2102e-04, -1.1903e-04,  ...,  3.9956e-04,
          4.5473e-04, -1.6474e-04],
        [-4.0606e-06, -2.8200e-06,  2.7381e-06,  ..., -3.5465e-06,
         -2.8647e-06, -3.2112e-06],
        [-1.2189e-05, -8.8662e-06,  9.0301e-06,  ..., -1.0461e-05,
         -8.5384e-06, -9.4920e-06],
        [-8.9109e-06, -6.0946e-06,  6.2734e-06,  ..., -7.8082e-06,
         -6.3777e-06, -7.4208e-06],
        [-1.0476e-05, -7.9125e-06,  7.5102e-06,  ..., -9.2089e-06,
         -7.8976e-06, -7.5698e-06]], device='cuda:0')
Loss: 0.9524121880531311


Running epoch 1, step 1619, batch 571
Sampled inputs[:2]: tensor([[   0,  300, 4402,  ..., 2013,   13, 6825],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2037e-04, -2.3074e-04, -2.1702e-04,  ...,  6.0561e-04,
          6.0452e-04, -2.2619e-04],
        [-5.3421e-06, -3.6769e-06,  3.7216e-06,  ..., -4.6566e-06,
         -3.7141e-06, -4.1611e-06],
        [-1.6093e-05, -1.1608e-05,  1.2249e-05,  ..., -1.3784e-05,
         -1.1101e-05, -1.2338e-05],
        [-1.1846e-05, -7.9945e-06,  8.6129e-06,  ..., -1.0356e-05,
         -8.3297e-06, -9.7454e-06],
        [-1.3635e-05, -1.0222e-05,  1.0043e-05,  ..., -1.1936e-05,
         -1.0103e-05, -9.6858e-06]], device='cuda:0')
Loss: 0.9819290637969971


Running epoch 1, step 1620, batch 572
Sampled inputs[:2]: tensor([[   0,  278, 5210,  ..., 1968, 2002,  923],
        [   0,  409,  699,  ...,   12,  546,  696]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6140e-04, -1.8761e-04, -2.5181e-04,  ...,  4.9869e-04,
          6.1053e-04, -3.1323e-04],
        [-6.6310e-06, -4.5598e-06,  4.7348e-06,  ..., -5.7295e-06,
         -4.5896e-06, -5.1148e-06],
        [-1.9968e-05, -1.4380e-05,  1.5557e-05,  ..., -1.6958e-05,
         -1.3694e-05, -1.5140e-05],
        [-1.4782e-05, -9.9316e-06,  1.1027e-05,  ..., -1.2785e-05,
         -1.0297e-05, -1.2025e-05],
        [-1.6823e-05, -1.2636e-05,  1.2696e-05,  ..., -1.4618e-05,
         -1.2428e-05, -1.1802e-05]], device='cuda:0')
Loss: 0.9581969380378723


Running epoch 1, step 1621, batch 573
Sampled inputs[:2]: tensor([[   0, 1871,  518,  ...,  271,  259, 1110],
        [   0,  300, 7239,  ..., 2283, 4890,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0335e-04, -1.5117e-04, -3.3581e-04,  ...,  4.5140e-04,
          5.6934e-04, -2.2097e-04],
        [-7.9125e-06, -5.4017e-06,  5.8077e-06,  ..., -6.7204e-06,
         -5.4315e-06, -6.0238e-06],
        [-2.3901e-05, -1.7092e-05,  1.9103e-05,  ..., -1.9968e-05,
         -1.6257e-05, -1.7866e-05],
        [-1.7762e-05, -1.1824e-05,  1.3664e-05,  ..., -1.5065e-05,
         -1.2249e-05, -1.4246e-05],
        [-1.9982e-05, -1.4931e-05,  1.5482e-05,  ..., -1.7107e-05,
         -1.4663e-05, -1.3828e-05]], device='cuda:0')
Loss: 0.9437326192855835


Running epoch 1, step 1622, batch 574
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   908,  1086,    12],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4528e-04, -1.5444e-04, -4.4731e-04,  ...,  4.7271e-04,
          4.0390e-04, -1.6670e-04],
        [-9.2313e-06, -6.3851e-06,  6.9700e-06,  ..., -7.8157e-06,
         -6.3293e-06, -6.9588e-06],
        [-2.7984e-05, -2.0236e-05,  2.2858e-05,  ..., -2.3365e-05,
         -1.9059e-05, -2.0802e-05],
        [-2.0832e-05, -1.4044e-05,  1.6496e-05,  ..., -1.7613e-05,
         -1.4320e-05, -1.6540e-05],
        [-2.3231e-05, -1.7568e-05,  1.8388e-05,  ..., -1.9878e-05,
         -1.7062e-05, -1.6004e-05]], device='cuda:0')
Loss: 0.9782204031944275


Running epoch 1, step 1623, batch 575
Sampled inputs[:2]: tensor([[    0,  7638,   720,  ...,  3059, 10777,   292],
        [    0,   328,   266,  ...,   271,   706,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4528e-04, -1.7354e-04, -6.5964e-04,  ...,  5.1026e-04,
          5.7855e-04, -2.4585e-04],
        [-1.0625e-05, -7.2941e-06,  7.8492e-06,  ..., -8.9854e-06,
         -7.3053e-06, -8.0839e-06],
        [-3.2216e-05, -2.3142e-05,  2.5824e-05,  ..., -2.6822e-05,
         -2.1964e-05, -2.4110e-05],
        [-2.3887e-05, -1.5981e-05,  1.8477e-05,  ..., -2.0191e-05,
         -1.6496e-05, -1.9133e-05],
        [-2.6807e-05, -2.0072e-05,  2.0817e-05,  ..., -2.2843e-05,
         -1.9640e-05, -1.8567e-05]], device='cuda:0')
Loss: 0.9492502808570862
Graident accumulation at epoch 1, step 1623, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.6638e-05,  1.2339e-05, -2.3834e-04,  ...,  7.1354e-05,
          1.7316e-04, -4.3142e-05],
        [-4.2973e-06, -3.4684e-06,  3.7126e-06,  ..., -4.2488e-06,
         -1.3832e-06, -2.0656e-06],
        [ 7.0717e-05,  7.9253e-05, -5.7637e-05,  ...,  6.3258e-05,
          7.5167e-05,  2.9757e-05],
        [ 6.1240e-06,  1.8300e-05, -1.1719e-05,  ...,  1.4788e-05,
          1.7541e-05,  5.7804e-06],
        [-2.6128e-05, -1.9264e-05,  2.0325e-05,  ..., -2.1992e-05,
         -1.8273e-05, -1.7731e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4250e-08, 5.6411e-08, 6.7292e-08,  ..., 2.6642e-08, 1.5688e-07,
         3.8914e-08],
        [8.9940e-11, 5.5133e-11, 2.5748e-11,  ..., 6.1086e-11, 2.6230e-11,
         3.4187e-11],
        [4.2008e-09, 2.8169e-09, 1.6917e-09,  ..., 3.4038e-09, 1.6714e-09,
         1.1594e-09],
        [1.1268e-09, 1.2081e-09, 4.6613e-10,  ..., 9.9084e-10, 7.6738e-10,
         4.2306e-10],
        [4.0777e-10, 2.3238e-10, 9.0156e-11,  ..., 3.0001e-10, 9.4364e-11,
         1.1559e-10]], device='cuda:0')
optimizer state dict: 203.0
lr: [2.5438431226169712e-06, 2.5438431226169712e-06]
scheduler_last_epoch: 203


Running epoch 1, step 1624, batch 576
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0,  292,   58,  ...,  319,  221, 1061]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6514e-04,  1.0646e-04, -3.4046e-05,  ...,  1.0348e-04,
         -3.5628e-04, -3.0564e-04],
        [-1.3709e-06, -8.8662e-07,  1.0282e-06,  ..., -1.1325e-06,
         -8.7172e-07, -1.0133e-06],
        [-4.2617e-06, -2.9057e-06,  3.4422e-06,  ..., -3.4869e-06,
         -2.7418e-06, -3.1143e-06],
        [-3.1143e-06, -1.9670e-06,  2.4438e-06,  ..., -2.5779e-06,
         -1.9968e-06, -2.4140e-06],
        [-3.3826e-06, -2.4289e-06,  2.6524e-06,  ..., -2.8312e-06,
         -2.3395e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9486333131790161


Running epoch 1, step 1625, batch 577
Sampled inputs[:2]: tensor([[    0,  1477,  5648,  ...,  4391,  1722,   369],
        [    0,  1254,  1773,  ..., 19459,  2447,  2613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1443e-04,  8.0638e-05, -1.1911e-04,  ...,  1.6957e-04,
         -4.4222e-04, -2.5182e-04],
        [-2.7344e-06, -1.8999e-06,  2.1085e-06,  ..., -2.2501e-06,
         -1.8179e-06, -1.9670e-06],
        [-8.5831e-06, -6.3032e-06,  7.0781e-06,  ..., -7.0632e-06,
         -5.7817e-06, -6.2138e-06],
        [-6.2436e-06, -4.2617e-06,  5.0366e-06,  ..., -5.1558e-06,
         -4.1723e-06, -4.7386e-06],
        [-6.7055e-06, -5.1409e-06,  5.3793e-06,  ..., -5.6475e-06,
         -4.8429e-06, -4.5300e-06]], device='cuda:0')
Loss: 0.9862227439880371


Running epoch 1, step 1626, batch 578
Sampled inputs[:2]: tensor([[    0,   677, 35427,  ..., 30465,  2783,     9],
        [    0,   344, 10706,  ...,  1184,   578,   825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2742e-04,  7.0857e-05, -1.2747e-04,  ...,  1.0933e-04,
         -5.8910e-04, -1.9912e-04],
        [-4.0531e-06, -2.8051e-06,  3.2037e-06,  ..., -3.3006e-06,
         -2.6710e-06, -2.8759e-06],
        [-1.2547e-05, -9.1791e-06,  1.0595e-05,  ..., -1.0237e-05,
         -8.3894e-06, -8.9556e-06],
        [-9.2536e-06, -6.3032e-06,  7.6741e-06,  ..., -7.5698e-06,
         -6.1393e-06, -6.9439e-06],
        [-9.8646e-06, -7.5251e-06,  8.0913e-06,  ..., -8.2403e-06,
         -7.0781e-06, -6.5565e-06]], device='cuda:0')
Loss: 0.96800696849823


Running epoch 1, step 1627, batch 579
Sampled inputs[:2]: tensor([[    0,  3059,  2013,  ...,   278,  1997,    14],
        [    0, 15931,    14,  ...,  2645,   699,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1627e-04,  2.9580e-04, -1.4581e-04,  ...,  1.5865e-04,
         -7.1025e-04, -4.2849e-04],
        [-5.3793e-06, -3.5614e-06,  4.1462e-06,  ..., -4.4182e-06,
         -3.5502e-06, -3.9712e-06],
        [-1.6540e-05, -1.1608e-05,  1.3724e-05,  ..., -1.3545e-05,
         -1.1027e-05, -1.2174e-05],
        [-1.2159e-05, -7.9125e-06,  9.8646e-06,  ..., -1.0028e-05,
         -8.0764e-06, -9.4324e-06],
        [-1.3113e-05, -9.5665e-06,  1.0535e-05,  ..., -1.0997e-05,
         -9.3579e-06, -8.9854e-06]], device='cuda:0')
Loss: 0.9411606788635254


Running epoch 1, step 1628, batch 580
Sampled inputs[:2]: tensor([[   0, 2645,   12,  ...,    5, 1239, 7200],
        [   0, 6978, 2285,  ..., 4477,  271,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0775e-04,  2.3953e-04, -9.5475e-05,  ...,  2.9483e-04,
         -6.3631e-04, -2.5649e-04],
        [-6.6981e-06, -4.5896e-06,  5.1297e-06,  ..., -5.5432e-06,
         -4.5188e-06, -4.9323e-06],
        [-2.0623e-05, -1.4961e-05,  1.6972e-05,  ..., -1.7062e-05,
         -1.4082e-05, -1.5199e-05],
        [-1.5110e-05, -1.0177e-05,  1.2159e-05,  ..., -1.2562e-05,
         -1.0267e-05, -1.1727e-05],
        [-1.6406e-05, -1.2398e-05,  1.3098e-05,  ..., -1.3903e-05,
         -1.1995e-05, -1.1265e-05]], device='cuda:0')
Loss: 0.9975594282150269


Running epoch 1, step 1629, batch 581
Sampled inputs[:2]: tensor([[   0,  259, 6887,  ..., 1400,  292,  474],
        [   0,  300, 3808,  ...,  496,   14, 1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7992e-04,  1.8979e-04, -1.7146e-04,  ...,  3.2976e-04,
         -4.5235e-04, -2.6808e-04],
        [-7.9796e-06, -5.5432e-06,  6.0908e-06,  ..., -6.6608e-06,
         -5.4277e-06, -5.8226e-06],
        [-2.4557e-05, -1.8001e-05,  2.0161e-05,  ..., -2.0459e-05,
         -1.6823e-05, -1.7941e-05],
        [-1.8001e-05, -1.2293e-05,  1.4454e-05,  ..., -1.5080e-05,
         -1.2308e-05, -1.3873e-05],
        [-1.9625e-05, -1.4991e-05,  1.5646e-05,  ..., -1.6734e-05,
         -1.4395e-05, -1.3337e-05]], device='cuda:0')
Loss: 0.971280574798584


Running epoch 1, step 1630, batch 582
Sampled inputs[:2]: tensor([[    0,   395,  5949,  ...,   341,    13,   635],
        [    0, 24414,  4865,  ...,  8720,   344,  1566]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4852e-04,  5.9620e-05, -2.4767e-04,  ...,  3.9251e-04,
         -5.5206e-04, -5.2082e-04],
        [-9.2089e-06, -6.4149e-06,  7.1265e-06,  ..., -7.6592e-06,
         -6.2063e-06, -6.7391e-06],
        [-2.8431e-05, -2.0802e-05,  2.3589e-05,  ..., -2.3574e-05,
         -1.9237e-05, -2.0802e-05],
        [-2.0921e-05, -1.4275e-05,  1.7017e-05,  ..., -1.7419e-05,
         -1.4104e-05, -1.6153e-05],
        [-2.2680e-05, -1.7315e-05,  1.8284e-05,  ..., -1.9237e-05,
         -1.6451e-05, -1.5438e-05]], device='cuda:0')
Loss: 0.9326329231262207


Running epoch 1, step 1631, batch 583
Sampled inputs[:2]: tensor([[    0,    12,  6426,  ...,  2629, 13422,    12],
        [    0,    35,  3815,  ...,   278,  7097,  4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7405e-04,  5.0493e-05, -3.1363e-04,  ...,  3.8433e-04,
         -5.2711e-04, -4.2723e-04],
        [-1.0528e-05, -7.3239e-06,  8.1323e-06,  ..., -8.7544e-06,
         -7.1675e-06, -7.7449e-06],
        [-3.2485e-05, -2.3752e-05,  2.6911e-05,  ..., -2.6911e-05,
         -2.2218e-05, -2.3827e-05],
        [-2.3812e-05, -1.6212e-05,  1.9327e-05,  ..., -1.9804e-05,
         -1.6220e-05, -1.8448e-05],
        [-2.5958e-05, -1.9804e-05,  2.0891e-05,  ..., -2.1979e-05,
         -1.9014e-05, -1.7717e-05]], device='cuda:0')
Loss: 0.9389293193817139
Graident accumulation at epoch 1, step 1631, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2569e-05,  1.6154e-05, -2.4587e-04,  ...,  1.0265e-04,
          1.0313e-04, -8.1551e-05],
        [-4.9203e-06, -3.8540e-06,  4.1546e-06,  ..., -4.6993e-06,
         -1.9616e-06, -2.6336e-06],
        [ 6.0397e-05,  6.8953e-05, -4.9182e-05,  ...,  5.4241e-05,
          6.5428e-05,  2.4399e-05],
        [ 3.1304e-06,  1.4849e-05, -8.6144e-06,  ...,  1.1329e-05,
          1.4165e-05,  3.3576e-06],
        [-2.6111e-05, -1.9318e-05,  2.0381e-05,  ..., -2.1991e-05,
         -1.8347e-05, -1.7730e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4270e-08, 5.6357e-08, 6.7323e-08,  ..., 2.6763e-08, 1.5700e-07,
         3.9058e-08],
        [8.9961e-11, 5.5132e-11, 2.5788e-11,  ..., 6.1102e-11, 2.6255e-11,
         3.4213e-11],
        [4.1976e-09, 2.8147e-09, 1.6907e-09,  ..., 3.4011e-09, 1.6702e-09,
         1.1588e-09],
        [1.1263e-09, 1.2072e-09, 4.6604e-10,  ..., 9.9024e-10, 7.6687e-10,
         4.2298e-10],
        [4.0804e-10, 2.3254e-10, 9.0502e-11,  ..., 3.0019e-10, 9.4631e-11,
         1.1578e-10]], device='cuda:0')
optimizer state dict: 204.0
lr: [2.4620397327550194e-06, 2.4620397327550194e-06]
scheduler_last_epoch: 204


Running epoch 1, step 1632, batch 584
Sampled inputs[:2]: tensor([[    0,  1235,    14,  ...,  3301,   549,    14],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6288e-04,  6.2947e-05,  3.1911e-05,  ..., -1.1845e-04,
         -5.0720e-05,  1.6282e-05],
        [-1.3337e-06, -9.2387e-07,  1.0654e-06,  ..., -1.0952e-06,
         -8.7917e-07, -9.1270e-07],
        [-4.1425e-06, -3.0100e-06,  3.5316e-06,  ..., -3.3975e-06,
         -2.7269e-06, -2.8312e-06],
        [-3.0845e-06, -2.1011e-06,  2.5779e-06,  ..., -2.5332e-06,
         -2.0266e-06, -2.2501e-06],
        [-3.2037e-06, -2.4289e-06,  2.6524e-06,  ..., -2.6822e-06,
         -2.2650e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9668765664100647


Running epoch 1, step 1633, batch 585
Sampled inputs[:2]: tensor([[   0, 1481,  278,  ..., 3940, 4938,    5],
        [   0, 1732,  292,  ..., 3440, 4010, 1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7115e-04,  6.2947e-05, -7.2900e-05,  ..., -7.6362e-05,
          7.8882e-05,  1.1961e-05],
        [-2.6450e-06, -1.8403e-06,  2.1234e-06,  ..., -2.1830e-06,
         -1.7248e-06, -1.7956e-06],
        [-8.2552e-06, -5.9903e-06,  7.0482e-06,  ..., -6.8098e-06,
         -5.3644e-06, -5.5879e-06],
        [-6.0648e-06, -4.1425e-06,  5.0962e-06,  ..., -5.0217e-06,
         -3.9488e-06, -4.3809e-06],
        [-6.4224e-06, -4.8727e-06,  5.3495e-06,  ..., -5.4091e-06,
         -4.4852e-06, -4.0531e-06]], device='cuda:0')
Loss: 0.9760602116584778


Running epoch 1, step 1634, batch 586
Sampled inputs[:2]: tensor([[   0, 1611,  266,  ...,  266, 2673, 6277],
        [   0, 1911,  679,  ...,   19, 3737,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9640e-04,  6.1067e-05, -1.5353e-04,  ..., -1.4013e-04,
          2.7230e-05,  2.4271e-04],
        [-3.9414e-06, -2.7642e-06,  3.1292e-06,  ..., -3.2857e-06,
         -2.5705e-06, -2.7195e-06],
        [ 8.0112e-05,  6.8761e-05, -3.8029e-05,  ...,  7.5346e-05,
          2.8542e-05,  3.0276e-05],
        [-9.0748e-06, -6.2287e-06,  7.5549e-06,  ..., -7.5698e-06,
         -5.8860e-06, -6.6310e-06],
        [-9.5665e-06, -7.3165e-06,  7.8827e-06,  ..., -8.1360e-06,
         -6.6757e-06, -6.1393e-06]], device='cuda:0')
Loss: 0.9853801727294922


Running epoch 1, step 1635, batch 587
Sampled inputs[:2]: tensor([[    0,   669,   292,  ...,  4032,   271,  4442],
        [    0, 20291,  1990,  ...,   298,   732,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7832e-04,  1.0505e-04, -5.9195e-05,  ..., -1.6120e-04,
          2.1477e-04,  2.6293e-04],
        [-5.2527e-06, -3.7253e-06,  4.0308e-06,  ..., -4.4405e-06,
         -3.5241e-06, -3.7104e-06],
        [ 7.6029e-05,  6.5602e-05, -3.4959e-05,  ...,  7.1784e-05,
          2.5547e-05,  2.7252e-05],
        [-1.2070e-05, -8.3894e-06,  9.7007e-06,  ..., -1.0207e-05,
         -8.0764e-06, -8.9854e-06],
        [-1.2934e-05, -9.9689e-06,  1.0356e-05,  ..., -1.1101e-05,
         -9.2536e-06, -8.4788e-06]], device='cuda:0')
Loss: 0.9924150705337524


Running epoch 1, step 1636, batch 588
Sampled inputs[:2]: tensor([[    0,  1477,   591,  ...,  4111, 18012, 11991],
        [    0,  2140,    12,  ...,   696,   688,  1998]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1390e-04,  2.4012e-04,  8.9166e-05,  ..., -2.8518e-04,
         -2.7487e-05,  3.3818e-04],
        [-6.6385e-06, -4.7088e-06,  5.1558e-06,  ..., -5.5134e-06,
         -4.4145e-06, -4.6119e-06],
        [ 7.1767e-05,  6.2398e-05, -3.1249e-05,  ...,  6.8461e-05,
          2.2745e-05,  2.4420e-05],
        [-1.5259e-05, -1.0639e-05,  1.2442e-05,  ..., -1.2666e-05,
         -1.0133e-05, -1.1191e-05],
        [-1.6257e-05, -1.2591e-05,  1.3173e-05,  ..., -1.3754e-05,
         -1.1593e-05, -1.0535e-05]], device='cuda:0')
Loss: 0.9761508107185364


Running epoch 1, step 1637, batch 589
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2538e-04,  1.8541e-04, -1.3285e-05,  ..., -3.3048e-04,
         -1.0717e-04,  3.7418e-04],
        [-8.0019e-06, -5.5656e-06,  6.2138e-06,  ..., -6.6087e-06,
         -5.2378e-06, -5.5172e-06],
        [ 6.7655e-05,  5.9686e-05, -2.7822e-05,  ...,  6.5153e-05,
          2.0257e-05,  2.1664e-05],
        [-1.8299e-05, -1.2517e-05,  1.4946e-05,  ..., -1.5095e-05,
         -1.1943e-05, -1.3337e-05],
        [-1.9461e-05, -1.4827e-05,  1.5751e-05,  ..., -1.6376e-05,
         -1.3694e-05, -1.2517e-05]], device='cuda:0')
Loss: 0.9666337966918945


Running epoch 1, step 1638, batch 590
Sampled inputs[:2]: tensor([[   0,  287, 9430,  ..., 3121,  352,  360],
        [   0,  409,  729,  ...,  391,  266,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9281e-04,  2.1463e-04, -2.6810e-05,  ..., -3.6137e-04,
         -2.5557e-04,  3.4614e-04],
        [-9.3058e-06, -6.5342e-06,  7.3016e-06,  ..., -7.6964e-06,
         -6.1169e-06, -6.4671e-06],
        [ 6.3631e-05,  5.6587e-05, -2.4275e-05,  ...,  6.1801e-05,
          1.7545e-05,  1.8758e-05],
        [-2.1249e-05, -1.4648e-05,  1.7509e-05,  ..., -1.7554e-05,
         -1.3925e-05, -1.5602e-05],
        [-2.2680e-05, -1.7405e-05,  1.8522e-05,  ..., -1.9118e-05,
         -1.6019e-05, -1.4707e-05]], device='cuda:0')
Loss: 0.9568434953689575


Running epoch 1, step 1639, batch 591
Sampled inputs[:2]: tensor([[   0,   14, 3921,  ...,  199, 2038, 1963],
        [   0,   14,   69,  ...,  287,  259, 5158]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3362e-04,  2.6495e-04, -1.8784e-04,  ..., -4.9935e-04,
         -2.2261e-04,  5.3605e-04],
        [-1.0677e-05, -7.4320e-06,  8.2329e-06,  ..., -8.8736e-06,
         -7.0557e-06, -7.5400e-06],
        [ 5.9369e-05,  5.3666e-05, -2.1161e-05,  ...,  5.8150e-05,
          1.4639e-05,  1.5435e-05],
        [-2.4304e-05, -1.6585e-05,  1.9655e-05,  ..., -2.0176e-05,
         -1.6011e-05, -1.8090e-05],
        [-2.6122e-05, -1.9863e-05,  2.0966e-05,  ..., -2.2143e-05,
         -1.8552e-05, -1.7241e-05]], device='cuda:0')
Loss: 0.9479095935821533
Graident accumulation at epoch 1, step 1639, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4050e-05,  4.1034e-05, -2.4007e-04,  ...,  4.2451e-05,
          7.0559e-05, -1.9790e-05],
        [-5.4960e-06, -4.2118e-06,  4.5624e-06,  ..., -5.1168e-06,
         -2.4710e-06, -3.1242e-06],
        [ 6.0294e-05,  6.7424e-05, -4.6380e-05,  ...,  5.4632e-05,
          6.0349e-05,  2.3503e-05],
        [ 3.8699e-07,  1.1705e-05, -5.7875e-06,  ...,  8.1787e-06,
          1.1147e-05,  1.2129e-06],
        [-2.6112e-05, -1.9373e-05,  2.0440e-05,  ..., -2.2006e-05,
         -1.8367e-05, -1.7681e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4404e-08, 5.6371e-08, 6.7291e-08,  ..., 2.6985e-08, 1.5689e-07,
         3.9306e-08],
        [8.9985e-11, 5.5132e-11, 2.5830e-11,  ..., 6.1119e-11, 2.6278e-11,
         3.4235e-11],
        [4.1970e-09, 2.8147e-09, 1.6895e-09,  ..., 3.4011e-09, 1.6688e-09,
         1.1579e-09],
        [1.1257e-09, 1.2063e-09, 4.6596e-10,  ..., 9.8965e-10, 7.6636e-10,
         4.2288e-10],
        [4.0831e-10, 2.3270e-10, 9.0851e-11,  ..., 3.0038e-10, 9.4881e-11,
         1.1596e-10]], device='cuda:0')
optimizer state dict: 205.0
lr: [2.381388209021682e-06, 2.381388209021682e-06]
scheduler_last_epoch: 205


Running epoch 1, step 1640, batch 592
Sampled inputs[:2]: tensor([[   0,  287, 2421,  ..., 6612,  352,  344],
        [   0,  607,  259,  ...,  271,  669,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6708e-06, -1.0003e-04, -1.1120e-04,  ..., -1.5723e-04,
          4.4528e-04, -6.9470e-05],
        [-1.4305e-06, -9.8348e-07,  9.0897e-07,  ..., -1.2293e-06,
         -9.9093e-07, -1.1101e-06],
        [-4.4405e-06, -3.1590e-06,  2.9951e-06,  ..., -3.7849e-06,
         -3.0547e-06, -3.4422e-06],
        [ 1.4728e-04,  6.8282e-05, -6.6361e-05,  ...,  2.2038e-04,
          1.2684e-04,  7.3534e-05],
        [-3.8743e-06, -2.8163e-06,  2.5034e-06,  ..., -3.3975e-06,
         -2.8461e-06, -2.8610e-06]], device='cuda:0')
Loss: 0.9796348810195923


Running epoch 1, step 1641, batch 593
Sampled inputs[:2]: tensor([[    0,   422,    13,  ..., 14026,   368,  4999],
        [    0,   266, 10726,  ..., 13973, 22191, 15913]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1197e-04, -1.4891e-04, -5.2341e-05,  ..., -1.3474e-04,
          4.6519e-04,  4.5128e-05],
        [-2.8685e-06, -1.9595e-06,  1.9744e-06,  ..., -2.4289e-06,
         -1.9968e-06, -2.1830e-06],
        [-8.7917e-06, -6.2436e-06,  6.4373e-06,  ..., -7.4059e-06,
         -6.0946e-06, -6.6757e-06],
        [ 1.4424e-04,  6.6256e-05, -6.4021e-05,  ...,  2.1783e-04,
          1.2472e-04,  7.1179e-05],
        [-7.5698e-06, -5.5432e-06,  5.3048e-06,  ..., -6.5565e-06,
         -5.6177e-06, -5.4836e-06]], device='cuda:0')
Loss: 0.9714553356170654


Running epoch 1, step 1642, batch 594
Sampled inputs[:2]: tensor([[   0, 3468,  278,  ..., 2442,  292,  380],
        [   0, 2785, 1061,  ..., 1194,  692, 4339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9782e-04, -3.1542e-04, -1.8237e-04,  ...,  3.3698e-05,
          3.2410e-04,  1.2163e-04],
        [-4.2394e-06, -2.7493e-06,  2.9728e-06,  ..., -3.6135e-06,
         -2.8275e-06, -3.2485e-06],
        [-1.2964e-05, -8.7321e-06,  9.6858e-06,  ..., -1.0952e-05,
         -8.5980e-06, -9.8199e-06],
        [ 1.4123e-04,  6.4565e-05, -6.1741e-05,  ...,  2.1522e-04,
          1.2290e-04,  6.8736e-05],
        [-1.1012e-05, -7.6443e-06,  7.9274e-06,  ..., -9.5069e-06,
         -7.7933e-06, -7.9274e-06]], device='cuda:0')
Loss: 0.9664069414138794


Running epoch 1, step 1643, batch 595
Sampled inputs[:2]: tensor([[    0,  3756,    13,  ...,  1704,   278,  5851],
        [    0,   367,   925,  ..., 25491,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2990e-04, -3.7536e-04, -2.5967e-04,  ...,  6.7657e-05,
          3.4017e-04,  4.5963e-05],
        [-5.6103e-06, -3.7029e-06,  4.0978e-06,  ..., -4.7013e-06,
         -3.6731e-06, -4.1574e-06],
        [-1.7315e-05, -1.1861e-05,  1.3441e-05,  ..., -1.4395e-05,
         -1.1280e-05, -1.2726e-05],
        [ 1.3811e-04,  6.2434e-05, -5.9089e-05,  ...,  2.1276e-04,
          1.2100e-04,  6.6560e-05],
        [-1.4350e-05, -1.0177e-05,  1.0729e-05,  ..., -1.2219e-05,
         -1.0028e-05, -9.9987e-06]], device='cuda:0')
Loss: 1.0035518407821655


Running epoch 1, step 1644, batch 596
Sampled inputs[:2]: tensor([[   0,   17,  292,  ..., 8055,  365, 3125],
        [   0,  271,  266,  ...,   14,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7853e-04, -3.1306e-04, -3.9096e-04,  ..., -5.5673e-05,
          2.8028e-04, -3.0476e-06],
        [-6.9737e-06, -4.6082e-06,  5.0105e-06,  ..., -5.9083e-06,
         -4.6343e-06, -5.1633e-06],
        [-2.1428e-05, -1.4737e-05,  1.6436e-05,  ..., -1.8016e-05,
         -1.4201e-05, -1.5736e-05],
        [ 1.3510e-04,  6.0497e-05, -5.7003e-05,  ...,  2.1011e-04,
          1.1889e-04,  6.4236e-05],
        [-1.7807e-05, -1.2726e-05,  1.3158e-05,  ..., -1.5318e-05,
         -1.2651e-05, -1.2368e-05]], device='cuda:0')
Loss: 0.9590718150138855


Running epoch 1, step 1645, batch 597
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,   765,   292,  ...,   623,    12,  7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8764e-04, -1.4884e-04, -3.9096e-04,  ..., -2.6259e-05,
          2.4964e-04,  7.4352e-05],
        [-8.2627e-06, -5.4501e-06,  5.9083e-06,  ..., -7.0333e-06,
         -5.5060e-06, -6.1169e-06],
        [-2.5421e-05, -1.7554e-05,  1.9446e-05,  ..., -2.1502e-05,
         -1.6913e-05, -1.8671e-05],
        [ 1.3214e-04,  5.8604e-05, -5.4872e-05,  ...,  2.0749e-04,
          1.1685e-04,  6.1881e-05],
        [-2.0951e-05, -1.5050e-05,  1.5482e-05,  ..., -1.8090e-05,
         -1.4916e-05, -1.4514e-05]], device='cuda:0')
Loss: 0.9577330946922302


Running epoch 1, step 1646, batch 598
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0, 4100,   12,  ...,   13, 4710, 1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3463e-04, -1.2178e-04, -3.7533e-04,  ..., -3.6995e-05,
          1.2029e-04, -1.3422e-04],
        [-9.5367e-06, -6.2995e-06,  6.9067e-06,  ..., -8.1062e-06,
         -6.3702e-06, -7.0781e-06],
        [-2.9266e-05, -2.0206e-05,  2.2724e-05,  ..., -2.4647e-05,
         -1.9461e-05, -2.1398e-05],
        [ 1.2931e-04,  5.6786e-05, -5.2532e-05,  ...,  2.0512e-04,
          1.1494e-04,  5.9661e-05],
        [-2.4095e-05, -1.7300e-05,  1.8090e-05,  ..., -2.0698e-05,
         -1.7136e-05, -1.6570e-05]], device='cuda:0')
Loss: 0.9310110807418823


Running epoch 1, step 1647, batch 599
Sampled inputs[:2]: tensor([[    0,   221,   374,  ...,  2296,   365,  4579],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4882e-04,  1.4810e-05,  3.9715e-06,  ...,  2.2210e-05,
         -3.8814e-05, -5.5121e-05],
        [-1.0855e-05, -7.2420e-06,  7.8157e-06,  ..., -9.2909e-06,
         -7.2643e-06, -8.0839e-06],
        [-3.3349e-05, -2.3201e-05,  2.5809e-05,  ..., -2.8238e-05,
         -2.2218e-05, -2.4408e-05],
        [ 1.2637e-04,  5.4730e-05, -5.0416e-05,  ...,  2.0248e-04,
          1.1294e-04,  5.7336e-05],
        [-2.7418e-05, -1.9819e-05,  2.0519e-05,  ..., -2.3663e-05,
         -1.9535e-05, -1.8865e-05]], device='cuda:0')
Loss: 0.9601452350616455
Graident accumulation at epoch 1, step 1647, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.7526e-05,  3.8412e-05, -2.1567e-04,  ...,  4.0427e-05,
          5.9622e-05, -2.3323e-05],
        [-6.0319e-06, -4.5148e-06,  4.8877e-06,  ..., -5.5342e-06,
         -2.9504e-06, -3.6202e-06],
        [ 5.0930e-05,  5.8361e-05, -3.9161e-05,  ...,  4.6345e-05,
          5.2093e-05,  1.8711e-05],
        [ 1.2985e-05,  1.6008e-05, -1.0250e-05,  ...,  2.7609e-05,
          2.1327e-05,  6.8252e-06],
        [-2.6243e-05, -1.9417e-05,  2.0448e-05,  ..., -2.2172e-05,
         -1.8484e-05, -1.7799e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4651e-08, 5.6315e-08, 6.7224e-08,  ..., 2.6959e-08, 1.5673e-07,
         3.9270e-08],
        [9.0013e-11, 5.5129e-11, 2.5865e-11,  ..., 6.1145e-11, 2.6305e-11,
         3.4266e-11],
        [4.1939e-09, 2.8125e-09, 1.6884e-09,  ..., 3.3985e-09, 1.6676e-09,
         1.1573e-09],
        [1.1406e-09, 1.2081e-09, 4.6804e-10,  ..., 1.0297e-09, 7.7835e-10,
         4.2575e-10],
        [4.0866e-10, 2.3286e-10, 9.1182e-11,  ..., 3.0064e-10, 9.5168e-11,
         1.1620e-10]], device='cuda:0')
optimizer state dict: 206.0
lr: [2.3019008756738137e-06, 2.3019008756738137e-06]
scheduler_last_epoch: 206


Running epoch 1, step 1648, batch 600
Sampled inputs[:2]: tensor([[   0,   12, 1631,  ..., 1143,  271,  266],
        [   0, 7094,  596,  ..., 4764, 9514,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0007e-05,  8.5412e-05,  2.2833e-05,  ..., -4.3647e-05,
         -1.2481e-04, -7.0501e-06],
        [-1.4305e-06, -9.3877e-07,  8.3074e-07,  ..., -1.2070e-06,
         -9.8348e-07, -1.1176e-06],
        [-4.5300e-06, -3.0994e-06,  2.8908e-06,  ..., -3.7849e-06,
         -3.0696e-06, -3.5167e-06],
        [-3.1590e-06, -2.0266e-06,  1.9222e-06,  ..., -2.6822e-06,
         -2.1458e-06, -2.5630e-06],
        [-3.8445e-06, -2.6673e-06,  2.3395e-06,  ..., -3.2783e-06,
         -2.7567e-06, -2.8163e-06]], device='cuda:0')
Loss: 0.9326508641242981


Running epoch 1, step 1649, batch 601
Sampled inputs[:2]: tensor([[   0, 3484,  437,  ...,  298,  995, 4009],
        [   0,  266, 6071,  ..., 1061, 1107,  839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0083e-04,  1.8614e-04,  2.2459e-04,  ..., -1.4169e-05,
         -1.6533e-04,  4.3255e-05],
        [-2.7344e-06, -1.7919e-06,  1.8887e-06,  ..., -2.2873e-06,
         -1.7136e-06, -2.0862e-06],
        [-8.7321e-06, -5.9307e-06,  6.5118e-06,  ..., -7.2420e-06,
         -5.4240e-06, -6.5714e-06],
        [-6.2138e-06, -3.9786e-06,  4.5151e-06,  ..., -5.2154e-06,
         -3.8520e-06, -4.9323e-06],
        [-6.9886e-06, -4.8727e-06,  5.0068e-06,  ..., -5.9158e-06,
         -4.6641e-06, -4.9621e-06]], device='cuda:0')
Loss: 0.9455121159553528


Running epoch 1, step 1650, batch 602
Sampled inputs[:2]: tensor([[    0,    83,   292,  ...,   445,    11, 16109],
        [    0,   380,   560,  ...,   287,  6769,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3820e-04,  4.9356e-05,  1.9320e-04,  ..., -1.9082e-04,
          3.2183e-04,  1.2581e-04],
        [-4.1053e-06, -2.7195e-06,  2.8275e-06,  ..., -3.4273e-06,
         -2.5816e-06, -3.0398e-06],
        [-1.2934e-05, -8.8960e-06,  9.6411e-06,  ..., -1.0714e-05,
         -8.0913e-06, -9.4771e-06],
        [-9.2983e-06, -6.0350e-06,  6.7353e-06,  ..., -7.7933e-06,
         -5.8040e-06, -7.1973e-06],
        [-1.0312e-05, -7.3165e-06,  7.4208e-06,  ..., -8.7023e-06,
         -6.9290e-06, -7.0781e-06]], device='cuda:0')
Loss: 0.9757415056228638


Running epoch 1, step 1651, batch 603
Sampled inputs[:2]: tensor([[   0,  287,  271,  ..., 5090,  631, 3276],
        [   0,  747, 7890,  ...,  706, 8667,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6969e-04,  1.1246e-04,  2.0306e-04,  ..., -1.4609e-04,
          1.1282e-04,  2.1932e-04],
        [-5.4613e-06, -3.5428e-06,  3.7812e-06,  ..., -4.5747e-06,
         -3.4235e-06, -4.0829e-06],
        [-1.7166e-05, -1.1593e-05,  1.2875e-05,  ..., -1.4246e-05,
         -1.0714e-05, -1.2740e-05],
        [-1.2413e-05, -7.8678e-06,  8.9854e-06,  ..., -1.0446e-05,
         -7.7561e-06, -9.7156e-06],
        [-1.3649e-05, -9.4920e-06,  9.8646e-06,  ..., -1.1533e-05,
         -9.1195e-06, -9.4771e-06]], device='cuda:0')
Loss: 0.918511152267456


Running epoch 1, step 1652, batch 604
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,   401,  3704,  ...,    14,  1062,  1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6969e-04,  2.3883e-04,  3.6046e-04,  ..., -3.7612e-04,
         -5.8474e-05,  1.0649e-04],
        [-6.7726e-06, -4.3735e-06,  4.6417e-06,  ..., -5.6922e-06,
         -4.2617e-06, -5.0813e-06],
        [-2.1070e-05, -1.4216e-05,  1.5676e-05,  ..., -1.7524e-05,
         -1.3202e-05, -1.5661e-05],
        [-1.5348e-05, -9.6560e-06,  1.1012e-05,  ..., -1.2949e-05,
         -9.6336e-06, -1.2055e-05],
        [-1.6928e-05, -1.1787e-05,  1.2130e-05,  ..., -1.4350e-05,
         -1.1370e-05, -1.1772e-05]], device='cuda:0')
Loss: 0.9185812473297119


Running epoch 1, step 1653, batch 605
Sampled inputs[:2]: tensor([[    0,    13, 38195,  ...,   950,   298,   257],
        [    0,  5603,  6598,  ...,  1692,  1713,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5335e-04,  3.3622e-04,  3.0655e-04,  ..., -3.9851e-04,
         -9.2295e-05,  1.0327e-05],
        [-8.1137e-06, -5.2638e-06,  5.5917e-06,  ..., -6.8322e-06,
         -5.1446e-06, -6.0573e-06],
        [-2.5153e-05, -1.6987e-05,  1.8835e-05,  ..., -2.0921e-05,
         -1.5825e-05, -1.8582e-05],
        [-1.8343e-05, -1.1578e-05,  1.3232e-05,  ..., -1.5482e-05,
         -1.1586e-05, -1.4350e-05],
        [-2.0251e-05, -1.4141e-05,  1.4633e-05,  ..., -1.7181e-05,
         -1.3679e-05, -1.3977e-05]], device='cuda:0')
Loss: 0.9437332153320312


Running epoch 1, step 1654, batch 606
Sampled inputs[:2]: tensor([[    0, 16064, 10937,  ...,   346,   462,   221],
        [    0,  1172,   365,  ...,  1119, 15573,  3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8264e-04,  3.9409e-04,  4.2108e-04,  ..., -4.3545e-04,
         -3.9935e-04, -1.3993e-04],
        [-9.3952e-06, -6.1914e-06,  6.5528e-06,  ..., -7.9274e-06,
         -6.0387e-06, -6.9588e-06],
        [-2.9176e-05, -1.9982e-05,  2.2084e-05,  ..., -2.4334e-05,
         -1.8612e-05, -2.1368e-05],
        [-2.1264e-05, -1.3620e-05,  1.5527e-05,  ..., -1.7971e-05,
         -1.3612e-05, -1.6510e-05],
        [-2.3440e-05, -1.6615e-05,  1.7136e-05,  ..., -1.9923e-05,
         -1.6019e-05, -1.6034e-05]], device='cuda:0')
Loss: 0.9633761048316956


Running epoch 1, step 1655, batch 607
Sampled inputs[:2]: tensor([[    0,  6668,   565,  ...,   360,   259,  8166],
        [    0,   721,  1717,  ...,   278, 26029,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8132e-04,  4.7668e-04,  4.4660e-04,  ..., -5.0439e-04,
         -4.5715e-04, -3.2275e-04],
        [-1.0729e-05, -7.1526e-06,  7.4469e-06,  ..., -9.0748e-06,
         -6.9477e-06, -7.9349e-06],
        [-3.3349e-05, -2.3127e-05,  2.5094e-05,  ..., -2.7910e-05,
         -2.1443e-05, -2.4408e-05],
        [-2.4289e-05, -1.5765e-05,  1.7628e-05,  ..., -2.0564e-05,
         -1.5654e-05, -1.8850e-05],
        [-2.6837e-05, -1.9267e-05,  1.9521e-05,  ..., -2.2888e-05,
         -1.8492e-05, -1.8343e-05]], device='cuda:0')
Loss: 0.9819463491439819
Graident accumulation at epoch 1, step 1655, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.8906e-05,  8.2238e-05, -1.4944e-04,  ..., -1.4055e-05,
          7.9445e-06, -5.3266e-05],
        [-6.5016e-06, -4.7786e-06,  5.1436e-06,  ..., -5.8882e-06,
         -3.3501e-06, -4.0516e-06],
        [ 4.2502e-05,  5.0213e-05, -3.2736e-05,  ...,  3.8920e-05,
          4.4739e-05,  1.4400e-05],
        [ 9.2580e-06,  1.2831e-05, -7.4625e-06,  ...,  2.2792e-05,
          1.7629e-05,  4.2577e-06],
        [-2.6302e-05, -1.9402e-05,  2.0355e-05,  ..., -2.2243e-05,
         -1.8485e-05, -1.7854e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4676e-08, 5.6486e-08, 6.7356e-08,  ..., 2.7186e-08, 1.5679e-07,
         3.9335e-08],
        [9.0038e-11, 5.5125e-11, 2.5895e-11,  ..., 6.1166e-11, 2.6327e-11,
         3.4295e-11],
        [4.1908e-09, 2.8102e-09, 1.6874e-09,  ..., 3.3959e-09, 1.6664e-09,
         1.1568e-09],
        [1.1400e-09, 1.2071e-09, 4.6788e-10,  ..., 1.0291e-09, 7.7782e-10,
         4.2568e-10],
        [4.0897e-10, 2.3300e-10, 9.1471e-11,  ..., 3.0086e-10, 9.5415e-11,
         1.1642e-10]], device='cuda:0')
optimizer state dict: 207.0
lr: [2.223589879069793e-06, 2.223589879069793e-06]
scheduler_last_epoch: 207


Running epoch 1, step 1656, batch 608
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  292,  474,  221],
        [   0, 2314,  516,  ..., 1871,   13, 1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5027e-04,  7.7944e-05, -1.3094e-04,  ..., -4.2699e-05,
         -3.7776e-04,  6.2861e-05],
        [-1.3113e-06, -7.4878e-07,  1.1250e-06,  ..., -1.0729e-06,
         -6.6310e-07, -8.9779e-07],
        [-4.1723e-06, -2.4438e-06,  3.7551e-06,  ..., -3.3677e-06,
         -2.0713e-06, -2.7865e-06],
        [-3.0994e-06, -1.6987e-06,  2.7716e-06,  ..., -2.5332e-06,
         -1.5274e-06, -2.2054e-06],
        [-3.0398e-06, -1.8924e-06,  2.6822e-06,  ..., -2.5034e-06,
         -1.6615e-06, -1.8924e-06]], device='cuda:0')
Loss: 0.928584098815918


Running epoch 1, step 1657, batch 609
Sampled inputs[:2]: tensor([[   0, 1932,  278,  ...,  609,  271,  266],
        [   0, 1234,  278,  ..., 1237, 1008,  417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9274e-04,  2.0504e-04, -1.1524e-04,  ..., -1.9685e-05,
         -3.1764e-04,  2.6074e-05],
        [-2.6599e-06, -1.5944e-06,  2.0117e-06,  ..., -2.2724e-06,
         -1.5534e-06, -1.9409e-06],
        [-8.1658e-06, -5.0962e-06,  6.6459e-06,  ..., -6.8545e-06,
         -4.6790e-06, -5.8115e-06],
        [-6.0350e-06, -3.5018e-06,  4.7833e-06,  ..., -5.1558e-06,
         -3.4794e-06, -4.5896e-06],
        [-6.4224e-06, -4.2170e-06,  5.0515e-06,  ..., -5.4985e-06,
         -4.0010e-06, -4.2617e-06]], device='cuda:0')
Loss: 0.9556678533554077


Running epoch 1, step 1658, batch 610
Sampled inputs[:2]: tensor([[    0,   935, 28368,  ...,   342,   259,  4600],
        [    0,  1184,   271,  ...,  7225,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3666e-04,  2.7144e-04, -1.5724e-04,  ..., -4.3219e-05,
         -3.9961e-04,  3.1297e-05],
        [-4.0233e-06, -2.5183e-06,  3.0026e-06,  ..., -3.4347e-06,
         -2.4065e-06, -2.9393e-06],
        [-1.2547e-05, -8.1807e-06,  1.0043e-05,  ..., -1.0535e-05,
         -7.3612e-06, -8.9854e-06],
        [ 4.0006e-04,  4.8430e-04, -3.1760e-04,  ...,  3.4209e-04,
          3.6489e-04,  2.6315e-04],
        [-9.8199e-06, -6.6906e-06,  7.5996e-06,  ..., -8.4192e-06,
         -6.2361e-06, -6.5714e-06]], device='cuda:0')
Loss: 0.9635264873504639


Running epoch 1, step 1659, batch 611
Sampled inputs[:2]: tensor([[    0,   694,  2326,  ...,   278,  1781,  9660],
        [    0,   292,    48,  ...,   199, 19047,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7117e-04,  3.1461e-04, -1.3819e-05,  ..., -2.5683e-05,
         -5.6807e-04, -3.1742e-05],
        [-5.3421e-06, -3.4459e-06,  4.0010e-06,  ..., -4.5896e-06,
         -3.2522e-06, -3.8594e-06],
        [-1.6600e-05, -1.1146e-05,  1.3337e-05,  ..., -1.4067e-05,
         -9.9689e-06, -1.1802e-05],
        [ 3.9713e-04,  4.8229e-04, -3.1527e-04,  ...,  3.3952e-04,
          3.6302e-04,  2.6102e-04],
        [-1.3024e-05, -9.1046e-06,  1.0133e-05,  ..., -1.1265e-05,
         -8.4266e-06, -8.6576e-06]], device='cuda:0')
Loss: 0.953434407711029


Running epoch 1, step 1660, batch 612
Sampled inputs[:2]: tensor([[    0,    47,    12,  ...,  4367,   278,   471],
        [    0, 29368,    13,  ...,   376,    88,  3333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0603e-04,  3.2245e-04,  9.3384e-05,  ..., -8.2343e-05,
         -4.6363e-04, -6.2761e-05],
        [-6.6981e-06, -4.4666e-06,  4.9695e-06,  ..., -5.7593e-06,
         -4.2059e-06, -4.7833e-06],
        [-2.0742e-05, -1.4380e-05,  1.6510e-05,  ..., -1.7658e-05,
         -1.2904e-05, -1.4648e-05],
        [ 3.9413e-04,  4.8007e-04, -3.1304e-04,  ...,  3.3693e-04,
          3.6092e-04,  2.5887e-04],
        [-1.6436e-05, -1.1846e-05,  1.2666e-05,  ..., -1.4260e-05,
         -1.0975e-05, -1.0833e-05]], device='cuda:0')
Loss: 1.0207948684692383


Running epoch 1, step 1661, batch 613
Sampled inputs[:2]: tensor([[   0,   16,   14,  ..., 5148,  259, 1951],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8363e-04,  4.0012e-04,  1.6888e-04,  ..., -7.4126e-05,
         -5.1435e-04, -4.8437e-05],
        [-8.0466e-06, -5.3309e-06,  5.9530e-06,  ..., -6.8694e-06,
         -4.9360e-06, -5.7220e-06],
        [-2.4915e-05, -1.7211e-05,  1.9789e-05,  ..., -2.1100e-05,
         -1.5229e-05, -1.7554e-05],
        [ 3.9111e-04,  4.7815e-04, -3.1073e-04,  ...,  3.3441e-04,
          3.5927e-04,  2.5667e-04],
        [-1.9550e-05, -1.4052e-05,  1.5050e-05,  ..., -1.6898e-05,
         -1.2852e-05, -1.2875e-05]], device='cuda:0')
Loss: 0.9371978640556335


Running epoch 1, step 1662, batch 614
Sampled inputs[:2]: tensor([[   0,   13, 2549,  ...,  221,  382,  298],
        [   0,  292,   41,  ...,  271, 9536,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9444e-04,  7.6389e-04,  6.5033e-04,  ...,  2.4954e-05,
         -4.8881e-04, -5.2851e-05],
        [-9.4622e-06, -6.1877e-06,  6.5416e-06,  ..., -8.1584e-06,
         -6.0387e-06, -6.9067e-06],
        [-2.9147e-05, -1.9938e-05,  2.1800e-05,  ..., -2.4900e-05,
         -1.8537e-05, -2.0996e-05],
        [ 3.8799e-04,  4.7633e-04, -3.0945e-04,  ...,  3.3155e-04,
          3.5678e-04,  2.5399e-04],
        [-2.3305e-05, -1.6496e-05,  1.6764e-05,  ..., -2.0310e-05,
         -1.5892e-05, -1.5765e-05]], device='cuda:0')
Loss: 0.9117289185523987


Running epoch 1, step 1663, batch 615
Sampled inputs[:2]: tensor([[    0,   287,   266,  ...,   998,   342, 17709],
        [    0,    14,   747,  ...,   367,   300,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2167e-04,  7.4926e-04,  5.0577e-04,  ...,  3.7795e-05,
         -5.4858e-04,  3.8567e-06],
        [-1.0781e-05, -7.1265e-06,  7.5325e-06,  ..., -9.2611e-06,
         -6.8694e-06, -7.8455e-06],
        [-3.3349e-05, -2.3037e-05,  2.5153e-05,  ..., -2.8402e-05,
         -2.1175e-05, -2.3976e-05],
        [ 3.8491e-04,  4.7418e-04, -3.0703e-04,  ...,  3.2897e-04,
          3.5484e-04,  2.5168e-04],
        [-2.6628e-05, -1.9044e-05,  1.9327e-05,  ..., -2.3112e-05,
         -1.8127e-05, -1.7941e-05]], device='cuda:0')
Loss: 0.9762453436851501
Graident accumulation at epoch 1, step 1663, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2218e-04,  1.4894e-04, -8.3918e-05,  ..., -8.8698e-06,
         -4.7708e-05, -4.7554e-05],
        [-6.9295e-06, -5.0134e-06,  5.3825e-06,  ..., -6.2255e-06,
         -3.7020e-06, -4.4310e-06],
        [ 3.4917e-05,  4.2888e-05, -2.6947e-05,  ...,  3.2188e-05,
          3.8148e-05,  1.0562e-05],
        [ 4.6823e-05,  5.8966e-05, -3.7420e-05,  ...,  5.3410e-05,
          5.1350e-05,  2.9000e-05],
        [-2.6335e-05, -1.9366e-05,  2.0252e-05,  ..., -2.2330e-05,
         -1.8449e-05, -1.7863e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4799e-08, 5.6991e-08, 6.7544e-08,  ..., 2.7160e-08, 1.5693e-07,
         3.9295e-08],
        [9.0064e-11, 5.5121e-11, 2.5926e-11,  ..., 6.1190e-11, 2.6348e-11,
         3.4322e-11],
        [4.1877e-09, 2.8079e-09, 1.6863e-09,  ..., 3.3933e-09, 1.6652e-09,
         1.1562e-09],
        [1.2870e-09, 1.4307e-09, 5.6168e-10,  ..., 1.1363e-09, 9.0295e-10,
         4.8859e-10],
        [4.0927e-10, 2.3313e-10, 9.1753e-11,  ..., 3.0110e-10, 9.5648e-11,
         1.1663e-10]], device='cuda:0')
optimizer state dict: 208.0
lr: [2.1464671858134968e-06, 2.1464671858134968e-06]
scheduler_last_epoch: 208


Running epoch 1, step 1664, batch 616
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2169e-04,  1.5687e-04, -7.6600e-05,  ..., -2.7565e-05,
          3.4539e-05,  4.4834e-05],
        [-1.3933e-06, -8.3447e-07,  9.1642e-07,  ..., -1.2517e-06,
         -8.6799e-07, -1.1101e-06],
        [-4.2915e-06, -2.6673e-06,  3.0994e-06,  ..., -3.7104e-06,
         -2.6226e-06, -3.2336e-06],
        [-2.9802e-06, -1.7434e-06,  2.0415e-06,  ..., -2.6822e-06,
         -1.8701e-06, -2.4289e-06],
        [-3.4720e-06, -2.1756e-06,  2.4587e-06,  ..., -3.0398e-06,
         -2.2352e-06, -2.4140e-06]], device='cuda:0')
Loss: 0.9252564311027527


Running epoch 1, step 1665, batch 617
Sampled inputs[:2]: tensor([[    0,  2612,   271,  ...,   369,  9862,   287],
        [    0, 33119,   391,  ...,   292,  4462,  2721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2332e-04,  1.4307e-04, -8.3745e-05,  ..., -2.3683e-05,
          2.3402e-05,  9.0388e-05],
        [-2.6897e-06, -1.7546e-06,  1.8179e-06,  ..., -2.3693e-06,
         -1.7360e-06, -2.0713e-06],
        [-8.2850e-06, -5.6475e-06,  6.1244e-06,  ..., -7.1377e-06,
         -5.2899e-06, -6.1840e-06],
        [-5.8562e-06, -3.7551e-06,  4.1425e-06,  ..., -5.1707e-06,
         -3.7923e-06, -4.6790e-06],
        [-6.7204e-06, -4.6641e-06,  4.8280e-06,  ..., -5.8860e-06,
         -4.5449e-06, -4.6492e-06]], device='cuda:0')
Loss: 0.9454078674316406


Running epoch 1, step 1666, batch 618
Sampled inputs[:2]: tensor([[    0, 25409,   287,  ...,  1005,   344,  3493],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5749e-05,  1.6024e-04, -1.1504e-04,  ..., -9.6874e-06,
          1.2815e-04,  6.6472e-05],
        [-4.0382e-06, -2.6897e-06,  2.7865e-06,  ..., -3.4869e-06,
         -2.6189e-06, -2.9914e-06],
        [-1.2487e-05, -8.7172e-06,  9.3579e-06,  ..., -1.0610e-05,
         -8.0466e-06, -9.0301e-06],
        [-8.9854e-06, -5.9009e-06,  6.4671e-06,  ..., -7.7784e-06,
         -5.8636e-06, -6.9439e-06],
        [-9.9987e-06, -7.1377e-06,  7.3016e-06,  ..., -8.6278e-06,
         -6.8247e-06, -6.7204e-06]], device='cuda:0')
Loss: 0.9793170094490051


Running epoch 1, step 1667, batch 619
Sampled inputs[:2]: tensor([[    0, 45050,   342,  ...,  3729,   287, 27888],
        [    0,  6328,    12,  ...,   417,   199,  1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4441e-05,  1.1231e-04, -7.9064e-05,  ...,  4.2799e-05,
          4.7730e-05,  2.1512e-04],
        [-5.3644e-06, -3.5316e-06,  3.7923e-06,  ..., -4.5896e-06,
         -3.4235e-06, -3.9339e-06],
        [-1.6600e-05, -1.1459e-05,  1.2711e-05,  ..., -1.4007e-05,
         -1.0520e-05, -1.1951e-05],
        [-1.2040e-05, -7.7859e-06,  8.8960e-06,  ..., -1.0312e-05,
         -7.7039e-06, -9.2536e-06],
        [-1.3173e-05, -9.3430e-06,  9.8050e-06,  ..., -1.1295e-05,
         -8.8662e-06, -8.7917e-06]], device='cuda:0')
Loss: 0.9694436192512512


Running epoch 1, step 1668, batch 620
Sampled inputs[:2]: tensor([[    0, 21448,   344,  ...,   365,  1501,   271],
        [    0,    12,   266,  ...,   278,   266, 10995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0857e-04,  1.2720e-04, -1.3554e-04,  ..., -7.2087e-05,
         -1.8765e-04,  1.4242e-04],
        [-6.7204e-06, -4.3772e-06,  4.9621e-06,  ..., -5.6475e-06,
         -4.1015e-06, -4.8205e-06],
        [-2.0891e-05, -1.4260e-05,  1.6585e-05,  ..., -1.7375e-05,
         -1.2696e-05, -1.4797e-05],
        [-1.5199e-05, -9.7081e-06,  1.1727e-05,  ..., -1.2770e-05,
         -9.2536e-06, -1.1429e-05],
        [-1.6242e-05, -1.1474e-05,  1.2532e-05,  ..., -1.3769e-05,
         -1.0602e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9367725849151611


Running epoch 1, step 1669, batch 621
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0,    14,   475,  ...,  4103,   278,  4190]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8175e-04,  1.7918e-04, -1.2952e-04,  ...,  7.2040e-05,
         -2.5662e-04,  2.0392e-04],
        [-8.1360e-06, -5.2638e-06,  5.9605e-06,  ..., -6.8024e-06,
         -4.9062e-06, -5.8338e-06],
        [-2.5332e-05, -1.7211e-05,  1.9953e-05,  ..., -2.0996e-05,
         -1.5259e-05, -1.7971e-05],
        [-1.8388e-05, -1.1675e-05,  1.4067e-05,  ..., -1.5378e-05,
         -1.1064e-05, -1.3813e-05],
        [-1.9684e-05, -1.3828e-05,  1.5065e-05,  ..., -1.6615e-05,
         -1.2718e-05, -1.2994e-05]], device='cuda:0')
Loss: 0.9387580752372742


Running epoch 1, step 1670, batch 622
Sampled inputs[:2]: tensor([[    0,   292, 44809,  ...,   642,   437,  9038],
        [    0,   365,  2849,  ...,     9,  3365,  5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9487e-04,  2.8054e-04, -1.2685e-04,  ...,  4.9583e-05,
         -2.0407e-04,  1.7128e-04],
        [-9.4399e-06, -6.1765e-06,  6.8732e-06,  ..., -7.9572e-06,
         -5.8413e-06, -6.7577e-06],
        [-2.9266e-05, -2.0117e-05,  2.2933e-05,  ..., -2.4438e-05,
         -1.8090e-05, -2.0713e-05],
        [-2.1338e-05, -1.3702e-05,  1.6212e-05,  ..., -1.8001e-05,
         -1.3225e-05, -1.6034e-05],
        [-2.2888e-05, -1.6287e-05,  1.7434e-05,  ..., -1.9461e-05,
         -1.5147e-05, -1.5065e-05]], device='cuda:0')
Loss: 0.9793060421943665


Running epoch 1, step 1671, batch 623
Sampled inputs[:2]: tensor([[    0,  1985,   278,  ...,   677, 12292, 17956],
        [    0, 11694,   292,  ...,   328,  1654,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9270e-04,  5.7569e-04, -1.9037e-04,  ...,  5.0929e-05,
         -1.3912e-04,  2.4311e-04],
        [-1.0714e-05, -7.1004e-06,  7.6145e-06,  ..., -9.0972e-06,
         -6.8992e-06, -7.7114e-06],
        [-3.3170e-05, -2.3097e-05,  2.5406e-05,  ..., -2.7925e-05,
         -2.1353e-05, -2.3603e-05],
        [-2.4304e-05, -1.5803e-05,  1.7934e-05,  ..., -2.0683e-05,
         -1.5773e-05, -1.8373e-05],
        [-2.6181e-05, -1.8835e-05,  1.9491e-05,  ..., -2.2426e-05,
         -1.7963e-05, -1.7345e-05]], device='cuda:0')
Loss: 0.9540385603904724
Graident accumulation at epoch 1, step 1671, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2923e-04,  1.9162e-04, -9.4563e-05,  ..., -2.8899e-06,
         -5.6849e-05, -1.8488e-05],
        [-7.3080e-06, -5.2221e-06,  5.6057e-06,  ..., -6.5127e-06,
         -4.0218e-06, -4.7591e-06],
        [ 2.8108e-05,  3.6289e-05, -2.1712e-05,  ...,  2.6176e-05,
          3.2198e-05,  7.1454e-06],
        [ 3.9710e-05,  5.1489e-05, -3.1884e-05,  ...,  4.6000e-05,
          4.4638e-05,  2.4262e-05],
        [-2.6320e-05, -1.9313e-05,  2.0176e-05,  ..., -2.2340e-05,
         -1.8401e-05, -1.7811e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4781e-08, 5.7265e-08, 6.7513e-08,  ..., 2.7136e-08, 1.5679e-07,
         3.9315e-08],
        [9.0089e-11, 5.5116e-11, 2.5958e-11,  ..., 6.1212e-11, 2.6369e-11,
         3.4347e-11],
        [4.1846e-09, 2.8056e-09, 1.6853e-09,  ..., 3.3907e-09, 1.6640e-09,
         1.1556e-09],
        [1.2864e-09, 1.4296e-09, 5.6144e-10,  ..., 1.1355e-09, 9.0230e-10,
         4.8844e-10],
        [4.0954e-10, 2.3325e-10, 9.2042e-11,  ..., 3.0130e-10, 9.5875e-11,
         1.1681e-10]], device='cuda:0')
optimizer state dict: 209.0
lr: [2.070544580925664e-06, 2.070544580925664e-06]
scheduler_last_epoch: 209


Running epoch 1, step 1672, batch 624
Sampled inputs[:2]: tensor([[    0, 25845,  4034,  ...,   474,   221,   474],
        [    0,   714,    14,  ...,  1501, 11397, 31940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3152e-04,  9.5121e-05, -2.5132e-04,  ...,  1.3492e-06,
         -4.2445e-04, -1.3662e-04],
        [-1.2740e-06, -8.5309e-07,  9.9838e-07,  ..., -1.0729e-06,
         -7.7486e-07, -9.4250e-07],
        [-4.0829e-06, -2.7418e-06,  3.4571e-06,  ..., -3.3379e-06,
         -2.4289e-06, -2.9057e-06],
        [-2.8759e-06, -1.8477e-06,  2.3693e-06,  ..., -2.3991e-06,
         -1.7285e-06, -2.2203e-06],
        [-3.0696e-06, -2.1756e-06,  2.5332e-06,  ..., -2.5630e-06,
         -1.9968e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9176601767539978


Running epoch 1, step 1673, batch 625
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0,   278,  5492,  ...,   328,   995,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7513e-05,  1.0978e-04, -3.0970e-04,  ..., -3.1877e-05,
         -3.6346e-04, -2.3216e-04],
        [-2.6152e-06, -1.7397e-06,  2.1383e-06,  ..., -2.1756e-06,
         -1.5758e-06, -1.8813e-06],
        [-8.2850e-06, -5.6028e-06,  7.2122e-06,  ..., -6.7502e-06,
         -4.8727e-06, -5.8413e-06],
        [-5.9307e-06, -3.8147e-06,  5.0813e-06,  ..., -4.8876e-06,
         -3.5092e-06, -4.4852e-06],
        [-6.1840e-06, -4.3958e-06,  5.2452e-06,  ..., -5.1558e-06,
         -3.9786e-06, -4.0084e-06]], device='cuda:0')
Loss: 0.977785587310791


Running epoch 1, step 1674, batch 626
Sampled inputs[:2]: tensor([[    0,  2173,   292,  ...,   344,  8106,   344],
        [    0,   560,   199,  ...,   292, 12605,  2096]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3206e-04,  1.4403e-04, -5.0116e-04,  ..., -1.3915e-04,
         -7.2952e-04, -6.1392e-04],
        [-4.0680e-06, -2.6375e-06,  3.0100e-06,  ..., -3.3751e-06,
         -2.5593e-06, -2.9989e-06],
        [-1.2785e-05, -8.5384e-06,  1.0177e-05,  ..., -1.0416e-05,
         -7.9125e-06, -9.2834e-06],
        [-9.1195e-06, -5.7518e-06,  7.0632e-06,  ..., -7.5251e-06,
         -5.6848e-06, -7.0333e-06],
        [-9.9242e-06, -6.8545e-06,  7.6145e-06,  ..., -8.2552e-06,
         -6.6161e-06, -6.6906e-06]], device='cuda:0')
Loss: 0.9199165105819702


Running epoch 1, step 1675, batch 627
Sampled inputs[:2]: tensor([[    0,   342,  3001,  ...,   369, 11195,   367],
        [    0,  1531,    14,  ...,  6169,    17,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3418e-04,  2.6500e-04, -3.7195e-04,  ..., -8.9215e-05,
         -7.4302e-04, -4.6276e-04],
        [-5.4315e-06, -3.5241e-06,  4.0457e-06,  ..., -4.5002e-06,
         -3.3863e-06, -3.9451e-06],
        [-1.7047e-05, -1.1459e-05,  1.3649e-05,  ..., -1.3918e-05,
         -1.0520e-05, -1.2234e-05],
        [-1.2144e-05, -7.6890e-06,  9.4771e-06,  ..., -1.0028e-05,
         -7.5251e-06, -9.2536e-06],
        [-1.3158e-05, -9.1940e-06,  1.0192e-05,  ..., -1.0982e-05,
         -8.7768e-06, -8.7619e-06]], device='cuda:0')
Loss: 0.9407109618186951


Running epoch 1, step 1676, batch 628
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   706,  2680,   365],
        [    0,   292, 15156,  ...,    35,  3815,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2037e-04,  4.3834e-04, -1.8500e-04,  ..., -1.4811e-05,
         -9.6439e-04, -4.0982e-04],
        [-6.9514e-06, -4.2878e-06,  5.0142e-06,  ..., -5.7742e-06,
         -4.2841e-06, -5.1223e-06],
        [-2.1636e-05, -1.3903e-05,  1.6809e-05,  ..., -1.7703e-05,
         -1.3232e-05, -1.5676e-05],
        [-1.5303e-05, -9.2685e-06,  1.1548e-05,  ..., -1.2711e-05,
         -9.4250e-06, -1.1772e-05],
        [-1.7062e-05, -1.1295e-05,  1.2800e-05,  ..., -1.4231e-05,
         -1.1191e-05, -1.1519e-05]], device='cuda:0')
Loss: 0.9164523482322693


Running epoch 1, step 1677, batch 629
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0, 2546,  300,  ...,   14, 1075,  756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5792e-04,  5.0012e-04, -6.8835e-05,  ..., -1.0497e-04,
         -1.0586e-03, -2.8626e-04],
        [-8.2776e-06, -5.1744e-06,  5.9679e-06,  ..., -6.9439e-06,
         -5.1074e-06, -6.1430e-06],
        [-2.5779e-05, -1.6779e-05,  2.0042e-05,  ..., -2.1309e-05,
         -1.5765e-05, -1.8775e-05],
        [-1.8239e-05, -1.1191e-05,  1.3769e-05,  ..., -1.5333e-05,
         -1.1250e-05, -1.4156e-05],
        [-2.0340e-05, -1.3635e-05,  1.5274e-05,  ..., -1.7121e-05,
         -1.3322e-05, -1.3784e-05]], device='cuda:0')
Loss: 0.9457381367683411


Running epoch 1, step 1678, batch 630
Sampled inputs[:2]: tensor([[   0, 1774, 1781,  ..., 4685,  409, 4614],
        [   0,  421, 6007,  ...,  408, 2105,  843]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7090e-04,  4.6731e-04,  3.8603e-06,  ..., -3.3393e-05,
         -9.8192e-04, -2.6651e-04],
        [-9.5740e-06, -6.0350e-06,  7.0855e-06,  ..., -8.0243e-06,
         -5.8562e-06, -7.0520e-06],
        [-2.9922e-05, -1.9640e-05,  2.3827e-05,  ..., -2.4736e-05,
         -1.8150e-05, -2.1696e-05],
        [-2.1294e-05, -1.3158e-05,  1.6510e-05,  ..., -1.7866e-05,
         -1.2994e-05, -1.6406e-05],
        [-2.3365e-05, -1.5795e-05,  1.7956e-05,  ..., -1.9670e-05,
         -1.5184e-05, -1.5765e-05]], device='cuda:0')
Loss: 0.9548364281654358


Running epoch 1, step 1679, batch 631
Sampled inputs[:2]: tensor([[    0,  1136,   944,  ...,   401, 13771,    12],
        [    0,   266,  1527,  ...,  2525,    14, 11570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9272e-04,  4.1306e-04, -1.9149e-04,  ...,  2.6264e-06,
         -9.6886e-04, -3.2348e-04],
        [-1.0975e-05, -6.9551e-06,  8.0764e-06,  ..., -9.1940e-06,
         -6.6869e-06, -8.0951e-06],
        [-3.4213e-05, -2.2560e-05,  2.7180e-05,  ..., -2.8238e-05,
         -2.0698e-05, -2.4736e-05],
        [-2.4348e-05, -1.5140e-05,  1.8775e-05,  ..., -2.0415e-05,
         -1.4819e-05, -1.8775e-05],
        [-2.6748e-05, -1.8165e-05,  2.0519e-05,  ..., -2.2501e-05,
         -1.7360e-05, -1.8001e-05]], device='cuda:0')
Loss: 0.9465946555137634
Graident accumulation at epoch 1, step 1679, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4558e-04,  2.1376e-04, -1.0426e-04,  ..., -2.3383e-06,
         -1.4805e-04, -4.8987e-05],
        [-7.6747e-06, -5.3954e-06,  5.8528e-06,  ..., -6.7808e-06,
         -4.2883e-06, -5.0927e-06],
        [ 2.1876e-05,  3.0404e-05, -1.6822e-05,  ...,  2.0735e-05,
          2.6908e-05,  3.9573e-06],
        [ 3.3304e-05,  4.4826e-05, -2.6818e-05,  ...,  3.9359e-05,
          3.8692e-05,  1.9959e-05],
        [-2.6362e-05, -1.9198e-05,  2.0210e-05,  ..., -2.2356e-05,
         -1.8297e-05, -1.7830e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4812e-08, 5.7378e-08, 6.7482e-08,  ..., 2.7109e-08, 1.5758e-07,
         3.9381e-08],
        [9.0119e-11, 5.5109e-11, 2.5997e-11,  ..., 6.1235e-11, 2.6387e-11,
         3.4379e-11],
        [4.1816e-09, 2.8033e-09, 1.6843e-09,  ..., 3.3881e-09, 1.6627e-09,
         1.1551e-09],
        [1.2857e-09, 1.4284e-09, 5.6123e-10,  ..., 1.1348e-09, 9.0162e-10,
         4.8831e-10],
        [4.0985e-10, 2.3335e-10, 9.2371e-11,  ..., 3.0150e-10, 9.6080e-11,
         1.1702e-10]], device='cuda:0')
optimizer state dict: 210.0
lr: [1.995833666043061e-06, 1.995833666043061e-06]
scheduler_last_epoch: 210


Running epoch 1, step 1680, batch 632
Sampled inputs[:2]: tensor([[   0,  275,  467,  ...,  298,  365, 2714],
        [   0, 6294,  367,  ...,  496,   14,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0137e-04,  8.4911e-06,  5.6397e-05,  ..., -5.0876e-05,
          1.7903e-04,  4.6409e-05],
        [-1.4603e-06, -9.2760e-07,  8.6427e-07,  ..., -1.2368e-06,
         -9.0897e-07, -1.0431e-06],
        [-4.4703e-06, -2.9653e-06,  2.9057e-06,  ..., -3.7104e-06,
         -2.7418e-06, -3.1441e-06],
        [-3.2037e-06, -1.9819e-06,  1.9670e-06,  ..., -2.7120e-06,
         -1.9819e-06, -2.3991e-06],
        [-3.6657e-06, -2.5034e-06,  2.3097e-06,  ..., -3.0994e-06,
         -2.3991e-06, -2.3991e-06]], device='cuda:0')
Loss: 0.9731042385101318


Running epoch 1, step 1681, batch 633
Sampled inputs[:2]: tensor([[   0,  516,  596,  ..., 3109,  287,  394],
        [   0,  278, 4191,  ...,  381, 3020,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9206e-04,  2.1495e-05,  1.0254e-04,  ..., -1.0733e-04,
          5.0373e-04,  1.1455e-04],
        [-2.8387e-06, -1.9260e-06,  1.8068e-06,  ..., -2.4810e-06,
         -1.8589e-06, -2.0489e-06],
        [-8.6129e-06, -6.0350e-06,  5.9605e-06,  ..., -7.4059e-06,
         -5.5581e-06, -6.1095e-06],
        [-6.2287e-06, -4.1276e-06,  4.1127e-06,  ..., -5.4687e-06,
         -4.0680e-06, -4.7088e-06],
        [-7.0930e-06, -5.0962e-06,  4.7833e-06,  ..., -6.1691e-06,
         -4.8429e-06, -4.6641e-06]], device='cuda:0')
Loss: 0.9743096828460693


Running epoch 1, step 1682, batch 634
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3050e-04,  3.0230e-05,  2.0549e-04,  ..., -2.1749e-04,
          5.5293e-04,  2.4496e-04],
        [-4.2170e-06, -2.9393e-06,  2.8498e-06,  ..., -3.6061e-06,
         -2.7642e-06, -2.9504e-06],
        [ 3.2724e-05,  6.0974e-05, -5.2929e-05,  ...,  3.4938e-05,
          6.0142e-05,  7.6787e-06],
        [-9.3430e-06, -6.4075e-06,  6.6012e-06,  ..., -8.0168e-06,
         -6.1095e-06, -6.8545e-06],
        [-1.0401e-05, -7.7933e-06,  7.3761e-06,  ..., -8.9556e-06,
         -7.2271e-06, -6.7204e-06]], device='cuda:0')
Loss: 0.988735556602478


Running epoch 1, step 1683, batch 635
Sampled inputs[:2]: tensor([[    0,   892,   271,  ...,   278,   266, 10237],
        [    0,   422,    14,  ...,   271,  1360,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3305e-04,  1.2992e-04,  3.0021e-04,  ..., -2.0260e-04,
          7.0589e-04,  3.0053e-04],
        [-5.6177e-06, -3.8445e-06,  3.7514e-06,  ..., -4.8801e-06,
         -3.7476e-06, -4.0382e-06],
        [ 2.8462e-05,  5.8143e-05, -4.9934e-05,  ...,  3.1168e-05,
          5.7207e-05,  4.5196e-06],
        [-1.2308e-05, -8.2552e-06,  8.5980e-06,  ..., -1.0699e-05,
         -8.1658e-06, -9.2238e-06],
        [-1.4096e-05, -1.0297e-05,  9.8944e-06,  ..., -1.2264e-05,
         -9.8944e-06, -9.2834e-06]], device='cuda:0')
Loss: 0.927287757396698


Running epoch 1, step 1684, batch 636
Sampled inputs[:2]: tensor([[    0,   879,    27,  ...,    13,  2764,  3860],
        [    0,  6192,   266,  ...,  3318,  9872, 10931]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5689e-04,  1.5137e-04,  3.4518e-04,  ..., -1.1929e-04,
          8.9424e-04,  3.9998e-04],
        [-7.0259e-06, -4.8429e-06,  4.6454e-06,  ..., -6.0797e-06,
         -4.6827e-06, -5.0440e-06],
        [ 2.4141e-05,  5.4894e-05, -4.6939e-05,  ...,  2.7487e-05,
          5.4301e-05,  1.4500e-06],
        [-1.5438e-05, -1.0431e-05,  1.0669e-05,  ..., -1.3381e-05,
         -1.0267e-05, -1.1593e-05],
        [-1.7583e-05, -1.2994e-05,  1.2234e-05,  ..., -1.5274e-05,
         -1.2398e-05, -1.1608e-05]], device='cuda:0')
Loss: 0.9857906699180603


Running epoch 1, step 1685, batch 637
Sampled inputs[:2]: tensor([[    0,   957,  1357,  ..., 26179,   287,  6458],
        [    0,   635,    13,  ...,    13,  4710,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9622e-04,  1.3475e-04,  3.0183e-04,  ..., -1.8122e-04,
          1.0636e-03,  4.9705e-04],
        [-8.3670e-06, -5.8413e-06,  5.7183e-06,  ..., -7.1898e-06,
         -5.5544e-06, -5.9344e-06],
        [ 1.9909e-05,  5.1601e-05, -4.3347e-05,  ...,  2.3941e-05,
          5.1530e-05, -1.3961e-06],
        [-1.8463e-05, -1.2636e-05,  1.3188e-05,  ..., -1.5900e-05,
         -1.2234e-05, -1.3724e-05],
        [-2.0847e-05, -1.5631e-05,  1.4916e-05,  ..., -1.8045e-05,
         -1.4678e-05, -1.3635e-05]], device='cuda:0')
Loss: 0.9967031478881836


Running epoch 1, step 1686, batch 638
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0,  2851,  5442,  ..., 38820,    14,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5309e-04,  1.5648e-04,  4.3188e-04,  ..., -1.6397e-04,
          1.2835e-03,  4.4231e-04],
        [-9.6560e-06, -6.8545e-06,  6.6534e-06,  ..., -8.2925e-06,
         -6.4783e-06, -6.8620e-06],
        [ 1.5796e-05,  4.8204e-05, -4.0144e-05,  ...,  2.0424e-05,
          4.8564e-05, -4.3168e-06],
        [-2.1353e-05, -1.4886e-05,  1.5363e-05,  ..., -1.8388e-05,
         -1.4335e-05, -1.5914e-05],
        [-2.4006e-05, -1.8314e-05,  1.7330e-05,  ..., -2.0787e-05,
         -1.7092e-05, -1.5721e-05]], device='cuda:0')
Loss: 0.9829491972923279


Running epoch 1, step 1687, batch 639
Sampled inputs[:2]: tensor([[    0,  2165,  9311,  ..., 10570,   437,   266],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6561e-04,  9.8269e-05,  2.4777e-04,  ..., -2.1742e-04,
          1.2184e-03,  4.0154e-04],
        [-1.0893e-05, -7.8231e-06,  7.6033e-06,  ..., -9.4026e-06,
         -7.3202e-06, -7.7561e-06],
        [ 1.1863e-05,  4.5059e-05, -3.6880e-05,  ...,  1.6982e-05,
          4.5927e-05, -7.0735e-06],
        [-2.4155e-05, -1.7032e-05,  1.7643e-05,  ..., -2.0906e-05,
         -1.6242e-05, -1.8060e-05],
        [-2.7016e-05, -2.0817e-05,  1.9759e-05,  ..., -2.3469e-05,
         -1.9252e-05, -1.7658e-05]], device='cuda:0')
Loss: 0.9395900368690491
Graident accumulation at epoch 1, step 1687, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0230,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.4463e-05,  2.0221e-04, -6.9052e-05,  ..., -2.3847e-05,
         -1.1401e-05, -3.9349e-06],
        [-7.9965e-06, -5.6381e-06,  6.0279e-06,  ..., -7.0430e-06,
         -4.5915e-06, -5.3590e-06],
        [ 2.0875e-05,  3.1870e-05, -1.8828e-05,  ...,  2.0360e-05,
          2.8810e-05,  2.8542e-06],
        [ 2.7558e-05,  3.8640e-05, -2.2372e-05,  ...,  3.3332e-05,
          3.3199e-05,  1.6157e-05],
        [-2.6428e-05, -1.9360e-05,  2.0165e-05,  ..., -2.2467e-05,
         -1.8392e-05, -1.7813e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5506e-08, 5.7331e-08, 6.7476e-08,  ..., 2.7129e-08, 1.5890e-07,
         3.9502e-08],
        [9.0148e-11, 5.5116e-11, 2.6029e-11,  ..., 6.1263e-11, 2.6414e-11,
         3.4404e-11],
        [4.1776e-09, 2.8026e-09, 1.6840e-09,  ..., 3.3850e-09, 1.6632e-09,
         1.1539e-09],
        [1.2850e-09, 1.4272e-09, 5.6098e-10,  ..., 1.1341e-09, 9.0098e-10,
         4.8814e-10],
        [4.1017e-10, 2.3355e-10, 9.2669e-11,  ..., 3.0175e-10, 9.6355e-11,
         1.1722e-10]], device='cuda:0')
optimizer state dict: 211.0
lr: [1.922345857645641e-06, 1.922345857645641e-06]
scheduler_last_epoch: 211
Epoch 1 | Batch 639/1048 | Training PPL: 1838.5275670637172 | time 66.50040531158447
Saving checkpoint at epoch 1, step 1687, batch 639
Epoch 1 | Validation PPL: 6.757836703691655 | Learning rate: 1.922345857645641e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1687, AFTER epoch 1, step 1687


Running epoch 1, step 1688, batch 640
Sampled inputs[:2]: tensor([[   0,  259, 3022,  ...,  437, 5100, 1782],
        [   0,  417,  199,  ..., 8762, 4204,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7195e-04, -1.2378e-04, -3.1093e-04,  ..., -2.3106e-04,
         -1.8671e-04, -2.5263e-04],
        [-1.2442e-06, -8.7917e-07,  9.3877e-07,  ..., -1.1101e-06,
         -7.9349e-07, -8.9407e-07],
        [-3.7998e-06, -2.7716e-06,  3.1441e-06,  ..., -3.3081e-06,
         -2.3842e-06, -2.5928e-06],
        [-2.6971e-06, -1.8477e-06,  2.1756e-06,  ..., -2.4140e-06,
         -1.7285e-06, -2.0564e-06],
        [-3.0249e-06, -2.2948e-06,  2.4140e-06,  ..., -2.6673e-06,
         -2.0266e-06, -1.8775e-06]], device='cuda:0')
Loss: 0.919273316860199


Running epoch 1, step 1689, batch 641
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0, 1167,  278,  ...,  278, 1853, 1424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6488e-04, -1.0747e-04, -3.0554e-04,  ..., -1.7535e-04,
         -3.5092e-04, -1.0222e-04],
        [-2.5928e-06, -1.8552e-06,  2.0713e-06,  ..., -2.2128e-06,
         -1.6280e-06, -1.7695e-06],
        [-8.1211e-06, -6.0052e-06,  6.9439e-06,  ..., -6.8545e-06,
         -5.0664e-06, -5.4836e-06],
        [-5.7966e-06, -4.0531e-06,  4.8876e-06,  ..., -4.9323e-06,
         -3.6135e-06, -4.1872e-06],
        [-6.2138e-06, -4.8131e-06,  5.1558e-06,  ..., -5.3495e-06,
         -4.2021e-06, -3.8594e-06]], device='cuda:0')
Loss: 0.9748029112815857


Running epoch 1, step 1690, batch 642
Sampled inputs[:2]: tensor([[   0,  709,  630,  ..., 6263,  409,  508],
        [   0, 7070,   86,  ...,  298, 4930,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5779e-04, -7.5509e-05, -4.2108e-04,  ..., -2.9600e-04,
         -4.2616e-04, -1.7658e-04],
        [-3.9190e-06, -2.7195e-06,  3.1218e-06,  ..., -3.2708e-06,
         -2.3358e-06, -2.6301e-06],
        [-1.2383e-05, -8.9258e-06,  1.0580e-05,  ..., -1.0252e-05,
         -7.3612e-06, -8.2552e-06],
        [-8.8364e-06, -6.0052e-06,  7.4357e-06,  ..., -7.3463e-06,
         -5.2154e-06, -6.2734e-06],
        [-9.2536e-06, -7.0035e-06,  7.6890e-06,  ..., -7.8380e-06,
         -6.0126e-06, -5.6922e-06]], device='cuda:0')
Loss: 0.9492796063423157


Running epoch 1, step 1691, batch 643
Sampled inputs[:2]: tensor([[    0,    43,   527,  ...,  4309,    14,  8050],
        [    0,    55,  2258,  ..., 32764,    75,   338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9566e-04, -4.6792e-05, -5.5461e-04,  ..., -3.1005e-04,
         -3.5492e-04, -1.9384e-04],
        [-5.2899e-06, -3.6098e-06,  3.9674e-06,  ..., -4.4480e-06,
         -3.2187e-06, -3.6284e-06],
        [-1.6853e-05, -1.1981e-05,  1.3649e-05,  ..., -1.4052e-05,
         -1.0207e-05, -1.1489e-05],
        [-1.1981e-05, -8.0317e-06,  9.4771e-06,  ..., -1.0028e-05,
         -7.2271e-06, -8.6576e-06],
        [-1.2681e-05, -9.4026e-06,  9.9838e-06,  ..., -1.0803e-05,
         -8.3223e-06, -8.0019e-06]], device='cuda:0')
Loss: 0.96004319190979


Running epoch 1, step 1692, batch 644
Sampled inputs[:2]: tensor([[    0,  2732,   413,  ...,   287,   266,  3668],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7376e-04, -1.3186e-04, -5.8304e-04,  ..., -3.0886e-04,
         -2.8678e-04, -2.9857e-04],
        [-6.5863e-06, -4.4703e-06,  4.9286e-06,  ..., -5.5805e-06,
         -4.0606e-06, -4.5896e-06],
        [-2.0906e-05, -1.4767e-05,  1.6928e-05,  ..., -1.7524e-05,
         -1.2815e-05, -1.4395e-05],
        [-1.4946e-05, -9.9242e-06,  1.1787e-05,  ..., -1.2621e-05,
         -9.1344e-06, -1.0937e-05],
        [-1.5810e-05, -1.1638e-05,  1.2413e-05,  ..., -1.3545e-05,
         -1.0483e-05, -1.0058e-05]], device='cuda:0')
Loss: 0.9677952527999878


Running epoch 1, step 1693, batch 645
Sampled inputs[:2]: tensor([[   0, 3380, 1197,  ...,  631,  369, 3123],
        [   0,  461,  654,  ..., 6548, 7171,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9847e-04, -2.0634e-04, -6.6877e-04,  ..., -3.5192e-04,
         -4.3146e-04, -3.6484e-04],
        [-7.9274e-06, -5.4464e-06,  6.0536e-06,  ..., -6.6608e-06,
         -4.8839e-06, -5.4799e-06],
        [-2.5257e-05, -1.8075e-05,  2.0772e-05,  ..., -2.1040e-05,
         -1.5527e-05, -1.7315e-05],
        [-1.8016e-05, -1.2130e-05,  1.4469e-05,  ..., -1.5080e-05,
         -1.1012e-05, -1.3083e-05],
        [-1.9014e-05, -1.4201e-05,  1.5184e-05,  ..., -1.6212e-05,
         -1.2659e-05, -1.2070e-05]], device='cuda:0')
Loss: 0.9848836660385132


Running epoch 1, step 1694, batch 646
Sampled inputs[:2]: tensor([[    0,   298,   452,  ..., 41263,     9,   367],
        [    0,    45,  6556,  ...,  1477,   352,  1611]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9336e-04, -6.7020e-05, -4.3753e-04,  ..., -4.8455e-04,
         -2.8583e-05, -3.8897e-04],
        [-9.4473e-06, -6.4671e-06,  6.8098e-06,  ..., -8.0243e-06,
         -5.9940e-06, -6.6347e-06],
        [-3.0056e-05, -2.1487e-05,  2.3425e-05,  ..., -2.5332e-05,
         -1.9103e-05, -2.0921e-05],
        [-2.1264e-05, -1.4290e-05,  1.6131e-05,  ..., -1.8030e-05,
         -1.3441e-05, -1.5646e-05],
        [-2.3067e-05, -1.7151e-05,  1.7390e-05,  ..., -1.9908e-05,
         -1.5818e-05, -1.4946e-05]], device='cuda:0')
Loss: 0.9683493971824646


Running epoch 1, step 1695, batch 647
Sampled inputs[:2]: tensor([[    0,  1042,  5738,  ...,    12,   287,  3643],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3201e-04, -1.2453e-04, -5.8150e-04,  ..., -4.7782e-04,
          3.2617e-05, -4.1977e-04],
        [-1.0714e-05, -7.3276e-06,  7.7523e-06,  ..., -9.1344e-06,
         -6.8285e-06, -7.5214e-06],
        [-3.3930e-05, -2.4214e-05,  2.6569e-05,  ..., -2.8655e-05,
         -2.1592e-05, -2.3574e-05],
        [-2.4110e-05, -1.6168e-05,  1.8381e-05,  ..., -2.0504e-05,
         -1.5296e-05, -1.7732e-05],
        [-2.6122e-05, -1.9401e-05,  1.9789e-05,  ..., -2.2590e-05,
         -1.7948e-05, -1.6853e-05]], device='cuda:0')
Loss: 0.9392734169960022
Graident accumulation at epoch 1, step 1695, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3218e-05,  1.6954e-04, -1.2030e-04,  ..., -6.9244e-05,
         -6.9990e-06, -4.5518e-05],
        [-8.2682e-06, -5.8071e-06,  6.2003e-06,  ..., -7.2521e-06,
         -4.8152e-06, -5.5752e-06],
        [ 1.5394e-05,  2.6261e-05, -1.4289e-05,  ...,  1.5458e-05,
          2.3770e-05,  2.1143e-07],
        [ 2.2392e-05,  3.3159e-05, -1.8297e-05,  ...,  2.7949e-05,
          2.8349e-05,  1.2768e-05],
        [-2.6397e-05, -1.9364e-05,  2.0128e-05,  ..., -2.2480e-05,
         -1.8348e-05, -1.7717e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5561e-08, 5.7289e-08, 6.7747e-08,  ..., 2.7330e-08, 1.5874e-07,
         3.9639e-08],
        [9.0172e-11, 5.5114e-11, 2.6063e-11,  ..., 6.1285e-11, 2.6435e-11,
         3.4427e-11],
        [4.1745e-09, 2.8003e-09, 1.6830e-09,  ..., 3.3824e-09, 1.6620e-09,
         1.1534e-09],
        [1.2843e-09, 1.4261e-09, 5.6076e-10,  ..., 1.1334e-09, 9.0031e-10,
         4.8797e-10],
        [4.1044e-10, 2.3369e-10, 9.2968e-11,  ..., 3.0196e-10, 9.6581e-11,
         1.1738e-10]], device='cuda:0')
optimizer state dict: 212.0
lr: [1.8500923853120123e-06, 1.8500923853120123e-06]
scheduler_last_epoch: 212


Running epoch 1, step 1696, batch 648
Sampled inputs[:2]: tensor([[    0,  6112,   278,  ...,  4092,   490,  2774],
        [    0, 15912,    14,  ..., 25535,    18,  3947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6663e-05,  1.2218e-04,  3.7812e-05,  ..., -2.3489e-04,
         -5.4621e-05, -1.1141e-04],
        [-1.2890e-06, -9.6858e-07,  1.1101e-06,  ..., -1.0878e-06,
         -7.7114e-07, -8.1956e-07],
        [ 6.1487e-05,  8.5994e-05, -3.9339e-05,  ...,  6.3186e-05,
          8.2645e-05,  3.6206e-05],
        [-2.9057e-06, -2.1607e-06,  2.6375e-06,  ..., -2.4587e-06,
         -1.7285e-06, -1.9670e-06],
        [-2.9504e-06, -2.4587e-06,  2.6375e-06,  ..., -2.5928e-06,
         -1.9819e-06, -1.7881e-06]], device='cuda:0')
Loss: 0.9471162557601929


Running epoch 1, step 1697, batch 649
Sampled inputs[:2]: tensor([[   0,  897,  328,  ...,  908,  696,  688],
        [   0,  199, 5990,  ...,  278,  638, 5513]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3618e-05,  1.8537e-04,  4.8188e-05,  ..., -3.7963e-04,
         -3.2934e-04, -1.8931e-04],
        [-2.6599e-06, -1.8962e-06,  2.2054e-06,  ..., -2.2352e-06,
         -1.6205e-06, -1.7397e-06],
        [ 5.7284e-05,  8.3013e-05, -3.5733e-05,  ...,  5.9684e-05,
          8.0037e-05,  3.3390e-05],
        [-6.0499e-06, -4.2468e-06,  5.2899e-06,  ..., -5.0813e-06,
         -3.6806e-06, -4.2021e-06],
        [-6.0946e-06, -4.8131e-06,  5.2750e-06,  ..., -5.2750e-06,
         -4.0978e-06, -3.7402e-06]], device='cuda:0')
Loss: 0.9451223015785217


Running epoch 1, step 1698, batch 650
Sampled inputs[:2]: tensor([[   0, 3037, 4511,  ..., 1711,   12, 2655],
        [   0,   12,  271,  ...,   12,  298,  273]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8588e-06,  2.9203e-04, -1.7067e-05,  ..., -4.6884e-04,
         -3.1678e-04, -2.6766e-04],
        [-4.0680e-06, -2.7455e-06,  3.1665e-06,  ..., -3.4347e-06,
         -2.4140e-06, -2.7753e-06],
        [ 5.2874e-05,  8.0212e-05, -3.2425e-05,  ...,  5.5974e-05,
          7.7549e-05,  3.0216e-05],
        [-9.1493e-06, -6.1020e-06,  7.5251e-06,  ..., -7.7486e-06,
         -5.4389e-06, -6.5565e-06],
        [-9.4473e-06, -7.0035e-06,  7.6890e-06,  ..., -8.1509e-06,
         -6.1542e-06, -5.9903e-06]], device='cuda:0')
Loss: 0.9218975901603699


Running epoch 1, step 1699, batch 651
Sampled inputs[:2]: tensor([[    0,  7110,   278,  ...,    66,    13,  9070],
        [    0,  9423,   298,  ...,  5274, 37902,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1340e-06,  3.1788e-04, -1.6481e-05,  ..., -4.2774e-04,
         -3.1904e-04, -2.6950e-04],
        [-5.4985e-06, -3.6769e-06,  4.0866e-06,  ..., -4.6492e-06,
         -3.2783e-06, -3.8184e-06],
        [ 4.8493e-05,  7.7217e-05, -2.9325e-05,  ...,  5.2308e-05,
          7.4881e-05,  2.7087e-05],
        [-1.2144e-05, -8.0392e-06,  9.5218e-06,  ..., -1.0312e-05,
         -7.2718e-06, -8.8215e-06],
        [-1.3009e-05, -9.4920e-06,  1.0133e-05,  ..., -1.1176e-05,
         -8.4788e-06, -8.3447e-06]], device='cuda:0')
Loss: 0.9838586449623108


Running epoch 1, step 1700, batch 652
Sampled inputs[:2]: tensor([[    0,  7432,   287,  ...,    12,   461,  2652],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3386e-05,  3.0991e-04,  6.6837e-05,  ..., -3.5257e-04,
         -1.5129e-04, -1.2588e-04],
        [-6.9812e-06, -4.6976e-06,  5.0142e-06,  ..., -5.9232e-06,
         -4.2841e-06, -4.8764e-06],
        [ 4.3993e-05,  7.3968e-05, -2.6301e-05,  ...,  4.8463e-05,
          7.1797e-05,  2.3913e-05],
        [-1.5229e-05, -1.0155e-05,  1.1533e-05,  ..., -1.2979e-05,
         -9.3877e-06, -1.1101e-05],
        [-1.6823e-05, -1.2308e-05,  1.2636e-05,  ..., -1.4514e-05,
         -1.1235e-05, -1.0908e-05]], device='cuda:0')
Loss: 0.9989962577819824


Running epoch 1, step 1701, batch 653
Sampled inputs[:2]: tensor([[    0,   749,     9,  ...,  2756,    14,  1062],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0111e-05,  2.9379e-04, -4.2523e-05,  ..., -3.1005e-04,
         -1.3719e-04, -1.1117e-04],
        [-8.3745e-06, -5.6326e-06,  6.1020e-06,  ..., -7.0781e-06,
         -5.1185e-06, -5.8524e-06],
        [ 3.9671e-05,  7.0943e-05, -2.2709e-05,  ...,  4.4887e-05,
          6.9204e-05,  2.0903e-05],
        [-1.8388e-05, -1.2241e-05,  1.4096e-05,  ..., -1.5602e-05,
         -1.1273e-05, -1.3426e-05],
        [-2.0072e-05, -1.4678e-05,  1.5274e-05,  ..., -1.7256e-05,
         -1.3337e-05, -1.3009e-05]], device='cuda:0')
Loss: 0.9776871204376221


Running epoch 1, step 1702, batch 654
Sampled inputs[:2]: tensor([[   0, 6640,   13,  ...,  292,  221,  273],
        [   0, 2270, 3279,  ...,  380,  475,  768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1246e-04,  3.6115e-04, -2.0501e-04,  ..., -4.0661e-04,
         -2.6504e-04, -1.9104e-04],
        [-9.7081e-06, -6.5155e-06,  6.9514e-06,  ..., -8.3074e-06,
         -6.0089e-06, -6.8285e-06],
        [ 3.5648e-05,  6.8097e-05, -1.9833e-05,  ...,  4.1236e-05,
          6.6552e-05,  1.8027e-05],
        [-2.1189e-05, -1.4089e-05,  1.5974e-05,  ..., -1.8224e-05,
         -1.3173e-05, -1.5572e-05],
        [-2.3335e-05, -1.7032e-05,  1.7509e-05,  ..., -2.0280e-05,
         -1.5646e-05, -1.5154e-05]], device='cuda:0')
Loss: 0.9241324067115784


Running epoch 1, step 1703, batch 655
Sampled inputs[:2]: tensor([[    0,  1603,    27,  ..., 19959, 22776,   328],
        [    0,   380,   341,  ...,   955,   644,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7157e-04,  4.0103e-04, -2.2922e-04,  ..., -2.7287e-04,
         -4.3587e-05, -4.2490e-05],
        [-1.1057e-05, -7.4692e-06,  7.9274e-06,  ..., -9.4026e-06,
         -6.8881e-06, -7.7374e-06],
        [ 1.0765e-04,  1.0528e-04, -4.8761e-05,  ...,  1.3124e-04,
          8.2772e-05,  7.5279e-05],
        [-2.4214e-05, -1.6190e-05,  1.8269e-05,  ..., -2.0668e-05,
         -1.5125e-05, -1.7717e-05],
        [-2.6479e-05, -1.9461e-05,  1.9878e-05,  ..., -2.2888e-05,
         -1.7852e-05, -1.7121e-05]], device='cuda:0')
Loss: 0.9547786116600037
Graident accumulation at epoch 1, step 1703, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.8739e-05,  1.9269e-04, -1.3119e-04,  ..., -8.9607e-05,
         -1.0658e-05, -4.5216e-05],
        [-8.5471e-06, -5.9733e-06,  6.3730e-06,  ..., -7.4672e-06,
         -5.0225e-06, -5.7915e-06],
        [ 2.4620e-05,  3.4164e-05, -1.7736e-05,  ...,  2.7036e-05,
          2.9670e-05,  7.7182e-06],
        [ 1.7731e-05,  2.8225e-05, -1.4640e-05,  ...,  2.3087e-05,
          2.4002e-05,  9.7192e-06],
        [-2.6405e-05, -1.9374e-05,  2.0103e-05,  ..., -2.2520e-05,
         -1.8298e-05, -1.7657e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5535e-08, 5.7392e-08, 6.7732e-08,  ..., 2.7377e-08, 1.5859e-07,
         3.9601e-08],
        [9.0204e-11, 5.5115e-11, 2.6100e-11,  ..., 6.1312e-11, 2.6456e-11,
         3.4452e-11],
        [4.1820e-09, 2.8086e-09, 1.6837e-09,  ..., 3.3962e-09, 1.6672e-09,
         1.1579e-09],
        [1.2836e-09, 1.4249e-09, 5.6053e-10,  ..., 1.1327e-09, 8.9964e-10,
         4.8780e-10],
        [4.1073e-10, 2.3384e-10, 9.3270e-11,  ..., 3.0218e-10, 9.6803e-11,
         1.1756e-10]], device='cuda:0')
optimizer state dict: 213.0
lr: [1.7790842900034565e-06, 1.7790842900034565e-06]
scheduler_last_epoch: 213


Running epoch 1, step 1704, batch 656
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,   199, 11296,  ...,   266, 10463,  8256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4718e-06, -1.0361e-05, -2.4037e-05,  ..., -1.6025e-06,
         -4.5406e-05,  1.3934e-04],
        [-1.3635e-06, -1.0207e-06,  9.6858e-07,  ..., -1.1474e-06,
         -8.9034e-07, -9.0897e-07],
        [-4.3809e-06, -3.4869e-06,  3.3528e-06,  ..., -3.7402e-06,
         -2.9504e-06, -2.9653e-06],
        [-3.1292e-06, -2.3395e-06,  2.3395e-06,  ..., -2.6524e-06,
         -2.0713e-06, -2.2203e-06],
        [-3.3379e-06, -2.7716e-06,  2.5034e-06,  ..., -2.9057e-06,
         -2.4140e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9897657036781311


Running epoch 1, step 1705, batch 657
Sampled inputs[:2]: tensor([[    0,  2923,   266,  ...,  7763,   360,  1255],
        [    0,    76,   472,  ..., 21215,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6923e-04, -1.2081e-04,  2.6603e-07,  ...,  4.7167e-04,
         -3.3542e-04, -4.8796e-05],
        [-2.6822e-06, -1.9334e-06,  1.8589e-06,  ..., -2.2501e-06,
         -1.6950e-06, -1.8217e-06],
        [-8.4937e-06, -6.4820e-06,  6.3926e-06,  ..., -7.1824e-06,
         -5.5134e-06, -5.7518e-06],
        [-6.1691e-06, -4.4405e-06,  4.4852e-06,  ..., -5.2452e-06,
         -3.9935e-06, -4.4554e-06],
        [-6.4522e-06, -5.1111e-06,  4.7684e-06,  ..., -5.5283e-06,
         -4.4852e-06, -4.0531e-06]], device='cuda:0')
Loss: 0.9698617458343506


Running epoch 1, step 1706, batch 658
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   376,   283,  ..., 29188,   292,  7627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9478e-05, -8.6781e-05,  1.1430e-04,  ...,  4.7718e-04,
         -1.9737e-04, -1.0018e-04],
        [-4.0531e-06, -2.9840e-06,  2.7306e-06,  ..., -3.4347e-06,
         -2.7232e-06, -2.7604e-06],
        [-1.2815e-05, -9.9540e-06,  9.3579e-06,  ..., -1.0908e-05,
         -8.7619e-06, -8.7023e-06],
        [-9.1791e-06, -6.7353e-06,  6.4671e-06,  ..., -7.8678e-06,
         -6.2734e-06, -6.6459e-06],
        [-9.9093e-06, -7.9721e-06,  7.0930e-06,  ..., -8.5533e-06,
         -7.1973e-06, -6.2436e-06]], device='cuda:0')
Loss: 1.0077954530715942


Running epoch 1, step 1707, batch 659
Sampled inputs[:2]: tensor([[    0,   587,   300,  ...,  4325,   278, 12564],
        [    0,   275,  1620,  ...,  3020,   278,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1858e-05, -6.8451e-05,  2.0257e-05,  ...,  3.6143e-04,
         -3.3266e-04, -3.3045e-04],
        [-5.3495e-06, -3.9078e-06,  3.7737e-06,  ..., -4.5300e-06,
         -3.5502e-06, -3.6135e-06],
        [-1.6928e-05, -1.2979e-05,  1.2904e-05,  ..., -1.4350e-05,
         -1.1355e-05, -1.1370e-05],
        [-1.2115e-05, -8.7768e-06,  8.9705e-06,  ..., -1.0341e-05,
         -8.1211e-06, -8.6874e-06],
        [-1.2964e-05, -1.0341e-05,  9.6560e-06,  ..., -1.1161e-05,
         -9.2834e-06, -8.0615e-06]], device='cuda:0')
Loss: 0.9672680497169495


Running epoch 1, step 1708, batch 660
Sampled inputs[:2]: tensor([[   0,  300, 5864,  ...,   12, 3667,  796],
        [   0, 7963,   17,  ...,   50,   13,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4768e-04, -1.4173e-04,  6.5323e-05,  ...,  2.6298e-04,
         -3.7848e-04, -4.2129e-04],
        [-6.6981e-06, -4.8764e-06,  4.7572e-06,  ..., -5.6773e-06,
         -4.4182e-06, -4.5449e-06],
        [-2.1309e-05, -1.6257e-05,  1.6332e-05,  ..., -1.8045e-05,
         -1.4156e-05, -1.4424e-05],
        [-1.5184e-05, -1.0923e-05,  1.1325e-05,  ..., -1.2919e-05,
         -1.0043e-05, -1.0908e-05],
        [-1.6227e-05, -1.2890e-05,  1.2159e-05,  ..., -1.3977e-05,
         -1.1548e-05, -1.0177e-05]], device='cuda:0')
Loss: 0.9714002013206482


Running epoch 1, step 1709, batch 661
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2861e-04, -7.8385e-05,  1.2708e-04,  ...,  1.9043e-04,
         -4.4559e-04, -4.5337e-04],
        [-8.0317e-06, -5.8301e-06,  5.7109e-06,  ..., -6.8173e-06,
         -5.2936e-06, -5.4948e-06],
        [-2.5600e-05, -1.9461e-05,  1.9655e-05,  ..., -2.1681e-05,
         -1.6928e-05, -1.7479e-05],
        [-1.8179e-05, -1.3039e-05,  1.3575e-05,  ..., -1.5482e-05,
         -1.1981e-05, -1.3143e-05],
        [-1.9476e-05, -1.5393e-05,  1.4573e-05,  ..., -1.6779e-05,
         -1.3798e-05, -1.2308e-05]], device='cuda:0')
Loss: 0.9584112167358398


Running epoch 1, step 1710, batch 662
Sampled inputs[:2]: tensor([[    0,  5160,   278,  ...,   496,    14, 46919],
        [    0,  6416,   367,  ...,   496,    14,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8612e-05, -8.0062e-05,  1.5938e-04,  ...,  1.7576e-04,
         -3.9251e-04, -5.2350e-04],
        [-9.4771e-06, -6.9030e-06,  6.6496e-06,  ..., -8.0615e-06,
         -6.3367e-06, -6.4410e-06],
        [-2.9922e-05, -2.2858e-05,  2.2650e-05,  ..., -2.5436e-05,
         -2.0117e-05, -2.0310e-05],
        [-2.1338e-05, -1.5378e-05,  1.5691e-05,  ..., -1.8239e-05,
         -1.4320e-05, -1.5333e-05],
        [-2.3022e-05, -1.8284e-05,  1.7002e-05,  ..., -1.9908e-05,
         -1.6555e-05, -1.4514e-05]], device='cuda:0')
Loss: 1.0313714742660522


Running epoch 1, step 1711, batch 663
Sampled inputs[:2]: tensor([[    0,   508, 22318,  ...,    13,  1107,  4093],
        [    0,   278, 19142,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8742e-05, -8.0840e-07, -3.0214e-07,  ...,  1.1673e-04,
         -3.3865e-04, -5.6744e-04],
        [-1.0923e-05, -7.7672e-06,  7.7523e-06,  ..., -9.1866e-06,
         -7.0930e-06, -7.4245e-06],
        [-3.4332e-05, -2.5749e-05,  2.6330e-05,  ..., -2.8908e-05,
         -2.2531e-05, -2.3365e-05],
        [-2.4572e-05, -1.7300e-05,  1.8328e-05,  ..., -2.0772e-05,
         -1.6041e-05, -1.7673e-05],
        [-2.6256e-05, -2.0504e-05,  1.9625e-05,  ..., -2.2531e-05,
         -1.8507e-05, -1.6585e-05]], device='cuda:0')
Loss: 0.9417966604232788
Graident accumulation at epoch 1, step 1711, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.2740e-05,  1.7334e-04, -1.1810e-04,  ..., -6.8973e-05,
         -4.3457e-05, -9.7438e-05],
        [-8.7846e-06, -6.1527e-06,  6.5109e-06,  ..., -7.6391e-06,
         -5.2295e-06, -5.9548e-06],
        [ 1.8725e-05,  2.8172e-05, -1.3329e-05,  ...,  2.1442e-05,
          2.4450e-05,  4.6098e-06],
        [ 1.3501e-05,  2.3672e-05, -1.1344e-05,  ...,  1.8701e-05,
          1.9998e-05,  6.9800e-06],
        [-2.6390e-05, -1.9487e-05,  2.0055e-05,  ..., -2.2521e-05,
         -1.8319e-05, -1.7550e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5487e-08, 5.7335e-08, 6.7664e-08,  ..., 2.7364e-08, 1.5854e-07,
         3.9884e-08],
        [9.0233e-11, 5.5120e-11, 2.6134e-11,  ..., 6.1335e-11, 2.6480e-11,
         3.4473e-11],
        [4.1790e-09, 2.8065e-09, 1.6827e-09,  ..., 3.3937e-09, 1.6660e-09,
         1.1573e-09],
        [1.2829e-09, 1.4238e-09, 5.6031e-10,  ..., 1.1320e-09, 8.9900e-10,
         4.8762e-10],
        [4.1101e-10, 2.3402e-10, 9.3562e-11,  ..., 3.0239e-10, 9.7048e-11,
         1.1772e-10]], device='cuda:0')
optimizer state dict: 214.0
lr: [1.7093324223767748e-06, 1.7093324223767748e-06]
scheduler_last_epoch: 214


Running epoch 1, step 1712, batch 664
Sampled inputs[:2]: tensor([[    0, 24440,  1918,  ...,   769,  1254,   596],
        [    0,  1471,   266,  ...,   525,  5202,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2055e-05,  3.1995e-05, -3.1662e-05,  ..., -1.6661e-05,
          1.0149e-04, -8.0286e-05],
        [-1.4082e-06, -8.6799e-07,  9.6858e-07,  ..., -1.1548e-06,
         -7.7859e-07, -9.7603e-07],
        [-4.4703e-06, -2.8759e-06,  3.3528e-06,  ..., -3.5912e-06,
         -2.4438e-06, -3.0249e-06],
        [-3.1590e-06, -1.8850e-06,  2.2650e-06,  ..., -2.5630e-06,
         -1.7211e-06, -2.2650e-06],
        [-3.3379e-06, -2.2352e-06,  2.4289e-06,  ..., -2.7418e-06,
         -1.9968e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9358060359954834


Running epoch 1, step 1713, batch 665
Sampled inputs[:2]: tensor([[    0, 15165,   287,  ..., 15049,   278,   266],
        [    0, 10251,   278,  ...,   278,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1878e-04,  3.4193e-05, -1.4379e-04,  ...,  1.5335e-05,
          2.3170e-04, -1.9047e-04],
        [-2.7195e-06, -1.7807e-06,  2.0191e-06,  ..., -2.2352e-06,
         -1.5907e-06, -1.8217e-06],
        [-8.4937e-06, -5.8413e-06,  6.8098e-06,  ..., -6.9439e-06,
         -5.0068e-06, -5.6177e-06],
        [-6.1542e-06, -3.9414e-06,  4.7982e-06,  ..., -5.0366e-06,
         -3.5986e-06, -4.2915e-06],
        [-6.3628e-06, -4.5747e-06,  4.9621e-06,  ..., -5.3048e-06,
         -4.0680e-06, -3.8669e-06]], device='cuda:0')
Loss: 0.9658870697021484


Running epoch 1, step 1714, batch 666
Sampled inputs[:2]: tensor([[    0,   266,   824,  ...,  1799,   287,  6250],
        [    0,   609,    12,  ...,   409, 11041,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1916e-04,  3.1312e-05, -7.5668e-05,  ...,  9.4137e-05,
          4.9404e-04, -1.7790e-04],
        [-3.9712e-06, -2.7157e-06,  2.8946e-06,  ..., -3.3677e-06,
         -2.4922e-06, -2.7716e-06],
        [-1.2487e-05, -8.8364e-06,  9.8348e-06,  ..., -1.0461e-05,
         -7.7784e-06, -8.5384e-06],
        [-8.9556e-06, -5.9530e-06,  6.8247e-06,  ..., -7.5698e-06,
         -5.5805e-06, -6.4969e-06],
        [-9.5367e-06, -7.0184e-06,  7.3165e-06,  ..., -8.1509e-06,
         -6.4075e-06, -6.0275e-06]], device='cuda:0')
Loss: 0.9793531894683838


Running epoch 1, step 1715, batch 667
Sampled inputs[:2]: tensor([[    0,  1403,    12,  ...,  1062,  2283, 13614],
        [    0,   341, 22766,  ...,   271,   266,  1176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9016e-04,  6.1815e-05, -1.6201e-04,  ...,  1.0177e-04,
          4.7169e-04, -6.3102e-05],
        [-5.5283e-06, -3.7290e-06,  3.7998e-06,  ..., -4.7162e-06,
         -3.5204e-06, -3.9712e-06],
        [-1.7226e-05, -1.2130e-05,  1.2860e-05,  ..., -1.4573e-05,
         -1.0997e-05, -1.2204e-05],
        [-1.2070e-05, -7.9796e-06,  8.7097e-06,  ..., -1.0297e-05,
         -7.6815e-06, -8.9705e-06],
        [-1.3709e-05, -9.9540e-06,  9.8944e-06,  ..., -1.1832e-05,
         -9.3877e-06, -9.0823e-06]], device='cuda:0')
Loss: 0.9012226462364197


Running epoch 1, step 1716, batch 668
Sampled inputs[:2]: tensor([[   0,  360,  508,  ...,  259,  554, 1319],
        [   0, 1850,  311,  ..., 3655, 3133, 9000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7929e-05, -2.6454e-05, -3.0628e-04,  ..., -2.1672e-05,
          4.8077e-04, -2.2167e-05],
        [-6.8992e-06, -4.6976e-06,  4.9025e-06,  ..., -5.8487e-06,
         -4.3288e-06, -4.8168e-06],
        [ 2.7046e-04,  3.2812e-04, -2.1169e-04,  ...,  2.0341e-04,
          2.7107e-04,  7.0715e-05],
        [-1.5154e-05, -1.0140e-05,  1.1317e-05,  ..., -1.2845e-05,
         -9.5218e-06, -1.0967e-05],
        [-1.6853e-05, -1.2442e-05,  1.2547e-05,  ..., -1.4514e-05,
         -1.1459e-05, -1.0893e-05]], device='cuda:0')
Loss: 0.9782575368881226


Running epoch 1, step 1717, batch 669
Sampled inputs[:2]: tensor([[    0,   521,   486,  ...,   278, 25182,   271],
        [    0,    14,   475,  ...,  2117,  2792, 12848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5250e-05,  4.3670e-05, -3.1830e-04,  ...,  3.6970e-05,
          3.0382e-04, -3.1068e-05],
        [-8.2403e-06, -5.5656e-06,  5.8226e-06,  ..., -7.0184e-06,
         -5.1446e-06, -5.8152e-06],
        [ 2.6632e-04,  3.2535e-04, -2.0853e-04,  ...,  1.9990e-04,
          2.6863e-04,  6.7765e-05],
        [-1.8120e-05, -1.2003e-05,  1.3493e-05,  ..., -1.5423e-05,
         -1.1310e-05, -1.3262e-05],
        [-2.0102e-05, -1.4693e-05,  1.4901e-05,  ..., -1.7315e-05,
         -1.3530e-05, -1.3009e-05]], device='cuda:0')
Loss: 0.9428280591964722


Running epoch 1, step 1718, batch 670
Sampled inputs[:2]: tensor([[   0, 1858,  499,  ...,   14, 1032,   14],
        [   0,   12,  358,  ...,  352,  266,  319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4192e-04,  2.0359e-04, -2.5631e-04,  ...,  1.6630e-04,
          4.4689e-04,  1.2822e-04],
        [-9.5665e-06, -6.5416e-06,  6.6757e-06,  ..., -8.1733e-06,
         -6.1281e-06, -6.7838e-06],
        [ 2.6209e-04,  3.2207e-04, -2.0554e-04,  ...,  1.9623e-04,
          2.6550e-04,  6.4680e-05],
        [-2.1175e-05, -1.4223e-05,  1.5520e-05,  ..., -1.8105e-05,
         -1.3605e-05, -1.5602e-05],
        [-2.3320e-05, -1.7256e-05,  1.7107e-05,  ..., -2.0161e-05,
         -1.6063e-05, -1.5199e-05]], device='cuda:0')
Loss: 0.9601581692695618


Running epoch 1, step 1719, batch 671
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,   461,  4182,  ...,  7461,   292,  4895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9819e-04,  1.9914e-04, -3.1384e-04,  ...,  2.0450e-04,
          4.6958e-04,  2.1433e-04],
        [-1.0982e-05, -7.5251e-06,  7.6294e-06,  ..., -9.3728e-06,
         -7.0706e-06, -7.7672e-06],
        [ 2.5762e-04,  3.1883e-04, -2.0231e-04,  ...,  1.9246e-04,
          2.6250e-04,  6.1581e-05],
        [ 4.5583e-05,  9.2113e-05,  6.5173e-06,  ...,  3.3504e-05,
          6.1411e-05,  6.2345e-06],
        [-2.6852e-05, -1.9908e-05,  1.9595e-05,  ..., -2.3201e-05,
         -1.8612e-05, -1.7494e-05]], device='cuda:0')
Loss: 0.972859799861908
Graident accumulation at epoch 1, step 1719, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.7647e-05,  1.7592e-04, -1.3767e-04,  ..., -4.1626e-05,
          7.8473e-06, -6.6262e-05],
        [-9.0044e-06, -6.2899e-06,  6.6228e-06,  ..., -7.8125e-06,
         -5.4136e-06, -6.1360e-06],
        [ 4.2614e-05,  5.7239e-05, -3.2227e-05,  ...,  3.8544e-05,
          4.8255e-05,  1.0307e-05],
        [ 1.6709e-05,  3.0516e-05, -9.5574e-06,  ...,  2.0181e-05,
          2.4139e-05,  6.9055e-06],
        [-2.6437e-05, -1.9529e-05,  2.0009e-05,  ..., -2.2589e-05,
         -1.8348e-05, -1.7544e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5471e-08, 5.7317e-08, 6.7695e-08,  ..., 2.7378e-08, 1.5861e-07,
         3.9890e-08],
        [9.0264e-11, 5.5122e-11, 2.6166e-11,  ..., 6.1361e-11, 2.6503e-11,
         3.4499e-11],
        [4.2411e-09, 2.9053e-09, 1.7220e-09,  ..., 3.4273e-09, 1.7333e-09,
         1.1599e-09],
        [1.2837e-09, 1.4308e-09, 5.5979e-10,  ..., 1.1320e-09, 9.0187e-10,
         4.8717e-10],
        [4.1132e-10, 2.3418e-10, 9.3852e-11,  ..., 3.0263e-10, 9.7298e-11,
         1.1791e-10]], device='cuda:0')
optimizer state dict: 215.0
lr: [1.6408474411262143e-06, 1.6408474411262143e-06]
scheduler_last_epoch: 215


Running epoch 1, step 1720, batch 672
Sampled inputs[:2]: tensor([[    0,   287,  6932,  ...,  1549,  1480,   518],
        [    0,   292, 41192,  ..., 34298,  8741,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9620e-05,  8.0923e-05, -6.4061e-05,  ..., -6.6939e-06,
          1.7511e-04, -6.1052e-05],
        [-1.3635e-06, -9.7603e-07,  9.2387e-07,  ..., -1.1995e-06,
         -9.2015e-07, -9.6858e-07],
        [-4.1723e-06, -3.1739e-06,  3.1143e-06,  ..., -3.6359e-06,
         -2.8312e-06, -2.9206e-06],
        [-2.9951e-06, -2.1309e-06,  2.1458e-06,  ..., -2.6375e-06,
         -2.0266e-06, -2.2352e-06],
        [-3.3230e-06, -2.6226e-06,  2.3842e-06,  ..., -2.9802e-06,
         -2.4289e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9683467745780945


Running epoch 1, step 1721, batch 673
Sampled inputs[:2]: tensor([[    0, 12165,    12,  ...,  2860, 10718,   278],
        [    0,   741,  4933,  ...,   932,   365,   838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7885e-05,  2.4536e-05, -5.3375e-05,  ...,  3.5414e-05,
          2.8258e-04,  2.6692e-06],
        [-2.6971e-06, -1.9744e-06,  1.9222e-06,  ..., -2.3544e-06,
         -1.8328e-06, -1.8664e-06],
        [-8.3745e-06, -6.4522e-06,  6.4671e-06,  ..., -7.2718e-06,
         -5.7220e-06, -5.7667e-06],
        [-6.0499e-06, -4.3660e-06,  4.5151e-06,  ..., -5.2750e-06,
         -4.0978e-06, -4.3958e-06],
        [-6.6012e-06, -5.2899e-06,  4.9323e-06,  ..., -5.8711e-06,
         -4.8429e-06, -4.2170e-06]], device='cuda:0')
Loss: 1.0021144151687622


Running epoch 1, step 1722, batch 674
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  381, 3513, 1501],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3311e-05, -1.5807e-05, -4.2773e-05,  ..., -3.0666e-05,
          2.9085e-04, -1.3164e-05],
        [-4.1053e-06, -2.9430e-06,  3.0175e-06,  ..., -3.5241e-06,
         -2.7157e-06, -2.8275e-06],
        [ 6.8498e-04,  5.8290e-04, -4.6429e-04,  ...,  5.8085e-04,
          3.6368e-04,  4.8730e-04],
        [-9.2089e-06, -6.4969e-06,  7.0781e-06,  ..., -7.8827e-06,
         -6.0499e-06, -6.6310e-06],
        [-9.9391e-06, -7.7635e-06,  7.6294e-06,  ..., -8.7023e-06,
         -7.1079e-06, -6.3330e-06]], device='cuda:0')
Loss: 0.9719294905662537


Running epoch 1, step 1723, batch 675
Sampled inputs[:2]: tensor([[    0,   344,   259,  ...,  6787, 10045,  9799],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4556e-05,  6.7404e-06, -2.3724e-04,  ..., -3.7018e-05,
          4.4169e-04, -1.1200e-04],
        [-5.4389e-06, -3.7961e-06,  3.8594e-06,  ..., -4.6939e-06,
         -3.5875e-06, -3.7476e-06],
        [ 6.8098e-04,  5.8020e-04, -4.6146e-04,  ...,  5.7741e-04,
          3.6108e-04,  4.8465e-04],
        [-1.2174e-05, -8.3521e-06,  9.0450e-06,  ..., -1.0505e-05,
         -8.0317e-06, -8.7768e-06],
        [-1.3083e-05, -9.9689e-06,  9.8050e-06,  ..., -1.1459e-05,
         -9.2983e-06, -8.2403e-06]], device='cuda:0')
Loss: 0.9250818490982056


Running epoch 1, step 1724, batch 676
Sampled inputs[:2]: tensor([[    0,  1336,   287,  ..., 15920,    12,   287],
        [    0,    12,   297,  ...,  2980,  1145, 17207]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8025e-05, -2.1374e-06, -3.0453e-04,  ..., -9.1437e-05,
          4.9226e-04, -5.5049e-05],
        [-6.8173e-06, -4.7795e-06,  4.9174e-06,  ..., -5.8189e-06,
         -4.5337e-06, -4.6752e-06],
        [ 6.7669e-04,  5.7696e-04, -4.5794e-04,  ...,  5.7390e-04,
          3.5810e-04,  4.8173e-04],
        [-1.5259e-05, -1.0528e-05,  1.1519e-05,  ..., -1.3009e-05,
         -1.0148e-05, -1.0967e-05],
        [-1.6361e-05, -1.2547e-05,  1.2413e-05,  ..., -1.4201e-05,
         -1.1757e-05, -1.0282e-05]], device='cuda:0')
Loss: 0.9742113947868347


Running epoch 1, step 1725, batch 677
Sampled inputs[:2]: tensor([[   0,  342,  266,  ..., 4998, 4756, 5139],
        [   0,   12, 1197,  ...,  352, 2513,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8823e-04, -3.0841e-04, -6.1212e-04,  ..., -1.2572e-05,
          8.9058e-04, -1.5092e-04],
        [-8.0913e-06, -5.7146e-06,  5.8934e-06,  ..., -6.9067e-06,
         -5.3607e-06, -5.5470e-06],
        [ 6.7261e-04,  5.7388e-04, -4.5451e-04,  ...,  5.7050e-04,
          3.5549e-04,  4.7903e-04],
        [-1.8120e-05, -1.2599e-05,  1.3858e-05,  ..., -1.5423e-05,
         -1.1988e-05, -1.3024e-05],
        [-1.9327e-05, -1.4871e-05,  1.4842e-05,  ..., -1.6734e-05,
         -1.3813e-05, -1.2070e-05]], device='cuda:0')
Loss: 0.9252477288246155


Running epoch 1, step 1726, batch 678
Sampled inputs[:2]: tensor([[    0,   287, 19777,  ...,   266,  5061,   278],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6366e-04, -1.8262e-04, -3.8618e-04,  ...,  2.2227e-05,
          9.3700e-04, -2.2770e-04],
        [-9.7007e-06, -6.7428e-06,  6.6757e-06,  ..., -8.2776e-06,
         -6.4410e-06, -6.7465e-06],
        [ 6.6772e-04,  5.7051e-04, -4.5188e-04,  ...,  5.6633e-04,
          3.5214e-04,  4.7540e-04],
        [-2.1338e-05, -1.4655e-05,  1.5445e-05,  ..., -1.8209e-05,
         -1.4208e-05, -1.5497e-05],
        [-2.3678e-05, -1.7896e-05,  1.7107e-05,  ..., -2.0489e-05,
         -1.6928e-05, -1.5125e-05]], device='cuda:0')
Loss: 0.9409083127975464


Running epoch 1, step 1727, batch 679
Sampled inputs[:2]: tensor([[   0,  266, 4505,  ...,   12,  461,  806],
        [   0,  278,  634,  ...,  598, 1722,  591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6811e-04, -4.8118e-05, -4.6871e-04,  ..., -9.2240e-05,
          1.0592e-03, -2.3459e-04],
        [-1.1057e-05, -7.6629e-06,  7.6592e-06,  ..., -9.4250e-06,
         -7.3351e-06, -7.7300e-06],
        [ 6.6334e-04,  5.6741e-04, -4.4844e-04,  ...,  5.6263e-04,
          3.4926e-04,  4.7222e-04],
        [-2.4512e-05, -1.6756e-05,  1.7859e-05,  ..., -2.0906e-05,
         -1.6309e-05, -1.7926e-05],
        [-2.7046e-05, -2.0370e-05,  1.9670e-05,  ..., -2.3395e-05,
         -1.9297e-05, -1.7390e-05]], device='cuda:0')
Loss: 0.9614847898483276
Graident accumulation at epoch 1, step 1727, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1929e-05,  1.5351e-04, -1.7078e-04,  ..., -4.6688e-05,
          1.1298e-04, -8.3095e-05],
        [-9.2096e-06, -6.4272e-06,  6.7264e-06,  ..., -7.9737e-06,
         -5.6058e-06, -6.2954e-06],
        [ 1.0469e-04,  1.0826e-04, -7.3848e-05,  ...,  9.0952e-05,
          7.8356e-05,  5.6498e-05],
        [ 1.2587e-05,  2.5789e-05, -6.8158e-06,  ...,  1.6073e-05,
          2.0094e-05,  4.4223e-06],
        [-2.6497e-05, -1.9613e-05,  1.9975e-05,  ..., -2.2670e-05,
         -1.8443e-05, -1.7529e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5635e-08, 5.7262e-08, 6.7847e-08,  ..., 2.7359e-08, 1.5957e-07,
         3.9905e-08],
        [9.0296e-11, 5.5125e-11, 2.6198e-11,  ..., 6.1389e-11, 2.6530e-11,
         3.4524e-11],
        [4.6769e-09, 3.2244e-09, 1.9213e-09,  ..., 3.7405e-09, 1.8535e-09,
         1.3817e-09],
        [1.2830e-09, 1.4297e-09, 5.5955e-10,  ..., 1.1313e-09, 9.0124e-10,
         4.8701e-10],
        [4.1164e-10, 2.3436e-10, 9.4145e-11,  ..., 3.0287e-10, 9.7573e-11,
         1.1809e-10]], device='cuda:0')
optimizer state dict: 216.0
lr: [1.5736398113547236e-06, 1.5736398113547236e-06]
scheduler_last_epoch: 216


Running epoch 1, step 1728, batch 680
Sampled inputs[:2]: tensor([[   0, 2805,  391,  ...,   12,  259, 1420],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4094e-05, -1.0493e-04, -1.7991e-04,  ..., -2.7344e-05,
          5.6419e-06,  6.1967e-06],
        [-1.3411e-06, -9.6858e-07,  1.0282e-06,  ..., -1.0878e-06,
         -8.4564e-07, -8.9407e-07],
        [-4.3213e-06, -3.2485e-06,  3.5614e-06,  ..., -3.5167e-06,
         -2.7567e-06, -2.9206e-06],
        [-3.0249e-06, -2.1756e-06,  2.4587e-06,  ..., -2.4587e-06,
         -1.9073e-06, -2.1309e-06],
        [-3.0696e-06, -2.4438e-06,  2.4587e-06,  ..., -2.5779e-06,
         -2.1458e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9817770719528198


Running epoch 1, step 1729, batch 681
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4294e-05, -8.0913e-05, -2.5319e-04,  ..., -1.3213e-04,
         -2.8693e-04, -1.5855e-04],
        [-2.5779e-06, -1.8999e-06,  2.0862e-06,  ..., -2.1458e-06,
         -1.6652e-06, -1.7770e-06],
        [-8.3447e-06, -6.3628e-06,  7.2271e-06,  ..., -6.9290e-06,
         -5.4240e-06, -5.7518e-06],
        [-5.9158e-06, -4.2915e-06,  5.0664e-06,  ..., -4.9025e-06,
         -3.7998e-06, -4.2915e-06],
        [-5.9903e-06, -4.8131e-06,  5.0366e-06,  ..., -5.1111e-06,
         -4.2468e-06, -3.8147e-06]], device='cuda:0')
Loss: 0.958713173866272


Running epoch 1, step 1730, batch 682
Sampled inputs[:2]: tensor([[    0,   565,    27,  ...,    88,  4451,    14],
        [    0,   221,   264,  ...,  3613,  3222, 14000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5377e-06, -1.4340e-05, -2.5792e-04,  ..., -8.5434e-05,
         -3.5373e-04, -1.0774e-04],
        [-4.0159e-06, -2.7716e-06,  2.9579e-06,  ..., -3.4496e-06,
         -2.6487e-06, -2.8871e-06],
        [-1.2755e-05, -9.1344e-06,  1.0148e-05,  ..., -1.0833e-05,
         -8.3894e-06, -9.0599e-06],
        [-8.9854e-06, -6.0946e-06,  6.9737e-06,  ..., -7.7039e-06,
         -5.9009e-06, -6.7502e-06],
        [-9.6560e-06, -7.1526e-06,  7.4059e-06,  ..., -8.3894e-06,
         -6.8098e-06, -6.3777e-06]], device='cuda:0')
Loss: 0.9339597225189209


Running epoch 1, step 1731, batch 683
Sampled inputs[:2]: tensor([[    0,  2310,   292,  ...,   462,   508,   586],
        [    0, 11435,  1226,  ...,    13,  1875,  6394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4538e-05,  2.9252e-06, -3.2187e-04,  ..., -1.0936e-04,
         -3.8075e-04, -1.9013e-04],
        [-5.2825e-06, -3.6843e-06,  3.7588e-06,  ..., -4.6492e-06,
         -3.6098e-06, -3.8929e-06],
        [-1.6630e-05, -1.2010e-05,  1.2860e-05,  ..., -1.4409e-05,
         -1.1295e-05, -1.2010e-05],
        [-1.1772e-05, -8.0764e-06,  8.8215e-06,  ..., -1.0356e-05,
         -8.0615e-06, -9.0748e-06],
        [-1.2875e-05, -9.5963e-06,  9.6262e-06,  ..., -1.1384e-05,
         -9.3430e-06, -8.6278e-06]], device='cuda:0')
Loss: 0.9460555911064148


Running epoch 1, step 1732, batch 684
Sampled inputs[:2]: tensor([[   0, 4441, 1821,  ...,  642, 2310,   14],
        [   0, 3087,  401,  ..., 1875, 4122,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1899e-04,  6.2786e-06, -2.7072e-04,  ..., -7.8712e-05,
         -3.9156e-04, -2.4053e-04],
        [-6.6236e-06, -4.6194e-06,  4.7423e-06,  ..., -5.7742e-06,
         -4.4554e-06, -4.8429e-06],
        [-2.1070e-05, -1.5184e-05,  1.6332e-05,  ..., -1.8090e-05,
         -1.4067e-05, -1.5140e-05],
        [-1.4827e-05, -1.0177e-05,  1.1161e-05,  ..., -1.2904e-05,
         -9.9689e-06, -1.1325e-05],
        [-1.6198e-05, -1.2070e-05,  1.2144e-05,  ..., -1.4216e-05,
         -1.1578e-05, -1.0803e-05]], device='cuda:0')
Loss: 0.9898695945739746


Running epoch 1, step 1733, batch 685
Sampled inputs[:2]: tensor([[   0,  266, 1553,  ..., 8954,   21,  409],
        [   0, 6803, 6298,  ...,  490, 1781,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3434e-05, -6.5569e-05, -2.2811e-04,  ..., -1.2190e-04,
         -4.6730e-04, -2.3290e-04],
        [-7.9572e-06, -5.6326e-06,  5.7630e-06,  ..., -6.9365e-06,
         -5.3309e-06, -5.7667e-06],
        [-2.5332e-05, -1.8552e-05,  1.9774e-05,  ..., -2.1815e-05,
         -1.6868e-05, -1.8135e-05],
        [-1.7762e-05, -1.2368e-05,  1.3515e-05,  ..., -1.5453e-05,
         -1.1869e-05, -1.3471e-05],
        [-1.9357e-05, -1.4693e-05,  1.4633e-05,  ..., -1.7047e-05,
         -1.3828e-05, -1.2875e-05]], device='cuda:0')
Loss: 0.9655623435974121


Running epoch 1, step 1734, batch 686
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,   607,  2697,  ...,   391, 14410, 14997]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7142e-04, -1.3620e-04, -2.1711e-04,  ..., -8.0331e-05,
         -2.7987e-04, -2.1281e-04],
        [ 7.5489e-05,  8.7279e-05, -5.4445e-05,  ...,  5.0885e-05,
          7.8661e-05,  3.9771e-05],
        [-2.9564e-05, -2.1473e-05,  2.2933e-05,  ..., -2.5481e-05,
         -1.9565e-05, -2.1160e-05],
        [-2.0787e-05, -1.4365e-05,  1.5691e-05,  ..., -1.8135e-05,
         -1.3851e-05, -1.5810e-05],
        [-2.2605e-05, -1.7017e-05,  1.7002e-05,  ..., -1.9893e-05,
         -1.6034e-05, -1.5020e-05]], device='cuda:0')
Loss: 0.9739552736282349


Running epoch 1, step 1735, batch 687
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,  1125,   609,   292],
        [    0,   266,  5232,  ...,  2719,    13, 25385]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8457e-04, -1.1368e-04, -3.6670e-04,  ..., -2.0396e-04,
         -4.0180e-04, -2.3242e-04],
        [ 7.4096e-05,  8.6351e-05, -5.3387e-05,  ...,  4.9768e-05,
          7.7845e-05,  3.8851e-05],
        [-3.3915e-05, -2.4512e-05,  2.6479e-05,  ..., -2.8953e-05,
         -2.2143e-05, -2.4021e-05],
        [-2.3857e-05, -1.6376e-05,  1.8120e-05,  ..., -2.0593e-05,
         -1.5669e-05, -1.7956e-05],
        [-2.5839e-05, -1.9431e-05,  1.9595e-05,  ..., -2.2545e-05,
         -1.8135e-05, -1.6972e-05]], device='cuda:0')
Loss: 0.9411264061927795
Graident accumulation at epoch 1, step 1735, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.8194e-05,  1.2679e-04, -1.9037e-04,  ..., -6.2415e-05,
          6.1505e-05, -9.8028e-05],
        [-8.7903e-07,  2.8506e-06,  7.1508e-07,  ..., -2.1996e-06,
          2.7393e-06, -1.7808e-06],
        [ 9.0826e-05,  9.4979e-05, -6.3815e-05,  ...,  7.8962e-05,
          6.8306e-05,  4.8447e-05],
        [ 8.9424e-06,  2.1572e-05, -4.3222e-06,  ...,  1.2406e-05,
          1.6518e-05,  2.1845e-06],
        [-2.6432e-05, -1.9595e-05,  1.9937e-05,  ..., -2.2657e-05,
         -1.8412e-05, -1.7473e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5613e-08, 5.7218e-08, 6.7913e-08,  ..., 2.7373e-08, 1.5957e-07,
         3.9919e-08],
        [9.5696e-11, 6.2527e-11, 2.9022e-11,  ..., 6.3804e-11, 3.2564e-11,
         3.5999e-11],
        [4.6734e-09, 3.2218e-09, 1.9201e-09,  ..., 3.7376e-09, 1.8521e-09,
         1.3809e-09],
        [1.2823e-09, 1.4285e-09, 5.5932e-10,  ..., 1.1306e-09, 9.0058e-10,
         4.8684e-10],
        [4.1190e-10, 2.3451e-10, 9.4435e-11,  ..., 3.0308e-10, 9.7804e-11,
         1.1826e-10]], device='cuda:0')
optimizer state dict: 217.0
lr: [1.5077198029747941e-06, 1.5077198029747941e-06]
scheduler_last_epoch: 217


Running epoch 1, step 1736, batch 688
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0,   17, 3737,  ...,  298,  396,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8933e-04,  1.6312e-04,  1.8881e-04,  ...,  6.7398e-05,
         -8.6191e-07, -1.7507e-05],
        [-1.4976e-06, -8.6054e-07,  7.7114e-07,  ..., -1.4007e-06,
         -1.0580e-06, -1.2517e-06],
        [-4.2319e-06, -2.5034e-06,  2.4289e-06,  ..., -3.8147e-06,
         -2.9355e-06, -3.3826e-06],
        [-3.1292e-06, -1.7509e-06,  1.6466e-06,  ..., -2.9504e-06,
         -2.2352e-06, -2.6673e-06],
        [-3.9935e-06, -2.3544e-06,  2.2501e-06,  ..., -3.6359e-06,
         -2.8461e-06, -2.9951e-06]], device='cuda:0')
Loss: 0.9124295115470886


Running epoch 1, step 1737, batch 689
Sampled inputs[:2]: tensor([[    0,  4645,  7688,  ..., 26535,   471,   287],
        [    0,   756,   401,  ...,  8385,  1004,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7151e-04,  9.1940e-05,  8.5279e-05,  ...,  5.8524e-05,
         -2.1542e-04, -2.1373e-04],
        [-2.8983e-06, -1.8515e-06,  1.8291e-06,  ..., -2.5257e-06,
         -1.9185e-06, -2.2128e-06],
        [-8.8215e-06, -5.8562e-06,  6.1095e-06,  ..., -7.4953e-06,
         -5.7667e-06, -6.5714e-06],
        [-6.2883e-06, -3.9414e-06,  4.1351e-06,  ..., -5.4836e-06,
         -4.1574e-06, -4.9323e-06],
        [-7.3463e-06, -4.9025e-06,  4.8429e-06,  ..., -6.4075e-06,
         -5.0962e-06, -5.1409e-06]], device='cuda:0')
Loss: 0.984624445438385


Running epoch 1, step 1738, batch 690
Sampled inputs[:2]: tensor([[    0, 13856,   278,  ...,    14,    69,   462],
        [    0,   995,    13,  ...,  3494,   367,  6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3692e-04,  1.2956e-04,  1.0527e-04,  ...,  2.3556e-05,
         -1.3798e-04, -1.2729e-04],
        [-4.2170e-06, -2.8871e-06,  2.7716e-06,  ..., -3.6731e-06,
         -2.8871e-06, -3.1404e-06],
        [-1.3083e-05, -9.3728e-06,  9.3281e-06,  ..., -1.1235e-05,
         -8.9407e-06, -9.6112e-06],
        [-9.3281e-06, -6.2957e-06,  6.3851e-06,  ..., -8.1360e-06,
         -6.3926e-06, -7.1824e-06],
        [-1.0580e-05, -7.6592e-06,  7.2420e-06,  ..., -9.2983e-06,
         -7.6443e-06, -7.2867e-06]], device='cuda:0')
Loss: 0.9809272289276123


Running epoch 1, step 1739, batch 691
Sampled inputs[:2]: tensor([[    0,  1231,   352,  ...,  8524,    14,   381],
        [    0,  5722, 20126,  ...,  1500,   696,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6476e-04,  1.0633e-04,  1.8464e-04,  ..., -4.4482e-05,
         -3.9768e-05,  1.8888e-05],
        [-5.5879e-06, -3.8408e-06,  3.7849e-06,  ..., -4.8354e-06,
         -3.7402e-06, -4.0941e-06],
        [-1.7405e-05, -1.2547e-05,  1.2770e-05,  ..., -1.4901e-05,
         -1.1668e-05, -1.2606e-05],
        [-1.2383e-05, -8.4117e-06,  8.7544e-06,  ..., -1.0744e-05,
         -8.3148e-06, -9.4324e-06],
        [-1.3724e-05, -1.0088e-05,  9.7007e-06,  ..., -1.2040e-05,
         -9.7901e-06, -9.2983e-06]], device='cuda:0')
Loss: 0.9419920444488525


Running epoch 1, step 1740, batch 692
Sampled inputs[:2]: tensor([[    0,   266, 10262,  ...,   271,  3437,  4392],
        [    0,   266,  6737,  ...,  2409,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0202e-04,  1.0866e-04,  4.2236e-04,  ..., -1.9441e-04,
          4.3805e-04,  4.8048e-05],
        [-7.0482e-06, -4.8392e-06,  4.5076e-06,  ..., -6.1318e-06,
         -4.8354e-06, -5.2489e-06],
        [-2.1994e-05, -1.5840e-05,  1.5348e-05,  ..., -1.8924e-05,
         -1.5095e-05, -1.6168e-05],
        [-1.5512e-05, -1.0498e-05,  1.0341e-05,  ..., -1.3530e-05,
         -1.0684e-05, -1.1981e-05],
        [-1.7539e-05, -1.2860e-05,  1.1742e-05,  ..., -1.5438e-05,
         -1.2785e-05, -1.2055e-05]], device='cuda:0')
Loss: 0.9420435428619385


Running epoch 1, step 1741, batch 693
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0, 1978,  352,  ..., 2276,   12,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5998e-04, -5.6507e-07,  4.0291e-04,  ..., -3.3594e-04,
          4.3275e-04, -1.4182e-04],
        [-8.3819e-06, -5.6922e-06,  5.6028e-06,  ..., -7.2122e-06,
         -5.6028e-06, -6.1207e-06],
        [-2.6017e-05, -1.8477e-05,  1.8895e-05,  ..., -2.2113e-05,
         -1.7390e-05, -1.8701e-05],
        [-1.8522e-05, -1.2361e-05,  1.2964e-05,  ..., -1.5944e-05,
         -1.2405e-05, -1.4022e-05],
        [-2.0534e-05, -1.4946e-05,  1.4320e-05,  ..., -1.7866e-05,
         -1.4648e-05, -1.3798e-05]], device='cuda:0')
Loss: 0.9257024526596069


Running epoch 1, step 1742, batch 694
Sampled inputs[:2]: tensor([[   0,   12,  266,  ...,   13,  635,   13],
        [   0, 1067,  271,  ...,  266,  940,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8600e-04,  5.1743e-05,  4.5483e-04,  ..., -2.8106e-04,
          4.1912e-04, -1.2276e-04],
        [-9.7603e-06, -6.6385e-06,  6.4746e-06,  ..., -8.4341e-06,
         -6.5453e-06, -7.2010e-06],
        [-3.0398e-05, -2.1517e-05,  2.1935e-05,  ..., -2.5839e-05,
         -2.0280e-05, -2.1949e-05],
        [-2.1607e-05, -1.4402e-05,  1.4991e-05,  ..., -1.8656e-05,
         -1.4506e-05, -1.6510e-05],
        [-2.3961e-05, -1.7360e-05,  1.6645e-05,  ..., -2.0832e-05,
         -1.7032e-05, -1.6138e-05]], device='cuda:0')
Loss: 0.9418926239013672


Running epoch 1, step 1743, batch 695
Sampled inputs[:2]: tensor([[    0,    12,  2418,  ...,   446,   381,  2204],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4841e-04, -1.6955e-05,  5.8475e-04,  ..., -4.1817e-04,
          5.8328e-04,  3.9929e-06],
        [-1.1191e-05, -7.6741e-06,  7.3574e-06,  ..., -9.6634e-06,
         -7.5586e-06, -8.2292e-06],
        [-3.4958e-05, -2.4974e-05,  2.5019e-05,  ..., -2.9713e-05,
         -2.3514e-05, -2.5257e-05],
        [-2.4676e-05, -1.6622e-05,  1.6987e-05,  ..., -2.1309e-05,
         -1.6712e-05, -1.8865e-05],
        [-2.7493e-05, -2.0146e-05,  1.8939e-05,  ..., -2.3931e-05,
         -1.9744e-05, -1.8522e-05]], device='cuda:0')
Loss: 0.9525250196456909
Graident accumulation at epoch 1, step 1743, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9215e-05,  1.1242e-04, -1.1286e-04,  ..., -9.7990e-05,
          1.1368e-04, -8.7826e-05],
        [-1.9102e-06,  1.7981e-06,  1.3793e-06,  ..., -2.9460e-06,
          1.7095e-06, -2.4256e-06],
        [ 7.8248e-05,  8.2984e-05, -5.4932e-05,  ...,  6.8094e-05,
          5.9124e-05,  4.1076e-05],
        [ 5.5806e-06,  1.7753e-05, -2.1913e-06,  ...,  9.0346e-06,
          1.3195e-05,  7.9560e-08],
        [-2.6538e-05, -1.9650e-05,  1.9837e-05,  ..., -2.2785e-05,
         -1.8545e-05, -1.7578e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5679e-08, 5.7161e-08, 6.8187e-08,  ..., 2.7521e-08, 1.5975e-07,
         3.9879e-08],
        [9.5725e-11, 6.2523e-11, 2.9047e-11,  ..., 6.3834e-11, 3.2588e-11,
         3.6030e-11],
        [4.6699e-09, 3.2192e-09, 1.9188e-09,  ..., 3.7347e-09, 1.8508e-09,
         1.3802e-09],
        [1.2816e-09, 1.4274e-09, 5.5905e-10,  ..., 1.1299e-09, 8.9996e-10,
         4.8671e-10],
        [4.1224e-10, 2.3468e-10, 9.4699e-11,  ..., 3.0335e-10, 9.8096e-11,
         1.1848e-10]], device='cuda:0')
optimizer state dict: 218.0
lr: [1.4430974891391325e-06, 1.4430974891391325e-06]
scheduler_last_epoch: 218


Running epoch 1, step 1744, batch 696
Sampled inputs[:2]: tensor([[   0,  792,   83,  ...,  300,  768,  932],
        [   0,  591,  688,  ...,  271, 3390,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4323e-04,  1.0440e-05, -3.0634e-05,  ..., -1.3955e-04,
         -3.2079e-04, -6.1118e-05],
        [-1.3486e-06, -8.9407e-07,  1.0729e-06,  ..., -1.0878e-06,
         -8.2701e-07, -8.7544e-07],
        [-4.2319e-06, -2.9504e-06,  3.6061e-06,  ..., -3.4124e-06,
         -2.6226e-06, -2.7716e-06],
        [-3.0547e-06, -1.9670e-06,  2.5481e-06,  ..., -2.4438e-06,
         -1.8552e-06, -2.1011e-06],
        [-3.0994e-06, -2.2948e-06,  2.5779e-06,  ..., -2.5779e-06,
         -2.1011e-06, -1.8775e-06]], device='cuda:0')
Loss: 0.970238208770752


Running epoch 1, step 1745, batch 697
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,    14,  1364, 19388],
        [    0,    12,  3570,  ...,   273,   298,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6179e-05,  8.4381e-06, -4.2991e-05,  ..., -4.2801e-05,
         -3.0186e-04, -9.4475e-05],
        [-2.8312e-06, -1.8999e-06,  1.9558e-06,  ..., -2.3395e-06,
         -1.8179e-06, -1.9334e-06],
        [-8.6725e-06, -6.1691e-06,  6.4969e-06,  ..., -7.1526e-06,
         -5.6773e-06, -5.9158e-06],
        [-6.1691e-06, -4.0680e-06,  4.4703e-06,  ..., -5.0962e-06,
         -4.0010e-06, -4.4107e-06],
        [-6.7502e-06, -5.0217e-06,  4.8727e-06,  ..., -5.7220e-06,
         -4.7535e-06, -4.2915e-06]], device='cuda:0')
Loss: 0.9721669554710388


Running epoch 1, step 1746, batch 698
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,    13,  7805,  ...,  2733,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4364e-05,  1.6449e-05, -4.5246e-05,  ..., -1.0908e-04,
         -2.5871e-04, -7.0798e-05],
        [-4.2021e-06, -2.8983e-06,  2.9765e-06,  ..., -3.4943e-06,
         -2.7381e-06, -2.8685e-06],
        [-1.2934e-05, -9.4473e-06,  9.9391e-06,  ..., -1.0744e-05,
         -8.5831e-06, -8.8215e-06],
        [-9.2089e-06, -6.2585e-06,  6.8694e-06,  ..., -7.6592e-06,
         -6.0424e-06, -6.5863e-06],
        [-9.9093e-06, -7.5549e-06,  7.3761e-06,  ..., -8.4490e-06,
         -7.0632e-06, -6.2883e-06]], device='cuda:0')
Loss: 0.9642059803009033


Running epoch 1, step 1747, batch 699
Sampled inputs[:2]: tensor([[    0,  3445,   328,  ...,   278, 12323,   554],
        [    0,  2485,    12,  ...,   293,   259, 14600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4796e-04,  1.2843e-05,  5.8522e-05,  ..., -1.2004e-04,
         -1.9565e-04,  2.5066e-05],
        [-5.4911e-06, -3.8594e-06,  3.8669e-06,  ..., -4.6417e-06,
         -3.7216e-06, -3.7886e-06],
        [-1.6928e-05, -1.2621e-05,  1.2994e-05,  ..., -1.4290e-05,
         -1.1653e-05, -1.1653e-05],
        [-1.2189e-05, -8.4937e-06,  9.0450e-06,  ..., -1.0327e-05,
         -8.3521e-06, -8.8662e-06],
        [-1.3009e-05, -1.0103e-05,  9.6858e-06,  ..., -1.1235e-05,
         -9.5814e-06, -8.2999e-06]], device='cuda:0')
Loss: 0.983600378036499


Running epoch 1, step 1748, batch 700
Sampled inputs[:2]: tensor([[    0,  1526,  3502,  ..., 11727,  3736,  1661],
        [    0,   824,    14,  ...,   278,  9328,  1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2260e-05, -1.1450e-04,  5.2357e-05,  ..., -1.1662e-04,
         -1.1477e-04,  1.2309e-05],
        [-6.9514e-06, -4.9025e-06,  4.8727e-06,  ..., -5.8934e-06,
         -4.6827e-06, -4.7795e-06],
        [-2.1487e-05, -1.6078e-05,  1.6391e-05,  ..., -1.8224e-05,
         -1.4737e-05, -1.4782e-05],
        [-1.5393e-05, -1.0788e-05,  1.1355e-05,  ..., -1.3098e-05,
         -1.0498e-05, -1.1146e-05],
        [-1.6585e-05, -1.2890e-05,  1.2264e-05,  ..., -1.4395e-05,
         -1.2174e-05, -1.0595e-05]], device='cuda:0')
Loss: 0.9729706048965454


Running epoch 1, step 1749, batch 701
Sampled inputs[:2]: tensor([[    0,   266,  2511,  ...,  3220,  4164,  1173],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2238e-04, -1.1199e-04,  7.2681e-05,  ..., -1.7008e-04,
          2.1325e-04, -6.1107e-05],
        [-8.2850e-06, -5.7817e-06,  5.7258e-06,  ..., -7.0408e-06,
         -5.5581e-06, -5.7481e-06],
        [-2.5690e-05, -1.8984e-05,  1.9342e-05,  ..., -2.1785e-05,
         -1.7494e-05, -1.7717e-05],
        [-1.8328e-05, -1.2696e-05,  1.3292e-05,  ..., -1.5646e-05,
         -1.2465e-05, -1.3337e-05],
        [-1.9714e-05, -1.5125e-05,  1.4409e-05,  ..., -1.7092e-05,
         -1.4350e-05, -1.2621e-05]], device='cuda:0')
Loss: 0.9436682462692261


Running epoch 1, step 1750, batch 702
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0,   287,  1070,  ...,   292,   221,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4354e-04, -6.9839e-05,  4.3237e-05,  ..., -9.9740e-05,
          1.7340e-04, -7.6573e-05],
        [-9.6038e-06, -6.6757e-06,  6.5900e-06,  ..., -8.2254e-06,
         -6.5416e-06, -6.7092e-06],
        [-2.9743e-05, -2.1890e-05,  2.2277e-05,  ..., -2.5377e-05,
         -2.0459e-05, -2.0608e-05],
        [-2.1204e-05, -1.4603e-05,  1.5274e-05,  ..., -1.8254e-05,
         -1.4640e-05, -1.5542e-05],
        [-2.2918e-05, -1.7464e-05,  1.6645e-05,  ..., -1.9982e-05,
         -1.6823e-05, -1.4737e-05]], device='cuda:0')
Loss: 0.937924325466156


Running epoch 1, step 1751, batch 703
Sampled inputs[:2]: tensor([[   0,  266, 1513,  ...,  367, 1941,  344],
        [   0,   12, 1471,  ..., 1356,  600,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9049e-04, -1.0957e-04, -1.5106e-04,  ..., -1.1421e-04,
          1.0062e-04,  5.2431e-05],
        [-1.0952e-05, -7.6219e-06,  7.5661e-06,  ..., -9.4026e-06,
         -7.4916e-06, -7.6517e-06],
        [-3.3855e-05, -2.4885e-05,  2.5466e-05,  ..., -2.8908e-05,
         -2.3350e-05, -2.3425e-05],
        [-2.4125e-05, -1.6600e-05,  1.7479e-05,  ..., -2.0802e-05,
         -1.6712e-05, -1.7703e-05],
        [-2.6077e-05, -1.9878e-05,  1.9044e-05,  ..., -2.2754e-05,
         -1.9208e-05, -1.6719e-05]], device='cuda:0')
Loss: 0.9402364492416382
Graident accumulation at epoch 1, step 1751, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.1343e-05,  9.0220e-05, -1.1668e-04,  ..., -9.9612e-05,
          1.1238e-04, -7.3800e-05],
        [-2.8144e-06,  8.5612e-07,  1.9980e-06,  ..., -3.5916e-06,
          7.8944e-07, -2.9482e-06],
        [ 6.7038e-05,  7.2197e-05, -4.6892e-05,  ...,  5.8394e-05,
          5.0877e-05,  3.4626e-05],
        [ 2.6100e-06,  1.4318e-05, -2.2424e-07,  ...,  6.0509e-06,
          1.0204e-05, -1.6987e-06],
        [-2.6492e-05, -1.9673e-05,  1.9758e-05,  ..., -2.2782e-05,
         -1.8612e-05, -1.7492e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5660e-08, 5.7116e-08, 6.8142e-08,  ..., 2.7506e-08, 1.5960e-07,
         3.9842e-08],
        [9.5749e-11, 6.2518e-11, 2.9075e-11,  ..., 6.3859e-11, 3.2612e-11,
         3.6053e-11],
        [4.6664e-09, 3.2166e-09, 1.9176e-09,  ..., 3.7318e-09, 1.8495e-09,
         1.3793e-09],
        [1.2809e-09, 1.4262e-09, 5.5880e-10,  ..., 1.1292e-09, 8.9934e-10,
         4.8654e-10],
        [4.1251e-10, 2.3484e-10, 9.4967e-11,  ..., 3.0356e-10, 9.8367e-11,
         1.1865e-10]], device='cuda:0')
optimizer state dict: 219.0
lr: [1.3797827447013867e-06, 1.3797827447013867e-06]
scheduler_last_epoch: 219


Running epoch 1, step 1752, batch 704
Sampled inputs[:2]: tensor([[    0,  5635,   328,  ...,   287, 27260,   271],
        [    0,    21,    66,  ...,  1377,   278,  1634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4436e-04, -1.2252e-04, -1.7474e-04,  ...,  2.1494e-05,
         -1.0472e-04, -3.2034e-05],
        [-1.4678e-06, -9.8348e-07,  9.2015e-07,  ..., -1.2740e-06,
         -1.0952e-06, -1.1027e-06],
        [-4.7684e-06, -3.3081e-06,  3.2037e-06,  ..., -4.0829e-06,
         -3.5167e-06, -3.5912e-06],
        [-3.2336e-06, -2.1607e-06,  2.1309e-06,  ..., -2.7865e-06,
         -2.4140e-06, -2.5183e-06],
        [-3.9041e-06, -2.7418e-06,  2.5183e-06,  ..., -3.4124e-06,
         -3.0100e-06, -2.7716e-06]], device='cuda:0')
Loss: 0.9975284934043884


Running epoch 1, step 1753, batch 705
Sampled inputs[:2]: tensor([[    0,    14, 30840,  ...,   287,   932,    14],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0483e-04, -1.1441e-04, -1.6662e-04,  ...,  2.0418e-04,
         -8.8848e-05, -1.6073e-04],
        [-2.7716e-06, -1.9819e-06,  1.9781e-06,  ..., -2.3618e-06,
         -1.9781e-06, -1.9968e-06],
        [-8.9109e-06, -6.6161e-06,  6.8098e-06,  ..., -7.5400e-06,
         -6.3628e-06, -6.4522e-06],
        [-6.1840e-06, -4.3958e-06,  4.6641e-06,  ..., -5.2452e-06,
         -4.4107e-06, -4.6641e-06],
        [-6.9737e-06, -5.2899e-06,  5.1260e-06,  ..., -6.0350e-06,
         -5.2750e-06, -4.7386e-06]], device='cuda:0')
Loss: 0.9775815606117249


Running epoch 1, step 1754, batch 706
Sampled inputs[:2]: tensor([[    0,   689,  3953,  ...,   461,   943,   352],
        [    0,   850,    13,  ..., 11823,    13, 30706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4299e-04, -2.5110e-04, -1.3310e-04,  ...,  5.1474e-05,
         -2.3258e-05, -2.5562e-04],
        [-4.0978e-06, -3.0100e-06,  3.0659e-06,  ..., -3.4794e-06,
         -2.8834e-06, -2.8685e-06],
        [-1.3143e-05, -1.0014e-05,  1.0446e-05,  ..., -1.1131e-05,
         -9.2536e-06, -9.2983e-06],
        [-9.2536e-06, -6.7204e-06,  7.2718e-06,  ..., -7.8231e-06,
         -6.4820e-06, -6.8098e-06],
        [-1.0088e-05, -7.9274e-06,  7.7337e-06,  ..., -8.7321e-06,
         -7.5847e-06, -6.6757e-06]], device='cuda:0')
Loss: 0.9843772053718567


Running epoch 1, step 1755, batch 707
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0,   266,  4411,  ...,   368,  6388,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9599e-04, -3.8629e-04, -1.3879e-04,  ...,  6.5902e-05,
         -2.0245e-04, -3.4478e-04],
        [-5.4315e-06, -3.9265e-06,  4.1090e-06,  ..., -4.6045e-06,
         -3.7029e-06, -3.7886e-06],
        [-1.7315e-05, -1.2994e-05,  1.3992e-05,  ..., -1.4603e-05,
         -1.1817e-05, -1.2130e-05],
        [-1.2159e-05, -8.6874e-06,  9.6858e-06,  ..., -1.0252e-05,
         -8.2627e-06, -8.9258e-06],
        [-1.3158e-05, -1.0237e-05,  1.0267e-05,  ..., -1.1355e-05,
         -9.6411e-06, -8.5980e-06]], device='cuda:0')
Loss: 0.9395936727523804


Running epoch 1, step 1756, batch 708
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0, 44210,    89,  ...,    43,  1707,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9742e-04, -4.2639e-04, -2.9879e-04,  ...,  7.4859e-05,
         -3.5700e-04, -3.2798e-04],
        [-6.7651e-06, -4.9248e-06,  5.1372e-06,  ..., -5.7518e-06,
         -4.6119e-06, -4.7013e-06],
        [-2.1607e-05, -1.6361e-05,  1.7554e-05,  ..., -1.8269e-05,
         -1.4767e-05, -1.5065e-05],
        [-1.5199e-05, -1.0923e-05,  1.2144e-05,  ..., -1.2860e-05,
         -1.0334e-05, -1.1116e-05],
        [-1.6332e-05, -1.2830e-05,  1.2830e-05,  ..., -1.4126e-05,
         -1.1995e-05, -1.0595e-05]], device='cuda:0')
Loss: 0.9710121750831604


Running epoch 1, step 1757, batch 709
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,   221,   474,  ..., 19245,   565,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8594e-04, -4.4609e-04, -2.6193e-04,  ...,  7.1783e-05,
         -6.1314e-06, -3.3809e-04],
        [-8.1062e-06, -5.8636e-06,  6.0536e-06,  ..., -6.9737e-06,
         -5.5730e-06, -5.6624e-06],
        [-2.5719e-05, -1.9431e-05,  2.0623e-05,  ..., -2.2009e-05,
         -1.7747e-05, -1.7971e-05],
        [-1.8209e-05, -1.3024e-05,  1.4290e-05,  ..., -1.5631e-05,
         -1.2524e-05, -1.3351e-05],
        [-1.9565e-05, -1.5318e-05,  1.5169e-05,  ..., -1.7107e-05,
         -1.4454e-05, -1.2726e-05]], device='cuda:0')
Loss: 0.97716224193573


Running epoch 1, step 1758, batch 710
Sampled inputs[:2]: tensor([[    0,    14,  1075,  ..., 22182,  5948,  8401],
        [    0,   879,    27,  ...,  3958,  2875,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7628e-04, -2.2188e-04, -8.2132e-05,  ...,  9.8793e-05,
         -1.8161e-04, -1.7449e-04],
        [-9.4846e-06, -6.9216e-06,  7.0445e-06,  ..., -8.1360e-06,
         -6.5565e-06, -6.6049e-06],
        [-3.0220e-05, -2.3052e-05,  2.4080e-05,  ..., -2.5809e-05,
         -2.0966e-05, -2.1085e-05],
        [-2.1368e-05, -1.5423e-05,  1.6674e-05,  ..., -1.8284e-05,
         -1.4775e-05, -1.5616e-05],
        [-2.2843e-05, -1.8030e-05,  1.7598e-05,  ..., -1.9923e-05,
         -1.6958e-05, -1.4812e-05]], device='cuda:0')
Loss: 0.9414849281311035


Running epoch 1, step 1759, batch 711
Sampled inputs[:2]: tensor([[   0, 1760,  446,  ...,  329, 1405,  422],
        [   0,  741, 2985,  ...,  199,  769,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0713e-04, -1.3425e-04,  4.7012e-05,  ...,  8.1416e-05,
         -2.9569e-04, -1.4154e-04],
        [-1.0870e-05, -7.8231e-06,  8.0131e-06,  ..., -9.3058e-06,
         -7.4618e-06, -7.6629e-06],
        [-3.4630e-05, -2.6092e-05,  2.7388e-05,  ..., -2.9519e-05,
         -2.3827e-05, -2.4453e-05],
        [-2.4393e-05, -1.7360e-05,  1.8895e-05,  ..., -2.0817e-05,
         -1.6727e-05, -1.8001e-05],
        [-2.6301e-05, -2.0489e-05,  2.0072e-05,  ..., -2.2888e-05,
         -1.9357e-05, -1.7270e-05]], device='cuda:0')
Loss: 0.9306858777999878
Graident accumulation at epoch 1, step 1759, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2495e-05,  6.7772e-05, -1.0031e-04,  ..., -8.1509e-05,
          7.1570e-05, -8.0574e-05],
        [-3.6200e-06, -1.1800e-08,  2.5995e-06,  ..., -4.1631e-06,
         -3.5684e-08, -3.4197e-06],
        [ 5.6871e-05,  6.2368e-05, -3.9464e-05,  ...,  4.9603e-05,
          4.3406e-05,  2.8718e-05],
        [-9.0318e-08,  1.1150e-05,  1.6877e-06,  ...,  3.3641e-06,
          7.5112e-06, -3.3288e-06],
        [-2.6472e-05, -1.9755e-05,  1.9789e-05,  ..., -2.2792e-05,
         -1.8686e-05, -1.7470e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5647e-08, 5.7077e-08, 6.8076e-08,  ..., 2.7485e-08, 1.5953e-07,
         3.9822e-08],
        [9.5772e-11, 6.2517e-11, 2.9111e-11,  ..., 6.3881e-11, 3.2635e-11,
         3.6076e-11],
        [4.6630e-09, 3.2140e-09, 1.9164e-09,  ..., 3.7289e-09, 1.8483e-09,
         1.3786e-09],
        [1.2802e-09, 1.4251e-09, 5.5859e-10,  ..., 1.1285e-09, 8.9872e-10,
         4.8637e-10],
        [4.1279e-10, 2.3502e-10, 9.5275e-11,  ..., 3.0378e-10, 9.8643e-11,
         1.1883e-10]], device='cuda:0')
optimizer state dict: 220.0
lr: [1.3177852447071903e-06, 1.3177852447071903e-06]
scheduler_last_epoch: 220


Running epoch 1, step 1760, batch 712
Sampled inputs[:2]: tensor([[   0, 2849, 1173,  ..., 1481,   12,  287],
        [   0,  365, 5392,  ...,   14,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4584e-04,  1.1501e-04, -1.6025e-05,  ..., -6.8360e-05,
         -2.7876e-04,  1.6029e-05],
        [-1.3262e-06, -9.1642e-07,  1.1101e-06,  ..., -1.0878e-06,
         -8.3074e-07, -8.4564e-07],
        [-4.1127e-06, -3.0249e-06,  3.7104e-06,  ..., -3.4124e-06,
         -2.6375e-06, -2.6673e-06],
        [-3.0547e-06, -2.0862e-06,  2.7120e-06,  ..., -2.5034e-06,
         -1.9073e-06, -2.1011e-06],
        [-2.9653e-06, -2.3246e-06,  2.5928e-06,  ..., -2.5332e-06,
         -2.0862e-06, -1.7583e-06]], device='cuda:0')
Loss: 0.956258237361908


Running epoch 1, step 1761, batch 713
Sampled inputs[:2]: tensor([[    0,  1943,   300,  ..., 43803,   368,  2400],
        [    0,   677,  6499,  ...,  2738,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9968e-04,  1.0624e-04, -2.2373e-06,  ..., -7.3101e-05,
         -4.6273e-04,  2.8485e-05],
        [-2.7120e-06, -1.8552e-06,  2.2128e-06,  ..., -2.2352e-06,
         -1.6950e-06, -1.8142e-06],
        [-8.5533e-06, -6.1393e-06,  7.4506e-06,  ..., -7.0781e-06,
         -5.3793e-06, -5.7817e-06],
        [-6.1989e-06, -4.1574e-06,  5.3197e-06,  ..., -5.0962e-06,
         -3.8445e-06, -4.4107e-06],
        [-6.1840e-06, -4.7088e-06,  5.2154e-06,  ..., -5.2601e-06,
         -4.2617e-06, -3.8594e-06]], device='cuda:0')
Loss: 0.9460873007774353


Running epoch 1, step 1762, batch 714
Sampled inputs[:2]: tensor([[    0,    13, 41550,  ...,    12,   546,  1996],
        [    0,   368,   266,  ...,   591,   767,   824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1033e-04,  1.1766e-04, -6.5457e-05,  ..., -1.3422e-04,
         -4.6187e-04,  1.0069e-04],
        [-4.1723e-06, -2.8312e-06,  3.1665e-06,  ..., -3.4794e-06,
         -2.6859e-06, -2.8349e-06],
        [-1.3143e-05, -9.3728e-06,  1.0639e-05,  ..., -1.0982e-05,
         -8.5086e-06, -9.0003e-06],
        [-9.4026e-06, -6.2734e-06,  7.4804e-06,  ..., -7.8082e-06,
         -6.0052e-06, -6.7502e-06],
        [-9.7305e-06, -7.3165e-06,  7.6294e-06,  ..., -8.3447e-06,
         -6.8694e-06, -6.2138e-06]], device='cuda:0')
Loss: 0.9681644439697266


Running epoch 1, step 1763, batch 715
Sampled inputs[:2]: tensor([[    0,  6809,   344,  ...,    14,  1266,   795],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7101e-04,  1.6384e-04, -1.7792e-04,  ..., -2.4870e-04,
         -6.6974e-04,  2.1434e-04],
        [-5.5805e-06, -3.8147e-06,  4.1500e-06,  ..., -4.6566e-06,
         -3.6061e-06, -3.8184e-06],
        [-1.7583e-05, -1.2562e-05,  1.3977e-05,  ..., -1.4648e-05,
         -1.1384e-05, -1.2055e-05],
        [-1.2502e-05, -8.4043e-06,  9.7454e-06,  ..., -1.0386e-05,
         -8.0168e-06, -9.0003e-06],
        [-1.3053e-05, -9.8050e-06,  1.0073e-05,  ..., -1.1146e-05,
         -9.1940e-06, -8.3297e-06]], device='cuda:0')
Loss: 0.9471970796585083


Running epoch 1, step 1764, batch 716
Sampled inputs[:2]: tensor([[    0,  2258, 10315,  ...,  4185,  9433,   221],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5858e-04,  7.2542e-05, -3.4873e-04,  ..., -2.6926e-04,
         -7.0720e-04,  2.7516e-04],
        [-6.8471e-06, -4.7833e-06,  5.1558e-06,  ..., -5.7891e-06,
         -4.5598e-06, -4.7088e-06],
        [-2.1577e-05, -1.5706e-05,  1.7360e-05,  ..., -1.8194e-05,
         -1.4365e-05, -1.4856e-05],
        [-1.5348e-05, -1.0520e-05,  1.2100e-05,  ..., -1.2919e-05,
         -1.0148e-05, -1.1101e-05],
        [-1.6078e-05, -1.2279e-05,  1.2547e-05,  ..., -1.3888e-05,
         -1.1593e-05, -1.0282e-05]], device='cuda:0')
Loss: 1.009372353553772


Running epoch 1, step 1765, batch 717
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,     9,   287,  ...,   369,  2968,  8347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5075e-05,  1.2379e-04, -3.0624e-04,  ..., -2.9751e-04,
         -7.1370e-04,  3.5052e-04],
        [-8.3521e-06, -5.6736e-06,  5.9344e-06,  ..., -7.1228e-06,
         -5.5730e-06, -5.9232e-06],
        [-2.6286e-05, -1.8671e-05,  2.0057e-05,  ..., -2.2337e-05,
         -1.7613e-05, -1.8582e-05],
        [-1.8477e-05, -1.2353e-05,  1.3754e-05,  ..., -1.5721e-05,
         -1.2308e-05, -1.3664e-05],
        [-2.0072e-05, -1.4782e-05,  1.4752e-05,  ..., -1.7434e-05,
         -1.4439e-05, -1.3232e-05]], device='cuda:0')
Loss: 0.9041387438774109


Running epoch 1, step 1766, batch 718
Sampled inputs[:2]: tensor([[    0, 39200,  1828,  ...,   300,  3067,  4443],
        [    0,   391,  7750,  ...,  4133,   271,   668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7964e-04,  5.1429e-05, -3.4743e-04,  ..., -1.4900e-04,
         -4.4498e-04,  2.2355e-04],
        [-9.7007e-06, -6.5044e-06,  6.8471e-06,  ..., -8.2850e-06,
         -6.5118e-06, -6.8732e-06],
        [-3.0547e-05, -2.1383e-05,  2.3186e-05,  ..., -2.5928e-05,
         -2.0534e-05, -2.1487e-05],
        [-2.1502e-05, -1.4149e-05,  1.5900e-05,  ..., -1.8314e-05,
         -1.4424e-05, -1.5885e-05],
        [-2.3276e-05, -1.6913e-05,  1.7017e-05,  ..., -2.0191e-05,
         -1.6794e-05, -1.5259e-05]], device='cuda:0')
Loss: 0.9366135001182556


Running epoch 1, step 1767, batch 719
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,   287,  3121,   292],
        [    0,  2670, 31283,  ...,    18,  9106,  1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1956e-04,  5.8586e-05, -2.6580e-04,  ..., -2.8474e-04,
         -2.8506e-04,  2.0523e-04],
        [-1.0967e-05, -7.4878e-06,  7.6964e-06,  ..., -9.4697e-06,
         -7.5474e-06, -7.8417e-06],
        [-3.4571e-05, -2.4557e-05,  2.6092e-05,  ..., -2.9579e-05,
         -2.3797e-05, -2.4378e-05],
        [-2.4453e-05, -1.6369e-05,  1.7941e-05,  ..., -2.1055e-05,
         -1.6898e-05, -1.8209e-05],
        [-2.6509e-05, -1.9535e-05,  1.9312e-05,  ..., -2.3156e-05,
         -1.9521e-05, -1.7434e-05]], device='cuda:0')
Loss: 0.9777102470397949
Graident accumulation at epoch 1, step 1767, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.5290e-05,  6.6854e-05, -1.1686e-04,  ..., -1.0183e-04,
          3.5907e-05, -5.1994e-05],
        [-4.3547e-06, -7.5940e-07,  3.1092e-06,  ..., -4.6937e-06,
         -7.8686e-07, -3.8619e-06],
        [ 4.7727e-05,  5.3675e-05, -3.2908e-05,  ...,  4.1685e-05,
          3.6686e-05,  2.3409e-05],
        [-2.5266e-06,  8.3980e-06,  3.3130e-06,  ...,  9.2220e-07,
          5.0703e-06, -4.8169e-06],
        [-2.6476e-05, -1.9733e-05,  1.9741e-05,  ..., -2.2829e-05,
         -1.8770e-05, -1.7466e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5639e-08, 5.7023e-08, 6.8079e-08,  ..., 2.7539e-08, 1.5945e-07,
         3.9824e-08],
        [9.5796e-11, 6.2511e-11, 2.9141e-11,  ..., 6.3907e-11, 3.2659e-11,
         3.6101e-11],
        [4.6595e-09, 3.2114e-09, 1.9152e-09,  ..., 3.7261e-09, 1.8470e-09,
         1.3778e-09],
        [1.2795e-09, 1.4239e-09, 5.5836e-10,  ..., 1.1278e-09, 8.9811e-10,
         4.8622e-10],
        [4.1308e-10, 2.3517e-10, 9.5553e-11,  ..., 3.0401e-10, 9.8926e-11,
         1.1901e-10]], device='cuda:0')
optimizer state dict: 221.0
lr: [1.2571144629157273e-06, 1.2571144629157273e-06]
scheduler_last_epoch: 221


Running epoch 1, step 1768, batch 720
Sampled inputs[:2]: tensor([[    0,   741,   266,  ...,   271,  5166,   596],
        [    0, 31571,    13,  ...,   367,  2177,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3788e-05,  7.0592e-05, -5.7161e-05,  ..., -6.0164e-05,
         -2.5197e-04, -9.7354e-05],
        [-1.2964e-06, -9.6858e-07,  1.0058e-06,  ..., -1.1101e-06,
         -8.9034e-07, -9.0897e-07],
        [-4.2021e-06, -3.2037e-06,  3.4720e-06,  ..., -3.5167e-06,
         -2.8014e-06, -2.9057e-06],
        [-3.0249e-06, -2.1905e-06,  2.4289e-06,  ..., -2.5630e-06,
         -2.0415e-06, -2.2203e-06],
        [-3.0547e-06, -2.4438e-06,  2.4438e-06,  ..., -2.6226e-06,
         -2.2203e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9572649002075195


Running epoch 1, step 1769, batch 721
Sampled inputs[:2]: tensor([[    0,  2663,   328,  ...,   342,   266,  1163],
        [    0,   654,   300,  ..., 21762,  3597, 11117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0534e-04,  3.3361e-05, -1.0235e-06,  ..., -5.9745e-05,
         -1.8963e-04, -1.4506e-04],
        [-2.6524e-06, -1.9968e-06,  2.0117e-06,  ..., -2.2799e-06,
         -1.8664e-06, -1.8217e-06],
        [-8.5533e-06, -6.6310e-06,  6.8992e-06,  ..., -7.2718e-06,
         -5.9456e-06, -5.8860e-06],
        [-6.1244e-06, -4.5002e-06,  4.8280e-06,  ..., -5.2303e-06,
         -4.2468e-06, -4.4107e-06],
        [-6.3032e-06, -5.1111e-06,  4.9323e-06,  ..., -5.4836e-06,
         -4.7386e-06, -3.9935e-06]], device='cuda:0')
Loss: 0.9874655604362488


Running epoch 1, step 1770, batch 722
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,  266, 1371, 4811],
        [   0, 2085,   12,  ...,  496,   14,  747]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1667e-04,  7.8607e-05, -2.2023e-04,  ..., -1.1976e-04,
         -2.6713e-04, -5.1948e-04],
        [-4.0010e-06, -2.9802e-06,  2.9169e-06,  ..., -3.4720e-06,
         -2.9020e-06, -2.7753e-06],
        [-1.2636e-05, -9.7305e-06,  9.8646e-06,  ..., -1.0833e-05,
         -9.0301e-06, -8.7321e-06],
        [-9.1046e-06, -6.6310e-06,  6.8992e-06,  ..., -7.8678e-06,
         -6.5267e-06, -6.6608e-06],
        [-9.5516e-06, -7.6741e-06,  7.2271e-06,  ..., -8.3745e-06,
         -7.3314e-06, -6.0946e-06]], device='cuda:0')
Loss: 0.9744186401367188


Running epoch 1, step 1771, batch 723
Sampled inputs[:2]: tensor([[   0, 2923,  391,  ...,   14, 5424,  298],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2417e-05,  2.1730e-05, -4.3905e-04,  ..., -2.1229e-04,
         -5.3795e-04, -6.5508e-04],
        [-5.3123e-06, -3.9786e-06,  3.8370e-06,  ..., -4.6641e-06,
         -3.8855e-06, -3.6620e-06],
        [-1.6570e-05, -1.2860e-05,  1.2890e-05,  ..., -1.4380e-05,
         -1.2010e-05, -1.1325e-05],
        [-1.1966e-05, -8.7619e-06,  9.0152e-06,  ..., -1.0490e-05,
         -8.7172e-06, -8.6874e-06],
        [-1.2666e-05, -1.0237e-05,  9.5814e-06,  ..., -1.1206e-05,
         -9.8050e-06, -7.9796e-06]], device='cuda:0')
Loss: 0.9276213645935059


Running epoch 1, step 1772, batch 724
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0, 1431,  221,  ...,  756,  409,  275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3464e-04, -4.0484e-05, -4.0966e-04,  ..., -1.2619e-04,
         -7.2001e-04, -5.3938e-04],
        [-6.6459e-06, -4.7497e-06,  4.8876e-06,  ..., -5.8189e-06,
         -4.7088e-06, -4.6305e-06],
        [-2.0891e-05, -1.5438e-05,  1.6510e-05,  ..., -1.8060e-05,
         -1.4633e-05, -1.4395e-05],
        [-1.4976e-05, -1.0446e-05,  1.1489e-05,  ..., -1.3098e-05,
         -1.0557e-05, -1.0952e-05],
        [-1.5721e-05, -1.2130e-05,  1.2055e-05,  ..., -1.3843e-05,
         -1.1772e-05, -9.9912e-06]], device='cuda:0')
Loss: 0.9286178350448608


Running epoch 1, step 1773, batch 725
Sampled inputs[:2]: tensor([[   0,  352,  644,  ..., 2928,  590, 3040],
        [   0,   12, 5820,  ...,  221,  380,  560]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7828e-04,  1.0359e-04, -3.4437e-04,  ..., -2.5068e-05,
         -5.4820e-04, -3.2672e-04],
        [-7.9423e-06, -5.6811e-06,  5.7556e-06,  ..., -7.0259e-06,
         -5.6922e-06, -5.6140e-06],
        [-2.4766e-05, -1.8299e-05,  1.9357e-05,  ..., -2.1577e-05,
         -1.7479e-05, -1.7196e-05],
        [-1.7852e-05, -1.2457e-05,  1.3530e-05,  ..., -1.5780e-05,
         -1.2718e-05, -1.3217e-05],
        [-1.8910e-05, -1.4573e-05,  1.4335e-05,  ..., -1.6779e-05,
         -1.4260e-05, -1.2137e-05]], device='cuda:0')
Loss: 0.9594845771789551


Running epoch 1, step 1774, batch 726
Sampled inputs[:2]: tensor([[    0,  1085,  4878,  ...,   298,   894,   496],
        [    0,   409, 22809,  ...,   342,   720,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4074e-06,  1.4109e-04, -3.9677e-04,  ..., -6.7766e-05,
         -6.7396e-04, -3.5242e-04],
        [-9.2313e-06, -6.6720e-06,  6.7838e-06,  ..., -8.1658e-06,
         -6.5789e-06, -6.5193e-06],
        [-2.8908e-05, -2.1577e-05,  2.2888e-05,  ..., -2.5183e-05,
         -2.0310e-05, -2.0072e-05],
        [-2.0787e-05, -1.4663e-05,  1.5974e-05,  ..., -1.8358e-05,
         -1.4730e-05, -1.5393e-05],
        [-2.1949e-05, -1.7077e-05,  1.6838e-05,  ..., -1.9491e-05,
         -1.6496e-05, -1.4089e-05]], device='cuda:0')
Loss: 0.9590926170349121


Running epoch 1, step 1775, batch 727
Sampled inputs[:2]: tensor([[   0,  394,  292,  ..., 1711,  365,  897],
        [   0, 1920,   19,  ..., 5232,  796, 1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1594e-05,  1.3048e-04, -5.6093e-04,  ..., -1.1808e-04,
         -6.7665e-04, -2.8278e-04],
        [-1.0595e-05, -7.6480e-06,  7.7896e-06,  ..., -9.3505e-06,
         -7.5400e-06, -7.4580e-06],
        [-3.3170e-05, -2.4736e-05,  2.6256e-05,  ..., -2.8878e-05,
         -2.3291e-05, -2.3007e-05],
        [-2.3842e-05, -1.6809e-05,  1.8328e-05,  ..., -2.0996e-05,
         -1.6846e-05, -1.7598e-05],
        [-2.5183e-05, -1.9550e-05,  1.9327e-05,  ..., -2.2352e-05,
         -1.8910e-05, -1.6145e-05]], device='cuda:0')
Loss: 0.9759435057640076
Graident accumulation at epoch 1, step 1775, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3920e-05,  7.3216e-05, -1.6127e-04,  ..., -1.0346e-04,
         -3.5348e-05, -7.5073e-05],
        [-4.9787e-06, -1.4483e-06,  3.5772e-06,  ..., -5.1594e-06,
         -1.4622e-06, -4.2215e-06],
        [ 3.9637e-05,  4.5834e-05, -2.6992e-05,  ...,  3.4628e-05,
          3.0688e-05,  1.8767e-05],
        [-4.6581e-06,  5.8774e-06,  4.8145e-06,  ..., -1.2696e-06,
          2.8787e-06, -6.0950e-06],
        [-2.6347e-05, -1.9714e-05,  1.9700e-05,  ..., -2.2781e-05,
         -1.8784e-05, -1.7334e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5584e-08, 5.6983e-08, 6.8325e-08,  ..., 2.7525e-08, 1.5975e-07,
         3.9865e-08],
        [9.5813e-11, 6.2507e-11, 2.9172e-11,  ..., 6.3931e-11, 3.2683e-11,
         3.6121e-11],
        [4.6559e-09, 3.2088e-09, 1.9139e-09,  ..., 3.7232e-09, 1.8457e-09,
         1.3769e-09],
        [1.2788e-09, 1.4228e-09, 5.5813e-10,  ..., 1.1272e-09, 8.9749e-10,
         4.8604e-10],
        [4.1330e-10, 2.3532e-10, 9.5831e-11,  ..., 3.0421e-10, 9.9184e-11,
         1.1915e-10]], device='cuda:0')
optimizer state dict: 222.0
lr: [1.1977796703520529e-06, 1.1977796703520529e-06]
scheduler_last_epoch: 222


Running epoch 1, step 1776, batch 728
Sampled inputs[:2]: tensor([[   0, 9829,  292,  ..., 2928, 1029,  271],
        [   0,  953,  328,  ..., 2245,   12, 1253]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3818e-05,  7.8938e-05,  2.7683e-04,  ...,  1.9124e-04,
          9.2962e-05,  5.6846e-05],
        [-1.2964e-06, -1.0058e-06,  8.7917e-07,  ..., -1.1921e-06,
         -9.8348e-07, -9.4250e-07],
        [-4.2319e-06, -3.3975e-06,  3.0994e-06,  ..., -3.8445e-06,
         -3.1739e-06, -3.0696e-06],
        [-2.9653e-06, -2.2799e-06,  2.0862e-06,  ..., -2.7418e-06,
         -2.2501e-06, -2.2650e-06],
        [-3.2037e-06, -2.6524e-06,  2.2799e-06,  ..., -2.9802e-06,
         -2.5630e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9778968691825867


Running epoch 1, step 1777, batch 729
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   630,  3765, 19107],
        [    0,  2228,  1416,  ...,  3766,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8941e-05,  7.8938e-05,  2.4744e-04,  ...,  2.1435e-04,
         -1.6849e-04,  8.2501e-05],
        [-2.6450e-06, -1.9148e-06,  1.9073e-06,  ..., -2.3246e-06,
         -1.8589e-06, -1.8477e-06],
        [-8.5235e-06, -6.3926e-06,  6.5714e-06,  ..., -7.4059e-06,
         -5.9158e-06, -5.9307e-06],
        [-6.0350e-06, -4.3064e-06,  4.5151e-06,  ..., -5.3048e-06,
         -4.2170e-06, -4.4256e-06],
        [-6.2883e-06, -4.9025e-06,  4.7237e-06,  ..., -5.6028e-06,
         -4.6939e-06, -4.0829e-06]], device='cuda:0')
Loss: 0.9594025015830994


Running epoch 1, step 1778, batch 730
Sampled inputs[:2]: tensor([[   0,  342,  970,  ...,  401, 2907, 1657],
        [   0, 3087,  342,  ...,   14,  381, 1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9098e-04,  2.7491e-04,  3.3851e-04,  ...,  7.3735e-05,
         -1.9160e-04,  1.4109e-04],
        [-4.0382e-06, -2.9430e-06,  2.7753e-06,  ..., -3.5390e-06,
         -2.9318e-06, -2.8089e-06],
        [-1.3113e-05, -9.9838e-06,  9.6411e-06,  ..., -1.1459e-05,
         -9.5367e-06, -9.1791e-06],
        [-9.1642e-06, -6.5863e-06,  6.5267e-06,  ..., -8.0615e-06,
         -6.6608e-06, -6.6906e-06],
        [-9.7007e-06, -7.6741e-06,  6.9439e-06,  ..., -8.6874e-06,
         -7.5400e-06, -6.3330e-06]], device='cuda:0')
Loss: 0.9581925272941589


Running epoch 1, step 1779, batch 731
Sampled inputs[:2]: tensor([[    0,   591,   953,  ...,  4118,  5750,   292],
        [    0, 24063,   717,  ...,  2228,  1416,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2501e-04,  1.3317e-04,  2.2264e-04,  ...,  1.2016e-04,
         -2.1031e-04,  2.2888e-04],
        [-5.3942e-06, -3.8035e-06,  3.7365e-06,  ..., -4.7758e-06,
         -3.8631e-06, -3.8296e-06],
        [-1.7315e-05, -1.2740e-05,  1.2860e-05,  ..., -1.5199e-05,
         -1.2368e-05, -1.2219e-05],
        [-1.2070e-05, -8.3819e-06,  8.6725e-06,  ..., -1.0714e-05,
         -8.6427e-06, -8.9407e-06],
        [-1.2964e-05, -9.9242e-06,  9.3877e-06,  ..., -1.1653e-05,
         -9.8944e-06, -8.5086e-06]], device='cuda:0')
Loss: 0.9458197355270386


Running epoch 1, step 1780, batch 732
Sampled inputs[:2]: tensor([[   0,  266, 4908,  ..., 1209,  328, 1603],
        [   0,  275, 2351,  ...,   14, 4520,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1268e-05,  6.8861e-05,  6.9198e-05,  ...,  2.7370e-05,
         -4.4355e-04,  1.7754e-04],
        [-6.7949e-06, -4.6529e-06,  4.7423e-06,  ..., -5.9977e-06,
         -4.7199e-06, -4.8354e-06],
        [-2.1607e-05, -1.5497e-05,  1.6212e-05,  ..., -1.8865e-05,
         -1.4991e-05, -1.5229e-05],
        [-1.5095e-05, -1.0192e-05,  1.0967e-05,  ..., -1.3351e-05,
         -1.0498e-05, -1.1206e-05],
        [-1.6227e-05, -1.2100e-05,  1.1846e-05,  ..., -1.4499e-05,
         -1.2040e-05, -1.0625e-05]], device='cuda:0')
Loss: 0.9122331142425537


Running epoch 1, step 1781, batch 733
Sampled inputs[:2]: tensor([[    0,   300,   344,  ...,    14,  5077,  2715],
        [    0, 10205,   342,  ...,  2523,  4729, 13753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1579e-04,  7.7626e-05,  7.8427e-05,  ...,  1.3686e-04,
         -3.6686e-04,  2.2741e-04],
        [-8.2105e-06, -5.6289e-06,  5.7630e-06,  ..., -7.1675e-06,
         -5.6587e-06, -5.8115e-06],
        [-2.6137e-05, -1.8761e-05,  1.9684e-05,  ..., -2.2590e-05,
         -1.8045e-05, -1.8358e-05],
        [-1.8239e-05, -1.2353e-05,  1.3322e-05,  ..., -1.5929e-05,
         -1.2599e-05, -1.3456e-05],
        [-1.9610e-05, -1.4633e-05,  1.4380e-05,  ..., -1.7345e-05,
         -1.4469e-05, -1.2800e-05]], device='cuda:0')
Loss: 0.9859911203384399


Running epoch 1, step 1782, batch 734
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,  7106,   287,   266],
        [    0,  1934,  2413,  ..., 19697,    13, 16325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2217e-04,  1.7954e-04,  1.9574e-05,  ...,  8.5512e-05,
         -5.4284e-04,  2.1897e-04],
        [-9.5293e-06, -6.5006e-06,  6.7987e-06,  ..., -8.3074e-06,
         -6.5267e-06, -6.6869e-06],
        [-3.0369e-05, -2.1711e-05,  2.3246e-05,  ..., -2.6241e-05,
         -2.0817e-05, -2.1189e-05],
        [-2.1264e-05, -1.4305e-05,  1.5795e-05,  ..., -1.8522e-05,
         -1.4566e-05, -1.5602e-05],
        [-2.2635e-05, -1.6853e-05,  1.6853e-05,  ..., -2.0027e-05,
         -1.6615e-05, -1.4655e-05]], device='cuda:0')
Loss: 0.9747963547706604


Running epoch 1, step 1783, batch 735
Sampled inputs[:2]: tensor([[   0,  278, 1478,  ...,  266, 1607, 1220],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9869e-05,  1.5732e-04, -5.4927e-05,  ...,  1.3866e-04,
         -4.8910e-04,  2.1664e-04],
        [-1.0915e-05, -7.4282e-06,  7.8268e-06,  ..., -9.4920e-06,
         -7.4022e-06, -7.6555e-06],
        [-3.4630e-05, -2.4706e-05,  2.6658e-05,  ..., -2.9862e-05,
         -2.3499e-05, -2.4155e-05],
        [-2.4334e-05, -1.6317e-05,  1.8179e-05,  ..., -2.1130e-05,
         -1.6503e-05, -1.7866e-05],
        [-2.5839e-05, -1.9193e-05,  1.9342e-05,  ..., -2.2784e-05,
         -1.8790e-05, -1.6712e-05]], device='cuda:0')
Loss: 0.941828191280365
Graident accumulation at epoch 1, step 1783, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.8541e-05,  8.1626e-05, -1.5063e-04,  ..., -7.9245e-05,
         -8.0724e-05, -4.5902e-05],
        [-5.5724e-06, -2.0463e-06,  4.0022e-06,  ..., -5.5927e-06,
         -2.0562e-06, -4.5649e-06],
        [ 3.2210e-05,  3.8780e-05, -2.1627e-05,  ...,  2.8179e-05,
          2.5270e-05,  1.4475e-05],
        [-6.6256e-06,  3.6580e-06,  6.1510e-06,  ..., -3.2556e-06,
          9.4050e-07, -7.2722e-06],
        [-2.6296e-05, -1.9662e-05,  1.9664e-05,  ..., -2.2781e-05,
         -1.8784e-05, -1.7272e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5529e-08, 5.6951e-08, 6.8260e-08,  ..., 2.7517e-08, 1.5983e-07,
         3.9872e-08],
        [9.5836e-11, 6.2499e-11, 2.9204e-11,  ..., 6.3957e-11, 3.2705e-11,
         3.6143e-11],
        [4.6525e-09, 3.2062e-09, 1.9127e-09,  ..., 3.7204e-09, 1.8444e-09,
         1.3761e-09],
        [1.2781e-09, 1.4216e-09, 5.5791e-10,  ..., 1.1265e-09, 8.9687e-10,
         4.8588e-10],
        [4.1355e-10, 2.3545e-10, 9.6109e-11,  ..., 3.0442e-10, 9.9438e-11,
         1.1931e-10]], device='cuda:0')
optimizer state dict: 223.0
lr: [1.1397899338904206e-06, 1.1397899338904206e-06]
scheduler_last_epoch: 223


Running epoch 1, step 1784, batch 736
Sampled inputs[:2]: tensor([[    0,   733,   560,  ...,  1172, 22808,   271],
        [    0,   292, 21215,  ...,   266,   818,  1527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7739e-05,  2.2784e-05, -8.0896e-05,  ...,  2.4017e-04,
         -2.6142e-07,  3.2728e-04],
        [-1.4082e-06, -9.5367e-07,  9.1642e-07,  ..., -1.2442e-06,
         -9.8348e-07, -1.0356e-06],
        [-4.4107e-06, -3.1143e-06,  3.1292e-06,  ..., -3.8445e-06,
         -3.0994e-06, -3.2037e-06],
        [-3.1292e-06, -2.0862e-06,  2.1309e-06,  ..., -2.7716e-06,
         -2.2054e-06, -2.3991e-06],
        [-3.3826e-06, -2.4587e-06,  2.3395e-06,  ..., -3.0100e-06,
         -2.5183e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9503658413887024


Running epoch 1, step 1785, batch 737
Sampled inputs[:2]: tensor([[    0,   767,  1345,  ...,   276,   327,   328],
        [    0,   365,   984,  ..., 18562,  4237, 31813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4147e-05,  1.1585e-04, -2.9920e-05,  ...,  3.2855e-04,
          1.8691e-04,  3.4864e-04],
        [-2.7418e-06, -1.9521e-06,  1.7844e-06,  ..., -2.4214e-06,
         -1.9893e-06, -2.0042e-06],
        [ 1.9948e-04,  1.9335e-04, -2.5243e-04,  ...,  1.7040e-04,
          2.3833e-04,  3.1716e-05],
        [-6.1542e-06, -4.3213e-06,  4.1872e-06,  ..., -5.4538e-06,
         -4.5300e-06, -4.6939e-06],
        [-6.6012e-06, -5.0515e-06,  4.5449e-06,  ..., -5.8860e-06,
         -5.1260e-06, -4.4554e-06]], device='cuda:0')
Loss: 0.9647923111915588


Running epoch 1, step 1786, batch 738
Sampled inputs[:2]: tensor([[   0,  391, 9095,  ...,  417,  199, 2038],
        [   0,  586,  940,  ..., 1471, 2612,  591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8923e-05,  7.6703e-05, -1.3258e-04,  ...,  3.5261e-04,
          1.7534e-04,  3.2150e-04],
        [-4.1574e-06, -2.9206e-06,  2.9318e-06,  ..., -3.5912e-06,
         -2.8387e-06, -2.9318e-06],
        [ 1.9498e-04,  1.9011e-04, -2.4859e-04,  ...,  1.6667e-04,
          2.3559e-04,  2.8751e-05],
        [ 6.8671e-05,  7.5199e-05, -3.1122e-05,  ...,  5.5668e-05,
          9.3472e-05,  4.0448e-05],
        [-9.8199e-06, -7.4655e-06,  7.2122e-06,  ..., -8.5980e-06,
         -7.2271e-06, -6.4373e-06]], device='cuda:0')
Loss: 0.9727175831794739


Running epoch 1, step 1787, batch 739
Sampled inputs[:2]: tensor([[   0, 2680,  271,  ..., 4971,  278,  266],
        [   0,  278,  554,  ...,  365, 3125,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6676e-04, -1.7217e-05, -2.2161e-04,  ...,  3.4854e-04,
          7.2487e-05,  2.8355e-04],
        [-5.6177e-06, -3.8743e-06,  3.9227e-06,  ..., -4.8801e-06,
         -3.8072e-06, -4.0121e-06],
        [ 1.9054e-04,  1.8715e-04, -2.4532e-04,  ...,  1.6286e-04,
          2.3268e-04,  2.5562e-05],
        [ 6.5646e-05,  7.3232e-05, -2.8961e-05,  ...,  5.3000e-05,
          9.1460e-05,  3.8153e-05],
        [-1.3426e-05, -9.9242e-06,  9.7752e-06,  ..., -1.1742e-05,
         -9.7305e-06, -8.8513e-06]], device='cuda:0')
Loss: 0.9560052752494812


Running epoch 1, step 1788, batch 740
Sampled inputs[:2]: tensor([[   0,  527, 2811,  ...,  287, 1288,  352],
        [   0, 2663,  328,  ...,  266, 1040, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5209e-05, -6.9495e-05, -1.1706e-04,  ...,  2.8444e-04,
          7.9500e-05,  3.3254e-04],
        [-6.9216e-06, -4.8354e-06,  4.8988e-06,  ..., -5.9977e-06,
         -4.7982e-06, -4.9137e-06],
        [ 1.8648e-04,  1.8399e-04, -2.4206e-04,  ...,  1.5939e-04,
          2.2960e-04,  2.2731e-05],
        [ 6.2666e-05,  7.1057e-05, -2.6607e-05,  ...,  5.0452e-05,
          8.9195e-05,  3.5948e-05],
        [-1.6496e-05, -1.2428e-05,  1.2159e-05,  ..., -1.4439e-05,
         -1.2234e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.9636679887771606


Running epoch 1, step 1789, batch 741
Sampled inputs[:2]: tensor([[   0,  221,  381,  ...,  360, 8978,   14],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5894e-05, -1.2037e-04,  3.1050e-05,  ...,  2.9375e-04,
          2.6431e-04,  2.9006e-04],
        [-8.2403e-06, -5.8189e-06,  5.8375e-06,  ..., -7.1898e-06,
         -5.7481e-06, -5.8487e-06],
        [ 1.8237e-04,  1.8089e-04, -2.3890e-04,  ...,  1.5577e-04,
          2.2669e-04,  1.9915e-05],
        [ 5.9745e-05,  6.8911e-05, -2.4446e-05,  ...,  4.7815e-05,
          8.7079e-05,  3.3802e-05],
        [-1.9610e-05, -1.4856e-05,  1.4499e-05,  ..., -1.7241e-05,
         -1.4588e-05, -1.2830e-05]], device='cuda:0')
Loss: 0.986193835735321


Running epoch 1, step 1790, batch 742
Sampled inputs[:2]: tensor([[   0,   12,  344,  ...,  824,   12,  968],
        [   0,  287, 1477,  ...,  997,  292, 4471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5650e-05, -6.9478e-06,  2.4698e-04,  ...,  2.3932e-04,
          2.0818e-04,  2.9643e-04],
        [-9.6262e-06, -6.8545e-06,  6.8285e-06,  ..., -8.3819e-06,
         -6.7912e-06, -6.7428e-06],
        [ 1.7814e-04,  1.7752e-04, -2.3567e-04,  ...,  1.5209e-04,
          2.2343e-04,  1.7128e-05],
        [ 5.6660e-05,  6.6616e-05, -2.2151e-05,  ...,  4.5147e-05,
          8.4725e-05,  3.1671e-05],
        [-2.2858e-05, -1.7524e-05,  1.6913e-05,  ..., -2.0131e-05,
         -1.7241e-05, -1.4827e-05]], device='cuda:0')
Loss: 0.9864663481712341


Running epoch 1, step 1791, batch 743
Sampled inputs[:2]: tensor([[    0,   266,  9823,  ...,    14,  1062,  7676],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6143e-05, -1.0432e-04,  9.0958e-05,  ...,  1.1826e-04,
          2.0749e-04,  1.9090e-04],
        [-1.1012e-05, -7.8455e-06,  7.7635e-06,  ..., -9.6112e-06,
         -7.7672e-06, -7.7784e-06],
        [ 1.7385e-04,  1.7438e-04, -2.3251e-04,  ...,  1.4835e-04,
          2.2045e-04,  1.3984e-05],
        [ 5.3650e-05,  6.4515e-05, -2.0035e-05,  ...,  4.2495e-05,
          8.2624e-05,  2.9332e-05],
        [-2.6271e-05, -2.0087e-05,  1.9312e-05,  ..., -2.3156e-05,
         -1.9759e-05, -1.7166e-05]], device='cuda:0')
Loss: 0.9471335411071777
Graident accumulation at epoch 1, step 1791, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4073e-05,  6.3032e-05, -1.2647e-04,  ..., -5.9495e-05,
         -5.1902e-05, -2.2221e-05],
        [-6.1163e-06, -2.6262e-06,  4.3783e-06,  ..., -5.9945e-06,
         -2.6273e-06, -4.8863e-06],
        [ 4.6374e-05,  5.2340e-05, -4.2715e-05,  ...,  4.0196e-05,
          4.4787e-05,  1.4426e-05],
        [-5.9805e-07,  9.7437e-06,  3.5324e-06,  ...,  1.3194e-06,
          9.1088e-06, -3.6118e-06],
        [-2.6293e-05, -1.9705e-05,  1.9629e-05,  ..., -2.2819e-05,
         -1.8882e-05, -1.7261e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5474e-08, 5.6905e-08, 6.8200e-08,  ..., 2.7504e-08, 1.5971e-07,
         3.9868e-08],
        [9.5862e-11, 6.2498e-11, 2.9235e-11,  ..., 6.3985e-11, 3.2733e-11,
         3.6167e-11],
        [4.6780e-09, 3.2334e-09, 1.9649e-09,  ..., 3.7386e-09, 1.8911e-09,
         1.3750e-09],
        [1.2797e-09, 1.4244e-09, 5.5775e-10,  ..., 1.1271e-09, 9.0280e-10,
         4.8625e-10],
        [4.1383e-10, 2.3562e-10, 9.6386e-11,  ..., 3.0465e-10, 9.9729e-11,
         1.1949e-10]], device='cuda:0')
optimizer state dict: 224.0
lr: [1.083154114868752e-06, 1.083154114868752e-06]
scheduler_last_epoch: 224


Running epoch 1, step 1792, batch 744
Sampled inputs[:2]: tensor([[    0,   401,  9370,  ...,     9,   287,   518],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6499e-05, -2.2843e-05,  1.6408e-04,  ...,  4.7495e-05,
          2.8782e-05, -1.5511e-04],
        [-1.3113e-06, -8.2329e-07,  1.1027e-06,  ..., -1.0878e-06,
         -8.2329e-07, -9.2387e-07],
        [-4.1127e-06, -2.6673e-06,  3.6657e-06,  ..., -3.3528e-06,
         -2.5481e-06, -2.8312e-06],
        [-2.9653e-06, -1.8179e-06,  2.6077e-06,  ..., -2.4438e-06,
         -1.8477e-06, -2.1756e-06],
        [-2.9206e-06, -1.9670e-06,  2.5332e-06,  ..., -2.4438e-06,
         -1.9372e-06, -1.8626e-06]], device='cuda:0')
Loss: 0.9214860796928406


Running epoch 1, step 1793, batch 745
Sampled inputs[:2]: tensor([[    0,  1855,    14,  ...,    12,   287, 16479],
        [    0,   391,  1351,  ...,    13,    40,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1299e-04, -1.9396e-05,  9.0409e-05,  ..., -6.1641e-06,
          1.0365e-05, -1.0820e-04],
        [-2.6375e-06, -1.7919e-06,  2.0713e-06,  ..., -2.2352e-06,
         -1.7472e-06, -1.8328e-06],
        [-8.1658e-06, -5.7518e-06,  6.8843e-06,  ..., -6.7949e-06,
         -5.3197e-06, -5.5730e-06],
        [-5.9307e-06, -3.9339e-06,  4.8727e-06,  ..., -4.9919e-06,
         -3.9041e-06, -4.3213e-06],
        [-5.9903e-06, -4.3809e-06,  4.9025e-06,  ..., -5.0962e-06,
         -4.1872e-06, -3.7476e-06]], device='cuda:0')
Loss: 0.9513140916824341


Running epoch 1, step 1794, batch 746
Sampled inputs[:2]: tensor([[    0,   278,   266,  ..., 10639,   292,  4723],
        [    0,  5522,  5662,  ...,   638,  1231,  1098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8778e-04, -4.3373e-05,  1.4649e-05,  ..., -5.6204e-05,
          8.5758e-05,  4.9708e-06],
        [-4.0680e-06, -2.8275e-06,  3.1441e-06,  ..., -3.4496e-06,
         -2.7381e-06, -2.8312e-06],
        [-1.2547e-05, -9.0748e-06,  1.0386e-05,  ..., -1.0520e-05,
         -8.4192e-06, -8.6576e-06],
        [-9.0152e-06, -6.1542e-06,  7.3016e-06,  ..., -7.6145e-06,
         -6.0499e-06, -6.5714e-06],
        [-9.3430e-06, -7.0184e-06,  7.5102e-06,  ..., -8.0168e-06,
         -6.7204e-06, -5.9530e-06]], device='cuda:0')
Loss: 0.9362791776657104


Running epoch 1, step 1795, batch 747
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1385e-04, -7.3934e-05,  9.5963e-05,  ..., -1.0603e-04,
          1.2461e-05,  1.6243e-04],
        [-5.5358e-06, -3.6843e-06,  4.0792e-06,  ..., -4.6939e-06,
         -3.7067e-06, -3.9861e-06],
        [-1.6987e-05, -1.1787e-05,  1.3456e-05,  ..., -1.4186e-05,
         -1.1325e-05, -1.2040e-05],
        [-1.2174e-05, -7.9572e-06,  9.4026e-06,  ..., -1.0282e-05,
         -8.1509e-06, -9.1344e-06],
        [-1.2949e-05, -9.2685e-06,  9.9242e-06,  ..., -1.1057e-05,
         -9.2089e-06, -8.5309e-06]], device='cuda:0')
Loss: 0.9265715479850769


Running epoch 1, step 1796, batch 748
Sampled inputs[:2]: tensor([[    0,    13,  1311,  ...,   271,   795,   957],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8211e-04, -8.0231e-05,  1.5341e-04,  ..., -1.3490e-04,
         -1.3598e-05,  2.0549e-04],
        [-6.8322e-06, -4.6305e-06,  4.9882e-06,  ..., -5.8860e-06,
         -4.6752e-06, -4.9025e-06],
        [-2.0951e-05, -1.4856e-05,  1.6481e-05,  ..., -1.7792e-05,
         -1.4275e-05, -1.4827e-05],
        [-1.5080e-05, -1.0058e-05,  1.1519e-05,  ..., -1.2979e-05,
         -1.0356e-05, -1.1310e-05],
        [-1.5989e-05, -1.1712e-05,  1.2174e-05,  ..., -1.3888e-05,
         -1.1623e-05, -1.0498e-05]], device='cuda:0')
Loss: 0.9372382760047913


Running epoch 1, step 1797, batch 749
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,   292,   380,  ...,   287, 10086,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2438e-04, -2.5621e-05,  1.4352e-04,  ..., -3.5440e-05,
          5.9738e-05,  1.9432e-04],
        [-8.3297e-06, -5.5097e-06,  5.8934e-06,  ..., -7.2494e-06,
         -5.6662e-06, -6.0350e-06],
        [-2.5362e-05, -1.7568e-05,  1.9372e-05,  ..., -2.1756e-05,
         -1.7211e-05, -1.8045e-05],
        [-1.8165e-05, -1.1839e-05,  1.3426e-05,  ..., -1.5795e-05,
         -1.2428e-05, -1.3679e-05],
        [-1.9714e-05, -1.4022e-05,  1.4558e-05,  ..., -1.7226e-05,
         -1.4171e-05, -1.3046e-05]], device='cuda:0')
Loss: 0.9491976499557495


Running epoch 1, step 1798, batch 750
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,    83,    12,  ...,  3781,   292, 27247]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6011e-04,  6.1510e-06,  1.1857e-04,  ..., -8.8282e-05,
         -4.1515e-05,  2.2395e-04],
        [-9.7305e-06, -6.5230e-06,  6.8918e-06,  ..., -8.4713e-06,
         -6.6347e-06, -7.0035e-06],
        [-2.9743e-05, -2.0921e-05,  2.2709e-05,  ..., -2.5600e-05,
         -2.0251e-05, -2.1100e-05],
        [-2.1294e-05, -1.4074e-05,  1.5765e-05,  ..., -1.8522e-05,
         -1.4573e-05, -1.5929e-05],
        [-2.3037e-05, -1.6645e-05,  1.7002e-05,  ..., -2.0191e-05,
         -1.6630e-05, -1.5222e-05]], device='cuda:0')
Loss: 0.9544684886932373


Running epoch 1, step 1799, batch 751
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0, 20596,  2943,  ...,  5560,  2512,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2580e-04,  6.4597e-05,  1.2308e-04,  ..., -2.2664e-04,
         -2.6520e-04,  2.3880e-04],
        [-1.1116e-05, -7.4767e-06,  8.0392e-06,  ..., -9.6187e-06,
         -7.5474e-06, -7.9162e-06],
        [-3.4004e-05, -2.3961e-05,  2.6450e-05,  ..., -2.9102e-05,
         -2.3037e-05, -2.3901e-05],
        [-2.4453e-05, -1.6190e-05,  1.8477e-05,  ..., -2.1100e-05,
         -1.6615e-05, -1.8090e-05],
        [-2.6181e-05, -1.8984e-05,  1.9670e-05,  ..., -2.2829e-05,
         -1.8835e-05, -1.7144e-05]], device='cuda:0')
Loss: 0.9444745779037476
Graident accumulation at epoch 1, step 1799, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5245e-05,  6.3188e-05, -1.0152e-04,  ..., -7.6210e-05,
         -7.3232e-05,  3.8811e-06],
        [-6.6163e-06, -3.1112e-06,  4.7444e-06,  ..., -6.3569e-06,
         -3.1193e-06, -5.1893e-06],
        [ 3.8336e-05,  4.4710e-05, -3.5799e-05,  ...,  3.3266e-05,
          3.8005e-05,  1.0593e-05],
        [-2.9835e-06,  7.1503e-06,  5.0269e-06,  ..., -9.2250e-07,
          6.5364e-06, -5.0596e-06],
        [-2.6282e-05, -1.9633e-05,  1.9633e-05,  ..., -2.2820e-05,
         -1.8877e-05, -1.7250e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5470e-08, 5.6852e-08, 6.8147e-08,  ..., 2.7528e-08, 1.5962e-07,
         3.9885e-08],
        [9.5889e-11, 6.2492e-11, 2.9271e-11,  ..., 6.4014e-11, 3.2757e-11,
         3.6194e-11],
        [4.6745e-09, 3.2308e-09, 1.9636e-09,  ..., 3.7358e-09, 1.8898e-09,
         1.3742e-09],
        [1.2791e-09, 1.4232e-09, 5.5753e-10,  ..., 1.1265e-09, 9.0217e-10,
         4.8609e-10],
        [4.1410e-10, 2.3574e-10, 9.6676e-11,  ..., 3.0487e-10, 9.9984e-11,
         1.1966e-10]], device='cuda:0')
optimizer state dict: 225.0
lr: [1.027880867734583e-06, 1.027880867734583e-06]
scheduler_last_epoch: 225


Running epoch 1, step 1800, batch 752
Sampled inputs[:2]: tensor([[    0,   271,  4219,  ...,   644,    14,  3607],
        [    0, 28590,    12,  ...,   342, 29639,  1693]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2547e-05, -4.4690e-05, -1.8391e-04,  ..., -1.7775e-04,
         -7.6094e-05, -1.4308e-04],
        [-1.2890e-06, -9.0897e-07,  9.7603e-07,  ..., -1.1325e-06,
         -9.3132e-07, -9.3132e-07],
        [-3.8743e-06, -2.8163e-06,  3.1888e-06,  ..., -3.3379e-06,
         -2.7567e-06, -2.6971e-06],
        [-2.8014e-06, -1.9222e-06,  2.2501e-06,  ..., -2.4438e-06,
         -2.0266e-06, -2.1011e-06],
        [-2.9951e-06, -2.2501e-06,  2.3991e-06,  ..., -2.6226e-06,
         -2.2650e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.9056041836738586


Running epoch 1, step 1801, batch 753
Sampled inputs[:2]: tensor([[    0,  1266,  2257,  ..., 27146,  1141,  1196],
        [    0,   898,   266,  ...,    12,  3222,  8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4889e-05, -1.7269e-05, -3.7868e-04,  ..., -1.6841e-04,
         -7.4794e-05, -1.4675e-04],
        [-2.6226e-06, -1.9073e-06,  2.0713e-06,  ..., -2.2650e-06,
         -1.7993e-06, -1.8030e-06],
        [-8.1360e-06, -6.1095e-06,  6.8694e-06,  ..., -6.9439e-06,
         -5.5581e-06, -5.4985e-06],
        [-5.7667e-06, -4.0829e-06,  4.7833e-06,  ..., -4.9472e-06,
         -3.9488e-06, -4.1425e-06],
        [-6.0648e-06, -4.7386e-06,  4.9919e-06,  ..., -5.2750e-06,
         -4.4554e-06, -3.7700e-06]], device='cuda:0')
Loss: 0.9843287467956543


Running epoch 1, step 1802, batch 754
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5824e-05,  4.1396e-05, -2.5356e-04,  ..., -2.4594e-04,
         -8.7597e-05, -1.4604e-04],
        [-4.0382e-06, -2.9728e-06,  3.0324e-06,  ..., -3.4943e-06,
         -2.8349e-06, -2.7232e-06],
        [-1.2249e-05, -9.4324e-06,  9.9391e-06,  ..., -1.0550e-05,
         -8.6874e-06, -8.1807e-06],
        [-8.7321e-06, -6.3479e-06,  6.9141e-06,  ..., -7.5549e-06,
         -6.1989e-06, -6.1840e-06],
        [-9.3132e-06, -7.4804e-06,  7.3612e-06,  ..., -8.1807e-06,
         -7.0781e-06, -5.7369e-06]], device='cuda:0')
Loss: 0.9979346990585327


Running epoch 1, step 1803, batch 755
Sampled inputs[:2]: tensor([[    0,  2715,  1478,  ...,  1171,  4697, 41847],
        [    0,  4191,   368,  ...,   367,  4182,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8700e-05,  7.2351e-05, -1.7963e-04,  ..., -1.9433e-04,
          1.2206e-04, -1.3299e-04],
        [-5.3793e-06, -3.8892e-06,  4.0084e-06,  ..., -4.6119e-06,
         -3.7141e-06, -3.7290e-06],
        [-1.6540e-05, -1.2502e-05,  1.3262e-05,  ..., -1.4096e-05,
         -1.1504e-05, -1.1355e-05],
        [-1.1742e-05, -8.3745e-06,  9.2089e-06,  ..., -1.0058e-05,
         -8.1807e-06, -8.5235e-06],
        [-1.2457e-05, -9.8199e-06,  9.7156e-06,  ..., -1.0848e-05,
         -9.2983e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.9271726012229919


Running epoch 1, step 1804, batch 756
Sampled inputs[:2]: tensor([[    0,   720,  1122,  ...,   656,   287, 14258],
        [    0,    14, 49601,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3428e-05,  5.8813e-05, -2.0036e-04,  ..., -2.0820e-04,
          1.3857e-04, -6.4861e-05],
        [-6.7875e-06, -4.8392e-06,  5.0664e-06,  ..., -5.7518e-06,
         -4.6417e-06, -4.6901e-06],
        [-2.0891e-05, -1.5661e-05,  1.6779e-05,  ..., -1.7643e-05,
         -1.4454e-05, -1.4335e-05],
        [-1.4946e-05, -1.0520e-05,  1.1742e-05,  ..., -1.2651e-05,
         -1.0327e-05, -1.0818e-05],
        [-1.5631e-05, -1.2219e-05,  1.2204e-05,  ..., -1.3500e-05,
         -1.1608e-05, -9.9391e-06]], device='cuda:0')
Loss: 0.9265409708023071


Running epoch 1, step 1805, batch 757
Sampled inputs[:2]: tensor([[    0,    14,   759,  ..., 15790,   278,   706],
        [    0,   685,  2461,  ...,   287,   298,  7943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6977e-04, -5.5563e-05, -3.0001e-04,  ..., -1.2897e-04,
          2.7579e-05, -1.4166e-04],
        [-8.1211e-06, -5.7369e-06,  6.0946e-06,  ..., -6.8843e-06,
         -5.5432e-06, -5.5730e-06],
        [-2.5153e-05, -1.8641e-05,  2.0280e-05,  ..., -2.1249e-05,
         -1.7360e-05, -1.7181e-05],
        [-1.7941e-05, -1.2502e-05,  1.4171e-05,  ..., -1.5199e-05,
         -1.2353e-05, -1.2904e-05],
        [-1.8775e-05, -1.4529e-05,  1.4722e-05,  ..., -1.6242e-05,
         -1.3918e-05, -1.1891e-05]], device='cuda:0')
Loss: 0.9853310585021973


Running epoch 1, step 1806, batch 758
Sampled inputs[:2]: tensor([[   0, 3141,  311,  ...,  328, 7818,  408],
        [   0,  600,  287,  ..., 1933,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0235e-04,  5.5126e-05, -1.5111e-04,  ..., -1.0352e-04,
          4.2333e-04,  6.0992e-05],
        [-9.4697e-06, -6.7353e-06,  6.8806e-06,  ..., -8.1286e-06,
         -6.6981e-06, -6.5416e-06],
        [-2.9266e-05, -2.1860e-05,  2.2918e-05,  ..., -2.5004e-05,
         -2.0862e-05, -2.0117e-05],
        [-2.0951e-05, -1.4722e-05,  1.5967e-05,  ..., -1.8030e-05,
         -1.5005e-05, -1.5229e-05],
        [-2.2113e-05, -1.7226e-05,  1.6809e-05,  ..., -1.9327e-05,
         -1.6883e-05, -1.4111e-05]], device='cuda:0')
Loss: 0.9868678450584412


Running epoch 1, step 1807, batch 759
Sampled inputs[:2]: tensor([[   0, 5007, 7551,  ...,    9, 2095,  300],
        [   0,  554, 1034,  ..., 3313,  365,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5817e-04,  6.8243e-05, -1.8600e-04,  ..., -1.2411e-04,
          2.6156e-04,  3.1099e-05],
        [-1.0893e-05, -7.6964e-06,  7.9162e-06,  ..., -9.3281e-06,
         -7.6517e-06, -7.4767e-06],
        [-3.3468e-05, -2.4885e-05,  2.6241e-05,  ..., -2.8536e-05,
         -2.3752e-05, -2.2858e-05],
        [-2.3976e-05, -1.6749e-05,  1.8276e-05,  ..., -2.0593e-05,
         -1.7077e-05, -1.7315e-05],
        [-2.5302e-05, -1.9640e-05,  1.9252e-05,  ..., -2.2084e-05,
         -1.9237e-05, -1.6034e-05]], device='cuda:0')
Loss: 0.9684749245643616
Graident accumulation at epoch 1, step 1807, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.7538e-05,  6.3694e-05, -1.0997e-04,  ..., -8.1000e-05,
         -3.9753e-05,  6.6029e-06],
        [-7.0440e-06, -3.5698e-06,  5.0616e-06,  ..., -6.6541e-06,
         -3.5725e-06, -5.4180e-06],
        [ 3.1156e-05,  3.7750e-05, -2.9595e-05,  ...,  2.7086e-05,
          3.1829e-05,  7.2478e-06],
        [-5.0828e-06,  4.7604e-06,  6.3518e-06,  ..., -2.8896e-06,
          4.1751e-06, -6.2851e-06],
        [-2.6184e-05, -1.9633e-05,  1.9595e-05,  ..., -2.2746e-05,
         -1.8913e-05, -1.7128e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5543e-08, 5.6800e-08, 6.8113e-08,  ..., 2.7515e-08, 1.5953e-07,
         3.9846e-08],
        [9.5912e-11, 6.2489e-11, 2.9304e-11,  ..., 6.4037e-11, 3.2783e-11,
         3.6214e-11],
        [4.6710e-09, 3.2281e-09, 1.9623e-09,  ..., 3.7328e-09, 1.8884e-09,
         1.3733e-09],
        [1.2783e-09, 1.4221e-09, 5.5731e-10,  ..., 1.1258e-09, 9.0156e-10,
         4.8591e-10],
        [4.1433e-10, 2.3589e-10, 9.6950e-11,  ..., 3.0505e-10, 1.0025e-10,
         1.1980e-10]], device='cuda:0')
optimizer state dict: 226.0
lr: [9.739786387225548e-07, 9.739786387225548e-07]
scheduler_last_epoch: 226


Running epoch 1, step 1808, batch 760
Sampled inputs[:2]: tensor([[    0,   437,   266,  ...,   630,   586,   824],
        [    0,    12,   287,  ...,    15, 35654,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2201e-04,  2.6373e-05,  1.4232e-04,  ..., -5.4652e-05,
          2.4073e-04,  5.7097e-05],
        [-1.4007e-06, -9.8348e-07,  8.9407e-07,  ..., -1.1921e-06,
         -1.0356e-06, -9.8348e-07],
        [-4.2617e-06, -3.1590e-06,  2.9951e-06,  ..., -3.6061e-06,
         -3.1441e-06, -2.9951e-06],
        [-3.0547e-06, -2.1309e-06,  2.0415e-06,  ..., -2.6077e-06,
         -2.2799e-06, -2.2650e-06],
        [-3.2485e-06, -2.4736e-06,  2.1905e-06,  ..., -2.8014e-06,
         -2.5332e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9362353682518005


Running epoch 1, step 1809, batch 761
Sampled inputs[:2]: tensor([[    0,  2577,   995,  ...,  6104,    14,  2032],
        [    0, 43788,    12,  ...,    12,  6288,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0029e-04,  4.6925e-05,  2.3144e-04,  ..., -4.6645e-05,
          4.2742e-05,  9.4586e-05],
        [-2.7865e-06, -1.8477e-06,  1.8477e-06,  ..., -2.4065e-06,
         -1.9968e-06, -2.0117e-06],
        [-8.5533e-06, -5.9903e-06,  6.2585e-06,  ..., -7.3463e-06,
         -6.1542e-06, -6.1691e-06],
        [-6.1244e-06, -4.0308e-06,  4.2766e-06,  ..., -5.3197e-06,
         -4.4554e-06, -4.6641e-06],
        [-6.5714e-06, -4.7386e-06,  4.6194e-06,  ..., -5.7518e-06,
         -5.0217e-06, -4.3511e-06]], device='cuda:0')
Loss: 0.9379808902740479


Running epoch 1, step 1810, batch 762
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,   278,   638,  ...,   278,   266,  9387]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0058e-04,  3.5284e-05,  1.8988e-04,  ...,  7.6323e-05,
         -7.1481e-05,  3.6621e-04],
        [-4.1202e-06, -2.7455e-06,  2.7567e-06,  ..., -3.5614e-06,
         -2.9169e-06, -2.9579e-06],
        [-1.2666e-05, -8.9109e-06,  9.2983e-06,  ..., -1.0863e-05,
         -8.9556e-06, -9.0450e-06],
        [-9.1493e-06, -6.0126e-06,  6.4224e-06,  ..., -7.9274e-06,
         -6.5416e-06, -6.9290e-06],
        [-9.6411e-06, -7.0184e-06,  6.8396e-06,  ..., -8.4341e-06,
         -7.2569e-06, -6.3479e-06]], device='cuda:0')
Loss: 0.9480558037757874


Running epoch 1, step 1811, batch 763
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0, 2088, 5370,  ..., 1110, 3380,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0020e-04, -1.2305e-04,  8.4068e-05,  ...,  1.1874e-04,
         -1.3227e-04,  1.8108e-04],
        [-5.4687e-06, -3.7141e-06,  3.8594e-06,  ..., -4.6566e-06,
         -3.7961e-06, -3.8333e-06],
        [-1.6987e-05, -1.2085e-05,  1.3009e-05,  ..., -1.4365e-05,
         -1.1757e-05, -1.1846e-05],
        [-1.2234e-05, -8.1733e-06,  9.0301e-06,  ..., -1.0416e-05,
         -8.5086e-06, -9.0003e-06],
        [-1.2696e-05, -9.3728e-06,  9.3877e-06,  ..., -1.0967e-05,
         -9.3877e-06, -8.1807e-06]], device='cuda:0')
Loss: 0.9719429016113281


Running epoch 1, step 1812, batch 764
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,   287,  4599,  ..., 11812,   266,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8437e-04, -1.9283e-04,  2.7018e-04,  ...,  2.1607e-05,
         -1.0263e-04,  1.2422e-04],
        [-6.7875e-06, -4.7050e-06,  4.7982e-06,  ..., -5.8338e-06,
         -4.7870e-06, -4.7572e-06],
        [-2.1130e-05, -1.5318e-05,  1.6183e-05,  ..., -1.8030e-05,
         -1.4842e-05, -1.4722e-05],
        [-1.5154e-05, -1.0349e-05,  1.1191e-05,  ..., -1.3024e-05,
         -1.0699e-05, -1.1161e-05],
        [-1.5900e-05, -1.1966e-05,  1.1772e-05,  ..., -1.3843e-05,
         -1.1891e-05, -1.0237e-05]], device='cuda:0')
Loss: 0.9730392694473267


Running epoch 1, step 1813, batch 765
Sampled inputs[:2]: tensor([[    0,  3386, 43625,  ...,    19,  2125,   271],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0528e-04, -2.6741e-04,  2.7184e-04,  ...,  2.0614e-04,
          2.6808e-04,  7.0200e-05],
        [-8.0243e-06, -5.5879e-06,  5.6773e-06,  ..., -6.9663e-06,
         -5.7034e-06, -5.6624e-06],
        [-2.4974e-05, -1.8060e-05,  1.9163e-05,  ..., -2.1443e-05,
         -1.7568e-05, -1.7405e-05],
        [-1.7911e-05, -1.2249e-05,  1.3247e-05,  ..., -1.5542e-05,
         -1.2740e-05, -1.3292e-05],
        [-1.8865e-05, -1.4156e-05,  1.4022e-05,  ..., -1.6481e-05,
         -1.4111e-05, -1.2122e-05]], device='cuda:0')
Loss: 0.9771079421043396


Running epoch 1, step 1814, batch 766
Sampled inputs[:2]: tensor([[    0,  1099,  2851,  ...,   518,   496,   287],
        [    0,  1665,  6306,  ...,   300, 10204,   582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2945e-04, -2.9529e-04,  2.5546e-04,  ...,  2.1237e-04,
          3.3246e-04,  4.7884e-05],
        [-9.3654e-06, -6.5789e-06,  6.6757e-06,  ..., -8.1211e-06,
         -6.6943e-06, -6.5677e-06],
        [-2.9117e-05, -2.1264e-05,  2.2516e-05,  ..., -2.5004e-05,
         -2.0638e-05, -2.0191e-05],
        [-2.0877e-05, -1.4424e-05,  1.5572e-05,  ..., -1.8120e-05,
         -1.4976e-05, -1.5423e-05],
        [-2.1949e-05, -1.6630e-05,  1.6451e-05,  ..., -1.9193e-05,
         -1.6555e-05, -1.4022e-05]], device='cuda:0')
Loss: 0.9713022708892822


Running epoch 1, step 1815, batch 767
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0,   13,  711,  ...,  591,  953,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0113e-04, -2.3561e-04,  2.3350e-04,  ...,  1.2165e-04,
          1.7546e-04,  4.8909e-06],
        [-1.0610e-05, -7.4767e-06,  7.6592e-06,  ..., -9.2015e-06,
         -7.5512e-06, -7.4841e-06],
        [-3.2991e-05, -2.4125e-05,  2.5839e-05,  ..., -2.8268e-05,
         -2.3276e-05, -2.2873e-05],
        [-2.3708e-05, -1.6406e-05,  1.7911e-05,  ..., -2.0564e-05,
         -1.6913e-05, -1.7583e-05],
        [-2.4825e-05, -1.8835e-05,  1.8850e-05,  ..., -2.1666e-05,
         -1.8641e-05, -1.5840e-05]], device='cuda:0')
Loss: 0.9316482543945312
Graident accumulation at epoch 1, step 1815, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1090e-04,  3.3763e-05, -7.5619e-05,  ..., -6.0734e-05,
         -1.8232e-05,  6.4317e-06],
        [-7.4005e-06, -3.9604e-06,  5.3214e-06,  ..., -6.9088e-06,
         -3.9704e-06, -5.6246e-06],
        [ 2.4741e-05,  3.1563e-05, -2.4051e-05,  ...,  2.1551e-05,
          2.6319e-05,  4.2357e-06],
        [-6.9453e-06,  2.6437e-06,  7.5078e-06,  ..., -4.6570e-06,
          2.0663e-06, -7.4150e-06],
        [-2.6048e-05, -1.9554e-05,  1.9520e-05,  ..., -2.2638e-05,
         -1.8886e-05, -1.6999e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5738e-08, 5.6799e-08, 6.8100e-08,  ..., 2.7503e-08, 1.5940e-07,
         3.9807e-08],
        [9.5929e-11, 6.2482e-11, 2.9334e-11,  ..., 6.4057e-11, 3.2807e-11,
         3.6233e-11],
        [4.6674e-09, 3.2255e-09, 1.9610e-09,  ..., 3.7299e-09, 1.8871e-09,
         1.3724e-09],
        [1.2776e-09, 1.4209e-09, 5.5707e-10,  ..., 1.1251e-09, 9.0094e-10,
         4.8573e-10],
        [4.1453e-10, 2.3601e-10, 9.7209e-11,  ..., 3.0522e-10, 1.0050e-10,
         1.1993e-10]], device='cuda:0')
optimizer state dict: 227.0
lr: [9.214556645637851e-07, 9.214556645637851e-07]
scheduler_last_epoch: 227


Running epoch 1, step 1816, batch 768
Sampled inputs[:2]: tensor([[    0,   342,  4781,  ...,   630,   940,   271],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2312e-04,  1.4269e-04,  7.8513e-05,  ..., -5.4412e-05,
         -1.5196e-05, -1.8458e-05],
        [-1.2517e-06, -9.6858e-07,  8.4192e-07,  ..., -1.1697e-06,
         -9.8348e-07, -9.9093e-07],
        [-3.9041e-06, -3.0845e-06,  2.8908e-06,  ..., -3.5614e-06,
         -3.0100e-06, -2.9653e-06],
        [-2.8163e-06, -2.1309e-06,  1.9521e-06,  ..., -2.6226e-06,
         -2.2352e-06, -2.2948e-06],
        [-3.0100e-06, -2.4289e-06,  2.1607e-06,  ..., -2.7865e-06,
         -2.4289e-06, -2.1011e-06]], device='cuda:0')
Loss: 0.9392309188842773


Running epoch 1, step 1817, batch 769
Sampled inputs[:2]: tensor([[   0, 7018,   14,  ..., 8288,   12, 1250],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1520e-04,  1.5320e-04, -3.5583e-05,  ..., -1.2641e-04,
          4.9417e-06,  3.0349e-05],
        [-2.5630e-06, -1.9968e-06,  1.7881e-06,  ..., -2.3320e-06,
         -1.9893e-06, -1.8924e-06],
        [-8.0764e-06, -6.4969e-06,  6.1244e-06,  ..., -7.2420e-06,
         -6.1989e-06, -5.8413e-06],
        [-5.7369e-06, -4.3809e-06,  4.1574e-06,  ..., -5.2005e-06,
         -4.4703e-06, -4.4256e-06],
        [-6.0946e-06, -5.0515e-06,  4.5002e-06,  ..., -5.5730e-06,
         -4.9323e-06, -4.0829e-06]], device='cuda:0')
Loss: 0.9495236873626709


Running epoch 1, step 1818, batch 770
Sampled inputs[:2]: tensor([[    0,   300, 11040,  ...,   266,  1736,  3487],
        [    0,   266,   298,  ...,   266,   818,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5148e-06,  1.5320e-04,  3.1125e-05,  ..., -1.2575e-04,
         -1.1715e-04,  8.0723e-05],
        [-4.1574e-06, -2.8498e-06,  2.7567e-06,  ..., -3.6508e-06,
         -2.9430e-06, -3.0994e-06],
        [-1.2845e-05, -9.1642e-06,  9.2685e-06,  ..., -1.1146e-05,
         -9.0450e-06, -9.3728e-06],
        [-8.8513e-06, -6.0275e-06,  6.1393e-06,  ..., -7.7933e-06,
         -6.3404e-06, -6.8098e-06],
        [-1.0148e-05, -7.3016e-06,  7.0035e-06,  ..., -8.9407e-06,
         -7.4506e-06, -6.9141e-06]], device='cuda:0')
Loss: 0.924274742603302


Running epoch 1, step 1819, batch 771
Sampled inputs[:2]: tensor([[    0,   287,   298,  ..., 14121,  3121,   409],
        [    0,   874,   445,  ...,    14, 16205,  8510]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2031e-05,  2.1672e-04, -5.5825e-05,  ..., -3.1034e-05,
          1.8241e-06,  2.9997e-04],
        [-5.6252e-06, -3.7923e-06,  3.7104e-06,  ..., -4.9248e-06,
         -3.9786e-06, -4.1798e-06],
        [-1.7434e-05, -1.2234e-05,  1.2502e-05,  ..., -1.5050e-05,
         -1.2264e-05, -1.2711e-05],
        [-1.1921e-05, -7.9945e-06,  8.2105e-06,  ..., -1.0490e-05,
         -8.5607e-06, -9.1642e-06],
        [-1.3754e-05, -9.7901e-06,  9.4622e-06,  ..., -1.2070e-05,
         -1.0118e-05, -9.3430e-06]], device='cuda:0')
Loss: 0.943015456199646


Running epoch 1, step 1820, batch 772
Sampled inputs[:2]: tensor([[    0,   266,  1422,  ...,   446,  1992,   586],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2716e-05,  2.7129e-04, -5.5838e-05,  ...,  3.9882e-05,
          1.2325e-05,  1.9425e-04],
        [-7.0110e-06, -4.7609e-06,  4.6305e-06,  ..., -6.1542e-06,
         -4.9323e-06, -5.1931e-06],
        [-2.1666e-05, -1.5289e-05,  1.5587e-05,  ..., -1.8746e-05,
         -1.5184e-05, -1.5706e-05],
        [-1.4886e-05, -1.0066e-05,  1.0297e-05,  ..., -1.3143e-05,
         -1.0632e-05, -1.1399e-05],
        [-1.6987e-05, -1.2174e-05,  1.1757e-05,  ..., -1.4931e-05,
         -1.2472e-05, -1.1474e-05]], device='cuda:0')
Loss: 0.934256374835968


Running epoch 1, step 1821, batch 773
Sampled inputs[:2]: tensor([[   0,  266, 1441,  ..., 1817, 1589,  278],
        [   0, 5646,   12,  ..., 1952,  287, 3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3372e-05,  3.1324e-04, -1.8932e-04,  ..., -3.4859e-05,
         -1.3161e-04,  5.2088e-05],
        [-8.3148e-06, -5.8040e-06,  5.7258e-06,  ..., -7.2494e-06,
         -5.8413e-06, -6.0424e-06],
        [-2.5839e-05, -1.8746e-05,  1.9312e-05,  ..., -2.2247e-05,
         -1.8105e-05, -1.8433e-05],
        [-1.7881e-05, -1.2435e-05,  1.2949e-05,  ..., -1.5661e-05,
         -1.2718e-05, -1.3456e-05],
        [-1.9863e-05, -1.4648e-05,  1.4246e-05,  ..., -1.7405e-05,
         -1.4633e-05, -1.3195e-05]], device='cuda:0')
Loss: 0.9428898096084595


Running epoch 1, step 1822, batch 774
Sampled inputs[:2]: tensor([[    0,   843,  3365,  ...,  1136,  1615,   292],
        [    0, 28107,    14,  ...,   864,   298,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4446e-05,  6.2405e-04,  4.8216e-05,  ..., -1.7110e-04,
         -6.8384e-05, -5.1508e-05],
        [-9.6932e-06, -6.7800e-06,  6.4410e-06,  ..., -8.4639e-06,
         -6.9290e-06, -7.0035e-06],
        [-3.0071e-05, -2.1964e-05,  2.1800e-05,  ..., -2.5973e-05,
         -2.1487e-05, -2.1368e-05],
        [-2.0951e-05, -1.4611e-05,  1.4544e-05,  ..., -1.8418e-05,
         -1.5251e-05, -1.5691e-05],
        [-2.3186e-05, -1.7181e-05,  1.6101e-05,  ..., -2.0370e-05,
         -1.7390e-05, -1.5326e-05]], device='cuda:0')
Loss: 0.935064435005188


Running epoch 1, step 1823, batch 775
Sampled inputs[:2]: tensor([[    0,   391,  1761,  ...,   346,    14,   292],
        [    0, 18125, 16419,  ...,   278,   638, 11744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1130e-05,  5.5428e-04,  2.2944e-05,  ..., -1.3812e-04,
         -3.8626e-05, -1.0425e-04],
        [-1.1027e-05, -7.7784e-06,  7.4767e-06,  ..., -9.5889e-06,
         -7.8976e-06, -7.8976e-06],
        [-3.4362e-05, -2.5302e-05,  2.5317e-05,  ..., -2.9594e-05,
         -2.4617e-05, -2.4304e-05],
        [-2.4021e-05, -1.6876e-05,  1.7032e-05,  ..., -2.0996e-05,
         -1.7457e-05, -1.7866e-05],
        [-2.6345e-05, -1.9759e-05,  1.8604e-05,  ..., -2.3112e-05,
         -1.9863e-05, -1.7338e-05]], device='cuda:0')
Loss: 0.9911540150642395
Graident accumulation at epoch 1, step 1823, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0392e-04,  8.5815e-05, -6.5763e-05,  ..., -6.8473e-05,
         -2.0271e-05, -4.6370e-06],
        [-7.7632e-06, -4.3422e-06,  5.5369e-06,  ..., -7.1768e-06,
         -4.3631e-06, -5.8519e-06],
        [ 1.8831e-05,  2.5876e-05, -1.9114e-05,  ...,  1.6436e-05,
          2.1225e-05,  1.3818e-06],
        [-8.6528e-06,  6.9179e-07,  8.4602e-06,  ..., -6.2909e-06,
          1.1403e-07, -8.4601e-06],
        [-2.6078e-05, -1.9574e-05,  1.9429e-05,  ..., -2.2686e-05,
         -1.8984e-05, -1.7033e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5684e-08, 5.7049e-08, 6.8032e-08,  ..., 2.7494e-08, 1.5925e-07,
         3.9778e-08],
        [9.5954e-11, 6.2480e-11, 2.9360e-11,  ..., 6.4085e-11, 3.2837e-11,
         3.6260e-11],
        [4.6639e-09, 3.2229e-09, 1.9597e-09,  ..., 3.7270e-09, 1.8858e-09,
         1.3717e-09],
        [1.2769e-09, 1.4198e-09, 5.5681e-10,  ..., 1.1244e-09, 9.0035e-10,
         4.8556e-10],
        [4.1481e-10, 2.3617e-10, 9.7458e-11,  ..., 3.0545e-10, 1.0080e-10,
         1.2011e-10]], device='cuda:0')
optimizer state dict: 228.0
lr: [8.703199712272026e-07, 8.703199712272026e-07]
scheduler_last_epoch: 228


Running epoch 1, step 1824, batch 776
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7414e-05,  1.1952e-05,  1.1002e-04,  ...,  1.0429e-04,
          1.0211e-04, -9.4797e-05],
        [-1.3709e-06, -1.0356e-06,  9.9093e-07,  ..., -1.2070e-06,
         -9.7603e-07, -8.6799e-07],
        [-4.2617e-06, -3.3826e-06,  3.2932e-06,  ..., -3.7700e-06,
         -3.0696e-06, -2.7269e-06],
        [-3.0249e-06, -2.2650e-06,  2.2501e-06,  ..., -2.6673e-06,
         -2.1607e-06, -2.0117e-06],
        [-3.2485e-06, -2.6673e-06,  2.4289e-06,  ..., -2.9355e-06,
         -2.4736e-06, -1.9222e-06]], device='cuda:0')
Loss: 1.0095939636230469


Running epoch 1, step 1825, batch 777
Sampled inputs[:2]: tensor([[    0,   328,  1690,  ...,  2670,   287, 11287],
        [    0,    26,  4044,  ...,  9531,   365,   993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4411e-05,  7.9615e-07,  2.7175e-04,  ...,  1.4402e-04,
          1.1251e-04, -1.7265e-04],
        [-2.6822e-06, -2.1011e-06,  1.8105e-06,  ..., -2.4438e-06,
         -2.1309e-06, -1.8291e-06],
        [-8.2850e-06, -6.7353e-06,  6.0350e-06,  ..., -7.4953e-06,
         -6.4969e-06, -5.6028e-06],
        [-5.9456e-06, -4.5747e-06,  4.1425e-06,  ..., -5.4389e-06,
         -4.7386e-06, -4.2766e-06],
        [-6.5565e-06, -5.4985e-06,  4.6194e-06,  ..., -6.0350e-06,
         -5.4091e-06, -4.1276e-06]], device='cuda:0')
Loss: 0.9823426604270935


Running epoch 1, step 1826, batch 778
Sampled inputs[:2]: tensor([[    0, 15402, 44149,  ...,   266,  1403,   271],
        [    0,  3448,   278,  ...,   380,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3093e-05,  1.0910e-05,  2.2836e-04,  ...,  1.3233e-04,
          7.4018e-05, -8.5483e-05],
        [-4.0755e-06, -3.1143e-06,  2.8908e-06,  ..., -3.5614e-06,
         -3.0473e-06, -2.7157e-06],
        [-1.2636e-05, -1.0028e-05,  9.6411e-06,  ..., -1.0997e-05,
         -9.4175e-06, -8.4192e-06],
        [-9.1493e-06, -6.8694e-06,  6.7502e-06,  ..., -7.9870e-06,
         -6.8396e-06, -6.4224e-06],
        [-9.6560e-06, -7.9721e-06,  7.1228e-06,  ..., -8.6129e-06,
         -7.6741e-06, -5.9828e-06]], device='cuda:0')
Loss: 0.9676688313484192


Running epoch 1, step 1827, batch 779
Sampled inputs[:2]: tensor([[   0,  278, 9939,  ..., 1238,   14,  445],
        [   0,  278, 3358,  ...,   12,  287, 9612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9661e-04,  1.5531e-04,  3.5126e-04,  ...,  2.1913e-04,
          1.3065e-04, -9.4316e-05],
        [-5.5432e-06, -4.1500e-06,  3.7588e-06,  ..., -4.8727e-06,
         -4.1202e-06, -3.7514e-06],
        [-1.7136e-05, -1.3411e-05,  1.2562e-05,  ..., -1.5020e-05,
         -1.2755e-05, -1.1608e-05],
        [-1.2264e-05, -9.0599e-06,  8.6427e-06,  ..., -1.0803e-05,
         -9.1791e-06, -8.7321e-06],
        [-1.3262e-05, -1.0759e-05,  9.3877e-06,  ..., -1.1891e-05,
         -1.0490e-05, -8.3819e-06]], device='cuda:0')
Loss: 0.9629722833633423


Running epoch 1, step 1828, batch 780
Sampled inputs[:2]: tensor([[   0, 2286, 1085,  ..., 1387, 1184, 1802],
        [   0,   12, 2085,  ...,  287,  593, 4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1047e-05,  1.5149e-04,  2.7767e-04,  ...,  1.8185e-04,
         -1.0188e-04, -9.6917e-05],
        [-6.8471e-06, -5.1111e-06,  4.8392e-06,  ..., -6.0499e-06,
         -5.0105e-06, -4.6492e-06],
        [-2.1160e-05, -1.6510e-05,  1.6123e-05,  ..., -1.8641e-05,
         -1.5527e-05, -1.4380e-05],
        [-1.5199e-05, -1.1176e-05,  1.1206e-05,  ..., -1.3441e-05,
         -1.1176e-05, -1.0863e-05],
        [-1.6183e-05, -1.3113e-05,  1.1891e-05,  ..., -1.4573e-05,
         -1.2666e-05, -1.0237e-05]], device='cuda:0')
Loss: 0.9388422966003418


Running epoch 1, step 1829, batch 781
Sampled inputs[:2]: tensor([[    0,   273,   298,  ..., 23554,    12,  1530],
        [    0,   278,  6653,  ...,  7524,   271, 28279]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7795e-05,  7.8433e-05,  3.1263e-04,  ...,  2.5253e-04,
          8.3359e-05, -5.8069e-05],
        [-8.3670e-06, -6.2212e-06,  5.7444e-06,  ..., -7.3761e-06,
         -6.0983e-06, -5.7518e-06],
        [-2.5690e-05, -2.0012e-05,  1.9044e-05,  ..., -2.2605e-05,
         -1.8895e-05, -1.7658e-05],
        [-1.8388e-05, -1.3515e-05,  1.3188e-05,  ..., -1.6227e-05,
         -1.3515e-05, -1.3262e-05],
        [-2.0027e-05, -1.6138e-05,  1.4305e-05,  ..., -1.7986e-05,
         -1.5661e-05, -1.2860e-05]], device='cuda:0')
Loss: 0.9742128252983093


Running epoch 1, step 1830, batch 782
Sampled inputs[:2]: tensor([[   0, 8416,  669,  ...,  298,  894,  496],
        [   0, 6067, 1188,  ..., 5282,  756,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7700e-05,  4.9568e-05,  3.5469e-04,  ...,  2.7613e-04,
          3.2199e-05,  9.8962e-05],
        [-9.7081e-06, -7.1675e-06,  6.6832e-06,  ..., -8.5309e-06,
         -7.0594e-06, -6.6608e-06],
        [-2.9922e-05, -2.3142e-05,  2.2218e-05,  ..., -2.6211e-05,
         -2.1920e-05, -2.0534e-05],
        [-2.1398e-05, -1.5602e-05,  1.5363e-05,  ..., -1.8820e-05,
         -1.5676e-05, -1.5393e-05],
        [-2.3201e-05, -1.8567e-05,  1.6615e-05,  ..., -2.0757e-05,
         -1.8075e-05, -1.4856e-05]], device='cuda:0')
Loss: 0.9626802802085876


Running epoch 1, step 1831, batch 783
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0,    13, 20773,  ..., 22463,  2587,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7120e-05,  1.5542e-05,  3.1348e-04,  ...,  2.8548e-04,
         -2.0074e-04,  2.1509e-04],
        [-1.1057e-05, -8.0988e-06,  7.6815e-06,  ..., -9.6858e-06,
         -7.9796e-06, -7.5847e-06],
        [-3.4124e-05, -2.6211e-05,  2.5555e-05,  ..., -2.9802e-05,
         -2.4840e-05, -2.3410e-05],
        [-2.4423e-05, -1.7658e-05,  1.7703e-05,  ..., -2.1428e-05,
         -1.7777e-05, -1.7568e-05],
        [-2.6226e-05, -2.0877e-05,  1.8939e-05,  ..., -2.3410e-05,
         -2.0310e-05, -1.6779e-05]], device='cuda:0')
Loss: 0.9478159546852112
Graident accumulation at epoch 1, step 1831, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.6240e-05,  7.8787e-05, -2.7838e-05,  ..., -3.3078e-05,
         -3.8318e-05,  1.7336e-05],
        [-8.0925e-06, -4.7179e-06,  5.7514e-06,  ..., -7.4277e-06,
         -4.7248e-06, -6.0252e-06],
        [ 1.3535e-05,  2.0668e-05, -1.4647e-05,  ...,  1.1812e-05,
          1.6619e-05, -1.0974e-06],
        [-1.0230e-05, -1.1432e-06,  9.3844e-06,  ..., -7.8046e-06,
         -1.6751e-06, -9.3710e-06],
        [-2.6093e-05, -1.9704e-05,  1.9380e-05,  ..., -2.2758e-05,
         -1.9116e-05, -1.7008e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5629e-08, 5.6992e-08, 6.8062e-08,  ..., 2.7548e-08, 1.5913e-07,
         3.9784e-08],
        [9.5981e-11, 6.2483e-11, 2.9390e-11,  ..., 6.4115e-11, 3.2868e-11,
         3.6281e-11],
        [4.6604e-09, 3.2204e-09, 1.9584e-09,  ..., 3.7242e-09, 1.8845e-09,
         1.3708e-09],
        [1.2763e-09, 1.4187e-09, 5.5656e-10,  ..., 1.1237e-09, 8.9976e-10,
         4.8539e-10],
        [4.1508e-10, 2.3637e-10, 9.7719e-11,  ..., 3.0569e-10, 1.0111e-10,
         1.2027e-10]], device='cuda:0')
optimizer state dict: 229.0
lr: [8.205793726931199e-07, 8.205793726931199e-07]
scheduler_last_epoch: 229


Running epoch 1, step 1832, batch 784
Sampled inputs[:2]: tensor([[    0,    17,   292,  ...,  2269,  3887,   278],
        [    0, 47831,   266,  ...,    66,    17, 20005]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5432e-04,  9.9045e-05,  1.0908e-04,  ...,  1.1034e-04,
         -2.0255e-04,  4.1743e-05],
        [-1.4156e-06, -9.3877e-07,  1.0058e-06,  ..., -1.2517e-06,
         -9.4995e-07, -9.7603e-07],
        [-4.3511e-06, -2.9951e-06,  3.3081e-06,  ..., -3.7998e-06,
         -2.8908e-06, -2.9653e-06],
        [-3.0547e-06, -1.9670e-06,  2.2501e-06,  ..., -2.6822e-06,
         -2.0266e-06, -2.1756e-06],
        [-3.4273e-06, -2.4289e-06,  2.5034e-06,  ..., -3.0398e-06,
         -2.4140e-06, -2.1905e-06]], device='cuda:0')
Loss: 0.9703208208084106


Running epoch 1, step 1833, batch 785
Sampled inputs[:2]: tensor([[    0,   300,  6263,  ..., 18488,  1665,  1640],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1333e-04,  9.9045e-05,  1.1994e-04,  ..., -2.6639e-05,
         -4.7487e-04, -1.6928e-05],
        [-2.9132e-06, -1.8477e-06,  1.9409e-06,  ..., -2.5406e-06,
         -1.9185e-06, -2.0489e-06],
        [-8.7917e-06, -5.8860e-06,  6.3628e-06,  ..., -7.6443e-06,
         -5.8711e-06, -6.1244e-06],
        [-6.1393e-06, -3.8296e-06,  4.2617e-06,  ..., -5.3644e-06,
         -4.0680e-06, -4.4405e-06],
        [-6.8992e-06, -4.7237e-06,  4.7982e-06,  ..., -6.0946e-06,
         -4.8727e-06, -4.5002e-06]], device='cuda:0')
Loss: 0.9326456785202026


Running epoch 1, step 1834, batch 786
Sampled inputs[:2]: tensor([[   0,   12,  344,  ..., 2337, 1122,  408],
        [   0, 9611,  278,  ...,  278,  638,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3157e-04,  6.3019e-05,  2.8525e-05,  ..., -2.8625e-05,
         -4.1845e-04,  9.4550e-05],
        [-4.2692e-06, -2.7716e-06,  2.9542e-06,  ..., -3.7104e-06,
         -2.8536e-06, -2.9802e-06],
        [-1.3024e-05, -8.9109e-06,  9.7752e-06,  ..., -1.1310e-05,
         -8.8215e-06, -9.0152e-06],
        [-9.1493e-06, -5.8413e-06,  6.6459e-06,  ..., -7.9721e-06,
         -6.1542e-06, -6.6012e-06],
        [-1.0058e-05, -7.0930e-06,  7.2420e-06,  ..., -8.8662e-06,
         -7.2122e-06, -6.4969e-06]], device='cuda:0')
Loss: 0.9644331336021423


Running epoch 1, step 1835, batch 787
Sampled inputs[:2]: tensor([[    0,   292, 17181,  ...,   634,  5039,   266],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7345e-05,  7.4623e-05, -2.1849e-05,  ..., -6.0474e-05,
         -6.4375e-04,  2.2996e-05],
        [-5.5879e-06, -3.7923e-06,  3.9376e-06,  ..., -4.8876e-06,
         -3.7923e-06, -3.9041e-06],
        [-1.7107e-05, -1.2174e-05,  1.3053e-05,  ..., -1.4931e-05,
         -1.1742e-05, -1.1802e-05],
        [-1.2085e-05, -8.0615e-06,  8.9407e-06,  ..., -1.0580e-05,
         -8.2552e-06, -8.7470e-06],
        [-1.3128e-05, -9.6560e-06,  9.6411e-06,  ..., -1.1653e-05,
         -9.5665e-06, -8.4490e-06]], device='cuda:0')
Loss: 0.9616453647613525


Running epoch 1, step 1836, batch 788
Sampled inputs[:2]: tensor([[    0,  1062,   648,  ...,   266,  4939,   278],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0074e-05,  1.0763e-04, -7.1136e-05,  ..., -9.0122e-06,
         -5.8028e-04, -4.1708e-04],
        [-6.8545e-06, -4.6343e-06,  4.8801e-06,  ..., -6.0275e-06,
         -4.7162e-06, -4.8243e-06],
        [-2.0862e-05, -1.4782e-05,  1.6183e-05,  ..., -1.8224e-05,
         -1.4439e-05, -1.4409e-05],
        [-1.4901e-05, -9.8720e-06,  1.1176e-05,  ..., -1.3113e-05,
         -1.0341e-05, -1.0908e-05],
        [-1.6004e-05, -1.1742e-05,  1.1936e-05,  ..., -1.4201e-05,
         -1.1757e-05, -1.0267e-05]], device='cuda:0')
Loss: 0.930190920829773


Running epoch 1, step 1837, batch 789
Sampled inputs[:2]: tensor([[   0,   21,   13,  ...,   14,  747,  806],
        [   0, 7117,  278,  ...,  287,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5623e-05,  3.1514e-04,  9.3984e-05,  ..., -2.5203e-05,
         -6.1930e-04, -3.5693e-04],
        [-8.2627e-06, -5.6103e-06,  5.7630e-06,  ..., -7.2420e-06,
         -5.7146e-06, -5.7854e-06],
        [-2.5183e-05, -1.7986e-05,  1.9163e-05,  ..., -2.1979e-05,
         -1.7554e-05, -1.7345e-05],
        [-1.7971e-05, -1.1988e-05,  1.3158e-05,  ..., -1.5795e-05,
         -1.2562e-05, -1.3083e-05],
        [-1.9327e-05, -1.4290e-05,  1.4126e-05,  ..., -1.7166e-05,
         -1.4335e-05, -1.2368e-05]], device='cuda:0')
Loss: 0.948764979839325


Running epoch 1, step 1838, batch 790
Sampled inputs[:2]: tensor([[    0,  2958,   298,  ...,    12,   709,   616],
        [    0, 10676,   328,  ...,     9,   360,  2583]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9386e-04,  3.1497e-04,  3.3352e-05,  ..., -1.0222e-04,
         -8.0071e-04, -1.8387e-04],
        [-9.6112e-06, -6.5044e-06,  6.7912e-06,  ..., -8.4117e-06,
         -6.5975e-06, -6.7689e-06],
        [-2.9474e-05, -2.0996e-05,  2.2635e-05,  ..., -2.5615e-05,
         -2.0370e-05, -2.0340e-05],
        [-2.1040e-05, -1.4000e-05,  1.5602e-05,  ..., -1.8433e-05,
         -1.4558e-05, -1.5378e-05],
        [-2.2560e-05, -1.6615e-05,  1.6689e-05,  ..., -1.9953e-05,
         -1.6585e-05, -1.4499e-05]], device='cuda:0')
Loss: 0.9520888924598694


Running epoch 1, step 1839, batch 791
Sampled inputs[:2]: tensor([[    0,  3441,   796,  ...,  7561,  1711,   857],
        [    0,  4154, 14296,  ...,   516,  1796, 18233]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5547e-04,  3.4997e-04,  7.3580e-05,  ...,  5.3154e-05,
         -5.6736e-04, -1.9147e-04],
        [-1.0990e-05, -7.3984e-06,  7.6778e-06,  ..., -9.6411e-06,
         -7.5810e-06, -7.7672e-06],
        [-3.3736e-05, -2.3857e-05,  2.5660e-05,  ..., -2.9340e-05,
         -2.3410e-05, -2.3335e-05],
        [-2.4110e-05, -1.5952e-05,  1.7658e-05,  ..., -2.1189e-05,
         -1.6779e-05, -1.7688e-05],
        [-2.5854e-05, -1.8895e-05,  1.8939e-05,  ..., -2.2888e-05,
         -1.9059e-05, -1.6645e-05]], device='cuda:0')
Loss: 0.9734866619110107
Graident accumulation at epoch 1, step 1839, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.1069e-05,  1.0591e-04, -1.7696e-05,  ..., -2.4455e-05,
         -9.1223e-05, -3.5450e-06],
        [-8.3822e-06, -4.9859e-06,  5.9440e-06,  ..., -7.6490e-06,
         -5.0104e-06, -6.1994e-06],
        [ 8.8081e-06,  1.6215e-05, -1.0617e-05,  ...,  7.6970e-06,
          1.2616e-05, -3.3212e-06],
        [-1.1618e-05, -2.6240e-06,  1.0212e-05,  ..., -9.1431e-06,
         -3.1854e-06, -1.0203e-05],
        [-2.6069e-05, -1.9623e-05,  1.9336e-05,  ..., -2.2771e-05,
         -1.9111e-05, -1.6971e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5598e-08, 5.7058e-08, 6.8000e-08,  ..., 2.7524e-08, 1.5929e-07,
         3.9781e-08],
        [9.6006e-11, 6.2475e-11, 2.9419e-11,  ..., 6.4144e-11, 3.2892e-11,
         3.6305e-11],
        [4.6569e-09, 3.2177e-09, 1.9571e-09,  ..., 3.7213e-09, 1.8832e-09,
         1.3700e-09],
        [1.2756e-09, 1.4175e-09, 5.5632e-10,  ..., 1.1230e-09, 8.9914e-10,
         4.8521e-10],
        [4.1534e-10, 2.3649e-10, 9.7980e-11,  ..., 3.0591e-10, 1.0137e-10,
         1.2043e-10]], device='cuda:0')
optimizer state dict: 230.0
lr: [7.722414697591851e-07, 7.722414697591851e-07]
scheduler_last_epoch: 230


Running epoch 1, step 1840, batch 792
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   394,   360,  5911],
        [    0,    14,   475,  ...,  7903,   266, 27772]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1702e-04,  8.8980e-05, -4.7526e-05,  ...,  9.9057e-05,
          2.1718e-05, -1.1788e-04],
        [-1.3709e-06, -9.9838e-07,  9.7603e-07,  ..., -1.2070e-06,
         -9.6858e-07, -9.5367e-07],
        [-4.3511e-06, -3.2932e-06,  3.2932e-06,  ..., -3.7998e-06,
         -3.0845e-06, -3.0100e-06],
        [-3.0547e-06, -2.1905e-06,  2.2650e-06,  ..., -2.6822e-06,
         -2.1607e-06, -2.2054e-06],
        [-3.1888e-06, -2.4885e-06,  2.3544e-06,  ..., -2.8461e-06,
         -2.3991e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9630879759788513


Running epoch 1, step 1841, batch 793
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  278, 4697,   14],
        [   0, 1042, 2548,  ...,  328,  259, 2771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1524e-06,  3.8807e-05, -1.4729e-05,  ...,  8.8759e-05,
          3.0440e-04, -5.3771e-05],
        [-2.6971e-06, -1.9893e-06,  1.9222e-06,  ..., -2.3767e-06,
         -1.9670e-06, -1.8515e-06],
        [-8.5533e-06, -6.5863e-06,  6.5416e-06,  ..., -7.4953e-06,
         -6.2585e-06, -5.8562e-06],
        [-6.0201e-06, -4.3958e-06,  4.5002e-06,  ..., -5.3048e-06,
         -4.4107e-06, -4.3213e-06],
        [-6.2734e-06, -5.0217e-06,  4.6641e-06,  ..., -5.6177e-06,
         -4.8727e-06, -3.9935e-06]], device='cuda:0')
Loss: 0.9896672368049622


Running epoch 1, step 1842, batch 794
Sampled inputs[:2]: tensor([[    0,  3611, 10765,  ...,   271,  4317,    13],
        [    0, 41010,  6737,  ...,   963,   409,   382]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8372e-05,  4.3786e-05, -6.6645e-05,  ...,  5.9152e-05,
          1.8841e-04, -1.0020e-05],
        [-4.0159e-06, -2.9877e-06,  3.0026e-06,  ..., -3.5390e-06,
         -2.8536e-06, -2.7493e-06],
        [-1.2577e-05, -9.7454e-06,  1.0088e-05,  ..., -1.1027e-05,
         -8.9854e-06, -8.5682e-06],
        [-8.9258e-06, -6.5863e-06,  7.0035e-06,  ..., -7.8678e-06,
         -6.3777e-06, -6.4075e-06],
        [-9.2983e-06, -7.4953e-06,  7.2569e-06,  ..., -8.3297e-06,
         -7.0930e-06, -5.8636e-06]], device='cuda:0')
Loss: 0.9761594533920288


Running epoch 1, step 1843, batch 795
Sampled inputs[:2]: tensor([[    0,  2738,   278,  ...,   292,    35,  2147],
        [    0, 13312,  9048,  ..., 33470,  8672,  3524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0104e-04,  2.0338e-04,  2.4205e-05,  ..., -5.9574e-05,
          1.8717e-04, -8.9318e-05],
        [-5.3644e-06, -3.9563e-06,  3.9265e-06,  ..., -4.6939e-06,
         -3.8743e-06, -3.6955e-06],
        [ 1.1495e-04,  1.5999e-04, -6.6518e-05,  ...,  9.0741e-05,
          1.5605e-04,  4.3302e-05],
        [-1.1921e-05, -8.7023e-06,  9.1046e-06,  ..., -1.0446e-05,
         -8.6725e-06, -8.6129e-06],
        [-1.2517e-05, -1.0014e-05,  9.5516e-06,  ..., -1.1131e-05,
         -9.6858e-06, -7.9647e-06]], device='cuda:0')
Loss: 0.980518639087677


Running epoch 1, step 1844, batch 796
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7468e-05,  1.4136e-04, -2.9707e-05,  ..., -1.9866e-05,
          3.7495e-04,  3.6028e-05],
        [-6.7055e-06, -5.0366e-06,  4.9323e-06,  ..., -5.8636e-06,
         -4.8876e-06, -4.5635e-06],
        [ 1.7454e-04,  2.8243e-04, -9.4506e-05,  ...,  1.1862e-04,
          2.1949e-04,  8.3775e-05],
        [-1.5005e-05, -1.1176e-05,  1.1519e-05,  ..., -1.3158e-05,
         -1.1012e-05, -1.0729e-05],
        [-1.5691e-05, -1.2785e-05,  1.1995e-05,  ..., -1.3992e-05,
         -1.2279e-05, -9.9167e-06]], device='cuda:0')
Loss: 1.0046348571777344


Running epoch 1, step 1845, batch 797
Sampled inputs[:2]: tensor([[    0,   278, 11554,  ...,  4713,  1039, 17088],
        [    0,   221, 18844,  ...,   199, 10174,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8923e-05,  8.2558e-05, -2.7362e-05,  ...,  4.1187e-05,
          3.8788e-04,  9.7469e-06],
        [-8.0913e-06, -5.9232e-06,  5.8636e-06,  ..., -7.0632e-06,
         -5.8375e-06, -5.6066e-06],
        [ 1.7013e-04,  2.7944e-04, -9.1272e-05,  ...,  1.1485e-04,
          2.1647e-04,  8.0482e-05],
        [-1.8030e-05, -1.3083e-05,  1.3635e-05,  ..., -1.5795e-05,
         -1.3083e-05, -1.3113e-05],
        [-1.9073e-05, -1.5125e-05,  1.4380e-05,  ..., -1.6943e-05,
         -1.4722e-05, -1.2256e-05]], device='cuda:0')
Loss: 0.9693753719329834


Running epoch 1, step 1846, batch 798
Sampled inputs[:2]: tensor([[    0,  5143,  3877,  ...,   292, 44003,    12],
        [    0, 48007,   417,  ...,   944,   278,  2903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6918e-06, -1.1374e-04, -6.9600e-05,  ...,  2.3493e-04,
          2.3933e-04,  4.4816e-06],
        [-9.4995e-06, -6.8508e-06,  6.8620e-06,  ..., -8.2329e-06,
         -6.7726e-06, -6.6273e-06],
        [ 1.6557e-04,  2.7633e-04, -8.7815e-05,  ...,  1.1113e-04,
          2.1344e-04,  7.7204e-05],
        [-2.1160e-05, -1.5095e-05,  1.5929e-05,  ..., -1.8373e-05,
         -1.5154e-05, -1.5467e-05],
        [-2.2441e-05, -1.7509e-05,  1.6853e-05,  ..., -1.9774e-05,
         -1.7107e-05, -1.4491e-05]], device='cuda:0')
Loss: 0.9624701142311096


Running epoch 1, step 1847, batch 799
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   446, 23105,  ..., 11867,   824,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0973e-04, -1.8282e-05,  6.5718e-05,  ...,  1.9575e-04,
          3.8924e-04,  1.3348e-04],
        [-1.0923e-05, -7.9535e-06,  7.7710e-06,  ..., -9.4995e-06,
         -7.8380e-06, -7.6182e-06],
        [ 1.6113e-04,  2.7272e-04, -8.4745e-05,  ...,  1.0719e-04,
          2.1010e-04,  7.4089e-05],
        [-2.4199e-05, -1.7434e-05,  1.7941e-05,  ..., -2.1100e-05,
         -1.7449e-05, -1.7703e-05],
        [-2.5898e-05, -2.0415e-05,  1.9178e-05,  ..., -2.2918e-05,
         -1.9863e-05, -1.6741e-05]], device='cuda:0')
Loss: 0.9642881155014038
Graident accumulation at epoch 1, step 1847, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.4935e-05,  9.3487e-05, -9.3547e-06,  ..., -2.4341e-06,
         -4.3177e-05,  1.0158e-05],
        [-8.6363e-06, -5.2827e-06,  6.1267e-06,  ..., -7.8341e-06,
         -5.2931e-06, -6.3413e-06],
        [ 2.4041e-05,  4.1866e-05, -1.8030e-05,  ...,  1.7647e-05,
          3.2364e-05,  4.4199e-06],
        [-1.2876e-05, -4.1051e-06,  1.0985e-05,  ..., -1.0339e-05,
         -4.6118e-06, -1.0953e-05],
        [-2.6052e-05, -1.9702e-05,  1.9320e-05,  ..., -2.2786e-05,
         -1.9186e-05, -1.6948e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5554e-08, 5.7001e-08, 6.7936e-08,  ..., 2.7534e-08, 1.5928e-07,
         3.9759e-08],
        [9.6029e-11, 6.2476e-11, 2.9450e-11,  ..., 6.4170e-11, 3.2921e-11,
         3.6327e-11],
        [4.6782e-09, 3.2889e-09, 1.9623e-09,  ..., 3.7291e-09, 1.9255e-09,
         1.3741e-09],
        [1.2749e-09, 1.4164e-09, 5.5608e-10,  ..., 1.1224e-09, 8.9855e-10,
         4.8504e-10],
        [4.1559e-10, 2.3667e-10, 9.8250e-11,  ..., 3.0613e-10, 1.0166e-10,
         1.2059e-10]], device='cuda:0')
optimizer state dict: 231.0
lr: [7.253136488789125e-07, 7.253136488789125e-07]
scheduler_last_epoch: 231
Epoch 1 | Batch 799/1048 | Training PPL: 2240.165271419923 | time 84.83049273490906
Saving checkpoint at epoch 1, step 1847, batch 799
Epoch 1 | Validation PPL: 6.733561050575794 | Learning rate: 7.253136488789125e-07
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1847, AFTER epoch 1, step 1847


Running epoch 1, step 1848, batch 800
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3931e-04, -1.1817e-05,  3.0848e-05,  ...,  1.6413e-05,
          8.2999e-05,  0.0000e+00],
        [-1.3784e-06, -1.0505e-06,  1.0878e-06,  ..., -1.1399e-06,
         -9.7603e-07, -8.9407e-07],
        [-4.2915e-06, -3.4273e-06,  3.5912e-06,  ..., -3.5763e-06,
         -3.0994e-06, -2.8312e-06],
        [-3.1143e-06, -2.3544e-06,  2.5630e-06,  ..., -2.5928e-06,
         -2.2352e-06, -2.1458e-06],
        [-3.1292e-06, -2.6226e-06,  2.5481e-06,  ..., -2.6971e-06,
         -2.4289e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9875972867012024


Running epoch 1, step 1849, batch 801
Sampled inputs[:2]: tensor([[    0,  3152,  1385,  ...,  1403,   518,  2088],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3370e-04, -8.0004e-05, -3.4565e-06,  ...,  1.0872e-05,
          3.0674e-07, -1.4307e-04],
        [-2.7195e-06, -1.8775e-06,  2.1085e-06,  ..., -2.2724e-06,
         -1.8366e-06, -1.7732e-06],
        [-8.2552e-06, -6.0201e-06,  6.8694e-06,  ..., -6.8843e-06,
         -5.6773e-06, -5.3644e-06],
        [-6.1393e-06, -4.1872e-06,  4.9919e-06,  ..., -5.1260e-06,
         -4.1872e-06, -4.2021e-06],
        [-6.1393e-06, -4.6790e-06,  4.9621e-06,  ..., -5.2601e-06,
         -4.5300e-06, -3.6880e-06]], device='cuda:0')
Loss: 0.9439297914505005


Running epoch 1, step 1850, batch 802
Sampled inputs[:2]: tensor([[   0,  515,  266,  ...,   18, 3770, 1345],
        [   0, 7555, 3908,  ...,  259, 8477,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8052e-04, -1.2509e-04,  8.9518e-05,  ...,  5.2707e-05,
         -1.8684e-04, -7.0909e-05],
        [-4.0978e-06, -2.8275e-06,  3.0845e-06,  ..., -3.4496e-06,
         -2.7455e-06, -2.7195e-06],
        [-1.2755e-05, -9.2387e-06,  1.0252e-05,  ..., -1.0684e-05,
         -8.6129e-06, -8.4192e-06],
        [-9.1940e-06, -6.2585e-06,  7.2271e-06,  ..., -7.7337e-06,
         -6.1989e-06, -6.3926e-06],
        [-9.4473e-06, -7.1228e-06,  7.3910e-06,  ..., -8.1062e-06,
         -6.8247e-06, -5.7742e-06]], device='cuda:0')
Loss: 0.9463925361633301


Running epoch 1, step 1851, batch 803
Sampled inputs[:2]: tensor([[    0,  3473,   278,  ..., 11743,   472,   346],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5936e-04, -9.2314e-05,  3.6195e-05,  ..., -6.1722e-05,
         -1.2280e-04, -5.6543e-05],
        [-5.4687e-06, -3.8184e-06,  4.1723e-06,  ..., -4.5896e-06,
         -3.6843e-06, -3.6173e-06],
        [-1.7107e-05, -1.2517e-05,  1.3858e-05,  ..., -1.4305e-05,
         -1.1623e-05, -1.1295e-05],
        [-1.2308e-05, -8.4490e-06,  9.7752e-06,  ..., -1.0297e-05,
         -8.2999e-06, -8.5235e-06],
        [-1.2606e-05, -9.6112e-06,  9.9540e-06,  ..., -1.0803e-05,
         -9.1791e-06, -7.7114e-06]], device='cuda:0')
Loss: 0.9860690236091614


Running epoch 1, step 1852, batch 804
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 1238,   14,  ...,  368,  940,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0772e-04, -2.5297e-05, -6.1823e-06,  ...,  9.4311e-05,
         -1.7100e-05, -9.8801e-05],
        [-6.7428e-06, -4.7572e-06,  5.0589e-06,  ..., -5.7369e-06,
         -4.6082e-06, -4.5151e-06],
        [-2.1189e-05, -1.5646e-05,  1.6913e-05,  ..., -1.7941e-05,
         -1.4558e-05, -1.4171e-05],
        [-1.5169e-05, -1.0535e-05,  1.1817e-05,  ..., -1.2904e-05,
         -1.0401e-05, -1.0654e-05],
        [-1.5602e-05, -1.1981e-05,  1.2130e-05,  ..., -1.3515e-05,
         -1.1459e-05, -9.6336e-06]], device='cuda:0')
Loss: 0.9629324674606323


Running epoch 1, step 1853, batch 805
Sampled inputs[:2]: tensor([[    0,  2919,  1482,  ...,   587, 20186,   275],
        [    0, 14949,    12,  ...,   669, 10168,  7166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7134e-04, -4.7902e-05, -2.1205e-04,  ...,  1.9005e-05,
         -2.0424e-04, -8.9503e-05],
        [-8.0690e-06, -5.6811e-06,  6.2510e-06,  ..., -6.8322e-06,
         -5.5246e-06, -5.4128e-06],
        [-2.5302e-05, -1.8582e-05,  2.0817e-05,  ..., -2.1294e-05,
         -1.7390e-05, -1.6913e-05],
        [-1.8194e-05, -1.2591e-05,  1.4663e-05,  ..., -1.5393e-05,
         -1.2487e-05, -1.2800e-05],
        [-1.8537e-05, -1.4186e-05,  1.4842e-05,  ..., -1.5974e-05,
         -1.3649e-05, -1.1422e-05]], device='cuda:0')
Loss: 0.9465270638465881


Running epoch 1, step 1854, batch 806
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,   767,  9289,  ...,   494,   287,  8957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4593e-05, -3.6449e-05, -1.5518e-04,  ..., -3.2626e-05,
         -2.3505e-04, -7.3438e-05],
        [-9.3654e-06, -6.6198e-06,  7.1749e-06,  ..., -8.0094e-06,
         -6.4448e-06, -6.3367e-06],
        [-2.9296e-05, -2.1547e-05,  2.3887e-05,  ..., -2.4855e-05,
         -2.0191e-05, -1.9699e-05],
        [-2.1055e-05, -1.4618e-05,  1.6794e-05,  ..., -1.8001e-05,
         -1.4514e-05, -1.4931e-05],
        [-2.1607e-05, -1.6525e-05,  1.7136e-05,  ..., -1.8731e-05,
         -1.5914e-05, -1.3389e-05]], device='cuda:0')
Loss: 0.9605603218078613


Running epoch 1, step 1855, batch 807
Sampled inputs[:2]: tensor([[    0,   374,  5195,  ...,   266,  5555,    14],
        [    0,   446, 21112,  ..., 22092,    22,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4593e-05, -1.1958e-05, -1.4579e-04,  ..., -1.2824e-04,
         -1.0707e-04, -1.4321e-04],
        [-1.0699e-05, -7.6108e-06,  8.1360e-06,  ..., -9.1940e-06,
         -7.4580e-06, -7.2420e-06],
        [-3.3349e-05, -2.4676e-05,  2.6986e-05,  ..., -2.8431e-05,
         -2.3276e-05, -2.2396e-05],
        [-2.3991e-05, -1.6749e-05,  1.8984e-05,  ..., -2.0593e-05,
         -1.6734e-05, -1.7002e-05],
        [-2.4810e-05, -1.9088e-05,  1.9521e-05,  ..., -2.1592e-05,
         -1.8477e-05, -1.5371e-05]], device='cuda:0')
Loss: 0.9849021434783936
Graident accumulation at epoch 1, step 1855, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.7982e-05,  8.2942e-05, -2.2998e-05,  ..., -1.5015e-05,
         -4.9566e-05, -5.1790e-06],
        [-8.8425e-06, -5.5155e-06,  6.3276e-06,  ..., -7.9701e-06,
         -5.5096e-06, -6.4313e-06],
        [ 1.8302e-05,  3.5212e-05, -1.3528e-05,  ...,  1.3039e-05,
          2.6800e-05,  1.7383e-06],
        [-1.3988e-05, -5.3694e-06,  1.1785e-05,  ..., -1.1364e-05,
         -5.8240e-06, -1.1558e-05],
        [-2.5928e-05, -1.9641e-05,  1.9340e-05,  ..., -2.2666e-05,
         -1.9115e-05, -1.6791e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5508e-08, 5.6944e-08, 6.7889e-08,  ..., 2.7523e-08, 1.5913e-07,
         3.9740e-08],
        [9.6047e-11, 6.2472e-11, 2.9487e-11,  ..., 6.4190e-11, 3.2944e-11,
         3.6343e-11],
        [4.6746e-09, 3.2862e-09, 1.9611e-09,  ..., 3.7262e-09, 1.9241e-09,
         1.3733e-09],
        [1.2742e-09, 1.4153e-09, 5.5589e-10,  ..., 1.1217e-09, 8.9793e-10,
         4.8484e-10],
        [4.1579e-10, 2.3680e-10, 9.8532e-11,  ..., 3.0629e-10, 1.0190e-10,
         1.2070e-10]], device='cuda:0')
optimizer state dict: 232.0
lr: [6.798030810329725e-07, 6.798030810329725e-07]
scheduler_last_epoch: 232


Running epoch 1, step 1856, batch 808
Sampled inputs[:2]: tensor([[   0, 1795,  650,  ...,  516, 2793, 1109],
        [   0,  328,  843,  ...,  298,  292,   37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4941e-06,  1.0428e-05,  5.5276e-05,  ..., -3.0093e-05,
          8.3156e-05,  7.1316e-05],
        [-1.3858e-06, -9.4250e-07,  9.8348e-07,  ..., -1.1846e-06,
         -9.4995e-07, -1.0207e-06],
        [-4.4107e-06, -3.1441e-06,  3.3528e-06,  ..., -3.7551e-06,
         -3.0100e-06, -3.2485e-06],
        [-3.0994e-06, -2.0862e-06,  2.2650e-06,  ..., -2.6524e-06,
         -2.1309e-06, -2.3991e-06],
        [-3.3230e-06, -2.4438e-06,  2.4438e-06,  ..., -2.8908e-06,
         -2.4289e-06, -2.2650e-06]], device='cuda:0')
Loss: 0.951732337474823


Running epoch 1, step 1857, batch 809
Sampled inputs[:2]: tensor([[    0,    13, 37178,  ...,  1692,  3287, 10652],
        [    0,  2440,   709,  ...,  4505,  1549,  4111]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6882e-05,  1.3859e-05,  4.1685e-05,  ...,  3.6744e-05,
          2.5004e-04,  7.3964e-05],
        [-2.7195e-06, -1.8813e-06,  1.8738e-06,  ..., -2.3842e-06,
         -1.9781e-06, -1.9222e-06],
        [-8.3447e-06, -6.0350e-06,  6.2138e-06,  ..., -7.2420e-06,
         -6.0499e-06, -5.8115e-06],
        [-6.0499e-06, -4.1127e-06,  4.3213e-06,  ..., -5.3048e-06,
         -4.4554e-06, -4.4852e-06],
        [-6.5565e-06, -4.9323e-06,  4.7237e-06,  ..., -5.8115e-06,
         -5.0813e-06, -4.2170e-06]], device='cuda:0')
Loss: 0.9782130122184753


Running epoch 1, step 1858, batch 810
Sampled inputs[:2]: tensor([[    0,   638,  1862,  ...,    14,  7869,    14],
        [    0,  6143,   642,  ...,   199, 14300,    41]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8077e-05, -9.1132e-05, -6.5640e-05,  ...,  9.1238e-06,
          2.0072e-04, -1.4709e-05],
        [-4.0978e-06, -2.7120e-06,  2.8498e-06,  ..., -3.5390e-06,
         -2.8610e-06, -2.9206e-06],
        [-1.2547e-05, -8.7470e-06,  9.4175e-06,  ..., -1.0729e-05,
         -8.7768e-06, -8.8215e-06],
        [-9.0748e-06, -5.9232e-06,  6.5565e-06,  ..., -7.8529e-06,
         -6.4224e-06, -6.7651e-06],
        [-9.7156e-06, -7.0632e-06,  7.0482e-06,  ..., -8.5235e-06,
         -7.2867e-06, -6.3181e-06]], device='cuda:0')
Loss: 0.9475602507591248


Running epoch 1, step 1859, batch 811
Sampled inputs[:2]: tensor([[   0, 1428,  266,  ..., 3169, 3058,  278],
        [   0,  221, 4070,  ..., 1061, 3189,   26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1439e-05, -8.5257e-05,  6.4956e-05,  ...,  1.0046e-04,
          1.0327e-04, -2.7426e-05],
        [-5.4911e-06, -3.7327e-06,  3.8259e-06,  ..., -4.7833e-06,
         -3.8743e-06, -3.9712e-06],
        [-1.6898e-05, -1.2010e-05,  1.2681e-05,  ..., -1.4544e-05,
         -1.1876e-05, -1.2025e-05],
        [-1.2040e-05, -8.0541e-06,  8.7172e-06,  ..., -1.0490e-05,
         -8.5682e-06, -9.0897e-06],
        [-1.3128e-05, -9.6709e-06,  9.5367e-06,  ..., -1.1548e-05,
         -9.8348e-06, -8.6576e-06]], device='cuda:0')
Loss: 0.9785381555557251


Running epoch 1, step 1860, batch 812
Sampled inputs[:2]: tensor([[   0, 1049,   12,  ...,  292, 3963,  755],
        [   0,   14,  417,  ..., 8821, 6845,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4807e-04, -2.4302e-04, -1.0767e-04,  ...,  7.8470e-05,
          4.1131e-04, -1.9597e-04],
        [-6.9141e-06, -4.6454e-06,  4.8019e-06,  ..., -6.0424e-06,
         -4.8093e-06, -5.0068e-06],
        [-2.1160e-05, -1.4782e-05,  1.5855e-05,  ..., -1.8194e-05,
         -1.4618e-05, -1.5005e-05],
        [-1.4961e-05, -9.8646e-06,  1.0833e-05,  ..., -1.3039e-05,
         -1.0468e-05, -1.1265e-05],
        [-1.6525e-05, -1.1966e-05,  1.1966e-05,  ..., -1.4529e-05,
         -1.2174e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.949874997138977


Running epoch 1, step 1861, batch 813
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0, 1103,  271,  ...,  957,  756,  368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9795e-04, -2.1547e-04, -1.5402e-04,  ..., -5.7751e-06,
          4.3491e-04, -1.4807e-04],
        [-8.3223e-06, -5.7034e-06,  5.8971e-06,  ..., -7.2345e-06,
         -5.8003e-06, -5.9046e-06],
        [-2.5451e-05, -1.8179e-05,  1.9416e-05,  ..., -2.1845e-05,
         -1.7688e-05, -1.7807e-05],
        [-1.8120e-05, -1.2234e-05,  1.3396e-05,  ..., -1.5706e-05,
         -1.2703e-05, -1.3396e-05],
        [-1.9744e-05, -1.4633e-05,  1.4573e-05,  ..., -1.7330e-05,
         -1.4663e-05, -1.2785e-05]], device='cuda:0')
Loss: 0.9772399663925171


Running epoch 1, step 1862, batch 814
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,  1086,   292,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6646e-06, -3.0634e-04, -4.3941e-04,  ..., -9.4606e-05,
          1.5602e-04, -2.4027e-04],
        [-9.6858e-06, -6.5602e-06,  6.9477e-06,  ..., -8.3521e-06,
         -6.6310e-06, -6.7912e-06],
        [-2.9534e-05, -2.0877e-05,  2.2843e-05,  ..., -2.5153e-05,
         -2.0221e-05, -2.0444e-05],
        [-2.1204e-05, -1.4141e-05,  1.5929e-05,  ..., -1.8224e-05,
         -1.4596e-05, -1.5497e-05],
        [-2.2635e-05, -1.6659e-05,  1.6928e-05,  ..., -1.9759e-05,
         -1.6630e-05, -1.4499e-05]], device='cuda:0')
Loss: 0.9268280267715454


Running epoch 1, step 1863, batch 815
Sampled inputs[:2]: tensor([[   0, 5775,  292,  ..., 8671, 1339,  642],
        [   0,   13, 5005,  ...,  654,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6451e-06, -3.1407e-04, -4.1699e-04,  ..., -1.7593e-04,
          1.3662e-04, -2.7731e-04],
        [-1.1034e-05, -7.4655e-06,  8.0280e-06,  ..., -9.4771e-06,
         -7.5176e-06, -7.7114e-06],
        [-3.3766e-05, -2.3872e-05,  2.6450e-05,  ..., -2.8700e-05,
         -2.2992e-05, -2.3380e-05],
        [-2.4214e-05, -1.6138e-05,  1.8463e-05,  ..., -2.0757e-05,
         -1.6563e-05, -1.7688e-05],
        [-2.5705e-05, -1.8939e-05,  1.9446e-05,  ..., -2.2382e-05,
         -1.8790e-05, -1.6466e-05]], device='cuda:0')
Loss: 0.9483949542045593
Graident accumulation at epoch 1, step 1863, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0237, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1219e-05,  4.3241e-05, -6.2397e-05,  ..., -3.1107e-05,
         -3.0948e-05, -3.2392e-05],
        [-9.0617e-06, -5.7105e-06,  6.4977e-06,  ..., -8.1208e-06,
         -5.7104e-06, -6.5593e-06],
        [ 1.3095e-05,  2.9303e-05, -9.5303e-06,  ...,  8.8651e-06,
          2.1821e-05, -7.7355e-07],
        [-1.5010e-05, -6.4463e-06,  1.2452e-05,  ..., -1.2304e-05,
         -6.8979e-06, -1.2171e-05],
        [-2.5905e-05, -1.9571e-05,  1.9351e-05,  ..., -2.2638e-05,
         -1.9083e-05, -1.6758e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5452e-08, 5.6986e-08, 6.7995e-08,  ..., 2.7527e-08, 1.5899e-07,
         3.9777e-08],
        [9.6073e-11, 6.2465e-11, 2.9522e-11,  ..., 6.4216e-11, 3.2967e-11,
         3.6366e-11],
        [4.6711e-09, 3.2835e-09, 1.9598e-09,  ..., 3.7233e-09, 1.9227e-09,
         1.3724e-09],
        [1.2735e-09, 1.4141e-09, 5.5567e-10,  ..., 1.1210e-09, 8.9731e-10,
         4.8467e-10],
        [4.1604e-10, 2.3692e-10, 9.8812e-11,  ..., 3.0648e-10, 1.0215e-10,
         1.2086e-10]], device='cuda:0')
optimizer state dict: 233.0
lr: [6.357167206333992e-07, 6.357167206333992e-07]
scheduler_last_epoch: 233


Running epoch 1, step 1864, batch 816
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,   365,  2714,  ...,   298,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2327e-05,  2.1901e-05, -9.1845e-05,  ..., -7.6355e-05,
          1.9413e-05, -6.7709e-05],
        [-1.3858e-06, -9.1642e-07,  1.0133e-06,  ..., -1.1474e-06,
         -8.7544e-07, -9.1642e-07],
        [-4.2617e-06, -3.0100e-06,  3.3677e-06,  ..., -3.5167e-06,
         -2.7716e-06, -2.8014e-06],
        [-3.1590e-06, -2.0713e-06,  2.4140e-06,  ..., -2.6077e-06,
         -2.0266e-06, -2.1756e-06],
        [-3.0994e-06, -2.3097e-06,  2.3842e-06,  ..., -2.6375e-06,
         -2.1905e-06, -1.8999e-06]], device='cuda:0')
Loss: 0.9584885835647583


Running epoch 1, step 1865, batch 817
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5712e-04,  6.9180e-06, -1.6899e-04,  ..., -2.1511e-04,
         -1.4721e-04, -5.4605e-05],
        [-2.7716e-06, -1.9148e-06,  1.9968e-06,  ..., -2.3842e-06,
         -1.8813e-06, -1.9148e-06],
        [ 5.9576e-05,  8.0436e-05, -6.6227e-05,  ...,  3.5894e-05,
          8.9125e-05,  3.7253e-05],
        [-6.1989e-06, -4.2617e-06,  4.6641e-06,  ..., -5.3346e-06,
         -4.2617e-06, -4.4852e-06],
        [-6.2734e-06, -4.8131e-06,  4.7386e-06,  ..., -5.5432e-06,
         -4.6790e-06, -4.0606e-06]], device='cuda:0')
Loss: 0.9564793705940247


Running epoch 1, step 1866, batch 818
Sampled inputs[:2]: tensor([[    0, 11030,    72,  ...,   259, 16979,  9415],
        [    0,   278,   490,  ...,   434,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4817e-04,  3.6640e-05, -2.0275e-04,  ..., -2.6487e-04,
         -1.9809e-04,  1.4836e-05],
        [-4.1127e-06, -2.8461e-06,  3.0324e-06,  ..., -3.5539e-06,
         -2.8498e-06, -2.8424e-06],
        [ 5.5553e-05,  7.7531e-05, -6.2874e-05,  ...,  3.2437e-05,
          8.6174e-05,  3.4556e-05],
        [-9.1046e-06, -6.2585e-06,  7.0333e-06,  ..., -7.8827e-06,
         -6.4075e-06, -6.5863e-06],
        [-9.3579e-06, -7.1675e-06,  7.2420e-06,  ..., -8.2552e-06,
         -7.0781e-06, -5.9679e-06]], device='cuda:0')
Loss: 0.9577494859695435


Running epoch 1, step 1867, batch 819
Sampled inputs[:2]: tensor([[   0, 3351,  352,  ...,   17,  287,  357],
        [   0, 6795, 1728,  ...,  578,   19,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6436e-04,  1.3429e-05, -1.7657e-04,  ..., -2.5548e-04,
         -9.3285e-05, -1.0854e-05],
        [-5.3644e-06, -3.7253e-06,  3.9488e-06,  ..., -4.7013e-06,
         -3.8110e-06, -3.7551e-06],
        [ 5.1797e-05,  7.4863e-05, -5.9939e-05,  ...,  2.9129e-05,
          8.3402e-05,  3.1978e-05],
        [-1.1981e-05, -8.1956e-06,  9.2238e-06,  ..., -1.0476e-05,
         -8.5831e-06, -8.7470e-06],
        [-1.2398e-05, -9.4175e-06,  9.5367e-06,  ..., -1.0967e-05,
         -9.4324e-06, -7.9051e-06]], device='cuda:0')
Loss: 0.9498655200004578


Running epoch 1, step 1868, batch 820
Sampled inputs[:2]: tensor([[    0,  3036,   471,  ...,   287,  1906,    12],
        [    0,   298, 39056,  ...,   221,  1061,  2165]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7559e-05, -3.6742e-05,  4.5730e-05,  ..., -2.5087e-04,
         -1.0953e-04,  4.9600e-05],
        [-6.8471e-06, -4.7162e-06,  4.7721e-06,  ..., -5.9903e-06,
         -4.8392e-06, -4.8578e-06],
        [ 4.7059e-05,  7.1526e-05, -5.7063e-05,  ...,  2.5046e-05,
          8.0124e-05,  2.8476e-05],
        [-1.5095e-05, -1.0282e-05,  1.1019e-05,  ..., -1.3202e-05,
         -1.0759e-05, -1.1116e-05],
        [-1.6242e-05, -1.2115e-05,  1.1742e-05,  ..., -1.4335e-05,
         -1.2204e-05, -1.0587e-05]], device='cuda:0')
Loss: 0.9376955628395081


Running epoch 1, step 1869, batch 821
Sampled inputs[:2]: tensor([[    0, 12440,   578,  ..., 25918,   287,   996],
        [    0,   266, 11692,  ...,   278, 14620, 12718]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4994e-04, -1.0937e-04,  9.5769e-05,  ..., -3.2380e-04,
         -1.4242e-04,  1.6146e-04],
        [-8.1882e-06, -5.6662e-06,  5.9120e-06,  ..., -7.1079e-06,
         -5.6997e-06, -5.7481e-06],
        [ 4.2827e-05,  6.8337e-05, -5.3278e-05,  ...,  2.1485e-05,
          7.7323e-05,  2.5615e-05],
        [-1.8135e-05, -1.2413e-05,  1.3731e-05,  ..., -1.5721e-05,
         -1.2711e-05, -1.3247e-05],
        [-1.9222e-05, -1.4499e-05,  1.4335e-05,  ..., -1.6928e-05,
         -1.4350e-05, -1.2457e-05]], device='cuda:0')
Loss: 0.9663152694702148


Running epoch 1, step 1870, batch 822
Sampled inputs[:2]: tensor([[   0,  266, 3536,  ...,  266, 1883,  266],
        [   0, 2771,   13,  ..., 4169,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6222e-04, -1.0125e-04,  5.9826e-05,  ..., -3.3839e-04,
         -1.2533e-04,  1.0774e-04],
        [-9.4771e-06, -6.6422e-06,  6.8396e-06,  ..., -8.2776e-06,
         -6.6496e-06, -6.6906e-06],
        [ 3.8774e-05,  6.5148e-05, -5.0134e-05,  ...,  1.7834e-05,
          7.4343e-05,  2.2650e-05],
        [-2.0966e-05, -1.4514e-05,  1.5862e-05,  ..., -1.8284e-05,
         -1.4797e-05, -1.5438e-05],
        [-2.2233e-05, -1.6958e-05,  1.6585e-05,  ..., -1.9684e-05,
         -1.6689e-05, -1.4484e-05]], device='cuda:0')
Loss: 0.9918001890182495


Running epoch 1, step 1871, batch 823
Sampled inputs[:2]: tensor([[    0, 23070,   367,  ...,   287,   790,  3252],
        [    0,   287,  2269,  ..., 22413,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0291e-04, -1.0971e-04, -1.9073e-05,  ..., -1.7332e-04,
         -1.0104e-04,  2.6194e-04],
        [-1.0736e-05, -7.4022e-06,  7.8306e-06,  ..., -9.3877e-06,
         -7.4171e-06, -7.7039e-06],
        [ 3.4840e-05,  6.2749e-05, -4.6811e-05,  ...,  1.4481e-05,
          7.1988e-05,  1.9670e-05],
        [-2.3887e-05, -1.6205e-05,  1.8291e-05,  ..., -2.0832e-05,
         -1.6570e-05, -1.7822e-05],
        [-2.5064e-05, -1.8761e-05,  1.8895e-05,  ..., -2.2128e-05,
         -1.8507e-05, -1.6466e-05]], device='cuda:0')
Loss: 0.9289148449897766
Graident accumulation at epoch 1, step 1871, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.5806e-05,  2.7946e-05, -5.8065e-05,  ..., -4.5328e-05,
         -3.7957e-05, -2.9593e-06],
        [-9.2292e-06, -5.8797e-06,  6.6310e-06,  ..., -8.2475e-06,
         -5.8811e-06, -6.6738e-06],
        [ 1.5269e-05,  3.2648e-05, -1.3258e-05,  ...,  9.4267e-06,
          2.6838e-05,  1.2708e-06],
        [-1.5898e-05, -7.4222e-06,  1.3036e-05,  ..., -1.3156e-05,
         -7.8651e-06, -1.2736e-05],
        [-2.5821e-05, -1.9490e-05,  1.9305e-05,  ..., -2.2587e-05,
         -1.9025e-05, -1.6729e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5438e-08, 5.6941e-08, 6.7928e-08,  ..., 2.7529e-08, 1.5884e-07,
         3.9806e-08],
        [9.6092e-11, 6.2457e-11, 2.9554e-11,  ..., 6.4240e-11, 3.2989e-11,
         3.6389e-11],
        [4.6676e-09, 3.2841e-09, 1.9601e-09,  ..., 3.7198e-09, 1.9260e-09,
         1.3715e-09],
        [1.2728e-09, 1.4130e-09, 5.5545e-10,  ..., 1.1203e-09, 8.9668e-10,
         4.8451e-10],
        [4.1625e-10, 2.3703e-10, 9.9070e-11,  ..., 3.0667e-10, 1.0239e-10,
         1.2101e-10]], device='cuda:0')
optimizer state dict: 234.0
lr: [5.930613044608946e-07, 5.930613044608946e-07]
scheduler_last_epoch: 234


Running epoch 1, step 1872, batch 824
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,   301,   298,  ...,   806,   352, 22105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1958e-05, -2.1086e-04, -9.0583e-05,  ..., -1.8017e-04,
         -1.4604e-04, -8.7280e-05],
        [-1.4678e-06, -9.2015e-07,  1.0207e-06,  ..., -1.2666e-06,
         -9.6112e-07, -1.1399e-06],
        [-4.6790e-06, -3.1143e-06,  3.4869e-06,  ..., -3.9935e-06,
         -3.0696e-06, -3.5912e-06],
        [-3.0845e-06, -1.9073e-06,  2.2203e-06,  ..., -2.6673e-06,
         -2.0266e-06, -2.4587e-06],
        [-3.6210e-06, -2.4587e-06,  2.6077e-06,  ..., -3.1739e-06,
         -2.5332e-06, -2.6375e-06]], device='cuda:0')
Loss: 0.9341209530830383


Running epoch 1, step 1873, batch 825
Sampled inputs[:2]: tensor([[   0,  292,   33,  ...,  352,  266, 9129],
        [   0, 2518,  437,  ...,   12, 1041,  283]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4546e-06, -1.3214e-04, -4.2087e-05,  ..., -1.3497e-04,
         -1.5978e-05,  4.2622e-05],
        [-2.9430e-06, -1.9260e-06,  1.9148e-06,  ..., -2.5630e-06,
         -2.0117e-06, -2.1979e-06],
        [-9.1195e-06, -6.3181e-06,  6.4522e-06,  ..., -7.8678e-06,
         -6.2883e-06, -6.7502e-06],
        [-6.1691e-06, -3.9786e-06,  4.1723e-06,  ..., -5.3942e-06,
         -4.2617e-06, -4.7535e-06],
        [-7.3165e-06, -5.1558e-06,  4.9323e-06,  ..., -6.4671e-06,
         -5.3197e-06, -5.0962e-06]], device='cuda:0')
Loss: 0.9328359961509705


Running epoch 1, step 1874, batch 826
Sampled inputs[:2]: tensor([[   0,  333,  199,  ...,  292,   48, 1792],
        [   0, 2422,  300,  ...,  630,  729, 3400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7763e-04, -1.0863e-04,  5.6586e-05,  ...,  3.1377e-05,
         -1.6315e-04,  3.0939e-04],
        [-4.3064e-06, -2.8946e-06,  2.6450e-06,  ..., -3.7849e-06,
         -3.0547e-06, -3.2187e-06],
        [-1.3411e-05, -9.5963e-06,  9.0450e-06,  ..., -1.1712e-05,
         -9.6709e-06, -9.9540e-06],
        [-9.2536e-06, -6.1989e-06,  5.8562e-06,  ..., -8.2254e-06,
         -6.7353e-06, -7.1824e-06],
        [-1.0520e-05, -7.6443e-06,  6.8173e-06,  ..., -9.3877e-06,
         -7.9423e-06, -7.3165e-06]], device='cuda:0')
Loss: 0.9335200190544128


Running epoch 1, step 1875, batch 827
Sampled inputs[:2]: tensor([[   0,  590,   16,  ...,   13,   35, 1151],
        [   0,   69,  462,  ...,  437,  266,  634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1269e-04,  1.2407e-04,  9.3341e-05,  ...,  4.8187e-06,
         -4.9341e-05,  4.5091e-04],
        [-5.7295e-06, -3.9302e-06,  3.4831e-06,  ..., -5.0589e-06,
         -4.1127e-06, -4.2617e-06],
        [-1.7732e-05, -1.2919e-05,  1.1876e-05,  ..., -1.5557e-05,
         -1.2919e-05, -1.3083e-05],
        [-1.2413e-05, -8.4937e-06,  7.8082e-06,  ..., -1.1086e-05,
         -9.1642e-06, -9.6112e-06],
        [-1.3873e-05, -1.0282e-05,  8.9481e-06,  ..., -1.2413e-05,
         -1.0580e-05, -9.5665e-06]], device='cuda:0')
Loss: 0.9532327055931091


Running epoch 1, step 1876, batch 828
Sampled inputs[:2]: tensor([[   0,   76,   15,  ...,   14,  333,  199],
        [   0, 1099,  644,  ..., 5481,   14, 8782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8063e-04,  2.5682e-06,  1.5602e-04,  ..., -4.9975e-05,
         -1.3393e-04,  4.7916e-04],
        [-7.0333e-06, -4.8839e-06,  4.5039e-06,  ..., -6.1989e-06,
         -5.0627e-06, -5.1782e-06],
        [-2.1785e-05, -1.5944e-05,  1.5244e-05,  ..., -1.9044e-05,
         -1.5810e-05, -1.5900e-05],
        [-1.5348e-05, -1.0610e-05,  1.0192e-05,  ..., -1.3649e-05,
         -1.1295e-05, -1.1772e-05],
        [-1.7002e-05, -1.2726e-05,  1.1481e-05,  ..., -1.5154e-05,
         -1.2964e-05, -1.1578e-05]], device='cuda:0')
Loss: 0.9865065217018127


Running epoch 1, step 1877, batch 829
Sampled inputs[:2]: tensor([[    0, 10296,   809,  ..., 27683,    12,   287],
        [    0,   292,  1820,  ...,   591,  6619,  1607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0401e-04, -1.1236e-05,  2.1069e-04,  ..., -1.1329e-04,
         -1.5904e-04,  3.7445e-04],
        [-8.2478e-06, -5.7817e-06,  5.3495e-06,  ..., -7.3090e-06,
         -5.9903e-06, -6.0946e-06],
        [-2.5436e-05, -1.8746e-05,  1.8105e-05,  ..., -2.2292e-05,
         -1.8567e-05, -1.8507e-05],
        [-1.8105e-05, -1.2606e-05,  1.2234e-05,  ..., -1.6168e-05,
         -1.3441e-05, -1.3947e-05],
        [-1.9819e-05, -1.4961e-05,  1.3627e-05,  ..., -1.7688e-05,
         -1.5214e-05, -1.3411e-05]], device='cuda:0')
Loss: 0.9226790070533752


Running epoch 1, step 1878, batch 830
Sampled inputs[:2]: tensor([[    0,  1867,   300,  ...,   259,  3095,  1842],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9894e-05,  4.8557e-05,  2.0419e-04,  ..., -1.1056e-04,
         -1.0966e-04,  3.5698e-04],
        [-9.5144e-06, -6.7353e-06,  6.3702e-06,  ..., -8.3968e-06,
         -6.8694e-06, -6.9514e-06],
        [ 5.7204e-05,  6.7085e-05, -3.0771e-05,  ...,  2.0385e-05,
          9.2237e-05, -3.5181e-07],
        [-2.1040e-05, -1.4782e-05,  1.4707e-05,  ..., -1.8671e-05,
         -1.5453e-05, -1.6049e-05],
        [-2.2739e-05, -1.7345e-05,  1.6056e-05,  ..., -2.0236e-05,
         -1.7345e-05, -1.5244e-05]], device='cuda:0')
Loss: 0.9832762479782104


Running epoch 1, step 1879, batch 831
Sampled inputs[:2]: tensor([[    0,   380,  3584,  ..., 24402,  2057,     9],
        [    0, 16763,  1538,  ...,   631,  3299,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2096e-06,  1.7897e-04, -7.7213e-05,  ..., -8.8936e-05,
         -7.3717e-04,  3.2748e-04],
        [-1.0833e-05, -7.6741e-06,  7.4431e-06,  ..., -9.5144e-06,
         -7.7710e-06, -7.8268e-06],
        [ 5.3151e-05,  6.4075e-05, -2.7254e-05,  ...,  1.6987e-05,
          8.9466e-05, -3.0340e-06],
        [-2.4021e-05, -1.6868e-05,  1.7241e-05,  ..., -2.1175e-05,
         -1.7479e-05, -1.8135e-05],
        [-2.5719e-05, -1.9684e-05,  1.8559e-05,  ..., -2.2799e-05,
         -1.9550e-05, -1.7054e-05]], device='cuda:0')
Loss: 0.9599664211273193
Graident accumulation at epoch 1, step 1879, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3447e-05,  4.3049e-05, -5.9980e-05,  ..., -4.9689e-05,
         -1.0788e-04,  3.0085e-05],
        [-9.3896e-06, -6.0591e-06,  6.7122e-06,  ..., -8.3742e-06,
         -6.0701e-06, -6.7891e-06],
        [ 1.9058e-05,  3.5791e-05, -1.4658e-05,  ...,  1.0183e-05,
          3.3101e-05,  8.4028e-07],
        [-1.6710e-05, -8.3668e-06,  1.3457e-05,  ..., -1.3958e-05,
         -8.8265e-06, -1.3276e-05],
        [-2.5811e-05, -1.9509e-05,  1.9230e-05,  ..., -2.2608e-05,
         -1.9078e-05, -1.6761e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5383e-08, 5.6916e-08, 6.7866e-08,  ..., 2.7510e-08, 1.5923e-07,
         3.9873e-08],
        [9.6113e-11, 6.2454e-11, 2.9580e-11,  ..., 6.4266e-11, 3.3017e-11,
         3.6414e-11],
        [4.6658e-09, 3.2850e-09, 1.9589e-09,  ..., 3.7163e-09, 1.9320e-09,
         1.3701e-09],
        [1.2721e-09, 1.4118e-09, 5.5519e-10,  ..., 1.1196e-09, 8.9609e-10,
         4.8435e-10],
        [4.1649e-10, 2.3718e-10, 9.9316e-11,  ..., 3.0688e-10, 1.0267e-10,
         1.2118e-10]], device='cuda:0')
optimizer state dict: 235.0
lr: [5.518433506354004e-07, 5.518433506354004e-07]
scheduler_last_epoch: 235


Running epoch 1, step 1880, batch 832
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   298,   527,   496],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9627e-04, -2.9012e-04, -1.3209e-04,  ...,  1.9074e-04,
         -2.0353e-05, -2.6090e-05],
        [-1.3486e-06, -9.7603e-07,  1.0580e-06,  ..., -1.1176e-06,
         -9.3877e-07, -9.2015e-07],
        [ 7.6411e-05,  8.2349e-05, -4.3335e-05,  ...,  1.0479e-05,
          7.9331e-05,  1.1842e-05],
        [-3.0100e-06, -2.1458e-06,  2.4736e-06,  ..., -2.4885e-06,
         -2.1011e-06, -2.1458e-06],
        [-3.0547e-06, -2.4289e-06,  2.5034e-06,  ..., -2.5630e-06,
         -2.2799e-06, -1.8999e-06]], device='cuda:0')
Loss: 0.967036247253418


Running epoch 1, step 1881, batch 833
Sampled inputs[:2]: tensor([[    0, 50208,   292,  ...,   408,   266,  3775],
        [    0,   472,   346,  ...,   266,   720,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1195e-04, -2.8917e-04, -2.2774e-04,  ...,  3.0855e-04,
         -2.4787e-04, -4.3914e-05],
        [-2.7195e-06, -1.8813e-06,  2.0117e-06,  ..., -2.3097e-06,
         -1.8589e-06, -1.8813e-06],
        [ 7.2179e-05,  7.9368e-05, -4.0176e-05,  ...,  6.7987e-06,
          7.6440e-05,  8.9065e-06],
        [-6.1095e-06, -4.1723e-06,  4.7237e-06,  ..., -5.2005e-06,
         -4.2319e-06, -4.4405e-06],
        [-6.1989e-06, -4.7535e-06,  4.7982e-06,  ..., -5.3495e-06,
         -4.5449e-06, -3.9265e-06]], device='cuda:0')
Loss: 0.9741357564926147


Running epoch 1, step 1882, batch 834
Sampled inputs[:2]: tensor([[    0,   313,    66,  ...,   894,  2973, 25074],
        [    0,   365,  1462,  ...,   518,  6104,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2885e-04, -3.7102e-04, -2.4549e-04,  ...,  3.2156e-04,
         -2.6716e-04, -5.6322e-05],
        [-4.0531e-06, -2.9020e-06,  3.0398e-06,  ..., -3.4645e-06,
         -2.7753e-06, -2.8014e-06],
        [ 6.7887e-05,  7.6045e-05, -3.6719e-05,  ...,  3.1181e-06,
          7.3520e-05,  5.9263e-06],
        [-9.1344e-06, -6.4373e-06,  7.1377e-06,  ..., -7.7933e-06,
         -6.2734e-06, -6.6161e-06],
        [-9.3579e-06, -7.2867e-06,  7.2718e-06,  ..., -8.1062e-06,
         -6.8247e-06, -5.9530e-06]], device='cuda:0')
Loss: 0.9916723370552063


Running epoch 1, step 1883, batch 835
Sampled inputs[:2]: tensor([[    0,  1688,   790,  ...,   546,   696,    12],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6978e-04, -4.3925e-04, -2.0002e-04,  ...,  2.3739e-04,
         -1.1688e-04,  3.7444e-05],
        [-5.4538e-06, -3.9749e-06,  4.0382e-06,  ..., -4.6864e-06,
         -3.7886e-06, -3.7216e-06],
        [ 3.9286e-04,  4.5345e-04, -2.0007e-04,  ...,  2.4184e-04,
          3.9605e-04,  7.6215e-05],
        [-1.2249e-05, -8.8066e-06,  9.4324e-06,  ..., -1.0520e-05,
         -8.5682e-06, -8.7470e-06],
        [-1.2666e-05, -1.0073e-05,  9.7305e-06,  ..., -1.1072e-05,
         -9.4324e-06, -8.0094e-06]], device='cuda:0')
Loss: 0.9873553514480591


Running epoch 1, step 1884, batch 836
Sampled inputs[:2]: tensor([[    0, 20080, 11069,  ...,   300,  5768,   271],
        [    0, 12456,    14,  ...,  1822,  1016,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4930e-04, -4.0984e-04, -2.6705e-04,  ...,  2.6428e-04,
          1.2076e-04, -2.7833e-05],
        [-6.9067e-06, -4.8727e-06,  4.9733e-06,  ..., -5.9158e-06,
         -4.7423e-06, -4.7721e-06],
        [ 3.8850e-04,  4.5056e-04, -1.9701e-04,  ...,  2.3816e-04,
          3.9321e-04,  7.3101e-05],
        [-1.5333e-05, -1.0692e-05,  1.1533e-05,  ..., -1.3143e-05,
         -1.0595e-05, -1.1086e-05],
        [-1.6153e-05, -1.2472e-05,  1.2025e-05,  ..., -1.4082e-05,
         -1.1891e-05, -1.0334e-05]], device='cuda:0')
Loss: 0.9434157609939575


Running epoch 1, step 1885, batch 837
Sampled inputs[:2]: tensor([[    0,  3504,     9,  ...,  7166, 10945,  3119],
        [    0,   494,   221,  ...,   437,   266,  2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1690e-05, -4.7659e-04, -1.6279e-04,  ...,  5.1379e-04,
          2.3121e-05, -1.8229e-04],
        [-8.2701e-06, -5.7593e-06,  6.0089e-06,  ..., -7.0706e-06,
         -5.6624e-06, -5.7183e-06],
        [ 3.8421e-04,  4.4765e-04, -1.9357e-04,  ...,  2.3458e-04,
          3.9038e-04,  7.0135e-05],
        [-1.8358e-05, -1.2599e-05,  1.3903e-05,  ..., -1.5676e-05,
         -1.2591e-05, -1.3277e-05],
        [-1.9416e-05, -1.4767e-05,  1.4558e-05,  ..., -1.6853e-05,
         -1.4186e-05, -1.2405e-05]], device='cuda:0')
Loss: 0.9677252173423767


Running epoch 1, step 1886, batch 838
Sampled inputs[:2]: tensor([[   0,  278,  668,  ..., 2743,  638,  609],
        [   0,  510,   13,  ..., 3454,  513,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0553e-05, -3.3460e-04,  2.4019e-05,  ...,  6.3008e-04,
          5.4137e-05, -4.3106e-05],
        [-9.6709e-06, -6.7092e-06,  6.9104e-06,  ..., -8.2627e-06,
         -6.6087e-06, -6.7092e-06],
        [ 3.7980e-04,  4.4449e-04, -1.9046e-04,  ...,  2.3084e-04,
          3.8739e-04,  6.7006e-05],
        [-2.1487e-05, -1.4685e-05,  1.6004e-05,  ..., -1.8343e-05,
         -1.4707e-05, -1.5587e-05],
        [-2.2724e-05, -1.7226e-05,  1.6749e-05,  ..., -1.9759e-05,
         -1.6630e-05, -1.4581e-05]], device='cuda:0')
Loss: 0.9307020306587219


Running epoch 1, step 1887, batch 839
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  2165,  1323,  ...,   199,   677,  8376]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9661e-04, -3.6128e-04, -9.5526e-05,  ...,  6.0562e-04,
         -3.5541e-05, -4.4570e-05],
        [-1.1094e-05, -7.6927e-06,  7.9088e-06,  ..., -9.4548e-06,
         -7.5363e-06, -7.6778e-06],
        [ 3.7530e-04,  4.4124e-04, -1.8708e-04,  ...,  2.2707e-04,
          3.8447e-04,  6.3921e-05],
        [-2.4587e-05, -1.6816e-05,  1.8284e-05,  ..., -2.0936e-05,
         -1.6719e-05, -1.7792e-05],
        [-2.6122e-05, -1.9744e-05,  1.9208e-05,  ..., -2.2665e-05,
         -1.8969e-05, -1.6741e-05]], device='cuda:0')
Loss: 0.9629733562469482
Graident accumulation at epoch 1, step 1887, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4409e-06,  2.6160e-06, -6.3534e-05,  ...,  1.5842e-05,
         -1.0065e-04,  2.2619e-05],
        [-9.5600e-06, -6.2225e-06,  6.8318e-06,  ..., -8.4822e-06,
         -6.2167e-06, -6.8780e-06],
        [ 5.4682e-05,  7.6336e-05, -3.1900e-05,  ...,  3.1872e-05,
          6.8238e-05,  7.1484e-06],
        [-1.7498e-05, -9.2117e-06,  1.3939e-05,  ..., -1.4656e-05,
         -9.6158e-06, -1.3727e-05],
        [-2.5842e-05, -1.9533e-05,  1.9228e-05,  ..., -2.2614e-05,
         -1.9067e-05, -1.6759e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5366e-08, 5.6990e-08, 6.7807e-08,  ..., 2.7849e-08, 1.5907e-07,
         3.9835e-08],
        [9.6140e-11, 6.2450e-11, 2.9613e-11,  ..., 6.4291e-11, 3.3040e-11,
         3.6436e-11],
        [4.8020e-09, 3.4764e-09, 1.9919e-09,  ..., 3.7642e-09, 2.0779e-09,
         1.3728e-09],
        [1.2714e-09, 1.4107e-09, 5.5497e-10,  ..., 1.1189e-09, 8.9548e-10,
         4.8418e-10],
        [4.1676e-10, 2.3734e-10, 9.9585e-11,  ..., 3.0709e-10, 1.0293e-10,
         1.2133e-10]], device='cuda:0')
optimizer state dict: 236.0
lr: [5.120691576200498e-07, 5.120691576200498e-07]
scheduler_last_epoch: 236


Running epoch 1, step 1888, batch 840
Sampled inputs[:2]: tensor([[    0,  6660, 13165,  ...,   380,   333,   199],
        [    0,   504,   409,  ...,  5863,  2621,   824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1662e-05,  6.6411e-05, -9.9267e-06,  ..., -1.4613e-04,
         -1.0266e-04, -6.5026e-05],
        [-1.3188e-06, -1.0133e-06,  8.7172e-07,  ..., -1.2517e-06,
         -1.0580e-06, -9.9093e-07],
        [-4.0829e-06, -3.3230e-06,  2.9802e-06,  ..., -3.8743e-06,
         -3.3230e-06, -3.0547e-06],
        [-2.9355e-06, -2.2501e-06,  2.0415e-06,  ..., -2.8014e-06,
         -2.3991e-06, -2.3395e-06],
        [-3.2187e-06, -2.6822e-06,  2.2650e-06,  ..., -3.0845e-06,
         -2.7269e-06, -2.2203e-06]], device='cuda:0')
Loss: 0.9705690145492554


Running epoch 1, step 1889, batch 841
Sampled inputs[:2]: tensor([[   0,  413,   16,  ...,  493, 2104,   14],
        [   0,   13, 1320,  ..., 8686, 6851,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2894e-05,  2.7062e-04,  4.2538e-04,  ..., -1.9093e-04,
         -2.5917e-04, -2.5947e-06],
        [-2.6301e-06, -2.0489e-06,  1.6578e-06,  ..., -2.4438e-06,
         -2.0787e-06, -1.9595e-06],
        [-8.3148e-06, -6.8247e-06,  5.7966e-06,  ..., -7.6890e-06,
         -6.6161e-06, -6.1393e-06],
        [-5.9009e-06, -4.5896e-06,  3.8892e-06,  ..., -5.5283e-06,
         -4.7684e-06, -4.6343e-06],
        [-6.3628e-06, -5.3495e-06,  4.2617e-06,  ..., -6.0052e-06,
         -5.3495e-06, -4.3511e-06]], device='cuda:0')
Loss: 0.9312284588813782


Running epoch 1, step 1890, batch 842
Sampled inputs[:2]: tensor([[    0,  1795,   365,  ...,   266, 46932,   293],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8436e-04,  2.4278e-04,  3.7288e-04,  ..., -1.7645e-04,
          1.5095e-04,  3.8578e-05],
        [-3.9563e-06, -3.0622e-06,  2.4997e-06,  ..., -3.6433e-06,
         -3.1218e-06, -2.9132e-06],
        [-1.2338e-05, -1.0058e-05,  8.6278e-06,  ..., -1.1280e-05,
         -9.7752e-06, -8.9705e-06],
        [-8.8662e-06, -6.8545e-06,  5.8562e-06,  ..., -8.2552e-06,
         -7.1824e-06, -6.9141e-06],
        [-9.5069e-06, -7.9423e-06,  6.4075e-06,  ..., -8.8364e-06,
         -7.9274e-06, -6.3628e-06]], device='cuda:0')
Loss: 0.9831405878067017


Running epoch 1, step 1891, batch 843
Sampled inputs[:2]: tensor([[   0, 1979,  352,  ...,  292, 1591,  446],
        [   0,  600,   14,  ...,  221, 8187, 1802]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9612e-04,  1.9179e-04,  2.7879e-04,  ..., -2.9786e-04,
          2.2682e-04, -4.4433e-05],
        [-5.2378e-06, -3.8780e-06,  3.5577e-06,  ..., -4.7609e-06,
         -3.9600e-06, -3.7849e-06],
        [-1.6302e-05, -1.2621e-05,  1.2115e-05,  ..., -1.4603e-05,
         -1.2249e-05, -1.1548e-05],
        [-1.1787e-05, -8.6427e-06,  8.4043e-06,  ..., -1.0759e-05,
         -9.0450e-06, -8.9705e-06],
        [-1.2428e-05, -9.9242e-06,  8.8960e-06,  ..., -1.1340e-05,
         -9.9242e-06, -8.0988e-06]], device='cuda:0')
Loss: 0.9167343378067017


Running epoch 1, step 1892, batch 844
Sampled inputs[:2]: tensor([[   0,   18,  998,  ..., 5322,  504,  287],
        [   0, 3070, 9719,  ...,  600, 4207, 4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5122e-05,  2.0616e-04,  2.9886e-04,  ..., -2.6296e-04,
          3.0150e-04, -8.5681e-05],
        [-6.6310e-06, -4.8988e-06,  4.4852e-06,  ..., -6.0052e-06,
         -5.0031e-06, -4.7535e-06],
        [-2.0683e-05, -1.6019e-05,  1.5274e-05,  ..., -1.8507e-05,
         -1.5542e-05, -1.4618e-05],
        [-1.4827e-05, -1.0878e-05,  1.0520e-05,  ..., -1.3500e-05,
         -1.1340e-05, -1.1206e-05],
        [-1.5795e-05, -1.2636e-05,  1.1250e-05,  ..., -1.4409e-05,
         -1.2606e-05, -1.0274e-05]], device='cuda:0')
Loss: 0.9773629903793335


Running epoch 1, step 1893, batch 845
Sampled inputs[:2]: tensor([[    0,   278, 39533,  ...,   277,  1395, 47607],
        [    0,  7926,  6750,  ...,   259,  1524,  6257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9902e-05,  1.4144e-04,  2.8994e-04,  ..., -2.7894e-04,
          3.6613e-04, -2.2759e-04],
        [-7.9647e-06, -5.9195e-06,  5.4985e-06,  ..., -7.1898e-06,
         -5.9865e-06, -5.6773e-06],
        [-2.4796e-05, -1.9282e-05,  1.8626e-05,  ..., -2.2128e-05,
         -1.8567e-05, -1.7479e-05],
        [-1.7792e-05, -1.3113e-05,  1.2875e-05,  ..., -1.6138e-05,
         -1.3530e-05, -1.3396e-05],
        [-1.8939e-05, -1.5244e-05,  1.3739e-05,  ..., -1.7256e-05,
         -1.5080e-05, -1.2301e-05]], device='cuda:0')
Loss: 0.9775350689888


Running epoch 1, step 1894, batch 846
Sampled inputs[:2]: tensor([[   0,  396,  221,  ..., 1279,  720,  292],
        [   0, 4209,  278,  ...,  287, 9971,  717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9943e-04,  2.2585e-04,  3.9303e-05,  ..., -5.5919e-04,
          3.8779e-04, -2.0337e-04],
        [-9.6336e-06, -6.9030e-06,  6.4299e-06,  ..., -8.6129e-06,
         -7.0296e-06, -6.9812e-06],
        [-2.9773e-05, -2.2441e-05,  2.1666e-05,  ..., -2.6390e-05,
         -2.1771e-05, -2.1353e-05],
        [-2.1115e-05, -1.5065e-05,  1.4812e-05,  ..., -1.8999e-05,
         -1.5661e-05, -1.6063e-05],
        [-2.3261e-05, -1.8016e-05,  1.6257e-05,  ..., -2.0996e-05,
         -1.8001e-05, -1.5490e-05]], device='cuda:0')
Loss: 0.8924122452735901


Running epoch 1, step 1895, batch 847
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,  9161,   300,  4460],
        [    0,  1456, 32380,  ...,    12,  1172, 12557]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1190e-04,  2.6261e-04, -3.8520e-05,  ..., -6.1699e-04,
         -4.7105e-04, -3.9617e-04],
        [-1.1005e-05, -7.7300e-06,  7.5400e-06,  ..., -9.7379e-06,
         -7.8455e-06, -7.8976e-06],
        [-3.3885e-05, -2.5019e-05,  2.5213e-05,  ..., -2.9728e-05,
         -2.4199e-05, -2.4065e-05],
        [-2.4259e-05, -1.6905e-05,  1.7479e-05,  ..., -2.1562e-05,
         -1.7509e-05, -1.8269e-05],
        [-2.6271e-05, -2.0042e-05,  1.8790e-05,  ..., -2.3499e-05,
         -1.9953e-05, -1.7315e-05]], device='cuda:0')
Loss: 0.9422706365585327
Graident accumulation at epoch 1, step 1895, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9894e-05,  2.8616e-05, -6.1033e-05,  ..., -4.7441e-05,
         -1.3769e-04, -1.9259e-05],
        [-9.7045e-06, -6.3732e-06,  6.9026e-06,  ..., -8.6078e-06,
         -6.3796e-06, -6.9799e-06],
        [ 4.5825e-05,  6.6201e-05, -2.6188e-05,  ...,  2.5712e-05,
          5.8994e-05,  4.0270e-06],
        [-1.8174e-05, -9.9810e-06,  1.4293e-05,  ..., -1.5347e-05,
         -1.0405e-05, -1.4181e-05],
        [-2.5885e-05, -1.9584e-05,  1.9184e-05,  ..., -2.2702e-05,
         -1.9155e-05, -1.6815e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5408e-08, 5.7002e-08, 6.7741e-08,  ..., 2.8202e-08, 1.5913e-07,
         3.9952e-08],
        [9.6165e-11, 6.2448e-11, 2.9640e-11,  ..., 6.4322e-11, 3.3069e-11,
         3.6462e-11],
        [4.7983e-09, 3.4735e-09, 1.9905e-09,  ..., 3.7613e-09, 2.0764e-09,
         1.3720e-09],
        [1.2707e-09, 1.4096e-09, 5.5472e-10,  ..., 1.1183e-09, 8.9489e-10,
         4.8403e-10],
        [4.1703e-10, 2.3750e-10, 9.9839e-11,  ..., 3.0733e-10, 1.0323e-10,
         1.2151e-10]], device='cuda:0')
optimizer state dict: 237.0
lr: [4.737448032587366e-07, 4.737448032587366e-07]
scheduler_last_epoch: 237


Running epoch 1, step 1896, batch 848
Sampled inputs[:2]: tensor([[    0,   368,   275,  ...,  6389,  9102,    12],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8603e-05, -5.4262e-05, -8.0185e-05,  ..., -8.3022e-06,
          5.7328e-06,  9.8119e-05],
        [-1.3337e-06, -9.7603e-07,  8.9779e-07,  ..., -1.1474e-06,
         -1.0058e-06, -9.1642e-07],
        [-4.0829e-06, -3.0845e-06,  2.9653e-06,  ..., -3.4869e-06,
         -3.0398e-06, -2.7716e-06],
        [-2.9504e-06, -2.1309e-06,  2.0713e-06,  ..., -2.5481e-06,
         -2.2352e-06, -2.1607e-06],
        [-3.1590e-06, -2.4736e-06,  2.2501e-06,  ..., -2.7418e-06,
         -2.4885e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9682242274284363


Running epoch 1, step 1897, batch 849
Sampled inputs[:2]: tensor([[    0,  2530,   634,  ...,    15,  8808,     9],
        [    0, 15372, 10123,  ...,  1782,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3875e-04, -2.4546e-05,  8.9888e-05,  ...,  1.5535e-05,
          3.1268e-04,  2.6000e-04],
        [-2.6599e-06, -2.0489e-06,  1.7770e-06,  ..., -2.3767e-06,
         -2.1085e-06, -1.8589e-06],
        [-8.2254e-06, -6.5118e-06,  5.9009e-06,  ..., -7.3016e-06,
         -6.4969e-06, -5.6922e-06],
        [ 1.2519e-04,  1.9216e-04, -8.3925e-05,  ...,  9.6039e-05,
          1.5109e-04,  7.3274e-05],
        [-6.4820e-06, -5.2899e-06,  4.5598e-06,  ..., -5.8562e-06,
         -5.3793e-06, -4.1872e-06]], device='cuda:0')
Loss: 0.9906423687934875


Running epoch 1, step 1898, batch 850
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0,   298,   669,  ...,   287, 19731,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8187e-06,  8.6000e-05,  1.8611e-05,  ..., -4.3028e-06,
          8.3141e-05,  4.7183e-04],
        [-4.1202e-06, -2.9802e-06,  2.8498e-06,  ..., -3.6210e-06,
         -3.0696e-06, -2.8573e-06],
        [-1.2726e-05, -9.5218e-06,  9.4622e-06,  ..., -1.1086e-05,
         -9.4920e-06, -8.7470e-06],
        [ 1.2203e-04,  1.9018e-04, -8.1482e-05,  ...,  9.3356e-05,
          1.4900e-04,  7.1039e-05],
        [-9.8944e-06, -7.6890e-06,  7.1675e-06,  ..., -8.8215e-06,
         -7.8380e-06, -6.3330e-06]], device='cuda:0')
Loss: 0.9491893649101257


Running epoch 1, step 1899, batch 851
Sampled inputs[:2]: tensor([[    0,  3665,  1419,  ...,   600,   847,   328],
        [    0,    14,  3741,  ...,   278, 12472, 10257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1289e-05,  8.8724e-05,  1.8611e-05,  ..., -1.0550e-04,
          2.5582e-08,  4.5637e-04],
        [-5.4911e-06, -3.8482e-06,  3.8780e-06,  ..., -4.7758e-06,
         -3.9116e-06, -3.8259e-06],
        [-1.6838e-05, -1.2189e-05,  1.2785e-05,  ..., -1.4469e-05,
         -1.1995e-05, -1.1563e-05],
        [ 1.1911e-04,  1.8839e-04, -7.9187e-05,  ...,  9.0927e-05,
          1.4722e-04,  6.8879e-05],
        [-1.3009e-05, -9.8348e-06,  9.6262e-06,  ..., -1.1459e-05,
         -9.9093e-06, -8.2850e-06]], device='cuda:0')
Loss: 0.9264222979545593


Running epoch 1, step 1900, batch 852
Sampled inputs[:2]: tensor([[    0,    12,  3367,  ..., 16917, 12221, 12138],
        [    0,  1615,   292,  ...,  4824,   292,  9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7237e-05,  1.1887e-04, -3.8539e-05,  ..., -7.5010e-05,
         -7.9729e-05,  5.9608e-04],
        [-6.8545e-06, -4.8988e-06,  4.8541e-06,  ..., -5.9530e-06,
         -4.9472e-06, -4.7609e-06],
        [-2.1130e-05, -1.5646e-05,  1.6093e-05,  ..., -1.8179e-05,
         -1.5289e-05, -1.4544e-05],
        [ 1.1613e-04,  1.8611e-04, -7.6952e-05,  ...,  8.8350e-05,
          1.4494e-04,  6.6703e-05],
        [-1.6242e-05, -1.2577e-05,  1.2025e-05,  ..., -1.4335e-05,
         -1.2562e-05, -1.0371e-05]], device='cuda:0')
Loss: 0.9766157269477844


Running epoch 1, step 1901, batch 853
Sampled inputs[:2]: tensor([[    0,  8353,  1842,  ...,    38,   643,   472],
        [    0,   346,   462,  ..., 37683,    14,  1500]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8465e-04,  3.5066e-05, -7.8715e-05,  ...,  1.6049e-04,
         -6.8642e-04,  4.1346e-04],
        [-8.1807e-06, -5.8413e-06,  5.8599e-06,  ..., -7.1004e-06,
         -5.8450e-06, -5.7295e-06],
        [-2.5332e-05, -1.8790e-05,  1.9491e-05,  ..., -2.1785e-05,
         -1.8150e-05, -1.7583e-05],
        [ 1.1313e-04,  1.8402e-04, -7.4568e-05,  ...,  8.5787e-05,
          1.4293e-04,  6.4423e-05],
        [-1.9386e-05, -1.5020e-05,  1.4499e-05,  ..., -1.7107e-05,
         -1.4856e-05, -1.2502e-05]], device='cuda:0')
Loss: 0.9127801060676575


Running epoch 1, step 1902, batch 854
Sampled inputs[:2]: tensor([[    0,  3217, 16714,  ...,   462,   221,   474],
        [    0,    14,  4494,  ...,  4830,   368,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5344e-05,  2.4943e-05, -9.1418e-05,  ...,  1.5317e-04,
         -7.6840e-04,  4.9658e-04],
        [-9.5516e-06, -6.8173e-06,  6.8732e-06,  ..., -8.2925e-06,
         -6.7726e-06, -6.6757e-06],
        [-2.9653e-05, -2.1979e-05,  2.2888e-05,  ..., -2.5526e-05,
         -2.1085e-05, -2.0579e-05],
        [ 1.1012e-04,  1.8191e-04, -7.2228e-05,  ...,  8.3164e-05,
          1.4089e-04,  6.2248e-05],
        [-2.2620e-05, -1.7509e-05,  1.6958e-05,  ..., -1.9968e-05,
         -1.7196e-05, -1.4588e-05]], device='cuda:0')
Loss: 0.9868113398551941


Running epoch 1, step 1903, batch 855
Sampled inputs[:2]: tensor([[    0,    13,  2615,  ..., 31594, 15867,  3484],
        [    0, 21413,  1735,  ..., 10789, 12523,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2217e-05,  1.7335e-05,  2.3164e-05,  ...,  5.1951e-05,
         -8.2577e-04,  5.3167e-04],
        [-1.0870e-05, -7.8604e-06,  7.8566e-06,  ..., -9.4622e-06,
         -7.7263e-06, -7.6033e-06],
        [-3.3736e-05, -2.5362e-05,  2.6211e-05,  ..., -2.9147e-05,
         -2.4065e-05, -2.3469e-05],
        [ 1.0723e-04,  1.7963e-04, -6.9948e-05,  ...,  8.0586e-05,
          1.3879e-04,  6.0102e-05],
        [-2.5734e-05, -2.0191e-05,  1.9401e-05,  ..., -2.2784e-05,
         -1.9625e-05, -1.6600e-05]], device='cuda:0')
Loss: 0.9695026874542236
Graident accumulation at epoch 1, step 1903, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.6126e-05,  2.7488e-05, -5.2613e-05,  ..., -3.7502e-05,
         -2.0649e-04,  3.5833e-05],
        [-9.8210e-06, -6.5219e-06,  6.9980e-06,  ..., -8.6932e-06,
         -6.5142e-06, -7.0423e-06],
        [ 3.7869e-05,  5.7044e-05, -2.0948e-05,  ...,  2.0226e-05,
          5.0688e-05,  1.2774e-06],
        [-5.6334e-06,  8.9799e-06,  5.8692e-06,  ..., -5.7533e-06,
          4.5143e-06, -6.7531e-06],
        [-2.5870e-05, -1.9644e-05,  1.9206e-05,  ..., -2.2710e-05,
         -1.9202e-05, -1.6793e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5361e-08, 5.6945e-08, 6.7673e-08,  ..., 2.8176e-08, 1.5966e-07,
         4.0195e-08],
        [9.6187e-11, 6.2447e-11, 2.9672e-11,  ..., 6.4347e-11, 3.3096e-11,
         3.6484e-11],
        [4.7947e-09, 3.4707e-09, 1.9892e-09,  ..., 3.7584e-09, 2.0749e-09,
         1.3712e-09],
        [1.2810e-09, 1.4404e-09, 5.5906e-10,  ..., 1.1237e-09, 9.1326e-10,
         4.8716e-10],
        [4.1728e-10, 2.3767e-10, 1.0012e-10,  ..., 3.0754e-10, 1.0351e-10,
         1.2167e-10]], device='cuda:0')
optimizer state dict: 238.0
lr: [4.36876143847339e-07, 4.36876143847339e-07]
scheduler_last_epoch: 238


Running epoch 1, step 1904, batch 856
Sampled inputs[:2]: tensor([[    0,   360,  3285,  ...,   423,  3579,   468],
        [    0,   346,   462,  ...,   474, 38333,    87]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0130e-04, -5.7301e-05, -1.1570e-04,  ...,  9.2514e-05,
         -2.8336e-04, -1.3609e-05],
        [-1.3039e-06, -8.1956e-07,  1.0282e-06,  ..., -1.1176e-06,
         -7.8976e-07, -9.3505e-07],
        [-4.0829e-06, -2.6822e-06,  3.4571e-06,  ..., -3.4422e-06,
         -2.4140e-06, -2.8461e-06],
        [-2.9951e-06, -1.8328e-06,  2.4736e-06,  ..., -2.5779e-06,
         -1.8105e-06, -2.2650e-06],
        [-2.9206e-06, -1.9968e-06,  2.3991e-06,  ..., -2.5034e-06,
         -1.8701e-06, -1.8701e-06]], device='cuda:0')
Loss: 0.9565120935440063


Running epoch 1, step 1905, batch 857
Sampled inputs[:2]: tensor([[    0, 16847,  2027,  ...,     5,  1460,   496],
        [    0,  6132,   300,  ...,    37,   271,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2538e-04, -9.5197e-05, -1.7607e-05,  ...,  7.9545e-05,
         -3.0874e-04, -2.1277e-05],
        [-2.6375e-06, -1.7583e-06,  1.9111e-06,  ..., -2.2799e-06,
         -1.7509e-06, -1.8701e-06],
        [-8.2850e-06, -5.8115e-06,  6.4820e-06,  ..., -7.0781e-06,
         -5.4985e-06, -5.7667e-06],
        [-5.9605e-06, -3.8743e-06,  4.5151e-06,  ..., -5.1558e-06,
         -3.9712e-06, -4.4405e-06],
        [-6.0648e-06, -4.4107e-06,  4.6045e-06,  ..., -5.2750e-06,
         -4.2990e-06, -3.8967e-06]], device='cuda:0')
Loss: 0.9407197833061218


Running epoch 1, step 1906, batch 858
Sampled inputs[:2]: tensor([[    0, 21410, 13160,  ...,   292,    69,    14],
        [    0,   292, 16983,  ...,   221,   474,  4800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5317e-04, -1.1549e-04, -4.5990e-05,  ...,  1.3045e-04,
         -1.7206e-04,  2.3162e-04],
        [-3.9265e-06, -2.7493e-06,  2.6859e-06,  ..., -3.5241e-06,
         -2.8238e-06, -2.8834e-06],
        [-1.2279e-05, -9.0152e-06,  9.1344e-06,  ..., -1.0848e-05,
         -8.7917e-06, -8.8066e-06],
        [-8.8364e-06, -6.0499e-06,  6.2734e-06,  ..., -7.9572e-06,
         -6.4299e-06, -6.8098e-06],
        [-9.2238e-06, -7.0035e-06,  6.6906e-06,  ..., -8.2850e-06,
         -6.9961e-06, -6.1318e-06]], device='cuda:0')
Loss: 0.9763330221176147


Running epoch 1, step 1907, batch 859
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,  1529,  5227,  ...,  1480,   367,   925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0142e-04, -1.8881e-04, -1.2632e-04,  ...,  2.3807e-04,
         -1.0723e-04,  2.3745e-04],
        [-5.2527e-06, -3.6918e-06,  3.7588e-06,  ..., -4.6343e-06,
         -3.6769e-06, -3.7812e-06],
        [-1.6540e-05, -1.2189e-05,  1.2726e-05,  ..., -1.4409e-05,
         -1.1533e-05, -1.1727e-05],
        [-1.1921e-05, -8.2254e-06,  8.8513e-06,  ..., -1.0520e-05,
         -8.3968e-06, -9.0003e-06],
        [-1.2279e-05, -9.3877e-06,  9.1940e-06,  ..., -1.0908e-05,
         -9.1270e-06, -8.0690e-06]], device='cuda:0')
Loss: 0.9605616331100464


Running epoch 1, step 1908, batch 860
Sampled inputs[:2]: tensor([[    0,   437,  1119,  ..., 32831,    83,   623],
        [    0,  2467, 18011,  ...,  5913,  9281,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5776e-04, -1.8523e-04, -2.8609e-05,  ...,  3.0090e-04,
         -2.8012e-05,  3.2993e-04],
        [-6.5714e-06, -4.6007e-06,  4.6939e-06,  ..., -5.7891e-06,
         -4.6268e-06, -4.7274e-06],
        [-2.0683e-05, -1.5214e-05,  1.5870e-05,  ..., -1.8016e-05,
         -1.4558e-05, -1.4678e-05],
        [-1.4886e-05, -1.0252e-05,  1.1042e-05,  ..., -1.3128e-05,
         -1.0557e-05, -1.1235e-05],
        [-1.5348e-05, -1.1712e-05,  1.1459e-05,  ..., -1.3635e-05,
         -1.1496e-05, -1.0096e-05]], device='cuda:0')
Loss: 0.9535108804702759


Running epoch 1, step 1909, batch 861
Sampled inputs[:2]: tensor([[    0,   437,   266,  ...,   266, 16084,  1781],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1225e-04, -1.8462e-04,  5.1871e-06,  ...,  4.0528e-04,
          7.1711e-05,  3.6018e-04],
        [-7.9498e-06, -5.7183e-06,  5.7071e-06,  ..., -6.9737e-06,
         -5.6922e-06, -5.6885e-06],
        [-2.5034e-05, -1.8910e-05,  1.9282e-05,  ..., -2.1771e-05,
         -1.7911e-05, -1.7732e-05],
        [-1.7956e-05, -1.2726e-05,  1.3396e-05,  ..., -1.5780e-05,
         -1.2957e-05, -1.3500e-05],
        [-1.8656e-05, -1.4633e-05,  1.3977e-05,  ..., -1.6555e-05,
         -1.4208e-05, -1.2256e-05]], device='cuda:0')
Loss: 1.0023611783981323


Running epoch 1, step 1910, batch 862
Sampled inputs[:2]: tensor([[   0,  593,  300,  ...,  278, 4694,   12],
        [   0, 3261, 1518,  ..., 5019,  287, 1906]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1290e-04, -2.3827e-04,  6.3160e-05,  ...,  3.3870e-04,
          7.7377e-05,  4.0500e-04],
        [-9.3058e-06, -6.7689e-06,  6.7279e-06,  ..., -8.1435e-06,
         -6.7204e-06, -6.5975e-06],
        [-2.9385e-05, -2.2456e-05,  2.2754e-05,  ..., -2.5541e-05,
         -2.1234e-05, -2.0698e-05],
        [-2.0996e-05, -1.5065e-05,  1.5780e-05,  ..., -1.8403e-05,
         -1.5281e-05, -1.5661e-05],
        [-2.1800e-05, -1.7315e-05,  1.6421e-05,  ..., -1.9342e-05,
         -1.6756e-05, -1.4238e-05]], device='cuda:0')
Loss: 1.005722999572754


Running epoch 1, step 1911, batch 863
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0, 21178,  1952,  ..., 14930,     9,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1170e-04, -1.2420e-04,  1.0222e-04,  ...,  3.7735e-04,
          2.2309e-05,  4.3679e-04],
        [-1.0647e-05, -7.7374e-06,  7.7263e-06,  ..., -9.2909e-06,
         -7.6331e-06, -7.5474e-06],
        [-3.3677e-05, -2.5645e-05,  2.6152e-05,  ..., -2.9176e-05,
         -2.4125e-05, -2.3708e-05],
        [-2.4065e-05, -1.7241e-05,  1.8135e-05,  ..., -2.1011e-05,
         -1.7352e-05, -1.7926e-05],
        [-2.4989e-05, -1.9774e-05,  1.8865e-05,  ..., -2.2098e-05,
         -1.9051e-05, -1.6294e-05]], device='cuda:0')
Loss: 0.9751531481742859
Graident accumulation at epoch 1, step 1911, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.3683e-05,  1.2319e-05, -3.7130e-05,  ...,  3.9836e-06,
         -1.8361e-04,  7.5929e-05],
        [-9.9036e-06, -6.6435e-06,  7.0709e-06,  ..., -8.7530e-06,
         -6.6261e-06, -7.0928e-06],
        [ 3.0715e-05,  4.8775e-05, -1.6238e-05,  ...,  1.5286e-05,
          4.3207e-05, -1.2211e-06],
        [-7.4766e-06,  6.3579e-06,  7.0958e-06,  ..., -7.2790e-06,
          2.3276e-06, -7.8704e-06],
        [-2.5782e-05, -1.9657e-05,  1.9172e-05,  ..., -2.2649e-05,
         -1.9187e-05, -1.6744e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5350e-08, 5.6903e-08, 6.7616e-08,  ..., 2.8290e-08, 1.5950e-07,
         4.0346e-08],
        [9.6205e-11, 6.2444e-11, 2.9702e-11,  ..., 6.4369e-11, 3.3121e-11,
         3.6504e-11],
        [4.7910e-09, 3.4679e-09, 1.9879e-09,  ..., 3.7555e-09, 2.0734e-09,
         1.3704e-09],
        [1.2803e-09, 1.4393e-09, 5.5883e-10,  ..., 1.1230e-09, 9.1264e-10,
         4.8699e-10],
        [4.1748e-10, 2.3782e-10, 1.0037e-10,  ..., 3.0772e-10, 1.0377e-10,
         1.2181e-10]], device='cuda:0')
optimizer state dict: 239.0
lr: [4.014688132388511e-07, 4.014688132388511e-07]
scheduler_last_epoch: 239


Running epoch 1, step 1912, batch 864
Sampled inputs[:2]: tensor([[   0,    5, 7523,  ...,  199, 8871,  266],
        [   0,  300, 2607,  ..., 1279,  368,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0624e-04,  5.2834e-05,  8.3251e-05,  ...,  1.4607e-04,
          7.0939e-05,  6.6818e-05],
        [-1.3188e-06, -1.0878e-06,  8.3447e-07,  ..., -1.2442e-06,
         -1.0729e-06, -9.6858e-07],
        [-4.2021e-06, -3.4869e-06,  2.8908e-06,  ..., -3.8743e-06,
         -3.3379e-06, -2.9951e-06],
        [-2.9206e-06, -2.3544e-06,  1.9222e-06,  ..., -2.7567e-06,
         -2.3693e-06, -2.2501e-06],
        [-3.2187e-06, -2.7567e-06,  2.1607e-06,  ..., -3.0100e-06,
         -2.6822e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9651585221290588


Running epoch 1, step 1913, batch 865
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 1034, 1928,   15],
        [   0,  287, 2997,  ...,  437,  266, 1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2644e-04, -3.4604e-05,  8.9626e-06,  ...,  1.3719e-04,
          2.5071e-05,  4.6591e-05],
        [-2.7195e-06, -2.1309e-06,  1.8552e-06,  ..., -2.4438e-06,
         -2.1011e-06, -1.9036e-06],
        [ 5.6543e-05,  6.7069e-05, -4.6688e-05,  ...,  6.2812e-05,
          5.2612e-05,  2.7625e-05],
        [-5.9605e-06, -4.5896e-06,  4.2170e-06,  ..., -5.3644e-06,
         -4.6045e-06, -4.3958e-06],
        [-6.5416e-06, -5.4836e-06,  4.6641e-06,  ..., -5.9456e-06,
         -5.3048e-06, -4.2170e-06]], device='cuda:0')
Loss: 0.980132520198822


Running epoch 1, step 1914, batch 866
Sampled inputs[:2]: tensor([[    0,   287, 39084,  ...,   266,  1817,  1589],
        [    0,  4868,  3106,  ...,  2637,   278,   521]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1615e-04,  1.5312e-05, -2.5532e-05,  ...,  1.7332e-04,
         -3.9461e-05,  1.1329e-04],
        [-3.9488e-06, -3.0100e-06,  2.9355e-06,  ..., -3.5614e-06,
         -2.9244e-06, -2.7418e-06],
        [ 5.2803e-05,  6.4298e-05, -4.3126e-05,  ...,  5.9444e-05,
          5.0123e-05,  2.5136e-05],
        [-8.7321e-06, -6.5267e-06,  6.7949e-06,  ..., -7.8827e-06,
         -6.4597e-06, -6.3926e-06],
        [-9.1940e-06, -7.5400e-06,  7.1079e-06,  ..., -8.3745e-06,
         -7.2047e-06, -5.8189e-06]], device='cuda:0')
Loss: 0.9476655125617981


Running epoch 1, step 1915, batch 867
Sampled inputs[:2]: tensor([[   0,   14,   22,  ..., 1319,  271,  266],
        [   0,  266, 1624,  ...,   14,   19,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5144e-04,  3.1449e-05,  3.9363e-05,  ...,  8.0147e-05,
          2.9758e-04,  1.2693e-04],
        [-5.4464e-06, -3.9712e-06,  3.9637e-06,  ..., -4.7982e-06,
         -3.8370e-06, -3.8072e-06],
        [ 4.8034e-05,  6.1079e-05, -3.9639e-05,  ...,  5.5510e-05,
          4.7173e-05,  2.1739e-05],
        [-1.1936e-05, -8.5384e-06,  9.0748e-06,  ..., -1.0520e-05,
         -8.3968e-06, -8.7619e-06],
        [-1.2726e-05, -1.0014e-05,  9.5963e-06,  ..., -1.1325e-05,
         -9.5293e-06, -8.1584e-06]], device='cuda:0')
Loss: 0.9443423748016357


Running epoch 1, step 1916, batch 868
Sampled inputs[:2]: tensor([[    0, 15152,  1106,  ...,   607,   266,  2529],
        [    0, 26074,   486,  ...,  2314,   266,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4003e-04, -8.5813e-05, -1.2647e-04,  ...,  1.8936e-04,
          3.0020e-04,  8.8950e-05],
        [-6.7502e-06, -4.8988e-06,  4.8876e-06,  ..., -5.8934e-06,
         -4.7013e-06, -4.7274e-06],
        [ 4.3773e-05,  5.7965e-05, -3.6346e-05,  ...,  5.2008e-05,
          4.4401e-05,  1.8773e-05],
        [-1.4916e-05, -1.0625e-05,  1.1310e-05,  ..., -1.3024e-05,
         -1.0379e-05, -1.0982e-05],
        [-1.5810e-05, -1.2353e-05,  1.1906e-05,  ..., -1.3918e-05,
         -1.1690e-05, -1.0110e-05]], device='cuda:0')
Loss: 0.9541364908218384


Running epoch 1, step 1917, batch 869
Sampled inputs[:2]: tensor([[   0,  292,  380,  ..., 1725,  271,  266],
        [   0,   14,  333,  ...,  328, 5453, 4713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8489e-04,  2.8899e-06, -8.1805e-06,  ..., -2.0507e-05,
          1.1973e-04, -9.9626e-05],
        [-8.0764e-06, -5.9493e-06,  5.8785e-06,  ..., -7.0408e-06,
         -5.6550e-06, -5.5805e-06],
        [ 3.9690e-05,  5.4538e-05, -3.3038e-05,  ...,  4.8432e-05,
          4.1406e-05,  1.6091e-05],
        [-1.7911e-05, -1.2994e-05,  1.3679e-05,  ..., -1.5616e-05,
         -1.2524e-05, -1.3024e-05],
        [-1.8865e-05, -1.5005e-05,  1.4305e-05,  ..., -1.6645e-05,
         -1.4089e-05, -1.1973e-05]], device='cuda:0')
Loss: 0.9613994359970093


Running epoch 1, step 1918, batch 870
Sampled inputs[:2]: tensor([[    0,    19,     9,  ...,  4971,   367,  1675],
        [    0,  1067,   292,  ..., 10792, 11280,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8489e-04, -5.9295e-06, -3.0753e-05,  ..., -4.8261e-05,
          1.4776e-04, -1.8635e-05],
        [-9.6262e-06, -6.9477e-06,  6.6943e-06,  ..., -8.4043e-06,
         -6.7651e-06, -6.7130e-06],
        [ 3.5279e-05,  5.1423e-05, -3.0460e-05,  ...,  4.4498e-05,
          3.8128e-05,  1.2858e-05],
        [-2.1130e-05, -1.5095e-05,  1.5438e-05,  ..., -1.8492e-05,
         -1.4924e-05, -1.5453e-05],
        [-2.2605e-05, -1.7658e-05,  1.6391e-05,  ..., -2.0027e-05,
         -1.6980e-05, -1.4566e-05]], device='cuda:0')
Loss: 0.900058925151825


Running epoch 1, step 1919, batch 871
Sampled inputs[:2]: tensor([[    0, 38495, 36253,  ..., 11006,  5699,    19],
        [    0,   594,    84,  ..., 24411, 14140, 12720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0431e-04, -2.5202e-05, -1.7683e-04,  ..., -1.0334e-04,
          6.4300e-05, -1.8635e-05],
        [-1.1019e-05, -7.9311e-06,  7.6480e-06,  ..., -9.5963e-06,
         -7.7784e-06, -7.6890e-06],
        [ 3.0987e-05,  4.8249e-05, -2.7286e-05,  ...,  4.0862e-05,
          3.4998e-05,  9.8624e-06],
        [-2.4185e-05, -1.7211e-05,  1.7628e-05,  ..., -2.1100e-05,
         -1.7144e-05, -1.7717e-05],
        [-2.5839e-05, -2.0146e-05,  1.8716e-05,  ..., -2.2814e-05,
         -1.9483e-05, -1.6652e-05]], device='cuda:0')
Loss: 0.9564130902290344
Graident accumulation at epoch 1, step 1919, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1155e-06,  8.5667e-06, -5.1099e-05,  ..., -6.7488e-06,
         -1.5882e-04,  6.6473e-05],
        [-1.0015e-05, -6.7723e-06,  7.1286e-06,  ..., -8.8373e-06,
         -6.7414e-06, -7.1524e-06],
        [ 3.0742e-05,  4.8723e-05, -1.7343e-05,  ...,  1.7843e-05,
          4.2386e-05, -1.1277e-07],
        [-9.1474e-06,  4.0010e-06,  8.1490e-06,  ..., -8.6611e-06,
          3.8047e-07, -8.8551e-06],
        [-2.5788e-05, -1.9706e-05,  1.9126e-05,  ..., -2.2666e-05,
         -1.9217e-05, -1.6734e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5549e-08, 5.6847e-08, 6.7580e-08,  ..., 2.8273e-08, 1.5934e-07,
         4.0306e-08],
        [9.6230e-11, 6.2445e-11, 2.9731e-11,  ..., 6.4397e-11, 3.3148e-11,
         3.6527e-11],
        [4.7872e-09, 3.4667e-09, 1.9867e-09,  ..., 3.7534e-09, 2.0726e-09,
         1.3691e-09],
        [1.2796e-09, 1.4381e-09, 5.5858e-10,  ..., 1.1223e-09, 9.1202e-10,
         4.8682e-10],
        [4.1773e-10, 2.3799e-10, 1.0062e-10,  ..., 3.0794e-10, 1.0404e-10,
         1.2197e-10]], device='cuda:0')
optimizer state dict: 240.0
lr: [3.6752822198246384e-07, 3.6752822198246384e-07]
scheduler_last_epoch: 240


Running epoch 1, step 1920, batch 872
Sampled inputs[:2]: tensor([[    0,    14,   469,  ...,   367,  2564,   368],
        [    0,   266,  1890,  ...,   287, 38242,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0730e-05, -2.9231e-05,  1.2709e-05,  ...,  1.6940e-04,
          3.5859e-05,  4.7783e-05],
        [-1.3560e-06, -9.7603e-07,  9.1642e-07,  ..., -1.1846e-06,
         -9.0897e-07, -9.9838e-07],
        [-4.2915e-06, -3.2037e-06,  3.1739e-06,  ..., -3.6955e-06,
         -2.8908e-06, -3.0994e-06],
        [-3.0398e-06, -2.1756e-06,  2.1607e-06,  ..., -2.6524e-06,
         -2.0564e-06, -2.3395e-06],
        [-3.2783e-06, -2.5034e-06,  2.3544e-06,  ..., -2.8610e-06,
         -2.3246e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.975588858127594


Running epoch 1, step 1921, batch 873
Sampled inputs[:2]: tensor([[   0,  341, 2802,  ..., 1798,   12,  266],
        [   0, 4890, 1528,  ...,  847,  328, 1703]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2892e-04, -2.0388e-05, -8.3111e-05,  ...,  2.0889e-04,
         -1.1332e-04, -8.1248e-05],
        [-2.6673e-06, -1.9073e-06,  1.8589e-06,  ..., -2.3246e-06,
         -1.8179e-06, -1.9409e-06],
        [-8.5533e-06, -6.3181e-06,  6.4522e-06,  ..., -7.3463e-06,
         -5.7966e-06, -6.1244e-06],
        [-6.0052e-06, -4.2319e-06,  4.3958e-06,  ..., -5.2154e-06,
         -4.0978e-06, -4.5896e-06],
        [-6.3330e-06, -4.8280e-06,  4.6343e-06,  ..., -5.5432e-06,
         -4.5598e-06, -4.1723e-06]], device='cuda:0')
Loss: 0.95278400182724


Running epoch 1, step 1922, batch 874
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,    14,   298,  ...,   333,   199,   769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2946e-05, -1.4552e-04, -1.2199e-04,  ...,  3.1655e-04,
          2.8942e-05, -2.2768e-04],
        [-4.0010e-06, -2.7604e-06,  2.6636e-06,  ..., -3.6210e-06,
         -2.7940e-06, -3.0212e-06],
        [-1.2696e-05, -9.0897e-06,  9.2685e-06,  ..., -1.1161e-05,
         -8.6576e-06, -9.2685e-06],
        [-8.9705e-06, -6.0946e-06,  6.2585e-06,  ..., -8.0764e-06,
         -6.2436e-06, -7.0632e-06],
        [-9.6560e-06, -6.9886e-06,  6.7800e-06,  ..., -8.6278e-06,
         -6.9290e-06, -6.5118e-06]], device='cuda:0')
Loss: 0.9342945218086243


Running epoch 1, step 1923, batch 875
Sampled inputs[:2]: tensor([[    0,  3984, 13077,  ...,   287,   650,   413],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4208e-05, -1.3556e-04, -1.1017e-04,  ...,  4.3915e-04,
          1.7715e-04, -1.4305e-04],
        [-5.3197e-06, -3.7663e-06,  3.6545e-06,  ..., -4.7460e-06,
         -3.7551e-06, -3.9376e-06],
        [ 1.5267e-04,  1.1946e-04, -7.4164e-05,  ...,  1.5216e-04,
          1.0532e-04,  5.6458e-05],
        [-1.2040e-05, -8.3894e-06,  8.6576e-06,  ..., -1.0684e-05,
         -8.4788e-06, -9.2983e-06],
        [-1.2785e-05, -9.5367e-06,  9.2089e-06,  ..., -1.1340e-05,
         -9.3430e-06, -8.5384e-06]], device='cuda:0')
Loss: 0.9910750985145569


Running epoch 1, step 1924, batch 876
Sampled inputs[:2]: tensor([[    0,  1340,   800,  ...,   259, 13583,   422],
        [    0,    73,    14,  ...,   650,    13,  3658]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2635e-05, -1.5436e-04,  2.3004e-05,  ...,  5.6616e-04,
          3.4439e-04, -6.7895e-05],
        [-6.7055e-06, -4.7795e-06,  4.5858e-06,  ..., -5.9083e-06,
         -4.7237e-06, -4.9360e-06],
        [ 1.4832e-04,  1.1613e-04, -7.1020e-05,  ...,  1.4853e-04,
          1.0225e-04,  5.3359e-05],
        [-1.5140e-05, -1.0639e-05,  1.0818e-05,  ..., -1.3307e-05,
         -1.0669e-05, -1.1638e-05],
        [-1.6078e-05, -1.2130e-05,  1.1533e-05,  ..., -1.4141e-05,
         -1.1802e-05, -1.0729e-05]], device='cuda:0')
Loss: 0.9607619643211365


Running epoch 1, step 1925, batch 877
Sampled inputs[:2]: tensor([[   0,  768, 2351,  ..., 3768,  401, 2463],
        [   0,  287,  358,  ...,  328, 1704, 3227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2260e-04, -1.0112e-04,  3.3884e-05,  ...,  4.4399e-04,
          4.7107e-04, -1.3468e-04],
        [-7.9870e-06, -5.7630e-06,  5.5283e-06,  ..., -7.0482e-06,
         -5.7444e-06, -5.8077e-06],
        [ 1.4436e-04,  1.1300e-04, -6.7890e-05,  ...,  1.4506e-04,
          9.9154e-05,  5.0706e-05],
        [-1.8016e-05, -1.2785e-05,  1.3039e-05,  ..., -1.5840e-05,
         -1.2934e-05, -1.3679e-05],
        [-1.9208e-05, -1.4678e-05,  1.3933e-05,  ..., -1.6913e-05,
         -1.4380e-05, -1.2651e-05]], device='cuda:0')
Loss: 0.9652709364891052


Running epoch 1, step 1926, batch 878
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   352, 13107,  4302],
        [    0,   221,   380,  ...,   508,  1853,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1370e-04, -3.7035e-05,  9.4542e-05,  ...,  4.5857e-04,
          2.1353e-04, -1.4954e-04],
        [-9.4026e-06, -6.6645e-06,  6.5789e-06,  ..., -8.2254e-06,
         -6.6161e-06, -6.7391e-06],
        [ 1.4007e-04,  1.1011e-04, -6.4478e-05,  ...,  1.4149e-04,
          9.6472e-05,  4.7890e-05],
        [-2.1040e-05, -1.4678e-05,  1.5393e-05,  ..., -1.8358e-05,
         -1.4797e-05, -1.5765e-05],
        [-2.2411e-05, -1.6958e-05,  1.6406e-05,  ..., -1.9640e-05,
         -1.6540e-05, -1.4603e-05]], device='cuda:0')
Loss: 0.9618924856185913


Running epoch 1, step 1927, batch 879
Sampled inputs[:2]: tensor([[   0,   25,   26,  ...,    9,  287,  298],
        [   0, 1781,  659,  ...,   12, 1478,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3495e-04, -1.7249e-04,  1.2412e-04,  ...,  5.2780e-04,
          2.7362e-04, -1.4954e-04],
        [-1.0803e-05, -7.5810e-06,  7.4841e-06,  ..., -9.5367e-06,
         -7.6070e-06, -7.8194e-06],
        [ 1.3548e-04,  1.0704e-04, -6.1244e-05,  ...,  1.3732e-04,
          9.3283e-05,  4.4478e-05],
        [-2.4095e-05, -1.6645e-05,  1.7419e-05,  ..., -2.1219e-05,
         -1.6958e-05, -1.8179e-05],
        [-2.5898e-05, -1.9327e-05,  1.8805e-05,  ..., -2.2858e-05,
         -1.9088e-05, -1.7002e-05]], device='cuda:0')
Loss: 0.9580739140510559
Graident accumulation at epoch 1, step 1927, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1591e-05, -9.5389e-06, -3.3577e-05,  ...,  4.6706e-05,
         -1.1558e-04,  4.4872e-05],
        [-1.0094e-05, -6.8531e-06,  7.1641e-06,  ..., -8.9073e-06,
         -6.8279e-06, -7.2191e-06],
        [ 4.1215e-05,  5.4554e-05, -2.1733e-05,  ...,  2.9791e-05,
          4.7476e-05,  4.3463e-06],
        [-1.0642e-05,  1.9365e-06,  9.0761e-06,  ..., -9.9170e-06,
         -1.3533e-06, -9.7875e-06],
        [-2.5799e-05, -1.9668e-05,  1.9094e-05,  ..., -2.2685e-05,
         -1.9204e-05, -1.6761e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5549e-08, 5.6820e-08, 6.7528e-08,  ..., 2.8523e-08, 1.5926e-07,
         4.0288e-08],
        [9.6250e-11, 6.2440e-11, 2.9757e-11,  ..., 6.4423e-11, 3.3173e-11,
         3.6551e-11],
        [4.8007e-09, 3.4747e-09, 1.9885e-09,  ..., 3.7685e-09, 2.0792e-09,
         1.3697e-09],
        [1.2789e-09, 1.4370e-09, 5.5833e-10,  ..., 1.1216e-09, 9.1140e-10,
         4.8667e-10],
        [4.1799e-10, 2.3813e-10, 1.0087e-10,  ..., 3.0815e-10, 1.0430e-10,
         1.2213e-10]], device='cuda:0')
optimizer state dict: 241.0
lr: [3.3505955649678844e-07, 3.3505955649678844e-07]
scheduler_last_epoch: 241


Running epoch 1, step 1928, batch 880
Sampled inputs[:2]: tensor([[    0,   352,   927,  ...,  1521,  3513,   292],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1924e-04,  4.3403e-05, -1.0752e-04,  ...,  2.8327e-05,
          2.4545e-04,  9.6226e-05],
        [-1.4305e-06, -8.9034e-07,  7.7859e-07,  ..., -1.2368e-06,
         -1.0505e-06, -1.0058e-06],
        [-4.0829e-06, -2.7269e-06,  2.5928e-06,  ..., -3.4869e-06,
         -3.0398e-06, -2.7716e-06],
        [-3.1143e-06, -1.9073e-06,  1.7881e-06,  ..., -2.7269e-06,
         -2.3842e-06, -2.2799e-06],
        [-3.2783e-06, -2.2501e-06,  1.9670e-06,  ..., -2.8610e-06,
         -2.5630e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9023719429969788


Running epoch 1, step 1929, batch 881
Sampled inputs[:2]: tensor([[    0, 10026,   992,  ...,   273,  2831,  8716],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0968e-05, -6.2233e-06, -1.2801e-04,  ..., -1.5805e-05,
          9.9420e-05,  5.0889e-05],
        [-2.8536e-06, -1.7546e-06,  1.7919e-06,  ..., -2.4214e-06,
         -1.9632e-06, -2.0117e-06],
        [-8.4043e-06, -5.5134e-06,  5.9307e-06,  ..., -7.0482e-06,
         -5.8413e-06, -5.7667e-06],
        [-6.2138e-06, -3.7849e-06,  4.1127e-06,  ..., -5.3048e-06,
         -4.3809e-06, -4.5449e-06],
        [-6.5863e-06, -4.4256e-06,  4.4256e-06,  ..., -5.6326e-06,
         -4.8429e-06, -4.1723e-06]], device='cuda:0')
Loss: 0.9725995659828186


Running epoch 1, step 1930, batch 882
Sampled inputs[:2]: tensor([[   0,  508,  586,  ..., 6157, 3146, 7647],
        [   0, 1159,  278,  ...,    9,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7838e-05, -1.2453e-05, -3.7584e-04,  ...,  4.1381e-07,
         -1.0789e-04, -1.3003e-04],
        [-4.2319e-06, -2.7753e-06,  2.8275e-06,  ..., -3.6061e-06,
         -2.9393e-06, -2.9542e-06],
        [-1.2666e-05, -8.8215e-06,  9.3430e-06,  ..., -1.0714e-05,
         -8.8811e-06, -8.6725e-06],
        [-9.2238e-06, -5.9754e-06,  6.4671e-06,  ..., -7.8976e-06,
         -6.5267e-06, -6.7055e-06],
        [-9.7752e-06, -7.0035e-06,  6.8992e-06,  ..., -8.4341e-06,
         -7.2718e-06, -6.1989e-06]], device='cuda:0')
Loss: 0.9718686938285828


Running epoch 1, step 1931, batch 883
Sampled inputs[:2]: tensor([[    0,  3968,   446,  ...,    22,   722,   342],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5371e-05,  1.1081e-04, -3.8889e-04,  ..., -1.1705e-04,
         -1.8580e-04, -1.0961e-04],
        [-5.5060e-06, -3.7290e-06,  3.7216e-06,  ..., -4.7237e-06,
         -3.9227e-06, -3.8818e-06],
        [-1.6749e-05, -1.1995e-05,  1.2398e-05,  ..., -1.4275e-05,
         -1.1995e-05, -1.1623e-05],
        [-1.2189e-05, -8.1509e-06,  8.6129e-06,  ..., -1.0505e-05,
         -8.8364e-06, -8.9705e-06],
        [-1.2830e-05, -9.4622e-06,  9.1344e-06,  ..., -1.1146e-05,
         -9.7305e-06, -8.2552e-06]], device='cuda:0')
Loss: 0.9674429297447205


Running epoch 1, step 1932, batch 884
Sampled inputs[:2]: tensor([[   0,  834,   89,  ..., 4030,   12, 6528],
        [   0,  271,  266,  ..., 4298, 1231,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3617e-05,  7.7551e-05, -4.2459e-04,  ...,  8.6092e-05,
         -3.6880e-04, -2.2154e-04],
        [-6.7875e-06, -4.5635e-06,  4.6007e-06,  ..., -5.8562e-06,
         -4.8093e-06, -4.8801e-06],
        [-2.0802e-05, -1.4678e-05,  1.5453e-05,  ..., -1.7703e-05,
         -1.4707e-05, -1.4588e-05],
        [-1.5065e-05, -9.9540e-06,  1.0684e-05,  ..., -1.3039e-05,
         -1.0818e-05, -1.1280e-05],
        [-1.5914e-05, -1.1548e-05,  1.1384e-05,  ..., -1.3798e-05,
         -1.1906e-05, -1.0341e-05]], device='cuda:0')
Loss: 0.9455776214599609


Running epoch 1, step 1933, batch 885
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0, 24086,   266,  ..., 18814,    19,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9698e-04,  1.9114e-04, -6.4979e-04,  ...,  6.7469e-05,
         -7.9342e-05, -1.4225e-04],
        [-8.2031e-06, -5.5693e-06,  5.4985e-06,  ..., -7.0781e-06,
         -5.8822e-06, -5.8934e-06],
        [-2.5302e-05, -1.8001e-05,  1.8492e-05,  ..., -2.1577e-05,
         -1.8135e-05, -1.7822e-05],
        [-1.8254e-05, -1.2189e-05,  1.2770e-05,  ..., -1.5810e-05,
         -1.3262e-05, -1.3679e-05],
        [-1.9416e-05, -1.4275e-05,  1.3694e-05,  ..., -1.6898e-05,
         -1.4752e-05, -1.2726e-05]], device='cuda:0')
Loss: 0.984510064125061


Running epoch 1, step 1934, batch 886
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,    12,  1197,  ...,   516,  1136,  9774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5329e-04,  7.5864e-05, -6.1113e-04,  ...,  2.7042e-04,
          1.1287e-04, -1.1256e-04],
        [-9.4622e-06, -6.4708e-06,  6.4038e-06,  ..., -8.2254e-06,
         -6.7949e-06, -6.8061e-06],
        [-2.9266e-05, -2.0891e-05,  2.1577e-05,  ..., -2.5094e-05,
         -2.0921e-05, -2.0608e-05],
        [-2.1160e-05, -1.4201e-05,  1.4961e-05,  ..., -1.8433e-05,
         -1.5348e-05, -1.5900e-05],
        [-2.2411e-05, -1.6540e-05,  1.5959e-05,  ..., -1.9580e-05,
         -1.6972e-05, -1.4648e-05]], device='cuda:0')
Loss: 0.9691852927207947


Running epoch 1, step 1935, batch 887
Sampled inputs[:2]: tensor([[   0, 5982, 9385,  ...,   26,  469,  446],
        [   0, 3825, 1626,  ..., 5096, 3775,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7795e-04,  7.7633e-05, -5.6463e-04,  ...,  1.9808e-04,
          5.2558e-05,  4.2337e-05],
        [-1.0781e-05, -7.4245e-06,  7.4618e-06,  ..., -9.3430e-06,
         -7.7076e-06, -7.6815e-06],
        [-3.3557e-05, -2.4155e-05,  2.5243e-05,  ..., -2.8729e-05,
         -2.3916e-05, -2.3514e-05],
        [-2.4199e-05, -1.6361e-05,  1.7509e-05,  ..., -2.0981e-05,
         -1.7434e-05, -1.8045e-05],
        [-2.5362e-05, -1.8910e-05,  1.8403e-05,  ..., -2.2143e-05,
         -1.9193e-05, -1.6488e-05]], device='cuda:0')
Loss: 0.9637975096702576
Graident accumulation at epoch 1, step 1935, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6377e-06, -8.2172e-07, -8.6682e-05,  ...,  6.1843e-05,
         -9.8764e-05,  4.4619e-05],
        [-1.0163e-05, -6.9103e-06,  7.1939e-06,  ..., -8.9509e-06,
         -6.9159e-06, -7.2654e-06],
        [ 3.3738e-05,  4.6683e-05, -1.7036e-05,  ...,  2.3939e-05,
          4.0337e-05,  1.5603e-06],
        [-1.1998e-05,  1.0666e-07,  9.9193e-06,  ..., -1.1023e-05,
         -2.9614e-06, -1.0613e-05],
        [-2.5755e-05, -1.9592e-05,  1.9025e-05,  ..., -2.2631e-05,
         -1.9203e-05, -1.6734e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5525e-08, 5.6769e-08, 6.7779e-08,  ..., 2.8534e-08, 1.5910e-07,
         4.0249e-08],
        [9.6270e-11, 6.2433e-11, 2.9783e-11,  ..., 6.4446e-11, 3.3199e-11,
         3.6574e-11],
        [4.7971e-09, 3.4718e-09, 1.9871e-09,  ..., 3.7656e-09, 2.0777e-09,
         1.3689e-09],
        [1.2782e-09, 1.4358e-09, 5.5808e-10,  ..., 1.1209e-09, 9.1079e-10,
         4.8650e-10],
        [4.1821e-10, 2.3825e-10, 1.0111e-10,  ..., 3.0833e-10, 1.0457e-10,
         1.2228e-10]], device='cuda:0')
optimizer state dict: 242.0
lr: [3.040677782773405e-07, 3.040677782773405e-07]
scheduler_last_epoch: 242


Running epoch 1, step 1936, batch 888
Sampled inputs[:2]: tensor([[    0, 42306,   278,  ...,  1110,  3427,  4224],
        [    0,  2440,  1458,  ...,  7650,   328,  2297]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6209e-07,  1.7359e-05,  6.8943e-05,  ..., -7.8199e-06,
          6.5599e-07, -1.6165e-05],
        [-1.3262e-06, -1.1027e-06,  9.8348e-07,  ..., -1.1474e-06,
         -9.8348e-07, -8.6799e-07],
        [-4.2915e-06, -3.6210e-06,  3.3826e-06,  ..., -3.6806e-06,
         -3.1739e-06, -2.7865e-06],
        [-2.9355e-06, -2.4140e-06,  2.2799e-06,  ..., -2.5332e-06,
         -2.1905e-06, -2.0117e-06],
        [-3.2037e-06, -2.7865e-06,  2.4736e-06,  ..., -2.8014e-06,
         -2.4885e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9972456097602844


Running epoch 1, step 1937, batch 889
Sampled inputs[:2]: tensor([[    0,   259,  1513,  ...,   275, 19511,  2350],
        [    0,   300,  1064,  ...,  6953,   944,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5598e-05,  2.1421e-05, -7.1777e-06,  ..., -1.2102e-04,
          1.4173e-04, -5.9982e-05],
        [-2.6748e-06, -2.0377e-06,  2.0266e-06,  ..., -2.2501e-06,
         -1.7993e-06, -1.7360e-06],
        [-8.3745e-06, -6.6310e-06,  6.7949e-06,  ..., -7.0482e-06,
         -5.7071e-06, -5.4687e-06],
        [-5.9754e-06, -4.5002e-06,  4.7535e-06,  ..., -5.0217e-06,
         -4.0233e-06, -4.0829e-06],
        [-6.1393e-06, -5.0515e-06,  4.8429e-06,  ..., -5.2750e-06,
         -4.4554e-06, -3.6880e-06]], device='cuda:0')
Loss: 0.9504538774490356


Running epoch 1, step 1938, batch 890
Sampled inputs[:2]: tensor([[    0,   461,   654,  ...,  4145,  7600,  4142],
        [    0, 32878,   593,  ...,   437,  1329,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1065e-08, -6.8146e-05,  7.5154e-05,  ..., -1.6854e-04,
          5.6651e-05, -1.3988e-04],
        [-3.9935e-06, -3.0287e-06,  3.1739e-06,  ..., -3.3379e-06,
         -2.6934e-06, -2.5965e-06],
        [ 1.1945e-04,  1.6161e-04, -8.7238e-05,  ...,  9.7464e-05,
          1.4742e-04,  7.1363e-05],
        [-9.0003e-06, -6.7502e-06,  7.4953e-06,  ..., -7.5102e-06,
         -6.0499e-06, -6.1542e-06],
        [-9.1940e-06, -7.5549e-06,  7.5549e-06,  ..., -7.8678e-06,
         -6.7055e-06, -5.5656e-06]], device='cuda:0')
Loss: 1.0121392011642456


Running epoch 1, step 1939, batch 891
Sampled inputs[:2]: tensor([[    0,   380,  2114,  ...,   456, 28979,   472],
        [    0,   858,    13,  ...,  2253,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3076e-04,  3.0998e-05,  1.6594e-04,  ..., -2.8951e-04,
          2.6869e-04, -2.8671e-04],
        [-5.2452e-06, -3.9972e-06,  4.0270e-06,  ..., -4.5300e-06,
         -3.7141e-06, -3.5465e-06],
        [ 1.1563e-04,  1.5865e-04, -8.4362e-05,  ...,  9.3947e-05,
          1.4436e-04,  6.8606e-05],
        [-1.1742e-05, -8.8066e-06,  9.4771e-06,  ..., -1.0103e-05,
         -8.2850e-06, -8.3297e-06],
        [-1.2219e-05, -1.0014e-05,  9.7603e-06,  ..., -1.0714e-05,
         -9.2834e-06, -7.5772e-06]], device='cuda:0')
Loss: 0.958591639995575


Running epoch 1, step 1940, batch 892
Sampled inputs[:2]: tensor([[    0,   369,  4492,  ...,  9415,  4365,   352],
        [    0, 21801, 13084,  ...,  1738,  2946,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0011e-04,  1.0589e-04,  2.7539e-04,  ..., -3.1959e-04,
          2.1642e-04, -1.4138e-04],
        [-6.7651e-06, -4.9174e-06,  4.7311e-06,  ..., -5.8264e-06,
         -4.7646e-06, -4.6343e-06],
        [ 1.1104e-04,  1.5559e-04, -8.1918e-05,  ...,  9.0013e-05,
          1.4105e-04,  6.5313e-05],
        [-1.5050e-05, -1.0803e-05,  1.1072e-05,  ..., -1.2964e-05,
         -1.0654e-05, -1.0774e-05],
        [-1.5944e-05, -1.2532e-05,  1.1578e-05,  ..., -1.3977e-05,
         -1.2115e-05, -1.0096e-05]], device='cuda:0')
Loss: 0.9185734391212463


Running epoch 1, step 1941, batch 893
Sampled inputs[:2]: tensor([[    0, 17301,   300,  ...,   278,   546,  1576],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4741e-04,  3.4835e-05,  3.9886e-04,  ..., -1.6227e-04,
          2.4272e-04, -1.9517e-05],
        [-8.1211e-06, -5.8562e-06,  5.6773e-06,  ..., -7.0482e-06,
         -5.7332e-06, -5.6177e-06],
        [ 1.0672e-04,  1.5243e-04, -7.8670e-05,  ...,  8.6139e-05,
          1.3794e-04,  6.2169e-05],
        [-1.8030e-05, -1.2830e-05,  1.3232e-05,  ..., -1.5676e-05,
         -1.2815e-05, -1.3068e-05],
        [-1.9222e-05, -1.4991e-05,  1.3947e-05,  ..., -1.6972e-05,
         -1.4588e-05, -1.2316e-05]], device='cuda:0')
Loss: 0.9707790017127991


Running epoch 1, step 1942, batch 894
Sampled inputs[:2]: tensor([[    0,   822,  5085,  ...,   293,  1608,   391],
        [    0,  2588, 25531,  ...,  1977,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9503e-05,  1.1998e-04,  4.2258e-04,  ..., -1.6449e-04,
          1.8900e-04,  3.9498e-05],
        [-9.4995e-06, -6.6869e-06,  6.6310e-06,  ..., -8.2403e-06,
         -6.6422e-06, -6.6459e-06],
        [ 1.0249e-04,  1.4972e-04, -7.5481e-05,  ...,  8.2518e-05,
          1.3514e-04,  5.9055e-05],
        [-2.1130e-05, -1.4655e-05,  1.5482e-05,  ..., -1.8328e-05,
         -1.4842e-05, -1.5438e-05],
        [-2.2471e-05, -1.7136e-05,  1.6287e-05,  ..., -1.9819e-05,
         -1.6898e-05, -1.4551e-05]], device='cuda:0')
Loss: 0.9639688730239868


Running epoch 1, step 1943, batch 895
Sampled inputs[:2]: tensor([[    0, 13642, 14635,  ...,   367,  1040,  8580],
        [    0,   607,   259,  ...,   995,    13,  6507]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1389e-04,  1.9598e-04,  5.7483e-04,  ..., -2.1444e-04,
          2.8318e-04,  1.4623e-04],
        [-1.0848e-05, -7.7747e-06,  7.5810e-06,  ..., -9.4175e-06,
         -7.6704e-06, -7.5586e-06],
        [ 9.8290e-05,  1.4616e-04, -7.2337e-05,  ...,  7.8852e-05,
          1.3189e-04,  5.6194e-05],
        [-2.4170e-05, -1.7099e-05,  1.7703e-05,  ..., -2.0981e-05,
         -1.7181e-05, -1.7613e-05],
        [-2.5675e-05, -1.9938e-05,  1.8626e-05,  ..., -2.2680e-05,
         -1.9521e-05, -1.6592e-05]], device='cuda:0')
Loss: 0.9726455807685852
Graident accumulation at epoch 1, step 1943, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2863e-05,  1.8859e-05, -2.0530e-05,  ...,  3.4215e-05,
         -6.0570e-05,  5.4779e-05],
        [-1.0231e-05, -6.9967e-06,  7.2326e-06,  ..., -8.9975e-06,
         -6.9913e-06, -7.2947e-06],
        [ 4.0193e-05,  5.6631e-05, -2.2566e-05,  ...,  2.9430e-05,
          4.9492e-05,  7.0236e-06],
        [-1.3215e-05, -1.6139e-06,  1.0698e-05,  ..., -1.2019e-05,
         -4.3834e-06, -1.1313e-05],
        [-2.5747e-05, -1.9627e-05,  1.8985e-05,  ..., -2.2636e-05,
         -1.9235e-05, -1.6720e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5482e-08, 5.6751e-08, 6.8042e-08,  ..., 2.8551e-08, 1.5902e-07,
         4.0231e-08],
        [9.6292e-11, 6.2431e-11, 2.9811e-11,  ..., 6.4470e-11, 3.3225e-11,
         3.6594e-11],
        [4.8019e-09, 3.4897e-09, 1.9904e-09,  ..., 3.7680e-09, 2.0930e-09,
         1.3707e-09],
        [1.2775e-09, 1.4347e-09, 5.5783e-10,  ..., 1.1203e-09, 9.1018e-10,
         4.8633e-10],
        [4.1845e-10, 2.3841e-10, 1.0136e-10,  ..., 3.0854e-10, 1.0484e-10,
         1.2244e-10]], device='cuda:0')
optimizer state dict: 243.0
lr: [2.745576231383573e-07, 2.745576231383573e-07]
scheduler_last_epoch: 243


Running epoch 1, step 1944, batch 896
Sampled inputs[:2]: tensor([[   0,  287,  266,  ...,  333,  199, 3217],
        [   0, 4538,  271,  ..., 1603,  591,  688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2443e-05, -7.0113e-05, -1.8644e-04,  ...,  3.6106e-05,
          1.1609e-05,  1.2688e-04],
        [-1.4007e-06, -1.0282e-06,  1.0207e-06,  ..., -1.1474e-06,
         -9.2387e-07, -9.1642e-07],
        [-4.4703e-06, -3.4124e-06,  3.4571e-06,  ..., -3.6508e-06,
         -2.9355e-06, -2.9206e-06],
        [-3.1143e-06, -2.2799e-06,  2.3544e-06,  ..., -2.5630e-06,
         -2.0862e-06, -2.1458e-06],
        [-3.2187e-06, -2.5481e-06,  2.4438e-06,  ..., -2.6822e-06,
         -2.2501e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9439216256141663


Running epoch 1, step 1945, batch 897
Sampled inputs[:2]: tensor([[    0, 12923,  2489,  ...,   474,  3301,    54],
        [    0,  2834,   266,  ..., 39474,    12, 15441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7144e-04, -1.8208e-04, -3.6269e-04,  ...,  6.6145e-05,
         -8.1972e-05,  3.7830e-06],
        [-2.7642e-06, -1.9409e-06,  2.0862e-06,  ..., -2.2501e-06,
         -1.7397e-06, -1.8328e-06],
        [-8.7917e-06, -6.3777e-06,  7.0482e-06,  ..., -7.1228e-06,
         -5.5134e-06, -5.7817e-06],
        [-6.0648e-06, -4.2021e-06,  4.7535e-06,  ..., -4.9472e-06,
         -3.8370e-06, -4.2021e-06],
        [-6.2883e-06, -4.7535e-06,  4.9323e-06,  ..., -5.2154e-06,
         -4.2617e-06, -3.8072e-06]], device='cuda:0')
Loss: 0.9406053423881531


Running epoch 1, step 1946, batch 898
Sampled inputs[:2]: tensor([[    0,   292,   494,  ...,   259, 14134, 11544],
        [    0,  1016,   271,  ...,   461,   616,   993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3622e-04, -1.7098e-04, -3.8141e-04,  ..., -3.5509e-05,
          6.6541e-05, -1.0322e-04],
        [-4.1723e-06, -3.0212e-06,  3.0026e-06,  ..., -3.5167e-06,
         -2.8349e-06, -2.7530e-06],
        [-1.2994e-05, -9.8199e-06,  1.0058e-05,  ..., -1.0908e-05,
         -8.8662e-06, -8.5235e-06],
        [-9.1344e-06, -6.5714e-06,  6.8545e-06,  ..., -7.7188e-06,
         -6.2659e-06, -6.2883e-06],
        [-9.6709e-06, -7.5698e-06,  7.3016e-06,  ..., -8.2850e-06,
         -7.0632e-06, -5.8636e-06]], device='cuda:0')
Loss: 0.9910989999771118


Running epoch 1, step 1947, batch 899
Sampled inputs[:2]: tensor([[    0,   266,  2374,  ...,  1551,   518,   638],
        [    0,   266,   283,  ...,   271, 48829,   580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0068e-04, -2.1190e-04, -4.8995e-04,  ..., -2.7191e-04,
          2.6954e-04, -3.1154e-04],
        [-5.4613e-06, -4.0494e-06,  3.9861e-06,  ..., -4.6343e-06,
         -3.7886e-06, -3.6731e-06],
        [-1.7047e-05, -1.3128e-05,  1.3366e-05,  ..., -1.4380e-05,
         -1.1891e-05, -1.1355e-05],
        [-1.2055e-05, -8.8662e-06,  9.1791e-06,  ..., -1.0252e-05,
         -8.4564e-06, -8.4490e-06],
        [-1.2681e-05, -1.0088e-05,  9.6858e-06,  ..., -1.0908e-05,
         -9.4324e-06, -7.8008e-06]], device='cuda:0')
Loss: 0.9957706332206726


Running epoch 1, step 1948, batch 900
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,    12,  4957,  ...,   944,   278,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1511e-04, -1.9883e-04, -6.0415e-04,  ..., -3.3043e-04,
          3.2637e-05, -3.0644e-04],
        [-6.7949e-06, -4.9770e-06,  4.9211e-06,  ..., -5.7817e-06,
         -4.7572e-06, -4.5784e-06],
        [-2.1249e-05, -1.6168e-05,  1.6540e-05,  ..., -1.7956e-05,
         -1.4901e-05, -1.4186e-05],
        [-1.5080e-05, -1.0908e-05,  1.1370e-05,  ..., -1.2845e-05,
         -1.0647e-05, -1.0595e-05],
        [-1.5870e-05, -1.2472e-05,  1.2010e-05,  ..., -1.3679e-05,
         -1.1846e-05, -9.7677e-06]], device='cuda:0')
Loss: 0.9609285593032837


Running epoch 1, step 1949, batch 901
Sampled inputs[:2]: tensor([[   0,   17,   14,  ...,  650, 1711,  897],
        [   0,  278, 1620,  ...,  360, 1758,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2191e-04, -1.0683e-04, -5.4997e-04,  ..., -2.2822e-04,
         -9.4366e-05, -2.2963e-04],
        [-8.2701e-06, -5.8860e-06,  5.8115e-06,  ..., -7.0333e-06,
         -5.7258e-06, -5.6438e-06],
        [-2.5719e-05, -1.9088e-05,  1.9506e-05,  ..., -2.1696e-05,
         -1.7852e-05, -1.7345e-05],
        [-1.8239e-05, -1.2815e-05,  1.3337e-05,  ..., -1.5542e-05,
         -1.2763e-05, -1.2949e-05],
        [-1.9401e-05, -1.4842e-05,  1.4260e-05,  ..., -1.6704e-05,
         -1.4320e-05, -1.2092e-05]], device='cuda:0')
Loss: 0.9381893873214722


Running epoch 1, step 1950, batch 902
Sampled inputs[:2]: tensor([[    0,    14, 12285,  ...,   616,   515,   352],
        [    0,   759,  1128,  ...,   221,   474,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3875e-04, -1.4801e-04, -6.0102e-04,  ..., -2.4936e-04,
         -1.7013e-04, -2.2963e-04],
        [-9.6709e-06, -6.8396e-06,  6.7577e-06,  ..., -8.2105e-06,
         -6.6869e-06, -6.6496e-06],
        [-3.0220e-05, -2.2337e-05,  2.2814e-05,  ..., -2.5466e-05,
         -2.0951e-05, -2.0623e-05],
        [-2.1398e-05, -1.4961e-05,  1.5572e-05,  ..., -1.8209e-05,
         -1.4953e-05, -1.5333e-05],
        [-2.2724e-05, -1.7315e-05,  1.6600e-05,  ..., -1.9565e-05,
         -1.6779e-05, -1.4342e-05]], device='cuda:0')
Loss: 0.9313474297523499


Running epoch 1, step 1951, batch 903
Sampled inputs[:2]: tensor([[    0,  3761,    12,  ...,    14,    22,   287],
        [    0,  1278,    69,  ...,    15,  7377, 20524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0897e-04, -2.0695e-04, -6.4059e-04,  ..., -5.3596e-05,
         -2.4304e-04, -1.6256e-04],
        [-1.0997e-05, -7.7151e-06,  7.7188e-06,  ..., -9.3579e-06,
         -7.5437e-06, -7.5810e-06],
        [-3.4362e-05, -2.5228e-05,  2.6062e-05,  ..., -2.9013e-05,
         -2.3633e-05, -2.3469e-05],
        [-2.4393e-05, -1.6913e-05,  1.7822e-05,  ..., -2.0802e-05,
         -1.6905e-05, -1.7524e-05],
        [-2.5794e-05, -1.9506e-05,  1.8939e-05,  ..., -2.2233e-05,
         -1.8880e-05, -1.6280e-05]], device='cuda:0')
Loss: 0.9495418071746826
Graident accumulation at epoch 1, step 1951, batch 903
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.2474e-05, -3.7217e-06, -8.2536e-05,  ...,  2.5434e-05,
         -7.8817e-05,  3.3046e-05],
        [-1.0308e-05, -7.0685e-06,  7.2812e-06,  ..., -9.0336e-06,
         -7.0466e-06, -7.3233e-06],
        [ 3.2738e-05,  4.8445e-05, -1.7703e-05,  ...,  2.3586e-05,
          4.2179e-05,  3.9743e-06],
        [-1.4333e-05, -3.1438e-06,  1.1410e-05,  ..., -1.2897e-05,
         -5.6356e-06, -1.1934e-05],
        [-2.5752e-05, -1.9615e-05,  1.8981e-05,  ..., -2.2595e-05,
         -1.9199e-05, -1.6676e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5594e-08, 5.6737e-08, 6.8384e-08,  ..., 2.8526e-08, 1.5892e-07,
         4.0217e-08],
        [9.6316e-11, 6.2428e-11, 2.9840e-11,  ..., 6.4493e-11, 3.3248e-11,
         3.6615e-11],
        [4.7983e-09, 3.4869e-09, 1.9890e-09,  ..., 3.7651e-09, 2.0915e-09,
         1.3699e-09],
        [1.2768e-09, 1.4335e-09, 5.5759e-10,  ..., 1.1196e-09, 9.0955e-10,
         4.8615e-10],
        [4.1870e-10, 2.3855e-10, 1.0161e-10,  ..., 3.0872e-10, 1.0510e-10,
         1.2258e-10]], device='cuda:0')
optimizer state dict: 244.0
lr: [2.465336004891461e-07, 2.465336004891461e-07]
scheduler_last_epoch: 244


Running epoch 1, step 1952, batch 904
Sampled inputs[:2]: tensor([[    0,   292, 49760,  ...,   275,  4474,    13],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 0.0000e+00, -5.5977e-05, -1.1041e-04,  ..., -6.0881e-05,
          2.9713e-05,  2.4663e-04],
        [-1.3411e-06, -9.9093e-07,  9.2387e-07,  ..., -1.1697e-06,
         -9.8348e-07, -9.5367e-07],
        [-4.2021e-06, -3.2187e-06,  3.1292e-06,  ..., -3.6359e-06,
         -3.0696e-06, -2.9206e-06],
        [-2.9802e-06, -2.1607e-06,  2.1309e-06,  ..., -2.6077e-06,
         -2.2054e-06, -2.2054e-06],
        [-3.1739e-06, -2.5332e-06,  2.2948e-06,  ..., -2.8014e-06,
         -2.4736e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.963428795337677


Running epoch 1, step 1953, batch 905
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 3795,  908,  587],
        [   0, 4494,   12,  ...,  341, 1619,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4358e-04, -5.7495e-05, -2.3784e-04,  ...,  2.7995e-04,
          1.4025e-04,  2.4864e-04],
        [-2.6673e-06, -1.8403e-06,  1.5534e-06,  ..., -2.4438e-06,
         -1.9670e-06, -2.0191e-06],
        [-8.3148e-06, -5.9456e-06,  5.3495e-06,  ..., -7.4506e-06,
         -6.0797e-06, -6.0797e-06],
        [-5.9009e-06, -3.9935e-06,  3.5539e-06,  ..., -5.4538e-06,
         -4.4554e-06, -4.6492e-06],
        [-6.6012e-06, -4.8131e-06,  4.1053e-06,  ..., -5.9903e-06,
         -5.0366e-06, -4.4703e-06]], device='cuda:0')
Loss: 0.8879038691520691


Running epoch 1, step 1954, batch 906
Sampled inputs[:2]: tensor([[    0,   278,   565,  ...,  1125,  5222,   287],
        [    0,   631,   516,  ..., 13374,   898,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1521e-04, -6.0844e-05, -2.9223e-04,  ...,  2.6733e-04,
          1.3088e-04,  2.5023e-04],
        [-4.1276e-06, -2.8536e-06,  2.5891e-06,  ..., -3.6284e-06,
         -2.8983e-06, -3.0249e-06],
        [ 5.0558e-05,  3.0140e-05, -4.6712e-05,  ...,  1.8837e-05,
          7.1538e-05,  3.7551e-05],
        [-9.0301e-06, -6.1840e-06,  5.8785e-06,  ..., -8.0019e-06,
         -6.4820e-06, -6.8843e-06],
        [-9.9689e-06, -7.3761e-06,  6.5789e-06,  ..., -8.7917e-06,
         -7.3761e-06, -6.6459e-06]], device='cuda:0')
Loss: 0.9666109681129456


Running epoch 1, step 1955, batch 907
Sampled inputs[:2]: tensor([[    0,   607, 27288,  ...,   445,  4712,   278],
        [    0,   397,  1267,  ...,  1276,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3709e-04,  2.2430e-05, -2.1644e-04,  ...,  2.6125e-04,
          3.7787e-04,  2.7907e-04],
        [-5.4762e-06, -3.8594e-06,  3.4422e-06,  ..., -4.8801e-06,
         -3.9712e-06, -4.0606e-06],
        [ 4.6446e-05,  2.6966e-05, -4.3880e-05,  ...,  1.5097e-05,
          6.8349e-05,  3.4466e-05],
        [-1.1936e-05, -8.3297e-06,  7.7635e-06,  ..., -1.0729e-05,
         -8.8364e-06, -9.2387e-06],
        [-1.3277e-05, -9.9987e-06,  8.7842e-06,  ..., -1.1846e-05,
         -1.0058e-05, -8.9556e-06]], device='cuda:0')
Loss: 0.9580566883087158


Running epoch 1, step 1956, batch 908
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  292,  334,  674],
        [   0, 1716,  271,  ...,  292,   78, 1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7904e-04, -6.9034e-05, -3.3733e-04,  ...,  2.6393e-04,
          1.9452e-04,  2.7907e-04],
        [-6.7800e-06, -4.8019e-06,  4.4182e-06,  ..., -6.0648e-06,
         -4.8988e-06, -4.9584e-06],
        [ 4.2214e-05,  2.3777e-05, -4.0513e-05,  ...,  1.1297e-05,
          6.5369e-05,  3.1576e-05],
        [-1.4871e-05, -1.0416e-05,  1.0043e-05,  ..., -1.3381e-05,
         -1.0908e-05, -1.1370e-05],
        [-1.6376e-05, -1.2428e-05,  1.1183e-05,  ..., -1.4678e-05,
         -1.2383e-05, -1.0923e-05]], device='cuda:0')
Loss: 0.9779815077781677


Running epoch 1, step 1957, batch 909
Sampled inputs[:2]: tensor([[   0,  598,  696,  ..., 4048, 1795,   14],
        [   0, 2853,  590,  ..., 1351, 2927,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6260e-05, -8.2828e-05, -2.9237e-04,  ...,  2.6393e-04,
         -4.8762e-05,  2.5540e-04],
        [-8.1360e-06, -5.7928e-06,  5.5060e-06,  ..., -7.1973e-06,
         -5.7928e-06, -5.8226e-06],
        [ 3.8071e-05,  2.0514e-05, -3.6937e-05,  ...,  7.7653e-06,
          6.2523e-05,  2.8878e-05],
        [-1.7956e-05, -1.2696e-05,  1.2681e-05,  ..., -1.5989e-05,
         -1.2994e-05, -1.3471e-05],
        [-1.9342e-05, -1.4856e-05,  1.3687e-05,  ..., -1.7270e-05,
         -1.4573e-05, -1.2718e-05]], device='cuda:0')
Loss: 0.9533013701438904


Running epoch 1, step 1958, batch 910
Sampled inputs[:2]: tensor([[   0, 1802, 4165,  ...,  298,  445,   28],
        [   0,  265, 1781,  ...,  334,  344,  984]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8515e-04, -8.5261e-05, -3.4076e-04,  ...,  2.6965e-04,
          2.1486e-05,  2.3000e-04],
        [-9.6858e-06, -6.6906e-06,  6.4485e-06,  ..., -8.5160e-06,
         -6.7391e-06, -6.9849e-06],
        [ 3.3422e-05,  1.7653e-05, -3.3867e-05,  ...,  3.8314e-06,
          5.9632e-05,  2.5451e-05],
        [-2.1130e-05, -1.4551e-05,  1.4678e-05,  ..., -1.8716e-05,
         -1.4976e-05, -1.5914e-05],
        [-2.3216e-05, -1.7256e-05,  1.6160e-05,  ..., -2.0593e-05,
         -1.7107e-05, -1.5415e-05]], device='cuda:0')
Loss: 0.987800121307373


Running epoch 1, step 1959, batch 911
Sampled inputs[:2]: tensor([[   0, 1730, 2068,  ...,  445, 2704,  445],
        [   0,  281,   82,  ..., 2485,  417,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3887e-04, -5.8102e-06, -2.8415e-04,  ...,  4.0488e-04,
          1.2082e-04,  4.7164e-04],
        [-1.0982e-05, -7.5549e-06,  7.2308e-06,  ..., -9.7603e-06,
         -7.7300e-06, -8.0131e-06],
        [ 2.9309e-05,  1.4777e-05, -3.1155e-05,  ..., -1.3135e-08,
          5.6548e-05,  2.2292e-05],
        [-2.4065e-05, -1.6458e-05,  1.6466e-05,  ..., -2.1577e-05,
         -1.7270e-05, -1.8373e-05],
        [-2.6420e-05, -1.9550e-05,  1.8232e-05,  ..., -2.3618e-05,
         -1.9595e-05, -1.7725e-05]], device='cuda:0')
Loss: 0.9400900602340698
Graident accumulation at epoch 1, step 1959, batch 911
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3339e-05, -3.9306e-06, -1.0270e-04,  ...,  6.3379e-05,
         -5.8854e-05,  7.6905e-05],
        [-1.0375e-05, -7.1172e-06,  7.2762e-06,  ..., -9.1062e-06,
         -7.1149e-06, -7.3923e-06],
        [ 3.2395e-05,  4.5078e-05, -1.9048e-05,  ...,  2.1226e-05,
          4.3616e-05,  5.8061e-06],
        [-1.5306e-05, -4.4753e-06,  1.1916e-05,  ..., -1.3765e-05,
         -6.7991e-06, -1.2578e-05],
        [-2.5818e-05, -1.9608e-05,  1.8906e-05,  ..., -2.2698e-05,
         -1.9239e-05, -1.6781e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5653e-08, 5.6680e-08, 6.8396e-08,  ..., 2.8661e-08, 1.5878e-07,
         4.0399e-08],
        [9.6341e-11, 6.2422e-11, 2.9863e-11,  ..., 6.4524e-11, 3.3275e-11,
         3.6643e-11],
        [4.7944e-09, 3.4836e-09, 1.9880e-09,  ..., 3.7613e-09, 2.0926e-09,
         1.3690e-09],
        [1.2761e-09, 1.4324e-09, 5.5731e-10,  ..., 1.1189e-09, 9.0894e-10,
         4.8600e-10],
        [4.1898e-10, 2.3869e-10, 1.0185e-10,  ..., 3.0897e-10, 1.0537e-10,
         1.2277e-10]], device='cuda:0')
optimizer state dict: 245.0
lr: [2.1999999264499027e-07, 2.1999999264499027e-07]
scheduler_last_epoch: 245


Running epoch 1, step 1960, batch 912
Sampled inputs[:2]: tensor([[    0,   374,   298,  ..., 11183,    12,   654],
        [    0,   694,   266,  ...,  3007,   300,  5726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4465e-05,  2.1565e-05, -8.6908e-05,  ..., -1.0103e-04,
         -8.2799e-05, -8.2928e-05],
        [-1.4603e-06, -9.4250e-07,  9.3505e-07,  ..., -1.2666e-06,
         -9.6112e-07, -1.0505e-06],
        [-4.3213e-06, -2.9504e-06,  3.0547e-06,  ..., -3.6955e-06,
         -2.9057e-06, -3.0249e-06],
        [-3.0547e-06, -1.9670e-06,  2.0862e-06,  ..., -2.6524e-06,
         -2.0415e-06, -2.2203e-06],
        [-3.5614e-06, -2.4587e-06,  2.3693e-06,  ..., -3.1292e-06,
         -2.5481e-06, -2.3842e-06]], device='cuda:0')
Loss: 0.9407151341438293


Running epoch 1, step 1961, batch 913
Sampled inputs[:2]: tensor([[    0, 23749, 27341,  ..., 34110,   342,  9672],
        [    0,   259,  1329,  ...,   266,   706,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5425e-04, -2.0904e-05, -2.2162e-04,  ..., -1.5078e-04,
         -2.2152e-04, -7.5040e-05],
        [-2.8163e-06, -2.0154e-06,  1.9856e-06,  ..., -2.4438e-06,
         -1.9222e-06, -1.9558e-06],
        [-8.6427e-06, -6.5118e-06,  6.6012e-06,  ..., -7.4804e-06,
         -6.0201e-06, -5.9605e-06],
        [-6.1691e-06, -4.4107e-06,  4.6045e-06,  ..., -5.3495e-06,
         -4.2617e-06, -4.4256e-06],
        [-6.7055e-06, -5.1856e-06,  4.8876e-06,  ..., -5.9605e-06,
         -4.9919e-06, -4.3660e-06]], device='cuda:0')
Loss: 0.998749315738678


Running epoch 1, step 1962, batch 914
Sampled inputs[:2]: tensor([[    0,   292, 23950,  ...,  9305,   287,  4401],
        [    0,  8878,  6716,  ...,  8878,   328, 31139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9223e-04, -6.9026e-05, -1.5286e-04,  ..., -1.5486e-04,
         -3.7509e-06,  1.2126e-04],
        [-4.1649e-06, -3.0883e-06,  2.9691e-06,  ..., -3.6061e-06,
         -2.9653e-06, -2.8834e-06],
        [-1.2934e-05, -1.0118e-05,  9.9242e-06,  ..., -1.1206e-05,
         -9.3877e-06, -8.9556e-06],
        [-9.2983e-06, -6.8992e-06,  6.9886e-06,  ..., -8.0764e-06,
         -6.7204e-06, -6.7055e-06],
        [-9.9093e-06, -7.9572e-06,  7.3165e-06,  ..., -8.7917e-06,
         -7.6294e-06, -6.4522e-06]], device='cuda:0')
Loss: 0.9904703497886658


Running epoch 1, step 1963, batch 915
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2750e-04, -1.1157e-04, -4.7279e-04,  ...,  4.8769e-05,
         -4.0234e-04, -1.8374e-04],
        [-5.4464e-06, -3.9898e-06,  3.9302e-06,  ..., -4.7460e-06,
         -3.9078e-06, -3.7998e-06],
        [-5.5570e-05,  4.8196e-05, -4.4723e-05,  ...,  7.8324e-05,
          6.2337e-05, -4.0935e-07],
        [-1.2189e-05, -8.8662e-06,  9.2685e-06,  ..., -1.0639e-05,
         -8.8513e-06, -8.8662e-06],
        [-1.2934e-05, -1.0177e-05,  9.7156e-06,  ..., -1.1429e-05,
         -9.9093e-06, -8.3074e-06]], device='cuda:0')
Loss: 0.9655185341835022


Running epoch 1, step 1964, batch 916
Sampled inputs[:2]: tensor([[    0, 23230,    12,  ...,  5092,   741,   266],
        [    0,  6847,   437,  ...,    17,    14,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7388e-06, -8.8863e-05, -3.6113e-04,  ...,  7.3272e-05,
         -3.6818e-04, -1.3935e-04],
        [-6.7726e-06, -5.0254e-06,  4.8205e-06,  ..., -5.9456e-06,
         -4.9733e-06, -4.7907e-06],
        [-5.9713e-05,  4.4769e-05, -4.1683e-05,  ...,  7.4598e-05,
          5.8984e-05, -3.5088e-06],
        [-1.5110e-05, -1.1146e-05,  1.1310e-05,  ..., -1.3292e-05,
         -1.1235e-05, -1.1161e-05],
        [-1.6153e-05, -1.2904e-05,  1.1981e-05,  ..., -1.4380e-05,
         -1.2666e-05, -1.0528e-05]], device='cuda:0')
Loss: 0.978323757648468


Running epoch 1, step 1965, batch 917
Sampled inputs[:2]: tensor([[    0,    12,   496,  ..., 11354,  4856,  1109],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6577e-05, -1.2969e-04, -3.7475e-04,  ..., -3.8005e-05,
         -4.4372e-04, -2.7371e-04],
        [-8.1286e-06, -6.0759e-06,  5.9083e-06,  ..., -7.1153e-06,
         -5.8897e-06, -5.7071e-06],
        [ 1.4515e-04,  2.7569e-04, -1.6903e-04,  ...,  3.4166e-04,
          2.0891e-04,  1.5578e-04],
        [-1.8150e-05, -1.3471e-05,  1.3858e-05,  ..., -1.5900e-05,
         -1.3277e-05, -1.3322e-05],
        [-1.9312e-05, -1.5542e-05,  1.4588e-05,  ..., -1.7151e-05,
         -1.4961e-05, -1.2510e-05]], device='cuda:0')
Loss: 0.9632152915000916


Running epoch 1, step 1966, batch 918
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0,  3227,   278,  ...,  2950,    14, 15544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6639e-05, -9.5000e-05, -4.9424e-04,  ..., -4.1310e-05,
         -5.5924e-04, -4.5854e-04],
        [-9.4697e-06, -6.9737e-06,  6.9737e-06,  ..., -8.2627e-06,
         -6.8210e-06, -6.6273e-06],
        [ 1.4091e-04,  2.7273e-04, -1.6548e-04,  ...,  3.3810e-04,
          2.0600e-04,  1.5292e-04],
        [-2.1145e-05, -1.5438e-05,  1.6332e-05,  ..., -1.8448e-05,
         -1.5348e-05, -1.5467e-05],
        [-2.2396e-05, -1.7807e-05,  1.7092e-05,  ..., -1.9819e-05,
         -1.7226e-05, -1.4447e-05]], device='cuda:0')
Loss: 0.9545173645019531


Running epoch 1, step 1967, batch 919
Sampled inputs[:2]: tensor([[    0, 34525,  2008,  ...,  1194,   300, 11120],
        [    0,    12,   298,  ...,   292,    36,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9366e-04, -2.3302e-05, -5.2438e-04,  ...,  9.1955e-05,
         -7.4963e-04, -3.9148e-04],
        [-1.0833e-05, -7.8529e-06,  7.9572e-06,  ..., -9.3952e-06,
         -7.6964e-06, -7.5884e-06],
        [ 1.3677e-04,  2.6991e-04, -1.6225e-04,  ...,  3.3470e-04,
          2.0334e-04,  1.5007e-04],
        [-2.4140e-05, -1.7330e-05,  1.8612e-05,  ..., -2.0951e-05,
         -1.7300e-05, -1.7688e-05],
        [-2.5615e-05, -2.0087e-05,  1.9535e-05,  ..., -2.2516e-05,
         -1.9431e-05, -1.6488e-05]], device='cuda:0')
Loss: 0.9190941452980042
Graident accumulation at epoch 1, step 1967, batch 919
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1371e-05, -5.8677e-06, -1.4487e-04,  ...,  6.6236e-05,
         -1.2793e-04,  3.0066e-05],
        [-1.0421e-05, -7.1907e-06,  7.3443e-06,  ..., -9.1351e-06,
         -7.1731e-06, -7.4119e-06],
        [ 4.2833e-05,  6.7561e-05, -3.3368e-05,  ...,  5.2574e-05,
          5.9588e-05,  2.0232e-05],
        [-1.6190e-05, -5.7607e-06,  1.2585e-05,  ..., -1.4484e-05,
         -7.8492e-06, -1.3089e-05],
        [-2.5798e-05, -1.9656e-05,  1.8969e-05,  ..., -2.2679e-05,
         -1.9258e-05, -1.6751e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5635e-08, 5.6624e-08, 6.8603e-08,  ..., 2.8641e-08, 1.5918e-07,
         4.0512e-08],
        [9.6362e-11, 6.2422e-11, 2.9896e-11,  ..., 6.4548e-11, 3.3301e-11,
         3.6664e-11],
        [4.8083e-09, 3.5530e-09, 2.0124e-09,  ..., 3.8696e-09, 2.1318e-09,
         1.3901e-09],
        [1.2754e-09, 1.4312e-09, 5.5709e-10,  ..., 1.1182e-09, 9.0833e-10,
         4.8583e-10],
        [4.1922e-10, 2.3886e-10, 1.0213e-10,  ..., 3.0917e-10, 1.0565e-10,
         1.2292e-10]], device='cuda:0')
optimizer state dict: 246.0
lr: [1.9496085417278542e-07, 1.9496085417278542e-07]
scheduler_last_epoch: 246


Running epoch 1, step 1968, batch 920
Sampled inputs[:2]: tensor([[   0,  756,  943,  ..., 4016,   12,  627],
        [   0, 1806,  319,  ..., 3427,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9846e-04, -2.7205e-06, -2.9897e-05,  ...,  1.5294e-04,
          8.9374e-05, -8.0526e-05],
        [-1.3188e-06, -8.4192e-07,  8.8662e-07,  ..., -1.1101e-06,
         -8.8662e-07, -1.0133e-06],
        [-4.1127e-06, -2.7120e-06,  3.0845e-06,  ..., -3.3677e-06,
         -2.6971e-06, -3.0398e-06],
        [-2.9206e-06, -1.8179e-06,  2.0713e-06,  ..., -2.4736e-06,
         -1.9819e-06, -2.3246e-06],
        [-3.1590e-06, -2.1309e-06,  2.2650e-06,  ..., -2.6375e-06,
         -2.2054e-06, -2.1458e-06]], device='cuda:0')
Loss: 0.9302273392677307


Running epoch 1, step 1969, batch 921
Sampled inputs[:2]: tensor([[    0,   895,  4110,  ...,  1578,  1245,    13],
        [    0,  6693,  1235,  ..., 10814,  1810,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1794e-04,  5.5238e-05, -1.5431e-04,  ...,  2.1465e-04,
          2.5916e-04, -1.3868e-04],
        [-2.6748e-06, -1.8030e-06,  1.8999e-06,  ..., -2.2128e-06,
         -1.7546e-06, -1.9819e-06],
        [-8.4639e-06, -5.9307e-06,  6.5267e-06,  ..., -6.8992e-06,
         -5.5134e-06, -6.1095e-06],
        [-6.0499e-06, -4.0084e-06,  4.5002e-06,  ..., -5.0217e-06,
         -3.9935e-06, -4.6492e-06],
        [-6.3479e-06, -4.5747e-06,  4.7088e-06,  ..., -5.2899e-06,
         -4.4107e-06, -4.2319e-06]], device='cuda:0')
Loss: 0.9873358011245728


Running epoch 1, step 1970, batch 922
Sampled inputs[:2]: tensor([[   0, 1086,   14,  ...,  963,  292,  221],
        [   0, 3630, 2199,  ..., 4157,   27, 4765]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9097e-05,  5.0986e-05, -6.2884e-05,  ...,  1.5629e-04,
          3.7033e-04, -5.0546e-05],
        [-4.1276e-06, -2.7865e-06,  2.7791e-06,  ..., -3.5241e-06,
         -2.8051e-06, -3.0547e-06],
        [-1.2934e-05, -9.0748e-06,  9.4473e-06,  ..., -1.0893e-05,
         -8.7023e-06, -9.3728e-06],
        [-9.0599e-06, -6.0201e-06,  6.3851e-06,  ..., -7.7486e-06,
         -6.1691e-06, -6.9439e-06],
        [-1.0043e-05, -7.1973e-06,  7.0184e-06,  ..., -8.6427e-06,
         -7.1824e-06, -6.7502e-06]], device='cuda:0')
Loss: 0.9206693172454834


Running epoch 1, step 1971, batch 923
Sampled inputs[:2]: tensor([[    0,   775,   266,  ...,   409,   328,  5768],
        [    0,  4014,    88,  ...,    14, 11961,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6526e-05, -6.3813e-05,  3.6125e-05,  ...,  1.8086e-04,
          4.3706e-04, -2.2369e-05],
        [-5.5060e-06, -3.7774e-06,  3.7774e-06,  ..., -4.7013e-06,
         -3.7514e-06, -4.0084e-06],
        [-1.7375e-05, -1.2368e-05,  1.2875e-05,  ..., -1.4678e-05,
         -1.1742e-05, -1.2457e-05],
        [-1.2249e-05, -8.2701e-06,  8.7842e-06,  ..., -1.0476e-05,
         -8.3447e-06, -9.2536e-06],
        [-1.3247e-05, -9.6858e-06,  9.4324e-06,  ..., -1.1429e-05,
         -9.5218e-06, -8.8066e-06]], device='cuda:0')
Loss: 0.9628167152404785


Running epoch 1, step 1972, batch 924
Sampled inputs[:2]: tensor([[   0,  266, 1211,  ..., 1336,  694,  516],
        [   0,  292, 2860,  ...,  266, 7000, 7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7787e-04, -1.3868e-04, -2.3988e-04,  ...,  1.0499e-04,
          4.3947e-04,  2.5759e-05],
        [-6.8620e-06, -4.7311e-06,  4.7907e-06,  ..., -5.8562e-06,
         -4.6752e-06, -4.9360e-06],
        [-2.1577e-05, -1.5482e-05,  1.6227e-05,  ..., -1.8254e-05,
         -1.4663e-05, -1.5318e-05],
        [-1.5318e-05, -1.0401e-05,  1.1168e-05,  ..., -1.3098e-05,
         -1.0476e-05, -1.1474e-05],
        [-1.6347e-05, -1.2100e-05,  1.1861e-05,  ..., -1.4126e-05,
         -1.1817e-05, -1.0774e-05]], device='cuda:0')
Loss: 0.9748297333717346


Running epoch 1, step 1973, batch 925
Sampled inputs[:2]: tensor([[    0,   642,   271,  ...,  5430,  2314,  6431],
        [    0,   935,   508,  ...,   287, 41582,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0385e-05, -2.3673e-04, -3.9463e-04,  ...,  1.1186e-05,
          3.2284e-04, -1.6160e-04],
        [-8.1360e-06, -5.6513e-06,  5.7518e-06,  ..., -6.9514e-06,
         -5.5395e-06, -5.8189e-06],
        [-2.5570e-05, -1.8418e-05,  1.9491e-05,  ..., -2.1577e-05,
         -1.7300e-05, -1.7941e-05],
        [-1.8239e-05, -1.2428e-05,  1.3493e-05,  ..., -1.5587e-05,
         -1.2442e-05, -1.3575e-05],
        [-1.9312e-05, -1.4380e-05,  1.4231e-05,  ..., -1.6645e-05,
         -1.3918e-05, -1.2562e-05]], device='cuda:0')
Loss: 0.8932405710220337


Running epoch 1, step 1974, batch 926
Sampled inputs[:2]: tensor([[    0,  2895,    26,  ..., 11645,  1535,  1558],
        [    0,    15, 43895,  ...,   292,   380, 16795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7727e-05, -2.4916e-04, -4.0045e-04,  ...,  5.0999e-05,
          3.8660e-04, -1.8190e-04],
        [-9.5516e-06, -6.5863e-06,  6.7651e-06,  ..., -8.1435e-06,
         -6.4857e-06, -6.8396e-06],
        [-3.0130e-05, -2.1592e-05,  2.3007e-05,  ..., -2.5421e-05,
         -2.0385e-05, -2.1219e-05],
        [-2.1353e-05, -1.4469e-05,  1.5832e-05,  ..., -1.8209e-05,
         -1.4529e-05, -1.5929e-05],
        [-2.2739e-05, -1.6838e-05,  1.6794e-05,  ..., -1.9610e-05,
         -1.6391e-05, -1.4871e-05]], device='cuda:0')
Loss: 0.972638726234436


Running epoch 1, step 1975, batch 927
Sampled inputs[:2]: tensor([[   0, 4371, 4806,  ...,  685,  461,  654],
        [   0, 9792, 3239,  ...,  699, 3636, 1761]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9233e-04, -2.8059e-04, -3.8519e-04,  ...,  5.3262e-05,
          4.0656e-04, -1.8190e-04],
        [-1.0870e-05, -7.5623e-06,  7.7337e-06,  ..., -9.3132e-06,
         -7.4282e-06, -7.7821e-06],
        [-3.4332e-05, -2.4796e-05,  2.6286e-05,  ..., -2.9102e-05,
         -2.3350e-05, -2.4214e-05],
        [-2.4378e-05, -1.6659e-05,  1.8112e-05,  ..., -2.0891e-05,
         -1.6674e-05, -1.8165e-05],
        [-2.5824e-05, -1.9267e-05,  1.9118e-05,  ..., -2.2367e-05,
         -1.8701e-05, -1.6913e-05]], device='cuda:0')
Loss: 0.9703922867774963
Graident accumulation at epoch 1, step 1975, batch 927
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.0009e-06, -3.3340e-05, -1.6890e-04,  ...,  6.4939e-05,
         -7.4482e-05,  8.8691e-06],
        [-1.0466e-05, -7.2279e-06,  7.3832e-06,  ..., -9.1529e-06,
         -7.1986e-06, -7.4489e-06],
        [ 3.5116e-05,  5.8326e-05, -2.7403e-05,  ...,  4.4406e-05,
          5.1294e-05,  1.5788e-05],
        [-1.7008e-05, -6.8506e-06,  1.3138e-05,  ..., -1.5125e-05,
         -8.7317e-06, -1.3597e-05],
        [-2.5801e-05, -1.9617e-05,  1.8984e-05,  ..., -2.2648e-05,
         -1.9202e-05, -1.6768e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5617e-08, 5.6646e-08, 6.8683e-08,  ..., 2.8615e-08, 1.5919e-07,
         4.0504e-08],
        [9.6383e-11, 6.2416e-11, 2.9926e-11,  ..., 6.4570e-11, 3.3323e-11,
         3.6688e-11],
        [4.8046e-09, 3.5500e-09, 2.0110e-09,  ..., 3.8666e-09, 2.1303e-09,
         1.3893e-09],
        [1.2747e-09, 1.4301e-09, 5.5687e-10,  ..., 1.1176e-09, 9.0770e-10,
         4.8567e-10],
        [4.1947e-10, 2.3899e-10, 1.0239e-10,  ..., 3.0936e-10, 1.0589e-10,
         1.2308e-10]], device='cuda:0')
optimizer state dict: 247.0
lr: [1.7142001127145812e-07, 1.7142001127145812e-07]
scheduler_last_epoch: 247


Running epoch 1, step 1976, batch 928
Sampled inputs[:2]: tensor([[    0,  5332,   391,  ...,   221,   334,  1530],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2069e-04,  3.5337e-05,  4.2749e-05,  ..., -1.9040e-04,
         -9.5204e-05, -6.2276e-05],
        [-1.2964e-06, -9.6858e-07,  8.8289e-07,  ..., -1.1772e-06,
         -9.6858e-07, -9.2387e-07],
        [-3.9339e-06, -3.0696e-06,  2.9653e-06,  ..., -3.5465e-06,
         -2.9206e-06, -2.7716e-06],
        [-2.8759e-06, -2.1309e-06,  2.0564e-06,  ..., -2.6226e-06,
         -2.1458e-06, -2.1458e-06],
        [-3.0100e-06, -2.4587e-06,  2.1607e-06,  ..., -2.7716e-06,
         -2.3991e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.9538668990135193


Running epoch 1, step 1977, batch 929
Sampled inputs[:2]: tensor([[    0,   344,   259,  ..., 47553,   287, 28978],
        [    0,    12,   287,  ...,    17,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1242e-04,  3.1906e-05, -6.4840e-05,  ..., -9.2659e-05,
         -2.9216e-04, -2.1711e-04],
        [-2.6152e-06, -1.8701e-06,  1.9036e-06,  ..., -2.2724e-06,
         -1.8105e-06, -1.8440e-06],
        [-8.0168e-06, -5.9456e-06,  6.3777e-06,  ..., -6.8694e-06,
         -5.4836e-06, -5.5432e-06],
        [-5.8562e-06, -4.0978e-06,  4.5002e-06,  ..., -5.0664e-06,
         -4.0233e-06, -4.3064e-06],
        [-6.0350e-06, -4.6790e-06,  4.5896e-06,  ..., -5.2899e-06,
         -4.4405e-06, -3.8445e-06]], device='cuda:0')
Loss: 0.9112593531608582


Running epoch 1, step 1978, batch 930
Sampled inputs[:2]: tensor([[    0,   278,  5717,  ...,  5342,  5147,    14],
        [    0,   352,   357,  ...,   461,   654, 19725]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7941e-04, -7.5992e-05, -1.4053e-04,  ..., -8.2292e-05,
         -3.2004e-04, -2.1113e-04],
        [-3.9488e-06, -2.8685e-06,  2.9691e-06,  ..., -3.4049e-06,
         -2.6710e-06, -2.7455e-06],
        [ 7.0198e-05,  4.0066e-05, -4.0329e-05,  ...,  5.7265e-05,
          3.7072e-07,  1.7464e-05],
        [-8.9109e-06, -6.3628e-06,  7.0632e-06,  ..., -7.6443e-06,
         -5.9903e-06, -6.4671e-06],
        [-9.0301e-06, -7.0930e-06,  7.0781e-06,  ..., -7.8976e-06,
         -6.5714e-06, -5.7518e-06]], device='cuda:0')
Loss: 0.9658401012420654


Running epoch 1, step 1979, batch 931
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0,   272,   352,  ...,   590,  4361,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0903e-04, -7.5992e-05, -1.6083e-04,  ..., -3.2481e-05,
         -3.6274e-04, -1.4731e-04],
        [-5.2452e-06, -3.8035e-06,  3.9674e-06,  ..., -4.5523e-06,
         -3.5577e-06, -3.6135e-06],
        [ 6.5906e-05,  3.6788e-05, -3.6812e-05,  ...,  5.3465e-05,
         -2.5797e-06,  1.4558e-05],
        [-1.1876e-05, -8.4937e-06,  9.4324e-06,  ..., -1.0267e-05,
         -8.0168e-06, -8.5682e-06],
        [-1.2025e-05, -9.4771e-06,  9.4622e-06,  ..., -1.0610e-05,
         -8.7768e-06, -7.6219e-06]], device='cuda:0')
Loss: 0.9883053302764893


Running epoch 1, step 1980, batch 932
Sampled inputs[:2]: tensor([[   0,  361, 1224,  ..., 4401, 4261, 1663],
        [   0, 6010,  829,  ...,  668, 1784,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6997e-05, -1.2168e-05,  1.2412e-05,  ..., -9.7104e-05,
         -8.7509e-05, -2.5938e-04],
        [-6.6385e-06, -4.8317e-06,  4.8019e-06,  ..., -5.8040e-06,
         -4.6827e-06, -4.6343e-06],
        [ 6.1555e-05,  3.3465e-05, -3.3981e-05,  ...,  4.9591e-05,
         -6.0666e-06,  1.1399e-05],
        [ 2.2034e-04,  2.3162e-04, -1.5445e-04,  ...,  2.1689e-04,
          2.9923e-04,  1.3411e-04],
        [-1.5512e-05, -1.2204e-05,  1.1668e-05,  ..., -1.3769e-05,
         -1.1683e-05, -9.9912e-06]], device='cuda:0')
Loss: 0.9935645461082458


Running epoch 1, step 1981, batch 933
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   221,   380,  ..., 10022,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9848e-04, -6.6992e-05, -4.5205e-05,  ..., -7.5103e-05,
         -2.2461e-04, -2.9265e-04],
        [-8.0168e-06, -5.7630e-06,  5.8599e-06,  ..., -6.9514e-06,
         -5.5730e-06, -5.5656e-06],
        [ 5.7174e-05,  3.0380e-05, -3.0405e-05,  ...,  4.5940e-05,
         -8.9127e-06,  8.4490e-06],
        [ 2.1719e-04,  2.2953e-04, -1.5191e-04,  ...,  2.1427e-04,
          2.9718e-04,  1.3188e-04],
        [-1.8701e-05, -1.4558e-05,  1.4216e-05,  ..., -1.6481e-05,
         -1.3903e-05, -1.1973e-05]], device='cuda:0')
Loss: 0.9626986384391785


Running epoch 1, step 1982, batch 934
Sampled inputs[:2]: tensor([[   0,   14,  560,  ...,   12, 8593,  266],
        [   0,   21, 1304,  ..., 3577,   13, 2497]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5386e-04,  3.2187e-05, -1.4931e-04,  ..., -2.0032e-04,
         -5.7800e-04, -4.3471e-04],
        [-9.3803e-06, -6.7987e-06,  6.5118e-06,  ..., -8.2329e-06,
         -6.7949e-06, -6.6087e-06],
        [ 5.2853e-05,  2.6923e-05, -2.8110e-05,  ...,  4.1887e-05,
         -1.2757e-05,  5.1260e-06],
        [ 2.1408e-04,  2.2722e-04, -1.5044e-04,  ...,  2.1132e-04,
          2.9434e-04,  1.2937e-04],
        [-2.2143e-05, -1.7345e-05,  1.5937e-05,  ..., -1.9759e-05,
         -1.7107e-05, -1.4476e-05]], device='cuda:0')
Loss: 0.9287648797035217


Running epoch 1, step 1983, batch 935
Sampled inputs[:2]: tensor([[    0,  7066,  2737,  ...,  2269,   271,   927],
        [    0,   806,   352,  ...,  3493,   352, 49256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5386e-04, -8.4675e-05, -1.7947e-04,  ..., -1.3234e-04,
         -5.1916e-04, -5.0644e-04],
        [-1.0736e-05, -7.7523e-06,  7.5251e-06,  ..., -9.3952e-06,
         -7.7374e-06, -7.4990e-06],
        [ 4.8710e-05,  2.3913e-05, -2.4817e-05,  ...,  3.8356e-05,
         -1.5603e-05,  2.4289e-06],
        [ 2.1113e-04,  2.2521e-04, -1.4815e-04,  ...,  2.0880e-04,
          2.9231e-04,  1.2733e-04],
        [-2.5317e-05, -1.9774e-05,  1.8395e-05,  ..., -2.2531e-05,
         -1.9446e-05, -1.6376e-05]], device='cuda:0')
Loss: 0.9998986721038818
Graident accumulation at epoch 1, step 1983, batch 935
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.3487e-05, -3.8474e-05, -1.6996e-04,  ...,  4.5211e-05,
         -1.1895e-04, -4.2662e-05],
        [-1.0493e-05, -7.2803e-06,  7.3974e-06,  ..., -9.1772e-06,
         -7.2525e-06, -7.4539e-06],
        [ 3.6476e-05,  5.4884e-05, -2.7144e-05,  ...,  4.3801e-05,
          4.4604e-05,  1.4452e-05],
        [ 5.8051e-06,  1.6356e-05, -2.9908e-06,  ...,  7.2675e-06,
          2.1373e-05,  4.9609e-07],
        [-2.5752e-05, -1.9633e-05,  1.8925e-05,  ..., -2.2636e-05,
         -1.9227e-05, -1.6728e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5686e-08, 5.6597e-08, 6.8646e-08,  ..., 2.8604e-08, 1.5930e-07,
         4.0720e-08],
        [9.6402e-11, 6.2414e-11, 2.9953e-11,  ..., 6.4594e-11, 3.3349e-11,
         3.6707e-11],
        [4.8022e-09, 3.5471e-09, 2.0096e-09,  ..., 3.8642e-09, 2.1284e-09,
         1.3880e-09],
        [1.3180e-09, 1.4794e-09, 5.7826e-10,  ..., 1.1600e-09, 9.9224e-10,
         5.0140e-10],
        [4.1969e-10, 2.3914e-10, 1.0262e-10,  ..., 3.0956e-10, 1.0616e-10,
         1.2323e-10]], device='cuda:0')
optimizer state dict: 248.0
lr: [1.493810611872959e-07, 1.493810611872959e-07]
scheduler_last_epoch: 248


Running epoch 1, step 1984, batch 936
Sampled inputs[:2]: tensor([[    0,  3115,  1640,  ...,   300,   266,  5453],
        [    0,   342,   266,  ...,   199,   395, 11578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5891e-05, -6.7035e-06, -1.6510e-04,  ...,  5.9099e-05,
         -2.2693e-04, -2.2037e-04],
        [-1.3113e-06, -8.0094e-07,  9.3505e-07,  ..., -1.1250e-06,
         -8.9779e-07, -9.0897e-07],
        [-3.9637e-06, -2.5034e-06,  3.1143e-06,  ..., -3.2932e-06,
         -2.6524e-06, -2.5928e-06],
        [-2.9206e-06, -1.7136e-06,  2.2054e-06,  ..., -2.4885e-06,
         -1.9968e-06, -2.1160e-06],
        [-3.0994e-06, -2.0415e-06,  2.3693e-06,  ..., -2.6226e-06,
         -2.2054e-06, -1.8552e-06]], device='cuda:0')
Loss: 0.929205060005188


Running epoch 1, step 1985, batch 937
Sampled inputs[:2]: tensor([[   0, 1415,  300,  ..., 1497, 5715, 4555],
        [   0,   19,  669,  ...,   14, 4053,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2483e-04,  3.3872e-05, -3.1490e-05,  ...,  1.6131e-04,
         -1.3958e-04, -1.3791e-04],
        [-2.6450e-06, -1.6801e-06,  1.8589e-06,  ..., -2.3097e-06,
         -1.7732e-06, -1.9521e-06],
        [-8.1658e-06, -5.3197e-06,  6.2883e-06,  ..., -6.9290e-06,
         -5.3346e-06, -5.7817e-06],
        [-5.8711e-06, -3.5912e-06,  4.3064e-06,  ..., -5.1111e-06,
         -3.9190e-06, -4.5002e-06],
        [-6.3032e-06, -4.2319e-06,  4.6641e-06,  ..., -5.4389e-06,
         -4.3958e-06, -4.1202e-06]], device='cuda:0')
Loss: 0.9651402831077576


Running epoch 1, step 1986, batch 938
Sampled inputs[:2]: tensor([[    0,    17,    12,  ...,    12,   461,   806],
        [    0,   266,  2552,  ...,    13, 16179,   800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1846e-04,  1.1011e-04,  1.0752e-04,  ...,  2.6165e-04,
         -2.3080e-05,  3.3073e-05],
        [-4.0680e-06, -2.7083e-06,  2.7418e-06,  ..., -3.5465e-06,
         -2.8536e-06, -2.9728e-06],
        [-1.2726e-05, -8.7470e-06,  9.3579e-06,  ..., -1.0863e-05,
         -8.7619e-06, -9.0450e-06],
        [-9.0450e-06, -5.8413e-06,  6.3330e-06,  ..., -7.8827e-06,
         -6.3628e-06, -6.9141e-06],
        [-9.8348e-06, -6.9886e-06,  6.9737e-06,  ..., -8.5682e-06,
         -7.2271e-06, -6.4746e-06]], device='cuda:0')
Loss: 0.9978460073471069


Running epoch 1, step 1987, batch 939
Sampled inputs[:2]: tensor([[   0, 4213, 1921,  ..., 1340, 1049,  292],
        [   0,    9,  287,  ...,  259, 8244, 1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9891e-04,  6.0681e-05, -2.0110e-04,  ...,  2.5509e-04,
         -2.6133e-04,  2.6742e-05],
        [-5.4091e-06, -3.6992e-06,  3.8296e-06,  ..., -4.6864e-06,
         -3.7663e-06, -3.8892e-06],
        [-1.7166e-05, -1.2115e-05,  1.3113e-05,  ..., -1.4588e-05,
         -1.1727e-05, -1.2070e-05],
        [-1.2159e-05, -8.1062e-06,  8.9407e-06,  ..., -1.0505e-05,
         -8.4490e-06, -9.1493e-06],
        [-1.3039e-05, -9.5069e-06,  9.6112e-06,  ..., -1.1310e-05,
         -9.5218e-06, -8.5011e-06]], device='cuda:0')
Loss: 0.9742388725280762


Running epoch 1, step 1988, batch 940
Sampled inputs[:2]: tensor([[    0,   365,  8790,  ...,  1172,  8806,   266],
        [    0,    13,  1581,  ...,    13, 11628, 14876]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0881e-04,  5.9061e-05, -3.0874e-04,  ...,  2.9622e-04,
         -3.0572e-04,  7.5703e-05],
        [-6.7279e-06, -4.6194e-06,  4.8503e-06,  ..., -5.8487e-06,
         -4.6901e-06, -4.7646e-06],
        [-2.1338e-05, -1.5169e-05,  1.6555e-05,  ..., -1.8254e-05,
         -1.4648e-05, -1.4842e-05],
        [-1.5169e-05, -1.0177e-05,  1.1370e-05,  ..., -1.3173e-05,
         -1.0565e-05, -1.1280e-05],
        [-1.6093e-05, -1.1846e-05,  1.2070e-05,  ..., -1.4052e-05,
         -1.1802e-05, -1.0371e-05]], device='cuda:0')
Loss: 0.9726868867874146


Running epoch 1, step 1989, batch 941
Sampled inputs[:2]: tensor([[    0,  3408,   300,  ...,  3868,   300,  2932],
        [    0, 13245,  1503,  ...,    14,  5605,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7189e-04,  1.2969e-04, -3.7133e-04,  ...,  2.5889e-04,
         -3.5556e-04, -3.2948e-05],
        [-8.1137e-06, -5.5321e-06,  5.9232e-06,  ..., -6.9886e-06,
         -5.5358e-06, -5.6252e-06],
        [-2.5600e-05, -1.8150e-05,  2.0087e-05,  ..., -2.1756e-05,
         -1.7256e-05, -1.7494e-05],
        [-1.8269e-05, -1.2159e-05,  1.3873e-05,  ..., -1.5706e-05,
         -1.2435e-05, -1.3307e-05],
        [-1.9193e-05, -1.4126e-05,  1.4558e-05,  ..., -1.6659e-05,
         -1.3843e-05, -1.2152e-05]], device='cuda:0')
Loss: 0.9715796709060669


Running epoch 1, step 1990, batch 942
Sampled inputs[:2]: tensor([[    0,  6088,  1172,  ...,   546,   401,   925],
        [    0,   380,  6119,  ..., 11823,   287,  6797]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0006e-04,  4.3678e-05, -4.0643e-04,  ...,  2.8140e-04,
         -3.5327e-04, -1.0101e-04],
        [-9.4324e-06, -6.4820e-06,  6.9216e-06,  ..., -8.1286e-06,
         -6.4857e-06, -6.4932e-06],
        [-2.9683e-05, -2.1249e-05,  2.3425e-05,  ..., -2.5317e-05,
         -2.0251e-05, -2.0221e-05],
        [-2.1309e-05, -1.4335e-05,  1.6302e-05,  ..., -1.8343e-05,
         -1.4655e-05, -1.5438e-05],
        [-2.2218e-05, -1.6510e-05,  1.6958e-05,  ..., -1.9327e-05,
         -1.6183e-05, -1.4000e-05]], device='cuda:0')
Loss: 0.9693787097930908


Running epoch 1, step 1991, batch 943
Sampled inputs[:2]: tensor([[  0, 328, 266,  ..., 382,  17,  13],
        [  0,  12, 287,  ..., 658, 221, 474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5079e-04, -1.5399e-05, -6.0875e-04,  ...,  3.4389e-04,
         -4.4197e-04, -1.6189e-04],
        [-1.0766e-05, -7.4059e-06,  7.8045e-06,  ..., -9.3579e-06,
         -7.4059e-06, -7.4692e-06],
        [-3.3796e-05, -2.4199e-05,  2.6405e-05,  ..., -2.9013e-05,
         -2.3037e-05, -2.3142e-05],
        [-2.4244e-05, -1.6347e-05,  1.8343e-05,  ..., -2.1040e-05,
         -1.6682e-05, -1.7673e-05],
        [-2.5481e-05, -1.8924e-05,  1.9208e-05,  ..., -2.2322e-05,
         -1.8552e-05, -1.6160e-05]], device='cuda:0')
Loss: 0.9506112933158875
Graident accumulation at epoch 1, step 1991, batch 943
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.0588e-06, -3.6166e-05, -2.1384e-04,  ...,  7.5078e-05,
         -1.5125e-04, -5.4585e-05],
        [-1.0520e-05, -7.2929e-06,  7.4381e-06,  ..., -9.1952e-06,
         -7.2678e-06, -7.4555e-06],
        [ 2.9448e-05,  4.6976e-05, -2.1790e-05,  ...,  3.6520e-05,
          3.7840e-05,  1.0693e-05],
        [ 2.8001e-06,  1.3085e-05, -8.5739e-07,  ...,  4.4367e-06,
          1.7567e-05, -1.3208e-06],
        [-2.5725e-05, -1.9562e-05,  1.8953e-05,  ..., -2.2605e-05,
         -1.9159e-05, -1.6672e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5754e-08, 5.6540e-08, 6.8948e-08,  ..., 2.8693e-08, 1.5933e-07,
         4.0706e-08],
        [9.6422e-11, 6.2406e-11, 2.9984e-11,  ..., 6.4617e-11, 3.3371e-11,
         3.6726e-11],
        [4.7985e-09, 3.5441e-09, 2.0083e-09,  ..., 3.8612e-09, 2.1268e-09,
         1.3871e-09],
        [1.3173e-09, 1.4782e-09, 5.7802e-10,  ..., 1.1593e-09, 9.9153e-10,
         5.0121e-10],
        [4.1992e-10, 2.3926e-10, 1.0289e-10,  ..., 3.0975e-10, 1.0640e-10,
         1.2337e-10]], device='cuda:0')
optimizer state dict: 249.0
lr: [1.2884737166425243e-07, 1.2884737166425243e-07]
scheduler_last_epoch: 249


Running epoch 1, step 1992, batch 944
Sampled inputs[:2]: tensor([[    0,   944,   278,  ...,  2374,   699,  8867],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0152e-05,  7.7354e-06, -1.0073e-04,  ...,  9.2308e-05,
          1.0205e-04,  5.6429e-05],
        [-1.3113e-06, -1.0058e-06,  9.3505e-07,  ..., -1.2219e-06,
         -1.0431e-06, -9.4250e-07],
        [-4.2021e-06, -3.3975e-06,  3.2037e-06,  ..., -3.9041e-06,
         -3.3826e-06, -3.0100e-06],
        [-2.9355e-06, -2.2501e-06,  2.1756e-06,  ..., -2.7418e-06,
         -2.3693e-06, -2.2054e-06],
        [-3.1441e-06, -2.6077e-06,  2.3395e-06,  ..., -2.9653e-06,
         -2.6524e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9516279697418213


Running epoch 1, step 1993, batch 945
Sampled inputs[:2]: tensor([[   0,   13, 1529,  ...,  943,  266, 9479],
        [   0,   14,  357,  ...,   30,  287,  839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1468e-06, -3.2348e-05,  1.3800e-04,  ...,  3.9561e-05,
          1.2967e-04,  5.7872e-05],
        [-2.6599e-06, -2.0191e-06,  1.9707e-06,  ..., -2.4065e-06,
         -2.0191e-06, -1.8403e-06],
        [-8.3745e-06, -6.6608e-06,  6.5714e-06,  ..., -7.5847e-06,
         -6.4373e-06, -5.8413e-06],
        [-5.9605e-06, -4.5002e-06,  4.5896e-06,  ..., -5.3942e-06,
         -4.5598e-06, -4.3213e-06],
        [-6.3479e-06, -5.2005e-06,  4.8578e-06,  ..., -5.8413e-06,
         -5.1558e-06, -4.1425e-06]], device='cuda:0')
Loss: 1.0066120624542236


Running epoch 1, step 1994, batch 946
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0, 1125,  278,  ..., 6447,  609,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0384e-04, -1.8548e-04,  1.4304e-04,  ...,  1.0091e-04,
          1.7128e-04,  1.4040e-05],
        [-4.0382e-06, -3.0249e-06,  2.9765e-06,  ..., -3.5986e-06,
         -2.9467e-06, -2.8089e-06],
        [-1.2875e-05, -1.0103e-05,  1.0073e-05,  ..., -1.1489e-05,
         -9.5665e-06, -9.0301e-06],
        [-9.0152e-06, -6.7055e-06,  6.9290e-06,  ..., -8.0466e-06,
         -6.6608e-06, -6.5863e-06],
        [-9.5963e-06, -7.7784e-06,  7.3165e-06,  ..., -8.7321e-06,
         -7.5549e-06, -6.2883e-06]], device='cuda:0')
Loss: 0.9565818309783936


Running epoch 1, step 1995, batch 947
Sampled inputs[:2]: tensor([[    0,   944,   278,  ..., 17330,  1683,   360],
        [    0,    14,  1266,  ...,  2288,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7866e-05, -2.7089e-04,  1.4753e-04,  ...,  1.8883e-04,
          5.9695e-05,  1.0356e-04],
        [-5.3495e-06, -4.0233e-06,  3.9227e-06,  ..., -4.7684e-06,
         -3.9600e-06, -3.7365e-06],
        [-1.6987e-05, -1.3441e-05,  1.3292e-05,  ..., -1.5214e-05,
         -1.2830e-05, -1.1966e-05],
        [-1.1936e-05, -8.9407e-06,  9.1493e-06,  ..., -1.0684e-05,
         -8.9705e-06, -8.7768e-06],
        [-1.2636e-05, -1.0341e-05,  9.6411e-06,  ..., -1.1533e-05,
         -1.0103e-05, -8.2999e-06]], device='cuda:0')
Loss: 0.9642810225486755


Running epoch 1, step 1996, batch 948
Sampled inputs[:2]: tensor([[   0,  266, 2604,  ...,  278, 4035, 4165],
        [   0,  417,  199,  ..., 1853,   12,  709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9344e-05, -3.5826e-04,  2.6610e-05,  ..., -3.4983e-05,
          3.4959e-05, -1.3036e-04],
        [-6.6981e-06, -5.0217e-06,  4.9658e-06,  ..., -5.9158e-06,
         -4.8652e-06, -4.6529e-06],
        [-2.1219e-05, -1.6734e-05,  1.6794e-05,  ..., -1.8835e-05,
         -1.5736e-05, -1.4871e-05],
        [-1.4961e-05, -1.1176e-05,  1.1593e-05,  ..., -1.3277e-05,
         -1.1027e-05, -1.0937e-05],
        [-1.5706e-05, -1.2845e-05,  1.2100e-05,  ..., -1.4231e-05,
         -1.2368e-05, -1.0237e-05]], device='cuda:0')
Loss: 0.9796158075332642


Running epoch 1, step 1997, batch 949
Sampled inputs[:2]: tensor([[    0,   266,   298,  ...,   654,   271,  4483],
        [    0,  1581, 11884,  ...,  7031,   689,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4089e-04, -4.3085e-04, -1.3504e-04,  ..., -3.2944e-04,
         -1.0162e-04, -2.6967e-04],
        [-8.0615e-06, -5.9679e-06,  5.9642e-06,  ..., -7.1079e-06,
         -5.8413e-06, -5.5991e-06],
        [-2.5392e-05, -1.9774e-05,  2.0072e-05,  ..., -2.2426e-05,
         -1.8701e-05, -1.7703e-05],
        [-1.7896e-05, -1.3173e-05,  1.3843e-05,  ..., -1.5840e-05,
         -1.3143e-05, -1.3083e-05],
        [-1.9029e-05, -1.5348e-05,  1.4618e-05,  ..., -1.7136e-05,
         -1.4842e-05, -1.2323e-05]], device='cuda:0')
Loss: 0.9319375157356262


Running epoch 1, step 1998, batch 950
Sampled inputs[:2]: tensor([[   0, 1615,  287,  ...,  259,  623,   12],
        [   0,  578,  221,  ...,  287, 1254, 4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5952e-04, -4.5229e-04,  1.1752e-04,  ..., -2.0841e-05,
         -4.6217e-04,  1.4224e-04],
        [-9.4399e-06, -6.8136e-06,  6.7204e-06,  ..., -8.4117e-06,
         -6.8024e-06, -6.6794e-06],
        [-2.9653e-05, -2.2560e-05,  2.2680e-05,  ..., -2.6390e-05,
         -2.1696e-05, -2.0951e-05],
        [-2.1011e-05, -1.5073e-05,  1.5587e-05,  ..., -1.8850e-05,
         -1.5393e-05, -1.5616e-05],
        [-2.2411e-05, -1.7524e-05,  1.6585e-05,  ..., -2.0280e-05,
         -1.7241e-05, -1.4737e-05]], device='cuda:0')
Loss: 0.9023136496543884


Running epoch 1, step 1999, batch 951
Sampled inputs[:2]: tensor([[   0, 3615,   16,  ..., 2140, 1098,  352],
        [   0,  829,  874,  ...,  292,  380,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6276e-04, -6.5671e-04,  9.1266e-05,  ...,  1.7460e-05,
         -4.2862e-04,  1.4048e-04],
        [-1.0781e-05, -7.8343e-06,  7.7412e-06,  ..., -9.5367e-06,
         -7.7337e-06, -7.5884e-06],
        [-3.3975e-05, -2.6017e-05,  2.6181e-05,  ..., -3.0026e-05,
         -2.4751e-05, -2.3931e-05],
        [-2.4050e-05, -1.7367e-05,  1.7986e-05,  ..., -2.1398e-05,
         -1.7494e-05, -1.7792e-05],
        [-2.5481e-05, -2.0072e-05,  1.8999e-05,  ..., -2.2918e-05,
         -1.9565e-05, -1.6689e-05]], device='cuda:0')
Loss: 0.9758791327476501
Graident accumulation at epoch 1, step 1999, batch 951
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.9929e-05, -9.8220e-05, -1.8333e-04,  ...,  6.9316e-05,
         -1.7899e-04, -3.5079e-05],
        [-1.0546e-05, -7.3470e-06,  7.4684e-06,  ..., -9.2294e-06,
         -7.3144e-06, -7.4688e-06],
        [ 2.3106e-05,  3.9677e-05, -1.6992e-05,  ...,  2.9865e-05,
          3.1581e-05,  7.2301e-06],
        [ 1.1508e-07,  1.0040e-05,  1.0269e-06,  ...,  1.8533e-06,
          1.4061e-05, -2.9679e-06],
        [-2.5701e-05, -1.9613e-05,  1.8958e-05,  ..., -2.2636e-05,
         -1.9200e-05, -1.6673e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5829e-08, 5.6915e-08, 6.8887e-08,  ..., 2.8665e-08, 1.5936e-07,
         4.0685e-08],
        [9.6442e-11, 6.2405e-11, 3.0014e-11,  ..., 6.4643e-11, 3.3397e-11,
         3.6747e-11],
        [4.7949e-09, 3.5412e-09, 2.0070e-09,  ..., 3.8582e-09, 2.1253e-09,
         1.3863e-09],
        [1.3166e-09, 1.4770e-09, 5.7776e-10,  ..., 1.1586e-09, 9.9084e-10,
         5.0102e-10],
        [4.2015e-10, 2.3942e-10, 1.0315e-10,  ..., 3.0997e-10, 1.0668e-10,
         1.2352e-10]], device='cuda:0')
optimizer state dict: 250.0
lr: [1.0982208042932707e-07, 1.0982208042932707e-07]
scheduler_last_epoch: 250


Running epoch 1, step 2000, batch 952
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,  1171,  2926,  ...,   259,  4288,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1032e-05, -8.8873e-05, -1.5873e-04,  ..., -2.8274e-05,
         -1.9344e-04, -5.4980e-05],
        [-1.3188e-06, -9.4622e-07,  9.5367e-07,  ..., -1.1399e-06,
         -8.9034e-07, -8.9407e-07],
        [-4.0233e-06, -3.0696e-06,  3.2187e-06,  ..., -3.4571e-06,
         -2.7567e-06, -2.7120e-06],
        [-2.9355e-06, -2.1160e-06,  2.2501e-06,  ..., -2.5481e-06,
         -2.0117e-06, -2.1160e-06],
        [-3.0100e-06, -2.3842e-06,  2.3395e-06,  ..., -2.6375e-06,
         -2.2054e-06, -1.8477e-06]], device='cuda:0')
Loss: 0.9702522158622742


Running epoch 1, step 2001, batch 953
Sampled inputs[:2]: tensor([[    0,     8,    19,  ..., 13359, 12377,   938],
        [    0, 10893, 10997,  ...,   367,   616,  7903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0032e-05, -1.2342e-04, -3.3647e-04,  ...,  2.7979e-06,
         -2.0555e-04,  2.2918e-05],
        [-2.5854e-06, -1.9222e-06,  1.9148e-06,  ..., -2.3320e-06,
         -1.8589e-06, -1.7956e-06],
        [-7.9572e-06, -6.1691e-06,  6.3777e-06,  ..., -7.0632e-06,
         -5.7220e-06, -5.4091e-06],
        [-5.7667e-06, -4.2468e-06,  4.5002e-06,  ..., -5.1856e-06,
         -4.1723e-06, -4.2170e-06],
        [-6.0350e-06, -4.8727e-06,  4.7088e-06,  ..., -5.4538e-06,
         -4.6194e-06, -3.7700e-06]], device='cuda:0')
Loss: 0.9381835460662842


Running epoch 1, step 2002, batch 954
Sampled inputs[:2]: tensor([[    0,    13,  1529,  ..., 15682,  1355,   259],
        [    0,  5506,   696,  ...,   607, 11129,   276]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2600e-04, -1.6487e-04, -3.3712e-04,  ..., -4.9999e-05,
         -1.7098e-04,  7.2002e-06],
        [-3.9116e-06, -2.9132e-06,  2.9132e-06,  ..., -3.4720e-06,
         -2.8126e-06, -2.6636e-06],
        [-1.1981e-05, -9.3281e-06,  9.6560e-06,  ..., -1.0490e-05,
         -8.5980e-06, -8.0168e-06],
        [-8.7172e-06, -6.4224e-06,  6.8247e-06,  ..., -7.7188e-06,
         -6.3032e-06, -6.2734e-06],
        [-9.1642e-06, -7.4208e-06,  7.1824e-06,  ..., -8.1658e-06,
         -7.0035e-06, -5.6326e-06]], device='cuda:0')
Loss: 0.9703177213668823


Running epoch 1, step 2003, batch 955
Sampled inputs[:2]: tensor([[    0,   680,   993,  ...,   699, 11426,    12],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0869e-04, -2.5835e-04, -2.9720e-04,  ..., -1.7341e-04,
         -1.3249e-04, -1.2152e-04],
        [-5.2527e-06, -3.9041e-06,  3.9488e-06,  ..., -4.5896e-06,
         -3.7141e-06, -3.6322e-06],
        [-1.6332e-05, -1.2666e-05,  1.3247e-05,  ..., -1.4096e-05,
         -1.1489e-05, -1.1146e-05],
        [-1.1861e-05, -8.7172e-06,  9.3579e-06,  ..., -1.0327e-05,
         -8.4043e-06, -8.6576e-06],
        [-1.2234e-05, -9.8795e-06,  9.6560e-06,  ..., -1.0774e-05,
         -9.2089e-06, -7.6890e-06]], device='cuda:0')
Loss: 0.9375510811805725


Running epoch 1, step 2004, batch 956
Sampled inputs[:2]: tensor([[    0,    12,  1808,  ...,   847,   300, 44349],
        [    0,  2906, 46441,  ..., 39156,   287, 11452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8522e-04, -2.9214e-04, -4.0686e-04,  ..., -1.3724e-04,
          1.4309e-04, -9.8186e-05],
        [-6.7428e-06, -4.9621e-06,  4.7795e-06,  ..., -5.9009e-06,
         -4.8317e-06, -4.6678e-06],
        [-2.0713e-05, -1.6034e-05,  1.5944e-05,  ..., -1.7971e-05,
         -1.4916e-05, -1.4141e-05],
        [-1.5065e-05, -1.1057e-05,  1.1221e-05,  ..., -1.3202e-05,
         -1.0923e-05, -1.0982e-05],
        [-1.5900e-05, -1.2740e-05,  1.1876e-05,  ..., -1.4067e-05,
         -1.2189e-05, -1.0058e-05]], device='cuda:0')
Loss: 0.9734666347503662


Running epoch 1, step 2005, batch 957
Sampled inputs[:2]: tensor([[    0,   380,   759,  ...,  1420,  1804,   490],
        [    0,    14,   475,  ..., 44038,    12,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2270e-04, -2.4789e-04, -1.5301e-04,  ..., -1.0983e-04,
          1.3771e-04,  2.2756e-05],
        [-8.0243e-06, -6.0350e-06,  5.5470e-06,  ..., -7.0930e-06,
         -5.8599e-06, -5.6177e-06],
        [-2.4825e-05, -1.9684e-05,  1.8671e-05,  ..., -2.1815e-05,
         -1.8284e-05, -1.7181e-05],
        [-1.8001e-05, -1.3515e-05,  1.3031e-05,  ..., -1.5974e-05,
         -1.3351e-05, -1.3277e-05],
        [-1.9044e-05, -1.5587e-05,  1.3888e-05,  ..., -1.7032e-05,
         -1.4871e-05, -1.2204e-05]], device='cuda:0')
Loss: 0.9589600563049316


Running epoch 1, step 2006, batch 958
Sampled inputs[:2]: tensor([[    0, 15033,   278,  ...,   266,  2937,    14],
        [    0, 18717,  2837,  ...,    48,    18,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9356e-05, -2.2951e-04, -1.4651e-04,  ..., -7.1884e-05,
          7.2557e-05, -5.2208e-05],
        [-9.3728e-06, -6.9961e-06,  6.4448e-06,  ..., -8.3372e-06,
         -6.8881e-06, -6.5565e-06],
        [-2.8998e-05, -2.2784e-05,  2.1666e-05,  ..., -2.5615e-05,
         -2.1443e-05, -2.0042e-05],
        [-2.0936e-05, -1.5572e-05,  1.5073e-05,  ..., -1.8701e-05,
         -1.5602e-05, -1.5438e-05],
        [-2.2277e-05, -1.8075e-05,  1.6153e-05,  ..., -2.0042e-05,
         -1.7464e-05, -1.4275e-05]], device='cuda:0')
Loss: 0.9706660509109497


Running epoch 1, step 2007, batch 959
Sampled inputs[:2]: tensor([[   0,  287, 1410,  ..., 1255, 1699,  328],
        [   0,   12,  287,  ...,   12, 5576,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9535e-05, -2.7460e-04, -2.9863e-04,  ..., -4.8641e-05,
          2.1140e-04, -2.2030e-05],
        [-1.0729e-05, -7.9237e-06,  7.4953e-06,  ..., -9.4697e-06,
         -7.8194e-06, -7.4953e-06],
        [-3.3259e-05, -2.5779e-05,  2.5198e-05,  ..., -2.9132e-05,
         -2.4334e-05, -2.2963e-05],
        [-2.4065e-05, -1.7673e-05,  1.7621e-05,  ..., -2.1309e-05,
         -1.7747e-05, -1.7717e-05],
        [-2.5421e-05, -2.0400e-05,  1.8686e-05,  ..., -2.2694e-05,
         -1.9744e-05, -1.6257e-05]], device='cuda:0')
Loss: 0.9558014869689941
Graident accumulation at epoch 1, step 2007, batch 959
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9982e-05, -1.1586e-04, -1.9486e-04,  ...,  5.7521e-05,
         -1.3995e-04, -3.3774e-05],
        [-1.0565e-05, -7.4047e-06,  7.4711e-06,  ..., -9.2534e-06,
         -7.3649e-06, -7.4714e-06],
        [ 1.7470e-05,  3.3131e-05, -1.2773e-05,  ...,  2.3966e-05,
          2.5990e-05,  4.2109e-06],
        [-2.3030e-06,  7.2689e-06,  2.6863e-06,  ..., -4.6294e-07,
          1.0880e-05, -4.4429e-06],
        [-2.5673e-05, -1.9692e-05,  1.8931e-05,  ..., -2.2642e-05,
         -1.9254e-05, -1.6632e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5777e-08, 5.6934e-08, 6.8908e-08,  ..., 2.8639e-08, 1.5924e-07,
         4.0645e-08],
        [9.6460e-11, 6.2406e-11, 3.0040e-11,  ..., 6.4668e-11, 3.3425e-11,
         3.6766e-11],
        [4.7912e-09, 3.5384e-09, 2.0056e-09,  ..., 3.8552e-09, 2.1237e-09,
         1.3854e-09],
        [1.3158e-09, 1.4758e-09, 5.7749e-10,  ..., 1.1579e-09, 9.9016e-10,
         5.0084e-10],
        [4.2037e-10, 2.3960e-10, 1.0340e-10,  ..., 3.1017e-10, 1.0696e-10,
         1.2366e-10]], device='cuda:0')
optimizer state dict: 251.0
lr: [9.230809471309388e-08, 9.230809471309388e-08]
scheduler_last_epoch: 251
Epoch 1 | Batch 959/1048 | Training PPL: 2093.121760270925 | time 103.17786979675293
Saving checkpoint at epoch 1, step 2007, batch 959
Epoch 1 | Validation PPL: 6.726930779171731 | Learning rate: 9.230809471309388e-08
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_2007, AFTER epoch 1, step 2007


Running epoch 1, step 2008, batch 960
Sampled inputs[:2]: tensor([[    0, 13081,   278,  ...,   368,   266,  1717],
        [    0,  1832,   292,  ...,  2176,  1345,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1417e-04, -1.1823e-04, -9.5547e-05,  ..., -1.7393e-05,
          1.9897e-04, -7.5339e-05],
        [-1.3486e-06, -9.0152e-07,  9.6858e-07,  ..., -1.1548e-06,
         -9.0152e-07, -9.2760e-07],
        [-4.1127e-06, -2.8908e-06,  3.1888e-06,  ..., -3.4720e-06,
         -2.7120e-06, -2.7716e-06],
        [-3.0100e-06, -1.9670e-06,  2.2501e-06,  ..., -2.5630e-06,
         -2.0117e-06, -2.1756e-06],
        [-3.1441e-06, -2.2948e-06,  2.3693e-06,  ..., -2.6971e-06,
         -2.2203e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9556311964988708


Running epoch 1, step 2009, batch 961
Sampled inputs[:2]: tensor([[    0,  5841,   328,  ...,  2051,   266,   756],
        [    0,   266,  1176,  ...,   199, 17791,  3662]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1712e-05, -2.0368e-04, -2.3823e-04,  ...,  1.0875e-04,
          7.5387e-05, -2.0297e-04],
        [-2.6375e-06, -1.7509e-06,  1.9073e-06,  ..., -2.3097e-06,
         -1.7807e-06, -1.8887e-06],
        [-8.3148e-06, -5.6922e-06,  6.4522e-06,  ..., -7.1228e-06,
         -5.4985e-06, -5.7966e-06],
        [-6.0648e-06, -3.9488e-06,  4.5747e-06,  ..., -5.3048e-06,
         -4.0978e-06, -4.5598e-06],
        [-6.2883e-06, -4.4554e-06,  4.7535e-06,  ..., -5.4687e-06,
         -4.4256e-06, -4.0233e-06]], device='cuda:0')
Loss: 0.934008777141571


Running epoch 1, step 2010, batch 962
Sampled inputs[:2]: tensor([[    0,  3889,  4039,  ...,   616, 22910,   259],
        [    0,  6022,   644,  ..., 14834,  3554,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9283e-06, -2.3026e-04, -2.6680e-04,  ...,  8.8834e-05,
          7.6214e-05, -1.7673e-04],
        [-4.0531e-06, -2.7046e-06,  2.8089e-06,  ..., -3.5167e-06,
         -2.7418e-06, -2.9095e-06],
        [-1.2726e-05, -8.9109e-06,  9.5516e-06,  ..., -1.0908e-05,
         -8.5682e-06, -9.0152e-06],
        [-9.1791e-06, -6.0648e-06,  6.6459e-06,  ..., -7.9870e-06,
         -6.2436e-06, -6.8843e-06],
        [-9.7305e-06, -7.0184e-06,  7.0632e-06,  ..., -8.4937e-06,
         -6.9886e-06, -6.3628e-06]], device='cuda:0')
Loss: 0.9341642260551453


Running epoch 1, step 2011, batch 963
Sampled inputs[:2]: tensor([[   0, 5260,  365,  ..., 7242,  471,  391],
        [   0,  638, 1276,  ..., 1589, 2432,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3299e-05, -2.3879e-04, -2.1619e-04,  ...,  1.3358e-05,
          1.9108e-04, -1.3458e-04],
        [-5.3942e-06, -3.6433e-06,  3.7216e-06,  ..., -4.6641e-06,
         -3.7178e-06, -3.8780e-06],
        [-1.6987e-05, -1.2055e-05,  1.2666e-05,  ..., -1.4558e-05,
         -1.1697e-05, -1.2085e-05],
        [-1.2219e-05, -8.1509e-06,  8.7768e-06,  ..., -1.0595e-05,
         -8.4788e-06, -9.2089e-06],
        [-1.2949e-05, -9.5069e-06,  9.3430e-06,  ..., -1.1310e-05,
         -9.5069e-06, -8.5086e-06]], device='cuda:0')
Loss: 0.9702643752098083


Running epoch 1, step 2012, batch 964
Sampled inputs[:2]: tensor([[    0,  6418,   446,  ...,   413,    29,   413],
        [    0,    13, 26335,  ...,     5,  2570, 34403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7500e-05, -1.8700e-05, -2.1619e-04,  ..., -5.8568e-05,
          1.9823e-04, -7.5382e-05],
        [-6.6906e-06, -4.7162e-06,  4.4443e-06,  ..., -5.8562e-06,
         -4.9248e-06, -4.8205e-06],
        [-2.1160e-05, -1.5661e-05,  1.5169e-05,  ..., -1.8403e-05,
         -1.5602e-05, -1.5110e-05],
        [-1.5184e-05, -1.0580e-05,  1.0401e-05,  ..., -1.3366e-05,
         -1.1355e-05, -1.1489e-05],
        [-1.6242e-05, -1.2413e-05,  1.1280e-05,  ..., -1.4380e-05,
         -1.2666e-05, -1.0759e-05]], device='cuda:0')
Loss: 0.9762040376663208


Running epoch 1, step 2013, batch 965
Sampled inputs[:2]: tensor([[    0,   266, 15258,  ...,  2366,   368,  3988],
        [    0,   266,  1916,  ...,   292, 12946,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4047e-04, -1.1246e-04, -3.4239e-04,  ..., -2.5089e-04,
          2.4495e-04, -4.4751e-06],
        [-8.1211e-06, -5.6997e-06,  5.4948e-06,  ..., -7.0632e-06,
         -5.8338e-06, -5.7742e-06],
        [-2.5421e-05, -1.8775e-05,  1.8522e-05,  ..., -2.2024e-05,
         -1.8373e-05, -1.7986e-05],
        [-1.8209e-05, -1.2651e-05,  1.2726e-05,  ..., -1.5914e-05,
         -1.3277e-05, -1.3605e-05],
        [-1.9521e-05, -1.4946e-05,  1.3784e-05,  ..., -1.7241e-05,
         -1.4976e-05, -1.2815e-05]], device='cuda:0')
Loss: 0.9381678700447083


Running epoch 1, step 2014, batch 966
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0, 1416,  367,  ...,  555,  764,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5955e-04, -1.0333e-04, -3.8644e-04,  ..., -3.1332e-04,
          1.0017e-04, -5.9177e-05],
        [-9.4548e-06, -6.6683e-06,  6.5900e-06,  ..., -8.1360e-06,
         -6.7204e-06, -6.6757e-06],
        [-2.9773e-05, -2.2054e-05,  2.2292e-05,  ..., -2.5526e-05,
         -2.1264e-05, -2.0951e-05],
        [-2.1264e-05, -1.4812e-05,  1.5348e-05,  ..., -1.8343e-05,
         -1.5274e-05, -1.5765e-05],
        [-2.2635e-05, -1.7405e-05,  1.6406e-05,  ..., -1.9819e-05,
         -1.7211e-05, -1.4782e-05]], device='cuda:0')
Loss: 0.9470754265785217


Running epoch 1, step 2015, batch 967
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  1878,   328,   259],
        [    0, 14652,    12,  ..., 17330,   996,  3294]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6176e-05,  4.6795e-05, -4.0910e-04,  ..., -4.2707e-04,
         -1.2993e-04,  7.4518e-05],
        [-1.0908e-05, -7.5065e-06,  7.5288e-06,  ..., -9.3728e-06,
         -7.5996e-06, -7.7635e-06],
        [-3.4124e-05, -2.4691e-05,  2.5377e-05,  ..., -2.9162e-05,
         -2.3901e-05, -2.4140e-05],
        [-2.4319e-05, -1.6555e-05,  1.7464e-05,  ..., -2.0951e-05,
         -1.7129e-05, -1.8120e-05],
        [-2.6211e-05, -1.9595e-05,  1.8790e-05,  ..., -2.2858e-05,
         -1.9521e-05, -1.7226e-05]], device='cuda:0')
Loss: 0.8918805122375488
Graident accumulation at epoch 1, step 2015, batch 967
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.6602e-05, -9.9593e-05, -2.1628e-04,  ...,  9.0616e-06,
         -1.3895e-04, -2.2945e-05],
        [-1.0599e-05, -7.4149e-06,  7.4769e-06,  ..., -9.2654e-06,
         -7.3884e-06, -7.5006e-06],
        [ 1.2310e-05,  2.7349e-05, -8.9584e-06,  ...,  1.8653e-05,
          2.1001e-05,  1.3758e-06],
        [-4.5045e-06,  4.8865e-06,  4.1641e-06,  ..., -2.5117e-06,
          8.0793e-06, -5.8106e-06],
        [-2.5727e-05, -1.9682e-05,  1.8917e-05,  ..., -2.2664e-05,
         -1.9281e-05, -1.6691e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5731e-08, 5.6879e-08, 6.9006e-08,  ..., 2.8793e-08, 1.5910e-07,
         4.0610e-08],
        [9.6483e-11, 6.2400e-11, 3.0067e-11,  ..., 6.4691e-11, 3.3449e-11,
         3.6790e-11],
        [4.7876e-09, 3.5354e-09, 2.0043e-09,  ..., 3.8522e-09, 2.1222e-09,
         1.3846e-09],
        [1.3151e-09, 1.4746e-09, 5.7722e-10,  ..., 1.1572e-09, 9.8947e-10,
         5.0067e-10],
        [4.2064e-10, 2.3974e-10, 1.0364e-10,  ..., 3.1038e-10, 1.0723e-10,
         1.2383e-10]], device='cuda:0')
optimizer state dict: 252.0
lr: [7.630809080545365e-08, 7.630809080545365e-08]
scheduler_last_epoch: 252


Running epoch 1, step 2016, batch 968
Sampled inputs[:2]: tensor([[   0, 1590, 2140,  ...,  287, 5342, 1319],
        [   0,   13, 2497,  ...,  943,  259, 2646]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2330e-05,  7.2631e-05, -6.1039e-05,  ...,  7.0500e-05,
         -2.0573e-05, -1.5693e-05],
        [-1.2890e-06, -1.0133e-06,  9.5367e-07,  ..., -1.1325e-06,
         -9.6858e-07, -8.9034e-07],
        [-4.1425e-06, -3.3528e-06,  3.2932e-06,  ..., -3.6061e-06,
         -3.0398e-06, -2.8461e-06],
        [-2.9653e-06, -2.2948e-06,  2.2948e-06,  ..., -2.6077e-06,
         -2.2203e-06, -2.1905e-06],
        [-3.0696e-06, -2.5928e-06,  2.3693e-06,  ..., -2.7269e-06,
         -2.3991e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9632485508918762


Running epoch 1, step 2017, batch 969
Sampled inputs[:2]: tensor([[   0,    9,  292,  ...,  944,  278, 1758],
        [   0,  445,  749,  ...,  850, 1028,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1643e-04,  5.9503e-05, -3.2755e-04,  ...,  1.6107e-05,
         -9.9034e-05, -8.7436e-05],
        [-2.5630e-06, -1.9819e-06,  1.9670e-06,  ..., -2.2277e-06,
         -1.8030e-06, -1.7658e-06],
        [-8.3148e-06, -6.6608e-06,  6.7949e-06,  ..., -7.1973e-06,
         -5.7667e-06, -5.7071e-06],
        [-5.9605e-06, -4.5598e-06,  4.7982e-06,  ..., -5.1856e-06,
         -4.2021e-06, -4.3660e-06],
        [-5.9754e-06, -4.9919e-06,  4.7684e-06,  ..., -5.2750e-06,
         -4.4405e-06, -3.7849e-06]], device='cuda:0')
Loss: 0.9545139074325562


Running epoch 1, step 2018, batch 970
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,  2945,     9,   287],
        [    0,    12, 30621,  ...,   578,  3126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2433e-04,  3.0735e-04, -1.7021e-04,  ..., -3.6838e-05,
         -4.2580e-04, -1.4741e-04],
        [-3.8296e-06, -2.8238e-06,  2.9802e-06,  ..., -3.3379e-06,
         -2.6189e-06, -2.6599e-06],
        [ 5.1062e-05,  6.5233e-05, -2.7568e-05,  ...,  6.7967e-05,
          6.6236e-05,  1.4053e-05],
        [-8.8662e-06, -6.4522e-06,  7.2271e-06,  ..., -7.7337e-06,
         -6.0797e-06, -6.5267e-06],
        [-8.9109e-06, -7.1377e-06,  7.2122e-06,  ..., -7.8380e-06,
         -6.4522e-06, -5.6177e-06]], device='cuda:0')
Loss: 0.9601355195045471


Running epoch 1, step 2019, batch 971
Sampled inputs[:2]: tensor([[    0,   795,  3185,  ...,    14,  1671,   199],
        [    0,   471,    14,  ..., 27104,     9,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6240e-04,  2.9345e-04, -2.8975e-04,  ..., -5.4102e-05,
         -5.8565e-04, -1.6403e-04],
        [-5.1558e-06, -3.8445e-06,  4.0159e-06,  ..., -4.4554e-06,
         -3.5092e-06, -3.5800e-06],
        [ 4.6830e-05,  6.1865e-05, -2.4066e-05,  ...,  6.4406e-05,
          6.3375e-05,  1.1132e-05],
        [-1.1891e-05, -8.7619e-06,  9.6858e-06,  ..., -1.0282e-05,
         -8.1211e-06, -8.7321e-06],
        [ 8.6258e-05,  1.6162e-04, -1.0053e-04,  ...,  1.0054e-04,
          6.3333e-05,  7.0736e-05]], device='cuda:0')
Loss: 0.955784797668457


Running epoch 1, step 2020, batch 972
Sampled inputs[:2]: tensor([[    0,   298,   301,  ...,    13, 10308,  2129],
        [    0,  4878,   607,  ...,    14, 17331,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5607e-04,  3.4777e-04, -3.7430e-04,  ..., -1.9867e-04,
         -5.3816e-04, -1.9237e-04],
        [-6.6534e-06, -4.8503e-06,  4.9770e-06,  ..., -5.7742e-06,
         -4.5002e-06, -4.6901e-06],
        [ 4.2092e-05,  5.8512e-05, -2.0803e-05,  ...,  6.0234e-05,
          6.0156e-05,  7.6158e-06],
        [-1.5050e-05, -1.0893e-05,  1.1787e-05,  ..., -1.3083e-05,
         -1.0252e-05, -1.1146e-05],
        [ 8.2503e-05,  1.5893e-04, -9.8041e-05,  ...,  9.7177e-05,
          6.0636e-05,  6.8098e-05]], device='cuda:0')
Loss: 0.9506634473800659


Running epoch 1, step 2021, batch 973
Sampled inputs[:2]: tensor([[   0, 9017,  600,  ..., 6133, 1098,  352],
        [   0, 1340, 1049,  ..., 1441, 1211, 4165]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1303e-04,  3.6106e-04, -3.7334e-04,  ..., -2.8497e-04,
         -5.5036e-04, -1.9307e-04],
        [-7.9721e-06, -5.9158e-06,  6.0573e-06,  ..., -6.8992e-06,
         -5.4464e-06, -5.5581e-06],
        [ 3.7830e-05,  5.4936e-05, -1.7152e-05,  ...,  5.6583e-05,
          5.7086e-05,  4.7697e-06],
        [-1.8105e-05, -1.3337e-05,  1.4395e-05,  ..., -1.5676e-05,
         -1.2428e-05, -1.3277e-05],
        [ 7.9449e-05,  1.5625e-04, -9.5478e-05,  ...,  9.4495e-05,
          5.8267e-05,  6.6213e-05]], device='cuda:0')
Loss: 0.9859783053398132


Running epoch 1, step 2022, batch 974
Sampled inputs[:2]: tensor([[   0, 3019,  278,  ...,  365, 1770,   12],
        [   0,  377,  472,  ..., 9256, 3807, 5499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1511e-03,  5.2554e-04, -5.2163e-04,  ..., -1.1719e-04,
         -4.8187e-04, -1.5595e-04],
        [-9.2536e-06, -6.8098e-06,  7.1079e-06,  ..., -7.9870e-06,
         -6.3032e-06, -6.4410e-06],
        [ 3.3926e-05,  5.2135e-05, -1.3725e-05,  ...,  5.3334e-05,
          5.4523e-05,  2.1470e-06],
        [-2.0981e-05, -1.5303e-05,  1.6868e-05,  ..., -1.8120e-05,
         -1.4350e-05, -1.5378e-05],
        [ 7.6558e-05,  1.5405e-04, -9.3019e-05,  ...,  9.2021e-05,
          5.6210e-05,  6.4440e-05]], device='cuda:0')
Loss: 0.9127231240272522


Running epoch 1, step 2023, batch 975
Sampled inputs[:2]: tensor([[   0,  280, 5656,  ..., 7369, 2276,   12],
        [   0, 1268,  278,  ...,  461,  925,  630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4108e-03,  6.0686e-04, -6.5849e-04,  ..., -1.5008e-04,
         -5.8929e-04, -2.0134e-04],
        [-1.0602e-05, -7.7859e-06,  8.1360e-06,  ..., -9.1493e-06,
         -7.1675e-06, -7.3351e-06],
        [ 2.9783e-05,  4.8961e-05, -1.0327e-05,  ...,  4.9728e-05,
          5.1796e-05, -5.9477e-07],
        [-2.3976e-05, -1.7479e-05,  1.9267e-05,  ..., -2.0728e-05,
         -1.6302e-05, -1.7479e-05],
        [ 7.3503e-05,  1.5161e-04, -9.0576e-05,  ...,  8.9309e-05,
          5.4035e-05,  6.2570e-05]], device='cuda:0')
Loss: 0.9614728093147278
Graident accumulation at epoch 1, step 2023, batch 975
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7402e-04, -2.8947e-05, -2.6050e-04,  ..., -6.8526e-06,
         -1.8398e-04, -4.0785e-05],
        [-1.0599e-05, -7.4520e-06,  7.5428e-06,  ..., -9.2538e-06,
         -7.3663e-06, -7.4841e-06],
        [ 1.4058e-05,  2.9510e-05, -9.0953e-06,  ...,  2.1760e-05,
          2.4080e-05,  1.1787e-06],
        [-6.4517e-06,  2.6499e-06,  5.6744e-06,  ..., -4.3333e-06,
          5.6412e-06, -6.9774e-06],
        [-1.5804e-05, -2.5528e-06,  7.9673e-06,  ..., -1.1466e-05,
         -1.1949e-05, -8.7650e-06]], device='cuda:0')
optimizer state dict: tensor([[5.7665e-08, 5.7190e-08, 6.9371e-08,  ..., 2.8786e-08, 1.5929e-07,
         4.0609e-08],
        [9.6499e-11, 6.2398e-11, 3.0103e-11,  ..., 6.4710e-11, 3.3467e-11,
         3.6807e-11],
        [4.7837e-09, 3.5343e-09, 2.0024e-09,  ..., 3.8508e-09, 2.1227e-09,
         1.3832e-09],
        [1.3143e-09, 1.4734e-09, 5.7702e-10,  ..., 1.1565e-09, 9.8874e-10,
         5.0047e-10],
        [4.2562e-10, 2.6249e-10, 1.1175e-10,  ..., 3.1805e-10, 1.1005e-10,
         1.2763e-10]], device='cuda:0')
optimizer state dict: 253.0
lr: [6.182451364666886e-08, 6.182451364666886e-08]
scheduler_last_epoch: 253


Running epoch 1, step 2024, batch 976
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,  1098,   259,  ...,  6572,  1477,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0565e-04, -2.0471e-04, -3.2525e-04,  ...,  3.5153e-04,
          3.7774e-04, -1.1453e-04],
        [-1.4007e-06, -8.5309e-07,  1.0580e-06,  ..., -1.2219e-06,
         -9.6112e-07, -1.0356e-06],
        [-4.1127e-06, -2.6524e-06,  3.3677e-06,  ..., -3.5167e-06,
         -2.8014e-06, -2.9057e-06],
        [-3.0249e-06, -1.8328e-06,  2.3842e-06,  ..., -2.6375e-06,
         -2.1011e-06, -2.2948e-06],
        [-3.2336e-06, -2.1309e-06,  2.5630e-06,  ..., -2.7865e-06,
         -2.3246e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9666625261306763


Running epoch 1, step 2025, batch 977
Sampled inputs[:2]: tensor([[   0,  923,   13,  ...,  199,  677, 3826],
        [   0, 3951,   77,  ..., 7062,  278,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0565e-04, -3.7853e-04, -2.3251e-04,  ...,  3.0893e-04,
          4.5004e-04, -1.6096e-04],
        [-2.6748e-06, -1.9334e-06,  1.9297e-06,  ..., -2.3544e-06,
         -1.9521e-06, -1.9409e-06],
        [-8.1956e-06, -6.2734e-06,  6.4075e-06,  ..., -7.1377e-06,
         -6.0201e-06, -5.8264e-06],
        [-5.9307e-06, -4.3064e-06,  4.4405e-06,  ..., -5.2303e-06,
         -4.4107e-06, -4.4703e-06],
        [-6.3032e-06, -4.9323e-06,  4.7833e-06,  ..., -5.5581e-06,
         -4.8727e-06, -4.1574e-06]], device='cuda:0')
Loss: 0.9714899659156799


Running epoch 1, step 2026, batch 978
Sampled inputs[:2]: tensor([[    0,  1619,   938,  ...,   292, 10026, 14367],
        [    0,  3594,   950,  ...,  6517,   344, 15386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5710e-04, -5.8239e-04, -2.5779e-04,  ...,  3.0893e-04,
          6.5199e-04, -2.1134e-04],
        [-4.0010e-06, -2.8498e-06,  2.7828e-06,  ..., -3.5763e-06,
         -2.9579e-06, -2.9318e-06],
        [-1.2249e-05, -9.2089e-06,  9.2536e-06,  ..., -1.0818e-05,
         -9.0748e-06, -8.7619e-06],
        [-8.8364e-06, -6.2585e-06,  6.3777e-06,  ..., -7.9274e-06,
         -6.6459e-06, -6.7502e-06],
        [-9.5218e-06, -7.3463e-06,  6.9886e-06,  ..., -8.5235e-06,
         -7.4357e-06, -6.3181e-06]], device='cuda:0')
Loss: 0.9772680997848511


Running epoch 1, step 2027, batch 979
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0,   927,   259,  ...,   328,  9430,  2330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8090e-04, -5.6972e-04, -3.5854e-04,  ...,  2.3300e-04,
          3.8712e-04, -3.2790e-04],
        [-5.2378e-06, -3.7178e-06,  3.7812e-06,  ..., -4.6715e-06,
         -3.7923e-06, -3.8184e-06],
        [-1.6242e-05, -1.2070e-05,  1.2681e-05,  ..., -1.4260e-05,
         -1.1697e-05, -1.1519e-05],
        [-1.1757e-05, -8.2403e-06,  8.8662e-06,  ..., -1.0505e-05,
         -8.5980e-06, -8.9407e-06],
        [-1.2413e-05, -9.5069e-06,  9.4026e-06,  ..., -1.1072e-05,
         -9.4771e-06, -8.1360e-06]], device='cuda:0')
Loss: 0.9327775835990906


Running epoch 1, step 2028, batch 980
Sampled inputs[:2]: tensor([[   0,  287, 4579,  ...,  909,   12,  344],
        [   0, 4852,  266,  ..., 2523, 2080, 2632]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9801e-05, -7.1324e-04, -4.3759e-04,  ...,  3.0212e-04,
          3.0119e-04, -3.7427e-04],
        [-6.5491e-06, -4.6045e-06,  4.7795e-06,  ..., -5.8115e-06,
         -4.6529e-06, -4.7609e-06],
        [-2.0534e-05, -1.5020e-05,  1.6198e-05,  ..., -1.7911e-05,
         -1.4454e-05, -1.4514e-05],
        [-1.4767e-05, -1.0222e-05,  1.1280e-05,  ..., -1.3113e-05,
         -1.0550e-05, -1.1191e-05],
        [-1.5542e-05, -1.1742e-05,  1.1891e-05,  ..., -1.3784e-05,
         -1.1638e-05, -1.0133e-05]], device='cuda:0')
Loss: 0.9570083618164062


Running epoch 1, step 2029, batch 981
Sampled inputs[:2]: tensor([[   0,  368, 2035,  ...,  266, 1122,  587],
        [   0, 4263, 4865,  ..., 1878,  278, 4450]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7804e-04, -7.2494e-04, -3.8286e-04,  ...,  3.3180e-04,
          4.6981e-04, -2.9900e-04],
        [-7.9274e-06, -5.5656e-06,  5.5172e-06,  ..., -7.1004e-06,
         -5.6960e-06, -5.8487e-06],
        [-2.4855e-05, -1.8150e-05,  1.8820e-05,  ..., -2.1845e-05,
         -1.7703e-05, -1.7792e-05],
        [-1.7732e-05, -1.2264e-05,  1.2934e-05,  ..., -1.5900e-05,
         -1.2845e-05, -1.3620e-05],
        [-1.9014e-05, -1.4290e-05,  1.3962e-05,  ..., -1.6972e-05,
         -1.4365e-05, -1.2577e-05]], device='cuda:0')
Loss: 0.9466044306755066


Running epoch 1, step 2030, batch 982
Sampled inputs[:2]: tensor([[   0, 1607,   12,  ...,  895, 1503,  369],
        [   0, 2377,  360,  ...,  266, 4745,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5532e-05, -7.3619e-04, -4.7569e-04,  ...,  3.3366e-04,
          3.8412e-04, -2.4361e-04],
        [-9.2760e-06, -6.4336e-06,  6.6422e-06,  ..., -8.1435e-06,
         -6.4410e-06, -6.7055e-06],
        [-2.9147e-05, -2.1115e-05,  2.2605e-05,  ..., -2.5213e-05,
         -2.0117e-05, -2.0534e-05],
        [-2.0891e-05, -1.4290e-05,  1.5706e-05,  ..., -1.8358e-05,
         -1.4596e-05, -1.5751e-05],
        [-2.1994e-05, -1.6451e-05,  1.6510e-05,  ..., -1.9372e-05,
         -1.6183e-05, -1.4335e-05]], device='cuda:0')
Loss: 0.9689558744430542


Running epoch 1, step 2031, batch 983
Sampled inputs[:2]: tensor([[    0,   287,  6761,  ...,  1918, 33351,    12],
        [    0,  3699,  3058,  ...,   820,  5327,  8055]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3260e-04, -6.6337e-04, -4.0920e-04,  ...,  4.0222e-04,
          6.0374e-04, -1.8966e-04],
        [-1.0602e-05, -7.5512e-06,  7.3761e-06,  ..., -9.4697e-06,
         -7.6406e-06, -7.6964e-06],
        [-3.3230e-05, -2.4736e-05,  2.5079e-05,  ..., -2.9296e-05,
         -2.3842e-05, -2.3574e-05],
        [-2.3827e-05, -1.6779e-05,  1.7375e-05,  ..., -2.1338e-05,
         -1.7338e-05, -1.8075e-05],
        [-2.5481e-05, -1.9580e-05,  1.8582e-05,  ..., -2.2858e-05,
         -1.9446e-05, -1.6779e-05]], device='cuda:0')
Loss: 1.0088552236557007
Graident accumulation at epoch 1, step 2031, batch 983
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2336e-04, -9.2390e-05, -2.7537e-04,  ...,  3.4054e-05,
         -1.0521e-04, -5.5673e-05],
        [-1.0600e-05, -7.4619e-06,  7.5261e-06,  ..., -9.2753e-06,
         -7.3937e-06, -7.5053e-06],
        [ 9.3288e-06,  2.4085e-05, -5.6779e-06,  ...,  1.6655e-05,
          1.9288e-05, -1.2965e-06],
        [-8.1892e-06,  7.0706e-07,  6.8444e-06,  ..., -6.0338e-06,
          3.3433e-06, -8.0872e-06],
        [-1.6771e-05, -4.2555e-06,  9.0288e-06,  ..., -1.2606e-05,
         -1.2699e-05, -9.5664e-06]], device='cuda:0')
optimizer state dict: tensor([[5.7718e-08, 5.7573e-08, 6.9469e-08,  ..., 2.8919e-08, 1.5949e-07,
         4.0605e-08],
        [9.6515e-11, 6.2393e-11, 3.0127e-11,  ..., 6.4735e-11, 3.3492e-11,
         3.6829e-11],
        [4.7800e-09, 3.5314e-09, 2.0010e-09,  ..., 3.8478e-09, 2.1212e-09,
         1.3824e-09],
        [1.3136e-09, 1.4722e-09, 5.7674e-10,  ..., 1.1558e-09, 9.8806e-10,
         5.0030e-10],
        [4.2584e-10, 2.6261e-10, 1.1198e-10,  ..., 3.1825e-10, 1.1032e-10,
         1.2778e-10]], device='cuda:0')
optimizer state dict: 254.0
lr: [4.885957645375916e-08, 4.885957645375916e-08]
scheduler_last_epoch: 254


Running epoch 1, step 2032, batch 984
Sampled inputs[:2]: tensor([[   0,  221, 6872,  ...,  806,  518,  266],
        [   0,  396,  298,  ...,   52, 5065,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5908e-04, -1.1831e-04,  1.8736e-04,  ...,  5.6586e-05,
          2.8172e-04,  7.9761e-05],
        [-1.4007e-06, -8.4564e-07,  9.6858e-07,  ..., -1.2442e-06,
         -8.3074e-07, -1.0580e-06],
        [-4.2617e-06, -2.6524e-06,  3.1888e-06,  ..., -3.6806e-06,
         -2.5183e-06, -3.0994e-06],
        [-2.9951e-06, -1.7583e-06,  2.1905e-06,  ..., -2.6524e-06,
         -1.7732e-06, -2.3246e-06],
        [-3.3081e-06, -2.1309e-06,  2.3991e-06,  ..., -2.8908e-06,
         -2.0862e-06, -2.2203e-06]], device='cuda:0')
Loss: 0.9282995462417603


Running epoch 1, step 2033, batch 985
Sampled inputs[:2]: tensor([[    0,  1890,   278,  ...,   578,    72,   815],
        [    0,   591, 18622,  ...,   955,  6118,  9191]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2368e-05, -2.4407e-04,  1.8685e-05,  ...,  6.2543e-05,
          2.2969e-04, -8.8303e-06],
        [-2.7940e-06, -1.7099e-06,  1.9521e-06,  ..., -2.3469e-06,
         -1.6540e-06, -1.9707e-06],
        [-8.5533e-06, -5.4687e-06,  6.4820e-06,  ..., -7.0632e-06,
         -5.0813e-06, -5.9158e-06],
        [-6.1542e-06, -3.6955e-06,  4.5449e-06,  ..., -5.1558e-06,
         -3.6508e-06, -4.5002e-06],
        [-6.4373e-06, -4.2915e-06,  4.7237e-06,  ..., -5.4240e-06,
         -4.1127e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9334179759025574


Running epoch 1, step 2034, batch 986
Sampled inputs[:2]: tensor([[    0,  5440,    13,  ...,  1878,   342,  2060],
        [    0,   381, 13565,  ...,     9,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2458e-04, -2.8711e-04,  4.2781e-05,  ...,  1.9102e-04,
          2.9276e-04,  9.3782e-05],
        [-4.0606e-06, -2.7232e-06,  2.7791e-06,  ..., -3.5167e-06,
         -2.6822e-06, -2.9318e-06],
        [-1.2577e-05, -8.7619e-06,  9.3281e-06,  ..., -1.0714e-05,
         -8.2701e-06, -8.8960e-06],
        [-9.0301e-06, -5.9754e-06,  6.4671e-06,  ..., -7.8231e-06,
         -6.0052e-06, -6.7800e-06],
        [-9.6112e-06, -6.9588e-06,  6.9141e-06,  ..., -8.3297e-06,
         -6.7353e-06, -6.3032e-06]], device='cuda:0')
Loss: 0.9540137648582458


Running epoch 1, step 2035, batch 987
Sampled inputs[:2]: tensor([[   0,  607,  443,  ...,  259, 2646, 1597],
        [   0,   13, 1529,  ..., 8197, 2700, 9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0436e-04, -3.5078e-04, -2.4335e-05,  ...,  1.5235e-04,
          1.0182e-04,  1.0551e-04],
        [-5.3719e-06, -3.6545e-06,  3.7625e-06,  ..., -4.6492e-06,
         -3.5688e-06, -3.8147e-06],
        [-1.6600e-05, -1.1742e-05,  1.2562e-05,  ..., -1.4156e-05,
         -1.0997e-05, -1.1578e-05],
        [-1.1936e-05, -8.0019e-06,  8.7470e-06,  ..., -1.0327e-05,
         -7.9721e-06, -8.8215e-06],
        [-1.2621e-05, -9.2834e-06,  9.2834e-06,  ..., -1.0967e-05,
         -8.9258e-06, -8.1658e-06]], device='cuda:0')
Loss: 0.9453423023223877


Running epoch 1, step 2036, batch 988
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0,  516,  689,  ...,  278,  516, 6137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6661e-04, -1.9573e-04,  2.6037e-05,  ...,  4.0486e-05,
         -6.2095e-05,  8.7873e-06],
        [-6.8024e-06, -4.4815e-06,  4.6603e-06,  ..., -5.8711e-06,
         -4.4368e-06, -4.9025e-06],
        [-2.0862e-05, -1.4335e-05,  1.5542e-05,  ..., -1.7747e-05,
         -1.3649e-05, -1.4707e-05],
        [-1.5035e-05, -9.7603e-06,  1.0818e-05,  ..., -1.2994e-05,
         -9.9093e-06, -1.1221e-05],
        [-1.6034e-05, -1.1384e-05,  1.1519e-05,  ..., -1.3888e-05,
         -1.1176e-05, -1.0476e-05]], device='cuda:0')
Loss: 0.8766270279884338


Running epoch 1, step 2037, batch 989
Sampled inputs[:2]: tensor([[   0,  271, 4787,  ...,  292,  494,  221],
        [   0, 1555,   12,  ...,  809,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3228e-04, -6.5749e-05,  7.1411e-07,  ...,  2.9236e-05,
         -5.6351e-05, -4.5651e-06],
        [-8.1435e-06, -5.5768e-06,  5.7109e-06,  ..., -6.9886e-06,
         -5.3197e-06, -5.7779e-06],
        [-2.5243e-05, -1.8060e-05,  1.9163e-05,  ..., -2.1398e-05,
         -1.6570e-05, -1.7598e-05],
        [-1.8105e-05, -1.2264e-05,  1.3337e-05,  ..., -1.5542e-05,
         -1.1936e-05, -1.3322e-05],
        [-1.9118e-05, -1.4096e-05,  1.4007e-05,  ..., -1.6510e-05,
         -1.3396e-05, -1.2361e-05]], device='cuda:0')
Loss: 0.9759323596954346


Running epoch 1, step 2038, batch 990
Sampled inputs[:2]: tensor([[   0, 1067,  408,  ..., 4657, 1016,  271],
        [   0, 4665,  909,  ..., 3607,  259, 1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0127e-04, -3.4246e-05,  5.9386e-05,  ..., -1.5964e-05,
         -5.2020e-05,  5.8645e-05],
        [-9.4995e-06, -6.5677e-06,  6.6608e-06,  ..., -8.1286e-06,
         -6.3404e-06, -6.7055e-06],
        [-2.9415e-05, -2.1324e-05,  2.2337e-05,  ..., -2.4945e-05,
         -1.9819e-05, -2.0489e-05],
        [-2.1160e-05, -1.4499e-05,  1.5557e-05,  ..., -1.8150e-05,
         -1.4320e-05, -1.5542e-05],
        [-2.2307e-05, -1.6674e-05,  1.6361e-05,  ..., -1.9282e-05,
         -1.6019e-05, -1.4417e-05]], device='cuda:0')
Loss: 0.9646564722061157


Running epoch 1, step 2039, batch 991
Sampled inputs[:2]: tensor([[    0,  1526,   422,  ..., 22454,   409, 31482],
        [    0,  4667,   446,  ...,  1868, 16028,   669]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3087e-05, -2.2106e-04, -1.1702e-04,  ...,  1.0275e-05,
         -2.6012e-04,  1.2630e-06],
        [-1.0885e-05, -7.4878e-06,  7.6815e-06,  ..., -9.3728e-06,
         -7.3537e-06, -7.6890e-06],
        [-3.3796e-05, -2.4378e-05,  2.5779e-05,  ..., -2.8878e-05,
         -2.3007e-05, -2.3633e-05],
        [-2.4170e-05, -1.6451e-05,  1.7852e-05,  ..., -2.0862e-05,
         -1.6496e-05, -1.7777e-05],
        [-2.5630e-05, -1.9073e-05,  1.8880e-05,  ..., -2.2322e-05,
         -1.8582e-05, -1.6637e-05]], device='cuda:0')
Loss: 0.9793330430984497
Graident accumulation at epoch 1, step 2039, batch 991
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0871e-04, -1.0526e-04, -2.5954e-04,  ...,  3.1676e-05,
         -1.2070e-04, -4.9979e-05],
        [-1.0628e-05, -7.4645e-06,  7.5417e-06,  ..., -9.2851e-06,
         -7.3897e-06, -7.5237e-06],
        [ 5.0164e-06,  1.9239e-05, -2.5322e-06,  ...,  1.2101e-05,
          1.5058e-05, -3.5302e-06],
        [-9.7873e-06, -1.0087e-06,  7.9451e-06,  ..., -7.5166e-06,
          1.3594e-06, -9.0562e-06],
        [-1.7657e-05, -5.7373e-06,  1.0014e-05,  ..., -1.3577e-05,
         -1.3287e-05, -1.0273e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7661e-08, 5.7564e-08, 6.9413e-08,  ..., 2.8890e-08, 1.5940e-07,
         4.0564e-08],
        [9.6537e-11, 6.2386e-11, 3.0156e-11,  ..., 6.4758e-11, 3.3513e-11,
         3.6852e-11],
        [4.7764e-09, 3.5284e-09, 1.9997e-09,  ..., 3.8448e-09, 2.1196e-09,
         1.3816e-09],
        [1.3129e-09, 1.4710e-09, 5.7648e-10,  ..., 1.1550e-09, 9.8734e-10,
         5.0011e-10],
        [4.2608e-10, 2.6271e-10, 1.1222e-10,  ..., 3.1843e-10, 1.1055e-10,
         1.2793e-10]], device='cuda:0')
optimizer state dict: 255.0
lr: [3.741526038229748e-08, 3.741526038229748e-08]
scheduler_last_epoch: 255


Running epoch 1, step 2040, batch 992
Sampled inputs[:2]: tensor([[    0,    13,  8982,  ...,   462,   221,   494],
        [    0,  9342,   600,  ...,   199, 12095,   291]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0544e-04, -6.6070e-05,  2.6313e-05,  ..., -2.3952e-05,
         -3.0279e-05,  1.8889e-04],
        [-1.2293e-06, -9.1270e-07,  1.0431e-06,  ..., -1.0580e-06,
         -8.0094e-07, -8.8289e-07],
        [-3.8147e-06, -2.9057e-06,  3.5018e-06,  ..., -3.2336e-06,
         -2.4885e-06, -2.6673e-06],
        [-2.8163e-06, -2.0415e-06,  2.5332e-06,  ..., -2.4140e-06,
         -1.8254e-06, -2.1011e-06],
        [-2.6971e-06, -2.1607e-06,  2.3991e-06,  ..., -2.3544e-06,
         -1.9222e-06, -1.7136e-06]], device='cuda:0')
Loss: 0.9069238305091858


Running epoch 1, step 2041, batch 993
Sampled inputs[:2]: tensor([[    0,   275,  1184,  ...,   328, 46278,  2117],
        [    0,   344,  8133,  ...,   278,  1603,   674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3238e-06, -7.5932e-05, -2.4787e-04,  ..., -1.2630e-04,
          1.1043e-04,  2.3596e-04],
        [-2.5630e-06, -1.8887e-06,  1.9483e-06,  ..., -2.2873e-06,
         -1.7695e-06, -1.8254e-06],
        [-7.8678e-06, -6.0499e-06,  6.4820e-06,  ..., -6.9439e-06,
         -5.4687e-06, -5.4836e-06],
        [-5.7966e-06, -4.1872e-06,  4.6343e-06,  ..., -5.1558e-06,
         -4.0159e-06, -4.3213e-06],
        [-5.8413e-06, -4.7088e-06,  4.6790e-06,  ..., -5.2899e-06,
         -4.3809e-06, -3.7402e-06]], device='cuda:0')
Loss: 0.9401530623435974


Running epoch 1, step 2042, batch 994
Sampled inputs[:2]: tensor([[   0,  342,  726,  ...,   12,  895,  367],
        [   0, 9582, 3645,  ..., 1027,   12,  461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5669e-06, -1.5225e-04, -2.7099e-04,  ..., -9.6628e-05,
          1.0141e-04,  1.1810e-04],
        [-3.8147e-06, -2.8498e-06,  2.9616e-06,  ..., -3.3975e-06,
         -2.7046e-06, -2.6748e-06],
        [-1.1772e-05, -9.1046e-06,  9.8497e-06,  ..., -1.0341e-05,
         -8.3447e-06, -8.0317e-06],
        [-8.5831e-06, -6.2883e-06,  7.0035e-06,  ..., -7.6294e-06,
         -6.1169e-06, -6.3032e-06],
        [-8.8066e-06, -7.1228e-06,  7.1824e-06,  ..., -7.9125e-06,
         -6.7204e-06, -5.5060e-06]], device='cuda:0')
Loss: 0.9429124593734741


Running epoch 1, step 2043, batch 995
Sampled inputs[:2]: tensor([[   0, 2700, 5221,  ...,  298,  259,  298],
        [   0,  259, 5918,  ...,  508, 3433, 1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6719e-04, -7.3410e-05,  5.5076e-05,  ..., -2.6008e-04,
          2.1184e-04,  1.1913e-04],
        [-5.1484e-06, -3.7588e-06,  3.6880e-06,  ..., -4.6268e-06,
         -3.7402e-06, -3.7849e-06],
        [-1.5855e-05, -1.2010e-05,  1.2308e-05,  ..., -1.4022e-05,
         -1.1519e-05, -1.1310e-05],
        [-1.1489e-05, -8.2403e-06,  8.6129e-06,  ..., -1.0327e-05,
         -8.4117e-06, -8.7619e-06],
        [-1.2144e-05, -9.4920e-06,  9.1344e-06,  ..., -1.0967e-05,
         -9.4175e-06, -8.0243e-06]], device='cuda:0')
Loss: 0.9511078596115112


Running epoch 1, step 2044, batch 996
Sampled inputs[:2]: tensor([[    0,    14,  2729,  ...,   266,  1659, 14362],
        [    0,   266,  1634,  ...,   310,  1372,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7525e-04, -2.0350e-04, -1.1201e-04,  ..., -4.0206e-05,
          4.5318e-04,  1.6723e-05],
        [-6.4373e-06, -4.6417e-06,  4.6715e-06,  ..., -5.7593e-06,
         -4.6268e-06, -4.7162e-06],
        [-2.0027e-05, -1.4901e-05,  1.5765e-05,  ..., -1.7598e-05,
         -1.4350e-05, -1.4216e-05],
        [-1.4469e-05, -1.0237e-05,  1.1027e-05,  ..., -1.2934e-05,
         -1.0468e-05, -1.0997e-05],
        [-1.5199e-05, -1.1683e-05,  1.1578e-05,  ..., -1.3635e-05,
         -1.1638e-05, -9.9614e-06]], device='cuda:0')
Loss: 0.9443128108978271


Running epoch 1, step 2045, batch 997
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,   278,   269, 10376],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9335e-04, -2.3066e-04, -1.2546e-04,  ..., -5.9969e-05,
          4.5589e-04, -1.0519e-05],
        [-7.8157e-06, -5.5581e-06,  5.7891e-06,  ..., -6.8918e-06,
         -5.4762e-06, -5.6699e-06],
        [-2.4527e-05, -1.8060e-05,  1.9640e-05,  ..., -2.1309e-05,
         -1.7181e-05, -1.7375e-05],
        [-1.7643e-05, -1.2308e-05,  1.3709e-05,  ..., -1.5527e-05,
         -1.2420e-05, -1.3277e-05],
        [-1.8433e-05, -1.4037e-05,  1.4260e-05,  ..., -1.6376e-05,
         -1.3828e-05, -1.2077e-05]], device='cuda:0')
Loss: 0.970848023891449


Running epoch 1, step 2046, batch 998
Sampled inputs[:2]: tensor([[    0,   328, 16219,  ..., 14559,   351,   587],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0505e-04, -1.5370e-04, -9.3620e-05,  ..., -2.3340e-05,
          6.6413e-04,  2.7546e-05],
        [-9.1493e-06, -6.5863e-06,  6.6161e-06,  ..., -8.1211e-06,
         -6.5342e-06, -6.6049e-06],
        [-2.8610e-05, -2.1413e-05,  2.2426e-05,  ..., -2.5064e-05,
         -2.0459e-05, -2.0251e-05],
        [-2.0668e-05, -1.4633e-05,  1.5661e-05,  ..., -1.8343e-05,
         -1.4894e-05, -1.5557e-05],
        [-2.1622e-05, -1.6749e-05,  1.6376e-05,  ..., -1.9357e-05,
         -1.6525e-05, -1.4134e-05]], device='cuda:0')
Loss: 0.9688006043434143


Running epoch 1, step 2047, batch 999
Sampled inputs[:2]: tensor([[    0,   221,   451,  ...,   741, 25712,   950],
        [    0,   199,   677,  ...,  2792,   271,  2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6024e-04, -1.0965e-04, -3.1927e-04,  ..., -1.3545e-04,
          2.9734e-04, -1.3589e-04],
        [-1.0476e-05, -7.3649e-06,  7.4916e-06,  ..., -9.3430e-06,
         -7.4543e-06, -7.6108e-06],
        [-3.2455e-05, -2.3767e-05,  2.5257e-05,  ..., -2.8476e-05,
         -2.3067e-05, -2.2978e-05],
        [-2.3559e-05, -1.6294e-05,  1.7673e-05,  ..., -2.1026e-05,
         -1.6920e-05, -1.7837e-05],
        [-2.4676e-05, -1.8649e-05,  1.8567e-05,  ..., -2.2084e-05,
         -1.8701e-05, -1.6131e-05]], device='cuda:0')
Loss: 0.8956307172775269
Graident accumulation at epoch 1, step 2047, batch 999
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.1819e-05, -1.0570e-04, -2.6551e-04,  ...,  1.4964e-05,
         -7.8898e-05, -5.8570e-05],
        [-1.0613e-05, -7.4545e-06,  7.5367e-06,  ..., -9.2909e-06,
         -7.3962e-06, -7.5324e-06],
        [ 1.2692e-06,  1.4938e-05,  2.4678e-07,  ...,  8.0437e-06,
          1.1246e-05, -5.4749e-06],
        [-1.1164e-05, -2.5373e-06,  8.9179e-06,  ..., -8.8675e-06,
         -4.6856e-07, -9.9342e-06],
        [-1.8359e-05, -7.0285e-06,  1.0869e-05,  ..., -1.4428e-05,
         -1.3829e-05, -1.0859e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7671e-08, 5.7519e-08, 6.9446e-08,  ..., 2.8880e-08, 1.5933e-07,
         4.0542e-08],
        [9.6550e-11, 6.2378e-11, 3.0182e-11,  ..., 6.4781e-11, 3.3535e-11,
         3.6873e-11],
        [4.7726e-09, 3.5255e-09, 1.9983e-09,  ..., 3.8418e-09, 2.1180e-09,
         1.3807e-09],
        [1.3121e-09, 1.4698e-09, 5.7622e-10,  ..., 1.1543e-09, 9.8664e-10,
         4.9993e-10],
        [4.2626e-10, 2.6280e-10, 1.1246e-10,  ..., 3.1860e-10, 1.1079e-10,
         1.2806e-10]], device='cuda:0')
optimizer state dict: 256.0
lr: [2.7493314223681067e-08, 2.7493314223681067e-08]
scheduler_last_epoch: 256


Running epoch 1, step 2048, batch 1000
Sampled inputs[:2]: tensor([[    0,    14,   747,  ..., 12545,    12, 15209],
        [    0,  6532,  6984,  ...,   271,  8212, 14409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1835e-06, -9.7282e-06,  4.3833e-05,  ...,  8.4767e-05,
         -2.7448e-05, -2.3177e-05],
        [-1.3486e-06, -8.7544e-07,  9.2387e-07,  ..., -1.1846e-06,
         -9.2760e-07, -1.0729e-06],
        [-4.2319e-06, -2.8759e-06,  3.1590e-06,  ..., -3.6359e-06,
         -2.8610e-06, -3.2634e-06],
        [-2.9206e-06, -1.8626e-06,  2.0862e-06,  ..., -2.5630e-06,
         -2.0117e-06, -2.4140e-06],
        [-3.2037e-06, -2.2501e-06,  2.3246e-06,  ..., -2.8312e-06,
         -2.3395e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9416602253913879


Running epoch 1, step 2049, batch 1001
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,  3164,    12,  ...,   984,   344,  3993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0308e-05, -1.1448e-04, -4.5068e-05,  ..., -1.3356e-05,
         -2.9140e-05, -1.0747e-04],
        [-2.7344e-06, -1.8030e-06,  1.9595e-06,  ..., -2.3320e-06,
         -1.8179e-06, -2.0005e-06],
        [-8.7023e-06, -6.0052e-06,  6.7204e-06,  ..., -7.3016e-06,
         -5.7220e-06, -6.1989e-06],
        [-6.1244e-06, -3.9637e-06,  4.5896e-06,  ..., -5.2154e-06,
         -4.0829e-06, -4.6790e-06],
        [-6.4820e-06, -4.6492e-06,  4.8727e-06,  ..., -5.5879e-06,
         -4.5896e-06, -4.2915e-06]], device='cuda:0')
Loss: 0.9619063138961792


Running epoch 1, step 2050, batch 1002
Sampled inputs[:2]: tensor([[    0, 14700,   717,  ..., 10570,   292,   221],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8666e-05, -2.4109e-04, -1.6036e-04,  ...,  1.0628e-04,
          2.9849e-05, -7.6854e-05],
        [-4.0829e-06, -2.6636e-06,  2.6375e-06,  ..., -3.6210e-06,
         -2.8610e-06, -3.1181e-06],
        [-1.2547e-05, -8.6129e-06,  9.0152e-06,  ..., -1.0863e-05,
         -8.6725e-06, -9.1940e-06],
        [-9.0152e-06, -5.7966e-06,  6.1020e-06,  ..., -8.0168e-06,
         -6.4075e-06, -7.1675e-06],
        [-9.7454e-06, -6.8396e-06,  6.7726e-06,  ..., -8.5980e-06,
         -7.1526e-06, -6.6161e-06]], device='cuda:0')
Loss: 0.8950119614601135


Running epoch 1, step 2051, batch 1003
Sampled inputs[:2]: tensor([[   0, 5689,  271,  ...,  352, 9985, 3260],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1513e-04, -1.1292e-04, -1.8900e-04,  ...,  2.2534e-04,
         -1.3994e-05, -1.4469e-04],
        [-5.4017e-06, -3.6545e-06,  3.6135e-06,  ..., -4.7833e-06,
         -3.7998e-06, -4.0531e-06],
        [-1.6719e-05, -1.1817e-05,  1.2353e-05,  ..., -1.4484e-05,
         -1.1623e-05, -1.2085e-05],
        [-1.2025e-05, -8.0019e-06,  8.4564e-06,  ..., -1.0654e-05,
         -8.5533e-06, -9.4026e-06],
        [-1.2875e-05, -9.3281e-06,  9.2015e-06,  ..., -1.1355e-05,
         -9.4920e-06, -8.5980e-06]], device='cuda:0')
Loss: 0.9674042463302612


Running epoch 1, step 2052, batch 1004
Sampled inputs[:2]: tensor([[   0, 3646, 1340,  ...,   13, 7800, 2872],
        [   0, 1594,  586,  ...,   13,  701,  308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2854e-04, -1.1937e-04, -2.2081e-05,  ...,  1.4986e-04,
          1.5824e-04, -2.1084e-04],
        [-6.7800e-06, -4.6827e-06,  4.5560e-06,  ..., -6.0424e-06,
         -4.8429e-06, -5.0589e-06],
        [-2.0891e-05, -1.5035e-05,  1.5408e-05,  ..., -1.8254e-05,
         -1.4767e-05, -1.5095e-05],
        [-1.4991e-05, -1.0177e-05,  1.0557e-05,  ..., -1.3351e-05,
         -1.0788e-05, -1.1653e-05],
        [-1.6212e-05, -1.1966e-05,  1.1586e-05,  ..., -1.4409e-05,
         -1.2144e-05, -1.0833e-05]], device='cuda:0')
Loss: 0.9608457088470459


Running epoch 1, step 2053, batch 1005
Sampled inputs[:2]: tensor([[    0,  1192, 11929,  ...,   266,  1551,  1860],
        [    0,  9029,   634,  ...,  1424,  6872,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8709e-04, -6.1463e-05, -1.2218e-04,  ...,  2.3475e-04,
         -9.9645e-05, -1.2054e-04],
        [-8.1733e-06, -5.5917e-06,  5.4985e-06,  ..., -7.2867e-06,
         -5.7779e-06, -6.1467e-06],
        [-2.5302e-05, -1.8060e-05,  1.8641e-05,  ..., -2.2128e-05,
         -1.7732e-05, -1.8448e-05],
        [-1.7986e-05, -1.2115e-05,  1.2644e-05,  ..., -1.6034e-05,
         -1.2830e-05, -1.4052e-05],
        [-1.9699e-05, -1.4380e-05,  1.4044e-05,  ..., -1.7509e-05,
         -1.4603e-05, -1.3292e-05]], device='cuda:0')
Loss: 0.9351177215576172


Running epoch 1, step 2054, batch 1006
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0,   292,   263,  ...,   342,  4575,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4916e-04, -9.8241e-05, -1.5145e-04,  ...,  1.6112e-04,
          7.1287e-05, -1.5429e-04],
        [-9.4846e-06, -6.6347e-06,  6.4448e-06,  ..., -8.4415e-06,
         -6.7838e-06, -7.0073e-06],
        [-2.9355e-05, -2.1443e-05,  2.1815e-05,  ..., -2.5690e-05,
         -2.0832e-05, -2.1070e-05],
        [-2.0936e-05, -1.4454e-05,  1.4879e-05,  ..., -1.8671e-05,
         -1.5154e-05, -1.6153e-05],
        [-2.2724e-05, -1.7002e-05,  1.6354e-05,  ..., -2.0206e-05,
         -1.7032e-05, -1.5080e-05]], device='cuda:0')
Loss: 0.9951459169387817


Running epoch 1, step 2055, batch 1007
Sampled inputs[:2]: tensor([[    0, 10705,   401,  ...,   768,  2392,   368],
        [    0,    12,  9328,  ...,    20,   408,   790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3964e-04, -2.3001e-04, -1.9633e-04,  ...,  1.8899e-04,
          3.2557e-04, -2.0344e-04],
        [-1.0893e-05, -7.5884e-06,  7.3127e-06,  ..., -9.7230e-06,
         -7.8343e-06, -8.0727e-06],
        [-3.3617e-05, -2.4498e-05,  2.4706e-05,  ..., -2.9534e-05,
         -2.4021e-05, -2.4244e-05],
        [-2.3901e-05, -1.6451e-05,  1.6779e-05,  ..., -2.1383e-05,
         -1.7390e-05, -1.8477e-05],
        [-2.6166e-05, -1.9506e-05,  1.8604e-05,  ..., -2.3350e-05,
         -1.9729e-05, -1.7449e-05]], device='cuda:0')
Loss: 0.9605101943016052
Graident accumulation at epoch 1, step 2055, batch 1007
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0673e-05, -1.1813e-04, -2.5859e-04,  ...,  3.2366e-05,
         -3.8452e-05, -7.3056e-05],
        [-1.0641e-05, -7.4679e-06,  7.5143e-06,  ..., -9.3341e-06,
         -7.4400e-06, -7.5864e-06],
        [-2.2194e-06,  1.0995e-05,  2.6927e-06,  ...,  4.2859e-06,
          7.7192e-06, -7.3518e-06],
        [-1.2438e-05, -3.9287e-06,  9.7040e-06,  ..., -1.0119e-05,
         -2.1607e-06, -1.0789e-05],
        [-1.9140e-05, -8.2762e-06,  1.1643e-05,  ..., -1.5320e-05,
         -1.4419e-05, -1.1518e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7905e-08, 5.7514e-08, 6.9415e-08,  ..., 2.8887e-08, 1.5928e-07,
         4.0543e-08],
        [9.6572e-11, 6.2373e-11, 3.0205e-11,  ..., 6.4811e-11, 3.3563e-11,
         3.6901e-11],
        [4.7690e-09, 3.5225e-09, 1.9969e-09,  ..., 3.8388e-09, 2.1165e-09,
         1.3800e-09],
        [1.3114e-09, 1.4686e-09, 5.7592e-10,  ..., 1.1536e-09, 9.8595e-10,
         4.9977e-10],
        [4.2652e-10, 2.6292e-10, 1.1269e-10,  ..., 3.1883e-10, 1.1107e-10,
         1.2824e-10]], device='cuda:0')
optimizer state dict: 257.0
lr: [1.9095254137893038e-08, 1.9095254137893038e-08]
scheduler_last_epoch: 257


Running epoch 1, step 2056, batch 1008
Sampled inputs[:2]: tensor([[    0, 10288,   300,  ...,  5365,    12,  3539],
        [    0,   462,  9202,  ...,    15,  3256,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5121e-05, -3.6347e-05,  8.4462e-05,  ...,  1.0628e-04,
         -5.9084e-05,  8.1397e-05],
        [-1.3560e-06, -9.4622e-07,  1.0952e-06,  ..., -1.1101e-06,
         -8.0466e-07, -9.0525e-07],
        [-4.4405e-06, -3.2485e-06,  3.7998e-06,  ..., -3.6657e-06,
         -2.6524e-06, -2.9951e-06],
        [-3.0994e-06, -2.1458e-06,  2.6375e-06,  ..., -2.5332e-06,
         -1.8328e-06, -2.1756e-06],
        [-2.9653e-06, -2.2650e-06,  2.4587e-06,  ..., -2.5034e-06,
         -1.9073e-06, -1.8477e-06]], device='cuda:0')
Loss: 0.9435215592384338


Running epoch 1, step 2057, batch 1009
Sampled inputs[:2]: tensor([[    0,   300,  5201,  ...,  1997,  7423,   417],
        [    0,  7779,    12,  ...,  1380, 10199,  1086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6718e-04, -1.2557e-04, -4.2553e-05,  ...,  1.4021e-04,
         -2.1106e-04,  1.7364e-04],
        [-2.6301e-06, -1.8440e-06,  2.2277e-06,  ..., -2.1458e-06,
         -1.5721e-06, -1.7732e-06],
        [-8.4341e-06, -6.1542e-06,  7.5847e-06,  ..., -6.8992e-06,
         -5.0664e-06, -5.6624e-06],
        [-6.0201e-06, -4.1574e-06,  5.3793e-06,  ..., -4.9025e-06,
         -3.5837e-06, -4.2766e-06],
        [-5.7518e-06, -4.4107e-06,  5.0068e-06,  ..., -4.8131e-06,
         -3.7476e-06, -3.5465e-06]], device='cuda:0')
Loss: 0.9635118246078491


Running epoch 1, step 2058, batch 1010
Sampled inputs[:2]: tensor([[    0,   949, 11135,  ...,   278,   772,    13],
        [    0,   298,   696,  ...,  3502,   287,  1047]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3346e-04, -1.4860e-04, -1.3155e-05,  ..., -4.4769e-05,
         -2.9082e-04,  2.3394e-04],
        [-4.0308e-06, -2.7828e-06,  3.2261e-06,  ..., -3.3677e-06,
         -2.4959e-06, -2.7865e-06],
        [-1.2904e-05, -9.3281e-06,  1.0982e-05,  ..., -1.0774e-05,
         -8.0466e-06, -8.9258e-06],
        [-9.1493e-06, -6.2436e-06,  7.7039e-06,  ..., -7.6443e-06,
         -5.6550e-06, -6.6012e-06],
        [-9.1642e-06, -6.8992e-06,  7.4655e-06,  ..., -7.8529e-06,
         -6.1765e-06, -5.8860e-06]], device='cuda:0')
Loss: 0.9781034588813782


Running epoch 1, step 2059, batch 1011
Sampled inputs[:2]: tensor([[   0,  741,  300,  ...,   83, 7111,  292],
        [   0,  401,  266,  ...,  266, 2236, 1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0530e-04, -1.1204e-04, -1.9866e-05,  ..., -3.4893e-05,
         -3.4817e-04,  3.4522e-04],
        [-5.4091e-06, -3.7588e-06,  4.3511e-06,  ..., -4.4778e-06,
         -3.3565e-06, -3.6433e-06],
        [-1.7256e-05, -1.2606e-05,  1.4752e-05,  ..., -1.4335e-05,
         -1.0848e-05, -1.1653e-05],
        [-1.2204e-05, -8.3894e-06,  1.0312e-05,  ..., -1.0103e-05,
         -7.5623e-06, -8.5831e-06],
        [-1.2264e-05, -9.3579e-06,  1.0103e-05,  ..., -1.0446e-05,
         -8.3372e-06, -7.6964e-06]], device='cuda:0')
Loss: 0.9765824675559998


Running epoch 1, step 2060, batch 1012
Sampled inputs[:2]: tensor([[   0, 6491, 3667,  ..., 5042,   14, 2152],
        [   0,  266,  996,  ...,  709,  616, 9378]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4294e-04, -1.6964e-04,  7.1376e-05,  ..., -1.3369e-04,
         -3.4817e-04,  3.6984e-04],
        [-6.7279e-06, -4.8615e-06,  5.3197e-06,  ..., -5.6699e-06,
         -4.4368e-06, -4.5449e-06],
        [-2.1487e-05, -1.6272e-05,  1.8045e-05,  ..., -1.8209e-05,
         -1.4380e-05, -1.4603e-05],
        [-1.5154e-05, -1.0818e-05,  1.2562e-05,  ..., -1.2785e-05,
         -1.0021e-05, -1.0714e-05],
        [-1.5438e-05, -1.2234e-05,  1.2517e-05,  ..., -1.3411e-05,
         -1.1154e-05, -9.7528e-06]], device='cuda:0')
Loss: 1.007616400718689


Running epoch 1, step 2061, batch 1013
Sampled inputs[:2]: tensor([[    0,   278,  6481,  ...,    13,  8970,    12],
        [    0,  2733,   278,  ..., 10936,    14,  6593]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4996e-04, -2.2849e-04,  3.6942e-05,  ..., -1.6208e-04,
         -2.6522e-04,  4.5021e-04],
        [-8.0094e-06, -5.7034e-06,  6.4224e-06,  ..., -6.7353e-06,
         -5.2191e-06, -5.4017e-06],
        [-2.5511e-05, -1.9044e-05,  2.1711e-05,  ..., -2.1532e-05,
         -1.6853e-05, -1.7300e-05],
        [-1.8060e-05, -1.2688e-05,  1.5184e-05,  ..., -1.5184e-05,
         -1.1772e-05, -1.2755e-05],
        [-1.8343e-05, -1.4335e-05,  1.5110e-05,  ..., -1.5885e-05,
         -1.3106e-05, -1.1556e-05]], device='cuda:0')
Loss: 0.9491147994995117


Running epoch 1, step 2062, batch 1014
Sampled inputs[:2]: tensor([[    0,  1040,   287,  ...,    14, 10209,    12],
        [    0,    20,     9,  ...,    12,  2212, 24950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9874e-04, -1.3774e-04,  7.8622e-05,  ..., -1.0981e-04,
         -4.1960e-04,  3.3984e-04],
        [-9.3877e-06, -6.6720e-06,  7.4059e-06,  ..., -7.9200e-06,
         -6.1616e-06, -6.3926e-06],
        [-2.9773e-05, -2.2233e-05,  2.5004e-05,  ..., -2.5198e-05,
         -1.9833e-05, -2.0340e-05],
        [-2.1100e-05, -1.4804e-05,  1.7449e-05,  ..., -1.7822e-05,
         -1.3888e-05, -1.5050e-05],
        [-2.1532e-05, -1.6809e-05,  1.7509e-05,  ..., -1.8686e-05,
         -1.5475e-05, -1.3657e-05]], device='cuda:0')
Loss: 0.9542489647865295


Running epoch 1, step 2063, batch 1015
Sampled inputs[:2]: tensor([[   0,  266, 2967,  ...,  287, 4432,   13],
        [   0,  367, 2063,  ..., 3022,  221,  733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6593e-04, -1.4258e-04, -5.0436e-05,  ..., -1.0981e-04,
         -5.2978e-04,  3.4237e-04],
        [-1.0714e-05, -7.5698e-06,  8.4490e-06,  ..., -9.0301e-06,
         -6.9998e-06, -7.3425e-06],
        [ 2.0008e-05,  3.2754e-05, -5.6133e-05,  ...,  1.9227e-05,
          1.6336e-05,  8.9944e-06],
        [-2.4095e-05, -1.6786e-05,  1.9878e-05,  ..., -2.0325e-05,
         -1.5758e-05, -1.7285e-05],
        [-2.4617e-05, -1.9103e-05,  1.9982e-05,  ..., -2.1338e-05,
         -1.7591e-05, -1.5728e-05]], device='cuda:0')
Loss: 0.9286563992500305
Graident accumulation at epoch 1, step 2063, batch 1015
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0038,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.6199e-05, -1.2057e-04, -2.3778e-04,  ...,  1.8148e-05,
         -8.7585e-05, -3.1513e-05],
        [-1.0648e-05, -7.4781e-06,  7.6077e-06,  ..., -9.3037e-06,
         -7.3960e-06, -7.5620e-06],
        [ 3.3978e-09,  1.3171e-05, -3.1899e-06,  ...,  5.7800e-06,
          8.5809e-06, -5.7172e-06],
        [-1.3604e-05, -5.2144e-06,  1.0721e-05,  ..., -1.1140e-05,
         -3.5204e-06, -1.1438e-05],
        [-1.9688e-05, -9.3589e-06,  1.2477e-05,  ..., -1.5922e-05,
         -1.4736e-05, -1.1939e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8064e-08, 5.7477e-08, 6.9348e-08,  ..., 2.8870e-08, 1.5940e-07,
         4.0620e-08],
        [9.6590e-11, 6.2368e-11, 3.0246e-11,  ..., 6.4827e-11, 3.3578e-11,
         3.6918e-11],
        [4.7646e-09, 3.5201e-09, 1.9981e-09,  ..., 3.8353e-09, 2.1146e-09,
         1.3787e-09],
        [1.3106e-09, 1.4675e-09, 5.7574e-10,  ..., 1.1529e-09, 9.8522e-10,
         4.9957e-10],
        [4.2670e-10, 2.6302e-10, 1.1298e-10,  ..., 3.1896e-10, 1.1127e-10,
         1.2836e-10]], device='cuda:0')
optimizer state dict: 258.0
lr: [1.2222363421819928e-08, 1.2222363421819928e-08]
scheduler_last_epoch: 258


Running epoch 1, step 2064, batch 1016
Sampled inputs[:2]: tensor([[    0,   401,  3408,  ...,   287, 19892,   328],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2920e-05, -2.6527e-05, -1.1489e-04,  ...,  5.5739e-06,
          1.2796e-05,  9.5632e-05],
        [-1.3337e-06, -9.0525e-07,  1.0803e-06,  ..., -1.0952e-06,
         -8.2329e-07, -9.3877e-07],
        [-4.3213e-06, -3.0547e-06,  3.6955e-06,  ..., -3.5465e-06,
         -2.6524e-06, -3.0398e-06],
        [-3.0547e-06, -2.0415e-06,  2.5928e-06,  ..., -2.5034e-06,
         -1.8626e-06, -2.2501e-06],
        [-3.0994e-06, -2.2799e-06,  2.5630e-06,  ..., -2.6077e-06,
         -2.0564e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9705196619033813


Running epoch 1, step 2065, batch 1017
Sampled inputs[:2]: tensor([[    0, 21748,   792,  ...,   408,   266, 31879],
        [    0,   685,  3482,  ..., 23113,    12,  6481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7846e-05, -5.4155e-05, -1.7130e-04,  ..., -9.4858e-05,
         -4.0815e-06, -1.7944e-04],
        [-2.7195e-06, -1.8179e-06,  1.9707e-06,  ..., -2.3469e-06,
         -1.8217e-06, -1.9446e-06],
        [ 4.5164e-05,  7.9888e-05, -4.6862e-05,  ...,  5.4383e-05,
          9.6165e-05,  4.3885e-05],
        [-6.2436e-06, -4.0829e-06,  4.6939e-06,  ..., -5.3942e-06,
         -4.1723e-06, -4.6641e-06],
        [-6.6161e-06, -4.7386e-06,  4.9472e-06,  ..., -5.7667e-06,
         -4.6492e-06, -4.3660e-06]], device='cuda:0')
Loss: 0.9885909557342529


Running epoch 1, step 2066, batch 1018
Sampled inputs[:2]: tensor([[   0, 2579,  278,  ...,   56,    9,  271],
        [   0, 7203,  271,  ...,   12,  275, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7307e-05,  1.1594e-06, -1.3572e-04,  ..., -1.7858e-04,
         -1.1193e-05, -2.1163e-04],
        [-4.0457e-06, -2.7083e-06,  3.0063e-06,  ..., -3.4794e-06,
         -2.6636e-06, -2.8759e-06],
        [ 4.1022e-05,  7.7027e-05, -4.3405e-05,  ...,  5.0911e-05,
          9.3572e-05,  4.1009e-05],
        [-9.1940e-06, -5.9903e-06,  7.0930e-06,  ..., -7.8827e-06,
         -6.0126e-06, -6.8098e-06],
        [-9.7454e-06, -7.0035e-06,  7.4655e-06,  ..., -8.4490e-06,
         -6.7502e-06, -6.3628e-06]], device='cuda:0')
Loss: 0.9468262791633606


Running epoch 1, step 2067, batch 1019
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6487e-05, -5.3572e-05, -2.3114e-04,  ..., -2.1948e-04,
          6.0673e-05, -2.8907e-04],
        [-5.3570e-06, -3.6620e-06,  3.9749e-06,  ..., -4.5598e-06,
         -3.5502e-06, -3.7290e-06],
        [ 3.6879e-05,  7.3868e-05, -4.0068e-05,  ...,  4.7499e-05,
          9.0785e-05,  3.8297e-05],
        [-1.2174e-05, -8.1211e-06,  9.4026e-06,  ..., -1.0341e-05,
         -8.0243e-06, -8.8662e-06],
        [-1.2770e-05, -9.4026e-06,  9.8050e-06,  ..., -1.0997e-05,
         -8.9407e-06, -8.1658e-06]], device='cuda:0')
Loss: 0.9625260829925537


Running epoch 1, step 2068, batch 1020
Sampled inputs[:2]: tensor([[    0,   259,  2416,  ..., 14474,    12,   259],
        [    0,   409, 35049,  ...,    12,   699,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4712e-04,  2.2506e-05, -2.2666e-04,  ..., -3.0831e-04,
          9.6907e-05, -2.9805e-04],
        [-6.6534e-06, -4.5076e-06,  4.9286e-06,  ..., -5.7220e-06,
         -4.4517e-06, -4.7348e-06],
        [ 3.2796e-05,  7.1141e-05, -3.6834e-05,  ...,  4.3923e-05,
          8.8043e-05,  3.5242e-05],
        [-1.5140e-05, -9.9838e-06,  1.1683e-05,  ..., -1.2979e-05,
         -1.0036e-05, -1.1221e-05],
        [-1.5914e-05, -1.1578e-05,  1.2189e-05,  ..., -1.3798e-05,
         -1.1206e-05, -1.0371e-05]], device='cuda:0')
Loss: 0.9420870542526245


Running epoch 1, step 2069, batch 1021
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ..., 1236,  768, 4316],
        [   0,  277,  279,  ...,   12,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1372e-05, -8.7602e-05, -3.3499e-04,  ..., -3.9654e-04,
         -8.1935e-05, -4.8129e-04],
        [-8.0019e-06, -5.4128e-06,  5.9865e-06,  ..., -6.8247e-06,
         -5.3011e-06, -5.6140e-06],
        [ 2.8594e-05,  6.8190e-05, -3.3273e-05,  ...,  4.0510e-05,
          8.5391e-05,  3.2545e-05],
        [-1.8209e-05, -1.1995e-05,  1.4231e-05,  ..., -1.5482e-05,
         -1.1973e-05, -1.3322e-05],
        [-1.8954e-05, -1.3813e-05,  1.4693e-05,  ..., -1.6317e-05,
         -1.3262e-05, -1.2167e-05]], device='cuda:0')
Loss: 0.9256858229637146


Running epoch 1, step 2070, batch 1022
Sampled inputs[:2]: tensor([[    0,  5885,   271,  ...,   278,  1049,    12],
        [    0,    13,    19,  ..., 22111,  2489,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1611e-05, -8.4948e-05, -3.6362e-04,  ..., -3.9631e-04,
         -1.8044e-04, -5.4870e-04],
        [-9.3207e-06, -6.3963e-06,  7.0669e-06,  ..., -7.9796e-06,
         -6.2138e-06, -6.5006e-06],
        [ 2.4362e-05,  6.4942e-05, -2.9622e-05,  ...,  3.6800e-05,
          8.2411e-05,  2.9669e-05],
        [-2.1234e-05, -1.4216e-05,  1.6823e-05,  ..., -1.8105e-05,
         -1.4044e-05, -1.5453e-05],
        [-2.1979e-05, -1.6272e-05,  1.7241e-05,  ..., -1.9044e-05,
         -1.5557e-05, -1.4089e-05]], device='cuda:0')
Loss: 0.9989475011825562


Running epoch 1, step 2071, batch 1023
Sampled inputs[:2]: tensor([[    0,  1295,   508,  ...,   829,   772,   278],
        [    0,  5301,   792,  ..., 27135, 34090,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1344e-05, -6.1887e-05, -3.3782e-04,  ..., -4.0286e-04,
         -9.7750e-05, -5.1141e-04],
        [-1.0677e-05, -7.4394e-06,  8.0131e-06,  ..., -9.1791e-06,
         -7.2047e-06, -7.4282e-06],
        [ 2.0160e-05,  6.1559e-05, -2.6433e-05,  ...,  3.3089e-05,
          7.9296e-05,  2.6808e-05],
        [-2.4229e-05, -1.6496e-05,  1.9029e-05,  ..., -2.0757e-05,
         -1.6250e-05, -1.7598e-05],
        [-2.5183e-05, -1.8954e-05,  1.9595e-05,  ..., -2.1949e-05,
         -1.8090e-05, -1.6131e-05]], device='cuda:0')
Loss: 0.9762014150619507
Graident accumulation at epoch 1, step 2071, batch 1023
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0039,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.8444e-05, -1.1470e-04, -2.4778e-04,  ..., -2.3952e-05,
         -8.8601e-05, -7.9503e-05],
        [-1.0651e-05, -7.4742e-06,  7.6483e-06,  ..., -9.2912e-06,
         -7.3768e-06, -7.5486e-06],
        [ 2.0191e-06,  1.8010e-05, -5.5142e-06,  ...,  8.5109e-06,
          1.5652e-05, -2.4647e-06],
        [-1.4666e-05, -6.3425e-06,  1.1552e-05,  ..., -1.2101e-05,
         -4.7933e-06, -1.2054e-05],
        [-2.0237e-05, -1.0318e-05,  1.3188e-05,  ..., -1.6525e-05,
         -1.5071e-05, -1.2358e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8006e-08, 5.7423e-08, 6.9393e-08,  ..., 2.9003e-08, 1.5925e-07,
         4.0841e-08],
        [9.6607e-11, 6.2361e-11, 3.0280e-11,  ..., 6.4847e-11, 3.3596e-11,
         3.6936e-11],
        [4.7603e-09, 3.5204e-09, 1.9968e-09,  ..., 3.8326e-09, 2.1188e-09,
         1.3780e-09],
        [1.3099e-09, 1.4663e-09, 5.7553e-10,  ..., 1.1522e-09, 9.8450e-10,
         4.9938e-10],
        [4.2690e-10, 2.6311e-10, 1.1325e-10,  ..., 3.1913e-10, 1.1148e-10,
         1.2849e-10]], device='cuda:0')
optimizer state dict: 259.0
lr: [6.875692313159655e-09, 6.875692313159655e-09]
scheduler_last_epoch: 259


Running epoch 1, step 2072, batch 1024
Sampled inputs[:2]: tensor([[   0,  772,  699,  ..., 1849,  287, 7134],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4333e-04,  5.2903e-06,  6.9705e-06,  ..., -2.0757e-04,
         -2.1913e-04, -2.9650e-05],
        [-1.3113e-06, -1.0133e-06,  1.0133e-06,  ..., -1.1399e-06,
         -9.7603e-07, -8.7917e-07],
        [-4.1425e-06, -3.3677e-06,  3.3975e-06,  ..., -3.6359e-06,
         -3.1441e-06, -2.8163e-06],
        [-2.9802e-06, -2.2799e-06,  2.3991e-06,  ..., -2.5928e-06,
         -2.2352e-06, -2.1160e-06],
        [-3.0696e-06, -2.6077e-06,  2.4587e-06,  ..., -2.7567e-06,
         -2.4736e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9887155890464783


Running epoch 1, step 2073, batch 1025
Sampled inputs[:2]: tensor([[    0, 14165,    14,  ..., 34395, 31103,  6905],
        [    0, 14867,   278,  ...,   674,   369,  4127]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3319e-05, -8.3952e-05, -2.7118e-04,  ..., -2.6300e-05,
         -4.5864e-06, -2.0527e-04],
        [-2.6301e-06, -1.9744e-06,  2.0415e-06,  ..., -2.2948e-06,
         -1.9148e-06, -1.8775e-06],
        [-8.3148e-06, -6.4224e-06,  6.8694e-06,  ..., -7.1824e-06,
         -6.0499e-06, -5.8413e-06],
        [-5.9307e-06, -4.3809e-06,  4.8131e-06,  ..., -5.1707e-06,
         -4.3362e-06, -4.4256e-06],
        [-6.1840e-06, -4.9621e-06,  4.9621e-06,  ..., -5.4538e-06,
         -4.7684e-06, -4.0084e-06]], device='cuda:0')
Loss: 0.9946199059486389


Running epoch 1, step 2074, batch 1026
Sampled inputs[:2]: tensor([[   0,  395, 4973,  ..., 5851,  409, 4370],
        [   0,  266, 4616,  ..., 1906, 7256,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1650e-05, -1.5754e-04, -3.9095e-04,  ..., -1.1497e-05,
         -1.2193e-05, -1.9416e-04],
        [-3.9488e-06, -2.8424e-06,  3.1292e-06,  ..., -3.3751e-06,
         -2.7157e-06, -2.7642e-06],
        [-1.2457e-05, -9.3281e-06,  1.0535e-05,  ..., -1.0610e-05,
         -8.6129e-06, -8.6278e-06],
        [-9.0003e-06, -6.3777e-06,  7.4655e-06,  ..., -7.6890e-06,
         -6.1989e-06, -6.5863e-06],
        [-9.1493e-06, -7.1377e-06,  7.4953e-06,  ..., -7.9572e-06,
         -6.7502e-06, -5.8562e-06]], device='cuda:0')
Loss: 0.9611729383468628


Running epoch 1, step 2075, batch 1027
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,   12,  850, 4952],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3759e-05, -1.7254e-04, -2.5905e-04,  ...,  7.0909e-05,
         -1.5108e-05, -1.4343e-04],
        [-5.3048e-06, -3.8706e-06,  4.2394e-06,  ..., -4.5002e-06,
         -3.5949e-06, -3.6508e-06],
        [-1.6749e-05, -1.2726e-05,  1.4260e-05,  ..., -1.4186e-05,
         -1.1459e-05, -1.1474e-05],
        [-1.2070e-05, -8.6725e-06,  1.0103e-05,  ..., -1.0222e-05,
         -8.1807e-06, -8.6874e-06],
        [-1.2264e-05, -9.7305e-06,  1.0148e-05,  ..., -1.0625e-05,
         -8.9854e-06, -7.7784e-06]], device='cuda:0')
Loss: 0.9648171663284302


Running epoch 1, step 2076, batch 1028
Sampled inputs[:2]: tensor([[   0, 1144, 2680,  ...,  963,    9, 1184],
        [   0,  266, 1194,  ..., 2267,   15, 1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6194e-05, -2.4207e-04, -3.1565e-04,  ..., -7.7056e-05,
         -3.2237e-04, -3.3144e-04],
        [-6.5342e-06, -4.7423e-06,  5.2527e-06,  ..., -5.5730e-06,
         -4.4219e-06, -4.4927e-06],
        [-2.0549e-05, -1.5438e-05,  1.7643e-05,  ..., -1.7360e-05,
         -1.3947e-05, -1.3918e-05],
        [-1.4827e-05, -1.0543e-05,  1.2517e-05,  ..., -1.2577e-05,
         -1.0014e-05, -1.0639e-05],
        [-1.5110e-05, -1.1861e-05,  1.2591e-05,  ..., -1.3053e-05,
         -1.0997e-05, -9.4250e-06]], device='cuda:0')
Loss: 0.8983888626098633


Running epoch 1, step 2077, batch 1029
Sampled inputs[:2]: tensor([[    0,  1478,    14,  ...,   266,  9417,  9105],
        [    0,  2523, 10780,  ...,  1041,    26, 13745]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0706e-04, -2.7302e-04, -1.4135e-04,  ...,  2.2423e-05,
         -1.2765e-04, -2.7554e-04],
        [-7.8529e-06, -5.7109e-06,  6.2138e-06,  ..., -6.7428e-06,
         -5.3830e-06, -5.4538e-06],
        [-2.4691e-05, -1.8626e-05,  2.0921e-05,  ..., -2.1026e-05,
         -1.6958e-05, -1.6972e-05],
        [-1.7732e-05, -1.2659e-05,  1.4752e-05,  ..., -1.5169e-05,
         -1.2130e-05, -1.2875e-05],
        [-1.8254e-05, -1.4380e-05,  1.4976e-05,  ..., -1.5900e-05,
         -1.3456e-05, -1.1556e-05]], device='cuda:0')
Loss: 0.9679810404777527


Running epoch 1, step 2078, batch 1030
Sampled inputs[:2]: tensor([[    0,   266,  1034,  ...,  6153,   263,   472],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5427e-05, -4.9252e-04, -3.1502e-04,  ...,  8.5763e-05,
         -4.9310e-05, -5.4582e-04],
        [-9.1568e-06, -6.6422e-06,  7.3090e-06,  ..., -7.8529e-06,
         -6.2212e-06, -6.3740e-06],
        [-2.8953e-05, -2.1800e-05,  2.4676e-05,  ..., -2.4676e-05,
         -1.9714e-05, -2.0012e-05],
        [-2.0728e-05, -1.4760e-05,  1.7375e-05,  ..., -1.7717e-05,
         -1.4037e-05, -1.5110e-05],
        [-2.1309e-05, -1.6779e-05,  1.7598e-05,  ..., -1.8582e-05,
         -1.5587e-05, -1.3568e-05]], device='cuda:0')
Loss: 0.9713187217712402


Running epoch 1, step 2079, batch 1031
Sampled inputs[:2]: tensor([[   0, 1336,  278,  ...,  266, 3269,  278],
        [   0, 6541,  287,  ..., 1061, 4786,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4822e-04, -5.4131e-04, -4.3639e-04,  ..., -2.8285e-05,
          5.8620e-05, -5.6971e-04],
        [-1.0461e-05, -7.5623e-06,  8.3074e-06,  ..., -8.9779e-06,
         -7.1228e-06, -7.2904e-06],
        [-3.3185e-05, -2.4855e-05,  2.8148e-05,  ..., -2.8268e-05,
         -2.2575e-05, -2.2963e-05],
        [-2.3723e-05, -1.6801e-05,  1.9759e-05,  ..., -2.0280e-05,
         -1.6078e-05, -1.7300e-05],
        [-2.4304e-05, -1.9029e-05,  1.9968e-05,  ..., -2.1189e-05,
         -1.7777e-05, -1.5467e-05]], device='cuda:0')
Loss: 0.946683943271637
Graident accumulation at epoch 1, step 2079, batch 1031
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0039,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.8778e-05, -1.5736e-04, -2.6664e-04,  ..., -2.4386e-05,
         -7.3879e-05, -1.2852e-04],
        [-1.0632e-05, -7.4830e-06,  7.7142e-06,  ..., -9.2599e-06,
         -7.3514e-06, -7.5228e-06],
        [-1.5013e-06,  1.3723e-05, -2.1479e-06,  ...,  4.8331e-06,
          1.1830e-05, -4.5145e-06],
        [-1.5572e-05, -7.3884e-06,  1.2373e-05,  ..., -1.2919e-05,
         -5.9218e-06, -1.2579e-05],
        [-2.0644e-05, -1.1189e-05,  1.3866e-05,  ..., -1.6991e-05,
         -1.5342e-05, -1.2669e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7970e-08, 5.7659e-08, 6.9514e-08,  ..., 2.8975e-08, 1.5909e-07,
         4.1124e-08],
        [9.6620e-11, 6.2356e-11, 3.0319e-11,  ..., 6.4863e-11, 3.3614e-11,
         3.6953e-11],
        [4.7566e-09, 3.5175e-09, 1.9956e-09,  ..., 3.8295e-09, 2.1172e-09,
         1.3771e-09],
        [1.3092e-09, 1.4651e-09, 5.7534e-10,  ..., 1.1514e-09, 9.8377e-10,
         4.9918e-10],
        [4.2707e-10, 2.6321e-10, 1.1353e-10,  ..., 3.1926e-10, 1.1169e-10,
         1.2860e-10]], device='cuda:0')
optimizer state dict: 260.0
lr: [3.0560578299276833e-09, 3.0560578299276833e-09]
scheduler_last_epoch: 260


Running epoch 1, step 2080, batch 1032
Sampled inputs[:2]: tensor([[    0,   401,  3740,  ...,  5980,   271,   266],
        [    0,   266,  1234,  ...,   908,   328, 26300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3078e-05, -1.8056e-04, -5.4959e-05,  ...,  7.8801e-05,
         -6.5136e-06, -9.6397e-05],
        [-1.3411e-06, -9.9838e-07,  1.1325e-06,  ..., -1.1027e-06,
         -8.3819e-07, -8.8662e-07],
        [ 2.2602e-04,  1.5967e-04, -1.4445e-04,  ...,  2.6723e-04,
          1.1883e-04,  1.2777e-04],
        [-3.0994e-06, -2.2650e-06,  2.7418e-06,  ..., -2.5481e-06,
         -1.9073e-06, -2.1756e-06],
        [-3.0696e-06, -2.4736e-06,  2.6375e-06,  ..., -2.5928e-06,
         -2.0564e-06, -1.8924e-06]], device='cuda:0')
Loss: 0.9760715961456299


Running epoch 1, step 2081, batch 1033
Sampled inputs[:2]: tensor([[   0,  342,  266,  ...,  586, 1944,  271],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3275e-04, -9.6259e-05, -1.4350e-04,  ..., -5.0967e-06,
         -6.8400e-05, -7.6842e-05],
        [-2.7642e-06, -1.9222e-06,  2.0526e-06,  ..., -2.3544e-06,
         -1.7807e-06, -1.9372e-06],
        [ 2.2172e-04,  1.5670e-04, -1.4140e-04,  ...,  2.6348e-04,
          1.1595e-04,  1.2462e-04],
        [-6.1542e-06, -4.2468e-06,  4.8429e-06,  ..., -5.2601e-06,
         -3.9488e-06, -4.5300e-06],
        [-6.6012e-06, -4.9770e-06,  5.0217e-06,  ..., -5.7667e-06,
         -4.6194e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.9243504405021667


Running epoch 1, step 2082, batch 1034
Sampled inputs[:2]: tensor([[   0,  446, 1115,  ..., 1869, 4971, 1954],
        [   0,  759, 4585,  ...,  360,  300,  670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0476e-04, -1.8746e-04, -1.2525e-04,  ...,  4.6479e-05,
         -7.3839e-05, -9.9803e-05],
        [-4.1053e-06, -2.9355e-06,  3.1479e-06,  ..., -3.4720e-06,
         -2.6636e-06, -2.7753e-06],
        [ 2.1740e-04,  1.5331e-04, -1.3767e-04,  ...,  2.5986e-04,
          1.1306e-04,  1.2189e-04],
        [-9.1791e-06, -6.4969e-06,  7.4208e-06,  ..., -7.7784e-06,
         -5.9456e-06, -6.5118e-06],
        [-9.7007e-06, -7.5251e-06,  7.6294e-06,  ..., -8.4192e-06,
         -6.8396e-06, -6.1616e-06]], device='cuda:0')
Loss: 0.9941704869270325


Running epoch 1, step 2083, batch 1035
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,     9,   287,  ..., 16261,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7512e-04, -2.1436e-04, -1.4508e-04,  ...,  4.0500e-05,
          9.0985e-05,  4.9925e-05],
        [-5.4613e-06, -4.0308e-06,  4.1388e-06,  ..., -4.6492e-06,
         -3.7067e-06, -3.7514e-06],
        [ 2.1311e-04,  1.4972e-04, -1.3432e-04,  ...,  2.5616e-04,
          1.0978e-04,  1.1884e-04],
        [-1.2174e-05, -8.9109e-06,  9.7156e-06,  ..., -1.0386e-05,
         -8.2552e-06, -8.7619e-06],
        [-1.3024e-05, -1.0356e-05,  1.0163e-05,  ..., -1.1325e-05,
         -9.5069e-06, -8.3670e-06]], device='cuda:0')
Loss: 0.9714133739471436


Running epoch 1, step 2084, batch 1036
Sampled inputs[:2]: tensor([[   0,  369,  726,  ...,   83,  409,  729],
        [   0, 3306, 4057,  ...,  287,  266, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1678e-04, -1.4428e-04, -1.8186e-04,  ..., -5.3641e-05,
          5.2209e-06, -5.2040e-05],
        [-6.7726e-06, -4.9770e-06,  5.2042e-06,  ..., -5.7593e-06,
         -4.5449e-06, -4.6231e-06],
        [ 2.0906e-04,  1.4665e-04, -1.3079e-04,  ...,  2.5275e-04,
          1.0716e-04,  1.1613e-04],
        [-1.5169e-05, -1.1042e-05,  1.2293e-05,  ..., -1.2919e-05,
         -1.0155e-05, -1.0878e-05],
        [-1.5914e-05, -1.2666e-05,  1.2606e-05,  ..., -1.3828e-05,
         -1.1548e-05, -1.0140e-05]], device='cuda:0')
Loss: 0.9580060243606567


Running epoch 1, step 2085, batch 1037
Sampled inputs[:2]: tensor([[    0, 12472,  1059,  ...,   642,   365,  6517],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3766e-04, -2.3302e-04, -1.7370e-04,  ..., -5.3553e-05,
         -9.4738e-05, -7.9510e-05],
        [-8.0615e-06, -5.9195e-06,  6.2473e-06,  ..., -6.8322e-06,
         -5.4352e-06, -5.5060e-06],
        [ 2.0489e-04,  1.4344e-04, -1.2718e-04,  ...,  2.4925e-04,
          1.0426e-04,  1.1322e-04],
        [-1.8209e-05, -1.3247e-05,  1.4871e-05,  ..., -1.5438e-05,
         -1.2241e-05, -1.3083e-05],
        [-1.8924e-05, -1.5080e-05,  1.5110e-05,  ..., -1.6406e-05,
         -1.3798e-05, -1.2092e-05]], device='cuda:0')
Loss: 0.9801701903343201


Running epoch 1, step 2086, batch 1038
Sampled inputs[:2]: tensor([[    0,  1265,   328,  ...,  2282, 35414,    13],
        [    0,   380,  1075,  ...,   298,   365,  4920]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4051e-04, -2.5586e-04, -2.9670e-04,  ..., -2.4192e-05,
         -7.6918e-05, -7.2417e-05],
        [-9.3505e-06, -6.8173e-06,  7.0669e-06,  ..., -8.0466e-06,
         -6.3814e-06, -6.4969e-06],
        [ 2.0101e-04,  1.4072e-04, -1.2441e-04,  ...,  2.4577e-04,
          1.0156e-04,  1.1048e-04],
        [-2.1026e-05, -1.5132e-05,  1.6741e-05,  ..., -1.8060e-05,
         -1.4283e-05, -1.5318e-05],
        [-2.2054e-05, -1.7345e-05,  1.7315e-05,  ..., -1.9237e-05,
         -1.6093e-05, -1.4119e-05]], device='cuda:0')
Loss: 0.9357354044914246


Running epoch 1, step 2087, batch 1039
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,   527, 37357,    12],
        [    0,    27,   417,  ...,    18,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2065e-04, -1.3860e-04, -4.0708e-04,  ..., -2.0336e-04,
          6.1739e-05, -2.5987e-04],
        [-1.0602e-05, -7.7412e-06,  7.8902e-06,  ..., -9.1791e-06,
         -7.2904e-06, -7.4580e-06],
        [ 1.9696e-04,  1.3759e-04, -1.2145e-04,  ...,  2.4218e-04,
          9.8579e-05,  1.0752e-04],
        [-2.3946e-05, -1.7278e-05,  1.8768e-05,  ..., -2.0713e-05,
         -1.6443e-05, -1.7643e-05],
        [-2.5138e-05, -1.9759e-05,  1.9521e-05,  ..., -2.1994e-05,
         -1.8463e-05, -1.6205e-05]], device='cuda:0')
Loss: 0.9614740014076233
Graident accumulation at epoch 1, step 2087, batch 1039
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0039,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.7964e-05, -1.5549e-04, -2.8069e-04,  ..., -4.2283e-05,
         -6.0318e-05, -1.4166e-04],
        [-1.0629e-05, -7.5089e-06,  7.7318e-06,  ..., -9.2518e-06,
         -7.3453e-06, -7.5163e-06],
        [ 1.8345e-05,  2.6109e-05, -1.4078e-05,  ...,  2.8568e-05,
          2.0505e-05,  6.6885e-06],
        [-1.6409e-05, -8.3773e-06,  1.3012e-05,  ..., -1.3699e-05,
         -6.9740e-06, -1.3085e-05],
        [-2.1093e-05, -1.2046e-05,  1.4432e-05,  ..., -1.7491e-05,
         -1.5654e-05, -1.3023e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7961e-08, 5.7621e-08, 6.9610e-08,  ..., 2.8988e-08, 1.5894e-07,
         4.1151e-08],
        [9.6636e-11, 6.2354e-11, 3.0351e-11,  ..., 6.4882e-11, 3.3633e-11,
         3.6971e-11],
        [4.7907e-09, 3.5329e-09, 2.0083e-09,  ..., 3.8844e-09, 2.1248e-09,
         1.3873e-09],
        [1.3084e-09, 1.4639e-09, 5.7512e-10,  ..., 1.1507e-09, 9.8306e-10,
         4.9899e-10],
        [4.2727e-10, 2.6334e-10, 1.1380e-10,  ..., 3.1942e-10, 1.1192e-10,
         1.2873e-10]], device='cuda:0')
optimizer state dict: 261.0
lr: [7.640436456168853e-10, 7.640436456168853e-10]
scheduler_last_epoch: 261


Running epoch 1, step 2088, batch 1040
Sampled inputs[:2]: tensor([[    0,  2320,    63,  ...,   858,    13, 40170],
        [    0,  1487,   409,  ...,  6979,  1273,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1099e-04, -3.9708e-05,  9.4150e-05,  ...,  3.7083e-05,
          2.5651e-04,  3.3102e-05],
        [-1.3486e-06, -1.1250e-06,  9.0525e-07,  ..., -1.2442e-06,
         -1.1101e-06, -9.0525e-07],
        [-4.2915e-06, -3.7402e-06,  3.0696e-06,  ..., -3.9339e-06,
         -3.5167e-06, -2.8759e-06],
        [-2.9653e-06, -2.4736e-06,  2.0564e-06,  ..., -2.7567e-06,
         -2.4736e-06, -2.1309e-06],
        [-3.3528e-06, -3.0100e-06,  2.3395e-06,  ..., -3.1292e-06,
         -2.8759e-06, -2.1011e-06]], device='cuda:0')
Loss: 1.0129318237304688


Running epoch 1, step 2089, batch 1041
Sampled inputs[:2]: tensor([[   0,  367, 1236,  ...,  344,  292,   20],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4182e-04,  9.5981e-06,  2.1044e-04,  ...,  1.5855e-04,
          4.1039e-04,  1.0735e-05],
        [-2.7344e-06, -2.0154e-06,  1.9260e-06,  ..., -2.4289e-06,
         -2.0079e-06, -1.9036e-06],
        [ 5.6019e-05,  8.5905e-05, -3.9626e-05,  ...,  7.4354e-05,
          3.6963e-05,  3.7487e-05],
        [-6.0350e-06, -4.4107e-06,  4.4107e-06,  ..., -5.3644e-06,
         -4.4405e-06, -4.4256e-06],
        [-6.5714e-06, -5.2750e-06,  4.7982e-06,  ..., -5.9307e-06,
         -5.1409e-06, -4.2319e-06]], device='cuda:0')
Loss: 0.9537609815597534


Running epoch 1, step 2090, batch 1042
Sampled inputs[:2]: tensor([[    0, 39004,   266,  ...,   287, 21972,   278],
        [    0, 18787, 27117,  ...,   287, 16139,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0367e-04, -5.7822e-05,  3.6894e-04,  ...,  2.1429e-04,
          3.2229e-04, -8.1486e-06],
        [-4.0606e-06, -2.9765e-06,  2.7418e-06,  ..., -3.6433e-06,
         -3.0287e-06, -2.9244e-06],
        [ 5.1816e-05,  8.2701e-05, -3.6824e-05,  ...,  7.0510e-05,
          3.3700e-05,  3.4283e-05],
        [-8.9705e-06, -6.4969e-06,  6.2585e-06,  ..., -8.0913e-06,
         -6.7502e-06, -6.7800e-06],
        [-9.8944e-06, -7.8678e-06,  6.9439e-06,  ..., -9.0152e-06,
         -7.8529e-06, -6.6012e-06]], device='cuda:0')
Loss: 0.9432576894760132


Running epoch 1, step 2091, batch 1043
Sampled inputs[:2]: tensor([[    0, 41638,  4573,  ...,   259,   790,  1416],
        [    0,    15,    19,  ...,    12,   287,  7897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4801e-04,  6.6724e-06,  3.6389e-04,  ...,  2.4638e-04,
          2.1579e-04,  9.5714e-05],
        [-5.4687e-06, -3.9153e-06,  3.6135e-06,  ..., -4.8503e-06,
         -4.0717e-06, -3.9749e-06],
        [ 4.7555e-05,  7.9662e-05, -3.3904e-05,  ...,  6.6874e-05,
          3.0481e-05,  3.1139e-05],
        [-1.2055e-05, -8.5533e-06,  8.2403e-06,  ..., -1.0759e-05,
         -9.0748e-06, -9.1642e-06],
        [-1.3307e-05, -1.0312e-05,  9.2089e-06,  ..., -1.1966e-05,
         -1.0535e-05, -8.9556e-06]], device='cuda:0')
Loss: 0.9419153928756714


Running epoch 1, step 2092, batch 1044
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,   287, 14752,  ...,   910, 26097,  1477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3534e-04, -6.4378e-05,  3.3907e-04,  ...,  2.5297e-04,
          2.2194e-04, -4.3759e-05],
        [-6.8098e-06, -4.8839e-06,  4.5970e-06,  ..., -5.9903e-06,
         -5.0105e-06, -4.8578e-06],
        [ 1.0937e-04,  1.3915e-04, -6.7783e-05,  ...,  1.8218e-04,
          1.5048e-04,  7.4522e-05],
        [-1.5125e-05, -1.0759e-05,  1.0580e-05,  ..., -1.3366e-05,
         -1.1250e-05, -1.1265e-05],
        [-1.6436e-05, -1.2755e-05,  1.1608e-05,  ..., -1.4678e-05,
         -1.2919e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.9735175967216492


Running epoch 1, step 2093, batch 1045
Sampled inputs[:2]: tensor([[    0, 11752,   280,  ..., 14814,  1128,   360],
        [    0,    12,   287,  ...,   266,  2105,  3925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5521e-04, -1.6425e-04,  1.2248e-04,  ...,  2.2207e-04,
          1.0784e-04, -2.1734e-04],
        [-8.1733e-06, -5.7481e-06,  5.6103e-06,  ..., -7.1079e-06,
         -5.8524e-06, -5.8264e-06],
        [ 1.0508e-04,  1.3629e-04, -6.4340e-05,  ...,  1.7871e-04,
          1.4784e-04,  7.1541e-05],
        [-1.8179e-05, -1.2666e-05,  1.2979e-05,  ..., -1.5855e-05,
         -1.3128e-05, -1.3515e-05],
        [-1.9610e-05, -1.4961e-05,  1.4082e-05,  ..., -1.7300e-05,
         -1.5020e-05, -1.2890e-05]], device='cuda:0')
Loss: 0.9592761397361755


Running epoch 1, step 2094, batch 1046
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,   634,   631,  ...,  3431,   287, 27947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0084e-04,  6.4298e-06,  1.9642e-04,  ...,  1.8863e-04,
          1.0469e-04, -2.3821e-04],
        [-9.5367e-06, -6.7763e-06,  6.7055e-06,  ..., -8.2552e-06,
         -6.7987e-06, -6.7614e-06],
        [ 1.8590e-04,  2.1585e-04, -1.3188e-04,  ...,  2.6307e-04,
          2.0616e-04,  9.8012e-05],
        [-2.1324e-05, -1.5005e-05,  1.5602e-05,  ..., -1.8492e-05,
         -1.5303e-05, -1.5765e-05],
        [-2.2799e-05, -1.7583e-05,  1.6719e-05,  ..., -2.0057e-05,
         -1.7434e-05, -1.4946e-05]], device='cuda:0')
Loss: 0.9731220006942749


Running epoch 1, step 2095, batch 1047
Sampled inputs[:2]: tensor([[    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0354e-04, -9.6233e-05,  2.6859e-04,  ...,  2.4613e-04,
          3.0720e-04, -1.2283e-04],
        [-1.0885e-05, -7.8715e-06,  7.7188e-06,  ..., -9.4473e-06,
         -7.8045e-06, -7.6555e-06],
        [ 1.8131e-04,  2.1200e-04, -1.2830e-04,  ...,  2.5902e-04,
          2.0268e-04,  9.4913e-05],
        [-2.4512e-05, -1.7554e-05,  1.8060e-05,  ..., -2.1279e-05,
         -1.7658e-05, -1.7971e-05],
        [-2.6062e-05, -2.0429e-05,  1.9222e-05,  ..., -2.3022e-05,
         -2.0057e-05, -1.7017e-05]], device='cuda:0')
Loss: 1.0121583938598633
Graident accumulation at epoch 1, step 2095, batch 1047
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0062, -0.0140,  0.0022,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0286, -0.0083,  0.0039,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0341, -0.0091,  0.0402,  ...,  0.0229,  0.0067, -0.0012],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0140, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.1859e-06, -1.4956e-04, -2.2576e-04,  ..., -1.3442e-05,
         -2.3565e-05, -1.3978e-04],
        [-1.0655e-05, -7.5451e-06,  7.7305e-06,  ..., -9.2714e-06,
         -7.3912e-06, -7.5303e-06],
        [ 3.4641e-05,  4.4699e-05, -2.5500e-05,  ...,  5.1613e-05,
          3.8722e-05,  1.5511e-05],
        [-1.7220e-05, -9.2950e-06,  1.3517e-05,  ..., -1.4457e-05,
         -8.0424e-06, -1.3574e-05],
        [-2.1590e-05, -1.2885e-05,  1.4911e-05,  ..., -1.8045e-05,
         -1.6094e-05, -1.3422e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8157e-08, 5.7572e-08, 6.9612e-08,  ..., 2.9019e-08, 1.5887e-07,
         4.1125e-08],
        [9.6658e-11, 6.2353e-11, 3.0380e-11,  ..., 6.4906e-11, 3.3660e-11,
         3.6993e-11],
        [4.8187e-09, 3.5743e-09, 2.0228e-09,  ..., 3.9476e-09, 2.1637e-09,
         1.3949e-09],
        [1.3077e-09, 1.4628e-09, 5.7487e-10,  ..., 1.1500e-09, 9.8239e-10,
         4.9882e-10],
        [4.2752e-10, 2.6349e-10, 1.1406e-10,  ..., 3.1963e-10, 1.1221e-10,
         1.2889e-10]], device='cuda:0')
optimizer state dict: 262.0
lr: [0.0, 0.0]
scheduler_last_epoch: 262
End of epoch 1 | Validation PPL: 6.7265723337723236 | Learning rate: 0.0
End of epoch checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/end_of_epoch_checkpoint.1, AFTER epoch 1
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà
wandb:   End of epoch ‚ñÅ‚ñà
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  Learning Rate ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: Validation PPL ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 959
wandb:   End of epoch 1
wandb:          Epoch 1
wandb:  Learning Rate 0.0
wandb:   Training PPL 2093.12176
wandb: Validation PPL 6.72657
wandb: 
wandb: üöÄ View run generous-plasma-311 at: https://wandb.ai/kenotron/brainlessgpt/runs/nqpb042a
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_134538-nqpb042a/logs
