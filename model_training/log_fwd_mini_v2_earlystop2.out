nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_133252-ah77duc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run dulcet-forest-310
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/ah77duc8
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Resuming training from checkpoint: exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319
Resumed training from epoch 0, step 320
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,   334,   287,  ...,  1348,  6139,   342]], device='cuda:0')
Skip epoch 0, step 0, batch 0


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,  1412, 11275,  ...,   668, 14849,   367]], device='cuda:0')
Skip epoch 0, step 1, batch 1


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
Skip epoch 0, step 2, batch 2


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0,  7555,  3908,  ...,   259,  8477,   278],
        [    0,  2895,    26,  ..., 11645,  1535,  1558]], device='cuda:0')
Skip epoch 0, step 3, batch 3


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[  0, 368, 266,  ..., 591, 767, 824],
        [  0,  25,  26,  ...,   9, 287, 298]], device='cuda:0')
Skip epoch 0, step 4, batch 4


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0,   29,  413,  ..., 1527, 1503,  369]], device='cuda:0')
Skip epoch 0, step 5, batch 5


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[    0,   221,   825,  ...,   616,  3661,  8052],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
Skip epoch 0, step 6, batch 6


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
Skip epoch 0, step 7, batch 7


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[    0,  9657,   300,  ...,    12,   271,   266],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
Skip epoch 0, step 8, batch 8


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
Skip epoch 0, step 9, batch 9


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[    0,    73,    30,  ...,  4112,    12,  9416],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
Skip epoch 0, step 10, batch 10


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[   0, 8416,  669,  ...,  298,  894,  496],
        [   0, 4890, 1528,  ...,  847,  328, 1703]], device='cuda:0')
Skip epoch 0, step 11, batch 11


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
Skip epoch 0, step 12, batch 12


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 1871,  401,  ...,   14, 4797,   12]], device='cuda:0')
Skip epoch 0, step 13, batch 13


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[    0, 21413,  1735,  ..., 10789, 12523,    12],
        [    0,  2771,    13,  ...,  1412,    35,    15]], device='cuda:0')
Skip epoch 0, step 14, batch 14


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
Skip epoch 0, step 15, batch 15


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,    27,  3961,  ...,   462,   221,   474],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
Skip epoch 0, step 16, batch 16


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,   13, 2920,  609],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
Skip epoch 0, step 17, batch 17


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[   0, 3408,  300,  ...,   14, 5870,   12],
        [   0, 2314,  266,  ...,  342, 7299, 1099]], device='cuda:0')
Skip epoch 0, step 18, batch 18


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[    0,   365,  5911,  ...,   925,   408,   266],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
Skip epoch 0, step 19, batch 19


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0, 10251,   278,  ...,   278,   319,    13],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
Skip epoch 0, step 20, batch 20


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
Skip epoch 0, step 21, batch 21


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[    0,  1635,   266,  ...,   437,  3302,   287],
        [    0,  2278,   292,  ..., 12060,  1319,   292]], device='cuda:0')
Skip epoch 0, step 22, batch 22


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   352,   644,  ...,  2928,   590,  3040]], device='cuda:0')
Skip epoch 0, step 23, batch 23


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[   0,   14,  496,  ...,  368,  259,  490],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
Skip epoch 0, step 24, batch 24


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[    0,  3037,  4511,  ...,  1711,    12,  2655],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
Skip epoch 0, step 25, batch 25


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,   369, 17432,  ...,   874,  2577,    14],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
Skip epoch 0, step 26, batch 26


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
Skip epoch 0, step 27, batch 27


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[   0,   20,   13,  ...,  496,   14, 1032],
        [   0,  199,  677,  ..., 2792,  271, 2386]], device='cuda:0')
Skip epoch 0, step 28, batch 28


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[   0,  721, 1119,  ...,  600,  328, 3363],
        [   0, 1478,   14,  ...,  266, 9417, 9105]], device='cuda:0')
Skip epoch 0, step 29, batch 29


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[   0,   12, 2418,  ...,  446,  381, 2204],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
Skip epoch 0, step 30, batch 30


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[   0, 2706,  292,  ...,   13, 8954,   13],
        [   0, 1596, 2700,  ...,  943,  266, 4086]], device='cuda:0')
Skip epoch 0, step 31, batch 31


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 6088, 1172,  ...,  546,  401,  925]], device='cuda:0')
Skip epoch 0, step 32, batch 32


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,    12,   266,  ...,   674,   369,   259]], device='cuda:0')
Skip epoch 0, step 33, batch 33


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[   0, 2652,  271,  ...,  634, 1921,  266],
        [   0,  266, 7407,  ...,  287,  365, 4371]], device='cuda:0')
Skip epoch 0, step 34, batch 34


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[    0,  4263,  4865,  ...,  1878,   278,  4450],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
Skip epoch 0, step 35, batch 35


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,   510,    13,  ...,  3454,   513,    13]], device='cuda:0')
Skip epoch 0, step 36, batch 36


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,   367,  6267,  ...,     9,   287, 17056],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
Skip epoch 0, step 37, batch 37


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[    0,  4323,  8213,  ...,  1153,   278,  4258],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
Skip epoch 0, step 38, batch 38


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
Skip epoch 0, step 39, batch 39


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1760,     9,  ...,  5996,    71,    19]], device='cuda:0')
Skip epoch 0, step 40, batch 40


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0,   266,  7264,  ...,  3211,   328,   275],
        [    0,  2496, 10545,  ...,   287, 13978,   408]], device='cuda:0')
Skip epoch 0, step 41, batch 41


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  6847,   437,  ...,    17,    14,    16]], device='cuda:0')
Skip epoch 0, step 42, batch 42


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 37683,    14,  1500],
        [    0,    12,  2085,  ...,   287,   593,  4137]], device='cuda:0')
Skip epoch 0, step 43, batch 43


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
Skip epoch 0, step 44, batch 44


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   271,  4787,  ...,   292,   494,   221]], device='cuda:0')
Skip epoch 0, step 45, batch 45


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[    0,  2923,   391,  ...,    14,  5424,   298],
        [    0,  4988, 36842,  ...,  7630, 18362,    13]], device='cuda:0')
Skip epoch 0, step 46, batch 46


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[    0,   908,    14,  ...,    19,    27,   287],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
Skip epoch 0, step 47, batch 47


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 5689,  271,  ...,  352, 9985, 3260]], device='cuda:0')
Skip epoch 0, step 48, batch 48


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
Skip epoch 0, step 49, batch 49


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,    13,  1529,  ...,  8197,  2700,  9629]], device='cuda:0')
Skip epoch 0, step 50, batch 50


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[   0, 8353, 1842,  ...,   38,  643,  472],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
Skip epoch 0, step 51, batch 51


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
Skip epoch 0, step 52, batch 52


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0,   271,   266,  ...,   275,  2576,  3588]], device='cuda:0')
Skip epoch 0, step 53, batch 53


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[   0, 3968,  446,  ...,   22,  722,  342],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
Skip epoch 0, step 54, batch 54


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[    0,  4538,   271,  ...,  1603,   591,   688],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
Skip epoch 0, step 55, batch 55


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,   396,   298,  ...,    52,  5065,    13]], device='cuda:0')
Skip epoch 0, step 56, batch 56


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   344, 10706,  ...,  1184,   578,   825],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
Skip epoch 0, step 57, batch 57


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
Skip epoch 0, step 58, batch 58


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 2728, 3139,  ..., 2254,  221,  380]], device='cuda:0')
Skip epoch 0, step 59, batch 59


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,   437,   638,  ...,  4514,    14,   333]], device='cuda:0')
Skip epoch 0, step 60, batch 60


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0,   638,  2708,  ..., 28492,  1814,    12],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
Skip epoch 0, step 61, batch 61


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[    0,   555,   764,  ...,   932,   709, 18731],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
Skip epoch 0, step 62, batch 62


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,  3990,   717,    12],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
Skip epoch 0, step 63, batch 63


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,   271,  3616,  ...,    12,  1348,  5037],
        [    0,   380,  2114,  ...,   456, 28979,   472]], device='cuda:0')
Skip epoch 0, step 64, batch 64


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0,  5583,   598,  ...,   199,   395,  6551],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
Skip epoch 0, step 65, batch 65


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,  1099,  2851,  ...,   518,   496,   287]], device='cuda:0')
Skip epoch 0, step 66, batch 66


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,   685,  3482,  ..., 23113,    12,  6481],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
Skip epoch 0, step 67, batch 67


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0,  287, 3609,  ..., 3661, 5944,  838]], device='cuda:0')
Skip epoch 0, step 68, batch 68


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,  3393,  3380,  ...,   292,  6502,   950]], device='cuda:0')
Skip epoch 0, step 69, batch 69


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
Skip epoch 0, step 70, batch 70


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[   0,  278,  668,  ..., 2743,  638,  609],
        [   0, 2836, 3084,  ..., 3634, 6464,  271]], device='cuda:0')
Skip epoch 0, step 71, batch 71


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
Skip epoch 0, step 72, batch 72


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[    0,  3164,    12,  ...,   984,   344,  3993],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
Skip epoch 0, step 73, batch 73


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[    0,   822,  5085,  ...,   293,  1608,   391],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
Skip epoch 0, step 74, batch 74


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0,   292, 23242,  ...,  6494,  3560,  1528],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
Skip epoch 0, step 75, batch 75


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0,  7111,   409,  ...,  1908,  1260,   883],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
Skip epoch 0, step 76, batch 76


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[   0,   12,  358,  ...,  352,  266,  319],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
Skip epoch 0, step 77, batch 77


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[    0,  1549,   824,  ...,  3609,   720,   417],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
Skip epoch 0, step 78, batch 78


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[    0,   292, 21215,  ...,   266,   818,  1527],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
Skip epoch 0, step 79, batch 79


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0, 5007, 7551,  ...,    9, 2095,  300],
        [   0,  271,  259,  ..., 1345,  352,  365]], device='cuda:0')
Skip epoch 0, step 80, batch 80


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[   0, 4337, 2057,  ..., 3020, 1722,  369],
        [   0,   15,   19,  ...,   12,  287, 7897]], device='cuda:0')
Skip epoch 0, step 81, batch 81


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[   0,   12, 3518,  ..., 1580, 2573,  409],
        [   0,  266, 1784,  ..., 1119, 1276,  292]], device='cuda:0')
Skip epoch 0, step 82, batch 82


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
Skip epoch 0, step 83, batch 83


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
Skip epoch 0, step 84, batch 84


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
Skip epoch 0, step 85, batch 85


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
Skip epoch 0, step 86, batch 86


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0, 16064, 10937,  ...,   346,   462,   221]], device='cuda:0')
Skip epoch 0, step 87, batch 87


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[   0, 2555,  984,  ..., 5900, 1576,  271],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
Skip epoch 0, step 88, batch 88


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0, 18905,  2311,  ..., 10213,   908,   694]], device='cuda:0')
Skip epoch 0, step 89, batch 89


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
Skip epoch 0, step 90, batch 90


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,  1083,   287,  ...,    12,   287,  2098]], device='cuda:0')
Skip epoch 0, step 91, batch 91


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[    0,  1265,  1545,  ...,   292, 36667, 36197],
        [    0,  6538,  1805,  ...,   298,   271,   721]], device='cuda:0')
Skip epoch 0, step 92, batch 92


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0,   634,  1621,  ...,   688,   586,  8477]], device='cuda:0')
Skip epoch 0, step 93, batch 93


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0,   259,  1380,  ...,   287, 10221,   280],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
Skip epoch 0, step 94, batch 94


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[   0,  759, 1184,  ...,  472,  346,   14],
        [   0,  560,  199,  ..., 6408,  278, 1119]], device='cuda:0')
Skip epoch 0, step 95, batch 95


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[   0, 3119,  278,  ...,  352,  674,  369],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
Skip epoch 0, step 96, batch 96


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
Skip epoch 0, step 97, batch 97


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
Skip epoch 0, step 98, batch 98


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[    0, 18125, 16419,  ...,   278,   638, 11744],
        [    0,   590,    16,  ...,    13,    35,  1151]], device='cuda:0')
Skip epoch 0, step 99, batch 99


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   17,  271,  266],
        [   0, 6112,  278,  ..., 4092,  490, 2774]], device='cuda:0')
Skip epoch 0, step 100, batch 100


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[   0, 1064, 1042,  ...,   12,  259, 4754],
        [   0,  278,  565,  ..., 1125, 5222,  287]], device='cuda:0')
Skip epoch 0, step 101, batch 101


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[    0,     9,   300,  ...,  6838,   328, 18619],
        [    0,  1934,  2413,  ..., 19697,    13, 16325]], device='cuda:0')
Skip epoch 0, step 102, batch 102


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0,  4485,   741,  ...,   292,   221,   341]], device='cuda:0')
Skip epoch 0, step 103, batch 103


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
Skip epoch 0, step 104, batch 104


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
Skip epoch 0, step 105, batch 105


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0, 3261, 5866,  ...,  593,  360, 2502],
        [   0,  527, 2811,  ...,  287, 1288,  352]], device='cuda:0')
Skip epoch 0, step 106, batch 106


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[   0,  300, 1064,  ..., 6953,  944,  278],
        [   0,   35, 3815,  ...,  278, 7097, 4601]], device='cuda:0')
Skip epoch 0, step 107, batch 107


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,   677, 20206,  ...,   292,   334,  1550]], device='cuda:0')
Skip epoch 0, step 108, batch 108


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[    0,  9017,   600,  ...,  6133,  1098,   352],
        [    0,   287,  6761,  ...,  1918, 33351,    12]], device='cuda:0')
Skip epoch 0, step 109, batch 109


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[    0,   445,     8,  ...,    13, 25386,    17],
        [    0,  1756,   271,  ...,   259, 48595, 19882]], device='cuda:0')
Skip epoch 0, step 110, batch 110


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,  3380,  1197,  ...,   631,   369,  3123]], device='cuda:0')
Skip epoch 0, step 111, batch 111


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0,   395,  5949,  ...,   341,    13,   635]], device='cuda:0')
Skip epoch 0, step 112, batch 112


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[   0,  360,  259,  ...,   14,  381, 1371],
        [   0,  995,   13,  ..., 3494,  367, 6768]], device='cuda:0')
Skip epoch 0, step 113, batch 113


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
Skip epoch 0, step 114, batch 114


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  658,  221,  474],
        [   0, 1732,  292,  ..., 3440, 4010, 1487]], device='cuda:0')
Skip epoch 0, step 115, batch 115


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[    0,  1874,   300,  ...,    14,  5372,    12],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
Skip epoch 0, step 116, batch 116


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,  1103,   271,  ...,   957,   756,   368],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
Skip epoch 0, step 117, batch 117


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0,  278, 8608,  ...,  293, 1608,  391]], device='cuda:0')
Skip epoch 0, step 118, batch 118


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
Skip epoch 0, step 119, batch 119


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
Skip epoch 0, step 120, batch 120


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0, 1503, 1785,  ...,  221,  380, 1869]], device='cuda:0')
Skip epoch 0, step 121, batch 121


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[   0,  462, 9202,  ...,   15, 3256,  271],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
Skip epoch 0, step 122, batch 122


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,   266,  8802,  ...,  8401,     9,   287],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
Skip epoch 0, step 123, batch 123


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
Skip epoch 0, step 124, batch 124


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[   0, 1268,  278,  ...,  461,  925,  630],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
Skip epoch 0, step 125, batch 125


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    14,   333,   199],
        [    0,   650,    14,  ...,  3687,   278, 26952]], device='cuda:0')
Skip epoch 0, step 126, batch 126


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  4665,   909,  ...,  3607,   259,  1108]], device='cuda:0')
Skip epoch 0, step 127, batch 127


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
Skip epoch 0, step 128, batch 128


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0, 1682,  271,  ...,  367, 3210,  271]], device='cuda:0')
Skip epoch 0, step 129, batch 129


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0,   471,    12,  ...,    13,  9909,  2673]], device='cuda:0')
Skip epoch 0, step 130, batch 130


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  221,  334,  ...,  706, 2680,  365]], device='cuda:0')
Skip epoch 0, step 131, batch 131


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[    0,  2159,   271,  ...,  1268,   344,   259],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
Skip epoch 0, step 132, batch 132


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[    0,    14,  3921,  ...,   199,  2038,  1963],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
Skip epoch 0, step 133, batch 133


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0, 1979,  352,  ...,  292, 1591,  446],
        [   0,  944,  278,  ..., 2374,  699, 8867]], device='cuda:0')
Skip epoch 0, step 134, batch 134


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[    0,  6673,   298,  ...,  4391,   292,   221],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
Skip epoch 0, step 135, batch 135


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[   0,  471,   14,  ..., 1260, 2129,  367],
        [   0,  278, 1099,  ...,  496,   14,  879]], device='cuda:0')
Skip epoch 0, step 136, batch 136


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0,  9430,   287,  ...,  1141,  2280,   408],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
Skip epoch 0, step 137, batch 137


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,    12,   630,  ...,  5049,    14,  2371],
        [    0,  2973, 20362,  ...,   271, 43821, 11776]], device='cuda:0')
Skip epoch 0, step 138, batch 138


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  3543,   391,  ...,  3370,  2926,  8090]], device='cuda:0')
Skip epoch 0, step 139, batch 139


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[    0,    13, 26335,  ...,     5,  2570, 34403],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
Skip epoch 0, step 140, batch 140


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,  1529,  5227,  ...,  1480,   367,   925]], device='cuda:0')
Skip epoch 0, step 141, batch 141


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,    14,  8058,  ..., 10316,   352,   266],
        [    0,   677,  9606,  ...,  9468,  9268,   328]], device='cuda:0')
Skip epoch 0, step 142, batch 142


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[    0,  4108,    85,  ...,    40,    12,  1530],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
Skip epoch 0, step 143, batch 143


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[    0,   452,   298,  ...,   287,  1575,  7856],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
Skip epoch 0, step 144, batch 144


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   685,   344,  ...,   680,   401,   616]], device='cuda:0')
Skip epoch 0, step 145, batch 145


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,  3001,  3325,  ..., 16332,  2661,  1200],
        [    0,    14,  6436,  ...,   271,  1211,  8917]], device='cuda:0')
Skip epoch 0, step 146, batch 146


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
Skip epoch 0, step 147, batch 147


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  266,  997,  ..., 2670,    5,  278]], device='cuda:0')
Skip epoch 0, step 148, batch 148


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[   0, 1412,   35,  ..., 6077,  298, 1826],
        [   0,  278, 5210,  ..., 1968, 2002,  923]], device='cuda:0')
Skip epoch 0, step 149, batch 149


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  380, 4053,  352],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
Skip epoch 0, step 150, batch 150


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
Skip epoch 0, step 151, batch 151


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,   300, 26138,  ...,  7856,    14, 17535]], device='cuda:0')
Skip epoch 0, step 152, batch 152


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[    0, 39224,    34,  ...,   401,  1716,   271],
        [    0,   741,  4933,  ...,   932,   365,   838]], device='cuda:0')
Skip epoch 0, step 153, batch 153


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0, 28590,    12,  ...,   342, 29639,  1693],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
Skip epoch 0, step 154, batch 154


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
Skip epoch 0, step 155, batch 155


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
Skip epoch 0, step 156, batch 156


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  4103,   278,  4190],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
Skip epoch 0, step 157, batch 157


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,   669,   292,  ...,  4032,   271,  4442]], device='cuda:0')
Skip epoch 0, step 158, batch 158


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,  361, 1224,  ..., 4401, 4261, 1663]], device='cuda:0')
Skip epoch 0, step 159, batch 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[   0, 1487,  409,  ..., 6979, 1273,  496],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
Skip epoch 0, step 160, batch 160


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   298,   527,   496],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
Skip epoch 0, step 161, batch 161


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0, 3086,  504,  ...,   14,  759,  935],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
Skip epoch 0, step 162, batch 162


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[   0, 4215, 1478,  ...,  644,  409, 3803],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
Skip epoch 0, step 163, batch 163


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[    0,    14,  4494,  ...,  4830,   368,   266],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
Skip epoch 0, step 164, batch 164


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,   593,   300,  ...,   278,  4694,    12],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
Skip epoch 0, step 165, batch 165


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,    15, 35654,     9],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
Skip epoch 0, step 166, batch 166


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0, 1529,  354,  ...,  709,  271,  266],
        [   0,  409,  699,  ...,   12,  546,  696]], device='cuda:0')
Skip epoch 0, step 167, batch 167


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,  3860,   694,  ...,  1027,   292,   221]], device='cuda:0')
Skip epoch 0, step 168, batch 168


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 6132,  300,  ...,   37,  271,  259]], device='cuda:0')
Skip epoch 0, step 169, batch 169


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[    0,    26,   874,  ...,    12, 21591,    12],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
Skip epoch 0, step 170, batch 170


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
Skip epoch 0, step 171, batch 171


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[   0,  287, 1070,  ...,  292,  221,  374],
        [   0,  935, 2613,  ...,  623, 4289, 6803]], device='cuda:0')
Skip epoch 0, step 172, batch 172


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0, 4100,   12,  ...,   13, 4710, 1558],
        [   0,  278,  266,  ...,   13, 2853,  445]], device='cuda:0')
Skip epoch 0, step 173, batch 173


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   344,  8133,  ...,   368,  1119,  5539]], device='cuda:0')
Skip epoch 0, step 174, batch 174


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0,   342,  4781,  ...,   630,   940,   271],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
Skip epoch 0, step 175, batch 175


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[   0,  275, 2351,  ...,   14, 4520,   12],
        [   0,  292,  685,  ...,  278, 3281,  298]], device='cuda:0')
Skip epoch 0, step 176, batch 176


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[   0,  638, 1276,  ..., 1589, 2432,  292],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
Skip epoch 0, step 177, batch 177


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,   598,   696,  ...,  4048,  1795,    14]], device='cuda:0')
Skip epoch 0, step 178, batch 178


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
Skip epoch 0, step 179, batch 179


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  607,  443,  ...,  259, 2646, 1597]], device='cuda:0')
Skip epoch 0, step 180, batch 180


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 46357, 11101, 10621],
        [    0, 19350,   271,  ...,   445,  1841,   446]], device='cuda:0')
Skip epoch 0, step 181, batch 181


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
Skip epoch 0, step 182, batch 182


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
Skip epoch 0, step 183, batch 183


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
Skip epoch 0, step 184, batch 184


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[   0,  285,  590,  ...,  199,  395, 3523],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
Skip epoch 0, step 185, batch 185


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0,    12, 30621,  ...,   578,  3126,    14],
        [    0,    13,  5005,  ...,   654,   344,   259]], device='cuda:0')
Skip epoch 0, step 186, batch 186


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[    0,  6762,   689,  ...,  7061,    14,   381],
        [    0, 31309,    83,  ...,  2923,   391,   266]], device='cuda:0')
Skip epoch 0, step 187, batch 187


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[    0, 31539,  1156,  ...,     9,   287, 26127],
        [    0, 13642, 14635,  ...,   367,  1040,  8580]], device='cuda:0')
Skip epoch 0, step 188, batch 188


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
Skip epoch 0, step 189, batch 189


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[    0, 21178,  1952,  ..., 14930,     9,   689],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
Skip epoch 0, step 190, batch 190


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,     9,   391,  ...,   300,  2646,  1717],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
Skip epoch 0, step 191, batch 191


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,  6541,   287,  ...,  1061,  4786,   292],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
Skip epoch 0, step 192, batch 192


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[    0,   199,   769,  ..., 12038, 15317,   342],
        [    0,  1832,   292,  ...,  2176,  1345,    14]], device='cuda:0')
Skip epoch 0, step 193, batch 193


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[    0, 14949,    12,  ...,   669, 10168,  7166],
        [    0,   401,   266,  ...,   266,  2236,  1458]], device='cuda:0')
Skip epoch 0, step 194, batch 194


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,   546,   360,  ...,  9107,  2772,  4496]], device='cuda:0')
Skip epoch 0, step 195, batch 195


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[    0,  7094,   596,  ...,  4764,  9514,    14],
        [    0, 10206,   342,  ...,  1336,  5046,   360]], device='cuda:0')
Skip epoch 0, step 196, batch 196


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[   0,  360, 2374,  ...,  221,  474,  357],
        [   0, 4882,   12,  ...,   12, 9575,  287]], device='cuda:0')
Skip epoch 0, step 197, batch 197


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[    0,  2261,     9,  ..., 15008,    14,   333],
        [    0,  2973,    30,  ...,   408,   259,  1914]], device='cuda:0')
Skip epoch 0, step 198, batch 198


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[   0, 5281, 4452,  ...,   14, 3391,   12],
        [   0, 2220, 1110,  ...,  382,   18,   13]], device='cuda:0')
Skip epoch 0, step 199, batch 199


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
Skip epoch 0, step 200, batch 200


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,    43,   503,    67],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
Skip epoch 0, step 201, batch 201


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  2377,   360,  ...,   266,  4745,   963]], device='cuda:0')
Skip epoch 0, step 202, batch 202


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
Skip epoch 0, step 203, batch 203


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
Skip epoch 0, step 204, batch 204


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[   0,  300,  369,  ...,   12,  970,   12],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
Skip epoch 0, step 205, batch 205


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[   0,  271, 3421,  ...,  306,  472,  346],
        [   0,  221, 4070,  ..., 1061, 3189,   26]], device='cuda:0')
Skip epoch 0, step 206, batch 206


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
Skip epoch 0, step 207, batch 207


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
Skip epoch 0, step 208, batch 208


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0,  2771,    13,  ...,  4169,   278,   266]], device='cuda:0')
Skip epoch 0, step 209, batch 209


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0,     9,   870,  ...,  2671,   965,  3229]], device='cuda:0')
Skip epoch 0, step 210, batch 210


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  2314,   516,  ...,  1871,    13,  1303]], device='cuda:0')
Skip epoch 0, step 211, batch 211


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0, 1358,  367,  ..., 1758, 2921,   12]], device='cuda:0')
Skip epoch 0, step 212, batch 212


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[    0,  1746,    14,  ...,  3134,  5968,     9],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
Skip epoch 0, step 213, batch 213


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0, 16803,   965,  ..., 36064,    12, 13769]], device='cuda:0')
Skip epoch 0, step 214, batch 214


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
Skip epoch 0, step 215, batch 215


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[   0,  275, 2101,  ..., 1145,  590, 1619],
        [   0, 1597,  278,  ...,   20,   38,  446]], device='cuda:0')
Skip epoch 0, step 216, batch 216


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[   0, 1862,  674,  ...,  391,  266, 7688],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
Skip epoch 0, step 217, batch 217


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
Skip epoch 0, step 218, batch 218


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,  6491,  3667,  ...,  5042,    14,  2152]], device='cuda:0')
Skip epoch 0, step 219, batch 219


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,  1932,    15,  ...,   344,   984,   344]], device='cuda:0')
Skip epoch 0, step 220, batch 220


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
Skip epoch 0, step 221, batch 221


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0, 5982, 9385,  ...,   26,  469,  446]], device='cuda:0')
Skip epoch 0, step 222, batch 222


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[   0, 3179,  221,  ...,  910,  706, 1102],
        [   0,  199, 2834,  ...,  287, 3121,  292]], device='cuda:0')
Skip epoch 0, step 223, batch 223


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[    0,  3235,   471,  ...,  1967,  4273,  2738],
        [    0, 21410, 13160,  ...,   292,    69,    14]], device='cuda:0')
Skip epoch 0, step 224, batch 224


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[   0,  278, 5717,  ..., 5342, 5147,   14],
        [   0, 6518,  681,  ...,  401, 9748,  391]], device='cuda:0')
Skip epoch 0, step 225, batch 225


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[    0,   292,    48,  ...,   199, 19047,   292],
        [    0, 23809, 27646,  ...,   266,  3373,   554]], device='cuda:0')
Skip epoch 0, step 226, batch 226


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,  2915,   275,  2565],
        [    0, 12472,  1059,  ...,   642,   365,  6517]], device='cuda:0')
Skip epoch 0, step 227, batch 227


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[    0,  1034,  5599,  ...,   259,   586,  1403],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
Skip epoch 0, step 228, batch 228


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[   0, 4995,  287,  ...,  300, 4531, 4729],
        [   0,  659,  278,  ..., 4032, 1109,  721]], device='cuda:0')
Skip epoch 0, step 229, batch 229


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,    12,  2735,  ...,    12,   344,  1496]], device='cuda:0')
Skip epoch 0, step 230, batch 230


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[    0,   287,  9430,  ...,  3121,   352,   360],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
Skip epoch 0, step 231, batch 231


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,  6909,   352,  ...,  1075,   706,  6909]], device='cuda:0')
Skip epoch 0, step 232, batch 232


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,    12,   546,  ..., 24994, 31107,   266]], device='cuda:0')
Skip epoch 0, step 233, batch 233


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[   0,   14,  747,  ...,  259, 6027, 1889],
        [   0,  642,  287,  ...,  800,   12, 3338]], device='cuda:0')
Skip epoch 0, step 234, batch 234


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0,   266,  2604,  ...,   278,  4035,  4165],
        [    0,   413,    20,  ...,  2089,    12, 21064]], device='cuda:0')
Skip epoch 0, step 235, batch 235


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0,  5151,   292,  ..., 13658,   401,  1070],
        [    0,  1875,  2117,  ...,  1422,  1059,   963]], device='cuda:0')
Skip epoch 0, step 236, batch 236


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
Skip epoch 0, step 237, batch 237


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
Skip epoch 0, step 238, batch 238


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0,   11,  360,  ..., 4524, 1553,  401]], device='cuda:0')
Skip epoch 0, step 239, batch 239


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
Skip epoch 0, step 240, batch 240


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[    0,   278,   554,  ...,   365,  3125,   271],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
Skip epoch 0, step 241, batch 241


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  1067,   271,  ...,   266,   940,   271],
        [    0,   365,  1941,  ..., 38029,  1790, 44066]], device='cuda:0')
Skip epoch 0, step 242, batch 242


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[    0,  1688,   790,  ...,   546,   696,    12],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
Skip epoch 0, step 243, batch 243


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0,  996, 2226,  ...,  516, 3470,   14],
        [   0, 1561,   14,  ..., 4433,  352, 1561]], device='cuda:0')
Skip epoch 0, step 244, batch 244


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[    0,    14,  1062,  ..., 10417,    13, 30579],
        [    0,    14,  7870,  ...,   284,   830,   292]], device='cuda:0')
Skip epoch 0, step 245, batch 245


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,  9582,  3645,  ...,  1027,    12,   461]], device='cuda:0')
Skip epoch 0, step 246, batch 246


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0, 24063,   717,  ...,  2228,  1416,     9],
        [    0,   741,   300,  ...,    83,  7111,   292]], device='cuda:0')
Skip epoch 0, step 247, batch 247


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[   0,  266, 3536,  ...,  266, 1883,  266],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
Skip epoch 0, step 248, batch 248


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[    0,  1713,   292,  ...,   596,   328,  1644],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
Skip epoch 0, step 249, batch 249


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[    0,   600,  9092,  ...,   554,  1485,   328],
        [    0, 42306,   278,  ...,  1110,  3427,  4224]], device='cuda:0')
Skip epoch 0, step 250, batch 250


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,   278,  7524,  ...,  1288,   669,   352],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
Skip epoch 0, step 251, batch 251


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,   12,  616,  ...,  278,  266, 2907]], device='cuda:0')
Skip epoch 0, step 252, batch 252


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[    0,    14,   469,  ...,   367,  2564,   368],
        [    0,   396,  1821,  ...,  5984, 18362,   278]], device='cuda:0')
Skip epoch 0, step 253, batch 253


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  342,  516,  ...,   12,  729, 3701]], device='cuda:0')
Skip epoch 0, step 254, batch 254


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,  1911,   679,  ...,    19,  3737,   609]], device='cuda:0')
Skip epoch 0, step 255, batch 255


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,    12,  4856,  ...,   342,   266,  1040],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
Skip epoch 0, step 256, batch 256


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[    0,  4902,   518,  ...,  5493,  3227,   278],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
Skip epoch 0, step 257, batch 257


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[   0, 3159,  278,  ...,  266, 2545,  863],
        [   0,  266, 2057,  ...,   88, 1801,   66]], device='cuda:0')
Skip epoch 0, step 258, batch 258


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
Skip epoch 0, step 259, batch 259


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
Skip epoch 0, step 260, batch 260


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[   0,  895, 4110,  ..., 1578, 1245,   13],
        [   0,   22, 2577,  ..., 4970,    9, 3868]], device='cuda:0')
Skip epoch 0, step 261, batch 261


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0, 3825, 1626,  ..., 5096, 3775,  266]], device='cuda:0')
Skip epoch 0, step 262, batch 262


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[    0,    13, 41550,  ...,    12,   546,  1996],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
Skip epoch 0, step 263, batch 263


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,  1106,   259,  ...,   271,   679,   382]], device='cuda:0')
Skip epoch 0, step 264, batch 264


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
Skip epoch 0, step 265, batch 265


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,   266,  2374,  ...,  1551,   518,   638],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
Skip epoch 0, step 266, batch 266


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[    0,   300,  5201,  ...,  1997,  7423,   417],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
Skip epoch 0, step 267, batch 267


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
Skip epoch 0, step 268, batch 268


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   518,  9048,  ...,  1354,   352,   266]], device='cuda:0')
Skip epoch 0, step 269, batch 269


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[    0,  1485,   271,  ...,  6359,  1799,  5442],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
Skip epoch 0, step 270, batch 270


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[   0,  328, 6875,  ...,  369,  654,  300],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
Skip epoch 0, step 271, batch 271


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,   560, 23501,  ...,   292,   494,   221]], device='cuda:0')
Skip epoch 0, step 272, batch 272


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[   0, 7185,  328,  ..., 1427, 1477, 1061],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
Skip epoch 0, step 273, batch 273


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
Skip epoch 0, step 274, batch 274


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0, 26473,  2117,  ...,    13,  3292,   950]], device='cuda:0')
Skip epoch 0, step 275, batch 275


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[    0,   685,  2461,  ...,   287,   298,  7943],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
Skip epoch 0, step 276, batch 276


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
Skip epoch 0, step 277, batch 277


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0, 2310,  292,  ...,  462,  508,  586]], device='cuda:0')
Skip epoch 0, step 278, batch 278


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  292,   33,  ...,  352,  266, 9129]], device='cuda:0')
Skip epoch 0, step 279, batch 279


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
Skip epoch 0, step 280, batch 280


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[   0, 1850,  311,  ..., 3655, 3133, 9000],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
Skip epoch 0, step 281, batch 281


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
Skip epoch 0, step 282, batch 282


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0,  461,  654,  ..., 6548, 7171,   14]], device='cuda:0')
Skip epoch 0, step 283, batch 283


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
Skip epoch 0, step 284, batch 284


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[    0,    14, 49045,  ...,    12,   706,   409],
        [    0,   806,   300,  ...,   360,  4918,  1106]], device='cuda:0')
Skip epoch 0, step 285, batch 285


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[  0, 669,  14,  ..., 596, 292, 494],
        [  0, 432, 984,  ..., 287, 496,  14]], device='cuda:0')
Skip epoch 0, step 286, batch 286


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  266,  720,  342],
        [   0, 2992,  352,  ...,  259, 2063, 6088]], device='cuda:0')
Skip epoch 0, step 287, batch 287


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,  1890,   278,  ...,  1400,   367,  1874]], device='cuda:0')
Skip epoch 0, step 288, batch 288


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,   843,  2621,  ...,  4589,   278, 14266],
        [    0,  7849,   278,  ...,   346,   462,   221]], device='cuda:0')
Skip epoch 0, step 289, batch 289


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[    0,  3658,   271,  ...,   278,   970,    12],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
Skip epoch 0, step 290, batch 290


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   298,   335,   298],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
Skip epoch 0, step 291, batch 291


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[   0,  397, 1267,  ..., 1276,  292,  221],
        [   0,   14,  475,  ..., 6895, 5842, 2239]], device='cuda:0')
Skip epoch 0, step 292, batch 292


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,   741,   266,  ...,   271,  5166,   596]], device='cuda:0')
Skip epoch 0, step 293, batch 293


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0,  1737,   278,  ...,  2604,   367,  2002]], device='cuda:0')
Skip epoch 0, step 294, batch 294


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
Skip epoch 0, step 295, batch 295


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,   292,    36,     9],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
Skip epoch 0, step 296, batch 296


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,  9466,    36,  ...,  1795,   437,   874],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
Skip epoch 0, step 297, batch 297


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0,   292,    33,  ..., 32754,   300, 14476]], device='cuda:0')
Skip epoch 0, step 298, batch 298


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[    0,  2344,   271,  ...,  5415,    14,  1075],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
Skip epoch 0, step 299, batch 299


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0,  578,  221,  ...,  287, 1254, 4318]], device='cuda:0')
Skip epoch 0, step 300, batch 300


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  9472, 15004,   511],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
Skip epoch 0, step 301, batch 301


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   401,  3740,  ...,  5980,   271,   266]], device='cuda:0')
Skip epoch 0, step 302, batch 302


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[   0, 1458,  365,  ..., 5399, 1110,  870],
        [   0,  365, 2849,  ...,    9, 3365, 5027]], device='cuda:0')
Skip epoch 0, step 303, batch 303


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,   377,   472,  ...,  9256,  3807,  5499]], device='cuda:0')
Skip epoch 0, step 304, batch 304


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[    0, 10565,  2677,  ...,   298,   292, 11188],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
Skip epoch 0, step 305, batch 305


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
Skip epoch 0, step 306, batch 306


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,   292,    65,  ...,    12,   857,   344],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
Skip epoch 0, step 307, batch 307


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[   0, 1159,  278,  ...,    9,  271,  266],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
Skip epoch 0, step 308, batch 308


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[  0, 342, 726,  ...,  12, 895, 367],
        [  0, 278, 266,  ..., 292, 474, 221]], device='cuda:0')
Skip epoch 0, step 309, batch 309


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,   586,   940,  ...,  1471,  2612,   591]], device='cuda:0')
Skip epoch 0, step 310, batch 310


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[   0,  401, 9370,  ...,    9,  287,  518],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
Skip epoch 0, step 311, batch 311


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
Skip epoch 0, step 312, batch 312


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[    0,  1471,   266,  ...,   525,  5202,   292],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
Skip epoch 0, step 313, batch 313


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0, 3978, 2697,  ...,  461, 5955, 3792]], device='cuda:0')
Skip epoch 0, step 314, batch 314


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0,   515,   352,  ..., 21190,  1871,   950]], device='cuda:0')
Skip epoch 0, step 315, batch 315


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0, 2663,  328,  ...,  342,  266, 1163]], device='cuda:0')
Skip epoch 0, step 316, batch 316


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
Skip epoch 0, step 317, batch 317


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
Skip epoch 0, step 318, batch 318


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
Skip epoch 0, step 319, batch 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[    0,  3036,   471,  ...,   287,  1906,    12],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-6.6034e-05, -3.2656e-05, -2.0594e-05,  ...,  2.4189e-05,
         -3.1029e-05,  1.7830e-05],
        [-1.9617e-05, -1.4003e-05,  2.7839e-06,  ..., -1.6031e-05,
         -1.6372e-06, -7.2464e-06],
        [ 6.5451e-05,  4.7150e-05, -1.2117e-05,  ...,  5.4358e-05,
          9.9076e-06,  2.3535e-05],
        [-3.1380e-05, -2.0713e-05,  5.5430e-06,  ..., -2.5218e-05,
         -3.7083e-06, -1.1165e-05],
        [-5.7018e-05, -3.9735e-05,  8.6860e-06,  ..., -4.6932e-05,
         -6.9917e-06, -2.1938e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9243e-08, 1.9922e-08, 3.0023e-08,  ..., 2.2275e-08, 5.2546e-08,
         1.1303e-08],
        [3.5513e-11, 2.1642e-11, 1.2695e-12,  ..., 2.3960e-11, 8.7104e-13,
         5.0996e-12],
        [6.4114e-10, 3.7711e-10, 2.2076e-11,  ..., 4.9322e-10, 1.2086e-11,
         9.5409e-11],
        [1.4688e-10, 2.1418e-10, 4.7553e-12,  ..., 1.1768e-10, 1.6752e-11,
         7.3225e-11],
        [1.3357e-10, 7.5910e-11, 6.2641e-12,  ..., 9.1211e-11, 1.6925e-12,
         1.9936e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1740e-06, -6.8921e-06, -2.1118e-05,  ..., -1.6407e-05,
          2.3121e-05,  1.0554e-05],
        [-2.9951e-06, -2.0713e-06,  7.4133e-07,  ..., -2.5332e-06,
         -7.0408e-07, -1.1474e-06],
        [-3.1888e-06, -2.2054e-06,  7.8976e-07,  ..., -2.6971e-06,
         -7.4878e-07, -1.2219e-06],
        [-4.3809e-06, -3.0398e-06,  1.0878e-06,  ..., -3.7104e-06,
         -1.0282e-06, -1.6838e-06],
        [-6.5863e-06, -4.5598e-06,  1.6317e-06,  ..., -5.5730e-06,
         -1.5497e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.1565660238265991


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[   0,  266, 2653,  ...,   29,   16,   14],
        [   0,  591,  953,  ..., 4118, 5750,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2432e-06, -1.6836e-05, -3.5724e-05,  ..., -1.8520e-05,
         -2.1239e-05,  5.9948e-06],
        [-5.9754e-06, -4.1127e-06,  1.4901e-06,  ..., -5.0366e-06,
         -1.3523e-06, -2.2873e-06],
        [-6.3628e-06, -4.3809e-06,  1.5907e-06,  ..., -5.3793e-06,
         -1.4417e-06, -2.4363e-06],
        [-8.7917e-06, -6.0648e-06,  2.1979e-06,  ..., -7.4208e-06,
         -1.9893e-06, -3.3677e-06],
        [-1.3113e-05, -9.0301e-06,  3.2783e-06,  ..., -1.1086e-05,
         -2.9728e-06, -5.0217e-06]], device='cuda:0')
Loss: 1.1449904441833496


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[   0,  287,  221,  ..., 1871, 1482,   12],
        [   0,  897,  328,  ...,  908,  696,  688]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8185e-05, -3.1638e-05, -6.2136e-05,  ..., -6.8809e-06,
         -5.6388e-05,  2.1366e-05],
        [-9.0003e-06, -6.1989e-06,  2.2314e-06,  ..., -7.5847e-06,
         -2.0079e-06, -3.4124e-06],
        [-9.5516e-06, -6.5863e-06,  2.3767e-06,  ..., -8.0764e-06,
         -2.1346e-06, -3.6284e-06],
        [-1.3232e-05, -9.1344e-06,  3.2932e-06,  ..., -1.1161e-05,
         -2.9504e-06, -5.0217e-06],
        [-1.9759e-05, -1.3620e-05,  4.9099e-06,  ..., -1.6689e-05,
         -4.4107e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1571062803268433


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5235e-05, -1.3642e-05, -7.8817e-05,  ..., -1.5784e-07,
         -5.7069e-05, -1.9948e-05],
        [-1.1981e-05, -8.2552e-06,  2.9914e-06,  ..., -1.0088e-05,
         -2.6524e-06, -4.5300e-06],
        [ 6.9772e-05,  5.3267e-05, -2.4274e-05,  ...,  6.7980e-05,
          2.6218e-05,  2.8134e-05],
        [-1.7643e-05, -1.2189e-05,  4.4182e-06,  ..., -1.4871e-05,
         -3.9041e-06, -6.6757e-06],
        [-2.6256e-05, -1.8120e-05,  6.5640e-06,  ..., -2.2173e-05,
         -5.8189e-06, -9.9391e-06]], device='cuda:0')
Loss: 1.1563949584960938


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,  6067,  1188,  ...,  5282,   756,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4143e-05, -2.0004e-05, -8.6145e-05,  ..., -4.4385e-05,
         -6.1843e-05, -8.5935e-06],
        [-1.4961e-05, -1.0312e-05,  3.7253e-06,  ..., -1.2621e-05,
         -3.3155e-06, -5.6475e-06],
        [ 6.6598e-05,  5.1092e-05, -2.3495e-05,  ...,  6.5283e-05,
          2.5514e-05,  2.6942e-05],
        [-2.2054e-05, -1.5214e-05,  5.4985e-06,  ..., -1.8612e-05,
         -4.8801e-06, -8.3223e-06],
        [-3.2842e-05, -2.2620e-05,  8.1733e-06,  ..., -2.7746e-05,
         -7.2792e-06, -1.2398e-05]], device='cuda:0')
Loss: 1.1582653522491455


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  2849,  1173,  ...,  1481,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4143e-05,  1.2763e-05, -3.3527e-05,  ..., -3.7067e-05,
         -6.2493e-05,  1.8547e-05],
        [-1.7986e-05, -1.2383e-05,  4.4629e-06,  ..., -1.5140e-05,
         -3.9749e-06, -6.7875e-06],
        [ 6.3365e-05,  4.8871e-05, -2.2709e-05,  ...,  6.2586e-05,
          2.4806e-05,  2.5720e-05],
        [-2.6524e-05, -1.8269e-05,  6.5863e-06,  ..., -2.2337e-05,
         -5.8562e-06, -1.0014e-05],
        [-3.9488e-05, -2.7180e-05,  9.7901e-06,  ..., -3.3289e-05,
         -8.7321e-06, -1.4901e-05]], device='cuda:0')
Loss: 1.1546097993850708


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[   0,  462,  221,  ...,   29,  413, 1801],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4480e-05,  2.3672e-06, -4.5450e-05,  ..., -2.7602e-05,
         -8.3725e-05,  1.4760e-05],
        [-2.0981e-05, -1.4439e-05,  5.2229e-06,  ..., -1.7658e-05,
         -4.6343e-06, -7.9349e-06],
        [ 6.0146e-05,  4.6666e-05, -2.1893e-05,  ...,  5.9889e-05,
          2.4098e-05,  2.4491e-05],
        [-3.0965e-05, -2.1324e-05,  7.7188e-06,  ..., -2.6062e-05,
         -6.8322e-06, -1.1712e-05],
        [-4.6074e-05, -3.1710e-05,  1.1459e-05,  ..., -3.8832e-05,
         -1.0185e-05, -1.7419e-05]], device='cuda:0')
Loss: 1.1484636068344116


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0, 2018, 4798,  ...,  292, 1919,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2951e-05,  8.8218e-07, -4.3526e-05,  ..., -4.4030e-05,
         -1.0836e-04, -1.4958e-05],
        [-2.3931e-05, -1.6481e-05,  5.9865e-06,  ..., -2.0161e-05,
         -5.2825e-06, -9.0748e-06],
        [ 5.6987e-05,  4.4475e-05, -2.1074e-05,  ...,  5.7206e-05,
          2.3402e-05,  2.3269e-05],
        [-3.5375e-05, -2.4378e-05,  8.8662e-06,  ..., -2.9802e-05,
         -7.8008e-06, -1.3418e-05],
        [-5.2571e-05, -3.6210e-05,  1.3135e-05,  ..., -4.4316e-05,
         -1.1608e-05, -1.9923e-05]], device='cuda:0')
Loss: 1.1499038934707642
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0150,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0270,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2725e-05, -2.9302e-05, -2.2887e-05,  ...,  1.7368e-05,
         -3.8761e-05,  1.4551e-05],
        [-2.0048e-05, -1.4251e-05,  3.1042e-06,  ..., -1.6444e-05,
         -2.0017e-06, -7.4292e-06],
        [ 6.4604e-05,  4.6883e-05, -1.3013e-05,  ...,  5.4643e-05,
          1.1257e-05,  2.3509e-05],
        [-3.1779e-05, -2.1079e-05,  5.8753e-06,  ..., -2.5676e-05,
         -4.1176e-06, -1.1390e-05],
        [-5.6573e-05, -3.9382e-05,  9.1310e-06,  ..., -4.6670e-05,
         -7.4533e-06, -2.1737e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9184e-08, 1.9902e-08, 2.9995e-08,  ..., 2.2255e-08, 5.2505e-08,
         1.1292e-08],
        [3.6050e-11, 2.1892e-11, 1.3040e-12,  ..., 2.4342e-11, 8.9808e-13,
         5.1769e-12],
        [6.4375e-10, 3.7871e-10, 2.2498e-11,  ..., 4.9599e-10, 1.2622e-11,
         9.5855e-11],
        [1.4798e-10, 2.1456e-10, 4.8291e-12,  ..., 1.1845e-10, 1.6796e-11,
         7.3332e-11],
        [1.3620e-10, 7.7145e-11, 6.4304e-12,  ..., 9.3084e-11, 1.8256e-12,
         2.0313e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.9172541214186228e-05, 1.9172541214186228e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[    0, 12456,    14,  ...,  1822,  1016,   365],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8856e-05,  3.9725e-05, -1.3340e-06,  ...,  1.2861e-05,
          1.8204e-05,  1.3412e-05],
        [-2.9951e-06, -2.0862e-06,  7.4133e-07,  ..., -2.5183e-06,
         -7.0408e-07, -1.1101e-06],
        [-3.2187e-06, -2.2352e-06,  7.9349e-07,  ..., -2.7120e-06,
         -7.5251e-07, -1.1921e-06],
        [-4.4107e-06, -3.0696e-06,  1.0952e-06,  ..., -3.7253e-06,
         -1.0356e-06, -1.6391e-06],
        [-6.5863e-06, -4.5598e-06,  1.6242e-06,  ..., -5.5432e-06,
         -1.5423e-06, -2.4289e-06]], device='cuda:0')
Loss: 1.1527483463287354


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[    0,   417,   199,  ...,  1853,    12,   709],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5053e-05,  3.8517e-05, -1.2086e-05,  ..., -1.1055e-05,
          7.7430e-06, -2.4851e-05],
        [-5.9307e-06, -4.1276e-06,  1.5013e-06,  ..., -5.0515e-06,
         -1.3821e-06, -2.2128e-06],
        [-6.4075e-06, -4.4554e-06,  1.6168e-06,  ..., -5.4538e-06,
         -1.4864e-06, -2.3916e-06],
        [-8.8215e-06, -6.1393e-06,  2.2352e-06,  ..., -7.5102e-06,
         -2.0489e-06, -3.2932e-06],
        [-1.3083e-05, -9.0897e-06,  3.3006e-06,  ..., -1.1116e-05,
         -3.0398e-06, -4.8727e-06]], device='cuda:0')
Loss: 1.1714617013931274


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7624e-05,  1.4769e-05, -4.8441e-05,  ..., -2.1421e-06,
          2.4882e-05, -2.1306e-05],
        [-8.8960e-06, -6.1840e-06,  2.2613e-06,  ..., -7.5400e-06,
         -2.0675e-06, -3.3155e-06],
        [-9.6411e-06, -6.7055e-06,  2.4475e-06,  ..., -8.1807e-06,
         -2.2352e-06, -3.5986e-06],
        [-1.3262e-05, -9.2238e-06,  3.3751e-06,  ..., -1.1235e-05,
         -3.0696e-06, -4.9472e-06],
        [-1.9640e-05, -1.3649e-05,  4.9770e-06,  ..., -1.6630e-05,
         -4.5523e-06, -7.3165e-06]], device='cuda:0')
Loss: 1.1366559267044067


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,     9,   287,  ...,   259,  8244,  1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1960e-05, -4.3951e-06, -7.0975e-05,  ..., -1.9574e-05,
          2.6155e-05, -3.3017e-06],
        [-1.1846e-05, -8.2552e-06,  3.0510e-06,  ..., -1.0058e-05,
         -2.7493e-06, -4.4107e-06],
        [-1.2830e-05, -8.9407e-06,  3.3006e-06,  ..., -1.0893e-05,
         -2.9691e-06, -4.7833e-06],
        [-1.7703e-05, -1.2338e-05,  4.5672e-06,  ..., -1.5020e-05,
         -4.0904e-06, -6.5938e-06],
        [-2.6107e-05, -1.8209e-05,  6.7055e-06,  ..., -2.2143e-05,
         -6.0499e-06, -9.7156e-06]], device='cuda:0')
Loss: 1.1493918895721436


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9003e-05,  8.8235e-06, -6.1041e-05,  ..., -4.7907e-06,
          1.4810e-05,  1.6764e-05],
        [-1.4812e-05, -1.0327e-05,  3.8408e-06,  ..., -1.2577e-05,
         -3.4571e-06, -5.5060e-06],
        [-1.6034e-05, -1.1176e-05,  4.1574e-06,  ..., -1.3620e-05,
         -3.7327e-06, -5.9754e-06],
        [-2.2143e-05, -1.5453e-05,  5.7593e-06,  ..., -1.8790e-05,
         -5.1484e-06, -8.2403e-06],
        [-3.2663e-05, -2.2799e-05,  8.4564e-06,  ..., -2.7716e-05,
         -7.6145e-06, -1.2144e-05]], device='cuda:0')
Loss: 1.1504765748977661


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[    0,    34,  3881,  ...,  1027,   271,   266],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3819e-05, -2.7488e-05, -3.7450e-05,  ..., -1.1475e-05,
         -3.8647e-05, -4.8941e-06],
        [-1.7732e-05, -1.2383e-05,  4.6007e-06,  ..., -1.5065e-05,
         -4.0978e-06, -6.5863e-06],
        [-1.9222e-05, -1.3411e-05,  4.9844e-06,  ..., -1.6347e-05,
         -4.4331e-06, -7.1526e-06],
        [-2.6584e-05, -1.8567e-05,  6.9141e-06,  ..., -2.2575e-05,
         -6.1169e-06, -9.8795e-06],
        [-3.9101e-05, -2.7329e-05,  1.0133e-05,  ..., -3.3230e-05,
         -9.0301e-06, -1.4529e-05]], device='cuda:0')
Loss: 1.1536362171173096


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 3795,  908,  587],
        [   0,  368, 2418,  ..., 3275, 1116, 5189]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8580e-05,  1.0993e-05, -9.3765e-05,  ...,  2.1073e-05,
         -3.9785e-05, -4.2623e-06],
        [-2.0683e-05, -1.4424e-05,  5.3830e-06,  ..., -1.7568e-05,
         -4.7795e-06, -7.7114e-06],
        [-2.2441e-05, -1.5631e-05,  5.8375e-06,  ..., -1.9073e-05,
         -5.1782e-06, -8.3819e-06],
        [-3.0994e-05, -2.1622e-05,  8.0839e-06,  ..., -2.6315e-05,
         -7.1377e-06, -1.1563e-05],
        [-4.5598e-05, -3.1829e-05,  1.1854e-05,  ..., -3.8743e-05,
         -1.0535e-05, -1.7002e-05]], device='cuda:0')
Loss: 1.146456003189087


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,  3605,  2572,  ...,   300,   259,  1513],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8988e-05,  1.0590e-05, -7.2088e-05,  ..., -1.3793e-05,
         -3.9654e-05, -2.5880e-05],
        [-2.3633e-05, -1.6496e-05,  6.1467e-06,  ..., -2.0102e-05,
         -5.4464e-06, -8.7991e-06],
        [-2.5630e-05, -1.7866e-05,  6.6571e-06,  ..., -2.1800e-05,
         -5.8971e-06, -9.5442e-06],
        [-3.5435e-05, -2.4736e-05,  9.2313e-06,  ..., -3.0100e-05,
         -8.1360e-06, -1.3188e-05],
        [-5.2154e-05, -3.6418e-05,  1.3538e-05,  ..., -4.4346e-05,
         -1.2018e-05, -1.9401e-05]], device='cuda:0')
Loss: 1.1641075611114502
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0340],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0165,  0.0146, -0.0271,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.9554e-05, -2.5313e-05, -2.7807e-05,  ...,  1.4252e-05,
         -3.8851e-05,  1.0508e-05],
        [-2.0407e-05, -1.4476e-05,  3.4084e-06,  ..., -1.6809e-05,
         -2.3462e-06, -7.5662e-06],
        [ 5.5581e-05,  4.0408e-05, -1.1046e-05,  ...,  4.6999e-05,
          9.5416e-06,  2.0203e-05],
        [-3.2145e-05, -2.1445e-05,  6.2109e-06,  ..., -2.6119e-05,
         -4.5194e-06, -1.1570e-05],
        [-5.6131e-05, -3.9086e-05,  9.5716e-06,  ..., -4.6438e-05,
         -7.9098e-06, -2.1503e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9130e-08, 1.9882e-08, 2.9970e-08,  ..., 2.2233e-08, 5.2454e-08,
         1.1282e-08],
        [3.6572e-11, 2.2142e-11, 1.3405e-12,  ..., 2.4722e-11, 9.2684e-13,
         5.2491e-12],
        [6.4376e-10, 3.7865e-10, 2.2520e-11,  ..., 4.9597e-10, 1.2644e-11,
         9.5850e-11],
        [1.4909e-10, 2.1496e-10, 4.9095e-12,  ..., 1.1924e-10, 1.6846e-11,
         7.3433e-11],
        [1.3879e-10, 7.8394e-11, 6.6072e-12,  ..., 9.4957e-11, 1.9682e-12,
         2.0669e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[  0, 792,  83,  ..., 300, 768, 932],
        [  0,  76,  15,  ...,  14, 333, 199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2201e-06, -1.4786e-06,  1.6626e-05,  ...,  5.6678e-06,
         -6.4427e-06,  1.6409e-05],
        [-2.8908e-06, -2.0266e-06,  7.5996e-07,  ..., -2.4736e-06,
         -6.9290e-07, -1.0878e-06],
        [-3.1888e-06, -2.2352e-06,  8.4192e-07,  ..., -2.7269e-06,
         -7.6368e-07, -1.1995e-06],
        [-4.4405e-06, -3.0994e-06,  1.1697e-06,  ..., -3.7849e-06,
         -1.0580e-06, -1.6615e-06],
        [-6.4075e-06, -4.4703e-06,  1.6913e-06,  ..., -5.4836e-06,
         -1.5348e-06, -2.4140e-06]], device='cuda:0')
Loss: 1.1636883020401


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[   0, 1042, 5738,  ...,   12,  287, 3643],
        [   0,  775,  266,  ...,  409,  328, 5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8916e-05,  1.9557e-05,  2.8498e-05,  ..., -8.5058e-06,
          2.1852e-05,  2.8641e-05],
        [-5.7966e-06, -4.0531e-06,  1.5236e-06,  ..., -5.0068e-06,
         -1.4007e-06, -2.1979e-06],
        [-6.3777e-06, -4.4703e-06,  1.6764e-06,  ..., -5.4985e-06,
         -1.5423e-06, -2.4214e-06],
        [-8.8513e-06, -6.1989e-06,  2.3320e-06,  ..., -7.6294e-06,
         -2.1383e-06, -3.3528e-06],
        [-1.2755e-05, -8.9109e-06,  3.3602e-06,  ..., -1.1027e-05,
         -3.0845e-06, -4.8429e-06]], device='cuda:0')
Loss: 1.1381300687789917


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,  1235,    14,  ...,  3301,   549,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4162e-06,  4.3048e-05,  1.7015e-05,  ..., -2.6197e-05,
          4.6975e-05,  3.8164e-05],
        [-8.6874e-06, -6.0797e-06,  2.2762e-06,  ..., -7.5102e-06,
         -2.0899e-06, -3.2857e-06],
        [-9.5665e-06, -6.7055e-06,  2.5071e-06,  ..., -8.2552e-06,
         -2.2985e-06, -3.6210e-06],
        [-1.3292e-05, -9.3132e-06,  3.4943e-06,  ..., -1.1474e-05,
         -3.1963e-06, -5.0291e-06],
        [-1.9133e-05, -1.3411e-05,  5.0217e-06,  ..., -1.6570e-05,
         -4.6045e-06, -7.2569e-06]], device='cuda:0')
Loss: 1.1515727043151855


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0, 1380,  342,  ..., 3904,  259,  624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1938e-05,  4.5277e-05,  1.8886e-05,  ..., -3.7330e-05,
          5.6110e-05,  4.8822e-05],
        [-1.1623e-05, -8.1211e-06,  3.0287e-06,  ..., -1.0058e-05,
         -2.7753e-06, -4.3958e-06],
        [-1.2770e-05, -8.9407e-06,  3.3304e-06,  ..., -1.1027e-05,
         -3.0473e-06, -4.8280e-06],
        [-1.7762e-05, -1.2428e-05,  4.6417e-06,  ..., -1.5348e-05,
         -4.2394e-06, -6.7130e-06],
        [-2.5600e-05, -1.7911e-05,  6.6757e-06,  ..., -2.2143e-05,
         -6.1095e-06, -9.6858e-06]], device='cuda:0')
Loss: 1.153490424156189


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,   300,   344,  ...,    14,  5077,  2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6012e-05,  6.5692e-05,  5.6253e-06,  ..., -5.8176e-05,
          9.0172e-05,  8.3765e-05],
        [-1.4544e-05, -1.0177e-05,  3.7849e-06,  ..., -1.2547e-05,
         -3.4943e-06, -5.5283e-06],
        [-1.6004e-05, -1.1206e-05,  4.1686e-06,  ..., -1.3784e-05,
         -3.8408e-06, -6.0797e-06],
        [-2.2233e-05, -1.5572e-05,  5.8040e-06,  ..., -1.9163e-05,
         -5.3346e-06, -8.4490e-06],
        [-3.2067e-05, -2.2471e-05,  8.3596e-06,  ..., -2.7686e-05,
         -7.7039e-06, -1.2204e-05]], device='cuda:0')
Loss: 1.133782148361206


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[   0, 4852,  266,  ..., 2523, 2080, 2632],
        [   0, 1234,  278,  ..., 8635,  271,  546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5934e-06,  3.8341e-05,  7.6326e-06,  ..., -5.3497e-05,
          6.7916e-05,  8.1420e-05],
        [-1.7434e-05, -1.2219e-05,  4.5486e-06,  ..., -1.5050e-05,
         -4.1649e-06, -6.6236e-06],
        [-1.9193e-05, -1.3456e-05,  5.0105e-06,  ..., -1.6555e-05,
         -4.5821e-06, -7.2867e-06],
        [-2.6643e-05, -1.8686e-05,  6.9737e-06,  ..., -2.3007e-05,
         -6.3628e-06, -1.0118e-05],
        [-3.8475e-05, -2.6971e-05,  1.0043e-05,  ..., -3.3230e-05,
         -9.1866e-06, -1.4618e-05]], device='cuda:0')
Loss: 1.1444469690322876


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,  4868,  1027,  ...,   409,  3047,  2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6058e-06,  6.1602e-05, -6.7943e-06,  ..., -4.3205e-05,
          6.0685e-05,  6.9524e-05],
        [-2.0370e-05, -1.4290e-05,  5.3011e-06,  ..., -1.7568e-05,
         -4.8615e-06, -7.7337e-06],
        [-2.2411e-05, -1.5721e-05,  5.8338e-06,  ..., -1.9297e-05,
         -5.3421e-06, -8.5011e-06],
        [-3.1084e-05, -2.1815e-05,  8.1137e-06,  ..., -2.6807e-05,
         -7.4133e-06, -1.1794e-05],
        [-4.4912e-05, -3.1501e-05,  1.1690e-05,  ..., -3.8743e-05,
         -1.0714e-05, -1.7047e-05]], device='cuda:0')
Loss: 1.1479434967041016


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[   0,   12, 1250,  ...,  381, 1524, 2204],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6621e-05,  6.0937e-05, -8.9633e-06,  ..., -3.5738e-05,
          5.8254e-05,  1.2594e-04],
        [-2.3305e-05, -1.6332e-05,  6.0722e-06,  ..., -2.0072e-05,
         -5.5321e-06, -8.8289e-06],
        [-2.5615e-05, -1.7956e-05,  6.6794e-06,  ..., -2.2039e-05,
         -6.0759e-06, -9.7007e-06],
        [-3.5554e-05, -2.4915e-05,  9.2909e-06,  ..., -3.0622e-05,
         -8.4266e-06, -1.3456e-05],
        [-5.1409e-05, -3.6031e-05,  1.3396e-05,  ..., -4.4286e-05,
         -1.2197e-05, -1.9461e-05]], device='cuda:0')
Loss: 1.1637566089630127
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0094, -0.0022, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0021],
        [-0.0165,  0.0147, -0.0271,  ...,  0.0282, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1937e-05, -1.6688e-05, -2.5923e-05,  ...,  9.2525e-06,
         -2.9140e-05,  2.2051e-05],
        [-2.0697e-05, -1.4661e-05,  3.6748e-06,  ..., -1.7136e-05,
         -2.6648e-06, -7.6925e-06],
        [ 4.7461e-05,  3.4571e-05, -9.2734e-06,  ...,  4.0095e-05,
          7.9798e-06,  1.7213e-05],
        [-3.2486e-05, -2.1792e-05,  6.5189e-06,  ..., -2.6569e-05,
         -4.9101e-06, -1.1759e-05],
        [-5.5659e-05, -3.8780e-05,  9.9541e-06,  ..., -4.6223e-05,
         -8.3385e-06, -2.1299e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9072e-08, 1.9866e-08, 2.9940e-08,  ..., 2.2212e-08, 5.2405e-08,
         1.1286e-08],
        [3.7079e-11, 2.2387e-11, 1.3760e-12,  ..., 2.5100e-11, 9.5652e-13,
         5.3218e-12],
        [6.4377e-10, 3.7860e-10, 2.2542e-11,  ..., 4.9596e-10, 1.2668e-11,
         9.5849e-11],
        [1.5021e-10, 2.1536e-10, 4.9909e-12,  ..., 1.2006e-10, 1.6900e-11,
         7.3540e-11],
        [1.4129e-10, 7.9614e-11, 6.7801e-12,  ..., 9.6823e-11, 2.1150e-12,
         2.1027e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.90712744520069e-05, 1.90712744520069e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[   0,   14,  560,  ..., 1248, 1398, 1268],
        [   0,  474,  221,  ..., 2945,    9,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4125e-05,  2.6063e-05,  0.0000e+00,  ..., -8.8539e-06,
         -3.5586e-05, -9.3466e-06],
        [-2.8312e-06, -2.0117e-06,  7.8231e-07,  ..., -2.4736e-06,
         -7.1526e-07, -1.1101e-06],
        [-3.1739e-06, -2.2650e-06,  8.7917e-07,  ..., -2.7865e-06,
         -8.0466e-07, -1.2517e-06],
        [-4.4405e-06, -3.1739e-06,  1.2368e-06,  ..., -3.9041e-06,
         -1.1250e-06, -1.7434e-06],
        [-6.2883e-06, -4.4703e-06,  1.7360e-06,  ..., -5.5134e-06,
         -1.5870e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.140528678894043


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0,   616,  4935,  ...,    89,  4448,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6542e-05,  6.1951e-05,  1.7438e-05,  ...,  5.7684e-07,
         -1.5118e-05, -2.7703e-05],
        [-5.7220e-06, -4.0382e-06,  1.5460e-06,  ..., -4.9770e-06,
         -1.4678e-06, -2.2650e-06],
        [-6.3926e-06, -4.5300e-06,  1.7323e-06,  ..., -5.5879e-06,
         -1.6466e-06, -2.5406e-06],
        [-8.8513e-06, -6.2734e-06,  2.4065e-06,  ..., -7.7486e-06,
         -2.2724e-06, -3.5092e-06],
        [-1.2726e-05, -9.0003e-06,  3.4347e-06,  ..., -1.1086e-05,
         -3.2634e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.166933298110962


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4562e-05,  6.9001e-05, -1.9659e-05,  ..., -3.9684e-06,
         -4.0378e-06, -4.8594e-05],
        [-8.5533e-06, -6.0648e-06,  2.3432e-06,  ..., -7.4804e-06,
         -2.1867e-06, -3.4198e-06],
        [-9.5218e-06, -6.7651e-06,  2.6152e-06,  ..., -8.3447e-06,
         -2.4438e-06, -3.8147e-06],
        [-1.3292e-05, -9.4324e-06,  3.6582e-06,  ..., -1.1653e-05,
         -3.3975e-06, -5.3123e-06],
        [-1.8954e-05, -1.3441e-05,  5.1856e-06,  ..., -1.6570e-05,
         -4.8503e-06, -7.5698e-06]], device='cuda:0')
Loss: 1.1502561569213867


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  2192,  2534,   287],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8636e-05,  6.9802e-05, -4.3237e-05,  ..., -2.4866e-05,
         -1.7412e-06, -5.5802e-05],
        [-1.1399e-05, -8.1062e-06,  3.1032e-06,  ..., -9.9838e-06,
         -2.8685e-06, -4.5300e-06],
        [-1.2696e-05, -9.0301e-06,  3.4608e-06,  ..., -1.1131e-05,
         -3.2037e-06, -5.0515e-06],
        [-1.7703e-05, -1.2577e-05,  4.8354e-06,  ..., -1.5527e-05,
         -4.4554e-06, -7.0333e-06],
        [-2.5272e-05, -1.7941e-05,  6.8620e-06,  ..., -2.2113e-05,
         -6.3628e-06, -1.0028e-05]], device='cuda:0')
Loss: 1.167754054069519


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[   0,  504,  546,  ...,  634,  328,  630],
        [   0, 1730, 2068,  ...,  445, 2704,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1181e-05,  1.1335e-04, -1.5305e-05,  ...,  2.3227e-05,
         -4.0270e-05, -3.7181e-05],
        [-1.4275e-05, -1.0163e-05,  3.8929e-06,  ..., -1.2502e-05,
         -3.5726e-06, -5.6624e-06],
        [-1.5929e-05, -1.1355e-05,  4.3511e-06,  ..., -1.3962e-05,
         -3.9972e-06, -6.3330e-06],
        [-2.2173e-05, -1.5765e-05,  6.0648e-06,  ..., -1.9431e-05,
         -5.5432e-06, -8.7917e-06],
        [-3.1650e-05, -2.2501e-05,  8.6129e-06,  ..., -2.7686e-05,
         -7.9274e-06, -1.2547e-05]], device='cuda:0')
Loss: 1.1552550792694092


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[    0,  5885,   271,  ...,   278,  1049,    12],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6525e-05,  1.0687e-04, -1.7038e-05,  ...,  3.1090e-05,
         -3.8530e-05, -5.1879e-05],
        [-1.7136e-05, -1.2204e-05,  4.6343e-06,  ..., -1.5005e-05,
         -4.2580e-06, -6.7726e-06],
        [-1.9118e-05, -1.3620e-05,  5.1782e-06,  ..., -1.6749e-05,
         -4.7609e-06, -7.5698e-06],
        [-2.6643e-05, -1.8939e-05,  7.2271e-06,  ..., -2.3335e-05,
         -6.6087e-06, -1.0528e-05],
        [-3.7998e-05, -2.7001e-05,  1.0259e-05,  ..., -3.3230e-05,
         -9.4473e-06, -1.5005e-05]], device='cuda:0')
Loss: 1.1634266376495361


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0,  360,  259,  ...,   12,  358,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3278e-06,  9.4014e-05,  4.5198e-06,  ...,  3.4562e-05,
         -5.4656e-05, -5.0769e-05],
        [-1.9997e-05, -1.4231e-05,  5.3756e-06,  ..., -1.7494e-05,
         -4.9546e-06, -7.8902e-06],
        [-2.2307e-05, -1.5885e-05,  6.0052e-06,  ..., -1.9535e-05,
         -5.5395e-06, -8.8215e-06],
        [-3.1084e-05, -2.2084e-05,  8.3819e-06,  ..., -2.7210e-05,
         -7.6890e-06, -1.2264e-05],
        [-4.4376e-05, -3.1501e-05,  1.1913e-05,  ..., -3.8803e-05,
         -1.1005e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.1490858793258667


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0312e-05,  1.0119e-04, -5.4618e-06,  ...,  3.7678e-05,
         -6.4024e-05, -5.7150e-05],
        [-2.2858e-05, -1.6257e-05,  6.1318e-06,  ..., -1.9968e-05,
         -5.6662e-06, -9.0227e-06],
        [ 7.2563e-05,  4.8574e-05, -1.6935e-05,  ...,  5.6446e-05,
          1.1214e-05,  2.5097e-05],
        [-3.5554e-05, -2.5243e-05,  9.5591e-06,  ..., -3.1054e-05,
         -8.7917e-06, -1.4022e-05],
        [-5.0753e-05, -3.6001e-05,  1.3590e-05,  ..., -4.4316e-05,
         -1.2584e-05, -1.9997e-05]], device='cuda:0')
Loss: 1.1450364589691162
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0155, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5712e-05, -4.9002e-06, -2.3877e-05,  ...,  1.2095e-05,
         -3.2628e-05,  1.4131e-05],
        [-2.0913e-05, -1.4821e-05,  3.9205e-06,  ..., -1.7419e-05,
         -2.9649e-06, -7.8255e-06],
        [ 4.9971e-05,  3.5972e-05, -1.0040e-05,  ...,  4.1730e-05,
          8.3032e-06,  1.8001e-05],
        [-3.2793e-05, -2.2137e-05,  6.8229e-06,  ..., -2.7018e-05,
         -5.2983e-06, -1.1985e-05],
        [-5.5168e-05, -3.8502e-05,  1.0318e-05,  ..., -4.6032e-05,
         -8.7630e-06, -2.1169e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9013e-08, 1.9856e-08, 2.9910e-08,  ..., 2.2191e-08, 5.2356e-08,
         1.1278e-08],
        [3.7564e-11, 2.2629e-11, 1.4123e-12,  ..., 2.5474e-11, 9.8767e-13,
         5.3979e-12],
        [6.4839e-10, 3.8058e-10, 2.2806e-11,  ..., 4.9865e-10, 1.2781e-11,
         9.6383e-11],
        [1.5132e-10, 2.1578e-10, 5.0773e-12,  ..., 1.2090e-10, 1.6960e-11,
         7.3663e-11],
        [1.4372e-10, 8.0830e-11, 6.9580e-12,  ..., 9.8690e-11, 2.2712e-12,
         2.1406e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0,   984,    13,  ...,    13, 37385,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8757e-05, -1.6804e-05,  2.0378e-05,  ..., -2.1504e-05,
          4.3384e-06,  1.8479e-05],
        [-2.8610e-06, -2.0415e-06,  7.7486e-07,  ..., -2.4885e-06,
         -7.2643e-07, -1.1325e-06],
        [-3.2634e-06, -2.3246e-06,  8.8289e-07,  ..., -2.8312e-06,
         -8.2701e-07, -1.2964e-06],
        [ 1.9606e-04,  1.1899e-04, -3.7819e-05,  ...,  1.3669e-04,
          4.2047e-05,  6.7152e-05],
        [-6.4373e-06, -4.5598e-06,  1.7360e-06,  ..., -5.5730e-06,
         -1.6317e-06, -2.5481e-06]], device='cuda:0')
Loss: 1.1520500183105469


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,  3351,   352,  ...,    17,   287,   357],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9844e-06, -2.7803e-05,  1.0720e-05,  ..., -1.3516e-05,
         -2.3968e-05,  2.9295e-06],
        [-5.6922e-06, -4.0531e-06,  1.5721e-06,  ..., -4.9919e-06,
         -1.4491e-06, -2.2948e-06],
        [-6.4373e-06, -4.5896e-06,  1.7770e-06,  ..., -5.6475e-06,
         -1.6354e-06, -2.6003e-06],
        [ 1.9162e-04,  1.1583e-04, -3.6560e-05,  ...,  1.3276e-04,
          4.0915e-05,  6.5327e-05],
        [-1.2696e-05, -9.0003e-06,  3.4943e-06,  ..., -1.1086e-05,
         -3.2261e-06, -5.1111e-06]], device='cuda:0')
Loss: 1.150604486465454


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  565,   27,  ...,   88, 4451,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1469e-05, -1.2335e-05,  2.1325e-05,  ..., -1.8798e-05,
         -2.7941e-05, -3.4473e-05],
        [-8.5235e-06, -6.0797e-06,  2.3395e-06,  ..., -7.4655e-06,
         -2.1644e-06, -3.4496e-06],
        [-9.6411e-06, -6.8843e-06,  2.6450e-06,  ..., -8.4341e-06,
         -2.4438e-06, -3.9041e-06],
        [ 1.8718e-04,  1.1264e-04, -3.5353e-05,  ...,  1.2888e-04,
          3.9797e-05,  6.3523e-05],
        [-1.9014e-05, -1.3530e-05,  5.2080e-06,  ..., -1.6630e-05,
         -4.8280e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.1334205865859985


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[   0, 1445, 3597,  ...,  281,   78,    9],
        [   0, 4448,   12,  ..., 3183,  328, 9559]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1286e-05,  1.0905e-05,  3.2559e-05,  ..., -1.8798e-05,
         -1.5511e-05, -3.5966e-06],
        [-1.1355e-05, -8.1062e-06,  3.1330e-06,  ..., -9.9540e-06,
         -2.8983e-06, -4.6119e-06],
        [-1.2860e-05, -9.1791e-06,  3.5465e-06,  ..., -1.1250e-05,
         -3.2745e-06, -5.2229e-06],
        [ 1.8274e-04,  1.0947e-04, -3.4109e-05,  ...,  1.2501e-04,
          3.8650e-05,  6.1706e-05],
        [-2.5332e-05, -1.8030e-05,  6.9737e-06,  ..., -2.2143e-05,
         -6.4597e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.1536585092544556


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,   292, 15087,  ...,  2675,  1663,    12],
        [    0,  5862,    13,  ..., 12497,   287,  3570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0356e-04,  4.0042e-05,  2.8691e-05,  ..., -3.6511e-05,
         -9.9329e-06, -2.4192e-05],
        [-1.4156e-05, -1.0133e-05,  3.9153e-06,  ..., -1.2428e-05,
         -3.6210e-06, -5.7742e-06],
        [-1.6049e-05, -1.1489e-05,  4.4368e-06,  ..., -1.4067e-05,
         -4.0978e-06, -6.5416e-06],
        [ 1.7833e-04,  1.0628e-04, -3.2872e-05,  ...,  1.2110e-04,
          3.7510e-05,  5.9880e-05],
        [-3.1620e-05, -2.2560e-05,  8.7246e-06,  ..., -2.7657e-05,
         -8.0764e-06, -1.2860e-05]], device='cuda:0')
Loss: 1.1441570520401


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[   0, 5489,   80,  ...,  221,  380,  333],
        [   0,  898,  266,  ...,   12, 3222, 8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0622e-04,  4.3998e-05, -2.2937e-05,  ..., -2.9478e-05,
         -2.9994e-05, -6.0530e-05],
        [-1.6972e-05, -1.2144e-05,  4.7013e-06,  ..., -1.4886e-05,
         -4.3288e-06, -6.9290e-06],
        [-1.9252e-05, -1.3769e-05,  5.3309e-06,  ..., -1.6868e-05,
         -4.9025e-06, -7.8529e-06],
        [ 1.7389e-04,  1.0312e-04, -3.1628e-05,  ...,  1.1723e-04,
          3.6400e-05,  5.8062e-05],
        [-3.7879e-05, -2.7031e-05,  1.0468e-05,  ..., -3.3110e-05,
         -9.6485e-06, -1.5423e-05]], device='cuda:0')
Loss: 1.1600276231765747


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[    0,   381, 13565,  ...,     9,   847,   300],
        [    0,   767,  1953,  ...,    14,  1364,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0657e-04,  8.1118e-05, -3.3429e-05,  ..., -4.1861e-05,
         -1.0869e-05, -4.4373e-05],
        [-1.9848e-05, -1.4201e-05,  5.4687e-06,  ..., -1.7390e-05,
         -5.0664e-06, -8.0913e-06],
        [-2.2486e-05, -1.6078e-05,  6.1952e-06,  ..., -1.9699e-05,
         -5.7332e-06, -9.1642e-06],
        [ 1.6942e-04,  9.9932e-05, -3.0436e-05,  ...,  1.1333e-04,
          3.5260e-05,  5.6259e-05],
        [-4.4316e-05, -3.1620e-05,  1.2182e-05,  ..., -3.8713e-05,
         -1.1295e-05, -1.8016e-05]], device='cuda:0')
Loss: 1.1493490934371948


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[   0, 5896,  352,  ..., 1168,  767, 1390],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3194e-04,  7.3896e-05, -5.7492e-05,  ..., -5.2571e-05,
         -6.9458e-06, -3.9191e-05],
        [-2.2665e-05, -1.6212e-05,  6.2436e-06,  ..., -1.9878e-05,
         -5.7966e-06, -9.2387e-06],
        [-2.5690e-05, -1.8373e-05,  7.0781e-06,  ..., -2.2531e-05,
         -6.5640e-06, -1.0476e-05],
        [ 1.6498e-04,  9.6773e-05, -2.9214e-05,  ...,  1.0942e-04,
          3.4112e-05,  5.4456e-05],
        [-5.0575e-05, -3.6091e-05,  1.3903e-05,  ..., -4.4227e-05,
         -1.2919e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.165358066558838
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0293, -0.0076,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5334e-05,  2.9794e-06, -2.7238e-05,  ...,  5.6285e-06,
         -3.0060e-05,  8.7985e-06],
        [-2.1088e-05, -1.4960e-05,  4.1528e-06,  ..., -1.7665e-05,
         -3.2481e-06, -7.9668e-06],
        [ 4.2405e-05,  3.0537e-05, -8.3278e-06,  ...,  3.5304e-05,
          6.8165e-06,  1.5154e-05],
        [-1.3016e-05, -1.0246e-05,  3.2192e-06,  ..., -1.3374e-05,
         -1.3572e-06, -5.3408e-06],
        [-5.4709e-05, -3.8261e-05,  1.0676e-05,  ..., -4.5852e-05,
         -9.1787e-06, -2.1108e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8971e-08, 1.9842e-08, 2.9883e-08,  ..., 2.2171e-08, 5.2304e-08,
         1.1268e-08],
        [3.8040e-11, 2.2869e-11, 1.4498e-12,  ..., 2.5843e-11, 1.0203e-12,
         5.4779e-12],
        [6.4841e-10, 3.8054e-10, 2.2834e-11,  ..., 4.9866e-10, 1.2811e-11,
         9.6396e-11],
        [1.7838e-10, 2.2493e-10, 5.9257e-12,  ..., 1.3275e-10, 1.8107e-11,
         7.6555e-11],
        [1.4614e-10, 8.2052e-11, 7.1443e-12,  ..., 1.0055e-10, 2.4358e-12,
         2.1807e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.896446322196428e-05, 1.896446322196428e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1266e-05, -5.3110e-07, -3.1088e-06,  ..., -2.0089e-06,
          1.3012e-05,  1.4503e-05],
        [-2.8163e-06, -2.0117e-06,  8.0466e-07,  ..., -2.4587e-06,
         -7.6368e-07, -1.1995e-06],
        [-3.2485e-06, -2.3097e-06,  9.2387e-07,  ..., -2.8312e-06,
         -8.7917e-07, -1.3784e-06],
        [-4.5002e-06, -3.2037e-06,  1.2815e-06,  ..., -3.9339e-06,
         -1.2144e-06, -1.9073e-06],
        [-6.3479e-06, -4.5300e-06,  1.8030e-06,  ..., -5.5432e-06,
         -1.7285e-06, -2.6971e-06]], device='cuda:0')
Loss: 1.1524543762207031


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[    0,    13,   786,  ...,   275,  2623,    13],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0739e-05,  4.2424e-05, -4.7350e-05,  ..., -1.1070e-05,
         -9.5753e-06,  3.0664e-05],
        [-5.6177e-06, -4.0233e-06,  1.6019e-06,  ..., -4.9174e-06,
         -1.5050e-06, -2.3767e-06],
        [ 9.0212e-05,  6.6648e-05, -1.3386e-05,  ...,  8.5288e-05,
          1.8807e-05,  3.6530e-05],
        [-8.9705e-06, -6.4224e-06,  2.5555e-06,  ..., -7.8380e-06,
         -2.3916e-06, -3.7849e-06],
        [-1.2636e-05, -9.0599e-06,  3.5912e-06,  ..., -1.1057e-05,
         -3.3975e-06, -5.3346e-06]], device='cuda:0')
Loss: 1.1650769710540771


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,  1304,   292,  ...,  2101,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6758e-06,  2.8907e-05, -5.6048e-05,  ...,  6.3279e-07,
         -8.8981e-06,  3.0664e-05],
        [-8.4490e-06, -6.0201e-06,  2.3842e-06,  ..., -7.3761e-06,
         -2.2613e-06, -3.5614e-06],
        [ 8.6948e-05,  6.4338e-05, -1.2481e-05,  ...,  8.2441e-05,
          1.7932e-05,  3.5167e-05],
        [-1.3530e-05, -9.6411e-06,  3.8147e-06,  ..., -1.1802e-05,
         -3.6061e-06, -5.6848e-06],
        [-1.8954e-05, -1.3530e-05,  5.3346e-06,  ..., -1.6540e-05,
         -5.0887e-06, -7.9721e-06]], device='cuda:0')
Loss: 1.1377182006835938


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[   0,   14,   23,  ...,  278,  266, 1462],
        [   0,   14,  381,  ..., 7106,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3359e-05,  4.3551e-05, -6.9536e-05,  ..., -1.6818e-05,
         -1.6033e-05,  6.2341e-05],
        [-1.1250e-05, -8.0168e-06,  3.1665e-06,  ..., -9.8348e-06,
         -2.9989e-06, -4.7162e-06],
        [ 8.3715e-05,  6.2029e-05, -1.1575e-05,  ...,  7.9595e-05,
          1.7082e-05,  3.3833e-05],
        [-1.8030e-05, -1.2860e-05,  5.0738e-06,  ..., -1.5765e-05,
         -4.7907e-06, -7.5400e-06],
        [-2.5243e-05, -1.8001e-05,  7.0855e-06,  ..., -2.2054e-05,
         -6.7428e-06, -1.0565e-05]], device='cuda:0')
Loss: 1.146453857421875


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[   0, 1101,  300,  ..., 6104,  367,  993],
        [   0, 3592,  417,  ..., 4893,  328,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0129e-05,  5.2857e-05, -8.7389e-05,  ..., -1.3718e-05,
         -5.4460e-05,  7.1773e-05],
        [-1.4067e-05, -9.9987e-06,  3.9674e-06,  ..., -1.2264e-05,
         -3.7439e-06, -5.8934e-06],
        [ 8.0451e-05,  5.9734e-05, -1.0644e-05,  ...,  7.6779e-05,
          1.6218e-05,  3.2470e-05],
        [-2.2560e-05, -1.6049e-05,  6.3628e-06,  ..., -1.9670e-05,
         -5.9903e-06, -9.4324e-06],
        [-3.1531e-05, -2.2411e-05,  8.8736e-06,  ..., -2.7478e-05,
         -8.4043e-06, -1.3188e-05]], device='cuda:0')
Loss: 1.1469249725341797


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  300, 2607,  ..., 1279,  368,  266],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7802e-05,  8.5999e-05, -1.2728e-04,  ..., -5.5687e-06,
         -5.4460e-05,  8.5672e-05],
        [-1.6898e-05, -1.2055e-05,  4.7721e-06,  ..., -1.4737e-05,
         -4.5225e-06, -7.1079e-06],
        [ 7.7203e-05,  5.7350e-05, -9.7166e-06,  ...,  7.3933e-05,
          1.5320e-05,  3.1069e-05],
        [-2.7061e-05, -1.9327e-05,  7.6443e-06,  ..., -2.3603e-05,
         -7.2271e-06, -1.1370e-05],
        [-3.7789e-05, -2.7001e-05,  1.0654e-05,  ..., -3.2961e-05,
         -1.0133e-05, -1.5885e-05]], device='cuda:0')
Loss: 1.1559287309646606


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,  1967,  6851,  ...,  1151,   809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1809e-05,  9.6880e-05, -1.0633e-04,  ..., -7.8662e-06,
         -6.6170e-05,  1.0185e-04],
        [-1.9759e-05, -1.4111e-05,  5.5544e-06,  ..., -1.7211e-05,
         -5.2899e-06, -8.2999e-06],
        [ 7.3925e-05,  5.4980e-05, -8.8188e-06,  ...,  7.1087e-05,
          1.4437e-05,  2.9705e-05],
        [-3.1590e-05, -2.2575e-05,  8.8811e-06,  ..., -2.7508e-05,
         -8.4415e-06, -1.3247e-05],
        [-4.4197e-05, -3.1620e-05,  1.2405e-05,  ..., -3.8505e-05,
         -1.1861e-05, -1.8552e-05]], device='cuda:0')
Loss: 1.1371839046478271


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0,  1716,   271,  ...,   292,    78,  1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8280e-06,  8.4150e-05, -1.0977e-04,  ...,  1.3587e-05,
         -7.4818e-05,  8.5462e-05],
        [-2.2590e-05, -1.6153e-05,  6.3442e-06,  ..., -1.9670e-05,
         -6.0573e-06, -9.4846e-06],
        [ 7.0646e-05,  5.2626e-05, -7.9061e-06,  ...,  6.8241e-05,
          1.3547e-05,  2.8335e-05],
        [-3.6091e-05, -2.5809e-05,  1.0133e-05,  ..., -3.1412e-05,
         -9.6634e-06, -1.5132e-05],
        [-5.0575e-05, -3.6180e-05,  1.4171e-05,  ..., -4.4018e-05,
         -1.3590e-05, -2.1219e-05]], device='cuda:0')
Loss: 1.151222825050354
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1484e-05,  1.1096e-05, -3.5492e-05,  ...,  6.4243e-06,
         -3.4536e-05,  1.6465e-05],
        [-2.1238e-05, -1.5079e-05,  4.3719e-06,  ..., -1.7865e-05,
         -3.5290e-06, -8.1186e-06],
        [ 4.5229e-05,  3.2746e-05, -8.2857e-06,  ...,  3.8598e-05,
          7.4895e-06,  1.6472e-05],
        [-1.5323e-05, -1.1802e-05,  3.9106e-06,  ..., -1.5177e-05,
         -2.1878e-06, -6.3199e-06],
        [-5.4296e-05, -3.8053e-05,  1.1026e-05,  ..., -4.5668e-05,
         -9.6198e-06, -2.1119e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8912e-08, 1.9829e-08, 2.9866e-08,  ..., 2.2149e-08, 5.2257e-08,
         1.1264e-08],
        [3.8513e-11, 2.3107e-11, 1.4886e-12,  ..., 2.6204e-11, 1.0560e-12,
         5.5624e-12],
        [6.5275e-10, 3.8293e-10, 2.2873e-11,  ..., 5.0282e-10, 1.2982e-11,
         9.7102e-11],
        [1.7951e-10, 2.2537e-10, 6.0225e-12,  ..., 1.3361e-10, 1.8182e-11,
         7.6708e-11],
        [1.4855e-10, 8.3279e-11, 7.3380e-12,  ..., 1.0238e-10, 2.6181e-12,
         2.2236e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,   199,  7513,  ...,   271,   259,   957],
        [    0,   266,   858,  ..., 11265,   607,  7455]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4983e-05, -2.8393e-05, -3.3114e-06,  ..., -1.8677e-05,
          1.2530e-06,  2.7248e-05],
        [-2.7865e-06, -1.9968e-06,  7.9721e-07,  ..., -2.4587e-06,
         -7.9349e-07, -1.2442e-06],
        [-3.2783e-06, -2.3544e-06,  9.3505e-07,  ..., -2.8759e-06,
         -9.3132e-07, -1.4603e-06],
        [-4.4703e-06, -3.2187e-06,  1.2815e-06,  ..., -3.9339e-06,
         -1.2666e-06, -1.9968e-06],
        [-6.2287e-06, -4.4703e-06,  1.7807e-06,  ..., -5.4836e-06,
         -1.7732e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.14253568649292


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,  1894,   317,  ...,  9920,    13, 19888],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2028e-05, -4.2852e-05, -1.9686e-05,  ..., -3.7222e-05,
         -2.2435e-05,  1.9261e-05],
        [-5.5432e-06, -3.9786e-06,  1.5944e-06,  ..., -4.9025e-06,
         -1.5460e-06, -2.4661e-06],
        [-6.4969e-06, -4.6641e-06,  1.8664e-06,  ..., -5.7369e-06,
         -1.8105e-06, -2.8908e-06],
        [-8.9407e-06, -6.4373e-06,  2.5779e-06,  ..., -7.8976e-06,
         -2.4885e-06, -3.9786e-06],
        [-1.2457e-05, -8.9407e-06,  3.5763e-06,  ..., -1.0997e-05,
         -3.4720e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.1591715812683105


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[    0,   344,  8260,  ..., 16020, 18216, 11348],
        [    0,  3211,   328,  ...,  2098,  1231, 35325]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6533e-05, -6.3346e-05, -2.7108e-05,  ..., -1.6000e-05,
         -2.2567e-05,  4.1247e-05],
        [-8.3297e-06, -5.9903e-06,  2.4065e-06,  ..., -7.3314e-06,
         -2.3060e-06, -3.6806e-06],
        [ 8.6820e-05,  5.9459e-05, -2.6159e-05,  ...,  7.4505e-05,
          2.5910e-05,  3.7086e-05],
        [-1.3471e-05, -9.7156e-06,  3.9041e-06,  ..., -1.1832e-05,
         -3.7178e-06, -5.9456e-06],
        [ 7.1943e-05,  4.8666e-05, -2.4174e-05,  ...,  6.3691e-05,
          2.2091e-05,  2.6556e-05]], device='cuda:0')
Loss: 1.1502400636672974


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   271,   266,  7246],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8044e-05, -8.2780e-05, -6.3459e-05,  ..., -8.5831e-06,
         -3.7176e-05,  3.2458e-05],
        [-1.1101e-05, -7.9870e-06,  3.2149e-06,  ..., -9.7752e-06,
         -3.0771e-06, -4.9174e-06],
        [ 8.3556e-05,  5.7105e-05, -2.5209e-05,  ...,  7.1629e-05,
          2.5005e-05,  3.5626e-05],
        [-1.7941e-05, -1.2949e-05,  5.2154e-06,  ..., -1.5795e-05,
         -4.9621e-06, -7.9423e-06],
        [ 6.5714e-05,  4.4165e-05, -2.2356e-05,  ...,  5.8208e-05,
          2.0355e-05,  2.3769e-05]], device='cuda:0')
Loss: 1.1453486680984497


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0, 10205,   342,  ...,  2523,  4729, 13753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2287e-05, -1.2315e-04, -5.1783e-05,  ...,  6.9916e-06,
         -5.1300e-05,  3.6295e-05],
        [-1.3888e-05, -1.0014e-05,  4.0270e-06,  ..., -1.2219e-05,
         -3.8408e-06, -6.1542e-06],
        [ 8.0263e-05,  5.4706e-05, -2.4255e-05,  ...,  6.8739e-05,
          2.4107e-05,  3.4173e-05],
        [-2.2441e-05, -1.6227e-05,  6.5267e-06,  ..., -1.9759e-05,
         -6.1914e-06, -9.9391e-06],
        [ 5.9426e-05,  3.9606e-05, -2.0530e-05,  ...,  5.2694e-05,
          1.8634e-05,  2.0998e-05]], device='cuda:0')
Loss: 1.1586556434631348


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9917e-05, -1.3250e-04, -3.4388e-05,  ..., -2.1043e-05,
         -3.5538e-05,  5.5702e-05],
        [-1.6645e-05, -1.2010e-05,  4.8243e-06,  ..., -1.4663e-05,
         -4.6007e-06, -7.3686e-06],
        [ 7.7030e-05,  5.2351e-05, -2.3320e-05,  ...,  6.5878e-05,
          2.3217e-05,  3.2758e-05],
        [-2.6911e-05, -1.9461e-05,  7.8231e-06,  ..., -2.3723e-05,
         -7.4208e-06, -1.1891e-05],
        [ 5.3227e-05,  3.5105e-05, -1.8742e-05,  ...,  4.7211e-05,
          1.6927e-05,  1.8286e-05]], device='cuda:0')
Loss: 1.130213975906372


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,    14,   747,  ...,   367,   300,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7091e-05, -1.2695e-04, -1.6417e-05,  ..., -8.1857e-06,
         -3.5244e-05,  5.9136e-05],
        [-1.9372e-05, -1.4007e-05,  5.6252e-06,  ..., -1.7092e-05,
         -5.3607e-06, -8.5831e-06],
        [ 7.3811e-05,  4.9997e-05, -2.2374e-05,  ...,  6.3016e-05,
          2.2323e-05,  3.1327e-05],
        [-3.1352e-05, -2.2694e-05,  9.1270e-06,  ..., -2.7657e-05,
         -8.6501e-06, -1.3858e-05],
        [ 4.7058e-05,  3.0635e-05, -1.6939e-05,  ...,  4.1727e-05,
          1.5214e-05,  1.5559e-05]], device='cuda:0')
Loss: 1.1507552862167358


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[   0,  496,   14,  ..., 1034, 4679,  278],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8593e-05, -1.2956e-04, -2.3691e-05,  ..., -8.2297e-06,
         -2.4912e-05,  4.3337e-05],
        [-2.2113e-05, -1.6019e-05,  6.4112e-06,  ..., -1.9521e-05,
         -6.1318e-06, -9.8348e-06],
        [ 7.0592e-05,  4.7642e-05, -2.1450e-05,  ...,  6.0155e-05,
          2.1417e-05,  2.9852e-05],
        [-3.5793e-05, -2.5943e-05,  1.0401e-05,  ..., -3.1590e-05,
         -9.8944e-06, -1.5885e-05],
        [ 4.0889e-05,  2.6105e-05, -1.5173e-05,  ...,  3.6243e-05,
          1.3485e-05,  1.2743e-05]], device='cuda:0')
Loss: 1.1362183094024658
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5476e-05, -2.9690e-06, -3.4312e-05,  ...,  4.9589e-06,
         -3.3574e-05,  1.9152e-05],
        [-2.1326e-05, -1.5173e-05,  4.5759e-06,  ..., -1.8031e-05,
         -3.7893e-06, -8.2902e-06],
        [ 4.7766e-05,  3.4236e-05, -9.6021e-06,  ...,  4.0753e-05,
          8.8823e-06,  1.7810e-05],
        [-1.7370e-05, -1.3216e-05,  4.5596e-06,  ..., -1.6819e-05,
         -2.9585e-06, -7.2764e-06],
        [-4.4777e-05, -3.1637e-05,  8.4058e-06,  ..., -3.7477e-05,
         -7.3093e-06, -1.7733e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8854e-08, 1.9826e-08, 2.9836e-08,  ..., 2.2127e-08, 5.2206e-08,
         1.1255e-08],
        [3.8963e-11, 2.3340e-11, 1.5282e-12,  ..., 2.6559e-11, 1.0925e-12,
         5.6535e-12],
        [6.5708e-10, 3.8481e-10, 2.3311e-11,  ..., 5.0594e-10, 1.3428e-11,
         9.7897e-11],
        [1.8061e-10, 2.2582e-10, 6.1246e-12,  ..., 1.3447e-10, 1.8262e-11,
         7.6883e-11],
        [1.5007e-10, 8.3877e-11, 7.5609e-12,  ..., 1.0360e-10, 2.7973e-12,
         2.2376e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.885217280831754e-05, 1.885217280831754e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0,  747, 7890,  ...,  706, 8667,  271],
        [   0,  328,  490,  ..., 6280, 4283, 4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5441e-05, -7.7798e-06, -1.0510e-05,  ...,  0.0000e+00,
         -1.8788e-05,  2.0652e-05],
        [-2.7418e-06, -1.9968e-06,  8.2701e-07,  ..., -2.4140e-06,
         -7.7114e-07, -1.2740e-06],
        [-3.2932e-06, -2.3991e-06,  9.9093e-07,  ..., -2.8908e-06,
         -9.2387e-07, -1.5199e-06],
        [-4.5002e-06, -3.2783e-06,  1.3560e-06,  ..., -3.9637e-06,
         -1.2666e-06, -2.0862e-06],
        [-6.2287e-06, -4.5300e-06,  1.8701e-06,  ..., -5.4538e-06,
         -1.7509e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1288381814956665


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[    0,  9058,  5481,  ...,   508, 15074,   300],
        [    0,  2790,   266,  ...,   401,  1496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3838e-08,  3.1896e-05, -1.1535e-05,  ..., -2.3383e-05,
         -6.0061e-06,  1.9049e-05],
        [-5.4985e-06, -4.0084e-06,  1.6317e-06,  ..., -4.8280e-06,
         -1.5385e-06, -2.5481e-06],
        [-6.5416e-06, -4.7684e-06,  1.9409e-06,  ..., -5.7518e-06,
         -1.8291e-06, -3.0249e-06],
        [-9.0003e-06, -6.5714e-06,  2.6748e-06,  ..., -7.9274e-06,
         -2.5183e-06, -4.1723e-06],
        [-1.2487e-05, -9.0897e-06,  3.6955e-06,  ..., -1.0967e-05,
         -3.4943e-06, -5.7667e-06]], device='cuda:0')
Loss: 1.1436798572540283


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[    0,  2834,   266,  ..., 39474,    12, 15441],
        [    0,   292,    46,  ...,  1217,    17,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2302e-06,  8.0718e-05,  9.5339e-06,  ..., -2.4840e-05,
         -4.5343e-06,  9.6306e-06],
        [-8.2552e-06, -5.9903e-06,  2.4252e-06,  ..., -7.2420e-06,
         -2.2911e-06, -3.8072e-06],
        [-9.8050e-06, -7.1228e-06,  2.8796e-06,  ..., -8.6129e-06,
         -2.7195e-06, -4.5225e-06],
        [-1.3530e-05, -9.8348e-06,  3.9786e-06,  ..., -1.1891e-05,
         -3.7551e-06, -6.2436e-06],
        [-1.8746e-05, -1.3590e-05,  5.4985e-06,  ..., -1.6451e-05,
         -5.2005e-06, -8.6278e-06]], device='cuda:0')
Loss: 1.1406916379928589


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0, 23842,   342,  ...,   365,  4011, 10151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0106e-05,  7.4663e-05, -1.6390e-06,  ..., -3.8765e-05,
          6.2228e-06,  2.5782e-05],
        [-1.1012e-05, -7.9870e-06,  3.2336e-06,  ..., -9.6858e-06,
         -3.0622e-06, -5.0738e-06],
        [-1.3068e-05, -9.4920e-06,  3.8408e-06,  ..., -1.1519e-05,
         -3.6359e-06, -6.0275e-06],
        [-1.8001e-05, -1.3083e-05,  5.2974e-06,  ..., -1.5855e-05,
         -5.0068e-06, -8.2999e-06],
        [-2.4974e-05, -1.8090e-05,  7.3165e-06,  ..., -2.1964e-05,
         -6.9439e-06, -1.1489e-05]], device='cuda:0')
Loss: 1.1594295501708984


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[   0, 4209,  278,  ...,  287, 9971,  717],
        [   0,  266, 1034,  ..., 6153,  263,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4278e-06,  4.8726e-05, -2.1548e-05,  ..., -3.9317e-05,
          1.5611e-05,  1.3367e-05],
        [-1.3739e-05, -9.9838e-06,  4.0419e-06,  ..., -1.2085e-05,
         -3.8408e-06, -6.3628e-06],
        [-1.6317e-05, -1.1861e-05,  4.8019e-06,  ..., -1.4380e-05,
         -4.5635e-06, -7.5549e-06],
        [-2.2471e-05, -1.6361e-05,  6.6310e-06,  ..., -1.9789e-05,
         -6.2808e-06, -1.0401e-05],
        [-3.1173e-05, -2.2620e-05,  9.1568e-06,  ..., -2.7418e-05,
         -8.7172e-06, -1.4409e-05]], device='cuda:0')
Loss: 1.1220781803131104


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[   0,  292,   41,  ...,  271, 9536,  287],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0487e-05,  1.2882e-04, -2.1817e-05,  ..., -3.4242e-05,
          5.0483e-05,  3.6040e-05],
        [-1.6525e-05, -1.2010e-05,  4.8503e-06,  ..., -1.4514e-05,
         -4.6678e-06, -7.6368e-06],
        [-1.9625e-05, -1.4260e-05,  5.7630e-06,  ..., -1.7270e-05,
         -5.5395e-06, -9.0674e-06],
        [-2.6971e-05, -1.9625e-05,  7.9423e-06,  ..., -2.3723e-05,
         -7.6145e-06, -1.2457e-05],
        [-3.7491e-05, -2.7210e-05,  1.0990e-05,  ..., -3.2932e-05,
         -1.0587e-05, -1.7300e-05]], device='cuda:0')
Loss: 1.1347291469573975


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[    0,  1824,    13,  ...,   266,  5940,    19],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5158e-05,  1.3231e-04, -4.6035e-05,  ..., -2.7183e-05,
          3.3647e-05,  3.5612e-05],
        [-1.9267e-05, -1.3992e-05,  5.6736e-06,  ..., -1.6928e-05,
         -5.4128e-06, -8.8811e-06],
        [-2.2873e-05, -1.6615e-05,  6.7391e-06,  ..., -2.0131e-05,
         -6.4187e-06, -1.0543e-05],
        [-3.1471e-05, -2.2873e-05,  9.2983e-06,  ..., -2.7686e-05,
         -8.8289e-06, -1.4499e-05],
        [-4.3690e-05, -3.1680e-05,  1.2852e-05,  ..., -3.8385e-05,
         -1.2264e-05, -2.0117e-05]], device='cuda:0')
Loss: 1.1447120904922485


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2542e-05,  1.3847e-04, -6.3841e-05,  ..., -2.5199e-05,
          5.7649e-05,  5.5549e-05],
        [-2.1994e-05, -1.5989e-05,  6.4932e-06,  ..., -1.9372e-05,
         -6.1877e-06, -1.0155e-05],
        [-2.6122e-05, -1.8999e-05,  7.7151e-06,  ..., -2.3037e-05,
         -7.3425e-06, -1.2055e-05],
        [-3.5912e-05, -2.6137e-05,  1.0639e-05,  ..., -3.1650e-05,
         -1.0088e-05, -1.6570e-05],
        [-4.9800e-05, -3.6150e-05,  1.4693e-05,  ..., -4.3869e-05,
         -1.4000e-05, -2.2963e-05]], device='cuda:0')
Loss: 1.1275378465652466
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0034,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0147, -0.0271,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8674e-05,  1.1175e-05, -3.7264e-05,  ...,  1.9431e-06,
         -2.4451e-05,  2.2792e-05],
        [-2.1393e-05, -1.5255e-05,  4.7676e-06,  ..., -1.8165e-05,
         -4.0291e-06, -8.4767e-06],
        [ 4.0377e-05,  2.8912e-05, -7.8704e-06,  ...,  3.4374e-05,
          7.2598e-06,  1.4823e-05],
        [-1.9224e-05, -1.4508e-05,  5.1676e-06,  ..., -1.8302e-05,
         -3.6714e-06, -8.2058e-06],
        [-4.5279e-05, -3.2089e-05,  9.0344e-06,  ..., -3.8116e-05,
         -7.9783e-06, -1.8256e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8796e-08, 1.9825e-08, 2.9810e-08,  ..., 2.2106e-08, 5.2157e-08,
         1.1247e-08],
        [3.9408e-11, 2.3573e-11, 1.5689e-12,  ..., 2.6908e-11, 1.1297e-12,
         5.7510e-12],
        [6.5710e-10, 3.8479e-10, 2.3347e-11,  ..., 5.0596e-10, 1.3468e-11,
         9.7944e-11],
        [1.8172e-10, 2.2628e-10, 6.2317e-12,  ..., 1.3534e-10, 1.8346e-11,
         7.7081e-11],
        [1.5240e-10, 8.5100e-11, 7.7692e-12,  ..., 1.0542e-10, 2.9905e-12,
         2.2881e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0,   12, 4957,  ...,  944,  278,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8173e-05, -1.8002e-06, -2.1337e-05,  ..., -9.7243e-06,
          1.2627e-05, -1.7107e-05],
        [-2.7120e-06, -1.9968e-06,  8.3819e-07,  ..., -2.4140e-06,
         -7.9721e-07, -1.3337e-06],
        [-3.2634e-06, -2.3991e-06,  1.0133e-06,  ..., -2.9057e-06,
         -9.6112e-07, -1.6019e-06],
        [-4.4703e-06, -3.2932e-06,  1.3858e-06,  ..., -3.9935e-06,
         -1.3113e-06, -2.1905e-06],
        [-6.1393e-06, -4.5300e-06,  1.8999e-06,  ..., -5.4836e-06,
         -1.8030e-06, -3.0100e-06]], device='cuda:0')
Loss: 1.1161174774169922


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,   285,    53,  ...,   259,  5012,  3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7619e-05, -1.5191e-05, -2.7849e-06,  ..., -6.7954e-07,
          3.9751e-05,  2.5429e-06],
        [-5.4538e-06, -3.9786e-06,  1.6615e-06,  ..., -4.8429e-06,
         -1.5870e-06, -2.6673e-06],
        [-6.5565e-06, -4.7833e-06,  2.0042e-06,  ..., -5.8264e-06,
         -1.9073e-06, -3.1963e-06],
        [-8.9705e-06, -6.5565e-06,  2.7418e-06,  ..., -7.9870e-06,
         -2.6077e-06, -4.3809e-06],
        [-1.2308e-05, -9.0003e-06,  3.7551e-06,  ..., -1.0967e-05,
         -3.5837e-06, -6.0052e-06]], device='cuda:0')
Loss: 1.1353075504302979


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  292,  263,  ...,  342, 4575,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6854e-05,  2.9685e-05, -4.1426e-06,  ..., -1.1275e-05,
          2.0845e-05, -1.4767e-05],
        [-8.1658e-06, -5.9456e-06,  2.4959e-06,  ..., -7.2569e-06,
         -2.3656e-06, -3.9861e-06],
        [-9.8050e-06, -7.1377e-06,  3.0100e-06,  ..., -8.7321e-06,
         -2.8424e-06, -4.7833e-06],
        [-1.3471e-05, -9.8050e-06,  4.1276e-06,  ..., -1.1981e-05,
         -3.8967e-06, -6.5714e-06],
        [-1.8477e-05, -1.3441e-05,  5.6550e-06,  ..., -1.6451e-05,
         -5.3570e-06, -9.0003e-06]], device='cuda:0')
Loss: 1.1647365093231201


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6360e-05,  3.1600e-05,  1.8502e-05,  ..., -9.9007e-06,
          2.6927e-05,  2.9968e-05],
        [-1.0893e-05, -7.9423e-06,  3.3267e-06,  ..., -9.6709e-06,
         -3.1218e-06, -5.2899e-06],
        [-1.3083e-05, -9.5516e-06,  4.0084e-06,  ..., -1.1638e-05,
         -3.7551e-06, -6.3553e-06],
        [-1.7971e-05, -1.3098e-05,  5.4985e-06,  ..., -1.5974e-05,
         -5.1484e-06, -8.7321e-06],
        [-2.4617e-05, -1.7941e-05,  7.5251e-06,  ..., -2.1875e-05,
         -7.0632e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1504147052764893


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,   278,  2305,  ...,  2529, 34181,  4555],
        [    0,   287,  6932,  ...,  1549,  1480,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7306e-05,  1.6443e-05, -7.9402e-06,  ..., -2.9899e-05,
          2.5955e-05,  2.7679e-05],
        [-1.3590e-05, -9.9391e-06,  4.1500e-06,  ..., -1.2085e-05,
         -3.8892e-06, -6.6087e-06],
        [-1.6347e-05, -1.1966e-05,  5.0068e-06,  ..., -1.4558e-05,
         -4.6827e-06, -7.9498e-06],
        [-2.2411e-05, -1.6391e-05,  6.8620e-06,  ..., -1.9938e-05,
         -6.4075e-06, -1.0908e-05],
        [-3.0726e-05, -2.2471e-05,  9.3952e-06,  ..., -2.7329e-05,
         -8.7991e-06, -1.4931e-05]], device='cuda:0')
Loss: 1.1406365633010864


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,    12,  1471,  ...,  1356,   600,    12],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5781e-05,  5.0009e-06, -7.9402e-06,  ..., -2.1204e-05,
          5.4479e-06,  3.4400e-05],
        [-1.6317e-05, -1.1936e-05,  4.9658e-06,  ..., -1.4529e-05,
         -4.6641e-06, -7.9200e-06],
        [-1.9625e-05, -1.4380e-05,  5.9903e-06,  ..., -1.7494e-05,
         -5.6177e-06, -9.5293e-06],
        [-2.6882e-05, -1.9670e-05,  8.2031e-06,  ..., -2.3931e-05,
         -7.6741e-06, -1.3053e-05],
        [-3.6865e-05, -2.6971e-05,  1.1235e-05,  ..., -3.2812e-05,
         -1.0550e-05, -1.7881e-05]], device='cuda:0')
Loss: 1.129813551902771


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2337e-04,  3.0810e-05, -2.4080e-05,  ..., -2.2711e-05,
          2.9458e-05,  3.5937e-05],
        [-1.9029e-05, -1.3947e-05,  5.7593e-06,  ..., -1.6958e-05,
         -5.4464e-06, -9.2462e-06],
        [-2.2888e-05, -1.6794e-05,  6.9402e-06,  ..., -2.0415e-05,
         -6.5528e-06, -1.1116e-05],
        [-3.1322e-05, -2.2963e-05,  9.4995e-06,  ..., -2.7895e-05,
         -8.9481e-06, -1.5214e-05],
        [-4.3035e-05, -3.1561e-05,  1.3031e-05,  ..., -3.8326e-05,
         -1.2323e-05, -2.0877e-05]], device='cuda:0')
Loss: 1.1560540199279785


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3903e-04, -1.0571e-05, -3.6725e-05,  ..., -2.4155e-05,
          3.2070e-05,  3.7173e-05],
        [-2.1726e-05, -1.5914e-05,  6.5640e-06,  ..., -1.9357e-05,
         -6.1952e-06, -1.0565e-05],
        [-2.6152e-05, -1.9178e-05,  7.9162e-06,  ..., -2.3320e-05,
         -7.4580e-06, -1.2711e-05],
        [-3.5793e-05, -2.6226e-05,  1.0833e-05,  ..., -3.1859e-05,
         -1.0185e-05, -1.7390e-05],
        [-4.9144e-05, -3.6031e-05,  1.4849e-05,  ..., -4.3750e-05,
         -1.4022e-05, -2.3857e-05]], device='cuda:0')
Loss: 1.1391383409500122
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0149,  0.0033,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0148, -0.0272,  ...,  0.0283, -0.0154, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9710e-05,  9.0004e-06, -3.7210e-05,  ..., -6.6669e-07,
         -1.8799e-05,  2.4230e-05],
        [-2.1426e-05, -1.5321e-05,  4.9472e-06,  ..., -1.8284e-05,
         -4.2457e-06, -8.6855e-06],
        [ 3.3724e-05,  2.4103e-05, -6.2917e-06,  ...,  2.8605e-05,
          5.7881e-06,  1.2070e-05],
        [-2.0881e-05, -1.5680e-05,  5.7342e-06,  ..., -1.9658e-05,
         -4.3228e-06, -9.1242e-06],
        [-4.5666e-05, -3.2483e-05,  9.6159e-06,  ..., -3.8680e-05,
         -8.5827e-06, -1.8816e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8757e-08, 1.9806e-08, 2.9782e-08,  ..., 2.2084e-08, 5.2106e-08,
         1.1237e-08],
        [3.9841e-11, 2.3802e-11, 1.6104e-12,  ..., 2.7256e-11, 1.1669e-12,
         5.8569e-12],
        [6.5713e-10, 3.8477e-10, 2.3386e-11,  ..., 5.0600e-10, 1.3511e-11,
         9.8008e-11],
        [1.8282e-10, 2.2674e-10, 6.3428e-12,  ..., 1.3622e-10, 1.8431e-11,
         7.7306e-11],
        [1.5467e-10, 8.6313e-11, 7.9819e-12,  ..., 1.0723e-10, 3.1841e-12,
         2.3427e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.8734471844266252e-05, 1.8734471844266252e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,   298, 11712,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2413e-05,  6.9118e-05, -2.2104e-05,  ...,  1.6443e-05,
         -1.4955e-05,  4.0348e-05],
        [-2.6673e-06, -1.9670e-06,  8.1956e-07,  ..., -2.3991e-06,
         -8.1584e-07, -1.3784e-06],
        [-3.2485e-06, -2.3842e-06,  9.9838e-07,  ..., -2.9057e-06,
         -9.9093e-07, -1.6764e-06],
        [-4.4107e-06, -3.2485e-06,  1.3560e-06,  ..., -3.9339e-06,
         -1.3411e-06, -2.2799e-06],
        [-6.0797e-06, -4.4703e-06,  1.8626e-06,  ..., -5.4538e-06,
         -1.8552e-06, -3.1441e-06]], device='cuda:0')
Loss: 1.1238845586776733


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[   0,   13, 4363,  ...,  271, 2462,  709],
        [   0,   12,  328,  ...,  578,   19,   40]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3918e-05,  8.7267e-05, -5.6223e-05,  ...,  1.4521e-05,
         -1.6662e-05,  7.3768e-05],
        [-5.3942e-06, -3.9786e-06,  1.6205e-06,  ..., -4.8429e-06,
         -1.5795e-06, -2.7567e-06],
        [-6.5267e-06, -4.7833e-06,  1.9595e-06,  ..., -5.8413e-06,
         -1.9036e-06, -3.3304e-06],
        [-8.9407e-06, -6.5714e-06,  2.6897e-06,  ..., -7.9870e-06,
         -2.6077e-06, -4.5747e-06],
        [-1.2279e-05, -9.0003e-06,  3.6806e-06,  ..., -1.0997e-05,
         -3.5837e-06, -6.2734e-06]], device='cuda:0')
Loss: 1.1412582397460938


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[    0,  7203,   271,  ...,    12,   275,  3338],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7664e-05,  9.4665e-05, -8.6347e-05,  ...,  1.5091e-05,
         -1.6662e-05,  6.8483e-05],
        [-8.0615e-06, -5.9605e-06,  2.4028e-06,  ..., -7.2569e-06,
         -2.3358e-06, -4.1127e-06],
        [-9.7454e-06, -7.1675e-06,  2.9020e-06,  ..., -8.7619e-06,
         -2.8126e-06, -4.9621e-06],
        [-1.3381e-05, -9.8795e-06,  3.9935e-06,  ..., -1.2010e-05,
         -3.8594e-06, -6.8247e-06],
        [-1.8388e-05, -1.3530e-05,  5.4687e-06,  ..., -1.6540e-05,
         -5.3123e-06, -9.3728e-06]], device='cuda:0')
Loss: 1.135072946548462


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7683e-05,  1.1357e-04, -7.9357e-05,  ..., -2.5902e-05,
         -2.0371e-05,  4.8116e-05],
        [-1.0759e-05, -7.9274e-06,  3.1963e-06,  ..., -9.6858e-06,
         -3.0808e-06, -5.4687e-06],
        [-1.2994e-05, -9.5367e-06,  3.8557e-06,  ..., -1.1683e-05,
         -3.7141e-06, -6.5938e-06],
        [-1.7881e-05, -1.3173e-05,  5.3197e-06,  ..., -1.6063e-05,
         -5.1036e-06, -9.0897e-06],
        [-2.4498e-05, -1.8001e-05,  7.2643e-06,  ..., -2.2054e-05,
         -7.0035e-06, -1.2442e-05]], device='cuda:0')
Loss: 1.1446863412857056


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,   278,  6046,  ...,  1671,   199,   395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6534e-05,  1.1574e-04, -1.0060e-04,  ...,  2.9076e-05,
         -5.2137e-05,  1.8286e-05],
        [-1.3441e-05, -9.8795e-06,  4.0159e-06,  ..., -1.2085e-05,
         -3.8520e-06, -6.8098e-06],
        [-1.6257e-05, -1.1921e-05,  4.8615e-06,  ..., -1.4603e-05,
         -4.6529e-06, -8.2329e-06],
        [-2.2322e-05, -1.6421e-05,  6.6906e-06,  ..., -2.0057e-05,
         -6.3777e-06, -1.1325e-05],
        [-3.0607e-05, -2.2441e-05,  9.1344e-06,  ..., -2.7508e-05,
         -8.7619e-06, -1.5497e-05]], device='cuda:0')
Loss: 1.1195210218429565


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,   278,  6318,  ...,   458,    17,     9],
        [    0,  7377, 30662,  ...,   287,   694, 13403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5433e-05,  1.4940e-04, -1.1399e-04,  ...,  8.1834e-05,
         -7.8328e-05,  2.9307e-05],
        [-1.6168e-05, -1.1876e-05,  4.8168e-06,  ..., -1.4514e-05,
         -4.6156e-06, -8.1658e-06],
        [-1.9550e-05, -1.4335e-05,  5.8301e-06,  ..., -1.7524e-05,
         -5.5730e-06, -9.8720e-06],
        [-2.6852e-05, -1.9729e-05,  8.0168e-06,  ..., -2.4080e-05,
         -7.6368e-06, -1.3575e-05],
        [-3.6836e-05, -2.7001e-05,  1.0960e-05,  ..., -3.3021e-05,
         -1.0505e-05, -1.8582e-05]], device='cuda:0')
Loss: 1.1238675117492676


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[    0,   759,  1128,  ...,   221,   474,   221],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5323e-05,  1.1061e-04, -1.1402e-04,  ...,  8.9075e-05,
         -9.4830e-05,  1.6837e-05],
        [-1.8865e-05, -1.3873e-05,  5.6289e-06,  ..., -1.6898e-05,
         -5.3793e-06, -9.5293e-06],
        [-2.2829e-05, -1.6749e-05,  6.8136e-06,  ..., -2.0415e-05,
         -6.5006e-06, -1.1533e-05],
        [-3.1352e-05, -2.3052e-05,  9.3803e-06,  ..., -2.8074e-05,
         -8.9109e-06, -1.5855e-05],
        [-4.2975e-05, -3.1531e-05,  1.2808e-05,  ..., -3.8445e-05,
         -1.2241e-05, -2.1681e-05]], device='cuda:0')
Loss: 1.1397958993911743


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[    0,  2297,   287,  ..., 10826, 13886,   292],
        [    0,    12,   287,  ...,   199,   769, 18432]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2535e-05,  1.1473e-04, -1.3302e-04,  ...,  7.7394e-05,
         -9.6796e-05,  3.4523e-05],
        [-2.1607e-05, -1.5870e-05,  6.4187e-06,  ..., -1.9327e-05,
         -6.1467e-06, -1.0885e-05],
        [-2.6122e-05, -1.9133e-05,  7.7598e-06,  ..., -2.3320e-05,
         -7.4171e-06, -1.3158e-05],
        [-3.5882e-05, -2.6345e-05,  1.0692e-05,  ..., -3.2097e-05,
         -1.0177e-05, -1.8105e-05],
        [-4.9174e-05, -3.6031e-05,  1.4588e-05,  ..., -4.3929e-05,
         -1.3970e-05, -2.4736e-05]], device='cuda:0')
Loss: 1.1494847536087036
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0164,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4992e-05,  1.9573e-05, -4.6791e-05,  ...,  7.1394e-06,
         -2.6599e-05,  2.5259e-05],
        [-2.1444e-05, -1.5376e-05,  5.0944e-06,  ..., -1.8388e-05,
         -4.4358e-06, -8.9055e-06],
        [ 2.7740e-05,  1.9780e-05, -4.8866e-06,  ...,  2.3412e-05,
          4.4675e-06,  9.5471e-06],
        [-2.2381e-05, -1.6747e-05,  6.2299e-06,  ..., -2.0901e-05,
         -4.9083e-06, -1.0022e-05],
        [-4.6017e-05, -3.2838e-05,  1.0113e-05,  ..., -3.9205e-05,
         -9.1214e-06, -1.9408e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8706e-08, 1.9799e-08, 2.9770e-08,  ..., 2.2068e-08, 5.2063e-08,
         1.1227e-08],
        [4.0268e-11, 2.4030e-11, 1.6500e-12,  ..., 2.7602e-11, 1.2036e-12,
         5.9695e-12],
        [6.5716e-10, 3.8475e-10, 2.3423e-11,  ..., 5.0604e-10, 1.3552e-11,
         9.8083e-11],
        [1.8392e-10, 2.2721e-10, 6.4508e-12,  ..., 1.3711e-10, 1.8516e-11,
         7.7557e-11],
        [1.5693e-10, 8.7525e-11, 8.1867e-12,  ..., 1.0905e-10, 3.3761e-12,
         2.4016e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 50


Running epoch 0, step 400, batch 400
Sampled inputs[:2]: tensor([[   0, 1883, 1090,  ...,  365, 1943,  298],
        [   0, 6957,  271,  ..., 9094,  266, 4320]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7084e-05, -3.8999e-07, -1.0423e-05,  ..., -5.0625e-06,
         -2.2707e-05, -3.8806e-05],
        [-2.6226e-06, -1.9372e-06,  7.8976e-07,  ..., -2.3991e-06,
         -7.2271e-07, -1.3858e-06],
        [-3.1888e-06, -2.3544e-06,  9.6112e-07,  ..., -2.9057e-06,
         -8.7544e-07, -1.6764e-06],
        [-4.4703e-06, -3.3081e-06,  1.3486e-06,  ..., -4.0829e-06,
         -1.2219e-06, -2.3544e-06],
        [-6.0201e-06, -4.4703e-06,  1.8179e-06,  ..., -5.5134e-06,
         -1.6540e-06, -3.1739e-06]], device='cuda:0')
Loss: 1.1503925323486328


Running epoch 0, step 401, batch 401
Sampled inputs[:2]: tensor([[    0,   508,  3282,  ...,   334,   287, 31884],
        [    0,  4994,  8429,  ...,    12,   795,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2652e-05, -8.2889e-06, -2.6170e-05,  ...,  1.7211e-05,
         -2.9762e-05, -6.0125e-05],
        [-5.3048e-06, -3.9190e-06,  1.5795e-06,  ..., -4.8280e-06,
         -1.4678e-06, -2.7940e-06],
        [-6.4224e-06, -4.7535e-06,  1.9148e-06,  ..., -5.8413e-06,
         -1.7770e-06, -3.3826e-06],
        [-8.9705e-06, -6.6310e-06,  2.6748e-06,  ..., -8.1658e-06,
         -2.4661e-06, -4.7088e-06],
        [-1.2100e-05, -8.9705e-06,  3.6061e-06,  ..., -1.1027e-05,
         -3.3453e-06, -6.3777e-06]], device='cuda:0')
Loss: 1.1313985586166382


Running epoch 0, step 402, batch 402
Sampled inputs[:2]: tensor([[   0,    9, 1471,  ...,  741,  266, 5821],
        [   0,  287, 2421,  ..., 6612,  352,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6928e-05,  4.1406e-05, -6.5662e-05,  ..., -1.0687e-05,
         -2.3966e-05, -3.5478e-05],
        [-8.0019e-06, -5.9009e-06,  2.3544e-06,  ..., -7.2420e-06,
         -2.2575e-06, -4.2021e-06],
        [-9.6709e-06, -7.1526e-06,  2.8498e-06,  ..., -8.7619e-06,
         -2.7306e-06, -5.0813e-06],
        [ 1.7620e-04,  1.2451e-04, -4.9253e-05,  ...,  1.5374e-04,
          3.9567e-05,  9.8865e-05],
        [-1.8269e-05, -1.3500e-05,  5.3719e-06,  ..., -1.6540e-05,
         -5.1484e-06, -9.5814e-06]], device='cuda:0')
Loss: 1.1275345087051392


Running epoch 0, step 403, batch 403
Sampled inputs[:2]: tensor([[    0,  3532,   300,  ...,    12,   461,   806],
        [    0, 14161,  1241,  ..., 15255,   768,  4239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8740e-05,  3.5190e-05, -5.8087e-05,  ..., -3.3129e-05,
         -6.0238e-05, -1.8150e-05],
        [-1.0654e-05, -7.8678e-06,  3.1367e-06,  ..., -9.6262e-06,
         -2.9989e-06, -5.5954e-06],
        [-1.2890e-05, -9.5516e-06,  3.8035e-06,  ..., -1.1653e-05,
         -3.6284e-06, -6.7726e-06],
        [ 1.7170e-04,  1.2118e-04, -4.7919e-05,  ...,  1.4968e-04,
          3.8315e-05,  9.6495e-05],
        [-2.4348e-05, -1.8001e-05,  7.1675e-06,  ..., -2.1994e-05,
         -6.8396e-06, -1.2770e-05]], device='cuda:0')
Loss: 1.124040961265564


Running epoch 0, step 404, batch 404
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,   741,  2985,  ...,   199,   769,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5089e-05,  2.2723e-05, -2.7641e-05,  ..., -5.1009e-05,
         -5.7614e-05, -6.6405e-05],
        [-1.3307e-05, -9.8497e-06,  3.9265e-06,  ..., -1.2040e-05,
         -3.7141e-06, -7.0110e-06],
        [-1.6108e-05, -1.1966e-05,  4.7572e-06,  ..., -1.4573e-05,
         -4.4964e-06, -8.4862e-06],
        [ 1.6717e-04,  1.1778e-04, -4.6571e-05,  ...,  1.4557e-04,
          3.7101e-05,  9.4081e-05],
        [-3.0458e-05, -2.2560e-05,  8.9705e-06,  ..., -2.7537e-05,
         -8.4862e-06, -1.6019e-05]], device='cuda:0')
Loss: 1.1200237274169922


Running epoch 0, step 405, batch 405
Sampled inputs[:2]: tensor([[    0,  7294, 23782,  ...,   471, 11528,  3437],
        [    0,   287, 39084,  ...,   266,  1817,  1589]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5007e-05,  1.6758e-05, -2.8493e-05,  ..., -6.0613e-05,
         -8.0820e-05, -2.4093e-05],
        [-1.5974e-05, -1.1861e-05,  4.7199e-06,  ..., -1.4469e-05,
         -4.4890e-06, -8.4043e-06],
        [-1.9327e-05, -1.4395e-05,  5.7109e-06,  ..., -1.7509e-05,
         -5.4277e-06, -1.0163e-05],
        [ 1.6270e-04,  1.1441e-04, -4.5237e-05,  ...,  1.4149e-04,
          3.5804e-05,  9.1757e-05],
        [-3.6508e-05, -2.7120e-05,  1.0766e-05,  ..., -3.3021e-05,
         -1.0237e-05, -1.9163e-05]], device='cuda:0')
Loss: 1.1427490711212158


Running epoch 0, step 406, batch 406
Sampled inputs[:2]: tensor([[    0,  3070,  9719,  ...,   600,  4207,  4293],
        [    0,   292, 23950,  ...,  9305,   287,  4401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3566e-05,  8.2589e-06, -5.4170e-05,  ..., -3.7480e-05,
         -7.7370e-05, -2.4187e-06],
        [-1.8656e-05, -1.3843e-05,  5.5134e-06,  ..., -1.6883e-05,
         -5.2340e-06, -9.8050e-06],
        [-2.2575e-05, -1.6794e-05,  6.6720e-06,  ..., -2.0429e-05,
         -6.3293e-06, -1.1854e-05],
        [ 1.5817e-04,  1.1106e-04, -4.3896e-05,  ...,  1.3744e-04,
          3.4553e-05,  8.9402e-05],
        [-4.2647e-05, -3.1680e-05,  1.2577e-05,  ..., -3.8534e-05,
         -1.1943e-05, -2.2367e-05]], device='cuda:0')
Loss: 1.1309839487075806


Running epoch 0, step 407, batch 407
Sampled inputs[:2]: tensor([[    0,    12, 17340,  ...,   408,  1550,  2415],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3836e-05,  2.6771e-05, -8.7110e-05,  ..., -5.3632e-05,
         -1.0783e-04,  8.0677e-06],
        [-2.1324e-05, -1.5825e-05,  6.3106e-06,  ..., -1.9282e-05,
         -5.9903e-06, -1.1206e-05],
        [-2.5794e-05, -1.9193e-05,  7.6331e-06,  ..., -2.3320e-05,
         -7.2420e-06, -1.3545e-05],
        [ 1.5370e-04,  1.0772e-04, -4.2555e-05,  ...,  1.3341e-04,
          3.3286e-05,  8.7048e-05],
        [-4.8727e-05, -3.6210e-05,  1.4395e-05,  ..., -4.4018e-05,
         -1.3672e-05, -2.5555e-05]], device='cuda:0')
Loss: 1.14457106590271
Graident accumulation at epoch 0, step 407, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3110e-05,  2.0293e-05, -5.0823e-05,  ...,  1.0623e-06,
         -3.4722e-05,  2.3540e-05],
        [-2.1432e-05, -1.5421e-05,  5.2160e-06,  ..., -1.8478e-05,
         -4.5913e-06, -9.1355e-06],
        [ 2.2386e-05,  1.5882e-05, -3.6346e-06,  ...,  1.8739e-05,
          3.2966e-06,  7.2379e-06],
        [-4.7727e-06, -4.2999e-06,  1.3514e-06,  ..., -5.4700e-06,
         -1.0888e-06, -3.1521e-07],
        [-4.6288e-05, -3.3175e-05,  1.0541e-05,  ..., -3.9686e-05,
         -9.5764e-06, -2.0023e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8653e-08, 1.9780e-08, 2.9748e-08,  ..., 2.2049e-08, 5.2023e-08,
         1.1216e-08],
        [4.0682e-11, 2.4257e-11, 1.6882e-12,  ..., 2.7946e-11, 1.2382e-12,
         6.0891e-12],
        [6.5716e-10, 3.8474e-10, 2.3458e-11,  ..., 5.0608e-10, 1.3591e-11,
         9.8168e-11],
        [2.0736e-10, 2.3858e-10, 8.2553e-12,  ..., 1.5477e-10, 1.9606e-11,
         8.5057e-11],
        [1.5915e-10, 8.8749e-11, 8.3858e-12,  ..., 1.1088e-10, 3.5596e-12,
         2.4645e-11]], device='cuda:0')
optimizer state dict: 51.0
lr: [1.8611432270000978e-05, 1.8611432270000978e-05]
scheduler_last_epoch: 51


Running epoch 0, step 408, batch 408
Sampled inputs[:2]: tensor([[   0,   18,   66,  ...,   65,   17,  287],
        [   0,   17,  292,  ..., 8055,  365, 3125]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2360e-05,  1.4186e-05, -1.9814e-05,  ..., -5.9790e-06,
         -1.8817e-05,  2.0976e-05],
        [-2.6822e-06, -1.9968e-06,  7.8604e-07,  ..., -2.4289e-06,
         -7.5996e-07, -1.4529e-06],
        [-3.2485e-06, -2.4289e-06,  9.4995e-07,  ..., -2.9504e-06,
         -9.1642e-07, -1.7583e-06],
        [ 4.4902e-04,  3.0112e-04, -1.5928e-04,  ...,  4.2610e-04,
          1.0481e-04,  2.6490e-04],
        [-6.1691e-06, -4.5896e-06,  1.8030e-06,  ..., -5.6028e-06,
         -1.7434e-06, -3.3379e-06]], device='cuda:0')
Loss: 1.1524864435195923


Running epoch 0, step 409, batch 409
Sampled inputs[:2]: tensor([[    0,  2173,   292,  ...,   344,  8106,   344],
        [    0,   957,  1357,  ..., 26179,   287,  6458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0369e-05,  1.2569e-05, -3.2624e-05,  ..., -2.3563e-05,
         -6.1181e-06,  2.6927e-05],
        [-5.3644e-06, -3.9935e-06,  1.5758e-06,  ..., -4.8429e-06,
         -1.5087e-06, -2.9206e-06],
        [-6.4820e-06, -4.8280e-06,  1.9036e-06,  ..., -5.8711e-06,
         -1.8217e-06, -3.5241e-06],
        [ 4.4449e-04,  2.9776e-04, -1.5794e-04,  ...,  4.2201e-04,
          1.0355e-04,  2.6243e-04],
        [-1.2308e-05, -9.1493e-06,  3.6135e-06,  ..., -1.1146e-05,
         -3.4571e-06, -6.6906e-06]], device='cuda:0')
Loss: 1.1328513622283936


Running epoch 0, step 410, batch 410
Sampled inputs[:2]: tensor([[    0,   266,  6737,  ...,  2409,    12,   287],
        [    0,    14,   747,  ..., 12545,    12, 15209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4645e-05,  1.3108e-06, -5.3788e-05,  ..., -2.0194e-05,
         -1.0853e-05,  4.2715e-05],
        [-8.0019e-06, -5.9903e-06,  2.3693e-06,  ..., -7.2569e-06,
         -2.2464e-06, -4.3884e-06],
        [-9.6709e-06, -7.2420e-06,  2.8647e-06,  ..., -8.7768e-06,
         -2.7120e-06, -5.2974e-06],
        [ 4.3999e-04,  2.9436e-04, -1.5659e-04,  ...,  4.1793e-04,
          1.0230e-04,  2.5994e-04],
        [-1.8358e-05, -1.3739e-05,  5.4240e-06,  ..., -1.6659e-05,
         -5.1409e-06, -1.0043e-05]], device='cuda:0')
Loss: 1.1241868734359741


Running epoch 0, step 411, batch 411
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,    21,    66,  ...,  1377,   278,  1634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9272e-05, -3.9566e-06, -8.2792e-05,  ..., -2.3869e-05,
         -2.3251e-05,  5.6474e-05],
        [-1.0625e-05, -7.9423e-06,  3.1479e-06,  ..., -9.6858e-06,
         -3.0212e-06, -5.8413e-06],
        [-1.2875e-05, -9.6262e-06,  3.8184e-06,  ..., -1.1742e-05,
         -3.6582e-06, -7.0632e-06],
        [ 4.3555e-04,  2.9105e-04, -1.5528e-04,  ...,  4.1382e-04,
          1.0099e-04,  2.5750e-04],
        [-2.4438e-05, -1.8269e-05,  7.2271e-06,  ..., -2.2262e-05,
         -6.9365e-06, -1.3381e-05]], device='cuda:0')
Loss: 1.1568092107772827


Running epoch 0, step 412, batch 412
Sampled inputs[:2]: tensor([[    0,   278,  9939,  ...,  1238,    14,   445],
        [    0,  2467, 18011,  ...,  5913,  9281,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5818e-05,  1.0878e-05, -9.7772e-05,  ..., -1.5084e-05,
         -2.1875e-05,  5.5034e-05],
        [-1.3277e-05, -9.9242e-06,  3.9488e-06,  ..., -1.2130e-05,
         -3.8035e-06, -7.3314e-06],
        [-1.6093e-05, -1.2025e-05,  4.7870e-06,  ..., -1.4707e-05,
         -4.6007e-06, -8.8587e-06],
        [ 4.3105e-04,  2.8770e-04, -1.5392e-04,  ...,  4.0967e-04,
          9.9667e-05,  2.5498e-04],
        [-3.0488e-05, -2.2799e-05,  9.0525e-06,  ..., -2.7865e-05,
         -8.7172e-06, -1.6779e-05]], device='cuda:0')
Loss: 1.1429405212402344


Running epoch 0, step 413, batch 413
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,   259,  5918,  ...,   508,  3433,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1508e-05,  3.4831e-05, -1.0515e-04,  ..., -5.0685e-05,
         -1.3958e-05,  3.5936e-05],
        [-1.5900e-05, -1.1876e-05,  4.7311e-06,  ..., -1.4514e-05,
         -4.5337e-06, -8.7693e-06],
        [-1.9312e-05, -1.4424e-05,  5.7481e-06,  ..., -1.7643e-05,
         -5.4985e-06, -1.0625e-05],
        [ 4.2658e-04,  2.8437e-04, -1.5259e-04,  ...,  4.0559e-04,
          9.8423e-05,  2.5252e-04],
        [-3.6538e-05, -2.7299e-05,  1.0863e-05,  ..., -3.3408e-05,
         -1.0408e-05, -2.0102e-05]], device='cuda:0')
Loss: 1.1414542198181152


Running epoch 0, step 414, batch 414
Sampled inputs[:2]: tensor([[    0,  5625,  2558,  ...,   680,   292,   494],
        [    0,   721,  1717,  ...,   278, 26029,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5771e-05,  2.8402e-05, -1.1649e-04,  ..., -7.3369e-05,
         -1.6294e-05,  3.9639e-05],
        [-1.8537e-05, -1.3843e-05,  5.5060e-06,  ..., -1.6913e-05,
         -5.2564e-06, -1.0200e-05],
        [-2.2531e-05, -1.6838e-05,  6.6943e-06,  ..., -2.0579e-05,
         -6.3814e-06, -1.2368e-05],
        [ 4.2208e-04,  2.8101e-04, -1.5127e-04,  ...,  4.0148e-04,
          9.7193e-05,  2.5009e-04],
        [-4.2647e-05, -3.1859e-05,  1.2651e-05,  ..., -3.8981e-05,
         -1.2085e-05, -2.3410e-05]], device='cuda:0')
Loss: 1.1336023807525635


Running epoch 0, step 415, batch 415
Sampled inputs[:2]: tensor([[    0,   266,  5232,  ...,  2719,    13, 25385],
        [    0,  1265,   328,  ...,  2282, 35414,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5150e-05,  5.0188e-05, -1.6303e-04,  ..., -7.4839e-05,
         -1.3183e-05,  4.3827e-05],
        [-2.1175e-05, -1.5810e-05,  6.2883e-06,  ..., -1.9327e-05,
         -5.9865e-06, -1.1630e-05],
        [-2.5719e-05, -1.9222e-05,  7.6406e-06,  ..., -2.3499e-05,
         -7.2680e-06, -1.4104e-05],
        [ 4.1758e-04,  2.7765e-04, -1.4993e-04,  ...,  3.9737e-04,
          9.5949e-05,  2.4765e-04],
        [-4.8727e-05, -3.6389e-05,  1.4447e-05,  ..., -4.4525e-05,
         -1.3769e-05, -2.6703e-05]], device='cuda:0')
Loss: 1.1246849298477173
Graident accumulation at epoch 0, step 415, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5314e-05,  2.3282e-05, -6.2044e-05,  ..., -6.5279e-06,
         -3.2568e-05,  2.5569e-05],
        [-2.1406e-05, -1.5460e-05,  5.3232e-06,  ..., -1.8563e-05,
         -4.7308e-06, -9.3850e-06],
        [ 1.7576e-05,  1.2372e-05, -2.5071e-06,  ...,  1.4515e-05,
          2.2401e-06,  5.1037e-06],
        [ 3.7463e-05,  2.3896e-05, -1.3777e-05,  ...,  3.4814e-05,
          8.6150e-06,  2.4481e-05],
        [-4.6532e-05, -3.3496e-05,  1.0932e-05,  ..., -4.0170e-05,
         -9.9957e-06, -2.0691e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8598e-08, 1.9763e-08, 2.9745e-08,  ..., 2.2033e-08, 5.1971e-08,
         1.1206e-08],
        [4.1090e-11, 2.4482e-11, 1.7260e-12,  ..., 2.8292e-11, 1.2728e-12,
         6.2183e-12],
        [6.5717e-10, 3.8472e-10, 2.3493e-11,  ..., 5.0612e-10, 1.3630e-11,
         9.8269e-11],
        [3.8153e-10, 3.1544e-10, 3.0727e-11,  ..., 3.1252e-10, 2.8792e-11,
         1.4630e-10],
        [1.6136e-10, 8.9984e-11, 8.5861e-12,  ..., 1.1275e-10, 3.7457e-12,
         2.5333e-11]], device='cuda:0')
optimizer state dict: 52.0
lr: [1.8547933878823103e-05, 1.8547933878823103e-05]
scheduler_last_epoch: 52


Running epoch 0, step 416, batch 416
Sampled inputs[:2]: tensor([[    0,   706,  6989,  ...,  6914,    15,  2537],
        [    0, 48705,   292,  ...,   266,  2548,  2697]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2519e-05, -1.2297e-05, -7.1775e-06,  ..., -1.5864e-05,
         -9.0676e-08,  2.9906e-05],
        [-2.6673e-06, -1.9819e-06,  7.9349e-07,  ..., -2.4587e-06,
         -7.4133e-07, -1.4901e-06],
        [-3.2485e-06, -2.4140e-06,  9.6858e-07,  ..., -2.9802e-06,
         -9.0152e-07, -1.8105e-06],
        [-4.5896e-06, -3.4124e-06,  1.3635e-06,  ..., -4.2021e-06,
         -1.2666e-06, -2.5481e-06],
        [-6.1393e-06, -4.5598e-06,  1.8179e-06,  ..., -5.6326e-06,
         -1.7062e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.1330528259277344


Running epoch 0, step 417, batch 417
Sampled inputs[:2]: tensor([[   0,  298,  696,  ..., 3502,  287, 1047],
        [   0,  278, 5492,  ...,  328,  995,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2559e-06, -7.4807e-07, -2.1269e-05,  ..., -2.0670e-05,
         -2.5920e-06,  3.7922e-05],
        [-5.3197e-06, -3.9637e-06,  1.5646e-06,  ..., -4.8876e-06,
         -1.4715e-06, -2.9653e-06],
        [-6.4522e-06, -4.8131e-06,  1.8999e-06,  ..., -5.9158e-06,
         -1.7807e-06, -3.5912e-06],
        [-9.1493e-06, -6.8247e-06,  2.6897e-06,  ..., -8.3745e-06,
         -2.5183e-06, -5.0813e-06],
        [-1.2249e-05, -9.1195e-06,  3.5912e-06,  ..., -1.1206e-05,
         -3.3826e-06, -6.7949e-06]], device='cuda:0')
Loss: 1.1396934986114502


Running epoch 0, step 418, batch 418
Sampled inputs[:2]: tensor([[   0, 1171, 2926,  ...,  259, 4288,  654],
        [   0,  443,   40,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7334e-05, -5.5313e-06, -7.0313e-05,  ..., -3.1895e-05,
         -1.3109e-05,  4.2859e-05],
        [-7.9572e-06, -5.9456e-06,  2.3432e-06,  ..., -7.3314e-06,
         -2.1905e-06, -4.4331e-06],
        [-9.6262e-06, -7.1973e-06,  2.8349e-06,  ..., -8.8513e-06,
         -2.6412e-06, -5.3495e-06],
        [-1.3679e-05, -1.0237e-05,  4.0308e-06,  ..., -1.2577e-05,
         -3.7476e-06, -7.5996e-06],
        [-1.8269e-05, -1.3649e-05,  5.3719e-06,  ..., -1.6779e-05,
         -5.0217e-06, -1.0133e-05]], device='cuda:0')
Loss: 1.1482510566711426


Running epoch 0, step 419, batch 419
Sampled inputs[:2]: tensor([[    0,   409, 22809,  ...,   342,   720,    14],
        [    0,    14,   560,  ...,    12,  8593,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4268e-05, -1.6654e-05, -9.1101e-05,  ..., -3.3034e-05,
         -1.0018e-05,  4.9239e-05],
        [-1.0625e-05, -7.9721e-06,  3.1330e-06,  ..., -9.7603e-06,
         -2.9169e-06, -5.9158e-06],
        [-1.2830e-05, -9.6411e-06,  3.7886e-06,  ..., -1.1787e-05,
         -3.5167e-06, -7.1377e-06],
        [-1.8269e-05, -1.3724e-05,  5.3942e-06,  ..., -1.6779e-05,
         -4.9993e-06, -1.0163e-05],
        [-2.4378e-05, -1.8299e-05,  7.1824e-06,  ..., -2.2352e-05,
         -6.6906e-06, -1.3530e-05]], device='cuda:0')
Loss: 1.138835072517395


Running epoch 0, step 420, batch 420
Sampled inputs[:2]: tensor([[   0, 2579,  278,  ...,   56,    9,  271],
        [   0,  221,  422,  ..., 2693,  733,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5593e-05, -8.1185e-06, -8.3567e-05,  ..., -9.6317e-06,
         -3.3551e-05,  5.6969e-05],
        [-1.3262e-05, -9.9242e-06,  3.9078e-06,  ..., -1.2144e-05,
         -3.6657e-06, -7.3761e-06],
        [-1.6049e-05, -1.2040e-05,  4.7348e-06,  ..., -1.4707e-05,
         -4.4294e-06, -8.9258e-06],
        [-2.2829e-05, -1.7121e-05,  6.7428e-06,  ..., -2.0921e-05,
         -6.2957e-06, -1.2696e-05],
        [-3.0458e-05, -2.2799e-05,  8.9630e-06,  ..., -2.7835e-05,
         -8.4117e-06, -1.6898e-05]], device='cuda:0')
Loss: 1.124553918838501


Running epoch 0, step 421, batch 421
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   300,   275,  1184],
        [    0, 49141,    14,  ...,   342,   259,  1943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0759e-06,  2.8074e-05, -5.8508e-05,  ..., -2.9457e-05,
         -5.0121e-05,  4.3415e-05],
        [-1.5914e-05, -1.1936e-05,  4.6976e-06,  ..., -1.4573e-05,
         -4.4070e-06, -8.8587e-06],
        [-1.9252e-05, -1.4469e-05,  5.6885e-06,  ..., -1.7643e-05,
         -5.3234e-06, -1.0714e-05],
        [-2.7448e-05, -2.0608e-05,  8.1137e-06,  ..., -2.5153e-05,
         -7.5772e-06, -1.5259e-05],
        [-3.6597e-05, -2.7448e-05,  1.0781e-05,  ..., -3.3438e-05,
         -1.0118e-05, -2.0310e-05]], device='cuda:0')
Loss: 1.1514281034469604


Running epoch 0, step 422, batch 422
Sampled inputs[:2]: tensor([[   0,  300, 1635,  ...,  437,  266, 1136],
        [   0, 6574, 1707,  ...,   14, 5077,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4679e-06,  4.4740e-05, -7.4428e-05,  ..., -2.2583e-05,
         -3.6941e-05,  1.3630e-05],
        [-1.8582e-05, -1.3903e-05,  5.4985e-06,  ..., -1.7002e-05,
         -5.1558e-06, -1.0349e-05],
        [-2.2456e-05, -1.6838e-05,  6.6496e-06,  ..., -2.0564e-05,
         -6.2250e-06, -1.2510e-05],
        [-3.2067e-05, -2.4021e-05,  9.4995e-06,  ..., -2.9355e-05,
         -8.8662e-06, -1.7837e-05],
        [-4.2707e-05, -3.1978e-05,  1.2614e-05,  ..., -3.8981e-05,
         -1.1832e-05, -2.3723e-05]], device='cuda:0')
Loss: 1.1304093599319458


Running epoch 0, step 423, batch 423
Sampled inputs[:2]: tensor([[    0,  3473,   278,  ..., 11743,   472,   346],
        [    0,    43,   527,  ...,  4309,    14,  8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3323e-05,  1.5012e-05, -1.0117e-04,  ..., -3.4185e-05,
         -5.0822e-05,  7.2281e-06],
        [-2.1249e-05, -1.5900e-05,  6.2957e-06,  ..., -1.9431e-05,
         -5.9083e-06, -1.1817e-05],
        [-2.5690e-05, -1.9267e-05,  7.6182e-06,  ..., -2.3529e-05,
         -7.1377e-06, -1.4298e-05],
        [-3.6657e-05, -2.7463e-05,  1.0878e-05,  ..., -3.3557e-05,
         -1.0163e-05, -2.0370e-05],
        [-4.8816e-05, -3.6567e-05,  1.4447e-05,  ..., -4.4584e-05,
         -1.3560e-05, -2.7105e-05]], device='cuda:0')
Loss: 1.141926884651184
Graident accumulation at epoch 0, step 423, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6115e-05,  2.2455e-05, -6.5956e-05,  ..., -9.2936e-06,
         -3.4394e-05,  2.3735e-05],
        [-2.1391e-05, -1.5504e-05,  5.4205e-06,  ..., -1.8649e-05,
         -4.8485e-06, -9.6282e-06],
        [ 1.3249e-05,  9.2080e-06, -1.4946e-06,  ...,  1.0711e-05,
          1.3024e-06,  3.1636e-06],
        [ 3.0051e-05,  1.8760e-05, -1.1312e-05,  ...,  2.7976e-05,
          6.7372e-06,  1.9996e-05],
        [-4.6760e-05, -3.3803e-05,  1.1283e-05,  ..., -4.0611e-05,
         -1.0352e-05, -2.1332e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8541e-08, 1.9743e-08, 2.9725e-08,  ..., 2.2012e-08, 5.1921e-08,
         1.1195e-08],
        [4.1500e-11, 2.4711e-11, 1.7639e-12,  ..., 2.8641e-11, 1.3065e-12,
         6.3517e-12],
        [6.5717e-10, 3.8471e-10, 2.3527e-11,  ..., 5.0617e-10, 1.3667e-11,
         9.8375e-11],
        [3.8249e-10, 3.1588e-10, 3.0815e-11,  ..., 3.1333e-10, 2.8867e-11,
         1.4657e-10],
        [1.6358e-10, 9.1231e-11, 8.7862e-12,  ..., 1.1462e-10, 3.9258e-12,
         2.6042e-11]], device='cuda:0')
optimizer state dict: 53.0
lr: [1.8483129288732575e-05, 1.8483129288732575e-05]
scheduler_last_epoch: 53


Running epoch 0, step 424, batch 424
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2208,   12, 1901],
        [   0,  368, 2035,  ...,  266, 1122,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1777e-05,  1.1108e-05, -4.7679e-05,  ...,  6.2692e-06,
         -2.6328e-05, -1.4265e-05],
        [-2.6226e-06, -2.0117e-06,  7.5623e-07,  ..., -2.4289e-06,
         -7.4133e-07, -1.5050e-06],
        [-3.1888e-06, -2.4438e-06,  9.2015e-07,  ..., -2.9504e-06,
         -9.0152e-07, -1.8254e-06],
        [-4.5598e-06, -3.5167e-06,  1.3188e-06,  ..., -4.2319e-06,
         -1.2890e-06, -2.6226e-06],
        [-5.9903e-06, -4.6194e-06,  1.7285e-06,  ..., -5.5730e-06,
         -1.6913e-06, -3.4273e-06]], device='cuda:0')
Loss: 1.1286953687667847


Running epoch 0, step 425, batch 425
Sampled inputs[:2]: tensor([[    0,   278,  6653,  ...,  7524,   271, 28279],
        [    0,  1802,  4165,  ...,   298,   445,    28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0908e-05,  2.6410e-05, -3.4891e-05,  ...,  6.7226e-06,
         -2.8439e-05, -2.3077e-05],
        [-5.3048e-06, -4.0382e-06,  1.5721e-06,  ..., -4.8578e-06,
         -1.4789e-06, -3.0175e-06],
        [-6.4075e-06, -4.8727e-06,  1.8962e-06,  ..., -5.8711e-06,
         -1.7844e-06, -3.6359e-06],
        [-9.1493e-06, -6.9886e-06,  2.7195e-06,  ..., -8.4043e-06,
         -2.5481e-06, -5.2154e-06],
        [-1.2100e-05, -9.2387e-06,  3.5912e-06,  ..., -1.1116e-05,
         -3.3751e-06, -6.8694e-06]], device='cuda:0')
Loss: 1.1520310640335083


Running epoch 0, step 426, batch 426
Sampled inputs[:2]: tensor([[   0,   40,  568,  ..., 3750,  300, 3421],
        [   0,  271, 5738,  ...,   12,   21, 9023]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5266e-05,  2.1848e-05, -4.2213e-05,  ...,  2.4750e-05,
         -3.4951e-05, -2.8912e-05],
        [-7.9721e-06, -6.0499e-06,  2.3469e-06,  ..., -7.2867e-06,
         -2.2128e-06, -4.4927e-06],
        [-9.6112e-06, -7.3016e-06,  2.8312e-06,  ..., -8.7917e-06,
         -2.6636e-06, -5.4091e-06],
        [-1.3798e-05, -1.0490e-05,  4.0680e-06,  ..., -1.2636e-05,
         -3.8147e-06, -7.7784e-06],
        [-1.8269e-05, -1.3888e-05,  5.3793e-06,  ..., -1.6719e-05,
         -5.0664e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.1334244012832642


Running epoch 0, step 427, batch 427
Sampled inputs[:2]: tensor([[    0,   382,    17,  ...,  8733,    13,  9306],
        [    0, 29258,   765,  ...,  4196,    19,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5571e-05,  3.3363e-06, -6.4946e-05,  ...,  2.8323e-07,
         -5.6697e-05, -3.9887e-05],
        [-1.0639e-05, -8.1062e-06,  3.1553e-06,  ..., -9.7454e-06,
         -2.9206e-06, -6.0275e-06],
        [-1.2830e-05, -9.7603e-06,  3.7998e-06,  ..., -1.1742e-05,
         -3.5167e-06, -7.2569e-06],
        [-1.8418e-05, -1.4037e-05,  5.4687e-06,  ..., -1.6898e-05,
         -5.0366e-06, -1.0431e-05],
        [-2.4319e-05, -1.8537e-05,  7.2047e-06,  ..., -2.2292e-05,
         -6.6757e-06, -1.3739e-05]], device='cuda:0')
Loss: 1.11251962184906


Running epoch 0, step 428, batch 428
Sampled inputs[:2]: tensor([[   0,  287, 1477,  ...,  997,  292, 4471],
        [   0,   12,  266,  ...,  287, 2888, 4845]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3356e-08, -1.1171e-05, -6.5097e-05,  ..., -4.1439e-06,
         -7.1764e-05, -2.5101e-05],
        [-1.3322e-05, -1.0118e-05,  3.9376e-06,  ..., -1.2159e-05,
         -3.6359e-06, -7.5027e-06],
        [-1.6078e-05, -1.2204e-05,  4.7497e-06,  ..., -1.4663e-05,
         -4.3847e-06, -9.0450e-06],
        [-2.3156e-05, -1.7598e-05,  6.8545e-06,  ..., -2.1160e-05,
         -6.2957e-06, -1.3039e-05],
        [-3.0488e-05, -2.3186e-05,  9.0078e-06,  ..., -2.7835e-05,
         -8.3223e-06, -1.7136e-05]], device='cuda:0')
Loss: 1.1309040784835815


Running epoch 0, step 429, batch 429
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   266,  2025,   287],
        [    0,   266,  2552,  ...,    13, 16179,   800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7868e-05, -2.1617e-05, -1.1601e-04,  ...,  2.5612e-05,
         -8.5445e-05, -4.5881e-05],
        [-1.5974e-05, -1.2130e-05,  4.7274e-06,  ..., -1.4603e-05,
         -4.3809e-06, -9.0003e-06],
        [-1.9312e-05, -1.4648e-05,  5.7109e-06,  ..., -1.7643e-05,
         -5.2899e-06, -1.0870e-05],
        [-2.7746e-05, -2.1070e-05,  8.2254e-06,  ..., -2.5392e-05,
         -7.5772e-06, -1.5631e-05],
        [-3.6597e-05, -2.7806e-05,  1.0826e-05,  ..., -3.3438e-05,
         -1.0036e-05, -2.0579e-05]], device='cuda:0')
Loss: 1.138263463973999


Running epoch 0, step 430, batch 430
Sampled inputs[:2]: tensor([[   0, 1086,  292,  ..., 1400,  367, 1874],
        [   0, 2140,   12,  ...,  696,  688, 1998]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4217e-05, -2.0018e-05, -1.3979e-04,  ...,  2.9480e-05,
         -1.2823e-04,  3.2692e-05],
        [-1.8656e-05, -1.4126e-05,  5.5134e-06,  ..., -1.7032e-05,
         -5.1036e-06, -1.0490e-05],
        [-2.2560e-05, -1.7062e-05,  6.6608e-06,  ..., -2.0579e-05,
         -6.1616e-06, -1.2666e-05],
        [-3.2425e-05, -2.4557e-05,  9.6038e-06,  ..., -2.9653e-05,
         -8.8364e-06, -1.8224e-05],
        [-4.2737e-05, -3.2365e-05,  1.2621e-05,  ..., -3.9011e-05,
         -1.1690e-05, -2.3976e-05]], device='cuda:0')
Loss: 1.1179163455963135


Running epoch 0, step 431, batch 431
Sampled inputs[:2]: tensor([[    0,  1356,    18,  ...,    31,   333,   199],
        [    0,  7527,    15,  ...,  2677,   292, 30654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1826e-05, -1.3174e-05, -1.8276e-04,  ...,  1.9207e-05,
         -1.6853e-04,  2.1239e-05],
        [-2.1309e-05, -1.6168e-05,  6.3293e-06,  ..., -1.9491e-05,
         -5.8450e-06, -1.2010e-05],
        [-2.5764e-05, -1.9535e-05,  7.6443e-06,  ..., -2.3544e-05,
         -7.0557e-06, -1.4499e-05],
        [ 1.2673e-04,  1.0230e-04, -3.9080e-05,  ...,  1.5536e-04,
          5.2295e-05,  8.8168e-05],
        [-4.8786e-05, -3.7014e-05,  1.4476e-05,  ..., -4.4614e-05,
         -1.3381e-05, -2.7433e-05]], device='cuda:0')
Loss: 1.134299397468567
Graident accumulation at epoch 0, step 431, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0163,  0.0148, -0.0272,  ...,  0.0284, -0.0153, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6686e-05,  1.8892e-05, -7.7636e-05,  ..., -6.4435e-06,
         -4.7807e-05,  2.3485e-05],
        [-2.1382e-05, -1.5570e-05,  5.5114e-06,  ..., -1.8734e-05,
         -4.9482e-06, -9.8664e-06],
        [ 9.3478e-06,  6.3337e-06, -5.8068e-07,  ...,  7.2854e-06,
          4.6655e-07,  1.3973e-06],
        [ 3.9719e-05,  2.7114e-05, -1.4088e-05,  ...,  4.0714e-05,
          1.1293e-05,  2.6813e-05],
        [-4.6963e-05, -3.4124e-05,  1.1603e-05,  ..., -4.1011e-05,
         -1.0655e-05, -2.1942e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8484e-08, 1.9724e-08, 2.9729e-08,  ..., 2.1990e-08, 5.1898e-08,
         1.1185e-08],
        [4.1913e-11, 2.4947e-11, 1.8022e-12,  ..., 2.8992e-11, 1.3393e-12,
         6.4896e-12],
        [6.5718e-10, 3.8470e-10, 2.3562e-11,  ..., 5.0622e-10, 1.3704e-11,
         9.8487e-11],
        [3.9817e-10, 3.2603e-10, 3.2311e-11,  ..., 3.3715e-10, 3.1573e-11,
         1.5420e-10],
        [1.6580e-10, 9.2510e-11, 8.9870e-12,  ..., 1.1650e-10, 4.1009e-12,
         2.6769e-11]], device='cuda:0')
optimizer state dict: 54.0
lr: [1.8417028402436446e-05, 1.8417028402436446e-05]
scheduler_last_epoch: 54


Running epoch 0, step 432, batch 432
Sampled inputs[:2]: tensor([[    0,   346,    14,  ...,   381,   535,   505],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4910e-05, -5.1510e-06,  2.8591e-06,  ..., -3.1105e-05,
         -4.6196e-06,  1.0314e-06],
        [-2.6822e-06, -2.0266e-06,  8.1956e-07,  ..., -2.4587e-06,
         -7.2643e-07, -1.5348e-06],
        [-3.2037e-06, -2.4289e-06,  9.7603e-07,  ..., -2.9355e-06,
         -8.6799e-07, -1.8328e-06],
        [-4.7088e-06, -3.5614e-06,  1.4380e-06,  ..., -4.2915e-06,
         -1.2666e-06, -2.6822e-06],
        [-6.0499e-06, -4.5896e-06,  1.8477e-06,  ..., -5.5432e-06,
         -1.6391e-06, -3.4571e-06]], device='cuda:0')
Loss: 1.1185256242752075


Running epoch 0, step 433, batch 433
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0,   374,   298,  ..., 11183,    12,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2815e-05,  3.4559e-05,  2.2694e-05,  ..., -5.0937e-05,
         -1.4199e-05,  4.7684e-05],
        [-5.3942e-06, -4.0531e-06,  1.6242e-06,  ..., -4.9472e-06,
         -1.5013e-06, -3.0771e-06],
        [-6.4224e-06, -4.8280e-06,  1.9297e-06,  ..., -5.8860e-06,
         -1.7844e-06, -3.6657e-06],
        [-9.3877e-06, -7.0482e-06,  2.8238e-06,  ..., -8.5831e-06,
         -2.5928e-06, -5.3495e-06],
        [-1.2159e-05, -9.1493e-06,  3.6582e-06,  ..., -1.1146e-05,
         -3.3751e-06, -6.9290e-06]], device='cuda:0')
Loss: 1.1288707256317139


Running epoch 0, step 434, batch 434
Sampled inputs[:2]: tensor([[   0,   12, 1197,  ...,  516, 1136, 9774],
        [   0,  278, 5798,  ...,  266,  729, 1798]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8239e-05,  3.2698e-05,  8.6999e-06,  ..., -3.9743e-05,
         -2.1698e-05,  5.5161e-05],
        [-8.0615e-06, -6.0499e-06,  2.4214e-06,  ..., -7.3612e-06,
         -2.1830e-06, -4.6045e-06],
        [-9.6262e-06, -7.2420e-06,  2.8908e-06,  ..., -8.7917e-06,
         -2.6040e-06, -5.4985e-06],
        [-1.4126e-05, -1.0595e-05,  4.2394e-06,  ..., -1.2875e-05,
         -3.7998e-06, -8.0615e-06],
        [-1.8179e-05, -1.3679e-05,  5.4613e-06,  ..., -1.6600e-05,
         -4.9174e-06, -1.0371e-05]], device='cuda:0')
Loss: 1.1106737852096558


Running epoch 0, step 435, batch 435
Sampled inputs[:2]: tensor([[    0,    13, 23070,  ...,   266,   319,    13],
        [    0,  1192, 11929,  ...,   266,  1551,  1860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7865e-05,  5.5448e-05, -8.7086e-06,  ..., -3.9743e-05,
         -4.2930e-05,  8.8808e-05],
        [-1.0714e-05, -8.0615e-06,  3.2485e-06,  ..., -9.7752e-06,
         -2.9244e-06, -6.1244e-06],
        [-1.2815e-05, -9.6560e-06,  3.8892e-06,  ..., -1.1697e-05,
         -3.4943e-06, -7.3239e-06],
        [-1.8805e-05, -1.4141e-05,  5.7071e-06,  ..., -1.7136e-05,
         -5.1036e-06, -1.0744e-05],
        [-2.4199e-05, -1.8239e-05,  7.3388e-06,  ..., -2.2084e-05,
         -6.6012e-06, -1.3813e-05]], device='cuda:0')
Loss: 1.1266512870788574


Running epoch 0, step 436, batch 436
Sampled inputs[:2]: tensor([[    0,   380,   981,  ...,   567,  5407,   472],
        [    0,  4154, 14296,  ...,   516,  1796, 18233]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5420e-05,  7.3658e-05, -2.1319e-05,  ..., -3.0941e-05,
         -4.7869e-05,  6.2057e-05],
        [-1.3426e-05, -1.0073e-05,  4.0606e-06,  ..., -1.2204e-05,
         -3.6582e-06, -7.6592e-06],
        [-1.6108e-05, -1.2100e-05,  4.8727e-06,  ..., -1.4648e-05,
         -4.3847e-06, -9.1940e-06],
        [-2.3603e-05, -1.7703e-05,  7.1451e-06,  ..., -2.1428e-05,
         -6.3926e-06, -1.3456e-05],
        [-3.0369e-05, -2.2829e-05,  9.1791e-06,  ..., -2.7597e-05,
         -8.2627e-06, -1.7300e-05]], device='cuda:0')
Loss: 1.1126784086227417


Running epoch 0, step 437, batch 437
Sampled inputs[:2]: tensor([[    0,     9,   278,  ...,   278,   298,   452],
        [    0,   292, 44809,  ...,   642,   437,  9038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2753e-05,  1.0751e-04, -7.1614e-05,  ..., -9.2672e-06,
         -4.7844e-05,  5.3876e-05],
        [-1.6108e-05, -1.2070e-05,  4.8876e-06,  ..., -1.4663e-05,
         -4.4107e-06, -9.2164e-06],
        [-1.9312e-05, -1.4484e-05,  5.8562e-06,  ..., -1.7583e-05,
         -5.2787e-06, -1.1042e-05],
        [-2.8312e-05, -2.1204e-05,  8.5905e-06,  ..., -2.5749e-05,
         -7.7039e-06, -1.6168e-05],
        [-3.6389e-05, -2.7329e-05,  1.1034e-05,  ..., -3.3140e-05,
         -9.9465e-06, -2.0772e-05]], device='cuda:0')
Loss: 1.131389856338501


Running epoch 0, step 438, batch 438
Sampled inputs[:2]: tensor([[    0,  3665,  1419,  ...,   600,   847,   328],
        [    0,    13, 20054,  ...,    19,     9,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5862e-05,  1.5080e-04, -8.2492e-05,  ..., -4.0714e-05,
         -4.4200e-05,  4.9239e-05],
        [-1.8790e-05, -1.4067e-05,  5.6960e-06,  ..., -1.7136e-05,
         -5.1297e-06, -1.0751e-05],
        [-2.2516e-05, -1.6883e-05,  6.8173e-06,  ..., -2.0549e-05,
         -6.1356e-06, -1.2875e-05],
        [-3.3021e-05, -2.4706e-05,  9.9987e-06,  ..., -3.0071e-05,
         -8.9556e-06, -1.8835e-05],
        [-4.2468e-05, -3.1859e-05,  1.2852e-05,  ..., -3.8743e-05,
         -1.1563e-05, -2.4229e-05]], device='cuda:0')
Loss: 1.1174248456954956


Running epoch 0, step 439, batch 439
Sampled inputs[:2]: tensor([[    0,   341,  2802,  ...,  1798,    12,   266],
        [    0,  5301,   792,  ..., 27135, 34090,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6177e-05,  1.6021e-04, -8.4874e-05,  ..., -4.5309e-05,
         -3.9691e-05,  2.3510e-05],
        [-2.1473e-05, -1.6093e-05,  6.4932e-06,  ..., -1.9595e-05,
         -5.8450e-06, -1.2293e-05],
        [-2.5734e-05, -1.9327e-05,  7.7784e-06,  ..., -2.3499e-05,
         -6.9961e-06, -1.4730e-05],
        [-3.7670e-05, -2.8238e-05,  1.1384e-05,  ..., -3.4332e-05,
         -1.0192e-05, -2.1517e-05],
        [-4.8578e-05, -3.6478e-05,  1.4663e-05,  ..., -4.4346e-05,
         -1.3188e-05, -2.7731e-05]], device='cuda:0')
Loss: 1.1415940523147583
Graident accumulation at epoch 0, step 439, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0032,  ..., -0.0028,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0163,  0.0148, -0.0273,  ...,  0.0284, -0.0152, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.6635e-05,  3.3024e-05, -7.8360e-05,  ..., -1.0330e-05,
         -4.6996e-05,  2.3488e-05],
        [-2.1391e-05, -1.5622e-05,  5.6095e-06,  ..., -1.8820e-05,
         -5.0379e-06, -1.0109e-05],
        [ 5.8396e-06,  3.7676e-06,  2.5523e-07,  ...,  4.2070e-06,
         -2.7972e-07, -2.1537e-07],
        [ 3.1980e-05,  2.1578e-05, -1.1541e-05,  ...,  3.3210e-05,
          9.1444e-06,  2.1980e-05],
        [-4.7124e-05, -3.4360e-05,  1.1909e-05,  ..., -4.1345e-05,
         -1.0908e-05, -2.2521e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8427e-08, 1.9729e-08, 2.9706e-08,  ..., 2.1970e-08, 5.1848e-08,
         1.1174e-08],
        [4.2332e-11, 2.5181e-11, 1.8426e-12,  ..., 2.9347e-11, 1.3722e-12,
         6.6342e-12],
        [6.5718e-10, 3.8469e-10, 2.3599e-11,  ..., 5.0626e-10, 1.3739e-11,
         9.8605e-11],
        [3.9919e-10, 3.2650e-10, 3.2408e-11,  ..., 3.3800e-10, 3.1645e-11,
         1.5451e-10],
        [1.6799e-10, 9.3748e-11, 9.1930e-12,  ..., 1.1835e-10, 4.2707e-12,
         2.7511e-11]], device='cuda:0')
optimizer state dict: 55.0
lr: [1.8349641320727145e-05, 1.8349641320727145e-05]
scheduler_last_epoch: 55


Running epoch 0, step 440, batch 440
Sampled inputs[:2]: tensor([[   0,  266, 1441,  ..., 1817, 1589,  278],
        [   0, 2771, 2070,  ...,  221,  396,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8471e-05, -4.1454e-05, -1.7038e-05,  ...,  1.5301e-05,
         -3.0226e-05, -1.8881e-05],
        [-2.7418e-06, -2.0266e-06,  8.4192e-07,  ..., -2.4289e-06,
         -7.2643e-07, -1.5423e-06],
        [-3.2634e-06, -2.4140e-06,  1.0058e-06,  ..., -2.8908e-06,
         -8.6427e-07, -1.8403e-06],
        [ 8.0210e-05,  6.0285e-05, -3.1957e-05,  ...,  7.9996e-05,
          3.1807e-05,  6.5620e-05],
        [-6.1691e-06, -4.5598e-06,  1.8999e-06,  ..., -5.4538e-06,
         -1.6391e-06, -3.4720e-06]], device='cuda:0')
Loss: 1.1224981546401978


Running epoch 0, step 441, batch 441
Sampled inputs[:2]: tensor([[    0,  5506,   696,  ...,   607, 11129,   276],
        [    0,   342,   266,  ...,  4998,  4756,  5139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7033e-05, -8.5623e-05, -4.3731e-05,  ...,  9.8789e-06,
         -6.6679e-06, -5.4689e-05],
        [-5.4836e-06, -4.0680e-06,  1.6764e-06,  ..., -4.8876e-06,
         -1.4678e-06, -3.0920e-06],
        [-6.5267e-06, -4.8578e-06,  2.0042e-06,  ..., -5.8264e-06,
         -1.7472e-06, -3.6880e-06],
        [ 7.5352e-05,  5.6664e-05, -3.0474e-05,  ...,  7.5645e-05,
          3.0503e-05,  6.2878e-05],
        [-1.2368e-05, -9.1791e-06,  3.7849e-06,  ..., -1.1027e-05,
         -3.3081e-06, -6.9737e-06]], device='cuda:0')
Loss: 1.1390904188156128


Running epoch 0, step 442, batch 442
Sampled inputs[:2]: tensor([[   0,  275, 4452,  ...,   12, 3516, 5227],
        [   0,   12,  287,  ...,  278, 4697,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1820e-05, -9.2381e-05, -7.4946e-05,  ...,  3.1275e-05,
         -2.5332e-05, -3.5833e-05],
        [-8.2105e-06, -6.0946e-06,  2.5295e-06,  ..., -7.3165e-06,
         -2.1979e-06, -4.6268e-06],
        [-9.7603e-06, -7.2867e-06,  3.0175e-06,  ..., -8.7321e-06,
         -2.6152e-06, -5.5134e-06],
        [ 7.0524e-05,  5.3058e-05, -2.8962e-05,  ...,  7.1323e-05,
          2.9214e-05,  6.0166e-05],
        [-1.8477e-05, -1.3739e-05,  5.7071e-06,  ..., -1.6510e-05,
         -4.9546e-06, -1.0416e-05]], device='cuda:0')
Loss: 1.1375324726104736


Running epoch 0, step 443, batch 443
Sampled inputs[:2]: tensor([[   0, 1867,  300,  ...,  259, 3095, 1842],
        [   0, 1416,  367,  ...,  555,  764,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0900e-05, -7.3559e-05, -8.0538e-05,  ...,  4.4845e-05,
         -2.4583e-05, -3.1542e-05],
        [-1.0952e-05, -8.1211e-06,  3.3602e-06,  ..., -9.7454e-06,
         -2.9132e-06, -6.1840e-06],
        [-1.3009e-05, -9.7007e-06,  4.0010e-06,  ..., -1.1608e-05,
         -3.4571e-06, -7.3612e-06],
        [ 6.5667e-05,  4.9467e-05, -2.7487e-05,  ...,  6.7032e-05,
          2.7955e-05,  5.7409e-05],
        [-2.4647e-05, -1.8328e-05,  7.5772e-06,  ..., -2.1994e-05,
         -6.5640e-06, -1.3918e-05]], device='cuda:0')
Loss: 1.124035358428955


Running epoch 0, step 444, batch 444
Sampled inputs[:2]: tensor([[    0,  2348,   565,  ...,    12,   709,   266],
        [    0,   341, 22766,  ...,   271,   266,  1176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9503e-05, -6.4610e-05, -1.1118e-04,  ...,  2.9613e-05,
         -2.4579e-05, -3.6081e-05],
        [-1.3679e-05, -1.0163e-05,  4.2357e-06,  ..., -1.2234e-05,
         -3.6694e-06, -7.7561e-06],
        [-1.6212e-05, -1.2115e-05,  5.0291e-06,  ..., -1.4544e-05,
         -4.3437e-06, -9.2089e-06],
        [ 6.0898e-05,  4.5890e-05, -2.5952e-05,  ...,  6.2681e-05,
          2.6644e-05,  5.4668e-05],
        [-3.0756e-05, -2.2918e-05,  9.5293e-06,  ..., -2.7567e-05,
         -8.2552e-06, -1.7434e-05]], device='cuda:0')
Loss: 1.1076027154922485


Running epoch 0, step 445, batch 445
Sampled inputs[:2]: tensor([[   0,  894,  496,  ...,  266,  623,  587],
        [   0,  266, 2967,  ...,  287, 4432,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0396e-05, -6.1923e-05, -1.4141e-04,  ..., -3.4908e-05,
         -1.6519e-05, -6.3288e-05],
        [-1.6391e-05, -1.2159e-05,  5.0627e-06,  ..., -1.4648e-05,
         -4.4070e-06, -9.3207e-06],
        [ 8.1207e-05,  4.6542e-05, -2.2480e-05,  ...,  6.4468e-05,
          1.5698e-05,  1.5653e-05],
        [ 5.6100e-05,  4.2329e-05, -2.4484e-05,  ...,  5.8389e-05,
          2.5347e-05,  5.1896e-05],
        [-3.6865e-05, -2.7448e-05,  1.1399e-05,  ..., -3.3021e-05,
         -9.9167e-06, -2.0966e-05]], device='cuda:0')
Loss: 1.0873849391937256


Running epoch 0, step 446, batch 446
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,  1049,   292,  ...,   221,   380,   341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9539e-06, -4.8562e-05, -1.7149e-04,  ..., -3.7968e-05,
         -1.7524e-05, -7.1395e-05],
        [-1.9103e-05, -1.4201e-05,  5.9120e-06,  ..., -1.7092e-05,
         -5.1819e-06, -1.0900e-05],
        [ 7.7988e-05,  4.4113e-05, -2.1474e-05,  ...,  6.1578e-05,
          1.4781e-05,  1.3776e-05],
        [ 5.1332e-05,  3.8723e-05, -2.2986e-05,  ...,  5.4098e-05,
          2.3991e-05,  4.9109e-05],
        [-4.2915e-05, -3.2037e-05,  1.3299e-05,  ..., -3.8475e-05,
         -1.1645e-05, -2.4498e-05]], device='cuda:0')
Loss: 1.1283600330352783


Running epoch 0, step 447, batch 447
Sampled inputs[:2]: tensor([[    0, 13466,    14,  ..., 11227,  1966,  4039],
        [    0,   273,    14,  ...,   271,   266, 25408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3073e-05, -3.8472e-05, -1.7298e-04,  ..., -4.2374e-05,
         -2.5828e-05, -7.0119e-05],
        [-2.1830e-05, -1.6272e-05,  6.7502e-06,  ..., -1.9535e-05,
         -5.9344e-06, -1.2465e-05],
        [ 7.4740e-05,  4.1654e-05, -2.0476e-05,  ...,  5.8672e-05,
          1.3891e-05,  1.1920e-05],
        [ 4.6563e-05,  3.5087e-05, -2.1511e-05,  ...,  4.9806e-05,
          2.2680e-05,  4.6368e-05],
        [-4.9084e-05, -3.6716e-05,  1.5199e-05,  ..., -4.4018e-05,
         -1.3344e-05, -2.8029e-05]], device='cuda:0')
Loss: 1.1213726997375488
Graident accumulation at epoch 0, step 447, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0148,  0.0032,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0163,  0.0149, -0.0273,  ...,  0.0284, -0.0152, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0664e-05,  2.5874e-05, -8.7822e-05,  ..., -1.3535e-05,
         -4.4879e-05,  1.4127e-05],
        [-2.1435e-05, -1.5687e-05,  5.7236e-06,  ..., -1.8891e-05,
         -5.1275e-06, -1.0345e-05],
        [ 1.2730e-05,  7.5562e-06, -1.8179e-06,  ...,  9.6534e-06,
          1.1373e-06,  9.9820e-07],
        [ 3.3438e-05,  2.2929e-05, -1.2538e-05,  ...,  3.4869e-05,
          1.0498e-05,  2.4419e-05],
        [-4.7320e-05, -3.4596e-05,  1.2238e-05,  ..., -4.1612e-05,
         -1.1152e-05, -2.3072e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8369e-08, 1.9711e-08, 2.9706e-08,  ..., 2.1950e-08, 5.1796e-08,
         1.1168e-08],
        [4.2766e-11, 2.5421e-11, 1.8863e-12,  ..., 2.9700e-11, 1.4060e-12,
         6.7830e-12],
        [6.6211e-10, 3.8604e-10, 2.3995e-11,  ..., 5.0920e-10, 1.3918e-11,
         9.8649e-11],
        [4.0096e-10, 3.2740e-10, 3.2839e-11,  ..., 3.4014e-10, 3.2128e-11,
         1.5650e-10],
        [1.7024e-10, 9.5003e-11, 9.4148e-12,  ..., 1.2017e-10, 4.4445e-12,
         2.8269e-11]], device='cuda:0')
optimizer state dict: 56.0
lr: [1.828097834093899e-05, 1.828097834093899e-05]
scheduler_last_epoch: 56


Running epoch 0, step 448, batch 448
Sampled inputs[:2]: tensor([[    0,    45,  6556,  ...,  1477,   352,  1611],
        [    0,   401,  3408,  ...,   287, 19892,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2963e-05,  4.1987e-05, -7.7711e-06,  ...,  2.2552e-06,
         -3.8236e-06,  1.3133e-05],
        [-2.7567e-06, -2.0117e-06,  8.6799e-07,  ..., -2.4736e-06,
         -7.3388e-07, -1.6168e-06],
        [-3.2634e-06, -2.3842e-06,  1.0282e-06,  ..., -2.9206e-06,
         -8.6427e-07, -1.9073e-06],
        [-4.9770e-06, -3.6359e-06,  1.5646e-06,  ..., -4.4405e-06,
         -1.3113e-06, -2.9057e-06],
        [-6.1691e-06, -4.5300e-06,  1.9372e-06,  ..., -5.5134e-06,
         -1.6391e-06, -3.6061e-06]], device='cuda:0')
Loss: 1.1266039609909058


Running epoch 0, step 449, batch 449
Sampled inputs[:2]: tensor([[    0, 14867,   278,  ...,   674,   369,  4127],
        [    0,  5646,    12,  ...,  1952,   287,  3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5127e-05,  4.0902e-05, -9.7756e-06,  ...,  3.5026e-05,
          4.2337e-06,  5.0376e-05],
        [-5.5283e-06, -4.0531e-06,  1.7509e-06,  ..., -4.9323e-06,
         -1.4827e-06, -3.2187e-06],
        [-6.5416e-06, -4.7982e-06,  2.0787e-06,  ..., -5.8264e-06,
         -1.7509e-06, -3.7998e-06],
        [-9.9242e-06, -7.2718e-06,  3.1441e-06,  ..., -8.8215e-06,
         -2.6450e-06, -5.7518e-06],
        [-1.2368e-05, -9.0897e-06,  3.9041e-06,  ..., -1.0997e-05,
         -3.3155e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.1353284120559692


Running epoch 0, step 450, batch 450
Sampled inputs[:2]: tensor([[    0, 10511,  3887,  ...,  3504,   298,   422],
        [    0,  5332,   391,  ...,   221,   334,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4068e-05,  4.9494e-05, -3.7493e-05,  ..., -3.3697e-06,
          1.2473e-05,  5.0743e-05],
        [-8.2850e-06, -6.0648e-06,  2.6114e-06,  ..., -7.4059e-06,
         -2.2538e-06, -4.7982e-06],
        [-9.7901e-06, -7.1824e-06,  3.0920e-06,  ..., -8.7470e-06,
         -2.6599e-06, -5.6624e-06],
        [-1.4752e-05, -1.0818e-05,  4.6641e-06,  ..., -1.3173e-05,
         -3.9935e-06, -8.5384e-06],
        [-1.8567e-05, -1.3620e-05,  5.8413e-06,  ..., -1.6540e-05,
         -5.0440e-06, -1.0744e-05]], device='cuda:0')
Loss: 1.1200569868087769


Running epoch 0, step 451, batch 451
Sampled inputs[:2]: tensor([[   0,  365, 5392,  ...,   14,  333,  199],
        [   0,  278, 4191,  ...,  381, 3020,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8669e-05,  5.7215e-05, -3.3665e-05,  ...,  1.1605e-05,
          1.0455e-05,  4.4323e-05],
        [-1.0997e-05, -8.0317e-06,  3.4459e-06,  ..., -9.8646e-06,
         -2.9579e-06, -6.3777e-06],
        [-1.2979e-05, -9.4920e-06,  4.0755e-06,  ..., -1.1653e-05,
         -3.4831e-06, -7.5176e-06],
        [-1.9640e-05, -1.4335e-05,  6.1691e-06,  ..., -1.7583e-05,
         -5.2452e-06, -1.1370e-05],
        [-2.4676e-05, -1.8030e-05,  7.7188e-06,  ..., -2.2084e-05,
         -6.6161e-06, -1.4290e-05]], device='cuda:0')
Loss: 1.1229678392410278


Running epoch 0, step 452, batch 452
Sampled inputs[:2]: tensor([[   0,  391, 1351,  ...,   13,   40,    9],
        [   0,  300, 5864,  ...,   12, 3667,  796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3308e-05,  3.7896e-05, -3.7884e-05,  ...,  1.1999e-05,
          1.6903e-05,  4.9443e-05],
        [-1.3754e-05, -1.0043e-05,  4.3213e-06,  ..., -1.2308e-05,
         -3.6918e-06, -7.9796e-06],
        [-1.6287e-05, -1.1906e-05,  5.1185e-06,  ..., -1.4573e-05,
         -4.3586e-06, -9.4250e-06],
        [-2.4587e-05, -1.7941e-05,  7.7337e-06,  ..., -2.1964e-05,
         -6.5491e-06, -1.4231e-05],
        [-3.0935e-05, -2.2590e-05,  9.7007e-06,  ..., -2.7597e-05,
         -8.2701e-06, -1.7896e-05]], device='cuda:0')
Loss: 1.1348719596862793


Running epoch 0, step 453, batch 453
Sampled inputs[:2]: tensor([[   0,   14, 4746,  ...,  266, 1119, 1705],
        [   0,  271,  266,  ...,  984,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2488e-05,  5.9759e-05, -2.4525e-05,  ...,  2.1728e-05,
          1.3067e-05,  5.6590e-05],
        [-1.6510e-05, -1.2055e-05,  5.2005e-06,  ..., -1.4737e-05,
         -4.4331e-06, -9.5516e-06],
        [-1.9565e-05, -1.4305e-05,  6.1616e-06,  ..., -1.7464e-05,
         -5.2378e-06, -1.1303e-05],
        [-2.9534e-05, -2.1562e-05,  9.3132e-06,  ..., -2.6315e-05,
         -7.8678e-06, -1.7062e-05],
        [-3.7134e-05, -2.7150e-05,  1.1683e-05,  ..., -3.3081e-05,
         -9.9391e-06, -2.1443e-05]], device='cuda:0')
Loss: 1.1399377584457397


Running epoch 0, step 454, batch 454
Sampled inputs[:2]: tensor([[    0,   221,   381,  ...,   360,  8978,    14],
        [    0,   634, 10095,  ...,   367, 24607,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3554e-05,  2.1158e-05, -9.1394e-07,  ...,  4.6216e-05,
         -3.0359e-05,  6.7351e-05],
        [-1.9222e-05, -1.4052e-05,  6.0573e-06,  ..., -1.7136e-05,
         -5.1558e-06, -1.1094e-05],
        [-2.2829e-05, -1.6719e-05,  7.1898e-06,  ..., -2.0355e-05,
         -6.1020e-06, -1.3158e-05],
        [-3.4422e-05, -2.5168e-05,  1.0855e-05,  ..., -3.0637e-05,
         -9.1568e-06, -1.9833e-05],
        [-4.3243e-05, -3.1680e-05,  1.3620e-05,  ..., -3.8505e-05,
         -1.1563e-05, -2.4930e-05]], device='cuda:0')
Loss: 1.112870693206787


Running epoch 0, step 455, batch 455
Sampled inputs[:2]: tensor([[    0,  6803,  6298,  ...,   490,  1781,    12],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5001e-05,  1.2094e-05,  2.8815e-05,  ...,  5.5623e-05,
         -3.8217e-05,  7.1695e-05],
        [-2.1935e-05, -1.6063e-05,  6.9067e-06,  ..., -1.9535e-05,
         -5.8524e-06, -1.2636e-05],
        [-2.6047e-05, -1.9103e-05,  8.1956e-06,  ..., -2.3216e-05,
         -6.9290e-06, -1.4991e-05],
        [-3.9309e-05, -2.8789e-05,  1.2390e-05,  ..., -3.4988e-05,
         -1.0408e-05, -2.2620e-05],
        [-4.9382e-05, -3.6240e-05,  1.5542e-05,  ..., -4.3958e-05,
         -1.3143e-05, -2.8431e-05]], device='cuda:0')
Loss: 1.1256675720214844
Graident accumulation at epoch 0, step 455, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0152, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2098e-05,  2.4496e-05, -7.6158e-05,  ..., -6.6187e-06,
         -4.4213e-05,  1.9884e-05],
        [-2.1485e-05, -1.5725e-05,  5.8419e-06,  ..., -1.8956e-05,
         -5.2000e-06, -1.0574e-05],
        [ 8.8519e-06,  4.8903e-06, -8.1652e-07,  ...,  6.3665e-06,
          3.3070e-07, -6.0068e-07],
        [ 2.6164e-05,  1.7757e-05, -1.0045e-05,  ...,  2.7884e-05,
          8.4074e-06,  1.9715e-05],
        [-4.7526e-05, -3.4760e-05,  1.2568e-05,  ..., -4.1847e-05,
         -1.1351e-05, -2.3608e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8313e-08, 1.9692e-08, 2.9678e-08,  ..., 2.1931e-08, 5.1746e-08,
         1.1162e-08],
        [4.3204e-11, 2.5654e-11, 1.9321e-12,  ..., 3.0052e-11, 1.4388e-12,
         6.9358e-12],
        [6.6213e-10, 3.8602e-10, 2.4038e-11,  ..., 5.0923e-10, 1.3952e-11,
         9.8775e-11],
        [4.0211e-10, 3.2790e-10, 3.2959e-11,  ..., 3.4102e-10, 3.2204e-11,
         1.5686e-10],
        [1.7250e-10, 9.6221e-11, 9.6469e-12,  ..., 1.2198e-10, 4.6128e-12,
         2.9049e-11]], device='cuda:0')
optimizer state dict: 57.0
lr: [1.8211049955374658e-05, 1.8211049955374658e-05]
scheduler_last_epoch: 57


Running epoch 0, step 456, batch 456
Sampled inputs[:2]: tensor([[    0,   266,  1890,  ...,   287, 38242,    13],
        [    0,   607,   259,  ...,   995,    13,  6507]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6565e-07,  5.7309e-06,  1.5986e-05,  ..., -2.4851e-05,
          8.4434e-06,  3.3791e-05],
        [-2.7567e-06, -1.9670e-06,  8.7544e-07,  ..., -2.4140e-06,
         -7.4133e-07, -1.5944e-06],
        [-3.2783e-06, -2.3395e-06,  1.0431e-06,  ..., -2.8759e-06,
         -8.7917e-07, -1.8999e-06],
        [-5.0366e-06, -3.5763e-06,  1.6019e-06,  ..., -4.4107e-06,
         -1.3411e-06, -2.9057e-06],
        [-6.2585e-06, -4.4405e-06,  1.9819e-06,  ..., -5.4836e-06,
         -1.6764e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.1252689361572266


Running epoch 0, step 457, batch 457
Sampled inputs[:2]: tensor([[    0,  1477,    12,  ..., 31038,   408,   298],
        [    0,   367,  3704,  ...,  1746,    14,   759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9198e-05,  9.6899e-06,  5.1109e-06,  ..., -2.1552e-05,
          6.2046e-07,  3.4503e-05],
        [-5.5134e-06, -3.9786e-06,  1.7472e-06,  ..., -4.8578e-06,
         -1.4789e-06, -3.2112e-06],
        [-6.5416e-06, -4.7088e-06,  2.0713e-06,  ..., -5.7667e-06,
         -1.7472e-06, -3.8072e-06],
        [-1.0043e-05, -7.2122e-06,  3.1814e-06,  ..., -8.8513e-06,
         -2.6673e-06, -5.8413e-06],
        [-1.2457e-05, -8.9407e-06,  3.9339e-06,  ..., -1.0967e-05,
         -3.3230e-06, -7.2420e-06]], device='cuda:0')
Loss: 1.1219736337661743


Running epoch 0, step 458, batch 458
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  654,  271, 4483],
        [   0, 6203,  352,  ...,  266, 3437,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9172e-05,  1.6689e-05, -2.2666e-05,  ..., -2.3037e-05,
          2.6281e-06,  3.1756e-05],
        [-8.2999e-06, -5.9456e-06,  2.6524e-06,  ..., -7.3016e-06,
         -2.2314e-06, -4.8280e-06],
        [-9.7901e-06, -7.0184e-06,  3.1292e-06,  ..., -8.6278e-06,
         -2.6263e-06, -5.6997e-06],
        [-1.5080e-05, -1.0788e-05,  4.8280e-06,  ..., -1.3292e-05,
         -4.0233e-06, -8.7619e-06],
        [-1.8656e-05, -1.3322e-05,  5.9456e-06,  ..., -1.6391e-05,
         -4.9993e-06, -1.0833e-05]], device='cuda:0')
Loss: 1.1133500337600708


Running epoch 0, step 459, batch 459
Sampled inputs[:2]: tensor([[   0, 8290,  391,  ...,  298, 1253,    7],
        [   0, 1760,  446,  ...,  329, 1405,  422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9526e-05,  2.4012e-05, -3.8878e-05,  ..., -2.9117e-05,
          1.5133e-05, -6.1050e-07],
        [-1.1057e-05, -7.9423e-06,  3.5539e-06,  ..., -9.7603e-06,
         -2.9877e-06, -6.4746e-06],
        [-1.3009e-05, -9.3579e-06,  4.1798e-06,  ..., -1.1504e-05,
         -3.5092e-06, -7.6219e-06],
        [-1.9968e-05, -1.4335e-05,  6.4299e-06,  ..., -1.7673e-05,
         -5.3570e-06, -1.1683e-05],
        [-2.4796e-05, -1.7762e-05,  7.9572e-06,  ..., -2.1875e-05,
         -6.6832e-06, -1.4499e-05]], device='cuda:0')
Loss: 1.1248934268951416


Running epoch 0, step 460, batch 460
Sampled inputs[:2]: tensor([[    0,  2853, 21042,  ...,  4120,   607, 11176],
        [    0,    12,  4567,  ...,  4154,  1799, 11883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1334e-05,  4.4779e-06, -3.8757e-05,  ..., -5.4777e-05,
          3.5217e-05, -8.5711e-06],
        [-1.3813e-05, -9.9093e-06,  4.4256e-06,  ..., -1.2189e-05,
         -3.7067e-06, -8.0690e-06],
        [-1.6272e-05, -1.1683e-05,  5.2154e-06,  ..., -1.4380e-05,
         -4.3586e-06, -9.5069e-06],
        [-2.4974e-05, -1.7896e-05,  8.0168e-06,  ..., -2.2084e-05,
         -6.6534e-06, -1.4573e-05],
        [-3.1054e-05, -2.2203e-05,  9.9242e-06,  ..., -2.7359e-05,
         -8.3074e-06, -1.8090e-05]], device='cuda:0')
Loss: 1.1309539079666138


Running epoch 0, step 461, batch 461
Sampled inputs[:2]: tensor([[   0,   13, 2549,  ...,  221,  382,  298],
        [   0, 4599, 9005,  ...,  809,   13, 1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6840e-05,  2.8033e-05, -4.5692e-05,  ..., -7.4530e-05,
          4.2871e-05, -7.9986e-06],
        [-1.6585e-05, -1.1876e-05,  5.3085e-06,  ..., -1.4633e-05,
         -4.4294e-06, -9.6858e-06],
        [-1.9521e-05, -1.3992e-05,  6.2510e-06,  ..., -1.7256e-05,
         -5.2005e-06, -1.1399e-05],
        [-2.9981e-05, -2.1443e-05,  9.6112e-06,  ..., -2.6494e-05,
         -7.9423e-06, -1.7479e-05],
        [-3.7253e-05, -2.6584e-05,  1.1906e-05,  ..., -3.2842e-05,
         -9.9167e-06, -2.1681e-05]], device='cuda:0')
Loss: 1.11614990234375


Running epoch 0, step 462, batch 462
Sampled inputs[:2]: tensor([[   0,  266, 1790,  ...,  292,   78,  527],
        [   0, 9818,  347,  ...,  413, 7359,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2197e-05,  1.4884e-05, -5.9238e-05,  ..., -8.8618e-05,
          5.2795e-05, -2.6372e-05],
        [-1.9372e-05, -1.3858e-05,  6.1877e-06,  ..., -1.7077e-05,
         -5.1670e-06, -1.1317e-05],
        [-2.2814e-05, -1.6332e-05,  7.2867e-06,  ..., -2.0146e-05,
         -6.0722e-06, -1.3322e-05],
        [-3.4988e-05, -2.5004e-05,  1.1191e-05,  ..., -3.0905e-05,
         -9.2611e-06, -2.0415e-05],
        [-4.3452e-05, -3.0994e-05,  1.3858e-05,  ..., -3.8296e-05,
         -1.1556e-05, -2.5302e-05]], device='cuda:0')
Loss: 1.1309925317764282


Running epoch 0, step 463, batch 463
Sampled inputs[:2]: tensor([[    0, 47354,  5923,  ...,   266, 14679,  8137],
        [    0,    13, 20773,  ..., 22463,  2587,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1028e-05,  2.5137e-05, -5.2791e-05,  ..., -8.1133e-05,
          3.9028e-05, -9.7838e-06],
        [-2.2128e-05, -1.5840e-05,  7.0669e-06,  ..., -1.9535e-05,
         -5.8860e-06, -1.2934e-05],
        [-2.6077e-05, -1.8671e-05,  8.3223e-06,  ..., -2.3052e-05,
         -6.9179e-06, -1.5229e-05],
        [-3.9995e-05, -2.8595e-05,  1.2785e-05,  ..., -3.5346e-05,
         -1.0550e-05, -2.3335e-05],
        [-4.9651e-05, -3.5435e-05,  1.5825e-05,  ..., -4.3809e-05,
         -1.3165e-05, -2.8923e-05]], device='cuda:0')
Loss: 1.1189579963684082
Graident accumulation at epoch 0, step 463, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0152, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9991e-05,  2.4560e-05, -7.3821e-05,  ..., -1.4070e-05,
         -3.5889e-05,  1.6917e-05],
        [-2.1549e-05, -1.5736e-05,  5.9644e-06,  ..., -1.9014e-05,
         -5.2686e-06, -1.0810e-05],
        [ 5.3590e-06,  2.5341e-06,  9.7359e-08,  ...,  3.4246e-06,
         -3.9415e-07, -2.0635e-06],
        [ 1.9548e-05,  1.3122e-05, -7.7622e-06,  ...,  2.1561e-05,
          6.5116e-06,  1.5410e-05],
        [-4.7739e-05, -3.4827e-05,  1.2894e-05,  ..., -4.2043e-05,
         -1.1532e-05, -2.4140e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8254e-08, 1.9673e-08, 2.9651e-08,  ..., 2.1916e-08, 5.1696e-08,
         1.1151e-08],
        [4.3651e-11, 2.5879e-11, 1.9801e-12,  ..., 3.0403e-11, 1.4720e-12,
         7.0962e-12],
        [6.6215e-10, 3.8599e-10, 2.4083e-11,  ..., 5.0925e-10, 1.3986e-11,
         9.8908e-11],
        [4.0331e-10, 3.2839e-10, 3.3090e-11,  ..., 3.4193e-10, 3.2283e-11,
         1.5724e-10],
        [1.7480e-10, 9.7380e-11, 9.8877e-12,  ..., 1.2378e-10, 4.7815e-12,
         2.9857e-11]], device='cuda:0')
optimizer state dict: 58.0
lr: [1.8139866849701876e-05, 1.8139866849701876e-05]
scheduler_last_epoch: 58


Running epoch 0, step 464, batch 464
Sampled inputs[:2]: tensor([[   0, 3059, 2013,  ...,  278, 1997,   14],
        [   0,  344, 2574,  ..., 2558, 2663,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8775e-05,  3.3113e-05, -5.1667e-05,  ..., -3.0476e-06,
         -1.8137e-05, -3.4757e-05],
        [-2.7716e-06, -1.9521e-06,  8.7917e-07,  ..., -2.3991e-06,
         -7.3761e-07, -1.6615e-06],
        [-3.2783e-06, -2.3246e-06,  1.0431e-06,  ..., -2.8461e-06,
         -8.7544e-07, -1.9670e-06],
        [-5.0068e-06, -3.5316e-06,  1.5944e-06,  ..., -4.3511e-06,
         -1.3262e-06, -2.9951e-06],
        [-6.2287e-06, -4.4107e-06,  1.9819e-06,  ..., -5.3942e-06,
         -1.6615e-06, -3.7253e-06]], device='cuda:0')
Loss: 1.1242408752441406


Running epoch 0, step 465, batch 465
Sampled inputs[:2]: tensor([[   0,   13, 3105,  ...,  496,   14,  879],
        [   0,  221,  474,  ...,  287,  271, 2540]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0756e-05,  6.2490e-05, -9.4570e-05,  ...,  5.5675e-05,
         -7.3656e-05, -4.9167e-05],
        [-5.5581e-06, -3.9339e-06,  1.7807e-06,  ..., -4.8131e-06,
         -1.4938e-06, -3.3155e-06],
        [-6.5565e-06, -4.6492e-06,  2.1011e-06,  ..., -5.6922e-06,
         -1.7583e-06, -3.9190e-06],
        [-1.0073e-05, -7.1377e-06,  3.2410e-06,  ..., -8.7619e-06,
         -2.6897e-06, -6.0052e-06],
        [-1.2428e-05, -8.7917e-06,  3.9786e-06,  ..., -1.0759e-05,
         -3.3304e-06, -7.4059e-06]], device='cuda:0')
Loss: 1.124434471130371


Running epoch 0, step 466, batch 466
Sampled inputs[:2]: tensor([[    0,   437,   266,  ...,   266, 16084,  1781],
        [    0,   281,   221,  ...,  2236, 15064,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1924e-05,  7.3223e-05, -1.1021e-04,  ...,  5.2090e-05,
         -6.1148e-05, -1.1159e-04],
        [-8.3148e-06, -5.9009e-06,  2.6710e-06,  ..., -7.2271e-06,
         -2.2650e-06, -4.9919e-06],
        [-9.8050e-06, -6.9588e-06,  3.1441e-06,  ..., -8.5235e-06,
         -2.6561e-06, -5.8860e-06],
        [-1.5140e-05, -1.0729e-05,  4.8727e-06,  ..., -1.3173e-05,
         -4.0829e-06, -9.0748e-06],
        [-1.8626e-05, -1.3173e-05,  5.9605e-06,  ..., -1.6153e-05,
         -5.0440e-06, -1.1161e-05]], device='cuda:0')
Loss: 1.1182188987731934


Running epoch 0, step 467, batch 467
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0,   729,  3430,  ...,  9715,    13, 42383]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4825e-06,  7.5155e-05, -1.1241e-04,  ...,  1.0073e-04,
         -8.7188e-05, -9.2691e-05],
        [-1.1086e-05, -7.8976e-06,  3.5577e-06,  ..., -9.6262e-06,
         -3.0175e-06, -6.6385e-06],
        [-1.3083e-05, -9.3281e-06,  4.1947e-06,  ..., -1.1370e-05,
         -3.5465e-06, -7.8380e-06],
        [-2.0236e-05, -1.4395e-05,  6.5118e-06,  ..., -1.7583e-05,
         -5.4613e-06, -1.2100e-05],
        [-2.4855e-05, -1.7673e-05,  7.9572e-06,  ..., -2.1547e-05,
         -6.7353e-06, -1.4856e-05]], device='cuda:0')
Loss: 1.1239759922027588


Running epoch 0, step 468, batch 468
Sampled inputs[:2]: tensor([[   0,   14,  298,  ...,  333,  199,  769],
        [   0,  278, 1059,  ...,  300, 1877,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2072e-05,  1.3399e-04, -2.2171e-04,  ...,  1.1306e-04,
         -5.9227e-05, -6.0291e-05],
        [-1.3903e-05, -9.8497e-06,  4.4480e-06,  ..., -1.2100e-05,
         -3.8072e-06, -8.3372e-06],
        [-1.6332e-05, -1.1593e-05,  5.2229e-06,  ..., -1.4231e-05,
         -4.4554e-06, -9.7901e-06],
        [-2.5272e-05, -1.7911e-05,  8.1137e-06,  ..., -2.2024e-05,
         -6.8694e-06, -1.5140e-05],
        [-3.0994e-05, -2.1964e-05,  9.9093e-06,  ..., -2.6971e-05,
         -8.4564e-06, -1.8552e-05]], device='cuda:0')
Loss: 1.1070724725723267


Running epoch 0, step 469, batch 469
Sampled inputs[:2]: tensor([[    0,   677,  6499,  ...,  2738,    12,   287],
        [    0,   376,   283,  ..., 29188,   292,  7627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9824e-05,  1.5714e-04, -2.1804e-04,  ...,  1.0455e-04,
         -6.9137e-05, -7.1623e-05],
        [-1.6734e-05, -1.1817e-05,  5.3495e-06,  ..., -1.4573e-05,
         -4.5784e-06, -9.9987e-06],
        [-1.9655e-05, -1.3918e-05,  6.2883e-06,  ..., -1.7136e-05,
         -5.3570e-06, -1.1742e-05],
        [-3.0398e-05, -2.1487e-05,  9.7528e-06,  ..., -2.6494e-05,
         -8.2552e-06, -1.8150e-05],
        [-3.7342e-05, -2.6375e-05,  1.1936e-05,  ..., -3.2514e-05,
         -1.0177e-05, -2.2277e-05]], device='cuda:0')
Loss: 1.132682204246521


Running epoch 0, step 470, batch 470
Sampled inputs[:2]: tensor([[    0,   287,   266,  ...,   998,   342, 17709],
        [    0,  8158,  1416,  ...,   413,    29,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9413e-05,  1.6459e-04, -2.4871e-04,  ...,  1.1009e-04,
         -5.4555e-05, -9.4805e-05],
        [-1.9521e-05, -1.3784e-05,  6.2287e-06,  ..., -1.7002e-05,
         -5.3085e-06, -1.1608e-05],
        [-2.2933e-05, -1.6227e-05,  7.3239e-06,  ..., -1.9982e-05,
         -6.2101e-06, -1.3635e-05],
        [-3.5465e-05, -2.5064e-05,  1.1355e-05,  ..., -3.0905e-05,
         -9.5665e-06, -2.1070e-05],
        [-4.3601e-05, -3.0786e-05,  1.3903e-05,  ..., -3.7968e-05,
         -1.1809e-05, -2.5883e-05]], device='cuda:0')
Loss: 1.1313072443008423


Running epoch 0, step 471, batch 471
Sampled inputs[:2]: tensor([[    0, 13312,  9048,  ..., 33470,  8672,  3524],
        [    0,   409,   394,  ...,   475,  5458,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8493e-05,  2.0659e-04, -2.2950e-04,  ...,  1.2494e-04,
         -4.6280e-05, -8.5367e-05],
        [-2.2307e-05, -1.5706e-05,  7.1079e-06,  ..., -1.9416e-05,
         -6.0387e-06, -1.3284e-05],
        [-2.6196e-05, -1.8477e-05,  8.3521e-06,  ..., -2.2814e-05,
         -7.0632e-06, -1.5602e-05],
        [-4.0442e-05, -2.8506e-05,  1.2934e-05,  ..., -3.5256e-05,
         -1.0870e-05, -2.4080e-05],
        [-4.9889e-05, -3.5107e-05,  1.5885e-05,  ..., -4.3422e-05,
         -1.3456e-05, -2.9668e-05]], device='cuda:0')
Loss: 1.1444026231765747
Graident accumulation at epoch 0, step 471, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0151, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4841e-05,  4.2763e-05, -8.9389e-05,  ..., -1.6876e-07,
         -3.6928e-05,  6.6885e-06],
        [-2.1625e-05, -1.5733e-05,  6.0788e-06,  ..., -1.9054e-05,
         -5.3456e-06, -1.1057e-05],
        [ 2.2035e-06,  4.3298e-07,  9.2283e-07,  ...,  8.0081e-07,
         -1.0611e-06, -3.4173e-06],
        [ 1.3549e-05,  8.9594e-06, -5.6926e-06,  ...,  1.5879e-05,
          4.7734e-06,  1.1461e-05],
        [-4.7954e-05, -3.4855e-05,  1.3193e-05,  ..., -4.2181e-05,
         -1.1725e-05, -2.4692e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8202e-08, 1.9696e-08, 2.9674e-08,  ..., 2.1910e-08, 5.1646e-08,
         1.1147e-08],
        [4.4105e-11, 2.6100e-11, 2.0287e-12,  ..., 3.0750e-11, 1.5070e-12,
         7.2656e-12],
        [6.6217e-10, 3.8594e-10, 2.4129e-11,  ..., 5.0926e-10, 1.4022e-11,
         9.9053e-11],
        [4.0454e-10, 3.2888e-10, 3.3224e-11,  ..., 3.4283e-10, 3.2369e-11,
         1.5767e-10],
        [1.7711e-10, 9.8516e-11, 1.0130e-11,  ..., 1.2554e-10, 4.9578e-12,
         3.0707e-11]], device='cuda:0')
optimizer state dict: 59.0
lr: [1.806743990132056e-05, 1.806743990132056e-05]
scheduler_last_epoch: 59


Running epoch 0, step 472, batch 472
Sampled inputs[:2]: tensor([[    0,   346,   462,  ..., 35247,  2547,   278],
        [    0,   199,  2834,  ...,  1236,   768,  4316]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1077e-05, -1.0719e-05, -7.9597e-05,  ..., -1.8655e-05,
         -3.1522e-05, -2.7209e-05],
        [-2.7865e-06, -1.9372e-06,  9.4622e-07,  ..., -2.4438e-06,
         -8.3074e-07, -1.7211e-06],
        [-3.2783e-06, -2.2799e-06,  1.1176e-06,  ..., -2.8759e-06,
         -9.7603e-07, -2.0266e-06],
        [-5.0664e-06, -3.5316e-06,  1.7285e-06,  ..., -4.4405e-06,
         -1.4976e-06, -3.1292e-06],
        [-6.1989e-06, -4.3213e-06,  2.1160e-06,  ..., -5.4538e-06,
         -1.8477e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.1172125339508057


Running epoch 0, step 473, batch 473
Sampled inputs[:2]: tensor([[    0,   677, 35427,  ..., 30465,  2783,     9],
        [    0,   287,  1410,  ...,  1255,  1699,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9182e-05, -3.6460e-05, -9.5636e-05,  ..., -1.6458e-05,
         -5.1792e-05, -5.1524e-05],
        [-5.5879e-06, -3.8445e-06,  1.8664e-06,  ..., -4.8280e-06,
         -1.5721e-06, -3.3900e-06],
        [-6.6012e-06, -4.5449e-06,  2.2054e-06,  ..., -5.7071e-06,
         -1.8477e-06, -4.0084e-06],
        [-1.0252e-05, -7.0781e-06,  3.4347e-06,  ..., -8.8513e-06,
         -2.8610e-06, -6.2138e-06],
        [-1.2547e-05, -8.6725e-06,  4.2021e-06,  ..., -1.0848e-05,
         -3.5241e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.1167927980422974


Running epoch 0, step 474, batch 474
Sampled inputs[:2]: tensor([[   0,  367, 2063,  ..., 3022,  221,  733],
        [   0, 1241, 2098,  ..., 1862,  631,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6050e-05, -7.6651e-05, -1.0240e-04,  ...,  1.4835e-05,
         -3.4658e-05, -5.7093e-05],
        [-8.3447e-06, -5.7667e-06,  2.7940e-06,  ..., -7.1675e-06,
         -2.2985e-06, -5.0515e-06],
        [-9.8944e-06, -6.8396e-06,  3.3155e-06,  ..., -8.5086e-06,
         -2.7120e-06, -5.9903e-06],
        [-1.5348e-05, -1.0625e-05,  5.1558e-06,  ..., -1.3202e-05,
         -4.1947e-06, -9.2834e-06],
        [-1.8805e-05, -1.3024e-05,  6.3032e-06,  ..., -1.6153e-05,
         -5.1633e-06, -1.1384e-05]], device='cuda:0')
Loss: 1.1126779317855835


Running epoch 0, step 475, batch 475
Sampled inputs[:2]: tensor([[    0,    25,     5,  ...,  3935,    14,    16],
        [    0,  3217, 16714,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5122e-05, -8.0912e-05, -1.2080e-04,  ...,  6.4966e-05,
         -7.7865e-05, -2.9753e-05],
        [-1.1146e-05, -7.6666e-06,  3.7290e-06,  ..., -9.5516e-06,
         -3.0622e-06, -6.7353e-06],
        [-1.3217e-05, -9.0897e-06,  4.4182e-06,  ..., -1.1325e-05,
         -3.6098e-06, -7.9870e-06],
        [-2.0504e-05, -1.4111e-05,  6.8694e-06,  ..., -1.7583e-05,
         -5.5805e-06, -1.2368e-05],
        [-2.5123e-05, -1.7315e-05,  8.4192e-06,  ..., -2.1517e-05,
         -6.8769e-06, -1.5184e-05]], device='cuda:0')
Loss: 1.1309776306152344


Running epoch 0, step 476, batch 476
Sampled inputs[:2]: tensor([[   0,   15, 2537,  ...,   14, 3544,  417],
        [   0,  623,   12,  ..., 4792, 6572,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6773e-05, -9.6764e-05, -1.0864e-04,  ...,  3.5723e-05,
         -9.5407e-05, -4.0869e-05],
        [-1.3888e-05, -9.5889e-06,  4.6231e-06,  ..., -1.1906e-05,
         -3.7886e-06, -8.3745e-06],
        [-1.6496e-05, -1.1384e-05,  5.4911e-06,  ..., -1.4126e-05,
         -4.4703e-06, -9.9391e-06],
        [-2.5600e-05, -1.7688e-05,  8.5384e-06,  ..., -2.1964e-05,
         -6.9141e-06, -1.5408e-05],
        [-3.1382e-05, -2.1696e-05,  1.0461e-05,  ..., -2.6882e-05,
         -8.5235e-06, -1.8910e-05]], device='cuda:0')
Loss: 1.1089826822280884


Running epoch 0, step 477, batch 477
Sampled inputs[:2]: tensor([[    0,  2386,  4012,  ...,   300, 15480,  1036],
        [    0,    12,   266,  ...,  1125,   609,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5145e-05, -1.2292e-04, -1.0766e-04,  ...,  4.7677e-05,
         -1.1409e-04, -7.4908e-05],
        [-1.6674e-05, -1.1511e-05,  5.5507e-06,  ..., -1.4290e-05,
         -4.5598e-06, -1.0021e-05],
        [-1.9789e-05, -1.3664e-05,  6.5863e-06,  ..., -1.6943e-05,
         -5.3756e-06, -1.1891e-05],
        [-3.0696e-05, -2.1219e-05,  1.0237e-05,  ..., -2.6315e-05,
         -8.3074e-06, -1.8418e-05],
        [-3.7640e-05, -2.6017e-05,  1.2532e-05,  ..., -3.2216e-05,
         -1.0237e-05, -2.2590e-05]], device='cuda:0')
Loss: 1.1058579683303833


Running epoch 0, step 478, batch 478
Sampled inputs[:2]: tensor([[    0,  1477,   591,  ...,  4111, 18012, 11991],
        [    0,   756,   943,  ...,  4016,    12,   627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0501e-04, -1.3566e-04, -8.6592e-05,  ...,  6.2947e-05,
         -1.1445e-04, -7.6875e-05],
        [-1.9461e-05, -1.3448e-05,  6.4597e-06,  ..., -1.6674e-05,
         -5.3383e-06, -1.1705e-05],
        [-2.3067e-05, -1.5944e-05,  7.6592e-06,  ..., -1.9759e-05,
         -6.2883e-06, -1.3873e-05],
        [-3.5793e-05, -2.4751e-05,  1.1906e-05,  ..., -3.0667e-05,
         -9.7230e-06, -2.1487e-05],
        [-4.3958e-05, -3.0398e-05,  1.4588e-05,  ..., -3.7611e-05,
         -1.1995e-05, -2.6405e-05]], device='cuda:0')
Loss: 1.1259019374847412


Running epoch 0, step 479, batch 479
Sampled inputs[:2]: tensor([[    0,    33,    12,  ...,  1110,   467, 17467],
        [    0,   277,   279,  ...,    12,   287,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4879e-05, -1.9643e-04, -1.1395e-04,  ...,  9.5155e-05,
         -1.2065e-04, -9.9043e-05],
        [-2.2218e-05, -1.5371e-05,  7.3500e-06,  ..., -1.9059e-05,
         -6.0946e-06, -1.3359e-05],
        [-2.6360e-05, -1.8239e-05,  8.7321e-06,  ..., -2.2620e-05,
         -7.1898e-06, -1.5855e-05],
        [-4.0889e-05, -2.8297e-05,  1.3553e-05,  ..., -3.5077e-05,
         -1.1109e-05, -2.4542e-05],
        [-5.0187e-05, -3.4750e-05,  1.6600e-05,  ..., -4.3005e-05,
         -1.3702e-05, -3.0145e-05]], device='cuda:0')
Loss: 1.0960465669631958
Graident accumulation at epoch 0, step 479, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0031,  ..., -0.0027,  0.0227, -0.0199],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0149, -0.0273,  ...,  0.0285, -0.0151, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9845e-05,  1.8844e-05, -9.1846e-05,  ...,  9.3636e-06,
         -4.5300e-05, -3.8846e-06],
        [-2.1684e-05, -1.5697e-05,  6.2059e-06,  ..., -1.9054e-05,
         -5.4205e-06, -1.1287e-05],
        [-6.5289e-07, -1.4342e-06,  1.7038e-06,  ..., -1.5413e-06,
         -1.6739e-06, -4.6611e-06],
        [ 8.1051e-06,  5.2337e-06, -3.7681e-06,  ...,  1.0783e-05,
          3.1852e-06,  7.8607e-06],
        [-4.8177e-05, -3.4845e-05,  1.3534e-05,  ..., -4.2263e-05,
         -1.1922e-05, -2.5238e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8151e-08, 1.9715e-08, 2.9657e-08,  ..., 2.1897e-08, 5.1609e-08,
         1.1145e-08],
        [4.4554e-11, 2.6310e-11, 2.0807e-12,  ..., 3.1082e-11, 1.5427e-12,
         7.4368e-12],
        [6.6220e-10, 3.8589e-10, 2.4181e-11,  ..., 5.0926e-10, 1.4060e-11,
         9.9205e-11],
        [4.0580e-10, 3.2935e-10, 3.3375e-11,  ..., 3.4372e-10, 3.2460e-11,
         1.5811e-10],
        [1.7945e-10, 9.9625e-11, 1.0396e-11,  ..., 1.2726e-10, 5.1406e-12,
         3.1585e-11]], device='cuda:0')
optimizer state dict: 60.0
lr: [1.799378017770064e-05, 1.799378017770064e-05]
scheduler_last_epoch: 60
Epoch 0 | Batch 479/1048 | Training PPL: 6427.70309013335 | time 12.712898254394531
Saving checkpoint at epoch 0, step 479, batch 479
Epoch 0 | Validation PPL: 9.320351887367028 | Learning rate: 1.799378017770064e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_479, AFTER epoch 0, step 479


Running epoch 0, step 480, batch 480
Sampled inputs[:2]: tensor([[   0, 5221, 7166,  ..., 4309,  342,  996],
        [   0, 1615,  287,  ...,  259,  623,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8819e-05, -2.2687e-05, -4.9628e-05,  ...,  7.5657e-06,
          1.8172e-05,  1.1404e-05],
        [-2.8312e-06, -1.8850e-06,  9.4995e-07,  ..., -2.3842e-06,
         -7.9721e-07, -1.7136e-06],
        [-3.3528e-06, -2.2352e-06,  1.1250e-06,  ..., -2.8163e-06,
         -9.3877e-07, -2.0266e-06],
        [-5.1558e-06, -3.4273e-06,  1.7211e-06,  ..., -4.3213e-06,
         -1.4380e-06, -3.0994e-06],
        [-6.4075e-06, -4.2617e-06,  2.1458e-06,  ..., -5.3644e-06,
         -1.7956e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.1185146570205688


Running epoch 0, step 481, batch 481
Sampled inputs[:2]: tensor([[   0, 1552,  271,  ...,   13,  287,  995],
        [   0,   17,   14,  ...,  650, 1711,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0370e-05, -1.5540e-05, -3.7887e-05,  ...,  3.1763e-05,
          2.5345e-05,  1.0426e-05],
        [-5.6624e-06, -3.7923e-06,  1.8738e-06,  ..., -4.7535e-06,
         -1.5683e-06, -3.4347e-06],
        [-6.6906e-06, -4.4852e-06,  2.2203e-06,  ..., -5.6177e-06,
         -1.8403e-06, -4.0531e-06],
        [-1.0282e-05, -6.8843e-06,  3.3975e-06,  ..., -8.6427e-06,
         -2.8238e-06, -6.2138e-06],
        [-1.2785e-05, -8.5533e-06,  4.2319e-06,  ..., -1.0729e-05,
         -3.5241e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.1415401697158813


Running epoch 0, step 482, batch 482
Sampled inputs[:2]: tensor([[    0,   365,  1462,  ...,   518,  6104,   278],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1287e-05, -4.2867e-05, -8.7524e-05,  ...,  4.2672e-05,
          5.6109e-05,  1.4982e-05],
        [-8.4788e-06, -5.7146e-06,  2.7940e-06,  ..., -7.1079e-06,
         -2.3320e-06, -5.1484e-06],
        [-1.0028e-05, -6.7651e-06,  3.3081e-06,  ..., -8.4192e-06,
         -2.7381e-06, -6.0797e-06],
        [-1.5378e-05, -1.0356e-05,  5.0664e-06,  ..., -1.2904e-05,
         -4.1947e-06, -9.3132e-06],
        [-1.9193e-05, -1.2904e-05,  6.3181e-06,  ..., -1.6093e-05,
         -5.2527e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.1257383823394775


Running epoch 0, step 483, batch 483
Sampled inputs[:2]: tensor([[    0,    13,  1311,  ...,   271,   795,   957],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9995e-05, -5.7280e-05, -1.2132e-04,  ...,  2.9021e-05,
          6.7707e-05,  4.4750e-05],
        [-1.1265e-05, -7.5847e-06,  3.7178e-06,  ..., -9.4771e-06,
         -3.0957e-06, -6.8545e-06],
        [ 1.8080e-04,  1.0904e-04, -6.6315e-05,  ...,  1.3742e-04,
          4.0100e-05,  1.0559e-04],
        [-2.0444e-05, -1.3769e-05,  6.7502e-06,  ..., -1.7226e-05,
         -5.5730e-06, -1.2428e-05],
        [-2.5511e-05, -1.7166e-05,  8.4043e-06,  ..., -2.1458e-05,
         -6.9737e-06, -1.5467e-05]], device='cuda:0')
Loss: 1.1228865385055542


Running epoch 0, step 484, batch 484
Sampled inputs[:2]: tensor([[   0,   12, 9248,  ..., 2673, 4239,  292],
        [   0, 4110,  271,  ...,  944,  278, 3230]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2861e-05, -5.8244e-05, -1.4061e-04,  ...,  9.5649e-05,
          8.5360e-05,  4.8927e-05],
        [-1.4067e-05, -9.4771e-06,  4.6603e-06,  ..., -1.1876e-05,
         -3.9190e-06, -8.5682e-06],
        [ 7.1950e-04,  4.8712e-04, -2.2939e-04,  ...,  6.1873e-04,
          2.1801e-04,  4.7443e-04],
        [-2.5511e-05, -1.7196e-05,  8.4639e-06,  ..., -2.1547e-05,
         -7.0557e-06, -1.5527e-05],
        [-3.1859e-05, -2.1458e-05,  1.0535e-05,  ..., -2.6882e-05,
         -8.8364e-06, -1.9342e-05]], device='cuda:0')
Loss: 1.153916597366333


Running epoch 0, step 485, batch 485
Sampled inputs[:2]: tensor([[    0,   266,  6449,  ...,   474,   221,   474],
        [    0,    14,  3228,  ..., 13747,   287, 20295]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6813e-05, -9.5106e-05, -1.2894e-04,  ...,  1.0550e-04,
          1.1288e-04,  4.3756e-05],
        [-1.6868e-05, -1.1355e-05,  5.6028e-06,  ..., -1.4231e-05,
         -4.7088e-06, -1.0267e-05],
        [ 1.0184e-03,  6.7600e-04, -3.3269e-04,  ...,  8.6875e-04,
          3.0479e-04,  6.1878e-04],
        [-3.0637e-05, -2.0623e-05,  1.0192e-05,  ..., -2.5868e-05,
         -8.4862e-06, -1.8626e-05],
        [-3.8236e-05, -2.5719e-05,  1.2681e-05,  ..., -3.2246e-05,
         -1.0625e-05, -2.3186e-05]], device='cuda:0')
Loss: 1.120155692100525


Running epoch 0, step 486, batch 486
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2422e-04, -1.0248e-04, -1.4405e-04,  ...,  1.1933e-04,
          9.5855e-05,  5.2723e-05],
        [-1.9655e-05, -1.3217e-05,  6.5267e-06,  ..., -1.6585e-05,
         -5.4799e-06, -1.1973e-05],
        [ 1.0151e-03,  6.7378e-04, -3.3159e-04,  ...,  8.6595e-04,
          3.0388e-04,  6.1676e-04],
        [-3.5763e-05, -2.4050e-05,  1.1891e-05,  ..., -3.0190e-05,
         -9.8869e-06, -2.1756e-05],
        [-4.4584e-05, -2.9951e-05,  1.4782e-05,  ..., -3.7611e-05,
         -1.2375e-05, -2.7061e-05]], device='cuda:0')
Loss: 1.115842342376709


Running epoch 0, step 487, batch 487
Sampled inputs[:2]: tensor([[   0, 7061,  437,  ...,  278, 9500,   18],
        [   0,    9,  287,  ...,  369, 2968, 8347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3270e-04, -1.6193e-05, -1.4271e-04,  ...,  1.3668e-04,
          9.5586e-05,  9.7719e-05],
        [-2.2516e-05, -1.5102e-05,  7.4506e-06,  ..., -1.8984e-05,
         -6.3442e-06, -1.3731e-05],
        [ 1.0117e-03,  6.7154e-04, -3.3050e-04,  ...,  8.6312e-04,
          3.0286e-04,  6.1467e-04],
        [-4.0859e-05, -2.7418e-05,  1.3538e-05,  ..., -3.4451e-05,
         -1.1414e-05, -2.4885e-05],
        [-5.1051e-05, -3.4213e-05,  1.6853e-05,  ..., -4.3005e-05,
         -1.4313e-05, -3.1024e-05]], device='cuda:0')
Loss: 1.1132237911224365
Graident accumulation at epoch 0, step 487, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0031,  ..., -0.0027,  0.0228, -0.0198],
        [ 0.0292, -0.0077,  0.0035,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0149, -0.0274,  ...,  0.0285, -0.0151, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2590e-05,  1.5340e-05, -9.6932e-05,  ...,  2.2095e-05,
         -3.1211e-05,  6.2757e-06],
        [-2.1768e-05, -1.5638e-05,  6.3304e-06,  ..., -1.9047e-05,
         -5.5129e-06, -1.1532e-05],
        [ 1.0059e-04,  6.5863e-05, -3.1517e-05,  ...,  8.4925e-05,
          2.8780e-05,  5.7272e-05],
        [ 3.2087e-06,  1.9685e-06, -2.0375e-06,  ...,  6.2600e-06,
          1.7252e-06,  4.5861e-06],
        [-4.8465e-05, -3.4782e-05,  1.3866e-05,  ..., -4.2338e-05,
         -1.2161e-05, -2.5816e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8111e-08, 1.9695e-08, 2.9648e-08,  ..., 2.1893e-08, 5.1567e-08,
         1.1144e-08],
        [4.5017e-11, 2.6512e-11, 2.1341e-12,  ..., 3.1412e-11, 1.5814e-12,
         7.6179e-12],
        [1.6851e-09, 8.3647e-10, 1.3339e-10,  ..., 1.2537e-09, 1.0577e-10,
         4.7692e-10],
        [4.0707e-10, 3.2977e-10, 3.3524e-11,  ..., 3.4456e-10, 3.2558e-11,
         1.5857e-10],
        [1.8188e-10, 1.0070e-10, 1.0669e-11,  ..., 1.2899e-10, 5.3403e-12,
         3.2516e-11]], device='cuda:0')
optimizer state dict: 61.0
lr: [1.791889893469088e-05, 1.791889893469088e-05]
scheduler_last_epoch: 61


Running epoch 0, step 488, batch 488
Sampled inputs[:2]: tensor([[   0,  394,  292,  ..., 1711,  365,  897],
        [   0,  328, 6379,  ...,  287, 1342,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7543e-05, -5.8748e-06, -1.7560e-05,  ..., -1.9856e-06,
         -2.6116e-06,  4.4062e-05],
        [-2.8014e-06, -1.8552e-06,  9.5367e-07,  ..., -2.3544e-06,
         -7.8976e-07, -1.7360e-06],
        [-3.3826e-06, -2.2352e-06,  1.1548e-06,  ..., -2.8461e-06,
         -9.4622e-07, -2.1011e-06],
        [-5.1558e-06, -3.4273e-06,  1.7658e-06,  ..., -4.3511e-06,
         -1.4380e-06, -3.2037e-06],
        [-6.3181e-06, -4.1723e-06,  2.1607e-06,  ..., -5.3048e-06,
         -1.7732e-06, -3.9041e-06]], device='cuda:0')
Loss: 1.1125876903533936


Running epoch 0, step 489, batch 489
Sampled inputs[:2]: tensor([[   0,   12,  287,  ..., 2336,  221,  334],
        [   0, 2286, 1085,  ..., 1387, 1184, 1802]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8423e-05,  5.1456e-06, -6.5653e-05,  ...,  1.5195e-06,
         -2.6056e-05,  6.6684e-05],
        [-5.6177e-06, -3.7029e-06,  1.8999e-06,  ..., -4.7237e-06,
         -1.6391e-06, -3.4645e-06],
        [-6.7055e-06, -4.4256e-06,  2.2724e-06,  ..., -5.6475e-06,
         -1.9446e-06, -4.1425e-06],
        [-1.0252e-05, -6.7651e-06,  3.4794e-06,  ..., -8.6427e-06,
         -2.9579e-06, -6.3181e-06],
        [-1.2577e-05, -8.2850e-06,  4.2617e-06,  ..., -1.0580e-05,
         -3.6508e-06, -7.7486e-06]], device='cuda:0')
Loss: 1.1127145290374756


Running epoch 0, step 490, batch 490
Sampled inputs[:2]: tensor([[    0,   806,  1255,  ...,   474,   221,   380],
        [    0, 13509,   472,  ...,  1805,    13, 27816]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3902e-04,  5.1456e-06, -1.7377e-04,  ..., -2.9202e-05,
         -5.9794e-05,  3.4726e-05],
        [-8.4043e-06, -5.5581e-06,  2.8685e-06,  ..., -7.1079e-06,
         -2.5146e-06, -5.2303e-06],
        [-1.0028e-05, -6.6459e-06,  3.4347e-06,  ..., -8.5086e-06,
         -2.9802e-06, -6.2436e-06],
        [-1.5318e-05, -1.0133e-05,  5.2452e-06,  ..., -1.2994e-05,
         -4.5300e-06, -9.5218e-06],
        [-1.8775e-05, -1.2428e-05,  6.4224e-06,  ..., -1.5914e-05,
         -5.5879e-06, -1.1683e-05]], device='cuda:0')
Loss: 1.1254127025604248


Running epoch 0, step 491, batch 491
Sampled inputs[:2]: tensor([[   0,  587,  292,  ...,   12,  287, 2261],
        [   0,  278, 1041,  ..., 2098, 1837,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0866e-04, -4.6138e-05, -2.2136e-04,  ..., -5.3400e-06,
         -4.4628e-05,  6.9293e-05],
        [-1.1221e-05, -7.4357e-06,  3.7998e-06,  ..., -9.4771e-06,
         -3.3341e-06, -6.9588e-06],
        [-1.3396e-05, -8.8811e-06,  4.5449e-06,  ..., -1.1340e-05,
         -3.9563e-06, -8.2999e-06],
        [-2.0415e-05, -1.3530e-05,  6.9290e-06,  ..., -1.7285e-05,
         -5.9977e-06, -1.2636e-05],
        [-2.5034e-05, -1.6600e-05,  8.4937e-06,  ..., -2.1189e-05,
         -7.4059e-06, -1.5527e-05]], device='cuda:0')
Loss: 1.0861296653747559


Running epoch 0, step 492, batch 492
Sampled inputs[:2]: tensor([[    0,  3634,  3444,  ...,   642,  2156,   266],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2216e-04, -6.3013e-05, -2.4243e-04,  ...,  8.7029e-06,
         -4.6746e-05,  7.7767e-05],
        [-1.4037e-05, -9.2983e-06,  4.7535e-06,  ..., -1.1861e-05,
         -4.1164e-06, -8.6650e-06],
        [-1.6794e-05, -1.1131e-05,  5.6997e-06,  ..., -1.4231e-05,
         -4.8950e-06, -1.0371e-05],
        [-2.5600e-05, -1.6958e-05,  8.6874e-06,  ..., -2.1666e-05,
         -7.4208e-06, -1.5780e-05],
        [-3.1412e-05, -2.0832e-05,  1.0654e-05,  ..., -2.6613e-05,
         -9.1717e-06, -1.9401e-05]], device='cuda:0')
Loss: 1.126646876335144


Running epoch 0, step 493, batch 493
Sampled inputs[:2]: tensor([[    0,   298,   301,  ...,    13, 10308,  2129],
        [    0,   221,   380,  ..., 10022,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2545e-04, -1.1896e-05, -2.9452e-04,  ...,  4.8186e-05,
         -3.9410e-05,  1.5816e-04],
        [-1.6868e-05, -1.1146e-05,  5.7071e-06,  ..., -1.4260e-05,
         -4.9658e-06, -1.0423e-05],
        [-2.0206e-05, -1.3351e-05,  6.8471e-06,  ..., -1.7121e-05,
         -5.9083e-06, -1.2487e-05],
        [-3.0726e-05, -2.0310e-05,  1.0423e-05,  ..., -2.6017e-05,
         -8.9407e-06, -1.8954e-05],
        [-3.7789e-05, -2.4974e-05,  1.2800e-05,  ..., -3.2037e-05,
         -1.1072e-05, -2.3335e-05]], device='cuda:0')
Loss: 1.0947843790054321


Running epoch 0, step 494, batch 494
Sampled inputs[:2]: tensor([[    0, 20080, 11069,  ...,   300,  5768,   271],
        [    0,   927,   259,  ...,   328,  9430,  2330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6477e-05, -2.0129e-05, -3.0027e-04,  ...,  9.4351e-05,
         -7.8530e-05,  9.0825e-05],
        [-1.9655e-05, -1.2971e-05,  6.6757e-06,  ..., -1.6615e-05,
         -5.7146e-06, -1.2137e-05],
        [-2.3529e-05, -1.5527e-05,  8.0094e-06,  ..., -1.9923e-05,
         -6.7949e-06, -1.4529e-05],
        [-3.5912e-05, -2.3708e-05,  1.2241e-05,  ..., -3.0398e-05,
         -1.0319e-05, -2.2143e-05],
        [-4.4048e-05, -2.9057e-05,  1.4976e-05,  ..., -3.7313e-05,
         -1.2740e-05, -2.7180e-05]], device='cuda:0')
Loss: 1.1004301309585571


Running epoch 0, step 495, batch 495
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,   957,  1231,  ...,   800,   342,  1398]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5637e-05,  5.0617e-05, -3.1690e-04,  ...,  1.2431e-04,
         -6.0985e-05,  9.3576e-05],
        [-2.2441e-05, -1.4804e-05,  7.6182e-06,  ..., -1.8969e-05,
         -6.5379e-06, -1.3895e-05],
        [-2.6867e-05, -1.7717e-05,  9.1344e-06,  ..., -2.2739e-05,
         -7.7710e-06, -1.6630e-05],
        [-4.0948e-05, -2.7031e-05,  1.3947e-05,  ..., -3.4660e-05,
         -1.1787e-05, -2.5317e-05],
        [-5.0277e-05, -3.3140e-05,  1.7077e-05,  ..., -4.2558e-05,
         -1.4566e-05, -3.1084e-05]], device='cuda:0')
Loss: 1.100937008857727
Graident accumulation at epoch 0, step 495, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0268,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0031,  ..., -0.0027,  0.0228, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0162,  0.0150, -0.0274,  ...,  0.0285, -0.0151, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7895e-05,  1.8868e-05, -1.1893e-04,  ...,  3.2317e-05,
         -3.4189e-05,  1.5006e-05],
        [-2.1835e-05, -1.5554e-05,  6.4591e-06,  ..., -1.9040e-05,
         -5.6154e-06, -1.1768e-05],
        [ 8.7840e-05,  5.7505e-05, -2.7452e-05,  ...,  7.4159e-05,
          2.5125e-05,  4.9882e-05],
        [-1.2070e-06, -9.3139e-07, -4.3898e-07,  ...,  2.1679e-06,
          3.7404e-07,  1.5958e-06],
        [-4.8646e-05, -3.4617e-05,  1.4187e-05,  ..., -4.2360e-05,
         -1.2402e-05, -2.6343e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8058e-08, 1.9678e-08, 2.9718e-08,  ..., 2.1887e-08, 5.1519e-08,
         1.1141e-08],
        [4.5475e-11, 2.6704e-11, 2.1900e-12,  ..., 3.1740e-11, 1.6225e-12,
         7.8034e-12],
        [1.6842e-09, 8.3595e-10, 1.3334e-10,  ..., 1.2530e-09, 1.0573e-10,
         4.7672e-10],
        [4.0834e-10, 3.3017e-10, 3.3685e-11,  ..., 3.4542e-10, 3.2664e-11,
         1.5906e-10],
        [1.8422e-10, 1.0169e-10, 1.0950e-11,  ..., 1.3067e-10, 5.5471e-12,
         3.3450e-11]], device='cuda:0')
optimizer state dict: 62.0
lr: [1.7842807614798848e-05, 1.7842807614798848e-05]
scheduler_last_epoch: 62


Running epoch 0, step 496, batch 496
Sampled inputs[:2]: tensor([[    0, 15912,    14,  ..., 25535,    18,  3947],
        [    0,  1590,  2140,  ...,   287,  5342,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6134e-05,  9.4613e-06, -1.7060e-05,  ...,  2.0268e-05,
         -2.0826e-05, -1.1068e-05],
        [-2.7716e-06, -1.8030e-06,  9.5367e-07,  ..., -2.3544e-06,
         -7.6741e-07, -1.6913e-06],
        [-3.3975e-06, -2.2054e-06,  1.1697e-06,  ..., -2.8908e-06,
         -9.2760e-07, -2.0564e-06],
        [-5.1856e-06, -3.3826e-06,  1.7881e-06,  ..., -4.4107e-06,
         -1.4156e-06, -3.1441e-06],
        [-6.2287e-06, -4.0531e-06,  2.1458e-06,  ..., -5.3048e-06,
         -1.7062e-06, -3.7849e-06]], device='cuda:0')
Loss: 1.0991489887237549


Running epoch 0, step 497, batch 497
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0,    12,   496,  ...,   437,   266,  3767]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2799e-05, -8.8270e-06, -8.3963e-07,  ...,  3.2972e-05,
         -5.8456e-05,  1.5990e-06],
        [-5.5581e-06, -3.6135e-06,  1.9372e-06,  ..., -4.7088e-06,
         -1.5795e-06, -3.4049e-06],
        [-6.8396e-06, -4.4405e-06,  2.3916e-06,  ..., -5.7966e-06,
         -1.9185e-06, -4.1574e-06],
        [-1.0341e-05, -6.7353e-06,  3.6135e-06,  ..., -8.7619e-06,
         -2.8983e-06, -6.3032e-06],
        [-1.2517e-05, -8.1360e-06,  4.3660e-06,  ..., -1.0610e-05,
         -3.5241e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.121071219444275


Running epoch 0, step 498, batch 498
Sampled inputs[:2]: tensor([[    0,  5260,   365,  ...,  7242,   471,   391],
        [    0,  4645,  7688,  ..., 26535,   471,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0794e-05, -2.1634e-05,  2.0296e-05,  ...,  2.1791e-05,
         -8.7860e-05,  2.8127e-05],
        [-8.3745e-06, -5.4166e-06,  2.8908e-06,  ..., -7.0781e-06,
         -2.4140e-06, -5.1111e-06],
        [-1.0297e-05, -6.6608e-06,  3.5688e-06,  ..., -8.7172e-06,
         -2.9393e-06, -6.2585e-06],
        [-1.5467e-05, -1.0028e-05,  5.3570e-06,  ..., -1.3083e-05,
         -4.4033e-06, -9.4026e-06],
        [-1.8865e-05, -1.2189e-05,  6.5118e-06,  ..., -1.5944e-05,
         -5.3942e-06, -1.1474e-05]], device='cuda:0')
Loss: 1.1218780279159546


Running epoch 0, step 499, batch 499
Sampled inputs[:2]: tensor([[    0,   266,  3574,  ...,  7052,  3829,   292],
        [    0, 11657,   367,  ..., 31468,    26,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5995e-05, -5.9415e-05,  2.0296e-05,  ..., -1.8161e-05,
         -1.5732e-04, -5.2740e-06],
        [-1.1131e-05, -7.2345e-06,  3.8520e-06,  ..., -9.4026e-06,
         -3.2000e-06, -6.7726e-06],
        [-1.3709e-05, -8.9109e-06,  4.7609e-06,  ..., -1.1608e-05,
         -3.9078e-06, -8.3148e-06],
        [-2.0534e-05, -1.3366e-05,  7.1228e-06,  ..., -1.7375e-05,
         -5.8338e-06, -1.2457e-05],
        [-2.5123e-05, -1.6302e-05,  8.6874e-06,  ..., -2.1219e-05,
         -7.1675e-06, -1.5229e-05]], device='cuda:0')
Loss: 1.1104580163955688


Running epoch 0, step 500, batch 500
Sampled inputs[:2]: tensor([[    0,  3756,    13,  ...,  1704,   278,  5851],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6814e-06, -6.8867e-05, -4.4919e-05,  ..., -4.1465e-05,
         -1.5607e-04, -2.1741e-05],
        [-1.3903e-05, -9.0376e-06,  4.8429e-06,  ..., -1.1787e-05,
         -4.0270e-06, -8.5011e-06],
        [-1.7077e-05, -1.1101e-05,  5.9679e-06,  ..., -1.4499e-05,
         -4.9062e-06, -1.0401e-05],
        [-2.5630e-05, -1.6674e-05,  8.9481e-06,  ..., -2.1756e-05,
         -7.3314e-06, -1.5616e-05],
        [-3.1292e-05, -2.0295e-05,  1.0893e-05,  ..., -2.6494e-05,
         -8.9854e-06, -1.9044e-05]], device='cuda:0')
Loss: 1.1347682476043701


Running epoch 0, step 501, batch 501
Sampled inputs[:2]: tensor([[    0, 45050,   342,  ...,  3729,   287, 27888],
        [    0,  1428,   266,  ...,  3169,  3058,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1342e-05, -1.3071e-04, -4.3470e-05,  ..., -2.7232e-05,
         -1.8030e-04, -3.9051e-05],
        [-1.6674e-05, -1.0841e-05,  5.8115e-06,  ..., -1.4141e-05,
         -4.8317e-06, -1.0207e-05],
        [-2.0459e-05, -1.3307e-05,  7.1526e-06,  ..., -1.7390e-05,
         -5.8822e-06, -1.2487e-05],
        [-3.0726e-05, -1.9997e-05,  1.0744e-05,  ..., -2.6107e-05,
         -8.8066e-06, -1.8761e-05],
        [-3.7462e-05, -2.4319e-05,  1.3053e-05,  ..., -3.1769e-05,
         -1.0774e-05, -2.2843e-05]], device='cuda:0')
Loss: 1.123974323272705


Running epoch 0, step 502, batch 502
Sampled inputs[:2]: tensor([[   0, 4868, 3106,  ..., 2637,  278,  521],
        [   0,  287,  552,  ..., 7407, 2401,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5233e-05, -1.7444e-04, -7.2266e-05,  ...,  1.7130e-05,
         -2.1468e-04, -2.5911e-05],
        [-1.9446e-05, -1.2621e-05,  6.7800e-06,  ..., -1.6510e-05,
         -5.6103e-06, -1.1906e-05],
        [-2.3842e-05, -1.5467e-05,  8.3297e-06,  ..., -2.0280e-05,
         -6.8210e-06, -1.4558e-05],
        [-3.5822e-05, -2.3261e-05,  1.2524e-05,  ..., -3.0458e-05,
         -1.0222e-05, -2.1875e-05],
        [-4.3631e-05, -2.8282e-05,  1.5199e-05,  ..., -3.7044e-05,
         -1.2495e-05, -2.6628e-05]], device='cuda:0')
Loss: 1.122281789779663


Running epoch 0, step 503, batch 503
Sampled inputs[:2]: tensor([[    0,  2950,    13,  ..., 16513,   300,  2205],
        [    0,    14,   221,  ...,   298,   408,  1849]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5068e-05, -1.3033e-04, -7.7371e-05,  ..., -1.2352e-05,
         -2.0266e-04,  2.0005e-05],
        [-2.2262e-05, -1.4454e-05,  7.7486e-06,  ..., -1.8880e-05,
         -6.4857e-06, -1.3635e-05],
        [-2.7299e-05, -1.7703e-05,  9.5218e-06,  ..., -2.3171e-05,
         -7.8864e-06, -1.6674e-05],
        [-4.0889e-05, -2.6554e-05,  1.4268e-05,  ..., -3.4720e-05,
         -1.1779e-05, -2.4974e-05],
        [-4.9949e-05, -3.2365e-05,  1.7375e-05,  ..., -4.2319e-05,
         -1.4432e-05, -3.0473e-05]], device='cuda:0')
Loss: 1.0963802337646484
Graident accumulation at epoch 0, step 503, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0031,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0151, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1599e-05,  3.9477e-06, -1.1477e-04,  ...,  2.7850e-05,
         -5.1036e-05,  1.5506e-05],
        [-2.1878e-05, -1.5444e-05,  6.5881e-06,  ..., -1.9024e-05,
         -5.7024e-06, -1.1955e-05],
        [ 7.6326e-05,  4.9984e-05, -2.3754e-05,  ...,  6.4426e-05,
          2.1824e-05,  4.3226e-05],
        [-5.1752e-06, -3.4936e-06,  1.0317e-06,  ..., -1.5208e-06,
         -8.4130e-07, -1.0612e-06],
        [-4.8776e-05, -3.4392e-05,  1.4505e-05,  ..., -4.2355e-05,
         -1.2605e-05, -2.6756e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8002e-08, 1.9675e-08, 2.9695e-08,  ..., 2.1865e-08, 5.1508e-08,
         1.1131e-08],
        [4.5926e-11, 2.6887e-11, 2.2478e-12,  ..., 3.2065e-11, 1.6630e-12,
         7.9815e-12],
        [1.6832e-09, 8.3542e-10, 1.3330e-10,  ..., 1.2523e-09, 1.0568e-10,
         4.7653e-10],
        [4.0960e-10, 3.3055e-10, 3.3855e-11,  ..., 3.4628e-10, 3.2770e-11,
         1.5952e-10],
        [1.8654e-10, 1.0264e-10, 1.1241e-11,  ..., 1.3233e-10, 5.7498e-12,
         3.4345e-11]], device='cuda:0')
optimizer state dict: 63.0
lr: [1.7765517845442444e-05, 1.7765517845442444e-05]
scheduler_last_epoch: 63


Running epoch 0, step 504, batch 504
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5527e-05, -2.6164e-06,  4.9245e-06,  ...,  2.1464e-05,
         -4.7113e-06, -2.0452e-05],
        [-2.7716e-06, -1.7285e-06,  9.8348e-07,  ..., -2.3544e-06,
         -8.4937e-07, -1.7509e-06],
        [-3.4571e-06, -2.1607e-06,  1.2219e-06,  ..., -2.9355e-06,
         -1.0505e-06, -2.1756e-06],
        [-5.1260e-06, -3.1888e-06,  1.8105e-06,  ..., -4.3213e-06,
         -1.5423e-06, -3.2187e-06],
        [-6.2287e-06, -3.8743e-06,  2.1905e-06,  ..., -5.2750e-06,
         -1.8850e-06, -3.9041e-06]], device='cuda:0')
Loss: 1.1134434938430786


Running epoch 0, step 505, batch 505
Sampled inputs[:2]: tensor([[   0, 1098,  259,  ..., 6572, 1477,   12],
        [   0, 1070, 5746,  ...,  278,  689,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6228e-05,  4.7141e-05, -3.6212e-05,  ...,  3.0199e-05,
         -3.6057e-06, -2.9669e-05],
        [-5.6028e-06, -3.5018e-06,  1.9521e-06,  ..., -4.7237e-06,
         -1.7062e-06, -3.4943e-06],
        [-6.9886e-06, -4.3660e-06,  2.4289e-06,  ..., -5.8860e-06,
         -2.1085e-06, -4.3511e-06],
        [-1.0341e-05, -6.4522e-06,  3.5912e-06,  ..., -8.6725e-06,
         -3.0994e-06, -6.4224e-06],
        [-1.2577e-05, -7.8380e-06,  4.3511e-06,  ..., -1.0580e-05,
         -3.7923e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.089239239692688


Running epoch 0, step 506, batch 506
Sampled inputs[:2]: tensor([[    0,   271,  8130,  ...,   609, 28676,   965],
        [    0,  2816,   292,  ...,  3662,   461,  2723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5799e-05,  4.7141e-05, -5.3110e-05,  ...,  6.0884e-05,
         -2.6592e-05, -5.8429e-05],
        [-8.3297e-06, -5.2378e-06,  2.9355e-06,  ..., -7.0930e-06,
         -2.5146e-06, -5.2229e-06],
        [-1.0371e-05, -6.5118e-06,  3.6508e-06,  ..., -8.8215e-06,
         -3.0994e-06, -6.4969e-06],
        [-1.5438e-05, -9.6858e-06,  5.4315e-06,  ..., -1.3083e-05,
         -4.5747e-06, -9.6262e-06],
        [-1.8716e-05, -1.1742e-05,  6.5565e-06,  ..., -1.5885e-05,
         -5.5805e-06, -1.1683e-05]], device='cuda:0')
Loss: 1.12102210521698


Running epoch 0, step 507, batch 507
Sampled inputs[:2]: tensor([[    0,   749,     9,  ...,  2756,    14,  1062],
        [    0,  9423,   298,  ...,  5274, 37902,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8211e-05,  7.2788e-05, -6.6564e-05,  ...,  7.8866e-05,
         -3.1145e-06, -1.5928e-05],
        [-1.1116e-05, -6.9886e-06,  3.9265e-06,  ..., -9.4771e-06,
         -3.3565e-06, -6.9365e-06],
        [-1.3813e-05, -8.6725e-06,  4.8727e-06,  ..., -1.1772e-05,
         -4.1276e-06, -8.6129e-06],
        [-2.0534e-05, -1.2875e-05,  7.2345e-06,  ..., -1.7434e-05,
         -6.0797e-06, -1.2755e-05],
        [-2.4974e-05, -1.5676e-05,  8.7768e-06,  ..., -2.1249e-05,
         -7.4580e-06, -1.5527e-05]], device='cuda:0')
Loss: 1.1332964897155762


Running epoch 0, step 508, batch 508
Sampled inputs[:2]: tensor([[   0, 3125,  271,  ..., 1041, 1032,   15],
        [   0,  328,  843,  ...,  298,  292,   37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2608e-05,  8.0265e-05, -8.6880e-05,  ...,  7.6710e-05,
         -3.3682e-07, -4.1769e-05],
        [-1.3918e-05, -8.7768e-06,  4.9174e-06,  ..., -1.1906e-05,
         -4.2245e-06, -8.6948e-06],
        [-1.7285e-05, -1.0878e-05,  6.1020e-06,  ..., -1.4782e-05,
         -5.1931e-06, -1.0774e-05],
        [-2.5719e-05, -1.6168e-05,  9.0674e-06,  ..., -2.1905e-05,
         -7.6592e-06, -1.5989e-05],
        [-3.1233e-05, -1.9670e-05,  1.0997e-05,  ..., -2.6673e-05,
         -9.3803e-06, -1.9431e-05]], device='cuda:0')
Loss: 1.1204036474227905


Running epoch 0, step 509, batch 509
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    12,   259,  1220],
        [    0,   278,   266,  ..., 10639,   292,  4723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1099e-05,  7.9991e-05, -1.3036e-04,  ...,  1.0382e-04,
         -3.3286e-05, -2.7236e-05],
        [-1.6689e-05, -1.0550e-05,  5.9158e-06,  ..., -1.4275e-05,
         -5.0627e-06, -1.0423e-05],
        [-2.0728e-05, -1.3083e-05,  7.3463e-06,  ..., -1.7732e-05,
         -6.2287e-06, -1.2919e-05],
        [-3.0816e-05, -1.9431e-05,  1.0908e-05,  ..., -2.6286e-05,
         -9.1791e-06, -1.9163e-05],
        [-3.7402e-05, -2.3633e-05,  1.3232e-05,  ..., -3.1978e-05,
         -1.1243e-05, -2.3276e-05]], device='cuda:0')
Loss: 1.0839109420776367


Running epoch 0, step 510, batch 510
Sampled inputs[:2]: tensor([[    0, 17301,   300,  ...,   278,   546,  1576],
        [    0,   266,  9823,  ...,    14,  1062,  7676]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7002e-05,  2.2700e-05, -1.3987e-04,  ...,  8.3503e-05,
         -6.4501e-05, -4.4537e-05],
        [-1.9461e-05, -1.2286e-05,  6.8769e-06,  ..., -1.6645e-05,
         -5.8860e-06, -1.2182e-05],
        [-2.4140e-05, -1.5229e-05,  8.5384e-06,  ..., -2.0668e-05,
         -7.2345e-06, -1.5095e-05],
        [-3.5942e-05, -2.2650e-05,  1.2696e-05,  ..., -3.0696e-05,
         -1.0684e-05, -2.2426e-05],
        [-4.3601e-05, -2.7508e-05,  1.5393e-05,  ..., -3.7313e-05,
         -1.3076e-05, -2.7210e-05]], device='cuda:0')
Loss: 1.1170051097869873


Running epoch 0, step 511, batch 511
Sampled inputs[:2]: tensor([[    0,   368, 46614,  ...,  1070,   278,  1028],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8651e-05,  1.0756e-04, -1.6626e-04,  ...,  1.2997e-04,
         -8.1310e-05, -1.3938e-05],
        [-2.2262e-05, -1.4044e-05,  7.8678e-06,  ..., -1.9044e-05,
         -6.7204e-06, -1.3903e-05],
        [-2.7627e-05, -1.7419e-05,  9.7752e-06,  ..., -2.3648e-05,
         -8.2627e-06, -1.7226e-05],
        [-4.1097e-05, -2.5883e-05,  1.4529e-05,  ..., -3.5107e-05,
         -1.2197e-05, -2.5570e-05],
        [-4.9829e-05, -3.1412e-05,  1.7598e-05,  ..., -4.2617e-05,
         -1.4909e-05, -3.1024e-05]], device='cuda:0')
Loss: 1.095116376876831
Graident accumulation at epoch 0, step 511, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0147,  0.0031,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0023, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0150, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3574e-05,  1.4309e-05, -1.1992e-04,  ...,  3.8062e-05,
         -5.4063e-05,  1.2561e-05],
        [-2.1916e-05, -1.5304e-05,  6.7161e-06,  ..., -1.9026e-05,
         -5.8042e-06, -1.2150e-05],
        [ 6.5931e-05,  4.3244e-05, -2.0401e-05,  ...,  5.5618e-05,
          1.8815e-05,  3.7181e-05],
        [-8.7674e-06, -5.7326e-06,  2.3814e-06,  ..., -4.8795e-06,
         -1.9768e-06, -3.5121e-06],
        [-4.8881e-05, -3.4094e-05,  1.4815e-05,  ..., -4.2382e-05,
         -1.2835e-05, -2.7183e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7947e-08, 1.9667e-08, 2.9693e-08,  ..., 2.1860e-08, 5.1464e-08,
         1.1120e-08],
        [4.6375e-11, 2.7057e-11, 2.3075e-12,  ..., 3.2395e-11, 1.7065e-12,
         8.1668e-12],
        [1.6823e-09, 8.3489e-10, 1.3326e-10,  ..., 1.2516e-09, 1.0564e-10,
         4.7635e-10],
        [4.1088e-10, 3.3089e-10, 3.4033e-11,  ..., 3.4717e-10, 3.2886e-11,
         1.6001e-10],
        [1.8883e-10, 1.0352e-10, 1.1540e-11,  ..., 1.3401e-10, 5.9664e-12,
         3.5273e-11]], device='cuda:0')
optimizer state dict: 64.0
lr: [1.7687041437173095e-05, 1.7687041437173095e-05]
scheduler_last_epoch: 64


Running epoch 0, step 512, batch 512
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3627e-05,  3.6586e-05,  3.1252e-05,  ..., -8.8846e-06,
         -4.3078e-05, -1.8505e-05],
        [-2.7418e-06, -1.6764e-06,  9.9093e-07,  ..., -2.3693e-06,
         -8.8289e-07, -1.7434e-06],
        [-3.5018e-06, -2.1607e-06,  1.2666e-06,  ..., -3.0398e-06,
         -1.1176e-06, -2.2352e-06],
        [-5.1260e-06, -3.1441e-06,  1.8552e-06,  ..., -4.4405e-06,
         -1.6242e-06, -3.2634e-06],
        [-6.2287e-06, -3.8147e-06,  2.2501e-06,  ..., -5.3942e-06,
         -1.9819e-06, -3.9637e-06]], device='cuda:0')
Loss: 1.1123696565628052


Running epoch 0, step 513, batch 513
Sampled inputs[:2]: tensor([[   0, 2355, 2728,  ...,  554, 9025,  368],
        [   0, 4929, 4214,  ..., 1172,  591, 4422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2244e-05,  5.1264e-05, -1.0350e-05,  ...,  3.0311e-05,
         -6.7119e-05,  6.1471e-06],
        [-5.4985e-06, -3.3602e-06,  1.9744e-06,  ..., -4.7088e-06,
         -1.7509e-06, -3.4720e-06],
        [-7.0333e-06, -4.3213e-06,  2.5257e-06,  ..., -6.0499e-06,
         -2.2203e-06, -4.4554e-06],
        [-1.0252e-05, -6.2883e-06,  3.6880e-06,  ..., -8.8215e-06,
         -3.2187e-06, -6.4820e-06],
        [-1.2457e-05, -7.6294e-06,  4.4703e-06,  ..., -1.0699e-05,
         -3.9339e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.1150873899459839


Running epoch 0, step 514, batch 514
Sampled inputs[:2]: tensor([[    0,  1644,  1742,  ...,   287,  1704,  2044],
        [    0, 17900,   554,  ...,   266,  7912,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4396e-04, -8.3322e-06, -8.0750e-06,  ...,  5.0067e-05,
         -1.0090e-04,  1.8576e-05],
        [-8.1956e-06, -5.0217e-06,  2.9504e-06,  ..., -6.9886e-06,
         -2.5705e-06, -5.1484e-06],
        [-1.0580e-05, -6.5118e-06,  3.8072e-06,  ..., -9.0599e-06,
         -3.2932e-06, -6.6608e-06],
        [-1.5438e-05, -9.4920e-06,  5.5656e-06,  ..., -1.3232e-05,
         -4.7758e-06, -9.7156e-06],
        [-1.8716e-05, -1.1474e-05,  6.7353e-06,  ..., -1.6004e-05,
         -5.8264e-06, -1.1772e-05]], device='cuda:0')
Loss: 1.09712815284729


Running epoch 0, step 515, batch 515
Sampled inputs[:2]: tensor([[    0,  7333,   342,  ...,    13,  1818,  6183],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1332e-04, -4.6750e-05, -2.2368e-05,  ...,  4.2715e-05,
         -8.7825e-05,  1.8630e-05],
        [-1.0908e-05, -6.7055e-06,  3.9339e-06,  ..., -9.3132e-06,
         -3.3826e-06, -6.8173e-06],
        [-1.4067e-05, -8.6725e-06,  5.0664e-06,  ..., -1.2040e-05,
         -4.3213e-06, -8.8066e-06],
        [-2.0534e-05, -1.2651e-05,  7.4133e-06,  ..., -1.7583e-05,
         -6.2734e-06, -1.2845e-05],
        [-2.4855e-05, -1.5289e-05,  8.9556e-06,  ..., -2.1249e-05,
         -7.6443e-06, -1.5542e-05]], device='cuda:0')
Loss: 1.099205732345581


Running epoch 0, step 516, batch 516
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,   508, 12163,  ...,  4920,   344, 11003]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1419e-04, -8.6852e-05, -4.0193e-05,  ...,  5.0699e-05,
         -1.0884e-04,  3.1782e-06],
        [-1.3649e-05, -8.3894e-06,  4.9248e-06,  ..., -1.1668e-05,
         -4.2543e-06, -8.5384e-06],
        [-1.7568e-05, -1.0818e-05,  6.3330e-06,  ..., -1.5050e-05,
         -5.4240e-06, -1.0997e-05],
        [-2.5630e-05, -1.5780e-05,  9.2685e-06,  ..., -2.1964e-05,
         -7.8753e-06, -1.6034e-05],
        [-3.1084e-05, -1.9103e-05,  1.1206e-05,  ..., -2.6613e-05,
         -9.6112e-06, -1.9446e-05]], device='cuda:0')
Loss: 1.1198588609695435


Running epoch 0, step 517, batch 517
Sampled inputs[:2]: tensor([[    0,  1176,    13,  ...,  1919,   221,   380],
        [    0, 18971,   278,  ...,  1934,  1916,  2612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6785e-05, -1.1712e-04, -4.8920e-05,  ...,  3.3662e-05,
         -1.0859e-04,  1.5878e-06],
        [-1.6406e-05, -1.0073e-05,  5.9083e-06,  ..., -1.4007e-05,
         -5.0999e-06, -1.0267e-05],
        [ 7.3699e-05,  4.7229e-05, -1.1668e-05,  ...,  6.8387e-05,
          1.9504e-05,  4.7649e-05],
        [-3.0816e-05, -1.8954e-05,  1.1124e-05,  ..., -2.6345e-05,
         -9.4473e-06, -1.9282e-05],
        [-3.7402e-05, -2.2948e-05,  1.3456e-05,  ..., -3.1948e-05,
         -1.1533e-05, -2.3380e-05]], device='cuda:0')
Loss: 1.0868901014328003


Running epoch 0, step 518, batch 518
Sampled inputs[:2]: tensor([[    0,  5129,  1245,  ...,   292, 24298,    13],
        [    0,    14,   475,  ...,  2117,  2792, 12848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0106e-05, -7.8461e-05, -6.1154e-05,  ...,  5.6430e-05,
         -1.7022e-04,  2.5037e-06],
        [-1.9148e-05, -1.1757e-05,  6.9067e-06,  ..., -1.6376e-05,
         -5.9940e-06, -1.2003e-05],
        [ 7.0198e-05,  4.5069e-05, -1.0394e-05,  ...,  6.5362e-05,
          1.8379e-05,  4.5444e-05],
        [-3.5912e-05, -2.2084e-05,  1.2979e-05,  ..., -3.0726e-05,
         -1.1072e-05, -2.2486e-05],
        [-4.3601e-05, -2.6748e-05,  1.5706e-05,  ..., -3.7283e-05,
         -1.3530e-05, -2.7284e-05]], device='cuda:0')
Loss: 1.112960934638977


Running epoch 0, step 519, batch 519
Sampled inputs[:2]: tensor([[    0, 21748,   792,  ...,   408,   266, 31879],
        [    0,    21,   292,  ...,    13,  1861,  4254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5193e-05,  1.0132e-04, -7.6401e-05,  ...,  6.5079e-05,
         -1.5141e-04,  7.2784e-05],
        [-2.1935e-05, -1.3456e-05,  7.8976e-06,  ..., -1.8746e-05,
         -6.9998e-06, -1.3836e-05],
        [ 1.4630e-04,  1.2161e-04, -5.2869e-05,  ...,  1.3331e-04,
          6.9083e-05,  1.0991e-04],
        [-4.0948e-05, -2.5153e-05,  1.4767e-05,  ..., -3.5018e-05,
         -1.2867e-05, -2.5809e-05],
        [-4.9859e-05, -3.0562e-05,  1.7926e-05,  ..., -4.2617e-05,
         -1.5765e-05, -3.1397e-05]], device='cuda:0')
Loss: 1.1160683631896973
Graident accumulation at epoch 0, step 519, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0198],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0095, -0.0024, -0.0341],
        [ 0.0336, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0274,  ...,  0.0286, -0.0150, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4735e-05,  2.3010e-05, -1.1557e-04,  ...,  4.0764e-05,
         -6.3798e-05,  1.8584e-05],
        [-2.1918e-05, -1.5119e-05,  6.8342e-06,  ..., -1.8998e-05,
         -5.9238e-06, -1.2318e-05],
        [ 7.3968e-05,  5.1081e-05, -2.3648e-05,  ...,  6.3387e-05,
          2.3842e-05,  4.4454e-05],
        [-1.1986e-05, -7.6747e-06,  3.6200e-06,  ..., -7.8933e-06,
         -3.0659e-06, -5.7418e-06],
        [-4.8979e-05, -3.3741e-05,  1.5126e-05,  ..., -4.2405e-05,
         -1.3128e-05, -2.7604e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7890e-08, 1.9658e-08, 2.9669e-08,  ..., 2.1843e-08, 5.1435e-08,
         1.1114e-08],
        [4.6810e-11, 2.7211e-11, 2.3676e-12,  ..., 3.2714e-11, 1.7538e-12,
         8.3500e-12],
        [1.7020e-09, 8.4885e-10, 1.3592e-10,  ..., 1.2681e-09, 1.1031e-10,
         4.8795e-10],
        [4.1215e-10, 3.3119e-10, 3.4217e-11,  ..., 3.4805e-10, 3.3019e-11,
         1.6052e-10],
        [1.9113e-10, 1.0435e-10, 1.1849e-11,  ..., 1.3569e-10, 6.2089e-12,
         3.6224e-11]], device='cuda:0')
optimizer state dict: 65.0
lr: [1.7607390381871007e-05, 1.7607390381871007e-05]
scheduler_last_epoch: 65


Running epoch 0, step 520, batch 520
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,   12, 5576,   12],
        [   0,   12,  342,  ..., 3458,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6775e-05,  5.8274e-05, -1.9925e-05,  ...,  3.5733e-05,
         -4.6890e-05,  7.9008e-06],
        [-2.6673e-06, -1.6242e-06,  9.6858e-07,  ..., -2.3693e-06,
         -9.3877e-07, -1.7434e-06],
        [-3.4571e-06, -2.1160e-06,  1.2591e-06,  ..., -3.0696e-06,
         -1.1995e-06, -2.2501e-06],
        [-5.0068e-06, -3.0547e-06,  1.8254e-06,  ..., -4.4405e-06,
         -1.7285e-06, -3.2634e-06],
        [-6.0499e-06, -3.6955e-06,  2.2203e-06,  ..., -5.3942e-06,
         -2.1160e-06, -3.9637e-06]], device='cuda:0')
Loss: 1.1173877716064453


Running epoch 0, step 521, batch 521
Sampled inputs[:2]: tensor([[    0,   733,   560,  ...,  1172, 22808,   271],
        [    0,    14, 38914,  ...,   266,  5690,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1942e-05,  1.5125e-04, -5.3823e-06,  ...,  7.3265e-05,
         -7.2936e-05,  3.6352e-05],
        [-5.4091e-06, -3.2410e-06,  1.9819e-06,  ..., -4.6939e-06,
         -1.9670e-06, -3.5688e-06],
        [-7.0333e-06, -4.2319e-06,  2.5779e-06,  ..., -6.1095e-06,
         -2.5257e-06, -4.6343e-06],
        [-1.0103e-05, -6.0499e-06,  3.7104e-06,  ..., -8.7917e-06,
         -3.6061e-06, -6.6608e-06],
        [-1.2249e-05, -7.3463e-06,  4.5002e-06,  ..., -1.0669e-05,
         -4.4107e-06, -8.0764e-06]], device='cuda:0')
Loss: 1.0940951108932495


Running epoch 0, step 522, batch 522
Sampled inputs[:2]: tensor([[   0,  554, 1034,  ..., 3313,  365,  654],
        [   0, 1575, 4384,  ...,  328,  722, 6124]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8174e-05,  1.5946e-04,  1.2807e-05,  ...,  9.4624e-05,
         -6.4461e-05,  3.7314e-05],
        [-8.0764e-06, -4.8503e-06,  2.9653e-06,  ..., -7.0184e-06,
         -2.8871e-06, -5.3272e-06],
        [-1.0550e-05, -6.3628e-06,  3.8818e-06,  ..., -9.1642e-06,
         -3.7253e-06, -6.9439e-06],
        [-1.5140e-05, -9.0897e-06,  5.5805e-06,  ..., -1.3173e-05,
         -5.3197e-06, -9.9838e-06],
        [-1.8358e-05, -1.1027e-05,  6.7502e-06,  ..., -1.5974e-05,
         -6.4969e-06, -1.2070e-05]], device='cuda:0')
Loss: 1.103615164756775


Running epoch 0, step 523, batch 523
Sampled inputs[:2]: tensor([[    0,   389, 18984,  ...,   287,   768,  1070],
        [    0,    13,  2497,  ...,   943,   259,  2646]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5562e-05,  2.2492e-04, -5.4439e-05,  ...,  1.2254e-04,
         -6.3571e-05,  9.3803e-05],
        [-1.0788e-05, -6.4969e-06,  3.9563e-06,  ..., -9.3579e-06,
         -3.8557e-06, -7.1079e-06],
        [-1.4111e-05, -8.5235e-06,  5.1856e-06,  ..., -1.2234e-05,
         -4.9844e-06, -9.2685e-06],
        [-2.0176e-05, -1.2159e-05,  7.4208e-06,  ..., -1.7524e-05,
         -7.0930e-06, -1.3277e-05],
        [-2.4557e-05, -1.4797e-05,  9.0152e-06,  ..., -2.1309e-05,
         -8.6874e-06, -1.6123e-05]], device='cuda:0')
Loss: 1.1126818656921387


Running epoch 0, step 524, batch 524
Sampled inputs[:2]: tensor([[    0,  9577,  2789,  ...,  1042,  9086,   623],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5392e-06,  2.0371e-04, -6.2755e-05,  ...,  1.5955e-04,
         -1.4314e-04,  1.1863e-04],
        [-1.3471e-05, -8.0988e-06,  4.9323e-06,  ..., -1.1653e-05,
         -4.7050e-06, -8.8066e-06],
        [ 4.1057e-04,  3.1121e-04, -9.9990e-05,  ...,  3.6637e-04,
          1.5850e-04,  3.0780e-04],
        [-2.5302e-05, -1.5214e-05,  9.2909e-06,  ..., -2.1935e-05,
         -8.6874e-06, -1.6510e-05],
        [-3.0696e-05, -1.8448e-05,  1.1250e-05,  ..., -2.6554e-05,
         -1.0610e-05, -1.9997e-05]], device='cuda:0')
Loss: 1.1148293018341064


Running epoch 0, step 525, batch 525
Sampled inputs[:2]: tensor([[   0, 5150, 1030,  ...,   14,  475, 1763],
        [   0,  278, 2097,  ..., 1754,  287,  631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0593e-05,  2.4993e-04, -1.3963e-04,  ...,  1.9146e-04,
         -1.4900e-04,  1.0577e-04],
        [-1.6138e-05, -9.6858e-06,  5.9232e-06,  ..., -1.3962e-05,
         -5.6028e-06, -1.0535e-05],
        [ 4.0707e-04,  3.0912e-04, -9.8686e-05,  ...,  3.6333e-04,
          1.5734e-04,  3.0553e-04],
        [-3.0428e-05, -1.8269e-05,  1.1191e-05,  ..., -2.6375e-05,
         -1.0371e-05, -1.9819e-05],
        [-3.6776e-05, -2.2084e-05,  1.3515e-05,  ..., -3.1829e-05,
         -1.2636e-05, -2.3931e-05]], device='cuda:0')
Loss: 1.1048659086227417


Running epoch 0, step 526, batch 526
Sampled inputs[:2]: tensor([[   0,    9,  292,  ...,  944,  278, 1758],
        [   0,  391, 9095,  ...,  417,  199, 2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0593e-05,  2.4278e-04, -1.5862e-04,  ...,  2.5634e-04,
         -1.9631e-04,  9.6082e-05],
        [-1.8775e-05, -1.1317e-05,  6.9141e-06,  ..., -1.6272e-05,
         -6.4299e-06, -1.2234e-05],
        [ 4.0358e-04,  3.0698e-04, -9.7375e-05,  ...,  3.6028e-04,
          1.5627e-04,  3.0330e-04],
        [ 5.5471e-05,  2.8490e-05, -3.0972e-05,  ...,  5.0514e-05,
          3.3636e-05,  3.2484e-05],
        [-4.2856e-05, -2.5839e-05,  1.5810e-05,  ..., -3.7163e-05,
         -1.4521e-05, -2.7835e-05]], device='cuda:0')
Loss: 1.1153746843338013


Running epoch 0, step 527, batch 527
Sampled inputs[:2]: tensor([[    0,  2162,    73,  ...,   278,   266,  1059],
        [    0,   269,    12,  ..., 45645,    14,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3074e-04,  3.2780e-04, -1.9064e-04,  ...,  2.4880e-04,
         -2.1588e-04,  1.0879e-04],
        [-2.1473e-05, -1.2942e-05,  7.8902e-06,  ..., -1.8612e-05,
         -7.3835e-06, -1.3962e-05],
        [ 4.0007e-04,  3.0486e-04, -9.6108e-05,  ...,  3.5722e-04,
          1.5505e-04,  3.0105e-04],
        [ 5.0464e-05,  2.5465e-05, -2.9161e-05,  ...,  4.6163e-05,
          3.1900e-05,  2.9280e-05],
        [-4.8995e-05, -2.9534e-05,  1.8030e-05,  ..., -4.2498e-05,
         -1.6667e-05, -3.1769e-05]], device='cuda:0')
Loss: 1.1053657531738281
Graident accumulation at epoch 0, step 527, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0055, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0292, -0.0077,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0150, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.6336e-05,  5.3490e-05, -1.2308e-04,  ...,  6.1567e-05,
         -7.9006e-05,  2.7604e-05],
        [-2.1873e-05, -1.4902e-05,  6.9398e-06,  ..., -1.8959e-05,
         -6.0697e-06, -1.2483e-05],
        [ 1.0658e-04,  7.6459e-05, -3.0894e-05,  ...,  9.2771e-05,
          3.6962e-05,  7.0114e-05],
        [-5.7405e-06, -4.3607e-06,  3.4186e-07,  ..., -2.4877e-06,
          4.3072e-07, -2.2396e-06],
        [-4.8981e-05, -3.3320e-05,  1.5416e-05,  ..., -4.2415e-05,
         -1.3482e-05, -2.8021e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7849e-08, 1.9746e-08, 2.9676e-08,  ..., 2.1883e-08, 5.1430e-08,
         1.1115e-08],
        [4.7224e-11, 2.7351e-11, 2.4275e-12,  ..., 3.3028e-11, 1.8065e-12,
         8.5366e-12],
        [1.8604e-09, 9.4094e-10, 1.4502e-10,  ..., 1.3944e-09, 1.3424e-10,
         5.7809e-10],
        [4.1428e-10, 3.3151e-10, 3.5033e-11,  ..., 3.4983e-10, 3.4003e-11,
         1.6122e-10],
        [1.9334e-10, 1.0512e-10, 1.2163e-11,  ..., 1.3736e-10, 6.4805e-12,
         3.7197e-11]], device='cuda:0')
optimizer state dict: 66.0
lr: [1.7526576850912724e-05, 1.7526576850912724e-05]
scheduler_last_epoch: 66


Running epoch 0, step 528, batch 528
Sampled inputs[:2]: tensor([[   0,  689, 3953,  ...,  461,  943,  352],
        [   0, 1726, 3775,  ...,  300,  266, 1686]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5063e-05,  8.3847e-06,  5.4220e-05,  ..., -1.2444e-05,
         -4.9207e-05, -4.6880e-07],
        [-2.5779e-06, -1.6093e-06,  9.9093e-07,  ..., -2.2948e-06,
         -9.2015e-07, -1.6838e-06],
        [-3.5018e-06, -2.1905e-06,  1.3411e-06,  ..., -3.1292e-06,
         -1.2368e-06, -2.2799e-06],
        [-4.9770e-06, -3.1143e-06,  1.9073e-06,  ..., -4.4405e-06,
         -1.7434e-06, -3.2485e-06],
        [-5.9307e-06, -3.7104e-06,  2.2799e-06,  ..., -5.3048e-06,
         -2.1011e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.1220695972442627


Running epoch 0, step 529, batch 529
Sampled inputs[:2]: tensor([[    0,  1136,   944,  ...,   401, 13771,    12],
        [    0, 47831,   266,  ...,    66,    17, 20005]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1903e-05,  1.3071e-05,  2.1646e-05,  ...,  4.0716e-05,
         -6.8045e-05,  7.5302e-06],
        [-5.1707e-06, -3.1516e-06,  1.9446e-06,  ..., -4.5747e-06,
         -1.8664e-06, -3.4124e-06],
        [-7.0333e-06, -4.2915e-06,  2.6375e-06,  ..., -6.2287e-06,
         -2.5034e-06, -4.6194e-06],
        [-9.9838e-06, -6.0946e-06,  3.7551e-06,  ..., -8.8513e-06,
         -3.5390e-06, -6.5714e-06],
        [-1.1891e-05, -7.2569e-06,  4.4703e-06,  ..., -1.0550e-05,
         -4.2468e-06, -7.8380e-06]], device='cuda:0')
Loss: 1.1082669496536255


Running epoch 0, step 530, batch 530
Sampled inputs[:2]: tensor([[   0, 6328,   12,  ...,  417,  199, 1726],
        [   0,  494,  221,  ...,  437,  266, 2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1785e-05,  2.7721e-05,  2.1646e-05,  ...,  5.5825e-05,
         -1.3575e-04, -1.0180e-05],
        [-7.7486e-06, -4.7162e-06,  2.9430e-06,  ..., -6.8694e-06,
         -2.8424e-06, -5.1856e-06],
        [-1.0505e-05, -6.4075e-06,  3.9786e-06,  ..., -9.3132e-06,
         -3.7998e-06, -7.0035e-06],
        [-1.4961e-05, -9.1046e-06,  5.6773e-06,  ..., -1.3262e-05,
         -5.3868e-06, -9.9689e-06],
        [-1.7762e-05, -1.0818e-05,  6.7353e-06,  ..., -1.5765e-05,
         -6.4522e-06, -1.1861e-05]], device='cuda:0')
Loss: 1.1215410232543945


Running epoch 0, step 531, batch 531
Sampled inputs[:2]: tensor([[    0,  1480,   518,  ...,   445,    28,   445],
        [    0,   271,   266,  ...,  2805,   607, 10848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4147e-06, -2.1147e-05, -1.2887e-05,  ...,  8.2996e-05,
         -1.8721e-04, -4.9111e-06],
        [-1.0282e-05, -6.2659e-06,  3.8929e-06,  ..., -9.1195e-06,
         -3.6880e-06, -6.8620e-06],
        [ 7.5461e-05,  4.4802e-05, -3.2502e-05,  ...,  8.3712e-05,
          3.8441e-05,  4.4088e-05],
        [-1.9938e-05, -1.2159e-05,  7.5474e-06,  ..., -1.7703e-05,
         -7.0110e-06, -1.3262e-05],
        [-2.3663e-05, -1.4424e-05,  8.9407e-06,  ..., -2.1011e-05,
         -8.4043e-06, -1.5765e-05]], device='cuda:0')
Loss: 1.0908509492874146


Running epoch 0, step 532, batch 532
Sampled inputs[:2]: tensor([[    0,   969,   258,  ...,   726,  5303,  6512],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1616e-05,  1.8984e-06, -1.7405e-05,  ...,  4.0703e-05,
         -1.4725e-04, -4.9111e-06],
        [-1.2890e-05, -7.8455e-06,  4.8541e-06,  ..., -1.1399e-05,
         -4.6343e-06, -8.5682e-06],
        [ 7.1945e-05,  4.2671e-05, -3.1205e-05,  ...,  8.0627e-05,
          3.7182e-05,  4.1794e-05],
        [-2.4915e-05, -1.5169e-05,  9.3877e-06,  ..., -2.2084e-05,
         -8.7842e-06, -1.6510e-05],
        [-2.9653e-05, -1.8045e-05,  1.1146e-05,  ..., -2.6256e-05,
         -1.0550e-05, -1.9670e-05]], device='cuda:0')
Loss: 1.1346372365951538


Running epoch 0, step 533, batch 533
Sampled inputs[:2]: tensor([[    0,   278,   264,  ..., 21836,   344,   259],
        [    0,  2523, 10780,  ...,  1041,    26, 13745]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2901e-05,  5.2365e-06, -3.7310e-05,  ...,  7.6740e-05,
         -1.8313e-04,  2.1422e-05],
        [-1.5453e-05, -9.4026e-06,  5.8152e-06,  ..., -1.3694e-05,
         -5.4911e-06, -1.0274e-05],
        [ 6.8458e-05,  4.0555e-05, -2.9902e-05,  ...,  7.7513e-05,
          3.6034e-05,  3.9484e-05],
        [-2.9922e-05, -1.8209e-05,  1.1273e-05,  ..., -2.6584e-05,
         -1.0423e-05, -1.9833e-05],
        [-3.5584e-05, -2.1636e-05,  1.3366e-05,  ..., -3.1561e-05,
         -1.2502e-05, -2.3603e-05]], device='cuda:0')
Loss: 1.0947296619415283


Running epoch 0, step 534, batch 534
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   908,  1086,    12],
        [    0,   475,   668,  ..., 17680,   368,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7828e-05,  1.3078e-05, -3.1583e-05,  ...,  1.0825e-04,
         -2.1985e-04,  4.5184e-05],
        [-1.8045e-05, -1.0967e-05,  6.7987e-06,  ..., -1.5959e-05,
         -6.4336e-06, -1.1958e-05],
        [ 6.4881e-05,  3.8395e-05, -2.8546e-05,  ...,  7.4384e-05,
          3.4760e-05,  3.7174e-05],
        [-3.4928e-05, -2.1249e-05,  1.3173e-05,  ..., -3.0994e-05,
         -1.2197e-05, -2.3082e-05],
        [-4.1574e-05, -2.5257e-05,  1.5631e-05,  ..., -3.6806e-05,
         -1.4648e-05, -2.7478e-05]], device='cuda:0')
Loss: 1.102271318435669


Running epoch 0, step 535, batch 535
Sampled inputs[:2]: tensor([[    0,  3231,   271,  ...,  9279,  8231, 28871],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8392e-05,  5.5361e-05, -1.3246e-04,  ...,  1.4471e-04,
         -2.5392e-04,  3.6925e-05],
        [-2.0564e-05, -1.2524e-05,  7.7598e-06,  ..., -1.8239e-05,
         -7.3947e-06, -1.3672e-05],
        [ 6.1454e-05,  3.6279e-05, -2.7234e-05,  ...,  7.1284e-05,
          3.3479e-05,  3.4850e-05],
        [-3.9756e-05, -2.4229e-05,  1.5020e-05,  ..., -3.5375e-05,
         -1.4000e-05, -2.6360e-05],
        [-4.7326e-05, -2.8804e-05,  1.7837e-05,  ..., -4.2021e-05,
         -1.6809e-05, -3.1382e-05]], device='cuda:0')
Loss: 1.1097744703292847
Graident accumulation at epoch 0, step 535, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0165],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0150, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.6542e-05,  5.3677e-05, -1.2402e-04,  ...,  6.9882e-05,
         -9.6498e-05,  2.8537e-05],
        [-2.1742e-05, -1.4664e-05,  7.0218e-06,  ..., -1.8887e-05,
         -6.2022e-06, -1.2602e-05],
        [ 1.0207e-04,  7.2441e-05, -3.0528e-05,  ...,  9.0622e-05,
          3.6614e-05,  6.6587e-05],
        [-9.1421e-06, -6.3476e-06,  1.8097e-06,  ..., -5.7765e-06,
         -1.0123e-06, -4.6516e-06],
        [-4.8815e-05, -3.2869e-05,  1.5658e-05,  ..., -4.2375e-05,
         -1.3815e-05, -2.8357e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7792e-08, 1.9729e-08, 2.9663e-08,  ..., 2.1882e-08, 5.1443e-08,
         1.1105e-08],
        [4.7600e-11, 2.7481e-11, 2.4852e-12,  ..., 3.3328e-11, 1.8594e-12,
         8.7150e-12],
        [1.8623e-09, 9.4131e-10, 1.4562e-10,  ..., 1.3981e-09, 1.3523e-10,
         5.7873e-10],
        [4.1545e-10, 3.3176e-10, 3.5223e-11,  ..., 3.5073e-10, 3.4165e-11,
         1.6175e-10],
        [1.9538e-10, 1.0585e-10, 1.2469e-11,  ..., 1.3899e-10, 6.7566e-12,
         3.8144e-11]], device='cuda:0')
optimizer state dict: 67.0
lr: [1.7444613193311206e-05, 1.7444613193311206e-05]
scheduler_last_epoch: 67


Running epoch 0, step 536, batch 536
Sampled inputs[:2]: tensor([[   0, 1555,   12,  ...,  809,  287,  259],
        [   0, 6275,   12,  ..., 2027, 2887,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5962e-05, -1.5337e-05, -4.3739e-07,  ...,  1.0764e-05,
         -9.0216e-06,  5.5971e-06],
        [-2.5332e-06, -1.5646e-06,  9.7603e-07,  ..., -2.2352e-06,
         -9.2015e-07, -1.6764e-06],
        [-3.5465e-06, -2.2054e-06,  1.3635e-06,  ..., -3.1441e-06,
         -1.2740e-06, -2.3544e-06],
        [-4.9174e-06, -3.0398e-06,  1.8850e-06,  ..., -4.3511e-06,
         -1.7509e-06, -3.2485e-06],
        [-5.8711e-06, -3.6359e-06,  2.2501e-06,  ..., -5.1856e-06,
         -2.1160e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.1250123977661133


Running epoch 0, step 537, batch 537
Sampled inputs[:2]: tensor([[    0,   494,   360,  ...,   391, 24104, 35211],
        [    0,  2827,  5744,  ...,   365,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6206e-05,  3.6364e-05, -5.0948e-05,  ...,  1.1693e-05,
         -6.9422e-05,  3.4080e-05],
        [-5.0813e-06, -3.0994e-06,  1.9446e-06,  ..., -4.5002e-06,
         -1.9483e-06, -3.4347e-06],
        [-7.0781e-06, -4.3362e-06,  2.7120e-06,  ..., -6.2883e-06,
         -2.6748e-06, -4.7833e-06],
        [-9.8348e-06, -6.0052e-06,  3.7625e-06,  ..., -8.7321e-06,
         -3.6880e-06, -6.6310e-06],
        [-1.1683e-05, -7.1377e-06,  4.4703e-06,  ..., -1.0371e-05,
         -4.4256e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.090751051902771


Running epoch 0, step 538, batch 538
Sampled inputs[:2]: tensor([[   0,  767, 9289,  ...,  494,  287, 8957],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1808e-04,  7.6880e-05, -7.5784e-05,  ...,  1.3036e-05,
         -1.2579e-04, -2.9582e-05],
        [-7.6145e-06, -4.6417e-06,  2.9355e-06,  ..., -6.7502e-06,
         -2.8610e-06, -5.1185e-06],
        [-1.0610e-05, -6.4969e-06,  4.0904e-06,  ..., -9.4175e-06,
         -3.9265e-06, -7.1228e-06],
        [-1.4812e-05, -9.0301e-06,  5.6997e-06,  ..., -1.3143e-05,
         -5.4315e-06, -9.9093e-06],
        [-1.7524e-05, -1.0699e-05,  6.7502e-06,  ..., -1.5557e-05,
         -6.4969e-06, -1.1742e-05]], device='cuda:0')
Loss: 1.0981496572494507


Running epoch 0, step 539, batch 539
Sampled inputs[:2]: tensor([[    0,  3889,  4039,  ...,   616, 22910,   259],
        [    0,    14,  3948,  ...,   571, 10097,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1590e-04,  6.1304e-05, -9.0577e-05,  ...,  3.7033e-05,
         -1.1951e-04, -5.3326e-05],
        [-1.0118e-05, -6.1467e-06,  3.8780e-06,  ..., -9.0152e-06,
         -3.8147e-06, -6.7800e-06],
        [-1.4141e-05, -8.6129e-06,  5.4166e-06,  ..., -1.2621e-05,
         -5.2527e-06, -9.4622e-06],
        [-1.9699e-05, -1.1966e-05,  7.5400e-06,  ..., -1.7583e-05,
         -7.2494e-06, -1.3143e-05],
        [-2.3395e-05, -1.4231e-05,  8.9556e-06,  ..., -2.0862e-05,
         -8.7023e-06, -1.5616e-05]], device='cuda:0')
Loss: 1.1076440811157227


Running epoch 0, step 540, batch 540
Sampled inputs[:2]: tensor([[   0, 7070,   86,  ...,  298, 4930,  518],
        [   0, 5319,   14,  ..., 2372, 2356, 4093]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5997e-05,  2.9743e-05, -9.3549e-05,  ...,  6.5418e-05,
         -1.2638e-04, -4.1275e-05],
        [-1.2636e-05, -7.6815e-06,  4.8317e-06,  ..., -1.1250e-05,
         -4.7237e-06, -8.4564e-06],
        [-1.7673e-05, -1.0759e-05,  6.7502e-06,  ..., -1.5765e-05,
         -6.4969e-06, -1.1802e-05],
        [-2.4647e-05, -1.4976e-05,  9.4101e-06,  ..., -2.1994e-05,
         -8.9854e-06, -1.6421e-05],
        [-2.9176e-05, -1.7762e-05,  1.1146e-05,  ..., -2.6017e-05,
         -1.0759e-05, -1.9461e-05]], device='cuda:0')
Loss: 1.1104097366333008


Running epoch 0, step 541, batch 541
Sampled inputs[:2]: tensor([[   0,   27,  417,  ...,   18,  365,  806],
        [   0,  493,  221,  ...,  259,  726, 2786]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3822e-05,  1.4659e-05, -1.1511e-04,  ...,  1.1176e-04,
         -1.6315e-04, -5.3621e-05],
        [-1.5140e-05, -9.2089e-06,  5.8077e-06,  ..., -1.3500e-05,
         -5.6550e-06, -1.0163e-05],
        [-2.1219e-05, -1.2919e-05,  8.1286e-06,  ..., -1.8939e-05,
         -7.7859e-06, -1.4201e-05],
        [-2.9594e-05, -1.7986e-05,  1.1332e-05,  ..., -2.6405e-05,
         -1.0774e-05, -1.9774e-05],
        [-3.5048e-05, -2.1338e-05,  1.3426e-05,  ..., -3.1263e-05,
         -1.2904e-05, -2.3454e-05]], device='cuda:0')
Loss: 1.1123416423797607


Running epoch 0, step 542, batch 542
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,   271, 16217,  ...,  6352,  4546,  2558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1738e-05,  2.5686e-05, -1.8062e-04,  ...,  6.5001e-05,
         -1.3722e-04, -7.1713e-05],
        [-1.7688e-05, -1.0736e-05,  6.7614e-06,  ..., -1.5751e-05,
         -6.6608e-06, -1.1884e-05],
        [-2.4706e-05, -1.5005e-05,  9.4399e-06,  ..., -2.2024e-05,
         -9.1344e-06, -1.6555e-05],
        [-3.4481e-05, -2.0921e-05,  1.3165e-05,  ..., -3.0726e-05,
         -1.2659e-05, -2.3067e-05],
        [-4.0829e-05, -2.4810e-05,  1.5602e-05,  ..., -3.6389e-05,
         -1.5154e-05, -2.7359e-05]], device='cuda:0')
Loss: 1.1266382932662964


Running epoch 0, step 543, batch 543
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   292,   334,   674],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0410e-05,  1.1088e-04, -2.3346e-04,  ...,  5.1951e-05,
         -1.6688e-04, -2.9850e-05],
        [-2.0206e-05, -1.2256e-05,  7.7002e-06,  ..., -1.8001e-05,
         -7.6033e-06, -1.3605e-05],
        [-2.8208e-05, -1.7121e-05,  1.0744e-05,  ..., -2.5138e-05,
         -1.0416e-05, -1.8939e-05],
        [-3.9399e-05, -2.3887e-05,  1.4991e-05,  ..., -3.5107e-05,
         -1.4454e-05, -2.6405e-05],
        [-4.6611e-05, -2.8297e-05,  1.7747e-05,  ..., -4.1544e-05,
         -1.7270e-05, -3.1292e-05]], device='cuda:0')
Loss: 1.1096522808074951
Graident accumulation at epoch 0, step 543, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0150, -0.0275,  ...,  0.0286, -0.0149, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0847e-05,  5.9397e-05, -1.3496e-04,  ...,  6.8089e-05,
         -1.0354e-04,  2.2698e-05],
        [-2.1589e-05, -1.4423e-05,  7.0896e-06,  ..., -1.8798e-05,
         -6.3424e-06, -1.2702e-05],
        [ 8.9038e-05,  6.3484e-05, -2.6401e-05,  ...,  7.9046e-05,
          3.1911e-05,  5.8035e-05],
        [-1.2168e-05, -8.1015e-06,  3.1278e-06,  ..., -8.7095e-06,
         -2.3565e-06, -6.8270e-06],
        [-4.8595e-05, -3.2412e-05,  1.5867e-05,  ..., -4.2292e-05,
         -1.4160e-05, -2.8650e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7735e-08, 1.9721e-08, 2.9688e-08,  ..., 2.1863e-08, 5.1420e-08,
         1.1095e-08],
        [4.7961e-11, 2.7603e-11, 2.5420e-12,  ..., 3.3618e-11, 1.9154e-12,
         8.8914e-12],
        [1.8612e-09, 9.4066e-10, 1.4559e-10,  ..., 1.3974e-09, 1.3520e-10,
         5.7851e-10],
        [4.1658e-10, 3.3200e-10, 3.5413e-11,  ..., 3.5161e-10, 3.4340e-11,
         1.6229e-10],
        [1.9736e-10, 1.0654e-10, 1.2771e-11,  ..., 1.4058e-10, 7.0481e-12,
         3.9085e-11]], device='cuda:0')
optimizer state dict: 68.0
lr: [1.73615119338288e-05, 1.73615119338288e-05]
scheduler_last_epoch: 68


Running epoch 0, step 544, batch 544
Sampled inputs[:2]: tensor([[   0, 2612,  271,  ...,  369, 9862,  287],
        [   0,  756,  401,  ..., 8385, 1004,  775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0349e-05,  1.6271e-05,  2.8672e-05,  ...,  2.7152e-06,
         -4.4079e-05, -3.8950e-05],
        [-2.3991e-06, -1.4976e-06,  9.0152e-07,  ..., -2.1458e-06,
         -9.0897e-07, -1.6391e-06],
        [-3.4124e-06, -2.1309e-06,  1.2815e-06,  ..., -3.0696e-06,
         -1.2591e-06, -2.3246e-06],
        [-4.7684e-06, -2.9951e-06,  1.8030e-06,  ..., -4.2915e-06,
         -1.7509e-06, -3.2634e-06],
        [-5.5134e-06, -3.4422e-06,  2.0713e-06,  ..., -4.9472e-06,
         -2.0415e-06, -3.7551e-06]], device='cuda:0')
Loss: 1.0853146314620972


Running epoch 0, step 545, batch 545
Sampled inputs[:2]: tensor([[    0,   271, 10474,  ...,   298,  2286,    29],
        [    0,  1041,    14,  ...,   360,   266, 14966]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3602e-05,  4.7038e-05,  2.7871e-05,  ...,  4.4508e-05,
         -7.2804e-05, -1.5656e-05],
        [-4.8131e-06, -2.9653e-06,  1.8403e-06,  ..., -4.3064e-06,
         -1.8850e-06, -3.2857e-06],
        [-6.8843e-06, -4.2468e-06,  2.6301e-06,  ..., -6.1691e-06,
         -2.6226e-06, -4.6790e-06],
        [-9.5963e-06, -5.9307e-06,  3.6806e-06,  ..., -8.6129e-06,
         -3.6359e-06, -6.5416e-06],
        [-1.1116e-05, -6.8396e-06,  4.2468e-06,  ..., -9.9540e-06,
         -4.2468e-06, -7.5549e-06]], device='cuda:0')
Loss: 1.106492280960083


Running epoch 0, step 546, batch 546
Sampled inputs[:2]: tensor([[   0,  298,  374,  ...,  298,  413,   28],
        [   0,  352,  927,  ..., 1521, 3513,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1226e-06,  1.8607e-04, -3.4008e-05,  ...,  6.8813e-05,
         -8.7191e-05,  6.7387e-05],
        [-7.2867e-06, -4.4629e-06,  2.7865e-06,  ..., -6.5267e-06,
         -2.9430e-06, -4.9844e-06],
        [-1.0312e-05, -6.3181e-06,  3.9414e-06,  ..., -9.2387e-06,
         -4.0531e-06, -7.0184e-06],
        [-1.4365e-05, -8.8364e-06,  5.5209e-06,  ..., -1.2904e-05,
         -5.6177e-06, -9.8199e-06],
        [-1.6600e-05, -1.0163e-05,  6.3479e-06,  ..., -1.4871e-05,
         -6.5416e-06, -1.1310e-05]], device='cuda:0')
Loss: 1.093249797821045


Running epoch 0, step 547, batch 547
Sampled inputs[:2]: tensor([[    0,  3699,  3058,  ...,   820,  5327,  8055],
        [    0,   266, 11692,  ...,   278, 14620, 12718]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9932e-05,  1.5355e-04,  1.3136e-06,  ...,  2.4598e-05,
         -3.1873e-05,  1.3982e-05],
        [-9.7156e-06, -5.9530e-06,  3.7141e-06,  ..., -8.7321e-06,
         -3.8892e-06, -6.6236e-06],
        [-1.3769e-05, -8.4341e-06,  5.2601e-06,  ..., -1.2368e-05,
         -5.3719e-06, -9.3430e-06],
        [-1.9163e-05, -1.1787e-05,  7.3537e-06,  ..., -1.7256e-05,
         -7.4431e-06, -1.3053e-05],
        [-2.2233e-05, -1.3620e-05,  8.4937e-06,  ..., -1.9968e-05,
         -8.7023e-06, -1.5110e-05]], device='cuda:0')
Loss: 1.1172629594802856


Running epoch 0, step 548, batch 548
Sampled inputs[:2]: tensor([[    0, 13856,   278,  ...,    14,    69,   462],
        [    0,   437,   266,  ...,  5512,   822,    89]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0653e-04,  1.6485e-04,  1.5757e-05,  ...,  1.9241e-05,
         -2.4149e-05,  4.0911e-05],
        [-1.2174e-05, -7.4357e-06,  4.6529e-06,  ..., -1.0937e-05,
         -4.8727e-06, -8.2776e-06],
        [-1.7241e-05, -1.0520e-05,  6.5863e-06,  ..., -1.5497e-05,
         -6.7353e-06, -1.1683e-05],
        [-2.3991e-05, -1.4693e-05,  9.2015e-06,  ..., -2.1607e-05,
         -9.3207e-06, -1.6302e-05],
        [-2.7865e-05, -1.7002e-05,  1.0639e-05,  ..., -2.5034e-05,
         -1.0923e-05, -1.8880e-05]], device='cuda:0')
Loss: 1.0977948904037476


Running epoch 0, step 549, batch 549
Sampled inputs[:2]: tensor([[    0,  1197, 10640,  ...,  2405,   437,  5880],
        [    0,   694,   266,  ...,  3007,   300,  5726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2115e-04,  1.3165e-04,  1.2986e-05,  ...,  5.3082e-05,
         -2.0687e-05,  4.7526e-05],
        [-1.4588e-05, -8.9034e-06,  5.5768e-06,  ..., -1.3098e-05,
         -5.7667e-06, -9.8720e-06],
        [-2.0713e-05, -1.2636e-05,  7.9125e-06,  ..., -1.8612e-05,
         -7.9945e-06, -1.3962e-05],
        [-2.8878e-05, -1.7673e-05,  1.1072e-05,  ..., -2.5988e-05,
         -1.1079e-05, -1.9521e-05],
        [-3.3468e-05, -2.0415e-05,  1.2785e-05,  ..., -3.0071e-05,
         -1.2964e-05, -2.2560e-05]], device='cuda:0')
Loss: 1.10731041431427


Running epoch 0, step 550, batch 550
Sampled inputs[:2]: tensor([[    0,   259,  2416,  ..., 14474,    12,   259],
        [    0,   266,  4411,  ...,   368,  6388,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4599e-04,  1.5178e-05,  2.6940e-05,  ...,  9.2331e-05,
         -9.3392e-05, -6.9848e-05],
        [-1.6943e-05, -1.0356e-05,  6.4857e-06,  ..., -1.5244e-05,
         -6.5975e-06, -1.1459e-05],
        [-2.4095e-05, -1.4722e-05,  9.2238e-06,  ..., -2.1696e-05,
         -9.1568e-06, -1.6227e-05],
        [-3.3706e-05, -2.0653e-05,  1.2942e-05,  ..., -3.0398e-05,
         -1.2726e-05, -2.2754e-05],
        [-3.8892e-05, -2.3767e-05,  1.4886e-05,  ..., -3.5018e-05,
         -1.4842e-05, -2.6211e-05]], device='cuda:0')
Loss: 1.0970505475997925


Running epoch 0, step 551, batch 551
Sampled inputs[:2]: tensor([[    0,  3227,   278,  ...,  2950,    14, 15544],
        [    0,   287,  3284,  ...,   221,   493,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7432e-04, -2.5884e-05,  1.2001e-05,  ...,  1.2694e-04,
         -1.1503e-04, -1.0051e-04],
        [-1.9386e-05, -1.1824e-05,  7.4096e-06,  ..., -1.7434e-05,
         -7.4841e-06, -1.3076e-05],
        [-2.7627e-05, -1.6853e-05,  1.0557e-05,  ..., -2.4855e-05,
         -1.0408e-05, -1.8552e-05],
        [-3.8624e-05, -2.3618e-05,  1.4804e-05,  ..., -3.4809e-05,
         -1.4462e-05, -2.6003e-05],
        [-4.4614e-05, -2.7224e-05,  1.7047e-05,  ..., -4.0144e-05,
         -1.6883e-05, -2.9996e-05]], device='cuda:0')
Loss: 1.0993728637695312
Graident accumulation at epoch 0, step 551, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0026,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0161,  0.0151, -0.0275,  ...,  0.0287, -0.0149, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3303e-06,  5.0869e-05, -1.2026e-04,  ...,  7.3974e-05,
         -1.0469e-04,  1.0377e-05],
        [-2.1369e-05, -1.4163e-05,  7.1216e-06,  ..., -1.8662e-05,
         -6.4565e-06, -1.2739e-05],
        [ 7.7372e-05,  5.5451e-05, -2.2705e-05,  ...,  6.8656e-05,
          2.7679e-05,  5.0376e-05],
        [-1.4813e-05, -9.6532e-06,  4.2954e-06,  ..., -1.1319e-05,
         -3.5670e-06, -8.7445e-06],
        [-4.8197e-05, -3.1893e-05,  1.5985e-05,  ..., -4.2077e-05,
         -1.4433e-05, -2.8785e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7708e-08, 1.9702e-08, 2.9659e-08,  ..., 2.1857e-08, 5.1381e-08,
         1.1094e-08],
        [4.8288e-11, 2.7716e-11, 2.5944e-12,  ..., 3.3889e-11, 1.9695e-12,
         9.0535e-12],
        [1.8601e-09, 9.4001e-10, 1.4555e-10,  ..., 1.3966e-09, 1.3517e-10,
         5.7828e-10],
        [4.1766e-10, 3.3223e-10, 3.5597e-11,  ..., 3.5247e-10, 3.4515e-11,
         1.6280e-10],
        [1.9916e-10, 1.0718e-10, 1.3049e-11,  ..., 1.4205e-10, 7.3261e-12,
         3.9946e-11]], device='cuda:0')
optimizer state dict: 69.0
lr: [1.7277285771063362e-05, 1.7277285771063362e-05]
scheduler_last_epoch: 69


Running epoch 0, step 552, batch 552
Sampled inputs[:2]: tensor([[    0,  1167,   278,  ...,   278,  1853,  1424],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9353e-05, -6.5810e-05, -4.2087e-05,  ...,  2.4069e-05,
         -4.6972e-06, -4.1900e-05],
        [-2.2650e-06, -1.4305e-06,  9.0897e-07,  ..., -2.0564e-06,
         -8.8662e-07, -1.5423e-06],
        [-3.3230e-06, -2.1011e-06,  1.3262e-06,  ..., -3.0100e-06,
         -1.2591e-06, -2.2501e-06],
        [-4.6492e-06, -2.9355e-06,  1.8552e-06,  ..., -4.2021e-06,
         -1.7360e-06, -3.1441e-06],
        [-5.2154e-06, -3.2932e-06,  2.0862e-06,  ..., -4.7386e-06,
         -1.9819e-06, -3.5316e-06]], device='cuda:0')
Loss: 1.1131136417388916


Running epoch 0, step 553, batch 553
Sampled inputs[:2]: tensor([[   0, 1927,  287,  ..., 1027,  271,  266],
        [   0,  328,  266,  ...,  271,  706,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2846e-05, -7.9539e-05, -7.6941e-05,  ...,  2.9545e-05,
         -3.4962e-05, -3.5433e-05],
        [-4.5747e-06, -2.8834e-06,  1.8105e-06,  ..., -4.1574e-06,
         -1.8254e-06, -3.1441e-06],
        [-6.7502e-06, -4.2617e-06,  2.6599e-06,  ..., -6.1244e-06,
         -2.6003e-06, -4.6045e-06],
        [-9.3877e-06, -5.9158e-06,  3.6955e-06,  ..., -8.4937e-06,
         -3.5763e-06, -6.4075e-06],
        [-1.0520e-05, -6.6310e-06,  4.1574e-06,  ..., -9.5665e-06,
         -4.0680e-06, -7.1973e-06]], device='cuda:0')
Loss: 1.0749657154083252


Running epoch 0, step 554, batch 554
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0841e-05, -4.2670e-05, -6.1615e-05,  ..., -5.8819e-05,
         -3.6354e-05, -5.5358e-05],
        [-6.8992e-06, -4.3064e-06,  2.7046e-06,  ..., -6.2287e-06,
         -2.7679e-06, -4.7088e-06],
        [-1.0133e-05, -6.3330e-06,  3.9488e-06,  ..., -9.1493e-06,
         -3.9339e-06, -6.8545e-06],
        [-1.4067e-05, -8.7768e-06,  5.4911e-06,  ..., -1.2696e-05,
         -5.4091e-06, -9.5367e-06],
        [-1.5825e-05, -9.8646e-06,  6.1840e-06,  ..., -1.4305e-05,
         -6.1691e-06, -1.0744e-05]], device='cuda:0')
Loss: 1.0813157558441162


Running epoch 0, step 555, batch 555
Sampled inputs[:2]: tensor([[    0,   413,    28,  ...,   328, 37605,  6499],
        [    0,   494,   221,  ...,   298,  1062,  4923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2591e-04,  9.1695e-05, -1.2346e-04,  ...,  4.2610e-05,
         -9.6123e-05, -3.1967e-05],
        [-9.2089e-06, -5.7444e-06,  3.6545e-06,  ..., -8.3745e-06,
         -3.9302e-06, -6.3777e-06],
        [-1.3471e-05, -8.4192e-06,  5.3197e-06,  ..., -1.2249e-05,
         -5.5730e-06, -9.2536e-06],
        [-1.8567e-05, -1.1578e-05,  7.3388e-06,  ..., -1.6868e-05,
         -7.5996e-06, -1.2770e-05],
        [-2.0981e-05, -1.3083e-05,  8.2999e-06,  ..., -1.9103e-05,
         -8.7023e-06, -1.4454e-05]], device='cuda:0')
Loss: 1.1103763580322266


Running epoch 0, step 556, batch 556
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1543e-04,  2.4603e-04, -2.0569e-04,  ...,  1.0535e-04,
         -1.7664e-04,  2.5028e-05],
        [-1.1563e-05, -7.1824e-06,  4.6231e-06,  ..., -1.0520e-05,
         -5.0478e-06, -8.0615e-06],
        [-1.6823e-05, -1.0476e-05,  6.6981e-06,  ..., -1.5303e-05,
         -7.1302e-06, -1.1653e-05],
        [-2.3186e-05, -1.4395e-05,  9.2238e-06,  ..., -2.1070e-05,
         -9.7156e-06, -1.6049e-05],
        [-2.6166e-05, -1.6257e-05,  1.0431e-05,  ..., -2.3812e-05,
         -1.1116e-05, -1.8150e-05]], device='cuda:0')
Loss: 1.0945948362350464


Running epoch 0, step 557, batch 557
Sampled inputs[:2]: tensor([[    0,   720,  1122,  ...,   656,   287, 14258],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1795e-04,  2.0435e-04, -2.2193e-04,  ...,  1.2210e-04,
         -2.8558e-04,  3.3291e-05],
        [-1.3873e-05, -8.5831e-06,  5.5470e-06,  ..., -1.2577e-05,
         -5.8934e-06, -9.5889e-06],
        [-2.0266e-05, -1.2562e-05,  8.0764e-06,  ..., -1.8388e-05,
         -8.3596e-06, -1.3918e-05],
        [-2.8074e-05, -1.7360e-05,  1.1176e-05,  ..., -2.5451e-05,
         -1.1444e-05, -1.9252e-05],
        [-3.1531e-05, -1.9506e-05,  1.2577e-05,  ..., -2.8610e-05,
         -1.3039e-05, -2.1666e-05]], device='cuda:0')
Loss: 1.073161005973816


Running epoch 0, step 558, batch 558
Sampled inputs[:2]: tensor([[   0, 2319,   30,  ...,  508, 6703,   12],
        [   0, 1732,  699,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3765e-04,  2.7114e-04, -2.5301e-04,  ...,  1.3779e-04,
         -3.4389e-04,  4.0905e-05],
        [-1.6138e-05, -1.0006e-05,  6.4485e-06,  ..., -1.4678e-05,
         -6.6943e-06, -1.1086e-05],
        [-2.3633e-05, -1.4678e-05,  9.4175e-06,  ..., -2.1517e-05,
         -9.5144e-06, -1.6138e-05],
        [-3.2872e-05, -2.0370e-05,  1.3083e-05,  ..., -2.9892e-05,
         -1.3076e-05, -2.2411e-05],
        [-3.6776e-05, -2.2799e-05,  1.4648e-05,  ..., -3.3468e-05,
         -1.4842e-05, -2.5108e-05]], device='cuda:0')
Loss: 1.0943838357925415


Running epoch 0, step 559, batch 559
Sampled inputs[:2]: tensor([[   0,   21,   13,  ...,   14,  747,  806],
        [   0,  278, 6481,  ...,   13, 8970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0964e-04,  2.4711e-04, -2.8331e-04,  ...,  1.3978e-04,
         -4.0027e-04,  7.1927e-05],
        [-1.8403e-05, -1.1422e-05,  7.3165e-06,  ..., -1.6704e-05,
         -7.4990e-06, -1.2569e-05],
        [-2.7016e-05, -1.6794e-05,  1.0714e-05,  ..., -2.4557e-05,
         -1.0677e-05, -1.8343e-05],
        [-3.7611e-05, -2.3320e-05,  1.4894e-05,  ..., -3.4153e-05,
         -1.4685e-05, -2.5496e-05],
        [-4.2140e-05, -2.6152e-05,  1.6704e-05,  ..., -3.8296e-05,
         -1.6697e-05, -2.8610e-05]], device='cuda:0')
Loss: 1.0939134359359741
Graident accumulation at epoch 0, step 559, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0030, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0030,  ..., -0.0025,  0.0229, -0.0197],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0160,  0.0151, -0.0275,  ...,  0.0287, -0.0149, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2161e-05,  7.0493e-05, -1.3657e-04,  ...,  8.0555e-05,
         -1.3424e-04,  1.6532e-05],
        [-2.1072e-05, -1.3889e-05,  7.1411e-06,  ..., -1.8466e-05,
         -6.5608e-06, -1.2722e-05],
        [ 6.6933e-05,  4.8226e-05, -1.9363e-05,  ...,  5.9335e-05,
          2.3843e-05,  4.3504e-05],
        [-1.7093e-05, -1.1020e-05,  5.3553e-06,  ..., -1.3603e-05,
         -4.6788e-06, -1.0420e-05],
        [-4.7591e-05, -3.1319e-05,  1.6057e-05,  ..., -4.1699e-05,
         -1.4659e-05, -2.8768e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7662e-08, 1.9744e-08, 2.9709e-08,  ..., 2.1855e-08, 5.1490e-08,
         1.1088e-08],
        [4.8579e-11, 2.7818e-11, 2.6453e-12,  ..., 3.4134e-11, 2.0237e-12,
         9.2024e-12],
        [1.8590e-09, 9.3935e-10, 1.4552e-10,  ..., 1.3958e-09, 1.3515e-10,
         5.7803e-10],
        [4.1866e-10, 3.3244e-10, 3.5783e-11,  ..., 3.5329e-10, 3.4696e-11,
         1.6329e-10],
        [2.0073e-10, 1.0775e-10, 1.3315e-11,  ..., 1.4338e-10, 7.5975e-12,
         4.0725e-11]], device='cuda:0')
optimizer state dict: 70.0
lr: [1.7191947575507777e-05, 1.7191947575507777e-05]
scheduler_last_epoch: 70


Running epoch 0, step 560, batch 560
Sampled inputs[:2]: tensor([[   0,  259, 6022,  ..., 1871, 1209, 1241],
        [   0, 1765, 5370,  ..., 1711,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3059e-05, -7.5189e-06,  2.8995e-05,  ...,  6.0638e-06,
         -7.5207e-06,  1.7097e-05],
        [-2.2054e-06, -1.4156e-06,  8.9407e-07,  ..., -1.9968e-06,
         -8.1956e-07, -1.4603e-06],
        [ 7.8495e-05,  4.3801e-05, -4.9841e-05,  ...,  1.0539e-04,
          3.6691e-05,  7.4387e-05],
        [-4.6492e-06, -2.9802e-06,  1.8701e-06,  ..., -4.2021e-06,
         -1.6615e-06, -3.0696e-06],
        [-5.1558e-06, -3.3081e-06,  2.0862e-06,  ..., -4.6492e-06,
         -1.8775e-06, -3.3975e-06]], device='cuda:0')
Loss: 1.116199016571045


Running epoch 0, step 561, batch 561
Sampled inputs[:2]: tensor([[    0,   391,  4356,  ...,   287, 32873,  5362],
        [    0,   474,   706,  ...,    83, 38084,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4582e-05,  1.3604e-04, -9.1163e-06,  ..., -4.3694e-06,
         -1.6524e-05,  2.7008e-05],
        [-4.4256e-06, -2.8238e-06,  1.7956e-06,  ..., -4.0382e-06,
         -1.8850e-06, -3.0547e-06],
        [ 7.5247e-05,  4.1745e-05, -4.8522e-05,  ...,  1.0242e-04,
          3.5194e-05,  7.2077e-05],
        [-9.0301e-06, -5.7667e-06,  3.6657e-06,  ..., -8.2254e-06,
         -3.6731e-06, -6.1840e-06],
        [-1.0103e-05, -6.4373e-06,  4.1127e-06,  ..., -9.1791e-06,
         -4.1723e-06, -6.9141e-06]], device='cuda:0')
Loss: 1.0815435647964478


Running epoch 0, step 562, batch 562
Sampled inputs[:2]: tensor([[    0,   409,  3669,  ...,    12,   374,    20],
        [    0,    67,   695,  ...,   437,   266, 44563]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5631e-06,  1.3313e-04, -2.1546e-05,  ..., -1.6959e-05,
         -2.7665e-05,  4.1665e-05],
        [-6.5714e-06, -4.2170e-06,  2.6375e-06,  ..., -5.9754e-06,
         -2.6450e-06, -4.4629e-06],
        [ 7.1909e-05,  3.9569e-05, -4.7211e-05,  ...,  9.9399e-05,
          3.4046e-05,  6.9887e-05],
        [-1.3620e-05, -8.7619e-06,  5.4687e-06,  ..., -1.2398e-05,
         -5.2378e-06, -9.1940e-06],
        [-1.5199e-05, -9.7454e-06,  6.1095e-06,  ..., -1.3798e-05,
         -5.9307e-06, -1.0237e-05]], device='cuda:0')
Loss: 1.086195468902588


Running epoch 0, step 563, batch 563
Sampled inputs[:2]: tensor([[    0,  1016,  1387,  ..., 12156, 14838,  3550],
        [    0,  2228,  1416,  ...,  3766,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8528e-06,  1.7143e-04, -5.0361e-05,  ...,  8.5642e-06,
          1.3247e-05,  1.7630e-05],
        [-8.7619e-06, -5.6028e-06,  3.5278e-06,  ..., -7.9870e-06,
         -3.5651e-06, -5.9754e-06],
        [ 6.8631e-05,  3.7498e-05, -4.5877e-05,  ...,  9.6389e-05,
          3.2720e-05,  6.7636e-05],
        [-1.8209e-05, -1.1653e-05,  7.3314e-06,  ..., -1.6600e-05,
         -7.0706e-06, -1.2338e-05],
        [-2.0176e-05, -1.2875e-05,  8.1211e-06,  ..., -1.8358e-05,
         -7.9423e-06, -1.3635e-05]], device='cuda:0')
Loss: 1.1015468835830688


Running epoch 0, step 564, batch 564
Sampled inputs[:2]: tensor([[   0,  278,  634,  ...,  598, 1722,  591],
        [   0, 1603,  694,  ...,   36,   18,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8871e-05,  2.5587e-04, -8.2347e-05,  ...,  3.2759e-05,
          1.9295e-06,  5.3376e-05],
        [-1.0967e-05, -7.0184e-06,  4.4368e-06,  ..., -1.0014e-05,
         -4.5337e-06, -7.5325e-06],
        [ 6.5338e-05,  3.5397e-05, -4.4521e-05,  ...,  9.3379e-05,
          3.1342e-05,  6.5342e-05],
        [-2.2709e-05, -1.4529e-05,  9.1866e-06,  ..., -2.0713e-05,
         -8.9407e-06, -1.5467e-05],
        [-2.5153e-05, -1.6049e-05,  1.0163e-05,  ..., -2.2888e-05,
         -1.0028e-05, -1.7092e-05]], device='cuda:0')
Loss: 1.089080810546875


Running epoch 0, step 565, batch 565
Sampled inputs[:2]: tensor([[    0, 11661,    12,  ...,  1707,   394,   264],
        [    0,  1295,  1178,  ...,  4808,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9543e-06,  2.1041e-04, -1.3724e-04,  ...,  8.3415e-05,
          1.0163e-05,  3.8770e-05],
        [-1.3128e-05, -8.3968e-06,  5.3048e-06,  ..., -1.1966e-05,
         -5.3011e-06, -8.9481e-06],
        [ 6.1985e-05,  3.3266e-05, -4.3180e-05,  ...,  9.0340e-05,
          3.0194e-05,  6.3166e-05],
        [-2.7329e-05, -1.7464e-05,  1.1042e-05,  ..., -2.4915e-05,
         -1.0498e-05, -1.8463e-05],
        [-3.0220e-05, -1.9267e-05,  1.2204e-05,  ..., -2.7478e-05,
         -1.1772e-05, -2.0385e-05]], device='cuda:0')
Loss: 1.0910251140594482


Running epoch 0, step 566, batch 566
Sampled inputs[:2]: tensor([[   0,   14,  381,  ..., 2195,  278,  266],
        [   0, 3448,  278,  ...,  380,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9702e-05,  2.4839e-04, -2.0528e-04,  ...,  6.5806e-05,
          7.5116e-05,  6.1275e-05],
        [-1.5289e-05, -9.7826e-06,  6.1654e-06,  ..., -1.3918e-05,
         -6.0536e-06, -1.0364e-05],
        [ 5.8692e-05,  3.1150e-05, -4.1862e-05,  ...,  8.7344e-05,
          2.9092e-05,  6.1020e-05],
        [-3.1918e-05, -2.0415e-05,  1.2875e-05,  ..., -2.9087e-05,
         -1.2010e-05, -2.1458e-05],
        [-3.5226e-05, -2.2471e-05,  1.4201e-05,  ..., -3.2008e-05,
         -1.3456e-05, -2.3633e-05]], device='cuda:0')
Loss: 1.0899940729141235


Running epoch 0, step 567, batch 567
Sampled inputs[:2]: tensor([[    0, 41010,  6737,  ...,   963,   409,   382],
        [    0,   292, 15156,  ...,    35,  3815,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1695e-05,  3.2319e-04, -2.1019e-04,  ...,  8.9869e-05,
          5.1927e-05,  1.3461e-04],
        [-1.7494e-05, -1.1161e-05,  7.0781e-06,  ..., -1.5929e-05,
         -6.9663e-06, -1.1832e-05],
        [ 5.5399e-05,  2.9094e-05, -4.0498e-05,  ...,  8.4334e-05,
          2.7788e-05,  5.8845e-05],
        [-3.6448e-05, -2.3246e-05,  1.4752e-05,  ..., -3.3230e-05,
         -1.3784e-05, -2.4453e-05],
        [-4.0263e-05, -2.5615e-05,  1.6287e-05,  ..., -3.6627e-05,
         -1.5467e-05, -2.6971e-05]], device='cuda:0')
Loss: 1.0931774377822876
Graident accumulation at epoch 0, step 567, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0029,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0149, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5115e-05,  9.5763e-05, -1.4393e-04,  ...,  8.1486e-05,
         -1.1563e-04,  2.8340e-05],
        [-2.0714e-05, -1.3616e-05,  7.1348e-06,  ..., -1.8212e-05,
         -6.6013e-06, -1.2633e-05],
        [ 6.5779e-05,  4.6313e-05, -2.1477e-05,  ...,  6.1835e-05,
          2.4238e-05,  4.5038e-05],
        [-1.9029e-05, -1.2242e-05,  6.2950e-06,  ..., -1.5566e-05,
         -5.5893e-06, -1.1823e-05],
        [-4.6858e-05, -3.0748e-05,  1.6080e-05,  ..., -4.1192e-05,
         -1.4740e-05, -2.8588e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7606e-08, 1.9828e-08, 2.9724e-08,  ..., 2.1841e-08, 5.1441e-08,
         1.1095e-08],
        [4.8836e-11, 2.7915e-11, 2.6928e-12,  ..., 3.4353e-11, 2.0702e-12,
         9.3332e-12],
        [1.8602e-09, 9.3926e-10, 1.4702e-10,  ..., 1.4015e-09, 1.3579e-10,
         5.8092e-10],
        [4.1957e-10, 3.3265e-10, 3.5965e-11,  ..., 3.5404e-10, 3.4851e-11,
         1.6372e-10],
        [2.0215e-10, 1.0830e-10, 1.3567e-11,  ..., 1.4457e-10, 7.8292e-12,
         4.1411e-11]], device='cuda:0')
optimizer state dict: 71.0
lr: [1.710551038758326e-05, 1.710551038758326e-05]
scheduler_last_epoch: 71


Running epoch 0, step 568, batch 568
Sampled inputs[:2]: tensor([[    0,   221,   264,  ...,  3613,  3222, 14000],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4757e-05,  6.4138e-05, -5.7769e-05,  ...,  3.8118e-05,
         -1.6399e-05,  1.0129e-04],
        [-2.1458e-06, -1.3635e-06,  9.3505e-07,  ..., -1.9819e-06,
         -1.0133e-06, -1.4678e-06],
        [-3.2783e-06, -2.0713e-06,  1.4156e-06,  ..., -3.0100e-06,
         -1.4827e-06, -2.2203e-06],
        [-4.4405e-06, -2.8014e-06,  1.9222e-06,  ..., -4.0829e-06,
         -1.9968e-06, -3.0100e-06],
        [-4.8280e-06, -3.0398e-06,  2.0862e-06,  ..., -4.4107e-06,
         -2.1905e-06, -3.2634e-06]], device='cuda:0')
Loss: 1.095100998878479


Running epoch 0, step 569, batch 569
Sampled inputs[:2]: tensor([[    0,   680,   271,  ..., 12942,   271,   266],
        [    0,  1477,  3205,  ...,  6441,  9363,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1921e-05,  1.4761e-05, -7.6132e-05,  ...,  2.4588e-05,
         -4.2491e-06,  7.9398e-05],
        [-4.2319e-06, -2.7046e-06,  1.8254e-06,  ..., -3.8669e-06,
         -1.8068e-06, -2.8014e-06],
        [-6.5267e-06, -4.1723e-06,  2.7940e-06,  ..., -5.9456e-06,
         -2.6524e-06, -4.2766e-06],
        [-8.8811e-06, -5.6624e-06,  3.8072e-06,  ..., -8.1062e-06,
         -3.5763e-06, -5.8115e-06],
        [-9.6858e-06, -6.1840e-06,  4.1574e-06,  ..., -8.8215e-06,
         -3.9712e-06, -6.3479e-06]], device='cuda:0')
Loss: 1.104836106300354


Running epoch 0, step 570, batch 570
Sampled inputs[:2]: tensor([[   0,  944,  278,  ..., 5755,  292,  221],
        [   0, 3019,  278,  ...,  365, 1770,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0817e-05, -3.6696e-05, -5.5912e-05,  ...,  2.9275e-05,
         -6.1024e-06, -6.4555e-06],
        [-6.3479e-06, -4.0233e-06,  2.7344e-06,  ..., -5.7891e-06,
         -2.6487e-06, -4.2468e-06],
        [-9.7752e-06, -6.2138e-06,  4.1872e-06,  ..., -8.8960e-06,
         -3.8967e-06, -6.4969e-06],
        [-1.3322e-05, -8.4490e-06,  5.7146e-06,  ..., -1.2159e-05,
         -5.2601e-06, -8.8364e-06],
        [-1.4514e-05, -9.2089e-06,  6.2287e-06,  ..., -1.3202e-05,
         -5.8338e-06, -9.6411e-06]], device='cuda:0')
Loss: 1.093640685081482


Running epoch 0, step 571, batch 571
Sampled inputs[:2]: tensor([[   0, 1500,  367,  ...,  344, 4250,  287],
        [   0, 1042, 2548,  ...,  328,  259, 2771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4224e-04, -5.6998e-06,  1.5575e-05,  ...,  3.8804e-06,
          5.3379e-05,  2.7051e-05],
        [-8.5086e-06, -5.3942e-06,  3.6173e-06,  ..., -7.7710e-06,
         -3.5986e-06, -5.7071e-06],
        [-1.3053e-05, -8.2999e-06,  5.5209e-06,  ..., -1.1891e-05,
         -5.2825e-06, -8.6874e-06],
        [-1.7762e-05, -1.1265e-05,  7.5176e-06,  ..., -1.6212e-05,
         -7.1153e-06, -1.1802e-05],
        [-1.9372e-05, -1.2293e-05,  8.1956e-06,  ..., -1.7613e-05,
         -7.8902e-06, -1.2890e-05]], device='cuda:0')
Loss: 1.0975472927093506


Running epoch 0, step 572, batch 572
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  391,  721, 1576],
        [   0, 2626,   13,  ...,  300,  369,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5999e-04,  1.1658e-05, -5.7500e-06,  ...,  3.3414e-05,
          6.9815e-05,  2.4026e-06],
        [-1.0610e-05, -6.7800e-06,  4.5076e-06,  ..., -9.6783e-06,
         -4.3958e-06, -7.0781e-06],
        [-1.6317e-05, -1.0461e-05,  6.8992e-06,  ..., -1.4856e-05,
         -6.4671e-06, -1.0803e-05],
        [-2.2262e-05, -1.4231e-05,  9.4175e-06,  ..., -2.0295e-05,
         -8.7321e-06, -1.4722e-05],
        [-2.4199e-05, -1.5467e-05,  1.0237e-05,  ..., -2.1964e-05,
         -9.6560e-06, -1.6004e-05]], device='cuda:0')
Loss: 1.0906507968902588


Running epoch 0, step 573, batch 573
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0,   709,   630,  ...,  6263,   409,   508]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2345e-04,  1.2246e-05, -7.8426e-05,  ...,  7.5077e-05,
          3.1821e-05,  7.3154e-06],
        [-1.2651e-05, -8.0988e-06,  5.3681e-06,  ..., -1.1548e-05,
         -5.1446e-06, -8.4266e-06],
        [-1.9521e-05, -1.2532e-05,  8.2478e-06,  ..., -1.7792e-05,
         -7.5847e-06, -1.2904e-05],
        [-2.6673e-05, -1.7077e-05,  1.1273e-05,  ..., -2.4348e-05,
         -1.0252e-05, -1.7613e-05],
        [-2.8938e-05, -1.8537e-05,  1.2234e-05,  ..., -2.6315e-05,
         -1.1325e-05, -1.9118e-05]], device='cuda:0')
Loss: 1.0891300439834595


Running epoch 0, step 574, batch 574
Sampled inputs[:2]: tensor([[    0,   471,    14,  ..., 27104,     9,   631],
        [    0,    12, 17906,  ...,  2086,   287,  4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8337e-05,  4.1993e-06, -7.7800e-05,  ...,  9.1573e-05,
          3.1821e-05,  8.7748e-05],
        [-1.4707e-05, -9.4399e-06,  6.2324e-06,  ..., -1.3404e-05,
         -5.8673e-06, -9.7677e-06],
        [ 7.6324e-05,  2.8842e-05, -1.7435e-05,  ...,  6.9655e-05,
          2.5079e-05,  3.9916e-05],
        [-3.1203e-05, -2.0027e-05,  1.3165e-05,  ..., -2.8431e-05,
         -1.1742e-05, -2.0549e-05],
        [ 1.2568e-04,  1.3095e-04, -5.2934e-05,  ...,  1.2885e-04,
          5.0132e-05,  7.6870e-05]], device='cuda:0')
Loss: 1.0850739479064941


Running epoch 0, step 575, batch 575
Sampled inputs[:2]: tensor([[    0,  1266,  2257,  ..., 27146,  1141,  1196],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3424e-04,  5.5632e-06, -1.1112e-04,  ...,  1.1891e-04,
          3.3620e-05,  7.7097e-05],
        [-1.6764e-05, -1.0818e-05,  7.1116e-06,  ..., -1.5303e-05,
         -6.7167e-06, -1.1183e-05],
        [ 7.3165e-05,  2.6726e-05, -1.6093e-05,  ...,  6.6734e-05,
          2.3842e-05,  3.7755e-05],
        [-3.5524e-05, -2.2903e-05,  1.4998e-05,  ..., -3.2425e-05,
         -1.3418e-05, -2.3499e-05],
        [ 1.2103e-04,  1.2785e-04, -5.0952e-05,  ...,  1.2456e-04,
          4.8291e-05,  7.3696e-05]], device='cuda:0')
Loss: 1.0815330743789673
Graident accumulation at epoch 0, step 575, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0146,  0.0029,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0291, -0.0078,  0.0036,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0062, -0.0021],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0149, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7027e-05,  8.6743e-05, -1.4065e-04,  ...,  8.5228e-05,
         -1.0070e-04,  3.3215e-05],
        [-2.0319e-05, -1.3336e-05,  7.1325e-06,  ..., -1.7922e-05,
         -6.6129e-06, -1.2488e-05],
        [ 6.6518e-05,  4.4354e-05, -2.0938e-05,  ...,  6.2325e-05,
          2.4198e-05,  4.4310e-05],
        [-2.0678e-05, -1.3309e-05,  7.1653e-06,  ..., -1.7251e-05,
         -6.3722e-06, -1.2991e-05],
        [-3.0069e-05, -1.4888e-05,  9.3769e-06,  ..., -2.4617e-05,
         -8.4367e-06, -1.8359e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7566e-08, 1.9809e-08, 2.9706e-08,  ..., 2.1833e-08, 5.1391e-08,
         1.1090e-08],
        [4.9068e-11, 2.8004e-11, 2.7407e-12,  ..., 3.4553e-11, 2.1133e-12,
         9.4489e-12],
        [1.8637e-09, 9.3903e-10, 1.4713e-10,  ..., 1.4046e-09, 1.3622e-10,
         5.8176e-10],
        [4.2041e-10, 3.3284e-10, 3.6154e-11,  ..., 3.5473e-10, 3.4997e-11,
         1.6411e-10],
        [2.1660e-10, 1.2454e-10, 1.6149e-11,  ..., 1.5994e-10, 1.0153e-11,
         4.6801e-11]], device='cuda:0')
optimizer state dict: 72.0
lr: [1.7017987415646643e-05, 1.7017987415646643e-05]
scheduler_last_epoch: 72


Running epoch 0, step 576, batch 576
Sampled inputs[:2]: tensor([[    0,    19,    14,  ...,    13,  6673,   298],
        [    0, 39004,   266,  ...,   287, 21972,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5574e-05,  3.9241e-05, -7.1572e-05,  ...,  5.3614e-05,
         -4.4650e-05, -8.3617e-05],
        [-2.0713e-06, -1.2890e-06,  9.3505e-07,  ..., -1.9670e-06,
         -1.2368e-06, -1.5572e-06],
        [-3.0249e-06, -1.8701e-06,  1.3635e-06,  ..., -2.8461e-06,
         -1.6987e-06, -2.2203e-06],
        [-4.0233e-06, -2.4736e-06,  1.8030e-06,  ..., -3.7700e-06,
         -2.2203e-06, -2.9504e-06],
        [-4.4405e-06, -2.7269e-06,  1.9968e-06,  ..., -4.1425e-06,
         -2.4736e-06, -3.2336e-06]], device='cuda:0')
Loss: 1.0779390335083008


Running epoch 0, step 577, batch 577
Sampled inputs[:2]: tensor([[   0, 5182,  446,  ...,  417,  199,   50],
        [   0, 7230,   13,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1176e-05,  1.9162e-05, -1.1391e-04,  ...,  7.1264e-05,
         -8.2604e-05, -1.2838e-04],
        [-4.0531e-06, -2.5630e-06,  1.8328e-06,  ..., -3.7849e-06,
         -2.0005e-06, -2.8610e-06],
        [-6.1244e-06, -3.8520e-06,  2.7567e-06,  ..., -5.6773e-06,
         -2.8089e-06, -4.2319e-06],
        [-8.2552e-06, -5.1707e-06,  3.7029e-06,  ..., -7.6443e-06,
         -3.7178e-06, -5.7071e-06],
        [-8.9705e-06, -5.6326e-06,  4.0382e-06,  ..., -8.2850e-06,
         -4.1276e-06, -6.1989e-06]], device='cuda:0')
Loss: 1.0854086875915527


Running epoch 0, step 578, batch 578
Sampled inputs[:2]: tensor([[   0,  380, 8157,  ...,  943,  352, 2278],
        [   0,   14, 1266,  ..., 2288,  417,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5872e-04,  4.9222e-05, -1.2494e-04,  ...,  7.8396e-05,
          2.8833e-05, -6.2762e-05],
        [-6.1244e-06, -3.8818e-06,  2.7753e-06,  ..., -5.6922e-06,
         -3.0287e-06, -4.3064e-06],
        [-9.2834e-06, -5.8636e-06,  4.1872e-06,  ..., -8.5831e-06,
         -4.2915e-06, -6.4075e-06],
        [-1.2428e-05, -7.8231e-06,  5.5954e-06,  ..., -1.1489e-05,
         -5.6550e-06, -8.5831e-06],
        [-1.3560e-05, -8.5533e-06,  6.1244e-06,  ..., -1.2517e-05,
         -6.3181e-06, -9.3728e-06]], device='cuda:0')
Loss: 1.1021326780319214


Running epoch 0, step 579, batch 579
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   630,  3765, 19107],
        [    0,   365,  8790,  ...,  1172,  8806,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8249e-04,  2.2001e-05, -1.6292e-04,  ...,  1.0247e-04,
          4.1859e-05, -5.0279e-05],
        [-8.1509e-06, -5.1558e-06,  3.6843e-06,  ..., -7.5102e-06,
         -3.8818e-06, -5.6177e-06],
        [-1.2487e-05, -7.8753e-06,  5.6103e-06,  ..., -1.1459e-05,
         -5.5656e-06, -8.4639e-06],
        [-1.6749e-05, -1.0520e-05,  7.5176e-06,  ..., -1.5363e-05,
         -7.3537e-06, -1.1355e-05],
        [-1.8179e-05, -1.1459e-05,  8.1956e-06,  ..., -1.6659e-05,
         -8.1807e-06, -1.2338e-05]], device='cuda:0')
Loss: 1.0879851579666138


Running epoch 0, step 580, batch 580
Sampled inputs[:2]: tensor([[    0, 48545,    26,  ...,  1471,   266,   319],
        [    0,    20,  2637,  ..., 14044,     9,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8095e-04,  6.6835e-05, -2.2144e-04,  ...,  1.1085e-04,
          1.8550e-05, -1.4598e-05],
        [-1.0252e-05, -6.4522e-06,  4.7274e-06,  ..., -9.4473e-06,
         -5.0664e-06, -7.1749e-06],
        [-1.5572e-05, -9.7752e-06,  7.1302e-06,  ..., -1.4290e-05,
         -7.2345e-06, -1.0729e-05],
        [-2.0862e-05, -1.3039e-05,  9.5442e-06,  ..., -1.9133e-05,
         -9.5591e-06, -1.4365e-05],
        [-2.2709e-05, -1.4231e-05,  1.0416e-05,  ..., -2.0802e-05,
         -1.0625e-05, -1.5646e-05]], device='cuda:0')
Loss: 1.0871978998184204


Running epoch 0, step 581, batch 581
Sampled inputs[:2]: tensor([[    0,    12,   221,  ...,   292, 27729,  9837],
        [    0, 11853,  1611,  ...,  4413,  4240,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4588e-04,  7.4938e-06, -2.9350e-04,  ...,  1.6002e-04,
         -4.4907e-05, -8.6679e-05],
        [-1.2219e-05, -7.7188e-06,  5.6140e-06,  ..., -1.1258e-05,
         -5.8860e-06, -8.4862e-06],
        [-1.8641e-05, -1.1742e-05,  8.5160e-06,  ..., -1.7107e-05,
         -8.4490e-06, -1.2755e-05],
        [-2.5094e-05, -1.5736e-05,  1.1444e-05,  ..., -2.3007e-05,
         -1.1206e-05, -1.7151e-05],
        [-2.7239e-05, -1.7136e-05,  1.2457e-05,  ..., -2.4945e-05,
         -1.2428e-05, -1.8641e-05]], device='cuda:0')
Loss: 1.0840203762054443


Running epoch 0, step 582, batch 582
Sampled inputs[:2]: tensor([[    0, 28107,    14,  ...,   864,   298,   413],
        [    0,  2372,  1319,  ...,  1253,   292, 34166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1538e-04,  6.6460e-05, -3.5974e-04,  ...,  2.1019e-04,
         -8.4114e-05, -7.2979e-07],
        [-1.4365e-05, -9.0450e-06,  6.5304e-06,  ..., -1.3128e-05,
         -6.9588e-06, -9.8944e-06],
        [-2.1875e-05, -1.3754e-05,  9.9018e-06,  ..., -1.9938e-05,
         -9.9987e-06, -1.4871e-05],
        [-2.9385e-05, -1.8403e-05,  1.3284e-05,  ..., -2.6777e-05,
         -1.3232e-05, -1.9953e-05],
        [-3.2008e-05, -2.0102e-05,  1.4514e-05,  ..., -2.9117e-05,
         -1.4722e-05, -2.1756e-05]], device='cuda:0')
Loss: 1.0664639472961426


Running epoch 0, step 583, batch 583
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1538e-04,  1.3820e-04, -3.1773e-04,  ...,  1.8767e-04,
         -5.3547e-05,  9.8948e-05],
        [-1.6496e-05, -1.0386e-05,  7.4916e-06,  ..., -1.5020e-05,
         -7.9498e-06, -1.1317e-05],
        [-2.5123e-05, -1.5795e-05,  1.1362e-05,  ..., -2.2829e-05,
         -1.1459e-05, -1.7032e-05],
        [-3.3736e-05, -2.1130e-05,  1.5236e-05,  ..., -3.0652e-05,
         -1.5154e-05, -2.2829e-05],
        [-3.6806e-05, -2.3112e-05,  1.6659e-05,  ..., -3.3379e-05,
         -1.6883e-05, -2.4930e-05]], device='cuda:0')
Loss: 1.127420425415039
Graident accumulation at epoch 0, step 583, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0145,  0.0029,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0024, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0021],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5863e-05,  9.1889e-05, -1.5836e-04,  ...,  9.5472e-05,
         -9.5987e-05,  3.9789e-05],
        [-1.9937e-05, -1.3041e-05,  7.1684e-06,  ..., -1.7631e-05,
         -6.7466e-06, -1.2371e-05],
        [ 5.7354e-05,  3.8339e-05, -1.7708e-05,  ...,  5.3809e-05,
          2.0633e-05,  3.8176e-05],
        [-2.1984e-05, -1.4091e-05,  7.9724e-06,  ..., -1.8592e-05,
         -7.2504e-06, -1.3974e-05],
        [-3.0743e-05, -1.5711e-05,  1.0105e-05,  ..., -2.5493e-05,
         -9.2814e-06, -1.9016e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7555e-08, 1.9808e-08, 2.9778e-08,  ..., 2.1846e-08, 5.1343e-08,
         1.1088e-08],
        [4.9291e-11, 2.8084e-11, 2.7941e-12,  ..., 3.4744e-11, 2.1744e-12,
         9.5675e-12],
        [1.8625e-09, 9.3834e-10, 1.4711e-10,  ..., 1.4037e-09, 1.3622e-10,
         5.8147e-10],
        [4.2113e-10, 3.3295e-10, 3.6350e-11,  ..., 3.5532e-10, 3.5191e-11,
         1.6447e-10],
        [2.1774e-10, 1.2495e-10, 1.6411e-11,  ..., 1.6090e-10, 1.0428e-11,
         4.7376e-11]], device='cuda:0')
optimizer state dict: 73.0
lr: [1.6929392033972038e-05, 1.6929392033972038e-05]
scheduler_last_epoch: 73


Running epoch 0, step 584, batch 584
Sampled inputs[:2]: tensor([[    0, 16765,   367,  ..., 30192,  7038,  8135],
        [    0,   199, 11296,  ...,   266, 10463,  8256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7977e-05,  1.3875e-05, -1.7188e-05,  ...,  3.0031e-05,
          5.7207e-05,  3.0128e-05],
        [-2.0415e-06, -1.2666e-06,  9.3877e-07,  ..., -1.8030e-06,
         -9.0152e-07, -1.2740e-06],
        [-3.2187e-06, -1.9968e-06,  1.4752e-06,  ..., -2.8461e-06,
         -1.3486e-06, -1.9968e-06],
        [-4.2915e-06, -2.6524e-06,  1.9670e-06,  ..., -3.7849e-06,
         -1.7658e-06, -2.6524e-06],
        [-4.6492e-06, -2.8908e-06,  2.1309e-06,  ..., -4.1127e-06,
         -1.9670e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.0820798873901367


Running epoch 0, step 585, batch 585
Sampled inputs[:2]: tensor([[    0,   381,  1795,  ...,    12,   344,   593],
        [    0,    12,   496,  ..., 11354,  4856,  1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2970e-04, -5.8247e-06,  3.6575e-05,  ..., -4.1895e-05,
          1.0238e-04,  7.6299e-05],
        [-4.0680e-06, -2.5481e-06,  1.9595e-06,  ..., -3.5837e-06,
         -1.8403e-06, -2.5630e-06],
        [ 2.1833e-04,  1.9151e-04, -1.2372e-04,  ...,  2.4019e-04,
          8.9356e-05,  1.9723e-04],
        [-8.4937e-06, -5.3197e-06,  4.0829e-06,  ..., -7.5102e-06,
         -3.5763e-06, -5.3197e-06],
        [-9.2983e-06, -5.8264e-06,  4.4703e-06,  ..., -8.1956e-06,
         -4.0233e-06, -5.8115e-06]], device='cuda:0')
Loss: 1.108233094215393


Running epoch 0, step 586, batch 586
Sampled inputs[:2]: tensor([[    0,    12, 32425,  ...,   389,   221,   494],
        [    0,   346,   462,  ...,   474, 38333,    87]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4957e-04,  8.4283e-05,  2.7085e-05,  ..., -2.6719e-05,
          8.0627e-05,  9.0686e-05],
        [-6.0648e-06, -3.7700e-06,  2.8871e-06,  ..., -5.3942e-06,
         -2.7418e-06, -3.8594e-06],
        [ 2.1523e-04,  1.8961e-04, -1.2229e-04,  ...,  2.3738e-04,
          8.8038e-05,  1.9524e-04],
        [-1.2606e-05, -7.8380e-06,  5.9828e-06,  ..., -1.1265e-05,
         -5.3048e-06, -7.9721e-06],
        [-1.3798e-05, -8.5682e-06,  6.5416e-06,  ..., -1.2279e-05,
         -5.9605e-06, -8.6874e-06]], device='cuda:0')
Loss: 1.0944299697875977


Running epoch 0, step 587, batch 587
Sampled inputs[:2]: tensor([[    0,    13,  1581,  ...,    13, 11628, 14876],
        [    0,  5116,  4330,  ...,   925,   699,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5967e-04,  9.3212e-05, -2.2722e-05,  ..., -2.4370e-05,
          9.0578e-05,  8.4713e-05],
        [-8.0019e-06, -4.9472e-06,  3.8110e-06,  ..., -7.1749e-06,
         -3.7327e-06, -5.1707e-06],
        [ 2.1225e-04,  1.8780e-04, -1.2087e-04,  ...,  2.3464e-04,
          8.6614e-05,  1.9325e-04],
        [-1.6630e-05, -1.0282e-05,  7.8902e-06,  ..., -1.4976e-05,
         -7.1973e-06, -1.0654e-05],
        [-1.8209e-05, -1.1235e-05,  8.6278e-06,  ..., -1.6302e-05,
         -8.0913e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.0906494855880737


Running epoch 0, step 588, batch 588
Sampled inputs[:2]: tensor([[    0,   259, 19567,  ...,   266,  3899,  2123],
        [    0,  4350,    14,  ...,   266,  9479,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2087e-04,  4.9916e-05,  2.1770e-05,  ..., -4.0271e-05,
          9.8931e-05,  9.2827e-05],
        [-9.9838e-06, -6.1765e-06,  4.7199e-06,  ..., -8.9481e-06,
         -4.5896e-06, -6.4000e-06],
        [ 2.0912e-04,  1.8585e-04, -1.1944e-04,  ...,  2.3182e-04,
          8.5340e-05,  1.9132e-04],
        [-2.0832e-05, -1.2890e-05,  9.8124e-06,  ..., -1.8761e-05,
         -8.8885e-06, -1.3247e-05],
        [-2.2799e-05, -1.4082e-05,  1.0729e-05,  ..., -2.0415e-05,
         -9.9912e-06, -1.4439e-05]], device='cuda:0')
Loss: 1.093997597694397


Running epoch 0, step 589, batch 589
Sampled inputs[:2]: tensor([[    0, 21930,    12,  ...,  2849,   863,   578],
        [    0,  1196,  2612,  ...,  2489,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0823e-04,  4.0798e-05, -9.1578e-05,  ..., -1.5902e-05,
          9.3367e-05,  6.0704e-05],
        [-1.1921e-05, -7.4059e-06,  5.6736e-06,  ..., -1.0714e-05,
         -5.5246e-06, -7.6517e-06],
        [ 2.0607e-04,  1.8391e-04, -1.1794e-04,  ...,  2.2903e-04,
          8.3962e-05,  1.8937e-04],
        [-2.4974e-05, -1.5512e-05,  1.1839e-05,  ..., -2.2545e-05,
         -1.0736e-05, -1.5900e-05],
        [-2.7239e-05, -1.6898e-05,  1.2904e-05,  ..., -2.4438e-05,
         -1.2018e-05, -1.7270e-05]], device='cuda:0')
Loss: 1.1001535654067993


Running epoch 0, step 590, batch 590
Sampled inputs[:2]: tensor([[   0,  600,   14,  ...,  221, 8187, 1802],
        [   0, 9116,  278,  ..., 6997, 3244, 1192]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1608e-04,  2.9653e-05, -1.7090e-04,  ..., -1.7580e-07,
          1.1350e-04,  8.0016e-05],
        [-1.3843e-05, -8.6129e-06,  6.5938e-06,  ..., -1.2495e-05,
         -6.4038e-06, -8.9034e-06],
        [ 2.0304e-04,  1.8201e-04, -1.1650e-04,  ...,  2.2623e-04,
          8.2673e-05,  1.8741e-04],
        [-2.9117e-05, -1.8120e-05,  1.3806e-05,  ..., -2.6390e-05,
         -1.2480e-05, -1.8567e-05],
        [-3.1650e-05, -1.9670e-05,  1.5020e-05,  ..., -2.8521e-05,
         -1.3925e-05, -2.0117e-05]], device='cuda:0')
Loss: 1.0721911191940308


Running epoch 0, step 591, batch 591
Sampled inputs[:2]: tensor([[   0,  271, 2862,  ...,  287, 5699,   18],
        [   0, 2530,  634,  ...,   15, 8808,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8588e-04,  2.8843e-05, -1.7115e-04,  ..., -2.7031e-05,
          1.8196e-04,  1.3590e-04],
        [-1.5870e-05, -9.9167e-06,  7.5474e-06,  ..., -1.4372e-05,
         -7.4767e-06, -1.0237e-05],
        [ 1.9996e-04,  1.8001e-04, -1.1505e-04,  ...,  2.2337e-04,
          8.1108e-05,  1.8540e-04],
        [-3.3200e-05, -2.0742e-05,  1.5728e-05,  ..., -3.0175e-05,
         -1.4521e-05, -2.1234e-05],
        [-3.6150e-05, -2.2560e-05,  1.7136e-05,  ..., -3.2693e-05,
         -1.6220e-05, -2.3052e-05]], device='cuda:0')
Loss: 1.1033636331558228
Graident accumulation at epoch 0, step 591, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0056, -0.0145,  0.0029,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0025, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0021],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9865e-05,  8.5584e-05, -1.5964e-04,  ...,  8.3221e-05,
         -6.8192e-05,  4.9400e-05],
        [-1.9530e-05, -1.2729e-05,  7.2063e-06,  ..., -1.7306e-05,
         -6.8196e-06, -1.2158e-05],
        [ 7.1614e-05,  5.2506e-05, -2.7442e-05,  ...,  7.0765e-05,
          2.6680e-05,  5.2898e-05],
        [-2.3106e-05, -1.4756e-05,  8.7480e-06,  ..., -1.9750e-05,
         -7.9775e-06, -1.4700e-05],
        [-3.1284e-05, -1.6396e-05,  1.0808e-05,  ..., -2.6213e-05,
         -9.9752e-06, -1.9420e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7579e-08, 1.9789e-08, 2.9777e-08,  ..., 2.1825e-08, 5.1324e-08,
         1.1096e-08],
        [4.9494e-11, 2.8154e-11, 2.8482e-12,  ..., 3.4916e-11, 2.2281e-12,
         9.6628e-12],
        [1.9006e-09, 9.6981e-10, 1.6020e-10,  ..., 1.4522e-09, 1.4266e-10,
         6.1526e-10],
        [4.2181e-10, 3.3305e-10, 3.6561e-11,  ..., 3.5587e-10, 3.5367e-11,
         1.6475e-10],
        [2.1883e-10, 1.2533e-10, 1.6688e-11,  ..., 1.6181e-10, 1.0681e-11,
         4.7860e-11]], device='cuda:0')
optimizer state dict: 74.0
lr: [1.6839737780707125e-05, 1.6839737780707125e-05]
scheduler_last_epoch: 74


Running epoch 0, step 592, batch 592
Sampled inputs[:2]: tensor([[    0,  4494,    12,  ...,   341,  1619,    12],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2938e-05,  6.3261e-05, -1.0642e-04,  ..., -5.5407e-05,
          3.8225e-05,  1.5286e-05],
        [-1.9372e-06, -1.1846e-06,  1.0058e-06,  ..., -1.7658e-06,
         -1.0654e-06, -1.2517e-06],
        [-2.9206e-06, -1.7732e-06,  1.5050e-06,  ..., -2.6375e-06,
         -1.4752e-06, -1.8477e-06],
        [-3.9935e-06, -2.4140e-06,  2.0415e-06,  ..., -3.6061e-06,
         -1.9819e-06, -2.5183e-06],
        [-4.2915e-06, -2.6077e-06,  2.2203e-06,  ..., -3.8743e-06,
         -2.1905e-06, -2.7120e-06]], device='cuda:0')
Loss: 1.09836745262146


Running epoch 0, step 593, batch 593
Sampled inputs[:2]: tensor([[   0, 1477, 5648,  ..., 4391, 1722,  369],
        [   0, 6416,  367,  ...,  496,   14,   20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1312e-04,  6.5251e-05, -8.8788e-05,  ..., -9.4275e-05,
          1.4573e-04,  1.3045e-05],
        [-3.9786e-06, -2.4065e-06,  2.0042e-06,  ..., -3.5837e-06,
         -2.1458e-06, -2.4810e-06],
        [-6.0201e-06, -3.6359e-06,  3.0100e-06,  ..., -5.3942e-06,
         -3.0324e-06, -3.7029e-06],
        [-8.1360e-06, -4.9025e-06,  4.0531e-06,  ..., -7.3016e-06,
         -4.0382e-06, -5.0068e-06],
        [-8.8215e-06, -5.3197e-06,  4.4256e-06,  ..., -7.8976e-06,
         -4.5002e-06, -5.4240e-06]], device='cuda:0')
Loss: 1.1183748245239258


Running epoch 0, step 594, batch 594
Sampled inputs[:2]: tensor([[   0,  694, 2326,  ...,  278, 1781, 9660],
        [   0,  287, 7763,  ...,  689, 2409,  699]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2746e-04,  9.7432e-05, -1.6658e-04,  ..., -4.7421e-05,
          1.3610e-04, -8.8693e-06],
        [-5.9456e-06, -3.6508e-06,  3.0100e-06,  ..., -5.3644e-06,
         -3.2261e-06, -3.7029e-06],
        [-9.0450e-06, -5.5581e-06,  4.5598e-06,  ..., -8.1360e-06,
         -4.5821e-06, -5.5581e-06],
        [-1.2189e-05, -7.4655e-06,  6.1244e-06,  ..., -1.0982e-05,
         -6.0797e-06, -7.4804e-06],
        [-1.3173e-05, -8.0615e-06,  6.6459e-06,  ..., -1.1832e-05,
         -6.7353e-06, -8.0764e-06]], device='cuda:0')
Loss: 1.0840678215026855


Running epoch 0, step 595, batch 595
Sampled inputs[:2]: tensor([[    0,    14,  1845,  ...,   806,   352,   408],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5575e-04,  1.0002e-04, -1.5163e-04,  ..., -8.6740e-05,
          2.0917e-04,  1.3108e-04],
        [-7.9870e-06, -4.9174e-06,  4.0531e-06,  ..., -7.1749e-06,
         -4.3437e-06, -4.9323e-06],
        [-1.2189e-05, -7.5102e-06,  6.1542e-06,  ..., -1.0923e-05,
         -6.2287e-06, -7.4357e-06],
        [-1.6361e-05, -1.0043e-05,  8.2403e-06,  ..., -1.4693e-05,
         -8.2254e-06, -9.9689e-06],
        [-1.7732e-05, -1.0893e-05,  8.9556e-06,  ..., -1.5885e-05,
         -9.1493e-06, -1.0803e-05]], device='cuda:0')
Loss: 1.0958654880523682


Running epoch 0, step 596, batch 596
Sampled inputs[:2]: tensor([[   0, 7926, 6750,  ...,  259, 1524, 6257],
        [   0, 9855,  278,  ...,  266, 3134,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7183e-04,  1.1447e-04, -1.6734e-04,  ..., -1.1628e-04,
          2.4530e-04,  9.9130e-05],
        [-9.9689e-06, -6.1393e-06,  5.0515e-06,  ..., -8.9407e-06,
         -5.4389e-06, -6.1542e-06],
        [-1.5289e-05, -9.4324e-06,  7.7114e-06,  ..., -1.3694e-05,
         -7.8380e-06, -9.3281e-06],
        [-2.0474e-05, -1.2577e-05,  1.0297e-05,  ..., -1.8373e-05,
         -1.0341e-05, -1.2472e-05],
        [-2.2233e-05, -1.3664e-05,  1.1206e-05,  ..., -1.9908e-05,
         -1.1504e-05, -1.3530e-05]], device='cuda:0')
Loss: 1.0882903337478638


Running epoch 0, step 597, batch 597
Sampled inputs[:2]: tensor([[    0,  2380,  2667,  ...,    14,   381,  5621],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8003e-04,  1.1556e-04, -1.6039e-04,  ..., -1.2061e-04,
          2.8337e-04,  6.5185e-05],
        [-1.1981e-05, -7.3612e-06,  6.0499e-06,  ..., -1.0692e-05,
         -6.5267e-06, -7.3910e-06],
        [-1.8388e-05, -1.1317e-05,  9.2387e-06,  ..., -1.6391e-05,
         -9.4175e-06, -1.1213e-05],
        [-2.4557e-05, -1.5065e-05,  1.2323e-05,  ..., -2.1949e-05,
         -1.2398e-05, -1.4961e-05],
        [-2.6703e-05, -1.6376e-05,  1.3411e-05,  ..., -2.3812e-05,
         -1.3798e-05, -1.6242e-05]], device='cuda:0')
Loss: 1.116515874862671


Running epoch 0, step 598, batch 598
Sampled inputs[:2]: tensor([[   0, 4137,  300,  ..., 2579,  278,  266],
        [   0,  401, 3704,  ...,   14, 1062, 1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2345e-04,  2.2076e-04, -1.6812e-04,  ..., -1.1835e-04,
          2.7934e-04,  6.8732e-05],
        [-1.3873e-05, -8.5160e-06,  6.9812e-06,  ..., -1.2442e-05,
         -7.4282e-06, -8.5309e-06],
        [-2.1338e-05, -1.3120e-05,  1.0677e-05,  ..., -1.9118e-05,
         -1.0706e-05, -1.2964e-05],
        [-2.8640e-05, -1.7539e-05,  1.4305e-05,  ..., -2.5719e-05,
         -1.4156e-05, -1.7375e-05],
        [-3.0994e-05, -1.8984e-05,  1.5497e-05,  ..., -2.7746e-05,
         -1.5698e-05, -1.8775e-05]], device='cuda:0')
Loss: 1.0775707960128784


Running epoch 0, step 599, batch 599
Sampled inputs[:2]: tensor([[   0,   24,   15,  ...,  221,  380,  417],
        [   0, 4359, 5768,  ..., 4402,  292,   69]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5536e-04,  3.3212e-04, -2.5015e-04,  ..., -1.2371e-04,
          3.1841e-04,  5.5803e-05],
        [-1.5840e-05, -9.6783e-06,  7.9647e-06,  ..., -1.4246e-05,
         -8.5756e-06, -9.8199e-06],
        [-2.4304e-05, -1.4879e-05,  1.2144e-05,  ..., -2.1830e-05,
         -1.2323e-05, -1.4886e-05],
        [-3.2574e-05, -1.9863e-05,  1.6257e-05,  ..., -2.9325e-05,
         -1.6257e-05, -1.9908e-05],
        [-3.5286e-05, -2.1517e-05,  1.7628e-05,  ..., -3.1680e-05,
         -1.8053e-05, -2.1547e-05]], device='cuda:0')
Loss: 1.0584425926208496
Graident accumulation at epoch 0, step 599, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0029,  ..., -0.0025,  0.0230, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0025, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.8414e-05,  1.1024e-04, -1.6869e-04,  ...,  6.2529e-05,
         -2.9532e-05,  5.0040e-05],
        [-1.9161e-05, -1.2424e-05,  7.2821e-06,  ..., -1.7000e-05,
         -6.9952e-06, -1.1924e-05],
        [ 6.2022e-05,  4.5768e-05, -2.3483e-05,  ...,  6.1506e-05,
          2.2780e-05,  4.6120e-05],
        [-2.4052e-05, -1.5267e-05,  9.4989e-06,  ..., -2.0707e-05,
         -8.8055e-06, -1.5221e-05],
        [-3.1684e-05, -1.6908e-05,  1.1490e-05,  ..., -2.6760e-05,
         -1.0783e-05, -1.9633e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7546e-08, 1.9879e-08, 2.9810e-08,  ..., 2.1819e-08, 5.1374e-08,
         1.1088e-08],
        [4.9695e-11, 2.8220e-11, 2.9088e-12,  ..., 3.5084e-11, 2.2994e-12,
         9.7495e-12],
        [1.8993e-09, 9.6906e-10, 1.6019e-10,  ..., 1.4512e-09, 1.4267e-10,
         6.1487e-10],
        [4.2245e-10, 3.3311e-10, 3.6788e-11,  ..., 3.5638e-10, 3.5596e-11,
         1.6499e-10],
        [2.1985e-10, 1.2567e-10, 1.6982e-11,  ..., 1.6265e-10, 1.0996e-11,
         4.8276e-11]], device='cuda:0')
optimizer state dict: 75.0
lr: [1.67490383558044e-05, 1.67490383558044e-05]
scheduler_last_epoch: 75


Running epoch 0, step 600, batch 600
Sampled inputs[:2]: tensor([[    0,   328,   471,  ..., 11137,   679,  6585],
        [    0,   546,   360,  ...,    12,   461,  8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3190e-05, -1.4283e-05,  1.4266e-05,  ...,  5.2193e-05,
         -8.6634e-06, -7.7854e-05],
        [-2.0266e-06, -1.1846e-06,  1.0729e-06,  ..., -1.7956e-06,
         -1.1995e-06, -1.1846e-06],
        [-2.9653e-06, -1.7360e-06,  1.5721e-06,  ..., -2.6226e-06,
         -1.6391e-06, -1.7211e-06],
        [-4.0233e-06, -2.3395e-06,  2.1309e-06,  ..., -3.5614e-06,
         -2.2054e-06, -2.3395e-06],
        [-4.4107e-06, -2.5779e-06,  2.3544e-06,  ..., -3.9041e-06,
         -2.4885e-06, -2.5630e-06]], device='cuda:0')
Loss: 1.1026862859725952


Running epoch 0, step 601, batch 601
Sampled inputs[:2]: tensor([[    0, 38717,  1679,  ...,   472,   346,   462],
        [    0,  5105,   271,  ...,   308,  3056,  3640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0076e-05,  6.8681e-05, -4.8518e-05,  ...,  8.5283e-05,
         -7.9333e-05, -7.4575e-05],
        [-4.0233e-06, -2.3842e-06,  2.1532e-06,  ..., -3.5465e-06,
         -2.3693e-06, -2.3544e-06],
        [-5.9158e-06, -3.5018e-06,  3.1590e-06,  ..., -5.2005e-06,
         -3.2336e-06, -3.4198e-06],
        [-7.9870e-06, -4.7088e-06,  4.2617e-06,  ..., -7.0333e-06,
         -4.3213e-06, -4.6343e-06],
        [-8.7917e-06, -5.1707e-06,  4.7088e-06,  ..., -7.7039e-06,
         -4.8727e-06, -5.0664e-06]], device='cuda:0')
Loss: 1.0814636945724487


Running epoch 0, step 602, batch 602
Sampled inputs[:2]: tensor([[   0, 5159,  292,  ...,  772,  271, 3728],
        [   0, 3440, 5745,  ...,  360, 4998,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2813e-05, -8.4940e-05, -9.8692e-05,  ...,  9.7324e-05,
         -7.3291e-05, -1.5740e-04],
        [-5.9903e-06, -3.5688e-06,  3.1665e-06,  ..., -5.2303e-06,
         -3.3155e-06, -3.4496e-06],
        [-8.9556e-06, -5.3346e-06,  4.7162e-06,  ..., -7.8082e-06,
         -4.5970e-06, -5.1036e-06],
        [-1.2040e-05, -7.1526e-06,  6.3479e-06,  ..., -1.0535e-05,
         -6.1169e-06, -6.8843e-06],
        [-1.3232e-05, -7.8529e-06,  6.9886e-06,  ..., -1.1519e-05,
         -6.8992e-06, -7.5251e-06]], device='cuda:0')
Loss: 1.0735772848129272


Running epoch 0, step 603, batch 603
Sampled inputs[:2]: tensor([[    0,  1188,   278,  ...,   271,  8368,   292],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8981e-05, -1.0980e-04, -8.5633e-05,  ...,  4.3332e-05,
         -1.5048e-05, -1.2714e-05],
        [-8.1360e-06, -4.8280e-06,  4.2617e-06,  ..., -7.1079e-06,
         -4.8354e-06, -4.7386e-06],
        [-1.2070e-05, -7.1675e-06,  6.2883e-06,  ..., -1.0520e-05,
         -6.6534e-06, -6.9365e-06],
        [-1.5944e-05, -9.4473e-06,  8.3148e-06,  ..., -1.3933e-05,
         -8.6501e-06, -9.1791e-06],
        [-1.7792e-05, -1.0535e-05,  9.2983e-06,  ..., -1.5482e-05,
         -9.9540e-06, -1.0207e-05]], device='cuda:0')
Loss: 1.107818603515625


Running epoch 0, step 604, batch 604
Sampled inputs[:2]: tensor([[    0,  2013,    13,  ...,   271,   266,   908],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0580e-04, -1.0369e-04, -2.1900e-04,  ...,  4.7934e-05,
         -5.0761e-05, -6.5456e-05],
        [-1.0118e-05, -6.0424e-06,  5.3421e-06,  ..., -8.8885e-06,
         -5.9828e-06, -5.9009e-06],
        [-1.5020e-05, -8.9779e-06,  7.8902e-06,  ..., -1.3158e-05,
         -8.2403e-06, -8.6352e-06],
        [-1.9878e-05, -1.1846e-05,  1.0446e-05,  ..., -1.7464e-05,
         -1.0751e-05, -1.1444e-05],
        [-2.2084e-05, -1.3158e-05,  1.1638e-05,  ..., -1.9297e-05,
         -1.2279e-05, -1.2666e-05]], device='cuda:0')
Loss: 1.078800916671753


Running epoch 0, step 605, batch 605
Sampled inputs[:2]: tensor([[    0, 24086,   266,  ..., 18814,    19,   292],
        [    0,   607,   259,  ...,   271,   669,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4283e-05, -3.9541e-04, -1.7838e-04,  ...,  1.4082e-05,
          9.1171e-05, -1.3837e-04],
        [-1.2055e-05, -7.2047e-06,  6.3777e-06,  ..., -1.0543e-05,
         -7.0035e-06, -6.9886e-06],
        [-1.8060e-05, -1.0803e-05,  9.5069e-06,  ..., -1.5765e-05,
         -9.7305e-06, -1.0327e-05],
        [-2.3901e-05, -1.4246e-05,  1.2577e-05,  ..., -2.0906e-05,
         -1.2688e-05, -1.3679e-05],
        [-2.6554e-05, -1.5825e-05,  1.4022e-05,  ..., -2.3112e-05,
         -1.4514e-05, -1.5140e-05]], device='cuda:0')
Loss: 1.0903689861297607


Running epoch 0, step 606, batch 606
Sampled inputs[:2]: tensor([[    0,  1086,    14,  ...,   963,   292,   221],
        [    0,    13,  2615,  ..., 31594, 15867,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6904e-05, -3.5022e-04, -1.2756e-04,  ..., -5.4547e-05,
          6.3370e-05,  2.5914e-05],
        [-1.4141e-05, -8.4490e-06,  7.5474e-06,  ..., -1.2346e-05,
         -8.5235e-06, -8.2776e-06],
        [-2.1011e-05, -1.2569e-05,  1.1146e-05,  ..., -1.8314e-05,
         -1.1727e-05, -1.2122e-05],
        [-2.7701e-05, -1.6510e-05,  1.4678e-05,  ..., -2.4185e-05,
         -1.5222e-05, -1.5989e-05],
        [-3.0935e-05, -1.8433e-05,  1.6466e-05,  ..., -2.6867e-05,
         -1.7494e-05, -1.7792e-05]], device='cuda:0')
Loss: 1.0669310092926025


Running epoch 0, step 607, batch 607
Sampled inputs[:2]: tensor([[    0, 25938,   359,  ...,    36, 15859,   504],
        [    0, 11325,   278,  ...,   446,  1869,   642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9391e-05, -3.7574e-04, -1.7048e-04,  ..., -9.9148e-06,
          3.4823e-05,  9.8044e-06],
        [-1.6123e-05, -9.6336e-06,  8.6129e-06,  ..., -1.4059e-05,
         -9.6187e-06, -9.4026e-06],
        [-2.4006e-05, -1.4372e-05,  1.2740e-05,  ..., -2.0906e-05,
         -1.3247e-05, -1.3798e-05],
        [-3.1695e-05, -1.8910e-05,  1.6809e-05,  ..., -2.7657e-05,
         -1.7233e-05, -1.8224e-05],
        [-3.5286e-05, -2.1055e-05,  1.8805e-05,  ..., -3.0637e-05,
         -1.9744e-05, -2.0221e-05]], device='cuda:0')
Loss: 1.1003361940383911
Graident accumulation at epoch 0, step 607, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0028,  ..., -0.0025,  0.0231, -0.0196],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0025, -0.0342],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.6512e-05,  6.1640e-05, -1.6887e-04,  ...,  5.5284e-05,
         -2.3096e-05,  4.6016e-05],
        [-1.8857e-05, -1.2145e-05,  7.4152e-06,  ..., -1.6706e-05,
         -7.2575e-06, -1.1672e-05],
        [ 5.3419e-05,  3.9754e-05, -1.9861e-05,  ...,  5.3265e-05,
          1.9177e-05,  4.0128e-05],
        [-2.4817e-05, -1.5631e-05,  1.0230e-05,  ..., -2.1402e-05,
         -9.6482e-06, -1.5521e-05],
        [-3.2044e-05, -1.7323e-05,  1.2222e-05,  ..., -2.7148e-05,
         -1.1679e-05, -1.9692e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7492e-08, 2.0001e-08, 2.9809e-08,  ..., 2.1797e-08, 5.1324e-08,
         1.1077e-08],
        [4.9906e-11, 2.8284e-11, 2.9801e-12,  ..., 3.5247e-11, 2.3896e-12,
         9.8282e-12],
        [1.8980e-09, 9.6830e-10, 1.6019e-10,  ..., 1.4502e-09, 1.4270e-10,
         6.1445e-10],
        [4.2303e-10, 3.3313e-10, 3.7034e-11,  ..., 3.5679e-10, 3.5857e-11,
         1.6515e-10],
        [2.2088e-10, 1.2599e-10, 1.7319e-11,  ..., 1.6342e-10, 1.1375e-11,
         4.8637e-11]], device='cuda:0')
optimizer state dict: 76.0
lr: [1.6657307618927726e-05, 1.6657307618927726e-05]
scheduler_last_epoch: 76


Running epoch 0, step 608, batch 608
Sampled inputs[:2]: tensor([[   0, 1751,  287,  ..., 6079, 1059,  287],
        [   0,  292, 3030,  ..., 1231, 2156,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0922e-04, -1.8955e-04,  3.7940e-05,  ..., -1.2051e-04,
          4.8753e-05, -9.3166e-06],
        [-2.1309e-06, -1.2144e-06,  1.1697e-06,  ..., -1.7136e-06,
         -1.2144e-06, -1.1176e-06],
        [-3.0845e-06, -1.7732e-06,  1.6913e-06,  ..., -2.5034e-06,
         -1.6615e-06, -1.6093e-06],
        [-4.0531e-06, -2.3097e-06,  2.2054e-06,  ..., -3.2783e-06,
         -2.1607e-06, -2.1011e-06],
        [-4.7684e-06, -2.7418e-06,  2.6226e-06,  ..., -3.8445e-06,
         -2.6077e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.104149580001831


Running epoch 0, step 609, batch 609
Sampled inputs[:2]: tensor([[    0,   491, 10524,  ...,  2218,  5627,  4199],
        [    0,  1253,   287,  ...,  2988,    14,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0665e-04, -2.3422e-04,  8.0509e-05,  ..., -1.1544e-04,
          9.6062e-06, -7.3838e-05],
        [-4.2915e-06, -2.4065e-06,  2.3246e-06,  ..., -3.4869e-06,
         -2.5406e-06, -2.3097e-06],
        [-6.2585e-06, -3.5316e-06,  3.3826e-06,  ..., -5.1111e-06,
         -3.5092e-06, -3.3453e-06],
        [-8.2254e-06, -4.6045e-06,  4.4256e-06,  ..., -6.7055e-06,
         -4.5598e-06, -4.3809e-06],
        [-9.5963e-06, -5.4091e-06,  5.2005e-06,  ..., -7.8082e-06,
         -5.4538e-06, -5.1111e-06]], device='cuda:0')
Loss: 1.0923718214035034


Running epoch 0, step 610, batch 610
Sampled inputs[:2]: tensor([[    0,  6192,   266,  ...,  3318,  9872, 10931],
        [    0,   714,    14,  ...,  1501, 11397, 31940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1627e-04, -1.7054e-04, -5.3964e-05,  ..., -6.8379e-05,
         -1.0918e-04, -1.0511e-04],
        [-6.3181e-06, -3.5688e-06,  3.4496e-06,  ..., -5.1782e-06,
         -3.6433e-06, -3.3751e-06],
        [-9.3132e-06, -5.2750e-06,  5.0664e-06,  ..., -7.6592e-06,
         -5.0515e-06, -4.9248e-06],
        [-1.2279e-05, -6.9141e-06,  6.6608e-06,  ..., -1.0103e-05,
         -6.5863e-06, -6.4820e-06],
        [-1.4186e-05, -8.0317e-06,  7.7337e-06,  ..., -1.1623e-05,
         -7.8082e-06, -7.4804e-06]], device='cuda:0')
Loss: 1.080522060394287


Running epoch 0, step 611, batch 611
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0,   870,   278,  ...,   478,   401,   897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3804e-04, -1.1838e-04, -9.7629e-05,  ..., -6.7696e-05,
         -1.0918e-04, -1.0394e-04],
        [-8.3297e-06, -4.7609e-06,  4.5747e-06,  ..., -6.9439e-06,
         -4.7907e-06, -4.4927e-06],
        [-1.2249e-05, -7.0184e-06,  6.7055e-06,  ..., -1.0237e-05,
         -6.6012e-06, -6.5342e-06],
        [-1.6212e-05, -9.2387e-06,  8.8364e-06,  ..., -1.3545e-05,
         -8.6278e-06, -8.6278e-06],
        [-1.8597e-05, -1.0639e-05,  1.0192e-05,  ..., -1.5467e-05,
         -1.0148e-05, -9.8795e-06]], device='cuda:0')
Loss: 1.0939854383468628


Running epoch 0, step 612, batch 612
Sampled inputs[:2]: tensor([[    0,  1234,   408,  ...,   292, 17323,   221],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0024e-04, -4.9210e-05, -2.2201e-04,  ..., -5.5783e-05,
         -6.4304e-05, -1.4010e-05],
        [-1.0327e-05, -5.9009e-06,  5.6997e-06,  ..., -8.6650e-06,
         -6.0797e-06, -5.6103e-06],
        [-1.5110e-05, -8.6576e-06,  8.3148e-06,  ..., -1.2696e-05,
         -8.2701e-06, -8.0988e-06],
        [-2.0087e-05, -1.1444e-05,  1.1012e-05,  ..., -1.6868e-05,
         -1.0863e-05, -1.0744e-05],
        [-2.3007e-05, -1.3158e-05,  1.2681e-05,  ..., -1.9222e-05,
         -1.2740e-05, -1.2279e-05]], device='cuda:0')
Loss: 1.055306077003479


Running epoch 0, step 613, batch 613
Sampled inputs[:2]: tensor([[    0,   344,  2183,  ...,    14,   759,   596],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0317e-04,  1.2149e-04, -3.3222e-04,  ..., -4.4610e-05,
         -1.3849e-04,  4.7675e-05],
        [-1.2338e-05, -7.0333e-06,  6.8322e-06,  ..., -1.0453e-05,
         -7.4580e-06, -6.8173e-06],
        [-1.7941e-05, -1.0259e-05,  9.9093e-06,  ..., -1.5199e-05,
         -1.0066e-05, -9.7752e-06],
        [-2.3901e-05, -1.3590e-05,  1.3158e-05,  ..., -2.0251e-05,
         -1.3277e-05, -1.3009e-05],
        [-2.7359e-05, -1.5602e-05,  1.5125e-05,  ..., -2.3037e-05,
         -1.5527e-05, -1.4842e-05]], device='cuda:0')
Loss: 1.083614468574524


Running epoch 0, step 614, batch 614
Sampled inputs[:2]: tensor([[    0,   266,  1176,  ...,   199, 17791,  3662],
        [    0,  1356,   634,  ...,  6604,   634,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4957e-04, -6.1780e-06, -4.3635e-04,  ..., -5.5176e-05,
         -1.8392e-04, -1.1552e-04],
        [-1.4350e-05, -8.2329e-06,  7.9796e-06,  ..., -1.2144e-05,
         -8.6203e-06, -7.9349e-06],
        [-2.0891e-05, -1.2033e-05,  1.1601e-05,  ..., -1.7688e-05,
         -1.1630e-05, -1.1392e-05],
        [-2.7865e-05, -1.5974e-05,  1.5438e-05,  ..., -2.3589e-05,
         -1.5348e-05, -1.5184e-05],
        [-3.1799e-05, -1.8254e-05,  1.7673e-05,  ..., -2.6762e-05,
         -1.7911e-05, -1.7256e-05]], device='cuda:0')
Loss: 1.0610891580581665


Running epoch 0, step 615, batch 615
Sampled inputs[:2]: tensor([[   0, 1197,  729,  ...,  674,  369, 8222],
        [   0, 3152, 1385,  ..., 1403,  518, 2088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8114e-04,  3.2731e-06, -4.5284e-04,  ..., -9.0143e-05,
         -1.5461e-04, -2.4498e-04],
        [-1.6332e-05, -9.3505e-06,  9.1046e-06,  ..., -1.3806e-05,
         -9.6038e-06, -8.9705e-06],
        [ 5.6035e-04,  2.9643e-04, -3.9093e-04,  ...,  4.4169e-04,
          2.8057e-04,  2.1499e-04],
        [-3.2008e-05, -1.8314e-05,  1.7777e-05,  ..., -2.7090e-05,
         -1.7241e-05, -1.7345e-05],
        [-3.6448e-05, -2.0891e-05,  2.0310e-05,  ..., -3.0696e-05,
         -2.0102e-05, -1.9684e-05]], device='cuda:0')
Loss: 1.0991535186767578
Graident accumulation at epoch 0, step 615, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0028,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0096, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0287, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0747e-05,  5.5803e-05, -1.9726e-04,  ...,  4.0741e-05,
         -3.6247e-05,  1.6917e-05],
        [-1.8605e-05, -1.1865e-05,  7.5842e-06,  ..., -1.6416e-05,
         -7.4922e-06, -1.1402e-05],
        [ 1.0411e-04,  6.5421e-05, -5.6968e-05,  ...,  9.2107e-05,
          4.5316e-05,  5.7614e-05],
        [-2.5536e-05, -1.5899e-05,  1.0985e-05,  ..., -2.1971e-05,
         -1.0407e-05, -1.5704e-05],
        [-3.2485e-05, -1.7679e-05,  1.3031e-05,  ..., -2.7503e-05,
         -1.2521e-05, -1.9691e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7467e-08, 1.9981e-08, 2.9984e-08,  ..., 2.1783e-08, 5.1297e-08,
         1.1126e-08],
        [5.0123e-11, 2.8344e-11, 3.0600e-12,  ..., 3.5402e-11, 2.4795e-12,
         9.8988e-12],
        [2.2101e-09, 1.0552e-09, 3.1286e-10,  ..., 1.6438e-09, 2.2128e-10,
         6.6005e-10],
        [4.2363e-10, 3.3314e-10, 3.7313e-11,  ..., 3.5716e-10, 3.6119e-11,
         1.6529e-10],
        [2.2199e-10, 1.2630e-10, 1.7714e-11,  ..., 1.6420e-10, 1.1768e-11,
         4.8976e-11]], device='cuda:0')
optimizer state dict: 77.0
lr: [1.656455958733442e-05, 1.656455958733442e-05]
scheduler_last_epoch: 77


Running epoch 0, step 616, batch 616
Sampled inputs[:2]: tensor([[   0,  365, 1110,  ..., 4130,  221,  199],
        [   0, 2296,  446,  ..., 2937,  287, 2795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9467e-05, -6.5980e-05,  6.9573e-05,  ..., -3.0850e-05,
          5.9184e-07,  1.5837e-05],
        [-2.0713e-06, -1.1623e-06,  1.1846e-06,  ..., -1.6689e-06,
         -1.1325e-06, -1.0952e-06],
        [-3.0100e-06, -1.7062e-06,  1.7136e-06,  ..., -2.4438e-06,
         -1.5274e-06, -1.5795e-06],
        [-4.0531e-06, -2.2799e-06,  2.3097e-06,  ..., -3.2783e-06,
         -2.0266e-06, -2.1160e-06],
        [-4.7386e-06, -2.6673e-06,  2.6971e-06,  ..., -3.8147e-06,
         -2.4289e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.0818520784378052


Running epoch 0, step 617, batch 617
Sampled inputs[:2]: tensor([[   0, 2341, 7956,  ..., 2355,  413,   72],
        [   0,   12,  689,  ..., 1110, 1712, 2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6477e-05, -1.0901e-04,  9.5225e-05,  ..., -4.4913e-05,
          1.8199e-05,  2.8673e-05],
        [-4.1276e-06, -2.3469e-06,  2.2873e-06,  ..., -3.3006e-06,
         -2.1607e-06, -2.1085e-06],
        [-6.1095e-06, -3.4943e-06,  3.3677e-06,  ..., -4.9025e-06,
         -2.9504e-06, -3.0845e-06],
        [ 8.9433e-05,  4.1894e-05, -3.4200e-05,  ...,  6.0026e-05,
          3.6607e-05,  3.9066e-05],
        [-9.5963e-06, -5.4687e-06,  5.2899e-06,  ..., -7.6592e-06,
         -4.7088e-06, -4.8280e-06]], device='cuda:0')
Loss: 1.08930242061615


Running epoch 0, step 618, batch 618
Sampled inputs[:2]: tensor([[   0,  266, 1634,  ...,  310, 1372,  287],
        [   0,  858,   13,  ..., 2253,  847,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5724e-04, -6.1607e-05,  3.1047e-05,  ..., -9.8729e-05,
          1.0730e-04, -4.0258e-05],
        [-6.2734e-06, -3.5986e-06,  3.5390e-06,  ..., -5.1111e-06,
         -3.5912e-06, -3.2932e-06],
        [-9.0897e-06, -5.2229e-06,  5.1036e-06,  ..., -7.3910e-06,
         -4.7907e-06, -4.7088e-06],
        [ 8.5439e-05,  3.9584e-05, -3.1876e-05,  ...,  5.6673e-05,
          3.4163e-05,  3.6890e-05],
        [-1.4246e-05, -8.1509e-06,  8.0019e-06,  ..., -1.1533e-05,
         -7.5847e-06, -7.3314e-06]], device='cuda:0')
Loss: 1.095758318901062


Running epoch 0, step 619, batch 619
Sampled inputs[:2]: tensor([[    0,  1431,   221,  ...,   756,   409,   275],
        [    0, 26074,   486,  ...,  2314,   266,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7623e-04, -2.7220e-05,  5.9437e-06,  ..., -8.0316e-05,
          1.4151e-04, -3.9054e-05],
        [-8.2701e-06, -4.7758e-06,  4.6641e-06,  ..., -6.7428e-06,
         -4.5970e-06, -4.2990e-06],
        [-1.2159e-05, -7.0333e-06,  6.8396e-06,  ..., -9.8944e-06,
         -6.2063e-06, -6.2361e-06],
        [ 8.1386e-05,  3.7200e-05, -2.9581e-05,  ...,  5.3335e-05,
          3.2308e-05,  3.4863e-05],
        [-1.8865e-05, -1.0863e-05,  1.0610e-05,  ..., -1.5289e-05,
         -9.7454e-06, -9.6112e-06]], device='cuda:0')
Loss: 1.0722345113754272


Running epoch 0, step 620, batch 620
Sampled inputs[:2]: tensor([[    0,    13, 37178,  ...,  1692,  3287, 10652],
        [    0,   598,   278,  ...,   437,   266,  2388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2453e-04,  1.0620e-04, -2.3606e-05,  ..., -3.9626e-05,
          9.8734e-05,  2.1432e-05],
        [-1.0371e-05, -5.9679e-06,  5.8338e-06,  ..., -8.4490e-06,
         -5.7742e-06, -5.3644e-06],
        [-1.5229e-05, -8.7768e-06,  8.5309e-06,  ..., -1.2368e-05,
         -7.7933e-06, -7.7635e-06],
        [ 7.7184e-05,  3.4816e-05, -2.7256e-05,  ...,  4.9938e-05,
          3.0147e-05,  3.2762e-05],
        [-2.3663e-05, -1.3590e-05,  1.3262e-05,  ..., -1.9163e-05,
         -1.2264e-05, -1.1981e-05]], device='cuda:0')
Loss: 1.1007100343704224


Running epoch 0, step 621, batch 621
Sampled inputs[:2]: tensor([[    0,   790, 43134,  ...,   446,   381,  1034],
        [    0,  1550,  2013,  ...,  9970,   638,  6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9955e-04,  6.8953e-05, -2.2484e-05,  ..., -7.4416e-05,
          6.9393e-05,  1.2922e-06],
        [-1.2398e-05, -7.1451e-06,  6.9812e-06,  ..., -1.0081e-05,
         -6.8024e-06, -6.3926e-06],
        [-1.8284e-05, -1.0557e-05,  1.0267e-05,  ..., -1.4827e-05,
         -9.2164e-06, -9.3058e-06],
        [ 7.3041e-05,  3.2402e-05, -2.4902e-05,  ...,  4.6570e-05,
          2.8225e-05,  3.0661e-05],
        [-2.8431e-05, -1.6361e-05,  1.5974e-05,  ..., -2.3007e-05,
         -1.4529e-05, -1.4365e-05]], device='cuda:0')
Loss: 1.077500820159912


Running epoch 0, step 622, batch 622
Sampled inputs[:2]: tensor([[    0,   365,   984,  ..., 18562,  4237, 31813],
        [    0,    14,   357,  ...,    30,   287,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2941e-04,  1.0898e-04,  9.9972e-05,  ..., -1.8314e-04,
          2.5803e-04,  9.2019e-05],
        [-1.4409e-05, -8.3819e-06,  8.1286e-06,  ..., -1.1742e-05,
         -7.7412e-06, -7.4208e-06],
        [ 2.7970e-04,  1.5500e-04, -1.6802e-04,  ...,  1.7713e-04,
          6.0540e-05, -2.3388e-06],
        [ 6.8929e-05,  2.9869e-05, -2.2577e-05,  ...,  4.3143e-05,
          2.6481e-05,  2.8575e-05],
        [-3.3140e-05, -1.9267e-05,  1.8641e-05,  ..., -2.6911e-05,
         -1.6600e-05, -1.6749e-05]], device='cuda:0')
Loss: 1.104464054107666


Running epoch 0, step 623, batch 623
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0,    14,  6707,  ..., 17771,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4826e-04,  3.4079e-04, -2.1541e-05,  ..., -5.1105e-05,
          1.5574e-04,  7.1638e-05],
        [-1.6496e-05, -9.5442e-06,  9.3654e-06,  ..., -1.3560e-05,
         -9.3579e-06, -8.7097e-06],
        [ 2.7690e-04,  1.5344e-04, -1.6636e-04,  ...,  1.7472e-04,
          5.8529e-05, -4.0301e-06],
        [ 6.5159e-05,  2.7768e-05, -2.0327e-05,  ...,  3.9880e-05,
          2.3769e-05,  2.6280e-05],
        [-3.7491e-05, -2.1681e-05,  2.1219e-05,  ..., -3.0637e-05,
         -1.9729e-05, -1.9357e-05]], device='cuda:0')
Loss: 1.0942389965057373
Graident accumulation at epoch 0, step 623, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0028,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0291, -0.0078,  0.0037,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0160,  0.0151, -0.0276,  ...,  0.0288, -0.0148, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0846e-05,  8.4302e-05, -1.7969e-04,  ...,  3.1557e-05,
         -1.7049e-05,  2.2389e-05],
        [-1.8394e-05, -1.1633e-05,  7.7623e-06,  ..., -1.6130e-05,
         -7.6787e-06, -1.1132e-05],
        [ 1.2139e-04,  7.4223e-05, -6.7908e-05,  ...,  1.0037e-04,
          4.6637e-05,  5.1450e-05],
        [-1.6466e-05, -1.1532e-05,  7.8534e-06,  ..., -1.5786e-05,
         -6.9898e-06, -1.1505e-05],
        [-3.2985e-05, -1.8080e-05,  1.3849e-05,  ..., -2.7816e-05,
         -1.3242e-05, -1.9657e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7531e-08, 2.0077e-08, 2.9955e-08,  ..., 2.1764e-08, 5.1270e-08,
         1.1120e-08],
        [5.0344e-11, 2.8406e-11, 3.1447e-12,  ..., 3.5550e-11, 2.5646e-12,
         9.9648e-12],
        [2.2845e-09, 1.0777e-09, 3.4022e-10,  ..., 1.6727e-09, 2.2448e-10,
         6.5941e-10],
        [4.2745e-10, 3.3357e-10, 3.7689e-11,  ..., 3.5840e-10, 3.6647e-11,
         1.6581e-10],
        [2.2317e-10, 1.2664e-10, 1.8146e-11,  ..., 1.6498e-10, 1.2145e-11,
         4.9301e-11]], device='cuda:0')
optimizer state dict: 78.0
lr: [1.6470808433733317e-05, 1.6470808433733317e-05]
scheduler_last_epoch: 78


Running epoch 0, step 624, batch 624
Sampled inputs[:2]: tensor([[    0, 35449,   824,  ...,   278, 30449,  3659],
        [    0,  6508,  4305,  ...,   806,  3888,  4431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1756e-04,  2.3834e-04, -1.8326e-06,  ...,  6.0822e-05,
         -6.8945e-05,  5.5297e-05],
        [-2.1756e-06, -1.2219e-06,  1.1176e-06,  ..., -1.6540e-06,
         -1.1772e-06, -1.0580e-06],
        [-3.2932e-06, -1.8626e-06,  1.6913e-06,  ..., -2.5183e-06,
         -1.6540e-06, -1.5795e-06],
        [-4.2617e-06, -2.4140e-06,  2.2054e-06,  ..., -3.2634e-06,
         -2.1160e-06, -2.0564e-06],
        [-5.1856e-06, -2.9057e-06,  2.6524e-06,  ..., -3.9339e-06,
         -2.6524e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.0582940578460693


Running epoch 0, step 625, batch 625
Sampled inputs[:2]: tensor([[    0,   380,   333,  ...,  8127,   504,   679],
        [    0,   292, 17181,  ...,   634,  5039,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3017e-04,  3.7194e-04, -1.0356e-04,  ...,  1.6722e-04,
         -1.3542e-04,  1.0995e-04],
        [-4.1872e-06, -2.4289e-06,  2.2799e-06,  ..., -3.3230e-06,
         -2.2352e-06, -2.0936e-06],
        [-6.2138e-06, -3.6210e-06,  3.3900e-06,  ..., -4.9323e-06,
         -3.0547e-06, -3.0622e-06],
        [-8.3148e-06, -4.8429e-06,  4.5598e-06,  ..., -6.6161e-06,
         -4.0531e-06, -4.1127e-06],
        [-9.7156e-06, -5.6177e-06,  5.2899e-06,  ..., -7.6592e-06,
         -4.8727e-06, -4.7535e-06]], device='cuda:0')
Loss: 1.0668928623199463


Running epoch 0, step 626, batch 626
Sampled inputs[:2]: tensor([[    0,  1039,   259,  ...,   221,   685,   546],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2831e-04,  4.7343e-04, -1.4314e-04,  ...,  2.2197e-04,
         -1.7917e-04,  1.3041e-04],
        [-6.2585e-06, -3.6210e-06,  3.4347e-06,  ..., -4.9472e-06,
         -3.2708e-06, -3.1367e-06],
        [-9.3132e-06, -5.4166e-06,  5.1185e-06,  ..., -7.3612e-06,
         -4.4852e-06, -4.6119e-06],
        [-1.2368e-05, -7.1824e-06,  6.8098e-06,  ..., -9.8050e-06,
         -5.9009e-06, -6.1393e-06],
        [-1.4484e-05, -8.3745e-06,  7.9572e-06,  ..., -1.1384e-05,
         -7.1079e-06, -7.1228e-06]], device='cuda:0')
Loss: 1.0815081596374512


Running epoch 0, step 627, batch 627
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 8271,  365,  437],
        [   0, 7432,  287,  ...,   12,  461, 2652]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3070e-04,  6.4183e-04, -1.1642e-04,  ...,  2.3935e-04,
         -2.4497e-04,  1.7299e-04],
        [-8.3297e-06, -4.8950e-06,  4.6045e-06,  ..., -6.6459e-06,
         -4.3958e-06, -4.1723e-06],
        [-1.2279e-05, -7.2494e-06,  6.7949e-06,  ..., -9.7901e-06,
         -5.9679e-06, -6.0648e-06],
        [-1.6451e-05, -9.6858e-06,  9.1195e-06,  ..., -1.3158e-05,
         -7.9423e-06, -8.1509e-06],
        [-1.9193e-05, -1.1250e-05,  1.0610e-05,  ..., -1.5199e-05,
         -9.4920e-06, -9.4175e-06]], device='cuda:0')
Loss: 1.0959532260894775


Running epoch 0, step 628, batch 628
Sampled inputs[:2]: tensor([[    0, 15666,   609,  ...,   527,  4486,     9],
        [    0,   367,  1236,  ...,   344,   292,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9481e-04,  8.6438e-04, -1.8500e-04,  ...,  3.4167e-04,
         -3.2585e-04,  2.4532e-04],
        [-1.0371e-05, -6.1393e-06,  5.7817e-06,  ..., -8.2999e-06,
         -5.4315e-06, -5.2080e-06],
        [ 7.2448e-05,  7.0245e-05, -2.4354e-05,  ...,  7.5256e-05,
          5.9685e-06,  4.1408e-05],
        [-2.0623e-05, -1.2219e-05,  1.1533e-05,  ..., -1.6525e-05,
         -9.8497e-06, -1.0237e-05],
        [-2.3812e-05, -1.4067e-05,  1.3292e-05,  ..., -1.8924e-05,
         -1.1653e-05, -1.1712e-05]], device='cuda:0')
Loss: 1.0730173587799072


Running epoch 0, step 629, batch 629
Sampled inputs[:2]: tensor([[    0,  4441,  1821,  ...,   642,  2310,    14],
        [    0,  1869,   596,  ..., 13055, 17051,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8148e-04,  8.3024e-04, -1.6447e-04,  ...,  4.0282e-04,
         -3.6380e-04,  2.5796e-04],
        [-1.2398e-05, -7.3984e-06,  6.9067e-06,  ..., -9.8720e-06,
         -6.3479e-06, -6.1616e-06],
        [ 6.9289e-05,  6.8278e-05, -2.2595e-05,  ...,  7.2797e-05,
          4.6497e-06,  3.9940e-05],
        [-2.4885e-05, -1.4856e-05,  1.3903e-05,  ..., -1.9848e-05,
         -1.1615e-05, -1.2219e-05],
        [-2.8729e-05, -1.7107e-05,  1.6034e-05,  ..., -2.2724e-05,
         -1.3739e-05, -1.3977e-05]], device='cuda:0')
Loss: 1.071365237236023


Running epoch 0, step 630, batch 630
Sampled inputs[:2]: tensor([[    0,   287,   266,  ...,   333,   199,  3217],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9890e-04,  9.5857e-04, -1.5724e-04,  ...,  3.6416e-04,
         -3.3517e-04,  3.7794e-04],
        [-1.4454e-05, -8.7097e-06,  8.0690e-06,  ..., -1.1519e-05,
         -7.3761e-06, -7.1377e-06],
        [ 6.6264e-05,  6.6355e-05, -2.0897e-05,  ...,  7.0398e-05,
          3.2937e-06,  3.8532e-05],
        [-2.8938e-05, -1.7434e-05,  1.6198e-05,  ..., -2.3097e-05,
         -1.3426e-05, -1.4119e-05],
        [-3.3438e-05, -2.0102e-05,  1.8701e-05,  ..., -2.6464e-05,
         -1.5885e-05, -1.6153e-05]], device='cuda:0')
Loss: 1.0781244039535522


Running epoch 0, step 631, batch 631
Sampled inputs[:2]: tensor([[    0, 24414,  4865,  ...,  8720,   344,  1566],
        [    0,  2698,   221,  ...,  8352,  5680,   782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1342e-04,  9.9771e-04, -1.5127e-04,  ...,  3.9167e-04,
         -3.9779e-04,  3.6960e-04],
        [-1.6436e-05, -9.9540e-06,  9.2685e-06,  ..., -1.3098e-05,
         -8.3521e-06, -8.1733e-06],
        [ 6.3329e-05,  6.4500e-05, -1.9116e-05,  ...,  6.8059e-05,
          1.9749e-06,  3.7019e-05],
        [-3.3051e-05, -2.0027e-05,  1.8701e-05,  ..., -2.6390e-05,
         -1.5259e-05, -1.6250e-05],
        [-3.8028e-05, -2.2978e-05,  2.1473e-05,  ..., -3.0100e-05,
         -1.7986e-05, -1.8492e-05]], device='cuda:0')
Loss: 1.0576673746109009
Graident accumulation at epoch 0, step 631, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0028,  ..., -0.0024,  0.0231, -0.0195],
        [ 0.0290, -0.0078,  0.0037,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0405,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0151, -0.0277,  ...,  0.0288, -0.0147, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.1103e-05,  1.7564e-04, -1.7685e-04,  ...,  6.7568e-05,
         -5.5123e-05,  5.7110e-05],
        [-1.8198e-05, -1.1465e-05,  7.9129e-06,  ..., -1.5827e-05,
         -7.7461e-06, -1.0837e-05],
        [ 1.1558e-04,  7.3251e-05, -6.3029e-05,  ...,  9.7137e-05,
          4.2171e-05,  5.0007e-05],
        [-1.8125e-05, -1.2382e-05,  8.9381e-06,  ..., -1.6846e-05,
         -7.8167e-06, -1.1980e-05],
        [-3.3489e-05, -1.8569e-05,  1.4612e-05,  ..., -2.8044e-05,
         -1.3716e-05, -1.9541e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8135e-08, 2.1052e-08, 2.9948e-08,  ..., 2.1896e-08, 5.1377e-08,
         1.1245e-08],
        [5.0564e-11, 2.8477e-11, 3.2274e-12,  ..., 3.5686e-11, 2.6318e-12,
         1.0022e-11],
        [2.2862e-09, 1.0808e-09, 3.4025e-10,  ..., 1.6757e-09, 2.2426e-10,
         6.6012e-10],
        [4.2812e-10, 3.3364e-10, 3.8001e-11,  ..., 3.5874e-10, 3.6844e-11,
         1.6591e-10],
        [2.2439e-10, 1.2704e-10, 1.8589e-11,  ..., 1.6572e-10, 1.2456e-11,
         4.9594e-11]], device='cuda:0')
optimizer state dict: 79.0
lr: [1.6376068484119055e-05, 1.6376068484119055e-05]
scheduler_last_epoch: 79


Running epoch 0, step 632, batch 632
Sampled inputs[:2]: tensor([[   0, 2680,  271,  ..., 4971,  278,  266],
        [   0, 1760,    9,  ...,  278, 6607,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1947e-05,  4.8608e-05, -4.6536e-05,  ...,  1.5807e-05,
         -7.1930e-05,  2.1871e-05],
        [-2.0713e-06, -1.3262e-06,  1.2517e-06,  ..., -1.6466e-06,
         -1.3784e-06, -1.1846e-06],
        [-2.9355e-06, -1.8850e-06,  1.7732e-06,  ..., -2.3097e-06,
         -1.7509e-06, -1.6317e-06],
        [-3.9339e-06, -2.5183e-06,  2.3693e-06,  ..., -3.0994e-06,
         -2.3097e-06, -2.1905e-06],
        [-4.5896e-06, -2.9206e-06,  2.7716e-06,  ..., -3.5912e-06,
         -2.7418e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.0559000968933105


Running epoch 0, step 633, batch 633
Sampled inputs[:2]: tensor([[   0, 2379,   13,  ...,  287,  259, 2193],
        [   0, 2734, 2703,  ..., 7851,  280, 1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1937e-05,  7.3438e-05, -1.6314e-05,  ..., -4.9044e-06,
         -1.1381e-04, -2.1129e-05],
        [-4.1127e-06, -2.6375e-06,  2.4363e-06,  ..., -3.2261e-06,
         -2.5406e-06, -2.2352e-06],
        [-5.9456e-06, -3.8072e-06,  3.5241e-06,  ..., -4.6194e-06,
         -3.3006e-06, -3.1516e-06],
        [-7.9274e-06, -5.0664e-06,  4.6939e-06,  ..., -6.1840e-06,
         -4.3660e-06, -4.2170e-06],
        [-9.3281e-06, -5.9307e-06,  5.5283e-06,  ..., -7.2122e-06,
         -5.2303e-06, -4.8876e-06]], device='cuda:0')
Loss: 1.0870147943496704


Running epoch 0, step 634, batch 634
Sampled inputs[:2]: tensor([[    0,    17,  2736,  ...,   352,   422,    13],
        [    0,    15, 14761,  ...,   278,  3218,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6339e-05,  4.9102e-05,  7.2628e-05,  ..., -4.7328e-05,
          6.1740e-06, -3.3787e-05],
        [-6.2287e-06, -3.9935e-06,  3.7029e-06,  ..., -4.8876e-06,
         -3.9712e-06, -3.4571e-06],
        [-8.8364e-06, -5.6624e-06,  5.2527e-06,  ..., -6.8694e-06,
         -5.0589e-06, -4.7833e-06],
        [-1.1861e-05, -7.5698e-06,  7.0482e-06,  ..., -9.2387e-06,
         -6.7502e-06, -6.4373e-06],
        [-1.3918e-05, -8.8662e-06,  8.2850e-06,  ..., -1.0774e-05,
         -8.0615e-06, -7.4655e-06]], device='cuda:0')
Loss: 1.0607492923736572


Running epoch 0, step 635, batch 635
Sampled inputs[:2]: tensor([[    0,   300, 11040,  ...,   266,  1736,  3487],
        [    0,   792,   287,  ...,   706,  9751,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1225e-06,  7.7032e-05, -1.2909e-05,  ...,  2.9905e-05,
         -1.0310e-04,  5.8023e-06],
        [-8.3297e-06, -5.3123e-06,  4.9025e-06,  ..., -6.4969e-06,
         -5.0962e-06, -4.4778e-06],
        [-1.1906e-05, -7.5847e-06,  7.0035e-06,  ..., -9.2089e-06,
         -6.5491e-06, -6.2585e-06],
        [-1.5974e-05, -1.0148e-05,  9.4026e-06,  ..., -1.2383e-05,
         -8.7321e-06, -8.4192e-06],
        [-1.8686e-05, -1.1861e-05,  1.1012e-05,  ..., -1.4395e-05,
         -1.0431e-05, -9.7454e-06]], device='cuda:0')
Loss: 1.0709556341171265


Running epoch 0, step 636, batch 636
Sampled inputs[:2]: tensor([[    0,   531,  9804,  ...,  1027,   360,  1576],
        [    0, 25241,   717,  ...,   413,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9611e-05,  1.5791e-04, -2.2520e-05,  ...,  5.1305e-05,
         -1.2379e-04,  5.1688e-05],
        [-1.0327e-05, -6.6087e-06,  6.0573e-06,  ..., -8.0541e-06,
         -6.1989e-06, -5.5283e-06],
        [-1.4946e-05, -9.5367e-06,  8.7544e-06,  ..., -1.1563e-05,
         -8.0764e-06, -7.8231e-06],
        [-2.0057e-05, -1.2770e-05,  1.1772e-05,  ..., -1.5572e-05,
         -1.0774e-05, -1.0535e-05],
        [-2.3484e-05, -1.4946e-05,  1.3784e-05,  ..., -1.8120e-05,
         -1.2875e-05, -1.2204e-05]], device='cuda:0')
Loss: 1.0880683660507202


Running epoch 0, step 637, batch 637
Sampled inputs[:2]: tensor([[   0,  992,  409,  ..., 5843,  344,  259],
        [   0,  292,  380,  ..., 6156,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6405e-05,  2.2325e-04, -2.6253e-04,  ...,  1.1330e-04,
         -1.8709e-04,  6.7221e-05],
        [-1.2398e-05, -7.8753e-06,  7.2345e-06,  ..., -9.6485e-06,
         -7.5251e-06, -6.6459e-06],
        [-1.7896e-05, -1.1340e-05,  1.0438e-05,  ..., -1.3813e-05,
         -9.7603e-06, -9.3654e-06],
        [-2.4080e-05, -1.5214e-05,  1.4052e-05,  ..., -1.8626e-05,
         -1.3053e-05, -1.2636e-05],
        [-2.8193e-05, -1.7822e-05,  1.6481e-05,  ..., -2.1696e-05,
         -1.5602e-05, -1.4648e-05]], device='cuda:0')
Loss: 1.0781779289245605


Running epoch 0, step 638, batch 638
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,  9161,   300,  4460],
        [    0,   287, 17044,  ...,   496,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0546e-05,  3.4755e-04, -3.2071e-04,  ...,  1.1855e-04,
         -3.6558e-04,  6.6044e-05],
        [-1.4439e-05, -9.1568e-06,  8.4415e-06,  ..., -1.1235e-05,
         -8.6576e-06, -7.7635e-06],
        [-2.0862e-05, -1.3195e-05,  1.2197e-05,  ..., -1.6108e-05,
         -1.1250e-05, -1.0960e-05],
        [-2.8193e-05, -1.7792e-05,  1.6510e-05,  ..., -2.1830e-05,
         -1.5125e-05, -1.4871e-05],
        [-3.2902e-05, -2.0772e-05,  1.9297e-05,  ..., -2.5332e-05,
         -1.8016e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.0861608982086182


Running epoch 0, step 639, batch 639
Sampled inputs[:2]: tensor([[   0, 7180,  266,  ..., 1805,   12,  221],
        [   0,  765,  292,  ...,  623,   12, 7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6586e-04,  5.3807e-04, -6.2128e-04,  ...,  2.1369e-04,
         -5.6158e-04, -6.6930e-05],
        [-1.6481e-05, -1.0371e-05,  9.6560e-06,  ..., -1.2919e-05,
         -1.0036e-05, -9.0376e-06],
        [-2.3738e-05, -1.4901e-05,  1.3918e-05,  ..., -1.8448e-05,
         -1.2994e-05, -1.2703e-05],
        [-3.2127e-05, -2.0117e-05,  1.8880e-05,  ..., -2.5034e-05,
         -1.7479e-05, -1.7270e-05],
        [-3.7313e-05, -2.3380e-05,  2.1935e-05,  ..., -2.8893e-05,
         -2.0698e-05, -1.9819e-05]], device='cuda:0')
Loss: 1.071964979171753
Graident accumulation at epoch 0, step 639, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0028,  ..., -0.0024,  0.0232, -0.0195],
        [ 0.0290, -0.0079,  0.0037,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0151, -0.0277,  ...,  0.0288, -0.0147, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.8579e-05,  2.1189e-04, -2.2129e-04,  ...,  8.2180e-05,
         -1.0577e-04,  4.4706e-05],
        [-1.8026e-05, -1.1356e-05,  8.0872e-06,  ..., -1.5536e-05,
         -7.9751e-06, -1.0657e-05],
        [ 1.0165e-04,  6.4436e-05, -5.5334e-05,  ...,  8.5578e-05,
          3.6655e-05,  4.3736e-05],
        [-1.9525e-05, -1.3155e-05,  9.9323e-06,  ..., -1.7665e-05,
         -8.7829e-06, -1.2509e-05],
        [-3.3872e-05, -1.9050e-05,  1.5344e-05,  ..., -2.8129e-05,
         -1.4415e-05, -1.9569e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8105e-08, 2.1321e-08, 3.0304e-08,  ..., 2.1920e-08, 5.1641e-08,
         1.1238e-08],
        [5.0785e-11, 2.8556e-11, 3.3174e-12,  ..., 3.5818e-11, 2.7298e-12,
         1.0093e-11],
        [2.2845e-09, 1.0799e-09, 3.4010e-10,  ..., 1.6743e-09, 2.2421e-10,
         6.5962e-10],
        [4.2872e-10, 3.3371e-10, 3.8319e-11,  ..., 3.5900e-10, 3.7112e-11,
         1.6604e-10],
        [2.2556e-10, 1.2746e-10, 1.9052e-11,  ..., 1.6639e-10, 1.2872e-11,
         4.9937e-11]], device='cuda:0')
optimizer state dict: 80.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 80
Epoch 0 | Batch 639/1048 | Training PPL: 5301.366165365151 | time 30.946584939956665
Saving checkpoint at epoch 0, step 639, batch 639
Epoch 0 | Validation PPL: 8.542193819330398 | Learning rate: 1.628035421558293e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_639, AFTER epoch 0, step 639


Running epoch 0, step 640, batch 640
Sampled inputs[:2]: tensor([[    0,   270,   472,  ...,   292,    73,    14],
        [    0,   292,   380,  ...,   527, 37357,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1817e-04, -9.1169e-05, -6.3250e-05,  ..., -8.1889e-05,
          2.9082e-05, -1.3154e-05],
        [-2.0266e-06, -1.2964e-06,  1.1995e-06,  ..., -1.5870e-06,
         -1.4678e-06, -1.2368e-06],
        [-2.8312e-06, -1.8030e-06,  1.6913e-06,  ..., -2.1905e-06,
         -1.8477e-06, -1.6838e-06],
        [-3.8147e-06, -2.4289e-06,  2.2948e-06,  ..., -2.9653e-06,
         -2.5034e-06, -2.2948e-06],
        [-4.5002e-06, -2.8461e-06,  2.6971e-06,  ..., -3.4422e-06,
         -2.9355e-06, -2.6375e-06]], device='cuda:0')
Loss: 1.0772583484649658


Running epoch 0, step 641, batch 641
Sampled inputs[:2]: tensor([[    0,   342,   721,  ...,  2429,    14,   475],
        [    0,  1607, 26394,  ...,    19,   471,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6565e-04, -2.5040e-05, -9.4494e-05,  ...,  2.6354e-05,
         -7.9996e-05, -3.5954e-05],
        [-3.9935e-06, -2.6450e-06,  2.3618e-06,  ..., -3.1367e-06,
         -2.5108e-06, -2.3320e-06],
        [-5.7071e-06, -3.7849e-06,  3.4124e-06,  ..., -4.4405e-06,
         -3.2112e-06, -3.2634e-06],
        [-7.7784e-06, -5.1409e-06,  4.6641e-06,  ..., -6.0797e-06,
         -4.3660e-06, -4.4852e-06],
        [-9.0301e-06, -5.9605e-06,  5.4240e-06,  ..., -6.9886e-06,
         -5.1409e-06, -5.1111e-06]], device='cuda:0')
Loss: 1.0628037452697754


Running epoch 0, step 642, batch 642
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1260e-04, -9.2798e-05, -7.2338e-05,  ...,  1.2287e-04,
         -2.5268e-04, -8.0470e-05],
        [-5.9456e-06, -3.9637e-06,  3.5316e-06,  ..., -4.6417e-06,
         -3.4720e-06, -3.3975e-06],
        [-8.6725e-06, -5.7817e-06,  5.1856e-06,  ..., -6.7204e-06,
         -4.5300e-06, -4.8503e-06],
        [-1.1832e-05, -7.8529e-06,  7.0930e-06,  ..., -9.2089e-06,
         -6.1542e-06, -6.6757e-06],
        [-1.3649e-05, -9.0748e-06,  8.2105e-06,  ..., -1.0550e-05,
         -7.2569e-06, -7.5847e-06]], device='cuda:0')
Loss: 1.0690604448318481


Running epoch 0, step 643, batch 643
Sampled inputs[:2]: tensor([[    0,  8125,  5241,  ...,   328,  3227,   278],
        [    0,   949, 11135,  ...,   278,   772,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4208e-04, -2.2941e-05, -8.0638e-05,  ...,  1.6177e-04,
         -4.0884e-04,  4.0060e-05],
        [-8.0019e-06, -5.2974e-06,  4.7386e-06,  ..., -6.2436e-06,
         -4.8503e-06, -4.5896e-06],
        [-1.1593e-05, -7.6890e-06,  6.9290e-06,  ..., -8.9854e-06,
         -6.2883e-06, -6.5118e-06],
        [-1.5825e-05, -1.0446e-05,  9.4622e-06,  ..., -1.2308e-05,
         -8.5384e-06, -8.9407e-06],
        [-1.8299e-05, -1.2100e-05,  1.0982e-05,  ..., -1.4141e-05,
         -1.0088e-05, -1.0207e-05]], device='cuda:0')
Loss: 1.0810894966125488


Running epoch 0, step 644, batch 644
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  266, 2105, 3925],
        [   0,  475,  266,  ...,  843,  287, 1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0023e-04, -1.2231e-04, -1.0784e-04,  ...,  1.9535e-04,
         -3.9893e-04,  4.3879e-05],
        [-1.0028e-05, -6.6906e-06,  5.9307e-06,  ..., -7.7859e-06,
         -6.0499e-06, -5.7071e-06],
        [-1.4558e-05, -9.7305e-06,  8.6874e-06,  ..., -1.1221e-05,
         -7.8604e-06, -8.1137e-06],
        [-1.9759e-05, -1.3128e-05,  1.1787e-05,  ..., -1.5289e-05,
         -1.0625e-05, -1.1072e-05],
        [-2.3007e-05, -1.5318e-05,  1.3784e-05,  ..., -1.7688e-05,
         -1.2636e-05, -1.2726e-05]], device='cuda:0')
Loss: 1.0853551626205444


Running epoch 0, step 645, batch 645
Sampled inputs[:2]: tensor([[   0, 1603,   12,  ...,   12,  756,  437],
        [   0, 2085,   12,  ...,  496,   14,  747]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7279e-04,  2.3915e-05, -1.4401e-04,  ...,  3.2026e-04,
         -5.4806e-04,  2.2538e-04],
        [-1.2159e-05, -8.0019e-06,  7.1898e-06,  ..., -9.4101e-06,
         -7.5921e-06, -6.9290e-06],
        [-1.7464e-05, -1.1526e-05,  1.0423e-05,  ..., -1.3411e-05,
         -9.7826e-06, -9.7379e-06],
        [-2.3574e-05, -1.5497e-05,  1.4052e-05,  ..., -1.8194e-05,
         -1.3098e-05, -1.3202e-05],
        [-2.7627e-05, -1.8179e-05,  1.6555e-05,  ..., -2.1175e-05,
         -1.5721e-05, -1.5289e-05]], device='cuda:0')
Loss: 1.0710101127624512


Running epoch 0, step 646, batch 646
Sampled inputs[:2]: tensor([[    0,    14,   417,  ...,  8821,  6845,   278],
        [    0, 10296,   809,  ..., 27683,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5183e-04,  1.7854e-04, -2.5783e-04,  ...,  4.5231e-04,
         -7.0050e-04,  2.1518e-04],
        [-1.4246e-05, -9.3654e-06,  8.4043e-06,  ..., -1.1064e-05,
         -9.1940e-06, -8.2478e-06],
        [-2.0251e-05, -1.3329e-05,  1.2055e-05,  ..., -1.5572e-05,
         -1.1720e-05, -1.1452e-05],
        [-2.7314e-05, -1.7911e-05,  1.6272e-05,  ..., -2.1130e-05,
         -1.5706e-05, -1.5527e-05],
        [-3.2097e-05, -2.1070e-05,  1.9193e-05,  ..., -2.4632e-05,
         -1.8880e-05, -1.8016e-05]], device='cuda:0')
Loss: 1.0640056133270264


Running epoch 0, step 647, batch 647
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5700e-04,  1.7518e-04, -2.7114e-04,  ...,  4.1274e-04,
         -6.8533e-04,  2.3688e-04],
        [-1.6347e-05, -1.0796e-05,  9.6709e-06,  ..., -1.2659e-05,
         -1.0535e-05, -9.4846e-06],
        [-2.3291e-05, -1.5400e-05,  1.3888e-05,  ..., -1.7866e-05,
         -1.3471e-05, -1.3202e-05],
        [-3.1278e-05, -2.0579e-05,  1.8656e-05,  ..., -2.4110e-05,
         -1.7971e-05, -1.7822e-05],
        [-3.6865e-05, -2.4289e-05,  2.2069e-05,  ..., -2.8208e-05,
         -2.1651e-05, -2.0728e-05]], device='cuda:0')
Loss: 1.0833607912063599
Graident accumulation at epoch 0, step 647, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0027,  ..., -0.0024,  0.0232, -0.0195],
        [ 0.0290, -0.0079,  0.0037,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0277,  ...,  0.0288, -0.0147, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3021e-05,  2.0821e-04, -2.2628e-04,  ...,  1.1524e-04,
         -1.6372e-04,  6.3923e-05],
        [-1.7858e-05, -1.1300e-05,  8.2456e-06,  ..., -1.5248e-05,
         -8.2311e-06, -1.0539e-05],
        [ 8.9158e-05,  5.6452e-05, -4.8412e-05,  ...,  7.5234e-05,
          3.1642e-05,  3.8042e-05],
        [-2.0700e-05, -1.3898e-05,  1.0805e-05,  ..., -1.8310e-05,
         -9.7017e-06, -1.3040e-05],
        [-3.4171e-05, -1.9574e-05,  1.6017e-05,  ..., -2.8137e-05,
         -1.5138e-05, -1.9685e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8357e-08, 2.1330e-08, 3.0347e-08,  ..., 2.2068e-08, 5.2059e-08,
         1.1283e-08],
        [5.1002e-11, 2.8644e-11, 3.4076e-12,  ..., 3.5942e-11, 2.8381e-12,
         1.0173e-11],
        [2.2828e-09, 1.0791e-09, 3.3995e-10,  ..., 1.6730e-09, 2.2416e-10,
         6.5913e-10],
        [4.2927e-10, 3.3380e-10, 3.8629e-11,  ..., 3.5923e-10, 3.7398e-11,
         1.6620e-10],
        [2.2669e-10, 1.2792e-10, 1.9520e-11,  ..., 1.6702e-10, 1.3328e-11,
         5.0317e-11]], device='cuda:0')
optimizer state dict: 81.0
lr: [1.6183680254100683e-05, 1.6183680254100683e-05]
scheduler_last_epoch: 81


Running epoch 0, step 648, batch 648
Sampled inputs[:2]: tensor([[    0,   515,   266,  ...,    18,  3770,  1345],
        [    0,  1978, 20360,  ...,   898,   699, 10262]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1494e-04, -2.0964e-04,  6.4864e-05,  ..., -4.6476e-05,
          2.3814e-05,  6.0010e-05],
        [-1.8999e-06, -1.3486e-06,  1.1697e-06,  ..., -1.4082e-06,
         -1.0207e-06, -1.1325e-06],
        [-2.9504e-06, -2.0862e-06,  1.8328e-06,  ..., -2.1756e-06,
         -1.4231e-06, -1.7211e-06],
        [-3.8743e-06, -2.7269e-06,  2.3991e-06,  ..., -2.8610e-06,
         -1.8626e-06, -2.2799e-06],
        [-4.7684e-06, -3.3528e-06,  2.9504e-06,  ..., -3.5018e-06,
         -2.3395e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0779247283935547


Running epoch 0, step 649, batch 649
Sampled inputs[:2]: tensor([[   0, 6584,  278,  ..., 1039,  965, 1410],
        [   0, 6809,  344,  ...,   14, 1266,  795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6341e-04, -4.2025e-04,  4.7638e-05,  ..., -1.3573e-04,
          2.0137e-04,  3.5223e-04],
        [-3.7923e-06, -2.6226e-06,  2.2650e-06,  ..., -3.0100e-06,
         -2.6897e-06, -2.5928e-06],
        [-5.5581e-06, -3.8445e-06,  3.3751e-06,  ..., -4.3064e-06,
         -3.4198e-06, -3.6284e-06],
        [-7.3016e-06, -5.0217e-06,  4.4405e-06,  ..., -5.6773e-06,
         -4.5002e-06, -4.8280e-06],
        [-8.8811e-06, -6.1244e-06,  5.3942e-06,  ..., -6.8396e-06,
         -5.5134e-06, -5.7071e-06]], device='cuda:0')
Loss: 1.0548992156982422


Running epoch 0, step 650, batch 650
Sampled inputs[:2]: tensor([[    0,  3227,   300,  ...,  1817,  5709,   300],
        [    0,  1795,   365,  ...,   266, 46932,   293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8550e-04, -3.3902e-04,  9.4199e-05,  ..., -1.0808e-04,
          2.7074e-04,  5.2210e-04],
        [-5.7146e-06, -3.9265e-06,  3.3677e-06,  ..., -4.4852e-06,
         -3.7849e-06, -3.7476e-06],
        [-8.4639e-06, -5.8264e-06,  5.0664e-06,  ..., -6.5118e-06,
         -4.8652e-06, -5.3272e-06],
        [-1.1176e-05, -7.6592e-06,  6.7055e-06,  ..., -8.6427e-06,
         -6.4522e-06, -7.1228e-06],
        [-1.3500e-05, -9.2834e-06,  8.1062e-06,  ..., -1.0341e-05,
         -7.8827e-06, -8.3894e-06]], device='cuda:0')
Loss: 1.0811750888824463


Running epoch 0, step 651, batch 651
Sampled inputs[:2]: tensor([[    0,  3398,   271,  ...,    13,  1581, 13600],
        [    0,  1064,   266,  ...,  2971,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1953e-04, -3.7666e-04,  4.9563e-05,  ..., -1.3507e-04,
          3.8815e-04,  5.0808e-04],
        [-7.5698e-06, -5.2676e-06,  4.5151e-06,  ..., -5.9158e-06,
         -4.6752e-06, -4.8429e-06],
        [-1.1370e-05, -7.9274e-06,  6.8769e-06,  ..., -8.7321e-06,
         -6.1095e-06, -7.0184e-06],
        [-1.5050e-05, -1.0461e-05,  9.1344e-06,  ..., -1.1638e-05,
         -8.1137e-06, -9.4026e-06],
        [-1.8209e-05, -1.2696e-05,  1.1042e-05,  ..., -1.3947e-05,
         -9.9540e-06, -1.1101e-05]], device='cuda:0')
Loss: 1.071098804473877


Running epoch 0, step 652, batch 652
Sampled inputs[:2]: tensor([[   0,  591, 1545,  ...,   71,  462,  221],
        [   0, 4154,   12,  ...,   14,  560,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9886e-05, -3.2956e-04,  7.6287e-05,  ..., -8.5860e-05,
          4.2308e-04,  5.2602e-04],
        [-9.3803e-06, -6.5863e-06,  5.5879e-06,  ..., -7.3686e-06,
         -5.4948e-06, -5.8711e-06],
        [-1.4246e-05, -9.9987e-06,  8.6054e-06,  ..., -1.0982e-05,
         -7.2122e-06, -8.5980e-06],
        [-1.8954e-05, -1.3292e-05,  1.1489e-05,  ..., -1.4722e-05,
         -9.6262e-06, -1.1563e-05],
        [-2.2769e-05, -1.5989e-05,  1.3813e-05,  ..., -1.7524e-05,
         -1.1772e-05, -1.3575e-05]], device='cuda:0')
Loss: 1.0551419258117676


Running epoch 0, step 653, batch 653
Sampled inputs[:2]: tensor([[   0, 6640,   13,  ...,  292,  221,  273],
        [   0,  266, 1194,  ..., 2267,   15, 1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0947e-04, -3.1686e-04,  8.8277e-05,  ..., -1.1096e-04,
          3.4495e-04,  5.1951e-04],
        [-1.1168e-05, -7.9498e-06,  6.7428e-06,  ..., -8.8364e-06,
         -6.7092e-06, -7.1228e-06],
        [-1.6883e-05, -1.1995e-05,  1.0327e-05,  ..., -1.3083e-05,
         -8.7395e-06, -1.0371e-05],
        [-2.2501e-05, -1.5974e-05,  1.3813e-05,  ..., -1.7568e-05,
         -1.1697e-05, -1.3977e-05],
        [-2.6852e-05, -1.9073e-05,  1.6481e-05,  ..., -2.0757e-05,
         -1.4201e-05, -1.6287e-05]], device='cuda:0')
Loss: 1.0485572814941406


Running epoch 0, step 654, batch 654
Sampled inputs[:2]: tensor([[    0,   266,  1422,  ...,   446,  1992,   586],
        [    0,  7935,  6521,  ..., 41312,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4440e-04, -4.1471e-04,  1.4354e-04,  ..., -1.3912e-04,
          2.8492e-04,  5.6592e-04],
        [-1.3076e-05, -9.3207e-06,  7.8380e-06,  ..., -1.0297e-05,
         -7.7374e-06, -8.2776e-06],
        [-1.9804e-05, -1.4096e-05,  1.2025e-05,  ..., -1.5303e-05,
         -1.0163e-05, -1.2107e-05],
        [-2.6345e-05, -1.8716e-05,  1.6049e-05,  ..., -2.0504e-05,
         -1.3560e-05, -1.6272e-05],
        [-3.1561e-05, -2.2426e-05,  1.9237e-05,  ..., -2.4319e-05,
         -1.6540e-05, -1.9044e-05]], device='cuda:0')
Loss: 1.070194125175476


Running epoch 0, step 655, batch 655
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,    12,  3367,  ..., 16917, 12221, 12138]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5149e-04, -7.1441e-04,  2.0629e-04,  ..., -2.0692e-04,
          5.2158e-04,  5.6592e-04],
        [-1.4983e-05, -1.0669e-05,  8.9929e-06,  ..., -1.1757e-05,
         -8.9295e-06, -9.4995e-06],
        [-2.2724e-05, -1.6153e-05,  1.3821e-05,  ..., -1.7494e-05,
         -1.1757e-05, -1.3918e-05],
        [-3.0145e-05, -2.1368e-05,  1.8388e-05,  ..., -2.3365e-05,
         -1.5646e-05, -1.8641e-05],
        [-3.6269e-05, -2.5719e-05,  2.2143e-05,  ..., -2.7820e-05,
         -1.9148e-05, -2.1920e-05]], device='cuda:0')
Loss: 1.0838311910629272
Graident accumulation at epoch 0, step 655, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0145,  0.0027,  ..., -0.0024,  0.0232, -0.0195],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0277,  ...,  0.0288, -0.0147, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.4301e-06,  1.1595e-04, -1.8302e-04,  ...,  8.3021e-05,
         -9.5194e-05,  1.1412e-04],
        [-1.7571e-05, -1.1237e-05,  8.3203e-06,  ..., -1.4899e-05,
         -8.3009e-06, -1.0435e-05],
        [ 7.7970e-05,  4.9192e-05, -4.2188e-05,  ...,  6.5961e-05,
          2.7302e-05,  3.2846e-05],
        [-2.1645e-05, -1.4645e-05,  1.1563e-05,  ..., -1.8815e-05,
         -1.0296e-05, -1.3600e-05],
        [-3.4381e-05, -2.0189e-05,  1.6629e-05,  ..., -2.8105e-05,
         -1.5539e-05, -1.9908e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8422e-08, 2.1819e-08, 3.0359e-08,  ..., 2.2089e-08, 5.2279e-08,
         1.1592e-08],
        [5.1175e-11, 2.8729e-11, 3.4851e-12,  ..., 3.6044e-11, 2.9150e-12,
         1.0253e-11],
        [2.2810e-09, 1.0783e-09, 3.3980e-10,  ..., 1.6716e-09, 2.2408e-10,
         6.5867e-10],
        [4.2975e-10, 3.3393e-10, 3.8929e-11,  ..., 3.5941e-10, 3.7606e-11,
         1.6638e-10],
        [2.2778e-10, 1.2846e-10, 1.9991e-11,  ..., 1.6762e-10, 1.3682e-11,
         5.0747e-11]], device='cuda:0')
optimizer state dict: 82.0
lr: [1.6086061372297498e-05, 1.6086061372297498e-05]
scheduler_last_epoch: 82


Running epoch 0, step 656, batch 656
Sampled inputs[:2]: tensor([[   0,   26, 4044,  ..., 9531,  365,  993],
        [   0, 1796,  342,  ...,  668, 2903,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9160e-05,  1.0399e-04,  9.1485e-05,  ...,  1.0577e-04,
         -9.6275e-05,  1.3727e-04],
        [-1.9670e-06, -1.4156e-06,  1.1176e-06,  ..., -1.5274e-06,
         -1.1176e-06, -1.2070e-06],
        [-3.0845e-06, -2.2203e-06,  1.7807e-06,  ..., -2.3693e-06,
         -1.5795e-06, -1.8328e-06],
        [-4.0829e-06, -2.9206e-06,  2.3544e-06,  ..., -3.1441e-06,
         -2.1160e-06, -2.4587e-06],
        [-5.0068e-06, -3.6061e-06,  2.9057e-06,  ..., -3.8445e-06,
         -2.6077e-06, -2.9653e-06]], device='cuda:0')
Loss: 1.0991404056549072


Running epoch 0, step 657, batch 657
Sampled inputs[:2]: tensor([[    0,    14, 13078,  ...,  1994,    12,   287],
        [    0,   199,  5990,  ...,   278,   638,  5513]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5747e-05, -3.2241e-05,  9.7181e-05,  ...,  2.7526e-06,
         -1.3986e-05,  6.5961e-05],
        [-3.8594e-06, -2.7567e-06,  2.2054e-06,  ..., -3.0324e-06,
         -2.0452e-06, -2.4363e-06],
        [-6.1244e-06, -4.3958e-06,  3.5614e-06,  ..., -4.7684e-06,
         -2.8685e-06, -3.7551e-06],
        [-8.0764e-06, -5.7518e-06,  4.6939e-06,  ..., -6.3032e-06,
         -3.8072e-06, -4.9919e-06],
        [-9.8050e-06, -7.0333e-06,  5.7220e-06,  ..., -7.6145e-06,
         -4.6939e-06, -5.9605e-06]], device='cuda:0')
Loss: 1.0652742385864258


Running epoch 0, step 658, batch 658
Sampled inputs[:2]: tensor([[    0,   902, 11331,  ...,  1795,   365,   654],
        [    0,     5,  7523,  ...,   199,  8871,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3246e-05, -4.7186e-06,  2.2979e-04,  ..., -1.0205e-05,
         -4.8119e-05,  9.6207e-05],
        [-5.7817e-06, -4.1649e-06,  3.2932e-06,  ..., -4.5598e-06,
         -2.9616e-06, -3.5837e-06],
        [-9.2387e-06, -6.6757e-06,  5.3495e-06,  ..., -7.2122e-06,
         -4.2170e-06, -5.5730e-06],
        [-1.2189e-05, -8.7470e-06,  7.0482e-06,  ..., -9.5516e-06,
         -5.5730e-06, -7.4059e-06],
        [-1.4812e-05, -1.0699e-05,  8.5980e-06,  ..., -1.1548e-05,
         -6.8992e-06, -8.8513e-06]], device='cuda:0')
Loss: 1.0873411893844604


Running epoch 0, step 659, batch 659
Sampled inputs[:2]: tensor([[   0,  259, 5112,  ..., 3520,  278,  298],
        [   0, 1611,  266,  ...,  266, 2673, 6277]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1145e-04,  1.1718e-04,  2.6752e-04,  ...,  5.4512e-05,
         -1.9924e-04,  4.4201e-05],
        [-7.6368e-06, -5.5730e-06,  4.3660e-06,  ..., -6.0275e-06,
         -3.8035e-06, -4.7013e-06],
        [ 8.6727e-05,  6.2097e-05, -4.0905e-05,  ...,  5.9871e-05,
          2.1638e-05,  2.4851e-05],
        [-1.6153e-05, -1.1757e-05,  9.4026e-06,  ..., -1.2666e-05,
         -7.1228e-06, -9.7454e-06],
        [-1.9670e-05, -1.4395e-05,  1.1474e-05,  ..., -1.5318e-05,
         -8.8513e-06, -1.1653e-05]], device='cuda:0')
Loss: 1.0794199705123901


Running epoch 0, step 660, batch 660
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    14, 10961,    12],
        [    0,    14,   475,  ..., 44038,    12,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8654e-05,  2.0885e-04,  2.8036e-04,  ...,  9.4109e-05,
         -3.1157e-04,  1.1769e-04],
        [-9.4920e-06, -7.0035e-06,  5.4836e-06,  ..., -7.4580e-06,
         -4.6082e-06, -5.8338e-06],
        [ 8.3642e-05,  5.9713e-05, -3.9020e-05,  ...,  5.7502e-05,
          2.0438e-05,  2.3003e-05],
        [-2.0087e-05, -1.4782e-05,  1.1802e-05,  ..., -1.5691e-05,
         -8.6576e-06, -1.2130e-05],
        [-2.4527e-05, -1.8135e-05,  1.4454e-05,  ..., -1.9044e-05,
         -1.0788e-05, -1.4529e-05]], device='cuda:0')
Loss: 1.072630524635315


Running epoch 0, step 661, batch 661
Sampled inputs[:2]: tensor([[   0,   14,  333,  ...,  328, 5453, 4713],
        [   0,   17,  292,  ..., 2269, 3887,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5844e-04,  4.8717e-04,  3.7374e-04,  ...,  1.4056e-04,
         -4.1112e-04,  1.9749e-04],
        [-1.1295e-05, -8.3894e-06,  6.5491e-06,  ..., -8.8885e-06,
         -5.5060e-06, -6.9588e-06],
        [ 8.0647e-05,  5.7433e-05, -3.7224e-05,  ...,  5.5192e-05,
          1.9164e-05,  2.1207e-05],
        [-2.3901e-05, -1.7688e-05,  1.4096e-05,  ..., -1.8671e-05,
         -1.0297e-05, -1.4454e-05],
        [-2.9266e-05, -2.1741e-05,  1.7315e-05,  ..., -2.2694e-05,
         -1.2845e-05, -1.7330e-05]], device='cuda:0')
Loss: 1.0696903467178345


Running epoch 0, step 662, batch 662
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0, 39200,  1828,  ...,   300,  3067,  4443]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0999e-04,  6.9543e-04,  3.5461e-04,  ...,  2.5742e-04,
         -3.8532e-04,  1.7748e-04],
        [-1.3001e-05, -9.7156e-06,  7.6368e-06,  ..., -1.0394e-05,
         -6.5342e-06, -8.1360e-06],
        [ 7.7950e-05,  5.5347e-05, -3.5473e-05,  ...,  5.2897e-05,
          1.7778e-05,  1.9434e-05],
        [-2.7552e-05, -2.0504e-05,  1.6496e-05,  ..., -2.1830e-05,
         -1.2219e-05, -1.6913e-05],
        [-3.3438e-05, -2.4974e-05,  2.0057e-05,  ..., -2.6241e-05,
         -1.5035e-05, -2.0042e-05]], device='cuda:0')
Loss: 1.0648287534713745


Running epoch 0, step 663, batch 663
Sampled inputs[:2]: tensor([[   0, 2202,  292,  ..., 2431, 2267, 3423],
        [   0,  266, 2086,  ..., 4283,  720,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9500e-05,  4.7637e-04,  3.9318e-04,  ...,  1.8897e-04,
         -1.6396e-04,  1.2996e-04],
        [-1.4924e-05, -1.1116e-05,  8.7917e-06,  ..., -1.1861e-05,
         -7.4431e-06, -9.2760e-06],
        [ 7.4880e-05,  5.3097e-05, -3.3611e-05,  ...,  5.0558e-05,
          1.6452e-05,  1.7631e-05],
        [-3.1605e-05, -2.3454e-05,  1.8954e-05,  ..., -2.4930e-05,
         -1.3962e-05, -1.9312e-05],
        [-3.8475e-05, -2.8670e-05,  2.3112e-05,  ..., -3.0085e-05,
         -1.7270e-05, -2.2978e-05]], device='cuda:0')
Loss: 1.0948387384414673
Graident accumulation at epoch 0, step 663, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0057, -0.0144,  0.0027,  ..., -0.0024,  0.0232, -0.0195],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0025, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0277,  ...,  0.0288, -0.0147, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0630e-06,  1.5199e-04, -1.2540e-04,  ...,  9.3616e-05,
         -1.0207e-04,  1.1571e-04],
        [-1.7306e-05, -1.1225e-05,  8.3674e-06,  ..., -1.4595e-05,
         -8.2151e-06, -1.0320e-05],
        [ 7.7661e-05,  4.9582e-05, -4.1331e-05,  ...,  6.4421e-05,
          2.6217e-05,  3.1324e-05],
        [-2.2641e-05, -1.5526e-05,  1.2302e-05,  ..., -1.9427e-05,
         -1.0663e-05, -1.4171e-05],
        [-3.4790e-05, -2.1037e-05,  1.7277e-05,  ..., -2.8303e-05,
         -1.5712e-05, -2.0215e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8372e-08, 2.2024e-08, 3.0484e-08,  ..., 2.2102e-08, 5.2253e-08,
         1.1598e-08],
        [5.1347e-11, 2.8824e-11, 3.5589e-12,  ..., 3.6149e-11, 2.9675e-12,
         1.0329e-11],
        [2.2843e-09, 1.0800e-09, 3.4059e-10,  ..., 1.6725e-09, 2.2412e-10,
         6.5832e-10],
        [4.3032e-10, 3.3414e-10, 3.9249e-11,  ..., 3.5967e-10, 3.7763e-11,
         1.6658e-10],
        [2.2903e-10, 1.2915e-10, 2.0505e-11,  ..., 1.6836e-10, 1.3966e-11,
         5.1224e-11]], device='cuda:0')
optimizer state dict: 83.0
lr: [1.5987512487190642e-05, 1.5987512487190642e-05]
scheduler_last_epoch: 83


Running epoch 0, step 664, batch 664
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0, 41921,  1955,  ...,    75,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1375e-04,  1.7179e-04, -1.6397e-04,  ..., -1.6407e-05,
         -1.8840e-04, -2.0808e-04],
        [-1.7285e-06, -1.4231e-06,  1.0729e-06,  ..., -1.4827e-06,
         -6.7428e-07, -1.0580e-06],
        [-4.6948e-05,  7.4387e-05, -4.2945e-05,  ...,  4.1357e-05,
          6.3636e-05,  3.2894e-05],
        [-3.9041e-06, -3.1739e-06,  2.4885e-06,  ..., -3.2634e-06,
         -1.2964e-06, -2.3097e-06],
        [-4.9174e-06, -3.9637e-06,  3.1292e-06,  ..., -4.0233e-06,
         -1.6168e-06, -2.7865e-06]], device='cuda:0')
Loss: 1.0776623487472534


Running epoch 0, step 665, batch 665
Sampled inputs[:2]: tensor([[    0,  1456, 32380,  ...,    12,  1172, 12557],
        [    0,   380,   333,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5030e-04,  4.2091e-04, -2.0089e-04,  ...,  1.5186e-04,
         -3.2301e-04, -2.7023e-04],
        [-3.5688e-06, -2.8163e-06,  2.1309e-06,  ..., -2.9728e-06,
         -1.4044e-06, -2.1234e-06],
        [-5.0092e-05,  7.2002e-05, -4.1112e-05,  ...,  3.8839e-05,
          6.2511e-05,  3.1121e-05],
        [-7.9870e-06, -6.2436e-06,  4.8727e-06,  ..., -6.5416e-06,
         -2.7567e-06, -4.6343e-06],
        [-9.9242e-06, -7.7635e-06,  6.0648e-06,  ..., -8.0466e-06,
         -3.4571e-06, -5.5879e-06]], device='cuda:0')
Loss: 1.061639666557312


Running epoch 0, step 666, batch 666
Sampled inputs[:2]: tensor([[   0,  266, 1513,  ...,  367, 1941,  344],
        [   0, 3103, 2134,  ..., 6627,  275, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7967e-04,  3.8536e-04, -2.2192e-04,  ...,  1.2300e-04,
         -2.7689e-04, -2.7449e-04],
        [-5.4762e-06, -4.2617e-06,  3.2112e-06,  ..., -4.5076e-06,
         -2.3060e-06, -3.2485e-06],
        [-5.3251e-05,  6.9603e-05, -3.9286e-05,  ...,  3.6321e-05,
          6.1155e-05,  2.9310e-05],
        [-1.1921e-05, -9.2238e-06,  7.1526e-06,  ..., -9.7007e-06,
         -4.4629e-06, -6.9290e-06],
        [-1.5080e-05, -1.1668e-05,  9.0301e-06,  ..., -1.2130e-05,
         -5.7071e-06, -8.4937e-06]], device='cuda:0')
Loss: 1.0840353965759277


Running epoch 0, step 667, batch 667
Sampled inputs[:2]: tensor([[   0,   14,  292,  ..., 1385,   12,  287],
        [   0, 2974,  278,  ...,  365, 8758,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7967e-04,  6.2661e-04, -3.8957e-04,  ...,  3.6660e-04,
         -5.2612e-04, -3.5713e-04],
        [-7.2420e-06, -5.6624e-06,  4.2170e-06,  ..., -5.9903e-06,
         -3.0398e-06, -4.3511e-06],
        [-5.6321e-05,  6.7189e-05, -3.7491e-05,  ...,  3.3832e-05,
          6.0082e-05,  2.7507e-05],
        [-1.5855e-05, -1.2323e-05,  9.4622e-06,  ..., -1.2949e-05,
         -5.8711e-06, -9.3132e-06],
        [-1.9938e-05, -1.5482e-05,  1.1876e-05,  ..., -1.6063e-05,
         -7.4357e-06, -1.1295e-05]], device='cuda:0')
Loss: 1.0616058111190796


Running epoch 0, step 668, batch 668
Sampled inputs[:2]: tensor([[    0, 21540,   527,  ...,   824,    14,   381],
        [    0,   456,    17,  ...,  1553, 29477,  2713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8878e-04,  9.8337e-04, -2.7549e-04,  ...,  4.3551e-04,
         -6.4237e-04, -3.0030e-04],
        [-9.1344e-06, -7.0184e-06,  5.2601e-06,  ..., -7.5176e-06,
         -4.1947e-06, -5.6028e-06],
        [-5.9494e-05,  6.4909e-05, -3.5695e-05,  ...,  3.1344e-05,
          5.8361e-05,  2.5495e-05],
        [-1.9729e-05, -1.5080e-05,  1.1638e-05,  ..., -1.6004e-05,
         -7.9870e-06, -1.1802e-05],
        [-2.5004e-05, -1.9133e-05,  1.4752e-05,  ..., -2.0027e-05,
         -1.0207e-05, -1.4469e-05]], device='cuda:0')
Loss: 1.057045578956604


Running epoch 0, step 669, batch 669
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0, 13649,  7841,  ...,   287,  4713,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4959e-04,  9.6838e-04, -2.1748e-04,  ...,  4.1788e-04,
         -6.4237e-04, -2.8683e-04],
        [-1.0997e-05, -8.5160e-06,  6.3628e-06,  ..., -8.9929e-06,
         -4.8950e-06, -6.6459e-06],
        [-6.2728e-05,  6.2302e-05, -3.3743e-05,  ...,  2.8795e-05,
          5.7251e-05,  2.3715e-05],
        [-2.3782e-05, -1.8343e-05,  1.4082e-05,  ..., -1.9222e-05,
         -9.3654e-06, -1.4067e-05],
        [-3.0190e-05, -2.3305e-05,  1.7881e-05,  ..., -2.4110e-05,
         -1.2048e-05, -1.7285e-05]], device='cuda:0')
Loss: 1.073742151260376


Running epoch 0, step 670, batch 670
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   394,   360,  5911],
        [    0,   631,   516,  ..., 13374,   898,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5441e-04,  9.8113e-04, -2.3414e-04,  ...,  4.8163e-04,
         -6.8698e-04, -4.7868e-04],
        [-1.2860e-05, -9.9912e-06,  7.4431e-06,  ..., -1.0476e-05,
         -5.6438e-06, -7.7784e-06],
        [ 2.0167e-05,  1.2173e-04, -8.8149e-05,  ...,  7.8726e-05,
          1.0483e-04,  5.7093e-05],
        [-2.7776e-05, -2.1487e-05,  1.6451e-05,  ..., -2.2367e-05,
         -1.0781e-05, -1.6466e-05],
        [-3.5256e-05, -2.7299e-05,  2.0877e-05,  ..., -2.8044e-05,
         -1.3873e-05, -2.0221e-05]], device='cuda:0')
Loss: 1.064511775970459


Running epoch 0, step 671, batch 671
Sampled inputs[:2]: tensor([[    0,   654,   300,  ..., 21762,  3597, 11117],
        [    0,  6795,  1728,  ...,   578,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0235e-04,  1.0920e-03, -1.3739e-04,  ...,  5.0385e-04,
         -7.0433e-04, -4.6133e-04],
        [-1.4715e-05, -1.1384e-05,  8.5160e-06,  ..., -1.1966e-05,
         -6.5528e-06, -8.9630e-06],
        [ 1.7053e-05,  1.1938e-04, -8.6309e-05,  ...,  7.6283e-05,
          1.0348e-04,  5.5171e-05],
        [-3.1829e-05, -2.4512e-05,  1.8835e-05,  ..., -2.5570e-05,
         -1.2554e-05, -1.8999e-05],
        [-4.0382e-05, -3.1143e-05,  2.3901e-05,  ..., -3.2067e-05,
         -1.6123e-05, -2.3335e-05]], device='cuda:0')
Loss: 1.0833734273910522
Graident accumulation at epoch 0, step 671, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0027,  ..., -0.0024,  0.0232, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0277,  ...,  0.0288, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3892e-05,  2.4600e-04, -1.2660e-04,  ...,  1.3464e-04,
         -1.6230e-04,  5.8002e-05],
        [-1.7047e-05, -1.1241e-05,  8.3823e-06,  ..., -1.4332e-05,
         -8.0489e-06, -1.0184e-05],
        [ 7.1600e-05,  5.6562e-05, -4.5828e-05,  ...,  6.5607e-05,
          3.3943e-05,  3.3709e-05],
        [-2.3560e-05, -1.6424e-05,  1.2955e-05,  ..., -2.0041e-05,
         -1.0852e-05, -1.4654e-05],
        [-3.5350e-05, -2.2048e-05,  1.7940e-05,  ..., -2.8680e-05,
         -1.5753e-05, -2.0527e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8566e-08, 2.3195e-08, 3.0472e-08,  ..., 2.2334e-08, 5.2697e-08,
         1.1799e-08],
        [5.1512e-11, 2.8925e-11, 3.6279e-12,  ..., 3.6256e-11, 3.0075e-12,
         1.0399e-11],
        [2.2824e-09, 1.0932e-09, 3.4770e-10,  ..., 1.6766e-09, 2.3461e-10,
         6.6071e-10],
        [4.3090e-10, 3.3441e-10, 3.9565e-11,  ..., 3.5997e-10, 3.7883e-11,
         1.6678e-10],
        [2.3044e-10, 1.2999e-10, 2.1056e-11,  ..., 1.6922e-10, 1.4212e-11,
         5.1718e-11]], device='cuda:0')
optimizer state dict: 84.0
lr: [1.5888048657910018e-05, 1.5888048657910018e-05]
scheduler_last_epoch: 84


Running epoch 0, step 672, batch 672
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,  1806,   319,  ...,  3427,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6976e-05,  5.4434e-05, -3.5062e-05,  ..., -3.7190e-05,
         -8.1899e-05, -1.8167e-04],
        [-1.8403e-06, -1.5572e-06,  1.0207e-06,  ..., -1.4603e-06,
         -7.1898e-07, -1.0878e-06],
        [-3.3528e-06, -2.8014e-06,  1.9073e-06,  ..., -2.5928e-06,
         -1.1474e-06, -1.9073e-06],
        [-3.9935e-06, -3.3528e-06,  2.2948e-06,  ..., -3.1292e-06,
         -1.3784e-06, -2.3246e-06],
        [-5.3346e-06, -4.4405e-06,  3.0398e-06,  ..., -4.1127e-06,
         -1.8701e-06, -2.9653e-06]], device='cuda:0')
Loss: 1.0402865409851074


Running epoch 0, step 673, batch 673
Sampled inputs[:2]: tensor([[    0,    15,    72,  ...,   380, 22463,  2587],
        [    0,   342,   266,  ...,   586,  1944,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2477e-04,  1.7765e-04, -6.7256e-05,  ...,  5.9144e-05,
         -1.7812e-04, -3.0436e-04],
        [-3.5241e-06, -3.0994e-06,  1.9670e-06,  ..., -2.9281e-06,
         -1.4156e-06, -2.1383e-06],
        [-6.4075e-06, -5.5283e-06,  3.6955e-06,  ..., -5.1260e-06,
         -2.1532e-06, -3.6731e-06],
        [-7.7039e-06, -6.6757e-06,  4.4703e-06,  ..., -6.2585e-06,
         -2.6375e-06, -4.5300e-06],
        [-1.0163e-05, -8.7321e-06,  5.8860e-06,  ..., -8.1062e-06,
         -3.5018e-06, -5.7071e-06]], device='cuda:0')
Loss: 1.0405466556549072


Running epoch 0, step 674, batch 674
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   266,  6071,  ...,  1061,  1107,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5924e-05, -1.4064e-04, -2.5594e-05,  ..., -3.0697e-05,
         -4.1088e-05, -3.4917e-04],
        [-5.1409e-06, -4.5002e-06,  2.9504e-06,  ..., -4.4033e-06,
         -2.4140e-06, -3.3528e-06],
        [-9.2536e-06, -7.9721e-06,  5.4985e-06,  ..., -7.6145e-06,
         -3.6508e-06, -5.6848e-06],
        [-1.1221e-05, -9.6858e-06,  6.7055e-06,  ..., -9.3728e-06,
         -4.5225e-06, -7.0781e-06],
        [-1.4693e-05, -1.2606e-05,  8.7619e-06,  ..., -1.2040e-05,
         -5.9456e-06, -8.8513e-06]], device='cuda:0')
Loss: 1.0573325157165527


Running epoch 0, step 675, batch 675
Sampled inputs[:2]: tensor([[    0, 11752,   280,  ..., 14814,  1128,   360],
        [    0,   259,  2283,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9824e-04,  4.3119e-04, -3.3650e-04,  ...,  2.3723e-04,
         -1.2613e-04, -5.7072e-04],
        [-6.6906e-06, -5.8860e-06,  3.8892e-06,  ..., -5.9158e-06,
         -3.3900e-06, -4.5598e-06],
        [-1.1951e-05, -1.0312e-05,  7.2122e-06,  ..., -1.0043e-05,
         -5.0217e-06, -7.5623e-06],
        [-1.4648e-05, -1.2651e-05,  8.8960e-06,  ..., -1.2532e-05,
         -6.3255e-06, -9.5665e-06],
        [-1.8924e-05, -1.6257e-05,  1.1459e-05,  ..., -1.5810e-05,
         -8.1062e-06, -1.1727e-05]], device='cuda:0')
Loss: 1.0416104793548584


Running epoch 0, step 676, batch 676
Sampled inputs[:2]: tensor([[   0,  365,  925,  ...,  909,  598,  328],
        [   0, 3761,   12,  ...,   14,   22,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6668e-04,  3.8475e-04, -3.6401e-04,  ...,  2.0761e-04,
          7.5869e-05, -6.0373e-04],
        [-8.4564e-06, -7.4059e-06,  4.8503e-06,  ..., -7.4133e-06,
         -4.3511e-06, -5.7667e-06],
        [-1.4976e-05, -1.2890e-05,  8.9407e-06,  ..., -1.2487e-05,
         -6.4149e-06, -9.4846e-06],
        [-1.8314e-05, -1.5765e-05,  1.0997e-05,  ..., -1.5542e-05,
         -8.0466e-06, -1.1981e-05],
        [-2.3782e-05, -2.0370e-05,  1.4231e-05,  ..., -1.9714e-05,
         -1.0371e-05, -1.4737e-05]], device='cuda:0')
Loss: 1.0506223440170288


Running epoch 0, step 677, batch 677
Sampled inputs[:2]: tensor([[   0, 4304, 7406,  ...,  957, 7366,  328],
        [   0, 3308,  259,  ...,   14, 6349, 1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1429e-04,  4.4255e-04, -3.7502e-04,  ...,  2.5705e-04,
          4.8098e-05, -7.2058e-04],
        [-1.0341e-05, -8.9929e-06,  5.9009e-06,  ..., -8.9556e-06,
         -5.2601e-06, -6.8694e-06],
        [-1.8299e-05, -1.5676e-05,  1.0833e-05,  ..., -1.5154e-05,
         -7.8604e-06, -1.1362e-05],
        [-2.2307e-05, -1.9118e-05,  1.3277e-05,  ..., -1.8775e-05,
         -9.7975e-06, -1.4275e-05],
        [-2.9087e-05, -2.4810e-05,  1.7256e-05,  ..., -2.3946e-05,
         -1.2726e-05, -1.7688e-05]], device='cuda:0')
Loss: 1.0653570890426636


Running epoch 0, step 678, batch 678
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0, 29368,    13,  ...,   376,    88,  3333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8863e-04,  5.5850e-04, -4.0085e-04,  ...,  3.1886e-04,
         -3.0946e-06, -7.2975e-04],
        [-1.2174e-05, -1.0498e-05,  6.9737e-06,  ..., -1.0461e-05,
         -6.0163e-06, -7.8604e-06],
        [-2.1577e-05, -1.8358e-05,  1.2785e-05,  ..., -1.7807e-05,
         -9.1046e-06, -1.3091e-05],
        [-2.6360e-05, -2.2441e-05,  1.5706e-05,  ..., -2.2084e-05,
         -1.1332e-05, -1.6451e-05],
        [-3.4392e-05, -2.9162e-05,  2.0415e-05,  ..., -2.8238e-05,
         -1.4797e-05, -2.0444e-05]], device='cuda:0')
Loss: 1.1096158027648926


Running epoch 0, step 679, batch 679
Sampled inputs[:2]: tensor([[    0, 19641,   437,  ...,  2992,   518,   266],
        [    0,  1420,  2337,  ...,   722, 28860,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0878e-04,  6.2177e-04, -4.0085e-04,  ...,  3.6561e-04,
         -1.6899e-05, -6.8502e-04],
        [-1.4015e-05, -1.2055e-05,  7.9721e-06,  ..., -1.1921e-05,
         -6.8210e-06, -8.9258e-06],
        [-2.4885e-05, -2.1145e-05,  1.4625e-05,  ..., -2.0385e-05,
         -1.0423e-05, -1.4938e-05],
        [-3.0324e-05, -2.5764e-05,  1.7911e-05,  ..., -2.5198e-05,
         -1.2919e-05, -1.8701e-05],
        [-3.9637e-05, -3.3572e-05,  2.3350e-05,  ..., -3.2321e-05,
         -1.6943e-05, -2.3335e-05]], device='cuda:0')
Loss: 1.0813335180282593
Graident accumulation at epoch 0, step 679, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0027,  ..., -0.0024,  0.0233, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0225,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0278,  ...,  0.0288, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0938e-04,  2.8357e-04, -1.5402e-04,  ...,  1.5774e-04,
         -1.4776e-04, -1.6300e-05],
        [-1.6744e-05, -1.1322e-05,  8.3413e-06,  ..., -1.4091e-05,
         -7.9261e-06, -1.0058e-05],
        [ 6.1952e-05,  4.8791e-05, -3.9783e-05,  ...,  5.7008e-05,
          2.9507e-05,  2.8844e-05],
        [-2.4236e-05, -1.7358e-05,  1.3451e-05,  ..., -2.0557e-05,
         -1.1059e-05, -1.5059e-05],
        [-3.5778e-05, -2.3200e-05,  1.8481e-05,  ..., -2.9044e-05,
         -1.5872e-05, -2.0808e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8878e-08, 2.3558e-08, 3.0602e-08,  ..., 2.2446e-08, 5.2645e-08,
         1.2256e-08],
        [5.1657e-11, 2.9041e-11, 3.6878e-12,  ..., 3.6362e-11, 3.0510e-12,
         1.0468e-11],
        [2.2807e-09, 1.0925e-09, 3.4757e-10,  ..., 1.6754e-09, 2.3448e-10,
         6.6027e-10],
        [4.3139e-10, 3.3474e-10, 3.9846e-11,  ..., 3.6024e-10, 3.8012e-11,
         1.6696e-10],
        [2.3178e-10, 1.3099e-10, 2.1580e-11,  ..., 1.7010e-10, 1.4485e-11,
         5.2210e-11]], device='cuda:0')
optimizer state dict: 85.0
lr: [1.5787685083396957e-05, 1.5787685083396957e-05]
scheduler_last_epoch: 85


Running epoch 0, step 680, batch 680
Sampled inputs[:2]: tensor([[   0,  374, 5195,  ...,  266, 5555,   14],
        [   0,  266, 1254,  ...,  369, 2870,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1099e-05,  5.6959e-05, -1.0162e-04,  ..., -2.0119e-05,
         -7.3577e-05, -1.2106e-04],
        [-1.6689e-06, -1.4901e-06,  8.9407e-07,  ..., -1.6019e-06,
         -9.2387e-07, -1.1101e-06],
        [-2.8759e-06, -2.5332e-06,  1.6242e-06,  ..., -2.6375e-06,
         -1.3784e-06, -1.7807e-06],
        [-3.7402e-06, -3.2932e-06,  2.1160e-06,  ..., -3.4869e-06,
         -1.8254e-06, -2.3991e-06],
        [-4.5896e-06, -4.0531e-06,  2.6077e-06,  ..., -4.1723e-06,
         -2.2501e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.0604668855667114


Running epoch 0, step 681, batch 681
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0,  278, 1478,  ...,  266, 1607, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0763e-03, -2.3512e-03,  4.2857e-04,  ..., -1.6165e-03,
          2.6690e-03,  1.0623e-03],
        [-3.1739e-06, -2.7940e-06,  1.6876e-06,  ..., -3.2336e-06,
         -2.2426e-06, -2.5406e-06],
        [-5.5730e-06, -4.8727e-06,  3.1367e-06,  ..., -5.2601e-06,
         -3.1963e-06, -3.9712e-06],
        [-6.9439e-06, -6.0648e-06,  3.9265e-06,  ..., -6.6310e-06,
         -3.9861e-06, -5.0515e-06],
        [-8.8513e-06, -7.7635e-06,  5.0217e-06,  ..., -8.2254e-06,
         -5.0217e-06, -6.0350e-06]], device='cuda:0')
Loss: 1.0459353923797607


Running epoch 0, step 682, batch 682
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,   221, 18844,  ...,   199, 10174,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3298e-03, -2.4105e-03,  4.4360e-04,  ..., -1.8488e-03,
          2.9532e-03,  1.2081e-03],
        [-4.8503e-06, -4.3139e-06,  2.6003e-06,  ..., -4.7609e-06,
         -3.1516e-06, -3.6359e-06],
        [-8.5831e-06, -7.5996e-06,  4.8503e-06,  ..., -7.9125e-06,
         -4.6268e-06, -5.8264e-06],
        [-1.0595e-05, -9.3579e-06,  6.0126e-06,  ..., -9.8944e-06,
         -5.7369e-06, -7.3761e-06],
        [-1.3649e-05, -1.2085e-05,  7.7486e-06,  ..., -1.2428e-05,
         -7.3463e-06, -8.9109e-06]], device='cuda:0')
Loss: 1.0705416202545166


Running epoch 0, step 683, batch 683
Sampled inputs[:2]: tensor([[    0,   287,  2997,  ...,   437,   266,  1040],
        [    0,  4347,   638,  ...,  1345,   292, 15343]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3279e-03, -2.4287e-03,  2.7656e-04,  ..., -1.7511e-03,
          2.7721e-03,  1.1636e-03],
        [-6.5193e-06, -5.8189e-06,  3.5316e-06,  ..., -6.3404e-06,
         -4.1351e-06, -4.7386e-06],
        [-1.1504e-05, -1.0222e-05,  6.5416e-06,  ..., -1.0580e-05,
         -6.1542e-06, -7.6517e-06],
        [-1.4231e-05, -1.2621e-05,  8.1435e-06,  ..., -1.3292e-05,
         -7.6741e-06, -9.7305e-06],
        [-1.8269e-05, -1.6227e-05,  1.0416e-05,  ..., -1.6630e-05,
         -9.8050e-06, -1.1742e-05]], device='cuda:0')
Loss: 1.086100697517395


Running epoch 0, step 684, batch 684
Sampled inputs[:2]: tensor([[   0,   45,   17,  ...,  278, 4112,   14],
        [   0,   12,  344,  ...,  824,   12,  968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2723e-03, -2.2067e-03,  2.4198e-04,  ..., -1.6700e-03,
          2.5298e-03,  9.3992e-04],
        [-8.1956e-06, -7.3016e-06,  4.3996e-06,  ..., -7.8529e-06,
         -4.9919e-06, -5.8338e-06],
        [-1.4514e-05, -1.2860e-05,  8.1733e-06,  ..., -1.3188e-05,
         -7.4878e-06, -9.4920e-06],
        [-1.7941e-05, -1.5855e-05,  1.0170e-05,  ..., -1.6540e-05,
         -9.3356e-06, -1.2085e-05],
        [-2.3127e-05, -2.0459e-05,  1.3053e-05,  ..., -2.0802e-05,
         -1.1995e-05, -1.4648e-05]], device='cuda:0')
Loss: 1.057278037071228


Running epoch 0, step 685, batch 685
Sampled inputs[:2]: tensor([[    0,    13,    19,  ..., 22111,  2489,    14],
        [    0,   360,   508,  ...,   259,   554,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4861e-03, -2.4512e-03,  2.9896e-04,  ..., -1.7608e-03,
          2.4199e-03,  9.9919e-04],
        [-9.8869e-06, -8.8215e-06,  5.3234e-06,  ..., -9.4101e-06,
         -5.9009e-06, -6.9216e-06],
        [-1.7524e-05, -1.5572e-05,  9.8720e-06,  ..., -1.5914e-05,
         -8.9556e-06, -1.1355e-05],
        [-2.1592e-05, -1.9133e-05,  1.2241e-05,  ..., -1.9878e-05,
         -1.1124e-05, -1.4395e-05],
        [-2.7835e-05, -2.4691e-05,  1.5721e-05,  ..., -2.5064e-05,
         -1.4350e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.0713603496551514


Running epoch 0, step 686, batch 686
Sampled inputs[:2]: tensor([[    0,  1340,   800,  ...,   259, 13583,   422],
        [    0,  5775,   292,  ...,  8671,  1339,   642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5301e-03, -2.7174e-03,  2.8592e-04,  ..., -1.8229e-03,
          2.4739e-03,  9.7193e-04],
        [-1.1459e-05, -1.0312e-05,  6.2212e-06,  ..., -1.0885e-05,
         -6.7577e-06, -8.0764e-06],
        [-2.0370e-05, -1.8224e-05,  1.1563e-05,  ..., -1.8448e-05,
         -1.0267e-05, -1.3277e-05],
        [-2.5168e-05, -2.2486e-05,  1.4387e-05,  ..., -2.3142e-05,
         -1.2822e-05, -1.6928e-05],
        [-3.2276e-05, -2.8834e-05,  1.8358e-05,  ..., -2.8998e-05,
         -1.6436e-05, -2.0429e-05]], device='cuda:0')
Loss: 1.0612872838974


Running epoch 0, step 687, batch 687
Sampled inputs[:2]: tensor([[    0,   586,  1016,  ...,  7151,  8280,   300],
        [    0,   446, 21112,  ..., 22092,    22,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5952e-03, -2.8168e-03,  2.2921e-04,  ..., -1.8523e-03,
          2.4937e-03,  8.8229e-04],
        [-1.3225e-05, -1.1854e-05,  7.1377e-06,  ..., -1.2420e-05,
         -7.6182e-06, -9.1270e-06],
        [-2.3574e-05, -2.1011e-05,  1.3284e-05,  ..., -2.1160e-05,
         -1.1690e-05, -1.5102e-05],
        [-2.9042e-05, -2.5868e-05,  1.6473e-05,  ..., -2.6479e-05,
         -1.4566e-05, -1.9208e-05],
        [-3.7313e-05, -3.3215e-05,  2.1070e-05,  ..., -3.3289e-05,
         -1.8746e-05, -2.3246e-05]], device='cuda:0')
Loss: 1.0805778503417969
Graident accumulation at epoch 0, step 687, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0233, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0225,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0278,  ...,  0.0288, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6107e-04, -2.6462e-05, -1.1570e-04,  ..., -4.3271e-05,
          1.1639e-04,  7.3560e-05],
        [-1.6392e-05, -1.1375e-05,  8.2209e-06,  ..., -1.3924e-05,
         -7.8953e-06, -9.9649e-06],
        [ 5.3399e-05,  4.1811e-05, -3.4476e-05,  ...,  4.9191e-05,
          2.5387e-05,  2.4450e-05],
        [-2.4717e-05, -1.8209e-05,  1.3753e-05,  ..., -2.1149e-05,
         -1.1409e-05, -1.5474e-05],
        [-3.5932e-05, -2.4202e-05,  1.8740e-05,  ..., -2.9468e-05,
         -1.6160e-05, -2.1052e-05]], device='cuda:0')
optimizer state dict: tensor([[6.5554e-08, 3.1469e-08, 3.0624e-08,  ..., 2.5854e-08, 5.8811e-08,
         1.3022e-08],
        [5.1780e-11, 2.9153e-11, 3.7351e-12,  ..., 3.6480e-11, 3.1060e-12,
         1.0541e-11],
        [2.2790e-09, 1.0919e-09, 3.4740e-10,  ..., 1.6741e-09, 2.3438e-10,
         6.5984e-10],
        [4.3180e-10, 3.3507e-10, 4.0077e-11,  ..., 3.6058e-10, 3.8186e-11,
         1.6716e-10],
        [2.3294e-10, 1.3196e-10, 2.2002e-11,  ..., 1.7103e-10, 1.4822e-11,
         5.2699e-11]], device='cuda:0')
optimizer state dict: 86.0
lr: [1.5686437100081734e-05, 1.5686437100081734e-05]
scheduler_last_epoch: 86


Running epoch 0, step 688, batch 688
Sampled inputs[:2]: tensor([[    0,    30,  1869,  ...,  4998, 44266,    12],
        [    0,   266, 30368,  ...,   950,   266,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1639e-05,  4.5308e-06,  5.9047e-05,  ..., -1.7996e-05,
          1.1497e-05, -4.1822e-05],
        [-1.7583e-06, -1.5497e-06,  8.3074e-07,  ..., -1.5572e-06,
         -7.1153e-07, -9.9838e-07],
        [-3.3081e-06, -2.8908e-06,  1.6168e-06,  ..., -2.8610e-06,
         -1.2368e-06, -1.7807e-06],
        [-3.9339e-06, -3.4273e-06,  1.9222e-06,  ..., -3.4273e-06,
         -1.4752e-06, -2.1756e-06],
        [-5.0664e-06, -4.4107e-06,  2.4587e-06,  ..., -4.3809e-06,
         -1.9372e-06, -2.6673e-06]], device='cuda:0')
Loss: 1.0687108039855957


Running epoch 0, step 689, batch 689
Sampled inputs[:2]: tensor([[    0,     9, 25368,  ...,   271,   266,  1136],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2630e-04,  1.8026e-06, -7.5819e-05,  ..., -5.4643e-05,
         -2.5166e-05, -1.1944e-04],
        [-3.3379e-06, -3.0324e-06,  1.5683e-06,  ..., -3.0845e-06,
         -1.4603e-06, -2.0564e-06],
        [-6.3330e-06, -5.6475e-06,  3.1441e-06,  ..., -5.6177e-06,
         -2.4512e-06, -3.6284e-06],
        [-7.6294e-06, -6.8098e-06,  3.7849e-06,  ..., -6.8545e-06,
         -2.9951e-06, -4.5151e-06],
        [-9.5963e-06, -8.5235e-06,  4.7535e-06,  ..., -8.4937e-06,
         -3.7923e-06, -5.3495e-06]], device='cuda:0')
Loss: 1.0573859214782715


Running epoch 0, step 690, batch 690
Sampled inputs[:2]: tensor([[   0,  273,  298,  ..., 7437, 2767,  518],
        [   0, 1217,    9,  ..., 1821,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6243e-04,  3.1723e-04, -1.6094e-04,  ...,  5.5207e-05,
         -2.7953e-04, -1.5971e-04],
        [-4.9546e-06, -4.4703e-06,  2.3469e-06,  ..., -4.6641e-06,
         -2.2389e-06, -3.1292e-06],
        [-9.4175e-06, -8.3596e-06,  4.7237e-06,  ..., -8.4788e-06,
         -3.7402e-06, -5.5060e-06],
        [-1.1370e-05, -1.0088e-05,  5.6922e-06,  ..., -1.0386e-05,
         -4.5896e-06, -6.8843e-06],
        [-1.4246e-05, -1.2606e-05,  7.1228e-06,  ..., -1.2785e-05,
         -5.7891e-06, -8.0913e-06]], device='cuda:0')
Loss: 1.0557219982147217


Running epoch 0, step 691, batch 691
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0,  809,  367,  ...,  717,  287, 1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1982e-04,  6.2940e-04, -4.0528e-04,  ...,  2.0477e-04,
         -4.7292e-04, -1.4939e-04],
        [-6.6459e-06, -5.9083e-06,  3.0361e-06,  ..., -6.2138e-06,
         -3.1106e-06, -4.2915e-06],
        [-1.2606e-05, -1.1042e-05,  6.1393e-06,  ..., -1.1250e-05,
         -5.1856e-06, -7.5027e-06],
        [-1.5169e-05, -1.3292e-05,  7.3761e-06,  ..., -1.3754e-05,
         -6.3330e-06, -9.3579e-06],
        [-1.9014e-05, -1.6600e-05,  9.2536e-06,  ..., -1.6898e-05,
         -7.9647e-06, -1.0982e-05]], device='cuda:0')
Loss: 1.0117818117141724


Running epoch 0, step 692, batch 692
Sampled inputs[:2]: tensor([[    0,   266, 20604,  ...,   409, 13764,  6048],
        [    0,  1690,  2558,  ...,  2025,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3238e-04,  4.1609e-04, -4.2886e-04,  ...,  1.9078e-04,
         -4.5663e-04, -2.1482e-04],
        [-8.5160e-06, -7.5176e-06,  3.8706e-06,  ..., -7.7933e-06,
         -3.9414e-06, -5.3272e-06],
        [-1.6019e-05, -1.3992e-05,  7.7188e-06,  ..., -1.4111e-05,
         -6.6310e-06, -9.3505e-06],
        [-1.9282e-05, -1.6838e-05,  9.2834e-06,  ..., -1.7226e-05,
         -8.0615e-06, -1.1623e-05],
        [-2.4229e-05, -2.1100e-05,  1.1668e-05,  ..., -2.1279e-05,
         -1.0230e-05, -1.3739e-05]], device='cuda:0')
Loss: 1.09714937210083


Running epoch 0, step 693, batch 693
Sampled inputs[:2]: tensor([[    0,   516,   596,  ...,  3109,   287,   394],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3384e-04,  4.2538e-04, -5.3938e-04,  ...,  2.3358e-04,
         -5.5416e-04, -2.2287e-04],
        [-1.0401e-05, -9.1419e-06,  4.7870e-06,  ..., -9.3207e-06,
         -4.7311e-06, -6.3628e-06],
        [-1.9565e-05, -1.7047e-05,  9.4995e-06,  ..., -1.6958e-05,
         -8.0019e-06, -1.1258e-05],
        [ 4.9638e-04,  4.2072e-04, -1.3529e-04,  ...,  3.8764e-04,
          2.1590e-04,  2.5307e-04],
        [-2.9564e-05, -2.5690e-05,  1.4320e-05,  ..., -2.5541e-05,
         -1.2346e-05, -1.6540e-05]], device='cuda:0')
Loss: 1.074266791343689


Running epoch 0, step 694, batch 694
Sampled inputs[:2]: tensor([[    0,  2923,   266,  ...,  7763,   360,  1255],
        [    0,   266,   944,  ..., 14981,  1952,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1225e-04,  3.9331e-04, -5.4493e-04,  ...,  1.9835e-04,
         -4.9913e-04, -2.6274e-04],
        [-1.2264e-05, -1.0677e-05,  5.5693e-06,  ..., -1.0893e-05,
         -5.6103e-06, -7.4431e-06],
        [-2.3007e-05, -1.9878e-05,  1.1034e-05,  ..., -1.9789e-05,
         -9.4697e-06, -1.3150e-05],
        [ 4.9230e-04,  4.1738e-04, -1.3349e-04,  ...,  3.8425e-04,
          2.1414e-04,  2.5077e-04],
        [-3.4720e-05, -2.9922e-05,  1.6615e-05,  ..., -2.9773e-05,
         -1.4596e-05, -1.9297e-05]], device='cuda:0')
Loss: 1.0777957439422607


Running epoch 0, step 695, batch 695
Sampled inputs[:2]: tensor([[    0,   527,   496,  ...,    12,   795,  8296],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0660e-04,  2.9414e-04, -5.6813e-04,  ...,  2.1248e-04,
         -4.8536e-04, -2.3996e-04],
        [-1.4082e-05, -1.2256e-05,  6.3665e-06,  ..., -1.2472e-05,
         -6.4634e-06, -8.5160e-06],
        [-2.6435e-05, -2.2858e-05,  1.2614e-05,  ..., -2.2724e-05,
         -1.0937e-05, -1.5087e-05],
        [ 4.8828e-04,  4.1389e-04, -1.3164e-04,  ...,  3.8080e-04,
          2.1243e-04,  2.4843e-04],
        [-3.9935e-05, -3.4451e-05,  1.8999e-05,  ..., -3.4213e-05,
         -1.6890e-05, -2.2173e-05]], device='cuda:0')
Loss: 1.0825697183609009
Graident accumulation at epoch 0, step 695, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0233, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0225,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0278,  ...,  0.0288, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2431e-04,  5.5977e-06, -1.6094e-04,  ..., -1.7696e-05,
          5.6211e-05,  4.2208e-05],
        [-1.6161e-05, -1.1463e-05,  8.0355e-06,  ..., -1.3779e-05,
         -7.7521e-06, -9.8201e-06],
        [ 4.5416e-05,  3.5344e-05, -2.9767e-05,  ...,  4.2000e-05,
          2.1755e-05,  2.0496e-05],
        [ 2.6583e-05,  2.5001e-05, -7.8638e-07,  ...,  1.9045e-05,
          1.0975e-05,  1.0916e-05],
        [-3.6332e-05, -2.5227e-05,  1.8766e-05,  ..., -2.9943e-05,
         -1.6233e-05, -2.1164e-05]], device='cuda:0')
optimizer state dict: tensor([[6.5531e-08, 3.1524e-08, 3.0916e-08,  ..., 2.5874e-08, 5.8987e-08,
         1.3067e-08],
        [5.1927e-11, 2.9274e-11, 3.7719e-12,  ..., 3.6599e-11, 3.1446e-12,
         1.0603e-11],
        [2.2774e-09, 1.0913e-09, 3.4721e-10,  ..., 1.6730e-09, 2.3427e-10,
         6.5941e-10],
        [6.6979e-10, 5.0605e-10, 5.7367e-11,  ..., 5.0523e-10, 8.3275e-11,
         2.2871e-10],
        [2.3430e-10, 1.3302e-10, 2.2341e-11,  ..., 1.7203e-10, 1.5092e-11,
         5.3138e-11]], device='cuda:0')
optimizer state dict: 87.0
lr: [1.5584320179540008e-05, 1.5584320179540008e-05]
scheduler_last_epoch: 87


Running epoch 0, step 696, batch 696
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0,   300,  3808,  ...,   496,    14,  1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2356e-05,  6.5483e-05, -6.0510e-05,  ...,  6.3332e-05,
          1.1532e-05, -1.7871e-05],
        [-1.6913e-06, -1.3635e-06,  6.5193e-07,  ..., -1.6168e-06,
         -8.1956e-07, -1.1101e-06],
        [-3.2037e-06, -2.5779e-06,  1.3486e-06,  ..., -2.9057e-06,
         -1.3784e-06, -1.9222e-06],
        [-3.8743e-06, -3.1143e-06,  1.6242e-06,  ..., -3.6210e-06,
         -1.7211e-06, -2.4438e-06],
        [-4.7088e-06, -3.7998e-06,  1.9968e-06,  ..., -4.2617e-06,
         -2.0713e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0738959312438965


Running epoch 0, step 697, batch 697
Sampled inputs[:2]: tensor([[    0,  1008,   266,  ...,  1941,   437,  1626],
        [    0,   328, 16219,  ..., 14559,   351,   587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1379e-05,  4.2550e-05, -1.4683e-04,  ...,  7.7777e-05,
          6.8129e-05, -3.0357e-05],
        [-3.5465e-06, -2.8461e-06,  1.4231e-06,  ..., -3.1963e-06,
         -1.5497e-06, -2.2203e-06],
        [-6.6906e-06, -5.3942e-06,  2.8685e-06,  ..., -5.8115e-06,
         -2.6599e-06, -3.8892e-06],
        [-8.1658e-06, -6.5416e-06,  3.4869e-06,  ..., -7.2271e-06,
         -3.2932e-06, -4.9621e-06],
        [-9.8944e-06, -7.9721e-06,  4.2617e-06,  ..., -8.5533e-06,
         -4.0233e-06, -5.6028e-06]], device='cuda:0')
Loss: 1.057471513748169


Running epoch 0, step 698, batch 698
Sampled inputs[:2]: tensor([[   0,  638, 1862,  ...,   14, 7869,   14],
        [   0,  635,   13,  ...,   13, 4710, 1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0875e-04,  1.9109e-04, -7.0575e-05,  ...,  2.9084e-05,
          4.7909e-05,  6.5322e-05],
        [-5.3123e-06, -4.3362e-06,  2.1160e-06,  ..., -4.7833e-06,
         -2.3693e-06, -3.3304e-06],
        [-9.9987e-06, -8.1956e-06,  4.2617e-06,  ..., -8.7023e-06,
         -4.0755e-06, -5.8264e-06],
        [-1.2070e-05, -9.8199e-06,  5.1185e-06,  ..., -1.0684e-05,
         -4.9844e-06, -7.3463e-06],
        [-1.4782e-05, -1.2085e-05,  6.3181e-06,  ..., -1.2815e-05,
         -6.1393e-06, -8.3745e-06]], device='cuda:0')
Loss: 1.0549840927124023


Running epoch 0, step 699, batch 699
Sampled inputs[:2]: tensor([[    0,   266, 28695,  ...,   278,   266,  6087],
        [    0,  1487,  2511,  ..., 27735,   760,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2794e-04,  2.7747e-04, -1.1782e-04,  ...,  1.1610e-04,
          3.4009e-05, -1.1886e-05],
        [-7.2345e-06, -5.8040e-06,  2.9393e-06,  ..., -6.3404e-06,
         -3.0398e-06, -4.3288e-06],
        [-1.3843e-05, -1.1146e-05,  5.9679e-06,  ..., -1.1787e-05,
         -5.3346e-06, -7.7635e-06],
        [-1.6481e-05, -1.3202e-05,  7.0855e-06,  ..., -1.4260e-05,
         -6.4149e-06, -9.6411e-06],
        [-2.0325e-05, -1.6347e-05,  8.7768e-06,  ..., -1.7256e-05,
         -8.0019e-06, -1.1116e-05]], device='cuda:0')
Loss: 1.0671429634094238


Running epoch 0, step 700, batch 700
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ...,   298,   365,  4920],
        [    0, 23749, 27341,  ..., 34110,   342,  9672]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5538e-04,  5.7248e-04, -1.8544e-04,  ...,  3.4706e-04,
         -3.1528e-04, -1.3814e-04],
        [-8.9779e-06, -7.2122e-06,  3.6471e-06,  ..., -7.9647e-06,
         -3.7178e-06, -5.3793e-06],
        [-1.7107e-05, -1.3769e-05,  7.4059e-06,  ..., -1.4707e-05,
         -6.4597e-06, -9.5740e-06],
        [-2.0504e-05, -1.6421e-05,  8.8438e-06,  ..., -1.7926e-05,
         -7.8157e-06, -1.1966e-05],
        [-2.5213e-05, -2.0251e-05,  1.0923e-05,  ..., -2.1607e-05,
         -9.7305e-06, -1.3754e-05]], device='cuda:0')
Loss: 1.0572768449783325


Running epoch 0, step 701, batch 701
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0,   494,   298,  ...,   408, 32859, 14550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0213e-04,  8.3204e-04, -2.5185e-04,  ...,  5.2647e-04,
         -4.5483e-04, -2.2051e-04],
        [-1.0692e-05, -8.5458e-06,  4.2543e-06,  ..., -9.5442e-06,
         -4.7162e-06, -6.6012e-06],
        [-2.0415e-05, -1.6317e-05,  8.7097e-06,  ..., -1.7598e-05,
         -8.1658e-06, -1.1720e-05],
        [-2.4438e-05, -1.9431e-05,  1.0379e-05,  ..., -2.1398e-05,
         -9.8571e-06, -1.4588e-05],
        [-3.0041e-05, -2.3976e-05,  1.2845e-05,  ..., -2.5779e-05,
         -1.2234e-05, -1.6764e-05]], device='cuda:0')
Loss: 1.0563808679580688


Running epoch 0, step 702, batch 702
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,  3253,  1573,  ...,   298,   358,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0367e-04,  1.1089e-03, -1.6012e-04,  ...,  5.4297e-04,
         -5.8651e-04, -2.1778e-04],
        [-1.2577e-05, -1.0043e-05,  5.0068e-06,  ..., -1.1161e-05,
         -5.4911e-06, -7.6294e-06],
        [-2.4095e-05, -1.9222e-05,  1.0245e-05,  ..., -2.0698e-05,
         -9.5814e-06, -1.3657e-05],
        [-2.8670e-05, -2.2784e-05,  1.2144e-05,  ..., -2.4989e-05,
         -1.1466e-05, -1.6883e-05],
        [ 1.4096e-04,  9.0939e-05, -3.9229e-05,  ...,  1.2365e-04,
          1.1979e-04,  9.0711e-05]], device='cuda:0')
Loss: 1.0838109254837036


Running epoch 0, step 703, batch 703
Sampled inputs[:2]: tensor([[    0,  1049,    12,  ...,   292,  3963,   755],
        [    0, 10446,    14,  ...,   266,  1164,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1490e-04,  1.1508e-03, -1.8087e-04,  ...,  5.8627e-04,
         -6.1477e-04, -3.4151e-04],
        [-1.4208e-05, -1.1444e-05,  5.6773e-06,  ..., -1.2755e-05,
         -6.2920e-06, -8.7619e-06],
        [-2.7180e-05, -2.1830e-05,  1.1638e-05,  ..., -2.3514e-05,
         -1.0885e-05, -1.5564e-05],
        [-3.2395e-05, -2.5928e-05,  1.3821e-05,  ..., -2.8476e-05,
         -1.3083e-05, -1.9327e-05],
        [ 1.3646e-04,  8.7154e-05, -3.7203e-05,  ...,  1.1960e-04,
          1.1787e-04,  8.8043e-05]], device='cuda:0')
Loss: 1.0696356296539307
Graident accumulation at epoch 0, step 703, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0233, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0278,  ...,  0.0289, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.0385e-05,  1.2012e-04, -1.6294e-04,  ...,  4.2701e-05,
         -1.0888e-05,  3.8367e-06],
        [-1.5966e-05, -1.1462e-05,  7.7997e-06,  ..., -1.3677e-05,
         -7.6061e-06, -9.7142e-06],
        [ 3.8156e-05,  2.9627e-05, -2.5627e-05,  ...,  3.5448e-05,
          1.8491e-05,  1.6890e-05],
        [ 2.0685e-05,  1.9908e-05,  6.7434e-07,  ...,  1.4293e-05,
          8.5690e-06,  7.8919e-06],
        [-1.9053e-05, -1.3988e-05,  1.3169e-05,  ..., -1.4989e-05,
         -2.8223e-06, -1.0243e-05]], device='cuda:0')
optimizer state dict: tensor([[6.5976e-08, 3.2817e-08, 3.0918e-08,  ..., 2.6191e-08, 5.9306e-08,
         1.3171e-08],
        [5.2077e-11, 2.9376e-11, 3.8003e-12,  ..., 3.6725e-11, 3.1811e-12,
         1.0669e-11],
        [2.2758e-09, 1.0907e-09, 3.4700e-10,  ..., 1.6719e-09, 2.3415e-10,
         6.5899e-10],
        [6.7017e-10, 5.0621e-10, 5.7501e-11,  ..., 5.0553e-10, 8.3363e-11,
         2.2886e-10],
        [2.5269e-10, 1.4048e-10, 2.3703e-11,  ..., 1.8616e-10, 2.8971e-11,
         6.0836e-11]], device='cuda:0')
optimizer state dict: 88.0
lr: [1.5481349926128634e-05, 1.5481349926128634e-05]
scheduler_last_epoch: 88


Running epoch 0, step 704, batch 704
Sampled inputs[:2]: tensor([[   0,  266, 5528,  ...,  685,  266, 1231],
        [   0, 3468,  278,  ..., 2442,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5893e-04, -1.7883e-04, -7.1826e-05,  ...,  1.9112e-05,
         -8.3243e-06, -7.6254e-05],
        [-1.8477e-06, -1.4156e-06,  7.0035e-07,  ..., -1.6019e-06,
         -7.6741e-07, -1.2144e-06],
        [-3.4720e-06, -2.6673e-06,  1.4007e-06,  ..., -2.9355e-06,
         -1.3262e-06, -2.1160e-06],
        [-4.1425e-06, -3.1590e-06,  1.6615e-06,  ..., -3.5465e-06,
         -1.5870e-06, -2.6524e-06],
        [-4.9770e-06, -3.8147e-06,  1.9968e-06,  ..., -4.1723e-06,
         -1.9372e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.0595860481262207


Running epoch 0, step 705, batch 705
Sampled inputs[:2]: tensor([[    0,   680,   401,  ...,  2872,   292, 23535],
        [    0,   271,  4219,  ...,   644,    14,  3607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5003e-04,  3.7988e-04, -4.3089e-04,  ...,  6.0094e-04,
         -9.3898e-04, -7.1934e-04],
        [-3.3230e-06, -2.5854e-06,  1.2591e-06,  ..., -3.2634e-06,
         -1.7583e-06, -2.6450e-06],
        [-6.1542e-06, -4.7982e-06,  2.5854e-06,  ..., -5.6624e-06,
         -2.8461e-06, -4.3213e-06],
        [-7.5698e-06, -5.8413e-06,  3.1814e-06,  ..., -7.2122e-06,
         -3.6284e-06, -5.7369e-06],
        [-8.9109e-06, -6.9290e-06,  3.7327e-06,  ..., -8.0764e-06,
         -4.1574e-06, -5.9605e-06]], device='cuda:0')
Loss: 1.0263473987579346


Running epoch 0, step 706, batch 706
Sampled inputs[:2]: tensor([[    0,   894,    73,  ...,  2323,   909,  4103],
        [    0, 12165,    12,  ...,  2860, 10718,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7992e-04,  2.7485e-04, -4.4641e-04,  ...,  5.6589e-04,
         -8.7064e-04, -7.1934e-04],
        [-5.3048e-06, -4.0382e-06,  2.0340e-06,  ..., -4.8652e-06,
         -2.5034e-06, -3.8147e-06],
        [ 1.0735e-04,  9.3623e-05,  1.3042e-05,  ...,  4.2375e-05,
          9.4224e-05,  4.1932e-05],
        [-1.1981e-05, -9.0897e-06,  4.9844e-06,  ..., -1.0759e-05,
         -5.1782e-06, -8.3148e-06],
        [-1.4246e-05, -1.0893e-05,  5.8934e-06,  ..., -1.2338e-05,
         -6.1244e-06, -8.8960e-06]], device='cuda:0')
Loss: 1.0821441411972046


Running epoch 0, step 707, batch 707
Sampled inputs[:2]: tensor([[    0,  1188,    12,  ...,   292, 23032,   689],
        [    0, 34525,  2008,  ...,  1194,   300, 11120]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5758e-04,  4.3132e-04, -5.8983e-04,  ...,  8.8802e-04,
         -1.0251e-03, -7.6222e-04],
        [-7.0557e-06, -5.3644e-06,  2.7455e-06,  ..., -6.4671e-06,
         -3.2708e-06, -5.0068e-06],
        [ 1.0409e-04,  9.1135e-05,  1.4472e-05,  ...,  3.9499e-05,
          9.2920e-05,  3.9890e-05],
        [-1.5974e-05, -1.2115e-05,  6.7353e-06,  ..., -1.4365e-05,
         -6.8024e-06, -1.0967e-05],
        [-1.9014e-05, -1.4544e-05,  7.9796e-06,  ..., -1.6510e-05,
         -8.0913e-06, -1.1772e-05]], device='cuda:0')
Loss: 1.0615642070770264


Running epoch 0, step 708, batch 708
Sampled inputs[:2]: tensor([[    0,   292,   960,  ...,   271,  1356,    14],
        [    0,   292, 41192,  ..., 34298,  8741,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7867e-04,  5.1346e-04, -5.9924e-04,  ...,  8.2208e-04,
         -1.1577e-03, -9.1403e-04],
        [-8.8438e-06, -6.6906e-06,  3.4049e-06,  ..., -8.0764e-06,
         -4.0792e-06, -6.2510e-06],
        [ 1.0065e-04,  8.8587e-05,  1.5836e-05,  ...,  3.6504e-05,
          9.1497e-05,  3.7670e-05],
        [-2.0057e-05, -1.5110e-05,  8.3372e-06,  ..., -1.7971e-05,
         -8.4862e-06, -1.3709e-05],
        [-2.3961e-05, -1.8224e-05,  9.9316e-06,  ..., -2.0802e-05,
         -1.0192e-05, -1.4871e-05]], device='cuda:0')
Loss: 1.058816909790039


Running epoch 0, step 709, batch 709
Sampled inputs[:2]: tensor([[    0,    73,    14,  ...,   650,    13,  3658],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4323e-04,  5.6230e-04, -4.2929e-04,  ...,  8.9580e-04,
         -1.2076e-03, -9.4681e-04],
        [-1.0781e-05, -8.1956e-06,  4.1723e-06,  ..., -9.7230e-06,
         -4.8950e-06, -7.4729e-06],
        [ 9.6967e-05,  8.5711e-05,  1.7385e-05,  ...,  3.3449e-05,
          9.0074e-05,  3.5495e-05],
        [-2.4348e-05, -1.8448e-05,  1.0140e-05,  ..., -2.1592e-05,
         -1.0155e-05, -1.6376e-05],
        [-2.9206e-05, -2.2337e-05,  1.2137e-05,  ..., -2.5123e-05,
         -1.2264e-05, -1.7896e-05]], device='cuda:0')
Loss: 1.0568280220031738


Running epoch 0, step 710, batch 710
Sampled inputs[:2]: tensor([[    0,    80, 10802,  ...,   287, 28533, 25359],
        [    0,  2544,   394,  ...,    14,  1062,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9940e-04,  4.4699e-04, -4.5998e-04,  ...,  9.2024e-04,
         -9.3696e-04, -9.5070e-04],
        [-1.2569e-05, -9.5665e-06,  4.7982e-06,  ..., -1.1303e-05,
         -5.7556e-06, -8.6650e-06],
        [ 9.3391e-05,  8.2984e-05,  1.8749e-05,  ...,  3.0424e-05,
          8.8517e-05,  3.3289e-05],
        [-2.8431e-05, -2.1547e-05,  1.1690e-05,  ..., -2.5108e-05,
         -1.1951e-05, -1.8999e-05],
        [-3.4332e-05, -2.6211e-05,  1.4104e-05,  ..., -2.9445e-05,
         -1.4514e-05, -2.0921e-05]], device='cuda:0')
Loss: 1.094864010810852


Running epoch 0, step 711, batch 711
Sampled inputs[:2]: tensor([[    0, 32878,   593,  ...,   437,  1329,   644],
        [    0, 23230,    12,  ...,  5092,   741,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3582e-04,  3.7961e-04, -3.9226e-04,  ...,  9.4328e-04,
         -9.2393e-04, -9.0561e-04],
        [-1.4380e-05, -1.0960e-05,  5.5395e-06,  ..., -1.2897e-05,
         -6.4969e-06, -9.8869e-06],
        [ 8.9934e-05,  8.0316e-05,  2.0239e-05,  ...,  2.7444e-05,
          8.7198e-05,  3.1114e-05],
        [-3.2574e-05, -2.4706e-05,  1.3478e-05,  ..., -2.8700e-05,
         -1.3523e-05, -2.1711e-05],
        [-3.9190e-05, -2.9966e-05,  1.6190e-05,  ..., -3.3617e-05,
         -1.6421e-05, -2.3872e-05]], device='cuda:0')
Loss: 1.0590275526046753
Graident accumulation at epoch 0, step 711, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0233, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0159,  0.0152, -0.0278,  ...,  0.0289, -0.0146, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.2765e-05,  1.4607e-04, -1.8587e-04,  ...,  1.3276e-04,
         -1.0219e-04, -8.7108e-05],
        [-1.5807e-05, -1.1411e-05,  7.5736e-06,  ..., -1.3599e-05,
         -7.4952e-06, -9.7315e-06],
        [ 4.3334e-05,  3.4696e-05, -2.1040e-05,  ...,  3.4648e-05,
          2.5361e-05,  1.8312e-05],
        [ 1.5359e-05,  1.5447e-05,  1.9547e-06,  ...,  9.9940e-06,
          6.3598e-06,  4.9316e-06],
        [-2.1066e-05, -1.5586e-05,  1.3471e-05,  ..., -1.6852e-05,
         -4.1821e-06, -1.1606e-05]], device='cuda:0')
optimizer state dict: tensor([[6.5929e-08, 3.2928e-08, 3.1041e-08,  ..., 2.7055e-08, 6.0101e-08,
         1.3978e-08],
        [5.2231e-11, 2.9466e-11, 3.8272e-12,  ..., 3.6855e-11, 3.2201e-12,
         1.0756e-11],
        [2.2817e-09, 1.0960e-09, 3.4706e-10,  ..., 1.6709e-09, 2.4152e-10,
         6.5930e-10],
        [6.7056e-10, 5.0632e-10, 5.7625e-11,  ..., 5.0585e-10, 8.3463e-11,
         2.2910e-10],
        [2.5397e-10, 1.4124e-10, 2.3941e-11,  ..., 1.8711e-10, 2.9212e-11,
         6.1345e-11]], device='cuda:0')
optimizer state dict: 89.0
lr: [1.537754207460116e-05, 1.537754207460116e-05]
scheduler_last_epoch: 89


Running epoch 0, step 712, batch 712
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,    12,   287,  ...,  4626,    27,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3015e-04, -3.8827e-05,  7.5957e-05,  ..., -1.5125e-04,
         -4.8574e-05, -1.4588e-04],
        [-1.8403e-06, -1.3188e-06,  7.0781e-07,  ..., -1.6242e-06,
         -8.7544e-07, -1.3262e-06],
        [-3.3975e-06, -2.4736e-06,  1.4082e-06,  ..., -2.8908e-06,
         -1.5050e-06, -2.2203e-06],
        [-4.1127e-06, -2.9504e-06,  1.7211e-06,  ..., -3.5763e-06,
         -1.8328e-06, -2.8610e-06],
        [-4.8578e-06, -3.5465e-06,  2.0117e-06,  ..., -4.1127e-06,
         -2.2203e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.0576586723327637


Running epoch 0, step 713, batch 713
Sampled inputs[:2]: tensor([[    0,    14, 30840,  ...,   287,   932,    14],
        [    0, 20202,   300,  ..., 15185,   287,  6573]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3769e-04, -1.6149e-05,  1.8401e-04,  ..., -7.7681e-05,
         -6.3064e-05,  4.8676e-05],
        [-3.6433e-06, -2.6748e-06,  1.4231e-06,  ..., -3.1888e-06,
         -1.7099e-06, -2.6524e-06],
        [-6.8247e-06, -5.0664e-06,  2.8685e-06,  ..., -5.7817e-06,
         -2.9802e-06, -4.5747e-06],
        [-8.1658e-06, -5.9754e-06,  3.4571e-06,  ..., -7.0184e-06,
         -3.5539e-06, -5.7817e-06],
        [-9.6262e-06, -7.1675e-06,  4.0382e-06,  ..., -8.1360e-06,
         -4.3362e-06, -6.1691e-06]], device='cuda:0')
Loss: 1.0469428300857544


Running epoch 0, step 714, batch 714
Sampled inputs[:2]: tensor([[   0,  531,   20,  ...,   12, 1644,  680],
        [   0,   14, 3445,  ...,  298,  527, 2732]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6859e-04, -1.3104e-04,  1.6830e-04,  ...,  2.2817e-05,
         -2.0186e-06,  1.1222e-04],
        [-5.3793e-06, -3.9488e-06,  2.0489e-06,  ..., -4.7684e-06,
         -2.5183e-06, -3.9563e-06],
        [-1.0207e-05, -7.5698e-06,  4.2319e-06,  ..., -8.6874e-06,
         -4.3809e-06, -6.8247e-06],
        [-1.2159e-05, -8.8662e-06,  5.0515e-06,  ..., -1.0520e-05,
         -5.2154e-06, -8.6129e-06],
        [-1.4335e-05, -1.0669e-05,  5.9381e-06,  ..., -1.2159e-05,
         -6.3330e-06, -9.1642e-06]], device='cuda:0')
Loss: 1.0374549627304077


Running epoch 0, step 715, batch 715
Sampled inputs[:2]: tensor([[    0,  1665,  6306,  ...,   300, 10204,   582],
        [    0, 10084,    12,  ..., 24717,   365,  1616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1522e-04, -2.8448e-04,  6.7090e-05,  ...,  3.9475e-05,
          1.0415e-04,  1.2697e-04],
        [-7.2643e-06, -5.3197e-06,  2.8200e-06,  ..., -6.4075e-06,
         -3.4049e-06, -5.2676e-06],
        [-1.3739e-05, -1.0163e-05,  5.7742e-06,  ..., -1.1668e-05,
         -5.9307e-06, -9.1195e-06],
        [-1.6421e-05, -1.1951e-05,  6.9141e-06,  ..., -1.4156e-05,
         -7.0706e-06, -1.1519e-05],
        [-1.9342e-05, -1.4365e-05,  8.1137e-06,  ..., -1.6361e-05,
         -8.5831e-06, -1.2279e-05]], device='cuda:0')
Loss: 1.066335678100586


Running epoch 0, step 716, batch 716
Sampled inputs[:2]: tensor([[   0,  367, 3675,  ...,   22, 3180,   14],
        [   0,  446, 1115,  ..., 1869, 4971, 1954]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3762e-04, -4.3751e-04,  5.9228e-06,  ...,  7.3606e-05,
          1.4625e-04,  2.6308e-04],
        [-9.2462e-06, -6.7875e-06,  3.6135e-06,  ..., -8.0243e-06,
         -4.2096e-06, -6.4373e-06],
        [-1.7464e-05, -1.2949e-05,  7.3388e-06,  ..., -1.4663e-05,
         -7.3686e-06, -1.1206e-05],
        [-2.0832e-05, -1.5199e-05,  8.7544e-06,  ..., -1.7732e-05,
         -8.7619e-06, -1.4082e-05],
        [-2.4766e-05, -1.8418e-05,  1.0364e-05,  ..., -2.0713e-05,
         -1.0744e-05, -1.5214e-05]], device='cuda:0')
Loss: 1.0857840776443481


Running epoch 0, step 717, batch 717
Sampled inputs[:2]: tensor([[   0, 1932,  278,  ...,  609,  271,  266],
        [   0, 1486,  292,  ..., 7484,   15, 5357]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0178e-04, -4.7896e-04,  1.5766e-04,  ...,  3.0248e-05,
          3.7586e-04,  3.9522e-04],
        [-1.1168e-05, -8.1733e-06,  4.3176e-06,  ..., -9.6336e-06,
         -5.0887e-06, -7.8306e-06],
        [-2.1204e-05, -1.5676e-05,  8.8215e-06,  ..., -1.7673e-05,
         -8.9258e-06, -1.3679e-05],
        [-2.5004e-05, -1.8209e-05,  1.0408e-05,  ..., -2.1145e-05,
         -1.0505e-05, -1.7002e-05],
        [-2.9981e-05, -2.2233e-05,  1.2420e-05,  ..., -2.4885e-05,
         -1.2964e-05, -1.8507e-05]], device='cuda:0')
Loss: 1.053331732749939


Running epoch 0, step 718, batch 718
Sampled inputs[:2]: tensor([[   0,  271,  957,  ..., 1597, 1276,  292],
        [   0, 1527,  292,  ..., 2122,  278, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8987e-04, -6.9927e-04,  2.7163e-04,  ..., -2.4535e-05,
          5.8465e-04,  6.3596e-04],
        [-1.2957e-05, -9.3654e-06,  4.9621e-06,  ..., -1.1228e-05,
         -6.0946e-06, -9.3132e-06],
        [-2.4632e-05, -1.8016e-05,  1.0185e-05,  ..., -2.0549e-05,
         -1.0639e-05, -1.6227e-05],
        [-2.9057e-05, -2.0921e-05,  1.2010e-05,  ..., -2.4632e-05,
         -1.2562e-05, -2.0206e-05],
        [-3.4779e-05, -2.5526e-05,  1.4320e-05,  ..., -2.8878e-05,
         -1.5393e-05, -2.1920e-05]], device='cuda:0')
Loss: 1.0293445587158203


Running epoch 0, step 719, batch 719
Sampled inputs[:2]: tensor([[    0,   298,   669,  ...,   287, 19731,    13],
        [    0,   344,  8133,  ...,   278,  1603,   674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3299e-03, -8.4110e-04,  5.3295e-04,  ..., -1.7465e-04,
          9.4244e-04,  9.6225e-04],
        [-1.4737e-05, -1.0587e-05,  5.5730e-06,  ..., -1.2837e-05,
         -7.0259e-06, -1.0692e-05],
        [-2.8193e-05, -2.0504e-05,  1.1563e-05,  ..., -2.3618e-05,
         -1.2301e-05, -1.8686e-05],
        [-3.3081e-05, -2.3678e-05,  1.3538e-05,  ..., -2.8148e-05,
         -1.4447e-05, -2.3142e-05],
        [-3.9697e-05, -2.8998e-05,  1.6227e-05,  ..., -3.3110e-05,
         -1.7747e-05, -2.5168e-05]], device='cuda:0')
Loss: 1.0344674587249756
Graident accumulation at epoch 0, step 719, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0234, -0.0194],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0152, -0.0278,  ...,  0.0289, -0.0146, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5348e-04,  4.7352e-05, -1.1399e-04,  ...,  1.0202e-04,
          2.2710e-06,  1.7827e-05],
        [-1.5700e-05, -1.1329e-05,  7.3736e-06,  ..., -1.3523e-05,
         -7.4483e-06, -9.8275e-06],
        [ 3.6181e-05,  2.9176e-05, -1.7780e-05,  ...,  2.8821e-05,
          2.1595e-05,  1.4612e-05],
        [ 1.0515e-05,  1.1534e-05,  3.1130e-06,  ...,  6.1798e-06,
          4.2792e-06,  2.1243e-06],
        [-2.2929e-05, -1.6927e-05,  1.3747e-05,  ..., -1.8478e-05,
         -5.5386e-06, -1.2962e-05]], device='cuda:0')
optimizer state dict: tensor([[6.7631e-08, 3.3603e-08, 3.1294e-08,  ..., 2.7058e-08, 6.0929e-08,
         1.4889e-08],
        [5.2396e-11, 2.9549e-11, 3.8544e-12,  ..., 3.6983e-11, 3.2662e-12,
         1.0860e-11],
        [2.2802e-09, 1.0954e-09, 3.4685e-10,  ..., 1.6698e-09, 2.4143e-10,
         6.5899e-10],
        [6.7098e-10, 5.0637e-10, 5.7751e-11,  ..., 5.0614e-10, 8.3588e-11,
         2.2940e-10],
        [2.5529e-10, 1.4194e-10, 2.4181e-11,  ..., 1.8802e-10, 2.9498e-11,
         6.1917e-11]], device='cuda:0')
optimizer state dict: 90.0
lr: [1.5272912487703465e-05, 1.5272912487703465e-05]
scheduler_last_epoch: 90


Running epoch 0, step 720, batch 720
Sampled inputs[:2]: tensor([[    0,  8588,  3937,  ...,   516,  1128,  2341],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1701e-05,  1.8070e-04, -1.3367e-05,  ...,  2.0450e-04,
         -1.5866e-04, -3.0351e-05],
        [-1.9968e-06, -1.3635e-06,  8.2329e-07,  ..., -1.6615e-06,
         -8.5309e-07, -1.2591e-06],
        [-3.8445e-06, -2.6673e-06,  1.6838e-06,  ..., -3.1292e-06,
         -1.5721e-06, -2.2352e-06],
        [-4.4107e-06, -2.9951e-06,  1.9372e-06,  ..., -3.6359e-06,
         -1.7732e-06, -2.7269e-06],
        [-5.5432e-06, -3.8445e-06,  2.4140e-06,  ..., -4.5002e-06,
         -2.3097e-06, -3.0845e-06]], device='cuda:0')
Loss: 1.0606275796890259


Running epoch 0, step 721, batch 721
Sampled inputs[:2]: tensor([[    0,  1550,   685,  ...,   943,  1239,   996],
        [    0, 24674,   513,  ...,  6099,    12,  4863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3438e-04,  1.3293e-04, -1.2496e-04,  ...,  3.3663e-04,
         -3.3403e-04, -1.3536e-04],
        [-3.9488e-06, -2.7791e-06,  1.6913e-06,  ..., -3.2410e-06,
         -1.5460e-06, -2.3842e-06],
        [-7.6294e-06, -5.4538e-06,  3.4347e-06,  ..., -6.1691e-06,
         -2.8983e-06, -4.2915e-06],
        [-8.6725e-06, -6.0946e-06,  3.9339e-06,  ..., -7.0781e-06,
         -3.2187e-06, -5.1707e-06],
        [-1.0908e-05, -7.8082e-06,  4.8727e-06,  ..., -8.8215e-06,
         -4.2766e-06, -5.9009e-06]], device='cuda:0')
Loss: 1.0314959287643433


Running epoch 0, step 722, batch 722
Sampled inputs[:2]: tensor([[    0, 10893, 10997,  ...,   367,   616,  7903],
        [    0,  9342,   600,  ...,   199, 12095,   291]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4616e-05,  3.8623e-04, -5.9504e-04,  ...,  8.0133e-04,
         -1.0962e-03, -4.3902e-04],
        [-5.6848e-06, -4.0382e-06,  2.4661e-06,  ..., -4.9248e-06,
         -2.3842e-06, -3.8147e-06],
        [-1.0803e-05, -7.7933e-06,  4.9770e-06,  ..., -9.0599e-06,
         -4.2766e-06, -6.5714e-06],
        [-1.2547e-05, -8.8811e-06,  5.8413e-06,  ..., -1.0744e-05,
         -4.9323e-06, -8.2254e-06],
        [-1.5467e-05, -1.1191e-05,  7.0781e-06,  ..., -1.2934e-05,
         -6.3032e-06, -8.9854e-06]], device='cuda:0')
Loss: 1.0054455995559692


Running epoch 0, step 723, batch 723
Sampled inputs[:2]: tensor([[    0,    34,     9,  ...,    19,    14, 45576],
        [    0,   259,  1329,  ...,   266,   706,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7973e-05,  4.5238e-04, -5.8701e-04,  ...,  7.6864e-04,
         -1.2887e-03, -4.2238e-04],
        [-7.7561e-06, -5.4687e-06,  3.2932e-06,  ..., -6.5640e-06,
         -3.2149e-06, -5.0142e-06],
        [-1.4946e-05, -1.0699e-05,  6.6981e-06,  ..., -1.2323e-05,
         -5.9009e-06, -8.8811e-06],
        [-1.7047e-05, -1.1995e-05,  7.7263e-06,  ..., -1.4305e-05,
         -6.6608e-06, -1.0833e-05],
        [-2.1309e-05, -1.5303e-05,  9.4920e-06,  ..., -1.7554e-05,
         -8.6725e-06, -1.2130e-05]], device='cuda:0')
Loss: 1.0691722631454468


Running epoch 0, step 724, batch 724
Sampled inputs[:2]: tensor([[    0,    14,  3741,  ...,   278, 12472, 10257],
        [    0,   292,  2860,  ...,   266,  7000,  7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1478e-04,  6.4822e-04, -6.8312e-04,  ...,  7.1753e-04,
         -1.5723e-03, -4.5536e-04],
        [-9.5144e-06, -6.7130e-06,  4.0792e-06,  ..., -8.1435e-06,
         -3.9749e-06, -6.2883e-06],
        [-1.8343e-05, -1.3128e-05,  8.3223e-06,  ..., -1.5229e-05,
         -7.2494e-06, -1.1057e-05],
        [-2.1011e-05, -1.4752e-05,  9.6485e-06,  ..., -1.7777e-05,
         -8.2254e-06, -1.3605e-05],
        [-2.6286e-05, -1.8880e-05,  1.1846e-05,  ..., -2.1785e-05,
         -1.0729e-05, -1.5125e-05]], device='cuda:0')
Loss: 1.0425838232040405


Running epoch 0, step 725, batch 725
Sampled inputs[:2]: tensor([[   0,  829,  874,  ...,  292,  380,  759],
        [   0, 2736, 2523,  ..., 4086, 4798, 7701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8435e-04,  6.2690e-04, -6.7543e-04,  ...,  6.6218e-04,
         -1.5082e-03, -3.6496e-04],
        [-1.1571e-05, -8.1807e-06,  4.9472e-06,  ..., -9.8124e-06,
         -4.8280e-06, -7.4431e-06],
        [-2.2247e-05, -1.5959e-05,  1.0043e-05,  ..., -1.8373e-05,
         -8.8289e-06, -1.3143e-05],
        [ 1.4470e-04,  1.0983e-04, -1.1576e-04,  ...,  2.5978e-04,
          9.7776e-05,  7.2913e-05],
        [-3.1859e-05, -2.2933e-05,  1.4305e-05,  ..., -2.6286e-05,
         -1.3068e-05, -1.8001e-05]], device='cuda:0')
Loss: 1.0829908847808838


Running epoch 0, step 726, batch 726
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,  4014,    88,  ...,  1103,    14,  1771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1655e-04,  6.1936e-04, -8.2107e-04,  ...,  8.0098e-04,
         -1.5972e-03, -3.1805e-04],
        [-1.3508e-05, -9.5293e-06,  5.7742e-06,  ..., -1.1422e-05,
         -5.5991e-06, -8.6203e-06],
        [ 6.7081e-05,  4.3331e-05, -2.1635e-05,  ...,  6.1659e-05,
          3.6709e-05,  3.0968e-05],
        [ 1.4038e-04,  1.0681e-04, -1.1381e-04,  ...,  2.5621e-04,
          9.6144e-05,  7.0320e-05],
        [-3.7253e-05, -2.6777e-05,  1.6734e-05,  ..., -3.0696e-05,
         -1.5199e-05, -2.0906e-05]], device='cuda:0')
Loss: 1.085874319076538


Running epoch 0, step 727, batch 727
Sampled inputs[:2]: tensor([[    0,  9829,   292,  ...,  2928,  1029,   271],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7315e-04,  6.4698e-04, -8.2645e-04,  ...,  8.4835e-04,
         -1.6784e-03, -3.3000e-04],
        [-1.5415e-05, -1.0855e-05,  6.5491e-06,  ..., -1.3024e-05,
         -6.4149e-06, -9.8944e-06],
        [ 6.3356e-05,  4.0693e-05, -2.0033e-05,  ...,  5.8589e-05,
          3.5189e-05,  2.8644e-05],
        [ 1.3618e-04,  1.0389e-04, -1.1201e-04,  ...,  2.5269e-04,
          9.4438e-05,  6.7548e-05],
        [-4.2588e-05, -3.0577e-05,  1.8999e-05,  ..., -3.5077e-05,
         -1.7434e-05, -2.4095e-05]], device='cuda:0')
Loss: 1.0621464252471924
Graident accumulation at epoch 0, step 727, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0026,  ..., -0.0023,  0.0234, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0152, -0.0279,  ...,  0.0289, -0.0146, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1081e-04,  1.0731e-04, -1.8523e-04,  ...,  1.7665e-04,
         -1.6579e-04, -1.6956e-05],
        [-1.5672e-05, -1.1282e-05,  7.2911e-06,  ..., -1.3473e-05,
         -7.3449e-06, -9.8342e-06],
        [ 3.8899e-05,  3.0327e-05, -1.8005e-05,  ...,  3.1798e-05,
          2.2954e-05,  1.6016e-05],
        [ 2.3082e-05,  2.0769e-05, -8.3992e-06,  ...,  3.0831e-05,
          1.3295e-05,  8.6667e-06],
        [-2.4895e-05, -1.8292e-05,  1.4272e-05,  ..., -2.0138e-05,
         -6.7282e-06, -1.4075e-05]], device='cuda:0')
optimizer state dict: tensor([[6.7638e-08, 3.3988e-08, 3.1946e-08,  ..., 2.7751e-08, 6.3685e-08,
         1.4983e-08],
        [5.2581e-11, 2.9637e-11, 3.8935e-12,  ..., 3.7115e-11, 3.3041e-12,
         1.0947e-11],
        [2.2819e-09, 1.0959e-09, 3.4690e-10,  ..., 1.6716e-09, 2.4243e-10,
         6.5915e-10],
        [6.8885e-10, 5.1666e-10, 7.0239e-11,  ..., 5.6949e-10, 9.2423e-11,
         2.3374e-10],
        [2.5685e-10, 1.4273e-10, 2.4517e-11,  ..., 1.8906e-10, 2.9772e-11,
         6.2436e-11]], device='cuda:0')
optimizer state dict: 91.0
lr: [1.5167477153749745e-05, 1.5167477153749745e-05]
scheduler_last_epoch: 91


Running epoch 0, step 728, batch 728
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,   768,  3227,  ...,  3487,    13, 31431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 0.0000e+00, -3.8980e-05,  7.6112e-05,  ..., -1.7123e-06,
          1.3543e-04,  6.9491e-06],
        [-1.9521e-06, -1.2740e-06,  8.2701e-07,  ..., -1.6987e-06,
         -8.3074e-07, -1.2740e-06],
        [-3.6657e-06, -2.4885e-06,  1.6913e-06,  ..., -3.1441e-06,
         -1.5348e-06, -2.1607e-06],
        [-4.1723e-06, -2.7567e-06,  1.9372e-06,  ..., -3.6359e-06,
         -1.7211e-06, -2.6524e-06],
        [-5.4538e-06, -3.7402e-06,  2.5034e-06,  ..., -4.6790e-06,
         -2.3842e-06, -3.0249e-06]], device='cuda:0')
Loss: 1.064892053604126


Running epoch 0, step 729, batch 729
Sampled inputs[:2]: tensor([[    0,   221,  6872,  ...,   806,   518,   266],
        [    0, 12440,   578,  ..., 25918,   287,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7233e-04, -7.0521e-04,  2.6873e-04,  ..., -1.3111e-05,
          1.1415e-03,  7.8266e-04],
        [-3.6210e-06, -2.4438e-06,  1.7658e-06,  ..., -3.2932e-06,
         -1.4976e-06, -2.5257e-06],
        [-6.9141e-06, -4.8131e-06,  3.5837e-06,  ..., -6.0946e-06,
         -2.7642e-06, -4.2915e-06],
        [-8.0168e-06, -5.4389e-06,  4.2319e-06,  ..., -7.2122e-06,
         -3.1292e-06, -5.4091e-06],
        [-1.0222e-05, -7.1675e-06,  5.2452e-06,  ..., -8.9705e-06,
         -4.2692e-06, -5.9456e-06]], device='cuda:0')
Loss: 1.0377435684204102


Running epoch 0, step 730, batch 730
Sampled inputs[:2]: tensor([[    0,  1167,  2667,  ...,  4769,    13,  5019],
        [    0,   565,  5539,  ...,    12,   516, 14426]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8581e-04, -8.2053e-04,  3.1550e-04,  ..., -1.6485e-04,
          1.3269e-03,  9.5321e-04],
        [-5.5730e-06, -3.7998e-06,  2.6971e-06,  ..., -4.9323e-06,
         -2.2538e-06, -3.6508e-06],
        [-1.0759e-05, -7.5251e-06,  5.4911e-06,  ..., -9.2983e-06,
         -4.2096e-06, -6.3777e-06],
        [-1.2279e-05, -8.3894e-06,  6.3628e-06,  ..., -1.0774e-05,
         -4.6790e-06, -7.8678e-06],
        [-1.5855e-05, -1.1131e-05,  8.0168e-06,  ..., -1.3649e-05,
         -6.4597e-06, -8.8513e-06]], device='cuda:0')
Loss: 1.0682240724563599


Running epoch 0, step 731, batch 731
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,   299,   292,  ...,   266,  2474,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6214e-04, -7.7370e-04,  1.5028e-04,  ..., -1.1545e-04,
          1.2968e-03,  9.2998e-04],
        [-7.2271e-06, -4.8652e-06,  3.4831e-06,  ..., -6.5714e-06,
         -3.1702e-06, -5.0068e-06],
        [-1.4037e-05, -9.7007e-06,  7.1675e-06,  ..., -1.2368e-05,
         -5.8860e-06, -8.7321e-06],
        [-1.6063e-05, -1.0803e-05,  8.3148e-06,  ..., -1.4424e-05,
         -6.6459e-06, -1.0848e-05],
        [-2.0593e-05, -1.4305e-05,  1.0431e-05,  ..., -1.8030e-05,
         -8.8885e-06, -1.2025e-05]], device='cuda:0')
Loss: 1.0529307126998901


Running epoch 0, step 732, batch 732
Sampled inputs[:2]: tensor([[    0,  2939,    14,  ...,  1702,  1481,   278],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7843e-04, -1.1699e-03,  2.7619e-04,  ..., -5.0574e-04,
          1.7790e-03,  1.2996e-03],
        [-8.8960e-06, -6.0424e-06,  4.2319e-06,  ..., -8.2180e-06,
         -3.9041e-06, -6.2585e-06],
        [-1.7405e-05, -1.2055e-05,  8.8587e-06,  ..., -1.5423e-05,
         -7.2122e-06, -1.0833e-05],
        [-1.9968e-05, -1.3486e-05,  1.0267e-05,  ..., -1.8060e-05,
         -8.1658e-06, -1.3545e-05],
        [-2.5570e-05, -1.7792e-05,  1.2949e-05,  ..., -2.2501e-05,
         -1.0900e-05, -1.4946e-05]], device='cuda:0')
Loss: 1.051282286643982


Running epoch 0, step 733, batch 733
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,   298, 49038,  ...,   288,  1690,  2736]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0996e-04, -1.2029e-03,  3.9054e-04,  ..., -8.6026e-04,
          2.3033e-03,  1.6429e-03],
        [-1.0788e-05, -7.2792e-06,  4.9956e-06,  ..., -9.8273e-06,
         -4.7795e-06, -7.6294e-06],
        [-2.1338e-05, -1.4693e-05,  1.0587e-05,  ..., -1.8656e-05,
         -8.9481e-06, -1.3366e-05],
        [-2.4110e-05, -1.6183e-05,  1.2077e-05,  ..., -2.1502e-05,
         -9.9689e-06, -1.6421e-05],
        [-3.1173e-05, -2.1547e-05,  1.5408e-05,  ..., -2.7061e-05,
         -1.3418e-05, -1.8343e-05]], device='cuda:0')
Loss: 1.0380759239196777


Running epoch 0, step 734, batch 734
Sampled inputs[:2]: tensor([[    0, 38136,    12,  ...,   367, 12851,  1040],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0144e-03, -1.5640e-03,  3.1801e-04,  ..., -1.0372e-03,
          2.7012e-03,  2.0523e-03],
        [-1.2740e-05, -8.6203e-06,  5.9195e-06,  ..., -1.1466e-05,
         -5.4874e-06, -8.7246e-06],
        [-2.5123e-05, -1.7345e-05,  1.2472e-05,  ..., -2.1785e-05,
         -1.0304e-05, -1.5318e-05],
        [-2.8253e-05, -1.9044e-05,  1.4164e-05,  ..., -2.4974e-05,
         -1.1392e-05, -1.8716e-05],
        [-3.6776e-05, -2.5541e-05,  1.8194e-05,  ..., -3.1739e-05,
         -1.5549e-05, -2.1100e-05]], device='cuda:0')
Loss: 1.0587823390960693


Running epoch 0, step 735, batch 735
Sampled inputs[:2]: tensor([[    0, 21801, 13084,  ...,  1738,  2946,    12],
        [    0,    12,  9328,  ...,    20,   408,   790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1320e-04, -1.1080e-03,  3.3457e-04,  ..., -1.0494e-03,
          2.8984e-03,  2.1669e-03],
        [-1.4752e-05, -9.8944e-06,  6.6273e-06,  ..., -1.3120e-05,
         -6.4634e-06, -1.0066e-05],
        [-2.9325e-05, -2.0042e-05,  1.4074e-05,  ..., -2.5094e-05,
         -1.2226e-05, -1.7837e-05],
        [-3.2574e-05, -2.1785e-05,  1.5795e-05,  ..., -2.8461e-05,
         -1.3389e-05, -2.1487e-05],
        [-4.2796e-05, -2.9445e-05,  2.0504e-05,  ..., -3.6448e-05,
         -1.8321e-05, -2.4527e-05]], device='cuda:0')
Loss: 1.0536155700683594
Graident accumulation at epoch 0, step 735, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0025,  ..., -0.0023,  0.0234, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0152, -0.0279,  ...,  0.0289, -0.0146, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8105e-04, -1.4215e-05, -1.3325e-04,  ...,  5.4049e-05,
          1.4062e-04,  2.0143e-04],
        [-1.5580e-05, -1.1143e-05,  7.2247e-06,  ..., -1.3437e-05,
         -7.2568e-06, -9.8574e-06],
        [ 3.2076e-05,  2.5290e-05, -1.4797e-05,  ...,  2.6109e-05,
          1.9436e-05,  1.2630e-05],
        [ 1.7516e-05,  1.6514e-05, -5.9798e-06,  ...,  2.4902e-05,
          1.0627e-05,  5.6513e-06],
        [-2.6685e-05, -1.9408e-05,  1.4895e-05,  ..., -2.1769e-05,
         -7.8875e-06, -1.5121e-05]], device='cuda:0')
optimizer state dict: tensor([[6.8232e-08, 3.5181e-08, 3.2026e-08,  ..., 2.8825e-08, 7.2021e-08,
         1.9664e-08],
        [5.2747e-11, 2.9705e-11, 3.9335e-12,  ..., 3.7250e-11, 3.3426e-12,
         1.1037e-11],
        [2.2805e-09, 1.0952e-09, 3.4675e-10,  ..., 1.6706e-09, 2.4234e-10,
         6.5881e-10],
        [6.8923e-10, 5.1661e-10, 7.0418e-11,  ..., 5.6973e-10, 9.2510e-11,
         2.3397e-10],
        [2.5842e-10, 1.4345e-10, 2.4913e-11,  ..., 1.9020e-10, 3.0078e-11,
         6.2975e-11]], device='cuda:0')
optimizer state dict: 92.0
lr: [1.5061252184179384e-05, 1.5061252184179384e-05]
scheduler_last_epoch: 92


Running epoch 0, step 736, batch 736
Sampled inputs[:2]: tensor([[    0, 47684,   292,  ...,   287, 49958, 22022],
        [    0,  3386, 43625,  ...,    19,  2125,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5255e-04,  3.7078e-04, -1.1669e-04,  ...,  9.2979e-05,
         -4.3159e-04, -3.1391e-04],
        [-1.6615e-06, -9.5367e-07,  7.6368e-07,  ..., -1.5721e-06,
         -7.8604e-07, -1.1772e-06],
        [-3.5018e-06, -2.0862e-06,  1.7509e-06,  ..., -3.0994e-06,
         -1.5274e-06, -2.0862e-06],
        [-4.1127e-06, -2.3246e-06,  2.0713e-06,  ..., -3.7998e-06,
         -1.8403e-06, -2.8312e-06],
        [-4.9174e-06, -2.9504e-06,  2.4587e-06,  ..., -4.2617e-06,
         -2.1756e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0398660898208618


Running epoch 0, step 737, batch 737
Sampled inputs[:2]: tensor([[    0,  1254,  1773,  ..., 19459,  2447,  2613],
        [    0,  2906, 46441,  ..., 39156,   287, 11452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9579e-04,  2.9181e-04,  9.4261e-05,  ...,  3.2329e-05,
         -1.6976e-04, -1.5908e-04],
        [-3.6284e-06, -2.2352e-06,  1.6429e-06,  ..., -3.1814e-06,
         -1.6503e-06, -2.3022e-06],
        [-7.7635e-06, -4.9174e-06,  3.7327e-06,  ..., -6.5267e-06,
         -3.3602e-06, -4.3511e-06],
        [-8.6129e-06, -5.2452e-06,  4.1574e-06,  ..., -7.4357e-06,
         -3.7402e-06, -5.3495e-06],
        [-1.0818e-05, -6.8843e-06,  5.2154e-06,  ..., -9.0301e-06,
         -4.7684e-06, -5.7369e-06]], device='cuda:0')
Loss: 1.0741257667541504


Running epoch 0, step 738, batch 738
Sampled inputs[:2]: tensor([[    0,  1099,   644,  ...,  5481,    14,  8782],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4424e-04,  6.0140e-04, -1.7073e-05,  ...,  1.7403e-04,
         -5.1185e-04, -4.5846e-04],
        [-5.4315e-06, -3.3900e-06,  2.4326e-06,  ..., -4.7535e-06,
         -2.4289e-06, -3.3975e-06],
        [-1.1697e-05, -7.4804e-06,  5.5656e-06,  ..., -9.8199e-06,
         -4.9695e-06, -6.5118e-06],
        [-1.2964e-05, -8.0317e-06,  6.1840e-06,  ..., -1.1161e-05,
         -5.5060e-06, -7.9572e-06],
        [-1.6272e-05, -1.0490e-05,  7.7784e-06,  ..., -1.3590e-05,
         -7.0781e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.0409409999847412


Running epoch 0, step 739, batch 739
Sampled inputs[:2]: tensor([[   0,  221,  709,  ..., 3365, 3504,  278],
        [   0, 1531,   14,  ..., 6169,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0600e-04,  7.5581e-04, -5.8252e-05,  ...,  4.2614e-04,
         -6.5654e-04, -4.3655e-04],
        [-7.2941e-06, -4.5672e-06,  3.2485e-06,  ..., -6.3404e-06,
         -3.1516e-06, -4.3735e-06],
        [-1.5542e-05, -9.9689e-06,  7.3463e-06,  ..., -1.3068e-05,
         -6.4522e-06, -8.3894e-06],
        [-1.7196e-05, -1.0714e-05,  8.1509e-06,  ..., -1.4782e-05,
         -7.1004e-06, -1.0177e-05],
        [-2.1935e-05, -1.4186e-05,  1.0401e-05,  ..., -1.8388e-05,
         -9.3430e-06, -1.1221e-05]], device='cuda:0')
Loss: 1.0392532348632812


Running epoch 0, step 740, batch 740
Sampled inputs[:2]: tensor([[    0,   300, 16683,  ...,  8709,    40,  9817],
        [    0,   825,  1243,  ...,    15,    22,    42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2679e-04,  8.4982e-04, -1.6238e-04,  ...,  4.8739e-04,
         -6.2640e-04, -3.1382e-04],
        [-9.2313e-06, -5.7891e-06,  4.1649e-06,  ..., -7.9051e-06,
         -3.7849e-06, -5.3719e-06],
        [-1.9595e-05, -1.2562e-05,  9.3132e-06,  ..., -1.6347e-05,
         -7.7710e-06, -1.0401e-05],
        [-2.1815e-05, -1.3605e-05,  1.0416e-05,  ..., -1.8507e-05,
         -8.5309e-06, -1.2577e-05],
        [-2.7537e-05, -1.7792e-05,  1.3098e-05,  ..., -2.2918e-05,
         -1.1280e-05, -1.3858e-05]], device='cuda:0')
Loss: 1.0306133031845093


Running epoch 0, step 741, batch 741
Sampled inputs[:2]: tensor([[    0,   266,  2109,  ...,  6730, 11558,   287],
        [    0,    14,  8383,  ...,   266,  1717,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7646e-04,  9.2107e-04, -3.2404e-04,  ...,  5.3975e-04,
         -9.0121e-04, -3.2533e-04],
        [-1.1072e-05, -6.9663e-06,  5.0180e-06,  ..., -9.4548e-06,
         -4.4629e-06, -6.2995e-06],
        [-2.3469e-05, -1.5095e-05,  1.1176e-05,  ..., -1.9580e-05,
         -9.1866e-06, -1.2256e-05],
        [-2.6256e-05, -1.6436e-05,  1.2577e-05,  ..., -2.2233e-05,
         -1.0103e-05, -1.4842e-05],
        [-3.2932e-05, -2.1324e-05,  1.5676e-05,  ..., -2.7418e-05,
         -1.3337e-05, -1.6317e-05]], device='cuda:0')
Loss: 1.0733267068862915


Running epoch 0, step 742, batch 742
Sampled inputs[:2]: tensor([[    0,   923,    13,  ...,   199,   677,  3826],
        [    0, 10205,   342,  ...,  6354, 12230,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4537e-04,  9.8487e-04, -3.0359e-04,  ...,  6.1312e-04,
         -1.0607e-03, -2.5939e-04],
        [-1.2994e-05, -8.2552e-06,  5.9046e-06,  ..., -1.1064e-05,
         -5.1707e-06, -7.3351e-06],
        [-2.7463e-05, -1.7837e-05,  1.3083e-05,  ..., -2.2933e-05,
         -1.0647e-05, -1.4313e-05],
        [-3.0786e-05, -1.9476e-05,  1.4767e-05,  ..., -2.6032e-05,
         -1.1697e-05, -1.7300e-05],
        [-3.8654e-05, -2.5257e-05,  1.8388e-05,  ..., -3.2216e-05,
         -1.5497e-05, -1.9118e-05]], device='cuda:0')
Loss: 1.0313429832458496


Running epoch 0, step 743, batch 743
Sampled inputs[:2]: tensor([[    0,  9010,    17,  ...,  3813,  1147,   199],
        [    0,   278, 30377,  ...,    13,    83,  2908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0876e-04,  1.1382e-03, -2.2491e-04,  ...,  5.6661e-04,
         -8.8269e-04, -1.8214e-04],
        [-1.5050e-05, -9.5218e-06,  6.7428e-06,  ..., -1.2726e-05,
         -6.0201e-06, -8.4229e-06],
        [-3.1695e-05, -2.0504e-05,  1.4916e-05,  ..., -2.6330e-05,
         -1.2361e-05, -1.6414e-05],
        [-3.5375e-05, -2.2307e-05,  1.6734e-05,  ..., -2.9743e-05,
         -1.3530e-05, -1.9714e-05],
        [-4.4733e-05, -2.9102e-05,  2.0996e-05,  ..., -3.7044e-05,
         -1.8016e-05, -2.1994e-05]], device='cuda:0')
Loss: 1.0890244245529175
Graident accumulation at epoch 0, step 743, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0058, -0.0144,  0.0025,  ..., -0.0023,  0.0234, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0152, -0.0279,  ...,  0.0289, -0.0146, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.2070e-05,  1.0102e-04, -1.4242e-04,  ...,  1.0530e-04,
          3.8291e-05,  1.6307e-04],
        [-1.5527e-05, -1.0981e-05,  7.1765e-06,  ..., -1.3366e-05,
         -7.1331e-06, -9.7139e-06],
        [ 2.5699e-05,  2.0711e-05, -1.1826e-05,  ...,  2.0865e-05,
          1.6257e-05,  9.7260e-06],
        [ 1.2227e-05,  1.2632e-05, -3.7084e-06,  ...,  1.9437e-05,
          8.2110e-06,  3.1147e-06],
        [-2.8490e-05, -2.0377e-05,  1.5505e-05,  ..., -2.3296e-05,
         -8.9003e-06, -1.5808e-05]], device='cuda:0')
optimizer state dict: tensor([[6.8666e-08, 3.6441e-08, 3.2044e-08,  ..., 2.9117e-08, 7.2729e-08,
         1.9678e-08],
        [5.2920e-11, 2.9766e-11, 3.9750e-12,  ..., 3.7375e-11, 3.3755e-12,
         1.1097e-11],
        [2.2792e-09, 1.0946e-09, 3.4663e-10,  ..., 1.6696e-09, 2.4225e-10,
         6.5842e-10],
        [6.8979e-10, 5.1660e-10, 7.0628e-11,  ..., 5.7004e-10, 9.2600e-11,
         2.3412e-10],
        [2.6017e-10, 1.4416e-10, 2.5329e-11,  ..., 1.9138e-10, 3.0373e-11,
         6.3396e-11]], device='cuda:0')
optimizer state dict: 93.0
lr: [1.4954253811094988e-05, 1.4954253811094988e-05]
scheduler_last_epoch: 93


Running epoch 0, step 744, batch 744
Sampled inputs[:2]: tensor([[   0, 3441,  796,  ..., 7561, 1711,  857],
        [   0,  494,  825,  ...,  897,  328,  275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6773e-05,  1.7447e-04, -1.3922e-04,  ...,  4.0616e-05,
         -1.2710e-04, -4.5782e-05],
        [-1.8179e-06, -1.1176e-06,  7.4878e-07,  ..., -1.5572e-06,
         -7.5623e-07, -9.8348e-07],
        [-3.9637e-06, -2.4736e-06,  1.7360e-06,  ..., -3.3230e-06,
         -1.6019e-06, -1.9819e-06],
        [-4.5896e-06, -2.8014e-06,  2.0117e-06,  ..., -3.9041e-06,
         -1.8254e-06, -2.4885e-06],
        [-5.4240e-06, -3.4124e-06,  2.3693e-06,  ..., -4.5300e-06,
         -2.2650e-06, -2.5779e-06]], device='cuda:0')
Loss: 1.085609793663025


Running epoch 0, step 745, batch 745
Sampled inputs[:2]: tensor([[   0,  320,  472,  ..., 1345,   14, 1869],
        [   0, 1250, 1797,  ...,  266, 1417,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6148e-05,  3.9850e-04, -1.2101e-04,  ...,  3.3883e-04,
         -4.8304e-04, -9.7287e-05],
        [-3.5465e-06, -2.1160e-06,  1.5274e-06,  ..., -3.0547e-06,
         -1.4715e-06, -2.0340e-06],
        [-7.8082e-06, -4.7535e-06,  3.5390e-06,  ..., -6.6012e-06,
         -3.1665e-06, -4.1425e-06],
        [-9.0003e-06, -5.3197e-06,  4.0978e-06,  ..., -7.7188e-06,
         -3.5763e-06, -5.1707e-06],
        [-1.0490e-05, -6.4522e-06,  4.7088e-06,  ..., -8.8215e-06,
         -4.3958e-06, -5.2750e-06]], device='cuda:0')
Loss: 1.0381078720092773


Running epoch 0, step 746, batch 746
Sampled inputs[:2]: tensor([[    0,   300, 12579,  ...,  1722,   369,  5049],
        [    0,  2793,   271,  ...,   374,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2198e-04,  5.2973e-04, -1.0003e-04,  ...,  3.2815e-04,
         -4.1381e-04, -7.0090e-05],
        [-5.1782e-06, -3.1739e-06,  2.2016e-06,  ..., -4.5374e-06,
         -2.2128e-06, -2.9951e-06],
        [-1.1519e-05, -7.1526e-06,  5.2154e-06,  ..., -9.8050e-06,
         -4.7535e-06, -6.0797e-06],
        [-1.3232e-05, -8.0168e-06,  6.0052e-06,  ..., -1.1474e-05,
         -5.3644e-06, -7.6145e-06],
        [-1.5587e-05, -9.7603e-06,  7.0333e-06,  ..., -1.3173e-05,
         -6.6161e-06, -7.7784e-06]], device='cuda:0')
Loss: 1.0537015199661255


Running epoch 0, step 747, batch 747
Sampled inputs[:2]: tensor([[   0, 9677,  609,  ...,  199, 1919,  298],
        [   0,  668, 2474,  ...,  668, 4599,  360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1320e-04,  5.1961e-04, -4.9361e-05,  ...,  3.4568e-04,
         -4.5609e-04, -1.1433e-04],
        [-6.9290e-06, -4.2766e-06,  3.0212e-06,  ..., -6.0201e-06,
         -2.8498e-06, -3.9488e-06],
        [-1.5423e-05, -9.6411e-06,  7.1079e-06,  ..., -1.3068e-05,
         -6.1616e-06, -8.0913e-06],
        [-1.7643e-05, -1.0774e-05,  8.1658e-06,  ..., -1.5199e-05,
         -6.9141e-06, -1.0043e-05],
        [-2.0772e-05, -1.3113e-05,  9.5367e-06,  ..., -1.7524e-05,
         -8.5831e-06, -1.0312e-05]], device='cuda:0')
Loss: 1.048471450805664


Running epoch 0, step 748, batch 748
Sampled inputs[:2]: tensor([[    0,  2823,   287,  ...,  3504,     9, 13910],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0763e-04,  8.3309e-04, -2.5126e-04,  ...,  6.3030e-04,
         -7.8927e-04, -4.4127e-04],
        [-8.5682e-06, -5.3123e-06,  3.7290e-06,  ..., -7.5027e-06,
         -3.6471e-06, -4.9993e-06],
        [-1.9133e-05, -1.2025e-05,  8.8289e-06,  ..., -1.6272e-05,
         -7.8678e-06, -1.0192e-05],
        [-2.1845e-05, -1.3396e-05,  1.0103e-05,  ..., -1.8924e-05,
         -8.8513e-06, -1.2696e-05],
        [-2.5749e-05, -1.6347e-05,  1.1846e-05,  ..., -2.1785e-05,
         -1.0908e-05, -1.2964e-05]], device='cuda:0')
Loss: 1.0537457466125488


Running epoch 0, step 749, batch 749
Sampled inputs[:2]: tensor([[    0,   266,  1658,  ...,   278,  1083,  5993],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8567e-04,  1.2287e-03, -4.1262e-04,  ...,  8.1045e-04,
         -1.1077e-03, -6.9551e-04],
        [-1.0282e-05, -6.3851e-06,  4.4517e-06,  ..., -9.0078e-06,
         -4.3809e-06, -6.0722e-06],
        [-2.2918e-05, -1.4454e-05,  1.0535e-05,  ..., -1.9491e-05,
         -9.4399e-06, -1.2323e-05],
        [-2.6047e-05, -1.6004e-05,  1.2003e-05,  ..., -2.2605e-05,
         -1.0572e-05, -1.5318e-05],
        [-3.0816e-05, -1.9625e-05,  1.4111e-05,  ..., -2.6047e-05,
         -1.3083e-05, -1.5646e-05]], device='cuda:0')
Loss: 1.0292779207229614


Running epoch 0, step 750, batch 750
Sampled inputs[:2]: tensor([[    0, 10676,   328,  ...,     9,   360,  2583],
        [    0,   965,   300,  ...,   546,   857,  4350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2559e-04,  1.3630e-03, -2.9290e-04,  ...,  8.9936e-04,
         -1.0793e-03, -6.0489e-04],
        [-1.2144e-05, -7.5996e-06,  5.2750e-06,  ..., -1.0543e-05,
         -5.0887e-06, -6.9886e-06],
        [-2.7239e-05, -1.7300e-05,  1.2487e-05,  ..., -2.3037e-05,
         -1.1079e-05, -1.4380e-05],
        [-3.0667e-05, -1.8999e-05,  1.4104e-05,  ..., -2.6405e-05,
         -1.2264e-05, -1.7628e-05],
        [-3.6508e-05, -2.3410e-05,  1.6674e-05,  ..., -3.0756e-05,
         -1.5318e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.0626332759857178


Running epoch 0, step 751, batch 751
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,  2720,    14,  ...,   300, 15867,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1179e-03,  1.7234e-03, -8.1492e-04,  ...,  1.2479e-03,
         -1.8717e-03, -1.2641e-03],
        [-1.3873e-05, -8.5980e-06,  5.9940e-06,  ..., -1.2025e-05,
         -5.7667e-06, -8.0019e-06],
        [-3.1024e-05, -1.9550e-05,  1.4178e-05,  ..., -2.6211e-05,
         -1.2532e-05, -1.6406e-05],
        [-3.5048e-05, -2.1502e-05,  1.6071e-05,  ..., -3.0160e-05,
         -1.3910e-05, -2.0206e-05],
        [-4.1544e-05, -2.6450e-05,  1.8910e-05,  ..., -3.4958e-05,
         -1.7330e-05, -2.0802e-05]], device='cuda:0')
Loss: 1.0449410676956177
Graident accumulation at epoch 0, step 751, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0025,  ..., -0.0023,  0.0234, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0279,  ...,  0.0289, -0.0146, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8932e-05,  2.6326e-04, -2.0967e-04,  ...,  2.1956e-04,
         -1.5271e-04,  2.0358e-05],
        [-1.5361e-05, -1.0742e-05,  7.0583e-06,  ..., -1.3232e-05,
         -6.9965e-06, -9.5427e-06],
        [ 2.0027e-05,  1.6685e-05, -9.2255e-06,  ...,  1.6157e-05,
          1.3378e-05,  7.1128e-06],
        [ 7.4995e-06,  9.2185e-06, -1.7305e-06,  ...,  1.4478e-05,
          5.9989e-06,  7.8266e-07],
        [-2.9796e-05, -2.0984e-05,  1.5846e-05,  ..., -2.4462e-05,
         -9.7433e-06, -1.6307e-05]], device='cuda:0')
optimizer state dict: tensor([[6.9847e-08, 3.9375e-08, 3.2676e-08,  ..., 3.0645e-08, 7.6159e-08,
         2.1256e-08],
        [5.3060e-11, 2.9811e-11, 4.0070e-12,  ..., 3.7482e-11, 3.4054e-12,
         1.1150e-11],
        [2.2779e-09, 1.0938e-09, 3.4648e-10,  ..., 1.6686e-09, 2.4216e-10,
         6.5803e-10],
        [6.9033e-10, 5.1654e-10, 7.0815e-11,  ..., 5.7038e-10, 9.2701e-11,
         2.3430e-10],
        [2.6163e-10, 1.4471e-10, 2.5661e-11,  ..., 1.9241e-10, 3.0642e-11,
         6.3765e-11]], device='cuda:0')
optimizer state dict: 94.0
lr: [1.4846498384781962e-05, 1.4846498384781962e-05]
scheduler_last_epoch: 94


Running epoch 0, step 752, batch 752
Sampled inputs[:2]: tensor([[   0,  259, 6887,  ..., 1400,  292,  474],
        [   0,  272,  352,  ...,  590, 4361,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5561e-05,  8.5827e-05, -9.3831e-06,  ...,  5.4211e-05,
         -9.4820e-05, -4.3608e-05],
        [-1.6019e-06, -9.6112e-07,  7.1153e-07,  ..., -1.4380e-06,
         -7.9721e-07, -9.9093e-07],
        [-3.5316e-06, -2.1756e-06,  1.6317e-06,  ..., -3.1292e-06,
         -1.7285e-06, -2.0266e-06],
        [-4.2915e-06, -2.5481e-06,  1.9968e-06,  ..., -3.8445e-06,
         -2.0713e-06, -2.6822e-06],
        [-4.5896e-06, -2.8908e-06,  2.1011e-06,  ..., -4.0531e-06,
         -2.3246e-06, -2.4587e-06]], device='cuda:0')
Loss: 1.0512888431549072


Running epoch 0, step 753, batch 753
Sampled inputs[:2]: tensor([[    0,   892,   271,  ...,   278,   266, 10237],
        [    0,   365,  2714,  ...,   298,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4163e-05,  1.9938e-04,  1.7876e-05,  ...,  2.1051e-04,
         -2.4432e-04, -2.9440e-04],
        [-3.2857e-06, -1.9670e-06,  1.4678e-06,  ..., -2.8610e-06,
         -1.5870e-06, -2.0564e-06],
        [-7.2718e-06, -4.4852e-06,  3.4049e-06,  ..., -6.2287e-06,
         -3.4496e-06, -4.2170e-06],
        [-8.7619e-06, -5.2005e-06,  4.1425e-06,  ..., -7.6145e-06,
         -4.0829e-06, -5.5581e-06],
        [-9.4473e-06, -5.9158e-06,  4.3809e-06,  ..., -8.0764e-06,
         -4.6343e-06, -5.1260e-06]], device='cuda:0')
Loss: 1.0159765481948853


Running epoch 0, step 754, batch 754
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  266,  818,  278],
        [   0, 1184, 1451,  ...,  934,  352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9670e-05,  6.2796e-04,  3.7657e-05,  ...,  2.6493e-04,
         -2.4220e-04, -6.6100e-04],
        [-5.1335e-06, -2.9877e-06,  2.1905e-06,  ..., -4.3139e-06,
         -2.4177e-06, -3.2187e-06],
        [-1.1593e-05, -6.9141e-06,  5.1633e-06,  ..., -9.6112e-06,
         -5.3868e-06, -6.8396e-06],
        [-1.3471e-05, -7.7933e-06,  6.0797e-06,  ..., -1.1325e-05,
         -6.1393e-06, -8.5384e-06],
        [-1.4871e-05, -9.0003e-06,  6.5863e-06,  ..., -1.2308e-05,
         -7.1228e-06, -8.2552e-06]], device='cuda:0')
Loss: 1.0419217348098755


Running epoch 0, step 755, batch 755
Sampled inputs[:2]: tensor([[   0,  257,  298,  ..., 1878,  328,  259],
        [   0, 1295,  508,  ...,  829,  772,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9505e-04,  9.1549e-04,  1.0162e-04,  ...,  4.1837e-04,
         -6.5303e-04, -7.6415e-04],
        [-6.9588e-06, -3.9935e-06,  3.0026e-06,  ..., -5.7295e-06,
         -3.2447e-06, -4.3586e-06],
        [-1.5825e-05, -9.2983e-06,  7.1004e-06,  ..., -1.2845e-05,
         -7.2867e-06, -9.3430e-06],
        [-1.8239e-05, -1.0416e-05,  8.2850e-06,  ..., -1.4991e-05,
         -8.2254e-06, -1.1504e-05],
        [-2.0117e-05, -1.2010e-05,  8.9854e-06,  ..., -1.6332e-05,
         -9.5516e-06, -1.1221e-05]], device='cuda:0')
Loss: 1.0115493535995483


Running epoch 0, step 756, batch 756
Sampled inputs[:2]: tensor([[    0,    89,  2023,  ...,  3230,   328,   790],
        [    0,    12, 12774,  ...,  1231,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5260e-04,  1.1089e-03,  9.2384e-05,  ...,  4.7841e-04,
         -8.1855e-04, -8.4459e-04],
        [-8.6650e-06, -4.9770e-06,  3.6880e-06,  ..., -7.1526e-06,
         -4.0606e-06, -5.4091e-06],
        [-1.9595e-05, -1.1533e-05,  8.7097e-06,  ..., -1.5900e-05,
         -9.0227e-06, -1.1474e-05],
        [-2.2769e-05, -1.3009e-05,  1.0222e-05,  ..., -1.8731e-05,
         -1.0282e-05, -1.4305e-05],
        [-2.5153e-05, -1.5035e-05,  1.1116e-05,  ..., -2.0385e-05,
         -1.1951e-05, -1.3858e-05]], device='cuda:0')
Loss: 1.0406159162521362


Running epoch 0, step 757, batch 757
Sampled inputs[:2]: tensor([[    0, 28684,   472,  ...,   317,     9,  1926],
        [    0, 33315,   266,  ...,    12,  1126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7553e-04,  1.2439e-03, -2.8313e-04,  ...,  5.1922e-04,
         -1.2552e-03, -1.2790e-03],
        [-1.0267e-05, -5.8897e-06,  4.4219e-06,  ..., -8.5682e-06,
         -4.8801e-06, -6.5342e-06],
        [-2.3112e-05, -1.3635e-05,  1.0423e-05,  ..., -1.8924e-05,
         -1.0774e-05, -1.3664e-05],
        [-2.7001e-05, -1.5423e-05,  1.2323e-05,  ..., -2.2501e-05,
         -1.2413e-05, -1.7300e-05],
        [-2.9683e-05, -1.7807e-05,  1.3292e-05,  ..., -2.4259e-05,
         -1.4275e-05, -1.6481e-05]], device='cuda:0')
Loss: 1.0380100011825562


Running epoch 0, step 758, batch 758
Sampled inputs[:2]: tensor([[    0,   445,   749,  ...,   850,  1028,    13],
        [    0,  6022,   644,  ..., 14834,  3554,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8713e-04,  1.5055e-03, -2.6033e-04,  ...,  6.2093e-04,
         -1.3374e-03, -1.3516e-03],
        [-1.1951e-05, -6.9253e-06,  5.1260e-06,  ..., -9.9540e-06,
         -5.5432e-06, -7.5772e-06],
        [-2.6807e-05, -1.5959e-05,  1.2040e-05,  ..., -2.1935e-05,
         -1.2234e-05, -1.5810e-05],
        [-3.1352e-05, -1.8090e-05,  1.4260e-05,  ..., -2.6077e-05,
         -1.4067e-05, -2.0012e-05],
        [-3.4422e-05, -2.0832e-05,  1.5348e-05,  ..., -2.8133e-05,
         -1.6242e-05, -1.9044e-05]], device='cuda:0')
Loss: 1.0172123908996582


Running epoch 0, step 759, batch 759
Sampled inputs[:2]: tensor([[    0,   292, 49760,  ...,   275,  4474,    13],
        [    0,  4371,  4806,  ...,   685,   461,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3954e-04,  1.4373e-03, -3.2355e-04,  ...,  6.2218e-04,
         -1.2654e-03, -1.3302e-03],
        [-1.3657e-05, -7.9535e-06,  5.9418e-06,  ..., -1.1377e-05,
         -6.2361e-06, -8.6129e-06],
        [-3.0473e-05, -1.8224e-05,  1.3843e-05,  ..., -2.4989e-05,
         -1.3731e-05, -1.7941e-05],
        [-3.5852e-05, -2.0787e-05,  1.6510e-05,  ..., -2.9832e-05,
         -1.5825e-05, -2.2799e-05],
        [-3.9130e-05, -2.3767e-05,  1.7643e-05,  ..., -3.2067e-05,
         -1.8269e-05, -2.1622e-05]], device='cuda:0')
Loss: 1.0324921607971191
Graident accumulation at epoch 0, step 759, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0025,  ..., -0.0023,  0.0234, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0279,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9993e-05,  3.8066e-04, -2.2106e-04,  ...,  2.5983e-04,
         -2.6398e-04, -1.1469e-04],
        [-1.5191e-05, -1.0464e-05,  6.9466e-06,  ..., -1.3047e-05,
         -6.9204e-06, -9.4497e-06],
        [ 1.4977e-05,  1.3194e-05, -6.9186e-06,  ...,  1.2043e-05,
          1.0667e-05,  4.6074e-06],
        [ 3.1643e-06,  6.2179e-06,  9.3609e-08,  ...,  1.0047e-05,
          3.8165e-06, -1.5755e-06],
        [-3.0729e-05, -2.1263e-05,  1.6025e-05,  ..., -2.5223e-05,
         -1.0596e-05, -1.6839e-05]], device='cuda:0')
optimizer state dict: tensor([[6.9971e-08, 4.1401e-08, 3.2748e-08,  ..., 3.1001e-08, 7.7684e-08,
         2.3004e-08],
        [5.3193e-11, 2.9844e-11, 4.0383e-12,  ..., 3.7574e-11, 3.4409e-12,
         1.1213e-11],
        [2.2765e-09, 1.0931e-09, 3.4633e-10,  ..., 1.6676e-09, 2.4211e-10,
         6.5769e-10],
        [6.9092e-10, 5.1646e-10, 7.1017e-11,  ..., 5.7070e-10, 9.2859e-11,
         2.3458e-10],
        [2.6290e-10, 1.4513e-10, 2.5947e-11,  ..., 1.9325e-10, 3.0946e-11,
         6.4169e-11]], device='cuda:0')
optimizer state dict: 95.0
lr: [1.4738002371210062e-05, 1.4738002371210062e-05]
scheduler_last_epoch: 95


Running epoch 0, step 760, batch 760
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0, 49018,   292,  ...,  8774,   642,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2424e-04,  1.7153e-05,  1.2318e-04,  ..., -1.7899e-04,
          3.8133e-04,  2.2651e-04],
        [-1.6838e-06, -1.0431e-06,  7.7859e-07,  ..., -1.3486e-06,
         -8.5682e-07, -1.1101e-06],
        [-3.8147e-06, -2.4587e-06,  1.8477e-06,  ..., -3.0696e-06,
         -1.9670e-06, -2.3991e-06],
        [-4.5002e-06, -2.7865e-06,  2.2054e-06,  ..., -3.6210e-06,
         -2.2352e-06, -3.0249e-06],
        [-4.8876e-06, -3.1739e-06,  2.3246e-06,  ..., -3.9339e-06,
         -2.6226e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.049219012260437


Running epoch 0, step 761, batch 761
Sampled inputs[:2]: tensor([[   0,  756,  401,  ...,  271, 7272, 1663],
        [   0, 5136,  446,  ..., 1173,  300,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3478e-04, -6.2223e-04,  1.9150e-04,  ..., -4.9345e-04,
          1.1569e-03,  7.6622e-04],
        [-3.1516e-06, -1.7397e-06,  1.4454e-06,  ..., -2.6226e-06,
         -1.7583e-06, -2.4885e-06],
        [-7.1228e-06, -4.1872e-06,  3.4496e-06,  ..., -5.8264e-06,
         -3.9190e-06, -5.1260e-06],
        [-8.6427e-06, -4.7237e-06,  4.2915e-06,  ..., -7.1824e-06,
         -4.6939e-06, -6.9290e-06],
        [-9.1195e-06, -5.4836e-06,  4.3213e-06,  ..., -7.4357e-06,
         -5.1856e-06, -6.0350e-06]], device='cuda:0')
Loss: 1.0134437084197998


Running epoch 0, step 762, batch 762
Sampled inputs[:2]: tensor([[   0,   12, 5820,  ...,  221,  380,  560],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9299e-04, -7.0497e-04,  4.0517e-04,  ..., -6.5123e-04,
          1.8073e-03,  1.1599e-03],
        [-4.8280e-06, -2.6301e-06,  2.1644e-06,  ..., -4.0606e-06,
         -2.6636e-06, -3.7551e-06],
        [-1.0863e-05, -6.2883e-06,  5.1782e-06,  ..., -8.9854e-06,
         -5.9009e-06, -7.7337e-06],
        [-1.3143e-05, -7.1228e-06,  6.3777e-06,  ..., -1.1057e-05,
         -7.0632e-06, -1.0356e-05],
        [-1.3798e-05, -8.1807e-06,  6.4671e-06,  ..., -1.1370e-05,
         -7.7486e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.059035062789917


Running epoch 0, step 763, batch 763
Sampled inputs[:2]: tensor([[   0, 1712,   12,  ..., 1255, 1688,  266],
        [   0, 3767, 2337,  ...,  950,  847,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7894e-04, -1.0058e-03,  6.8559e-04,  ..., -8.4488e-04,
          1.9201e-03,  1.8123e-03],
        [-6.4597e-06, -3.5577e-06,  2.8685e-06,  ..., -5.3942e-06,
         -3.5726e-06, -4.9993e-06],
        [-1.4603e-05, -8.4937e-06,  6.8545e-06,  ..., -1.1951e-05,
         -7.9274e-06, -1.0356e-05],
        [-1.7554e-05, -9.6112e-06,  8.4341e-06,  ..., -1.4633e-05,
         -9.4473e-06, -1.3739e-05],
        [-1.8477e-05, -1.1012e-05,  8.5533e-06,  ..., -1.5080e-05,
         -1.0371e-05, -1.2174e-05]], device='cuda:0')
Loss: 1.0329439640045166


Running epoch 0, step 764, batch 764
Sampled inputs[:2]: tensor([[    0,    12,  3570,  ...,   273,   298,   894],
        [    0,  8450,   292,  ...,   352,   722, 37719]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6821e-04, -1.0282e-03,  7.5116e-04,  ..., -8.0335e-04,
          1.8549e-03,  1.9136e-03],
        [-8.1807e-06, -4.4107e-06,  3.5465e-06,  ..., -6.7800e-06,
         -4.4778e-06, -6.3553e-06],
        [-1.8448e-05, -1.0505e-05,  8.5086e-06,  ..., -1.5005e-05,
         -9.9391e-06, -1.3173e-05],
        [-2.2084e-05, -1.1846e-05,  1.0386e-05,  ..., -1.8269e-05,
         -1.1787e-05, -1.7300e-05],
        [-2.3365e-05, -1.3635e-05,  1.0639e-05,  ..., -1.8924e-05,
         -1.2979e-05, -1.5527e-05]], device='cuda:0')
Loss: 1.027127742767334


Running epoch 0, step 765, batch 765
Sampled inputs[:2]: tensor([[    0,  5332,   266,  ...,   300,   259, 15369],
        [    0,   381,  1659,  ...,  1403,   271,  6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2452e-04, -1.2079e-03,  8.1103e-04,  ..., -1.1248e-03,
          2.2741e-03,  2.3013e-03],
        [-9.8497e-06, -5.3644e-06,  4.3251e-06,  ..., -8.1211e-06,
         -5.3048e-06, -7.4804e-06],
        [-2.2024e-05, -1.2636e-05,  1.0245e-05,  ..., -1.7881e-05,
         -1.1720e-05, -1.5438e-05],
        [-2.6613e-05, -1.4424e-05,  1.2621e-05,  ..., -2.1905e-05,
         -1.3962e-05, -2.0429e-05],
        [-2.7955e-05, -1.6436e-05,  1.2845e-05,  ..., -2.2635e-05,
         -1.5378e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.048974871635437


Running epoch 0, step 766, batch 766
Sampled inputs[:2]: tensor([[    0, 15165,   287,  ..., 15049,   278,   266],
        [    0,  3406,   300,  ...,  1726,  3521,  4481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2896e-03, -1.8918e-03,  1.1077e-03,  ..., -1.6467e-03,
          3.4949e-03,  3.0131e-03],
        [-1.1176e-05, -6.0871e-06,  5.0366e-06,  ..., -9.4324e-06,
         -6.1281e-06, -8.7172e-06],
        [-2.4855e-05, -1.4313e-05,  1.1869e-05,  ..., -2.0564e-05,
         -1.3411e-05, -1.7747e-05],
        [-3.0354e-05, -1.6436e-05,  1.4827e-05,  ..., -2.5570e-05,
         -1.6198e-05, -2.3916e-05],
        [-3.1695e-05, -1.8746e-05,  1.4946e-05,  ..., -2.6107e-05,
         -1.7703e-05, -2.0996e-05]], device='cuda:0')
Loss: 1.0269230604171753


Running epoch 0, step 767, batch 767
Sampled inputs[:2]: tensor([[    0,     9,  9925,  ...,   527, 23286,  6062],
        [    0,  2088,  5370,  ...,  1110,  3380,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5215e-03, -2.2054e-03,  1.2911e-03,  ..., -1.8771e-03,
          3.9003e-03,  3.2874e-03],
        [-1.2800e-05, -7.0706e-06,  5.7854e-06,  ..., -1.0826e-05,
         -7.0594e-06, -9.8795e-06],
        [-2.8417e-05, -1.6518e-05,  1.3590e-05,  ..., -2.3589e-05,
         -1.5408e-05, -2.0161e-05],
        [-3.4705e-05, -1.9059e-05,  1.6972e-05,  ..., -2.9311e-05,
         -1.8626e-05, -2.7105e-05],
        [-3.6225e-05, -2.1607e-05,  1.7107e-05,  ..., -2.9951e-05,
         -2.0340e-05, -2.3887e-05]], device='cuda:0')
Loss: 1.0487875938415527
Graident accumulation at epoch 0, step 767, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0025,  ..., -0.0022,  0.0235, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0279,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.9156e-05,  1.2206e-04, -6.9844e-05,  ...,  4.6128e-05,
          1.5246e-04,  2.2551e-04],
        [-1.4952e-05, -1.0124e-05,  6.8305e-06,  ..., -1.2825e-05,
         -6.9343e-06, -9.4927e-06],
        [ 1.0638e-05,  1.0223e-05, -4.8677e-06,  ...,  8.4795e-06,
          8.0594e-06,  2.1305e-06],
        [-6.2262e-07,  3.6902e-06,  1.7815e-06,  ...,  6.1109e-06,
          1.5722e-06, -4.1285e-06],
        [-3.1279e-05, -2.1297e-05,  1.6133e-05,  ..., -2.5696e-05,
         -1.1570e-05, -1.7544e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2216e-08, 4.6224e-08, 3.4382e-08,  ..., 3.4494e-08, 9.2819e-08,
         3.3788e-08],
        [5.3304e-11, 2.9864e-11, 4.0677e-12,  ..., 3.7654e-11, 3.4873e-12,
         1.1300e-11],
        [2.2751e-09, 1.0923e-09, 3.4617e-10,  ..., 1.6664e-09, 2.4210e-10,
         6.5744e-10],
        [6.9144e-10, 5.1630e-10, 7.1234e-11,  ..., 5.7099e-10, 9.3113e-11,
         2.3508e-10],
        [2.6395e-10, 1.4545e-10, 2.6214e-11,  ..., 1.9395e-10, 3.1328e-11,
         6.4675e-11]], device='cuda:0')
optimizer state dict: 96.0
lr: [1.4628782349517233e-05, 1.4628782349517233e-05]
scheduler_last_epoch: 96


Running epoch 0, step 768, batch 768
Sampled inputs[:2]: tensor([[    0,    12,  1197,  ...,   352,  2513,   266],
        [    0, 18717,  2837,  ...,    48,    18,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1336e-05, -1.3728e-05, -6.7514e-05,  ...,  1.1550e-06,
         -9.3336e-05,  1.2824e-05],
        [-1.8403e-06, -1.1250e-06,  8.2701e-07,  ..., -1.5274e-06,
         -1.0356e-06, -1.1995e-06],
        [-4.0531e-06, -2.5481e-06,  1.8999e-06,  ..., -3.3230e-06,
         -2.2650e-06, -2.4736e-06],
        [-4.7386e-06, -2.8610e-06,  2.2650e-06,  ..., -3.9637e-06,
         -2.6226e-06, -3.1739e-06],
        [-5.0664e-06, -3.2336e-06,  2.3693e-06,  ..., -4.1425e-06,
         -2.8908e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.060367465019226


Running epoch 0, step 769, batch 769
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,  5775,    12,  ...,    12,  1034,  9257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9057e-05,  3.4266e-05, -2.6980e-04,  ...,  1.1550e-06,
         -5.5841e-04, -1.2892e-04],
        [-3.5986e-06, -2.2203e-06,  1.6689e-06,  ..., -2.9430e-06,
         -1.9744e-06, -2.3320e-06],
        [ 1.0335e-04,  6.2634e-05, -2.4695e-05,  ...,  6.5037e-05,
          5.4111e-05,  3.6947e-05],
        [-9.4175e-06, -5.7518e-06,  4.6641e-06,  ..., -7.7486e-06,
         -5.0515e-06, -6.3032e-06],
        [-1.0043e-05, -6.4969e-06,  4.8280e-06,  ..., -8.1360e-06,
         -5.6326e-06, -5.8115e-06]], device='cuda:0')
Loss: 1.035075068473816


Running epoch 0, step 770, batch 770
Sampled inputs[:2]: tensor([[    0,  1336,   287,  ..., 15920,    12,   287],
        [    0,   591,  2036,  ...,   266,  1027,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4516e-04, -1.3047e-04, -3.6288e-04,  ..., -1.4723e-05,
         -6.7036e-04, -8.2772e-05],
        [-5.4538e-06, -3.3826e-06,  2.5593e-06,  ..., -4.4182e-06,
         -2.9281e-06, -3.3453e-06],
        [ 9.9263e-05,  6.0042e-05, -2.2698e-05,  ...,  6.1788e-05,
          5.2010e-05,  3.4787e-05],
        [-1.4275e-05, -8.7619e-06,  7.0781e-06,  ..., -1.1623e-05,
         -7.4804e-06, -9.0450e-06],
        [-1.5289e-05, -9.8795e-06,  7.3612e-06,  ..., -1.2338e-05,
         -8.4341e-06, -8.4639e-06]], device='cuda:0')
Loss: 1.0390011072158813


Running epoch 0, step 771, batch 771
Sampled inputs[:2]: tensor([[    0,    55,  2258,  ..., 32764,    75,   338],
        [    0,   266,  1211,  ...,  1336,   694,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7976e-04, -3.1831e-04, -4.4396e-04,  ..., -1.3051e-04,
         -6.4953e-04,  8.4523e-05],
        [-7.2494e-06, -4.3809e-06,  3.4124e-06,  ..., -5.8413e-06,
         -3.8967e-06, -4.5970e-06],
        [ 9.5270e-05,  5.7732e-05, -2.0732e-05,  ...,  5.8689e-05,
          4.9879e-05,  3.2194e-05],
        [-1.8984e-05, -1.1355e-05,  9.4622e-06,  ..., -1.5348e-05,
         -9.9838e-06, -1.2413e-05],
        [-2.0295e-05, -1.2860e-05,  9.8199e-06,  ..., -1.6242e-05,
         -1.1206e-05, -1.1548e-05]], device='cuda:0')
Loss: 1.0470014810562134


Running epoch 0, step 772, batch 772
Sampled inputs[:2]: tensor([[    0, 31571,    13,  ...,   367,  2177,   271],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4733e-04, -3.1831e-04, -5.4564e-04,  ..., -1.1990e-04,
         -9.0835e-04, -1.4484e-04],
        [-9.0972e-06, -5.4911e-06,  4.2543e-06,  ..., -7.3090e-06,
         -4.8727e-06, -5.7444e-06],
        [ 9.1127e-05,  5.5199e-05, -1.8765e-05,  ...,  5.5425e-05,
          4.7718e-05,  2.9750e-05],
        [-2.3872e-05, -1.4216e-05,  1.1787e-05,  ..., -1.9222e-05,
         -1.2472e-05, -1.5512e-05],
        [-2.5570e-05, -1.6138e-05,  1.2308e-05,  ..., -2.0385e-05,
         -1.4037e-05, -1.4499e-05]], device='cuda:0')
Loss: 1.0378590822219849


Running epoch 0, step 773, batch 773
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,   508,   586,  ...,  6157,  3146,  7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8059e-04, -4.6554e-04, -5.9622e-04,  ..., -1.7205e-04,
         -8.4170e-04, -9.5018e-05],
        [-1.0937e-05, -6.6310e-06,  5.0813e-06,  ..., -8.7917e-06,
         -5.8934e-06, -6.8247e-06],
        [ 8.7044e-05,  5.2591e-05, -1.6872e-05,  ...,  5.2147e-05,
          4.5468e-05,  2.7455e-05],
        [-2.8640e-05, -1.7166e-05,  1.4007e-05,  ..., -2.3097e-05,
         -1.5080e-05, -1.8403e-05],
        [-3.0816e-05, -1.9521e-05,  1.4707e-05,  ..., -2.4587e-05,
         -1.6987e-05, -1.7315e-05]], device='cuda:0')
Loss: 1.0462470054626465


Running epoch 0, step 774, batch 774
Sampled inputs[:2]: tensor([[   0,  278, 7914,  ..., 1194,  300, 4419],
        [   0, 3630, 2199,  ..., 4157,   27, 4765]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1089e-04, -4.5439e-04, -5.9905e-04,  ..., -4.5966e-05,
         -9.2495e-04, -8.7688e-06],
        [-1.2748e-05, -7.7561e-06,  5.9120e-06,  ..., -1.0304e-05,
         -6.8471e-06, -7.8976e-06],
        [ 8.3110e-05,  5.0088e-05, -1.4995e-05,  ...,  4.8899e-05,
          4.3427e-05,  2.5220e-05],
        [-3.3349e-05, -2.0087e-05,  1.6287e-05,  ..., -2.7031e-05,
         -1.7479e-05, -2.1294e-05],
        [-3.6031e-05, -2.2873e-05,  1.7181e-05,  ..., -2.8878e-05,
         -1.9789e-05, -2.0117e-05]], device='cuda:0')
Loss: 1.0439845323562622


Running epoch 0, step 775, batch 775
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  266, 1040, 1679],
        [   0,  328, 2097,  ...,  365, 1941,  607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5327e-04, -4.4068e-04, -6.4227e-04,  ..., -2.2194e-05,
         -1.1110e-03, -1.0587e-04],
        [-1.4618e-05, -8.8736e-06,  6.7651e-06,  ..., -1.1802e-05,
         -7.8455e-06, -9.0450e-06],
        [ 7.8968e-05,  4.7540e-05, -1.3057e-05,  ...,  4.5591e-05,
          4.1221e-05,  2.2791e-05],
        [-3.8236e-05, -2.2992e-05,  1.8612e-05,  ..., -3.0965e-05,
         -2.0012e-05, -2.4378e-05],
        [-4.1455e-05, -2.6256e-05,  1.9699e-05,  ..., -3.3230e-05,
         -2.2754e-05, -2.3171e-05]], device='cuda:0')
Loss: 1.051686406135559
Graident accumulation at epoch 0, step 775, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0025,  ..., -0.0022,  0.0235, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0279,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3557e-04,  6.5784e-05, -1.2709e-04,  ...,  3.9296e-05,
          2.6110e-05,  1.9238e-04],
        [-1.4918e-05, -9.9992e-06,  6.8240e-06,  ..., -1.2722e-05,
         -7.0254e-06, -9.4479e-06],
        [ 1.7471e-05,  1.3954e-05, -5.6867e-06,  ...,  1.2191e-05,
          1.1376e-05,  4.1966e-06],
        [-4.3840e-06,  1.0220e-06,  3.4645e-06,  ...,  2.4033e-06,
         -5.8626e-07, -6.1534e-06],
        [-3.2296e-05, -2.1793e-05,  1.6490e-05,  ..., -2.6449e-05,
         -1.2689e-05, -1.8106e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2449e-08, 4.6372e-08, 3.4761e-08,  ..., 3.4460e-08, 9.3961e-08,
         3.3765e-08],
        [5.3464e-11, 2.9913e-11, 4.1094e-12,  ..., 3.7755e-11, 3.5453e-12,
         1.1370e-11],
        [2.2790e-09, 1.0934e-09, 3.4599e-10,  ..., 1.6669e-09, 2.4356e-10,
         6.5730e-10],
        [6.9221e-10, 5.1632e-10, 7.1509e-11,  ..., 5.7138e-10, 9.3421e-11,
         2.3544e-10],
        [2.6541e-10, 1.4600e-10, 2.6576e-11,  ..., 1.9486e-10, 3.1815e-11,
         6.5147e-11]], device='cuda:0')
optimizer state dict: 97.0
lr: [1.4518855009476186e-05, 1.4518855009476186e-05]
scheduler_last_epoch: 97


Running epoch 0, step 776, batch 776
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  631, 3370,  940],
        [   0, 2732,  413,  ...,  287,  266, 3668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5892e-05,  1.6877e-04, -6.0140e-05,  ...,  1.1715e-04,
         -2.7022e-04, -1.5765e-04],
        [-1.9222e-06, -1.1548e-06,  8.8662e-07,  ..., -1.5274e-06,
         -9.8348e-07, -1.1474e-06],
        [-4.2021e-06, -2.5928e-06,  2.0266e-06,  ..., -3.2783e-06,
         -2.1309e-06, -2.3544e-06],
        [-4.8876e-06, -2.8759e-06,  2.3693e-06,  ..., -3.8445e-06,
         -2.4289e-06, -2.9951e-06],
        [-5.4538e-06, -3.4124e-06,  2.6077e-06,  ..., -4.2319e-06,
         -2.8312e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.0075573921203613


Running epoch 0, step 777, batch 777
Sampled inputs[:2]: tensor([[   0,   19,    9,  ..., 4971,  367, 1675],
        [   0, 3804,  300,  ..., 5062, 9848, 3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8761e-05,  1.1597e-04, -2.5228e-05,  ...,  1.0832e-04,
         -2.3968e-04, -9.2227e-05],
        [-3.9488e-06, -2.4065e-06,  1.7919e-06,  ..., -3.0920e-06,
         -2.0042e-06, -2.2128e-06],
        [-8.5831e-06, -5.3644e-06,  4.0382e-06,  ..., -6.6608e-06,
         -4.3213e-06, -4.5747e-06],
        [-1.0014e-05, -6.0052e-06,  4.7088e-06,  ..., -7.8082e-06,
         -4.9323e-06, -5.7817e-06],
        [-1.1027e-05, -6.9886e-06,  5.1409e-06,  ..., -8.5235e-06,
         -5.6922e-06, -5.5581e-06]], device='cuda:0')
Loss: 1.042068362236023


Running epoch 0, step 778, batch 778
Sampled inputs[:2]: tensor([[    0,  1858,   499,  ...,    14,  1032,    14],
        [    0, 41638,  4573,  ...,   259,   790,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5157e-06,  2.4944e-04, -2.9707e-05,  ...,  1.0995e-04,
         -4.4778e-04, -1.4780e-04],
        [-5.9009e-06, -3.6210e-06,  2.6114e-06,  ..., -4.6045e-06,
         -3.0249e-06, -3.2559e-06],
        [-1.3024e-05, -8.1658e-06,  6.0201e-06,  ..., -1.0043e-05,
         -6.6012e-06, -6.8396e-06],
        [-1.4871e-05, -9.0152e-06,  6.8545e-06,  ..., -1.1548e-05,
         -7.4059e-06, -8.4192e-06],
        [-1.6749e-05, -1.0625e-05,  7.7039e-06,  ..., -1.2875e-05,
         -8.6725e-06, -8.3297e-06]], device='cuda:0')
Loss: 1.0267497301101685


Running epoch 0, step 779, batch 779
Sampled inputs[:2]: tensor([[    0,  1172,   365,  ...,  1119, 15573,  3701],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3072e-05,  2.2181e-04, -1.2904e-04,  ...,  1.5808e-04,
         -4.7332e-04, -2.0724e-04],
        [-7.8082e-06, -4.8503e-06,  3.4757e-06,  ..., -6.1616e-06,
         -4.0680e-06, -4.2468e-06],
        [-1.7107e-05, -1.0848e-05,  7.9423e-06,  ..., -1.3337e-05,
         -8.7917e-06, -8.8513e-06],
        [-1.9640e-05, -1.2040e-05,  9.1046e-06,  ..., -1.5453e-05,
         -9.9391e-06, -1.0997e-05],
        [-2.2024e-05, -1.4126e-05,  1.0177e-05,  ..., -1.7107e-05,
         -1.1563e-05, -1.0788e-05]], device='cuda:0')
Loss: 1.060987949371338


Running epoch 0, step 780, batch 780
Sampled inputs[:2]: tensor([[    0,  4258,   717,  ...,    34,   609,  1169],
        [    0,   313,    66,  ...,   894,  2973, 25074]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9720e-05,  2.7387e-04, -1.6261e-04,  ...,  1.4679e-04,
         -4.6404e-04, -3.0484e-04],
        [-9.7454e-06, -6.1169e-06,  4.4368e-06,  ..., -7.7337e-06,
         -5.0440e-06, -5.2676e-06],
        [-2.1368e-05, -1.3679e-05,  1.0118e-05,  ..., -1.6749e-05,
         -1.0893e-05, -1.0982e-05],
        [-2.4557e-05, -1.5199e-05,  1.1623e-05,  ..., -1.9416e-05,
         -1.2308e-05, -1.3620e-05],
        [-2.7508e-05, -1.7777e-05,  1.2964e-05,  ..., -2.1487e-05,
         -1.4305e-05, -1.3381e-05]], device='cuda:0')
Loss: 1.0585484504699707


Running epoch 0, step 781, batch 781
Sampled inputs[:2]: tensor([[   0,  409,  729,  ...,  391,  266,  996],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2056e-05,  1.3611e-04, -1.4903e-04,  ...,  1.4051e-04,
         -5.6234e-04, -3.3350e-04],
        [-1.1712e-05, -7.4282e-06,  5.3979e-06,  ..., -9.2387e-06,
         -5.9679e-06, -6.2436e-06],
        [-2.5690e-05, -1.6630e-05,  1.2279e-05,  ..., -2.0087e-05,
         -1.2949e-05, -1.3083e-05],
        [-2.9534e-05, -1.8507e-05,  1.4141e-05,  ..., -2.3231e-05,
         -1.4573e-05, -1.6183e-05],
        [-3.3140e-05, -2.1622e-05,  1.5751e-05,  ..., -2.5839e-05,
         -1.7062e-05, -1.6004e-05]], device='cuda:0')
Loss: 1.0461103916168213


Running epoch 0, step 782, batch 782
Sampled inputs[:2]: tensor([[   0,   12,  722,  ...,  674,  369,  897],
        [   0,  360,  259,  ..., 5710,  278, 2433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0001e-04,  2.2805e-04, -5.4582e-04,  ...,  4.6141e-04,
         -1.3268e-03, -7.9584e-04],
        [-1.3538e-05, -8.5011e-06,  6.3218e-06,  ..., -1.0736e-05,
         -6.9812e-06, -7.4431e-06],
        [-2.9713e-05, -1.9088e-05,  1.4395e-05,  ..., -2.3350e-05,
         -1.5169e-05, -1.5572e-05],
        [-3.4362e-05, -2.1324e-05,  1.6734e-05,  ..., -2.7224e-05,
         -1.7211e-05, -1.9446e-05],
        [-3.8296e-05, -2.4810e-05,  1.8418e-05,  ..., -3.0011e-05,
         -1.9968e-05, -1.8999e-05]], device='cuda:0')
Loss: 1.0173802375793457


Running epoch 0, step 783, batch 783
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0,   14, 3080,  ...,  910,  266, 5275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3155e-04,  5.9739e-04, -6.6824e-04,  ...,  7.9353e-04,
         -1.5509e-03, -1.2829e-03],
        [-1.5549e-05, -9.7752e-06,  7.2382e-06,  ..., -1.2234e-05,
         -7.9721e-06, -8.6054e-06],
        [-3.4362e-05, -2.2084e-05,  1.6570e-05,  ..., -2.6777e-05,
         -1.7434e-05, -1.8150e-05],
        [-3.9458e-05, -2.4527e-05,  1.9178e-05,  ..., -3.1009e-05,
         -1.9625e-05, -2.2441e-05],
        [-4.4197e-05, -2.8625e-05,  2.1160e-05,  ..., -3.4362e-05,
         -2.2903e-05, -2.2113e-05]], device='cuda:0')
Loss: 1.0167980194091797
Graident accumulation at epoch 0, step 783, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0235, -0.0193],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0279,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.8855e-05,  1.1894e-04, -1.8120e-04,  ...,  1.1472e-04,
         -1.3159e-04,  4.4853e-05],
        [-1.4981e-05, -9.9768e-06,  6.8654e-06,  ..., -1.2673e-05,
         -7.1201e-06, -9.3637e-06],
        [ 1.2287e-05,  1.0351e-05, -3.4610e-06,  ...,  8.2938e-06,
          8.4946e-06,  1.9620e-06],
        [-7.8914e-06, -1.5330e-06,  5.0358e-06,  ..., -9.3795e-07,
         -2.4901e-06, -7.7822e-06],
        [-3.3486e-05, -2.2476e-05,  1.6957e-05,  ..., -2.7240e-05,
         -1.3710e-05, -1.8507e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2563e-08, 4.6682e-08, 3.5172e-08,  ..., 3.5055e-08, 9.6272e-08,
         3.5377e-08],
        [5.3653e-11, 2.9979e-11, 4.1577e-12,  ..., 3.7867e-11, 3.6053e-12,
         1.1433e-11],
        [2.2779e-09, 1.0928e-09, 3.4592e-10,  ..., 1.6659e-09, 2.4362e-10,
         6.5698e-10],
        [6.9307e-10, 5.1640e-10, 7.1806e-11,  ..., 5.7177e-10, 9.3712e-11,
         2.3571e-10],
        [2.6709e-10, 1.4667e-10, 2.6997e-11,  ..., 1.9585e-10, 3.2308e-11,
         6.5571e-11]], device='cuda:0')
optimizer state dict: 98.0
lr: [1.4408237148944047e-05, 1.4408237148944047e-05]
scheduler_last_epoch: 98


Running epoch 0, step 784, batch 784
Sampled inputs[:2]: tensor([[   0, 9611,  278,  ...,  278,  638,  600],
        [   0, 2587,   27,  ...,  259, 2462, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7860e-05,  1.6905e-04, -1.9241e-05,  ...,  8.2984e-05,
         -4.2057e-05, -8.4455e-06],
        [-2.0862e-06, -1.3337e-06,  9.2387e-07,  ..., -1.5721e-06,
         -1.0580e-06, -1.0803e-06],
        [-4.7088e-06, -3.0398e-06,  2.1756e-06,  ..., -3.5018e-06,
         -2.3246e-06, -2.3246e-06],
        [-4.9770e-06, -3.1590e-06,  2.2948e-06,  ..., -3.7402e-06,
         -2.4438e-06, -2.6226e-06],
        [-6.0201e-06, -3.9041e-06,  2.7865e-06,  ..., -4.4703e-06,
         -3.0249e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.0391429662704468


Running epoch 0, step 785, batch 785
Sampled inputs[:2]: tensor([[   0,  380,  759,  ..., 1420, 1804,  490],
        [   0, 5902,  518,  ..., 3126,   12,  497]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2590e-05,  4.5308e-04,  2.8229e-05,  ...,  1.8363e-04,
         -2.4065e-04, -2.3308e-05],
        [-4.0084e-06, -2.5854e-06,  1.8179e-06,  ..., -3.1069e-06,
         -2.0638e-06, -2.1234e-06],
        [-8.9705e-06, -5.8711e-06,  4.2468e-06,  ..., -6.8247e-06,
         -4.4852e-06, -4.4852e-06],
        [-9.7156e-06, -6.1840e-06,  4.5896e-06,  ..., -7.4953e-06,
         -4.8280e-06, -5.2750e-06],
        [-1.1474e-05, -7.5549e-06,  5.4240e-06,  ..., -8.7023e-06,
         -5.8413e-06, -5.4687e-06]], device='cuda:0')
Loss: 1.0328962802886963


Running epoch 0, step 786, batch 786
Sampled inputs[:2]: tensor([[    0,   607, 27288,  ...,   445,  4712,   278],
        [    0,  8754,    14,  ...,  6125,   394,   927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7626e-05,  4.7337e-04, -4.2327e-05,  ...,  1.9935e-04,
         -2.5587e-04, -8.1132e-05],
        [-6.0499e-06, -3.9637e-06,  2.8685e-06,  ..., -4.6790e-06,
         -3.0845e-06, -3.0845e-06],
        [-1.3471e-05, -8.9705e-06,  6.6310e-06,  ..., -1.0297e-05,
         -6.7204e-06, -6.5416e-06],
        [-1.4693e-05, -9.4920e-06,  7.2122e-06,  ..., -1.1310e-05,
         -7.2271e-06, -7.7039e-06],
        [-1.7256e-05, -1.1578e-05,  8.4490e-06,  ..., -1.3173e-05,
         -8.7768e-06, -7.9870e-06]], device='cuda:0')
Loss: 1.0661414861679077


Running epoch 0, step 787, batch 787
Sampled inputs[:2]: tensor([[    0, 15033,   278,  ...,   266,  2937,    14],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4581e-04,  5.8292e-04, -2.5836e-04,  ...,  4.1040e-04,
         -7.4135e-04, -4.8221e-04],
        [-8.0019e-06, -5.2601e-06,  3.8669e-06,  ..., -6.2659e-06,
         -4.1425e-06, -4.0755e-06],
        [-1.7703e-05, -1.1817e-05,  8.8364e-06,  ..., -1.3694e-05,
         -8.9854e-06, -8.5682e-06],
        [-1.9401e-05, -1.2532e-05,  9.7156e-06,  ..., -1.5154e-05,
         -9.7156e-06, -1.0207e-05],
        [-2.2858e-05, -1.5393e-05,  1.1355e-05,  ..., -1.7673e-05,
         -1.1846e-05, -1.0520e-05]], device='cuda:0')
Loss: 1.0324592590332031


Running epoch 0, step 788, batch 788
Sampled inputs[:2]: tensor([[    0,  1234,   278,  ...,  1237,  1008,   417],
        [    0,   287,  1790,  ..., 11367,  9476,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7522e-04,  7.0276e-04, -2.5996e-04,  ...,  4.6897e-04,
         -9.4426e-04, -5.5690e-04],
        [-9.9540e-06, -6.6087e-06,  4.8727e-06,  ..., -7.8529e-06,
         -5.1931e-06, -5.0142e-06],
        [-2.1845e-05, -1.4737e-05,  1.1027e-05,  ..., -1.7032e-05,
         -1.1176e-05, -1.0453e-05],
        [-2.4050e-05, -1.5706e-05,  1.2174e-05,  ..., -1.8924e-05,
         -1.2159e-05, -1.2532e-05],
        [-2.8312e-05, -1.9267e-05,  1.4216e-05,  ..., -2.2054e-05,
         -1.4782e-05, -1.2860e-05]], device='cuda:0')
Loss: 1.0353450775146484


Running epoch 0, step 789, batch 789
Sampled inputs[:2]: tensor([[    0,  1581, 11884,  ...,  7031,   689,   527],
        [    0,    18,    14,  ...,   380,   981,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1487e-04,  7.0454e-04, -4.3502e-04,  ...,  6.4026e-04,
         -1.7733e-03, -8.5095e-04],
        [-1.1832e-05, -7.7263e-06,  5.8413e-06,  ..., -9.3430e-06,
         -6.2287e-06, -6.2063e-06],
        [-2.5958e-05, -1.7256e-05,  1.3217e-05,  ..., -2.0221e-05,
         -1.3396e-05, -1.2867e-05],
        [-2.8819e-05, -1.8477e-05,  1.4782e-05,  ..., -2.2694e-05,
         -1.4707e-05, -1.5676e-05],
        [-3.3647e-05, -2.2575e-05,  1.7032e-05,  ..., -2.6166e-05,
         -1.7717e-05, -1.5795e-05]], device='cuda:0')
Loss: 1.0112122297286987


Running epoch 0, step 790, batch 790
Sampled inputs[:2]: tensor([[    0,    15, 43895,  ...,   292,   380, 16795],
        [    0,  2025,   287,  ...,   381,  1487,  3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9176e-04,  7.6089e-04, -3.7335e-04,  ...,  6.5033e-04,
         -1.8691e-03, -9.2164e-04],
        [-1.3858e-05, -9.0599e-06,  6.8322e-06,  ..., -1.0908e-05,
         -7.2718e-06, -7.1973e-06],
        [-3.0577e-05, -2.0340e-05,  1.5527e-05,  ..., -2.3767e-05,
         -1.5751e-05, -1.5058e-05],
        [-3.3766e-05, -2.1711e-05,  1.7270e-05,  ..., -2.6509e-05,
         -1.7196e-05, -1.8165e-05],
        [-3.9637e-05, -2.6599e-05,  2.0027e-05,  ..., -3.0786e-05,
         -2.0847e-05, -1.8537e-05]], device='cuda:0')
Loss: 1.0615078210830688


Running epoch 0, step 791, batch 791
Sampled inputs[:2]: tensor([[   0, 1231,  352,  ..., 8524,   14,  381],
        [   0,   14, 3609,  ...,  298,  413,   29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9577e-04,  9.0516e-04, -3.5026e-04,  ...,  7.6882e-04,
         -2.0734e-03, -1.0062e-03],
        [-1.5825e-05, -1.0379e-05,  7.7933e-06,  ..., -1.2465e-05,
         -8.2627e-06, -8.1398e-06],
        [-3.4899e-05, -2.3305e-05,  1.7717e-05,  ..., -2.7180e-05,
         -1.7926e-05, -1.7054e-05],
        [-3.8445e-05, -2.4825e-05,  1.9655e-05,  ..., -3.0220e-05,
         -1.9506e-05, -2.0489e-05],
        [-4.5270e-05, -3.0473e-05,  2.2843e-05,  ..., -3.5226e-05,
         -2.3738e-05, -2.0996e-05]], device='cuda:0')
Loss: 1.0243090391159058
Graident accumulation at epoch 0, step 791, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0235, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0280,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1393e-05,  1.9757e-04, -1.9811e-04,  ...,  1.8013e-04,
         -3.2577e-04, -6.0255e-05],
        [-1.5066e-05, -1.0017e-05,  6.9582e-06,  ..., -1.2653e-05,
         -7.2344e-06, -9.2413e-06],
        [ 7.5687e-06,  6.9850e-06, -1.3432e-06,  ...,  4.7464e-06,
          5.8525e-06,  6.0332e-08],
        [-1.0947e-05, -3.8622e-06,  6.4977e-06,  ..., -3.8661e-06,
         -4.1917e-06, -9.0529e-06],
        [-3.4665e-05, -2.3276e-05,  1.7546e-05,  ..., -2.8039e-05,
         -1.4713e-05, -1.8756e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2737e-08, 4.7455e-08, 3.5260e-08,  ..., 3.5611e-08, 1.0047e-07,
         3.6354e-08],
        [5.3849e-11, 3.0056e-11, 4.2143e-12,  ..., 3.7985e-11, 3.6700e-12,
         1.1488e-11],
        [2.2769e-09, 1.0923e-09, 3.4589e-10,  ..., 1.6650e-09, 2.4370e-10,
         6.5661e-10],
        [6.9386e-10, 5.1650e-10, 7.2120e-11,  ..., 5.7211e-10, 9.3999e-11,
         2.3589e-10],
        [2.6888e-10, 1.4745e-10, 2.7492e-11,  ..., 1.9689e-10, 3.2839e-11,
         6.5947e-11]], device='cuda:0')
optimizer state dict: 99.0
lr: [1.4296945671295503e-05, 1.4296945671295503e-05]
scheduler_last_epoch: 99


Running epoch 0, step 792, batch 792
Sampled inputs[:2]: tensor([[    0,   461,  1169,  ..., 14135,  2771,    13],
        [    0,  1774,  1781,  ...,  4685,   409,  4614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8060e-05, -3.6426e-04,  1.3255e-04,  ..., -1.1540e-04,
          6.9427e-04,  4.8008e-04],
        [-1.8328e-06, -1.3560e-06,  1.1623e-06,  ..., -1.4380e-06,
         -9.3132e-07, -1.0729e-06],
        [-3.8743e-06, -2.9206e-06,  2.5034e-06,  ..., -2.9951e-06,
         -1.9521e-06, -2.1458e-06],
        [-4.4107e-06, -3.2187e-06,  2.9355e-06,  ..., -3.4422e-06,
         -2.1458e-06, -2.6822e-06],
        [-5.2154e-06, -3.9637e-06,  3.2932e-06,  ..., -4.0531e-06,
         -2.7567e-06, -2.6822e-06]], device='cuda:0')
Loss: 1.0228089094161987


Running epoch 0, step 793, batch 793
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,   328,   266,  ...,   382,    17,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1280e-05, -3.6426e-04,  2.2215e-04,  ..., -4.2002e-05,
          8.3658e-04,  5.8784e-04],
        [-3.8743e-06, -2.6599e-06,  2.1756e-06,  ..., -2.9802e-06,
         -2.0340e-06, -2.1830e-06],
        [-8.2850e-06, -5.8115e-06,  4.7982e-06,  ..., -6.2883e-06,
         -4.2915e-06, -4.4107e-06],
        [-9.2685e-06, -6.2585e-06,  5.4240e-06,  ..., -7.1079e-06,
         -4.6939e-06, -5.4091e-06],
        [-1.1265e-05, -7.9870e-06,  6.4224e-06,  ..., -8.5533e-06,
         -6.0201e-06, -5.6177e-06]], device='cuda:0')
Loss: 1.04438054561615


Running epoch 0, step 794, batch 794
Sampled inputs[:2]: tensor([[   0,  461, 4182,  ..., 7461,  292, 4895],
        [   0,  278, 1295,  ..., 4337,  271, 1268]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1699e-05, -3.0505e-04,  3.6086e-04,  ...,  5.0250e-07,
          1.0669e-03,  8.4288e-04],
        [-5.8264e-06, -3.9786e-06,  3.2857e-06,  ..., -4.4629e-06,
         -2.9765e-06, -3.1665e-06],
        [-1.2398e-05, -8.6427e-06,  7.1973e-06,  ..., -9.4175e-06,
         -6.2734e-06, -6.4224e-06],
        [-1.3947e-05, -9.3728e-06,  8.1956e-06,  ..., -1.0654e-05,
         -6.8694e-06, -7.8976e-06],
        [-1.6898e-05, -1.1891e-05,  9.6560e-06,  ..., -1.2845e-05,
         -8.8364e-06, -8.1956e-06]], device='cuda:0')
Loss: 1.042292594909668


Running epoch 0, step 795, batch 795
Sampled inputs[:2]: tensor([[    0,    12,   638,  ...,   374,   221,   527],
        [    0, 11054,    12,  ...,   560,   199,   677]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1163e-04, -2.3205e-04,  5.7968e-04,  ..., -1.1078e-04,
          1.2731e-03,  1.0115e-03],
        [-7.7263e-06, -5.2974e-06,  4.2766e-06,  ..., -6.0052e-06,
         -3.9674e-06, -4.1500e-06],
        [-1.6332e-05, -1.1414e-05,  9.3579e-06,  ..., -1.2562e-05,
         -8.2850e-06, -8.3447e-06],
        [-1.8448e-05, -1.2428e-05,  1.0654e-05,  ..., -1.4260e-05,
         -9.1195e-06, -1.0282e-05],
        [-2.2441e-05, -1.5825e-05,  1.2681e-05,  ..., -1.7285e-05,
         -1.1772e-05, -1.0714e-05]], device='cuda:0')
Loss: 1.0228766202926636


Running epoch 0, step 796, batch 796
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,  1725,   271,   266],
        [    0,   968,   266,  ...,   287,  2143, 15228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1166e-04, -2.8202e-04,  5.1174e-04,  ..., -3.7180e-04,
          1.5969e-03,  1.2726e-03],
        [-9.5218e-06, -6.5416e-06,  5.4091e-06,  ..., -7.4655e-06,
         -4.9174e-06, -5.1185e-06],
        [-2.0131e-05, -1.4096e-05,  1.1787e-05,  ..., -1.5602e-05,
         -1.0237e-05, -1.0267e-05],
        [-2.2888e-05, -1.5438e-05,  1.3560e-05,  ..., -1.7837e-05,
         -1.1355e-05, -1.2785e-05],
        [-2.7686e-05, -1.9565e-05,  1.5989e-05,  ..., -2.1487e-05,
         -1.4573e-05, -1.3188e-05]], device='cuda:0')
Loss: 1.0256164073944092


Running epoch 0, step 797, batch 797
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,    19,   669,  ...,    14,  4053,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8689e-04, -1.3454e-04,  5.6673e-04,  ..., -4.5107e-04,
          1.8557e-03,  1.4279e-03],
        [-1.1489e-05, -7.8529e-06,  6.4671e-06,  ..., -8.9407e-06,
         -5.9009e-06, -6.1840e-06],
        [-2.4304e-05, -1.6943e-05,  1.4141e-05,  ..., -1.8686e-05,
         -1.2279e-05, -1.2413e-05],
        [-2.7508e-05, -1.8477e-05,  1.6168e-05,  ..., -2.1279e-05,
         -1.3560e-05, -1.5378e-05],
        [-3.3379e-05, -2.3469e-05,  1.9178e-05,  ..., -2.5690e-05,
         -1.7434e-05, -1.5929e-05]], device='cuda:0')
Loss: 1.0494478940963745


Running epoch 0, step 798, batch 798
Sampled inputs[:2]: tensor([[   0,   21, 1304,  ..., 3577,   13, 2497],
        [   0,  259, 1143,  ...,  593,  360,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8499e-04, -5.2851e-05,  8.0488e-04,  ..., -3.8583e-04,
          1.9849e-03,  1.4912e-03],
        [-1.3575e-05, -9.3281e-06,  7.5847e-06,  ..., -1.0476e-05,
         -7.0184e-06, -7.1973e-06],
        [-2.8864e-05, -2.0206e-05,  1.6600e-05,  ..., -2.2024e-05,
         -1.4707e-05, -1.4588e-05],
        [-3.2425e-05, -2.1920e-05,  1.8865e-05,  ..., -2.4885e-05,
         -1.6123e-05, -1.7866e-05],
        [-3.9577e-05, -2.7940e-05,  2.2516e-05,  ..., -3.0249e-05,
         -2.0817e-05, -1.8761e-05]], device='cuda:0')
Loss: 1.0410478115081787


Running epoch 0, step 799, batch 799
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  292,   86,   16],
        [   0,  259,  587,  ...,   14,   71,  462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8283e-04, -1.3507e-04,  8.3807e-04,  ..., -5.1589e-04,
          2.3376e-03,  1.5230e-03],
        [-1.5512e-05, -1.0669e-05,  8.6874e-06,  ..., -1.1981e-05,
         -7.9237e-06, -8.1882e-06],
        [-3.2917e-05, -2.3067e-05,  1.8984e-05,  ..., -2.5168e-05,
         -1.6607e-05, -1.6555e-05],
        [-3.6895e-05, -2.4989e-05,  2.1532e-05,  ..., -2.8357e-05,
         -1.8135e-05, -2.0236e-05],
        [-4.5121e-05, -3.1903e-05,  2.5749e-05,  ..., -3.4571e-05,
         -2.3529e-05, -2.1279e-05]], device='cuda:0')
Loss: 1.033705711364746
Graident accumulation at epoch 0, step 799, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0235, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0280,  ...,  0.0289, -0.0145, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.7537e-05,  1.6430e-04, -9.4491e-05,  ...,  1.1053e-04,
         -5.9426e-05,  9.8071e-05],
        [-1.5110e-05, -1.0082e-05,  7.1311e-06,  ..., -1.2585e-05,
         -7.3033e-06, -9.1360e-06],
        [ 3.5202e-06,  3.9798e-06,  6.8955e-07,  ...,  1.7550e-06,
          3.6066e-06, -1.6012e-06],
        [-1.3542e-05, -5.9749e-06,  8.0012e-06,  ..., -6.3152e-06,
         -5.5860e-06, -1.0171e-05],
        [-3.5710e-05, -2.4139e-05,  1.8366e-05,  ..., -2.8692e-05,
         -1.5594e-05, -1.9008e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3003e-08, 4.7426e-08, 3.5927e-08,  ..., 3.5842e-08, 1.0584e-07,
         3.8637e-08],
        [5.4036e-11, 3.0140e-11, 4.2855e-12,  ..., 3.8090e-11, 3.7291e-12,
         1.1543e-11],
        [2.2757e-09, 1.0917e-09, 3.4590e-10,  ..., 1.6639e-09, 2.4373e-10,
         6.5623e-10],
        [6.9452e-10, 5.1661e-10, 7.2512e-11,  ..., 5.7234e-10, 9.4234e-11,
         2.3607e-10],
        [2.7064e-10, 1.4832e-10, 2.8127e-11,  ..., 1.9789e-10, 3.3359e-11,
         6.6333e-11]], device='cuda:0')
optimizer state dict: 100.0
lr: [1.418499758283982e-05, 1.418499758283982e-05]
scheduler_last_epoch: 100
Epoch 0 | Batch 799/1048 | Training PPL: 3903.565678336011 | time 49.17501163482666
Saving checkpoint at epoch 0, step 799, batch 799
Epoch 0 | Validation PPL: 7.83650223326948 | Learning rate: 1.418499758283982e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_799, AFTER epoch 0, step 799


Running epoch 0, step 800, batch 800
Sampled inputs[:2]: tensor([[    0,  1144,  2680,  ...,   963,     9,  1184],
        [    0,  5685,   565,  ..., 23968,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2040e-04, -4.6920e-04,  1.0619e-04,  ..., -3.4795e-04,
          1.0933e-03,  4.7828e-04],
        [-1.7062e-06, -1.1325e-06,  1.0803e-06,  ..., -1.4454e-06,
         -9.7603e-07, -1.1846e-06],
        [-3.5167e-06, -2.4289e-06,  2.2799e-06,  ..., -2.8163e-06,
         -1.9222e-06, -2.1011e-06],
        [-4.0233e-06, -2.6077e-06,  2.7269e-06,  ..., -3.3379e-06,
         -2.2054e-06, -2.8163e-06],
        [-5.0366e-06, -3.5316e-06,  3.2037e-06,  ..., -3.9935e-06,
         -2.8312e-06, -2.7120e-06]], device='cuda:0')
Loss: 1.0028753280639648


Running epoch 0, step 801, batch 801
Sampled inputs[:2]: tensor([[    0,   266,   283,  ...,   271, 48829,   580],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3633e-04, -6.8088e-04,  1.4344e-04,  ..., -6.9708e-04,
          1.8241e-03,  8.6335e-04],
        [-3.6061e-06, -2.5257e-06,  2.2799e-06,  ..., -2.8908e-06,
         -1.9446e-06, -2.3022e-06],
        [-7.3910e-06, -5.3048e-06,  4.7982e-06,  ..., -5.7369e-06,
         -3.8892e-06, -4.2468e-06],
        [-8.1956e-06, -5.6177e-06,  5.4985e-06,  ..., -6.4820e-06,
         -4.2468e-06, -5.3197e-06],
        [-1.0312e-05, -7.4655e-06,  6.5863e-06,  ..., -7.9870e-06,
         -5.6177e-06, -5.4091e-06]], device='cuda:0')
Loss: 1.0455876588821411


Running epoch 0, step 802, batch 802
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,  3047,  4878,  ...,   352, 10854, 34025]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5057e-04, -5.8085e-04,  1.0946e-04,  ..., -6.4917e-04,
          1.6869e-03,  7.8820e-04],
        [-5.4017e-06, -3.7774e-06,  3.3677e-06,  ..., -4.3362e-06,
         -3.0026e-06, -3.3528e-06],
        [-1.1042e-05, -7.9125e-06,  7.1079e-06,  ..., -8.6129e-06,
         -5.9456e-06, -6.2585e-06],
        [-1.2428e-05, -8.4937e-06,  8.2105e-06,  ..., -9.8348e-06,
         -6.6012e-06, -7.9125e-06],
        [-1.5557e-05, -1.1280e-05,  9.8944e-06,  ..., -1.2130e-05,
         -8.6874e-06, -8.0764e-06]], device='cuda:0')
Loss: 1.0221823453903198


Running epoch 0, step 803, batch 803
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ...,  271,   13,  704],
        [   0,  596,  292,  ...,   13, 6673,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9894e-04, -5.8085e-04,  2.7354e-04,  ..., -7.5544e-04,
          1.9775e-03,  1.1042e-03],
        [-7.2420e-06, -5.0738e-06,  4.4703e-06,  ..., -5.7817e-06,
         -4.0159e-06, -4.4033e-06],
        [-1.4827e-05, -1.0625e-05,  9.4324e-06,  ..., -1.1548e-05,
         -7.9870e-06, -8.3148e-06],
        [-1.6659e-05, -1.1429e-05,  1.0878e-05,  ..., -1.3143e-05,
         -8.8364e-06, -1.0431e-05],
        [-2.0862e-05, -1.5125e-05,  1.3113e-05,  ..., -1.6272e-05,
         -1.1668e-05, -1.0744e-05]], device='cuda:0')
Loss: 1.0386021137237549


Running epoch 0, step 804, batch 804
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,  9636,   417,   199],
        [    0,  2270,   278,  ..., 36325,  5892,  3558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3607e-04, -5.5131e-04,  2.7613e-04,  ..., -7.3995e-04,
          1.9753e-03,  1.1214e-03],
        [-9.0376e-06, -6.4224e-06,  5.6401e-06,  ..., -7.2569e-06,
         -5.0366e-06, -5.4017e-06],
        [-1.8448e-05, -1.3396e-05,  1.1861e-05,  ..., -1.4484e-05,
         -1.0014e-05, -1.0215e-05],
        [-2.0713e-05, -1.4439e-05,  1.3635e-05,  ..., -1.6466e-05,
         -1.1072e-05, -1.2770e-05],
        [-2.5898e-05, -1.9029e-05,  1.6436e-05,  ..., -2.0385e-05,
         -1.4588e-05, -1.3188e-05]], device='cuda:0')
Loss: 1.0428798198699951


Running epoch 0, step 805, batch 805
Sampled inputs[:2]: tensor([[    0,   266,  3634,  ...,   694,   266,  1784],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9178e-04, -6.8441e-04,  4.4563e-04,  ..., -8.3550e-04,
          2.0925e-03,  1.3293e-03],
        [-1.1064e-05, -7.8529e-06,  6.7204e-06,  ..., -8.8215e-06,
         -6.1542e-06, -6.4746e-06],
        [-2.2531e-05, -1.6361e-05,  1.4111e-05,  ..., -1.7628e-05,
         -1.2264e-05, -1.2286e-05],
        [-2.5064e-05, -1.7494e-05,  1.6063e-05,  ..., -1.9833e-05,
         -1.3426e-05, -1.5169e-05],
        [-3.1680e-05, -2.3261e-05,  1.9595e-05,  ..., -2.4855e-05,
         -1.7881e-05, -1.5914e-05]], device='cuda:0')
Loss: 1.0427184104919434


Running epoch 0, step 806, batch 806
Sampled inputs[:2]: tensor([[    0, 40995,  5863,  ...,    13,  9819,   609],
        [    0,   292,    17,  ...,  5760,  1345,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2547e-04, -6.9653e-04,  4.7873e-04,  ..., -8.2939e-04,
          1.9437e-03,  1.3599e-03],
        [-1.3061e-05, -9.2238e-06,  7.8455e-06,  ..., -1.0312e-05,
         -7.1451e-06, -7.5474e-06],
        [-2.6673e-05, -1.9237e-05,  1.6540e-05,  ..., -2.0698e-05,
         -1.4305e-05, -1.4417e-05],
        [-2.9624e-05, -2.0564e-05,  1.8746e-05,  ..., -2.3186e-05,
         -1.5587e-05, -1.7673e-05],
        [-3.7551e-05, -2.7344e-05,  2.2978e-05,  ..., -2.9206e-05,
         -2.0877e-05, -1.8746e-05]], device='cuda:0')
Loss: 1.0202889442443848


Running epoch 0, step 807, batch 807
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0,    52, 26766,  ...,  4411,  4226,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4770e-04, -6.2204e-04,  7.5381e-04,  ..., -7.0446e-04,
          2.3633e-03,  1.4684e-03],
        [-1.5013e-05, -1.0617e-05,  8.9929e-06,  ..., -1.1802e-05,
         -8.1807e-06, -8.5682e-06],
        [-3.0726e-05, -2.2203e-05,  1.8984e-05,  ..., -2.3797e-05,
         -1.6421e-05, -1.6473e-05],
        [-3.4004e-05, -2.3648e-05,  2.1413e-05,  ..., -2.6524e-05,
         -1.7822e-05, -2.0072e-05],
        [-4.3154e-05, -3.1486e-05,  2.6330e-05,  ..., -3.3528e-05,
         -2.3916e-05, -2.1398e-05]], device='cuda:0')
Loss: 1.0514806509017944
Graident accumulation at epoch 0, step 807, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0235, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0158,  0.0153, -0.0280,  ...,  0.0290, -0.0145, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1455e-04,  8.5668e-05, -9.6602e-06,  ...,  2.9028e-05,
          1.8285e-04,  2.3510e-04],
        [-1.5101e-05, -1.0136e-05,  7.3173e-06,  ..., -1.2507e-05,
         -7.3910e-06, -9.0792e-06],
        [ 9.5531e-08,  1.3616e-06,  2.5190e-06,  ..., -8.0022e-07,
          1.6038e-06, -3.0884e-06],
        [-1.5588e-05, -7.7422e-06,  9.3423e-06,  ..., -8.3361e-06,
         -6.8096e-06, -1.1161e-05],
        [-3.6455e-05, -2.4873e-05,  1.9162e-05,  ..., -2.9176e-05,
         -1.6427e-05, -1.9247e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3131e-08, 4.7765e-08, 3.6459e-08,  ..., 3.6302e-08, 1.1132e-07,
         4.0755e-08],
        [5.4208e-11, 3.0223e-11, 4.3621e-12,  ..., 3.8192e-11, 3.7923e-12,
         1.1605e-11],
        [2.2743e-09, 1.0911e-09, 3.4592e-10,  ..., 1.6628e-09, 2.4376e-10,
         6.5584e-10],
        [6.9498e-10, 5.1665e-10, 7.2898e-11,  ..., 5.7247e-10, 9.4457e-11,
         2.3623e-10],
        [2.7224e-10, 1.4917e-10, 2.8792e-11,  ..., 1.9882e-10, 3.3898e-11,
         6.6725e-11]], device='cuda:0')
optimizer state dict: 101.0
lr: [1.4072409990222116e-05, 1.4072409990222116e-05]
scheduler_last_epoch: 101


Running epoch 0, step 808, batch 808
Sampled inputs[:2]: tensor([[    0,  2919,  1482,  ...,   587, 20186,   275],
        [    0,  8840,    26,  ...,    28,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1991e-04,  4.2018e-05, -1.1318e-04,  ...,  3.9473e-05,
          3.8568e-05,  1.4564e-04],
        [-1.9222e-06, -1.3635e-06,  1.1325e-06,  ..., -1.4752e-06,
         -1.1474e-06, -1.0803e-06],
        [-3.8743e-06, -2.8014e-06,  2.3395e-06,  ..., -2.9504e-06,
         -2.2799e-06, -2.1160e-06],
        [-4.3213e-06, -3.0398e-06,  2.6375e-06,  ..., -3.3230e-06,
         -2.5183e-06, -2.5481e-06],
        [-5.3644e-06, -3.9339e-06,  3.2336e-06,  ..., -4.1127e-06,
         -3.2634e-06, -2.7567e-06]], device='cuda:0')
Loss: 1.05115807056427


Running epoch 0, step 809, batch 809
Sampled inputs[:2]: tensor([[    0,  1871,   518,  ...,   271,   259,  1110],
        [    0, 17561,    12,  ...,   741,   496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3981e-04,  1.5778e-05, -1.8782e-04,  ...,  1.4476e-04,
         -1.8494e-04,  8.8398e-05],
        [-3.7849e-06, -2.6897e-06,  2.2501e-06,  ..., -2.9504e-06,
         -2.2277e-06, -2.1383e-06],
        [-7.5996e-06, -5.4985e-06,  4.6194e-06,  ..., -5.8562e-06,
         -4.3809e-06, -4.1127e-06],
        [-8.4341e-06, -5.9158e-06,  5.2303e-06,  ..., -6.5714e-06,
         -4.8280e-06, -5.0068e-06],
        [-1.0610e-05, -7.7784e-06,  6.4075e-06,  ..., -8.2254e-06,
         -6.3181e-06, -5.3793e-06]], device='cuda:0')
Loss: 1.0215201377868652


Running epoch 0, step 810, batch 810
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,  3141,   311,  ...,   328,  7818,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5681e-04,  4.7366e-05, -1.1001e-04,  ...,  1.4015e-04,
         -7.9927e-05,  2.4960e-04],
        [-5.7369e-06, -4.0680e-06,  3.3453e-06,  ..., -4.4703e-06,
         -3.3230e-06, -3.1516e-06],
        [-1.1444e-05, -8.2850e-06,  6.8396e-06,  ..., -8.8364e-06,
         -6.5267e-06, -6.0648e-06],
        [-1.2875e-05, -9.0152e-06,  7.7933e-06,  ..., -1.0028e-05,
         -7.2718e-06, -7.4208e-06],
        [-1.5944e-05, -1.1683e-05,  9.4771e-06,  ..., -1.2398e-05,
         -9.3877e-06, -7.9423e-06]], device='cuda:0')
Loss: 1.0664013624191284


Running epoch 0, step 811, batch 811
Sampled inputs[:2]: tensor([[   0,  287,  768,  ...,  221,  474,  221],
        [   0,  266, 4908,  ..., 1209,  328, 1603]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3569e-04, -4.0428e-06, -1.8515e-04,  ...,  1.7623e-04,
         -8.5453e-05,  1.4958e-04],
        [-7.5251e-06, -5.3570e-06,  4.4331e-06,  ..., -5.9456e-06,
         -4.3064e-06, -4.2319e-06],
        [-1.4871e-05, -1.0788e-05,  8.9854e-06,  ..., -1.1623e-05,
         -8.3819e-06, -8.0317e-06],
        [-1.6898e-05, -1.1876e-05,  1.0371e-05,  ..., -1.3322e-05,
         -9.4175e-06, -9.9242e-06],
        [-2.0802e-05, -1.5274e-05,  1.2487e-05,  ..., -1.6332e-05,
         -1.2144e-05, -1.0505e-05]], device='cuda:0')
Loss: 1.004287600517273


Running epoch 0, step 812, batch 812
Sampled inputs[:2]: tensor([[    0,   287, 19777,  ...,   266,  5061,   278],
        [    0,    13,  7805,  ...,  2733,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3346e-04,  1.2987e-04, -7.9060e-05,  ...,  2.4567e-04,
         -1.1479e-04,  1.0890e-04],
        [-9.5516e-06, -6.8173e-06,  5.4911e-06,  ..., -7.4357e-06,
         -5.3272e-06, -5.3123e-06],
        [-1.9163e-05, -1.3933e-05,  1.1295e-05,  ..., -1.4767e-05,
         -1.0528e-05, -1.0282e-05],
        [-2.1398e-05, -1.5095e-05,  1.2800e-05,  ..., -1.6600e-05,
         -1.1608e-05, -1.2398e-05],
        [-2.6643e-05, -1.9565e-05,  1.5602e-05,  ..., -2.0623e-05,
         -1.5154e-05, -1.3396e-05]], device='cuda:0')
Loss: 1.0407960414886475


Running epoch 0, step 813, batch 813
Sampled inputs[:2]: tensor([[    0,   689,    13,  ...,   756,   271, 31773],
        [    0,  2416,   352,  ...,   278,  1036, 16832]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4594e-04, -1.1089e-05, -1.7457e-04,  ...,  3.5566e-04,
         -2.5212e-04,  5.2264e-05],
        [-1.1258e-05, -8.1062e-06,  6.5193e-06,  ..., -8.8289e-06,
         -6.2734e-06, -6.2808e-06],
        [-2.2516e-05, -1.6510e-05,  1.3351e-05,  ..., -1.7494e-05,
         -1.2375e-05, -1.2130e-05],
        [-2.5421e-05, -1.8075e-05,  1.5303e-05,  ..., -1.9863e-05,
         -1.3769e-05, -1.4782e-05],
        [-3.1412e-05, -2.3276e-05,  1.8492e-05,  ..., -2.4498e-05,
         -1.7881e-05, -1.5840e-05]], device='cuda:0')
Loss: 1.0348323583602905


Running epoch 0, step 814, batch 814
Sampled inputs[:2]: tensor([[    0,    61, 22315,  ..., 36901,    17,   360],
        [    0,  1481,   278,  ...,  3940,  4938,     5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1898e-04, -1.9154e-04, -4.9258e-05,  ...,  3.0005e-04,
         -1.5282e-05,  1.9100e-04],
        [-1.3113e-05, -9.4622e-06,  7.6145e-06,  ..., -1.0289e-05,
         -7.3239e-06, -7.2792e-06],
        [ 5.3840e-05,  5.9619e-05, -4.8270e-05,  ...,  6.6659e-05,
          7.3455e-05,  5.1558e-06],
        [-2.9594e-05, -2.1100e-05,  1.7881e-05,  ..., -2.3142e-05,
         -1.6063e-05, -1.7166e-05],
        [-3.6627e-05, -2.7210e-05,  2.1636e-05,  ..., -2.8580e-05,
         -2.0891e-05, -1.8358e-05]], device='cuda:0')
Loss: 1.0436354875564575


Running epoch 0, step 815, batch 815
Sampled inputs[:2]: tensor([[    0,  3829,   278,  ..., 11978,     9,   968],
        [    0,   328,  5180,  ...,   344,  2356,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4812e-05, -3.2393e-04, -9.8101e-05,  ...,  3.4291e-04,
          1.1076e-04,  3.7226e-04],
        [-1.4991e-05, -1.0788e-05,  8.7544e-06,  ..., -1.1802e-05,
         -8.3372e-06, -8.3596e-06],
        [ 5.0204e-05,  5.6997e-05, -4.5990e-05,  ...,  6.3783e-05,
          7.1533e-05,  3.1739e-06],
        [-3.3826e-05, -2.4021e-05,  2.0579e-05,  ..., -2.6524e-05,
         -1.8269e-05, -1.9714e-05],
        [-4.1664e-05, -3.0890e-05,  2.4766e-05,  ..., -3.2574e-05,
         -2.3663e-05, -2.0891e-05]], device='cuda:0')
Loss: 1.05790376663208
Graident accumulation at epoch 0, step 815, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0153, -0.0280,  ...,  0.0290, -0.0145, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.3617e-05,  4.4708e-05, -1.8504e-05,  ...,  6.0416e-05,
          1.7564e-04,  2.4882e-04],
        [-1.5090e-05, -1.0201e-05,  7.4610e-06,  ..., -1.2436e-05,
         -7.4857e-06, -9.0072e-06],
        [ 5.1064e-06,  6.9251e-06, -2.3319e-06,  ...,  5.6581e-06,
          8.5967e-06, -2.4622e-06],
        [-1.7412e-05, -9.3701e-06,  1.0466e-05,  ..., -1.0155e-05,
         -7.9555e-06, -1.2017e-05],
        [-3.6976e-05, -2.5475e-05,  1.9723e-05,  ..., -2.9516e-05,
         -1.7150e-05, -1.9412e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3067e-08, 4.7822e-08, 3.6432e-08,  ..., 3.6383e-08, 1.1122e-07,
         4.0853e-08],
        [5.4378e-11, 3.0309e-11, 4.4344e-12,  ..., 3.8293e-11, 3.8580e-12,
         1.1663e-11],
        [2.2746e-09, 1.0933e-09, 3.4769e-10,  ..., 1.6653e-09, 2.4863e-10,
         6.5520e-10],
        [6.9543e-10, 5.1671e-10, 7.3248e-11,  ..., 5.7260e-10, 9.4697e-11,
         2.3638e-10],
        [2.7370e-10, 1.4997e-10, 2.9377e-11,  ..., 1.9968e-10, 3.4424e-11,
         6.7095e-11]], device='cuda:0')
optimizer state dict: 102.0
lr: [1.3959200097809337e-05, 1.3959200097809337e-05]
scheduler_last_epoch: 102


Running epoch 0, step 816, batch 816
Sampled inputs[:2]: tensor([[    0,   689,  2149,  ...,  4263,    14,   292],
        [    0,  6481,   298,  ...,  6145, 16858,   824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8439e-05,  1.4636e-04,  3.1416e-06,  ...,  1.0613e-04,
         -2.7412e-04, -8.9403e-06],
        [-1.8701e-06, -1.1921e-06,  9.6858e-07,  ..., -1.4231e-06,
         -9.6858e-07, -1.2517e-06],
        [-3.7849e-06, -2.5034e-06,  2.0266e-06,  ..., -2.8312e-06,
         -1.9222e-06, -2.3991e-06],
        [-4.1127e-06, -2.5630e-06,  2.2352e-06,  ..., -3.0994e-06,
         -2.0564e-06, -2.8610e-06],
        [-5.1260e-06, -3.4571e-06,  2.7120e-06,  ..., -3.8743e-06,
         -2.7120e-06, -3.0398e-06]], device='cuda:0')
Loss: 1.0014584064483643


Running epoch 0, step 817, batch 817
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  365, 1943,  259],
        [   0,   17,   12,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8651e-05,  3.1033e-04,  7.0078e-05,  ...,  1.2803e-04,
         -1.6064e-04,  1.9737e-04],
        [-3.8818e-06, -2.6301e-06,  2.0191e-06,  ..., -2.9877e-06,
         -2.1830e-06, -2.4214e-06],
        [-7.8082e-06, -5.4240e-06,  4.1872e-06,  ..., -5.9456e-06,
         -4.3213e-06, -4.6939e-06],
        [-8.6427e-06, -5.7518e-06,  4.6641e-06,  ..., -6.5863e-06,
         -4.7088e-06, -5.5879e-06],
        [-1.0729e-05, -7.5698e-06,  5.6922e-06,  ..., -8.2254e-06,
         -6.1393e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.0501295328140259


Running epoch 0, step 818, batch 818
Sampled inputs[:2]: tensor([[   0, 5440,   13,  ..., 1878,  342, 2060],
        [   0, 6477,   12,  ..., 2931,  221,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9942e-05,  1.9280e-04,  4.2037e-05,  ...,  2.0068e-04,
         -1.7625e-04,  1.7806e-04],
        [-5.7593e-06, -3.9935e-06,  3.0547e-06,  ..., -4.4852e-06,
         -3.2783e-06, -3.4347e-06],
        [-1.1533e-05, -8.1658e-06,  6.2883e-06,  ..., -8.8960e-06,
         -6.4671e-06, -6.6906e-06],
        [ 6.0319e-05,  3.8546e-05, -5.6319e-05,  ...,  6.7917e-05,
          4.0976e-05,  8.3441e-06],
        [-1.6004e-05, -1.1474e-05,  8.6278e-06,  ..., -1.2428e-05,
         -9.2536e-06, -8.7619e-06]], device='cuda:0')
Loss: 1.0410547256469727


Running epoch 0, step 819, batch 819
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,    19,    14,  ...,   278,  2588,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8347e-05,  1.2511e-04,  2.6052e-05,  ...,  1.9315e-04,
         -2.5213e-04,  2.0770e-04],
        [-7.7114e-06, -5.4017e-06,  4.1053e-06,  ..., -6.0424e-06,
         -4.3809e-06, -4.4927e-06],
        [-1.5497e-05, -1.1086e-05,  8.4788e-06,  ..., -1.2055e-05,
         -8.7172e-06, -8.8066e-06],
        [ 5.5998e-05,  3.5431e-05, -5.3920e-05,  ...,  6.4460e-05,
          3.8562e-05,  5.9152e-06],
        [-2.1428e-05, -1.5557e-05,  1.1608e-05,  ..., -1.6809e-05,
         -1.2442e-05, -1.1533e-05]], device='cuda:0')
Loss: 1.0493106842041016


Running epoch 0, step 820, batch 820
Sampled inputs[:2]: tensor([[   0, 3306, 4057,  ...,  287,  266, 1692],
        [   0, 3951,   77,  ..., 7062,  278,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2500e-04,  2.2376e-05,  8.3127e-05,  ...,  2.1606e-04,
         -2.6502e-04,  3.1652e-04],
        [-9.6187e-06, -6.8173e-06,  5.1856e-06,  ..., -7.5251e-06,
         -5.3942e-06, -5.5060e-06],
        [-1.9208e-05, -1.3903e-05,  1.0639e-05,  ..., -1.4961e-05,
         -1.0699e-05, -1.0774e-05],
        [ 5.1736e-05,  3.2272e-05, -5.1417e-05,  ...,  6.1152e-05,
          3.6357e-05,  3.5459e-06],
        [-2.6584e-05, -1.9521e-05,  1.4573e-05,  ..., -2.0862e-05,
         -1.5333e-05, -1.4111e-05]], device='cuda:0')
Loss: 1.038118600845337


Running epoch 0, step 821, batch 821
Sampled inputs[:2]: tensor([[    0,   342,  4014,  ...,   368,   408,  2105],
        [    0,   266, 10262,  ...,   271,  3437,  4392]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4209e-04,  1.1203e-04,  2.7160e-04,  ...,  2.1978e-04,
          3.8257e-05,  3.9387e-04],
        [-1.1526e-05, -8.1733e-06,  6.1356e-06,  ..., -8.9929e-06,
         -6.5491e-06, -6.6981e-06],
        [-2.3201e-05, -1.6764e-05,  1.2696e-05,  ..., -1.7971e-05,
         -1.3039e-05, -1.3173e-05],
        [ 4.7474e-05,  2.9292e-05, -4.9197e-05,  ...,  5.7889e-05,
          3.3884e-05,  8.1900e-07],
        [-3.2037e-05, -2.3454e-05,  1.7375e-05,  ..., -2.4974e-05,
         -1.8597e-05, -1.7196e-05]], device='cuda:0')
Loss: 1.0252882242202759


Running epoch 0, step 822, batch 822
Sampled inputs[:2]: tensor([[    0,  1238,    14,  ...,   368,   940,   437],
        [    0, 10026,   992,  ...,   273,  2831,  8716]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6464e-04,  2.5652e-04,  3.7475e-04,  ...,  3.5061e-04,
         -3.2034e-05,  4.2847e-04],
        [-1.3322e-05, -9.4250e-06,  7.0781e-06,  ..., -1.0476e-05,
         -7.6294e-06, -7.8455e-06],
        [-2.6822e-05, -1.9312e-05,  1.4678e-05,  ..., -2.0891e-05,
         -1.5140e-05, -1.5363e-05],
        [ 4.3481e-05,  2.6565e-05, -4.7006e-05,  ...,  5.4640e-05,
          3.1589e-05, -1.7887e-06],
        [-3.7074e-05, -2.7031e-05,  2.0117e-05,  ..., -2.8998e-05,
         -2.1577e-05, -2.0042e-05]], device='cuda:0')
Loss: 1.036144733428955


Running epoch 0, step 823, batch 823
Sampled inputs[:2]: tensor([[    0,   850,    13,  ..., 11823,    13, 30706],
        [    0,   380,  6119,  ..., 11823,   287,  6797]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2966e-05,  1.3243e-04,  4.2535e-04,  ...,  3.4925e-04,
          1.8770e-06,  4.0178e-04],
        [-1.5095e-05, -1.0699e-05,  8.0988e-06,  ..., -1.1876e-05,
         -8.6650e-06, -8.9258e-06],
        [-3.0324e-05, -2.1890e-05,  1.6749e-05,  ..., -2.3633e-05,
         -1.7136e-05, -1.7434e-05],
        [ 3.9368e-05,  2.3660e-05, -4.4532e-05,  ...,  5.1407e-05,
          2.9264e-05, -4.4411e-06],
        [-4.2021e-05, -3.0741e-05,  2.3007e-05,  ..., -3.2902e-05,
         -2.4498e-05, -2.2784e-05]], device='cuda:0')
Loss: 1.0446991920471191
Graident accumulation at epoch 0, step 823, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0024,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0153, -0.0280,  ...,  0.0290, -0.0145, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.4958e-05,  5.3480e-05,  2.5881e-05,  ...,  8.9300e-05,
          1.5826e-04,  2.6411e-04],
        [-1.5090e-05, -1.0251e-05,  7.5248e-06,  ..., -1.2380e-05,
         -7.6036e-06, -8.9991e-06],
        [ 1.5634e-06,  4.0436e-06, -4.2385e-07,  ...,  2.7290e-06,
          6.0234e-06, -3.9594e-06],
        [-1.1734e-05, -6.0671e-06,  4.9661e-06,  ..., -3.9987e-06,
         -4.2335e-06, -1.1259e-05],
        [-3.7480e-05, -2.6002e-05,  2.0051e-05,  ..., -2.9854e-05,
         -1.7885e-05, -1.9749e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3002e-08, 4.7792e-08, 3.6577e-08,  ..., 3.6469e-08, 1.1111e-07,
         4.0973e-08],
        [5.4552e-11, 3.0393e-11, 4.4955e-12,  ..., 3.8395e-11, 3.9293e-12,
         1.1731e-11],
        [2.2732e-09, 1.0927e-09, 3.4762e-10,  ..., 1.6641e-09, 2.4867e-10,
         6.5485e-10],
        [6.9629e-10, 5.1675e-10, 7.5158e-11,  ..., 5.7467e-10, 9.5458e-11,
         2.3617e-10],
        [2.7519e-10, 1.5077e-10, 2.9877e-11,  ..., 2.0056e-10, 3.4990e-11,
         6.7547e-11]], device='cuda:0')
optimizer state dict: 103.0
lr: [1.3845385205061268e-05, 1.3845385205061268e-05]
scheduler_last_epoch: 103


Running epoch 0, step 824, batch 824
Sampled inputs[:2]: tensor([[    0, 16847,  2027,  ...,     5,  1460,   496],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4604e-05,  5.8470e-05,  4.7816e-06,  ..., -1.9788e-05,
         -2.7885e-04, -7.4015e-05],
        [-1.7658e-06, -1.2144e-06,  8.9779e-07,  ..., -1.4082e-06,
         -1.1101e-06, -1.1921e-06],
        [ 8.4408e-05,  4.9935e-05, -5.4350e-05,  ...,  6.5368e-05,
          6.7711e-05,  2.6404e-05],
        [-4.1723e-06, -2.8312e-06,  2.2054e-06,  ..., -3.3081e-06,
         -2.5928e-06, -2.9057e-06],
        [-4.9174e-06, -3.5018e-06,  2.6077e-06,  ..., -3.8743e-06,
         -3.1143e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.021913766860962


Running epoch 0, step 825, batch 825
Sampled inputs[:2]: tensor([[   0,  221,  374,  ..., 2296,  365, 4579],
        [   0, 7117,  278,  ...,  287,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2630e-04,  2.1553e-04,  1.6212e-04,  ...,  4.3632e-05,
         -1.4893e-04, -2.0412e-04],
        [-3.6731e-06, -2.5183e-06,  1.7434e-06,  ..., -2.9057e-06,
         -2.3171e-06, -2.4214e-06],
        [ 8.0534e-05,  4.7253e-05, -5.2517e-05,  ...,  6.2388e-05,
          6.5327e-05,  2.4020e-05],
        [-8.4639e-06, -5.7071e-06,  4.1872e-06,  ..., -6.6310e-06,
         -5.2303e-06, -5.7220e-06],
        [-1.0252e-05, -7.2420e-06,  5.1409e-06,  ..., -7.9572e-06,
         -6.4522e-06, -6.1393e-06]], device='cuda:0')
Loss: 1.0142351388931274


Running epoch 0, step 826, batch 826
Sampled inputs[:2]: tensor([[   0,   14,   69,  ...,  287,  259, 5158],
        [   0,  391, 7750,  ..., 4133,  271,  668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5157e-04,  2.8881e-04,  6.9924e-05,  ...,  4.5387e-05,
         -1.5706e-04, -1.8506e-04],
        [-5.5954e-06, -3.7402e-06,  2.6859e-06,  ..., -4.3586e-06,
         -3.5018e-06, -3.7625e-06],
        [ 7.6630e-05,  4.4705e-05, -5.0520e-05,  ...,  5.9467e-05,
          6.2957e-05,  2.1353e-05],
        [-1.2964e-05, -8.5086e-06,  6.4671e-06,  ..., -9.9987e-06,
         -7.9274e-06, -8.9556e-06],
        [-1.5438e-05, -1.0684e-05,  7.7635e-06,  ..., -1.1861e-05,
         -9.6858e-06, -9.5069e-06]], device='cuda:0')
Loss: 1.0050554275512695


Running epoch 0, step 827, batch 827
Sampled inputs[:2]: tensor([[    0, 27366,   504,  ...,  1358,   365,  6883],
        [    0,   278,  4452,  ...,    14,    18,  3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5740e-04,  1.8845e-04,  7.3135e-05,  ...,  1.2084e-04,
         -2.3176e-04, -1.0111e-04],
        [-7.3835e-06, -5.0217e-06,  3.7067e-06,  ..., -5.8189e-06,
         -4.5896e-06, -4.8801e-06],
        [ 7.3084e-05,  4.2127e-05, -4.8449e-05,  ...,  5.6577e-05,
          6.0827e-05,  1.9192e-05],
        [-1.7136e-05, -1.1444e-05,  8.9407e-06,  ..., -1.3426e-05,
         -1.0401e-05, -1.1683e-05],
        [-2.0355e-05, -1.4305e-05,  1.0595e-05,  ..., -1.5855e-05,
         -1.2740e-05, -1.2338e-05]], device='cuda:0')
Loss: 1.068326711654663


Running epoch 0, step 828, batch 828
Sampled inputs[:2]: tensor([[    0,  3256,   221,  ..., 18116,   292, 47989],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5365e-05, -1.5963e-04, -2.3804e-04,  ...,  1.5244e-04,
         -2.9571e-04, -5.0086e-04],
        [-9.0674e-06, -6.1095e-06,  4.6417e-06,  ..., -7.3016e-06,
         -5.7891e-06, -6.2361e-06],
        [ 6.9805e-05,  3.9907e-05, -4.6549e-05,  ...,  5.3805e-05,
          5.8606e-05,  1.6823e-05],
        [-2.1011e-05, -1.3843e-05,  1.1295e-05,  ..., -1.6838e-05,
         -1.3128e-05, -1.4946e-05],
        [-2.4974e-05, -1.7509e-05,  1.3217e-05,  ..., -1.9699e-05,
         -1.5900e-05, -1.5393e-05]], device='cuda:0')
Loss: 1.004806399345398


Running epoch 0, step 829, batch 829
Sampled inputs[:2]: tensor([[   0,   14, 2787,  ..., 9674, 2491,   12],
        [   0, 1197, 3025,  ...,   14,  747, 3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8583e-04, -2.9326e-04, -1.9586e-04,  ...,  2.3497e-04,
         -3.7516e-04, -4.5302e-04],
        [-1.0855e-05, -7.3761e-06,  5.6177e-06,  ..., -8.6874e-06,
         -6.7502e-06, -7.3314e-06],
        [ 6.6244e-05,  3.7359e-05, -4.4537e-05,  ...,  5.1048e-05,
          5.6714e-05,  1.4677e-05],
        [-2.5213e-05, -1.6764e-05,  1.3679e-05,  ..., -2.0057e-05,
         -1.5274e-05, -1.7613e-05],
        [-2.9832e-05, -2.1026e-05,  1.5929e-05,  ..., -2.3469e-05,
         -1.8597e-05, -1.8165e-05]], device='cuda:0')
Loss: 1.0387355089187622


Running epoch 0, step 830, batch 830
Sampled inputs[:2]: tensor([[    0,    16,    14,  ...,   300,  9283,    14],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5442e-04, -3.3462e-04, -5.7413e-04,  ...,  2.2042e-04,
         -5.0791e-04, -6.9368e-04],
        [-1.2755e-05, -8.7321e-06,  6.6012e-06,  ..., -1.0215e-05,
         -7.9498e-06, -8.5235e-06],
        [ 6.2563e-05,  3.4691e-05, -4.2555e-05,  ...,  4.8113e-05,
          5.4419e-05,  1.2427e-05],
        [-2.9445e-05, -1.9744e-05,  1.5959e-05,  ..., -2.3454e-05,
         -1.7896e-05, -2.0355e-05],
        [-3.4988e-05, -2.4825e-05,  1.8701e-05,  ..., -2.7582e-05,
         -2.1905e-05, -2.1145e-05]], device='cuda:0')
Loss: 1.037431001663208


Running epoch 0, step 831, batch 831
Sampled inputs[:2]: tensor([[    0,  1074,  1593,  ...,   992,  1810,   300],
        [    0,  2734,  2338,  ...,  3977,   970, 10537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2814e-04, -3.0405e-04, -6.4400e-04,  ...,  2.8291e-04,
         -7.6588e-04, -8.7780e-04],
        [-1.4506e-05, -9.9763e-06,  7.5288e-06,  ..., -1.1675e-05,
         -9.1195e-06, -9.6560e-06],
        [ 5.9136e-05,  3.2218e-05, -4.0648e-05,  ...,  4.5296e-05,
          5.2199e-05,  1.0296e-05],
        [-3.3498e-05, -2.2545e-05,  1.8209e-05,  ..., -2.6822e-05,
         -2.0534e-05, -2.3082e-05],
        [-3.9667e-05, -2.8268e-05,  2.1309e-05,  ..., -3.1427e-05,
         -2.5004e-05, -2.3872e-05]], device='cuda:0')
Loss: 1.047797441482544
Graident accumulation at epoch 0, step 831, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0059, -0.0143,  0.0023,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0097, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0153, -0.0280,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.0276e-05,  1.7728e-05, -4.1108e-05,  ...,  1.0866e-04,
          6.5849e-05,  1.4992e-04],
        [-1.5032e-05, -1.0223e-05,  7.5252e-06,  ..., -1.2310e-05,
         -7.7552e-06, -9.0648e-06],
        [ 7.3206e-06,  6.8610e-06, -4.4463e-06,  ...,  6.9857e-06,
          1.0641e-05, -2.5339e-06],
        [-1.3910e-05, -7.7149e-06,  6.2904e-06,  ..., -6.2811e-06,
         -5.8635e-06, -1.2441e-05],
        [-3.7699e-05, -2.6228e-05,  2.0177e-05,  ..., -3.0011e-05,
         -1.8597e-05, -2.0161e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2946e-08, 4.7837e-08, 3.6955e-08,  ..., 3.6513e-08, 1.1158e-07,
         4.1703e-08],
        [5.4707e-11, 3.0462e-11, 4.5477e-12,  ..., 3.8493e-11, 4.0085e-12,
         1.1813e-11],
        [2.2745e-09, 1.0926e-09, 3.4892e-10,  ..., 1.6645e-09, 2.5115e-10,
         6.5430e-10],
        [6.9671e-10, 5.1675e-10, 7.5415e-11,  ..., 5.7482e-10, 9.5784e-11,
         2.3646e-10],
        [2.7649e-10, 1.5142e-10, 3.0301e-11,  ..., 2.0135e-10, 3.5580e-11,
         6.8049e-11]], device='cuda:0')
optimizer state dict: 104.0
lr: [1.3730982703887026e-05, 1.3730982703887026e-05]
scheduler_last_epoch: 104


Running epoch 0, step 832, batch 832
Sampled inputs[:2]: tensor([[    0,  1235,   368,  ..., 12152,  8498,   287],
        [    0,   271,   768,  ..., 15555,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3385e-05, -5.5145e-06, -9.6793e-05,  ...,  2.3449e-05,
         -1.1113e-04,  6.1768e-05],
        [-1.7881e-06, -1.2666e-06,  9.4995e-07,  ..., -1.4082e-06,
         -9.9838e-07, -1.1697e-06],
        [-3.5763e-06, -2.5630e-06,  1.9521e-06,  ..., -2.8014e-06,
         -1.9521e-06, -2.2948e-06],
        [-4.2915e-06, -2.9951e-06,  2.3693e-06,  ..., -3.3677e-06,
         -2.3097e-06, -2.9206e-06],
        [-4.7982e-06, -3.5018e-06,  2.6077e-06,  ..., -3.7998e-06,
         -2.7418e-06, -2.9057e-06]], device='cuda:0')
Loss: 1.0401002168655396


Running epoch 0, step 833, batch 833
Sampled inputs[:2]: tensor([[    0,  6532,  6984,  ...,   271,  8212, 14409],
        [    0,   278,  3358,  ...,    12,   287,  9612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5071e-05,  3.5102e-05, -1.4701e-04,  ...,  9.9250e-05,
         -2.1472e-04,  6.9599e-07],
        [-3.5539e-06, -2.4885e-06,  1.8068e-06,  ..., -2.8536e-06,
         -2.1383e-06, -2.3916e-06],
        [-7.0781e-06, -5.0515e-06,  3.7327e-06,  ..., -5.6326e-06,
         -4.1574e-06, -4.6343e-06],
        [-8.3447e-06, -5.7518e-06,  4.4107e-06,  ..., -6.6608e-06,
         -4.8429e-06, -5.8264e-06],
        [-9.5069e-06, -6.8992e-06,  4.9919e-06,  ..., -7.5996e-06,
         -5.7817e-06, -5.8562e-06]], device='cuda:0')
Loss: 1.0441356897354126


Running epoch 0, step 834, batch 834
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,  1040,   287,  ...,    14, 10209,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4606e-05, -1.1155e-05, -1.9746e-04,  ...,  2.4826e-04,
         -3.2279e-04,  1.3136e-05],
        [-5.3197e-06, -3.7551e-06,  2.7753e-06,  ..., -4.2766e-06,
         -3.1963e-06, -3.5614e-06],
        [-1.0550e-05, -7.6145e-06,  5.6997e-06,  ..., -8.4192e-06,
         -6.2287e-06, -6.8694e-06],
        [-1.2487e-05, -8.7023e-06,  6.7800e-06,  ..., -1.0014e-05,
         -7.2867e-06, -8.6725e-06],
        [-1.4126e-05, -1.0386e-05,  7.5847e-06,  ..., -1.1355e-05,
         -8.6576e-06, -8.6725e-06]], device='cuda:0')
Loss: 1.0238220691680908


Running epoch 0, step 835, batch 835
Sampled inputs[:2]: tensor([[    0,  6660, 13165,  ...,   380,   333,   199],
        [    0,  2336,    26,  ...,  2564,   271,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5083e-04,  9.3772e-05, -1.0248e-04,  ...,  3.2571e-04,
         -4.5129e-04, -1.1651e-04],
        [-7.1749e-06, -4.9844e-06,  3.7253e-06,  ..., -5.6997e-06,
         -4.2841e-06, -4.7982e-06],
        [-1.4305e-05, -1.0163e-05,  7.6964e-06,  ..., -1.1295e-05,
         -8.4043e-06, -9.3281e-06],
        [-1.6928e-05, -1.1608e-05,  9.1344e-06,  ..., -1.3426e-05,
         -9.8348e-06, -1.1742e-05],
        [-1.9193e-05, -1.3888e-05,  1.0252e-05,  ..., -1.5259e-05,
         -1.1683e-05, -1.1817e-05]], device='cuda:0')
Loss: 1.028387427330017


Running epoch 0, step 836, batch 836
Sampled inputs[:2]: tensor([[    0, 13576,   431,  ...,    14,   475,   298],
        [    0,  4889,  3593,  ..., 19787,   287, 22475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6466e-05, -8.2001e-05, -2.1916e-04,  ...,  2.9537e-04,
         -4.3630e-04, -1.5669e-04],
        [-8.8885e-06, -6.2436e-06,  4.6790e-06,  ..., -7.0855e-06,
         -5.2527e-06, -5.9009e-06],
        [-1.7717e-05, -1.2696e-05,  9.6634e-06,  ..., -1.4022e-05,
         -1.0289e-05, -1.1474e-05],
        [-2.1040e-05, -1.4573e-05,  1.1519e-05,  ..., -1.6734e-05,
         -1.2070e-05, -1.4484e-05],
        [-2.3752e-05, -1.7360e-05,  1.2860e-05,  ..., -1.8939e-05,
         -1.4320e-05, -1.4514e-05]], device='cuda:0')
Loss: 1.0482871532440186


Running epoch 0, step 837, batch 837
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0, 14652,    12,  ..., 17330,   996,  3294]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2725e-05, -1.8467e-04, -2.3851e-04,  ...,  3.1606e-04,
         -3.4619e-04, -7.2839e-05],
        [-1.0528e-05, -7.4282e-06,  5.6326e-06,  ..., -8.4341e-06,
         -6.2734e-06, -7.0632e-06],
        [-2.1011e-05, -1.5110e-05,  1.1630e-05,  ..., -1.6659e-05,
         -1.2256e-05, -1.3679e-05],
        [-2.4974e-05, -1.7345e-05,  1.3947e-05,  ..., -1.9953e-05,
         -1.4424e-05, -1.7405e-05],
        [-2.8223e-05, -2.0728e-05,  1.5482e-05,  ..., -2.2545e-05,
         -1.7136e-05, -1.7256e-05]], device='cuda:0')
Loss: 1.0117335319519043


Running epoch 0, step 838, batch 838
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,   609,   271,  ...,  4684, 14107,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9642e-05, -3.0720e-04, -2.6268e-04,  ...,  2.5423e-04,
         -2.7549e-04, -7.2969e-05],
        [-1.2301e-05, -8.7172e-06,  6.5863e-06,  ..., -9.8795e-06,
         -7.3686e-06, -8.2031e-06],
        [-2.4602e-05, -1.7777e-05,  1.3612e-05,  ..., -1.9565e-05,
         -1.4417e-05, -1.5929e-05],
        [-2.9176e-05, -2.0355e-05,  1.6272e-05,  ..., -2.3350e-05,
         -1.6958e-05, -2.0206e-05],
        [-3.2932e-05, -2.4274e-05,  1.8060e-05,  ..., -2.6390e-05,
         -2.0072e-05, -2.0057e-05]], device='cuda:0')
Loss: 1.0485173463821411


Running epoch 0, step 839, batch 839
Sampled inputs[:2]: tensor([[    0,    13,  4467,  ...,  2390, 47857,   287],
        [    0,  3087,   401,  ...,  1875,  4122,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3402e-05, -1.0927e-04, -1.6951e-04,  ...,  2.7837e-04,
         -2.8794e-04, -1.3720e-04],
        [-1.4111e-05, -9.9689e-06,  7.4729e-06,  ..., -1.1310e-05,
         -8.5607e-06, -9.4548e-06],
        [-2.8238e-05, -2.0310e-05,  1.5482e-05,  ..., -2.2382e-05,
         -1.6727e-05, -1.8358e-05],
        [-3.3587e-05, -2.3335e-05,  1.8522e-05,  ..., -2.6777e-05,
         -1.9744e-05, -2.3335e-05],
        [-3.7819e-05, -2.7731e-05,  2.0579e-05,  ..., -3.0190e-05,
         -2.3246e-05, -2.3142e-05]], device='cuda:0')
Loss: 1.0535222291946411
Graident accumulation at epoch 0, step 839, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0143,  0.0023,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0153, -0.0281,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.9908e-05,  5.0276e-06, -5.3947e-05,  ...,  1.2563e-04,
          3.0470e-05,  1.2121e-04],
        [-1.4940e-05, -1.0198e-05,  7.5200e-06,  ..., -1.2210e-05,
         -7.8357e-06, -9.1038e-06],
        [ 3.7648e-06,  4.1439e-06, -2.4534e-06,  ...,  4.0490e-06,
          7.9042e-06, -4.1163e-06],
        [-1.5878e-05, -9.2770e-06,  7.5136e-06,  ..., -8.3307e-06,
         -7.2516e-06, -1.3531e-05],
        [-3.7711e-05, -2.6378e-05,  2.0217e-05,  ..., -3.0029e-05,
         -1.9062e-05, -2.0459e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2873e-08, 4.7801e-08, 3.6947e-08,  ..., 3.6554e-08, 1.1155e-07,
         4.1680e-08],
        [5.4852e-11, 3.0531e-11, 4.5990e-12,  ..., 3.8583e-11, 4.0778e-12,
         1.1890e-11],
        [2.2730e-09, 1.0919e-09, 3.4881e-10,  ..., 1.6634e-09, 2.5118e-10,
         6.5398e-10],
        [6.9715e-10, 5.1677e-10, 7.5682e-11,  ..., 5.7496e-10, 9.6078e-11,
         2.3677e-10],
        [2.7764e-10, 1.5203e-10, 3.0694e-11,  ..., 2.0206e-10, 3.6085e-11,
         6.8516e-11]], device='cuda:0')
optimizer state dict: 105.0
lr: [1.3616010075987416e-05, 1.3616010075987416e-05]
scheduler_last_epoch: 105


Running epoch 0, step 840, batch 840
Sampled inputs[:2]: tensor([[    0, 25778,  3804,  ...,  2354,    12,   554],
        [    0,   271, 21394,  ...,  1487,   287,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5339e-05, -6.4303e-05,  1.0856e-04,  ...,  6.0487e-05,
          3.5958e-05,  3.7176e-05],
        [-1.7285e-06, -1.2740e-06,  8.8662e-07,  ..., -1.4007e-06,
         -1.1176e-06, -1.1474e-06],
        [-3.5018e-06, -2.6524e-06,  1.8552e-06,  ..., -2.8014e-06,
         -2.2203e-06, -2.2650e-06],
        [-4.2021e-06, -3.0696e-06,  2.2203e-06,  ..., -3.3975e-06,
         -2.6524e-06, -2.9057e-06],
        [-4.6492e-06, -3.5763e-06,  2.4438e-06,  ..., -3.7551e-06,
         -3.0547e-06, -2.8312e-06]], device='cuda:0')
Loss: 1.0551079511642456


Running epoch 0, step 841, batch 841
Sampled inputs[:2]: tensor([[    0, 43788,    12,  ...,    12,  6288,   391],
        [    0,  2440,  1458,  ...,  7650,   328,  2297]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6475e-06, -1.1186e-04,  9.8544e-05,  ...,  9.3087e-05,
          2.1678e-04,  1.6169e-04],
        [-3.3602e-06, -2.3767e-06,  1.7956e-06,  ..., -2.7567e-06,
         -2.1383e-06, -2.3320e-06],
        [-6.7651e-06, -4.9323e-06,  3.7625e-06,  ..., -5.4836e-06,
         -4.2170e-06, -4.5151e-06],
        [-8.2254e-06, -5.7518e-06,  4.6045e-06,  ..., -6.7353e-06,
         -5.1111e-06, -5.9307e-06],
        [-9.0897e-06, -6.7651e-06,  5.0217e-06,  ..., -7.4357e-06,
         -5.8860e-06, -5.6922e-06]], device='cuda:0')
Loss: 1.0375449657440186


Running epoch 0, step 842, batch 842
Sampled inputs[:2]: tensor([[    0,  3352,   259,  ...,  3565,    12,   409],
        [    0,   352,   721,  ...,   634, 17642,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6310e-05, -3.3183e-04, -1.4594e-04,  ...,  1.0076e-04,
          1.9903e-04,  2.0099e-04],
        [-4.9621e-06, -3.5912e-06,  2.7418e-06,  ..., -4.0978e-06,
         -3.0398e-06, -3.4198e-06],
        [-1.0073e-05, -7.4804e-06,  5.7742e-06,  ..., -8.2403e-06,
         -6.0648e-06, -6.7204e-06],
        [-1.2159e-05, -8.7023e-06,  7.0482e-06,  ..., -1.0043e-05,
         -7.2867e-06, -8.7321e-06],
        [-1.3292e-05, -1.0088e-05,  7.5698e-06,  ..., -1.0982e-05,
         -8.3745e-06, -8.3148e-06]], device='cuda:0')
Loss: 1.0317862033843994


Running epoch 0, step 843, batch 843
Sampled inputs[:2]: tensor([[   0, 1920,   19,  ..., 5232,  796, 1303],
        [   0,  266,  824,  ..., 1799,  287, 6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6515e-05, -5.6032e-04, -3.3404e-04,  ...,  9.3974e-05,
          2.3854e-04,  1.9423e-04],
        [-6.5491e-06, -4.7758e-06,  3.6396e-06,  ..., -5.4464e-06,
         -4.0457e-06, -4.5076e-06],
        [-1.3337e-05, -9.9391e-06,  7.6815e-06,  ..., -1.0967e-05,
         -8.0466e-06, -8.8960e-06],
        [-1.6034e-05, -1.1533e-05,  9.3281e-06,  ..., -1.3292e-05,
         -9.6262e-06, -1.1474e-05],
        [-1.7583e-05, -1.3381e-05,  1.0043e-05,  ..., -1.4573e-05,
         -1.1101e-05, -1.0967e-05]], device='cuda:0')
Loss: 1.0562959909439087


Running epoch 0, step 844, batch 844
Sampled inputs[:2]: tensor([[    0,   508,  2322,  ...,   968,   266, 15123],
        [    0,  4191,   368,  ...,   367,  4182,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5011e-05, -4.0479e-04, -2.1967e-04,  ..., -4.2907e-06,
          2.3654e-04,  1.9142e-04],
        [-8.2850e-06, -5.9754e-06,  4.5300e-06,  ..., -6.7949e-06,
         -5.0440e-06, -5.7295e-06],
        [-1.6868e-05, -1.2428e-05,  9.5665e-06,  ..., -1.3679e-05,
         -1.0028e-05, -1.1310e-05],
        [-2.0236e-05, -1.4365e-05,  1.1578e-05,  ..., -1.6510e-05,
         -1.1951e-05, -1.4529e-05],
        [-2.2203e-05, -1.6704e-05,  1.2487e-05,  ..., -1.8150e-05,
         -1.3813e-05, -1.3918e-05]], device='cuda:0')
Loss: 1.0139517784118652


Running epoch 0, step 845, batch 845
Sampled inputs[:2]: tensor([[    0,  2280,   344,  ...,   287,   266,  3344],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2444e-04, -4.4753e-04, -2.2472e-04,  ...,  9.9512e-05,
          1.6159e-04,  1.9925e-04],
        [-9.8348e-06, -7.0557e-06,  5.4017e-06,  ..., -8.1062e-06,
         -6.0797e-06, -6.9290e-06],
        [-1.9923e-05, -1.4633e-05,  1.1355e-05,  ..., -1.6212e-05,
         -1.1995e-05, -1.3530e-05],
        [-2.4006e-05, -1.6943e-05,  1.3843e-05,  ..., -1.9684e-05,
         -1.4424e-05, -1.7568e-05],
        [-2.6226e-05, -1.9699e-05,  1.4827e-05,  ..., -2.1502e-05,
         -1.6525e-05, -1.6615e-05]], device='cuda:0')
Loss: 1.0116411447525024


Running epoch 0, step 846, batch 846
Sampled inputs[:2]: tensor([[    0,  3058,   292,  ...,  1387,  1236,   369],
        [    0,  4710,    12,  ...,  3969,     9, 11692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4483e-05, -3.6880e-04, -1.8298e-04,  ...,  5.4589e-05,
          1.8760e-04,  2.8534e-05],
        [-1.1608e-05, -8.2329e-06,  6.2622e-06,  ..., -9.5144e-06,
         -7.2122e-06, -8.2478e-06],
        [-2.3559e-05, -1.7121e-05,  1.3232e-05,  ..., -1.9059e-05,
         -1.4260e-05, -1.6138e-05],
        [-2.8118e-05, -1.9640e-05,  1.5929e-05,  ..., -2.2933e-05,
         -1.6987e-05, -2.0683e-05],
        [-3.1054e-05, -2.3067e-05,  1.7315e-05,  ..., -2.5287e-05,
         -1.9625e-05, -1.9878e-05]], device='cuda:0')
Loss: 1.016587495803833


Running epoch 0, step 847, batch 847
Sampled inputs[:2]: tensor([[    0,  7011,   650,  ..., 28839, 11610,  3222],
        [    0, 16763,  1538,  ...,   631,  3299,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0607e-04, -4.8905e-04, -3.3448e-04,  ..., -3.2650e-05,
          2.2328e-04,  3.4970e-04],
        [-1.3225e-05, -9.3654e-06,  7.1712e-06,  ..., -1.0848e-05,
         -8.2403e-06, -9.4399e-06],
        [-2.6807e-05, -1.9461e-05,  1.5110e-05,  ..., -2.1696e-05,
         -1.6272e-05, -1.8433e-05],
        [-3.2142e-05, -2.2396e-05,  1.8314e-05,  ..., -2.6226e-05,
         -1.9476e-05, -2.3782e-05],
        [-3.5375e-05, -2.6271e-05,  1.9789e-05,  ..., -2.8819e-05,
         -2.2411e-05, -2.2709e-05]], device='cuda:0')
Loss: 1.042070984840393
Graident accumulation at epoch 0, step 847, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3525e-05, -4.4380e-05, -8.2001e-05,  ...,  1.0980e-04,
          4.9751e-05,  1.4406e-04],
        [-1.4768e-05, -1.0115e-05,  7.4851e-06,  ..., -1.2074e-05,
         -7.8762e-06, -9.1374e-06],
        [ 7.0759e-07,  1.7834e-06, -6.9708e-07,  ...,  1.4745e-06,
          5.4866e-06, -5.5479e-06],
        [-1.7504e-05, -1.0589e-05,  8.5936e-06,  ..., -1.0120e-05,
         -8.4740e-06, -1.4556e-05],
        [-3.7477e-05, -2.6368e-05,  2.0174e-05,  ..., -2.9908e-05,
         -1.9397e-05, -2.0684e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2812e-08, 4.7992e-08, 3.7022e-08,  ..., 3.6518e-08, 1.1149e-07,
         4.1760e-08],
        [5.4972e-11, 3.0588e-11, 4.6459e-12,  ..., 3.8662e-11, 4.1416e-12,
         1.1968e-11],
        [2.2714e-09, 1.0912e-09, 3.4869e-10,  ..., 1.6622e-09, 2.5119e-10,
         6.5366e-10],
        [6.9748e-10, 5.1676e-10, 7.5942e-11,  ..., 5.7507e-10, 9.6362e-11,
         2.3710e-10],
        [2.7862e-10, 1.5257e-10, 3.1055e-11,  ..., 2.0269e-10, 3.6551e-11,
         6.8964e-11]], device='cuda:0')
optimizer state dict: 106.0
lr: [1.3500484890183603e-05, 1.3500484890183603e-05]
scheduler_last_epoch: 106


Running epoch 0, step 848, batch 848
Sampled inputs[:2]: tensor([[   0,   16,   52,  ...,   12,  298,  374],
        [   0,  790, 2816,  ...,   14, 1062,  668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9702e-05,  5.6986e-05, -7.8833e-05,  ...,  6.7414e-05,
         -8.3498e-05,  2.6658e-05],
        [-1.5721e-06, -1.1772e-06,  9.0897e-07,  ..., -1.3486e-06,
         -9.2760e-07, -1.0505e-06],
        [-3.2336e-06, -2.4736e-06,  1.9521e-06,  ..., -2.7567e-06,
         -1.8626e-06, -2.1160e-06],
        [-3.9935e-06, -2.9355e-06,  2.3991e-06,  ..., -3.4273e-06,
         -2.2948e-06, -2.8014e-06],
        [-4.1425e-06, -3.2336e-06,  2.4587e-06,  ..., -3.5614e-06,
         -2.4885e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.046097755432129


Running epoch 0, step 849, batch 849
Sampled inputs[:2]: tensor([[   0,  266, 4616,  ..., 1906, 7256,  287],
        [   0, 1415,  300,  ..., 1497, 5715, 4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2648e-05, -1.7912e-04, -2.8302e-04,  ...,  5.5408e-05,
         -1.0314e-05, -2.6251e-05],
        [-3.1069e-06, -2.2799e-06,  1.8142e-06,  ..., -2.6375e-06,
         -1.7732e-06, -2.1309e-06],
        [-6.4075e-06, -4.7982e-06,  3.9041e-06,  ..., -5.3793e-06,
         -3.5763e-06, -4.2766e-06],
        [-7.9572e-06, -5.7369e-06,  4.8578e-06,  ..., -6.7502e-06,
         -4.4256e-06, -5.7071e-06],
        [-8.1658e-06, -6.2436e-06,  4.9025e-06,  ..., -6.9141e-06,
         -4.7684e-06, -5.0664e-06]], device='cuda:0')
Loss: 1.0193045139312744


Running epoch 0, step 850, batch 850
Sampled inputs[:2]: tensor([[    0,   857,   352,  ...,  3608,   271,   995],
        [    0,  1171,   341,  ...,   278, 14713,    18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2648e-05, -2.2620e-04, -2.7252e-04,  ...,  1.8449e-04,
         -1.3057e-04, -2.6251e-05],
        [-4.6790e-06, -3.4496e-06,  2.7269e-06,  ..., -3.9339e-06,
         -2.7232e-06, -3.1739e-06],
        [-9.7305e-06, -7.3165e-06,  5.9009e-06,  ..., -8.0913e-06,
         -5.5432e-06, -6.4373e-06],
        [-1.2100e-05, -8.7619e-06,  7.3761e-06,  ..., -1.0133e-05,
         -6.8545e-06, -8.5682e-06],
        [-1.2457e-05, -9.5665e-06,  7.4506e-06,  ..., -1.0461e-05,
         -7.4208e-06, -7.6741e-06]], device='cuda:0')
Loss: 1.0331023931503296


Running epoch 0, step 851, batch 851
Sampled inputs[:2]: tensor([[    0,  2645,    12,  ...,     5,  1239,  7200],
        [    0, 17471,  7279,  ...,   328,  6179,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3856e-05, -8.5073e-05, -2.0285e-04,  ...,  1.4971e-04,
         -1.8768e-04,  6.7402e-05],
        [-6.4000e-06, -4.7237e-06,  3.6173e-06,  ..., -5.3644e-06,
         -3.8333e-06, -4.2692e-06],
        [-1.3292e-05, -1.0014e-05,  7.8008e-06,  ..., -1.1042e-05,
         -7.8231e-06, -8.6576e-06],
        [-1.6391e-05, -1.1921e-05,  9.6560e-06,  ..., -1.3679e-05,
         -9.5814e-06, -1.1370e-05],
        [-1.7166e-05, -1.3188e-05,  9.9540e-06,  ..., -1.4395e-05,
         -1.0520e-05, -1.0476e-05]], device='cuda:0')
Loss: 1.0598384141921997


Running epoch 0, step 852, batch 852
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0, 19191,   266,  ...,   287,   843,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1856e-05, -1.3976e-04, -2.3264e-04,  ...,  1.5906e-04,
         -1.8633e-04, -2.9236e-05],
        [-8.0019e-06, -5.9158e-06,  4.5672e-06,  ..., -6.6832e-06,
         -4.7609e-06, -5.3048e-06],
        [-1.6570e-05, -1.2487e-05,  9.7975e-06,  ..., -1.3739e-05,
         -9.7156e-06, -1.0759e-05],
        [-2.0504e-05, -1.4946e-05,  1.2189e-05,  ..., -1.7047e-05,
         -1.1906e-05, -1.4156e-05],
        [-2.1398e-05, -1.6466e-05,  1.2517e-05,  ..., -1.7926e-05,
         -1.3083e-05, -1.3039e-05]], device='cuda:0')
Loss: 1.0495412349700928


Running epoch 0, step 853, batch 853
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,   12, 1631,  ..., 1143,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5296e-05, -2.1339e-04, -1.5637e-04,  ..., -4.6140e-06,
          3.3755e-04,  2.1623e-04],
        [-9.5218e-06, -6.8918e-06,  5.4017e-06,  ..., -7.9498e-06,
         -5.7742e-06, -6.6683e-06],
        [-1.9789e-05, -1.4633e-05,  1.1608e-05,  ..., -1.6272e-05,
         -1.1697e-05, -1.3337e-05],
        [-2.4289e-05, -1.7285e-05,  1.4424e-05,  ..., -2.0146e-05,
         -1.4320e-05, -1.7613e-05],
        [-2.5630e-05, -1.9401e-05,  1.4886e-05,  ..., -2.1279e-05,
         -1.5810e-05, -1.6123e-05]], device='cuda:0')
Loss: 0.9699440002441406


Running epoch 0, step 854, batch 854
Sampled inputs[:2]: tensor([[    0,   287,  4599,  ..., 11812,   266,  1036],
        [    0,    17,  3978,  ...,  3988,   598,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1438e-04, -4.5694e-05, -2.5511e-04,  ...,  6.9383e-05,
          2.2101e-04,  1.7321e-04],
        [-1.1101e-05, -8.0317e-06,  6.2883e-06,  ..., -9.3058e-06,
         -6.7018e-06, -7.7337e-06],
        [-2.3067e-05, -1.7047e-05,  1.3530e-05,  ..., -1.9073e-05,
         -1.3568e-05, -1.5497e-05],
        [-2.8372e-05, -2.0161e-05,  1.6809e-05,  ..., -2.3663e-05,
         -1.6645e-05, -2.0489e-05],
        [-2.9832e-05, -2.2575e-05,  1.7330e-05,  ..., -2.4900e-05,
         -1.8299e-05, -1.8716e-05]], device='cuda:0')
Loss: 1.039880633354187


Running epoch 0, step 855, batch 855
Sampled inputs[:2]: tensor([[    0,  1985,   278,  ...,   677, 12292, 17956],
        [    0,  2548,   720,  ...,  1795,  1109, 32948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9628e-06, -2.0862e-04, -2.7158e-04,  ..., -2.5139e-06,
          2.7175e-04,  6.4748e-05],
        [-1.2577e-05, -9.1046e-06,  7.2047e-06,  ..., -1.0565e-05,
         -7.6704e-06, -8.8513e-06],
        [-2.6181e-05, -1.9372e-05,  1.5512e-05,  ..., -2.1666e-05,
         -1.5534e-05, -1.7732e-05],
        [-3.2216e-05, -2.2873e-05,  1.9342e-05,  ..., -2.6941e-05,
         -1.9118e-05, -2.3559e-05],
        [-3.3945e-05, -2.5734e-05,  1.9908e-05,  ..., -2.8357e-05,
         -2.1011e-05, -2.1443e-05]], device='cuda:0')
Loss: 1.0237085819244385
Graident accumulation at epoch 0, step 855, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0022,  0.0236, -0.0192],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0337, -0.0097,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.5676e-05, -6.0804e-05, -1.0096e-04,  ...,  9.8572e-05,
          7.1951e-05,  1.3613e-04],
        [-1.4549e-05, -1.0014e-05,  7.4570e-06,  ..., -1.1923e-05,
         -7.8556e-06, -9.1088e-06],
        [-1.9813e-06, -3.3211e-07,  9.2384e-07,  ..., -8.3958e-07,
          3.3845e-06, -6.7664e-06],
        [-1.8975e-05, -1.1817e-05,  9.6684e-06,  ..., -1.1802e-05,
         -9.5384e-06, -1.5456e-05],
        [-3.7124e-05, -2.6304e-05,  2.0148e-05,  ..., -2.9753e-05,
         -1.9558e-05, -2.0760e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2739e-08, 4.7988e-08, 3.7059e-08,  ..., 3.6482e-08, 1.1146e-07,
         4.1723e-08],
        [5.5075e-11, 3.0641e-11, 4.6931e-12,  ..., 3.8735e-11, 4.1963e-12,
         1.2034e-11],
        [2.2698e-09, 1.0905e-09, 3.4859e-10,  ..., 1.6610e-09, 2.5118e-10,
         6.5333e-10],
        [6.9782e-10, 5.1677e-10, 7.6240e-11,  ..., 5.7522e-10, 9.6631e-11,
         2.3742e-10],
        [2.7949e-10, 1.5308e-10, 3.1420e-11,  ..., 2.0329e-10, 3.6956e-11,
         6.9354e-11]], device='cuda:0')
optimizer state dict: 107.0
lr: [1.3384424799732402e-05, 1.3384424799732402e-05]
scheduler_last_epoch: 107


Running epoch 0, step 856, batch 856
Sampled inputs[:2]: tensor([[    0,   266, 15794,  ...,  3128,  6479,  2626],
        [    0,  1057,    14,  ...,    14,  4735,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0327e-05,  5.4661e-05,  9.8834e-05,  ..., -8.0259e-05,
          1.3363e-04, -1.3424e-05],
        [-1.6391e-06, -1.1325e-06,  8.5309e-07,  ..., -1.3113e-06,
         -9.8348e-07, -1.1697e-06],
        [-3.6061e-06, -2.5183e-06,  1.9372e-06,  ..., -2.8461e-06,
         -2.1011e-06, -2.5183e-06],
        [-4.2021e-06, -2.8312e-06,  2.2650e-06,  ..., -3.3230e-06,
         -2.4289e-06, -3.1143e-06],
        [-4.5002e-06, -3.2485e-06,  2.3991e-06,  ..., -3.5912e-06,
         -2.7567e-06, -2.9802e-06]], device='cuda:0')
Loss: 1.032155156135559


Running epoch 0, step 857, batch 857
Sampled inputs[:2]: tensor([[    0,    69,   462,  ...,   437,   266,   634],
        [    0, 33119,   391,  ...,   292,  4462,  2721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5083e-05,  1.9760e-04,  1.2067e-04,  ..., -3.4553e-05,
          1.1991e-04,  3.4856e-05],
        [-3.2708e-06, -2.3544e-06,  1.7062e-06,  ..., -2.6375e-06,
         -1.9819e-06, -2.2948e-06],
        [-7.0781e-06, -5.1856e-06,  3.8445e-06,  ..., -5.6326e-06,
         -4.1574e-06, -4.8280e-06],
        [-8.2850e-06, -5.8264e-06,  4.4852e-06,  ..., -6.6161e-06,
         -4.8578e-06, -6.0052e-06],
        [-8.9109e-06, -6.6906e-06,  4.8131e-06,  ..., -7.1526e-06,
         -5.4687e-06, -5.7220e-06]], device='cuda:0')
Loss: 1.010387659072876


Running epoch 0, step 858, batch 858
Sampled inputs[:2]: tensor([[    0, 15152,  1106,  ...,   607,   266,  2529],
        [    0,  1854,   292,  ...,   328,  1360,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5480e-05,  2.0801e-04,  4.9739e-05,  ..., -8.2608e-05,
          2.3307e-04,  3.0412e-05],
        [-4.7758e-06, -3.4422e-06,  2.5146e-06,  ..., -3.8967e-06,
         -2.9020e-06, -3.4869e-06],
        [-1.0312e-05, -7.5400e-06,  5.6997e-06,  ..., -8.2552e-06,
         -6.0499e-06, -7.2271e-06],
        [-1.2130e-05, -8.4937e-06,  6.6757e-06,  ..., -9.7454e-06,
         -7.0781e-06, -9.0599e-06],
        [-1.3053e-05, -9.7752e-06,  7.1824e-06,  ..., -1.0520e-05,
         -7.9721e-06, -8.5384e-06]], device='cuda:0')
Loss: 1.0316053628921509


Running epoch 0, step 859, batch 859
Sampled inputs[:2]: tensor([[    0,   266,   452,  ...,  1725,  2200,   342],
        [    0, 29883,   680,  ...,  3363,  1049,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9231e-06,  1.8724e-04, -7.9520e-05,  ..., -1.2643e-04,
          1.7462e-04, -1.5566e-04],
        [-6.2361e-06, -4.5598e-06,  3.3602e-06,  ..., -5.1856e-06,
         -3.7588e-06, -4.4480e-06],
        [-1.3441e-05, -9.9689e-06,  7.5921e-06,  ..., -1.0982e-05,
         -7.8380e-06, -9.2089e-06],
        [-1.5974e-05, -1.1399e-05,  9.0152e-06,  ..., -1.3143e-05,
         -9.2834e-06, -1.1697e-05],
        [-1.7047e-05, -1.2964e-05,  9.5963e-06,  ..., -1.4022e-05,
         -1.0371e-05, -1.0893e-05]], device='cuda:0')
Loss: 1.0333741903305054


Running epoch 0, step 860, batch 860
Sampled inputs[:2]: tensor([[    0,   287,   271,  ...,  1039,  4186,    13],
        [    0,    14, 49601,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4099e-04,  3.0879e-04,  2.8027e-05,  ..., -7.3659e-05,
          3.7221e-04, -5.6063e-05],
        [-7.8529e-06, -5.8189e-06,  4.2580e-06,  ..., -6.5193e-06,
         -4.7125e-06, -5.4687e-06],
        [-1.6913e-05, -1.2726e-05,  9.5889e-06,  ..., -1.3843e-05,
         -9.8497e-06, -1.1384e-05],
        [-2.0087e-05, -1.4558e-05,  1.1384e-05,  ..., -1.6525e-05,
         -1.1653e-05, -1.4409e-05],
        [-2.1487e-05, -1.6540e-05,  1.2115e-05,  ..., -1.7703e-05,
         -1.3039e-05, -1.3515e-05]], device='cuda:0')
Loss: 1.046547532081604


Running epoch 0, step 861, batch 861
Sampled inputs[:2]: tensor([[   0, 8822, 1486,  ...,   12,  287, 6903],
        [   0,  271, 3403,  ..., 6168,  300, 2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8865e-04,  2.5688e-04,  4.8482e-05,  ...,  1.2546e-05,
          3.2816e-04, -1.3349e-05],
        [-9.5367e-06, -7.0408e-06,  5.1856e-06,  ..., -7.8678e-06,
         -5.6960e-06, -6.5938e-06],
        [-2.0593e-05, -1.5453e-05,  1.1675e-05,  ..., -1.6794e-05,
         -1.1995e-05, -1.3828e-05],
        [-2.4199e-05, -1.7539e-05,  1.3754e-05,  ..., -1.9833e-05,
         -1.4037e-05, -1.7241e-05],
        [-2.6166e-05, -2.0102e-05,  1.4767e-05,  ..., -2.1517e-05,
         -1.5929e-05, -1.6451e-05]], device='cuda:0')
Loss: 1.025204062461853


Running epoch 0, step 862, batch 862
Sampled inputs[:2]: tensor([[    0,   278,   490,  ...,   434,   472,   346],
        [    0,  7120,   344,  ...,  6273,    52, 22639]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5652e-04,  3.4276e-04,  1.3485e-05,  ...,  9.3214e-05,
          2.9224e-04, -1.1026e-04],
        [-1.1005e-05, -8.1062e-06,  6.0163e-06,  ..., -9.1791e-06,
         -6.6198e-06, -7.6145e-06],
        [-2.3693e-05, -1.7762e-05,  1.3530e-05,  ..., -1.9535e-05,
         -1.3895e-05, -1.5870e-05],
        [-2.7984e-05, -2.0251e-05,  1.6019e-05,  ..., -2.3216e-05,
         -1.6391e-05, -1.9938e-05],
        [-3.0279e-05, -2.3201e-05,  1.7211e-05,  ..., -2.5123e-05,
         -1.8522e-05, -1.8969e-05]], device='cuda:0')
Loss: 1.0434601306915283


Running epoch 0, step 863, batch 863
Sampled inputs[:2]: tensor([[    0, 20596,  2943,  ...,  5560,  2512,   266],
        [    0,  1371,   287,  ...,   689,   278, 12774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7786e-04,  6.2407e-04, -2.1857e-06,  ...,  1.0719e-04,
          6.6783e-05, -2.7852e-04],
        [-1.2614e-05, -9.2685e-06,  6.9030e-06,  ..., -1.0476e-05,
         -7.6108e-06, -8.6874e-06],
        [-2.7120e-05, -2.0295e-05,  1.5482e-05,  ..., -2.2262e-05,
         -1.5937e-05, -1.8105e-05],
        [-3.2157e-05, -2.3186e-05,  1.8403e-05,  ..., -2.6539e-05,
         -1.8880e-05, -2.2843e-05],
        [-3.4750e-05, -2.6569e-05,  1.9744e-05,  ..., -2.8685e-05,
         -2.1294e-05, -2.1696e-05]], device='cuda:0')
Loss: 1.0208433866500854
Graident accumulation at epoch 0, step 863, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1322e-05,  7.6832e-06, -9.1082e-05,  ...,  9.9433e-05,
          7.1434e-05,  9.4664e-05],
        [-1.4356e-05, -9.9391e-06,  7.4016e-06,  ..., -1.1778e-05,
         -7.8311e-06, -9.0666e-06],
        [-4.4952e-06, -2.3284e-06,  2.3797e-06,  ..., -2.9819e-06,
          1.4523e-06, -7.9002e-06],
        [-2.0294e-05, -1.2954e-05,  1.0542e-05,  ..., -1.3276e-05,
         -1.0473e-05, -1.6195e-05],
        [-3.6887e-05, -2.6331e-05,  2.0107e-05,  ..., -2.9646e-05,
         -1.9732e-05, -2.0854e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2809e-08, 4.8329e-08, 3.7021e-08,  ..., 3.6457e-08, 1.1135e-07,
         4.1759e-08],
        [5.5179e-11, 3.0696e-11, 4.7361e-12,  ..., 3.8806e-11, 4.2500e-12,
         1.2097e-11],
        [2.2683e-09, 1.0898e-09, 3.4848e-10,  ..., 1.6598e-09, 2.5118e-10,
         6.5300e-10],
        [6.9816e-10, 5.1679e-10, 7.6503e-11,  ..., 5.7535e-10, 9.6891e-11,
         2.3770e-10],
        [2.8042e-10, 1.5363e-10, 3.1779e-11,  ..., 2.0391e-10, 3.7372e-11,
         6.9756e-11]], device='cuda:0')
optimizer state dict: 108.0
lr: [1.3267847539628745e-05, 1.3267847539628745e-05]
scheduler_last_epoch: 108


Running epoch 0, step 864, batch 864
Sampled inputs[:2]: tensor([[    0, 11541,  4784,  ...,  2837, 38541,    12],
        [    0,   380,  1075,  ..., 16948,   266,  1751]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0347e-05,  3.0559e-05, -1.5898e-05,  ...,  3.0830e-05,
         -7.0270e-05, -5.1292e-05],
        [-1.4901e-06, -1.1995e-06,  8.4937e-07,  ..., -1.3486e-06,
         -9.6112e-07, -9.7603e-07],
        [-3.1888e-06, -2.6077e-06,  1.8924e-06,  ..., -2.8610e-06,
         -1.9819e-06, -2.0564e-06],
        [-3.7402e-06, -2.9802e-06,  2.2054e-06,  ..., -3.3826e-06,
         -2.3544e-06, -2.5630e-06],
        [-4.1425e-06, -3.4422e-06,  2.4438e-06,  ..., -3.7253e-06,
         -2.6822e-06, -2.5034e-06]], device='cuda:0')
Loss: 1.0541863441467285


Running epoch 0, step 865, batch 865
Sampled inputs[:2]: tensor([[    0,   278,  4575,  ...,  1220,   278,  4575],
        [    0, 10288,   300,  ...,  5365,    12,  3539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5605e-05,  2.5597e-04,  3.9369e-05,  ...,  2.6286e-04,
         -2.5337e-04, -2.1948e-04],
        [-2.9281e-06, -2.3320e-06,  1.7025e-06,  ..., -2.5928e-06,
         -1.7919e-06, -2.0117e-06],
        [-6.3181e-06, -5.0813e-06,  3.8147e-06,  ..., -5.5283e-06,
         -3.7178e-06, -4.2170e-06],
        [-7.5251e-06, -5.8860e-06,  4.5896e-06,  ..., -6.6608e-06,
         -4.4852e-06, -5.4240e-06],
        [-8.0764e-06, -6.6161e-06,  4.8429e-06,  ..., -7.0930e-06,
         -4.9621e-06, -5.0515e-06]], device='cuda:0')
Loss: 1.007072925567627


Running epoch 0, step 866, batch 866
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0,   380,   341,  ...,   955,   644,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2974e-04,  2.4120e-04,  1.0318e-04,  ...,  4.2383e-04,
         -2.5787e-04, -3.7653e-04],
        [-4.4629e-06, -3.5539e-06,  2.6375e-06,  ..., -3.9190e-06,
         -2.7902e-06, -3.0845e-06],
        [ 6.4254e-05,  5.1443e-05, -1.8771e-05,  ...,  7.3790e-05,
          3.2703e-05,  4.2651e-05],
        [-1.1429e-05, -8.9258e-06,  7.0184e-06,  ..., -1.0028e-05,
         -6.9737e-06, -8.2701e-06],
        [-1.2398e-05, -1.0148e-05,  7.5102e-06,  ..., -1.0848e-05,
         -7.8082e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.0369373559951782


Running epoch 0, step 867, batch 867
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 19245,   565,    14],
        [    0, 16028,   669,  ...,   292,  6502,  7050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8224e-05,  1.7787e-04,  2.5908e-04,  ...,  4.9789e-04,
         -2.4049e-04, -3.5124e-04],
        [-6.1169e-06, -4.6492e-06,  3.4943e-06,  ..., -5.2527e-06,
         -3.8706e-06, -4.3884e-06],
        [ 6.0633e-05,  4.8939e-05, -1.6804e-05,  ...,  7.0899e-05,
          3.0408e-05,  3.9909e-05],
        [-1.5512e-05, -1.1608e-05,  9.2536e-06,  ..., -1.3307e-05,
         -9.5814e-06, -1.1533e-05],
        [-1.7047e-05, -1.3426e-05,  1.0043e-05,  ..., -1.4588e-05,
         -1.0848e-05, -1.1176e-05]], device='cuda:0')
Loss: 0.9948183298110962


Running epoch 0, step 868, batch 868
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  365, 2463,  391],
        [   0,  894,   16,  ...,  892,  300,  722]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2882e-04,  2.6080e-04,  2.5908e-04,  ...,  5.4691e-04,
         -1.3833e-04, -2.8953e-04],
        [-7.6964e-06, -5.8338e-06,  4.3251e-06,  ..., -6.5640e-06,
         -4.9062e-06, -5.5656e-06],
        [ 5.7220e-05,  4.6347e-05, -1.4897e-05,  ...,  6.8112e-05,
          2.8247e-05,  3.7465e-05],
        [-1.9416e-05, -1.4484e-05,  1.1429e-05,  ..., -1.6525e-05,
         -1.2070e-05, -1.4499e-05],
        [-2.1458e-05, -1.6823e-05,  1.2517e-05,  ..., -1.8209e-05,
         -1.3739e-05, -1.4111e-05]], device='cuda:0')
Loss: 1.008054256439209


Running epoch 0, step 869, batch 869
Sampled inputs[:2]: tensor([[    0,    14,  5551,  ...,   668, 11988,  2538],
        [    0,   266, 27347,  ...,   368,  3367,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7578e-04,  2.7811e-04,  2.0657e-04,  ...,  4.9790e-04,
          5.7033e-06, -3.3504e-04],
        [ 3.1657e-05,  7.6064e-05, -2.2105e-05,  ...,  6.4008e-05,
          8.1944e-05,  3.9733e-05],
        [ 5.4121e-05,  4.3992e-05, -1.3027e-05,  ...,  6.5430e-05,
          2.6377e-05,  3.5364e-05],
        [-2.3112e-05, -1.7211e-05,  1.3679e-05,  ..., -1.9804e-05,
         -1.4350e-05, -1.7256e-05],
        [-2.5421e-05, -1.9923e-05,  1.4916e-05,  ..., -2.1666e-05,
         -1.6242e-05, -1.6615e-05]], device='cuda:0')
Loss: 1.0191792249679565


Running epoch 0, step 870, batch 870
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,    12,   850,  4952],
        [    0,    83,   292,  ...,   445,    11, 16109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8311e-04,  2.3289e-04,  8.3612e-05,  ...,  5.2928e-04,
         -6.2342e-05, -4.3427e-04],
        [ 3.0114e-05,  7.4879e-05, -2.1211e-05,  ...,  6.2712e-05,
          8.1054e-05,  3.8705e-05],
        [ 5.0828e-05,  4.1444e-05, -1.1045e-05,  ...,  6.2674e-05,
          2.4507e-05,  3.3218e-05],
        [-2.6926e-05, -2.0102e-05,  1.5974e-05,  ..., -2.3007e-05,
         -1.6510e-05, -1.9893e-05],
        [-2.9594e-05, -2.3231e-05,  1.7449e-05,  ..., -2.5183e-05,
         -1.8716e-05, -1.9163e-05]], device='cuda:0')
Loss: 1.0110093355178833


Running epoch 0, step 871, batch 871
Sampled inputs[:2]: tensor([[    0,   874,   445,  ...,    14, 16205,  8510],
        [    0,  1619,   938,  ...,   292, 10026, 14367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2138e-04,  3.7447e-04,  1.9140e-05,  ...,  5.5962e-04,
         -2.1256e-04, -4.0635e-04],
        [ 2.8527e-05,  7.3725e-05, -2.0336e-05,  ...,  6.1348e-05,
          7.9996e-05,  3.7609e-05],
        [ 4.7371e-05,  3.8896e-05, -9.0483e-06,  ...,  5.9753e-05,
          2.2272e-05,  3.0894e-05],
        [-3.0860e-05, -2.2918e-05,  1.8254e-05,  ..., -2.6375e-05,
         -1.9088e-05, -2.2724e-05],
        [-3.3975e-05, -2.6524e-05,  1.9982e-05,  ..., -2.8923e-05,
         -2.1666e-05, -2.1935e-05]], device='cuda:0')
Loss: 1.0180522203445435
Graident accumulation at epoch 0, step 871, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0026, -0.0343],
        [ 0.0337, -0.0096,  0.0404,  ...,  0.0224,  0.0063, -0.0020],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0144, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2948e-05,  4.4361e-05, -8.0060e-05,  ...,  1.4545e-04,
          4.3035e-05,  4.4562e-05],
        [-1.0067e-05, -1.5728e-06,  4.6279e-06,  ..., -4.4655e-06,
          9.5156e-07, -4.3990e-06],
        [ 6.9141e-07,  1.7940e-06,  1.2369e-06,  ...,  3.2916e-06,
          3.5343e-06, -4.0208e-06],
        [-2.1350e-05, -1.3951e-05,  1.1313e-05,  ..., -1.4586e-05,
         -1.1334e-05, -1.6848e-05],
        [-3.6595e-05, -2.6350e-05,  2.0095e-05,  ..., -2.9574e-05,
         -1.9925e-05, -2.0962e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2839e-08, 4.8421e-08, 3.6985e-08,  ..., 3.6733e-08, 1.1128e-07,
         4.1882e-08],
        [5.5938e-11, 3.6101e-11, 5.1449e-12,  ..., 4.2531e-11, 1.0645e-11,
         1.3500e-11],
        [2.2683e-09, 1.0902e-09, 3.4821e-10,  ..., 1.6617e-09, 2.5143e-10,
         6.5330e-10],
        [6.9841e-10, 5.1679e-10, 7.6759e-11,  ..., 5.7547e-10, 9.7158e-11,
         2.3798e-10],
        [2.8129e-10, 1.5418e-10, 3.2146e-11,  ..., 2.0454e-10, 3.7804e-11,
         7.0167e-11]], device='cuda:0')
optimizer state dict: 109.0
lr: [1.3150770923895586e-05, 1.3150770923895586e-05]
scheduler_last_epoch: 109


Running epoch 0, step 872, batch 872
Sampled inputs[:2]: tensor([[    0,  9792,  3239,  ...,   699,  3636,  1761],
        [    0,     5,  4413,  ...,  9205, 16744,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4779e-05,  2.0554e-05,  2.2518e-05,  ..., -6.1460e-05,
          7.6257e-05,  1.3438e-04],
        [-1.5125e-06, -1.1325e-06,  8.4564e-07,  ..., -1.3188e-06,
         -1.0431e-06, -1.1548e-06],
        [-3.3528e-06, -2.5332e-06,  1.9819e-06,  ..., -2.8312e-06,
         -2.1905e-06, -2.4289e-06],
        [-3.8147e-06, -2.7865e-06,  2.2352e-06,  ..., -3.2634e-06,
         -2.5332e-06, -2.9802e-06],
        [-4.2915e-06, -3.3230e-06,  2.5481e-06,  ..., -3.6359e-06,
         -2.8759e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.0275238752365112


Running epoch 0, step 873, batch 873
Sampled inputs[:2]: tensor([[    0,   508,  1548,  ...,   494, 10792,     9],
        [    0,  4845,  1521,  ...,   963,   292,  6414]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6560e-04,  3.8488e-05, -6.0344e-05,  ..., -1.1628e-04,
          1.1201e-04,  1.4870e-04],
        [-2.8312e-06, -2.1309e-06,  1.5423e-06,  ..., -2.5406e-06,
         -1.9595e-06, -2.2799e-06],
        [-6.3181e-06, -4.7684e-06,  3.7104e-06,  ..., -5.4091e-06,
         -4.0457e-06, -4.6790e-06],
        [-7.2122e-06, -5.2452e-06,  4.1872e-06,  ..., -6.3032e-06,
         -4.7535e-06, -5.8562e-06],
        [-8.0317e-06, -6.1989e-06,  4.7684e-06,  ..., -6.8694e-06,
         -5.2750e-06, -5.4389e-06]], device='cuda:0')
Loss: 0.9894837141036987


Running epoch 0, step 874, batch 874
Sampled inputs[:2]: tensor([[    0,  2352,  4275,  ..., 10518,   342,   266],
        [    0,   935,   508,  ...,   287, 41582,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5252e-04, -1.2659e-04, -1.5472e-04,  ..., -2.1373e-04,
          2.5501e-04,  2.8044e-04],
        [-4.1723e-06, -3.1590e-06,  2.3991e-06,  ..., -3.7774e-06,
         -2.9579e-06, -3.4049e-06],
        [-9.2238e-06, -7.0482e-06,  5.6326e-06,  ..., -7.9721e-06,
         -6.0424e-06, -6.8396e-06],
        [-1.0580e-05, -7.7039e-06,  6.4969e-06,  ..., -9.3877e-06,
         -7.1973e-06, -8.7768e-06],
        [-1.1787e-05, -9.2387e-06,  7.2271e-06,  ..., -1.0192e-05,
         -7.9423e-06, -7.9572e-06]], device='cuda:0')
Loss: 0.9927794933319092


Running epoch 0, step 875, batch 875
Sampled inputs[:2]: tensor([[   0,  280, 5656,  ..., 7369, 2276,   12],
        [   0, 6010,  829,  ...,  668, 1784,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1132e-04,  9.1789e-05, -3.0774e-04,  ..., -1.6567e-04,
          1.7967e-04,  2.3437e-04],
        [-5.7966e-06, -4.3511e-06,  3.3192e-06,  ..., -5.1036e-06,
         -3.9488e-06, -4.5449e-06],
        [-1.2740e-05, -9.7007e-06,  7.7188e-06,  ..., -1.0848e-05,
         -8.1733e-06, -9.2834e-06],
        [-1.4603e-05, -1.0654e-05,  8.8811e-06,  ..., -1.2681e-05,
         -9.6262e-06, -1.1712e-05],
        [-1.6317e-05, -1.2726e-05,  9.8944e-06,  ..., -1.3918e-05,
         -1.0788e-05, -1.0937e-05]], device='cuda:0')
Loss: 1.0418570041656494


Running epoch 0, step 876, batch 876
Sampled inputs[:2]: tensor([[    0,  1125,   278,  ...,  6447,   609,    14],
        [    0,   680,   993,  ...,   699, 11426,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4648e-04,  8.4608e-05, -2.0564e-04,  ..., -1.6302e-04,
          2.0736e-04,  3.4713e-04],
        [-7.3239e-06, -5.5209e-06,  4.2357e-06,  ..., -6.4075e-06,
         -4.9174e-06, -5.7071e-06],
        [-1.6198e-05, -1.2368e-05,  9.8497e-06,  ..., -1.3739e-05,
         -1.0319e-05, -1.1846e-05],
        [-1.8507e-05, -1.3560e-05,  1.1310e-05,  ..., -1.5974e-05,
         -1.2040e-05, -1.4827e-05],
        [-2.0608e-05, -1.6123e-05,  1.2517e-05,  ..., -1.7568e-05,
         -1.3575e-05, -1.3933e-05]], device='cuda:0')
Loss: 1.018196940422058


Running epoch 0, step 877, batch 877
Sampled inputs[:2]: tensor([[   0,   12,  271,  ...,   12,  298,  273],
        [   0,  259, 2561,  ...,   77, 4830,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0597e-04,  2.4604e-04, -3.1092e-04,  ..., -1.6302e-04,
          4.2252e-04,  2.1765e-04],
        [-8.8885e-06, -6.6459e-06,  5.0664e-06,  ..., -7.7337e-06,
         -5.9307e-06, -6.9588e-06],
        [-1.9580e-05, -1.4842e-05,  1.1742e-05,  ..., -1.6540e-05,
         -1.2405e-05, -1.4424e-05],
        [-2.2247e-05, -1.6183e-05,  1.3396e-05,  ..., -1.9133e-05,
         -1.4395e-05, -1.7896e-05],
        [-2.4900e-05, -1.9372e-05,  1.4931e-05,  ..., -2.1160e-05,
         -1.6332e-05, -1.6958e-05]], device='cuda:0')
Loss: 0.9976823925971985


Running epoch 0, step 878, batch 878
Sampled inputs[:2]: tensor([[    0,   278, 14971,  ...,  2341,   266,   717],
        [    0,   452,    13,  ...,   358,    13, 12347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1105e-04,  2.4604e-04, -3.4889e-04,  ..., -1.6899e-04,
          4.8139e-04,  2.0414e-04],
        [-1.0341e-05, -7.6368e-06,  5.8673e-06,  ..., -8.9929e-06,
         -6.9663e-06, -8.1882e-06],
        [-2.2769e-05, -1.7077e-05,  1.3627e-05,  ..., -1.9208e-05,
         -1.4551e-05, -1.6928e-05],
        [-2.5898e-05, -1.8567e-05,  1.5542e-05,  ..., -2.2233e-05,
         -1.6898e-05, -2.1055e-05],
        [-2.9102e-05, -2.2426e-05,  1.7419e-05,  ..., -2.4706e-05,
         -1.9267e-05, -1.9982e-05]], device='cuda:0')
Loss: 0.9838510751724243


Running epoch 0, step 879, batch 879
Sampled inputs[:2]: tensor([[    0, 21448,   344,  ...,   365,  1501,   271],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8741e-04,  2.5490e-04, -4.8548e-04,  ..., -1.5435e-04,
          3.9234e-04,  1.3105e-04],
        [-1.1861e-05, -8.8066e-06,  6.8210e-06,  ..., -1.0274e-05,
         -7.8864e-06, -9.2238e-06],
        [ 3.6013e-04,  2.3242e-04, -2.4390e-04,  ...,  3.6187e-04,
          3.0919e-04,  2.0630e-04],
        [-2.9773e-05, -2.1517e-05,  1.8060e-05,  ..., -2.5496e-05,
         -1.9193e-05, -2.3827e-05],
        [-3.3274e-05, -2.5794e-05,  2.0102e-05,  ..., -2.8282e-05,
         -2.1920e-05, -2.2665e-05]], device='cuda:0')
Loss: 1.0322468280792236
Graident accumulation at epoch 0, step 879, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0224,  0.0064, -0.0020],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0144, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0394e-05,  6.5415e-05, -1.2060e-04,  ...,  1.1547e-04,
          7.7965e-05,  5.3211e-05],
        [-1.0247e-05, -2.2961e-06,  4.8472e-06,  ..., -5.0464e-06,
          6.7757e-08, -4.8815e-06],
        [ 3.6635e-05,  2.4856e-05, -2.3277e-05,  ...,  3.9150e-05,
          3.4100e-05,  1.7011e-05],
        [-2.2192e-05, -1.4707e-05,  1.1988e-05,  ..., -1.5677e-05,
         -1.2120e-05, -1.7546e-05],
        [-3.6263e-05, -2.6294e-05,  2.0095e-05,  ..., -2.9445e-05,
         -2.0125e-05, -2.1132e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2917e-08, 4.8438e-08, 3.7184e-08,  ..., 3.6720e-08, 1.1132e-07,
         4.1857e-08],
        [5.6022e-11, 3.6142e-11, 5.1863e-12,  ..., 4.2594e-11, 1.0697e-11,
         1.3571e-11],
        [2.3957e-09, 1.1432e-09, 4.0735e-10,  ..., 1.7910e-09, 3.4678e-10,
         6.9521e-10],
        [6.9860e-10, 5.1674e-10, 7.7009e-11,  ..., 5.7555e-10, 9.7429e-11,
         2.3831e-10],
        [2.8212e-10, 1.5470e-10, 3.2518e-11,  ..., 2.0514e-10, 3.8247e-11,
         7.0611e-11]], device='cuda:0')
optimizer state dict: 110.0
lr: [1.3033212842861785e-05, 1.3033212842861785e-05]
scheduler_last_epoch: 110


Running epoch 0, step 880, batch 880
Sampled inputs[:2]: tensor([[    0,  1029,  6068,  ..., 18017,   300,   259],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1785e-05, -5.4230e-05, -2.6212e-05,  ..., -8.0520e-05,
         -3.3332e-05,  9.7666e-05],
        [-1.5274e-06, -1.1176e-06,  8.7544e-07,  ..., -1.3113e-06,
         -9.2760e-07, -1.1399e-06],
        [-3.3379e-06, -2.5183e-06,  2.0117e-06,  ..., -2.8610e-06,
         -1.9819e-06, -2.4140e-06],
        [-3.8147e-06, -2.7567e-06,  2.2948e-06,  ..., -3.2783e-06,
         -2.2948e-06, -2.9504e-06],
        [-4.1127e-06, -3.2037e-06,  2.4885e-06,  ..., -3.5763e-06,
         -2.5630e-06, -2.7865e-06]], device='cuda:0')
Loss: 1.0245143175125122


Running epoch 0, step 881, batch 881
Sampled inputs[:2]: tensor([[    0,   292,   494,  ...,   259, 14134, 11544],
        [    0,   298,   894,  ...,  7605,  3220,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4895e-07, -2.2988e-05,  1.2621e-05,  ..., -1.5293e-04,
          3.1926e-05,  9.4787e-05],
        [-3.1367e-06, -2.2054e-06,  1.7583e-06,  ..., -2.6971e-06,
         -2.0601e-06, -2.4661e-06],
        [-6.8247e-06, -4.9919e-06,  4.0382e-06,  ..., -5.8264e-06,
         -4.3809e-06, -5.2005e-06],
        [-7.6592e-06, -5.3495e-06,  4.5151e-06,  ..., -6.5714e-06,
         -4.9770e-06, -6.1840e-06],
        [-8.6427e-06, -6.5565e-06,  5.1260e-06,  ..., -7.4804e-06,
         -5.8264e-06, -6.1542e-06]], device='cuda:0')
Loss: 1.0148732662200928


Running epoch 0, step 882, batch 882
Sampled inputs[:2]: tensor([[    0,   609,    12,  ...,   409, 11041,   292],
        [    0,  1420,  6319,  ...,   292,  4895,  4050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5585e-05,  1.9560e-05,  3.0082e-05,  ..., -3.3079e-04,
          2.9959e-04,  3.5032e-04],
        [-4.5523e-06, -3.2037e-06,  2.4997e-06,  ..., -3.9786e-06,
         -3.0734e-06, -3.6433e-06],
        [-9.9987e-06, -7.2569e-06,  5.8338e-06,  ..., -8.5980e-06,
         -6.4969e-06, -7.6741e-06],
        [-1.1280e-05, -7.8082e-06,  6.5267e-06,  ..., -9.7901e-06,
         -7.4655e-06, -9.2387e-06],
        [-1.2726e-05, -9.5516e-06,  7.4655e-06,  ..., -1.1072e-05,
         -8.6129e-06, -9.1344e-06]], device='cuda:0')
Loss: 1.018962025642395


Running epoch 0, step 883, batch 883
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,   278,   269, 10376],
        [    0, 13706,  1862,  ...,   275,  1036, 42948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5189e-04, -8.4819e-05, -5.9703e-05,  ..., -5.4625e-04,
          3.8602e-04,  2.9447e-04],
        [-6.0350e-06, -4.2841e-06,  3.4682e-06,  ..., -5.2154e-06,
         -3.9674e-06, -4.8056e-06],
        [-1.3381e-05, -9.7752e-06,  8.1137e-06,  ..., -1.1414e-05,
         -8.5086e-06, -1.0267e-05],
        [-1.5035e-05, -1.0490e-05,  9.1046e-06,  ..., -1.2919e-05,
         -9.6858e-06, -1.2308e-05],
        [-1.6809e-05, -1.2681e-05,  1.0177e-05,  ..., -1.4514e-05,
         -1.1161e-05, -1.2070e-05]], device='cuda:0')
Loss: 1.015904426574707


Running epoch 0, step 884, batch 884
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   680,   401,  1407],
        [    0,    20,     9,  ...,    12,  2212, 24950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5901e-05, -1.3881e-05,  4.8437e-05,  ..., -5.2153e-04,
          3.9310e-04,  3.3660e-04],
        [-7.5623e-06, -5.3421e-06,  4.3251e-06,  ..., -6.5118e-06,
         -4.9584e-06, -6.0201e-06],
        [-1.6883e-05, -1.2249e-05,  1.0200e-05,  ..., -1.4305e-05,
         -1.0669e-05, -1.2904e-05],
        [-1.8820e-05, -1.3024e-05,  1.1340e-05,  ..., -1.6063e-05,
         -1.2025e-05, -1.5363e-05],
        [-2.1160e-05, -1.5855e-05,  1.2770e-05,  ..., -1.8135e-05,
         -1.3962e-05, -1.5125e-05]], device='cuda:0')
Loss: 1.016887903213501


Running epoch 0, step 885, batch 885
Sampled inputs[:2]: tensor([[    0,   292,    40,  ..., 26995,   278,   717],
        [    0,  2958,   298,  ...,    12,   709,   616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4410e-04, -1.6410e-04, -9.7615e-05,  ..., -6.5973e-04,
          6.6745e-04,  5.9243e-04],
        [-8.9779e-06, -6.1803e-06,  5.0999e-06,  ..., -7.8082e-06,
         -5.9493e-06, -7.4729e-06],
        [-2.0117e-05, -1.4216e-05,  1.2152e-05,  ..., -1.7107e-05,
         -1.2740e-05, -1.5914e-05],
        [-2.2486e-05, -1.5050e-05,  1.3530e-05,  ..., -1.9297e-05,
         -1.4424e-05, -1.9088e-05],
        [-2.5332e-05, -1.8507e-05,  1.5303e-05,  ..., -2.1756e-05,
         -1.6734e-05, -1.8671e-05]], device='cuda:0')
Loss: 0.9962764382362366


Running epoch 0, step 886, batch 886
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 8122, 1387,  616],
        [   0, 1943, 1837,  ...,  870,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3116e-04, -4.6888e-04, -3.2421e-04,  ..., -6.5973e-04,
          8.6965e-04,  7.6493e-04],
        [-1.0438e-05, -7.2606e-06,  6.0201e-06,  ..., -9.0450e-06,
         -6.8061e-06, -8.5235e-06],
        [-2.3380e-05, -1.6689e-05,  1.4268e-05,  ..., -1.9848e-05,
         -1.4611e-05, -1.8194e-05],
        [-2.6241e-05, -1.7777e-05,  1.6004e-05,  ..., -2.2456e-05,
         -1.6555e-05, -2.1890e-05],
        [-2.9266e-05, -2.1607e-05,  1.7852e-05,  ..., -2.5153e-05,
         -1.9163e-05, -2.1264e-05]], device='cuda:0')
Loss: 1.0412927865982056


Running epoch 0, step 887, batch 887
Sampled inputs[:2]: tensor([[    0,  1853,  3373,  ...,  3020,  6695,   300],
        [    0,  2670, 31283,  ...,    18,  9106,  1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2697e-04, -4.2289e-04, -2.5062e-04,  ..., -7.2118e-04,
          9.2650e-04,  8.2298e-04],
        [-1.1943e-05, -8.4080e-06,  6.9141e-06,  ..., -1.0327e-05,
         -7.8194e-06, -9.6858e-06],
        [-2.6837e-05, -1.9357e-05,  1.6399e-05,  ..., -2.2754e-05,
         -1.6861e-05, -2.0742e-05],
        [-3.0115e-05, -2.0653e-05,  1.8433e-05,  ..., -2.5734e-05,
         -1.9118e-05, -2.4945e-05],
        [-3.3557e-05, -2.4989e-05,  2.0489e-05,  ..., -2.8774e-05,
         -2.2069e-05, -2.4244e-05]], device='cuda:0')
Loss: 1.0440579652786255
Graident accumulation at epoch 0, step 887, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0290, -0.0079,  0.0038,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0224,  0.0064, -0.0019],
        [-0.0157,  0.0154, -0.0281,  ...,  0.0290, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2657e-05,  1.6585e-05, -1.3360e-04,  ...,  3.1807e-05,
          1.6282e-04,  1.3019e-04],
        [-1.0416e-05, -2.9073e-06,  5.0539e-06,  ..., -5.5744e-06,
         -7.2096e-07, -5.3619e-06],
        [ 3.0288e-05,  2.0435e-05, -1.9310e-05,  ...,  3.2959e-05,
          2.9004e-05,  1.3236e-05],
        [-2.2985e-05, -1.5302e-05,  1.2632e-05,  ..., -1.6683e-05,
         -1.2820e-05, -1.8286e-05],
        [-3.5993e-05, -2.6164e-05,  2.0135e-05,  ..., -2.9378e-05,
         -2.0319e-05, -2.1443e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2860e-08, 4.8568e-08, 3.7209e-08,  ..., 3.7204e-08, 1.1207e-07,
         4.2493e-08],
        [5.6109e-11, 3.6177e-11, 5.2289e-12,  ..., 4.2658e-11, 1.0747e-11,
         1.3652e-11],
        [2.3940e-09, 1.1424e-09, 4.0721e-10,  ..., 1.7897e-09, 3.4671e-10,
         6.9494e-10],
        [6.9881e-10, 5.1665e-10, 7.7271e-11,  ..., 5.7563e-10, 9.7697e-11,
         2.3870e-10],
        [2.8296e-10, 1.5516e-10, 3.2905e-11,  ..., 2.0576e-10, 3.8696e-11,
         7.1128e-11]], device='cuda:0')
optimizer state dict: 111.0
lr: [1.2915191260428308e-05, 1.2915191260428308e-05]
scheduler_last_epoch: 111


Running epoch 0, step 888, batch 888
Sampled inputs[:2]: tensor([[    0,   292,   221,  ...,   796, 12886,   694],
        [    0,   266, 15324,  ...,   943,  1613,  7178]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0184e-05, -3.8538e-05,  4.5916e-05,  ...,  7.9858e-05,
         -1.5541e-04, -1.1634e-06],
        [-1.6466e-06, -1.0356e-06,  8.3447e-07,  ..., -1.3411e-06,
         -9.9093e-07, -1.3635e-06],
        [-3.7551e-06, -2.4289e-06,  2.0117e-06,  ..., -3.0100e-06,
         -2.1905e-06, -2.9951e-06],
        [-4.1127e-06, -2.5481e-06,  2.1905e-06,  ..., -3.2932e-06,
         -2.3842e-06, -3.4422e-06],
        [-4.5598e-06, -3.0547e-06,  2.4438e-06,  ..., -3.7104e-06,
         -2.7865e-06, -3.4422e-06]], device='cuda:0')
Loss: 1.0147759914398193


Running epoch 0, step 889, batch 889
Sampled inputs[:2]: tensor([[    0,  6668,   565,  ...,   360,   259,  8166],
        [    0,  4566,   300,  ...,   271,  1644, 16473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3570e-06,  7.8752e-05,  1.2302e-06,  ...,  8.5057e-05,
         -2.3757e-04, -2.3470e-04],
        [-3.0324e-06, -1.9707e-06,  1.6689e-06,  ..., -2.6152e-06,
         -1.8477e-06, -2.4065e-06],
        [-6.9439e-06, -4.6343e-06,  4.0233e-06,  ..., -5.8711e-06,
         -4.0680e-06, -5.3048e-06],
        [-7.7337e-06, -4.9025e-06,  4.4703e-06,  ..., -6.5714e-06,
         -4.5300e-06, -6.2734e-06],
        [-8.3596e-06, -5.7966e-06,  4.8280e-06,  ..., -7.1675e-06,
         -5.1558e-06, -5.9903e-06]], device='cuda:0')
Loss: 1.035295844078064


Running epoch 0, step 890, batch 890
Sampled inputs[:2]: tensor([[   0, 2785, 1061,  ..., 1194,  692, 4339],
        [   0, 2615,   13,  ...,  940, 3661, 6837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4116e-04, -3.0773e-05, -1.2475e-04,  ...,  1.5917e-04,
         -2.7281e-04, -3.0320e-04],
        [-4.3139e-06, -2.8163e-06,  2.4661e-06,  ..., -3.8147e-06,
         -2.6003e-06, -3.5837e-06],
        [-9.9987e-06, -6.6757e-06,  6.0052e-06,  ..., -8.6129e-06,
         -5.7593e-06, -7.9423e-06],
        [-1.1265e-05, -7.1079e-06,  6.7800e-06,  ..., -9.7901e-06,
         -6.4820e-06, -9.5814e-06],
        [-1.1921e-05, -8.2552e-06,  7.1228e-06,  ..., -1.0386e-05,
         -7.2271e-06, -8.8364e-06]], device='cuda:0')
Loss: 0.9825198650360107


Running epoch 0, step 891, batch 891
Sampled inputs[:2]: tensor([[    0,  1855,    14,  ...,    12,   287, 16479],
        [    0,   300, 13523,  ..., 42438,   786,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9391e-04,  2.0382e-05, -2.2877e-04,  ...,  2.2961e-04,
         -3.6516e-04, -3.7171e-04],
        [-5.6922e-06, -3.8147e-06,  3.3118e-06,  ..., -5.0664e-06,
         -3.4310e-06, -4.5598e-06],
        [-1.3098e-05, -8.9705e-06,  7.9870e-06,  ..., -1.1370e-05,
         -7.5549e-06, -1.0058e-05],
        [-1.4842e-05, -9.6262e-06,  9.0748e-06,  ..., -1.3009e-05,
         -8.5831e-06, -1.2234e-05],
        [-1.5661e-05, -1.1116e-05,  9.5069e-06,  ..., -1.3739e-05,
         -9.5069e-06, -1.1191e-05]], device='cuda:0')
Loss: 1.0082924365997314


Running epoch 0, step 892, batch 892
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,    18,   998,  ...,  5322,   504,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6805e-04,  6.8296e-06, -1.7088e-04,  ...,  2.9150e-04,
         -4.4971e-04, -4.1599e-04],
        [-7.1675e-06, -4.9397e-06,  4.2208e-06,  ..., -6.4075e-06,
         -4.3809e-06, -5.6028e-06],
        [-1.6421e-05, -1.1563e-05,  1.0118e-05,  ..., -1.4395e-05,
         -9.6560e-06, -1.2383e-05],
        [-1.8567e-05, -1.2413e-05,  1.1459e-05,  ..., -1.6406e-05,
         -1.0937e-05, -1.4991e-05],
        [-1.9714e-05, -1.4380e-05,  1.2085e-05,  ..., -1.7464e-05,
         -1.2189e-05, -1.3843e-05]], device='cuda:0')
Loss: 1.0511486530303955


Running epoch 0, step 893, batch 893
Sampled inputs[:2]: tensor([[   0,  275,  467,  ...,  298,  365, 2714],
        [   0,  421, 6007,  ...,  408, 2105,  843]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6311e-04,  1.2422e-04, -8.1561e-05,  ...,  1.4124e-04,
         -3.7140e-04, -3.1970e-04],
        [-8.6725e-06, -5.9605e-06,  5.0738e-06,  ..., -7.7039e-06,
         -5.2452e-06, -6.7577e-06],
        [-1.9863e-05, -1.3947e-05,  1.2159e-05,  ..., -1.7315e-05,
         -1.1563e-05, -1.4961e-05],
        [-2.2471e-05, -1.4991e-05,  1.3754e-05,  ..., -1.9729e-05,
         -1.3083e-05, -1.8075e-05],
        [-2.3767e-05, -1.7285e-05,  1.4484e-05,  ..., -2.0951e-05,
         -1.4558e-05, -1.6704e-05]], device='cuda:0')
Loss: 1.0228896141052246


Running epoch 0, step 894, batch 894
Sampled inputs[:2]: tensor([[    0,  2165,  1323,  ...,   199,   677,  8376],
        [    0,   271, 36770,  ...,   278,  1398,  4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0416e-04,  6.8880e-06, -2.5510e-04,  ...,  1.7519e-04,
         -4.6060e-04, -3.2957e-04],
        [-1.0230e-05, -7.0781e-06,  5.9679e-06,  ..., -9.0078e-06,
         -6.2212e-06, -7.8976e-06],
        [-2.3559e-05, -1.6659e-05,  1.4365e-05,  ..., -2.0415e-05,
         -1.3843e-05, -1.7643e-05],
        [-2.6375e-05, -1.7762e-05,  1.6078e-05,  ..., -2.2992e-05,
         -1.5467e-05, -2.1026e-05],
        [-2.8208e-05, -2.0638e-05,  1.7107e-05,  ..., -2.4721e-05,
         -1.7419e-05, -1.9744e-05]], device='cuda:0')
Loss: 1.0307918787002563


Running epoch 0, step 895, batch 895
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1290e-04, -6.7929e-05, -3.0396e-04,  ...,  1.4979e-04,
         -4.6060e-04, -3.2994e-04],
        [-1.1683e-05, -8.1584e-06,  6.8545e-06,  ..., -1.0312e-05,
         -7.0855e-06, -8.9034e-06],
        [-2.6822e-05, -1.9118e-05,  1.6436e-05,  ..., -2.3335e-05,
         -1.5765e-05, -1.9863e-05],
        [ 4.4125e-05,  4.7887e-05, -6.7227e-06,  ...,  3.2750e-05,
          6.5303e-05,  6.6998e-06],
        [-3.2082e-05, -2.3663e-05,  1.9565e-05,  ..., -2.8238e-05,
         -1.9833e-05, -2.2233e-05]], device='cuda:0')
Loss: 1.0379894971847534
Graident accumulation at epoch 0, step 895, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0023,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0290, -0.0079,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1898e-05,  8.1334e-06, -1.5064e-04,  ...,  4.3605e-05,
          1.0048e-04,  8.4175e-05],
        [-1.0543e-05, -3.4324e-06,  5.2340e-06,  ..., -6.0481e-06,
         -1.3574e-06, -5.7161e-06],
        [ 2.4577e-05,  1.6480e-05, -1.5735e-05,  ...,  2.7330e-05,
          2.4527e-05,  9.9259e-06],
        [-1.6274e-05, -8.9830e-06,  1.0697e-05,  ..., -1.1739e-05,
         -5.0075e-06, -1.5787e-05],
        [-3.5602e-05, -2.5914e-05,  2.0078e-05,  ..., -2.9264e-05,
         -2.0270e-05, -2.1522e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3050e-08, 4.8524e-08, 3.7264e-08,  ..., 3.7189e-08, 1.1217e-07,
         4.2559e-08],
        [5.6189e-11, 3.6207e-11, 5.2706e-12,  ..., 4.2721e-11, 1.0787e-11,
         1.3717e-11],
        [2.3924e-09, 1.1416e-09, 4.0708e-10,  ..., 1.7885e-09, 3.4662e-10,
         6.9464e-10],
        [7.0006e-10, 5.1843e-10, 7.7239e-11,  ..., 5.7613e-10, 1.0186e-10,
         2.3850e-10],
        [2.8371e-10, 1.5557e-10, 3.3255e-11,  ..., 2.0635e-10, 3.9051e-11,
         7.1551e-11]], device='cuda:0')
optimizer state dict: 112.0
lr: [1.2796724211323173e-05, 1.2796724211323173e-05]
scheduler_last_epoch: 112


Running epoch 0, step 896, batch 896
Sampled inputs[:2]: tensor([[   0,  368,  729,  ...,  221,  380, 2830],
        [   0,   12,  401,  ...,  504,  565,  590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6135e-05,  8.5750e-06, -1.3872e-04,  ..., -2.7468e-05,
         -3.8095e-06, -4.1255e-06],
        [-1.4007e-06, -1.0431e-06,  8.0839e-07,  ..., -1.2815e-06,
         -8.0094e-07, -9.8348e-07],
        [-3.2187e-06, -2.4438e-06,  1.9521e-06,  ..., -2.8908e-06,
         -1.7807e-06, -2.1905e-06],
        [-3.6061e-06, -2.6226e-06,  2.1756e-06,  ..., -3.2634e-06,
         -1.9968e-06, -2.6226e-06],
        [-3.6657e-06, -2.8610e-06,  2.2054e-06,  ..., -3.3230e-06,
         -2.1309e-06, -2.3246e-06]], device='cuda:0')
Loss: 1.0182617902755737


Running epoch 0, step 897, batch 897
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,   287,  5724,  ...,   298,   591,  2609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1160e-05,  3.7856e-05, -1.1675e-04,  ..., -2.2428e-05,
          4.7855e-05,  9.7797e-05],
        [-2.9057e-06, -2.1532e-06,  1.6876e-06,  ..., -2.6375e-06,
         -1.7099e-06, -1.9819e-06],
        [-6.6310e-06, -5.0217e-06,  4.0233e-06,  ..., -5.9456e-06,
         -3.7923e-06, -4.4107e-06],
        [-7.4059e-06, -5.3644e-06,  4.4703e-06,  ..., -6.6757e-06,
         -4.2468e-06, -5.2154e-06],
        [-7.6592e-06, -5.9456e-06,  4.6194e-06,  ..., -6.9290e-06,
         -4.5747e-06, -4.7982e-06]], device='cuda:0')
Loss: 1.037294626235962


Running epoch 0, step 898, batch 898
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,  2346, 17886,  ...,   287,  6769,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2443e-05,  4.8991e-05, -1.3136e-04,  ..., -1.8579e-05,
          3.8677e-05,  8.3899e-05],
        [-4.2990e-06, -3.2112e-06,  2.5108e-06,  ..., -3.9041e-06,
         -2.5257e-06, -2.8871e-06],
        [-9.7752e-06, -7.4506e-06,  5.9605e-06,  ..., -8.7917e-06,
         -5.5805e-06, -6.4224e-06],
        [-1.1012e-05, -8.0764e-06,  6.6906e-06,  ..., -9.9540e-06,
         -6.3032e-06, -7.6592e-06],
        [-1.1340e-05, -8.8662e-06,  6.8843e-06,  ..., -1.0282e-05,
         -6.7800e-06, -7.0035e-06]], device='cuda:0')
Loss: 1.0261058807373047


Running epoch 0, step 899, batch 899
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,  3988,  1049,   935],
        [    0,    19,     9,  ..., 11504,   446,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2326e-04,  1.3320e-04, -1.3150e-04,  ..., -1.8579e-05,
          9.9543e-05,  1.3497e-04],
        [-5.7518e-06, -4.2543e-06,  3.3267e-06,  ..., -5.1707e-06,
         -3.3602e-06, -4.0345e-06],
        [-1.3113e-05, -9.9391e-06,  7.9274e-06,  ..., -1.1668e-05,
         -7.4655e-06, -8.9854e-06],
        [-1.4693e-05, -1.0684e-05,  8.8513e-06,  ..., -1.3143e-05,
         -8.3745e-06, -1.0639e-05],
        [-1.5110e-05, -1.1757e-05,  9.1046e-06,  ..., -1.3560e-05,
         -9.0152e-06, -9.7007e-06]], device='cuda:0')
Loss: 0.9942338466644287


Running epoch 0, step 900, batch 900
Sampled inputs[:2]: tensor([[    0,   266,  1234,  ...,   908,   328, 26300],
        [    0,  5603,  6598,  ...,  1692,  1713,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3833e-05,  1.4277e-04, -2.8737e-04,  ...,  1.3083e-04,
         -8.1109e-05, -8.4687e-05],
        [-7.1749e-06, -5.3346e-06,  4.1723e-06,  ..., -6.4522e-06,
         -4.1500e-06, -4.9770e-06],
        [ 1.8180e-04,  1.5798e-04, -1.4999e-04,  ...,  2.6405e-04,
          9.6019e-05,  8.5093e-05],
        [-1.8358e-05, -1.3381e-05,  1.1101e-05,  ..., -1.6421e-05,
         -1.0327e-05, -1.3173e-05],
        [-1.8880e-05, -1.4722e-05,  1.1429e-05,  ..., -1.6958e-05,
         -1.1131e-05, -1.1995e-05]], device='cuda:0')
Loss: 1.0212949514389038


Running epoch 0, step 901, batch 901
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,   257,    13,  ...,   328,   630,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4597e-05,  1.7416e-04, -7.0624e-05,  ...,  1.3100e-04,
         -3.1158e-05,  6.9251e-05],
        [-8.6576e-06, -6.4820e-06,  5.0366e-06,  ..., -7.7784e-06,
         -5.0478e-06, -5.9530e-06],
        [ 1.7837e-04,  1.5530e-04, -1.4794e-04,  ...,  2.6101e-04,
          9.4007e-05,  8.2828e-05],
        [-2.2158e-05, -1.6257e-05,  1.3381e-05,  ..., -1.9804e-05,
         -1.2562e-05, -1.5795e-05],
        [-2.2933e-05, -1.7971e-05,  1.3828e-05,  ..., -2.0593e-05,
         -1.3620e-05, -1.4514e-05]], device='cuda:0')
Loss: 1.0656286478042603


Running epoch 0, step 902, batch 902
Sampled inputs[:2]: tensor([[    0, 29073,   916,  ...,    12,   287,   850],
        [    0,   341,   298,  ...,   298,  1304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6363e-06,  2.2653e-04, -1.1391e-04,  ...,  2.0125e-04,
          2.7153e-04,  2.6305e-04],
        [-1.0185e-05, -7.3798e-06,  5.8040e-06,  ..., -9.0972e-06,
         -6.0163e-06, -7.3165e-06],
        [ 1.7489e-04,  1.5316e-04, -1.4605e-04,  ...,  2.5805e-04,
          9.1862e-05,  7.9848e-05],
        [-2.5824e-05, -1.8373e-05,  1.5378e-05,  ..., -2.2963e-05,
         -1.4842e-05, -1.9103e-05],
        [-2.6986e-05, -2.0579e-05,  1.6019e-05,  ..., -2.4050e-05,
         -1.6198e-05, -1.7717e-05]], device='cuda:0')
Loss: 0.9906607270240784


Running epoch 0, step 903, batch 903
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,   287, 20640,   292],
        [    0,   445,    16,  ...,  7747,  5308,  6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1579e-06,  5.7951e-04, -6.6108e-05,  ...,  3.8672e-04,
         -3.7530e-06,  9.1579e-05],
        [-1.1504e-05, -8.3335e-06,  6.6310e-06,  ..., -1.0394e-05,
         -6.7502e-06, -8.2850e-06],
        [ 1.7191e-04,  1.5097e-04, -1.4407e-04,  ...,  2.5522e-04,
          9.0289e-05,  7.7762e-05],
        [-2.9311e-05, -2.0787e-05,  1.7688e-05,  ..., -2.6345e-05,
         -1.6697e-05, -2.1756e-05],
        [-3.0488e-05, -2.3246e-05,  1.8358e-05,  ..., -2.7403e-05,
         -1.8165e-05, -1.9982e-05]], device='cuda:0')
Loss: 0.9981638193130493
Graident accumulation at epoch 0, step 903, batch 903
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0079,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9192e-05,  6.5271e-05, -1.4219e-04,  ...,  7.7917e-05,
          9.0054e-05,  8.4915e-05],
        [-1.0639e-05, -3.9225e-06,  5.3737e-06,  ..., -6.4827e-06,
         -1.8967e-06, -5.9730e-06],
        [ 3.9310e-05,  2.9928e-05, -2.8569e-05,  ...,  5.0119e-05,
          3.1103e-05,  1.6710e-05],
        [-1.7577e-05, -1.0163e-05,  1.1396e-05,  ..., -1.3200e-05,
         -6.1765e-06, -1.6384e-05],
        [-3.5090e-05, -2.5647e-05,  1.9906e-05,  ..., -2.9078e-05,
         -2.0060e-05, -2.1368e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2977e-08, 4.8811e-08, 3.7231e-08,  ..., 3.7301e-08, 1.1206e-07,
         4.2525e-08],
        [5.6266e-11, 3.6240e-11, 5.3093e-12,  ..., 4.2787e-11, 1.0821e-11,
         1.3772e-11],
        [2.4195e-09, 1.1633e-09, 4.2743e-10,  ..., 1.8518e-09, 3.5442e-10,
         6.9999e-10],
        [7.0022e-10, 5.1834e-10, 7.7475e-11,  ..., 5.7625e-10, 1.0204e-10,
         2.3874e-10],
        [2.8435e-10, 1.5595e-10, 3.3559e-11,  ..., 2.0690e-10, 3.9341e-11,
         7.1879e-11]], device='cuda:0')
optimizer state dict: 113.0
lr: [1.2677829798345599e-05, 1.2677829798345599e-05]
scheduler_last_epoch: 113


Running epoch 0, step 904, batch 904
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,   266, 15258,  ...,  2366,   368,  3988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7862e-05, -1.8620e-05, -1.4813e-04,  ...,  6.6958e-05,
          2.5631e-05,  6.6166e-05],
        [-1.3858e-06, -9.4622e-07,  8.0094e-07,  ..., -1.2517e-06,
         -7.0035e-07, -1.0356e-06],
        [-3.1590e-06, -2.1756e-06,  1.9222e-06,  ..., -2.8014e-06,
         -1.5125e-06, -2.2501e-06],
        [-3.6210e-06, -2.3842e-06,  2.1905e-06,  ..., -3.2336e-06,
         -1.7434e-06, -2.7567e-06],
        [-3.5912e-06, -2.5779e-06,  2.1905e-06,  ..., -3.2037e-06,
         -1.8403e-06, -2.3693e-06]], device='cuda:0')
Loss: 0.9909437894821167


Running epoch 0, step 905, batch 905
Sampled inputs[:2]: tensor([[    0, 13964,    13,  ...,    14,   560,   199],
        [    0,    12,   266,  ...,   278,   266, 10995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6552e-06,  3.4991e-05, -1.3256e-04,  ...,  1.1282e-04,
         -7.6967e-05, -3.9290e-05],
        [-2.7940e-06, -1.9670e-06,  1.6913e-06,  ..., -2.5257e-06,
         -1.3746e-06, -1.9409e-06],
        [-6.3777e-06, -4.5747e-06,  4.0382e-06,  ..., -5.7220e-06,
         -3.0473e-06, -4.3064e-06],
        [-7.2420e-06, -4.9919e-06,  4.5747e-06,  ..., -6.5267e-06,
         -3.4422e-06, -5.1856e-06],
        [-7.1079e-06, -5.2899e-06,  4.4703e-06,  ..., -6.4373e-06,
         -3.6508e-06, -4.4703e-06]], device='cuda:0')
Loss: 0.9814715385437012


Running epoch 0, step 906, batch 906
Sampled inputs[:2]: tensor([[   0, 1231,  278,  ...,   12, 2606,  266],
        [   0, 7219,  591,  ...,  278,  266, 5908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9839e-05,  4.7795e-05, -2.1623e-04,  ...,  1.9607e-04,
         -1.9086e-04, -1.4287e-04],
        [-4.2468e-06, -3.0845e-06,  2.6003e-06,  ..., -3.8669e-06,
         -2.0936e-06, -2.8834e-06],
        [-9.6709e-06, -7.1675e-06,  6.1840e-06,  ..., -8.7768e-06,
         -4.6790e-06, -6.4373e-06],
        [-1.0908e-05, -7.7933e-06,  6.9737e-06,  ..., -9.9242e-06,
         -5.2229e-06, -7.6592e-06],
        [-1.0714e-05, -8.2403e-06,  6.8098e-06,  ..., -9.8348e-06,
         -5.6028e-06, -6.6459e-06]], device='cuda:0')
Loss: 1.0372635126113892


Running epoch 0, step 907, batch 907
Sampled inputs[:2]: tensor([[   0, 5998,  591,  ..., 3126,   12,  358],
        [   0,  767, 1811,  ..., 1441, 1428,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0141e-04,  1.7763e-04, -1.8762e-04,  ...,  1.6815e-04,
         -3.6938e-04, -4.0056e-04],
        [-5.5432e-06, -4.0196e-06,  3.4198e-06,  ..., -5.1484e-06,
         -2.8834e-06, -3.8520e-06],
        [-1.2547e-05, -9.2983e-06,  8.1211e-06,  ..., -1.1519e-05,
         -6.3330e-06, -8.4043e-06],
        [-1.4216e-05, -1.0088e-05,  9.2387e-06,  ..., -1.3158e-05,
         -7.2047e-06, -1.0207e-05],
        [-1.4052e-05, -1.0788e-05,  9.0301e-06,  ..., -1.3009e-05,
         -7.6145e-06, -8.7321e-06]], device='cuda:0')
Loss: 0.9766671657562256


Running epoch 0, step 908, batch 908
Sampled inputs[:2]: tensor([[   0,  395, 4973,  ..., 5851,  409, 4370],
        [   0,  729, 3084,  ...,  381, 1445,  642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2427e-04,  1.0316e-04, -2.1898e-04,  ...,  2.1801e-04,
         -4.1900e-04, -4.6332e-04],
        [-6.9663e-06, -5.0180e-06,  4.2841e-06,  ..., -6.4075e-06,
         -3.5837e-06, -4.8019e-06],
        [ 1.2602e-04,  1.1352e-04, -8.7036e-05,  ...,  1.1056e-04,
          8.0791e-05,  4.5189e-05],
        [-1.7971e-05, -1.2651e-05,  1.1608e-05,  ..., -1.6466e-05,
         -9.0152e-06, -1.2830e-05],
        [-1.7747e-05, -1.3545e-05,  1.1355e-05,  ..., -1.6317e-05,
         -9.5516e-06, -1.1012e-05]], device='cuda:0')
Loss: 1.0454707145690918


Running epoch 0, step 909, batch 909
Sampled inputs[:2]: tensor([[   0,  396,  221,  ..., 1279,  720,  292],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6978e-04,  2.8291e-04, -1.0342e-04,  ...,  2.7435e-04,
         -5.1085e-04, -5.3535e-04],
        [ 5.4035e-05,  8.4435e-05, -4.6261e-05,  ...,  2.0673e-05,
          3.2902e-05,  1.4545e-05],
        [ 1.2276e-04,  1.1137e-04, -8.5165e-05,  ...,  1.0779e-04,
          7.8884e-05,  4.2581e-05],
        [-2.1592e-05, -1.4871e-05,  1.3739e-05,  ..., -1.9640e-05,
         -1.1235e-05, -1.6049e-05],
        [-2.1592e-05, -1.6168e-05,  1.3560e-05,  ..., -1.9595e-05,
         -1.1891e-05, -1.3858e-05]], device='cuda:0')
Loss: 0.9471138119697571


Running epoch 0, step 910, batch 910
Sampled inputs[:2]: tensor([[   0, 4385,  342,  ..., 3644,  775,  874],
        [   0, 1336,  278,  ...,  266, 3269,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1054e-04,  1.9865e-04, -1.3791e-04,  ...,  2.2898e-04,
         -4.3990e-04, -5.7230e-04],
        [ 5.2590e-05,  8.3384e-05, -4.5367e-05,  ...,  1.9384e-05,
          3.2176e-05,  1.3618e-05],
        [ 1.1948e-04,  1.0893e-04, -8.3064e-05,  ...,  1.0486e-04,
          7.7252e-05,  4.0480e-05],
        [-2.5392e-05, -1.7598e-05,  1.6183e-05,  ..., -2.3022e-05,
         -1.3091e-05, -1.8597e-05],
        [-2.5272e-05, -1.9014e-05,  1.5914e-05,  ..., -2.2948e-05,
         -1.3873e-05, -1.6078e-05]], device='cuda:0')
Loss: 1.0389864444732666


Running epoch 0, step 911, batch 911
Sampled inputs[:2]: tensor([[   0,  422,   14,  ...,  271, 1360,   12],
        [   0,  199,  769,  ...,  685, 1423,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1251e-04,  1.8449e-04, -1.7254e-04,  ...,  1.5556e-04,
         -3.0350e-04, -6.3238e-04],
        [ 5.1122e-05,  8.2453e-05, -4.4607e-05,  ...,  1.8050e-05,
          3.1252e-05,  1.2403e-05],
        [ 1.1610e-04,  1.0674e-04, -8.1179e-05,  ...,  1.0189e-04,
          7.5255e-05,  3.7873e-05],
        [-2.9087e-05, -1.9863e-05,  1.8269e-05,  ..., -2.6330e-05,
         -1.5311e-05, -2.1696e-05],
        [-2.9296e-05, -2.1711e-05,  1.8165e-05,  ..., -2.6494e-05,
         -1.6347e-05, -1.8969e-05]], device='cuda:0')
Loss: 1.0061994791030884
Graident accumulation at epoch 0, step 911, batch 911
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3978e-05,  7.7194e-05, -1.4522e-04,  ...,  8.5681e-05,
          5.0698e-05,  1.3185e-05],
        [-4.4629e-06,  4.7150e-06,  3.7565e-07,  ..., -4.0294e-06,
          1.4182e-06, -4.1354e-06],
        [ 4.6988e-05,  3.7609e-05, -3.3830e-05,  ...,  5.5296e-05,
          3.5518e-05,  1.8826e-05],
        [-1.8728e-05, -1.1133e-05,  1.2083e-05,  ..., -1.4513e-05,
         -7.0899e-06, -1.6915e-05],
        [-3.4511e-05, -2.5253e-05,  1.9732e-05,  ..., -2.8819e-05,
         -1.9689e-05, -2.1128e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3002e-08, 4.8797e-08, 3.7224e-08,  ..., 3.7288e-08, 1.1204e-07,
         4.2882e-08],
        [5.8823e-11, 4.3002e-11, 7.2938e-12,  ..., 4.3070e-11, 1.1787e-11,
         1.3912e-11],
        [2.4306e-09, 1.1735e-09, 4.3359e-10,  ..., 1.8604e-09, 3.5973e-10,
         7.0073e-10],
        [7.0036e-10, 5.1822e-10, 7.7731e-11,  ..., 5.7637e-10, 1.0217e-10,
         2.3897e-10],
        [2.8493e-10, 1.5627e-10, 3.3856e-11,  ..., 2.0739e-10, 3.9569e-11,
         7.2167e-11]], device='cuda:0')
optimizer state dict: 114.0
lr: [1.255852618959973e-05, 1.255852618959973e-05]
scheduler_last_epoch: 114


Running epoch 0, step 912, batch 912
Sampled inputs[:2]: tensor([[    0,   342,  3001,  ...,   369, 11195,   367],
        [    0,   266,  1624,  ...,    14,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0253e-04,  1.0649e-04, -5.8662e-05,  ...,  9.7887e-05,
          4.8647e-05,  3.1607e-05],
        [-1.4678e-06, -9.8348e-07,  8.6427e-07,  ..., -1.3039e-06,
         -7.1153e-07, -1.0654e-06],
        [-3.3379e-06, -2.2948e-06,  2.0713e-06,  ..., -2.9206e-06,
         -1.5795e-06, -2.3693e-06],
        [-3.7253e-06, -2.4289e-06,  2.3097e-06,  ..., -3.2783e-06,
         -1.7434e-06, -2.8014e-06],
        [-3.6806e-06, -2.6226e-06,  2.2650e-06,  ..., -3.2634e-06,
         -1.8775e-06, -2.4438e-06]], device='cuda:0')
Loss: 0.9942020773887634


Running epoch 0, step 913, batch 913
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  2165,  9311,  ..., 10570,   437,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2326e-05,  1.0669e-04, -1.0388e-04,  ...,  1.1660e-04,
          4.7243e-05,  8.9071e-05],
        [-2.9355e-06, -2.0489e-06,  1.6987e-06,  ..., -2.6524e-06,
         -1.4976e-06, -2.0564e-06],
        [-6.6310e-06, -4.7386e-06,  4.0382e-06,  ..., -5.9009e-06,
         -3.2783e-06, -4.5151e-06],
        [-7.4059e-06, -5.0217e-06,  4.5151e-06,  ..., -6.6459e-06,
         -3.6508e-06, -5.3942e-06],
        [-7.4059e-06, -5.4836e-06,  4.4852e-06,  ..., -6.6757e-06,
         -3.9339e-06, -4.6939e-06]], device='cuda:0')
Loss: 0.9950451254844666


Running epoch 0, step 914, batch 914
Sampled inputs[:2]: tensor([[    0,  6693,  1235,  ..., 10814,  1810,   367],
        [    0,   271,  4728,  ...,   344,   259,  1774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7644e-05,  3.3149e-04, -8.4643e-05,  ...,  1.3633e-04,
          8.2307e-05,  1.3126e-04],
        [-4.3213e-06, -3.0100e-06,  2.5220e-06,  ..., -3.9190e-06,
         -2.2389e-06, -3.1069e-06],
        [-9.8050e-06, -6.9737e-06,  6.0350e-06,  ..., -8.7321e-06,
         -4.9099e-06, -6.8247e-06],
        [-1.0878e-05, -7.3612e-06,  6.7204e-06,  ..., -9.7901e-06,
         -5.4389e-06, -8.1062e-06],
        [-1.0967e-05, -8.0764e-06,  6.7204e-06,  ..., -9.8944e-06,
         -5.9009e-06, -7.0930e-06]], device='cuda:0')
Loss: 0.9824690222740173


Running epoch 0, step 915, batch 915
Sampled inputs[:2]: tensor([[    0,   445,    29,  ..., 20247,   272,   298],
        [    0,   474,   513,  ...,   221,  2951,  7773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8782e-05,  6.8807e-04, -1.5165e-04,  ...,  3.8557e-04,
         -9.4225e-05,  1.6096e-04],
        [-5.9232e-06, -3.9265e-06,  3.2224e-06,  ..., -5.2899e-06,
         -3.1479e-06, -4.4554e-06],
        [-1.3456e-05, -9.1642e-06,  7.7859e-06,  ..., -1.1772e-05,
         -6.8918e-06, -9.7603e-06],
        [-1.4782e-05, -9.5665e-06,  8.5533e-06,  ..., -1.3098e-05,
         -7.6145e-06, -1.1414e-05],
        [-1.5110e-05, -1.0639e-05,  8.7470e-06,  ..., -1.3351e-05,
         -8.2254e-06, -1.0192e-05]], device='cuda:0')
Loss: 0.9709839224815369


Running epoch 0, step 916, batch 916
Sampled inputs[:2]: tensor([[   0, 7018,   14,  ..., 8288,   12, 1250],
        [   0, 7066, 2737,  ..., 2269,  271,  927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1301e-05,  6.5110e-04, -1.5832e-04,  ...,  3.7541e-04,
         -4.0983e-05,  1.4579e-04],
        [-7.3984e-06, -5.0366e-06,  4.1202e-06,  ..., -6.6310e-06,
         -3.9563e-06, -5.4091e-06],
        [-1.6749e-05, -1.1697e-05,  9.8571e-06,  ..., -1.4752e-05,
         -8.6576e-06, -1.1876e-05],
        [-1.8448e-05, -1.2293e-05,  1.0863e-05,  ..., -1.6406e-05,
         -9.5665e-06, -1.3873e-05],
        [-1.8820e-05, -1.3590e-05,  1.1072e-05,  ..., -1.6749e-05,
         -1.0341e-05, -1.2428e-05]], device='cuda:0')
Loss: 1.0157089233398438


Running epoch 0, step 917, batch 917
Sampled inputs[:2]: tensor([[    0, 17694,    12,  ..., 12452,   446,   475],
        [    0,  6184,  1412,  ...,    12,   266,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0066e-04,  6.7865e-04, -2.0001e-04,  ...,  3.9819e-04,
         -9.9972e-05,  7.2496e-05],
        [-8.8289e-06, -6.0201e-06,  4.9584e-06,  ..., -7.9721e-06,
         -4.7721e-06, -6.4671e-06],
        [-1.9893e-05, -1.3947e-05,  1.1809e-05,  ..., -1.7628e-05,
         -1.0379e-05, -1.4082e-05],
        [-2.2084e-05, -1.4737e-05,  1.3098e-05,  ..., -1.9774e-05,
         -1.1578e-05, -1.6630e-05],
        [-2.2456e-05, -1.6287e-05,  1.3337e-05,  ..., -2.0117e-05,
         -1.2472e-05, -1.4797e-05]], device='cuda:0')
Loss: 1.0052953958511353


Running epoch 0, step 918, batch 918
Sampled inputs[:2]: tensor([[    0,  9088,  7217,  ...,   199, 17822,   278],
        [    0,   344,   259,  ..., 47553,   287, 28978]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9957e-04,  8.6838e-04, -1.6257e-04,  ...,  5.1029e-04,
         -1.2707e-04, -9.1555e-05],
        [-1.0192e-05, -7.0408e-06,  5.8040e-06,  ..., -9.2611e-06,
         -5.5470e-06, -7.4729e-06],
        [-2.2948e-05, -1.6302e-05,  1.3791e-05,  ..., -2.0444e-05,
         -1.2040e-05, -1.6212e-05],
        [-2.5436e-05, -1.7181e-05,  1.5318e-05,  ..., -2.2918e-05,
         -1.3426e-05, -1.9193e-05],
        [-2.6003e-05, -1.9103e-05,  1.5602e-05,  ..., -2.3425e-05,
         -1.4529e-05, -1.7062e-05]], device='cuda:0')
Loss: 0.9982789158821106


Running epoch 0, step 919, batch 919
Sampled inputs[:2]: tensor([[    0,  4014,    88,  ...,    14, 11961,    13],
        [    0,    14,  1075,  ..., 22182,  5948,  8401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2617e-04,  9.8192e-04, -1.1157e-04,  ...,  5.8090e-04,
         -1.5763e-04, -1.3399e-04],
        [-1.1668e-05, -8.1733e-06,  6.7391e-06,  ..., -1.0587e-05,
         -6.2883e-06, -8.3707e-06],
        [-2.6271e-05, -1.8910e-05,  1.5981e-05,  ..., -2.3440e-05,
         -1.3679e-05, -1.8254e-05],
        [-2.9251e-05, -2.0087e-05,  1.7837e-05,  ..., -2.6360e-05,
         -1.5281e-05, -2.1636e-05],
        [-2.9653e-05, -2.2039e-05,  1.7986e-05,  ..., -2.6762e-05,
         -1.6466e-05, -1.9148e-05]], device='cuda:0')
Loss: 1.004772663116455
Graident accumulation at epoch 0, step 919, batch 919
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5197e-05,  1.6767e-04, -1.4186e-04,  ...,  1.3520e-04,
          2.9865e-05, -1.5317e-06],
        [-5.1834e-06,  3.4262e-06,  1.0120e-06,  ..., -4.6852e-06,
          6.4752e-07, -4.5589e-06],
        [ 3.9663e-05,  3.1958e-05, -2.8849e-05,  ...,  4.7422e-05,
          3.0599e-05,  1.5118e-05],
        [-1.9781e-05, -1.2029e-05,  1.2659e-05,  ..., -1.5698e-05,
         -7.9090e-06, -1.7387e-05],
        [-3.4025e-05, -2.4932e-05,  1.9557e-05,  ..., -2.8614e-05,
         -1.9366e-05, -2.0930e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2980e-08, 4.9712e-08, 3.7199e-08,  ..., 3.7588e-08, 1.1195e-07,
         4.2858e-08],
        [5.8900e-11, 4.3026e-11, 7.3319e-12,  ..., 4.3139e-11, 1.1815e-11,
         1.3968e-11],
        [2.4288e-09, 1.1727e-09, 4.3341e-10,  ..., 1.8591e-09, 3.5956e-10,
         7.0036e-10],
        [7.0052e-10, 5.1810e-10, 7.7972e-11,  ..., 5.7648e-10, 1.0230e-10,
         2.3920e-10],
        [2.8552e-10, 1.5660e-10, 3.4145e-11,  ..., 2.0790e-10, 3.9801e-11,
         7.2461e-11]], device='cuda:0')
optimizer state dict: 115.0
lr: [1.24388316157184e-05, 1.24388316157184e-05]
scheduler_last_epoch: 115


Running epoch 0, step 920, batch 920
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0, 49570,   644,  ...,   461,   800,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4625e-05,  1.8208e-05,  9.8512e-05,  ...,  1.3010e-05,
          1.0641e-04,  1.5164e-04],
        [-1.4454e-06, -1.1101e-06,  9.0152e-07,  ..., -1.3337e-06,
         -6.8918e-07, -8.4937e-07],
        [-3.1143e-06, -2.4587e-06,  2.0415e-06,  ..., -2.8759e-06,
         -1.4752e-06, -1.8254e-06],
        [-3.5316e-06, -2.6971e-06,  2.3097e-06,  ..., -3.2485e-06,
         -1.6615e-06, -2.1756e-06],
        [-3.5316e-06, -2.8908e-06,  2.3097e-06,  ..., -3.3081e-06,
         -1.8254e-06, -1.9073e-06]], device='cuda:0')
Loss: 1.0093923807144165


Running epoch 0, step 921, batch 921
Sampled inputs[:2]: tensor([[   0,   13, 1320,  ..., 8686, 6851,   13],
        [   0,  591,  688,  ...,  271, 3390,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9427e-05,  2.9108e-05,  1.6076e-04,  ..., -1.9835e-05,
          1.1518e-05, -1.7042e-05],
        [-2.8461e-06, -2.2352e-06,  1.7434e-06,  ..., -2.6450e-06,
         -1.4156e-06, -1.7621e-06],
        [-6.3032e-06, -5.0813e-06,  4.0829e-06,  ..., -5.8264e-06,
         -3.0547e-06, -3.8370e-06],
        [-7.0184e-06, -5.4538e-06,  4.5151e-06,  ..., -6.5118e-06,
         -3.4124e-06, -4.5300e-06],
        [-7.0930e-06, -5.9009e-06,  4.5896e-06,  ..., -6.6459e-06,
         -3.7327e-06, -3.9637e-06]], device='cuda:0')
Loss: 1.0065287351608276


Running epoch 0, step 922, batch 922
Sampled inputs[:2]: tensor([[    0, 36122,  1085,  ...,  6231,     9,  7794],
        [    0,  1526,   422,  ..., 22454,   409, 31482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3807e-04, -1.2678e-04,  1.3281e-04,  ..., -5.3151e-05,
          1.6909e-04,  2.8104e-05],
        [-4.3288e-06, -3.2634e-06,  2.6263e-06,  ..., -4.0308e-06,
         -2.3022e-06, -2.9244e-06],
        [-9.6560e-06, -7.4506e-06,  6.1989e-06,  ..., -8.8960e-06,
         -4.9770e-06, -6.3851e-06],
        [-1.0669e-05, -7.9274e-06,  6.8098e-06,  ..., -9.8944e-06,
         -5.5134e-06, -7.4506e-06],
        [-1.0937e-05, -8.7321e-06,  7.0333e-06,  ..., -1.0222e-05,
         -6.0871e-06, -6.6608e-06]], device='cuda:0')
Loss: 1.055459976196289


Running epoch 0, step 923, batch 923
Sampled inputs[:2]: tensor([[    0,   504,   409,  ...,  5863,  2621,   824],
        [    0,   437,  1119,  ..., 32831,    83,   623]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2495e-04, -8.7547e-05,  2.1007e-04,  ..., -1.3694e-04,
          1.5669e-04,  4.0267e-05],
        [-5.9009e-06, -4.4107e-06,  3.4720e-06,  ..., -5.4166e-06,
         -3.2298e-06, -4.0494e-06],
        [-1.3217e-05, -1.0133e-05,  8.2403e-06,  ..., -1.2010e-05,
         -7.0184e-06, -8.8885e-06],
        [-1.4484e-05, -1.0684e-05,  8.9705e-06,  ..., -1.3247e-05,
         -7.7188e-06, -1.0282e-05],
        [-1.5020e-05, -1.1906e-05,  9.3579e-06,  ..., -1.3843e-05,
         -8.5607e-06, -9.3281e-06]], device='cuda:0')
Loss: 1.0088443756103516


Running epoch 0, step 924, batch 924
Sampled inputs[:2]: tensor([[   0, 4672,  278,  ..., 7523, 2305,   13],
        [   0,  843, 3365,  ..., 1136, 1615,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5076e-04, -1.5192e-04,  2.0133e-04,  ..., -1.3829e-04,
          2.2067e-04,  5.4022e-05],
        [-7.4133e-06, -5.5358e-06,  4.3400e-06,  ..., -6.8024e-06,
         -4.0978e-06, -5.0552e-06],
        [-1.6555e-05, -1.2711e-05,  1.0282e-05,  ..., -1.5035e-05,
         -8.8736e-06, -1.1034e-05],
        [-1.8120e-05, -1.3366e-05,  1.1161e-05,  ..., -1.6570e-05,
         -9.7752e-06, -1.2755e-05],
        [ 1.2311e-04,  1.4490e-04, -6.7139e-05,  ...,  1.4266e-04,
          9.6929e-05,  4.7527e-05]], device='cuda:0')
Loss: 1.0536309480667114


Running epoch 0, step 925, batch 925
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,  1034,   287,  ...,  9677,    13,  6687]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6284e-04, -1.2748e-04,  1.5901e-04,  ..., -1.5039e-04,
          2.4384e-04,  1.8755e-04],
        [-8.8438e-06, -6.4597e-06,  5.1409e-06,  ..., -8.1062e-06,
         -4.9025e-06, -6.1952e-06],
        [-1.9774e-05, -1.4827e-05,  1.2219e-05,  ..., -1.7866e-05,
         -1.0543e-05, -1.3448e-05],
        [-2.1681e-05, -1.5572e-05,  1.3307e-05,  ..., -1.9759e-05,
         -1.1653e-05, -1.5631e-05],
        [ 1.1940e-04,  1.4235e-04, -6.4904e-05,  ...,  1.3937e-04,
          9.4887e-05,  4.4979e-05]], device='cuda:0')
Loss: 0.9861738085746765


Running epoch 0, step 926, batch 926
Sampled inputs[:2]: tensor([[    0,   259,  3022,  ...,   437,  5100,  1782],
        [    0,  3703,   278,  ...,  9807,    14, 10365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7951e-04, -3.2531e-04, -3.4808e-05,  ..., -1.8550e-04,
          2.1474e-04,  7.3073e-05],
        [-1.0312e-05, -7.5474e-06,  6.0201e-06,  ..., -9.4399e-06,
         -5.6438e-06, -7.1637e-06],
        [-2.2963e-05, -1.7270e-05,  1.4260e-05,  ..., -2.0728e-05,
         -1.2085e-05, -1.5490e-05],
        [-2.5213e-05, -1.8150e-05,  1.5557e-05,  ..., -2.2948e-05,
         -1.3389e-05, -1.8075e-05],
        [ 1.1579e-04,  1.3948e-04, -6.2624e-05,  ...,  1.3609e-04,
          9.2987e-05,  4.2892e-05]], device='cuda:0')
Loss: 1.0240271091461182


Running epoch 0, step 927, batch 927
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,   271,   266,  ...,    70,    27,  5311]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2477e-04, -3.0625e-04, -9.3734e-05,  ..., -2.1756e-04,
          3.1892e-04,  1.7587e-04],
        [-1.1757e-05, -8.7023e-06,  6.9588e-06,  ..., -1.0759e-05,
         -6.4038e-06, -8.0392e-06],
        [-2.6196e-05, -1.9908e-05,  1.6436e-05,  ..., -2.3678e-05,
         -1.3746e-05, -1.7442e-05],
        [-2.8789e-05, -2.0966e-05,  1.7986e-05,  ..., -2.6211e-05,
         -1.5214e-05, -2.0340e-05],
        [ 1.1220e-04,  1.3644e-04, -6.0225e-05,  ...,  1.3275e-04,
          9.0976e-05,  4.0896e-05]], device='cuda:0')
Loss: 1.03659987449646
Graident accumulation at epoch 0, step 927, batch 927
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2009e-06,  1.2027e-04, -1.3704e-04,  ...,  9.9926e-05,
          5.8771e-05,  1.6208e-05],
        [-5.8408e-06,  2.2133e-06,  1.6067e-06,  ..., -5.2925e-06,
         -5.7611e-08, -4.9069e-06],
        [ 3.3077e-05,  2.6771e-05, -2.4320e-05,  ...,  4.0312e-05,
          2.6164e-05,  1.1862e-05],
        [-2.0681e-05, -1.2922e-05,  1.3191e-05,  ..., -1.6749e-05,
         -8.6395e-06, -1.7683e-05],
        [-1.9403e-05, -8.7952e-06,  1.1579e-05,  ..., -1.2477e-05,
         -8.3321e-06, -1.4748e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2957e-08, 4.9756e-08, 3.7171e-08,  ..., 3.7598e-08, 1.1194e-07,
         4.2846e-08],
        [5.8979e-11, 4.3059e-11, 7.3730e-12,  ..., 4.3211e-11, 1.1844e-11,
         1.4019e-11],
        [2.4271e-09, 1.1719e-09, 4.3325e-10,  ..., 1.8578e-09, 3.5939e-10,
         6.9997e-10],
        [7.0065e-10, 5.1802e-10, 7.8217e-11,  ..., 5.7659e-10, 1.0243e-10,
         2.3937e-10],
        [2.9783e-10, 1.7506e-10, 3.7738e-11,  ..., 2.2532e-10, 4.8038e-11,
         7.4061e-11]], device='cuda:0')
optimizer state dict: 116.0
lr: [1.2318764367077325e-05, 1.2318764367077325e-05]
scheduler_last_epoch: 116


Running epoch 0, step 928, batch 928
Sampled inputs[:2]: tensor([[   0,  287,  358,  ...,  328, 1704, 3227],
        [   0,  278, 1620,  ...,  360, 1758,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2640e-05,  2.0711e-04,  1.4108e-04,  ...,  0.0000e+00,
          7.0120e-05,  2.5670e-04],
        [-1.6093e-06, -1.1399e-06,  8.1584e-07,  ..., -1.4156e-06,
         -9.9093e-07, -1.1772e-06],
        [-3.4869e-06, -2.5779e-06,  1.9073e-06,  ..., -2.9951e-06,
         -2.0713e-06, -2.4438e-06],
        [-3.7402e-06, -2.6077e-06,  2.0117e-06,  ..., -3.2485e-06,
         -2.2501e-06, -2.7865e-06],
        [-4.0829e-06, -3.1143e-06,  2.2501e-06,  ..., -3.5763e-06,
         -2.5481e-06, -2.6375e-06]], device='cuda:0')
Loss: 1.0011892318725586


Running epoch 0, step 929, batch 929
Sampled inputs[:2]: tensor([[    0,    14, 21687,  ...,   943,  2153,  4089],
        [    0,   287, 21212,  ...,  3123,   944,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1142e-04,  1.8013e-04,  5.9503e-06,  ...,  9.0523e-06,
          1.5552e-04,  3.6464e-04],
        [-3.1516e-06, -2.3544e-06,  1.7695e-06,  ..., -2.8014e-06,
         -1.8291e-06, -2.1197e-06],
        [-6.8992e-06, -5.3346e-06,  4.0978e-06,  ..., -6.0499e-06,
         -3.8892e-06, -4.5300e-06],
        [-7.3910e-06, -5.4687e-06,  4.3660e-06,  ..., -6.5267e-06,
         -4.1723e-06, -5.1260e-06],
        [-7.8976e-06, -6.3181e-06,  4.6939e-06,  ..., -7.0781e-06,
         -4.7684e-06, -4.7982e-06]], device='cuda:0')
Loss: 1.0511584281921387


Running epoch 0, step 930, batch 930
Sampled inputs[:2]: tensor([[   0,  298,  894,  ...,  266, 2904, 1679],
        [   0, 6294,  367,  ...,  496,   14,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8255e-05,  1.0296e-04, -1.9128e-05,  ..., -2.3557e-04,
          4.8049e-04,  4.7332e-04],
        [-4.8727e-06, -3.4049e-06,  2.5481e-06,  ..., -4.3064e-06,
         -2.8126e-06, -3.4533e-06],
        [-1.0550e-05, -7.7039e-06,  5.9307e-06,  ..., -9.2089e-06,
         -5.9158e-06, -7.2569e-06],
        [-1.1235e-05, -7.8231e-06,  6.2510e-06,  ..., -9.8944e-06,
         -6.3479e-06, -8.1658e-06],
        [-1.2219e-05, -9.2387e-06,  6.8694e-06,  ..., -1.0848e-05,
         -7.3165e-06, -7.8082e-06]], device='cuda:0')
Loss: 0.9934295415878296


Running epoch 0, step 931, batch 931
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,   266, 10726,  ..., 13973, 22191, 15913]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1021e-04, -6.7434e-05, -1.3958e-04,  ..., -1.6550e-04,
          4.6388e-04,  4.4467e-04],
        [-6.3106e-06, -4.5076e-06,  3.5092e-06,  ..., -5.6401e-06,
         -3.6173e-06, -4.3884e-06],
        [-1.3739e-05, -1.0222e-05,  8.1807e-06,  ..., -1.2130e-05,
         -7.6443e-06, -9.2536e-06],
        [-1.4752e-05, -1.0476e-05,  8.7246e-06,  ..., -1.3143e-05,
         -8.2701e-06, -1.0535e-05],
        [-1.5825e-05, -1.2189e-05,  9.3877e-06,  ..., -1.4216e-05,
         -9.4324e-06, -9.8795e-06]], device='cuda:0')
Loss: 1.0248888731002808


Running epoch 0, step 932, batch 932
Sampled inputs[:2]: tensor([[   0,  342,  970,  ...,  401, 2907, 1657],
        [   0,   13,   41,  ...,    5,  271, 2936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0430e-04,  2.2308e-05, -1.7252e-05,  ..., -1.7838e-04,
          6.6591e-04,  5.7635e-04],
        [-7.9572e-06, -5.7444e-06,  4.3921e-06,  ..., -7.0482e-06,
         -4.6231e-06, -5.4687e-06],
        [-1.7405e-05, -1.3083e-05,  1.0237e-05,  ..., -1.5274e-05,
         -9.8497e-06, -1.1638e-05],
        [-1.8626e-05, -1.3381e-05,  1.0900e-05,  ..., -1.6466e-05,
         -1.0610e-05, -1.3188e-05],
        [-2.0057e-05, -1.5616e-05,  1.1772e-05,  ..., -1.7911e-05,
         -1.2144e-05, -1.2442e-05]], device='cuda:0')
Loss: 1.0304360389709473


Running epoch 0, step 933, batch 933
Sampled inputs[:2]: tensor([[    0,  1304,  1040,  ...,   287,  1665,   741],
        [    0,   775,   721,  ...,  5650,   518, 11548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0867e-04,  5.1650e-05, -9.3496e-05,  ..., -1.5588e-04,
          8.0986e-04,  5.6358e-04],
        [-9.4548e-06, -6.9588e-06,  5.2415e-06,  ..., -8.4117e-06,
         -5.4538e-06, -6.4299e-06],
        [-2.0742e-05, -1.5885e-05,  1.2249e-05,  ..., -1.8299e-05,
         -1.1660e-05, -1.3739e-05],
        [-2.2203e-05, -1.6287e-05,  1.3046e-05,  ..., -1.9729e-05,
         -1.2562e-05, -1.5572e-05],
        [-2.3738e-05, -1.8820e-05,  1.3977e-05,  ..., -2.1309e-05,
         -1.4320e-05, -1.4573e-05]], device='cuda:0')
Loss: 1.020769476890564


Running epoch 0, step 934, batch 934
Sampled inputs[:2]: tensor([[    0, 14700,   717,  ..., 10570,   292,   221],
        [    0,   271,   266,  ...,  4298,  1231,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1422e-04,  1.9414e-04, -1.4944e-04,  ..., -1.8414e-04,
          9.8673e-04,  9.0367e-04],
        [-1.0997e-05, -7.9200e-06,  5.8822e-06,  ..., -9.8720e-06,
         -6.4820e-06, -7.7933e-06],
        [-2.3946e-05, -1.8030e-05,  1.3895e-05,  ..., -2.1204e-05,
         -1.3657e-05, -1.6317e-05],
        [-2.5794e-05, -1.8492e-05,  1.4760e-05,  ..., -2.3052e-05,
         -1.4871e-05, -1.8701e-05],
        [-2.7552e-05, -2.1458e-05,  1.5989e-05,  ..., -2.4766e-05,
         -1.6764e-05, -1.7390e-05]], device='cuda:0')
Loss: 0.972421407699585


Running epoch 0, step 935, batch 935
Sampled inputs[:2]: tensor([[   0, 7963,   17,  ...,   50,   13,   18],
        [   0,  221,  380,  ...,  508, 1853,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5743e-05,  1.7087e-04, -1.1983e-04,  ...,  2.0567e-05,
          9.2602e-04,  1.0060e-03],
        [-1.2480e-05, -8.9705e-06,  6.7651e-06,  ..., -1.1243e-05,
         -7.3276e-06, -8.7991e-06],
        [-2.7195e-05, -2.0415e-05,  1.5952e-05,  ..., -2.4170e-05,
         -1.5430e-05, -1.8448e-05],
        [-2.9370e-05, -2.0981e-05,  1.7010e-05,  ..., -2.6330e-05,
         -1.6823e-05, -2.1219e-05],
        [-3.1263e-05, -2.4334e-05,  1.8343e-05,  ..., -2.8223e-05,
         -1.8999e-05, -1.9610e-05]], device='cuda:0')
Loss: 1.0057274103164673
Graident accumulation at epoch 0, step 935, batch 935
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1855e-05,  1.2533e-04, -1.3532e-04,  ...,  9.1990e-05,
          1.4550e-04,  1.1519e-04],
        [-6.5047e-06,  1.0950e-06,  2.1225e-06,  ..., -5.8876e-06,
         -7.8461e-07, -5.2961e-06],
        [ 2.7050e-05,  2.2052e-05, -2.0293e-05,  ...,  3.3864e-05,
          2.2005e-05,  8.8309e-06],
        [-2.1550e-05, -1.3728e-05,  1.3573e-05,  ..., -1.7707e-05,
         -9.4579e-06, -1.8036e-05],
        [-2.0589e-05, -1.0349e-05,  1.2255e-05,  ..., -1.4052e-05,
         -9.3987e-06, -1.5234e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2886e-08, 4.9736e-08, 3.7148e-08,  ..., 3.7561e-08, 1.1269e-07,
         4.3815e-08],
        [5.9076e-11, 4.3096e-11, 7.4114e-12,  ..., 4.3294e-11, 1.1886e-11,
         1.4082e-11],
        [2.4254e-09, 1.1712e-09, 4.3307e-10,  ..., 1.8565e-09, 3.5927e-10,
         6.9961e-10],
        [7.0081e-10, 5.1795e-10, 7.8428e-11,  ..., 5.7671e-10, 1.0261e-10,
         2.3958e-10],
        [2.9851e-10, 1.7547e-10, 3.8037e-11,  ..., 2.2589e-10, 4.8351e-11,
         7.4372e-11]], device='cuda:0')
optimizer state dict: 117.0
lr: [1.219834279100018e-05, 1.219834279100018e-05]
scheduler_last_epoch: 117


Running epoch 0, step 936, batch 936
Sampled inputs[:2]: tensor([[    0,  1640,  1103,  ...,   685,  1478,    14],
        [    0,    12,   696,  ..., 14275,  2661,  6129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2453e-05,  1.9047e-04,  1.1334e-06,  ..., -5.6396e-05,
          1.8870e-04,  1.5677e-04],
        [-1.5572e-06, -1.1846e-06,  7.8976e-07,  ..., -1.4976e-06,
         -9.6858e-07, -1.1250e-06],
        [-3.3528e-06, -2.6524e-06,  1.8999e-06,  ..., -3.1441e-06,
         -1.9819e-06, -2.2650e-06],
        [-3.5465e-06, -2.6822e-06,  1.9521e-06,  ..., -3.3975e-06,
         -2.1607e-06, -2.5928e-06],
        [-3.9637e-06, -3.2037e-06,  2.2799e-06,  ..., -3.7402e-06,
         -2.4587e-06, -2.4587e-06]], device='cuda:0')
Loss: 0.991125226020813


Running epoch 0, step 937, batch 937
Sampled inputs[:2]: tensor([[    0,   413,    29,  ...,   818,   278,   970],
        [    0,    13, 23904,  ...,   560,  8840,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6003e-05,  1.4246e-04,  3.2228e-05,  ...,  7.8595e-06,
          3.6104e-04,  2.7942e-04],
        [-3.3081e-06, -2.4512e-06,  1.5497e-06,  ..., -3.0175e-06,
         -2.1160e-06, -2.4512e-06],
        [-7.2569e-06, -5.5581e-06,  3.7849e-06,  ..., -6.4969e-06,
         -4.4703e-06, -5.1409e-06],
        [-7.4804e-06, -5.5134e-06,  3.8147e-06,  ..., -6.7949e-06,
         -4.6939e-06, -5.6177e-06],
        [-8.6129e-06, -6.7651e-06,  4.5449e-06,  ..., -7.7933e-06,
         -5.5879e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9858500957489014


Running epoch 0, step 938, batch 938
Sampled inputs[:2]: tensor([[    0, 25845,  4034,  ...,   474,   221,   474],
        [    0,  1067,   408,  ...,  4657,  1016,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8408e-04,  1.2646e-04, -1.2088e-06,  ..., -6.3288e-05,
          3.0224e-04,  2.7388e-04],
        [-4.7982e-06, -3.6061e-06,  2.3395e-06,  ..., -4.4033e-06,
         -2.9877e-06, -3.4422e-06],
        [-1.0580e-05, -8.1956e-06,  5.7369e-06,  ..., -9.5069e-06,
         -6.3255e-06, -7.2271e-06],
        [-1.1042e-05, -8.2105e-06,  5.8562e-06,  ..., -1.0073e-05,
         -6.7353e-06, -8.0466e-06],
        [-1.2368e-05, -9.8646e-06,  6.7651e-06,  ..., -1.1250e-05,
         -7.8380e-06, -7.8231e-06]], device='cuda:0')
Loss: 0.9722071290016174


Running epoch 0, step 939, batch 939
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   493,  2104,    14],
        [    0,   409, 35049,  ...,    12,   699,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2809e-04,  4.3456e-04,  3.0193e-04,  ...,  2.8426e-05,
          3.8605e-04,  3.9764e-04],
        [-6.4597e-06, -4.7982e-06,  3.0585e-06,  ..., -5.8636e-06,
         -4.0531e-06, -4.7386e-06],
        [-1.4395e-05, -1.1027e-05,  7.5623e-06,  ..., -1.2815e-05,
         -8.6948e-06, -1.0133e-05],
        [-1.4976e-05, -1.1012e-05,  7.6815e-06,  ..., -1.3500e-05,
         -9.1940e-06, -1.1191e-05],
        [-1.6868e-05, -1.3292e-05,  8.9407e-06,  ..., -1.5184e-05,
         -1.0788e-05, -1.1012e-05]], device='cuda:0')
Loss: 1.0044496059417725


Running epoch 0, step 940, batch 940
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3365e-04,  4.5665e-04,  2.3354e-04,  ...,  4.8302e-05,
          2.8979e-04,  4.0100e-04],
        [-8.0615e-06, -6.0275e-06,  4.0010e-06,  ..., -7.2941e-06,
         -4.8988e-06, -5.6773e-06],
        [-1.7971e-05, -1.3858e-05,  9.7826e-06,  ..., -1.6019e-05,
         -1.0557e-05, -1.2234e-05],
        [-1.8790e-05, -1.3947e-05,  1.0036e-05,  ..., -1.6913e-05,
         -1.1161e-05, -1.3530e-05],
        [-2.0891e-05, -1.6585e-05,  1.1414e-05,  ..., -1.8835e-05,
         -1.3053e-05, -1.3202e-05]], device='cuda:0')
Loss: 1.0274752378463745


Running epoch 0, step 941, batch 941
Sampled inputs[:2]: tensor([[    0, 18322,   287,  ...,   953,   271,   221],
        [    0,  2518,   437,  ...,    12,  1041,   283]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4402e-04,  4.1907e-04,  1.2681e-04,  ...,  9.7949e-05,
          2.3163e-04,  4.5870e-04],
        [-9.6560e-06, -7.1973e-06,  4.8876e-06,  ..., -8.6948e-06,
         -5.7556e-06, -6.7800e-06],
        [-2.1577e-05, -1.6585e-05,  1.1928e-05,  ..., -1.9163e-05,
         -1.2420e-05, -1.4678e-05],
        [-2.2560e-05, -1.6659e-05,  1.2256e-05,  ..., -2.0191e-05,
         -1.3113e-05, -1.6227e-05],
        [-2.4825e-05, -1.9684e-05,  1.3754e-05,  ..., -2.2322e-05,
         -1.5259e-05, -1.5661e-05]], device='cuda:0')
Loss: 0.9851342439651489


Running epoch 0, step 942, batch 942
Sampled inputs[:2]: tensor([[    0,   795,  3185,  ...,    14,  1671,   199],
        [    0,   634,   631,  ...,  3431,   287, 27947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1850e-04,  3.6857e-04,  1.8334e-04,  ...,  1.6944e-04,
          1.3014e-04,  5.1639e-04],
        [-1.1206e-05, -8.4564e-06,  5.8189e-06,  ..., -1.0110e-05,
         -6.6385e-06, -7.7337e-06],
        [-2.5049e-05, -1.9461e-05,  1.4134e-05,  ..., -2.2322e-05,
         -1.4372e-05, -1.6794e-05],
        [-2.6286e-05, -1.9670e-05,  1.4596e-05,  ..., -2.3589e-05,
         -1.5199e-05, -1.8612e-05],
        [-2.8789e-05, -2.3067e-05,  1.6272e-05,  ..., -2.5988e-05,
         -1.7643e-05, -1.7926e-05]], device='cuda:0')
Loss: 1.0083621740341187


Running epoch 0, step 943, batch 943
Sampled inputs[:2]: tensor([[    0,  3544,   417,  ...,   380,   381,  3794],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9807e-04,  3.8383e-04,  1.3751e-04,  ...,  3.3514e-04,
          2.2451e-05,  2.5403e-04],
        [-1.2599e-05, -9.4399e-06,  6.6459e-06,  ..., -1.1459e-05,
         -7.4841e-06, -8.7321e-06],
        [-2.8104e-05, -2.1696e-05,  1.6086e-05,  ..., -2.5183e-05,
         -1.6116e-05, -1.8820e-05],
        [-2.9728e-05, -2.2009e-05,  1.6816e-05,  ..., -2.6867e-05,
         -1.7241e-05, -2.1160e-05],
        [-3.2410e-05, -2.5824e-05,  1.8582e-05,  ..., -2.9400e-05,
         -1.9833e-05, -2.0131e-05]], device='cuda:0')
Loss: 0.9799397587776184
Graident accumulation at epoch 0, step 943, batch 943
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0343],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0154, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.1375e-06,  1.5118e-04, -1.0804e-04,  ...,  1.1630e-04,
          1.3319e-04,  1.2907e-04],
        [-7.1141e-06,  4.1469e-08,  2.5749e-06,  ..., -6.4447e-06,
         -1.4546e-06, -5.6397e-06],
        [ 2.1534e-05,  1.7678e-05, -1.6655e-05,  ...,  2.7959e-05,
          1.8193e-05,  6.0658e-06],
        [-2.2368e-05, -1.4556e-05,  1.3897e-05,  ..., -1.8623e-05,
         -1.0236e-05, -1.8349e-05],
        [-2.1771e-05, -1.1896e-05,  1.2888e-05,  ..., -1.5586e-05,
         -1.0442e-05, -1.5724e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2852e-08, 4.9833e-08, 3.7130e-08,  ..., 3.7636e-08, 1.1258e-07,
         4.3836e-08],
        [5.9176e-11, 4.3142e-11, 7.4481e-12,  ..., 4.3382e-11, 1.1930e-11,
         1.4145e-11],
        [2.4238e-09, 1.1705e-09, 4.3289e-10,  ..., 1.8553e-09, 3.5917e-10,
         6.9926e-10],
        [7.0099e-10, 5.1791e-10, 7.8633e-11,  ..., 5.7686e-10, 1.0281e-10,
         2.3979e-10],
        [2.9926e-10, 1.7597e-10, 3.8344e-11,  ..., 2.2653e-10, 4.8696e-11,
         7.4703e-11]], device='cuda:0')
optimizer state dict: 118.0
lr: [1.2077585288954968e-05, 1.2077585288954968e-05]
scheduler_last_epoch: 118


Running epoch 0, step 944, batch 944
Sampled inputs[:2]: tensor([[    0,   834,    89,  ...,  4030,    12,  6528],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5866e-05,  2.0949e-04,  1.3562e-05,  ...,  1.5491e-04,
         -1.4563e-04, -9.4656e-05],
        [-1.5274e-06, -1.1474e-06,  8.0839e-07,  ..., -1.4603e-06,
         -8.9779e-07, -1.0580e-06],
        [-3.3528e-06, -2.5779e-06,  1.9372e-06,  ..., -3.0994e-06,
         -1.8701e-06, -2.2054e-06],
        [-3.6955e-06, -2.7120e-06,  2.1011e-06,  ..., -3.5018e-06,
         -2.1309e-06, -2.6375e-06],
        [-3.8743e-06, -3.0696e-06,  2.2352e-06,  ..., -3.6061e-06,
         -2.2799e-06, -2.3395e-06]], device='cuda:0')
Loss: 0.9920019507408142


Running epoch 0, step 945, batch 945
Sampled inputs[:2]: tensor([[    0,   199,  3289,  ...,  2269,  6476,   271],
        [    0,   300,  6263,  ..., 18488,  1665,  1640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2176e-05,  4.7083e-04, -1.2248e-05,  ...,  4.4275e-05,
         -1.3753e-04, -1.5862e-04],
        [-3.2634e-06, -2.2128e-06,  1.5423e-06,  ..., -2.9728e-06,
         -1.8291e-06, -2.3171e-06],
        [-7.1079e-06, -4.9919e-06,  3.6582e-06,  ..., -6.3330e-06,
         -3.8669e-06, -4.8578e-06],
        [-7.5996e-06, -5.0962e-06,  3.8520e-06,  ..., -6.8843e-06,
         -4.2021e-06, -5.4985e-06],
        [-8.1658e-06, -5.9158e-06,  4.2170e-06,  ..., -7.3463e-06,
         -4.6790e-06, -5.1707e-06]], device='cuda:0')
Loss: 0.9998767375946045


Running epoch 0, step 946, batch 946
Sampled inputs[:2]: tensor([[   0,  380,  560,  ...,  287, 6769,  806],
        [   0, 6418,  446,  ...,  413,   29,  413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2487e-04,  7.2009e-04,  1.6837e-04,  ...,  1.4979e-04,
         -1.8199e-04, -1.5786e-04],
        [-4.9174e-06, -3.4943e-06,  2.3432e-06,  ..., -4.4629e-06,
         -2.9244e-06, -3.3528e-06],
        [-1.0833e-05, -7.9423e-06,  5.5581e-06,  ..., -9.6709e-06,
         -6.2808e-06, -7.1824e-06],
        [-1.1563e-05, -8.1360e-06,  5.8338e-06,  ..., -1.0446e-05,
         -6.7800e-06, -8.0764e-06],
        [-1.2577e-05, -9.4920e-06,  6.4671e-06,  ..., -1.1340e-05,
         -7.6592e-06, -7.7486e-06]], device='cuda:0')
Loss: 1.027845859527588


Running epoch 0, step 947, batch 947
Sampled inputs[:2]: tensor([[   0, 4175,  437,  ..., 1700,   14,  381],
        [   0, 3353,   17,  ...,  596,   12,  461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0911e-04,  7.3461e-04,  8.1670e-05,  ...,  1.2413e-04,
         -2.0335e-04, -2.0336e-04],
        [-6.4969e-06, -4.8280e-06,  3.2596e-06,  ..., -5.9083e-06,
         -3.8445e-06, -4.2580e-06],
        [ 7.2951e-05,  7.8504e-05, -2.4150e-05,  ...,  5.9704e-05,
          4.1410e-05,  3.5973e-05],
        [-1.5289e-05, -1.1250e-05,  8.0839e-06,  ..., -1.3828e-05,
         -8.8960e-06, -1.0327e-05],
        [-1.6630e-05, -1.3068e-05,  8.8960e-06,  ..., -1.5095e-05,
         -1.0148e-05, -9.9391e-06]], device='cuda:0')
Loss: 1.042481780052185


Running epoch 0, step 948, batch 948
Sampled inputs[:2]: tensor([[   0, 1549, 7052,  ..., 2529, 3958,   37],
        [   0,  516,  689,  ...,  278,  516, 6137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5927e-06,  7.1969e-04, -1.3077e-04,  ...,  1.6107e-04,
         -3.1565e-04, -2.5103e-04],
        [-7.9945e-06, -5.9307e-06,  4.1053e-06,  ..., -7.3016e-06,
         -4.7237e-06, -5.3085e-06],
        [ 6.9599e-05,  7.5971e-05, -2.2109e-05,  ...,  5.6664e-05,
          3.9532e-05,  3.3753e-05],
        [-1.8880e-05, -1.3798e-05,  1.0259e-05,  ..., -1.7121e-05,
         -1.0937e-05, -1.2934e-05],
        [-2.0504e-05, -1.6108e-05,  1.1265e-05,  ..., -1.8656e-05,
         -1.2472e-05, -1.2293e-05]], device='cuda:0')
Loss: 1.008940577507019


Running epoch 0, step 949, batch 949
Sampled inputs[:2]: tensor([[   0, 3101,  275,  ..., 2345,  609,  287],
        [   0,   14,   22,  ..., 1319,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5022e-05,  7.6384e-04, -5.8652e-05,  ...,  1.4878e-04,
         -2.7325e-04, -1.0952e-04],
        [-9.5889e-06, -7.1079e-06,  4.8578e-06,  ..., -8.7470e-06,
         -5.7146e-06, -6.5155e-06],
        [ 6.5888e-05,  7.3140e-05, -2.0171e-05,  ...,  5.3386e-05,
          3.7312e-05,  3.1086e-05],
        [-2.2560e-05, -1.6421e-05,  1.2137e-05,  ..., -2.0415e-05,
         -1.3158e-05, -1.5751e-05],
        [-2.4736e-05, -1.9431e-05,  1.3500e-05,  ..., -2.2441e-05,
         -1.5140e-05, -1.5095e-05]], device='cuda:0')
Loss: 0.9890258312225342


Running epoch 0, step 950, batch 950
Sampled inputs[:2]: tensor([[    0,  1145,    13,  ...,   721,  1119,  3495],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6489e-04,  8.9199e-04, -1.7743e-04,  ...,  2.1103e-04,
         -5.3889e-04, -3.0438e-04],
        [-1.1072e-05, -8.2254e-06,  5.6736e-06,  ..., -1.0148e-05,
         -6.5751e-06, -7.5065e-06],
        [ 6.2536e-05,  7.0547e-05, -1.8190e-05,  ...,  5.0257e-05,
          3.5435e-05,  2.8910e-05],
        [-2.6152e-05, -1.9059e-05,  1.4238e-05,  ..., -2.3797e-05,
         -1.5199e-05, -1.8239e-05],
        [-2.8551e-05, -2.2471e-05,  1.5765e-05,  ..., -2.6032e-05,
         -1.7405e-05, -1.7375e-05]], device='cuda:0')
Loss: 1.0008865594863892


Running epoch 0, step 951, batch 951
Sampled inputs[:2]: tensor([[   0, 2302,  287,  ..., 1522, 1666,  300],
        [   0, 1795,  650,  ...,  516, 2793, 1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5569e-04,  9.4723e-04, -2.6121e-04,  ...,  3.1474e-04,
         -7.3130e-04, -3.2417e-04],
        [-1.2644e-05, -9.3877e-06,  6.5677e-06,  ..., -1.1541e-05,
         -7.3686e-06, -8.5197e-06],
        [ 5.8959e-05,  6.7820e-05, -1.6044e-05,  ...,  4.7098e-05,
          3.3654e-05,  2.6645e-05],
        [-2.9862e-05, -2.1771e-05,  1.6458e-05,  ..., -2.7075e-05,
         -1.7032e-05, -2.0713e-05],
        [-3.2544e-05, -2.5630e-05,  1.8150e-05,  ..., -2.9624e-05,
         -1.9565e-05, -1.9699e-05]], device='cuda:0')
Loss: 0.9866236448287964
Graident accumulation at epoch 0, step 951, batch 951
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.3793e-05,  2.3079e-04, -1.2336e-04,  ...,  1.3615e-04,
          4.6742e-05,  8.3750e-05],
        [-7.6670e-06, -9.0145e-07,  2.9741e-06,  ..., -6.9543e-06,
         -2.0460e-06, -5.9277e-06],
        [ 2.5277e-05,  2.2692e-05, -1.6594e-05,  ...,  2.9873e-05,
          1.9739e-05,  8.1237e-06],
        [-2.3117e-05, -1.5278e-05,  1.4153e-05,  ..., -1.9468e-05,
         -1.0916e-05, -1.8585e-05],
        [-2.2848e-05, -1.3270e-05,  1.3414e-05,  ..., -1.6990e-05,
         -1.1355e-05, -1.6121e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2804e-08, 5.0681e-08, 3.7161e-08,  ..., 3.7697e-08, 1.1300e-07,
         4.3897e-08],
        [5.9277e-11, 4.3187e-11, 7.4838e-12,  ..., 4.3472e-11, 1.1972e-11,
         1.4203e-11],
        [2.4248e-09, 1.1739e-09, 4.3272e-10,  ..., 1.8556e-09, 3.5994e-10,
         6.9927e-10],
        [7.0118e-10, 5.1787e-10, 7.8825e-11,  ..., 5.7701e-10, 1.0300e-10,
         2.3998e-10],
        [3.0002e-10, 1.7645e-10, 3.8635e-11,  ..., 2.2718e-10, 4.9030e-11,
         7.5016e-11]], device='cuda:0')
optimizer state dict: 119.0
lr: [1.1956510313742102e-05, 1.1956510313742102e-05]
scheduler_last_epoch: 119


Running epoch 0, step 952, batch 952
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,   409,  4146,  ...,     9,   360,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5101e-04, -2.5520e-05,  1.4565e-04,  ..., -5.9718e-05,
          5.9711e-05,  1.5811e-04],
        [-1.5348e-06, -1.2144e-06,  8.6054e-07,  ..., -1.4231e-06,
         -9.2760e-07, -1.0282e-06],
        [-3.5465e-06, -2.8908e-06,  2.1160e-06,  ..., -3.2783e-06,
         -2.1160e-06, -2.3395e-06],
        [-3.7551e-06, -2.9653e-06,  2.2501e-06,  ..., -3.5018e-06,
         -2.2501e-06, -2.6077e-06],
        [-4.0531e-06, -3.3826e-06,  2.4140e-06,  ..., -3.8147e-06,
         -2.5630e-06, -2.5034e-06]], device='cuda:0')
Loss: 1.0024056434631348


Running epoch 0, step 953, batch 953
Sampled inputs[:2]: tensor([[   0,  825, 3066,  ..., 1184,  266, 7964],
        [   0, 3933, 6394,  ..., 1364,  950,  847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3214e-05,  6.2426e-05,  3.5828e-05,  ..., -3.4955e-06,
         -8.1733e-05, -6.0144e-05],
        [-3.1590e-06, -2.4438e-06,  1.7956e-06,  ..., -2.8461e-06,
         -1.7658e-06, -2.0638e-06],
        [ 1.6514e-04,  1.5356e-04, -1.1751e-04,  ...,  1.6861e-04,
          1.0342e-04,  5.0562e-05],
        [-7.6592e-06, -5.9158e-06,  4.6045e-06,  ..., -6.9439e-06,
         -4.2617e-06, -5.1707e-06],
        [-8.2850e-06, -6.8098e-06,  4.9770e-06,  ..., -7.6294e-06,
         -4.9174e-06, -5.0366e-06]], device='cuda:0')
Loss: 1.0357953310012817


Running epoch 0, step 954, batch 954
Sampled inputs[:2]: tensor([[    0,   446, 28686,  ...,    35,  2706, 19712],
        [    0,    15,    83,  ...,  6030,    14, 14080]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3816e-05,  2.8039e-06,  9.5432e-05,  ..., -7.5969e-05,
         -6.5525e-05, -1.1029e-04],
        [-4.6864e-06, -3.7327e-06,  2.6673e-06,  ..., -4.2915e-06,
         -2.7046e-06, -3.0138e-06],
        [ 1.6156e-04,  1.5051e-04, -1.1537e-04,  ...,  1.6528e-04,
          1.0130e-04,  4.8357e-05],
        [-1.1295e-05, -8.9556e-06,  6.7353e-06,  ..., -1.0371e-05,
         -6.4522e-06, -7.5400e-06],
        [-1.2368e-05, -1.0386e-05,  7.4059e-06,  ..., -1.1474e-05,
         -7.4655e-06, -7.3761e-06]], device='cuda:0')
Loss: 1.0602359771728516


Running epoch 0, step 955, batch 955
Sampled inputs[:2]: tensor([[   0, 1062,  648,  ...,  266, 4939,  278],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2711e-04,  1.0054e-04, -4.6178e-05,  ...,  3.3626e-04,
         -3.8479e-04, -4.3321e-04],
        [-6.1616e-06, -4.8503e-06,  3.5726e-06,  ..., -5.6699e-06,
         -3.5018e-06, -4.0494e-06],
        [ 1.5819e-04,  1.4790e-04, -1.1315e-04,  ...,  1.6218e-04,
          9.9537e-05,  4.6077e-05],
        [-1.4797e-05, -1.1548e-05,  9.0450e-06,  ..., -1.3635e-05,
         -8.3074e-06, -1.0088e-05],
        [-1.6153e-05, -1.3426e-05,  9.8944e-06,  ..., -1.5005e-05,
         -9.6112e-06, -9.7156e-06]], device='cuda:0')
Loss: 0.973811686038971


Running epoch 0, step 956, batch 956
Sampled inputs[:2]: tensor([[    0,   292, 17190,  ...,  3078,     9,   287],
        [    0,   221,  1771,  ..., 14547,  1705,  1003]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5457e-04,  1.6468e-04,  5.8239e-05,  ...,  3.0057e-04,
         -2.8273e-04, -2.3884e-04],
        [-7.8529e-06, -6.0722e-06,  4.3772e-06,  ..., -7.1675e-06,
         -4.5300e-06, -5.2862e-06],
        [ 1.5429e-04,  1.4501e-04, -1.1116e-04,  ...,  1.5876e-04,
          9.7242e-05,  4.3290e-05],
        [-1.8731e-05, -1.4350e-05,  1.0997e-05,  ..., -1.7092e-05,
         -1.0647e-05, -1.3024e-05],
        [-2.0742e-05, -1.6958e-05,  1.2279e-05,  ..., -1.9088e-05,
         -1.2472e-05, -1.2815e-05]], device='cuda:0')
Loss: 1.0279465913772583


Running epoch 0, step 957, batch 957
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,    12,   344,  ...,    14,  2295,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4497e-05, -1.0673e-04,  2.1626e-05,  ...,  3.0347e-04,
         -8.5880e-05, -2.4080e-04],
        [-9.4250e-06, -7.3835e-06,  5.3197e-06,  ..., -8.6129e-06,
         -5.4277e-06, -6.2212e-06],
        [ 3.1020e-04,  3.4762e-04, -1.7934e-04,  ...,  3.2801e-04,
          2.5495e-04,  6.0618e-05],
        [-2.2382e-05, -1.7390e-05,  1.3292e-05,  ..., -2.0459e-05,
         -1.2703e-05, -1.5303e-05],
        [-2.4855e-05, -2.0579e-05,  1.4827e-05,  ..., -2.2933e-05,
         -1.4946e-05, -1.5095e-05]], device='cuda:0')
Loss: 1.0226160287857056


Running epoch 0, step 958, batch 958
Sampled inputs[:2]: tensor([[   0, 4213, 1921,  ..., 1340, 1049,  292],
        [   0,  292,   58,  ...,  319,  221, 1061]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4664e-04, -3.0737e-04, -5.1622e-05,  ...,  3.1409e-04,
          9.1347e-05, -1.8377e-04],
        [-1.1019e-05, -8.5980e-06,  6.2287e-06,  ..., -1.0066e-05,
         -6.2920e-06, -7.2345e-06],
        [ 3.0652e-04,  3.4473e-04, -1.7712e-04,  ...,  3.2469e-04,
          2.5299e-04,  5.8293e-05],
        [-2.6181e-05, -2.0280e-05,  1.5572e-05,  ..., -2.3916e-05,
         -1.4715e-05, -1.7807e-05],
        [-2.9027e-05, -2.3961e-05,  1.7345e-05,  ..., -2.6777e-05,
         -1.7315e-05, -1.7554e-05]], device='cuda:0')
Loss: 1.0200560092926025


Running epoch 0, step 959, batch 959
Sampled inputs[:2]: tensor([[    0,  2229,   352,  ...,  4988,    33,    13],
        [    0,   292, 12522,  ...,   266,  1977,  8481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5246e-04, -2.4024e-04,  6.0842e-06,  ...,  4.3527e-04,
          1.5370e-06, -3.8105e-05],
        [-1.2681e-05, -9.8720e-06,  7.0781e-06,  ..., -1.1571e-05,
         -7.3053e-06, -8.3447e-06],
        [ 3.0286e-04,  3.4184e-04, -1.7509e-04,  ...,  3.2141e-04,
          2.5080e-04,  5.5939e-05],
        [-2.9936e-05, -2.3156e-05,  1.7598e-05,  ..., -2.7344e-05,
         -1.7025e-05, -2.0400e-05],
        [-3.3349e-05, -2.7493e-05,  1.9759e-05,  ..., -3.0681e-05,
         -2.0027e-05, -2.0146e-05]], device='cuda:0')
Loss: 1.0083407163619995
Graident accumulation at epoch 0, step 959, batch 959
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0282,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.6659e-05,  1.8368e-04, -1.1041e-04,  ...,  1.6606e-04,
          4.2221e-05,  7.1564e-05],
        [-8.1684e-06, -1.7985e-06,  3.3845e-06,  ..., -7.4160e-06,
         -2.5719e-06, -6.1694e-06],
        [ 5.3035e-05,  5.4607e-05, -3.2444e-05,  ...,  5.9027e-05,
          4.2845e-05,  1.2905e-05],
        [-2.3799e-05, -1.6066e-05,  1.4498e-05,  ..., -2.0256e-05,
         -1.1527e-05, -1.8766e-05],
        [-2.3898e-05, -1.4692e-05,  1.4049e-05,  ..., -1.8359e-05,
         -1.2222e-05, -1.6524e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2794e-08, 5.0688e-08, 3.7124e-08,  ..., 3.7849e-08, 1.1288e-07,
         4.3854e-08],
        [5.9378e-11, 4.3242e-11, 7.5264e-12,  ..., 4.3563e-11, 1.2014e-11,
         1.4259e-11],
        [2.5141e-09, 1.2896e-09, 4.6294e-10,  ..., 1.9571e-09, 4.2248e-10,
         7.0170e-10],
        [7.0138e-10, 5.1789e-10, 7.9056e-11,  ..., 5.7718e-10, 1.0318e-10,
         2.4016e-10],
        [3.0083e-10, 1.7703e-10, 3.8987e-11,  ..., 2.2789e-10, 4.9382e-11,
         7.5347e-11]], device='cuda:0')
optimizer state dict: 120.0
lr: [1.1835136366674677e-05, 1.1835136366674677e-05]
scheduler_last_epoch: 120
Epoch 0 | Batch 959/1048 | Training PPL: 3186.650776452411 | time 67.42045712471008
Saving checkpoint at epoch 0, step 959, batch 959
Epoch 0 | Validation PPL: 7.404573464719055 | Learning rate: 1.1835136366674677e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_959, AFTER epoch 0, step 959


Running epoch 0, step 960, batch 960
Sampled inputs[:2]: tensor([[    0,  1943,   300,  ..., 43803,   368,  2400],
        [    0,  7110,   278,  ...,    66,    13,  9070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4210e-05, -1.6949e-05, -5.0782e-05,  ...,  1.3100e-04,
         -1.2361e-04, -6.7582e-06],
        [-1.5274e-06, -1.1772e-06,  8.5309e-07,  ..., -1.3560e-06,
         -7.4133e-07, -1.0729e-06],
        [-3.5763e-06, -2.8163e-06,  2.1756e-06,  ..., -3.1292e-06,
         -1.6540e-06, -2.4289e-06],
        [-3.5912e-06, -2.7269e-06,  2.1458e-06,  ..., -3.1441e-06,
         -1.6764e-06, -2.5779e-06],
        [-3.9041e-06, -3.1739e-06,  2.3544e-06,  ..., -3.4571e-06,
         -1.9670e-06, -2.4140e-06]], device='cuda:0')
Loss: 0.989634096622467


Running epoch 0, step 961, batch 961
Sampled inputs[:2]: tensor([[    0, 32444,    41,  ...,    14,    18,    59],
        [    0,  2738,   278,  ...,   292,    35,  2147]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1054e-04, -1.9045e-04, -1.9845e-05,  ...,  1.4480e-04,
         -1.0086e-05,  1.2271e-04],
        [-3.1516e-06, -2.5406e-06,  1.8030e-06,  ..., -2.8089e-06,
         -1.7248e-06, -2.1383e-06],
        [ 1.2959e-04,  1.5538e-04, -6.8454e-05,  ...,  1.1549e-04,
          1.2057e-04,  3.1521e-05],
        [-7.2569e-06, -5.8115e-06,  4.3660e-06,  ..., -6.4522e-06,
         -3.8818e-06, -5.0664e-06],
        [-8.2254e-06, -6.9886e-06,  4.9770e-06,  ..., -7.4208e-06,
         -4.7237e-06, -5.0813e-06]], device='cuda:0')
Loss: 1.0415434837341309


Running epoch 0, step 962, batch 962
Sampled inputs[:2]: tensor([[   0, 6945, 2360,  ...,   30,  413,   16],
        [   0,  726, 3979,  ...,   27, 2085,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7567e-04, -1.3852e-04,  4.8942e-05,  ...,  6.2466e-05,
          3.3181e-04,  4.2374e-04],
        [-4.8354e-06, -3.7849e-06,  2.5891e-06,  ..., -4.2394e-06,
         -2.6934e-06, -3.4273e-06],
        [ 1.2562e-04,  1.5235e-04, -6.6457e-05,  ...,  1.1214e-04,
          1.1832e-04,  2.8541e-05],
        [-1.1101e-05, -8.6576e-06,  6.2734e-06,  ..., -9.7305e-06,
         -6.0871e-06, -8.1062e-06],
        [-1.2785e-05, -1.0625e-05,  7.2867e-06,  ..., -1.1384e-05,
         -7.5102e-06, -8.3148e-06]], device='cuda:0')
Loss: 0.9934137463569641


Running epoch 0, step 963, batch 963
Sampled inputs[:2]: tensor([[   0, 1823,   12,  ..., 1874,  271,  266],
        [   0, 5522, 5662,  ...,  638, 1231, 1098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7842e-05, -2.0803e-04,  2.1029e-05,  ...,  1.7747e-05,
          3.3979e-04,  4.4200e-04],
        [-6.4224e-06, -5.1111e-06,  3.5651e-06,  ..., -5.6475e-06,
         -3.6210e-06, -4.4107e-06],
        [ 1.2188e-04,  1.4918e-04, -6.4073e-05,  ...,  1.0882e-04,
          1.1619e-04,  2.6216e-05],
        [-1.4842e-05, -1.1757e-05,  8.6576e-06,  ..., -1.3053e-05,
         -8.2180e-06, -1.0520e-05],
        [-1.7047e-05, -1.4365e-05,  9.9987e-06,  ..., -1.5259e-05,
         -1.0133e-05, -1.0818e-05]], device='cuda:0')
Loss: 1.0161716938018799


Running epoch 0, step 964, batch 964
Sampled inputs[:2]: tensor([[   0,  874,  590,  ...,  300,  867,  638],
        [   0,  706, 1005,  ...,  278,  266, 5590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4805e-05, -1.5494e-04, -8.8817e-05,  ..., -4.1035e-05,
          3.3326e-04,  4.2525e-04],
        [-7.9572e-06, -6.2659e-06,  4.4517e-06,  ..., -7.0557e-06,
         -4.4815e-06, -5.4836e-06],
        [ 1.1835e-04,  1.4647e-04, -6.1882e-05,  ...,  1.0564e-04,
          1.1429e-04,  2.3802e-05],
        [-1.8463e-05, -1.4439e-05,  1.0893e-05,  ..., -1.6347e-05,
         -1.0200e-05, -1.3143e-05],
        [-2.1040e-05, -1.7568e-05,  1.2472e-05,  ..., -1.8924e-05,
         -1.2472e-05, -1.3337e-05]], device='cuda:0')
Loss: 1.0079219341278076


Running epoch 0, step 965, batch 965
Sampled inputs[:2]: tensor([[    0,  5750,   642,  ...,   221, 15441,   644],
        [    0,  1921,   843,  ...,  9420,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7872e-06, -1.8154e-04, -1.7038e-04,  ..., -1.1295e-05,
          3.7168e-04,  2.3996e-04],
        [-9.4920e-06, -7.5102e-06,  5.3756e-06,  ..., -8.4490e-06,
         -5.2787e-06, -6.4448e-06],
        [ 1.1471e-04,  1.4349e-04, -5.9588e-05,  ...,  1.0235e-04,
          1.1244e-04,  2.1537e-05],
        [-2.2069e-05, -1.7330e-05,  1.3158e-05,  ..., -1.9625e-05,
         -1.2033e-05, -1.5497e-05],
        [-2.5064e-05, -2.0966e-05,  1.4991e-05,  ..., -2.2620e-05,
         -1.4678e-05, -1.5661e-05]], device='cuda:0')
Loss: 1.0183496475219727


Running epoch 0, step 966, batch 966
Sampled inputs[:2]: tensor([[    0,  4653, 21419,  ...,  7845,   300,   565],
        [    0,  2027,   365,  ...,   368,  1782,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6182e-05,  4.7198e-05, -1.1445e-04,  ..., -7.2984e-05,
          4.0754e-04,  3.3862e-04],
        [-1.1265e-05, -8.7991e-06,  6.1318e-06,  ..., -9.9540e-06,
         -6.3814e-06, -7.7784e-06],
        [ 1.1051e-04,  1.4030e-04, -5.7650e-05,  ...,  9.8789e-05,
          1.0982e-04,  1.8408e-05],
        [ 2.4689e-04,  2.6858e-04, -1.9475e-04,  ...,  2.3665e-04,
          2.2620e-04,  1.7656e-04],
        [-3.0011e-05, -2.4810e-05,  1.7300e-05,  ..., -2.6882e-05,
         -1.7911e-05, -1.9133e-05]], device='cuda:0')
Loss: 0.9833677411079407


Running epoch 0, step 967, batch 967
Sampled inputs[:2]: tensor([[    0,  2383,  9843,  ...,   401,  3959,   300],
        [    0,   944,   278,  ..., 17330,  1683,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7549e-05,  4.4711e-05, -9.0756e-05,  ..., -5.6709e-05,
          3.0740e-04,  2.5995e-04],
        [-1.2800e-05, -9.9912e-06,  7.0594e-06,  ..., -1.1317e-05,
         -7.1451e-06, -8.7693e-06],
        [ 1.0685e-04,  1.3738e-04, -5.5311e-05,  ...,  9.5511e-05,
          1.0801e-04,  1.6024e-05],
        [ 2.4314e-04,  2.6569e-04, -1.9235e-04,  ...,  2.3330e-04,
          2.2436e-04,  1.7401e-04],
        [-3.4004e-05, -2.8118e-05,  1.9819e-05,  ..., -3.0518e-05,
         -2.0072e-05, -2.1532e-05]], device='cuda:0')
Loss: 1.0091546773910522
Graident accumulation at epoch 0, step 967, batch 967
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0143, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.5238e-05,  1.6979e-04, -1.0845e-04,  ...,  1.4378e-04,
          6.8740e-05,  9.0403e-05],
        [-8.6316e-06, -2.6178e-06,  3.7520e-06,  ..., -7.8061e-06,
         -3.0292e-06, -6.4294e-06],
        [ 5.8417e-05,  6.2884e-05, -3.4731e-05,  ...,  6.2675e-05,
          4.9362e-05,  1.3217e-05],
        [ 2.8942e-06,  1.2110e-05, -6.1871e-06,  ...,  5.0997e-06,
          1.2062e-05,  5.1108e-07],
        [-2.4909e-05, -1.6035e-05,  1.4626e-05,  ..., -1.9575e-05,
         -1.3007e-05, -1.7025e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2726e-08, 5.0639e-08, 3.7095e-08,  ..., 3.7814e-08, 1.1287e-07,
         4.3878e-08],
        [5.9483e-11, 4.3298e-11, 7.5688e-12,  ..., 4.3647e-11, 1.2053e-11,
         1.4321e-11],
        [2.5230e-09, 1.3071e-09, 4.6554e-10,  ..., 1.9643e-09, 4.3373e-10,
         7.0126e-10],
        [7.5979e-10, 5.8796e-10, 1.1598e-10,  ..., 6.3104e-10, 1.5342e-10,
         2.7020e-10],
        [3.0168e-10, 1.7764e-10, 3.9341e-11,  ..., 2.2859e-10, 4.9735e-11,
         7.5735e-11]], device='cuda:0')
optimizer state dict: 121.0
lr: [1.1713481994751294e-05, 1.1713481994751294e-05]
scheduler_last_epoch: 121


Running epoch 0, step 968, batch 968
Sampled inputs[:2]: tensor([[    0,    12,  6426,  ...,  2629, 13422,    12],
        [    0,   287,   298,  ..., 14121,  3121,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1328e-06,  4.4413e-05, -2.8128e-05,  ...,  6.4189e-05,
         -2.8900e-05,  1.1333e-04],
        [-1.5646e-06, -1.1399e-06,  8.6427e-07,  ..., -1.3337e-06,
         -8.5309e-07, -1.1995e-06],
        [-3.7998e-06, -2.8312e-06,  2.2352e-06,  ..., -3.2037e-06,
         -2.0266e-06, -2.8014e-06],
        [-3.6657e-06, -2.6226e-06,  2.1905e-06,  ..., -3.1292e-06,
         -1.9819e-06, -2.8759e-06],
        [-4.2319e-06, -3.2485e-06,  2.4736e-06,  ..., -3.6210e-06,
         -2.4140e-06, -2.9206e-06]], device='cuda:0')
Loss: 0.9745381474494934


Running epoch 0, step 969, batch 969
Sampled inputs[:2]: tensor([[    0,  4878,   607,  ...,    14, 17331,   287],
        [    0,    47,    12,  ...,  4367,   278,   471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6273e-04,  5.5208e-05, -7.5281e-06,  ...,  5.9374e-05,
         -1.8508e-06,  5.1413e-05],
        [-3.0920e-06, -2.3916e-06,  1.7807e-06,  ..., -2.7269e-06,
         -1.6615e-06, -2.1979e-06],
        [-7.5102e-06, -5.9158e-06,  4.5896e-06,  ..., -6.5714e-06,
         -3.9488e-06, -5.1856e-06],
        [-7.4208e-06, -5.6624e-06,  4.5449e-06,  ..., -6.5416e-06,
         -3.9339e-06, -5.4091e-06],
        [-8.2552e-06, -6.6757e-06,  5.0068e-06,  ..., -7.3165e-06,
         -4.6492e-06, -5.3197e-06]], device='cuda:0')
Loss: 1.0397398471832275


Running epoch 0, step 970, batch 970
Sampled inputs[:2]: tensor([[   0,   12,  895,  ...,   13, 2900,   14],
        [   0, 7110,  437,  ...,  266, 6724, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3086e-05,  8.7368e-05,  3.6585e-05,  ...,  6.6689e-05,
         -2.8006e-05,  8.9982e-05],
        [-4.5747e-06, -3.6210e-06,  2.7642e-06,  ..., -4.0978e-06,
         -2.3954e-06, -3.2112e-06],
        [-1.1116e-05, -8.9556e-06,  7.0930e-06,  ..., -9.8944e-06,
         -5.6997e-06, -7.6145e-06],
        [-1.1161e-05, -8.7321e-06,  7.1377e-06,  ..., -1.0014e-05,
         -5.7518e-06, -8.0466e-06],
        [-1.2159e-05, -1.0103e-05,  7.7039e-06,  ..., -1.0982e-05,
         -6.7204e-06, -7.7933e-06]], device='cuda:0')
Loss: 1.021795392036438


Running epoch 0, step 971, batch 971
Sampled inputs[:2]: tensor([[    0, 26138,    17,  ...,   401,  1867,  4977],
        [    0,  3412,  1707,  ..., 11114,    15,  1821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8658e-04,  1.0435e-05,  5.0832e-05,  ...,  6.1263e-05,
         -1.9833e-05, -8.3671e-06],
        [-6.1020e-06, -4.7907e-06,  3.7029e-06,  ..., -5.4687e-06,
         -3.2261e-06, -4.2245e-06],
        [-1.4752e-05, -1.1832e-05,  9.4473e-06,  ..., -1.3158e-05,
         -7.6666e-06, -9.9987e-06],
        [-1.4916e-05, -1.1623e-05,  9.5665e-06,  ..., -1.3396e-05,
         -7.8082e-06, -1.0610e-05],
        [-1.6212e-05, -1.3426e-05,  1.0341e-05,  ..., -1.4693e-05,
         -9.0748e-06, -1.0267e-05]], device='cuda:0')
Loss: 1.03218674659729


Running epoch 0, step 972, batch 972
Sampled inputs[:2]: tensor([[    0,  5160,   278,  ...,   496,    14, 46919],
        [    0,    12,   344,  ...,  2337,  1122,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9468e-04,  2.4894e-05, -2.6530e-05,  ...,  2.0732e-04,
         -9.1776e-05, -5.4530e-05],
        [-7.6294e-06, -5.9754e-06,  4.6790e-06,  ..., -6.8173e-06,
         -3.9600e-06, -5.1744e-06],
        [-1.8328e-05, -1.4693e-05,  1.1861e-05,  ..., -1.6317e-05,
         -9.3505e-06, -1.2189e-05],
        [-1.8761e-05, -1.4588e-05,  1.2144e-05,  ..., -1.6794e-05,
         -9.6187e-06, -1.3098e-05],
        [-2.0176e-05, -1.6689e-05,  1.2979e-05,  ..., -1.8224e-05,
         -1.1101e-05, -1.2502e-05]], device='cuda:0')
Loss: 1.0374069213867188


Running epoch 0, step 973, batch 973
Sampled inputs[:2]: tensor([[    0,   301,   298,  ...,   806,   352, 22105],
        [    0,   767,  4478,  ...,   278,   266, 19201]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3015e-04, -4.1212e-05, -1.3165e-04,  ...,  1.1371e-04,
         -2.1673e-04, -1.6073e-04],
        [-9.2089e-06, -7.2122e-06,  5.5693e-06,  ..., -8.2105e-06,
         -4.7944e-06, -6.2920e-06],
        [-2.2173e-05, -1.7807e-05,  1.4201e-05,  ..., -1.9729e-05,
         -1.1392e-05, -1.4886e-05],
        [-2.2516e-05, -1.7509e-05,  1.4395e-05,  ..., -2.0117e-05,
         -1.1586e-05, -1.5825e-05],
        [-2.4319e-05, -2.0146e-05,  1.5497e-05,  ..., -2.1964e-05,
         -1.3456e-05, -1.5199e-05]], device='cuda:0')
Loss: 0.9907913208007812


Running epoch 0, step 974, batch 974
Sampled inputs[:2]: tensor([[   0, 1270,  413,  ...,  413,  711,   14],
        [   0, 7692,   12,  ...,  266, 2042,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5451e-04,  2.5706e-05, -1.5444e-04,  ...,  1.8170e-04,
         -2.1673e-04, -3.2988e-04],
        [-1.0721e-05, -8.4266e-06,  6.4261e-06,  ..., -9.5516e-06,
         -5.6140e-06, -7.3500e-06],
        [-2.5958e-05, -2.0906e-05,  1.6481e-05,  ..., -2.3052e-05,
         -1.3389e-05, -1.7449e-05],
        [-2.6271e-05, -2.0459e-05,  1.6674e-05,  ..., -2.3440e-05,
         -1.3582e-05, -1.8537e-05],
        [-2.8372e-05, -2.3574e-05,  1.7941e-05,  ..., -2.5570e-05,
         -1.5765e-05, -1.7732e-05]], device='cuda:0')
Loss: 0.9842059016227722


Running epoch 0, step 975, batch 975
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,    20, 13016,  ...,    14,  2743,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8402e-04,  1.8593e-04, -2.0214e-04,  ...,  3.6460e-04,
         -2.8510e-04, -4.6130e-04],
        [-1.2316e-05, -9.6038e-06,  7.2159e-06,  ..., -1.0982e-05,
         -6.5453e-06, -8.5793e-06],
        [-2.9832e-05, -2.3827e-05,  1.8597e-05,  ..., -2.6479e-05,
         -1.5564e-05, -2.0355e-05],
        [-3.0071e-05, -2.3186e-05,  1.8686e-05,  ..., -2.6807e-05,
         -1.5743e-05, -2.1517e-05],
        [-3.2723e-05, -2.6926e-05,  2.0340e-05,  ..., -2.9445e-05,
         -1.8358e-05, -2.0742e-05]], device='cuda:0')
Loss: 0.9932298064231873
Graident accumulation at epoch 0, step 975, batch 975
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0191],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6688e-05,  1.7140e-04, -1.1782e-04,  ...,  1.6586e-04,
          3.3356e-05,  3.5232e-05],
        [-9.0000e-06, -3.3164e-06,  4.0984e-06,  ..., -8.1237e-06,
         -3.3808e-06, -6.6444e-06],
        [ 4.9592e-05,  5.4213e-05, -2.9398e-05,  ...,  5.3760e-05,
          4.2869e-05,  9.8599e-06],
        [-4.0231e-07,  8.5800e-06, -3.6997e-06,  ...,  1.9090e-06,
          9.2812e-06, -1.6918e-06],
        [-2.5690e-05, -1.7124e-05,  1.5197e-05,  ..., -2.0562e-05,
         -1.3542e-05, -1.7396e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2888e-08, 5.0623e-08, 3.7099e-08,  ..., 3.7909e-08, 1.1283e-07,
         4.4047e-08],
        [5.9575e-11, 4.3347e-11, 7.6133e-12,  ..., 4.3724e-11, 1.2084e-11,
         1.4380e-11],
        [2.5214e-09, 1.3064e-09, 4.6542e-10,  ..., 1.9630e-09, 4.3353e-10,
         7.0097e-10],
        [7.5993e-10, 5.8791e-10, 1.1621e-10,  ..., 6.3112e-10, 1.5351e-10,
         2.7039e-10],
        [3.0245e-10, 1.7819e-10, 3.9715e-11,  ..., 2.2923e-10, 5.0023e-11,
         7.6090e-11]], device='cuda:0')
optimizer state dict: 122.0
lr: [1.1591565787821919e-05, 1.1591565787821919e-05]
scheduler_last_epoch: 122


Running epoch 0, step 976, batch 976
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  381, 3513, 1501],
        [   0,  560,  199,  ...,   29,  445,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3423e-05,  1.2337e-04, -1.2698e-04,  ...,  1.4269e-05,
          1.6903e-04, -1.4201e-04],
        [-1.5870e-06, -1.1921e-06,  9.0525e-07,  ..., -1.3486e-06,
         -8.3819e-07, -1.0803e-06],
        [ 7.2688e-04,  7.1174e-04, -4.3896e-04,  ...,  7.1261e-04,
          2.8219e-04,  3.7471e-04],
        [-3.9339e-06, -2.9802e-06,  2.3544e-06,  ..., -3.3826e-06,
         -2.1160e-06, -2.7716e-06],
        [-4.0233e-06, -3.2634e-06,  2.4587e-06,  ..., -3.5018e-06,
         -2.2948e-06, -2.5183e-06]], device='cuda:0')
Loss: 1.0098775625228882


Running epoch 0, step 977, batch 977
Sampled inputs[:2]: tensor([[    0,   278, 11554,  ...,  4713,  1039, 17088],
        [    0,    13,  4831,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0643e-05,  1.5569e-04, -1.9555e-04,  ...,  6.9369e-05,
          1.2168e-04, -7.7392e-05],
        [-3.0547e-06, -2.3618e-06,  1.8142e-06,  ..., -2.6599e-06,
         -1.6466e-06, -2.1458e-06],
        [ 7.2332e-04,  7.0882e-04, -4.3661e-04,  ...,  7.0944e-04,
          2.8027e-04,  3.7218e-04],
        [-7.5400e-06, -5.8115e-06,  4.7237e-06,  ..., -6.5863e-06,
         -4.0531e-06, -5.4687e-06],
        [-7.7933e-06, -6.4373e-06,  4.9323e-06,  ..., -6.9141e-06,
         -4.4852e-06, -4.9919e-06]], device='cuda:0')
Loss: 0.9902169704437256


Running epoch 0, step 978, batch 978
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   352, 10572,   345],
        [    0,  2328,   271,  ...,   706,    13,  8961]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0923e-04,  1.3813e-04, -3.2348e-04,  ...,  8.9819e-05,
          1.5304e-04, -4.9025e-05],
        [-4.5896e-06, -3.5092e-06,  2.7381e-06,  ..., -3.9712e-06,
         -2.4624e-06, -3.2261e-06],
        [ 7.1967e-04,  7.0600e-04, -4.3427e-04,  ...,  7.0634e-04,
          2.7837e-04,  3.6966e-04],
        [-1.1355e-05, -8.6278e-06,  7.1377e-06,  ..., -9.8199e-06,
         -6.0350e-06, -8.2552e-06],
        [-1.1787e-05, -9.6411e-06,  7.4804e-06,  ..., -1.0371e-05,
         -6.7353e-06, -7.5251e-06]], device='cuda:0')
Loss: 0.9999772310256958


Running epoch 0, step 979, batch 979
Sampled inputs[:2]: tensor([[   0,  298, 2230,  ..., 2300, 3698, 4764],
        [   0, 9058, 4048,  ...,   14,  759, 1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5029e-04,  1.4309e-04, -2.4456e-04,  ..., -6.2288e-05,
          1.3238e-04, -1.8918e-05],
        [-6.1318e-06, -4.6864e-06,  3.6731e-06,  ..., -5.3644e-06,
         -3.2671e-06, -4.3362e-06],
        [ 7.1593e-04,  7.0308e-04, -4.3187e-04,  ...,  7.0297e-04,
          2.7643e-04,  3.6698e-04],
        [-1.5110e-05, -1.1489e-05,  9.5814e-06,  ..., -1.3217e-05,
         -7.9572e-06, -1.1027e-05],
        [-1.5751e-05, -1.2830e-05,  9.9987e-06,  ..., -1.3992e-05,
         -8.9556e-06, -1.0192e-05]], device='cuda:0')
Loss: 1.0024992227554321


Running epoch 0, step 980, batch 980
Sampled inputs[:2]: tensor([[   0,  391, 1761,  ...,  346,   14,  292],
        [   0,  560,  199,  ...,  266, 1371, 4811]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6188e-04, -1.1144e-04, -4.1408e-04,  ..., -2.6298e-04,
          1.1006e-04, -2.2167e-04],
        [-7.6219e-06, -5.8636e-06,  4.6268e-06,  ..., -6.6906e-06,
         -4.0345e-06, -5.3048e-06],
        [ 7.1241e-04,  7.0022e-04, -4.2947e-04,  ...,  6.9983e-04,
          2.7464e-04,  3.6469e-04],
        [-1.8865e-05, -1.4469e-05,  1.2115e-05,  ..., -1.6585e-05,
         -9.8795e-06, -1.3590e-05],
        [-1.9446e-05, -1.5974e-05,  1.2502e-05,  ..., -1.7375e-05,
         -1.1012e-05, -1.2428e-05]], device='cuda:0')
Loss: 1.0231951475143433


Running epoch 0, step 981, batch 981
Sampled inputs[:2]: tensor([[   0, 1781,  659,  ...,   12, 1478,   14],
        [   0,   13,  711,  ...,  591,  953,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4251e-04, -9.4388e-05, -2.6413e-04,  ..., -3.4995e-04,
          5.3567e-05, -1.2092e-04],
        [-9.1568e-06, -7.0632e-06,  5.4874e-06,  ..., -8.0168e-06,
         -4.9174e-06, -6.4299e-06],
        [ 7.0857e-04,  6.9717e-04, -4.2716e-04,  ...,  6.9652e-04,
          2.7248e-04,  3.6194e-04],
        [-2.2665e-05, -1.7390e-05,  1.4395e-05,  ..., -1.9863e-05,
         -1.2040e-05, -1.6436e-05],
        [-2.3529e-05, -1.9297e-05,  1.4976e-05,  ..., -2.0921e-05,
         -1.3441e-05, -1.5140e-05]], device='cuda:0')
Loss: 0.9939647316932678


Running epoch 0, step 982, batch 982
Sampled inputs[:2]: tensor([[    0,   759,  4585,  ...,   360,   300,   670],
        [    0,  1278,    69,  ...,    15,  7377, 20524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3557e-04, -6.8061e-05, -2.9209e-04,  ..., -3.7380e-04,
          3.2787e-05, -1.6733e-04],
        [-1.0587e-05, -8.2254e-06,  6.4559e-06,  ..., -9.3132e-06,
         -5.6438e-06, -7.3239e-06],
        [ 7.0517e-04,  6.9432e-04, -4.2473e-04,  ...,  6.9344e-04,
          2.7076e-04,  3.5983e-04],
        [-2.6241e-05, -2.0295e-05,  1.6943e-05,  ..., -2.3127e-05,
         -1.3851e-05, -1.8775e-05],
        [-2.7090e-05, -2.2382e-05,  1.7509e-05,  ..., -2.4229e-05,
         -1.5393e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.0201472043991089


Running epoch 0, step 983, batch 983
Sampled inputs[:2]: tensor([[   0,  635,   13,  ...,  292,   20,  445],
        [   0, 9029,  634,  ..., 1424, 6872,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3517e-04, -4.8797e-05, -1.9276e-04,  ..., -3.2742e-04,
          2.6955e-05,  4.0115e-05],
        [-1.2100e-05, -9.3132e-06,  7.3016e-06,  ..., -1.0647e-05,
         -6.4187e-06, -8.5011e-06],
        [ 7.0155e-04,  6.9165e-04, -4.2253e-04,  ...,  6.9029e-04,
          2.6897e-04,  3.5712e-04],
        [-2.9996e-05, -2.2948e-05,  1.9178e-05,  ..., -2.6405e-05,
         -1.5721e-05, -2.1741e-05],
        [-3.0994e-05, -2.5317e-05,  1.9878e-05,  ..., -2.7657e-05,
         -1.7434e-05, -1.9893e-05]], device='cuda:0')
Loss: 0.9790346622467041
Graident accumulation at epoch 0, step 983, batch 983
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0060, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0027, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8536e-05,  1.4938e-04, -1.2531e-04,  ...,  1.1654e-04,
          3.2716e-05,  3.5720e-05],
        [-9.3100e-06, -3.9161e-06,  4.4187e-06,  ..., -8.3760e-06,
         -3.6846e-06, -6.8301e-06],
        [ 1.1479e-04,  1.1796e-04, -6.8711e-05,  ...,  1.1741e-04,
          6.5479e-05,  4.4585e-05],
        [-3.3617e-06,  5.4272e-06, -1.4120e-06,  ..., -9.2239e-07,
          6.7810e-06, -3.6967e-06],
        [-2.6221e-05, -1.7943e-05,  1.5665e-05,  ..., -2.1272e-05,
         -1.3931e-05, -1.7646e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2927e-08, 5.0575e-08, 3.7099e-08,  ..., 3.7979e-08, 1.1272e-07,
         4.4005e-08],
        [5.9662e-11, 4.3391e-11, 7.6590e-12,  ..., 4.3794e-11, 1.2113e-11,
         1.4438e-11],
        [3.0110e-09, 1.7835e-09, 6.4349e-10,  ..., 2.4375e-09, 5.0545e-10,
         8.2780e-10],
        [7.6007e-10, 5.8785e-10, 1.1646e-10,  ..., 6.3119e-10, 1.5360e-10,
         2.7059e-10],
        [3.0311e-10, 1.7865e-10, 4.0070e-11,  ..., 2.2977e-10, 5.0276e-11,
         7.6409e-11]], device='cuda:0')
optimizer state dict: 123.0
lr: [1.1469406375747185e-05, 1.1469406375747185e-05]
scheduler_last_epoch: 123


Running epoch 0, step 984, batch 984
Sampled inputs[:2]: tensor([[    0,  3615,    16,  ...,  2140,  1098,   352],
        [    0, 13081,   278,  ...,   368,   266,  1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2336e-06, -2.5021e-04, -1.5744e-04,  ..., -4.9195e-05,
          2.3404e-04,  6.9118e-05],
        [-1.4752e-06, -1.1474e-06,  9.9838e-07,  ..., -1.3039e-06,
         -7.4878e-07, -1.0058e-06],
        [-3.5465e-06, -2.8163e-06,  2.5034e-06,  ..., -3.0994e-06,
         -1.7434e-06, -2.3991e-06],
        [-3.8147e-06, -2.9057e-06,  2.6971e-06,  ..., -3.3528e-06,
         -1.8850e-06, -2.7120e-06],
        [-3.6061e-06, -2.9653e-06,  2.5183e-06,  ..., -3.2187e-06,
         -1.9222e-06, -2.2501e-06]], device='cuda:0')
Loss: 1.0090806484222412


Running epoch 0, step 985, batch 985
Sampled inputs[:2]: tensor([[    0,  1890,   278,  ...,   578,    72,   815],
        [    0,   360,  5323,  ..., 29974,    25,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8923e-05, -3.2653e-04, -2.2165e-04,  ..., -4.4258e-05,
          3.2986e-04,  1.2941e-04],
        [-2.9355e-06, -2.2203e-06,  2.0266e-06,  ..., -2.5332e-06,
         -1.4156e-06, -1.9744e-06],
        [-6.9588e-06, -5.3793e-06,  5.0217e-06,  ..., -5.9605e-06,
         -3.2485e-06, -4.6343e-06],
        [-7.6890e-06, -5.7071e-06,  5.5730e-06,  ..., -6.6161e-06,
         -3.5912e-06, -5.4091e-06],
        [-7.1228e-06, -5.7220e-06,  5.0813e-06,  ..., -6.2287e-06,
         -3.6508e-06, -4.3660e-06]], device='cuda:0')
Loss: 0.9922332763671875


Running epoch 0, step 986, batch 986
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0,   271,   259,  ...,  4511,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0792e-04, -9.1793e-05, -2.8137e-05,  ..., -1.5011e-04,
          5.5819e-04,  4.1197e-04],
        [-4.5225e-06, -3.5018e-06,  2.9169e-06,  ..., -3.9041e-06,
         -2.3767e-06, -3.0473e-06],
        [-1.0684e-05, -8.4788e-06,  7.2718e-06,  ..., -9.1642e-06,
         -5.4836e-06, -7.1079e-06],
        [-1.1683e-05, -8.9109e-06,  7.9274e-06,  ..., -1.0028e-05,
         -6.0052e-06, -8.1807e-06],
        [-1.1146e-05, -9.1493e-06,  7.5102e-06,  ..., -9.7454e-06,
         -6.2138e-06, -6.8545e-06]], device='cuda:0')
Loss: 1.0087543725967407


Running epoch 0, step 987, batch 987
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,    13,   635,    13],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2064e-05, -1.9371e-04, -4.8573e-05,  ..., -6.7329e-05,
          5.7199e-04,  3.1654e-04],
        [-5.9754e-06, -4.6045e-06,  3.8482e-06,  ..., -5.2005e-06,
         -3.1590e-06, -4.0829e-06],
        [-1.4111e-05, -1.1116e-05,  9.6411e-06,  ..., -1.2144e-05,
         -7.2420e-06, -9.4324e-06],
        [-1.5408e-05, -1.1653e-05,  1.0476e-05,  ..., -1.3307e-05,
         -7.9572e-06, -1.0863e-05],
        [-1.4767e-05, -1.2010e-05,  1.0014e-05,  ..., -1.2934e-05,
         -8.2105e-06, -9.0897e-06]], device='cuda:0')
Loss: 0.9909308552742004


Running epoch 0, step 988, batch 988
Sampled inputs[:2]: tensor([[    0,    12,   297,  ...,  2980,  1145, 17207],
        [    0,   257,   221,  ...,  1474,  2044,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4342e-05, -1.1468e-04, -8.6502e-06,  ...,  1.4633e-04,
          5.6614e-04,  3.6323e-04],
        [-7.5400e-06, -5.5879e-06,  4.7050e-06,  ..., -6.6012e-06,
         -4.0419e-06, -5.3495e-06],
        [-1.7717e-05, -1.3471e-05,  1.1802e-05,  ..., -1.5303e-05,
         -9.2089e-06, -1.2249e-05],
        [-1.9282e-05, -1.4067e-05,  1.2770e-05,  ..., -1.6749e-05,
         -1.0118e-05, -1.4052e-05],
        [-1.8671e-05, -1.4648e-05,  1.2383e-05,  ..., -1.6391e-05,
         -1.0446e-05, -1.1921e-05]], device='cuda:0')
Loss: 0.9825130701065063


Running epoch 0, step 989, batch 989
Sampled inputs[:2]: tensor([[    0,   221,   451,  ...,   741, 25712,   950],
        [    0,  4823,    12,  ...,  1756,  3406,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7878e-05,  2.4759e-05,  6.0248e-05,  ...,  2.2774e-04,
          4.4008e-04,  2.5404e-04],
        [-8.9705e-06, -6.5938e-06,  5.5544e-06,  ..., -7.9349e-06,
         -4.8690e-06, -6.4522e-06],
        [ 1.9373e-04,  2.2399e-04, -1.7524e-04,  ...,  1.4727e-04,
          2.1613e-04,  5.7203e-05],
        [-2.3082e-05, -1.6630e-05,  1.5154e-05,  ..., -2.0221e-05,
         -1.2219e-05, -1.7047e-05],
        [-2.2292e-05, -1.7300e-05,  1.4737e-05,  ..., -1.9670e-05,
         -1.2517e-05, -1.4365e-05]], device='cuda:0')
Loss: 0.9877452254295349


Running epoch 0, step 990, batch 990
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,  271, 4111, 1188],
        [   0,   13, 6913,  ...,  278, 1317, 4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3534e-05,  1.3884e-04,  1.7710e-04,  ...,  2.7926e-04,
          3.8557e-04,  2.3753e-04],
        [-1.0349e-05, -7.6517e-06,  6.4746e-06,  ..., -9.2238e-06,
         -5.5954e-06, -7.3463e-06],
        [ 1.9047e-04,  2.2144e-04, -1.7293e-04,  ...,  1.4427e-04,
          2.1448e-04,  5.5147e-05],
        [-2.6733e-05, -1.9357e-05,  1.7717e-05,  ..., -2.3618e-05,
         -1.4082e-05, -1.9506e-05],
        [-2.5705e-05, -2.0057e-05,  1.7136e-05,  ..., -2.2829e-05,
         -1.4380e-05, -1.6317e-05]], device='cuda:0')
Loss: 1.0134012699127197


Running epoch 0, step 991, batch 991
Sampled inputs[:2]: tensor([[    0,  3611, 10765,  ...,   271,  4317,    13],
        [    0,   278,   266,  ...,  5503,   259,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2753e-05,  1.1858e-04,  9.8329e-05,  ...,  3.2523e-04,
          4.8009e-04,  4.1223e-04],
        [-1.1981e-05, -8.7544e-06,  7.3947e-06,  ..., -1.0602e-05,
         -6.4597e-06, -8.6129e-06],
        [ 1.8653e-04,  2.1871e-04, -1.7053e-04,  ...,  1.4100e-04,
          2.1244e-04,  5.2196e-05],
        [-3.0726e-05, -2.2024e-05,  2.0131e-05,  ..., -2.6956e-05,
         -1.6138e-05, -2.2590e-05],
        [-2.9936e-05, -2.3067e-05,  1.9729e-05,  ..., -2.6420e-05,
         -1.6734e-05, -1.9297e-05]], device='cuda:0')
Loss: 0.994516134262085
Graident accumulation at epoch 0, step 991, batch 991
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9407e-05,  1.4630e-04, -1.0295e-04,  ...,  1.3741e-04,
          7.7453e-05,  7.3371e-05],
        [-9.5770e-06, -4.3999e-06,  4.7163e-06,  ..., -8.5986e-06,
         -3.9621e-06, -7.0084e-06],
        [ 1.2196e-04,  1.2803e-04, -7.8893e-05,  ...,  1.1977e-04,
          8.0175e-05,  4.5347e-05],
        [-6.0981e-06,  2.6821e-06,  7.4235e-07,  ..., -3.5258e-06,
          4.4891e-06, -5.5860e-06],
        [-2.6592e-05, -1.8456e-05,  1.6072e-05,  ..., -2.1786e-05,
         -1.4211e-05, -1.7811e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2856e-08, 5.0538e-08, 3.7071e-08,  ..., 3.8047e-08, 1.1284e-07,
         4.4130e-08],
        [5.9745e-11, 4.3424e-11, 7.7060e-12,  ..., 4.3862e-11, 1.2142e-11,
         1.4498e-11],
        [3.0428e-09, 1.8295e-09, 6.7192e-10,  ..., 2.4550e-09, 5.5007e-10,
         8.2970e-10],
        [7.6026e-10, 5.8774e-10, 1.1675e-10,  ..., 6.3128e-10, 1.5371e-10,
         2.7083e-10],
        [3.0371e-10, 1.7900e-10, 4.0420e-11,  ..., 2.3024e-10, 5.0506e-11,
         7.6705e-11]], device='cuda:0')
optimizer state dict: 124.0
lr: [1.1347022425551613e-05, 1.1347022425551613e-05]
scheduler_last_epoch: 124


Running epoch 0, step 992, batch 992
Sampled inputs[:2]: tensor([[    0,  2258, 10315,  ...,  4185,  9433,   221],
        [    0,    17,  4110,  ...,   287,  7115,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7482e-05, -3.1004e-05,  4.5151e-06,  ..., -7.8371e-05,
          1.5535e-05,  4.7632e-05],
        [-1.4305e-06, -1.1101e-06,  1.0878e-06,  ..., -1.2293e-06,
         -7.1153e-07, -8.6054e-07],
        [-3.4124e-06, -2.6971e-06,  2.6673e-06,  ..., -2.9206e-06,
         -1.6764e-06, -2.0564e-06],
        [-3.9041e-06, -2.9951e-06,  3.0845e-06,  ..., -3.3379e-06,
         -1.8999e-06, -2.4438e-06],
        [-3.3826e-06, -2.7716e-06,  2.6226e-06,  ..., -2.9504e-06,
         -1.8105e-06, -1.8850e-06]], device='cuda:0')
Loss: 1.0459098815917969


Running epoch 0, step 993, batch 993
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,  266, 4505,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5184e-05, -1.5067e-04, -2.6876e-04,  ..., -2.5740e-04,
          1.8619e-04,  1.1421e-04],
        [-2.8759e-06, -2.2203e-06,  2.1830e-06,  ..., -2.4810e-06,
         -1.4193e-06, -1.7993e-06],
        [-6.7800e-06, -5.3048e-06,  5.3644e-06,  ..., -5.8115e-06,
         -3.2857e-06, -4.2319e-06],
        [-7.8678e-06, -5.9754e-06,  6.2436e-06,  ..., -6.7353e-06,
         -3.7849e-06, -5.0962e-06],
        [-6.7949e-06, -5.4985e-06,  5.3197e-06,  ..., -5.9307e-06,
         -3.5986e-06, -3.9116e-06]], device='cuda:0')
Loss: 1.0141502618789673


Running epoch 0, step 994, batch 994
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0, 4120,  278,  ...,  298,  273,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8516e-05, -3.1115e-04, -3.6731e-04,  ..., -4.2150e-04,
          2.9987e-04,  1.0576e-04],
        [-4.2617e-06, -3.3155e-06,  3.1441e-06,  ..., -3.6955e-06,
         -2.1011e-06, -2.7530e-06],
        [-1.0028e-05, -7.8678e-06,  7.7635e-06,  ..., -8.5980e-06,
         -4.8056e-06, -6.3926e-06],
        [-1.1548e-05, -8.7917e-06,  8.9407e-06,  ..., -9.9242e-06,
         -5.5283e-06, -7.6890e-06],
        [-1.0028e-05, -8.1360e-06,  7.6890e-06,  ..., -8.7470e-06,
         -5.2527e-06, -5.8785e-06]], device='cuda:0')
Loss: 1.0012719631195068


Running epoch 0, step 995, batch 995
Sampled inputs[:2]: tensor([[    0,  2426,   699,  ...,   221,  1551,   720],
        [    0,  6904,  6069,  ..., 17196,   471,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9204e-05, -3.1420e-04, -2.0144e-04,  ..., -3.8681e-04,
          4.0900e-04,  2.0190e-04],
        [ 5.8215e-05,  6.7733e-05, -6.3534e-05,  ...,  6.3114e-05,
          7.8401e-05,  7.2818e-05],
        [-1.3635e-05, -1.0446e-05,  1.0103e-05,  ..., -1.1683e-05,
         -6.8322e-06, -9.0748e-06],
        [-1.5453e-05, -1.1459e-05,  1.1414e-05,  ..., -1.3292e-05,
         -7.7635e-06, -1.0744e-05],
        [-1.3843e-05, -1.0952e-05,  1.0163e-05,  ..., -1.2055e-05,
         -7.5325e-06, -8.5011e-06]], device='cuda:0')
Loss: 1.0040955543518066


Running epoch 0, step 996, batch 996
Sampled inputs[:2]: tensor([[    0,  7712, 31756,  ...,   895,   360,   630],
        [    0,  5841,   328,  ...,  2051,   266,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5667e-05, -3.3347e-04, -1.8285e-04,  ..., -4.3283e-04,
          3.2640e-04,  2.2009e-04],
        [ 5.6763e-05,  6.6705e-05, -6.2491e-05,  ...,  6.1862e-05,
          7.7742e-05,  7.1898e-05],
        [-1.6972e-05, -1.2875e-05,  1.2621e-05,  ..., -1.4544e-05,
         -8.3148e-06, -1.1176e-05],
        [-1.9476e-05, -1.4246e-05,  1.4454e-05,  ..., -1.6749e-05,
         -9.5367e-06, -1.3411e-05],
        [-1.7226e-05, -1.3530e-05,  1.2696e-05,  ..., -1.5020e-05,
         -9.1940e-06, -1.0468e-05]], device='cuda:0')
Loss: 1.0030196905136108


Running epoch 0, step 997, batch 997
Sampled inputs[:2]: tensor([[   0,  953,  328,  ..., 2245,   12, 1253],
        [   0, 3408,  300,  ..., 3868,  300, 2932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5667e-05, -3.2336e-04, -1.0174e-04,  ..., -4.7918e-04,
          4.8407e-04,  3.6734e-04],
        [ 5.5258e-05,  6.5625e-05, -6.1462e-05,  ...,  6.0573e-05,
          7.7053e-05,  7.0937e-05],
        [-2.0459e-05, -1.5453e-05,  1.5110e-05,  ..., -1.7509e-05,
         -9.8869e-06, -1.3366e-05],
        [-2.3410e-05, -1.7002e-05,  1.7256e-05,  ..., -2.0087e-05,
         -1.1288e-05, -1.6004e-05],
        [-2.0757e-05, -1.6242e-05,  1.5199e-05,  ..., -1.8090e-05,
         -1.0937e-05, -1.2510e-05]], device='cuda:0')
Loss: 1.0154632329940796


Running epoch 0, step 998, batch 998
Sampled inputs[:2]: tensor([[    0,  1526,  3502,  ..., 11727,  3736,  1661],
        [    0,  2546,   300,  ...,    14,  1075,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0983e-04, -2.3552e-04, -1.1775e-04,  ..., -4.9312e-04,
          5.3957e-04,  4.7939e-04],
        [ 5.3678e-05,  6.4507e-05, -6.0553e-05,  ...,  5.9180e-05,
          7.6125e-05,  6.9797e-05],
        [-2.4155e-05, -1.8135e-05,  1.7405e-05,  ..., -2.0713e-05,
         -1.1988e-05, -1.5944e-05],
        [-2.7493e-05, -1.9848e-05,  1.9729e-05,  ..., -2.3663e-05,
         -1.3642e-05, -1.9029e-05],
        [-2.4661e-05, -1.9163e-05,  1.7628e-05,  ..., -2.1532e-05,
         -1.3292e-05, -1.5058e-05]], device='cuda:0')
Loss: 1.003920078277588


Running epoch 0, step 999, batch 999
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0, 25409,   287,  ...,  1005,   344,  3493]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3279e-04, -2.1703e-04, -1.7364e-04,  ..., -5.0512e-04,
          4.1637e-04,  4.9773e-04],
        [ 5.2270e-05,  6.3375e-05, -5.9592e-05,  ...,  5.7906e-05,
          7.5350e-05,  6.8843e-05],
        [-2.7433e-05, -2.0817e-05,  1.9789e-05,  ..., -2.3648e-05,
         -1.3724e-05, -1.8120e-05],
        [-3.1233e-05, -2.2784e-05,  2.2426e-05,  ..., -2.7031e-05,
         -1.5654e-05, -2.1666e-05],
        [-2.7925e-05, -2.1920e-05,  1.9982e-05,  ..., -2.4483e-05,
         -1.5154e-05, -1.7039e-05]], device='cuda:0')
Loss: 1.0049723386764526
Graident accumulation at epoch 0, step 999, batch 999
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2188e-05,  1.0997e-04, -1.1002e-04,  ...,  7.3153e-05,
          1.1135e-04,  1.1581e-04],
        [-3.3924e-06,  2.3776e-06, -1.7145e-06,  ..., -1.9482e-06,
          3.9691e-06,  5.7682e-07],
        [ 1.0702e-04,  1.1315e-04, -6.9025e-05,  ...,  1.0543e-04,
          7.0785e-05,  3.9000e-05],
        [-8.6116e-06,  1.3547e-07,  2.9107e-06,  ..., -5.8763e-06,
          2.4748e-06, -7.1940e-06],
        [-2.6725e-05, -1.8802e-05,  1.6463e-05,  ..., -2.2056e-05,
         -1.4306e-05, -1.7734e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2837e-08, 5.0535e-08, 3.7064e-08,  ..., 3.8264e-08, 1.1290e-07,
         4.4334e-08],
        [6.2418e-11, 4.7397e-11, 1.1250e-11,  ..., 4.7172e-11, 1.7808e-11,
         1.9223e-11],
        [3.0405e-09, 1.8281e-09, 6.7164e-10,  ..., 2.4531e-09, 5.4971e-10,
         8.2919e-10],
        [7.6047e-10, 5.8768e-10, 1.1714e-10,  ..., 6.3138e-10, 1.5380e-10,
         2.7103e-10],
        [3.0418e-10, 1.7931e-10, 4.0779e-11,  ..., 2.3061e-10, 5.0685e-11,
         7.6919e-11]], device='cuda:0')
optimizer state dict: 125.0
lr: [1.1224432638571088e-05, 1.1224432638571088e-05]
scheduler_last_epoch: 125


Running epoch 0, step 1000, batch 1000
Sampled inputs[:2]: tensor([[    0,   344, 14017,  ...,    65,   298,   634],
        [    0,  3087,   342,  ...,    14,   381,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6662e-06,  1.8644e-05,  8.0905e-06,  ..., -1.3992e-04,
         -1.0657e-04, -1.3752e-04],
        [-1.5199e-06, -1.1250e-06,  1.0580e-06,  ..., -1.3113e-06,
         -8.1956e-07, -9.5367e-07],
        [-3.5018e-06, -2.6524e-06,  2.5481e-06,  ..., -2.9951e-06,
         -1.8477e-06, -2.1905e-06],
        [-4.0233e-06, -2.9355e-06,  2.9057e-06,  ..., -3.4273e-06,
         -2.1011e-06, -2.6226e-06],
        [-3.5167e-06, -2.7567e-06,  2.5481e-06,  ..., -3.0696e-06,
         -2.0117e-06, -2.0415e-06]], device='cuda:0')
Loss: 1.0109330415725708


Running epoch 0, step 1001, batch 1001
Sampled inputs[:2]: tensor([[    0,   199,   769,  ...,   380,   560,   199],
        [    0,  3504,     9,  ...,  7166, 10945,  3119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5264e-04, -9.8789e-06, -3.4862e-05,  ..., -2.5294e-04,
         -7.9115e-06, -8.1468e-05],
        [-3.0920e-06, -2.1979e-06,  2.0564e-06,  ..., -2.5779e-06,
         -1.6317e-06, -2.0415e-06],
        [-7.3463e-06, -5.3495e-06,  5.1111e-06,  ..., -6.0648e-06,
         -3.7551e-06, -4.8280e-06],
        [-8.3148e-06, -5.7966e-06,  5.7369e-06,  ..., -6.8545e-06,
         -4.2468e-06, -5.7071e-06],
        [-7.3314e-06, -5.5134e-06,  5.0813e-06,  ..., -6.1691e-06,
         -4.0531e-06, -4.5002e-06]], device='cuda:0')
Loss: 1.0109175443649292


Running epoch 0, step 1002, batch 1002
Sampled inputs[:2]: tensor([[    0, 15003, 19278,  ...,   287,   847,   328],
        [    0,    15,  4291,  ...,  1685,   278,  2101]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7287e-05,  5.4052e-05, -2.8813e-05,  ..., -3.0320e-04,
          5.1438e-05, -8.2547e-05],
        [-4.6492e-06, -3.4049e-06,  3.1218e-06,  ..., -3.9116e-06,
         -2.4885e-06, -3.0175e-06],
        [-1.0937e-05, -8.1956e-06,  7.6890e-06,  ..., -9.1344e-06,
         -5.6922e-06, -7.0781e-06],
        [-1.2428e-05, -8.9407e-06,  8.6576e-06,  ..., -1.0356e-05,
         -6.4671e-06, -8.4043e-06],
        [-1.0893e-05, -8.4192e-06,  7.6145e-06,  ..., -9.2536e-06,
         -6.0946e-06, -6.5714e-06]], device='cuda:0')
Loss: 1.0101412534713745


Running epoch 0, step 1003, batch 1003
Sampled inputs[:2]: tensor([[    0,   266,  1586,  ...,  1888,  2117,   328],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4661e-04,  5.4206e-05, -5.9624e-05,  ..., -2.9576e-04,
          1.2366e-05, -1.6147e-04],
        [-6.2138e-06, -4.5523e-06,  4.1723e-06,  ..., -5.1782e-06,
         -3.2969e-06, -4.0531e-06],
        [-1.4588e-05, -1.0937e-05,  1.0267e-05,  ..., -1.2070e-05,
         -7.5400e-06, -9.4622e-06],
        [-1.6570e-05, -1.1936e-05,  1.1548e-05,  ..., -1.3694e-05,
         -8.5831e-06, -1.1221e-05],
        [-1.4514e-05, -1.1235e-05,  1.0163e-05,  ..., -1.2234e-05,
         -8.0615e-06, -8.7619e-06]], device='cuda:0')
Loss: 0.9877648949623108


Running epoch 0, step 1004, batch 1004
Sampled inputs[:2]: tensor([[    0,   591, 18622,  ...,   955,  6118,  9191],
        [    0,   292,   474,  ...,   446, 14932,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4443e-04,  8.0549e-05, -1.1389e-05,  ..., -2.1882e-04,
         -8.3373e-05, -1.2447e-04],
        [-7.8008e-06, -5.6326e-06,  5.1856e-06,  ..., -6.5193e-06,
         -4.1015e-06, -5.0366e-06],
        [-1.8209e-05, -1.3471e-05,  1.2726e-05,  ..., -1.5110e-05,
         -9.3281e-06, -1.1683e-05],
        [-2.0742e-05, -1.4737e-05,  1.4350e-05,  ..., -1.7226e-05,
         -1.0654e-05, -1.3918e-05],
        [-1.8150e-05, -1.3873e-05,  1.2606e-05,  ..., -1.5333e-05,
         -9.9838e-06, -1.0863e-05]], device='cuda:0')
Loss: 1.0021955966949463


Running epoch 0, step 1005, batch 1005
Sampled inputs[:2]: tensor([[    0,   369,   726,  ...,   292,   221,   358],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7490e-04,  1.4518e-04, -1.3236e-04,  ..., -1.2742e-04,
         -1.8208e-04, -2.8311e-05],
        [-9.2685e-06, -6.6981e-06,  6.1989e-06,  ..., -7.7859e-06,
         -4.9248e-06, -6.0275e-06],
        [-2.1577e-05, -1.5959e-05,  1.5199e-05,  ..., -1.7971e-05,
         -1.1146e-05, -1.3903e-05],
        [-2.4736e-05, -1.7539e-05,  1.7226e-05,  ..., -2.0623e-05,
         -1.2830e-05, -1.6689e-05],
        [-2.1547e-05, -1.6481e-05,  1.5080e-05,  ..., -1.8269e-05,
         -1.1951e-05, -1.2919e-05]], device='cuda:0')
Loss: 1.0041263103485107


Running epoch 0, step 1006, batch 1006
Sampled inputs[:2]: tensor([[    0,  7994,    12,  ..., 13800,   278,   795],
        [    0,    14, 15670,  ...,  2027,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1461e-05,  3.8529e-04,  6.7925e-05,  ..., -1.3019e-04,
         -3.4721e-04, -2.6111e-04],
        [-1.0751e-05, -7.7635e-06,  7.1973e-06,  ..., -9.1046e-06,
         -5.8189e-06, -7.0557e-06],
        [-2.4945e-05, -1.8448e-05,  1.7628e-05,  ..., -2.0921e-05,
         -1.3128e-05, -1.6153e-05],
        [-2.8640e-05, -2.0280e-05,  2.0012e-05,  ..., -2.4065e-05,
         -1.5169e-05, -1.9476e-05],
        [-2.5034e-05, -1.9133e-05,  1.7598e-05,  ..., -2.1353e-05,
         -1.4096e-05, -1.5080e-05]], device='cuda:0')
Loss: 0.9928250908851624


Running epoch 0, step 1007, batch 1007
Sampled inputs[:2]: tensor([[    0,  5024,  3846,  ...,  5880,  1377,    12],
        [    0, 38460,     9,  ...,   829,   870,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3789e-05,  5.1730e-04,  8.9792e-05,  ..., -2.8229e-04,
         -3.7323e-04, -8.8751e-05],
        [-1.2338e-05, -8.9630e-06,  8.2552e-06,  ..., -1.0461e-05,
         -6.7092e-06, -8.0243e-06],
        [-2.8536e-05, -2.1264e-05,  2.0161e-05,  ..., -2.4006e-05,
         -1.5140e-05, -1.8373e-05],
        [-3.2872e-05, -2.3484e-05,  2.2978e-05,  ..., -2.7701e-05,
         -1.7539e-05, -2.2203e-05],
        [-2.8685e-05, -2.2098e-05,  2.0161e-05,  ..., -2.4542e-05,
         -1.6287e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.018997073173523
Graident accumulation at epoch 0, step 1007, batch 1007
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0039,  ..., -0.0098, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4348e-05,  1.5070e-04, -9.0035e-05,  ...,  3.7609e-05,
          6.2887e-05,  9.5351e-05],
        [-4.2869e-06,  1.2435e-06, -7.1756e-07,  ..., -2.7994e-06,
          2.9013e-06, -2.8329e-07],
        [ 9.3466e-05,  9.9706e-05, -6.0106e-05,  ...,  9.2486e-05,
          6.2193e-05,  3.3263e-05],
        [-1.1038e-05, -2.2265e-06,  4.9174e-06,  ..., -8.0588e-06,
          4.7350e-07, -8.6949e-06],
        [-2.6921e-05, -1.9132e-05,  1.6833e-05,  ..., -2.2305e-05,
         -1.4504e-05, -1.7679e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2766e-08, 5.0752e-08, 3.7035e-08,  ..., 3.8305e-08, 1.1293e-07,
         4.4298e-08],
        [6.2508e-11, 4.7430e-11, 1.1306e-11,  ..., 4.7234e-11, 1.7835e-11,
         1.9268e-11],
        [3.0383e-09, 1.8268e-09, 6.7138e-10,  ..., 2.4512e-09, 5.4939e-10,
         8.2870e-10],
        [7.6079e-10, 5.8764e-10, 1.1755e-10,  ..., 6.3152e-10, 1.5396e-10,
         2.7125e-10],
        [3.0470e-10, 1.7961e-10, 4.1144e-11,  ..., 2.3098e-10, 5.0900e-11,
         7.7137e-11]], device='cuda:0')
optimizer state dict: 126.0
lr: [1.1101655747595168e-05, 1.1101655747595168e-05]
scheduler_last_epoch: 126


Running epoch 0, step 1008, batch 1008
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0,  1594,   586,  ...,    13,   701,   308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4407e-05, -1.4422e-07,  4.2870e-05,  ..., -1.0707e-04,
          1.1391e-05, -1.1599e-06],
        [-1.6093e-06, -1.0878e-06,  1.0431e-06,  ..., -1.3411e-06,
         -9.1270e-07, -1.1250e-06],
        [-3.8445e-06, -2.6524e-06,  2.6375e-06,  ..., -3.1590e-06,
         -2.0862e-06, -2.6673e-06],
        [-4.3213e-06, -2.8610e-06,  2.9355e-06,  ..., -3.5316e-06,
         -2.3395e-06, -3.0994e-06],
        [-3.7849e-06, -2.6971e-06,  2.5779e-06,  ..., -3.1590e-06,
         -2.2203e-06, -2.4438e-06]], device='cuda:0')
Loss: 0.9986447691917419


Running epoch 0, step 1009, batch 1009
Sampled inputs[:2]: tensor([[    0,  1086,  5564,  ..., 29319, 32982,   344],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1197e-04,  1.6149e-04,  1.6345e-04,  ..., -1.0999e-04,
         -4.5551e-05,  8.6989e-05],
        [-3.3155e-06, -2.2501e-06,  1.9483e-06,  ..., -2.7269e-06,
         -1.9409e-06, -2.4438e-06],
        [-7.9274e-06, -5.5432e-06,  4.9621e-06,  ..., -6.4522e-06,
         -4.5002e-06, -5.7817e-06],
        [-8.7023e-06, -5.8413e-06,  5.3942e-06,  ..., -7.0781e-06,
         -4.9472e-06, -6.6012e-06],
        [-7.8380e-06, -5.6624e-06,  4.8876e-06,  ..., -6.4969e-06,
         -4.7684e-06, -5.3495e-06]], device='cuda:0')
Loss: 0.9846776723861694


Running epoch 0, step 1010, batch 1010
Sampled inputs[:2]: tensor([[    0,   287, 14752,  ...,   910, 26097,  1477],
        [    0,   333,   199,  ...,   292,    48,  1792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0419e-04,  4.2733e-04,  3.6988e-04,  ..., -2.1428e-05,
          2.6192e-05,  2.0369e-04],
        [-5.0217e-06, -3.4645e-06,  2.9095e-06,  ..., -4.1425e-06,
         -2.9989e-06, -3.5986e-06],
        [ 6.9252e-05,  7.7483e-05, -1.4628e-05,  ...,  8.3175e-05,
          8.0105e-05,  2.8835e-05],
        [-1.3143e-05, -9.0450e-06,  8.0019e-06,  ..., -1.0788e-05,
         -7.7486e-06, -9.6858e-06],
        [-1.1802e-05, -8.6874e-06,  7.2718e-06,  ..., -9.8646e-06,
         -7.3612e-06, -7.8827e-06]], device='cuda:0')
Loss: 1.0136148929595947


Running epoch 0, step 1011, batch 1011
Sampled inputs[:2]: tensor([[   0,  287,  271,  ..., 5090,  631, 3276],
        [   0,  287, 4579,  ...,  909,   12,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5649e-05,  4.5287e-04,  2.8027e-04,  ..., -2.1428e-05,
         -1.1241e-04,  8.6037e-05],
        [-6.5565e-06, -4.5300e-06,  4.0047e-06,  ..., -5.4464e-06,
         -3.7774e-06, -4.5896e-06],
        [ 6.5676e-05,  7.4995e-05, -1.1946e-05,  ...,  8.0209e-05,
          7.8384e-05,  2.6570e-05],
        [-1.7345e-05, -1.1876e-05,  1.1146e-05,  ..., -1.4290e-05,
         -9.7752e-06, -1.2472e-05],
        [-1.5274e-05, -1.1191e-05,  9.8497e-06,  ..., -1.2800e-05,
         -9.1866e-06, -9.9093e-06]], device='cuda:0')
Loss: 0.9964479207992554


Running epoch 0, step 1012, batch 1012
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  380,  560,  199],
        [   0,   12, 5820,  ...,    5, 2122,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4639e-04,  6.4715e-04,  2.8778e-04,  ..., -1.0079e-04,
         -3.5722e-05,  2.6828e-04],
        [-8.1211e-06, -5.6326e-06,  5.0329e-06,  ..., -6.7875e-06,
         -4.7833e-06, -5.6475e-06],
        [ 6.2174e-05,  7.2432e-05, -9.4874e-06,  ...,  7.7229e-05,
          7.6164e-05,  2.4260e-05],
        [-2.1487e-05, -1.4722e-05,  1.4037e-05,  ..., -1.7807e-05,
         -1.2413e-05, -1.5348e-05],
        [-1.8895e-05, -1.3918e-05,  1.2383e-05,  ..., -1.5914e-05,
         -1.1601e-05, -1.2115e-05]], device='cuda:0')
Loss: 0.9936805963516235


Running epoch 0, step 1013, batch 1013
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0,  5143,  3877,  ...,   292, 44003,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2266e-04,  8.1071e-04,  2.8053e-04,  ..., -1.7513e-04,
          4.5093e-05,  5.2898e-04],
        [-9.7975e-06, -6.8098e-06,  6.0238e-06,  ..., -8.1882e-06,
         -5.7891e-06, -6.9067e-06],
        [ 5.8181e-05,  6.9556e-05, -6.9393e-06,  ...,  7.3921e-05,
          7.3809e-05,  2.1339e-05],
        [-2.5719e-05, -1.7643e-05,  1.6704e-05,  ..., -2.1309e-05,
         -1.4916e-05, -1.8552e-05],
        [-2.2799e-05, -1.6809e-05,  1.4886e-05,  ..., -1.9208e-05,
         -1.4029e-05, -1.4782e-05]], device='cuda:0')
Loss: 0.9711058139801025


Running epoch 0, step 1014, batch 1014
Sampled inputs[:2]: tensor([[   0,  593, 1387,  ...,  508, 8222, 1415],
        [   0,  221,  380,  ..., 5543,  768, 6375]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7755e-04,  8.9501e-04,  3.1647e-04,  ..., -3.1912e-05,
         -1.9999e-04,  4.4940e-04],
        [-1.1362e-05, -7.8157e-06,  7.1488e-06,  ..., -9.4846e-06,
         -6.5081e-06, -7.8902e-06],
        [ 5.4694e-05,  6.7231e-05, -4.3018e-06,  ...,  7.1030e-05,
          7.2222e-05,  1.9179e-05],
        [-2.9981e-05, -2.0340e-05,  1.9908e-05,  ..., -2.4825e-05,
         -1.6853e-05, -2.1324e-05],
        [-2.6196e-05, -1.9163e-05,  1.7419e-05,  ..., -2.2069e-05,
         -1.5721e-05, -1.6734e-05]], device='cuda:0')
Loss: 0.9835476279258728


Running epoch 0, step 1015, batch 1015
Sampled inputs[:2]: tensor([[   0,  292,  474,  ..., 1085,  494, 2665],
        [   0,   12,  287,  ..., 3359, 1751, 5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9677e-04,  9.7300e-04,  2.2545e-04,  ...,  4.6928e-05,
         -2.4870e-04,  4.9733e-04],
        [-1.2934e-05, -8.9258e-06,  8.1994e-06,  ..., -1.0788e-05,
         -7.4133e-06, -8.9332e-06],
        [ 5.1058e-05,  6.4609e-05, -1.7686e-06,  ...,  6.8050e-05,
          7.0181e-05,  1.6795e-05],
        [-3.4213e-05, -2.3291e-05,  2.2873e-05,  ..., -2.8312e-05,
         -1.9267e-05, -2.4214e-05],
        [-2.9787e-05, -2.1860e-05,  1.9923e-05,  ..., -2.5064e-05,
         -1.7866e-05, -1.8924e-05]], device='cuda:0')
Loss: 0.9720267653465271
Graident accumulation at epoch 0, step 1015, batch 1015
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0040,  ..., -0.0098, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0156,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.7633e-06,  2.3293e-04, -5.8487e-05,  ...,  3.8541e-05,
          3.1729e-05,  1.3555e-04],
        [-5.1517e-06,  2.2656e-07,  1.7413e-07,  ..., -3.5983e-06,
          1.8698e-06, -1.1483e-06],
        [ 8.9226e-05,  9.6197e-05, -5.4272e-05,  ...,  9.0042e-05,
          6.2992e-05,  3.1616e-05],
        [-1.3355e-05, -4.3329e-06,  6.7130e-06,  ..., -1.0084e-05,
         -1.5006e-06, -1.0247e-05],
        [-2.7208e-05, -1.9404e-05,  1.7142e-05,  ..., -2.2580e-05,
         -1.4840e-05, -1.7803e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2732e-08, 5.1648e-08, 3.7049e-08,  ..., 3.8269e-08, 1.1288e-07,
         4.4501e-08],
        [6.2612e-11, 4.7462e-11, 1.1362e-11,  ..., 4.7303e-11, 1.7872e-11,
         1.9329e-11],
        [3.0379e-09, 1.8291e-09, 6.7071e-10,  ..., 2.4534e-09, 5.5377e-10,
         8.2816e-10],
        [7.6120e-10, 5.8759e-10, 1.1795e-10,  ..., 6.3169e-10, 1.5417e-10,
         2.7157e-10],
        [3.0528e-10, 1.7991e-10, 4.1500e-11,  ..., 2.3137e-10, 5.1168e-11,
         7.7418e-11]], device='cuda:0')
optimizer state dict: 127.0
lr: [1.0978710514004527e-05, 1.0978710514004527e-05]
scheduler_last_epoch: 127


Running epoch 0, step 1016, batch 1016
Sampled inputs[:2]: tensor([[    0,   273,   298,  ..., 23554,    12,  1530],
        [    0,  1716,  1773,  ...,  5014,    12,   847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6239e-05,  6.8302e-05, -9.3486e-06,  ...,  1.6600e-04,
          5.4639e-05, -3.3815e-05],
        [-1.7881e-06, -1.2293e-06,  9.7603e-07,  ..., -1.4901e-06,
         -1.1027e-06, -1.2666e-06],
        [-4.1425e-06, -2.9802e-06,  2.4140e-06,  ..., -3.4571e-06,
         -2.5779e-06, -2.9057e-06],
        [-4.3809e-06, -3.0100e-06,  2.5034e-06,  ..., -3.6657e-06,
         -2.7120e-06, -3.1590e-06],
        [-4.2617e-06, -3.1441e-06,  2.4736e-06,  ..., -3.6210e-06,
         -2.7865e-06, -2.8461e-06]], device='cuda:0')
Loss: 1.0054824352264404


Running epoch 0, step 1017, batch 1017
Sampled inputs[:2]: tensor([[    0,  2270,  3279,  ...,   380,   475,   768],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2912e-05,  2.2964e-04, -7.0269e-05,  ...,  5.7249e-05,
         -2.7164e-05, -4.8300e-04],
        [-3.3304e-06, -2.3171e-06,  1.9968e-06,  ..., -2.8461e-06,
         -2.0191e-06, -2.2426e-06],
        [-7.6741e-06, -5.5581e-06,  4.8876e-06,  ..., -6.5267e-06,
         -4.5896e-06, -5.1111e-06],
        [-8.4937e-06, -5.8711e-06,  5.3495e-06,  ..., -7.2718e-06,
         -5.1111e-06, -5.9009e-06],
        [-7.7039e-06, -5.7518e-06,  4.8727e-06,  ..., -6.6608e-06,
         -4.8876e-06, -4.8429e-06]], device='cuda:0')
Loss: 1.0020005702972412


Running epoch 0, step 1018, batch 1018
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0,   600,   518,  ...,  3134,   278, 37342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0833e-05,  1.6759e-04, -1.6685e-04,  ..., -4.9012e-05,
         -1.2482e-04, -5.5080e-04],
        [-4.9099e-06, -3.4273e-06,  3.1441e-06,  ..., -4.1425e-06,
         -2.8722e-06, -3.2112e-06],
        [-1.1355e-05, -8.2105e-06,  7.6592e-06,  ..., -9.5367e-06,
         -6.5565e-06, -7.4059e-06],
        [-1.2755e-05, -8.8215e-06,  8.5682e-06,  ..., -1.0744e-05,
         -7.3612e-06, -8.6427e-06],
        [-1.1206e-05, -8.3745e-06,  7.4804e-06,  ..., -9.5963e-06,
         -6.9141e-06, -6.8694e-06]], device='cuda:0')
Loss: 1.0194472074508667


Running epoch 0, step 1019, batch 1019
Sampled inputs[:2]: tensor([[    0,  6408,   391,  ...,   870,   278,   266],
        [    0,   292,   380,  ...,   287, 10086,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0255e-04,  2.5012e-04, -1.5043e-04,  ..., -1.0750e-04,
          2.0334e-04, -3.2858e-04],
        [-6.5714e-06, -4.6119e-06,  4.1500e-06,  ..., -5.5879e-06,
         -3.8929e-06, -4.3213e-06],
        [-1.5169e-05, -1.1027e-05,  1.0133e-05,  ..., -1.2845e-05,
         -8.9109e-06, -9.8944e-06],
        [-1.6987e-05, -1.1817e-05,  1.1250e-05,  ..., -1.4424e-05,
         -9.9838e-06, -1.1533e-05],
        [-1.5005e-05, -1.1250e-05,  9.9540e-06,  ..., -1.2919e-05,
         -9.3430e-06, -9.2089e-06]], device='cuda:0')
Loss: 1.006506323814392


Running epoch 0, step 1020, batch 1020
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,   369,   726,  ...,    83,   409,   729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3593e-04,  2.5012e-04, -2.4958e-04,  ..., -1.1886e-04,
          1.4184e-04, -4.5117e-04],
        [-8.1286e-06, -5.6475e-06,  5.1856e-06,  ..., -6.8769e-06,
         -4.6901e-06, -5.3272e-06],
        [-1.8656e-05, -1.3411e-05,  1.2621e-05,  ..., -1.5676e-05,
         -1.0639e-05, -1.2085e-05],
        [-2.1160e-05, -1.4544e-05,  1.4201e-05,  ..., -1.7837e-05,
         -1.2070e-05, -1.4320e-05],
        [-1.8299e-05, -1.3620e-05,  1.2293e-05,  ..., -1.5646e-05,
         -1.1131e-05, -1.1116e-05]], device='cuda:0')
Loss: 0.995125412940979


Running epoch 0, step 1021, batch 1021
Sampled inputs[:2]: tensor([[    0,  1016,   271,  ...,   461,   616,   993],
        [    0, 18981,    13,  ...,   365,  2714,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6593e-04,  3.1148e-04, -2.2834e-04,  ..., -2.6267e-04,
          2.6630e-04, -4.7997e-04],
        [-9.7677e-06, -6.8545e-06,  6.2510e-06,  ..., -8.2627e-06,
         -5.7332e-06, -6.3255e-06],
        [-2.2367e-05, -1.6212e-05,  1.5169e-05,  ..., -1.8775e-05,
         -1.2949e-05, -1.4335e-05],
        [-2.5421e-05, -1.7658e-05,  1.7092e-05,  ..., -2.1398e-05,
         -1.4722e-05, -1.6987e-05],
        [-2.2084e-05, -1.6540e-05,  1.4886e-05,  ..., -1.8850e-05,
         -1.3620e-05, -1.3247e-05]], device='cuda:0')
Loss: 1.0175085067749023


Running epoch 0, step 1022, batch 1022
Sampled inputs[:2]: tensor([[   0,  266,  996,  ...,  709,  616, 9378],
        [   0, 1530,   17,  ...,  409, 1611,  895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3951e-04,  2.0244e-04, -2.6724e-04,  ..., -4.2506e-04,
          2.1791e-04, -4.1814e-04],
        [-1.1384e-05, -8.0764e-06,  7.4059e-06,  ..., -9.5963e-06,
         -6.6124e-06, -7.3090e-06],
        [-2.6062e-05, -1.9044e-05,  1.7926e-05,  ..., -2.1815e-05,
         -1.4931e-05, -1.6600e-05],
        [-2.9653e-05, -2.0817e-05,  2.0251e-05,  ..., -2.4870e-05,
         -1.6972e-05, -1.9670e-05],
        [-2.5660e-05, -1.9386e-05,  1.7539e-05,  ..., -2.1860e-05,
         -1.5706e-05, -1.5303e-05]], device='cuda:0')
Loss: 1.0254777669906616


Running epoch 0, step 1023, batch 1023
Sampled inputs[:2]: tensor([[   0,   12,  221,  ...,  593,  360,  726],
        [   0,  271,  266,  ..., 1034, 1928,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8244e-04,  1.7226e-04, -3.6611e-04,  ..., -2.8499e-04,
          9.9049e-05, -5.4732e-04],
        [-1.2986e-05, -9.1493e-06,  8.5309e-06,  ..., -1.0885e-05,
         -7.4580e-06, -8.3372e-06],
        [ 5.0769e-05,  1.4874e-05, -3.2380e-05,  ...,  6.1210e-05,
          1.7851e-05,  1.0850e-05],
        [-3.3975e-05, -2.3648e-05,  2.3410e-05,  ..., -2.8312e-05,
         -1.9193e-05, -2.2560e-05],
        [-2.9266e-05, -2.1979e-05,  2.0146e-05,  ..., -2.4796e-05,
         -1.7732e-05, -1.7449e-05]], device='cuda:0')
Loss: 1.0018881559371948
Graident accumulation at epoch 0, step 1023, batch 1023
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0080,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2157e-05,  2.2686e-04, -8.9249e-05,  ...,  6.1876e-06,
          3.8461e-05,  6.7262e-05],
        [-5.9351e-06, -7.1103e-07,  1.0098e-06,  ..., -4.3270e-06,
          9.3704e-07, -1.8672e-06],
        [ 8.5380e-05,  8.8064e-05, -5.2083e-05,  ...,  8.7159e-05,
          5.8477e-05,  2.9539e-05],
        [-1.5417e-05, -6.2644e-06,  8.3827e-06,  ..., -1.1907e-05,
         -3.2698e-06, -1.1478e-05],
        [-2.7414e-05, -1.9662e-05,  1.7442e-05,  ..., -2.2802e-05,
         -1.5129e-05, -1.7768e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2692e-08, 5.1626e-08, 3.7146e-08,  ..., 3.8312e-08, 1.1277e-07,
         4.4756e-08],
        [6.2718e-11, 4.7498e-11, 1.1424e-11,  ..., 4.7374e-11, 1.7910e-11,
         1.9379e-11],
        [3.0374e-09, 1.8275e-09, 6.7109e-10,  ..., 2.4547e-09, 5.5353e-10,
         8.2745e-10],
        [7.6160e-10, 5.8757e-10, 1.1838e-10,  ..., 6.3186e-10, 1.5439e-10,
         2.7180e-10],
        [3.0583e-10, 1.8022e-10, 4.1864e-11,  ..., 2.3176e-10, 5.1432e-11,
         7.7645e-11]], device='cuda:0')
optimizer state dict: 128.0
lr: [1.085561572490406e-05, 1.085561572490406e-05]
scheduler_last_epoch: 128


Running epoch 0, step 1024, batch 1024
Sampled inputs[:2]: tensor([[   0,  328, 1410,  ..., 7344,   12, 5067],
        [   0,   13, 4596,  ...,  408,  689,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2130e-05, -9.2562e-05, -7.4346e-05,  ..., -1.4479e-04,
          1.5965e-04,  3.0424e-05],
        [-1.4976e-06, -1.0133e-06,  9.8348e-07,  ..., -1.2964e-06,
         -8.8289e-07, -1.0729e-06],
        [-3.3379e-06, -2.2799e-06,  2.3991e-06,  ..., -2.7865e-06,
         -1.8701e-06, -2.2352e-06],
        [-3.9935e-06, -2.5779e-06,  2.8312e-06,  ..., -3.3826e-06,
         -2.3097e-06, -2.8759e-06],
        [-3.3230e-06, -2.3544e-06,  2.3842e-06,  ..., -2.8163e-06,
         -1.9819e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9688480496406555


Running epoch 0, step 1025, batch 1025
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0,  642,  271,  ..., 5430, 2314, 6431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8764e-05, -9.4333e-05, -1.2153e-04,  ..., -9.7785e-05,
          1.5710e-04, -7.3403e-05],
        [-3.1069e-06, -2.1085e-06,  2.1160e-06,  ..., -2.5630e-06,
         -1.7695e-06, -2.0936e-06],
        [-7.0632e-06, -4.9025e-06,  5.1856e-06,  ..., -5.6922e-06,
         -3.8818e-06, -4.5896e-06],
        [-8.2850e-06, -5.4389e-06,  6.0350e-06,  ..., -6.7204e-06,
         -4.6194e-06, -5.7220e-06],
        [-6.8694e-06, -4.9621e-06,  5.0217e-06,  ..., -5.6624e-06,
         -4.0680e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9659364223480225


Running epoch 0, step 1026, batch 1026
Sampled inputs[:2]: tensor([[    0,   857,   344,  ...,  1529,  9106,  1447],
        [    0, 22390,   292,  ...,  3552,   278,   317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3165e-05, -1.3078e-04, -1.5300e-05,  ..., -1.7511e-05,
          2.8465e-04,  3.8241e-06],
        [-4.7833e-06, -3.2634e-06,  3.2410e-06,  ..., -3.9339e-06,
         -2.7120e-06, -3.0994e-06],
        [-1.0997e-05, -7.6890e-06,  7.9572e-06,  ..., -8.8960e-06,
         -6.0424e-06, -6.9737e-06],
        [-1.2666e-05, -8.4192e-06,  9.1046e-06,  ..., -1.0267e-05,
         -7.0184e-06, -8.5086e-06],
        [-1.0654e-05, -7.7635e-06,  7.6592e-06,  ..., -8.8066e-06,
         -6.3032e-06, -6.2436e-06]], device='cuda:0')
Loss: 1.0049477815628052


Running epoch 0, step 1027, batch 1027
Sampled inputs[:2]: tensor([[   0,  368,  275,  ..., 6389, 9102,   12],
        [   0,  471,  590,  ..., 5007,   13, 2920]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6543e-05,  1.4666e-04,  9.5924e-05,  ..., -1.4428e-04,
          5.0023e-04,  4.1140e-05],
        [-6.5342e-06, -4.3958e-06,  4.1425e-06,  ..., -5.3272e-06,
         -3.8147e-06, -4.4182e-06],
        [-1.4991e-05, -1.0416e-05,  1.0207e-05,  ..., -1.2070e-05,
         -8.5756e-06, -9.9093e-06],
        [-1.7166e-05, -1.1325e-05,  1.1578e-05,  ..., -1.3843e-05,
         -9.8795e-06, -1.1966e-05],
        [-1.4707e-05, -1.0610e-05,  9.9540e-06,  ..., -1.2070e-05,
         -9.0003e-06, -9.0450e-06]], device='cuda:0')
Loss: 0.9739629626274109


Running epoch 0, step 1028, batch 1028
Sampled inputs[:2]: tensor([[    0,  1607,    12,  ...,   895,  1503,   369],
        [    0,   278, 39533,  ...,   277,  1395, 47607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9803e-05, -3.6847e-05,  1.4245e-04,  ..., -2.6490e-04,
          5.4478e-04,  1.4480e-04],
        [-8.1807e-06, -5.4985e-06,  5.2676e-06,  ..., -6.6459e-06,
         -4.6566e-06, -5.4091e-06],
        [-1.8790e-05, -1.3068e-05,  1.2949e-05,  ..., -1.5125e-05,
         -1.0498e-05, -1.2204e-05],
        [-2.1517e-05, -1.4216e-05,  1.4693e-05,  ..., -1.7315e-05,
         -1.2070e-05, -1.4707e-05],
        [-1.8269e-05, -1.3217e-05,  1.2502e-05,  ..., -1.5005e-05,
         -1.0967e-05, -1.1042e-05]], device='cuda:0')
Loss: 1.0149400234222412


Running epoch 0, step 1029, batch 1029
Sampled inputs[:2]: tensor([[    0, 20291,  1990,  ...,   298,   732,   298],
        [    0, 38495, 36253,  ..., 11006,  5699,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9460e-05, -6.9617e-05,  8.6311e-05,  ..., -3.1929e-04,
          5.8474e-04,  3.1645e-04],
        [-9.8795e-06, -6.6757e-06,  6.4075e-06,  ..., -8.0317e-06,
         -5.6624e-06, -6.4597e-06],
        [-2.2665e-05, -1.5870e-05,  1.5676e-05,  ..., -1.8284e-05,
         -1.2793e-05, -1.4603e-05],
        [-2.5898e-05, -1.7256e-05,  1.7762e-05,  ..., -2.0891e-05,
         -1.4663e-05, -1.7539e-05],
        [-2.2009e-05, -1.6034e-05,  1.5125e-05,  ..., -1.8135e-05,
         -1.3337e-05, -1.3202e-05]], device='cuda:0')
Loss: 1.0522148609161377


Running epoch 0, step 1030, batch 1030
Sampled inputs[:2]: tensor([[    0,   344,   259,  ...,  6787, 10045,  9799],
        [    0,   266, 12080,  ...,   674,   369, 10956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9839e-04, -2.6282e-04, -1.4926e-04,  ..., -4.7573e-04,
          6.6689e-04,  1.5386e-04],
        [-1.1377e-05, -7.6033e-06,  7.4506e-06,  ..., -9.2983e-06,
         -6.4932e-06, -7.4878e-06],
        [-2.6077e-05, -1.8045e-05,  1.8209e-05,  ..., -2.1085e-05,
         -1.4596e-05, -1.6823e-05],
        [-2.9922e-05, -1.9625e-05,  2.0728e-05,  ..., -2.4214e-05,
         -1.6809e-05, -2.0325e-05],
        [-2.5377e-05, -1.8269e-05,  1.7628e-05,  ..., -2.0951e-05,
         -1.5259e-05, -1.5229e-05]], device='cuda:0')
Loss: 0.9686276912689209


Running epoch 0, step 1031, batch 1031
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0,   367,   925,  ..., 25491,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2023e-04, -2.3319e-04, -5.1789e-05,  ..., -4.9321e-04,
          7.0981e-04,  2.4562e-04],
        [-1.3039e-05, -8.7805e-06,  8.5607e-06,  ..., -1.0625e-05,
         -7.4692e-06, -8.4937e-06],
        [-3.0041e-05, -2.0951e-05,  2.0981e-05,  ..., -2.4244e-05,
         -1.6890e-05, -1.9237e-05],
        [-3.4302e-05, -2.2724e-05,  2.3752e-05,  ..., -2.7701e-05,
         -1.9357e-05, -2.3097e-05],
        [-2.9132e-05, -2.1115e-05,  2.0236e-05,  ..., -2.4006e-05,
         -1.7583e-05, -1.7360e-05]], device='cuda:0')
Loss: 1.0306943655014038
Graident accumulation at epoch 0, step 1031, batch 1031
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0283,  ...,  0.0291, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1082e-05,  1.8086e-04, -8.5503e-05,  ..., -4.3752e-05,
          1.0560e-04,  8.5098e-05],
        [-6.6455e-06, -1.5180e-06,  1.7649e-06,  ..., -4.9568e-06,
          9.6417e-08, -2.5298e-06],
        [ 7.3838e-05,  7.7163e-05, -4.4777e-05,  ...,  7.6019e-05,
          5.0941e-05,  2.4662e-05],
        [-1.7306e-05, -7.9104e-06,  9.9197e-06,  ..., -1.3486e-05,
         -4.8785e-06, -1.2640e-05],
        [-2.7586e-05, -1.9807e-05,  1.7721e-05,  ..., -2.2922e-05,
         -1.5375e-05, -1.7727e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2722e-08, 5.1628e-08, 3.7112e-08,  ..., 3.8517e-08, 1.1316e-07,
         4.4771e-08],
        [6.2826e-11, 4.7528e-11, 1.1486e-11,  ..., 4.7440e-11, 1.7948e-11,
         1.9432e-11],
        [3.0353e-09, 1.8261e-09, 6.7085e-10,  ..., 2.4528e-09, 5.5326e-10,
         8.2699e-10],
        [7.6201e-10, 5.8749e-10, 1.1883e-10,  ..., 6.3200e-10, 1.5461e-10,
         2.7207e-10],
        [3.0638e-10, 1.8048e-10, 4.2232e-11,  ..., 2.3210e-10, 5.1689e-11,
         7.7869e-11]], device='cuda:0')
optimizer state dict: 129.0
lr: [1.0732390190252052e-05, 1.0732390190252052e-05]
scheduler_last_epoch: 129


Running epoch 0, step 1032, batch 1032
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,    14,  3352,   266],
        [    0,   688,  2353,  ..., 20538, 10393,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0397e-05,  5.1537e-05,  1.6110e-05,  ...,  4.3756e-05,
          5.1248e-05,  9.8743e-05],
        [-1.6466e-06, -1.0803e-06,  1.1101e-06,  ..., -1.3262e-06,
         -8.5309e-07, -9.6858e-07],
        [-3.7402e-06, -2.5481e-06,  2.6375e-06,  ..., -3.0100e-06,
         -1.9372e-06, -2.1905e-06],
        [-4.3511e-06, -2.8461e-06,  3.0696e-06,  ..., -3.5018e-06,
         -2.2501e-06, -2.6524e-06],
        [-3.5912e-06, -2.5481e-06,  2.5183e-06,  ..., -2.9504e-06,
         -2.0117e-06, -1.9670e-06]], device='cuda:0')
Loss: 1.0134608745574951


Running epoch 0, step 1033, batch 1033
Sampled inputs[:2]: tensor([[    0,     9,   342,  ...,    12,   709,   857],
        [    0,   278, 19142,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9769e-05,  1.9250e-04, -1.8163e-04,  ..., -6.0284e-05,
          1.4069e-04,  1.2932e-04],
        [-3.3379e-06, -2.0713e-06,  2.1160e-06,  ..., -2.6822e-06,
         -1.7732e-06, -2.1830e-06],
        [-7.5549e-06, -4.9174e-06,  5.1111e-06,  ..., -6.0499e-06,
         -3.9935e-06, -4.8429e-06],
        [-8.6725e-06, -5.3346e-06,  5.8115e-06,  ..., -6.9588e-06,
         -4.5896e-06, -5.8413e-06],
        [-7.2271e-06, -4.9174e-06,  4.8727e-06,  ..., -5.9009e-06,
         -4.1127e-06, -4.3064e-06]], device='cuda:0')
Loss: 0.957284688949585


Running epoch 0, step 1034, batch 1034
Sampled inputs[:2]: tensor([[    0,  2366,  5036,  ...,  1477,   352,   631],
        [    0,   677, 25912,  ...,  2337,   292,  4462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2890e-04,  4.5585e-04, -2.3577e-04,  ..., -2.3097e-05,
          2.7870e-04,  2.5630e-04],
        [-5.1036e-06, -3.1367e-06,  3.0249e-06,  ..., -4.0904e-06,
         -2.8387e-06, -3.5465e-06],
        [-1.1727e-05, -7.5549e-06,  7.4506e-06,  ..., -9.3579e-06,
         -6.4671e-06, -8.0168e-06],
        [-1.3083e-05, -7.9721e-06,  8.1956e-06,  ..., -1.0476e-05,
         -7.2569e-06, -9.2983e-06],
        [-1.1310e-05, -7.5847e-06,  7.1526e-06,  ..., -9.1940e-06,
         -6.6757e-06, -7.2122e-06]], device='cuda:0')
Loss: 0.9661972522735596


Running epoch 0, step 1035, batch 1035
Sampled inputs[:2]: tensor([[    0,   508, 22318,  ...,    13,  1107,  4093],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9300e-04,  4.1952e-04, -3.5378e-04,  ..., -1.3712e-04,
          2.1044e-04,  2.5126e-04],
        [-6.8471e-06, -4.1872e-06,  4.1202e-06,  ..., -5.4538e-06,
         -3.7998e-06, -4.8205e-06],
        [-1.5751e-05, -1.0088e-05,  1.0163e-05,  ..., -1.2517e-05,
         -8.6725e-06, -1.0923e-05],
        [-1.7583e-05, -1.0639e-05,  1.1206e-05,  ..., -1.3962e-05,
         -9.6560e-06, -1.2621e-05],
        [-1.5214e-05, -1.0133e-05,  9.7454e-06,  ..., -1.2308e-05,
         -8.9705e-06, -9.8497e-06]], device='cuda:0')
Loss: 0.972364068031311


Running epoch 0, step 1036, batch 1036
Sampled inputs[:2]: tensor([[    0,    71,    14,  ...,  1770,   391, 39516],
        [    0,  7428,  1566,  ...,   199,  1726,  5647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2601e-04,  3.3232e-04, -3.5983e-04,  ..., -1.4391e-04,
          1.3140e-04,  2.9154e-04],
        [-8.4788e-06, -5.3123e-06,  5.2303e-06,  ..., -6.8024e-06,
         -4.7535e-06, -5.8860e-06],
        [-1.9684e-05, -1.2875e-05,  1.2994e-05,  ..., -1.5736e-05,
         -1.0923e-05, -1.3441e-05],
        [-2.1845e-05, -1.3486e-05,  1.4231e-05,  ..., -1.7434e-05,
         -1.2085e-05, -1.5482e-05],
        [-1.8880e-05, -1.2800e-05,  1.2368e-05,  ..., -1.5333e-05,
         -1.1191e-05, -1.2025e-05]], device='cuda:0')
Loss: 0.9977977275848389


Running epoch 0, step 1037, batch 1037
Sampled inputs[:2]: tensor([[   0, 4834,  278,  ...,   13, 8382,  669],
        [   0, 2422,  300,  ...,  630,  729, 3400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7947e-04,  4.9842e-04, -4.1695e-04,  ..., -1.4501e-04,
          1.3513e-04,  4.2929e-04],
        [-1.0222e-05, -6.4895e-06,  6.2659e-06,  ..., -8.1956e-06,
         -5.8264e-06, -7.0333e-06],
        [-2.3797e-05, -1.5780e-05,  1.5616e-05,  ..., -1.9014e-05,
         -1.3441e-05, -1.6108e-05],
        [-2.6256e-05, -1.6466e-05,  1.6987e-05,  ..., -2.0951e-05,
         -1.4797e-05, -1.8463e-05],
        [-2.2843e-05, -1.5691e-05,  1.4901e-05,  ..., -1.8567e-05,
         -1.3784e-05, -1.4454e-05]], device='cuda:0')
Loss: 0.976620078086853


Running epoch 0, step 1038, batch 1038
Sampled inputs[:2]: tensor([[   0,  266, 3727,  ..., 1143,  271, 5213],
        [   0,  300, 4402,  ..., 2013,   13, 6825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2730e-04,  4.0500e-04, -2.4057e-04,  ..., -2.9364e-04,
          1.9837e-04,  6.0653e-04],
        [-1.1958e-05, -7.5847e-06,  7.2643e-06,  ..., -9.6187e-06,
         -6.8471e-06, -8.2254e-06],
        [-2.7999e-05, -1.8522e-05,  1.8150e-05,  ..., -2.2426e-05,
         -1.5870e-05, -1.8969e-05],
        [ 9.5436e-05,  5.5617e-05, -2.7239e-05,  ...,  5.1538e-05,
          5.8377e-05,  2.5034e-05],
        [-2.6956e-05, -1.8477e-05,  1.7390e-05,  ..., -2.1979e-05,
         -1.6317e-05, -1.7121e-05]], device='cuda:0')
Loss: 1.0224120616912842


Running epoch 0, step 1039, batch 1039
Sampled inputs[:2]: tensor([[    0,   431, 19346,  ...,    14,  3237, 18548],
        [    0,   587,   300,  ...,  4325,   278, 12564]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4117e-04,  4.5758e-04, -2.9312e-04,  ..., -3.4220e-04,
          3.5019e-04,  6.7899e-04],
        [-1.3635e-05, -8.6799e-06,  8.2776e-06,  ..., -1.1019e-05,
         -7.8306e-06, -9.3505e-06],
        [-3.1844e-05, -2.1070e-05,  2.0698e-05,  ..., -2.5570e-05,
         -1.8075e-05, -2.1443e-05],
        [ 9.1085e-05,  5.2860e-05, -2.4423e-05,  ...,  4.7962e-05,
          5.5874e-05,  2.2083e-05],
        [-3.0696e-05, -2.1055e-05,  1.9878e-05,  ..., -2.5094e-05,
         -1.8597e-05, -1.9357e-05]], device='cuda:0')
Loss: 1.0132540464401245
Graident accumulation at epoch 0, step 1039, batch 1039
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0283,  ...,  0.0292, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.3091e-05,  2.0853e-04, -1.0627e-04,  ..., -7.3596e-05,
          1.3006e-04,  1.4449e-04],
        [-7.3444e-06, -2.2342e-06,  2.4162e-06,  ..., -5.5630e-06,
         -6.9628e-07, -3.2119e-06],
        [ 6.3270e-05,  6.7340e-05, -3.8229e-05,  ...,  6.5860e-05,
          4.4039e-05,  2.0051e-05],
        [-6.4666e-06, -1.8333e-06,  6.4854e-06,  ..., -7.3415e-06,
          1.1968e-06, -9.1677e-06],
        [-2.7897e-05, -1.9932e-05,  1.7937e-05,  ..., -2.3139e-05,
         -1.5697e-05, -1.7890e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2708e-08, 5.1786e-08, 3.7160e-08,  ..., 3.8595e-08, 1.1317e-07,
         4.5188e-08],
        [6.2949e-11, 4.7556e-11, 1.1543e-11,  ..., 4.7514e-11, 1.7991e-11,
         1.9500e-11],
        [3.0333e-09, 1.8247e-09, 6.7061e-10,  ..., 2.4510e-09, 5.5304e-10,
         8.2662e-10],
        [7.6955e-10, 5.8970e-10, 1.1931e-10,  ..., 6.3366e-10, 1.5758e-10,
         2.7228e-10],
        [3.0701e-10, 1.8074e-10, 4.2585e-11,  ..., 2.3250e-10, 5.1983e-11,
         7.8166e-11]], device='cuda:0')
optimizer state dict: 130.0
lr: [1.060905273998585e-05, 1.060905273998585e-05]
scheduler_last_epoch: 130


Running epoch 0, step 1040, batch 1040
Sampled inputs[:2]: tensor([[   0,  287,  516,  ..., 2386, 3492, 1663],
        [   0,  767, 1345,  ...,  276,  327,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2274e-06,  9.5987e-05,  2.2961e-04,  ...,  1.8334e-04,
         -1.8363e-04, -8.1352e-05],
        [-1.6987e-06, -1.1027e-06,  1.0133e-06,  ..., -1.3858e-06,
         -1.0878e-06, -1.1772e-06],
        [-4.2319e-06, -2.8014e-06,  2.6524e-06,  ..., -3.4124e-06,
         -2.6822e-06, -2.9206e-06],
        [-4.4405e-06, -2.8014e-06,  2.7865e-06,  ..., -3.5763e-06,
         -2.7716e-06, -3.1739e-06],
        [-4.0531e-06, -2.8014e-06,  2.5332e-06,  ..., -3.3230e-06,
         -2.7269e-06, -2.6375e-06]], device='cuda:0')
Loss: 0.9887833595275879


Running epoch 0, step 1041, batch 1041
Sampled inputs[:2]: tensor([[   0, 2377,  271,  ...,  395,  394,   14],
        [   0, 5699,   20,  ..., 3502, 2051,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8902e-05,  3.4211e-05,  2.9099e-05,  ...,  1.8191e-04,
         -1.8812e-04, -4.2632e-05],
        [-3.4198e-06, -2.1756e-06,  2.2054e-06,  ..., -2.7418e-06,
         -1.9483e-06, -2.2054e-06],
        [-8.2850e-06, -5.4389e-06,  5.5879e-06,  ..., -6.6459e-06,
         -4.7386e-06, -5.3644e-06],
        [-8.8215e-06, -5.5134e-06,  5.9456e-06,  ..., -7.0333e-06,
         -4.9472e-06, -5.9009e-06],
        [-7.7337e-06, -5.3048e-06,  5.1707e-06,  ..., -6.3032e-06,
         -4.7535e-06, -4.7088e-06]], device='cuda:0')
Loss: 1.0026988983154297


Running epoch 0, step 1042, batch 1042
Sampled inputs[:2]: tensor([[   0, 3115, 1640,  ...,  300,  266, 5453],
        [   0,  298, 2587,  ...,  298,  894,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9114e-04,  6.8493e-05,  2.6493e-04,  ...,  1.5031e-04,
         -4.0081e-04, -2.3321e-04],
        [-5.0440e-06, -3.1665e-06,  3.1479e-06,  ..., -4.0680e-06,
         -2.8908e-06, -3.4794e-06],
        [-1.2219e-05, -7.9274e-06,  8.0466e-06,  ..., -9.8050e-06,
         -6.9588e-06, -8.3297e-06],
        [-1.2845e-05, -7.8678e-06,  8.4490e-06,  ..., -1.0282e-05,
         -7.2420e-06, -9.0599e-06],
        [-1.1608e-05, -7.8082e-06,  7.5847e-06,  ..., -9.4324e-06,
         -7.0632e-06, -7.4059e-06]], device='cuda:0')
Loss: 0.9472679495811462


Running epoch 0, step 1043, batch 1043
Sampled inputs[:2]: tensor([[   0,   28, 2973,  ..., 8762, 2134,   27],
        [   0,  879,   27,  ..., 3958, 2875,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5833e-04,  6.5191e-05,  2.7164e-04,  ...,  1.2341e-04,
         -4.5972e-04, -2.3321e-04],
        [-6.8098e-06, -4.3362e-06,  4.1984e-06,  ..., -5.4613e-06,
         -3.9414e-06, -4.6790e-06],
        [-1.6659e-05, -1.0967e-05,  1.0818e-05,  ..., -1.3307e-05,
         -9.5963e-06, -1.1355e-05],
        [-1.7405e-05, -1.0863e-05,  1.1280e-05,  ..., -1.3873e-05,
         -9.9391e-06, -1.2264e-05],
        [-1.5751e-05, -1.0759e-05,  1.0163e-05,  ..., -1.2770e-05,
         -9.6858e-06, -1.0073e-05]], device='cuda:0')
Loss: 1.0266977548599243


Running epoch 0, step 1044, batch 1044
Sampled inputs[:2]: tensor([[   0,  300, 5631,  ..., 2278, 2669, 3011],
        [   0, 1615,  328,  ...,  266, 3133,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1190e-04, -4.8825e-05,  1.7269e-04,  ...,  1.1467e-04,
         -5.8451e-04, -2.5160e-04],
        [-8.4788e-06, -5.3793e-06,  5.3011e-06,  ..., -6.8024e-06,
         -4.8093e-06, -5.7369e-06],
        [-2.0623e-05, -1.3500e-05,  1.3575e-05,  ..., -1.6466e-05,
         -1.1623e-05, -1.3873e-05],
        [-2.1845e-05, -1.3575e-05,  1.4350e-05,  ..., -1.7390e-05,
         -1.2174e-05, -1.5169e-05],
        [-1.9506e-05, -1.3277e-05,  1.2755e-05,  ..., -1.5840e-05,
         -1.1787e-05, -1.2308e-05]], device='cuda:0')
Loss: 0.9924737811088562


Running epoch 0, step 1045, batch 1045
Sampled inputs[:2]: tensor([[   0,  445,   18,  ..., 1478,  578,  494],
        [   0,  515,  352,  ..., 2326, 3595, 6887]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9937e-04, -5.1182e-06,  4.2311e-04,  ...,  1.6161e-04,
         -5.9089e-04, -6.5065e-05],
        [-1.0237e-05, -6.4895e-06,  6.2622e-06,  ..., -8.2031e-06,
         -5.8599e-06, -7.0035e-06],
        [-2.5004e-05, -1.6332e-05,  1.6063e-05,  ..., -1.9938e-05,
         -1.4246e-05, -1.7032e-05],
        [-2.6375e-05, -1.6376e-05,  1.6928e-05,  ..., -2.0966e-05,
         -1.4827e-05, -1.8492e-05],
        [-2.3797e-05, -1.6153e-05,  1.5184e-05,  ..., -1.9312e-05,
         -1.4514e-05, -1.5259e-05]], device='cuda:0')
Loss: 1.0194945335388184


Running epoch 0, step 1046, batch 1046
Sampled inputs[:2]: tensor([[    0,  3984, 13077,  ...,   287,   650,   413],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7095e-04,  1.3323e-05,  5.6281e-04,  ...,  2.1460e-04,
         -4.8243e-04,  9.4051e-05],
        [-1.1906e-05, -7.5996e-06,  7.3425e-06,  ..., -9.5814e-06,
         -6.8583e-06, -8.0988e-06],
        [-2.9057e-05, -1.9103e-05,  1.8835e-05,  ..., -2.3261e-05,
         -1.6645e-05, -1.9684e-05],
        [-3.0726e-05, -1.9222e-05,  1.9863e-05,  ..., -2.4512e-05,
         -1.7390e-05, -2.1428e-05],
        [-2.7642e-05, -1.8880e-05,  1.7792e-05,  ..., -2.2531e-05,
         -1.6958e-05, -1.7613e-05]], device='cuda:0')
Loss: 1.0087733268737793


Running epoch 0, step 1047, batch 1047
Sampled inputs[:2]: tensor([[    0,   474,   221,  ..., 32291,   360,  2458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2125e-04,  2.2468e-04,  5.1385e-04,  ...,  4.7582e-04,
         -6.5825e-04,  3.6501e-05],
        [-1.3635e-05, -8.5980e-06,  8.5197e-06,  ..., -1.0915e-05,
         -7.6592e-06, -9.1717e-06],
        [-3.3081e-05, -2.1502e-05,  2.1711e-05,  ..., -2.6360e-05,
         -1.8530e-05, -2.2173e-05],
        [-3.5226e-05, -2.1785e-05,  2.3067e-05,  ..., -2.7969e-05,
         -1.9446e-05, -2.4319e-05],
        [-3.1397e-05, -2.1249e-05,  2.0459e-05,  ..., -2.5511e-05,
         -1.8910e-05, -1.9789e-05]], device='cuda:0')
Loss: 0.981073260307312
Graident accumulation at epoch 0, step 1047, batch 1047
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0091e-04,  2.1015e-04, -4.4254e-05,  ..., -1.8655e-05,
          5.1224e-05,  1.3369e-04],
        [-7.9734e-06, -2.8705e-06,  3.0265e-06,  ..., -6.0983e-06,
         -1.3926e-06, -3.8079e-06],
        [ 5.3635e-05,  5.8455e-05, -3.2235e-05,  ...,  5.6638e-05,
          3.7782e-05,  1.5829e-05],
        [-9.3426e-06, -3.8286e-06,  8.1436e-06,  ..., -9.4043e-06,
         -8.6751e-07, -1.0683e-05],
        [-2.8247e-05, -2.0064e-05,  1.8189e-05,  ..., -2.3377e-05,
         -1.6018e-05, -1.8080e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3021e-08, 5.1785e-08, 3.7387e-08,  ..., 3.8783e-08, 1.1349e-07,
         4.5144e-08],
        [6.3072e-11, 4.7582e-11, 1.1604e-11,  ..., 4.7585e-11, 1.8032e-11,
         1.9564e-11],
        [3.0313e-09, 1.8234e-09, 6.7041e-10,  ..., 2.4492e-09, 5.5283e-10,
         8.2629e-10],
        [7.7002e-10, 5.8959e-10, 1.1972e-10,  ..., 6.3381e-10, 1.5780e-10,
         2.7260e-10],
        [3.0769e-10, 1.8101e-10, 4.2961e-11,  ..., 2.3292e-10, 5.2289e-11,
         7.8479e-11]], device='cuda:0')
optimizer state dict: 131.0
lr: [1.0485622221144482e-05, 1.0485622221144482e-05]
scheduler_last_epoch: 131
End of epoch 0 | Validation PPL: 7.253408806284569 | Learning rate: 1.0485622221144482e-05
[2025-03-25 13:34:23,434] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
End of epoch checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/end_of_epoch_checkpoint.0, AFTER epoch 0


Running epoch 1, step 1048, batch 0
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  221,  474,  ...,   12,  259, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6493e-05,  1.6484e-04, -1.3171e-04,  ...,  1.5002e-04,
         -2.3157e-04, -2.4768e-04],
        [-1.6615e-06, -1.0356e-06,  1.0207e-06,  ..., -1.3784e-06,
         -9.6112e-07, -1.1623e-06],
        [-3.9935e-06, -2.5630e-06,  2.6375e-06,  ..., -3.2485e-06,
         -2.2352e-06, -2.7120e-06],
        [-4.2617e-06, -2.5630e-06,  2.7567e-06,  ..., -3.4720e-06,
         -2.3991e-06, -3.0398e-06],
        [-3.7998e-06, -2.5183e-06,  2.5034e-06,  ..., -3.1441e-06,
         -2.2650e-06, -2.3842e-06]], device='cuda:0')
Loss: 0.9785149097442627


Running epoch 1, step 1049, batch 1
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0, 17561,    12,  ...,   741,   496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2662e-04,  8.0633e-05, -1.4446e-04,  ...,  1.3281e-04,
         -1.3652e-04, -1.0554e-04],
        [-3.2634e-06, -2.0415e-06,  1.9707e-06,  ..., -2.7120e-06,
         -1.9446e-06, -2.2948e-06],
        [-8.0168e-06, -5.1707e-06,  5.1707e-06,  ..., -6.5416e-06,
         -4.6492e-06, -5.4687e-06],
        [-8.3745e-06, -5.0962e-06,  5.3197e-06,  ..., -6.8843e-06,
         -4.9025e-06, -6.0201e-06],
        [-7.5102e-06, -5.0366e-06,  4.8429e-06,  ..., -6.2436e-06,
         -4.6343e-06, -4.7684e-06]], device='cuda:0')
Loss: 0.9759708046913147


Running epoch 1, step 1050, batch 2
Sampled inputs[:2]: tensor([[    0,   369,   726,  ...,   292,   221,   358],
        [    0,    14,  3741,  ...,   278, 12472, 10257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9105e-05,  8.7174e-05, -2.0189e-04,  ...,  1.8858e-04,
         -2.3495e-04, -3.6228e-05],
        [-4.8280e-06, -2.9802e-06,  2.9914e-06,  ..., -3.9786e-06,
         -2.8461e-06, -3.4794e-06],
        [-1.1832e-05, -7.4804e-06,  7.7933e-06,  ..., -9.5218e-06,
         -6.7353e-06, -8.1956e-06],
        [-1.2457e-05, -7.4208e-06,  8.1062e-06,  ..., -1.0088e-05,
         -7.1675e-06, -9.1344e-06],
        [-1.1176e-05, -7.3612e-06,  7.3463e-06,  ..., -9.1493e-06,
         -6.7800e-06, -7.1824e-06]], device='cuda:0')
Loss: 0.9835972189903259


Running epoch 1, step 1051, batch 3
Sampled inputs[:2]: tensor([[    0,   508,  1548,  ...,   494, 10792,     9],
        [    0,    12,   287,  ..., 12678,  2503,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5728e-04, -9.9531e-05, -2.4384e-04,  ...,  1.6010e-04,
         -7.9287e-05,  6.0892e-05],
        [-6.3851e-06, -3.9935e-06,  3.7961e-06,  ..., -5.3495e-06,
         -3.9786e-06, -4.7013e-06],
        [-1.5616e-05, -9.9987e-06,  9.9987e-06,  ..., -1.2711e-05,
         -9.3132e-06, -1.0937e-05],
        [-1.6391e-05, -9.8795e-06,  1.0297e-05,  ..., -1.3471e-05,
         -9.9540e-06, -1.2219e-05],
        [-1.5020e-05, -9.9838e-06,  9.6262e-06,  ..., -1.2413e-05,
         -9.4920e-06, -9.7454e-06]], device='cuda:0')
Loss: 0.9884073734283447


Running epoch 1, step 1052, batch 4
Sampled inputs[:2]: tensor([[    0,   298,   696,  ...,  3502,   287,  1047],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0280e-04, -1.3017e-04, -1.1148e-04,  ..., -9.1371e-05,
          3.0405e-04,  2.3982e-04],
        [-8.0466e-06, -5.0291e-06,  4.7237e-06,  ..., -6.7279e-06,
         -5.0068e-06, -5.9754e-06],
        [-1.9729e-05, -1.2651e-05,  1.2457e-05,  ..., -1.6049e-05,
         -1.1772e-05, -1.4022e-05],
        [-2.0534e-05, -1.2398e-05,  1.2711e-05,  ..., -1.6853e-05,
         -1.2472e-05, -1.5467e-05],
        [-1.9014e-05, -1.2666e-05,  1.2010e-05,  ..., -1.5721e-05,
         -1.2025e-05, -1.2547e-05]], device='cuda:0')
Loss: 0.9963997602462769


Running epoch 1, step 1053, batch 5
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,   275,  2576,  3588],
        [    0,  1869,   596,  ..., 13055, 17051,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0834e-04, -3.9169e-04, -3.1261e-04,  ..., -5.4613e-05,
          2.9659e-04,  1.8087e-04],
        [-9.6485e-06, -6.1095e-06,  5.8040e-06,  ..., -8.0168e-06,
         -5.9679e-06, -6.9961e-06],
        [-2.3663e-05, -1.5393e-05,  1.5259e-05,  ..., -1.9208e-05,
         -1.4096e-05, -1.6496e-05],
        [-2.4617e-05, -1.5125e-05,  1.5587e-05,  ..., -2.0146e-05,
         -1.4886e-05, -1.8165e-05],
        [-2.2724e-05, -1.5363e-05,  1.4618e-05,  ..., -1.8761e-05,
         -1.4365e-05, -1.4722e-05]], device='cuda:0')
Loss: 0.9940140247344971


Running epoch 1, step 1054, batch 6
Sampled inputs[:2]: tensor([[    0,  3134,   278,  ...,  2462,   300, 11015],
        [    0,    12,  1197,  ...,   352,  2513,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1134e-05, -4.5201e-04, -4.0919e-04,  ..., -9.9627e-05,
          3.6549e-04,  2.8541e-04],
        [-1.1303e-05, -7.1973e-06,  6.8843e-06,  ..., -9.3579e-06,
         -6.9365e-06, -8.1211e-06],
        [-2.7686e-05, -1.8105e-05,  1.8016e-05,  ..., -2.2441e-05,
         -1.6436e-05, -1.9178e-05],
        [-2.8878e-05, -1.7866e-05,  1.8492e-05,  ..., -2.3574e-05,
         -1.7360e-05, -2.1160e-05],
        [-2.6524e-05, -1.8030e-05,  1.7211e-05,  ..., -2.1890e-05,
         -1.6734e-05, -1.7107e-05]], device='cuda:0')
Loss: 0.9980348348617554


Running epoch 1, step 1055, batch 7
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0, 29073,   916,  ...,    12,   287,   850]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5868e-05, -5.7669e-04, -4.5889e-04,  ..., -2.1580e-04,
          5.3361e-04,  6.9549e-04],
        [-1.2904e-05, -8.3372e-06,  7.9423e-06,  ..., -1.0721e-05,
         -7.9498e-06, -9.2015e-06],
        [-3.1590e-05, -2.0921e-05,  2.0742e-05,  ..., -2.5719e-05,
         -1.8865e-05, -2.1756e-05],
        [-3.2961e-05, -2.0713e-05,  2.1324e-05,  ..., -2.7016e-05,
         -1.9908e-05, -2.3976e-05],
        [-3.0249e-05, -2.0817e-05,  1.9804e-05,  ..., -2.5064e-05,
         -1.9193e-05, -1.9386e-05]], device='cuda:0')
Loss: 1.0170724391937256
Graident accumulation at epoch 1, step 1055, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.9230e-05,  1.3146e-04, -8.5717e-05,  ..., -3.8369e-05,
          9.9463e-05,  1.8987e-04],
        [-8.4665e-06, -3.4172e-06,  3.5181e-06,  ..., -6.5606e-06,
         -2.0483e-06, -4.3472e-06],
        [ 4.5112e-05,  5.0518e-05, -2.6937e-05,  ...,  4.8402e-05,
          3.2117e-05,  1.2070e-05],
        [-1.1704e-05, -5.5170e-06,  9.4616e-06,  ..., -1.1165e-05,
         -2.7716e-06, -1.2012e-05],
        [-2.8447e-05, -2.0139e-05,  1.8351e-05,  ..., -2.3545e-05,
         -1.6336e-05, -1.8211e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2948e-08, 5.2066e-08, 3.7561e-08,  ..., 3.8791e-08, 1.1367e-07,
         4.5582e-08],
        [6.3175e-11, 4.7604e-11, 1.1655e-11,  ..., 4.7653e-11, 1.8077e-11,
         1.9629e-11],
        [3.0293e-09, 1.8220e-09, 6.7017e-10,  ..., 2.4475e-09, 5.5263e-10,
         8.2593e-10],
        [7.7033e-10, 5.8943e-10, 1.2005e-10,  ..., 6.3391e-10, 1.5803e-10,
         2.7290e-10],
        [3.0830e-10, 1.8127e-10, 4.3310e-11,  ..., 2.3331e-10, 5.2605e-11,
         7.8777e-11]], device='cuda:0')
optimizer state dict: 132.0
lr: [1.0362117494988668e-05, 1.0362117494988668e-05]
scheduler_last_epoch: 132


Running epoch 1, step 1056, batch 8
Sampled inputs[:2]: tensor([[    0,   809,   367,  ...,   717,   287,  1548],
        [    0,   287, 39084,  ...,   266,  1817,  1589]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4393e-05,  1.2440e-04,  1.2932e-04,  ...,  1.0098e-04,
         -6.9694e-05, -8.1992e-05],
        [-1.6466e-06, -1.0952e-06,  9.0525e-07,  ..., -1.3858e-06,
         -1.0282e-06, -1.1921e-06],
        [-4.0829e-06, -2.8163e-06,  2.4587e-06,  ..., -3.4124e-06,
         -2.5332e-06, -2.8908e-06],
        [-4.0829e-06, -2.7120e-06,  2.4140e-06,  ..., -3.4422e-06,
         -2.5630e-06, -3.0100e-06],
        [-3.9041e-06, -2.7418e-06,  2.3246e-06,  ..., -3.2932e-06,
         -2.5332e-06, -2.5779e-06]], device='cuda:0')
Loss: 0.9482399225234985


Running epoch 1, step 1057, batch 9
Sampled inputs[:2]: tensor([[    0,  1871,   401,  ...,    14,  4797,    12],
        [    0,  3829,   278,  ..., 11978,     9,   968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0980e-04,  1.5251e-04,  8.0445e-05,  ...,  1.8592e-04,
         -6.1674e-05, -3.1564e-04],
        [-3.2261e-06, -2.1234e-06,  1.9334e-06,  ..., -2.7120e-06,
         -2.0191e-06, -2.2650e-06],
        [-8.0168e-06, -5.4389e-06,  5.1856e-06,  ..., -6.6608e-06,
         -4.9323e-06, -5.5134e-06],
        [-8.1956e-06, -5.3048e-06,  5.2303e-06,  ..., -6.8545e-06,
         -5.0962e-06, -5.9158e-06],
        [-7.7188e-06, -5.3793e-06,  4.9472e-06,  ..., -6.4820e-06,
         -5.0068e-06, -4.9472e-06]], device='cuda:0')
Loss: 0.983029305934906


Running epoch 1, step 1058, batch 10
Sampled inputs[:2]: tensor([[   0, 3119,  278,  ...,  352,  674,  369],
        [   0,  271,  259,  ..., 4511,   14,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3970e-04,  2.5282e-04,  1.2090e-04,  ...,  1.2114e-04,
         -1.6423e-04, -3.0737e-04],
        [-4.9099e-06, -3.2932e-06,  2.9393e-06,  ..., -4.0978e-06,
         -3.0622e-06, -3.3826e-06],
        [-1.2219e-05, -8.4192e-06,  7.8380e-06,  ..., -1.0103e-05,
         -7.5251e-06, -8.2850e-06],
        [-1.2428e-05, -8.1807e-06,  7.8678e-06,  ..., -1.0312e-05,
         -7.6890e-06, -8.8066e-06],
        [-1.1742e-05, -8.3148e-06,  7.4506e-06,  ..., -9.8199e-06,
         -7.5996e-06, -7.4506e-06]], device='cuda:0')
Loss: 0.9983147978782654


Running epoch 1, step 1059, batch 11
Sampled inputs[:2]: tensor([[    0,   706,  1005,  ...,   278,   266,  5590],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5978e-04,  1.3549e-04,  1.8773e-04,  ...,  9.3740e-05,
         -2.0972e-04, -3.3721e-04],
        [-6.5491e-06, -4.4033e-06,  3.9451e-06,  ..., -5.4389e-06,
         -4.0680e-06, -4.4927e-06],
        [-1.6391e-05, -1.1295e-05,  1.0535e-05,  ..., -1.3500e-05,
         -1.0043e-05, -1.1086e-05],
        [-1.6630e-05, -1.0967e-05,  1.0550e-05,  ..., -1.3724e-05,
         -1.0207e-05, -1.1727e-05],
        [-1.5676e-05, -1.1101e-05,  9.9689e-06,  ..., -1.3068e-05,
         -1.0118e-05, -9.9391e-06]], device='cuda:0')
Loss: 1.0212507247924805


Running epoch 1, step 1060, batch 12
Sampled inputs[:2]: tensor([[    0,    13,  4467,  ...,  2390, 47857,   287],
        [    0,    18,    66,  ...,    65,    17,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7966e-04,  3.9080e-04,  3.1378e-04,  ...,  5.6488e-05,
         -2.9973e-04, -3.1110e-04],
        [-8.2552e-06, -5.5730e-06,  4.8205e-06,  ..., -6.8545e-06,
         -5.3048e-06, -5.7220e-06],
        [-2.0683e-05, -1.4335e-05,  1.2919e-05,  ..., -1.7032e-05,
         -1.3083e-05, -1.4141e-05],
        [ 3.1417e-04,  4.4465e-04, -2.9618e-04,  ...,  2.8933e-04,
          4.0493e-04,  2.1855e-04],
        [-1.9938e-05, -1.4171e-05,  1.2323e-05,  ..., -1.6615e-05,
         -1.3262e-05, -1.2815e-05]], device='cuda:0')
Loss: 1.0051517486572266


Running epoch 1, step 1061, batch 13
Sampled inputs[:2]: tensor([[    0,  7377, 30662,  ...,   287,   694, 13403],
        [    0,  1197,  3025,  ...,    14,   747,  3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8366e-04,  3.0277e-04,  3.7673e-04,  ...,  8.0758e-05,
         -3.2126e-04, -3.1242e-04],
        [-9.9316e-06, -6.6683e-06,  5.7817e-06,  ..., -8.1807e-06,
         -6.3106e-06, -6.8545e-06],
        [-2.5064e-05, -1.7270e-05,  1.5557e-05,  ..., -2.0474e-05,
         -1.5661e-05, -1.7092e-05],
        [ 3.0979e-04,  4.4183e-04, -2.9356e-04,  ...,  2.8591e-04,
          4.0235e-04,  2.1551e-04],
        [-2.4170e-05, -1.7107e-05,  1.4856e-05,  ..., -1.9997e-05,
         -1.5929e-05, -1.5527e-05]], device='cuda:0')
Loss: 1.012449026107788


Running epoch 1, step 1062, batch 14
Sampled inputs[:2]: tensor([[   0, 1295,  508,  ...,  829,  772,  278],
        [   0, 7117,  278,  ...,  287,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5403e-04,  3.1431e-04,  6.2068e-04,  ...,  1.0931e-04,
         -3.1556e-04, -2.9338e-04],
        [-1.1630e-05, -7.7859e-06,  6.7130e-06,  ..., -9.5740e-06,
         -7.3984e-06, -7.9796e-06],
        [-2.9296e-05, -2.0161e-05,  1.8060e-05,  ..., -2.3916e-05,
         -1.8373e-05, -1.9833e-05],
        [ 3.0556e-04,  4.3906e-04, -2.9112e-04,  ...,  2.8245e-04,
          3.9962e-04,  2.1265e-04],
        [-2.8253e-05, -2.0012e-05,  1.7256e-05,  ..., -2.3395e-05,
         -1.8716e-05, -1.8045e-05]], device='cuda:0')
Loss: 0.9750717282295227


Running epoch 1, step 1063, batch 15
Sampled inputs[:2]: tensor([[    0,    67,   695,  ...,   437,   266, 44563],
        [    0,   287,  7763,  ...,   689,  2409,   699]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7220e-04,  3.8868e-04,  5.3397e-04,  ...,  2.3346e-04,
         -4.6164e-04, -3.6557e-04],
        [-1.3284e-05, -8.8960e-06,  7.7039e-06,  ..., -1.0900e-05,
         -8.3968e-06, -9.0897e-06],
        [-3.3557e-05, -2.3067e-05,  2.0728e-05,  ..., -2.7299e-05,
         -2.0891e-05, -2.2680e-05],
        [ 3.0127e-04,  4.3623e-04, -2.8844e-04,  ...,  2.7904e-04,
          3.9709e-04,  2.0967e-04],
        [-3.2216e-05, -2.2814e-05,  1.9729e-05,  ..., -2.6613e-05,
         -2.1204e-05, -2.0564e-05]], device='cuda:0')
Loss: 0.9908051490783691
Graident accumulation at epoch 1, step 1063, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0142, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.3086e-05,  1.5718e-04, -2.3749e-05,  ..., -1.1186e-05,
          4.3352e-05,  1.3433e-04],
        [-8.9483e-06, -3.9651e-06,  3.9367e-06,  ..., -6.9945e-06,
         -2.6831e-06, -4.8215e-06],
        [ 3.7245e-05,  4.3159e-05, -2.2171e-05,  ...,  4.0832e-05,
          2.6817e-05,  8.5953e-06],
        [ 1.9593e-05,  3.8658e-05, -2.0328e-05,  ...,  1.7855e-05,
          3.7215e-05,  1.0156e-05],
        [-2.8824e-05, -2.0407e-05,  1.8489e-05,  ..., -2.3852e-05,
         -1.6823e-05, -1.8446e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2905e-08, 5.2165e-08, 3.7808e-08,  ..., 3.8807e-08, 1.1376e-07,
         4.5670e-08],
        [6.3289e-11, 4.7635e-11, 1.1703e-11,  ..., 4.7724e-11, 1.8130e-11,
         1.9692e-11],
        [3.0274e-09, 1.8207e-09, 6.6993e-10,  ..., 2.4458e-09, 5.5251e-10,
         8.2562e-10],
        [8.6032e-10, 7.7913e-10, 2.0313e-10,  ..., 7.1114e-10, 3.1556e-10,
         3.1659e-10],
        [3.0903e-10, 1.8161e-10, 4.3656e-11,  ..., 2.3379e-10, 5.3002e-11,
         7.9121e-11]], device='cuda:0')
optimizer state dict: 133.0
lr: [1.023855743411865e-05, 1.023855743411865e-05]
scheduler_last_epoch: 133


Running epoch 1, step 1064, batch 16
Sampled inputs[:2]: tensor([[   0,  287, 5724,  ...,  298,  591, 2609],
        [   0, 2958,  298,  ...,   12,  709,  616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2245e-05, -1.2010e-04,  3.1790e-06,  ...,  6.7462e-05,
         -3.7784e-04, -6.7017e-05],
        [-1.5795e-06, -1.0431e-06,  9.6112e-07,  ..., -1.3113e-06,
         -1.0356e-06, -1.1623e-06],
        [-4.1127e-06, -2.7567e-06,  2.6375e-06,  ..., -3.3677e-06,
         -2.6226e-06, -2.9057e-06],
        [-4.0829e-06, -2.5928e-06,  2.5928e-06,  ..., -3.3528e-06,
         -2.6077e-06, -3.0398e-06],
        [-4.0233e-06, -2.7716e-06,  2.5779e-06,  ..., -3.3230e-06,
         -2.6822e-06, -2.6673e-06]], device='cuda:0')
Loss: 0.9809408783912659


Running epoch 1, step 1065, batch 17
Sampled inputs[:2]: tensor([[    0, 12456,    14,  ...,  1822,  1016,   365],
        [    0,    13,   786,  ...,   275,  2623,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0680e-04, -2.1837e-04,  3.1790e-06,  ...,  1.1693e-04,
         -3.0081e-04, -7.3881e-05],
        [-3.2783e-06, -2.2352e-06,  1.8887e-06,  ..., -2.7120e-06,
         -2.2352e-06, -2.3469e-06],
        [ 6.4548e-05,  7.0676e-05, -4.9492e-05,  ...,  9.2039e-05,
          6.8637e-05,  2.3224e-05],
        [-8.2850e-06, -5.4985e-06,  4.9919e-06,  ..., -6.8098e-06,
         -5.5283e-06, -6.0648e-06],
        [-8.4639e-06, -6.0499e-06,  5.1260e-06,  ..., -7.0333e-06,
         -5.9009e-06, -5.6028e-06]], device='cuda:0')
Loss: 1.0191020965576172


Running epoch 1, step 1066, batch 18
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0, 10251,   278,  ...,   278,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5212e-04, -4.2243e-04, -3.3749e-05,  ...,  4.7816e-05,
         -2.1587e-04, -2.9485e-05],
        [-4.9099e-06, -3.3304e-06,  2.8796e-06,  ..., -4.0382e-06,
         -3.2485e-06, -3.3528e-06],
        [ 6.0465e-05,  6.7800e-05, -4.6899e-05,  ...,  8.8701e-05,
          6.6074e-05,  2.0706e-05],
        [-1.2457e-05, -8.2999e-06,  7.6294e-06,  ..., -1.0222e-05,
         -8.1360e-06, -8.7470e-06],
        [-1.2428e-05, -8.9407e-06,  7.6294e-06,  ..., -1.0341e-05,
         -8.5384e-06, -7.9125e-06]], device='cuda:0')
Loss: 1.0148168802261353


Running epoch 1, step 1067, batch 19
Sampled inputs[:2]: tensor([[    0,   984,    13,  ...,    13, 37385,   490],
        [    0,  4653, 21419,  ...,  7845,   300,   565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0247e-04, -3.8526e-04, -1.0788e-04,  ...,  5.2574e-05,
         -2.9607e-04, -5.7136e-05],
        [-6.5565e-06, -4.5300e-06,  3.9004e-06,  ..., -5.3942e-06,
         -4.3288e-06, -4.3809e-06],
        [ 5.6203e-05,  6.4611e-05, -4.4127e-05,  ...,  8.5200e-05,
          6.3288e-05,  1.8039e-05],
        [ 2.8805e-04,  3.0948e-04, -1.6857e-04,  ...,  2.8315e-04,
          2.8080e-04,  1.4152e-04],
        [-1.6540e-05, -1.2130e-05,  1.0282e-05,  ..., -1.3784e-05,
         -1.1370e-05, -1.0356e-05]], device='cuda:0')
Loss: 1.0034912824630737


Running epoch 1, step 1068, batch 20
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0, 21448,   344,  ...,   365,  1501,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3316e-04, -3.9731e-04, -1.7480e-04,  ...,  1.1477e-04,
         -2.9607e-04, -1.3793e-04],
        [-8.1584e-06, -5.5656e-06,  4.8354e-06,  ..., -6.6981e-06,
         -5.3570e-06, -5.5209e-06],
        [ 5.2061e-05,  6.1899e-05, -4.1579e-05,  ...,  8.1892e-05,
          6.0740e-05,  1.5163e-05],
        [ 2.8382e-04,  3.0683e-04, -1.6601e-04,  ...,  2.7976e-04,
          2.7818e-04,  1.3843e-04],
        [-2.0504e-05, -1.4856e-05,  1.2726e-05,  ..., -1.7032e-05,
         -1.3977e-05, -1.2964e-05]], device='cuda:0')
Loss: 0.9922284483909607


Running epoch 1, step 1069, batch 21
Sampled inputs[:2]: tensor([[    0,  1238,    14,  ...,   368,   940,   437],
        [    0,   380,  2114,  ...,   456, 28979,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7155e-04, -3.2666e-04, -6.3761e-05,  ...,  2.8087e-04,
         -3.0223e-04, -4.6862e-04],
        [-9.5963e-06, -6.6012e-06,  5.6885e-06,  ..., -7.9870e-06,
         -6.3851e-06, -6.6012e-06],
        [ 4.8276e-05,  5.9143e-05, -3.9180e-05,  ...,  7.8584e-05,
          5.8147e-05,  1.2421e-05],
        [ 2.8007e-04,  3.0422e-04, -1.6365e-04,  ...,  2.7646e-04,
          2.7557e-04,  1.3553e-04],
        [-2.4199e-05, -1.7643e-05,  1.5080e-05,  ..., -2.0295e-05,
         -1.6645e-05, -1.5467e-05]], device='cuda:0')
Loss: 0.9704157710075378


Running epoch 1, step 1070, batch 22
Sampled inputs[:2]: tensor([[    0,   367,  2063,  ...,  3022,   221,   733],
        [    0, 22599,  1336,  ...,   729,   923,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4823e-04, -2.6280e-04, -1.0439e-04,  ...,  3.4639e-04,
         -4.1365e-04, -6.1849e-04],
        [-1.1168e-05, -7.7039e-06,  6.6385e-06,  ..., -9.2834e-06,
         -7.3984e-06, -7.7188e-06],
        [ 4.4133e-05,  5.6192e-05, -3.6558e-05,  ...,  7.5201e-05,
          5.5554e-05,  9.5005e-06],
        [ 2.7590e-04,  3.0138e-04, -1.6104e-04,  ...,  2.7304e-04,
          2.7293e-04,  1.3246e-04],
        [-2.8133e-05, -2.0534e-05,  1.7554e-05,  ..., -2.3559e-05,
         -1.9252e-05, -1.8060e-05]], device='cuda:0')
Loss: 1.0028133392333984


Running epoch 1, step 1071, batch 23
Sampled inputs[:2]: tensor([[    0, 10386,  6404,  ...,   292,   325, 12071],
        [    0,  3380,  1197,  ...,   631,   369,  3123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7360e-04, -2.1165e-04, -4.8523e-05,  ...,  3.6184e-04,
         -4.8336e-04, -6.5159e-04],
        [-1.2822e-05, -8.8513e-06,  7.6219e-06,  ..., -1.0587e-05,
         -8.4192e-06, -8.7842e-06],
        [ 3.9782e-05,  5.3093e-05, -3.3861e-05,  ...,  7.1774e-05,
          5.2872e-05,  6.6991e-06],
        [ 2.7163e-04,  2.9843e-04, -1.5841e-04,  ...,  2.6968e-04,
          2.7031e-04,  1.2963e-04],
        [-3.2216e-05, -2.3559e-05,  2.0087e-05,  ..., -2.6852e-05,
         -2.1920e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.002657175064087
Graident accumulation at epoch 1, step 1071, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0064, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0141, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.9417e-05,  1.2030e-04, -2.6226e-05,  ...,  2.6117e-05,
         -9.3187e-06,  5.5733e-05],
        [-9.3357e-06, -4.4537e-06,  4.3052e-06,  ..., -7.3538e-06,
         -3.2567e-06, -5.2178e-06],
        [ 3.7499e-05,  4.4153e-05, -2.3340e-05,  ...,  4.3926e-05,
          2.9422e-05,  8.4057e-06],
        [ 4.4797e-05,  6.4634e-05, -3.4136e-05,  ...,  4.3037e-05,
          6.0524e-05,  2.2104e-05],
        [-2.9163e-05, -2.0722e-05,  1.8648e-05,  ..., -2.4152e-05,
         -1.7332e-05, -1.8658e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2862e-08, 5.2157e-08, 3.7773e-08,  ..., 3.8899e-08, 1.1388e-07,
         4.6049e-08],
        [6.3390e-11, 4.7666e-11, 1.1749e-11,  ..., 4.7788e-11, 1.8182e-11,
         1.9750e-11],
        [3.0259e-09, 1.8217e-09, 6.7041e-10,  ..., 2.4485e-09, 5.5476e-10,
         8.2484e-10],
        [9.3325e-10, 8.6741e-10, 2.2802e-10,  ..., 7.8315e-10, 3.8831e-10,
         3.3308e-10],
        [3.0976e-10, 1.8198e-10, 4.4016e-11,  ..., 2.3428e-10, 5.3430e-11,
         7.9464e-11]], device='cuda:0')
optimizer state dict: 134.0
lr: [1.01149609195903e-05, 1.01149609195903e-05]
scheduler_last_epoch: 134


Running epoch 1, step 1072, batch 24
Sampled inputs[:2]: tensor([[    0,  6904,  6069,  ..., 17196,   471,   221],
        [    0,  1336,   278,  ...,   266,  3269,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8258e-06, -1.0484e-04, -1.0745e-04,  ...,  3.0811e-05,
         -6.0648e-05, -4.0243e-05],
        [-1.6093e-06, -1.0356e-06,  9.2760e-07,  ..., -1.3188e-06,
         -1.0803e-06, -1.1101e-06],
        [-4.2021e-06, -2.7716e-06,  2.5630e-06,  ..., -3.3975e-06,
         -2.7567e-06, -2.8610e-06],
        [-4.2021e-06, -2.6226e-06,  2.5183e-06,  ..., -3.3826e-06,
         -2.7567e-06, -2.9951e-06],
        [-4.0233e-06, -2.7865e-06,  2.4438e-06,  ..., -3.3379e-06,
         -2.8163e-06, -2.5779e-06]], device='cuda:0')
Loss: 1.0005338191986084


Running epoch 1, step 1073, batch 25
Sampled inputs[:2]: tensor([[   0,   27, 3961,  ...,  462,  221,  474],
        [   0,  765,  292,  ...,  623,   12, 7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5242e-05, -1.6829e-04, -3.1418e-04,  ..., -8.1450e-05,
          4.3728e-05, -1.7214e-05],
        [-3.1814e-06, -2.0042e-06,  1.8962e-06,  ..., -2.5854e-06,
         -2.0713e-06, -2.2352e-06],
        [-8.1956e-06, -5.3644e-06,  5.1707e-06,  ..., -6.6310e-06,
         -5.2899e-06, -5.7071e-06],
        [-8.4043e-06, -5.1558e-06,  5.2452e-06,  ..., -6.7800e-06,
         -5.4240e-06, -6.1542e-06],
        [-7.8976e-06, -5.4538e-06,  4.9472e-06,  ..., -6.5416e-06,
         -5.4240e-06, -5.1856e-06]], device='cuda:0')
Loss: 0.998089611530304


Running epoch 1, step 1074, batch 26
Sampled inputs[:2]: tensor([[    0,   278,   565,  ...,  1125,  5222,   287],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2197e-04, -3.4941e-04, -4.4451e-04,  ...,  2.2511e-05,
         -1.1472e-05, -6.8180e-05],
        [-4.8280e-06, -3.1367e-06,  2.8163e-06,  ..., -3.8892e-06,
         -3.1069e-06, -3.3006e-06],
        [-1.2547e-05, -8.4341e-06,  7.7486e-06,  ..., -1.0058e-05,
         -8.0168e-06, -8.5086e-06],
        [-1.2577e-05, -8.0019e-06,  7.6890e-06,  ..., -1.0088e-05,
         -8.0466e-06, -8.9407e-06],
        [-1.2040e-05, -8.4788e-06,  7.3910e-06,  ..., -9.8795e-06,
         -8.1807e-06, -7.7039e-06]], device='cuda:0')
Loss: 1.0012500286102295


Running epoch 1, step 1075, batch 27
Sampled inputs[:2]: tensor([[    0, 32444,    41,  ...,    14,    18,    59],
        [    0,  4304,  7406,  ...,   957,  7366,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1080e-04, -4.0745e-04, -3.2005e-04,  ...,  8.4810e-05,
         -7.2360e-06,  1.5046e-05],
        [-6.4299e-06, -4.3437e-06,  3.7290e-06,  ..., -5.2452e-06,
         -4.2245e-06, -4.4033e-06],
        [-1.6898e-05, -1.1802e-05,  1.0371e-05,  ..., -1.3739e-05,
         -1.1042e-05, -1.1533e-05],
        [-1.6719e-05, -1.1101e-05,  1.0148e-05,  ..., -1.3575e-05,
         -1.0908e-05, -1.1876e-05],
        [-1.6183e-05, -1.1757e-05,  9.8646e-06,  ..., -1.3426e-05,
         -1.1191e-05, -1.0431e-05]], device='cuda:0')
Loss: 1.0035408735275269


Running epoch 1, step 1076, batch 28
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,  5597, 11929,  ...,   271,   275,   955]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7328e-04, -3.7929e-04, -4.5246e-04,  ...,  1.7970e-04,
         -8.6621e-05, -1.8260e-04],
        [-7.9647e-06, -5.3272e-06,  4.7199e-06,  ..., -6.5044e-06,
         -5.1558e-06, -5.4091e-06],
        [-2.0862e-05, -1.4439e-05,  1.3083e-05,  ..., -1.6972e-05,
         -1.3426e-05, -1.4111e-05],
        [-2.0802e-05, -1.3679e-05,  1.2934e-05,  ..., -1.6898e-05,
         -1.3351e-05, -1.4663e-05],
        [-1.9878e-05, -1.4335e-05,  1.2383e-05,  ..., -1.6510e-05,
         -1.3590e-05, -1.2681e-05]], device='cuda:0')
Loss: 0.9840181469917297


Running epoch 1, step 1077, batch 29
Sampled inputs[:2]: tensor([[   0, 1527,  292,  ..., 2122,  278, 1911],
        [   0, 2341, 7956,  ..., 2355,  413,   72]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7560e-05, -4.5542e-04, -4.0338e-04,  ...,  1.9507e-04,
          1.6518e-05, -1.5706e-04],
        [-9.5591e-06, -6.4746e-06,  5.5879e-06,  ..., -7.7933e-06,
         -6.3628e-06, -6.5342e-06],
        [-2.5153e-05, -1.7583e-05,  1.5557e-05,  ..., -2.0385e-05,
         -1.6570e-05, -1.7121e-05],
        [ 7.4999e-05,  6.6653e-05, -3.2883e-05,  ...,  6.2947e-05,
          5.5790e-05,  2.2575e-05],
        [-2.4021e-05, -1.7494e-05,  1.4767e-05,  ..., -1.9893e-05,
         -1.6809e-05, -1.5438e-05]], device='cuda:0')
Loss: 0.9905609488487244


Running epoch 1, step 1078, batch 30
Sampled inputs[:2]: tensor([[   0,  257,   13,  ...,  328,  630, 1403],
        [   0, 1594,  586,  ...,   13,  701,  308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8367e-06, -6.2872e-04, -3.7761e-04,  ...,  2.6888e-05,
          1.7346e-04, -1.6646e-04],
        [-1.1131e-05, -7.5847e-06,  6.4410e-06,  ..., -9.1270e-06,
         -7.4655e-06, -7.6592e-06],
        [-2.9325e-05, -2.0579e-05,  1.7956e-05,  ..., -2.3872e-05,
         -1.9386e-05, -2.0042e-05],
        [ 7.0916e-05,  6.3837e-05, -3.0588e-05,  ...,  5.9520e-05,
          5.2989e-05,  1.9595e-05],
        [-2.8223e-05, -2.0593e-05,  1.7181e-05,  ..., -2.3454e-05,
         -1.9789e-05, -1.8194e-05]], device='cuda:0')
Loss: 1.0130668878555298


Running epoch 1, step 1079, batch 31
Sampled inputs[:2]: tensor([[    0, 11853,  1611,  ...,  4413,  4240,   278],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5823e-05, -7.4082e-04, -3.7893e-04,  ...,  1.3091e-05,
          1.2387e-04, -2.3051e-04],
        [-1.2681e-05, -8.6203e-06,  7.3761e-06,  ..., -1.0379e-05,
         -8.4713e-06, -8.7246e-06],
        [ 4.8249e-05,  5.0346e-05, -1.9943e-05,  ...,  5.0335e-05,
          3.1556e-05,  7.1003e-06],
        [ 6.6744e-05,  6.1095e-05, -2.7980e-05,  ...,  5.6182e-05,
          5.0306e-05,  1.6630e-05],
        [-3.2187e-05, -2.3454e-05,  1.9699e-05,  ..., -2.6703e-05,
         -2.2486e-05, -2.0742e-05]], device='cuda:0')
Loss: 0.986690878868103
Graident accumulation at epoch 1, step 1079, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0021,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.9058e-05,  3.4188e-05, -6.1497e-05,  ...,  2.4814e-05,
          4.0004e-06,  2.7110e-05],
        [-9.6702e-06, -4.8704e-06,  4.6123e-06,  ..., -7.6563e-06,
         -3.7782e-06, -5.5684e-06],
        [ 3.8574e-05,  4.4772e-05, -2.3000e-05,  ...,  4.4567e-05,
          2.9636e-05,  8.2751e-06],
        [ 4.6991e-05,  6.4280e-05, -3.3520e-05,  ...,  4.4351e-05,
          5.9502e-05,  2.1556e-05],
        [-2.9465e-05, -2.0995e-05,  1.8753e-05,  ..., -2.4407e-05,
         -1.7848e-05, -1.8866e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2790e-08, 5.2654e-08, 3.7878e-08,  ..., 3.8860e-08, 1.1379e-07,
         4.6056e-08],
        [6.3487e-11, 4.7693e-11, 1.1792e-11,  ..., 4.7848e-11, 1.8236e-11,
         1.9806e-11],
        [3.0252e-09, 1.8224e-09, 6.7014e-10,  ..., 2.4485e-09, 5.5520e-10,
         8.2407e-10],
        [9.3677e-10, 8.7027e-10, 2.2857e-10,  ..., 7.8552e-10, 3.9045e-10,
         3.3302e-10],
        [3.1048e-10, 1.8235e-10, 4.4360e-11,  ..., 2.3475e-10, 5.3882e-11,
         7.9815e-11]], device='cuda:0')
optimizer state dict: 135.0
lr: [9.99134683802993e-06, 9.99134683802993e-06]
scheduler_last_epoch: 135


Running epoch 1, step 1080, batch 32
Sampled inputs[:2]: tensor([[   0, 4294,  278,  ...,   13, 2759, 5160],
        [   0, 4137,  300,  ..., 2579,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9034e-05, -4.3254e-05, -1.5881e-04,  ..., -2.7288e-05,
         -9.0883e-05, -4.3743e-06],
        [-1.5497e-06, -1.0580e-06,  9.9838e-07,  ..., -1.2442e-06,
         -9.7603e-07, -9.6112e-07],
        [-4.0829e-06, -2.8610e-06,  2.7418e-06,  ..., -3.2783e-06,
         -2.5481e-06, -2.5332e-06],
        [-4.1127e-06, -2.7567e-06,  2.7418e-06,  ..., -3.3081e-06,
         -2.5630e-06, -2.6673e-06],
        [-3.8445e-06, -2.8163e-06,  2.5779e-06,  ..., -3.1441e-06,
         -2.5630e-06, -2.2203e-06]], device='cuda:0')
Loss: 1.0038341283798218


Running epoch 1, step 1081, batch 33
Sampled inputs[:2]: tensor([[    0,   879,    27,  ...,  3958,  2875,    14],
        [    0,  1526,  3502,  ..., 11727,  3736,  1661]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3629e-05, -1.2996e-05, -6.1639e-05,  ..., -6.3337e-06,
         -1.1456e-04, -2.0811e-05],
        [-3.1590e-06, -2.2128e-06,  1.8291e-06,  ..., -2.5705e-06,
         -2.1011e-06, -2.1532e-06],
        [-8.5533e-06, -6.1840e-06,  5.2154e-06,  ..., -6.9439e-06,
         -5.6773e-06, -5.8264e-06],
        [-8.4341e-06, -5.8115e-06,  5.0664e-06,  ..., -6.8545e-06,
         -5.6028e-06, -5.9754e-06],
        [-8.2254e-06, -6.1542e-06,  4.9919e-06,  ..., -6.8098e-06,
         -5.7966e-06, -5.2750e-06]], device='cuda:0')
Loss: 0.991207480430603


Running epoch 1, step 1082, batch 34
Sampled inputs[:2]: tensor([[   0,  266, 2604,  ...,  278, 4035, 4165],
        [   0, 4902,  518,  ..., 5493, 3227,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9199e-05, -2.8520e-04, -1.8298e-04,  ..., -1.3666e-04,
         -9.1563e-05, -1.1449e-04],
        [-4.6715e-06, -3.2783e-06,  2.8200e-06,  ..., -3.8221e-06,
         -3.0845e-06, -3.1292e-06],
        [-1.2606e-05, -9.1642e-06,  8.0019e-06,  ..., -1.0297e-05,
         -8.2999e-06, -8.4341e-06],
        [-1.2517e-05, -8.6874e-06,  7.8529e-06,  ..., -1.0237e-05,
         -8.2552e-06, -8.7321e-06],
        [-1.2070e-05, -9.0897e-06,  7.6145e-06,  ..., -1.0073e-05,
         -8.4639e-06, -7.5847e-06]], device='cuda:0')
Loss: 1.0208693742752075


Running epoch 1, step 1083, batch 35
Sampled inputs[:2]: tensor([[   0,  452,  298,  ...,  287, 1575, 7856],
        [   0, 6184, 1412,  ...,   12,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4701e-05, -2.1995e-04, -1.7405e-04,  ...,  7.4676e-05,
         -2.0439e-04, -1.1531e-04],
        [-6.2436e-06, -4.3809e-06,  3.7439e-06,  ..., -5.0962e-06,
         -4.1425e-06, -4.2021e-06],
        [-1.6809e-05, -1.2189e-05,  1.0610e-05,  ..., -1.3664e-05,
         -1.1057e-05, -1.1280e-05],
        [-1.6749e-05, -1.1593e-05,  1.0446e-05,  ..., -1.3649e-05,
         -1.1072e-05, -1.1742e-05],
        [-1.6123e-05, -1.2100e-05,  1.0118e-05,  ..., -1.3396e-05,
         -1.1280e-05, -1.0163e-05]], device='cuda:0')
Loss: 0.971141517162323


Running epoch 1, step 1084, batch 36
Sampled inputs[:2]: tensor([[    0,   266, 28695,  ...,   278,   266,  6087],
        [    0,   352,   927,  ...,  1521,  3513,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2020e-05, -2.3251e-04, -2.6871e-04,  ...,  2.8366e-05,
         -1.7103e-04, -9.2713e-05],
        [-7.7710e-06, -5.4315e-06,  4.7348e-06,  ..., -6.3479e-06,
         -5.1260e-06, -5.1633e-06],
        [-2.0832e-05, -1.5035e-05,  1.3337e-05,  ..., -1.6928e-05,
         -1.3605e-05, -1.3784e-05],
        [-2.0921e-05, -1.4395e-05,  1.3247e-05,  ..., -1.7032e-05,
         -1.3709e-05, -1.4454e-05],
        [-1.9968e-05, -1.4931e-05,  1.2696e-05,  ..., -1.6585e-05,
         -1.3873e-05, -1.2398e-05]], device='cuda:0')
Loss: 1.001083254814148


Running epoch 1, step 1085, batch 37
Sampled inputs[:2]: tensor([[    0,    12,  2418,  ...,   446,   381,  2204],
        [    0,   266,  2552,  ...,    13, 16179,   800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8493e-06, -3.2099e-04, -2.1663e-04,  ..., -3.6345e-05,
         -8.6268e-05,  5.4003e-06],
        [-9.3505e-06, -6.6012e-06,  5.6699e-06,  ..., -7.6294e-06,
         -6.1989e-06, -6.2138e-06],
        [-2.5123e-05, -1.8299e-05,  1.6004e-05,  ..., -2.0400e-05,
         -1.6496e-05, -1.6645e-05],
        [-2.5064e-05, -1.7405e-05,  1.5795e-05,  ..., -2.0355e-05,
         -1.6496e-05, -1.7330e-05],
        [-2.4140e-05, -1.8209e-05,  1.5274e-05,  ..., -2.0042e-05,
         -1.6868e-05, -1.5020e-05]], device='cuda:0')
Loss: 1.0216925144195557


Running epoch 1, step 1086, batch 38
Sampled inputs[:2]: tensor([[    0,   278, 11554,  ...,  4713,  1039, 17088],
        [    0,  2220,  1110,  ...,   382,    18,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8596e-05, -3.6298e-04, -3.3645e-04,  ...,  8.9373e-05,
         -1.4500e-04,  3.2820e-05],
        [-1.0818e-05, -7.6592e-06,  6.6459e-06,  ..., -8.8215e-06,
         -7.1041e-06, -7.1824e-06],
        [-2.9147e-05, -2.1279e-05,  1.8820e-05,  ..., -2.3663e-05,
         -1.8954e-05, -1.9282e-05],
        [-2.9027e-05, -2.0221e-05,  1.8567e-05,  ..., -2.3559e-05,
         -1.8895e-05, -2.0042e-05],
        [-2.7820e-05, -2.1055e-05,  1.7807e-05,  ..., -2.3097e-05,
         -1.9297e-05, -1.7285e-05]], device='cuda:0')
Loss: 0.9876734018325806


Running epoch 1, step 1087, batch 39
Sampled inputs[:2]: tensor([[    0,    14,    28,  ..., 16032,   694,  1441],
        [    0,  3086,   504,  ...,    14,   759,   935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1623e-04, -2.9289e-04, -4.2439e-04,  ...,  9.0333e-05,
         -1.1157e-04,  2.1325e-04],
        [-1.2361e-05, -8.8289e-06,  7.6517e-06,  ..., -1.0110e-05,
         -8.1472e-06, -8.1062e-06],
        [-3.3259e-05, -2.4483e-05,  2.1607e-05,  ..., -2.7105e-05,
         -2.1726e-05, -2.1785e-05],
        [-3.3230e-05, -2.3395e-05,  2.1398e-05,  ..., -2.7061e-05,
         -2.1696e-05, -2.2665e-05],
        [-3.1784e-05, -2.4229e-05,  2.0459e-05,  ..., -2.6479e-05,
         -2.2113e-05, -1.9550e-05]], device='cuda:0')
Loss: 1.0119878053665161
Graident accumulation at epoch 1, step 1087, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0289, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0096,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.3529e-05,  1.4800e-06, -9.7786e-05,  ...,  3.1366e-05,
         -7.5563e-06,  4.5723e-05],
        [-9.9393e-06, -5.2662e-06,  4.9162e-06,  ..., -7.9017e-06,
         -4.2151e-06, -5.8222e-06],
        [ 3.1391e-05,  3.7846e-05, -1.8539e-05,  ...,  3.7400e-05,
          2.4499e-05,  5.2691e-06],
        [ 3.8969e-05,  5.5513e-05, -2.8029e-05,  ...,  3.7210e-05,
          5.1382e-05,  1.7134e-05],
        [-2.9697e-05, -2.1318e-05,  1.8924e-05,  ..., -2.4614e-05,
         -1.8274e-05, -1.8935e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2731e-08, 5.2687e-08, 3.8021e-08,  ..., 3.8829e-08, 1.1368e-07,
         4.6056e-08],
        [6.3576e-11, 4.7723e-11, 1.1839e-11,  ..., 4.7903e-11, 1.8284e-11,
         1.9852e-11],
        [3.0233e-09, 1.8212e-09, 6.6993e-10,  ..., 2.4468e-09, 5.5511e-10,
         8.2372e-10],
        [9.3694e-10, 8.6995e-10, 2.2880e-10,  ..., 7.8547e-10, 3.9053e-10,
         3.3320e-10],
        [3.1118e-10, 1.8275e-10, 4.4734e-11,  ..., 2.3522e-10, 5.4317e-11,
         8.0118e-11]], device='cuda:0')
optimizer state dict: 136.0
lr: [9.867734078748245e-06, 9.867734078748245e-06]
scheduler_last_epoch: 136


Running epoch 1, step 1088, batch 40
Sampled inputs[:2]: tensor([[   0,   45,   17,  ...,  278, 4112,   14],
        [   0,  344, 2183,  ...,   14,  759,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7715e-05,  4.8445e-05,  1.0430e-04,  ..., -2.1802e-05,
          6.8592e-05, -1.5146e-04],
        [-1.4603e-06, -9.1270e-07,  8.2329e-07,  ..., -1.2442e-06,
         -9.3505e-07, -1.0952e-06],
        [-3.8445e-06, -2.5034e-06,  2.3693e-06,  ..., -3.2336e-06,
         -2.4140e-06, -2.8014e-06],
        [-3.9041e-06, -2.3395e-06,  2.3395e-06,  ..., -3.2932e-06,
         -2.4438e-06, -3.0100e-06],
        [-3.8743e-06, -2.6077e-06,  2.3693e-06,  ..., -3.3081e-06,
         -2.5779e-06, -2.6375e-06]], device='cuda:0')
Loss: 0.9668734669685364


Running epoch 1, step 1089, batch 41
Sampled inputs[:2]: tensor([[    0,   897,   328,  ...,   908,   696,   688],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4305e-05,  1.2648e-04, -6.7452e-05,  ...,  8.2657e-05,
         -7.0542e-05, -3.5392e-04],
        [-2.8014e-06, -1.8515e-06,  1.7323e-06,  ..., -2.4065e-06,
         -1.8626e-06, -2.0228e-06],
        [-7.3761e-06, -5.0366e-06,  4.9621e-06,  ..., -6.2138e-06,
         -4.7833e-06, -5.1409e-06],
        [-7.5996e-06, -4.8280e-06,  5.0217e-06,  ..., -6.4522e-06,
         -4.9621e-06, -5.6475e-06],
        [-7.3612e-06, -5.2154e-06,  4.9025e-06,  ..., -6.3032e-06,
         -5.0664e-06, -4.7386e-06]], device='cuda:0')
Loss: 0.9349066019058228


Running epoch 1, step 1090, batch 42
Sampled inputs[:2]: tensor([[   0, 2270, 3279,  ...,  380,  475,  768],
        [   0,  560,  199,  ...,   29,  445,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7809e-05,  1.7064e-04, -2.7033e-04,  ...,  6.8483e-05,
         -5.8981e-05, -8.2011e-04],
        [-4.2468e-06, -2.9169e-06,  2.4922e-06,  ..., -3.6955e-06,
         -2.9057e-06, -3.0361e-06],
        [-1.1250e-05, -8.0168e-06,  7.2122e-06,  ..., -9.6262e-06,
         -7.5251e-06, -7.8082e-06],
        [-1.1504e-05, -7.6890e-06,  7.1824e-06,  ..., -9.9391e-06,
         -7.7933e-06, -8.4937e-06],
        [-1.1116e-05, -8.2105e-06,  7.0781e-06,  ..., -9.6858e-06,
         -7.8678e-06, -7.1377e-06]], device='cuda:0')
Loss: 0.9797930717468262


Running epoch 1, step 1091, batch 43
Sampled inputs[:2]: tensor([[    0,   471,    12,  ...,    13,  9909,  2673],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0417e-04,  2.3504e-04, -4.0607e-04,  ...,  7.8750e-05,
         -2.3947e-05, -8.4613e-04],
        [-5.7220e-06, -4.0196e-06,  3.4310e-06,  ..., -4.9174e-06,
         -3.8892e-06, -3.9376e-06],
        [-1.5214e-05, -1.1086e-05,  9.8646e-06,  ..., -1.2904e-05,
         -1.0177e-05, -1.0222e-05],
        [-1.5527e-05, -1.0684e-05,  9.8497e-06,  ..., -1.3247e-05,
         -1.0490e-05, -1.1042e-05],
        [-1.4901e-05, -1.1280e-05,  9.6112e-06,  ..., -1.2890e-05,
         -1.0610e-05, -9.2834e-06]], device='cuda:0')
Loss: 1.0161553621292114


Running epoch 1, step 1092, batch 44
Sampled inputs[:2]: tensor([[    0, 33315,   266,  ...,    12,  1126,    14],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0325e-05,  2.4791e-04, -4.8470e-04,  ...,  6.8655e-05,
         -8.2743e-05, -8.4997e-04],
        [-7.1898e-06, -5.0552e-06,  4.3251e-06,  ..., -6.1169e-06,
         -4.8392e-06, -4.9062e-06],
        [-1.9267e-05, -1.4052e-05,  1.2502e-05,  ..., -1.6212e-05,
         -1.2755e-05, -1.2890e-05],
        [-1.9580e-05, -1.3486e-05,  1.2428e-05,  ..., -1.6555e-05,
         -1.3098e-05, -1.3873e-05],
        [-1.8775e-05, -1.4216e-05,  1.2100e-05,  ..., -1.6108e-05,
         -1.3232e-05, -1.1653e-05]], device='cuda:0')
Loss: 0.9948360919952393


Running epoch 1, step 1093, batch 45
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,  5933,    35,  5621],
        [    0,   266,  1234,  ...,   908,   328, 26300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4145e-05,  1.7823e-04, -6.4188e-04,  ...,  2.7773e-05,
         -2.2697e-04, -1.0522e-03],
        [-8.6650e-06, -6.1803e-06,  5.3607e-06,  ..., -7.2941e-06,
         -5.7220e-06, -5.7295e-06],
        [ 1.9604e-04,  1.3464e-04, -1.3999e-04,  ...,  2.3644e-04,
          1.0504e-04,  1.1525e-04],
        [-2.3574e-05, -1.6496e-05,  1.5333e-05,  ..., -1.9729e-05,
         -1.5438e-05, -1.6212e-05],
        [-2.2531e-05, -1.7256e-05,  1.4797e-05,  ..., -1.9178e-05,
         -1.5616e-05, -1.3635e-05]], device='cuda:0')
Loss: 1.0278913974761963


Running epoch 1, step 1094, batch 46
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,   368,   266,  ...,   591,   767,   824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2423e-06,  1.7811e-04, -6.5989e-04,  ..., -5.1907e-05,
         -1.1555e-04, -1.0650e-03],
        [-1.0207e-05, -7.2308e-06,  6.2175e-06,  ..., -8.5756e-06,
         -6.7800e-06, -6.8173e-06],
        [ 1.9178e-04,  1.3165e-04, -1.3746e-04,  ...,  2.3293e-04,
          1.0219e-04,  1.1229e-04],
        [-2.7776e-05, -1.9327e-05,  1.7762e-05,  ..., -2.3216e-05,
         -1.8299e-05, -1.9297e-05],
        [-2.6643e-05, -2.0266e-05,  1.7226e-05,  ..., -2.2650e-05,
         -1.8522e-05, -1.6332e-05]], device='cuda:0')
Loss: 1.022670030593872


Running epoch 1, step 1095, batch 47
Sampled inputs[:2]: tensor([[    0,  4209,   278,  ...,   287,  9971,   717],
        [    0,   578, 26976,  ...,  1389,    14,  1742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7345e-05,  4.6880e-04, -5.9475e-04,  ..., -2.0259e-05,
         -6.0243e-05, -9.4899e-04],
        [-1.1854e-05, -8.3484e-06,  7.0073e-06,  ..., -9.8944e-06,
         -7.8529e-06, -8.1584e-06],
        [ 1.8716e-04,  1.2831e-04, -1.3503e-04,  ...,  2.2919e-04,
          9.9077e-05,  1.0853e-04],
        [-3.2008e-05, -2.2188e-05,  1.9893e-05,  ..., -2.6613e-05,
         -2.1115e-05, -2.2814e-05],
        [-3.1382e-05, -2.3738e-05,  1.9684e-05,  ..., -2.6524e-05,
         -2.1845e-05, -1.9968e-05]], device='cuda:0')
Loss: 0.9438072443008423
Graident accumulation at epoch 1, step 1095, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0155, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2442e-05,  4.8212e-05, -1.4748e-04,  ...,  2.6204e-05,
         -1.2825e-05, -5.3748e-05],
        [-1.0131e-05, -5.5744e-06,  5.1253e-06,  ..., -8.1010e-06,
         -4.5789e-06, -6.0558e-06],
        [ 4.6967e-05,  4.6893e-05, -3.0188e-05,  ...,  5.6578e-05,
          3.1957e-05,  1.5595e-05],
        [ 3.1872e-05,  4.7743e-05, -2.3236e-05,  ...,  3.0828e-05,
          4.4133e-05,  1.3139e-05],
        [-2.9866e-05, -2.1560e-05,  1.9000e-05,  ..., -2.4805e-05,
         -1.8631e-05, -1.9038e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2666e-08, 5.2854e-08, 3.8336e-08,  ..., 3.8791e-08, 1.1357e-07,
         4.6910e-08],
        [6.3653e-11, 4.7745e-11, 1.1876e-11,  ..., 4.7953e-11, 1.8327e-11,
         1.9899e-11],
        [3.0553e-09, 1.8358e-09, 6.8750e-10,  ..., 2.4969e-09, 5.6437e-10,
         8.3467e-10],
        [9.3703e-10, 8.6957e-10, 2.2897e-10,  ..., 7.8539e-10, 3.9059e-10,
         3.3339e-10],
        [3.1186e-10, 1.8313e-10, 4.5077e-11,  ..., 2.3569e-10, 5.4740e-11,
         8.0436e-11]], device='cuda:0')
optimizer state dict: 137.0
lr: [9.744141530853894e-06, 9.744141530853894e-06]
scheduler_last_epoch: 137


Running epoch 1, step 1096, batch 48
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,   968,   266,  ...,   287,  2143, 15228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4453e-04, -7.5520e-05, -1.4632e-04,  ...,  5.6665e-06,
         -9.0541e-05,  4.3648e-05],
        [-1.3709e-06, -1.0431e-06,  9.2760e-07,  ..., -1.1399e-06,
         -8.9407e-07, -8.2329e-07],
        [-3.7253e-06, -2.8908e-06,  2.6524e-06,  ..., -3.0696e-06,
         -2.3693e-06, -2.2203e-06],
        [-3.8445e-06, -2.8461e-06,  2.7269e-06,  ..., -3.1888e-06,
         -2.4736e-06, -2.4438e-06],
        [-3.5912e-06, -2.8908e-06,  2.5332e-06,  ..., -3.0249e-06,
         -2.4438e-06, -1.9819e-06]], device='cuda:0')
Loss: 0.9959492683410645


Running epoch 1, step 1097, batch 49
Sampled inputs[:2]: tensor([[   0, 6518,  681,  ...,  401, 9748,  391],
        [   0, 3441,  796,  ..., 7561, 1711,  857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8106e-05, -1.0175e-06, -1.5399e-04,  ..., -1.6268e-05,
          2.1189e-04,  2.2444e-04],
        [-2.8089e-06, -2.0340e-06,  1.7770e-06,  ..., -2.3246e-06,
         -1.8068e-06, -1.7695e-06],
        [-7.5698e-06, -5.6326e-06,  5.1111e-06,  ..., -6.1840e-06,
         -4.7535e-06, -4.6939e-06],
        [-7.8976e-06, -5.5581e-06,  5.2303e-06,  ..., -6.4969e-06,
         -5.0068e-06, -5.1707e-06],
        [-7.3314e-06, -5.6326e-06,  4.9025e-06,  ..., -6.1244e-06,
         -4.9323e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.0292143821716309


Running epoch 1, step 1098, batch 50
Sampled inputs[:2]: tensor([[   0,   12, 1471,  ..., 1356,  600,   12],
        [   0, 4175,  437,  ..., 1700,   14,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4040e-05,  1.0557e-04, -3.2155e-04,  ...,  1.4768e-04,
         -3.5454e-05,  1.8606e-04],
        [-4.1798e-06, -3.0473e-06,  2.7157e-06,  ..., -3.4571e-06,
         -2.6710e-06, -2.5854e-06],
        [-1.1176e-05, -8.3745e-06,  7.7486e-06,  ..., -9.1493e-06,
         -6.9886e-06, -6.7949e-06],
        [-1.1638e-05, -8.2552e-06,  7.9423e-06,  ..., -9.5665e-06,
         -7.3314e-06, -7.5251e-06],
        [-1.0729e-05, -8.3148e-06,  7.3612e-06,  ..., -8.9705e-06,
         -7.1824e-06, -6.0052e-06]], device='cuda:0')
Loss: 0.964329719543457


Running epoch 1, step 1099, batch 51
Sampled inputs[:2]: tensor([[    0,   287,  4599,  ..., 11812,   266,  1036],
        [    0,  2159,   271,  ...,  1268,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9778e-05,  1.0557e-04, -2.3833e-04,  ...,  9.5743e-05,
          1.0464e-04,  2.8222e-04],
        [-5.7071e-06, -4.1500e-06,  3.5726e-06,  ..., -4.7758e-06,
         -3.6992e-06, -3.6061e-06],
        [-1.5318e-05, -1.1459e-05,  1.0252e-05,  ..., -1.2681e-05,
         -9.7156e-06, -9.5367e-06],
        [-1.5721e-05, -1.1146e-05,  1.0356e-05,  ..., -1.3053e-05,
         -1.0028e-05, -1.0356e-05],
        [-1.4871e-05, -1.1474e-05,  9.8348e-06,  ..., -1.2547e-05,
         -1.0058e-05, -8.5682e-06]], device='cuda:0')
Loss: 0.998605489730835


Running epoch 1, step 1100, batch 52
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 23648,   292, 21424],
        [    0,  5024,  3846,  ...,  5880,  1377,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3321e-05,  8.8761e-05, -2.8341e-04,  ..., -1.1097e-04,
          2.0608e-04,  3.6675e-04],
        [-7.1302e-06, -5.2601e-06,  4.5486e-06,  ..., -5.9679e-06,
         -4.6678e-06, -4.4182e-06],
        [-1.9044e-05, -1.4469e-05,  1.2979e-05,  ..., -1.5780e-05,
         -1.2234e-05, -1.1638e-05],
        [-1.9684e-05, -1.4231e-05,  1.3217e-05,  ..., -1.6376e-05,
         -1.2740e-05, -1.2755e-05],
        [-1.8492e-05, -1.4529e-05,  1.2457e-05,  ..., -1.5646e-05,
         -1.2681e-05, -1.0461e-05]], device='cuda:0')
Loss: 1.0168863534927368


Running epoch 1, step 1101, batch 53
Sampled inputs[:2]: tensor([[   0,  446, 1115,  ..., 1869, 4971, 1954],
        [   0, 3227,  300,  ..., 1817, 5709,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7910e-04,  1.7146e-04, -2.8894e-04,  ..., -4.6747e-05,
          4.2511e-05,  3.1807e-04],
        [-8.5607e-06, -6.3032e-06,  5.5470e-06,  ..., -7.1302e-06,
         -5.5209e-06, -5.1893e-06],
        [-2.2888e-05, -1.7375e-05,  1.5795e-05,  ..., -1.8910e-05,
         -1.4529e-05, -1.3724e-05],
        [-2.3648e-05, -1.7107e-05,  1.6093e-05,  ..., -1.9580e-05,
         -1.5095e-05, -1.5005e-05],
        [-2.2113e-05, -1.7375e-05,  1.5080e-05,  ..., -1.8656e-05,
         -1.5005e-05, -1.2279e-05]], device='cuda:0')
Loss: 1.0061264038085938


Running epoch 1, step 1102, batch 54
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,   278,   266,  ...,    13,  2853,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3151e-04,  2.3046e-04, -2.8636e-04,  ...,  1.1325e-04,
          6.1537e-05,  4.1718e-04],
        [-9.9689e-06, -7.3165e-06,  6.4149e-06,  ..., -8.2925e-06,
         -6.4149e-06, -6.1654e-06],
        [-2.6733e-05, -2.0236e-05,  1.8314e-05,  ..., -2.2039e-05,
         -1.6913e-05, -1.6361e-05],
        [-2.7642e-05, -1.9908e-05,  1.8671e-05,  ..., -2.2843e-05,
         -1.7554e-05, -1.7866e-05],
        [-2.5839e-05, -2.0221e-05,  1.7464e-05,  ..., -2.1756e-05,
         -1.7479e-05, -1.4678e-05]], device='cuda:0')
Loss: 0.9656364917755127


Running epoch 1, step 1103, batch 55
Sampled inputs[:2]: tensor([[    0, 13751,    12,  ...,  1264,  5676,   367],
        [    0,   278,  7524,  ...,  1288,   669,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4699e-04,  2.5152e-04, -3.6295e-04,  ...,  2.3007e-04,
          5.4955e-05,  3.4743e-04],
        [-1.1280e-05, -8.2143e-06,  7.3165e-06,  ..., -9.4250e-06,
         -7.2308e-06, -7.0781e-06],
        [-3.0234e-05, -2.2635e-05,  2.0891e-05,  ..., -2.4959e-05,
         -1.8969e-05, -1.8686e-05],
        [-3.1456e-05, -2.2382e-05,  2.1458e-05,  ..., -2.6092e-05,
         -1.9848e-05, -2.0608e-05],
        [-2.9191e-05, -2.2605e-05,  1.9908e-05,  ..., -2.4587e-05,
         -1.9595e-05, -1.6704e-05]], device='cuda:0')
Loss: 0.9662415385246277
Graident accumulation at epoch 1, step 1103, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0028, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3502e-05,  6.8543e-05, -1.6903e-04,  ...,  4.6590e-05,
         -6.0470e-06, -1.3630e-05],
        [-1.0246e-05, -5.8384e-06,  5.3445e-06,  ..., -8.2334e-06,
         -4.8441e-06, -6.1581e-06],
        [ 3.9247e-05,  3.9940e-05, -2.5080e-05,  ...,  4.8425e-05,
          2.6865e-05,  1.2167e-05],
        [ 2.5539e-05,  4.0730e-05, -1.8767e-05,  ...,  2.5136e-05,
          3.7735e-05,  9.7646e-06],
        [-2.9798e-05, -2.1665e-05,  1.9091e-05,  ..., -2.4784e-05,
         -1.8728e-05, -1.8804e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2714e-08, 5.2865e-08, 3.8430e-08,  ..., 3.8805e-08, 1.1346e-07,
         4.6984e-08],
        [6.3717e-11, 4.7765e-11, 1.1918e-11,  ..., 4.7993e-11, 1.8361e-11,
         1.9929e-11],
        [3.0532e-09, 1.8345e-09, 6.8724e-10,  ..., 2.4950e-09, 5.6417e-10,
         8.3419e-10],
        [9.3708e-10, 8.6921e-10, 2.2920e-10,  ..., 7.8529e-10, 3.9059e-10,
         3.3348e-10],
        [3.1240e-10, 1.8346e-10, 4.5428e-11,  ..., 2.3606e-10, 5.5069e-11,
         8.0635e-11]], device='cuda:0')
optimizer state dict: 138.0
lr: [9.620588080367043e-06, 9.620588080367043e-06]
scheduler_last_epoch: 138


Running epoch 1, step 1104, batch 56
Sampled inputs[:2]: tensor([[    0, 29883,   680,  ...,  3363,  1049,   292],
        [    0, 17471,  7279,  ...,   328,  6179,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5873e-04,  2.0173e-05,  8.8453e-05,  ..., -8.8100e-05,
          7.8351e-05, -6.1987e-06],
        [-1.4007e-06, -1.1250e-06,  9.0897e-07,  ..., -1.1995e-06,
         -9.6112e-07, -7.6741e-07],
        [-3.7551e-06, -3.0696e-06,  2.5630e-06,  ..., -3.2187e-06,
         -2.5481e-06, -2.1011e-06],
        [-3.9637e-06, -3.1292e-06,  2.6673e-06,  ..., -3.3826e-06,
         -2.6673e-06, -2.2799e-06],
        [-3.7104e-06, -3.1590e-06,  2.4885e-06,  ..., -3.2634e-06,
         -2.7120e-06, -1.9521e-06]], device='cuda:0')
Loss: 1.0315927267074585


Running epoch 1, step 1105, batch 57
Sampled inputs[:2]: tensor([[   0,   12,  616,  ...,  278,  266, 2907],
        [   0,  461,  654,  ..., 4145, 7600, 4142]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9431e-04, -2.5309e-04, -1.2223e-05,  ..., -2.4673e-04,
          1.9912e-04,  1.5858e-04],
        [-2.7493e-06, -2.1532e-06,  1.9073e-06,  ..., -2.3097e-06,
         -1.7844e-06, -1.5162e-06],
        [ 7.6582e-05,  1.6700e-04, -7.6723e-05,  ...,  1.0821e-04,
          1.1603e-04,  5.0090e-05],
        [-7.8082e-06, -5.9903e-06,  5.6475e-06,  ..., -6.5267e-06,
         -4.9621e-06, -4.5300e-06],
        [-7.1228e-06, -5.9456e-06,  5.1260e-06,  ..., -6.1393e-06,
         -4.9472e-06, -3.7104e-06]], device='cuda:0')
Loss: 1.0053393840789795


Running epoch 1, step 1106, batch 58
Sampled inputs[:2]: tensor([[    0,  1086,   292,  ...,  1400,   367,  1874],
        [    0,   266,  1176,  ...,   199, 17791,  3662]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8833e-04, -1.9568e-04, -2.2201e-04,  ...,  6.5334e-06,
         -7.5388e-05, -5.9505e-05],
        [-4.2096e-06, -3.1292e-06,  2.8461e-06,  ..., -3.5018e-06,
         -2.6301e-06, -2.3656e-06],
        [ 7.2738e-05,  1.6433e-04, -7.4085e-05,  ...,  1.0508e-04,
          1.1380e-04,  4.7840e-05],
        [-1.1891e-05, -8.7023e-06,  8.4192e-06,  ..., -9.8497e-06,
         -7.3314e-06, -7.0184e-06],
        [-1.0774e-05, -8.5831e-06,  7.6294e-06,  ..., -9.1791e-06,
         -7.2569e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9686179161071777


Running epoch 1, step 1107, batch 59
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,    18,    14,  ...,   380,   981,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2710e-04, -2.7435e-05, -6.3026e-05,  ..., -7.3077e-06,
         -2.0620e-04, -1.2999e-04],
        [-5.6028e-06, -4.1127e-06,  3.7700e-06,  ..., -4.6641e-06,
         -3.5241e-06, -3.2559e-06],
        [ 6.9027e-05,  1.6160e-04, -7.1418e-05,  ...,  1.0201e-04,
          1.1142e-04,  4.5501e-05],
        [-1.5765e-05, -1.1370e-05,  1.1161e-05,  ..., -1.3068e-05,
         -9.8199e-06, -9.6112e-06],
        [-1.4380e-05, -1.1340e-05,  1.0192e-05,  ..., -1.2234e-05,
         -9.7305e-06, -7.7486e-06]], device='cuda:0')
Loss: 0.9996686577796936


Running epoch 1, step 1108, batch 60
Sampled inputs[:2]: tensor([[    0,    14, 49601,  ...,    12,   298,   374],
        [    0,   659,   278,  ...,   769,  1728,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1038e-04,  5.3096e-05, -1.1242e-05,  ..., -2.6793e-06,
         -1.2829e-04, -6.5592e-05],
        [-6.9961e-06, -5.1782e-06,  4.6380e-06,  ..., -5.8487e-06,
         -4.4703e-06, -4.1015e-06],
        [ 6.5377e-05,  1.5871e-04, -6.8974e-05,  ...,  9.8928e-05,
          1.0899e-04,  4.3280e-05],
        [-1.9535e-05, -1.4216e-05,  1.3635e-05,  ..., -1.6272e-05,
         -1.2383e-05, -1.2055e-05],
        [-1.7956e-05, -1.4275e-05,  1.2547e-05,  ..., -1.5318e-05,
         -1.2279e-05, -9.7305e-06]], device='cuda:0')
Loss: 0.9670602083206177


Running epoch 1, step 1109, batch 61
Sampled inputs[:2]: tensor([[    0,   287,  3609,  ...,  3661,  5944,   838],
        [    0,  1690, 16858,  ...,   199,   395,  3902]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9077e-04,  2.1323e-04, -3.6208e-05,  ...,  3.0838e-06,
          1.0081e-05, -7.1899e-05],
        [-8.3074e-06, -6.1169e-06,  5.4650e-06,  ..., -6.9663e-06,
         -5.3197e-06, -4.9137e-06],
        [ 6.1905e-05,  1.5614e-04, -6.6590e-05,  ...,  9.6007e-05,
          1.0678e-04,  4.1194e-05],
        [-2.3186e-05, -1.6734e-05,  1.6093e-05,  ..., -1.9342e-05,
         -1.4693e-05, -1.4395e-05],
        [-2.1368e-05, -1.6898e-05,  1.4871e-05,  ..., -1.8224e-05,
         -1.4588e-05, -1.1593e-05]], device='cuda:0')
Loss: 0.9722726345062256


Running epoch 1, step 1110, batch 62
Sampled inputs[:2]: tensor([[    0,   729,  3084,  ...,   381,  1445,   642],
        [    0,   368, 46614,  ...,  1070,   278,  1028]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1885e-04,  1.3116e-04, -1.7139e-04,  ...,  8.5536e-05,
          2.2633e-04, -3.4023e-05],
        [-9.7007e-06, -7.1824e-06,  6.3479e-06,  ..., -8.1435e-06,
         -6.2138e-06, -5.7742e-06],
        [ 2.1798e-04,  3.1140e-04, -1.5957e-04,  ...,  1.9932e-04,
          1.9298e-04,  9.2554e-05],
        [-2.7090e-05, -1.9625e-05,  1.8701e-05,  ..., -2.2605e-05,
         -1.7136e-05, -1.6913e-05],
        [-2.5034e-05, -1.9833e-05,  1.7345e-05,  ..., -2.1324e-05,
         -1.7032e-05, -1.3620e-05]], device='cuda:0')
Loss: 1.0174163579940796


Running epoch 1, step 1111, batch 63
Sampled inputs[:2]: tensor([[    0,    13,  1107,  ...,   287, 25185,    14],
        [    0, 43071,   278,  ...,   266, 21576,  5936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0595e-04, -1.7425e-04, -1.1520e-04,  ..., -9.0643e-05,
          5.6759e-04,  3.3095e-04],
        [-1.1101e-05, -8.1062e-06,  7.1786e-06,  ..., -9.3654e-06,
         -7.1079e-06, -6.8173e-06],
        [ 2.1416e-04,  3.0887e-04, -1.5714e-04,  ...,  1.9610e-04,
          1.9064e-04,  8.9797e-05],
        [-3.0994e-05, -2.2098e-05,  2.1145e-05,  ..., -2.5928e-05,
         -1.9535e-05, -1.9848e-05],
        [-2.8834e-05, -2.2426e-05,  1.9759e-05,  ..., -2.4572e-05,
         -1.9506e-05, -1.6153e-05]], device='cuda:0')
Loss: 0.9890820384025574
Graident accumulation at epoch 1, step 1111, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0022,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.1746e-05,  4.4264e-05, -1.6365e-04,  ...,  3.2866e-05,
          5.1317e-05,  2.0828e-05],
        [-1.0331e-05, -6.0652e-06,  5.5279e-06,  ..., -8.3466e-06,
         -5.0705e-06, -6.2240e-06],
        [ 5.6738e-05,  6.6833e-05, -3.8287e-05,  ...,  6.3192e-05,
          4.3242e-05,  1.9930e-05],
        [ 1.9885e-05,  3.4448e-05, -1.4776e-05,  ...,  2.0029e-05,
          3.2008e-05,  6.8033e-06],
        [-2.9702e-05, -2.1741e-05,  1.9158e-05,  ..., -2.4762e-05,
         -1.8805e-05, -1.8539e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2897e-08, 5.2842e-08, 3.8405e-08,  ..., 3.8775e-08, 1.1367e-07,
         4.7047e-08],
        [6.3776e-11, 4.7783e-11, 1.1957e-11,  ..., 4.8033e-11, 1.8394e-11,
         1.9956e-11],
        [3.0960e-09, 1.9281e-09, 7.1125e-10,  ..., 2.5310e-09, 5.9995e-10,
         8.4142e-10],
        [9.3710e-10, 8.6882e-10, 2.2942e-10,  ..., 7.8517e-10, 3.9058e-10,
         3.3354e-10],
        [3.1292e-10, 1.8378e-10, 4.5773e-11,  ..., 2.3643e-10, 5.5394e-11,
         8.0815e-11]], device='cuda:0')
optimizer state dict: 139.0
lr: [9.497092607333449e-06, 9.497092607333449e-06]
scheduler_last_epoch: 139


Running epoch 1, step 1112, batch 64
Sampled inputs[:2]: tensor([[   0,  278, 6318,  ...,  458,   17,    9],
        [   0,  590,   16,  ...,   13,   35, 1151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5410e-05,  1.6296e-04, -3.8193e-05,  ...,  3.4314e-05,
          8.4014e-05,  8.9271e-05],
        [-1.4603e-06, -1.0803e-06,  9.6858e-07,  ..., -1.2293e-06,
         -9.0897e-07, -8.3819e-07],
        [-3.6359e-06, -2.8312e-06,  2.5928e-06,  ..., -3.0845e-06,
         -2.2948e-06, -2.0713e-06],
        [-3.9041e-06, -2.9057e-06,  2.7418e-06,  ..., -3.3081e-06,
         -2.4587e-06, -2.3395e-06],
        [-3.5614e-06, -2.8759e-06,  2.5183e-06,  ..., -3.0696e-06,
         -2.3991e-06, -1.8626e-06]], device='cuda:0')
Loss: 1.0027227401733398


Running epoch 1, step 1113, batch 65
Sampled inputs[:2]: tensor([[   0,   14,  560,  ...,   12, 8593,  266],
        [   0, 6328,   12,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2488e-04,  2.7513e-04, -1.7116e-04,  ...,  4.4338e-05,
          4.1976e-05, -1.6725e-04],
        [-2.8983e-06, -2.1234e-06,  1.8664e-06,  ..., -2.4438e-06,
         -1.7658e-06, -1.6950e-06],
        [-7.4357e-06, -5.6624e-06,  5.1111e-06,  ..., -6.2436e-06,
         -4.4852e-06, -4.3362e-06],
        [-7.9274e-06, -5.7518e-06,  5.3644e-06,  ..., -6.6608e-06,
         -4.7982e-06, -4.8876e-06],
        [-7.1824e-06, -5.6922e-06,  4.9025e-06,  ..., -6.1691e-06,
         -4.6641e-06, -3.8594e-06]], device='cuda:0')
Loss: 0.9861343502998352


Running epoch 1, step 1114, batch 66
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0,    27,  5375,  ...,  5357, 14933, 10944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1952e-05,  4.4233e-04, -9.6194e-05,  ..., -1.3022e-04,
          3.6144e-04,  2.4083e-04],
        [-4.3884e-06, -3.2187e-06,  2.7381e-06,  ..., -3.6955e-06,
         -2.7344e-06, -2.7753e-06],
        [-1.1519e-05, -8.7470e-06,  7.7039e-06,  ..., -9.6112e-06,
         -7.1079e-06, -7.2122e-06],
        [-1.1891e-05, -8.6129e-06,  7.8231e-06,  ..., -9.9689e-06,
         -7.3612e-06, -7.8231e-06],
        [-1.1146e-05, -8.7768e-06,  7.4059e-06,  ..., -9.5218e-06,
         -7.3761e-06, -6.4671e-06]], device='cuda:0')
Loss: 0.9529323577880859


Running epoch 1, step 1115, batch 67
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9651e-05,  4.9731e-04, -9.2708e-05,  ...,  2.1350e-05,
          3.9912e-04,  3.5807e-04],
        [-5.7444e-06, -4.3064e-06,  3.6433e-06,  ..., -4.8876e-06,
         -3.6880e-06, -3.5539e-06],
        [-1.5110e-05, -1.1697e-05,  1.0252e-05,  ..., -1.2755e-05,
         -9.5665e-06, -9.2387e-06],
        [-1.5676e-05, -1.1578e-05,  1.0446e-05,  ..., -1.3277e-05,
         -9.9838e-06, -1.0088e-05],
        [-1.4693e-05, -1.1802e-05,  9.8944e-06,  ..., -1.2681e-05,
         -9.9689e-06, -8.3148e-06]], device='cuda:0')
Loss: 1.027478814125061


Running epoch 1, step 1116, batch 68
Sampled inputs[:2]: tensor([[    0, 31571,    13,  ...,   367,  2177,   271],
        [    0,  8290,   391,  ...,   298,  1253,     7]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9095e-05,  5.7085e-04, -1.5117e-05,  ...,  3.0119e-05,
          5.1291e-04,  3.5807e-04],
        [-7.0855e-06, -5.2340e-06,  4.5337e-06,  ..., -6.0499e-06,
         -4.5672e-06, -4.4554e-06],
        [-1.8597e-05, -1.4171e-05,  1.2755e-05,  ..., -1.5691e-05,
         -1.1742e-05, -1.1519e-05],
        [-1.9357e-05, -1.4007e-05,  1.3009e-05,  ..., -1.6406e-05,
         -1.2323e-05, -1.2681e-05],
        [-1.8135e-05, -1.4335e-05,  1.2338e-05,  ..., -1.5631e-05,
         -1.2264e-05, -1.0356e-05]], device='cuda:0')
Loss: 0.9885755181312561


Running epoch 1, step 1117, batch 69
Sampled inputs[:2]: tensor([[   0,  380,  560,  ...,  287, 6769,  806],
        [   0, 5007, 7551,  ...,    9, 2095,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0249e-08,  4.0293e-04, -1.6482e-04,  ..., -1.6711e-04,
          6.7522e-04,  3.8012e-04],
        [-8.5384e-06, -6.2399e-06,  5.4799e-06,  ..., -7.2271e-06,
         -5.4650e-06, -5.2825e-06],
        [-2.2277e-05, -1.6794e-05,  1.5318e-05,  ..., -1.8626e-05,
         -1.3977e-05, -1.3590e-05],
        [-2.3291e-05, -1.6674e-05,  1.5691e-05,  ..., -1.9565e-05,
         -1.4737e-05, -1.5035e-05],
        [-2.1785e-05, -1.7062e-05,  1.4856e-05,  ..., -1.8626e-05,
         -1.4648e-05, -1.2219e-05]], device='cuda:0')
Loss: 1.0022917985916138


Running epoch 1, step 1118, batch 70
Sampled inputs[:2]: tensor([[    0,  5281,  4452,  ...,    14,  3391,    12],
        [    0,    14, 22157,  ...,  2341,   508, 22960]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5620e-05,  4.3849e-04, -7.4266e-05,  ..., -2.3154e-04,
          6.4276e-04,  3.7094e-04],
        [-9.9912e-06, -7.3053e-06,  6.5006e-06,  ..., -8.3819e-06,
         -6.3404e-06, -6.0722e-06],
        [-2.6181e-05, -1.9729e-05,  1.8224e-05,  ..., -2.1741e-05,
         -1.6332e-05, -1.5751e-05],
        [-2.7314e-05, -1.9580e-05,  1.8671e-05,  ..., -2.2754e-05,
         -1.7136e-05, -1.7345e-05],
        [-2.5436e-05, -1.9923e-05,  1.7524e-05,  ..., -2.1607e-05,
         -1.7032e-05, -1.4082e-05]], device='cuda:0')
Loss: 0.9916287064552307


Running epoch 1, step 1119, batch 71
Sampled inputs[:2]: tensor([[   0, 1976, 1329,  ...,  278, 9469,  292],
        [   0,  266, 1441,  ..., 1817, 1589,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5417e-05,  4.0742e-04, -1.1177e-05,  ..., -2.2872e-04,
          7.0730e-04,  1.8248e-04],
        [-1.1377e-05, -8.3558e-06,  7.4171e-06,  ..., -9.5293e-06,
         -7.2196e-06, -6.8508e-06],
        [-2.9802e-05, -2.2575e-05,  2.0772e-05,  ..., -2.4721e-05,
         -1.8626e-05, -1.7762e-05],
        [-3.1099e-05, -2.2411e-05,  2.1309e-05,  ..., -2.5868e-05,
         -1.9535e-05, -1.9595e-05],
        [-2.8849e-05, -2.2739e-05,  1.9908e-05,  ..., -2.4483e-05,
         -1.9372e-05, -1.5825e-05]], device='cuda:0')
Loss: 0.9790515303611755
Graident accumulation at epoch 1, step 1119, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0284,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2030e-05,  8.0579e-05, -1.4840e-04,  ...,  6.7077e-06,
          1.1692e-04,  3.6993e-05],
        [-1.0436e-05, -6.2943e-06,  5.7168e-06,  ..., -8.4648e-06,
         -5.2854e-06, -6.2867e-06],
        [ 4.8084e-05,  5.7892e-05, -3.2381e-05,  ...,  5.4401e-05,
          3.7055e-05,  1.6161e-05],
        [ 1.4787e-05,  2.8762e-05, -1.1167e-05,  ...,  1.5440e-05,
          2.6853e-05,  4.1635e-06],
        [-2.9617e-05, -2.1841e-05,  1.9233e-05,  ..., -2.4734e-05,
         -1.8862e-05, -1.8268e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2825e-08, 5.2955e-08, 3.8366e-08,  ..., 3.8788e-08, 1.1406e-07,
         4.7033e-08],
        [6.3842e-11, 4.7805e-11, 1.2000e-11,  ..., 4.8076e-11, 1.8427e-11,
         1.9983e-11],
        [3.0938e-09, 1.9267e-09, 7.1097e-10,  ..., 2.5291e-09, 5.9970e-10,
         8.4089e-10],
        [9.3713e-10, 8.6846e-10, 2.2964e-10,  ..., 7.8506e-10, 3.9057e-10,
         3.3359e-10],
        [3.1344e-10, 1.8411e-10, 4.6124e-11,  ..., 2.3679e-10, 5.5714e-11,
         8.0985e-11]], device='cuda:0')
optimizer state dict: 140.0
lr: [9.373673982939395e-06, 9.373673982939395e-06]
scheduler_last_epoch: 140


Running epoch 1, step 1120, batch 72
Sampled inputs[:2]: tensor([[    0,  5625,  2558,  ...,   680,   292,   494],
        [    0, 13081,   278,  ...,   368,   266,  1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1434e-06, -1.1597e-04, -5.9659e-05,  ..., -3.2141e-05,
          1.1554e-04,  4.5882e-05],
        [-1.4529e-06, -1.0356e-06,  1.0207e-06,  ..., -1.1921e-06,
         -8.6427e-07, -8.1584e-07],
        [-3.6806e-06, -2.7269e-06,  2.7269e-06,  ..., -2.9951e-06,
         -2.1756e-06, -2.0862e-06],
        [-3.9339e-06, -2.7716e-06,  2.8759e-06,  ..., -3.1888e-06,
         -2.3097e-06, -2.3246e-06],
        [-3.5316e-06, -2.7418e-06,  2.5779e-06,  ..., -2.9653e-06,
         -2.2650e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.9740538001060486


Running epoch 1, step 1121, batch 73
Sampled inputs[:2]: tensor([[   0, 3353,   17,  ...,  596,   12,  461],
        [   0, 6477,   12,  ..., 2931,  221,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3006e-05, -1.1597e-04,  2.2162e-05,  ..., -4.2891e-05,
          1.9537e-04,  1.2687e-04],
        [-2.9355e-06, -2.0936e-06,  1.9893e-06,  ..., -2.4140e-06,
         -1.8328e-06, -1.6131e-06],
        [ 5.7207e-05,  5.6462e-05, -2.2049e-05,  ...,  7.3559e-05,
          4.9905e-05,  4.4193e-05],
        [ 5.5816e-05,  6.2991e-05, -8.5918e-05,  ...,  3.2844e-05,
          4.6543e-05,  2.7883e-05],
        [-7.2569e-06, -5.6028e-06,  5.1111e-06,  ..., -6.0946e-06,
         -4.8578e-06, -3.7104e-06]], device='cuda:0')
Loss: 1.018789529800415


Running epoch 1, step 1122, batch 74
Sampled inputs[:2]: tensor([[    0,  7011,   650,  ..., 28839, 11610,  3222],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1906e-04, -5.5270e-05,  2.5205e-05,  ..., -3.1195e-05,
          2.0849e-04,  1.2687e-04],
        [-4.3735e-06, -3.1143e-06,  2.9504e-06,  ..., -3.6284e-06,
         -2.7418e-06, -2.4252e-06],
        [ 5.3601e-05,  5.3810e-05, -1.9501e-05,  ...,  7.0534e-05,
          4.7655e-05,  4.2181e-05],
        [ 5.1972e-05,  6.0309e-05, -8.3220e-05,  ...,  2.9596e-05,
          4.4114e-05,  2.5589e-05],
        [-1.0774e-05, -8.3148e-06,  7.5698e-06,  ..., -9.1046e-06,
         -7.2122e-06, -5.5060e-06]], device='cuda:0')
Loss: 0.9897891879081726


Running epoch 1, step 1123, batch 75
Sampled inputs[:2]: tensor([[    0,   278,  2354,  ...,  4974,  7757,   472],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6388e-04, -1.1030e-04, -8.4740e-05,  ...,  2.6592e-05,
          2.0345e-04,  9.9064e-05],
        [-5.7444e-06, -4.1947e-06,  3.8631e-06,  ..., -4.8131e-06,
         -3.6843e-06, -3.2187e-06],
        [ 5.0159e-05,  5.1008e-05, -1.7027e-05,  ...,  6.7584e-05,
          4.5300e-05,  4.0229e-05],
        [ 4.8336e-05,  5.7493e-05, -8.0657e-05,  ...,  2.6466e-05,
          4.1611e-05,  2.3383e-05],
        [-1.4231e-05, -1.1250e-05,  1.0028e-05,  ..., -1.2130e-05,
         -9.7603e-06, -7.3016e-06]], device='cuda:0')
Loss: 0.9942122101783752


Running epoch 1, step 1124, batch 76
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,  4823,    12,  ...,  1756,  3406,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6410e-04, -1.5651e-04, -5.9149e-05,  ..., -9.3212e-05,
          2.4621e-04,  1.5182e-04],
        [-7.2494e-06, -5.1856e-06,  4.8019e-06,  ..., -6.0275e-06,
         -4.6045e-06, -4.1313e-06],
        [ 2.4348e-04,  2.8471e-04, -1.8383e-04,  ...,  2.0690e-04,
          2.4750e-04,  8.9910e-05],
        [ 4.4342e-05,  5.4930e-05, -7.8035e-05,  ...,  2.3263e-05,
          3.9212e-05,  2.0850e-05],
        [-1.8075e-05, -1.3977e-05,  1.2547e-05,  ..., -1.5289e-05,
         -1.2279e-05, -9.4920e-06]], device='cuda:0')
Loss: 0.9994434714317322


Running epoch 1, step 1125, batch 77
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,  4215,  1478,  ...,   644,   409,  3803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1234e-04, -8.0480e-05,  6.4670e-05,  ..., -1.0418e-04,
          3.9125e-04,  3.0715e-04],
        [-8.7619e-06, -6.1989e-06,  5.6550e-06,  ..., -7.2569e-06,
         -5.5470e-06, -5.1968e-06],
        [ 2.3940e-04,  2.8191e-04, -1.8136e-04,  ...,  2.0367e-04,
          2.4503e-04,  8.7064e-05],
        [ 4.0230e-05,  5.2247e-05, -7.5591e-05,  ...,  1.9970e-05,
          3.6694e-05,  1.7855e-05],
        [-2.2158e-05, -1.6883e-05,  1.5020e-05,  ..., -1.8626e-05,
         -1.4931e-05, -1.2159e-05]], device='cuda:0')
Loss: 0.9754790663719177


Running epoch 1, step 1126, batch 78
Sampled inputs[:2]: tensor([[    0,   431, 19346,  ...,    14,  3237, 18548],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1906e-04,  1.3952e-05,  6.9515e-05,  ..., -1.6770e-04,
          6.0673e-04,  4.4565e-04],
        [-1.0133e-05, -7.0967e-06,  6.4112e-06,  ..., -8.4862e-06,
         -6.4299e-06, -6.1169e-06],
        [ 2.3606e-04,  2.7964e-04, -1.7926e-04,  ...,  2.0079e-04,
          2.4294e-04,  8.4978e-05],
        [ 3.6504e-05,  4.9908e-05, -7.3356e-05,  ...,  1.6676e-05,
          3.4295e-05,  1.5262e-05],
        [-2.5570e-05, -1.9297e-05,  1.7181e-05,  ..., -2.1607e-05,
         -1.7196e-05, -1.4082e-05]], device='cuda:0')
Loss: 0.9794621467590332


Running epoch 1, step 1127, batch 79
Sampled inputs[:2]: tensor([[    0, 15165,   287,  ..., 15049,   278,   266],
        [    0, 18774,  4916,  ..., 35093,    19,    50]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6419e-04,  1.1552e-04,  8.0686e-05,  ..., -1.5715e-04,
          5.3105e-04,  2.7913e-04],
        [-1.1563e-05, -8.1249e-06,  7.4245e-06,  ..., -9.6858e-06,
         -7.3016e-06, -6.9328e-06],
        [ 2.3244e-04,  2.7698e-04, -1.7656e-04,  ...,  1.9777e-04,
          2.4077e-04,  8.2921e-05],
        [ 3.2511e-05,  4.7091e-05, -7.0405e-05,  ...,  1.3353e-05,
          3.1910e-05,  1.2878e-05],
        [-2.9117e-05, -2.2024e-05,  1.9804e-05,  ..., -2.4632e-05,
         -1.9506e-05, -1.5937e-05]], device='cuda:0')
Loss: 1.0039172172546387
Graident accumulation at epoch 1, step 1127, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0285,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2246e-05,  8.4073e-05, -1.2549e-04,  ..., -9.6777e-06,
          1.5833e-04,  6.1206e-05],
        [-1.0549e-05, -6.4773e-06,  5.8876e-06,  ..., -8.5869e-06,
         -5.4870e-06, -6.3513e-06],
        [ 6.6520e-05,  7.9800e-05, -4.6799e-05,  ...,  6.8738e-05,
          5.7427e-05,  2.2837e-05],
        [ 1.6559e-05,  3.0595e-05, -1.7091e-05,  ...,  1.5231e-05,
          2.7359e-05,  5.0349e-06],
        [-2.9567e-05, -2.1859e-05,  1.9290e-05,  ..., -2.4724e-05,
         -1.8926e-05, -1.8035e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2885e-08, 5.2916e-08, 3.8335e-08,  ..., 3.8774e-08, 1.1423e-07,
         4.7064e-08],
        [6.3912e-11, 4.7823e-11, 1.2043e-11,  ..., 4.8122e-11, 1.8462e-11,
         2.0011e-11],
        [3.1447e-09, 2.0014e-09, 7.4143e-10,  ..., 2.5657e-09, 6.5707e-10,
         8.4693e-10],
        [9.3725e-10, 8.6981e-10, 2.3437e-10,  ..., 7.8445e-10, 3.9120e-10,
         3.3342e-10],
        [3.1397e-10, 1.8441e-10, 4.6470e-11,  ..., 2.3716e-10, 5.6039e-11,
         8.1158e-11]], device='cuda:0')
optimizer state dict: 141.0
lr: [9.250351066628025e-06, 9.250351066628025e-06]
scheduler_last_epoch: 141


Running epoch 1, step 1128, batch 80
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0, 9058, 4048,  ...,   14,  759, 1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3470e-05, -1.3035e-04, -4.3233e-05,  ..., -6.5101e-05,
         -6.5856e-05, -1.9116e-06],
        [-1.4380e-06, -1.0133e-06,  1.0878e-06,  ..., -1.2070e-06,
         -7.8976e-07, -7.6368e-07],
        [-3.7104e-06, -2.6822e-06,  2.9057e-06,  ..., -3.1292e-06,
         -1.9968e-06, -2.0266e-06],
        [-3.9637e-06, -2.7418e-06,  3.0994e-06,  ..., -3.3230e-06,
         -2.1309e-06, -2.2203e-06],
        [-3.4273e-06, -2.5779e-06,  2.6524e-06,  ..., -2.9504e-06,
         -2.0266e-06, -1.7360e-06]], device='cuda:0')
Loss: 0.990311324596405


Running epoch 1, step 1129, batch 81
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   381,  3513,  1501],
        [    0,   300,  6263,  ..., 18488,  1665,  1640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0863e-04, -2.1115e-04, -2.9136e-05,  ..., -7.6989e-06,
         -5.0881e-05,  2.0894e-05],
        [-2.9281e-06, -2.0564e-06,  2.1160e-06,  ..., -2.4661e-06,
         -1.6838e-06, -1.6056e-06],
        [ 7.8996e-04,  5.6502e-04, -5.5173e-04,  ...,  5.8697e-04,
          3.7861e-04,  3.4889e-04],
        [-7.8976e-06, -5.4687e-06,  5.9456e-06,  ..., -6.6459e-06,
         -4.4852e-06, -4.5598e-06],
        [-6.9737e-06, -5.2601e-06,  5.2303e-06,  ..., -6.0201e-06,
         -4.3213e-06, -3.6061e-06]], device='cuda:0')
Loss: 1.0217015743255615


Running epoch 1, step 1130, batch 82
Sampled inputs[:2]: tensor([[   0,  685, 2461,  ...,  287,  298, 7943],
        [   0,  380,  759,  ..., 1420, 1804,  490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0106e-05, -7.7965e-05,  3.2094e-04,  ...,  3.3860e-05,
          1.5529e-05,  3.0644e-04],
        [-4.3362e-06, -3.0324e-06,  2.9318e-06,  ..., -3.6806e-06,
         -2.6822e-06, -2.5816e-06],
        [ 7.8628e-04,  5.6238e-04, -5.4945e-04,  ...,  5.8383e-04,
          3.7607e-04,  3.4636e-04],
        [-1.1802e-05, -8.1062e-06,  8.2701e-06,  ..., -9.9987e-06,
         -7.2122e-06, -7.3910e-06],
        [-1.0625e-05, -8.0168e-06,  7.4655e-06,  ..., -9.2238e-06,
         -7.0333e-06, -5.9456e-06]], device='cuda:0')
Loss: 0.9996618032455444


Running epoch 1, step 1131, batch 83
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,   12,  722,  ...,  674,  369,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0106e-05, -6.3642e-05,  1.9206e-04,  ...,  1.9888e-05,
         -2.7540e-05,  1.8254e-04],
        [-5.8115e-06, -4.0531e-06,  3.9898e-06,  ..., -4.9472e-06,
         -3.6135e-06, -3.4235e-06],
        [ 7.8273e-04,  5.5983e-04, -5.4675e-04,  ...,  5.8080e-04,
          3.7385e-04,  3.4435e-04],
        [-1.5736e-05, -1.0788e-05,  1.1250e-05,  ..., -1.3366e-05,
         -9.6858e-06, -9.7454e-06],
        [-1.4186e-05, -1.0684e-05,  1.0133e-05,  ..., -1.2323e-05,
         -9.4324e-06, -7.8157e-06]], device='cuda:0')
Loss: 0.9812946319580078


Running epoch 1, step 1132, batch 84
Sampled inputs[:2]: tensor([[    0,  2548,   720,  ...,  1795,  1109, 32948],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3737e-04, -1.6638e-04,  4.3670e-04,  ..., -2.8084e-05,
          8.1004e-05,  2.4486e-04],
        [-7.2494e-06, -5.0813e-06,  4.9397e-06,  ..., -6.1318e-06,
         -4.5076e-06, -4.2617e-06],
        [ 7.7910e-04,  5.5718e-04, -5.4422e-04,  ...,  5.7787e-04,
          3.7167e-04,  3.4224e-04],
        [-1.9699e-05, -1.3500e-05,  1.3977e-05,  ..., -1.6570e-05,
         -1.2070e-05, -1.2174e-05],
        [-1.7822e-05, -1.3441e-05,  1.2621e-05,  ..., -1.5333e-05,
         -1.1772e-05, -9.7528e-06]], device='cuda:0')
Loss: 1.0011755228042603


Running epoch 1, step 1133, batch 85
Sampled inputs[:2]: tensor([[    0, 13595,  3803,  ...,  1992,  4770,   818],
        [    0,  2377,   271,  ...,   395,   394,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5726e-04, -1.1802e-04,  3.6029e-04,  ..., -2.3460e-06,
          3.3086e-05,  2.5696e-04],
        [-8.7842e-06, -6.1616e-06,  6.0350e-06,  ..., -7.3686e-06,
         -5.3607e-06, -5.0887e-06],
        [ 7.7525e-04,  5.5436e-04, -5.4136e-04,  ...,  5.7474e-04,
          3.6950e-04,  3.4013e-04],
        [-2.3842e-05, -1.6421e-05,  1.7062e-05,  ..., -1.9893e-05,
         -1.4365e-05, -1.4514e-05],
        [-2.1473e-05, -1.6242e-05,  1.5318e-05,  ..., -1.8388e-05,
         -1.4022e-05, -1.1645e-05]], device='cuda:0')
Loss: 0.9916571378707886


Running epoch 1, step 1134, batch 86
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0,  1336, 10446,  ...,   409,   275, 12528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5150e-04, -2.3389e-05,  2.6970e-04,  ...,  2.7343e-05,
          2.3898e-05, -4.0484e-05],
        [-1.0192e-05, -7.1824e-06,  7.0259e-06,  ..., -8.5458e-06,
         -6.2361e-06, -5.9083e-06],
        [ 7.7162e-04,  5.5166e-04, -5.3866e-04,  ...,  5.7174e-04,
          3.6730e-04,  3.3803e-04],
        [-2.7716e-05, -1.9163e-05,  1.9908e-05,  ..., -2.3097e-05,
         -1.6719e-05, -1.6883e-05],
        [-2.5004e-05, -1.8954e-05,  1.7896e-05,  ..., -2.1353e-05,
         -1.6317e-05, -1.3538e-05]], device='cuda:0')
Loss: 0.9878374934196472


Running epoch 1, step 1135, batch 87
Sampled inputs[:2]: tensor([[    0,   266,   944,  ..., 14981,  1952,   271],
        [    0,   287,  2926,  ...,   266, 40854,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8853e-04, -3.9047e-05,  1.0822e-04,  ..., -1.7129e-04,
         -8.3390e-05,  9.5174e-05],
        [-1.1727e-05, -8.1584e-06,  7.9386e-06,  ..., -9.7975e-06,
         -7.2122e-06, -6.9290e-06],
        [ 7.6777e-04,  5.4913e-04, -5.3623e-04,  ...,  5.6863e-04,
          3.6491e-04,  3.3546e-04],
        [-3.1888e-05, -2.1741e-05,  2.2501e-05,  ..., -2.6450e-05,
         -1.9297e-05, -1.9744e-05],
        [-2.8938e-05, -2.1651e-05,  2.0310e-05,  ..., -2.4617e-05,
         -1.8954e-05, -1.5996e-05]], device='cuda:0')
Loss: 0.9858133792877197
Graident accumulation at epoch 1, step 1135, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0285,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.4169e-05,  7.1761e-05, -1.0212e-04,  ..., -2.5839e-05,
          1.3416e-04,  6.4603e-05],
        [-1.0666e-05, -6.6454e-06,  6.0927e-06,  ..., -8.7080e-06,
         -5.6595e-06, -6.4090e-06],
        [ 1.3665e-04,  1.2673e-04, -9.5742e-05,  ...,  1.1873e-04,
          8.8174e-05,  5.4100e-05],
        [ 1.1715e-05,  2.5361e-05, -1.3132e-05,  ...,  1.1063e-05,
          2.2693e-05,  2.5570e-06],
        [-2.9504e-05, -2.1838e-05,  1.9392e-05,  ..., -2.4713e-05,
         -1.8929e-05, -1.7831e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2963e-08, 5.2864e-08, 3.8308e-08,  ..., 3.8765e-08, 1.1412e-07,
         4.7026e-08],
        [6.3986e-11, 4.7842e-11, 1.2094e-11,  ..., 4.8170e-11, 1.8496e-11,
         2.0039e-11],
        [3.7311e-09, 2.3010e-09, 1.0282e-09,  ..., 2.8864e-09, 7.8957e-10,
         9.5862e-10],
        [9.3733e-10, 8.6941e-10, 2.3464e-10,  ..., 7.8437e-10, 3.9118e-10,
         3.3348e-10],
        [3.1449e-10, 1.8470e-10, 4.6836e-11,  ..., 2.3753e-10, 5.6342e-11,
         8.1332e-11]], device='cuda:0')
optimizer state dict: 142.0
lr: [9.12714270321745e-06, 9.12714270321745e-06]
scheduler_last_epoch: 142


Running epoch 1, step 1136, batch 88
Sampled inputs[:2]: tensor([[    0,  2734,  2338,  ...,  3977,   970, 10537],
        [    0, 38232,   446,  ...,   287,  2456, 29919]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9303e-05,  7.7203e-05,  5.9862e-05,  ..., -1.2020e-04,
          5.5963e-05,  5.7929e-05],
        [-1.4827e-06, -1.0803e-06,  9.0525e-07,  ..., -1.2890e-06,
         -1.1250e-06, -9.3505e-07],
        [-3.6359e-06, -2.7716e-06,  2.3842e-06,  ..., -3.1441e-06,
         -2.7418e-06, -2.2650e-06],
        [-3.8445e-06, -2.7865e-06,  2.4289e-06,  ..., -3.3379e-06,
         -2.9504e-06, -2.5630e-06],
        [-3.7700e-06, -2.9653e-06,  2.4587e-06,  ..., -3.3081e-06,
         -2.9504e-06, -2.2054e-06]], device='cuda:0')
Loss: 1.0258965492248535


Running epoch 1, step 1137, batch 89
Sampled inputs[:2]: tensor([[   0, 1049,  292,  ...,  221,  380,  341],
        [   0, 4337, 2057,  ..., 3020, 1722,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5253e-04,  1.8987e-04,  9.5074e-05,  ...,  3.3460e-05,
         -2.8610e-04, -2.4715e-04],
        [-2.7865e-06, -2.0340e-06,  1.9111e-06,  ..., -2.4512e-06,
         -2.0079e-06, -1.8254e-06],
        [-6.8098e-06, -5.1409e-06,  5.0068e-06,  ..., -5.9158e-06,
         -4.8280e-06, -4.3511e-06],
        [-7.3314e-06, -5.2452e-06,  5.3048e-06,  ..., -6.4075e-06,
         -5.3048e-06, -5.0515e-06],
        [-6.9588e-06, -5.4538e-06,  5.0515e-06,  ..., -6.1244e-06,
         -5.1707e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9260828495025635


Running epoch 1, step 1138, batch 90
Sampled inputs[:2]: tensor([[    0,  2853, 21042,  ...,  4120,   607, 11176],
        [    0,   199,  2834,  ...,  1236,   768,  4316]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0601e-04,  3.9640e-04,  8.8764e-05,  ..., -1.1806e-04,
         -1.6944e-04, -4.1010e-04],
        [-4.2394e-06, -3.0547e-06,  2.8647e-06,  ..., -3.7104e-06,
         -3.0063e-06, -2.7083e-06],
        [-1.0356e-05, -7.7486e-06,  7.4953e-06,  ..., -8.9705e-06,
         -7.2718e-06, -6.4820e-06],
        [-1.1265e-05, -7.9721e-06,  8.0168e-06,  ..., -9.8199e-06,
         -8.0317e-06, -7.5698e-06],
        [-1.0610e-05, -8.2403e-06,  7.5847e-06,  ..., -9.2983e-06,
         -7.8082e-06, -6.1542e-06]], device='cuda:0')
Loss: 0.9983533024787903


Running epoch 1, step 1139, batch 91
Sampled inputs[:2]: tensor([[    0,    13,  1581,  ...,    13, 11628, 14876],
        [    0,   546,   360,  ...,  9107,  2772,  4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5364e-04,  4.4072e-04,  8.8764e-05,  ..., -1.2516e-04,
         -1.9736e-04, -6.3278e-04],
        [-5.6475e-06, -4.0457e-06,  3.8631e-06,  ..., -4.9546e-06,
         -3.8743e-06, -3.5465e-06],
        [-1.3903e-05, -1.0297e-05,  1.0163e-05,  ..., -1.2070e-05,
         -9.4026e-06, -8.6129e-06],
        [-1.5140e-05, -1.0639e-05,  1.0893e-05,  ..., -1.3217e-05,
         -1.0356e-05, -9.9987e-06],
        [-1.4052e-05, -1.0818e-05,  1.0133e-05,  ..., -1.2383e-05,
         -1.0058e-05, -8.0764e-06]], device='cuda:0')
Loss: 1.001517653465271


Running epoch 1, step 1140, batch 92
Sampled inputs[:2]: tensor([[   0,   14, 3445,  ...,  298,  527, 2732],
        [   0, 7110,  437,  ...,  266, 6724, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1876e-04,  4.2925e-04,  1.8708e-04,  ..., -6.1920e-05,
         -1.9736e-04, -7.3707e-04],
        [-7.0855e-06, -5.1335e-06,  4.7982e-06,  ..., -6.2510e-06,
         -4.8578e-06, -4.4815e-06],
        [-1.7598e-05, -1.3173e-05,  1.2711e-05,  ..., -1.5363e-05,
         -1.1876e-05, -1.0982e-05],
        [-1.8984e-05, -1.3471e-05,  1.3471e-05,  ..., -1.6659e-05,
         -1.2949e-05, -1.2591e-05],
        [-1.7717e-05, -1.3754e-05,  1.2636e-05,  ..., -1.5691e-05,
         -1.2636e-05, -1.0282e-05]], device='cuda:0')
Loss: 0.9983586668968201


Running epoch 1, step 1141, batch 93
Sampled inputs[:2]: tensor([[    0, 19720,    12,  ...,  1239,    12, 22324],
        [    0,  6803,  6298,  ...,   490,  1781,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7083e-04,  4.3446e-04,  2.2942e-04,  ..., -5.0160e-05,
         -2.2673e-04, -8.2516e-04],
        [-8.5384e-06, -6.2138e-06,  5.8338e-06,  ..., -7.4655e-06,
         -5.7817e-06, -5.3756e-06],
        [-2.1324e-05, -1.5989e-05,  1.5467e-05,  ..., -1.8463e-05,
         -1.4231e-05, -1.3307e-05],
        [-2.2918e-05, -1.6332e-05,  1.6376e-05,  ..., -1.9923e-05,
         -1.5408e-05, -1.5125e-05],
        [-2.1338e-05, -1.6615e-05,  1.5289e-05,  ..., -1.8790e-05,
         -1.5110e-05, -1.2398e-05]], device='cuda:0')
Loss: 0.9728589653968811


Running epoch 1, step 1142, batch 94
Sampled inputs[:2]: tensor([[    0,  2085,    12,  ...,   496,    14,   747],
        [    0, 38460,     9,  ...,   829,   870,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6040e-04,  4.9006e-04,  3.1879e-04,  ..., -6.4326e-05,
         -1.7358e-04, -8.1341e-04],
        [-9.9838e-06, -7.2569e-06,  6.7391e-06,  ..., -8.7395e-06,
         -6.7949e-06, -6.2585e-06],
        [-2.4855e-05, -1.8626e-05,  1.7822e-05,  ..., -2.1547e-05,
         -1.6674e-05, -1.5467e-05],
        [-2.6792e-05, -1.9059e-05,  1.8880e-05,  ..., -2.3320e-05,
         -1.8090e-05, -1.7613e-05],
        [-2.4959e-05, -1.9416e-05,  1.7673e-05,  ..., -2.2009e-05,
         -1.7762e-05, -1.4454e-05]], device='cuda:0')
Loss: 1.0016335248947144


Running epoch 1, step 1143, batch 95
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  2523,  4729, 13753],
        [    0,   401,  9370,  ...,     9,   287,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3699e-04,  4.4503e-04,  2.3483e-04,  ...,  1.4530e-05,
         -1.5719e-04, -7.1543e-04],
        [-1.1466e-05, -8.3223e-06,  7.7821e-06,  ..., -9.9093e-06,
         -7.7188e-06, -7.1786e-06],
        [-2.8700e-05, -2.1443e-05,  2.0638e-05,  ..., -2.4542e-05,
         -1.9044e-05, -1.7837e-05],
        [-3.0816e-05, -2.1890e-05,  2.1815e-05,  ..., -2.6464e-05,
         -2.0579e-05, -2.0206e-05],
        [-2.8655e-05, -2.2218e-05,  2.0355e-05,  ..., -2.4959e-05,
         -2.0191e-05, -1.6570e-05]], device='cuda:0')
Loss: 1.0023459196090698
Graident accumulation at epoch 1, step 1143, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0285,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.3451e-05,  1.0909e-04, -6.8425e-05,  ..., -2.1802e-05,
          1.0502e-04, -1.3400e-05],
        [-1.0746e-05, -6.8131e-06,  6.2616e-06,  ..., -8.8281e-06,
         -5.8654e-06, -6.4860e-06],
        [ 1.2011e-04,  1.1192e-04, -8.4104e-05,  ...,  1.0440e-04,
          7.7453e-05,  4.6906e-05],
        [ 7.4616e-06,  2.0636e-05, -9.6373e-06,  ...,  7.3102e-06,
          1.8366e-05,  2.8071e-07],
        [-2.9419e-05, -2.1876e-05,  1.9488e-05,  ..., -2.4738e-05,
         -1.9055e-05, -1.7705e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2946e-08, 5.3009e-08, 3.8325e-08,  ..., 3.8726e-08, 1.1403e-07,
         4.7491e-08],
        [6.4053e-11, 4.7863e-11, 1.2143e-11,  ..., 4.8220e-11, 1.8537e-11,
         2.0070e-11],
        [3.7282e-09, 2.2991e-09, 1.0276e-09,  ..., 2.8842e-09, 7.8914e-10,
         9.5798e-10],
        [9.3734e-10, 8.6902e-10, 2.3488e-10,  ..., 7.8428e-10, 3.9121e-10,
         3.3356e-10],
        [3.1500e-10, 1.8501e-10, 4.7203e-11,  ..., 2.3791e-10, 5.6694e-11,
         8.1526e-11]], device='cuda:0')
optimizer state dict: 143.0
lr: [9.004067720021106e-06, 9.004067720021106e-06]
scheduler_last_epoch: 143


Running epoch 1, step 1144, batch 96
Sampled inputs[:2]: tensor([[    0,  7018,    14,  ...,  8288,    12,  1250],
        [    0,   555,   764,  ...,   932,   709, 18731]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8264e-05,  1.2067e-06,  4.3148e-05,  ...,  1.2174e-05,
         -4.2364e-05,  6.8782e-05],
        [-1.4305e-06, -1.0803e-06,  9.6112e-07,  ..., -1.2442e-06,
         -9.6112e-07, -9.8348e-07],
        [-3.7104e-06, -2.8461e-06,  2.6226e-06,  ..., -3.2037e-06,
         -2.4587e-06, -2.5630e-06],
        [-3.7849e-06, -2.8014e-06,  2.6375e-06,  ..., -3.2634e-06,
         -2.5034e-06, -2.7120e-06],
        [-3.6061e-06, -2.8610e-06,  2.5183e-06,  ..., -3.1739e-06,
         -2.5332e-06, -2.3246e-06]], device='cuda:0')
Loss: 0.9614237546920776


Running epoch 1, step 1145, batch 97
Sampled inputs[:2]: tensor([[    0,  5136,   446,  ...,  1173,   300,   266],
        [    0,   292, 15156,  ...,    35,  3815,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7823e-05,  7.7094e-06,  2.6893e-04,  ...,  1.1652e-04,
         -1.9232e-04,  9.6038e-05],
        [-2.9355e-06, -2.0787e-06,  1.8328e-06,  ..., -2.5555e-06,
         -1.9744e-06, -2.0713e-06],
        [-7.5847e-06, -5.4985e-06,  5.0068e-06,  ..., -6.5267e-06,
         -4.9919e-06, -5.3346e-06],
        [-7.7188e-06, -5.3346e-06,  4.9919e-06,  ..., -6.6608e-06,
         -5.0962e-06, -5.6475e-06],
        [-7.5102e-06, -5.6177e-06,  4.8876e-06,  ..., -6.5714e-06,
         -5.2303e-06, -4.9323e-06]], device='cuda:0')
Loss: 0.959090530872345


Running epoch 1, step 1146, batch 98
Sampled inputs[:2]: tensor([[   0,  278,  554,  ...,  365, 3125,  271],
        [   0, 3101,  275,  ..., 2345,  609,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3358e-05,  1.0572e-04,  2.8720e-04,  ...,  1.7930e-04,
         -2.8535e-04, -1.3313e-04],
        [-4.3288e-06, -3.0100e-06,  2.7381e-06,  ..., -3.7849e-06,
         -2.8834e-06, -3.0547e-06],
        [-1.1042e-05, -7.8529e-06,  7.4357e-06,  ..., -9.4920e-06,
         -7.1675e-06, -7.6741e-06],
        [-1.1459e-05, -7.7188e-06,  7.5400e-06,  ..., -9.9093e-06,
         -7.4804e-06, -8.3297e-06],
        [-1.1042e-05, -8.0913e-06,  7.3463e-06,  ..., -9.6560e-06,
         -7.5996e-06, -7.1377e-06]], device='cuda:0')
Loss: 0.9930703639984131


Running epoch 1, step 1147, batch 99
Sampled inputs[:2]: tensor([[    0, 23988, 26825,  ...,   373,   221,   334],
        [    0,   271,  4219,  ...,   644,    14,  3607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7414e-05,  1.5676e-04,  1.7491e-04,  ...,  2.3876e-04,
         -1.8809e-04, -6.5715e-04],
        [-5.6624e-06, -3.9712e-06,  3.6247e-06,  ..., -5.0366e-06,
         -3.8296e-06, -4.0233e-06],
        [-1.4260e-05, -1.0237e-05,  9.7752e-06,  ..., -1.2428e-05,
         -9.3728e-06, -9.8944e-06],
        [-1.4946e-05, -1.0163e-05,  1.0014e-05,  ..., -1.3143e-05,
         -9.9391e-06, -1.0923e-05],
        [-1.4365e-05, -1.0625e-05,  9.7305e-06,  ..., -1.2696e-05,
         -9.9689e-06, -9.2387e-06]], device='cuda:0')
Loss: 0.9521183967590332


Running epoch 1, step 1148, batch 100
Sampled inputs[:2]: tensor([[    0,  7952,   266,  ..., 10864, 24825,   927],
        [    0,   461,   654,  ...,  6548,  7171,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8413e-05, -1.2288e-05,  1.9001e-04,  ...,  1.6261e-04,
         -1.6466e-04, -5.7225e-04],
        [-7.1153e-06, -5.0515e-06,  4.6529e-06,  ..., -6.2957e-06,
         -4.8503e-06, -5.0217e-06],
        [-1.8060e-05, -1.3128e-05,  1.2577e-05,  ..., -1.5691e-05,
         -1.1995e-05, -1.2487e-05],
        [-1.8880e-05, -1.3039e-05,  1.2875e-05,  ..., -1.6510e-05,
         -1.2651e-05, -1.3709e-05],
        [-1.8120e-05, -1.3605e-05,  1.2472e-05,  ..., -1.5989e-05,
         -1.2740e-05, -1.1668e-05]], device='cuda:0')
Loss: 1.0064287185668945


Running epoch 1, step 1149, batch 101
Sampled inputs[:2]: tensor([[    0,   342,   970,  ...,   401,  2907,  1657],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4477e-04,  2.9447e-04,  4.4493e-04,  ...,  1.0000e-04,
         -2.3088e-04, -5.8734e-04],
        [-8.6129e-06, -6.2063e-06,  5.5097e-06,  ..., -7.5698e-06,
         -5.9679e-06, -6.0648e-06],
        [-2.1994e-05, -1.6272e-05,  1.4946e-05,  ..., -1.9044e-05,
         -1.4931e-05, -1.5259e-05],
        [-2.2903e-05, -1.6108e-05,  1.5244e-05,  ..., -1.9923e-05,
         -1.5661e-05, -1.6630e-05],
        [-2.2024e-05, -1.6838e-05,  1.4812e-05,  ..., -1.9401e-05,
         -1.5810e-05, -1.4275e-05]], device='cuda:0')
Loss: 1.0006909370422363


Running epoch 1, step 1150, batch 102
Sampled inputs[:2]: tensor([[    0,  9818,   347,  ...,   413,  7359,    15],
        [    0,  2270,   278,  ..., 36325,  5892,  3558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8184e-06,  2.8424e-04,  6.0753e-04,  ...,  1.5082e-06,
          4.7307e-06, -4.0810e-04],
        [-1.0110e-05, -7.2792e-06,  6.4000e-06,  ..., -8.8587e-06,
         -6.9737e-06, -7.1079e-06],
        [-2.5898e-05, -1.9133e-05,  1.7405e-05,  ..., -2.2367e-05,
         -1.7494e-05, -1.7956e-05],
        [-2.6837e-05, -1.8835e-05,  1.7673e-05,  ..., -2.3261e-05,
         -1.8224e-05, -1.9461e-05],
        [-2.5868e-05, -1.9759e-05,  1.7211e-05,  ..., -2.2739e-05,
         -1.8492e-05, -1.6779e-05]], device='cuda:0')
Loss: 1.0008703470230103


Running epoch 1, step 1151, batch 103
Sampled inputs[:2]: tensor([[    0,   790, 43134,  ...,   446,   381,  1034],
        [    0,  2018,  4798,  ...,   292,  1919,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8926e-05,  3.4006e-04,  5.9673e-04,  ...,  9.9313e-05,
         -4.8375e-05, -5.4283e-04],
        [-1.1533e-05, -8.3074e-06,  7.4208e-06,  ..., -1.0051e-05,
         -7.8753e-06, -8.0690e-06],
        [-2.9460e-05, -2.1815e-05,  2.0117e-05,  ..., -2.5332e-05,
         -1.9729e-05, -2.0310e-05],
        [-3.0681e-05, -2.1562e-05,  2.0549e-05,  ..., -2.6450e-05,
         -2.0638e-05, -2.2188e-05],
        [-2.9355e-05, -2.2486e-05,  1.9819e-05,  ..., -2.5690e-05,
         -2.0832e-05, -1.8910e-05]], device='cuda:0')
Loss: 0.9828742146492004
Graident accumulation at epoch 1, step 1151, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0155,  0.0156, -0.0285,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3214e-05,  1.3219e-04, -1.9098e-06,  ..., -9.6903e-06,
          8.9682e-05, -6.6343e-05],
        [-1.0825e-05, -6.9625e-06,  6.3775e-06,  ..., -8.9504e-06,
         -6.0664e-06, -6.6443e-06],
        [ 1.0515e-04,  9.8543e-05, -7.3682e-05,  ...,  9.1427e-05,
          6.7734e-05,  4.0184e-05],
        [ 3.6473e-06,  1.6416e-05, -6.6187e-06,  ...,  3.9342e-06,
          1.4466e-05, -1.9661e-06],
        [-2.9412e-05, -2.1937e-05,  1.9521e-05,  ..., -2.4833e-05,
         -1.9233e-05, -1.7825e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2874e-08, 5.3072e-08, 3.8642e-08,  ..., 3.8697e-08, 1.1392e-07,
         4.7738e-08],
        [6.4122e-11, 4.7884e-11, 1.2186e-11,  ..., 4.8272e-11, 1.8580e-11,
         2.0115e-11],
        [3.7253e-09, 2.2973e-09, 1.0270e-09,  ..., 2.8819e-09, 7.8874e-10,
         9.5743e-10],
        [9.3735e-10, 8.6862e-10, 2.3507e-10,  ..., 7.8420e-10, 3.9125e-10,
         3.3371e-10],
        [3.1555e-10, 1.8533e-10, 4.7549e-11,  ..., 2.3833e-10, 5.7071e-11,
         8.1802e-11]], device='cuda:0')
optimizer state dict: 144.0
lr: [8.881144923970756e-06, 8.881144923970756e-06]
scheduler_last_epoch: 144


Running epoch 1, step 1152, batch 104
Sampled inputs[:2]: tensor([[    0,  1682,   271,  ...,   300,   266, 10935],
        [    0,  5841,   328,  ...,  2051,   266,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8098e-05, -5.1430e-05, -2.1224e-05,  ..., -2.8621e-05,
         -1.1626e-04,  1.4307e-06],
        [-1.3635e-06, -1.0058e-06,  9.6858e-07,  ..., -1.2144e-06,
         -9.5367e-07, -1.0058e-06],
        [-3.6210e-06, -2.7269e-06,  2.7120e-06,  ..., -3.1739e-06,
         -2.4438e-06, -2.6375e-06],
        [-3.8445e-06, -2.7716e-06,  2.8610e-06,  ..., -3.3975e-06,
         -2.6524e-06, -2.9653e-06],
        [-3.5167e-06, -2.7418e-06,  2.6077e-06,  ..., -3.1292e-06,
         -2.5332e-06, -2.3842e-06]], device='cuda:0')
Loss: 0.9807940125465393


Running epoch 1, step 1153, batch 105
Sampled inputs[:2]: tensor([[    0, 33119,   391,  ...,   292,  4462,  2721],
        [    0,  2261,     9,  ..., 15008,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7692e-05, -1.5832e-04,  1.7219e-05,  ..., -3.6799e-05,
         -1.3562e-04,  1.4307e-06],
        [-2.7567e-06, -2.0117e-06,  1.9111e-06,  ..., -2.3842e-06,
         -1.7993e-06, -1.9409e-06],
        [-7.3612e-06, -5.4985e-06,  5.3793e-06,  ..., -6.3032e-06,
         -4.7088e-06, -5.1558e-06],
        [-7.6890e-06, -5.4687e-06,  5.5581e-06,  ..., -6.5863e-06,
         -4.9472e-06, -5.6475e-06],
        [-6.9290e-06, -5.3793e-06,  5.0068e-06,  ..., -6.0499e-06,
         -4.7833e-06, -4.5300e-06]], device='cuda:0')
Loss: 0.9724429845809937


Running epoch 1, step 1154, batch 106
Sampled inputs[:2]: tensor([[    0, 10206,   342,  ...,  1336,  5046,   360],
        [    0, 29368,    13,  ...,   376,    88,  3333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2664e-05, -2.5979e-04,  2.8716e-05,  ..., -3.5247e-05,
         -1.0491e-04,  5.4637e-05],
        [-4.1500e-06, -3.0845e-06,  2.8200e-06,  ..., -3.6135e-06,
         -2.8275e-06, -2.9020e-06],
        [-1.0967e-05, -8.3297e-06,  7.8529e-06,  ..., -9.4473e-06,
         -7.3016e-06, -7.6443e-06],
        [-1.1474e-05, -8.3148e-06,  8.1211e-06,  ..., -9.8795e-06,
         -7.6741e-06, -8.3745e-06],
        [-1.0520e-05, -8.2999e-06,  7.4506e-06,  ..., -9.2387e-06,
         -7.5251e-06, -6.8545e-06]], device='cuda:0')
Loss: 1.0319552421569824


Running epoch 1, step 1155, batch 107
Sampled inputs[:2]: tensor([[    0,    15,    83,  ...,  6030,    14, 14080],
        [    0,  1086,  5564,  ..., 29319, 32982,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2664e-05, -2.3192e-04, -7.1436e-06,  ..., -6.5655e-05,
         -1.8099e-04,  3.6077e-06],
        [-5.6177e-06, -4.1723e-06,  3.7961e-06,  ..., -4.8503e-06,
         -3.7663e-06, -3.9525e-06],
        [-1.4931e-05, -1.1355e-05,  1.0625e-05,  ..., -1.2770e-05,
         -9.8050e-06, -1.0490e-05],
        [-1.5378e-05, -1.1161e-05,  1.0818e-05,  ..., -1.3143e-05,
         -1.0118e-05, -1.1265e-05],
        [-1.4260e-05, -1.1250e-05,  1.0028e-05,  ..., -1.2442e-05,
         -1.0043e-05, -9.3728e-06]], device='cuda:0')
Loss: 1.01164972782135


Running epoch 1, step 1156, batch 108
Sampled inputs[:2]: tensor([[    0,  6978,  2285,  ...,  4477,   271,   221],
        [    0, 18905,  2311,  ..., 10213,   908,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7510e-04, -2.9519e-04, -1.6042e-04,  ..., -7.7549e-05,
         -2.0210e-04,  9.7630e-06],
        [-7.0482e-06, -5.2601e-06,  4.8019e-06,  ..., -6.0722e-06,
         -4.7497e-06, -4.8801e-06],
        [-1.8731e-05, -1.4305e-05,  1.3411e-05,  ..., -1.6019e-05,
         -1.2383e-05, -1.2994e-05],
        [-1.9222e-05, -1.4022e-05,  1.3605e-05,  ..., -1.6406e-05,
         -1.2711e-05, -1.3903e-05],
        [-1.7926e-05, -1.4201e-05,  1.2681e-05,  ..., -1.5646e-05,
         -1.2681e-05, -1.1638e-05]], device='cuda:0')
Loss: 1.0248440504074097


Running epoch 1, step 1157, batch 109
Sampled inputs[:2]: tensor([[   0,  417,  199,  ...,   13,   20, 6248],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3702e-04, -3.0389e-04, -1.0035e-04,  ..., -1.6515e-04,
         -2.5971e-04, -2.8520e-04],
        [-8.5160e-06, -6.2808e-06,  5.8077e-06,  ..., -7.3165e-06,
         -5.7109e-06, -5.9307e-06],
        [-2.2545e-05, -1.7062e-05,  1.6198e-05,  ..., -1.9252e-05,
         -1.4856e-05, -1.5721e-05],
        [-2.3156e-05, -1.6704e-05,  1.6421e-05,  ..., -1.9744e-05,
         -1.5259e-05, -1.6853e-05],
        [-2.1592e-05, -1.6943e-05,  1.5318e-05,  ..., -1.8805e-05,
         -1.5214e-05, -1.4067e-05]], device='cuda:0')
Loss: 0.9873582720756531


Running epoch 1, step 1158, batch 110
Sampled inputs[:2]: tensor([[    0,  1730,  2068,  ...,   445,  2704,   445],
        [    0, 16028,   669,  ...,   292,  6502,  7050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4805e-04, -4.0587e-05, -2.4617e-05,  ..., -4.2149e-05,
         -3.5581e-04, -1.2180e-04],
        [-1.0043e-05, -7.2718e-06,  6.5267e-06,  ..., -8.6427e-06,
         -6.7614e-06, -7.1824e-06],
        [-2.6509e-05, -1.9774e-05,  1.8224e-05,  ..., -2.2694e-05,
         -1.7598e-05, -1.8924e-05],
        [-2.7150e-05, -1.9267e-05,  1.8373e-05,  ..., -2.3261e-05,
         -1.8060e-05, -2.0206e-05],
        [-2.5734e-05, -1.9804e-05,  1.7449e-05,  ..., -2.2426e-05,
         -1.8179e-05, -1.7256e-05]], device='cuda:0')
Loss: 0.9241418242454529


Running epoch 1, step 1159, batch 111
Sampled inputs[:2]: tensor([[    0,  7070,    86,  ...,   298,  4930,   518],
        [    0,   292,   494,  ...,   259, 14134, 11544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1906e-04, -3.4222e-05, -8.9458e-05,  ..., -4.0735e-05,
         -4.0228e-04, -1.9893e-04],
        [-1.1474e-05, -8.3148e-06,  7.4692e-06,  ..., -9.8944e-06,
         -7.7076e-06, -8.1360e-06],
        [-3.0160e-05, -2.2516e-05,  2.0802e-05,  ..., -2.5854e-05,
         -1.9968e-05, -2.1353e-05],
        [-3.0994e-05, -2.2024e-05,  2.1040e-05,  ..., -2.6599e-05,
         -2.0564e-05, -2.2903e-05],
        [-2.9325e-05, -2.2605e-05,  1.9968e-05,  ..., -2.5615e-05,
         -2.0698e-05, -1.9491e-05]], device='cuda:0')
Loss: 1.0029053688049316
Graident accumulation at epoch 1, step 1159, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0040,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0292, -0.0141, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.0136e-06,  1.1554e-04, -1.0665e-05,  ..., -1.2795e-05,
          4.0487e-05, -7.9601e-05],
        [-1.0890e-05, -7.0978e-06,  6.4867e-06,  ..., -9.0448e-06,
         -6.2305e-06, -6.7935e-06],
        [ 9.1622e-05,  8.6437e-05, -6.4234e-05,  ...,  7.9699e-05,
          5.8964e-05,  3.4031e-05],
        [ 1.8311e-07,  1.2572e-05, -3.8528e-06,  ...,  8.8096e-07,
          1.0963e-05, -4.0598e-06],
        [-2.9404e-05, -2.2004e-05,  1.9566e-05,  ..., -2.4911e-05,
         -1.9379e-05, -1.7992e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3071e-08, 5.3020e-08, 3.8612e-08,  ..., 3.8660e-08, 1.1397e-07,
         4.7730e-08],
        [6.4190e-11, 4.7905e-11, 1.2229e-11,  ..., 4.8322e-11, 1.8621e-11,
         2.0161e-11],
        [3.7225e-09, 2.2955e-09, 1.0264e-09,  ..., 2.8797e-09, 7.8835e-10,
         9.5693e-10],
        [9.3737e-10, 8.6823e-10, 2.3528e-10,  ..., 7.8412e-10, 3.9128e-10,
         3.3391e-10],
        [3.1609e-10, 1.8565e-10, 4.7900e-11,  ..., 2.3875e-10, 5.7442e-11,
         8.2100e-11]], device='cuda:0')
optimizer state dict: 145.0
lr: [8.758393098742647e-06, 8.758393098742647e-06]
scheduler_last_epoch: 145


Running epoch 1, step 1160, batch 112
Sampled inputs[:2]: tensor([[    0,  3211,   328,  ...,  2098,  1231, 35325],
        [    0,  6132,   300,  ...,    37,   271,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2998e-05, -4.3917e-05, -2.5469e-04,  ...,  2.9082e-05,
         -2.1502e-05, -9.6043e-05],
        [-1.5050e-06, -1.1325e-06,  1.0878e-06,  ..., -1.2293e-06,
         -9.7603e-07, -9.7603e-07],
        [ 5.5487e-05,  1.1550e-04, -2.4520e-05,  ...,  4.8194e-05,
          7.8366e-05,  3.5323e-05],
        [-3.9339e-06, -2.9802e-06,  2.9951e-06,  ..., -3.2336e-06,
         -2.5779e-06, -2.6822e-06],
        [-3.6806e-06, -3.0547e-06,  2.7865e-06,  ..., -3.1441e-06,
         -2.6524e-06, -2.2799e-06]], device='cuda:0')
Loss: 0.9989533424377441


Running epoch 1, step 1161, batch 113
Sampled inputs[:2]: tensor([[    0, 10026,   992,  ...,   273,  2831,  8716],
        [    0,   380,  8157,  ...,   943,   352,  2278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1678e-06, -4.5891e-05, -3.6287e-04,  ..., -2.3428e-05,
          1.1766e-05, -7.4623e-05],
        [-2.9281e-06, -2.0191e-06,  1.9893e-06,  ..., -2.4587e-06,
         -1.8887e-06, -2.0713e-06],
        [ 5.1852e-05,  1.1317e-04, -2.2017e-05,  ...,  4.5139e-05,
          7.6101e-05,  3.2670e-05],
        [-7.6592e-06, -5.2154e-06,  5.4836e-06,  ..., -6.4075e-06,
         -4.9323e-06, -5.6028e-06],
        [-7.3165e-06, -5.4836e-06,  5.2601e-06,  ..., -6.2436e-06,
         -5.0664e-06, -4.7386e-06]], device='cuda:0')
Loss: 0.9638999104499817


Running epoch 1, step 1162, batch 114
Sampled inputs[:2]: tensor([[    0,   600,   518,  ...,  3134,   278, 37342],
        [    0,   271,   266,  ...,    14,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1609e-04, -1.1662e-04, -7.1356e-04,  ..., -3.7726e-05,
         -3.6741e-04, -3.2182e-04],
        [-4.2990e-06, -2.9020e-06,  3.0100e-06,  ..., -3.6210e-06,
         -2.7269e-06, -3.0771e-06],
        [ 4.8201e-05,  1.1072e-04, -1.9141e-05,  ...,  4.2055e-05,
          7.3896e-05,  2.9988e-05],
        [-1.1384e-05, -7.5549e-06,  8.3894e-06,  ..., -9.5516e-06,
         -7.1824e-06, -8.4639e-06],
        [-1.0729e-05, -7.8976e-06,  7.9125e-06,  ..., -9.1940e-06,
         -7.3165e-06, -7.0482e-06]], device='cuda:0')
Loss: 0.9575303196907043


Running epoch 1, step 1163, batch 115
Sampled inputs[:2]: tensor([[   0,  593, 1387,  ...,  508, 8222, 1415],
        [   0, 1416,  367,  ...,  555,  764,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5873e-05, -1.1581e-04, -8.8751e-04,  ...,  9.4349e-06,
         -5.0343e-04, -3.0650e-04],
        [-5.7966e-06, -3.8929e-06,  4.0680e-06,  ..., -4.7982e-06,
         -3.6061e-06, -4.1351e-06],
        [ 4.4267e-05,  1.0799e-04, -1.6220e-05,  ...,  3.8955e-05,
          7.1571e-05,  2.7187e-05],
        [-1.5408e-05, -1.0177e-05,  1.1340e-05,  ..., -1.2696e-05,
         -9.5218e-06, -1.1429e-05],
        [-1.4365e-05, -1.0535e-05,  1.0565e-05,  ..., -1.2144e-05,
         -9.6411e-06, -9.4771e-06]], device='cuda:0')
Loss: 0.968375027179718


Running epoch 1, step 1164, batch 116
Sampled inputs[:2]: tensor([[    0,  1431,   221,  ...,   756,   409,   275],
        [    0,  1016,  1387,  ..., 12156, 14838,  3550]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8731e-05, -2.0155e-04, -1.0078e-03,  ..., -1.3018e-04,
         -4.3717e-04, -2.4916e-04],
        [-7.2271e-06, -4.8243e-06,  5.0291e-06,  ..., -6.0126e-06,
         -4.5300e-06, -5.2303e-06],
        [ 4.0393e-05,  1.0539e-04, -1.3448e-05,  ...,  3.5707e-05,
          6.9127e-05,  2.4266e-05],
        [-1.9252e-05, -1.2621e-05,  1.4052e-05,  ..., -1.5959e-05,
         -1.1995e-05, -1.4499e-05],
        [-1.7926e-05, -1.3039e-05,  1.3098e-05,  ..., -1.5199e-05,
         -1.2055e-05, -1.1981e-05]], device='cuda:0')
Loss: 0.9711019396781921


Running epoch 1, step 1165, batch 117
Sampled inputs[:2]: tensor([[    0,  1241,  2098,  ...,  1862,   631,   369],
        [    0,     9,   287,  ..., 16261,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2231e-05, -3.5893e-04, -1.0845e-03,  ..., -1.0513e-04,
         -4.7362e-04, -1.9250e-04],
        [-8.6576e-06, -5.8971e-06,  6.0424e-06,  ..., -7.2196e-06,
         -5.4911e-06, -6.2734e-06],
        [ 3.6548e-05,  1.0244e-04, -1.0572e-05,  ...,  3.2488e-05,
          6.6594e-05,  2.1480e-05],
        [-2.3127e-05, -1.5467e-05,  1.6913e-05,  ..., -1.9193e-05,
         -1.4544e-05, -1.7419e-05],
        [-2.1651e-05, -1.5974e-05,  1.5825e-05,  ..., -1.8373e-05,
         -1.4663e-05, -1.4499e-05]], device='cuda:0')
Loss: 0.9879792332649231


Running epoch 1, step 1166, batch 118
Sampled inputs[:2]: tensor([[    0,   300, 26138,  ...,  7856,    14, 17535],
        [    0,   927, 13407,  ...,   616,  3955,  2567]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3829e-05, -4.0959e-04, -1.0543e-03,  ..., -5.5619e-05,
         -5.1483e-04, -1.3501e-04],
        [-1.0148e-05, -6.9626e-06,  7.1004e-06,  ..., -8.4564e-06,
         -6.4149e-06, -7.2867e-06],
        [ 1.0666e-04,  1.6978e-04, -6.3416e-05,  ...,  8.9728e-05,
          1.4694e-04,  6.4225e-05],
        [-2.7001e-05, -1.8239e-05,  1.9804e-05,  ..., -2.2426e-05,
         -1.6972e-05, -2.0206e-05],
        [-2.5257e-05, -1.8820e-05,  1.8507e-05,  ..., -2.1487e-05,
         -1.7121e-05, -1.6823e-05]], device='cuda:0')
Loss: 0.9940952658653259


Running epoch 1, step 1167, batch 119
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,    14, 15670,  ...,  2027,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3894e-05, -2.5019e-04, -8.3196e-04,  ..., -1.6197e-04,
         -5.6454e-04, -3.5860e-04],
        [-1.1586e-05, -7.9982e-06,  7.9535e-06,  ..., -9.7007e-06,
         -7.3910e-06, -8.3968e-06],
        [ 1.0288e-04,  1.6689e-04, -6.1002e-05,  ...,  8.6435e-05,
          1.4434e-04,  6.1319e-05],
        [-3.0816e-05, -2.0981e-05,  2.2188e-05,  ..., -2.5734e-05,
         -1.9595e-05, -2.3261e-05],
        [-2.8968e-05, -2.1741e-05,  2.0862e-05,  ..., -2.4796e-05,
         -1.9833e-05, -1.9521e-05]], device='cuda:0')
Loss: 0.9655978679656982
Graident accumulation at epoch 1, step 1167, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0292, -0.0140, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2228e-06,  7.8971e-05, -9.2795e-05,  ..., -2.7712e-05,
         -2.0016e-05, -1.0750e-04],
        [-1.0960e-05, -7.1878e-06,  6.6334e-06,  ..., -9.1104e-06,
         -6.3466e-06, -6.9538e-06],
        [ 9.2748e-05,  9.4482e-05, -6.3911e-05,  ...,  8.0372e-05,
          6.7501e-05,  3.6760e-05],
        [-2.9168e-06,  9.2169e-06, -1.2487e-06,  ..., -1.7806e-06,
          7.9070e-06, -5.9799e-06],
        [-2.9360e-05, -2.1978e-05,  1.9695e-05,  ..., -2.4900e-05,
         -1.9425e-05, -1.8145e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2999e-08, 5.3030e-08, 3.9265e-08,  ..., 3.8648e-08, 1.1417e-07,
         4.7810e-08],
        [6.4260e-11, 4.7922e-11, 1.2280e-11,  ..., 4.8368e-11, 1.8657e-11,
         2.0212e-11],
        [3.7293e-09, 2.3211e-09, 1.0291e-09,  ..., 2.8843e-09, 8.0839e-10,
         9.5973e-10],
        [9.3738e-10, 8.6780e-10, 2.3554e-10,  ..., 7.8400e-10, 3.9127e-10,
         3.3411e-10],
        [3.1661e-10, 1.8594e-10, 4.8287e-11,  ..., 2.3913e-10, 5.7778e-11,
         8.2399e-11]], device='cuda:0')
optimizer state dict: 146.0
lr: [8.635831001887192e-06, 8.635831001887192e-06]
scheduler_last_epoch: 146


Running epoch 1, step 1168, batch 120
Sampled inputs[:2]: tensor([[   0,  287,  358,  ...,  328, 1704, 3227],
        [   0,  908,   14,  ...,   19,   27,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0181e-04,  4.6007e-05,  1.1047e-04,  ..., -1.6225e-05,
          8.7028e-05, -7.1818e-05],
        [-1.4231e-06, -1.1176e-06,  9.9093e-07,  ..., -1.2293e-06,
         -1.0058e-06, -9.6112e-07],
        [-3.9041e-06, -3.1739e-06,  2.8461e-06,  ..., -3.3826e-06,
         -2.7716e-06, -2.6673e-06],
        [-3.8743e-06, -3.0100e-06,  2.8014e-06,  ..., -3.3379e-06,
         -2.7269e-06, -2.7567e-06],
        [-3.6508e-06, -3.0696e-06,  2.6226e-06,  ..., -3.2336e-06,
         -2.7418e-06, -2.3395e-06]], device='cuda:0')
Loss: 1.0230246782302856


Running epoch 1, step 1169, batch 121
Sampled inputs[:2]: tensor([[   0,   13, 1529,  ..., 8197, 2700, 9629],
        [   0,  292,  380,  ..., 6156,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7572e-04, -3.0510e-04, -2.1484e-04,  ..., -3.5731e-04,
          6.7056e-04,  2.1789e-04],
        [-2.8014e-06, -2.0936e-06,  1.8552e-06,  ..., -2.4438e-06,
         -1.9446e-06, -1.9893e-06],
        [-7.4655e-06, -5.8264e-06,  5.2899e-06,  ..., -6.4969e-06,
         -5.2005e-06, -5.2750e-06],
        [-7.5400e-06, -5.6028e-06,  5.2154e-06,  ..., -6.5863e-06,
         -5.2601e-06, -5.6028e-06],
        [-7.1228e-06, -5.7518e-06,  4.9919e-06,  ..., -6.3181e-06,
         -5.2601e-06, -4.6790e-06]], device='cuda:0')
Loss: 0.9970916509628296


Running epoch 1, step 1170, batch 122
Sampled inputs[:2]: tensor([[    0,  3665,  1419,  ...,   600,   847,   328],
        [    0,   768,  3227,  ...,  3487,    13, 31431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8909e-04, -3.0769e-04, -4.1451e-04,  ..., -3.1492e-04,
          4.6842e-04, -7.2310e-05],
        [-4.1500e-06, -3.0249e-06,  2.8536e-06,  ..., -3.6135e-06,
         -2.7902e-06, -2.9728e-06],
        [-1.1086e-05, -8.4192e-06,  8.1360e-06,  ..., -9.6112e-06,
         -7.4506e-06, -7.8827e-06],
        [-1.1250e-05, -8.1211e-06,  8.1062e-06,  ..., -9.7901e-06,
         -7.5698e-06, -8.4043e-06],
        [-1.0416e-05, -8.2105e-06,  7.5549e-06,  ..., -9.2238e-06,
         -7.4804e-06, -6.8843e-06]], device='cuda:0')
Loss: 0.9692817330360413


Running epoch 1, step 1171, batch 123
Sampled inputs[:2]: tensor([[   0, 3352,  259,  ..., 3565,   12,  409],
        [   0,  401, 3740,  ..., 5980,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9821e-04, -3.9439e-04, -5.4119e-04,  ..., -1.6673e-04,
          3.4014e-04, -2.0028e-04],
        [-5.5432e-06, -4.0904e-06,  3.9414e-06,  ..., -4.7907e-06,
         -3.6471e-06, -3.9712e-06],
        [-1.4991e-05, -1.1504e-05,  1.1340e-05,  ..., -1.2934e-05,
         -9.8497e-06, -1.0729e-05],
        [-1.5050e-05, -1.0967e-05,  1.1206e-05,  ..., -1.2979e-05,
         -9.8795e-06, -1.1265e-05],
        [-1.3858e-05, -1.1042e-05,  1.0341e-05,  ..., -1.2219e-05,
         -9.7752e-06, -9.2089e-06]], device='cuda:0')
Loss: 0.9916484355926514


Running epoch 1, step 1172, batch 124
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,  2326,  3595,  6887],
        [    0, 27366,   504,  ...,  1358,   365,  6883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9104e-04, -4.3121e-04, -7.9110e-04,  ...,  1.4135e-05,
          2.8016e-04, -2.5153e-04],
        [-6.9141e-06, -5.0403e-06,  4.9546e-06,  ..., -5.9977e-06,
         -4.5151e-06, -4.9546e-06],
        [-1.8671e-05, -1.4111e-05,  1.4231e-05,  ..., -1.6153e-05,
         -1.2159e-05, -1.3366e-05],
        [-1.8820e-05, -1.3530e-05,  1.4126e-05,  ..., -1.6272e-05,
         -1.2249e-05, -1.4082e-05],
        [-1.7300e-05, -1.3605e-05,  1.3024e-05,  ..., -1.5303e-05,
         -1.2115e-05, -1.1489e-05]], device='cuda:0')
Loss: 1.0285649299621582


Running epoch 1, step 1173, batch 125
Sampled inputs[:2]: tensor([[    0,   367,  2870,  ...,  1456, 17304,   292],
        [    0,   360,   259,  ...,  5710,   278,  2433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4602e-04, -5.0562e-04, -7.7487e-04,  ...,  1.1411e-04,
          2.1691e-04, -3.7264e-04],
        [-8.3521e-06, -5.9605e-06,  5.9754e-06,  ..., -7.2271e-06,
         -5.3532e-06, -6.0275e-06],
        [-2.2486e-05, -1.6645e-05,  1.7121e-05,  ..., -1.9386e-05,
         -1.4365e-05, -1.6183e-05],
        [-2.2784e-05, -1.6004e-05,  1.7062e-05,  ..., -1.9625e-05,
         -1.4529e-05, -1.7092e-05],
        [-2.0847e-05, -1.6049e-05,  1.5661e-05,  ..., -1.8358e-05,
         -1.4320e-05, -1.3918e-05]], device='cuda:0')
Loss: 0.9862508177757263


Running epoch 1, step 1174, batch 126
Sampled inputs[:2]: tensor([[    0, 39004,   266,  ...,   287, 21972,   278],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5876e-04, -5.8537e-04, -7.8580e-04,  ...,  1.4260e-04,
          1.9722e-04, -4.3697e-04],
        [-9.7975e-06, -6.9439e-06,  6.9886e-06,  ..., -8.4415e-06,
         -6.2920e-06, -7.1153e-06],
        [-2.6450e-05, -1.9491e-05,  2.0087e-05,  ..., -2.2739e-05,
         -1.6972e-05, -1.9208e-05],
        [-2.6748e-05, -1.8671e-05,  1.9982e-05,  ..., -2.2948e-05,
         -1.7107e-05, -2.0221e-05],
        [-2.4527e-05, -1.8820e-05,  1.8373e-05,  ..., -2.1562e-05,
         -1.6928e-05, -1.6555e-05]], device='cuda:0')
Loss: 0.9858765006065369


Running epoch 1, step 1175, batch 127
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,     9, 25368,  ...,   271,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5119e-04, -6.8037e-04, -8.6823e-04,  ...,  8.3484e-05,
          1.5278e-04, -6.8327e-04],
        [-1.1161e-05, -7.8976e-06,  7.8641e-06,  ..., -9.6560e-06,
         -7.2271e-06, -8.1733e-06],
        [-3.0085e-05, -2.2098e-05,  2.2650e-05,  ..., -2.5898e-05,
         -1.9386e-05, -2.1994e-05],
        [-3.0473e-05, -2.1204e-05,  2.2516e-05,  ..., -2.6211e-05,
         -1.9610e-05, -2.3201e-05],
        [-2.7984e-05, -2.1383e-05,  2.0787e-05,  ..., -2.4632e-05,
         -1.9401e-05, -1.8999e-05]], device='cuda:0')
Loss: 0.9688968658447266
Graident accumulation at epoch 1, step 1175, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0081,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0292, -0.0140, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.6220e-05,  3.0360e-06, -1.7034e-04,  ..., -1.6592e-05,
         -2.7362e-06, -1.6508e-04],
        [-1.0980e-05, -7.2588e-06,  6.7564e-06,  ..., -9.1649e-06,
         -6.4346e-06, -7.0758e-06],
        [ 8.0465e-05,  8.2824e-05, -5.5255e-05,  ...,  6.9745e-05,
          5.8813e-05,  3.0884e-05],
        [-5.6724e-06,  6.1748e-06,  1.1277e-06,  ..., -4.2236e-06,
          5.1553e-06, -7.7020e-06],
        [-2.9223e-05, -2.1918e-05,  1.9805e-05,  ..., -2.4873e-05,
         -1.9423e-05, -1.8230e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3129e-08, 5.3440e-08, 3.9980e-08,  ..., 3.8616e-08, 1.1408e-07,
         4.8229e-08],
        [6.4320e-11, 4.7936e-11, 1.2330e-11,  ..., 4.8413e-11, 1.8691e-11,
         2.0258e-11],
        [3.7265e-09, 2.3193e-09, 1.0286e-09,  ..., 2.8821e-09, 8.0796e-10,
         9.5926e-10],
        [9.3737e-10, 8.6739e-10, 2.3581e-10,  ..., 7.8390e-10, 3.9126e-10,
         3.3432e-10],
        [3.1708e-10, 1.8621e-10, 4.8671e-11,  ..., 2.3950e-10, 5.8097e-11,
         8.2677e-11]], device='cuda:0')
optimizer state dict: 147.0
lr: [8.513477361962645e-06, 8.513477361962645e-06]
scheduler_last_epoch: 147


Running epoch 1, step 1176, batch 128
Sampled inputs[:2]: tensor([[    0,    12,   344,  ...,   824,    12,   968],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6019e-05,  8.8967e-05, -4.4994e-05,  ...,  4.7401e-06,
          8.2947e-05, -8.8217e-05],
        [-1.3784e-06, -9.3877e-07,  1.0207e-06,  ..., -1.1697e-06,
         -8.8662e-07, -9.9838e-07],
        [-3.6806e-06, -2.6375e-06,  2.8908e-06,  ..., -3.1292e-06,
         -2.3693e-06, -2.6822e-06],
        [-3.8445e-06, -2.5928e-06,  2.9802e-06,  ..., -3.2634e-06,
         -2.4736e-06, -2.9355e-06],
        [-3.4571e-06, -2.6077e-06,  2.6822e-06,  ..., -3.0249e-06,
         -2.4140e-06, -2.3395e-06]], device='cuda:0')
Loss: 0.9829884171485901


Running epoch 1, step 1177, batch 129
Sampled inputs[:2]: tensor([[   0,  401,  266,  ...,  266, 2236, 1458],
        [   0,  445,   16,  ..., 7747, 5308, 6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9297e-05, -7.9669e-05, -1.6615e-04,  ..., -1.5242e-05,
         -7.8698e-05, -2.3772e-04],
        [-2.7120e-06, -1.9670e-06,  2.1011e-06,  ..., -2.3171e-06,
         -1.6913e-06, -1.9297e-06],
        [-7.4506e-06, -5.6624e-06,  6.1244e-06,  ..., -6.3628e-06,
         -4.6641e-06, -5.2899e-06],
        [-7.5251e-06, -5.3942e-06,  6.1095e-06,  ..., -6.4224e-06,
         -4.6790e-06, -5.6028e-06],
        [-6.7949e-06, -5.3793e-06,  5.5283e-06,  ..., -5.9605e-06,
         -4.6045e-06, -4.4852e-06]], device='cuda:0')
Loss: 0.9967907667160034


Running epoch 1, step 1178, batch 130
Sampled inputs[:2]: tensor([[    0,   266,  2555,  ...,   587,    14, 14947],
        [    0, 10511,  3887,  ...,  3504,   298,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3933e-04, -2.0821e-04, -2.2485e-04,  ..., -1.1538e-04,
         -6.3408e-05, -3.7845e-04],
        [-4.1425e-06, -3.0026e-06,  2.9244e-06,  ..., -3.5465e-06,
         -2.7120e-06, -3.1218e-06],
        [-1.1474e-05, -8.6278e-06,  8.6278e-06,  ..., -9.7156e-06,
         -7.4059e-06, -8.5682e-06],
        [-1.1250e-05, -8.0019e-06,  8.3447e-06,  ..., -9.5665e-06,
         -7.2867e-06, -8.8066e-06],
        [-1.0699e-05, -8.3148e-06,  7.9274e-06,  ..., -9.2983e-06,
         -7.4208e-06, -7.4655e-06]], device='cuda:0')
Loss: 0.943217396736145


Running epoch 1, step 1179, batch 131
Sampled inputs[:2]: tensor([[    0,   328,  6379,  ...,   287,  1342,     9],
        [    0, 13466,    14,  ..., 11227,  1966,  4039]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2440e-04, -2.1968e-04, -2.1284e-04,  ..., -1.3536e-04,
          6.3449e-05, -3.8938e-04],
        [-5.6103e-06, -4.0680e-06,  3.8445e-06,  ..., -4.8056e-06,
         -3.6545e-06, -4.3288e-06],
        [-1.5646e-05, -1.1712e-05,  1.1399e-05,  ..., -1.3247e-05,
         -1.0028e-05, -1.1936e-05],
        [-1.5244e-05, -1.0818e-05,  1.0937e-05,  ..., -1.2964e-05,
         -9.8050e-06, -1.2159e-05],
        [-1.4663e-05, -1.1310e-05,  1.0535e-05,  ..., -1.2711e-05,
         -1.0073e-05, -1.0476e-05]], device='cuda:0')
Loss: 0.9881731867790222


Running epoch 1, step 1180, batch 132
Sampled inputs[:2]: tensor([[   0,  273,  298,  ..., 7437, 2767,  518],
        [   0, 5998,  591,  ..., 3126,   12,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3365e-04, -1.4684e-04, -3.7080e-04,  ..., -2.0561e-04,
         -5.4641e-05, -6.4788e-04],
        [-6.8918e-06, -4.9956e-06,  4.8056e-06,  ..., -5.9754e-06,
         -4.5337e-06, -5.3421e-06],
        [-1.9133e-05, -1.4320e-05,  1.4186e-05,  ..., -1.6347e-05,
         -1.2338e-05, -1.4558e-05],
        [-1.8686e-05, -1.3217e-05,  1.3649e-05,  ..., -1.6063e-05,
         -1.2130e-05, -1.4976e-05],
        [-1.7896e-05, -1.3828e-05,  1.3098e-05,  ..., -1.5631e-05,
         -1.2368e-05, -1.2726e-05]], device='cuda:0')
Loss: 0.9524164199829102


Running epoch 1, step 1181, batch 133
Sampled inputs[:2]: tensor([[    0,    89,  6893,  ...,  5254,   278,  4531],
        [    0, 32878,   593,  ...,   437,  1329,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8985e-04, -2.2097e-04, -5.1643e-04,  ..., -2.9830e-04,
         -1.8253e-04, -9.1545e-04],
        [-8.1584e-06, -5.8115e-06,  5.8562e-06,  ..., -7.0855e-06,
         -5.3532e-06, -6.3032e-06],
        [-2.2501e-05, -1.6585e-05,  1.7151e-05,  ..., -1.9237e-05,
         -1.4484e-05, -1.6987e-05],
        [-2.2247e-05, -1.5423e-05,  1.6764e-05,  ..., -1.9148e-05,
         -1.4395e-05, -1.7732e-05],
        [-2.1011e-05, -1.6019e-05,  1.5795e-05,  ..., -1.8343e-05,
         -1.4484e-05, -1.4797e-05]], device='cuda:0')
Loss: 0.9607899188995361


Running epoch 1, step 1182, batch 134
Sampled inputs[:2]: tensor([[    0,  4710,    12,  ...,  3969,     9, 11692],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1836e-04, -1.2776e-04, -4.8489e-04,  ..., -2.2345e-04,
         -1.2701e-05, -9.0466e-04],
        [-9.5442e-06, -6.7428e-06,  6.7428e-06,  ..., -8.2925e-06,
         -6.2399e-06, -7.4431e-06],
        [-2.6286e-05, -1.9193e-05,  1.9804e-05,  ..., -2.2441e-05,
         -1.6823e-05, -1.9953e-05],
        [-2.6003e-05, -1.7881e-05,  1.9297e-05,  ..., -2.2396e-05,
         -1.6794e-05, -2.0877e-05],
        [-2.4483e-05, -1.8448e-05,  1.8239e-05,  ..., -2.1309e-05,
         -1.6734e-05, -1.7285e-05]], device='cuda:0')
Loss: 0.9619941115379333


Running epoch 1, step 1183, batch 135
Sampled inputs[:2]: tensor([[    0,  4359,  5768,  ...,  4402,   292,    69],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9236e-05, -2.8593e-04, -5.3108e-04,  ..., -3.2439e-04,
          1.6622e-04, -7.0677e-04],
        [-1.0982e-05, -7.7784e-06,  7.8306e-06,  ..., -9.5069e-06,
         -7.1153e-06, -8.4415e-06],
        [-3.0279e-05, -2.2218e-05,  2.2963e-05,  ..., -2.5839e-05,
         -1.9312e-05, -2.2769e-05],
        [-2.9936e-05, -2.0713e-05,  2.2411e-05,  ..., -2.5734e-05,
         -1.9222e-05, -2.3767e-05],
        [-2.8074e-05, -2.1309e-05,  2.1055e-05,  ..., -2.4453e-05,
         -1.9163e-05, -1.9670e-05]], device='cuda:0')
Loss: 1.0069125890731812
Graident accumulation at epoch 1, step 1183, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0061, -0.0142,  0.0021,  ..., -0.0020,  0.0237, -0.0190],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0293, -0.0140, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.0521e-05, -2.5860e-05, -2.0641e-04,  ..., -4.7373e-05,
          1.4159e-05, -2.1925e-04],
        [-1.0980e-05, -7.3108e-06,  6.8639e-06,  ..., -9.1991e-06,
         -6.5027e-06, -7.2123e-06],
        [ 6.9390e-05,  7.2320e-05, -4.7433e-05,  ...,  6.0187e-05,
          5.1000e-05,  2.5519e-05],
        [-8.0988e-06,  3.4860e-06,  3.2561e-06,  ..., -6.3747e-06,
          2.7175e-06, -9.3086e-06],
        [-2.9108e-05, -2.1857e-05,  1.9930e-05,  ..., -2.4831e-05,
         -1.9397e-05, -1.8374e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3064e-08, 5.3468e-08, 4.0222e-08,  ..., 3.8683e-08, 1.1399e-07,
         4.8681e-08],
        [6.4376e-11, 4.7949e-11, 1.2379e-11,  ..., 4.8455e-11, 1.8723e-11,
         2.0309e-11],
        [3.7237e-09, 2.3174e-09, 1.0281e-09,  ..., 2.8799e-09, 8.0753e-10,
         9.5881e-10],
        [9.3733e-10, 8.6695e-10, 2.3607e-10,  ..., 7.8378e-10, 3.9124e-10,
         3.3455e-10],
        [3.1755e-10, 1.8648e-10, 4.9066e-11,  ..., 2.3985e-10, 5.8406e-11,
         8.2981e-11]], device='cuda:0')
optimizer state dict: 148.0
lr: [8.391350875673234e-06, 8.391350875673234e-06]
scheduler_last_epoch: 148


Running epoch 1, step 1184, batch 136
Sampled inputs[:2]: tensor([[    0,  1415,   300,  ...,  1497,  5715,  4555],
        [    0, 15666,   609,  ...,   527,  4486,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0249e-05, -2.3272e-05, -1.6071e-04,  ...,  7.2287e-05,
         -3.7309e-05, -1.2756e-04],
        [-1.3560e-06, -8.7172e-07,  1.0282e-06,  ..., -1.1399e-06,
         -8.2701e-07, -1.0580e-06],
        [-3.6955e-06, -2.4587e-06,  3.0249e-06,  ..., -3.0696e-06,
         -2.1905e-06, -2.8312e-06],
        [-3.7104e-06, -2.3097e-06,  2.9504e-06,  ..., -3.0994e-06,
         -2.2352e-06, -3.0100e-06],
        [-3.2932e-06, -2.2948e-06,  2.6375e-06,  ..., -2.8014e-06,
         -2.1309e-06, -2.3395e-06]], device='cuda:0')
Loss: 0.9694535732269287


Running epoch 1, step 1185, batch 137
Sampled inputs[:2]: tensor([[    0, 25845,  4034,  ...,   474,   221,   474],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6273e-05, -9.5217e-05, -3.8530e-04,  ...,  1.6987e-04,
         -1.1960e-04, -2.5759e-04],
        [-2.6971e-06, -1.7397e-06,  1.9968e-06,  ..., -2.2799e-06,
         -1.6019e-06, -2.1458e-06],
        [-7.4953e-06, -4.9621e-06,  5.9903e-06,  ..., -6.2138e-06,
         -4.3213e-06, -5.7966e-06],
        [-7.3612e-06, -4.5747e-06,  5.7369e-06,  ..., -6.1393e-06,
         -4.2915e-06, -6.0201e-06],
        [-6.7204e-06, -4.6194e-06,  5.2601e-06,  ..., -5.6773e-06,
         -4.2021e-06, -4.7833e-06]], device='cuda:0')
Loss: 0.9371588230133057


Running epoch 1, step 1186, batch 138
Sampled inputs[:2]: tensor([[   0,  741, 4933,  ...,  932,  365,  838],
        [   0, 4995,  287,  ...,  300, 4531, 4729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6544e-05, -6.9369e-06, -4.0587e-04,  ...,  1.7901e-04,
         -1.1029e-04, -2.5942e-04],
        [-4.0531e-06, -2.6785e-06,  2.9951e-06,  ..., -3.4571e-06,
         -2.4959e-06, -3.1590e-06],
        [-1.1265e-05, -7.6592e-06,  8.9556e-06,  ..., -9.4622e-06,
         -6.7800e-06, -8.5831e-06],
        [-1.1116e-05, -7.1079e-06,  8.6427e-06,  ..., -9.3728e-06,
         -6.7353e-06, -8.9407e-06],
        [-1.0237e-05, -7.2271e-06,  7.9870e-06,  ..., -8.7470e-06,
         -6.6608e-06, -7.2122e-06]], device='cuda:0')
Loss: 0.9834208488464355


Running epoch 1, step 1187, batch 139
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0,  2140,    12,  ...,   696,   688,  1998]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2393e-04,  9.2241e-05, -3.3286e-04,  ...,  2.1623e-04,
         -2.5639e-04, -2.8066e-04],
        [-5.4613e-06, -3.6545e-06,  4.0829e-06,  ..., -4.6119e-06,
         -3.3006e-06, -4.0680e-06],
        [-1.5140e-05, -1.0461e-05,  1.2144e-05,  ..., -1.2666e-05,
         -9.0301e-06, -1.1116e-05],
        [-1.4991e-05, -9.7752e-06,  1.1802e-05,  ..., -1.2562e-05,
         -8.9556e-06, -1.1563e-05],
        [-1.3679e-05, -9.8497e-06,  1.0788e-05,  ..., -1.1683e-05,
         -8.8513e-06, -9.3132e-06]], device='cuda:0')
Loss: 0.9764667749404907


Running epoch 1, step 1188, batch 140
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,   923,  2583,  ..., 11385,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6162e-05,  4.2588e-05, -3.8124e-04,  ...,  2.4669e-04,
         -1.0901e-04, -1.7463e-04],
        [-6.8247e-06, -4.4554e-06,  5.0440e-06,  ..., -5.7295e-06,
         -4.0531e-06, -5.1931e-06],
        [-1.9014e-05, -1.2770e-05,  1.5035e-05,  ..., -1.5765e-05,
         -1.1072e-05, -1.4171e-05],
        [-1.8954e-05, -1.1995e-05,  1.4693e-05,  ..., -1.5765e-05,
         -1.1101e-05, -1.4916e-05],
        [-1.7151e-05, -1.2010e-05,  1.3366e-05,  ..., -1.4514e-05,
         -1.0818e-05, -1.1876e-05]], device='cuda:0')
Loss: 0.9642828106880188


Running epoch 1, step 1189, batch 141
Sampled inputs[:2]: tensor([[   0,  367, 3675,  ...,   22, 3180,   14],
        [   0,  365, 2714,  ...,  298,  273,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9278e-05, -3.8372e-06, -4.5701e-04,  ...,  3.2050e-04,
         -8.8991e-05, -7.9604e-05],
        [-8.1807e-06, -5.4538e-06,  6.0424e-06,  ..., -6.8918e-06,
         -4.9509e-06, -6.1356e-06],
        [-2.2769e-05, -1.5676e-05,  1.7971e-05,  ..., -1.8984e-05,
         -1.3560e-05, -1.6779e-05],
        [-2.2799e-05, -1.4812e-05,  1.7643e-05,  ..., -1.9073e-05,
         -1.3664e-05, -1.7717e-05],
        [-2.0623e-05, -1.4782e-05,  1.6049e-05,  ..., -1.7554e-05,
         -1.3262e-05, -1.4141e-05]], device='cuda:0')
Loss: 1.0050532817840576


Running epoch 1, step 1190, batch 142
Sampled inputs[:2]: tensor([[    0, 25241,   717,  ...,   413,    16,    14],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0964e-04, -5.9017e-05, -3.7577e-04,  ...,  3.4774e-04,
         -1.7744e-04, -1.5907e-04],
        [-9.5218e-06, -6.3963e-06,  7.0184e-06,  ..., -8.0168e-06,
         -5.8487e-06, -7.1190e-06],
        [-2.6643e-05, -1.8507e-05,  2.0951e-05,  ..., -2.2247e-05,
         -1.6168e-05, -1.9610e-05],
        [-2.6584e-05, -1.7419e-05,  2.0534e-05,  ..., -2.2247e-05,
         -1.6198e-05, -2.0608e-05],
        [-2.4140e-05, -1.7464e-05,  1.8716e-05,  ..., -2.0579e-05,
         -1.5795e-05, -1.6555e-05]], device='cuda:0')
Loss: 0.9951557517051697


Running epoch 1, step 1191, batch 143
Sampled inputs[:2]: tensor([[    0,   266, 10262,  ...,   271,  3437,  4392],
        [    0,  5160,   278,  ...,   496,    14, 46919]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9226e-05,  9.8875e-06, -2.9340e-04,  ...,  3.0574e-04,
         -2.9646e-04, -1.2109e-04],
        [-1.0997e-05, -7.4245e-06,  7.8902e-06,  ..., -9.2834e-06,
         -6.8694e-06, -8.2813e-06],
        [-3.0786e-05, -2.1502e-05,  2.3544e-05,  ..., -2.5809e-05,
         -1.9059e-05, -2.2888e-05],
        [-3.0607e-05, -2.0191e-05,  2.3007e-05,  ..., -2.5690e-05,
         -1.8969e-05, -2.3827e-05],
        [-2.8193e-05, -2.0429e-05,  2.1204e-05,  ..., -2.4110e-05,
         -1.8746e-05, -1.9595e-05]], device='cuda:0')
Loss: 0.9948379397392273
Graident accumulation at epoch 1, step 1191, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0142,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.1546e-05, -2.2285e-05, -2.1511e-04,  ..., -1.2061e-05,
         -1.6903e-05, -2.0943e-04],
        [-1.0982e-05, -7.3221e-06,  6.9665e-06,  ..., -9.2076e-06,
         -6.5394e-06, -7.3192e-06],
        [ 5.9373e-05,  6.2938e-05, -4.0335e-05,  ...,  5.1587e-05,
          4.3994e-05,  2.0678e-05],
        [-1.0350e-05,  1.1183e-06,  5.2312e-06,  ..., -8.3062e-06,
          5.4887e-07, -1.0760e-05],
        [-2.9016e-05, -2.1714e-05,  2.0057e-05,  ..., -2.4759e-05,
         -1.9331e-05, -1.8496e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2992e-08, 5.3415e-08, 4.0268e-08,  ..., 3.8737e-08, 1.1397e-07,
         4.8647e-08],
        [6.4433e-11, 4.7956e-11, 1.2429e-11,  ..., 4.8492e-11, 1.8751e-11,
         2.0357e-11],
        [3.7209e-09, 2.3156e-09, 1.0276e-09,  ..., 2.8776e-09, 8.0708e-10,
         9.5838e-10],
        [9.3733e-10, 8.6649e-10, 2.3637e-10,  ..., 7.8366e-10, 3.9121e-10,
         3.3478e-10],
        [3.1803e-10, 1.8671e-10, 4.9466e-11,  ..., 2.4020e-10, 5.8699e-11,
         8.3282e-11]], device='cuda:0')
optimizer state dict: 149.0
lr: [8.269470205012111e-06, 8.269470205012111e-06]
scheduler_last_epoch: 149


Running epoch 1, step 1192, batch 144
Sampled inputs[:2]: tensor([[    0,  3167,   300,  ...,  1109,   490,  1985],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0930e-04,  6.1734e-05,  1.4113e-04,  ...,  3.3224e-05,
          6.0806e-05,  6.4600e-05],
        [-1.4976e-06, -8.9034e-07,  9.0525e-07,  ..., -1.2666e-06,
         -9.0897e-07, -1.1697e-06],
        [-4.0233e-06, -2.5481e-06,  2.5928e-06,  ..., -3.3975e-06,
         -2.4587e-06, -3.1292e-06],
        [-3.9637e-06, -2.3395e-06,  2.5034e-06,  ..., -3.3677e-06,
         -2.4289e-06, -3.2187e-06],
        [-3.9637e-06, -2.6077e-06,  2.5034e-06,  ..., -3.3975e-06,
         -2.5928e-06, -2.9206e-06]], device='cuda:0')
Loss: 0.9805275797843933


Running epoch 1, step 1193, batch 145
Sampled inputs[:2]: tensor([[   0,  857,  352,  ..., 3608,  271,  995],
        [   0,  278, 7914,  ..., 1194,  300, 4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0915e-04,  1.0850e-04,  6.8453e-05,  ...,  1.6284e-04,
          6.2696e-05, -6.1646e-05],
        [-2.8163e-06, -1.7881e-06,  1.9185e-06,  ..., -2.3916e-06,
         -1.7099e-06, -2.0899e-06],
        [-7.6443e-06, -5.0962e-06,  5.5134e-06,  ..., -6.4522e-06,
         -4.6194e-06, -5.6326e-06],
        [-7.7784e-06, -4.8727e-06,  5.5581e-06,  ..., -6.5863e-06,
         -4.7237e-06, -6.0052e-06],
        [-7.2271e-06, -5.0068e-06,  5.1111e-06,  ..., -6.2138e-06,
         -4.6939e-06, -5.0217e-06]], device='cuda:0')
Loss: 0.9698606729507446


Running epoch 1, step 1194, batch 146
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  2117,  2792, 12848],
        [    0,   300,   266,  ...,   266,   912, 11457]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8095e-04,  2.1908e-04, -2.9389e-05,  ...,  1.7226e-04,
          4.9437e-05,  5.4538e-05],
        [-4.2617e-06, -2.7418e-06,  2.8722e-06,  ..., -3.6061e-06,
         -2.6301e-06, -3.1330e-06],
        [-1.1608e-05, -7.8380e-06,  8.3297e-06,  ..., -9.7603e-06,
         -7.1377e-06, -8.4639e-06],
        [ 6.1610e-05,  6.8517e-05, -3.1313e-05,  ...,  3.4753e-05,
          6.8168e-05,  1.6524e-05],
        [-1.0923e-05, -7.6443e-06,  7.6890e-06,  ..., -9.3579e-06,
         -7.1973e-06, -7.4804e-06]], device='cuda:0')
Loss: 0.9749752879142761


Running epoch 1, step 1195, batch 147
Sampled inputs[:2]: tensor([[    0,   199,  2834,  ...,   287,  3121,   292],
        [    0,   292, 16983,  ...,   221,   474,  4800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1861e-04,  2.6742e-04, -1.3652e-04,  ...,  2.1119e-04,
          1.0580e-04,  1.1872e-04],
        [-5.5209e-06, -3.5241e-06,  3.7327e-06,  ..., -4.7386e-06,
         -3.5018e-06, -4.1686e-06],
        [-1.5050e-05, -1.0073e-05,  1.0908e-05,  ..., -1.2785e-05,
         -9.4771e-06, -1.1161e-05],
        [ 5.8078e-05,  6.6416e-05, -2.8750e-05,  ...,  3.1579e-05,
          6.5695e-05,  1.3529e-05],
        [-1.4156e-05, -9.8348e-06,  1.0088e-05,  ..., -1.2234e-05,
         -9.5218e-06, -9.8050e-06]], device='cuda:0')
Loss: 0.9556501507759094


Running epoch 1, step 1196, batch 148
Sampled inputs[:2]: tensor([[   0,  516,  596,  ..., 3109,  287,  394],
        [   0, 1871,  518,  ...,  271,  259, 1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6479e-04,  2.6234e-04, -1.3525e-04,  ...,  1.2686e-04,
          2.7098e-04,  2.7631e-04],
        [-6.9141e-06, -4.5002e-06,  4.7386e-06,  ..., -5.9009e-06,
         -4.5002e-06, -5.1670e-06],
        [-1.8835e-05, -1.2845e-05,  1.3784e-05,  ..., -1.5929e-05,
         -1.2174e-05, -1.3858e-05],
        [ 5.4234e-05,  6.3763e-05, -2.5859e-05,  ...,  2.8391e-05,
          6.2938e-05,  1.0653e-05],
        [-1.7732e-05, -1.2547e-05,  1.2785e-05,  ..., -1.5259e-05,
         -1.2204e-05, -1.2189e-05]], device='cuda:0')
Loss: 0.9934963583946228


Running epoch 1, step 1197, batch 149
Sampled inputs[:2]: tensor([[   0,  380,  333,  ..., 8127,  504,  679],
        [   0, 2380, 2667,  ...,   14,  381, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0798e-04,  4.0194e-04, -2.4912e-04,  ...,  3.8142e-04,
          1.2239e-04, -6.4491e-05],
        [-8.2776e-06, -5.3905e-06,  5.7444e-06,  ..., -7.0930e-06,
         -5.4128e-06, -6.0946e-06],
        [-2.2471e-05, -1.5333e-05,  1.6615e-05,  ..., -1.9118e-05,
         -1.4618e-05, -1.6332e-05],
        [ 5.0479e-05,  6.1349e-05, -2.2969e-05,  ...,  2.5083e-05,
          6.0405e-05,  7.9561e-06],
        [-2.1115e-05, -1.4976e-05,  1.5393e-05,  ..., -1.8284e-05,
         -1.4633e-05, -1.4335e-05]], device='cuda:0')
Loss: 1.015285611152649


Running epoch 1, step 1198, batch 150
Sampled inputs[:2]: tensor([[   0,   25,    5,  ..., 3935,   14,   16],
        [   0,   12,  287,  ...,   17,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7667e-04,  3.7774e-04, -3.9146e-04,  ...,  4.3009e-04,
         -2.6236e-05,  4.6604e-05],
        [-9.6411e-06, -6.2846e-06,  6.7055e-06,  ..., -8.2478e-06,
         -6.2920e-06, -7.0706e-06],
        [-2.6166e-05, -1.7881e-05,  1.9431e-05,  ..., -2.2233e-05,
         -1.6987e-05, -1.8969e-05],
        [ 4.6679e-05,  5.8906e-05, -2.0137e-05,  ...,  2.1864e-05,
          5.7946e-05,  5.0951e-06],
        [-2.4572e-05, -1.7464e-05,  1.7986e-05,  ..., -2.1249e-05,
         -1.7017e-05, -1.6615e-05]], device='cuda:0')
Loss: 0.9540926218032837


Running epoch 1, step 1199, batch 151
Sampled inputs[:2]: tensor([[    0,  6203,   352,  ...,   266,  3437,   287],
        [    0,   437,  1690,  ...,  1274, 10695, 10762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4015e-05,  4.8970e-04, -4.0176e-04,  ...,  5.0642e-04,
         -4.7747e-05,  7.3773e-05],
        [-1.0982e-05, -7.2122e-06,  7.7561e-06,  ..., -9.3728e-06,
         -7.1451e-06, -8.0243e-06],
        [-3.0011e-05, -2.0638e-05,  2.2590e-05,  ..., -2.5436e-05,
         -1.9416e-05, -2.1666e-05],
        [ 4.2909e-05,  5.6343e-05, -1.7068e-05,  ...,  1.8735e-05,
          5.5577e-05,  2.3086e-06],
        [-2.8029e-05, -2.0042e-05,  2.0802e-05,  ..., -2.4170e-05,
         -1.9342e-05, -1.8895e-05]], device='cuda:0')
Loss: 0.9988723993301392
Graident accumulation at epoch 1, step 1199, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0142,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.4793e-05,  2.8914e-05, -2.3378e-04,  ...,  3.9787e-05,
         -1.9987e-05, -1.8111e-04],
        [-1.0982e-05, -7.3111e-06,  7.0454e-06,  ..., -9.2241e-06,
         -6.5999e-06, -7.3897e-06],
        [ 5.0434e-05,  5.4580e-05, -3.4043e-05,  ...,  4.3885e-05,
          3.7653e-05,  1.6444e-05],
        [-5.0238e-06,  6.6407e-06,  3.0013e-06,  ..., -5.6021e-06,
          6.0517e-06, -9.4535e-06],
        [-2.8918e-05, -2.1547e-05,  2.0132e-05,  ..., -2.4700e-05,
         -1.9332e-05, -1.8536e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2925e-08, 5.3601e-08, 4.0389e-08,  ..., 3.8955e-08, 1.1386e-07,
         4.8604e-08],
        [6.4489e-11, 4.7960e-11, 1.2477e-11,  ..., 4.8532e-11, 1.8783e-11,
         2.0401e-11],
        [3.7181e-09, 2.3137e-09, 1.0271e-09,  ..., 2.8754e-09, 8.0665e-10,
         9.5789e-10],
        [9.3824e-10, 8.6880e-10, 2.3642e-10,  ..., 7.8322e-10, 3.9391e-10,
         3.3445e-10],
        [3.1850e-10, 1.8693e-10, 4.9850e-11,  ..., 2.4054e-10, 5.9014e-11,
         8.3556e-11]], device='cuda:0')
optimizer state dict: 150.0
lr: [8.147853974409676e-06, 8.147853974409676e-06]
scheduler_last_epoch: 150


Running epoch 1, step 1200, batch 152
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0,   554,  1034,  ...,  3313,   365,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4571e-05,  1.8521e-05, -7.6484e-05,  ..., -4.8038e-05,
          9.3015e-05, -1.2321e-04],
        [-1.3635e-06, -8.7172e-07,  9.8348e-07,  ..., -1.1772e-06,
         -9.4995e-07, -9.9093e-07],
        [ 1.7833e-05,  1.0126e-04, -6.3078e-05,  ...,  6.4135e-05,
          9.7003e-05,  2.2234e-05],
        [-3.7402e-06, -2.3395e-06,  2.8312e-06,  ..., -3.2336e-06,
         -2.6375e-06, -2.8461e-06],
        [-3.3975e-06, -2.3842e-06,  2.6226e-06,  ..., -2.9653e-06,
         -2.5034e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.005391001701355


Running epoch 1, step 1201, batch 153
Sampled inputs[:2]: tensor([[   0, 1188,  278,  ...,  271, 8368,  292],
        [   0,  266, 2653,  ...,   29,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4403e-06,  1.1012e-04, -1.3614e-04,  ...,  4.6280e-05,
          6.0301e-05, -2.4727e-05],
        [-2.9132e-06, -1.7174e-06,  1.8217e-06,  ..., -2.4438e-06,
         -1.8999e-06, -2.2724e-06],
        [ 1.3393e-05,  9.8767e-05, -6.0530e-05,  ...,  6.0559e-05,
          9.4351e-05,  1.8642e-05],
        [-7.8529e-06, -4.5151e-06,  5.1409e-06,  ..., -6.5714e-06,
         -5.1111e-06, -6.2883e-06],
        [-7.7486e-06, -4.8578e-06,  5.0664e-06,  ..., -6.4969e-06,
         -5.2154e-06, -5.6177e-06]], device='cuda:0')
Loss: 0.9729517102241516


Running epoch 1, step 1202, batch 154
Sampled inputs[:2]: tensor([[    0,  1234,   278,  ...,  1237,  1008,   417],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4047e-07,  1.6037e-04, -1.2833e-04,  ..., -4.0273e-05,
          1.9466e-05, -1.1138e-04],
        [-4.2468e-06, -2.5965e-06,  2.7902e-06,  ..., -3.6359e-06,
         -2.8089e-06, -3.2112e-06],
        [ 9.8312e-06,  9.6308e-05, -5.7788e-05,  ...,  5.7415e-05,
          9.1982e-05,  1.6169e-05],
        [-1.1548e-05, -6.8843e-06,  7.9274e-06,  ..., -9.8497e-06,
         -7.6145e-06, -9.0450e-06],
        [-1.1072e-05, -7.2718e-06,  7.5847e-06,  ..., -9.4771e-06,
         -7.5698e-06, -7.7635e-06]], device='cuda:0')
Loss: 0.9938560128211975


Running epoch 1, step 1203, batch 155
Sampled inputs[:2]: tensor([[    0,   895,  4110,  ...,  1578,  1245,    13],
        [    0,   292, 23242,  ...,  6494,  3560,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1978e-05,  2.7801e-04, -9.3278e-05,  ..., -1.9243e-05,
          1.2302e-04, -7.8324e-05],
        [-5.6550e-06, -3.5129e-06,  3.7886e-06,  ..., -4.7907e-06,
         -3.7439e-06, -4.2021e-06],
        [ 5.9569e-06,  9.3641e-05, -5.4897e-05,  ...,  5.4211e-05,
          8.9389e-05,  1.3442e-05],
        [-1.5512e-05, -9.4324e-06,  1.0818e-05,  ..., -1.3113e-05,
         -1.0252e-05, -1.1951e-05],
        [-1.4722e-05, -9.8795e-06,  1.0267e-05,  ..., -1.2532e-05,
         -1.0148e-05, -1.0177e-05]], device='cuda:0')
Loss: 1.0093103647232056


Running epoch 1, step 1204, batch 156
Sampled inputs[:2]: tensor([[    0, 12182,  6294,  ...,  1042,  1070,  2228],
        [    0,  1136,   944,  ...,   401, 13771,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4030e-05,  2.9954e-04, -9.2301e-05,  ..., -2.0603e-05,
          3.5828e-04,  2.2955e-04],
        [-7.0184e-06, -4.3996e-06,  4.7125e-06,  ..., -5.9605e-06,
         -4.6417e-06, -5.2601e-06],
        [ 2.1869e-06,  9.1122e-05, -5.2126e-05,  ...,  5.1052e-05,
          8.6975e-05,  1.0655e-05],
        [-1.9237e-05, -1.1787e-05,  1.3471e-05,  ..., -1.6287e-05,
         -1.2696e-05, -1.4916e-05],
        [-1.8239e-05, -1.2293e-05,  1.2830e-05,  ..., -1.5527e-05,
         -1.2532e-05, -1.2577e-05]], device='cuda:0')
Loss: 0.9730979204177856


Running epoch 1, step 1205, batch 157
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,   278,   269, 10376],
        [    0,   271,  8130,  ...,   609, 28676,   965]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8708e-06,  3.6713e-04, -5.1637e-06,  ..., -6.6521e-05,
          5.0011e-04,  3.4256e-04],
        [-8.4192e-06, -5.3123e-06,  5.7332e-06,  ..., -7.1228e-06,
         -5.4948e-06, -6.3255e-06],
        [-1.9556e-06,  8.8351e-05, -4.8981e-05,  ...,  4.7640e-05,
          8.4472e-05,  7.4963e-06],
        [-2.3142e-05, -1.4275e-05,  1.6406e-05,  ..., -1.9506e-05,
         -1.5050e-05, -1.8001e-05],
        [-2.1890e-05, -1.4856e-05,  1.5572e-05,  ..., -1.8612e-05,
         -1.4916e-05, -1.5214e-05]], device='cuda:0')
Loss: 0.9869071841239929


Running epoch 1, step 1206, batch 158
Sampled inputs[:2]: tensor([[   0, 2923,  266,  ..., 7763,  360, 1255],
        [   0,  292,   58,  ...,  319,  221, 1061]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3578e-05,  4.2105e-04,  8.6734e-05,  ...,  5.8867e-05,
          5.5644e-04,  2.5015e-04],
        [-9.7901e-06, -6.1430e-06,  6.6720e-06,  ..., -8.2701e-06,
         -6.3255e-06, -7.3388e-06],
        [-5.7554e-06,  8.5907e-05, -4.6180e-05,  ...,  4.4466e-05,
          8.2177e-05,  4.7247e-06],
        [-2.7105e-05, -1.6659e-05,  1.9267e-05,  ..., -2.2858e-05,
         -1.7494e-05, -2.1070e-05],
        [-2.5317e-05, -1.7151e-05,  1.8075e-05,  ..., -2.1517e-05,
         -1.7121e-05, -1.7554e-05]], device='cuda:0')
Loss: 0.979027271270752


Running epoch 1, step 1207, batch 159
Sampled inputs[:2]: tensor([[   0, 2738,  278,  ...,  292,   35, 2147],
        [   0, 5699,   20,  ..., 3502, 2051,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2355e-05,  3.4115e-04, -1.2039e-04,  ...,  1.0255e-04,
          4.6612e-04,  2.5139e-04],
        [-1.1183e-05, -7.0892e-06,  7.7747e-06,  ..., -9.4324e-06,
         -7.2457e-06, -8.2441e-06],
        [ 1.2254e-04,  2.2816e-04, -1.3497e-04,  ...,  1.1747e-04,
          2.3706e-04,  5.7324e-05],
        [-3.0950e-05, -1.9237e-05,  2.2411e-05,  ..., -2.6047e-05,
         -2.0027e-05, -2.3693e-05],
        [-2.8729e-05, -1.9684e-05,  2.0877e-05,  ..., -2.4438e-05,
         -1.9565e-05, -1.9670e-05]], device='cuda:0')
Loss: 1.019484281539917
Graident accumulation at epoch 1, step 1207, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0142,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0099, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0285,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.8079e-05,  6.0138e-05, -2.2244e-04,  ...,  4.6064e-05,
          2.8624e-05, -1.3786e-04],
        [-1.1002e-05, -7.2889e-06,  7.1184e-06,  ..., -9.2449e-06,
         -6.6645e-06, -7.4752e-06],
        [ 5.7645e-05,  7.1939e-05, -4.4136e-05,  ...,  5.1243e-05,
          5.7594e-05,  2.0532e-05],
        [-7.6164e-06,  4.0529e-06,  4.9423e-06,  ..., -7.6466e-06,
          3.4438e-06, -1.0877e-05],
        [-2.8899e-05, -2.1361e-05,  2.0206e-05,  ..., -2.4674e-05,
         -1.9356e-05, -1.8649e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2852e-08, 5.3664e-08, 4.0363e-08,  ..., 3.8927e-08, 1.1396e-07,
         4.8618e-08],
        [6.4550e-11, 4.7962e-11, 1.2525e-11,  ..., 4.8572e-11, 1.8817e-11,
         2.0449e-11],
        [3.7294e-09, 2.3634e-09, 1.0443e-09,  ..., 2.8863e-09, 8.6204e-10,
         9.6022e-10],
        [9.3826e-10, 8.6830e-10, 2.3669e-10,  ..., 7.8312e-10, 3.9392e-10,
         3.3468e-10],
        [3.1900e-10, 1.8713e-10, 5.0236e-11,  ..., 2.4090e-10, 5.9338e-11,
         8.3859e-11]], device='cuda:0')
optimizer state dict: 151.0
lr: [8.026520767887557e-06, 8.026520767887557e-06]
scheduler_last_epoch: 151
Epoch 1 | Batch 159/1048 | Training PPL: 3483.7837002993715 | time 11.427246332168579
Saving checkpoint at epoch 1, step 1207, batch 159
Epoch 1 | Validation PPL: 7.046654143662163 | Learning rate: 8.026520767887557e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1207, AFTER epoch 1, step 1207


Running epoch 1, step 1208, batch 160
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   287,   271,  2540],
        [    0,   376,   283,  ..., 29188,   292,  7627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4402e-05,  2.3491e-04,  9.6783e-05,  ...,  2.2005e-04,
         -2.7457e-04, -1.1037e-04],
        [-1.3784e-06, -8.1584e-07,  9.9838e-07,  ..., -1.1250e-06,
         -9.2760e-07, -1.0133e-06],
        [-3.7551e-06, -2.3246e-06,  2.8759e-06,  ..., -3.0249e-06,
         -2.4885e-06, -2.7120e-06],
        [-3.8147e-06, -2.1905e-06,  2.8908e-06,  ..., -3.0994e-06,
         -2.5630e-06, -2.9355e-06],
        [-3.4273e-06, -2.2352e-06,  2.5928e-06,  ..., -2.8163e-06,
         -2.4140e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9849933385848999


Running epoch 1, step 1209, batch 161
Sampled inputs[:2]: tensor([[    0,   634,   631,  ...,  3431,   287, 27947],
        [    0,   494,   221,  ...,   437,   266,  2143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5133e-05,  3.7931e-04,  3.2225e-04,  ...,  2.5573e-04,
         -2.5037e-04, -2.2808e-04],
        [-2.7344e-06, -1.6950e-06,  2.0340e-06,  ..., -2.2873e-06,
         -1.8887e-06, -2.0191e-06],
        [-7.5251e-06, -4.8727e-06,  5.9307e-06,  ..., -6.2436e-06,
         -5.1409e-06, -5.4687e-06],
        [-7.5996e-06, -4.5896e-06,  5.9009e-06,  ..., -6.3330e-06,
         -5.2452e-06, -5.8413e-06],
        [-6.8992e-06, -4.6939e-06,  5.3644e-06,  ..., -5.8264e-06,
         -4.9919e-06, -4.6790e-06]], device='cuda:0')
Loss: 0.9926084876060486


Running epoch 1, step 1210, batch 162
Sampled inputs[:2]: tensor([[    0,  1531,    14,  ...,  6169,    17,     9],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0557e-04,  3.3700e-04,  2.2876e-04,  ...,  3.5027e-04,
         -8.6108e-05, -2.2808e-04],
        [-4.1127e-06, -2.5518e-06,  2.9728e-06,  ..., -3.4496e-06,
         -2.8424e-06, -3.0398e-06],
        [-1.1191e-05, -7.2718e-06,  8.6278e-06,  ..., -9.2834e-06,
         -7.6443e-06, -8.0913e-06],
        [-1.1310e-05, -6.8545e-06,  8.5533e-06,  ..., -9.4622e-06,
         -7.8231e-06, -8.7172e-06],
        [-1.0327e-05, -7.0184e-06,  7.8678e-06,  ..., -8.7172e-06,
         -7.4655e-06, -6.9588e-06]], device='cuda:0')
Loss: 0.9637653827667236


Running epoch 1, step 1211, batch 163
Sampled inputs[:2]: tensor([[    0,   271,   957,  ...,  1597,  1276,   292],
        [    0,    12, 47869,  ...,   259,  5698,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6795e-04,  4.1277e-04,  4.7032e-04,  ...,  2.9330e-04,
         -1.7233e-04, -3.1630e-04],
        [-5.4613e-06, -3.3565e-06,  3.9041e-06,  ..., -4.6492e-06,
         -3.7812e-06, -4.1127e-06],
        [-1.4901e-05, -9.5963e-06,  1.1384e-05,  ..., -1.2517e-05,
         -1.0163e-05, -1.0952e-05],
        [-1.5125e-05, -9.0599e-06,  1.1325e-05,  ..., -1.2830e-05,
         -1.0446e-05, -1.1861e-05],
        [-1.3754e-05, -9.2387e-06,  1.0371e-05,  ..., -1.1757e-05,
         -9.9242e-06, -9.4473e-06]], device='cuda:0')
Loss: 0.9442453980445862


Running epoch 1, step 1212, batch 164
Sampled inputs[:2]: tensor([[   0,   16,   52,  ...,   12,  298,  374],
        [   0,  508,  586,  ..., 6157, 3146, 7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1728e-04,  5.4251e-04,  4.8312e-04,  ...,  3.5236e-04,
         -1.3325e-04, -3.7868e-04],
        [-6.8545e-06, -4.2319e-06,  4.8652e-06,  ..., -5.8264e-06,
         -4.7013e-06, -5.0887e-06],
        [-1.8686e-05, -1.2115e-05,  1.4141e-05,  ..., -1.5706e-05,
         -1.2636e-05, -1.3590e-05],
        [-1.8999e-05, -1.1444e-05,  1.4082e-05,  ..., -1.6093e-05,
         -1.3009e-05, -1.4707e-05],
        [-1.7241e-05, -1.1653e-05,  1.2890e-05,  ..., -1.4737e-05,
         -1.2338e-05, -1.1712e-05]], device='cuda:0')
Loss: 0.9834257364273071


Running epoch 1, step 1213, batch 165
Sampled inputs[:2]: tensor([[    0, 49570,   644,  ...,   461,   800,   266],
        [    0,  6795,  1728,  ...,   578,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5836e-05,  5.6171e-04,  5.8687e-04,  ...,  3.0720e-04,
          1.1385e-05, -2.1477e-04],
        [-8.1882e-06, -5.0552e-06,  5.8711e-06,  ..., -6.9514e-06,
         -5.5693e-06, -6.0648e-06],
        [-2.2352e-05, -1.4454e-05,  1.7032e-05,  ..., -1.8790e-05,
         -1.4991e-05, -1.6212e-05],
        [-2.2873e-05, -1.3754e-05,  1.7092e-05,  ..., -1.9342e-05,
         -1.5512e-05, -1.7643e-05],
        [-2.0653e-05, -1.3933e-05,  1.5542e-05,  ..., -1.7643e-05,
         -1.4678e-05, -1.4022e-05]], device='cuda:0')
Loss: 0.9955974817276001


Running epoch 1, step 1214, batch 166
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,  2381, 12046,  2231],
        [    0,  3253,  1573,  ...,   298,   358,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6987e-05,  5.5237e-04,  6.4040e-04,  ...,  3.5885e-04,
         -2.7187e-05, -4.6572e-05],
        [-9.6560e-06, -5.9642e-06,  6.8471e-06,  ..., -8.1658e-06,
         -6.5193e-06, -7.1377e-06],
        [-2.6524e-05, -1.7121e-05,  1.9953e-05,  ..., -2.2233e-05,
         -1.7688e-05, -1.9267e-05],
        [-2.6986e-05, -1.6227e-05,  1.9923e-05,  ..., -2.2694e-05,
         -1.8135e-05, -2.0728e-05],
        [ 1.7151e-04,  1.0820e-04, -1.3247e-04,  ...,  1.4581e-04,
          9.1180e-05,  1.1064e-04]], device='cuda:0')
Loss: 0.9991398453712463


Running epoch 1, step 1215, batch 167
Sampled inputs[:2]: tensor([[    0, 29258,   765,  ...,  4196,    19,    12],
        [    0, 45050,   342,  ...,  3729,   287, 27888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8532e-05,  5.5875e-04,  6.2983e-04,  ...,  3.2361e-04,
         -2.7187e-05, -4.5179e-05],
        [-1.1027e-05, -6.7763e-06,  7.9349e-06,  ..., -9.3132e-06,
         -7.3612e-06, -8.1360e-06],
        [-3.0175e-05, -1.9386e-05,  2.3007e-05,  ..., -2.5272e-05,
         -1.9908e-05, -2.1920e-05],
        [-3.0771e-05, -1.8433e-05,  2.3067e-05,  ..., -2.5868e-05,
         -2.0459e-05, -2.3633e-05],
        [ 1.6826e-04,  1.0605e-04, -1.2979e-04,  ...,  1.4305e-04,
          8.9049e-05,  1.0844e-04]], device='cuda:0')
Loss: 0.9735905528068542
Graident accumulation at epoch 1, step 1215, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0142,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.2124e-05,  1.1000e-04, -1.3721e-04,  ...,  7.3818e-05,
          2.3043e-05, -1.2859e-04],
        [-1.1004e-05, -7.2377e-06,  7.2000e-06,  ..., -9.2518e-06,
         -6.7342e-06, -7.5413e-06],
        [ 4.8863e-05,  6.2806e-05, -3.7421e-05,  ...,  4.3592e-05,
          4.9843e-05,  1.6287e-05],
        [-9.9318e-06,  1.8044e-06,  6.7548e-06,  ..., -9.4688e-06,
          1.0535e-06, -1.2153e-05],
        [-9.1830e-06, -8.6196e-06,  5.2064e-06,  ..., -7.9013e-06,
         -8.5153e-06, -5.9406e-06]], device='cuda:0')
optimizer state dict: tensor([[7.2786e-08, 5.3922e-08, 4.0719e-08,  ..., 3.8993e-08, 1.1385e-07,
         4.8572e-08],
        [6.4607e-11, 4.7960e-11, 1.2575e-11,  ..., 4.8610e-11, 1.8852e-11,
         2.0495e-11],
        [3.7266e-09, 2.3614e-09, 1.0438e-09,  ..., 2.8841e-09, 8.6158e-10,
         9.5974e-10],
        [9.3826e-10, 8.6777e-10, 2.3698e-10,  ..., 7.8301e-10, 3.9394e-10,
         3.3490e-10],
        [3.4700e-10, 1.9819e-10, 6.7031e-11,  ..., 2.6112e-10, 6.7209e-11,
         9.5535e-11]], device='cuda:0')
optimizer state dict: 152.0
lr: [7.905489126218852e-06, 7.905489126218852e-06]
scheduler_last_epoch: 152


Running epoch 1, step 1216, batch 168
Sampled inputs[:2]: tensor([[   0, 1806,  319,  ..., 3427,  278,  266],
        [   0, 1854,  292,  ...,  328, 1360,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7306e-05, -4.0463e-05, -2.1098e-04,  ..., -1.2140e-05,
         -7.4751e-05, -1.3464e-04],
        [-1.4380e-06, -8.3819e-07,  9.0897e-07,  ..., -1.2070e-06,
         -9.1642e-07, -1.1474e-06],
        [-3.9637e-06, -2.4140e-06,  2.7120e-06,  ..., -3.2783e-06,
         -2.4885e-06, -3.0696e-06],
        [-3.8147e-06, -2.1458e-06,  2.5183e-06,  ..., -3.1888e-06,
         -2.4140e-06, -3.1292e-06],
        [-3.6657e-06, -2.2799e-06,  2.4736e-06,  ..., -3.0696e-06,
         -2.4289e-06, -2.6524e-06]], device='cuda:0')
Loss: 0.9670789241790771


Running epoch 1, step 1217, batch 169
Sampled inputs[:2]: tensor([[   0, 9116,  278,  ..., 6997, 3244, 1192],
        [   0,   14,  560,  ..., 1248, 1398, 1268]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8778e-04,  7.6998e-05, -3.6923e-04,  ...,  4.1913e-05,
         -2.3023e-04, -5.0973e-04],
        [-2.7940e-06, -1.7099e-06,  1.8999e-06,  ..., -2.3246e-06,
         -1.7658e-06, -2.1309e-06],
        [-7.7486e-06, -4.9770e-06,  5.6624e-06,  ..., -6.3777e-06,
         -4.8727e-06, -5.7966e-06],
        [-7.6890e-06, -4.5747e-06,  5.4985e-06,  ..., -6.3479e-06,
         -4.8280e-06, -6.0648e-06],
        [-7.0781e-06, -4.6790e-06,  5.0962e-06,  ..., -5.9158e-06,
         -4.7386e-06, -4.9472e-06]], device='cuda:0')
Loss: 0.9475597739219666


Running epoch 1, step 1218, batch 170
Sampled inputs[:2]: tensor([[   0,   17,   14,  ...,  650, 1711,  897],
        [   0,  275, 1911,  ..., 1371, 5151, 2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1835e-04,  2.0974e-04, -1.5286e-04,  ...,  4.1465e-05,
         -1.4687e-04, -5.1055e-04],
        [-4.2394e-06, -2.5965e-06,  2.8834e-06,  ..., -3.5241e-06,
         -2.6524e-06, -3.1143e-06],
        [-1.1563e-05, -7.4953e-06,  8.4639e-06,  ..., -9.5665e-06,
         -7.2718e-06, -8.4043e-06],
        [-1.1653e-05, -6.9886e-06,  8.3148e-06,  ..., -9.6560e-06,
         -7.2867e-06, -8.8811e-06],
        [-1.0490e-05, -7.0333e-06,  7.5549e-06,  ..., -8.8364e-06,
         -7.0333e-06, -7.1377e-06]], device='cuda:0')
Loss: 0.9868384003639221


Running epoch 1, step 1219, batch 171
Sampled inputs[:2]: tensor([[    0,   380,   333,  ...,   333,   199,  2038],
        [    0,  8023,  1309,  ...,  3370,   266, 14988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5588e-04,  2.7672e-04, -3.4114e-05,  ..., -2.6207e-05,
         -2.2479e-04, -5.1069e-04],
        [-5.5730e-06, -3.5018e-06,  3.8743e-06,  ..., -4.6417e-06,
         -3.5539e-06, -4.0233e-06],
        [-1.5289e-05, -1.0118e-05,  1.1355e-05,  ..., -1.2696e-05,
         -9.7752e-06, -1.0967e-05],
        [-1.5497e-05, -9.5367e-06,  1.1250e-05,  ..., -1.2875e-05,
         -9.8795e-06, -1.1638e-05],
        [-1.3828e-05, -9.4920e-06,  1.0118e-05,  ..., -1.1712e-05,
         -9.4473e-06, -9.3132e-06]], device='cuda:0')
Loss: 0.9903983473777771


Running epoch 1, step 1220, batch 172
Sampled inputs[:2]: tensor([[    0,   638,  1276,  ...,  1589,  2432,   292],
        [    0, 10893, 10997,  ...,   367,   616,  7903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3416e-04,  3.3121e-04, -2.1494e-04,  ...,  6.3020e-05,
         -3.2842e-04, -8.1057e-04],
        [-6.8471e-06, -4.2953e-06,  4.8727e-06,  ..., -5.7295e-06,
         -4.3660e-06, -4.9397e-06],
        [-1.8805e-05, -1.2338e-05,  1.4231e-05,  ..., -1.5616e-05,
         -1.1906e-05, -1.3396e-05],
        [-1.9163e-05, -1.1668e-05,  1.4231e-05,  ..., -1.5944e-05,
         -1.2115e-05, -1.4350e-05],
        [-1.7017e-05, -1.1623e-05,  1.2696e-05,  ..., -1.4409e-05,
         -1.1519e-05, -1.1355e-05]], device='cuda:0')
Loss: 0.9340940713882446


Running epoch 1, step 1221, batch 173
Sampled inputs[:2]: tensor([[   0, 1737,  278,  ..., 2604,  367, 2002],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2845e-04,  3.8855e-04, -1.0702e-04,  ...,  2.9368e-06,
         -2.1376e-04, -7.5714e-04],
        [-8.2776e-06, -5.2229e-06,  5.8860e-06,  ..., -6.8992e-06,
         -5.2527e-06, -5.9158e-06],
        [-2.2858e-05, -1.5050e-05,  1.7241e-05,  ..., -1.8910e-05,
         -1.4395e-05, -1.6168e-05],
        [-2.3186e-05, -1.4201e-05,  1.7196e-05,  ..., -1.9208e-05,
         -1.4573e-05, -1.7226e-05],
        [-2.0564e-05, -1.4126e-05,  1.5289e-05,  ..., -1.7360e-05,
         -1.3858e-05, -1.3649e-05]], device='cuda:0')
Loss: 0.9980295300483704


Running epoch 1, step 1222, batch 174
Sampled inputs[:2]: tensor([[    0, 10705,   401,  ...,   768,  2392,   368],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5193e-04,  2.5111e-04, -3.2192e-04,  ..., -1.0551e-04,
         -1.3860e-04, -7.2026e-04],
        [-9.7305e-06, -6.1207e-06,  6.8620e-06,  ..., -8.1137e-06,
         -6.2138e-06, -6.9514e-06],
        [-2.6673e-05, -1.7554e-05,  1.9953e-05,  ..., -2.2098e-05,
         -1.6943e-05, -1.8880e-05],
        [-2.7061e-05, -1.6600e-05,  1.9938e-05,  ..., -2.2456e-05,
         -1.7166e-05, -2.0117e-05],
        [-2.4110e-05, -1.6585e-05,  1.7792e-05,  ..., -2.0385e-05,
         -1.6391e-05, -1.6034e-05]], device='cuda:0')
Loss: 0.9704983234405518


Running epoch 1, step 1223, batch 175
Sampled inputs[:2]: tensor([[   0,  266, 8802,  ..., 8401,    9,  287],
        [   0,  266,  452,  ..., 1725, 2200,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3444e-04,  1.9697e-04, -3.9207e-04,  ..., -2.4259e-04,
         -1.3936e-04, -9.1080e-04],
        [-1.1131e-05, -7.0333e-06,  7.8604e-06,  ..., -9.2983e-06,
         -7.1228e-06, -7.9125e-06],
        [-3.0398e-05, -2.0102e-05,  2.2784e-05,  ..., -2.5243e-05,
         -1.9372e-05, -2.1383e-05],
        [-3.0905e-05, -1.9044e-05,  2.2814e-05,  ..., -2.5719e-05,
         -1.9684e-05, -2.2888e-05],
        [-2.7493e-05, -1.9029e-05,  2.0355e-05,  ..., -2.3291e-05,
         -1.8761e-05, -1.8150e-05]], device='cuda:0')
Loss: 0.9912010431289673
Graident accumulation at epoch 1, step 1223, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0029, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5329e-06,  1.1870e-04, -1.6270e-04,  ...,  4.2178e-05,
          6.8025e-06, -2.0681e-04],
        [-1.1017e-05, -7.2172e-06,  7.2661e-06,  ..., -9.2564e-06,
         -6.7730e-06, -7.5784e-06],
        [ 4.0937e-05,  5.4515e-05, -3.1401e-05,  ...,  3.6708e-05,
          4.2922e-05,  1.2520e-05],
        [-1.2029e-05, -2.8045e-07,  8.3607e-06,  ..., -1.1094e-05,
         -1.0203e-06, -1.3227e-05],
        [-1.1014e-05, -9.6605e-06,  6.7213e-06,  ..., -9.4402e-06,
         -9.5398e-06, -7.1615e-06]], device='cuda:0')
optimizer state dict: tensor([[7.2902e-08, 5.3907e-08, 4.0832e-08,  ..., 3.9012e-08, 1.1375e-07,
         4.9353e-08],
        [6.4666e-11, 4.7962e-11, 1.2624e-11,  ..., 4.8648e-11, 1.8884e-11,
         2.0537e-11],
        [3.7238e-09, 2.3595e-09, 1.0433e-09,  ..., 2.8818e-09, 8.6109e-10,
         9.5924e-10],
        [9.3828e-10, 8.6726e-10, 2.3727e-10,  ..., 7.8288e-10, 3.9393e-10,
         3.3509e-10],
        [3.4740e-10, 1.9835e-10, 6.7378e-11,  ..., 2.6140e-10, 6.7493e-11,
         9.5768e-11]], device='cuda:0')
optimizer state dict: 153.0
lr: [7.784777544094901e-06, 7.784777544094901e-06]
scheduler_last_epoch: 153


Running epoch 1, step 1224, batch 176
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ..., 5996,   71,   19],
        [   0,   12,  287,  ...,  658,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1706e-04, -7.2156e-06, -1.9815e-04,  ..., -1.1013e-05,
         -6.8134e-06,  1.6561e-05],
        [-1.3933e-06, -9.2387e-07,  1.0580e-06,  ..., -1.1027e-06,
         -8.6799e-07, -9.2015e-07],
        [-3.8147e-06, -2.6524e-06,  3.0547e-06,  ..., -3.0398e-06,
         -2.3991e-06, -2.5034e-06],
        [-3.9935e-06, -2.6077e-06,  3.1739e-06,  ..., -3.1739e-06,
         -2.4885e-06, -2.7567e-06],
        [-3.3379e-06, -2.4438e-06,  2.6375e-06,  ..., -2.7120e-06,
         -2.2650e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9930114150047302


Running epoch 1, step 1225, batch 177
Sampled inputs[:2]: tensor([[    0, 11752,   280,  ..., 14814,  1128,   360],
        [    0,   199,   677,  ...,  2792,   271,  2386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6203e-04,  2.2723e-06, -3.7399e-04,  ..., -5.0968e-05,
         -1.5784e-04, -2.3845e-04],
        [-2.7642e-06, -1.6652e-06,  1.9744e-06,  ..., -2.2128e-06,
         -1.7323e-06, -1.9856e-06],
        [-7.4506e-06, -4.7088e-06,  5.7071e-06,  ..., -5.9158e-06,
         -4.6492e-06, -5.1856e-06],
        [-7.7486e-06, -4.5598e-06,  5.8264e-06,  ..., -6.1840e-06,
         -4.8429e-06, -5.7369e-06],
        [-6.6608e-06, -4.4405e-06,  5.0515e-06,  ..., -5.3942e-06,
         -4.4703e-06, -4.3213e-06]], device='cuda:0')
Loss: 0.9303632378578186


Running epoch 1, step 1226, batch 178
Sampled inputs[:2]: tensor([[   0, 1615,  287,  ...,  259,  623,   12],
        [   0, 1067,  408,  ..., 4657, 1016,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5380e-04, -2.3515e-05, -2.2581e-04,  ..., -1.6921e-05,
         -1.6091e-04, -1.9608e-04],
        [-4.1276e-06, -2.5257e-06,  2.9169e-06,  ..., -3.3528e-06,
         -2.6859e-06, -2.9914e-06],
        [-1.1146e-05, -7.1973e-06,  8.4639e-06,  ..., -8.9854e-06,
         -7.2420e-06, -7.8678e-06],
        [-1.1653e-05, -6.9886e-06,  8.6576e-06,  ..., -9.4473e-06,
         -7.6145e-06, -8.7470e-06],
        [-9.9391e-06, -6.7353e-06,  7.4506e-06,  ..., -8.1658e-06,
         -6.9141e-06, -6.5267e-06]], device='cuda:0')
Loss: 0.9581074714660645


Running epoch 1, step 1227, batch 179
Sampled inputs[:2]: tensor([[    0,   221,   264,  ...,  3613,  3222, 14000],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8394e-05,  2.3140e-05, -2.5600e-04,  ...,  4.4806e-05,
         -1.6232e-04,  8.4354e-05],
        [-5.5730e-06, -3.2447e-06,  3.8855e-06,  ..., -4.5225e-06,
         -3.4980e-06, -4.0941e-06],
        [-1.5140e-05, -9.3132e-06,  1.1295e-05,  ..., -1.2204e-05,
         -9.4473e-06, -1.0848e-05],
        [-1.5825e-05, -8.9854e-06,  1.1548e-05,  ..., -1.2815e-05,
         -9.9093e-06, -1.2040e-05],
        [-1.3426e-05, -8.6576e-06,  9.8795e-06,  ..., -1.1027e-05,
         -8.9854e-06, -8.9854e-06]], device='cuda:0')
Loss: 0.9462503790855408


Running epoch 1, step 1228, batch 180
Sampled inputs[:2]: tensor([[    0,   591,  2036,  ...,   266,  1027,   278],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7577e-05, -3.8614e-06, -2.8286e-04,  ...,  1.6731e-05,
         -1.9877e-04,  1.2571e-04],
        [-6.9961e-06, -4.1984e-06,  4.9211e-06,  ..., -5.6773e-06,
         -4.3996e-06, -5.0478e-06],
        [-1.9073e-05, -1.2085e-05,  1.4305e-05,  ..., -1.5408e-05,
         -1.1966e-05, -1.3486e-05],
        [-1.9848e-05, -1.1653e-05,  1.4588e-05,  ..., -1.6078e-05,
         -1.2457e-05, -1.4886e-05],
        [-1.6868e-05, -1.1206e-05,  1.2487e-05,  ..., -1.3888e-05,
         -1.1355e-05, -1.1161e-05]], device='cuda:0')
Loss: 0.9998088479042053


Running epoch 1, step 1229, batch 181
Sampled inputs[:2]: tensor([[    0,   266,  6079,  ...,   437,   266, 44526],
        [    0,  1234,   278,  ...,  8635,   271,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2903e-04, -1.5467e-04, -5.0519e-04,  ..., -7.8412e-05,
         -1.5608e-04,  1.7690e-04],
        [-8.4192e-06, -5.1409e-06,  5.9418e-06,  ..., -6.8545e-06,
         -5.2713e-06, -6.0312e-06],
        [-2.3037e-05, -1.4767e-05,  1.7270e-05,  ..., -1.8641e-05,
         -1.4350e-05, -1.6198e-05],
        [-2.3812e-05, -1.4201e-05,  1.7509e-05,  ..., -1.9312e-05,
         -1.4827e-05, -1.7703e-05],
        [-2.0385e-05, -1.3694e-05,  1.5095e-05,  ..., -1.6838e-05,
         -1.3649e-05, -1.3426e-05]], device='cuda:0')
Loss: 1.0023796558380127


Running epoch 1, step 1230, batch 182
Sampled inputs[:2]: tensor([[    0,   721,  1119,  ...,   600,   328,  3363],
        [    0,   491, 10524,  ...,  2218,  5627,  4199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0155e-04, -6.7023e-05, -6.5514e-04,  ..., -7.1321e-06,
         -2.1078e-04,  1.1890e-04],
        [-9.8795e-06, -6.1318e-06,  7.0222e-06,  ..., -8.0168e-06,
         -6.2473e-06, -6.9849e-06],
        [-2.7061e-05, -1.7598e-05,  2.0370e-05,  ..., -2.1875e-05,
         -1.7062e-05, -1.8850e-05],
        [-2.7895e-05, -1.6943e-05,  2.0638e-05,  ..., -2.2575e-05,
         -1.7568e-05, -2.0519e-05],
        [-2.4006e-05, -1.6376e-05,  1.7866e-05,  ..., -1.9819e-05,
         -1.6257e-05, -1.5691e-05]], device='cuda:0')
Loss: 1.014823079109192


Running epoch 1, step 1231, batch 183
Sampled inputs[:2]: tensor([[    0,  3825,  1626,  ...,  5096,  3775,   266],
        [    0, 35449,   824,  ...,   278, 30449,  3659]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9104e-04,  1.8710e-05, -6.7054e-04,  ...,  2.7628e-06,
         -3.1082e-04,  4.3245e-05],
        [-1.1273e-05, -7.0222e-06,  8.0951e-06,  ..., -9.0897e-06,
         -7.0706e-06, -7.9237e-06],
        [-3.1084e-05, -2.0266e-05,  2.3618e-05,  ..., -2.4959e-05,
         -1.9401e-05, -2.1577e-05],
        [-3.1888e-05, -1.9431e-05,  2.3827e-05,  ..., -2.5615e-05,
         -1.9878e-05, -2.3350e-05],
        [-2.7344e-05, -1.8716e-05,  2.0534e-05,  ..., -2.2456e-05,
         -1.8373e-05, -1.7792e-05]], device='cuda:0')
Loss: 0.9744068384170532
Graident accumulation at epoch 1, step 1231, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4083e-05,  1.0870e-04, -2.1348e-04,  ...,  3.8236e-05,
         -2.4960e-05, -1.8181e-04],
        [-1.1043e-05, -7.1977e-06,  7.3490e-06,  ..., -9.2397e-06,
         -6.8028e-06, -7.6129e-06],
        [ 3.3735e-05,  4.7037e-05, -2.5899e-05,  ...,  3.0541e-05,
          3.6690e-05,  9.1100e-06],
        [-1.4015e-05, -2.1955e-06,  9.9073e-06,  ..., -1.2546e-05,
         -2.9061e-06, -1.4239e-05],
        [-1.2647e-05, -1.0566e-05,  8.1025e-06,  ..., -1.0742e-05,
         -1.0423e-05, -8.2245e-06]], device='cuda:0')
optimizer state dict: tensor([[7.2865e-08, 5.3854e-08, 4.1241e-08,  ..., 3.8973e-08, 1.1374e-07,
         4.9305e-08],
        [6.4728e-11, 4.7963e-11, 1.2677e-11,  ..., 4.8682e-11, 1.8915e-11,
         2.0579e-11],
        [3.7210e-09, 2.3575e-09, 1.0428e-09,  ..., 2.8796e-09, 8.6060e-10,
         9.5874e-10],
        [9.3836e-10, 8.6677e-10, 2.3760e-10,  ..., 7.8276e-10, 3.9394e-10,
         3.3530e-10],
        [3.4780e-10, 1.9850e-10, 6.7732e-11,  ..., 2.6164e-10, 6.7763e-11,
         9.5989e-11]], device='cuda:0')
optimizer state dict: 154.0
lr: [7.664404467299166e-06, 7.664404467299166e-06]
scheduler_last_epoch: 154


Running epoch 1, step 1232, batch 184
Sampled inputs[:2]: tensor([[   0,   43,  527,  ..., 4309,   14, 8050],
        [   0, 1167,  278,  ...,  278, 1853, 1424]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3359e-04, -4.0553e-04, -4.5469e-05,  ..., -1.6999e-04,
          3.1292e-04,  8.8650e-05],
        [-1.4156e-06, -9.5367e-07,  9.4622e-07,  ..., -1.1548e-06,
         -9.0152e-07, -9.4250e-07],
        [-4.0829e-06, -2.8908e-06,  2.9206e-06,  ..., -3.3379e-06,
         -2.5928e-06, -2.7567e-06],
        [-4.1127e-06, -2.7418e-06,  2.8908e-06,  ..., -3.3528e-06,
         -2.5928e-06, -2.8759e-06],
        [-3.4720e-06, -2.5630e-06,  2.4438e-06,  ..., -2.8908e-06,
         -2.3693e-06, -2.2054e-06]], device='cuda:0')
Loss: 1.004594326019287


Running epoch 1, step 1233, batch 185
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   417,   199, 30714],
        [    0,  1477,   591,  ...,  4111, 18012, 11991]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2872e-04, -5.0451e-04, -2.9543e-05,  ..., -1.4856e-04,
          1.0116e-04,  1.2118e-05],
        [-2.8238e-06, -1.8552e-06,  1.9893e-06,  ..., -2.2873e-06,
         -1.7844e-06, -1.8887e-06],
        [-7.9870e-06, -5.4985e-06,  5.9605e-06,  ..., -6.4671e-06,
         -5.0366e-06, -5.3793e-06],
        [-8.1956e-06, -5.2899e-06,  6.0201e-06,  ..., -6.6012e-06,
         -5.1111e-06, -5.7220e-06],
        [-6.8843e-06, -4.9472e-06,  5.0664e-06,  ..., -5.6773e-06,
         -4.6641e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.988555371761322


Running epoch 1, step 1234, batch 186
Sampled inputs[:2]: tensor([[    0,  2086, 10663,  ...,   271,   266,  6927],
        [    0,  1943,   300,  ..., 43803,   368,  2400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8514e-05, -5.0729e-04,  3.4379e-05,  ..., -2.1576e-04,
          1.0984e-05,  2.2047e-04],
        [-4.2170e-06, -2.7306e-06,  3.0994e-06,  ..., -3.3379e-06,
         -2.5183e-06, -2.8238e-06],
        [ 5.0261e-05,  7.5642e-05, -4.7207e-05,  ...,  4.1299e-05,
          9.2297e-05,  1.5656e-05],
        [-1.2279e-05, -7.7784e-06,  9.4026e-06,  ..., -9.6560e-06,
         -7.2271e-06, -8.5682e-06],
        [-1.0088e-05, -7.1675e-06,  7.6890e-06,  ..., -8.1658e-06,
         -6.5342e-06, -6.3628e-06]], device='cuda:0')
Loss: 0.9701798558235168


Running epoch 1, step 1235, batch 187
Sampled inputs[:2]: tensor([[   0, 2785, 1061,  ..., 1194,  692, 4339],
        [   0, 6668,  565,  ...,  360,  259, 8166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8514e-05, -4.9996e-04, -4.3884e-05,  ..., -2.4366e-04,
         -1.0439e-04,  8.4107e-06],
        [-5.5954e-06, -3.5278e-06,  3.9935e-06,  ..., -4.4703e-06,
         -3.3341e-06, -3.8967e-06],
        [ 4.6297e-05,  7.3242e-05, -4.4450e-05,  ...,  3.8096e-05,
          9.0002e-05,  1.2661e-05],
        [-1.6242e-05, -9.9987e-06,  1.2085e-05,  ..., -1.2875e-05,
         -9.5218e-06, -1.1742e-05],
        [-1.3545e-05, -9.3430e-06,  1.0073e-05,  ..., -1.0997e-05,
         -8.6799e-06, -8.8066e-06]], device='cuda:0')
Loss: 0.9732846021652222


Running epoch 1, step 1236, batch 188
Sampled inputs[:2]: tensor([[    0,    14,   221,  ...,   298,   408,  1849],
        [    0,    13, 37178,  ...,  1692,  3287, 10652]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0830e-05, -3.6949e-04,  2.4075e-04,  ..., -1.7658e-04,
          2.7006e-04,  2.0988e-04],
        [-7.1004e-06, -4.4666e-06,  4.7907e-06,  ..., -5.7891e-06,
         -4.4443e-06, -5.0142e-06],
        [ 4.2214e-05,  7.0560e-05, -4.2140e-05,  ...,  3.4564e-05,
          8.7022e-05,  9.6810e-06],
        [-2.0415e-05, -1.2547e-05,  1.4335e-05,  ..., -1.6540e-05,
         -1.2606e-05, -1.4916e-05],
        [-1.7449e-05, -1.1995e-05,  1.2293e-05,  ..., -1.4439e-05,
         -1.1660e-05, -1.1519e-05]], device='cuda:0')
Loss: 1.0258684158325195


Running epoch 1, step 1237, batch 189
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   342,   726,  ...,    12,   895,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1634e-05, -6.5856e-04,  7.8707e-05,  ..., -3.4357e-05,
          3.4768e-04,  3.2374e-04],
        [-8.4788e-06, -5.2564e-06,  5.6513e-06,  ..., -7.0184e-06,
         -5.3607e-06, -6.1840e-06],
        [ 3.8489e-05,  6.8355e-05, -3.9607e-05,  ...,  3.1345e-05,
          8.4623e-05,  6.6858e-06],
        [-2.4125e-05, -1.4573e-05,  1.6779e-05,  ..., -1.9819e-05,
         -1.5035e-05, -1.8105e-05],
        [-2.0966e-05, -1.4156e-05,  1.4648e-05,  ..., -1.7479e-05,
         -1.4015e-05, -1.4126e-05]], device='cuda:0')
Loss: 0.9506607055664062


Running epoch 1, step 1238, batch 190
Sampled inputs[:2]: tensor([[    0,  4014,    88,  ...,    14, 11961,    13],
        [    0,  2950,    13,  ..., 16513,   300,  2205]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4977e-06, -6.4702e-04,  2.9219e-04,  ..., -2.3662e-04,
          4.8894e-04,  3.8232e-04],
        [-9.9167e-06, -6.2995e-06,  6.7018e-06,  ..., -8.1733e-06,
         -6.2659e-06, -7.1004e-06],
        [ 3.4406e-05,  6.5315e-05, -3.6523e-05,  ...,  2.8067e-05,
          8.2060e-05,  4.0483e-06],
        [-2.8268e-05, -1.7509e-05,  1.9878e-05,  ..., -2.3112e-05,
         -1.7598e-05, -2.0862e-05],
        [-2.4498e-05, -1.6913e-05,  1.7285e-05,  ..., -2.0385e-05,
         -1.6399e-05, -1.6272e-05]], device='cuda:0')
Loss: 0.9872542023658752


Running epoch 1, step 1239, batch 191
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0856e-05, -6.8350e-04,  2.9219e-04,  ..., -2.0251e-04,
          4.8476e-04,  5.5273e-04],
        [-1.1362e-05, -7.1898e-06,  7.5325e-06,  ..., -9.3579e-06,
         -7.2159e-06, -8.2105e-06],
        [ 3.0472e-05,  6.2737e-05, -3.4064e-05,  ...,  2.4878e-05,
          7.9497e-05,  1.0681e-06],
        [-3.2261e-05, -1.9923e-05,  2.2262e-05,  ..., -2.6360e-05,
         -2.0221e-05, -2.4021e-05],
        [-2.8238e-05, -1.9431e-05,  1.9580e-05,  ..., -2.3469e-05,
         -1.8977e-05, -1.8939e-05]], device='cuda:0')
Loss: 0.9653918147087097
Graident accumulation at epoch 1, step 1239, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5589e-05,  2.9476e-05, -1.6291e-04,  ...,  1.4162e-05,
          2.6012e-05, -1.0835e-04],
        [-1.1075e-05, -7.1969e-06,  7.3673e-06,  ..., -9.2516e-06,
         -6.8441e-06, -7.6727e-06],
        [ 3.3408e-05,  4.8607e-05, -2.6715e-05,  ...,  2.9975e-05,
          4.0970e-05,  8.3058e-06],
        [-1.5840e-05, -3.9683e-06,  1.1143e-05,  ..., -1.3927e-05,
         -4.6376e-06, -1.5217e-05],
        [-1.4206e-05, -1.1453e-05,  9.2503e-06,  ..., -1.2015e-05,
         -1.1278e-05, -9.2960e-06]], device='cuda:0')
optimizer state dict: tensor([[7.2796e-08, 5.4267e-08, 4.1285e-08,  ..., 3.8975e-08, 1.1386e-07,
         4.9561e-08],
        [6.4793e-11, 4.7967e-11, 1.2721e-11,  ..., 4.8721e-11, 1.8949e-11,
         2.0626e-11],
        [3.7182e-09, 2.3591e-09, 1.0429e-09,  ..., 2.8773e-09, 8.6606e-10,
         9.5779e-10],
        [9.3846e-10, 8.6630e-10, 2.3786e-10,  ..., 7.8267e-10, 3.9395e-10,
         3.3554e-10],
        [3.4825e-10, 1.9868e-10, 6.8048e-11,  ..., 2.6193e-10, 6.8056e-11,
         9.6252e-11]], device='cuda:0')
optimizer state dict: 155.0
lr: [7.544388289888527e-06, 7.544388289888527e-06]
scheduler_last_epoch: 155


Running epoch 1, step 1240, batch 192
Sampled inputs[:2]: tensor([[    0, 12165,    12,  ...,  2860, 10718,   278],
        [    0,  5885,   271,  ...,   278,  1049,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7957e-05, -1.0535e-04,  6.6762e-05,  ..., -2.8064e-05,
          1.4174e-04, -7.4122e-05],
        [-1.3933e-06, -9.9093e-07,  9.9838e-07,  ..., -1.1250e-06,
         -8.8662e-07, -9.2015e-07],
        [-3.9339e-06, -2.8908e-06,  2.9653e-06,  ..., -3.1888e-06,
         -2.5332e-06, -2.5928e-06],
        [-4.0233e-06, -2.8312e-06,  2.9951e-06,  ..., -3.2485e-06,
         -2.5630e-06, -2.7716e-06],
        [-3.3528e-06, -2.5779e-06,  2.5034e-06,  ..., -2.7865e-06,
         -2.3395e-06, -2.0713e-06]], device='cuda:0')
Loss: 1.0278651714324951


Running epoch 1, step 1241, batch 193
Sampled inputs[:2]: tensor([[    0,  1550,  2013,  ...,  9970,   638,  6482],
        [    0,  3169, 12186,  ...,   940,   271, 13929]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3122e-05, -1.8471e-04, -3.8860e-05,  ..., -4.5268e-05,
          1.2061e-04, -2.2210e-05],
        [-2.7418e-06, -1.8924e-06,  1.9968e-06,  ..., -2.1905e-06,
         -1.7062e-06, -1.8328e-06],
        [-7.7486e-06, -5.5432e-06,  5.9456e-06,  ..., -6.1989e-06,
         -4.8131e-06, -5.1558e-06],
        [-7.9274e-06, -5.3495e-06,  6.0052e-06,  ..., -6.3181e-06,
         -4.8876e-06, -5.5581e-06],
        [-6.5863e-06, -4.9323e-06,  4.9770e-06,  ..., -5.3793e-06,
         -4.4256e-06, -4.0829e-06]], device='cuda:0')
Loss: 0.9759588837623596


Running epoch 1, step 1242, batch 194
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,  1412,    35,  ...,  6077,   298,  1826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6813e-04, -1.0900e-04,  4.2814e-05,  ..., -1.5584e-04,
          1.5354e-04,  6.8090e-05],
        [-4.2096e-06, -2.7567e-06,  2.7567e-06,  ..., -3.4422e-06,
         -2.7418e-06, -2.9951e-06],
        [-1.1742e-05, -8.0168e-06,  8.2552e-06,  ..., -9.5218e-06,
         -7.5698e-06, -8.1658e-06],
        [-1.1981e-05, -7.6443e-06,  8.1956e-06,  ..., -9.7454e-06,
         -7.7486e-06, -8.8066e-06],
        [-1.0356e-05, -7.3016e-06,  7.1377e-06,  ..., -8.5384e-06,
         -7.1228e-06, -6.7353e-06]], device='cuda:0')
Loss: 0.9419565796852112


Running epoch 1, step 1243, batch 195
Sampled inputs[:2]: tensor([[    0,   278,  6481,  ...,    13,  8970,    12],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0514e-05, -1.5675e-04, -6.9660e-05,  ..., -2.6798e-04,
          2.9270e-04,  1.9026e-04],
        [-5.5879e-06, -3.6433e-06,  3.7253e-06,  ..., -4.4927e-06,
         -3.5428e-06, -3.9153e-06],
        [-1.5616e-05, -1.0610e-05,  1.1116e-05,  ..., -1.2457e-05,
         -9.8050e-06, -1.0714e-05],
        [-1.5885e-05, -1.0088e-05,  1.1042e-05,  ..., -1.2711e-05,
         -9.9838e-06, -1.1519e-05],
        [-1.3649e-05, -9.6411e-06,  9.5516e-06,  ..., -1.1101e-05,
         -9.2089e-06, -8.7619e-06]], device='cuda:0')
Loss: 0.9767457842826843


Running epoch 1, step 1244, batch 196
Sampled inputs[:2]: tensor([[    0,   344,  2574,  ...,  2558,  2663,   328],
        [    0,   287,   266,  ...,   998,   342, 17709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1201e-04, -1.7240e-04, -1.5598e-04,  ..., -3.3456e-04,
          4.8300e-04,  2.4813e-04],
        [-6.9663e-06, -4.6343e-06,  4.6715e-06,  ..., -5.6326e-06,
         -4.4480e-06, -4.8466e-06],
        [-1.9491e-05, -1.3515e-05,  1.3947e-05,  ..., -1.5631e-05,
         -1.2308e-05, -1.3337e-05],
        [-1.9759e-05, -1.2830e-05,  1.3813e-05,  ..., -1.5900e-05,
         -1.2502e-05, -1.4275e-05],
        [-1.7077e-05, -1.2323e-05,  1.2010e-05,  ..., -1.3962e-05,
         -1.1578e-05, -1.0908e-05]], device='cuda:0')
Loss: 1.0028339624404907


Running epoch 1, step 1245, batch 197
Sampled inputs[:2]: tensor([[    0,    13, 26335,  ...,     5,  2570, 34403],
        [    0,     9,   287,  ...,   369,  2968,  8347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5977e-04, -7.5354e-05, -1.2819e-05,  ..., -3.9908e-04,
          6.0412e-04,  4.8837e-04],
        [-8.4937e-06, -5.6103e-06,  5.5172e-06,  ..., -6.8247e-06,
         -5.4538e-06, -5.9567e-06],
        [-2.3812e-05, -1.6466e-05,  1.6525e-05,  ..., -1.9044e-05,
         -1.5199e-05, -1.6466e-05],
        [-2.3872e-05, -1.5438e-05,  1.6168e-05,  ..., -1.9148e-05,
         -1.5259e-05, -1.7360e-05],
        [-2.1040e-05, -1.5080e-05,  1.4320e-05,  ..., -1.7136e-05,
         -1.4335e-05, -1.3635e-05]], device='cuda:0')
Loss: 0.9823357462882996


Running epoch 1, step 1246, batch 198
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  381, 1659,  ..., 1403,  271, 6324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1894e-04,  1.3009e-04,  1.0327e-04,  ..., -3.3681e-04,
          3.9171e-04,  4.3683e-04],
        [-9.9093e-06, -6.4820e-06,  6.3926e-06,  ..., -7.9796e-06,
         -6.3367e-06, -7.0594e-06],
        [-2.7657e-05, -1.8954e-05,  1.9103e-05,  ..., -2.2128e-05,
         -1.7583e-05, -1.9372e-05],
        [-2.7925e-05, -1.7881e-05,  1.8805e-05,  ..., -2.2471e-05,
         -1.7792e-05, -2.0608e-05],
        [-2.4587e-05, -1.7449e-05,  1.6674e-05,  ..., -2.0027e-05,
         -1.6674e-05, -1.6153e-05]], device='cuda:0')
Loss: 0.9440093636512756


Running epoch 1, step 1247, batch 199
Sampled inputs[:2]: tensor([[   0, 2336,   26,  ..., 2564,  271, 1422],
        [   0,   12, 5820,  ...,    5, 2122,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7064e-04,  3.7002e-04,  9.9109e-05,  ..., -3.4082e-04,
          3.6588e-04,  5.5132e-04],
        [-1.1452e-05, -7.3910e-06,  7.3016e-06,  ..., -9.2089e-06,
         -7.3053e-06, -8.2143e-06],
        [-3.2008e-05, -2.1696e-05,  2.1830e-05,  ..., -2.5600e-05,
         -2.0340e-05, -2.2605e-05],
        [-3.2157e-05, -2.0355e-05,  2.1383e-05,  ..., -2.5839e-05,
         -2.0459e-05, -2.3872e-05],
        [-2.8491e-05, -2.0012e-05,  1.9088e-05,  ..., -2.3216e-05,
         -1.9327e-05, -1.8895e-05]], device='cuda:0')
Loss: 0.966619074344635
Graident accumulation at epoch 1, step 1247, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0021,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0344],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.1095e-05,  6.3531e-05, -1.3671e-04,  ..., -2.1336e-05,
          5.9999e-05, -4.2386e-05],
        [-1.1112e-05, -7.2163e-06,  7.3607e-06,  ..., -9.2473e-06,
         -6.8902e-06, -7.7268e-06],
        [ 2.6867e-05,  4.1577e-05, -2.1861e-05,  ...,  2.4418e-05,
          3.4839e-05,  5.2147e-06],
        [-1.7471e-05, -5.6069e-06,  1.2167e-05,  ..., -1.5119e-05,
         -6.2198e-06, -1.6083e-05],
        [-1.5635e-05, -1.2309e-05,  1.0234e-05,  ..., -1.3135e-05,
         -1.2083e-05, -1.0256e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2797e-08, 5.4350e-08, 4.1254e-08,  ..., 3.9053e-08, 1.1388e-07,
         4.9816e-08],
        [6.4859e-11, 4.7973e-11, 1.2762e-11,  ..., 4.8757e-11, 1.8983e-11,
         2.0673e-11],
        [3.7155e-09, 2.3572e-09, 1.0423e-09,  ..., 2.8751e-09, 8.6561e-10,
         9.5734e-10],
        [9.3856e-10, 8.6585e-10, 2.3807e-10,  ..., 7.8256e-10, 3.9398e-10,
         3.3578e-10],
        [3.4872e-10, 1.9888e-10, 6.8344e-11,  ..., 2.6221e-10, 6.8361e-11,
         9.6513e-11]], device='cuda:0')
optimizer state dict: 156.0
lr: [7.424747351382533e-06, 7.424747351382533e-06]
scheduler_last_epoch: 156


Running epoch 1, step 1248, batch 200
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  1005,   292,  ...,   266, 19171,  2474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8189e-05,  3.4592e-05,  4.9332e-05,  ..., -1.4724e-04,
          1.6280e-04,  2.0569e-04],
        [-1.4156e-06, -9.6858e-07,  1.0803e-06,  ..., -1.1250e-06,
         -7.4878e-07, -9.4250e-07],
        [-4.0829e-06, -2.8908e-06,  3.2485e-06,  ..., -3.2336e-06,
         -2.1458e-06, -2.6971e-06],
        [-4.0233e-06, -2.6971e-06,  3.1590e-06,  ..., -3.1888e-06,
         -2.1011e-06, -2.7716e-06],
        [-3.2932e-06, -2.4438e-06,  2.5779e-06,  ..., -2.6524e-06,
         -1.8850e-06, -2.0117e-06]], device='cuda:0')
Loss: 0.9838157892227173


Running epoch 1, step 1249, batch 201
Sampled inputs[:2]: tensor([[    0, 18322,   287,  ...,   953,   271,   221],
        [    0,  2923,   391,  ...,    14,  5424,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9146e-05, -1.9506e-05, -5.8742e-05,  ..., -1.6962e-04,
          2.7695e-04,  3.3366e-04],
        [-2.9206e-06, -1.9372e-06,  2.0340e-06,  ..., -2.3022e-06,
         -1.6056e-06, -2.0154e-06],
        [-8.3148e-06, -5.7667e-06,  6.1244e-06,  ..., -6.5416e-06,
         -4.5747e-06, -5.6922e-06],
        [-8.1062e-06, -5.2899e-06,  5.8711e-06,  ..., -6.3926e-06,
         -4.4554e-06, -5.7966e-06],
        [-6.7800e-06, -4.9472e-06,  4.9472e-06,  ..., -5.4538e-06,
         -4.0755e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.9643054604530334


Running epoch 1, step 1250, batch 202
Sampled inputs[:2]: tensor([[    0,   689,  3953,  ...,   461,   943,   352],
        [    0,   685,  3482,  ..., 23113,    12,  6481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9112e-05, -1.4953e-04, -8.2895e-05,  ..., -3.1055e-04,
          3.2564e-04,  1.7744e-04],
        [-4.3809e-06, -2.8722e-06,  3.0696e-06,  ..., -3.4496e-06,
         -2.4252e-06, -2.9914e-06],
        [-1.2398e-05, -8.5384e-06,  9.1791e-06,  ..., -9.7752e-06,
         -6.8694e-06, -8.4341e-06],
        [-1.2219e-05, -7.8827e-06,  8.9109e-06,  ..., -9.6411e-06,
         -6.7502e-06, -8.6874e-06],
        [-1.0192e-05, -7.3910e-06,  7.4655e-06,  ..., -8.2105e-06,
         -6.1616e-06, -6.4671e-06]], device='cuda:0')
Loss: 0.9885139465332031


Running epoch 1, step 1251, batch 203
Sampled inputs[:2]: tensor([[   0,  795, 3185,  ...,   14, 1671,  199],
        [   0,  806,  300,  ...,  360, 4918, 1106]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9179e-06, -1.0440e-04, -1.3765e-04,  ..., -3.1069e-04,
          1.1698e-04, -2.1048e-05],
        [-5.7220e-06, -3.8855e-06,  4.0680e-06,  ..., -4.5449e-06,
         -3.2559e-06, -3.9376e-06],
        [-1.6063e-05, -1.1444e-05,  1.2115e-05,  ..., -1.2755e-05,
         -9.1791e-06, -1.0982e-05],
        [-1.5914e-05, -1.0669e-05,  1.1832e-05,  ..., -1.2651e-05,
         -9.0599e-06, -1.1414e-05],
        [-1.3322e-05, -9.9838e-06,  9.9391e-06,  ..., -1.0803e-05,
         -8.3074e-06, -8.4788e-06]], device='cuda:0')
Loss: 0.9630753397941589


Running epoch 1, step 1252, batch 204
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   586,  1944,   271],
        [    0,   365,   984,  ..., 18562,  4237, 31813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2781e-04, -6.2279e-05, -1.5330e-04,  ..., -2.7114e-04,
          1.5562e-04, -6.6095e-05],
        [-7.0855e-06, -4.8205e-06,  5.0664e-06,  ..., -5.6401e-06,
         -3.9898e-06, -4.8503e-06],
        [ 2.3366e-04,  2.0025e-04, -2.0751e-04,  ...,  1.9476e-04,
          2.6414e-04,  1.9124e-05],
        [-1.9819e-05, -1.3337e-05,  1.4842e-05,  ..., -1.5780e-05,
         -1.1191e-05, -1.4156e-05],
        [-1.6481e-05, -1.2398e-05,  1.2398e-05,  ..., -1.3411e-05,
         -1.0192e-05, -1.0446e-05]], device='cuda:0')
Loss: 0.9733792543411255


Running epoch 1, step 1253, batch 205
Sampled inputs[:2]: tensor([[    0,  2587,    27,  ...,   259,  2462,  1220],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9228e-05, -9.5131e-05, -1.6518e-04,  ..., -3.1485e-04,
          2.4260e-04,  1.0047e-05],
        [-8.4788e-06, -5.7667e-06,  5.8822e-06,  ..., -6.8173e-06,
         -4.8839e-06, -5.9530e-06],
        [ 2.2964e-04,  1.9745e-04, -2.0495e-04,  ...,  1.9146e-04,
          2.6166e-04,  1.6055e-05],
        [-2.3693e-05, -1.5900e-05,  1.7226e-05,  ..., -1.9014e-05,
         -1.3635e-05, -1.7300e-05],
        [-1.9997e-05, -1.4916e-05,  1.4603e-05,  ..., -1.6332e-05,
         -1.2487e-05, -1.2964e-05]], device='cuda:0')
Loss: 0.9767417907714844


Running epoch 1, step 1254, batch 206
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0,  1458,   365,  ...,  5399,  1110,   870]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8501e-04, -9.4701e-05, -3.9874e-05,  ..., -2.6482e-04,
          2.4801e-04, -1.7636e-04],
        [-9.8273e-06, -6.7279e-06,  6.8583e-06,  ..., -7.8976e-06,
         -5.7667e-06, -6.9067e-06],
        [ 2.2565e-04,  1.9453e-04, -2.0191e-04,  ...,  1.8829e-04,
          2.5912e-04,  1.3253e-05],
        [-2.7657e-05, -1.8641e-05,  2.0221e-05,  ..., -2.2158e-05,
         -1.6183e-05, -2.0236e-05],
        [-2.3320e-05, -1.7479e-05,  1.7107e-05,  ..., -1.9029e-05,
         -1.4782e-05, -1.5154e-05]], device='cuda:0')
Loss: 0.9999119639396667


Running epoch 1, step 1255, batch 207
Sampled inputs[:2]: tensor([[    0,   266,  1890,  ...,   287, 38242,    13],
        [    0,    14,  3228,  ..., 13747,   287, 20295]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0909e-04, -9.2314e-05,  1.8883e-05,  ..., -3.0673e-04,
          3.3603e-04, -2.9379e-06],
        [-1.1280e-05, -7.7486e-06,  7.8343e-06,  ..., -9.0301e-06,
         -6.6534e-06, -7.9349e-06],
        [ 4.3799e-04,  4.5412e-04, -3.6599e-04,  ...,  3.9744e-04,
          4.6380e-04,  1.7687e-04],
        [-3.1799e-05, -2.1502e-05,  2.3097e-05,  ..., -2.5377e-05,
         -1.8686e-05, -2.3246e-05],
        [-2.7001e-05, -2.0221e-05,  1.9670e-05,  ..., -2.1920e-05,
         -1.7151e-05, -1.7598e-05]], device='cuda:0')
Loss: 1.0105692148208618
Graident accumulation at epoch 1, step 1255, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.7894e-05,  4.7946e-05, -1.2115e-04,  ..., -4.9876e-05,
          8.7603e-05, -3.8441e-05],
        [-1.1129e-05, -7.2696e-06,  7.4081e-06,  ..., -9.2256e-06,
         -6.8665e-06, -7.7476e-06],
        [ 6.7980e-05,  8.2832e-05, -5.6274e-05,  ...,  6.1719e-05,
          7.7735e-05,  2.2380e-05],
        [-1.8904e-05, -7.1965e-06,  1.3260e-05,  ..., -1.6144e-05,
         -7.4664e-06, -1.6799e-05],
        [-1.6771e-05, -1.3100e-05,  1.1178e-05,  ..., -1.4013e-05,
         -1.2590e-05, -1.0990e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2768e-08, 5.4304e-08, 4.1213e-08,  ..., 3.9108e-08, 1.1388e-07,
         4.9766e-08],
        [6.4921e-11, 4.7985e-11, 1.2810e-11,  ..., 4.8790e-11, 1.9008e-11,
         2.0715e-11],
        [3.9037e-09, 2.5611e-09, 1.1752e-09,  ..., 3.0302e-09, 1.0799e-09,
         9.8766e-10],
        [9.3863e-10, 8.6545e-10, 2.3837e-10,  ..., 7.8242e-10, 3.9393e-10,
         3.3598e-10],
        [3.4910e-10, 1.9909e-10, 6.8663e-11,  ..., 2.6243e-10, 6.8587e-11,
         9.6726e-11]], device='cuda:0')
optimizer state dict: 157.0
lr: [7.305499933960942e-06, 7.305499933960942e-06]
scheduler_last_epoch: 157


Running epoch 1, step 1256, batch 208
Sampled inputs[:2]: tensor([[    0,    14,   357,  ...,    30,   287,   839],
        [    0, 14652,    12,  ..., 17330,   996,  3294]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2689e-05, -1.5453e-05,  9.9195e-05,  ..., -7.9252e-05,
          1.2019e-04, -1.9805e-06],
        [-1.3858e-06, -1.0207e-06,  1.0133e-06,  ..., -1.1325e-06,
         -8.0094e-07, -9.3132e-07],
        [-3.9935e-06, -3.0547e-06,  3.0845e-06,  ..., -3.2634e-06,
         -2.3246e-06, -2.6524e-06],
        [-3.8743e-06, -2.8163e-06,  2.9653e-06,  ..., -3.1590e-06,
         -2.2054e-06, -2.6971e-06],
        [-3.2485e-06, -2.6077e-06,  2.4885e-06,  ..., -2.7120e-06,
         -2.0713e-06, -2.0117e-06]], device='cuda:0')
Loss: 0.9918889999389648


Running epoch 1, step 1257, batch 209
Sampled inputs[:2]: tensor([[   0,    9, 1471,  ...,  741,  266, 5821],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2590e-05, -3.4193e-05, -3.8131e-05,  ..., -7.7010e-05,
          2.1260e-04, -5.6127e-05],
        [ 6.1930e-05,  8.8544e-05, -4.5910e-05,  ...,  3.4735e-05,
          8.1740e-05,  3.9651e-05],
        [-7.9274e-06, -5.8115e-06,  5.6475e-06,  ..., -6.4373e-06,
         -4.9025e-06, -5.5581e-06],
        [-7.5549e-06, -5.2303e-06,  5.2750e-06,  ..., -6.1840e-06,
         -4.6939e-06, -5.6475e-06],
        [-6.6608e-06, -5.0664e-06,  4.6790e-06,  ..., -5.4836e-06,
         -4.4256e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.9330528378486633


Running epoch 1, step 1258, batch 210
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,   287,  1477,  ...,   997,   292,  4471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3548e-05, -6.6557e-06, -9.9741e-05,  ..., -5.6078e-05,
          1.9889e-04, -1.3487e-04],
        [ 6.0485e-05,  8.7524e-05, -4.4949e-05,  ...,  3.3602e-05,
          8.0902e-05,  3.8690e-05],
        [-1.2010e-05, -8.8513e-06,  8.5682e-06,  ..., -9.6411e-06,
         -7.3314e-06, -8.2850e-06],
        [-1.1519e-05, -8.0466e-06,  8.0764e-06,  ..., -9.3132e-06,
         -7.0333e-06, -8.4192e-06],
        [-9.9987e-06, -7.6592e-06,  7.0482e-06,  ..., -8.1658e-06,
         -6.5714e-06, -6.4075e-06]], device='cuda:0')
Loss: 0.9900519251823425


Running epoch 1, step 1259, batch 211
Sampled inputs[:2]: tensor([[    0,  1428,   266,  ...,  3169,  3058,   278],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1468e-05, -1.2333e-04,  1.3800e-05,  ..., -1.5608e-04,
          4.6500e-04, -1.3105e-04],
        [ 5.9084e-05,  8.6406e-05, -4.4025e-05,  ...,  3.2455e-05,
          7.9934e-05,  3.7740e-05],
        [-1.6242e-05, -1.2293e-05,  1.1459e-05,  ..., -1.3083e-05,
         -1.0222e-05, -1.1161e-05],
        [-1.5453e-05, -1.1131e-05,  1.0744e-05,  ..., -1.2517e-05,
         -9.7305e-06, -1.1206e-05],
        [-1.3500e-05, -1.0625e-05,  9.4175e-06,  ..., -1.1086e-05,
         -9.1195e-06, -8.6576e-06]], device='cuda:0')
Loss: 1.0113359689712524


Running epoch 1, step 1260, batch 212
Sampled inputs[:2]: tensor([[   0,  278, 2097,  ..., 1754,  287,  631],
        [   0,  221,  474,  ..., 6451,  292,   34]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6737e-05, -9.8133e-05,  1.8321e-06,  ..., -1.6336e-04,
          5.2052e-04, -1.8345e-04],
        [ 5.7683e-05,  8.5504e-05, -4.3057e-05,  ...,  3.1307e-05,
          7.9051e-05,  3.6689e-05],
        [-2.0117e-05, -1.4901e-05,  1.4335e-05,  ..., -1.6183e-05,
         -1.2606e-05, -1.3933e-05],
        [-1.9297e-05, -1.3530e-05,  1.3530e-05,  ..., -1.5616e-05,
         -1.2130e-05, -1.4171e-05],
        [-1.6823e-05, -1.2994e-05,  1.1846e-05,  ..., -1.3784e-05,
         -1.1325e-05, -1.0863e-05]], device='cuda:0')
Loss: 0.9389978051185608


Running epoch 1, step 1261, batch 213
Sampled inputs[:2]: tensor([[    0,   287,  9430,  ...,  3121,   352,   360],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5915e-05,  9.2735e-05, -6.2165e-05,  ..., -1.5187e-04,
          5.2012e-04, -1.5984e-04],
        [ 5.6327e-05,  8.4528e-05, -4.2166e-05,  ...,  3.0138e-05,
          7.8108e-05,  3.5676e-05],
        [-2.4021e-05, -1.7837e-05,  1.7107e-05,  ..., -1.9506e-05,
         -1.5274e-05, -1.6764e-05],
        [-2.3171e-05, -1.6302e-05,  1.6212e-05,  ..., -1.8984e-05,
         -1.4856e-05, -1.7181e-05],
        [-2.0117e-05, -1.5542e-05,  1.4156e-05,  ..., -1.6615e-05,
         -1.3709e-05, -1.3083e-05]], device='cuda:0')
Loss: 0.9813799262046814


Running epoch 1, step 1262, batch 214
Sampled inputs[:2]: tensor([[    0,   401,  3408,  ...,   287, 19892,   328],
        [    0,  2523, 10780,  ...,  1041,    26, 13745]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4532e-04,  1.0551e-04, -6.2165e-05,  ..., -9.1218e-05,
          7.5363e-04,  6.0732e-06],
        [ 5.4934e-05,  8.3545e-05, -4.1198e-05,  ...,  2.8990e-05,
          7.7248e-05,  3.4693e-05],
        [-2.8044e-05, -2.0772e-05,  2.0057e-05,  ..., -2.2784e-05,
         -1.7717e-05, -1.9580e-05],
        [-2.7105e-05, -1.9029e-05,  1.9073e-05,  ..., -2.2203e-05,
         -1.7256e-05, -2.0087e-05],
        [-2.3454e-05, -1.8090e-05,  1.6570e-05,  ..., -1.9401e-05,
         -1.5914e-05, -1.5274e-05]], device='cuda:0')
Loss: 0.9972982406616211


Running epoch 1, step 1263, batch 215
Sampled inputs[:2]: tensor([[    0,  5301,   792,  ..., 27135, 34090,   292],
        [    0,   996,  2226,  ...,   516,  3470,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2262e-04,  2.4382e-05, -4.4346e-06,  ..., -1.1764e-04,
          8.5920e-04,  4.7176e-05],
        [ 5.3459e-05,  8.2502e-05, -4.0237e-05,  ...,  2.7768e-05,
          7.6357e-05,  3.3657e-05],
        [-3.2157e-05, -2.3872e-05,  2.2963e-05,  ..., -2.6211e-05,
         -2.0295e-05, -2.2456e-05],
        [-3.1099e-05, -2.1875e-05,  2.1830e-05,  ..., -2.5541e-05,
         -1.9729e-05, -2.3007e-05],
        [-2.6882e-05, -2.0787e-05,  1.8969e-05,  ..., -2.2322e-05,
         -1.8254e-05, -1.7539e-05]], device='cuda:0')
Loss: 0.9942708611488342
Graident accumulation at epoch 1, step 1263, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.4366e-05,  4.5590e-05, -1.0948e-04,  ..., -5.6652e-05,
          1.6476e-04, -2.9880e-05],
        [-4.6703e-06,  1.7076e-06,  2.6436e-06,  ..., -5.5262e-06,
          1.4559e-06, -3.6072e-06],
        [ 5.7966e-05,  7.2161e-05, -4.8350e-05,  ...,  5.2926e-05,
          6.7932e-05,  1.7896e-05],
        [-2.0124e-05, -8.6643e-06,  1.4117e-05,  ..., -1.7084e-05,
         -8.6927e-06, -1.7420e-05],
        [-1.7782e-05, -1.3868e-05,  1.1957e-05,  ..., -1.4844e-05,
         -1.3156e-05, -1.1645e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2744e-08, 5.4250e-08, 4.1172e-08,  ..., 3.9082e-08, 1.1450e-07,
         4.9718e-08],
        [6.7714e-11, 5.4744e-11, 1.4417e-11,  ..., 4.9512e-11, 2.4820e-11,
         2.1827e-11],
        [3.9008e-09, 2.5591e-09, 1.1746e-09,  ..., 3.0278e-09, 1.0792e-09,
         9.8718e-10],
        [9.3866e-10, 8.6506e-10, 2.3861e-10,  ..., 7.8229e-10, 3.9393e-10,
         3.3618e-10],
        [3.4947e-10, 1.9933e-10, 6.8954e-11,  ..., 2.6266e-10, 6.8852e-11,
         9.6937e-11]], device='cuda:0')
optimizer state dict: 158.0
lr: [7.186664259670068e-06, 7.186664259670068e-06]
scheduler_last_epoch: 158


Running epoch 1, step 1264, batch 216
Sampled inputs[:2]: tensor([[   0, 2728, 3139,  ..., 2254,  221,  380],
        [   0,  285,  590,  ...,  199,  395, 3523]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6338e-05,  8.3081e-05,  7.9923e-06,  ...,  7.8006e-05,
          1.4697e-04,  7.9208e-05],
        [-1.6913e-06, -9.3877e-07,  7.7114e-07,  ..., -1.3635e-06,
         -1.0356e-06, -1.4007e-06],
        [-4.5002e-06, -2.7269e-06,  2.3097e-06,  ..., -3.5912e-06,
         -2.8461e-06, -3.5912e-06],
        [-4.2617e-06, -2.3991e-06,  2.0415e-06,  ..., -3.4720e-06,
         -2.7269e-06, -3.5763e-06],
        [-4.2319e-06, -2.5928e-06,  2.1607e-06,  ..., -3.4273e-06,
         -2.8014e-06, -3.2187e-06]], device='cuda:0')
Loss: 0.9366850256919861


Running epoch 1, step 1265, batch 217
Sampled inputs[:2]: tensor([[    0, 25938,   359,  ...,    36, 15859,   504],
        [    0,  1487,   409,  ...,  6979,  1273,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1644e-04,  9.0086e-05,  2.9960e-05,  ...,  8.3947e-05,
          2.1308e-04, -3.3054e-05],
        [-3.0771e-06, -1.9744e-06,  1.6578e-06,  ..., -2.5481e-06,
         -1.9819e-06, -2.4065e-06],
        [-8.5235e-06, -5.8711e-06,  5.0664e-06,  ..., -7.0035e-06,
         -5.5730e-06, -6.4224e-06],
        [-8.1360e-06, -5.2303e-06,  4.6194e-06,  ..., -6.7651e-06,
         -5.3644e-06, -6.4820e-06],
        [-7.6443e-06, -5.3495e-06,  4.4703e-06,  ..., -6.3479e-06,
         -5.2303e-06, -5.4538e-06]], device='cuda:0')
Loss: 0.9805859923362732


Running epoch 1, step 1266, batch 218
Sampled inputs[:2]: tensor([[    0,    19,     9,  ..., 11504,   446,   381],
        [    0, 15003, 19278,  ...,   287,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3690e-04,  1.7034e-04,  7.3264e-06,  ...,  4.4691e-06,
          1.7729e-04, -3.5394e-05],
        [-4.5598e-06, -3.0547e-06,  2.5742e-06,  ..., -3.7551e-06,
         -2.8871e-06, -3.5092e-06],
        [-1.2934e-05, -9.2089e-06,  7.9721e-06,  ..., -1.0595e-05,
         -8.2552e-06, -9.6858e-06],
        [-1.2219e-05, -8.1509e-06,  7.2420e-06,  ..., -1.0103e-05,
         -7.8529e-06, -9.6411e-06],
        [-1.1235e-05, -8.1509e-06,  6.7949e-06,  ..., -9.3281e-06,
         -7.5698e-06, -7.9572e-06]], device='cuda:0')
Loss: 0.9742849469184875


Running epoch 1, step 1267, batch 219
Sampled inputs[:2]: tensor([[    0,   642,   271,  ...,  5430,  2314,  6431],
        [    0,   259,  2697,  ...,  1722, 12673, 15053]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9656e-05,  2.0384e-04, -4.3825e-05,  ..., -5.8899e-07,
          1.1217e-04, -1.9922e-04],
        [-5.8860e-06, -4.0308e-06,  3.5167e-06,  ..., -4.8354e-06,
         -3.6769e-06, -4.4331e-06],
        [-1.6928e-05, -1.2204e-05,  1.0952e-05,  ..., -1.3798e-05,
         -1.0580e-05, -1.2383e-05],
        [-1.6063e-05, -1.0908e-05,  1.0103e-05,  ..., -1.3232e-05,
         -1.0133e-05, -1.2428e-05],
        [-1.4424e-05, -1.0639e-05,  9.1493e-06,  ..., -1.1936e-05,
         -9.5814e-06, -9.9540e-06]], device='cuda:0')
Loss: 0.9400332570075989


Running epoch 1, step 1268, batch 220
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2208,   12, 1901],
        [   0,   12,  287,  ..., 2336,  221,  334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3459e-04,  2.8838e-04, -2.4378e-04,  ...,  2.2997e-04,
         -3.5131e-04, -3.5336e-04],
        [-7.2569e-06, -5.0515e-06,  4.3809e-06,  ..., -6.0275e-06,
         -4.6194e-06, -5.4762e-06],
        [-2.0802e-05, -1.5184e-05,  1.3635e-05,  ..., -1.7092e-05,
         -1.3188e-05, -1.5214e-05],
        [-1.9804e-05, -1.3620e-05,  1.2577e-05,  ..., -1.6451e-05,
         -1.2681e-05, -1.5378e-05],
        [-1.7688e-05, -1.3217e-05,  1.1370e-05,  ..., -1.4737e-05,
         -1.1906e-05, -1.2159e-05]], device='cuda:0')
Loss: 0.9516045451164246


Running epoch 1, step 1269, batch 221
Sampled inputs[:2]: tensor([[    0,   271, 12472,  ...,   374,    29,    16],
        [    0,    13, 20773,  ..., 22463,  2587,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8742e-04,  3.6496e-04, -1.7996e-04,  ...,  1.9091e-04,
         -3.0263e-04, -2.9422e-04],
        [-8.6799e-06, -6.0871e-06,  5.2638e-06,  ..., -7.2196e-06,
         -5.5395e-06, -6.4671e-06],
        [-2.4915e-05, -1.8343e-05,  1.6376e-05,  ..., -2.0564e-05,
         -1.5914e-05, -1.8075e-05],
        [-2.3797e-05, -1.6496e-05,  1.5154e-05,  ..., -1.9804e-05,
         -1.5303e-05, -1.8269e-05],
        [-2.1100e-05, -1.5929e-05,  1.3620e-05,  ..., -1.7643e-05,
         -1.4305e-05, -1.4395e-05]], device='cuda:0')
Loss: 0.9808224439620972


Running epoch 1, step 1270, batch 222
Sampled inputs[:2]: tensor([[   0,  278, 8608,  ...,  293, 1608,  391],
        [   0,   26, 4044,  ..., 9531,  365,  993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0621e-04,  3.6074e-04, -1.3252e-04,  ...,  2.5419e-04,
         -1.7010e-04, -2.1794e-04],
        [-1.0125e-05, -7.1079e-06,  6.1058e-06,  ..., -8.4490e-06,
         -6.5528e-06, -7.5549e-06],
        [-2.9117e-05, -2.1413e-05,  1.9014e-05,  ..., -2.4110e-05,
         -1.8835e-05, -2.1189e-05],
        [-2.7850e-05, -1.9312e-05,  1.7598e-05,  ..., -2.3261e-05,
         -1.8165e-05, -2.1443e-05],
        [-2.4691e-05, -1.8671e-05,  1.5855e-05,  ..., -2.0728e-05,
         -1.6972e-05, -1.6898e-05]], device='cuda:0')
Loss: 1.0105047225952148


Running epoch 1, step 1271, batch 223
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ..., 16948,   266,  1751],
        [    0,   275,  1620,  ...,  3020,   278,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1732e-04,  4.2295e-04, -2.5375e-04,  ...,  2.5946e-04,
         -1.9831e-04, -3.8116e-04],
        [-1.1504e-05, -8.1584e-06,  7.0371e-06,  ..., -9.5963e-06,
         -7.3723e-06, -8.4490e-06],
        [-3.3081e-05, -2.4527e-05,  2.1890e-05,  ..., -2.7373e-05,
         -2.1160e-05, -2.3723e-05],
        [-3.1650e-05, -2.2173e-05,  2.0310e-05,  ..., -2.6405e-05,
         -2.0415e-05, -2.4036e-05],
        [-2.7910e-05, -2.1309e-05,  1.8150e-05,  ..., -2.3440e-05,
         -1.9044e-05, -1.8805e-05]], device='cuda:0')
Loss: 0.9760866165161133
Graident accumulation at epoch 1, step 1271, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.8662e-05,  8.3326e-05, -1.2391e-04,  ..., -2.5041e-05,
          1.2846e-04, -6.5008e-05],
        [-5.3536e-06,  7.2098e-07,  3.0830e-06,  ..., -5.9332e-06,
          5.7304e-07, -4.0914e-06],
        [ 4.8861e-05,  6.2492e-05, -4.1326e-05,  ...,  4.4896e-05,
          5.9023e-05,  1.3734e-05],
        [-2.1276e-05, -1.0015e-05,  1.4736e-05,  ..., -1.8016e-05,
         -9.8649e-06, -1.8081e-05],
        [-1.8795e-05, -1.4613e-05,  1.2576e-05,  ..., -1.5704e-05,
         -1.3745e-05, -1.2361e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2772e-08, 5.4375e-08, 4.1195e-08,  ..., 3.9111e-08, 1.1442e-07,
         4.9814e-08],
        [6.7779e-11, 5.4756e-11, 1.4452e-11,  ..., 4.9555e-11, 2.4849e-11,
         2.1877e-11],
        [3.8980e-09, 2.5571e-09, 1.1739e-09,  ..., 3.0256e-09, 1.0786e-09,
         9.8676e-10],
        [9.3872e-10, 8.6469e-10, 2.3878e-10,  ..., 7.8220e-10, 3.9395e-10,
         3.3642e-10],
        [3.4990e-10, 1.9958e-10, 6.9215e-11,  ..., 2.6295e-10, 6.9145e-11,
         9.7193e-11]], device='cuda:0')
optimizer state dict: 159.0
lr: [7.068258487638268e-06, 7.068258487638268e-06]
scheduler_last_epoch: 159


Running epoch 1, step 1272, batch 224
Sampled inputs[:2]: tensor([[    0,  1119,   943,  ...,   759,   920,  8874],
        [    0,   680,   271,  ..., 12942,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4099e-05, -8.2668e-05, -3.0919e-04,  ...,  9.4372e-05,
         -2.6483e-04, -1.0809e-04],
        [-1.4380e-06, -1.0729e-06,  9.4250e-07,  ..., -1.1921e-06,
         -9.4995e-07, -9.8348e-07],
        [-4.2319e-06, -3.2932e-06,  2.9355e-06,  ..., -3.5167e-06,
         -2.8461e-06, -2.9206e-06],
        [-3.9935e-06, -2.9504e-06,  2.7418e-06,  ..., -3.2932e-06,
         -2.6375e-06, -2.8461e-06],
        [-3.5167e-06, -2.8461e-06,  2.4289e-06,  ..., -2.9653e-06,
         -2.5332e-06, -2.2799e-06]], device='cuda:0')
Loss: 0.9738060235977173


Running epoch 1, step 1273, batch 225
Sampled inputs[:2]: tensor([[    0,  1486,   292,  ...,  7484,    15,  5357],
        [    0,   287, 11638,  ...,    17,   221,   733]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6214e-05, -3.5412e-05, -2.7965e-04,  ...,  1.1760e-04,
         -3.4008e-04, -1.5707e-04],
        [-2.8834e-06, -2.1830e-06,  1.8775e-06,  ..., -2.3693e-06,
         -1.9036e-06, -1.9744e-06],
        [-8.5235e-06, -6.7204e-06,  5.8711e-06,  ..., -7.0333e-06,
         -5.7071e-06, -5.8711e-06],
        [-8.0168e-06, -5.9903e-06,  5.4538e-06,  ..., -6.5565e-06,
         -5.2750e-06, -5.6922e-06],
        [-6.9737e-06, -5.6922e-06,  4.7684e-06,  ..., -5.8413e-06,
         -4.9770e-06, -4.5300e-06]], device='cuda:0')
Loss: 0.9768978953361511


Running epoch 1, step 1274, batch 226
Sampled inputs[:2]: tensor([[    0,    14,  2729,  ...,   266,  1659, 14362],
        [    0,  1188,    12,  ...,   292, 23032,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2526e-04, -3.6859e-05, -3.6806e-04,  ...,  1.2046e-04,
         -4.1187e-04, -1.8131e-04],
        [-4.2915e-06, -3.2112e-06,  2.8051e-06,  ..., -3.5763e-06,
         -2.8387e-06, -3.0026e-06],
        [-1.2755e-05, -9.8646e-06,  8.8066e-06,  ..., -1.0610e-05,
         -8.4788e-06, -8.9407e-06],
        [-1.1981e-05, -8.8215e-06,  8.1658e-06,  ..., -9.9242e-06,
         -7.8827e-06, -8.7023e-06],
        [-1.0416e-05, -8.3596e-06,  7.1377e-06,  ..., -8.8066e-06,
         -7.3910e-06, -6.8843e-06]], device='cuda:0')
Loss: 0.9986242651939392


Running epoch 1, step 1275, batch 227
Sampled inputs[:2]: tensor([[    0,  8822,  1486,  ...,    12,   287,  6903],
        [    0,   298,   669,  ...,   287, 19731,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6097e-04,  2.8011e-05, -3.9169e-04,  ...,  1.3541e-04,
         -6.0140e-04, -8.2159e-05],
        [-5.8636e-06, -4.2915e-06,  3.6880e-06,  ..., -4.8876e-06,
         -3.8520e-06, -4.1872e-06],
        [-1.7464e-05, -1.3232e-05,  1.1623e-05,  ..., -1.4484e-05,
         -1.1489e-05, -1.2442e-05],
        [-1.6063e-05, -1.1593e-05,  1.0520e-05,  ..., -1.3322e-05,
         -1.0505e-05, -1.1832e-05],
        [-1.4380e-05, -1.1280e-05,  9.4920e-06,  ..., -1.2159e-05,
         -1.0103e-05, -9.6858e-06]], device='cuda:0')
Loss: 0.9418334364891052


Running epoch 1, step 1276, batch 228
Sampled inputs[:2]: tensor([[    0,  1234,   408,  ...,   292, 17323,   221],
        [    0,    83,    12,  ...,  3781,   292, 27247]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4683e-04,  4.9732e-05, -3.6516e-04,  ...,  9.9939e-05,
         -5.4232e-04,  4.5463e-05],
        [-7.3910e-06, -5.4091e-06,  4.6231e-06,  ..., -6.1616e-06,
         -4.8578e-06, -5.2899e-06],
        [-2.1935e-05, -1.6645e-05,  1.4514e-05,  ..., -1.8209e-05,
         -1.4424e-05, -1.5676e-05],
        [-2.0236e-05, -1.4633e-05,  1.3173e-05,  ..., -1.6823e-05,
         -1.3247e-05, -1.4976e-05],
        [-1.8135e-05, -1.4231e-05,  1.1891e-05,  ..., -1.5333e-05,
         -1.2711e-05, -1.2264e-05]], device='cuda:0')
Loss: 0.9671244621276855


Running epoch 1, step 1277, batch 229
Sampled inputs[:2]: tensor([[    0,   287,  6761,  ...,  1918, 33351,    12],
        [    0,    14,    22,  ...,  1319,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5087e-04,  2.1703e-04, -2.7977e-04,  ...,  9.9775e-06,
         -2.9172e-04,  3.0145e-04],
        [-8.9109e-06, -6.5938e-06,  5.4389e-06,  ..., -7.4580e-06,
         -5.9903e-06, -6.4597e-06],
        [-2.6494e-05, -2.0340e-05,  1.7121e-05,  ..., -2.2084e-05,
         -1.7837e-05, -1.9178e-05],
        [-2.4259e-05, -1.7747e-05,  1.5393e-05,  ..., -2.0266e-05,
         -1.6272e-05, -1.8179e-05],
        [-2.2128e-05, -1.7539e-05,  1.4141e-05,  ..., -1.8775e-05,
         -1.5825e-05, -1.5199e-05]], device='cuda:0')
Loss: 0.9980461001396179


Running epoch 1, step 1278, batch 230
Sampled inputs[:2]: tensor([[   0, 9010,   17,  ..., 3813, 1147,  199],
        [   0,  360, 2374,  ...,  221,  474,  357]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9395e-04,  1.3588e-04, -2.4442e-04,  ...,  9.9775e-06,
         -2.9172e-04,  3.2883e-04],
        [-1.0341e-05, -7.6666e-06,  6.3255e-06,  ..., -8.6650e-06,
         -6.9737e-06, -7.4729e-06],
        [-3.0577e-05, -2.3514e-05,  1.9863e-05,  ..., -2.5526e-05,
         -2.0653e-05, -2.2054e-05],
        [-2.8253e-05, -2.0698e-05,  1.7986e-05,  ..., -2.3633e-05,
         -1.9029e-05, -2.1130e-05],
        [-2.5600e-05, -2.0310e-05,  1.6406e-05,  ..., -2.1741e-05,
         -1.8343e-05, -1.7494e-05]], device='cuda:0')
Loss: 1.0096956491470337


Running epoch 1, step 1279, batch 231
Sampled inputs[:2]: tensor([[    0,    14,  5551,  ...,   668, 11988,  2538],
        [    0,  1716,  1773,  ...,  5014,    12,   847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6403e-04,  2.0095e-04, -1.5949e-04,  ..., -1.8928e-06,
         -2.1293e-04,  3.5954e-04],
        [-1.1764e-05, -8.7619e-06,  7.1488e-06,  ..., -9.9018e-06,
         -8.0019e-06, -8.5309e-06],
        [-3.4630e-05, -2.6748e-05,  2.2396e-05,  ..., -2.9013e-05,
         -2.3589e-05, -2.4989e-05],
        [-3.2097e-05, -2.3633e-05,  2.0295e-05,  ..., -2.6971e-05,
         -2.1845e-05, -2.4065e-05],
        [-2.9072e-05, -2.3156e-05,  1.8567e-05,  ..., -2.4766e-05,
         -2.0981e-05, -1.9863e-05]], device='cuda:0')
Loss: 0.9923274517059326
Graident accumulation at epoch 1, step 1279, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2520e-04,  9.5088e-05, -1.2746e-04,  ..., -2.2726e-05,
          9.4317e-05, -2.2554e-05],
        [-5.9947e-06, -2.2731e-07,  3.4896e-06,  ..., -6.3301e-06,
         -2.8446e-07, -4.5353e-06],
        [ 4.0512e-05,  5.3568e-05, -3.4954e-05,  ...,  3.7505e-05,
          5.0762e-05,  9.8621e-06],
        [-2.2358e-05, -1.1377e-05,  1.5292e-05,  ..., -1.8912e-05,
         -1.1063e-05, -1.8680e-05],
        [-1.9823e-05, -1.5467e-05,  1.3175e-05,  ..., -1.6610e-05,
         -1.4469e-05, -1.3111e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2832e-08, 5.4361e-08, 4.1179e-08,  ..., 3.9072e-08, 1.1436e-07,
         4.9893e-08],
        [6.7850e-11, 5.4778e-11, 1.4488e-11,  ..., 4.9603e-11, 2.4888e-11,
         2.1928e-11],
        [3.8953e-09, 2.5553e-09, 1.1732e-09,  ..., 3.0234e-09, 1.0780e-09,
         9.8639e-10],
        [9.3881e-10, 8.6438e-10, 2.3896e-10,  ..., 7.8215e-10, 3.9403e-10,
         3.3666e-10],
        [3.5040e-10, 1.9992e-10, 6.9490e-11,  ..., 2.6330e-10, 6.9516e-11,
         9.7491e-11]], device='cuda:0')
optimizer state dict: 160.0
lr: [6.950300711301095e-06, 6.950300711301095e-06]
scheduler_last_epoch: 160


Running epoch 1, step 1280, batch 232
Sampled inputs[:2]: tensor([[   0,  328, 2097,  ...,  365, 1941,  607],
        [   0, 2555,  984,  ..., 5900, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3472e-06,  5.2586e-05,  1.3036e-04,  ..., -5.7086e-05,
         -1.5783e-05,  2.0385e-05],
        [-1.3933e-06, -1.0952e-06,  8.6799e-07,  ..., -1.2219e-06,
         -1.0058e-06, -9.8348e-07],
        [-4.0233e-06, -3.2783e-06,  2.6524e-06,  ..., -3.5018e-06,
         -2.9206e-06, -2.8014e-06],
        [-3.9041e-06, -3.0398e-06,  2.5332e-06,  ..., -3.4273e-06,
         -2.8312e-06, -2.8610e-06],
        [-3.3975e-06, -2.8610e-06,  2.2054e-06,  ..., -3.0100e-06,
         -2.5928e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9945853352546692


Running epoch 1, step 1281, batch 233
Sampled inputs[:2]: tensor([[    0,    13,  7805,  ...,  2733,    12,   287],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0309e-04,  4.3587e-06,  1.7882e-04,  ..., -3.8044e-05,
         -1.5573e-04,  1.1063e-04],
        [-2.8312e-06, -2.2426e-06,  1.8068e-06,  ..., -2.4363e-06,
         -2.0266e-06, -2.0117e-06],
        [-8.3148e-06, -6.7949e-06,  5.6028e-06,  ..., -7.1377e-06,
         -5.9754e-06, -5.9009e-06],
        [-8.0466e-06, -6.3032e-06,  5.3197e-06,  ..., -6.9141e-06,
         -5.7667e-06, -5.9307e-06],
        [-6.8992e-06, -5.8264e-06,  4.5747e-06,  ..., -6.0201e-06,
         -5.2452e-06, -4.6343e-06]], device='cuda:0')
Loss: 1.0087794065475464


Running epoch 1, step 1282, batch 234
Sampled inputs[:2]: tensor([[    0,   391,  7750,  ...,  4133,   271,   668],
        [    0,   278,   266,  ...,   352, 10572,   345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4085e-05,  5.1698e-05,  1.2030e-04,  ..., -5.8474e-05,
         -3.8875e-04,  7.6926e-05],
        [-4.3139e-06, -3.1963e-06,  2.6338e-06,  ..., -3.6657e-06,
         -3.0473e-06, -3.1665e-06],
        [-1.2726e-05, -9.7454e-06,  8.2403e-06,  ..., -1.0759e-05,
         -8.9705e-06, -9.2685e-06],
        [-1.2130e-05, -8.8513e-06,  7.6741e-06,  ..., -1.0282e-05,
         -8.5682e-06, -9.2238e-06],
        [-1.0565e-05, -8.4043e-06,  6.7502e-06,  ..., -9.0897e-06,
         -7.8976e-06, -7.2867e-06]], device='cuda:0')
Loss: 0.9469701647758484


Running epoch 1, step 1283, batch 235
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,   199,   769,  ..., 12038, 15317,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0730e-04,  2.5652e-04,  2.5297e-04,  ..., -1.5774e-04,
         -5.3177e-04,  1.3593e-04],
        [-5.7817e-06, -4.2394e-06,  3.5018e-06,  ..., -4.9174e-06,
         -4.0680e-06, -4.1649e-06],
        [-1.6779e-05, -1.2800e-05,  1.0833e-05,  ..., -1.4231e-05,
         -1.1817e-05, -1.2010e-05],
        [-1.6212e-05, -1.1757e-05,  1.0207e-05,  ..., -1.3798e-05,
         -1.1444e-05, -1.2144e-05],
        [-1.3947e-05, -1.1057e-05,  8.8960e-06,  ..., -1.2025e-05,
         -1.0416e-05, -9.4324e-06]], device='cuda:0')
Loss: 0.982597827911377


Running epoch 1, step 1284, batch 236
Sampled inputs[:2]: tensor([[   0,  280, 5656,  ..., 7369, 2276,   12],
        [   0,  300, 5631,  ..., 2278, 2669, 3011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9674e-04,  2.6430e-04,  1.7636e-04,  ...,  2.7273e-05,
         -9.1765e-04,  8.3990e-05],
        [-7.2420e-06, -5.3495e-06,  4.4778e-06,  ..., -6.1318e-06,
         -4.9695e-06, -5.1558e-06],
        [-2.1011e-05, -1.6138e-05,  1.3798e-05,  ..., -1.7762e-05,
         -1.4484e-05, -1.4901e-05],
        [-2.0295e-05, -1.4856e-05,  1.3053e-05,  ..., -1.7196e-05,
         -1.3977e-05, -1.5035e-05],
        [-1.7345e-05, -1.3873e-05,  1.1265e-05,  ..., -1.4931e-05,
         -1.2740e-05, -1.1653e-05]], device='cuda:0')
Loss: 0.9874684810638428


Running epoch 1, step 1285, batch 237
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,  5308,   266, 14679],
        [    0,  1034,  5599,  ...,   259,   586,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4344e-04,  2.4978e-04,  8.6279e-05,  ...,  3.4491e-05,
         -1.0068e-03,  1.6598e-04],
        [-8.6799e-06, -6.4448e-06,  5.4687e-06,  ..., -7.3239e-06,
         -5.9046e-06, -6.1244e-06],
        [-2.5183e-05, -1.9386e-05,  1.6779e-05,  ..., -2.1204e-05,
         -1.7181e-05, -1.7717e-05],
        [-2.4348e-05, -1.7896e-05,  1.5944e-05,  ..., -2.0549e-05,
         -1.6600e-05, -1.7881e-05],
        [-2.0713e-05, -1.6600e-05,  1.3635e-05,  ..., -1.7762e-05,
         -1.5065e-05, -1.3813e-05]], device='cuda:0')
Loss: 0.9785459041595459


Running epoch 1, step 1286, batch 238
Sampled inputs[:2]: tensor([[    0,  1176,    13,  ...,  1919,   221,   380],
        [    0,   292,    40,  ..., 26995,   278,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8235e-04,  3.4258e-04,  1.0533e-04,  ...,  3.7309e-05,
         -9.6369e-04,  1.2640e-04],
        [-1.0163e-05, -7.4804e-06,  6.2734e-06,  ..., -8.6427e-06,
         -7.0147e-06, -7.3165e-06],
        [-2.9296e-05, -2.2337e-05,  1.9222e-05,  ..., -2.4810e-05,
         -2.0221e-05, -2.0936e-05],
        [-2.8253e-05, -2.0579e-05,  1.8165e-05,  ..., -2.4021e-05,
         -1.9535e-05, -2.1100e-05],
        [-2.4378e-05, -1.9282e-05,  1.5810e-05,  ..., -2.0996e-05,
         -1.7896e-05, -1.6510e-05]], device='cuda:0')
Loss: 0.9454393982887268


Running epoch 1, step 1287, batch 239
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0,   298,   452,  ..., 41263,     9,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6976e-04,  3.4258e-04,  6.2262e-05,  ...,  1.6167e-05,
         -1.0345e-03,  1.4078e-04],
        [-1.1735e-05, -8.5607e-06,  7.0781e-06,  ..., -9.9540e-06,
         -8.1249e-06, -8.5160e-06],
        [-3.4064e-05, -2.5764e-05,  2.1815e-05,  ..., -2.8804e-05,
         -2.3648e-05, -2.4557e-05],
        [-3.2485e-05, -2.3454e-05,  2.0429e-05,  ..., -2.7567e-05,
         -2.2560e-05, -2.4423e-05],
        [-2.8402e-05, -2.2277e-05,  1.7971e-05,  ..., -2.4423e-05,
         -2.0951e-05, -1.9446e-05]], device='cuda:0')
Loss: 0.9791825413703918
Graident accumulation at epoch 1, step 1287, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0020,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5703e-05,  1.1984e-04, -1.0849e-04,  ..., -1.8837e-05,
         -1.8569e-05, -6.2201e-06],
        [-6.5687e-06, -1.0606e-06,  3.8484e-06,  ..., -6.6925e-06,
         -1.0685e-06, -4.9334e-06],
        [ 3.3054e-05,  4.5635e-05, -2.9277e-05,  ...,  3.0874e-05,
          4.3321e-05,  6.4202e-06],
        [-2.3371e-05, -1.2585e-05,  1.5806e-05,  ..., -1.9777e-05,
         -1.2213e-05, -1.9254e-05],
        [-2.0681e-05, -1.6148e-05,  1.3655e-05,  ..., -1.7391e-05,
         -1.5117e-05, -1.3745e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2832e-08, 5.4424e-08, 4.1142e-08,  ..., 3.9033e-08, 1.1531e-07,
         4.9863e-08],
        [6.7919e-11, 5.4796e-11, 1.4524e-11,  ..., 4.9653e-11, 2.4930e-11,
         2.1978e-11],
        [3.8926e-09, 2.5534e-09, 1.1725e-09,  ..., 3.0212e-09, 1.0775e-09,
         9.8601e-10],
        [9.3893e-10, 8.6407e-10, 2.3913e-10,  ..., 7.8212e-10, 3.9415e-10,
         3.3692e-10],
        [3.5085e-10, 2.0021e-10, 6.9744e-11,  ..., 2.6363e-10, 6.9886e-11,
         9.7771e-11]], device='cuda:0')
optimizer state dict: 161.0
lr: [6.8328089556364305e-06, 6.8328089556364305e-06]
scheduler_last_epoch: 161


Running epoch 1, step 1288, batch 240
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,  1098,   259,  ...,  6572,  1477,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2357e-04, -9.6714e-05, -1.1833e-04,  ...,  1.4289e-06,
          3.5809e-05, -5.4959e-05],
        [-1.4305e-06, -1.1474e-06,  9.2387e-07,  ..., -1.2293e-06,
         -9.9838e-07, -9.8348e-07],
        [-4.0233e-06, -3.3826e-06,  2.7716e-06,  ..., -3.4720e-06,
         -2.8610e-06, -2.7269e-06],
        [-3.9637e-06, -3.1739e-06,  2.6673e-06,  ..., -3.4273e-06,
         -2.8312e-06, -2.8312e-06],
        [-3.3528e-06, -2.9206e-06,  2.2799e-06,  ..., -2.9206e-06,
         -2.5183e-06, -2.1458e-06]], device='cuda:0')
Loss: 1.0148588418960571


Running epoch 1, step 1289, batch 241
Sampled inputs[:2]: tensor([[   0,  292, 1820,  ...,  591, 6619, 1607],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3111e-05, -6.9738e-05, -9.4720e-05,  ...,  5.6998e-05,
         -6.7028e-06,  4.3374e-07],
        [-2.8387e-06, -2.2501e-06,  1.7844e-06,  ..., -2.4736e-06,
         -2.0266e-06, -1.9893e-06],
        [-8.1062e-06, -6.7204e-06,  5.4538e-06,  ..., -7.0632e-06,
         -5.8264e-06, -5.6177e-06],
        [-7.9274e-06, -6.2734e-06,  5.1856e-06,  ..., -6.9439e-06,
         -5.7369e-06, -5.7667e-06],
        [-6.7502e-06, -5.7667e-06,  4.4852e-06,  ..., -5.9456e-06,
         -5.1111e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9957435131072998


Running epoch 1, step 1290, batch 242
Sampled inputs[:2]: tensor([[    0,   287,   298,  ...,    14,  1147,   199],
        [    0,   733,   560,  ...,  1172, 22808,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1342e-05, -1.8134e-04, -2.4540e-04,  ...,  1.6081e-04,
         -1.0427e-04,  1.7125e-04],
        [-4.3958e-06, -3.2261e-06,  2.6226e-06,  ..., -3.7849e-06,
         -3.0845e-06, -3.2559e-06],
        [-1.2666e-05, -9.7305e-06,  8.1062e-06,  ..., -1.0878e-05,
         -8.9556e-06, -9.2834e-06],
        [-1.2159e-05, -8.8960e-06,  7.5549e-06,  ..., -1.0490e-05,
         -8.6427e-06, -9.2834e-06],
        [-1.0714e-05, -8.4639e-06,  6.7651e-06,  ..., -9.3132e-06,
         -7.9721e-06, -7.4506e-06]], device='cuda:0')
Loss: 0.9215981364250183


Running epoch 1, step 1291, batch 243
Sampled inputs[:2]: tensor([[    0,    14,   333,  ...,   328,  5453,  4713],
        [    0,   287, 16974,  ...,   300,  2283,  4013]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6244e-04, -6.1365e-05, -3.8030e-04,  ...,  3.0612e-04,
         -2.7111e-04, -2.0424e-05],
        [-5.8338e-06, -4.4256e-06,  3.5763e-06,  ..., -4.9993e-06,
         -4.1053e-06, -4.1872e-06],
        [-1.6809e-05, -1.3307e-05,  1.0997e-05,  ..., -1.4395e-05,
         -1.1906e-05, -1.1981e-05],
        [-1.6153e-05, -1.2219e-05,  1.0312e-05,  ..., -1.3858e-05,
         -1.1474e-05, -1.1981e-05],
        [-1.4096e-05, -1.1474e-05,  9.0897e-06,  ..., -1.2219e-05,
         -1.0520e-05, -9.5218e-06]], device='cuda:0')
Loss: 0.9968104362487793


Running epoch 1, step 1292, batch 244
Sampled inputs[:2]: tensor([[    0,  1603,    27,  ..., 19959, 22776,   328],
        [    0,  1911,   679,  ...,    19,  3737,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2623e-04, -5.5468e-05, -4.7793e-04,  ...,  3.9847e-04,
         -8.7916e-05,  1.1223e-04],
        [-7.2643e-06, -5.4911e-06,  4.3884e-06,  ..., -6.2883e-06,
         -5.1484e-06, -5.2080e-06],
        [-2.0981e-05, -1.6555e-05,  1.3545e-05,  ..., -1.8135e-05,
         -1.4916e-05, -1.4931e-05],
        [-2.0117e-05, -1.5125e-05,  1.2636e-05,  ..., -1.7434e-05,
         -1.4365e-05, -1.4946e-05],
        [-1.7554e-05, -1.4246e-05,  1.1176e-05,  ..., -1.5348e-05,
         -1.3128e-05, -1.1817e-05]], device='cuda:0')
Loss: 0.9767744541168213


Running epoch 1, step 1293, batch 245
Sampled inputs[:2]: tensor([[   0,   17,  292,  ..., 2269, 3887,  278],
        [   0,   12,  638,  ...,  380,  560,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9000e-04,  2.8451e-06, -4.7424e-04,  ...,  5.3173e-04,
         -4.8031e-04, -8.5222e-05],
        [-8.6948e-06, -6.5491e-06,  5.2229e-06,  ..., -7.5400e-06,
         -6.1989e-06, -6.3032e-06],
        [-2.5064e-05, -1.9684e-05,  1.6138e-05,  ..., -2.1651e-05,
         -1.7896e-05, -1.7956e-05],
        [-2.3961e-05, -1.7941e-05,  1.4991e-05,  ..., -2.0787e-05,
         -1.7226e-05, -1.7956e-05],
        [-2.1130e-05, -1.7047e-05,  1.3396e-05,  ..., -1.8448e-05,
         -1.5840e-05, -1.4305e-05]], device='cuda:0')
Loss: 0.9544683694839478


Running epoch 1, step 1294, batch 246
Sampled inputs[:2]: tensor([[    0,  1403,    12,  ...,  1062,  2283, 13614],
        [    0,    15,    19,  ...,   266,  6391,  1777]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3334e-04, -2.0796e-05, -5.1870e-04,  ...,  6.1841e-04,
         -4.7474e-04,  1.0010e-05],
        [-1.0230e-05, -7.6741e-06,  6.0573e-06,  ..., -8.8438e-06,
         -7.2867e-06, -7.4878e-06],
        [-2.9624e-05, -2.3142e-05,  1.8790e-05,  ..., -2.5496e-05,
         -2.1145e-05, -2.1443e-05],
        [-2.8014e-05, -2.0891e-05,  1.7300e-05,  ..., -2.4229e-05,
         -2.0117e-05, -2.1160e-05],
        [-2.5153e-05, -2.0146e-05,  1.5676e-05,  ..., -2.1890e-05,
         -1.8820e-05, -1.7256e-05]], device='cuda:0')
Loss: 0.9542983770370483


Running epoch 1, step 1295, batch 247
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0,  5379,  6922,  ...,  1115, 43884,  2843]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4200e-04, -6.8467e-05, -6.2755e-04,  ...,  6.8734e-04,
         -6.8530e-04, -4.4656e-05],
        [-1.1556e-05, -8.7395e-06,  6.9216e-06,  ..., -1.0014e-05,
         -8.2329e-06, -8.4229e-06],
        [-3.3557e-05, -2.6345e-05,  2.1517e-05,  ..., -2.8908e-05,
         -2.3872e-05, -2.4170e-05],
        [-3.1784e-05, -2.3872e-05,  1.9878e-05,  ..., -2.7537e-05,
         -2.2784e-05, -2.3946e-05],
        [-2.8268e-05, -2.2769e-05,  1.7807e-05,  ..., -2.4632e-05,
         -2.1130e-05, -1.9267e-05]], device='cuda:0')
Loss: 0.98235023021698
Graident accumulation at epoch 1, step 1295, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0156, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0133e-04,  1.0101e-04, -1.6040e-04,  ...,  5.1780e-05,
         -8.5242e-05, -1.0064e-05],
        [-7.0674e-06, -1.8285e-06,  4.1557e-06,  ..., -7.0246e-06,
         -1.7849e-06, -5.2823e-06],
        [ 2.6393e-05,  3.8437e-05, -2.4198e-05,  ...,  2.4896e-05,
          3.6601e-05,  3.3612e-06],
        [-2.4212e-05, -1.3713e-05,  1.6213e-05,  ..., -2.0553e-05,
         -1.3270e-05, -1.9723e-05],
        [-2.1439e-05, -1.6810e-05,  1.4070e-05,  ..., -1.8115e-05,
         -1.5718e-05, -1.4297e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2818e-08, 5.4374e-08, 4.1495e-08,  ..., 3.9466e-08, 1.1567e-07,
         4.9815e-08],
        [6.7985e-11, 5.4818e-11, 1.4557e-11,  ..., 4.9703e-11, 2.4972e-11,
         2.2027e-11],
        [3.8898e-09, 2.5516e-09, 1.1718e-09,  ..., 3.0190e-09, 1.0770e-09,
         9.8561e-10],
        [9.3900e-10, 8.6377e-10, 2.3929e-10,  ..., 7.8210e-10, 3.9427e-10,
         3.3716e-10],
        [3.5130e-10, 2.0053e-10, 6.9991e-11,  ..., 2.6398e-10, 7.0262e-11,
         9.8045e-11]], device='cuda:0')
optimizer state dict: 162.0
lr: [6.715801174410152e-06, 6.715801174410152e-06]
scheduler_last_epoch: 162


Running epoch 1, step 1296, batch 248
Sampled inputs[:2]: tensor([[    0,   894,    73,  ...,  2323,   909,  4103],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3144e-05,  1.1282e-04, -5.8851e-05,  ...,  3.7518e-05,
         -1.6868e-04, -9.1935e-05],
        [-1.3411e-06, -1.0431e-06,  9.3132e-07,  ..., -1.1623e-06,
         -9.2387e-07, -9.3877e-07],
        [ 7.4086e-05,  1.3872e-04, -1.1352e-05,  ...,  4.2819e-05,
          9.7569e-05,  2.1545e-05],
        [-3.7700e-06, -2.8759e-06,  2.7269e-06,  ..., -3.2783e-06,
         -2.5928e-06, -2.7567e-06],
        [-3.0696e-06, -2.5481e-06,  2.2054e-06,  ..., -2.6971e-06,
         -2.2352e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9744858145713806


Running epoch 1, step 1297, batch 249
Sampled inputs[:2]: tensor([[    0,   825,  1243,  ...,    15,    22,    42],
        [    0,  1029,  6068,  ..., 18017,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5122e-05,  2.5752e-06, -8.4023e-05,  ..., -5.6786e-05,
         -2.7596e-04,  1.1309e-04],
        [-2.7642e-06, -2.0862e-06,  1.8999e-06,  ..., -2.3544e-06,
         -1.8217e-06, -1.8999e-06],
        [ 6.9944e-05,  1.3553e-04, -8.4013e-06,  ...,  3.9302e-05,
          9.4902e-05,  1.8728e-05],
        [-7.7337e-06, -5.7817e-06,  5.5283e-06,  ..., -6.6310e-06,
         -5.1409e-06, -5.5581e-06],
        [-6.2734e-06, -5.1260e-06,  4.4554e-06,  ..., -5.4687e-06,
         -4.4405e-06, -4.0382e-06]], device='cuda:0')
Loss: 0.9846053123474121


Running epoch 1, step 1298, batch 250
Sampled inputs[:2]: tensor([[    0,   266, 15258,  ...,  2366,   368,  3988],
        [    0,   391,  1761,  ...,   346,    14,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4927e-05, -9.7632e-05, -2.4778e-04,  ..., -8.6846e-05,
         -2.8341e-04,  1.4222e-04],
        [-4.1798e-06, -3.1590e-06,  2.8908e-06,  ..., -3.4943e-06,
         -2.7157e-06, -2.8200e-06],
        [ 6.5920e-05,  1.3239e-04, -5.4807e-06,  ...,  3.6054e-05,
          9.2339e-05,  1.6091e-05],
        [-1.1668e-05, -8.7470e-06,  8.3894e-06,  ..., -9.8199e-06,
         -7.6294e-06, -8.2254e-06],
        [-9.4771e-06, -7.7635e-06,  6.7651e-06,  ..., -8.1211e-06,
         -6.6459e-06, -6.0201e-06]], device='cuda:0')
Loss: 0.9772754311561584


Running epoch 1, step 1299, batch 251
Sampled inputs[:2]: tensor([[   0, 2914,  352,  ...,  897,  328, 1679],
        [   0, 3658,  271,  ...,  278,  970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6547e-04, -9.8877e-05, -3.1466e-04,  ...,  7.1440e-05,
         -4.7756e-04,  1.7760e-04],
        [-5.7742e-06, -4.1351e-06,  3.8110e-06,  ..., -4.7609e-06,
         -3.7216e-06, -4.0121e-06],
        [ 6.1361e-05,  1.2948e-04, -2.6793e-06,  ...,  3.2433e-05,
          8.9448e-05,  1.2664e-05],
        [-1.5929e-05, -1.1310e-05,  1.0952e-05,  ..., -1.3202e-05,
         -1.0312e-05, -1.1474e-05],
        [-1.3292e-05, -1.0282e-05,  9.0301e-06,  ..., -1.1221e-05,
         -9.2238e-06, -8.7470e-06]], device='cuda:0')
Loss: 0.9836732149124146


Running epoch 1, step 1300, batch 252
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,   287,  1410,  ...,  1255,  1699,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5756e-04,  1.7702e-04, -1.6390e-04,  ...,  1.3297e-04,
         -6.3592e-04,  2.5649e-04],
        [-7.2569e-06, -5.1484e-06,  4.6231e-06,  ..., -6.0052e-06,
         -4.7572e-06, -5.1595e-06],
        [ 5.7009e-05,  1.2640e-04, -1.6100e-07,  ...,  2.8827e-05,
          8.6453e-05,  9.3556e-06],
        [-2.0131e-05, -1.4126e-05,  1.3337e-05,  ..., -1.6734e-05,
         -1.3247e-05, -1.4842e-05],
        [-1.6987e-05, -1.2994e-05,  1.1146e-05,  ..., -1.4350e-05,
         -1.1921e-05, -1.1429e-05]], device='cuda:0')
Loss: 0.9565364122390747


Running epoch 1, step 1301, batch 253
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  266,  818,  278],
        [   0,  741,  300,  ...,   83, 7111,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2432e-04,  2.6661e-04, -3.5194e-05,  ...,  1.5671e-04,
         -8.4374e-04,  2.3690e-04],
        [-8.8736e-06, -6.0685e-06,  5.4203e-06,  ..., -7.3388e-06,
         -5.7481e-06, -6.4932e-06],
        [ 5.2450e-05,  1.2364e-04,  2.2679e-06,  ...,  2.5072e-05,
          8.3622e-05,  5.6303e-06],
        [-2.4095e-05, -1.6421e-05,  1.5423e-05,  ..., -2.0027e-05,
         -1.5721e-05, -1.8135e-05],
        [-2.1070e-05, -1.5467e-05,  1.3232e-05,  ..., -1.7747e-05,
         -1.4573e-05, -1.4633e-05]], device='cuda:0')
Loss: 0.9404636025428772


Running epoch 1, step 1302, batch 254
Sampled inputs[:2]: tensor([[    0,   199, 11296,  ...,   266, 10463,  8256],
        [    0,   374,  5195,  ...,   266,  5555,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2943e-04,  2.6301e-04, -5.0501e-06,  ...,  4.3247e-05,
         -6.0993e-04,  2.6433e-04],
        [-1.0282e-05, -7.1116e-06,  6.2510e-06,  ..., -8.5756e-06,
         -6.7838e-06, -7.5065e-06],
        [ 4.8575e-05,  1.2064e-04,  4.7266e-06,  ...,  2.1704e-05,
          8.0761e-05,  2.9183e-06],
        [-2.7999e-05, -1.9297e-05,  1.7852e-05,  ..., -2.3469e-05,
         -1.8626e-05, -2.1055e-05],
        [-2.4393e-05, -1.8150e-05,  1.5333e-05,  ..., -2.0683e-05,
         -1.7166e-05, -1.6809e-05]], device='cuda:0')
Loss: 0.9698105454444885


Running epoch 1, step 1303, batch 255
Sampled inputs[:2]: tensor([[   0,  669,   14,  ...,  596,  292,  494],
        [   0,  759, 1184,  ...,  472,  346,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0075e-04,  3.4891e-04,  6.8656e-05,  ...,  2.9757e-04,
         -1.0537e-03,  1.0736e-04],
        [-1.1750e-05, -8.0429e-06,  7.1637e-06,  ..., -9.7975e-06,
         -7.6666e-06, -8.6017e-06],
        [ 4.4284e-05,  1.1781e-04,  7.5727e-06,  ...,  1.8158e-05,
          7.8183e-05, -2.4074e-07],
        [-3.2201e-05, -2.1905e-05,  2.0593e-05,  ..., -2.6941e-05,
         -2.1145e-05, -2.4259e-05],
        [-2.7880e-05, -2.0549e-05,  1.7598e-05,  ..., -2.3618e-05,
         -1.9416e-05, -1.9237e-05]], device='cuda:0')
Loss: 0.9496271014213562
Graident accumulation at epoch 1, step 1303, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1124e-05,  1.2580e-04, -1.3749e-04,  ...,  7.6359e-05,
         -1.8209e-04,  1.6783e-06],
        [-7.5356e-06, -2.4500e-06,  4.4565e-06,  ..., -7.3019e-06,
         -2.3731e-06, -5.6143e-06],
        [ 2.8182e-05,  4.6375e-05, -2.1021e-05,  ...,  2.4222e-05,
          4.0760e-05,  3.0010e-06],
        [-2.5011e-05, -1.4533e-05,  1.6651e-05,  ..., -2.1192e-05,
         -1.4057e-05, -2.0177e-05],
        [-2.2083e-05, -1.7184e-05,  1.4423e-05,  ..., -1.8666e-05,
         -1.6088e-05, -1.4791e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2905e-08, 5.4441e-08, 4.1458e-08,  ..., 3.9515e-08, 1.1666e-07,
         4.9777e-08],
        [6.8055e-11, 5.4828e-11, 1.4594e-11,  ..., 4.9750e-11, 2.5006e-11,
         2.2079e-11],
        [3.8879e-09, 2.5629e-09, 1.1707e-09,  ..., 3.0163e-09, 1.0820e-09,
         9.8462e-10],
        [9.3910e-10, 8.6339e-10, 2.3947e-10,  ..., 7.8204e-10, 3.9432e-10,
         3.3741e-10],
        [3.5173e-10, 2.0075e-10, 7.0231e-11,  ..., 2.6427e-10, 7.0569e-11,
         9.8317e-11]], device='cuda:0')
optimizer state dict: 163.0
lr: [6.599295247432596e-06, 6.599295247432596e-06]
scheduler_last_epoch: 163


Running epoch 1, step 1304, batch 256
Sampled inputs[:2]: tensor([[   0,  341,  298,  ...,  298, 1304,  292],
        [   0, 2698,  221,  ..., 8352, 5680,  782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7782e-05,  2.2907e-04,  1.2984e-04,  ...,  5.0639e-05,
         -1.8088e-04,  5.2483e-05],
        [-1.6093e-06, -9.3132e-07,  7.8976e-07,  ..., -1.3262e-06,
         -1.0282e-06, -1.2890e-06],
        [-4.5002e-06, -2.7865e-06,  2.4140e-06,  ..., -3.7104e-06,
         -2.9206e-06, -3.5763e-06],
        [-4.1723e-06, -2.4140e-06,  2.1756e-06,  ..., -3.4571e-06,
         -2.7120e-06, -3.4124e-06],
        [-4.0233e-06, -2.5332e-06,  2.0862e-06,  ..., -3.3677e-06,
         -2.7418e-06, -3.0398e-06]], device='cuda:0')
Loss: 0.9403321146965027


Running epoch 1, step 1305, batch 257
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,  2297,   287,  ..., 10826, 13886,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3960e-06,  1.1438e-04,  2.4585e-04,  ..., -3.7051e-05,
          5.0927e-05,  1.8264e-04],
        [-2.9728e-06, -1.9670e-06,  1.6168e-06,  ..., -2.5108e-06,
         -1.9632e-06, -2.1979e-06],
        [-8.3745e-06, -5.8860e-06,  4.9919e-06,  ..., -7.0632e-06,
         -5.5879e-06, -6.1691e-06],
        [-7.9870e-06, -5.3346e-06,  4.6641e-06,  ..., -6.7949e-06,
         -5.3644e-06, -6.1095e-06],
        [-7.1228e-06, -5.1260e-06,  4.1127e-06,  ..., -6.1095e-06,
         -5.0217e-06, -4.9621e-06]], device='cuda:0')
Loss: 1.0002638101577759


Running epoch 1, step 1306, batch 258
Sampled inputs[:2]: tensor([[    0,   221,  4070,  ...,  1061,  3189,    26],
        [    0,  2088,  1745,  ...,   293, 16489,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9580e-05,  2.1478e-04,  3.4079e-04,  ...,  1.4256e-05,
         -1.5239e-05,  2.1396e-04],
        [-4.4778e-06, -3.0398e-06,  2.4103e-06,  ..., -3.8370e-06,
         -3.0361e-06, -3.3379e-06],
        [-1.2666e-05, -9.0301e-06,  7.4506e-06,  ..., -1.0788e-05,
         -8.6129e-06, -9.3430e-06],
        [-1.2040e-05, -8.1956e-06,  6.8843e-06,  ..., -1.0356e-05,
         -8.2701e-06, -9.2685e-06],
        [-1.0923e-05, -7.9572e-06,  6.2734e-06,  ..., -9.4175e-06,
         -7.7933e-06, -7.5847e-06]], device='cuda:0')
Loss: 0.9945887923240662


Running epoch 1, step 1307, batch 259
Sampled inputs[:2]: tensor([[   0, 2302,  287,  ..., 1522, 1666,  300],
        [   0, 1555,   12,  ...,  809,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8013e-05,  2.1478e-04,  1.7493e-04,  ...,  5.2523e-05,
         -1.5083e-04,  3.1813e-04],
        [-5.8562e-06, -4.0904e-06,  3.3528e-06,  ..., -4.9695e-06,
         -3.8967e-06, -4.2655e-06],
        [-1.6689e-05, -1.2234e-05,  1.0341e-05,  ..., -1.4111e-05,
         -1.1146e-05, -1.2055e-05],
        [-1.6004e-05, -1.1191e-05,  9.7156e-06,  ..., -1.3635e-05,
         -1.0759e-05, -1.2055e-05],
        [-1.4067e-05, -1.0565e-05,  8.5086e-06,  ..., -1.2055e-05,
         -9.9242e-06, -9.5516e-06]], device='cuda:0')
Loss: 0.9872450232505798


Running epoch 1, step 1308, batch 260
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,   857,   344,  ...,  1529,  9106,  1447]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7152e-05,  2.8972e-04,  1.5743e-04,  ...,  1.5291e-04,
         -2.4513e-04,  3.7142e-04],
        [-7.2569e-06, -5.0962e-06,  4.3213e-06,  ..., -6.1169e-06,
         -4.7609e-06, -5.1819e-06],
        [ 3.9051e-05,  5.7969e-05, -2.1706e-05,  ...,  6.5004e-05,
          3.4814e-05,  1.0948e-05],
        [-1.9968e-05, -1.4007e-05,  1.2591e-05,  ..., -1.6898e-05,
         -1.3217e-05, -1.4797e-05],
        [-1.7241e-05, -1.3068e-05,  1.0788e-05,  ..., -1.4722e-05,
         -1.2070e-05, -1.1489e-05]], device='cuda:0')
Loss: 0.9661649465560913


Running epoch 1, step 1309, batch 261
Sampled inputs[:2]: tensor([[    0,  3406,   300,  ...,  1726,  3521,  4481],
        [    0,   389, 18984,  ...,   287,   768,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6734e-07,  4.0405e-04,  1.5743e-04,  ...,  2.2014e-04,
          5.6890e-05,  5.9791e-04],
        [-8.6576e-06, -5.9940e-06,  5.1335e-06,  ..., -7.3686e-06,
         -5.6997e-06, -6.2250e-06],
        [ 3.5237e-05,  5.5406e-05, -1.9277e-05,  ...,  6.1682e-05,
          3.2295e-05,  8.2505e-06],
        [-2.3812e-05, -1.6391e-05,  1.4931e-05,  ..., -2.0325e-05,
         -1.5825e-05, -1.7732e-05],
        [-2.0504e-05, -1.5363e-05,  1.2845e-05,  ..., -1.7598e-05,
         -1.4365e-05, -1.3635e-05]], device='cuda:0')
Loss: 0.9617483019828796


Running epoch 1, step 1310, batch 262
Sampled inputs[:2]: tensor([[    0,   508,  3282,  ...,   334,   287, 31884],
        [    0,    12,   401,  ...,  7665,  4101, 10193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2677e-05,  3.4782e-04,  1.9203e-04,  ...,  1.9135e-04,
          1.2748e-04,  5.8775e-04],
        [-1.0028e-05, -6.9365e-06,  6.0238e-06,  ..., -8.5309e-06,
         -6.5230e-06, -7.2084e-06],
        [ 3.1452e-05,  5.2694e-05, -1.6625e-05,  ...,  5.8522e-05,
          2.9986e-05,  5.6279e-06],
        [-2.7537e-05, -1.8910e-05,  1.7509e-05,  ..., -2.3469e-05,
         -1.8090e-05, -2.0474e-05],
        [-2.3574e-05, -1.7658e-05,  1.4961e-05,  ..., -2.0206e-05,
         -1.6376e-05, -1.5616e-05]], device='cuda:0')
Loss: 0.9505035281181335


Running epoch 1, step 1311, batch 263
Sampled inputs[:2]: tensor([[    0,   300,  1635,  ...,   437,   266,  1136],
        [    0, 17508,    65,  ...,  8848, 13900,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4388e-05,  2.8789e-04,  1.2096e-04,  ...,  2.3107e-04,
          2.2662e-04,  6.5781e-04],
        [-1.1474e-05, -7.8380e-06,  6.8136e-06,  ..., -9.7975e-06,
         -7.5065e-06, -8.3558e-06],
        [ 2.7458e-05,  5.0071e-05, -1.4226e-05,  ...,  5.5080e-05,
          2.7304e-05,  2.5583e-06],
        [-3.1501e-05, -2.1368e-05,  1.9789e-05,  ..., -2.6941e-05,
         -2.0817e-05, -2.3723e-05],
        [-2.7001e-05, -1.9968e-05,  1.6987e-05,  ..., -2.3186e-05,
         -1.8790e-05, -1.8090e-05]], device='cuda:0')
Loss: 0.97621750831604
Graident accumulation at epoch 1, step 1311, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0140, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0573e-05,  1.4201e-04, -1.1165e-04,  ...,  9.1831e-05,
         -1.4122e-04,  6.7291e-05],
        [-7.9295e-06, -2.9888e-06,  4.6922e-06,  ..., -7.5514e-06,
         -2.8864e-06, -5.8884e-06],
        [ 2.8110e-05,  4.6744e-05, -2.0341e-05,  ...,  2.7308e-05,
          3.9414e-05,  2.9567e-06],
        [-2.5660e-05, -1.5216e-05,  1.6965e-05,  ..., -2.1767e-05,
         -1.4733e-05, -2.0531e-05],
        [-2.2575e-05, -1.7462e-05,  1.4679e-05,  ..., -1.9118e-05,
         -1.6358e-05, -1.5121e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2836e-08, 5.4470e-08, 4.1431e-08,  ..., 3.9529e-08, 1.1660e-07,
         5.0160e-08],
        [6.8119e-11, 5.4834e-11, 1.4626e-11,  ..., 4.9796e-11, 2.5038e-11,
         2.2127e-11],
        [3.8847e-09, 2.5628e-09, 1.1697e-09,  ..., 3.0163e-09, 1.0817e-09,
         9.8364e-10],
        [9.3915e-10, 8.6298e-10, 2.3963e-10,  ..., 7.8199e-10, 3.9436e-10,
         3.3763e-10],
        [3.5210e-10, 2.0095e-10, 7.0449e-11,  ..., 2.6454e-10, 7.0852e-11,
         9.8546e-11]], device='cuda:0')
optimizer state dict: 164.0
lr: [6.4833089778264036e-06, 6.4833089778264036e-06]
scheduler_last_epoch: 164


Running epoch 1, step 1312, batch 264
Sampled inputs[:2]: tensor([[    0,   273,    14,  ...,   271,   266, 25408],
        [    0,  1008,   266,  ...,  1941,   437,  1626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4394e-04, -1.8018e-04,  9.5684e-06,  ..., -2.1645e-04,
          4.3419e-04,  2.2095e-04],
        [-1.4529e-06, -9.4995e-07,  8.5309e-07,  ..., -1.1697e-06,
         -8.5682e-07, -1.1176e-06],
        [-4.2021e-06, -2.9504e-06,  2.6822e-06,  ..., -3.3677e-06,
         -2.5630e-06, -3.1739e-06],
        [-4.0233e-06, -2.6226e-06,  2.5034e-06,  ..., -3.2634e-06,
         -2.4438e-06, -3.2037e-06],
        [-3.3677e-06, -2.4587e-06,  2.1160e-06,  ..., -2.7418e-06,
         -2.1905e-06, -2.3842e-06]], device='cuda:0')
Loss: 0.9752464890480042


Running epoch 1, step 1313, batch 265
Sampled inputs[:2]: tensor([[    0,   421,  6007,  ...,   408,  2105,   843],
        [    0,  2165,  9311,  ..., 10570,   437,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4761e-04, -2.3320e-04, -6.2057e-05,  ..., -1.9754e-04,
          4.5914e-04,  3.0116e-04],
        [-2.7567e-06, -1.9185e-06,  1.7360e-06,  ..., -2.2948e-06,
         -1.6913e-06, -2.0526e-06],
        [-8.0019e-06, -5.8711e-06,  5.4240e-06,  ..., -6.6459e-06,
         -4.9919e-06, -5.8711e-06],
        [-7.8082e-06, -5.3793e-06,  5.2005e-06,  ..., -6.5416e-06,
         -4.8727e-06, -6.0350e-06],
        [-6.3181e-06, -4.8280e-06,  4.2170e-06,  ..., -5.3197e-06,
         -4.2021e-06, -4.3213e-06]], device='cuda:0')
Loss: 0.9636964797973633


Running epoch 1, step 1314, batch 266
Sampled inputs[:2]: tensor([[   0,  369, 4492,  ..., 9415, 4365,  352],
        [   0, 4868, 1027,  ...,  409, 3047, 2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7310e-04, -2.6865e-04, -1.1681e-04,  ..., -2.3849e-04,
          6.6317e-04,  3.7501e-04],
        [-4.1649e-06, -2.9393e-06,  2.6748e-06,  ..., -3.4422e-06,
         -2.5257e-06, -3.0361e-06],
        [-1.1936e-05, -8.8960e-06,  8.2403e-06,  ..., -9.8795e-06,
         -7.4059e-06, -8.6129e-06],
        [-1.1683e-05, -8.1956e-06,  7.9572e-06,  ..., -9.7305e-06,
         -7.2122e-06, -8.8513e-06],
        [-9.4622e-06, -7.3612e-06,  6.4373e-06,  ..., -7.9572e-06,
         -6.2883e-06, -6.3777e-06]], device='cuda:0')
Loss: 0.9815858602523804


Running epoch 1, step 1315, batch 267
Sampled inputs[:2]: tensor([[    0, 12440,   578,  ..., 25918,   287,   996],
        [    0,   445,    29,  ..., 20247,   272,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8866e-04, -3.0796e-04, -2.6695e-04,  ..., -1.4518e-04,
          5.6707e-04,  8.7204e-04],
        [-5.6252e-06, -3.7365e-06,  3.4720e-06,  ..., -4.6641e-06,
         -3.3155e-06, -4.2208e-06],
        [-1.6198e-05, -1.1429e-05,  1.0729e-05,  ..., -1.3441e-05,
         -9.7603e-06, -1.2025e-05],
        [-1.5706e-05, -1.0401e-05,  1.0282e-05,  ..., -1.3143e-05,
         -9.4473e-06, -1.2204e-05],
        [-1.2979e-05, -9.5069e-06,  8.4341e-06,  ..., -1.0937e-05,
         -8.3447e-06, -9.0450e-06]], device='cuda:0')
Loss: 0.9268976449966431


Running epoch 1, step 1316, batch 268
Sampled inputs[:2]: tensor([[    0, 24414,  4865,  ...,  8720,   344,  1566],
        [    0,   292,   380,  ...,   527, 37357,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8370e-04, -5.0745e-04, -3.4182e-04,  ..., -2.5842e-04,
          1.1217e-03,  9.7502e-04],
        [-6.8173e-06, -4.5411e-06,  4.2133e-06,  ..., -5.7220e-06,
         -4.0419e-06, -5.2489e-06],
        [-1.9565e-05, -1.3784e-05,  1.3113e-05,  ..., -1.6317e-05,
         -1.1772e-05, -1.4678e-05],
        [-1.9208e-05, -1.2711e-05,  1.2666e-05,  ..., -1.6212e-05,
         -1.1608e-05, -1.5229e-05],
        [-1.5706e-05, -1.1474e-05,  1.0327e-05,  ..., -1.3277e-05,
         -1.0096e-05, -1.1012e-05]], device='cuda:0')
Loss: 0.9368651509284973


Running epoch 1, step 1317, batch 269
Sampled inputs[:2]: tensor([[    0,   278,  1478,  ...,   266,  1607,  1220],
        [    0,   266,  2109,  ...,  6730, 11558,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5173e-04, -5.8894e-04, -4.4528e-04,  ..., -1.9126e-04,
          1.0788e-03,  1.0324e-03],
        [-8.2105e-06, -5.4762e-06,  5.1223e-06,  ..., -6.8545e-06,
         -4.8280e-06, -6.2026e-06],
        [-2.3380e-05, -1.6496e-05,  1.5795e-05,  ..., -1.9431e-05,
         -1.3962e-05, -1.7285e-05],
        [-2.3142e-05, -1.5303e-05,  1.5393e-05,  ..., -1.9401e-05,
         -1.3828e-05, -1.8030e-05],
        [-1.8775e-05, -1.3784e-05,  1.2442e-05,  ..., -1.5825e-05,
         -1.2033e-05, -1.2949e-05]], device='cuda:0')
Loss: 0.9806578755378723


Running epoch 1, step 1318, batch 270
Sampled inputs[:2]: tensor([[    0,   367,  3704,  ...,  1746,    14,   759],
        [    0,  5685,   565,  ..., 23968,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8676e-04, -6.2688e-04, -4.5808e-04,  ..., -2.7627e-04,
          1.3360e-03,  1.1602e-03],
        [-9.6187e-06, -6.4522e-06,  5.9307e-06,  ..., -8.0615e-06,
         -5.7779e-06, -7.2904e-06],
        [-2.7403e-05, -1.9386e-05,  1.8299e-05,  ..., -2.2829e-05,
         -1.6659e-05, -2.0370e-05],
        [-2.7165e-05, -1.8030e-05,  1.7807e-05,  ..., -2.2829e-05,
         -1.6540e-05, -2.1249e-05],
        [-2.2113e-05, -1.6257e-05,  1.4469e-05,  ..., -1.8686e-05,
         -1.4402e-05, -1.5318e-05]], device='cuda:0')
Loss: 0.9760141968727112


Running epoch 1, step 1319, batch 271
Sampled inputs[:2]: tensor([[   0, 2286, 1085,  ..., 1387, 1184, 1802],
        [   0, 1070, 5746,  ...,  278,  689,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8557e-04, -5.0470e-04, -5.2141e-04,  ..., -2.1547e-04,
          1.0910e-03,  1.1421e-03],
        [-1.1124e-05, -7.3873e-06,  6.6832e-06,  ..., -9.3356e-06,
         -6.7391e-06, -8.5048e-06],
        [-3.1635e-05, -2.2233e-05,  2.0668e-05,  ..., -2.6405e-05,
         -1.9416e-05, -2.3738e-05],
        [-3.1158e-05, -2.0504e-05,  1.9923e-05,  ..., -2.6241e-05,
         -1.9163e-05, -2.4542e-05],
        [-2.5809e-05, -1.8805e-05,  1.6481e-05,  ..., -2.1845e-05,
         -1.6965e-05, -1.8075e-05]], device='cuda:0')
Loss: 0.9214457273483276
Graident accumulation at epoch 1, step 1319, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.0413e-06,  7.7336e-05, -1.5262e-04,  ...,  6.1100e-05,
         -1.7993e-05,  1.7477e-04],
        [-8.2489e-06, -3.4286e-06,  4.8913e-06,  ..., -7.7298e-06,
         -3.2717e-06, -6.1501e-06],
        [ 2.2135e-05,  3.9847e-05, -1.6240e-05,  ...,  2.1937e-05,
          3.3531e-05,  2.8729e-07],
        [-2.6210e-05, -1.5745e-05,  1.7261e-05,  ..., -2.2214e-05,
         -1.5176e-05, -2.0932e-05],
        [-2.2898e-05, -1.7597e-05,  1.4859e-05,  ..., -1.9390e-05,
         -1.6419e-05, -1.5416e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2911e-08, 5.4670e-08, 4.1662e-08,  ..., 3.9536e-08, 1.1767e-07,
         5.1414e-08],
        [6.8174e-11, 5.4834e-11, 1.4656e-11,  ..., 4.9833e-11, 2.5058e-11,
         2.2177e-11],
        [3.8818e-09, 2.5608e-09, 1.1690e-09,  ..., 3.0140e-09, 1.0810e-09,
         9.8322e-10],
        [9.3918e-10, 8.6254e-10, 2.3978e-10,  ..., 7.8189e-10, 3.9434e-10,
         3.3790e-10],
        [3.5242e-10, 2.0111e-10, 7.0650e-11,  ..., 2.6476e-10, 7.1069e-11,
         9.8774e-11]], device='cuda:0')
optimizer state dict: 165.0
lr: [6.367860089306028e-06, 6.367860089306028e-06]
scheduler_last_epoch: 165


Running epoch 1, step 1320, batch 272
Sampled inputs[:2]: tensor([[    0,   292, 41192,  ..., 34298,  8741,   271],
        [    0,  7180,   266,  ...,  1805,    12,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3415e-04, -1.8151e-04,  9.7564e-06,  ..., -1.5424e-04,
          7.1738e-04,  2.1203e-04],
        [-1.5125e-06, -8.0094e-07,  7.4506e-07,  ..., -1.3113e-06,
         -9.2760e-07, -1.2517e-06],
        [-4.0829e-06, -2.3097e-06,  2.2501e-06,  ..., -3.4273e-06,
         -2.4736e-06, -3.2187e-06],
        [-3.9935e-06, -2.0564e-06,  2.0564e-06,  ..., -3.4422e-06,
         -2.4736e-06, -3.3379e-06],
        [-3.5614e-06, -2.1160e-06,  1.9521e-06,  ..., -3.0398e-06,
         -2.2948e-06, -2.6226e-06]], device='cuda:0')
Loss: 0.9754147529602051


Running epoch 1, step 1321, batch 273
Sampled inputs[:2]: tensor([[   0,  935, 2613,  ...,  623, 4289, 6803],
        [   0,  271,  266,  ..., 1034, 1928,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1611e-04, -1.8764e-04, -1.0069e-05,  ..., -1.2368e-04,
          6.8215e-04,  1.4949e-04],
        [-2.8983e-06, -1.7174e-06,  1.6578e-06,  ..., -2.4363e-06,
         -1.7732e-06, -2.1867e-06],
        [ 5.6387e-05,  6.9323e-05, -3.1014e-05,  ...,  9.8018e-05,
          6.1259e-05,  5.2092e-05],
        [-7.8976e-06, -4.5896e-06,  4.7386e-06,  ..., -6.6310e-06,
         -4.8727e-06, -6.0946e-06],
        [-6.7055e-06, -4.4107e-06,  4.0978e-06,  ..., -5.6475e-06,
         -4.3809e-06, -4.5896e-06]], device='cuda:0')
Loss: 0.9762359857559204


Running epoch 1, step 1322, batch 274
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,    13, 41550,  ...,    12,   546,  1996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3591e-04, -1.5627e-04,  2.3389e-05,  ..., -2.4789e-04,
          7.7586e-04,  1.5317e-04],
        [-4.4182e-06, -2.6785e-06,  2.4065e-06,  ..., -3.6880e-06,
         -2.6859e-06, -3.3118e-06],
        [ 5.2125e-05,  6.6387e-05, -2.8689e-05,  ...,  9.4486e-05,
          5.8577e-05,  4.8933e-05],
        [-1.1951e-05, -7.1973e-06,  6.8843e-06,  ..., -1.0014e-05,
         -7.4208e-06, -9.1940e-06],
        [-1.0341e-05, -6.9588e-06,  6.0201e-06,  ..., -8.7172e-06,
         -6.8098e-06, -7.1675e-06]], device='cuda:0')
Loss: 0.9441449046134949


Running epoch 1, step 1323, batch 275
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   352, 13107,  4302],
        [    0,  1943,  1837,  ...,   870,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4576e-05, -1.8869e-04, -9.7579e-05,  ..., -2.3451e-04,
          8.1902e-04,  2.8421e-04],
        [-5.8338e-06, -3.6694e-06,  3.3192e-06,  ..., -4.8205e-06,
         -3.5241e-06, -4.2580e-06],
        [ 4.8072e-05,  6.3422e-05, -2.5918e-05,  ...,  9.1223e-05,
          5.6133e-05,  4.6236e-05],
        [-1.5914e-05, -9.9689e-06,  9.5665e-06,  ..., -1.3202e-05,
         -9.7901e-06, -1.1936e-05],
        [-1.3500e-05, -9.3877e-06,  8.1658e-06,  ..., -1.1310e-05,
         -8.8662e-06, -9.1344e-06]], device='cuda:0')
Loss: 0.998993456363678


Running epoch 1, step 1324, batch 276
Sampled inputs[:2]: tensor([[    0,  5332,   266,  ...,   300,   259, 15369],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3806e-05, -2.3012e-04,  2.2594e-04,  ..., -2.6074e-04,
          9.7282e-04,  1.8092e-04],
        [-7.2047e-06, -4.6380e-06,  3.9265e-06,  ..., -6.0424e-06,
         -4.5002e-06, -5.3011e-06],
        [ 4.4228e-05,  6.0561e-05, -2.3921e-05,  ...,  8.7855e-05,
          5.3376e-05,  4.3405e-05],
        [-1.9729e-05, -1.2636e-05,  1.1370e-05,  ..., -1.6615e-05,
         -1.2591e-05, -1.4916e-05],
        [-1.6779e-05, -1.1876e-05,  9.8273e-06,  ..., -1.4216e-05,
         -1.1355e-05, -1.1399e-05]], device='cuda:0')
Loss: 0.9473991990089417


Running epoch 1, step 1325, batch 277
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,  3543,   391,  ...,  3370,  2926,  8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8378e-04, -2.3783e-04,  3.7666e-04,  ...,  9.7926e-05,
          8.4705e-04, -1.4511e-04],
        [-8.6352e-06, -5.5544e-06,  4.8280e-06,  ..., -7.2047e-06,
         -5.3458e-06, -6.2697e-06],
        [ 4.0323e-05,  5.7894e-05, -2.1284e-05,  ...,  8.4666e-05,
          5.0977e-05,  4.0797e-05],
        [-2.3663e-05, -1.5154e-05,  1.3992e-05,  ..., -1.9819e-05,
         -1.4976e-05, -1.7688e-05],
        [-1.9968e-05, -1.4186e-05,  1.1958e-05,  ..., -1.6868e-05,
         -1.3471e-05, -1.3396e-05]], device='cuda:0')
Loss: 0.9835338592529297


Running epoch 1, step 1326, batch 278
Sampled inputs[:2]: tensor([[   0, 5775,   12,  ...,   12, 1034, 9257],
        [   0,  300,  266,  ...,  271, 4111, 1188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1766e-04, -2.5355e-04,  3.5786e-04,  ...,  1.2661e-04,
          8.5364e-04, -7.2156e-05],
        [-9.9987e-06, -6.5155e-06,  5.7071e-06,  ..., -8.3596e-06,
         -6.1691e-06, -7.2159e-06],
        [ 3.6419e-05,  5.5018e-05, -1.8601e-05,  ...,  8.1373e-05,
          4.8608e-05,  3.8100e-05],
        [-2.7627e-05, -1.7881e-05,  1.6674e-05,  ..., -2.3156e-05,
         -1.7345e-05, -2.0549e-05],
        [-2.3097e-05, -1.6600e-05,  1.4059e-05,  ..., -1.9550e-05,
         -1.5527e-05, -1.5408e-05]], device='cuda:0')
Loss: 0.9615052342414856


Running epoch 1, step 1327, batch 279
Sampled inputs[:2]: tensor([[   0,  278, 5210,  ..., 1968, 2002,  923],
        [   0,   14, 1845,  ...,  806,  352,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9506e-04, -2.5967e-04,  3.3862e-04,  ...,  4.0795e-05,
          7.9529e-04, -8.3342e-05],
        [-1.1414e-05, -7.4506e-06,  6.5304e-06,  ..., -9.5889e-06,
         -7.0632e-06, -8.2664e-06],
        [ 3.2515e-05,  5.2291e-05, -1.6113e-05,  ...,  7.8006e-05,
          4.6119e-05,  3.5254e-05],
        [-3.1561e-05, -2.0429e-05,  1.9088e-05,  ..., -2.6584e-05,
         -1.9863e-05, -2.3559e-05],
        [-2.6390e-05, -1.9029e-05,  1.6145e-05,  ..., -2.2456e-05,
         -1.7807e-05, -1.7628e-05]], device='cuda:0')
Loss: 0.9574799537658691
Graident accumulation at epoch 1, step 1327, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.1343e-05,  4.3635e-05, -1.0350e-04,  ...,  5.9070e-05,
          6.3335e-05,  1.4896e-04],
        [-8.5654e-06, -3.8308e-06,  5.0552e-06,  ..., -7.9158e-06,
         -3.6508e-06, -6.3617e-06],
        [ 2.3173e-05,  4.1091e-05, -1.6227e-05,  ...,  2.7544e-05,
          3.4790e-05,  3.7839e-06],
        [-2.6745e-05, -1.6213e-05,  1.7443e-05,  ..., -2.2651e-05,
         -1.5645e-05, -2.1195e-05],
        [-2.3248e-05, -1.7740e-05,  1.4988e-05,  ..., -1.9697e-05,
         -1.6558e-05, -1.5637e-05]], device='cuda:0')
optimizer state dict: tensor([[7.2877e-08, 5.4683e-08, 4.1735e-08,  ..., 3.9498e-08, 1.1818e-07,
         5.1370e-08],
        [6.8236e-11, 5.4835e-11, 1.4684e-11,  ..., 4.9875e-11, 2.5083e-11,
         2.2223e-11],
        [3.8790e-09, 2.5609e-09, 1.1681e-09,  ..., 3.0171e-09, 1.0820e-09,
         9.8348e-10],
        [9.3924e-10, 8.6210e-10, 2.3991e-10,  ..., 7.8182e-10, 3.9434e-10,
         3.3811e-10],
        [3.5276e-10, 2.0127e-10, 7.0840e-11,  ..., 2.6500e-10, 7.1315e-11,
         9.8986e-11]], device='cuda:0')
optimizer state dict: 166.0
lr: [6.252966223469409e-06, 6.252966223469409e-06]
scheduler_last_epoch: 166


Running epoch 1, step 1328, batch 280
Sampled inputs[:2]: tensor([[    0, 23749, 27341,  ..., 34110,   342,  9672],
        [    0,   352,   357,  ...,   461,   654, 19725]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3112e-05, -6.2575e-05, -8.5069e-05,  ..., -1.0926e-05,
         -1.9704e-04,  9.2194e-05],
        [-1.3933e-06, -9.8348e-07,  9.9093e-07,  ..., -1.1399e-06,
         -7.3388e-07, -9.0152e-07],
        [ 6.1414e-05,  3.9322e-05, -6.9557e-05,  ...,  8.3871e-05,
          2.9921e-05,  2.0128e-05],
        [-3.9935e-06, -2.8163e-06,  2.9951e-06,  ..., -3.2932e-06,
         -2.1458e-06, -2.7120e-06],
        [-2.9951e-06, -2.3544e-06,  2.2054e-06,  ..., -2.5332e-06,
         -1.7881e-06, -1.8328e-06]], device='cuda:0')
Loss: 1.0018168687820435


Running epoch 1, step 1329, batch 281
Sampled inputs[:2]: tensor([[   0,   34, 3881,  ..., 1027,  271,  266],
        [   0, 1184,  271,  ..., 7225,  292,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6760e-04, -1.3654e-04, -2.1193e-04,  ...,  2.5149e-05,
         -1.8400e-04,  7.9824e-05],
        [-2.7418e-06, -1.9372e-06,  1.7956e-06,  ..., -2.3246e-06,
         -1.5572e-06, -1.9148e-06],
        [ 5.7361e-05,  3.6401e-05, -6.6949e-05,  ...,  8.0384e-05,
          2.7507e-05,  1.7163e-05],
        [-7.8976e-06, -5.4985e-06,  5.4687e-06,  ..., -6.6906e-06,
         -4.4852e-06, -5.7518e-06],
        [-6.2287e-06, -4.7833e-06,  4.2468e-06,  ..., -5.3495e-06,
         -3.8743e-06, -4.0382e-06]], device='cuda:0')
Loss: 0.973203182220459


Running epoch 1, step 1330, batch 282
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   508,  1853,    14],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5249e-04, -1.0233e-04, -5.2629e-05,  ...,  1.5842e-04,
         -1.4807e-04,  1.0122e-04],
        [-4.1053e-06, -2.8498e-06,  2.5891e-06,  ..., -3.4943e-06,
         -2.4289e-06, -2.8908e-06],
        [ 5.3456e-05,  3.3689e-05, -6.4491e-05,  ...,  7.7091e-05,
          2.5048e-05,  1.4406e-05],
        [-1.1742e-05, -8.0019e-06,  7.8380e-06,  ..., -9.9987e-06,
         -6.9439e-06, -8.6278e-06],
        [-9.4175e-06, -7.1228e-06,  6.2287e-06,  ..., -8.1062e-06,
         -6.0350e-06, -6.1244e-06]], device='cuda:0')
Loss: 0.9715093970298767


Running epoch 1, step 1331, batch 283
Sampled inputs[:2]: tensor([[    0,   344,   259,  ..., 47553,   287, 28978],
        [    0,  1985,   278,  ...,   677, 12292, 17956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0823e-04,  6.1181e-05, -1.1882e-04,  ...,  4.0685e-04,
         -3.4840e-04, -2.7930e-04],
        [-5.3942e-06, -3.7625e-06,  3.4347e-06,  ..., -4.6119e-06,
         -3.2447e-06, -3.8221e-06],
        [ 4.9880e-05,  3.1097e-05, -6.1957e-05,  ...,  7.4066e-05,
          2.2828e-05,  1.1947e-05],
        [-1.5393e-05, -1.0490e-05,  1.0356e-05,  ..., -1.3128e-05,
         -9.2536e-06, -1.1340e-05],
        [-1.2368e-05, -9.3579e-06,  8.2701e-06,  ..., -1.0639e-05,
         -8.0019e-06, -7.9945e-06]], device='cuda:0')
Loss: 0.9519305229187012


Running epoch 1, step 1332, batch 284
Sampled inputs[:2]: tensor([[    0, 17262,   342,  ...,   472,   346,   462],
        [    0,    29,   413,  ...,  1527,  1503,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5506e-04,  1.7967e-04,  7.3126e-07,  ...,  5.2672e-04,
         -3.4495e-04, -1.4070e-04],
        [-6.9216e-06, -4.7460e-06,  4.1537e-06,  ..., -5.8711e-06,
         -4.1984e-06, -5.0068e-06],
        [ 4.5529e-05,  2.8116e-05, -5.9663e-05,  ...,  7.0505e-05,
          2.0071e-05,  8.6392e-06],
        [-1.9446e-05, -1.3068e-05,  1.2383e-05,  ..., -1.6466e-05,
         -1.1817e-05, -1.4544e-05],
        [-1.6078e-05, -1.1981e-05,  1.0177e-05,  ..., -1.3739e-05,
         -1.0520e-05, -1.0632e-05]], device='cuda:0')
Loss: 0.9233353734016418


Running epoch 1, step 1333, batch 285
Sampled inputs[:2]: tensor([[    0,  2663,   328,  ...,   266,  1040,  1679],
        [    0, 20202,   300,  ..., 15185,   287,  6573]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2205e-04,  2.3096e-04,  1.2281e-04,  ...,  5.5313e-04,
         -2.8304e-04, -3.0549e-05],
        [-8.2701e-06, -5.6587e-06,  4.9844e-06,  ..., -6.9961e-06,
         -5.0031e-06, -6.0201e-06],
        [ 4.1595e-05,  2.5345e-05, -5.7040e-05,  ...,  6.7271e-05,
          1.7746e-05,  5.7186e-06],
        [-2.3291e-05, -1.5616e-05,  1.4901e-05,  ..., -1.9655e-05,
         -1.4082e-05, -1.7554e-05],
        [-1.9178e-05, -1.4260e-05,  1.2204e-05,  ..., -1.6361e-05,
         -1.2517e-05, -1.2778e-05]], device='cuda:0')
Loss: 0.9422337412834167


Running epoch 1, step 1334, batch 286
Sampled inputs[:2]: tensor([[    0, 47354,  5923,  ...,   266, 14679,  8137],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5241e-04,  2.7060e-04,  1.2976e-04,  ...,  6.3591e-04,
         -1.7384e-04, -1.1778e-05],
        [-9.5516e-06, -6.5118e-06,  5.7034e-06,  ..., -8.1807e-06,
         -5.8562e-06, -7.0408e-06],
        [ 3.8064e-05,  2.2856e-05, -5.4790e-05,  ...,  6.4127e-05,
          1.5467e-05,  3.0960e-06],
        [-2.6852e-05, -1.7896e-05,  1.7017e-05,  ..., -2.2948e-05,
         -1.6496e-05, -2.0504e-05],
        [-2.2113e-05, -1.6421e-05,  1.4059e-05,  ..., -1.8984e-05,
         -1.4544e-05, -1.4745e-05]], device='cuda:0')
Loss: 0.9499877691268921


Running epoch 1, step 1335, batch 287
Sampled inputs[:2]: tensor([[   0,   13,   41,  ...,    5,  271, 2936],
        [   0,  278,  266,  ...,   12,  850, 4952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6829e-04,  2.5761e-04,  7.6945e-05,  ...,  6.1658e-04,
         -1.0972e-04, -1.6283e-04],
        [-1.0960e-05, -7.5400e-06,  6.6645e-06,  ..., -9.3356e-06,
         -6.6683e-06, -7.9572e-06],
        [ 3.4070e-05,  1.9802e-05, -5.1929e-05,  ...,  6.0834e-05,
          1.3112e-05,  4.7341e-07],
        [-3.0756e-05, -2.0757e-05,  1.9819e-05,  ..., -2.6166e-05,
         -1.8790e-05, -2.3156e-05],
        [-2.5257e-05, -1.8939e-05,  1.6280e-05,  ..., -2.1622e-05,
         -1.6570e-05, -1.6667e-05]], device='cuda:0')
Loss: 0.9894549250602722
Graident accumulation at epoch 1, step 1335, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.6038e-05,  6.5032e-05, -8.5455e-05,  ...,  1.1482e-04,
          4.6029e-05,  1.1778e-04],
        [-8.8049e-06, -4.2017e-06,  5.2162e-06,  ..., -8.0577e-06,
         -3.9526e-06, -6.5213e-06],
        [ 2.4263e-05,  3.8962e-05, -1.9798e-05,  ...,  3.0873e-05,
          3.2622e-05,  3.4529e-06],
        [-2.7146e-05, -1.6668e-05,  1.7681e-05,  ..., -2.3003e-05,
         -1.5959e-05, -2.1391e-05],
        [-2.3449e-05, -1.7860e-05,  1.5117e-05,  ..., -1.9889e-05,
         -1.6559e-05, -1.5740e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3127e-08, 5.4694e-08, 4.1699e-08,  ..., 3.9839e-08, 1.1808e-07,
         5.1345e-08],
        [6.8288e-11, 5.4837e-11, 1.4714e-11,  ..., 4.9913e-11, 2.5102e-11,
         2.2264e-11],
        [3.8763e-09, 2.5588e-09, 1.1696e-09,  ..., 3.0178e-09, 1.0811e-09,
         9.8250e-10],
        [9.3925e-10, 8.6166e-10, 2.4006e-10,  ..., 7.8172e-10, 3.9430e-10,
         3.3831e-10],
        [3.5305e-10, 2.0142e-10, 7.1034e-11,  ..., 2.6520e-10, 7.1518e-11,
         9.9165e-11]], device='cuda:0')
optimizer state dict: 167.0
lr: [6.138644937102163e-06, 6.138644937102163e-06]
scheduler_last_epoch: 167


Running epoch 1, step 1336, batch 288
Sampled inputs[:2]: tensor([[    0,  5041,    14,  ...,  1027,  1722,  6554],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3454e-05, -2.4954e-04, -2.5702e-04,  ...,  3.0926e-05,
          1.2166e-04,  1.6567e-04],
        [-1.3486e-06, -9.0525e-07,  7.6368e-07,  ..., -1.1697e-06,
         -8.8289e-07, -1.0431e-06],
        [-3.8445e-06, -2.6673e-06,  2.3991e-06,  ..., -3.2485e-06,
         -2.4587e-06, -2.8312e-06],
        [-3.7998e-06, -2.4736e-06,  2.2650e-06,  ..., -3.3081e-06,
         -2.5183e-06, -3.0398e-06],
        [-3.1739e-06, -2.2650e-06,  1.9521e-06,  ..., -2.6971e-06,
         -2.1309e-06, -2.1458e-06]], device='cuda:0')
Loss: 0.9933009743690491


Running epoch 1, step 1337, batch 289
Sampled inputs[:2]: tensor([[    0,   446, 21112,  ..., 22092,    22,    27],
        [    0,  5896,   352,  ...,  1168,   767,  1390]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8278e-05, -6.0514e-04, -5.3202e-04,  ..., -1.2852e-04,
          3.2301e-04,  3.2188e-04],
        [-2.7940e-06, -1.8813e-06,  1.6578e-06,  ..., -2.3469e-06,
         -1.7881e-06, -2.0117e-06],
        [-8.0764e-06, -5.6624e-06,  5.1707e-06,  ..., -6.7055e-06,
         -5.1409e-06, -5.6326e-06],
        [-7.8827e-06, -5.2154e-06,  4.8876e-06,  ..., -6.6310e-06,
         -5.0664e-06, -5.8264e-06],
        [-6.6012e-06, -4.7833e-06,  4.1723e-06,  ..., -5.5432e-06,
         -4.4554e-06, -4.2915e-06]], device='cuda:0')
Loss: 1.031246542930603


Running epoch 1, step 1338, batch 290
Sampled inputs[:2]: tensor([[    0,   328, 16219,  ..., 14559,   351,   587],
        [    0,  3978,  2697,  ...,   461,  5955,  3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5293e-05, -6.1939e-04, -4.9504e-04,  ..., -1.5382e-04,
          4.7674e-04,  4.3199e-04],
        [-4.3139e-06, -2.8275e-06,  2.3842e-06,  ..., -3.6657e-06,
         -2.8089e-06, -3.1963e-06],
        [-1.2279e-05, -8.4043e-06,  7.3910e-06,  ..., -1.0297e-05,
         -7.9572e-06, -8.8066e-06],
        [-1.1966e-05, -7.7486e-06,  6.9588e-06,  ..., -1.0192e-05,
         -7.8529e-06, -9.1046e-06],
        [-1.0386e-05, -7.2718e-06,  6.1244e-06,  ..., -8.7917e-06,
         -7.0781e-06, -7.0035e-06]], device='cuda:0')
Loss: 0.9914767742156982


Running epoch 1, step 1339, batch 291
Sampled inputs[:2]: tensor([[    0,  1596,  2700,  ...,   943,   266,  4086],
        [    0,   221,   467,  ..., 21991,   630,  3990]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7721e-04, -6.5313e-04, -5.5722e-04,  ..., -1.9484e-04,
          3.3240e-04,  5.4786e-04],
        [-5.7518e-06, -3.7812e-06,  3.2745e-06,  ..., -4.8429e-06,
         -3.6061e-06, -4.1872e-06],
        [-1.6421e-05, -1.1310e-05,  1.0148e-05,  ..., -1.3694e-05,
         -1.0297e-05, -1.1623e-05],
        [-1.5959e-05, -1.0386e-05,  9.5665e-06,  ..., -1.3500e-05,
         -1.0088e-05, -1.1966e-05],
        [-1.3649e-05, -9.6709e-06,  8.2701e-06,  ..., -1.1519e-05,
         -9.0897e-06, -9.0897e-06]], device='cuda:0')
Loss: 0.9958831667900085


Running epoch 1, step 1340, batch 292
Sampled inputs[:2]: tensor([[   0, 3377,   12,  ...,  333,  199,  769],
        [   0,  298,  894,  ...,  266, 2904, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6492e-04, -8.2670e-04, -4.1422e-04,  ..., -5.2453e-04,
          7.0657e-04,  5.7203e-04],
        [-7.3686e-06, -4.6156e-06,  3.9898e-06,  ..., -6.1691e-06,
         -4.5039e-06, -5.5060e-06],
        [-2.0772e-05, -1.3828e-05,  1.2323e-05,  ..., -1.7241e-05,
         -1.2800e-05, -1.5110e-05],
        [-1.9982e-05, -1.2457e-05,  1.1459e-05,  ..., -1.6838e-05,
         -1.2413e-05, -1.5274e-05],
        [-1.7613e-05, -1.1981e-05,  1.0192e-05,  ..., -1.4782e-05,
         -1.1489e-05, -1.2085e-05]], device='cuda:0')
Loss: 0.8970615267753601


Running epoch 1, step 1341, batch 293
Sampled inputs[:2]: tensor([[    0,   278,  1253,  ...,   266,  1274, 22300],
        [    0,  5221,  7166,  ...,  4309,   342,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8357e-04, -9.6027e-04, -3.4507e-04,  ..., -5.3403e-04,
          7.8025e-04,  6.2138e-04],
        [-8.7917e-06, -5.6066e-06,  4.8392e-06,  ..., -7.3239e-06,
         -5.2862e-06, -6.5118e-06],
        [-2.4945e-05, -1.6883e-05,  1.5005e-05,  ..., -2.0579e-05,
         -1.5095e-05, -1.8016e-05],
        [-2.3887e-05, -1.5169e-05,  1.3918e-05,  ..., -2.0012e-05,
         -1.4573e-05, -1.8150e-05],
        [-2.0891e-05, -1.4454e-05,  1.2264e-05,  ..., -1.7449e-05,
         -1.3441e-05, -1.4216e-05]], device='cuda:0')
Loss: 0.9685230255126953


Running epoch 1, step 1342, batch 294
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,    13,   265, 49987],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1838e-04, -1.1037e-03, -1.6319e-04,  ..., -5.6893e-04,
          1.0050e-03,  8.0484e-04],
        [-1.0215e-05, -6.7241e-06,  5.7183e-06,  ..., -8.5309e-06,
         -6.2548e-06, -7.4357e-06],
        [-2.9027e-05, -2.0236e-05,  1.7673e-05,  ..., -2.4065e-05,
         -1.7896e-05, -2.0698e-05],
        [-2.7880e-05, -1.8314e-05,  1.6481e-05,  ..., -2.3425e-05,
         -1.7315e-05, -2.0877e-05],
        [-2.4199e-05, -1.7270e-05,  1.4395e-05,  ..., -2.0325e-05,
         -1.5870e-05, -1.6257e-05]], device='cuda:0')
Loss: 1.024755835533142


Running epoch 1, step 1343, batch 295
Sampled inputs[:2]: tensor([[   0, 1552,  300,  ..., 1085,   12,  298],
        [   0, 4485,  741,  ...,  292,  221,  341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6404e-04, -1.3293e-03, -1.8320e-04,  ..., -4.1035e-04,
          1.2939e-03,  8.6559e-04],
        [-1.1653e-05, -7.5735e-06,  6.5006e-06,  ..., -9.7826e-06,
         -7.1675e-06, -8.5905e-06],
        [-3.3051e-05, -2.2665e-05,  2.0072e-05,  ..., -2.7433e-05,
         -2.0355e-05, -2.3723e-05],
        [-3.1844e-05, -2.0564e-05,  1.8731e-05,  ..., -2.6822e-05,
         -1.9804e-05, -2.4080e-05],
        [-2.7657e-05, -1.9401e-05,  1.6436e-05,  ..., -2.3231e-05,
         -1.8090e-05, -1.8686e-05]], device='cuda:0')
Loss: 0.9395444989204407
Graident accumulation at epoch 1, step 1343, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3484e-04, -7.4402e-05, -9.5229e-05,  ...,  6.2303e-05,
          1.7082e-04,  1.9256e-04],
        [-9.0896e-06, -4.5389e-06,  5.3446e-06,  ..., -8.2302e-06,
         -4.2741e-06, -6.7282e-06],
        [ 1.8532e-05,  3.2799e-05, -1.5811e-05,  ...,  2.5042e-05,
          2.7324e-05,  7.3533e-07],
        [-2.7616e-05, -1.7057e-05,  1.7786e-05,  ..., -2.3385e-05,
         -1.6344e-05, -2.1660e-05],
        [-2.3869e-05, -1.8014e-05,  1.5249e-05,  ..., -2.0224e-05,
         -1.6712e-05, -1.6035e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3494e-08, 5.6407e-08, 4.1691e-08,  ..., 3.9967e-08, 1.1963e-07,
         5.2043e-08],
        [6.8356e-11, 5.4839e-11, 1.4741e-11,  ..., 4.9958e-11, 2.5128e-11,
         2.2316e-11],
        [3.8735e-09, 2.5567e-09, 1.1688e-09,  ..., 3.0155e-09, 1.0805e-09,
         9.8208e-10],
        [9.3932e-10, 8.6123e-10, 2.4017e-10,  ..., 7.8166e-10, 3.9429e-10,
         3.3855e-10],
        [3.5346e-10, 2.0160e-10, 7.1233e-11,  ..., 2.6547e-10, 7.1774e-11,
         9.9415e-11]], device='cuda:0')
optimizer state dict: 168.0
lr: [6.0249136994947676e-06, 6.0249136994947676e-06]
scheduler_last_epoch: 168


Running epoch 1, step 1344, batch 296
Sampled inputs[:2]: tensor([[   0, 4108,   85,  ...,   40,   12, 1530],
        [   0, 2793,  271,  ...,  374,  298,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3644e-04, -2.6065e-05, -6.9391e-06,  ..., -6.6920e-06,
          2.4446e-04,  1.5996e-04],
        [-1.3337e-06, -1.0207e-06,  7.5623e-07,  ..., -1.1995e-06,
         -9.5367e-07, -9.6858e-07],
        [-3.7551e-06, -2.9206e-06,  2.3544e-06,  ..., -3.2634e-06,
         -2.6226e-06, -2.6077e-06],
        [-3.7253e-06, -2.7716e-06,  2.2054e-06,  ..., -3.3528e-06,
         -2.6822e-06, -2.8014e-06],
        [-3.2037e-06, -2.5481e-06,  1.9968e-06,  ..., -2.8014e-06,
         -2.3395e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.984088122844696


Running epoch 1, step 1345, batch 297
Sampled inputs[:2]: tensor([[    0,   344,  8133,  ...,   278,  1603,   674],
        [    0,    14,  1075,  ..., 22182,  5948,  8401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9637e-04,  1.5747e-04,  3.7691e-05,  ...,  8.9162e-05,
          1.6676e-04,  1.6331e-04],
        [-2.7269e-06, -2.0191e-06,  1.5981e-06,  ..., -2.4140e-06,
         -1.8366e-06, -1.9297e-06],
        [-7.8380e-06, -5.9605e-06,  4.9919e-06,  ..., -6.7800e-06,
         -5.1856e-06, -5.4091e-06],
        [-7.6890e-06, -5.5581e-06,  4.7088e-06,  ..., -6.7949e-06,
         -5.1856e-06, -5.6773e-06],
        [-6.3926e-06, -5.0217e-06,  4.0084e-06,  ..., -5.6028e-06,
         -4.5002e-06, -4.0978e-06]], device='cuda:0')
Loss: 0.9544312357902527


Running epoch 1, step 1346, batch 298
Sampled inputs[:2]: tensor([[    0,   515,   352,  ..., 21190,  1871,   950],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6004e-04,  6.9298e-05,  1.8348e-07,  ...,  6.9828e-05,
          1.7273e-04,  3.1431e-04],
        [-4.1202e-06, -3.0249e-06,  2.4810e-06,  ..., -3.5614e-06,
         -2.6934e-06, -2.8163e-06],
        [ 7.4205e-05,  4.4240e-05, -4.0665e-05,  ...,  7.4458e-05,
          5.3691e-05,  9.6691e-06],
        [-1.1623e-05, -8.3894e-06,  7.3165e-06,  ..., -1.0043e-05,
         -7.6145e-06, -8.2701e-06],
        [-9.5516e-06, -7.5400e-06,  6.1095e-06,  ..., -8.2701e-06,
         -6.6459e-06, -5.9903e-06]], device='cuda:0')
Loss: 0.9864464998245239


Running epoch 1, step 1347, batch 299
Sampled inputs[:2]: tensor([[   0,  342, 4014,  ...,  368,  408, 2105],
        [   0,  300, 4402,  ..., 2013,   13, 6825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5714e-04, -4.2303e-05,  1.8259e-04,  ...,  9.7409e-05,
          2.8264e-04,  4.9451e-04],
        [-5.4464e-06, -4.0829e-06,  3.3192e-06,  ..., -4.7088e-06,
         -3.5390e-06, -3.7029e-06],
        [ 7.0301e-05,  4.1051e-05, -3.8057e-05,  ...,  7.1120e-05,
          5.1232e-05,  7.0912e-06],
        [-1.5378e-05, -1.1340e-05,  9.7901e-06,  ..., -1.3292e-05,
         -1.0014e-05, -1.0923e-05],
        [-1.2651e-05, -1.0163e-05,  8.1509e-06,  ..., -1.0952e-05,
         -8.7172e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.9848710298538208


Running epoch 1, step 1348, batch 300
Sampled inputs[:2]: tensor([[   0, 9829,  292,  ..., 2928, 1029,  271],
        [   0,  531, 9804,  ..., 1027,  360, 1576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4923e-04, -6.6850e-06,  3.5676e-04,  ...,  1.8348e-04,
          3.0830e-04,  5.4441e-04],
        [-6.7651e-06, -5.0962e-06,  4.0755e-06,  ..., -5.8934e-06,
         -4.4666e-06, -4.6268e-06],
        [ 6.6487e-05,  3.7996e-05, -3.5673e-05,  ...,  6.7722e-05,
          4.8565e-05,  4.4388e-06],
        [-1.9148e-05, -1.4201e-05,  1.2070e-05,  ..., -1.6689e-05,
         -1.2666e-05, -1.3679e-05],
        [-1.5810e-05, -1.2770e-05,  1.0103e-05,  ..., -1.3784e-05,
         -1.1042e-05, -9.9391e-06]], device='cuda:0')
Loss: 0.9770643711090088


Running epoch 1, step 1349, batch 301
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0, 14409, 45007,  ...,  1197,   266,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8340e-04, -3.2869e-06,  4.7709e-04,  ...,  7.4637e-05,
          2.8219e-04,  6.2088e-04],
        [-8.1584e-06, -6.1095e-06,  4.8801e-06,  ..., -7.0781e-06,
         -5.4352e-06, -5.6028e-06],
        [ 6.2463e-05,  3.4972e-05, -3.3229e-05,  ...,  6.4310e-05,
          4.5749e-05,  1.6224e-06],
        [ 1.1223e-04,  1.8711e-04, -1.0451e-04,  ...,  7.8757e-05,
          1.5416e-04,  5.7168e-05],
        [-1.9073e-05, -1.5318e-05,  1.2055e-05,  ..., -1.6615e-05,
         -1.3486e-05, -1.2100e-05]], device='cuda:0')
Loss: 1.003684163093567


Running epoch 1, step 1350, batch 302
Sampled inputs[:2]: tensor([[    0,    13, 20054,  ...,    19,     9,   266],
        [    0,  5105,   271,  ...,   308,  3056,  3640]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3448e-04,  3.6675e-05,  4.9487e-04,  ...,  7.8683e-05,
          1.5317e-04,  5.0862e-04],
        [-9.6634e-06, -7.0520e-06,  5.7444e-06,  ..., -8.2999e-06,
         -6.3293e-06, -6.6832e-06],
        [ 5.8142e-05,  3.2170e-05, -3.0562e-05,  ...,  6.0838e-05,
          4.3186e-05, -1.4323e-06],
        [ 1.0823e-04,  1.8469e-04, -1.0212e-04,  ...,  7.5539e-05,
          1.5181e-04,  5.4217e-05],
        [-2.2694e-05, -1.7747e-05,  1.4246e-05,  ..., -1.9565e-05,
         -1.5765e-05, -1.4499e-05]], device='cuda:0')
Loss: 0.9599867463111877


Running epoch 1, step 1351, batch 303
Sampled inputs[:2]: tensor([[    0,    19, 18798,  ...,    13, 17982,    20],
        [    0,   292, 15087,  ...,  2675,  1663,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6526e-04,  1.8947e-04,  7.6919e-04,  ...,  1.7475e-04,
          6.5873e-05,  2.3804e-04],
        [-1.1049e-05, -7.9088e-06,  6.5416e-06,  ..., -9.5069e-06,
         -7.1898e-06, -7.7039e-06],
        [ 5.4357e-05,  2.9667e-05, -2.8178e-05,  ...,  5.7590e-05,
          4.0816e-05, -4.1294e-06],
        [ 1.0444e-04,  1.8238e-04, -9.9812e-05,  ...,  7.2201e-05,
          1.4941e-04,  5.1341e-05],
        [-2.5854e-05, -1.9908e-05,  1.6168e-05,  ..., -2.2307e-05,
         -1.7866e-05, -1.6615e-05]], device='cuda:0')
Loss: 0.9433472752571106
Graident accumulation at epoch 1, step 1351, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0019],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.4829e-05, -4.8015e-05, -8.7870e-06,  ...,  7.3548e-05,
          1.6032e-04,  1.9711e-04],
        [-9.2856e-06, -4.8759e-06,  5.4643e-06,  ..., -8.3579e-06,
         -4.5657e-06, -6.8258e-06],
        [ 2.2114e-05,  3.2486e-05, -1.7047e-05,  ...,  2.8297e-05,
          2.8674e-05,  2.4885e-07],
        [-1.4411e-05,  2.8866e-06,  6.0261e-06,  ..., -1.3826e-05,
          2.3131e-07, -1.4360e-05],
        [-2.4068e-05, -1.8203e-05,  1.5341e-05,  ..., -2.0432e-05,
         -1.6828e-05, -1.6093e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3554e-08, 5.6386e-08, 4.2241e-08,  ..., 3.9958e-08, 1.1952e-07,
         5.2047e-08],
        [6.8410e-11, 5.4847e-11, 1.4769e-11,  ..., 4.9999e-11, 2.5155e-11,
         2.2353e-11],
        [3.8726e-09, 2.5550e-09, 1.1685e-09,  ..., 3.0158e-09, 1.0811e-09,
         9.8112e-10],
        [9.4929e-10, 8.9363e-10, 2.4989e-10,  ..., 7.8609e-10, 4.1622e-10,
         3.4085e-10],
        [3.5377e-10, 2.0179e-10, 7.1424e-11,  ..., 2.6571e-10, 7.2021e-11,
         9.9591e-11]], device='cuda:0')
optimizer state dict: 169.0
lr: [5.9117898897731115e-06, 5.9117898897731115e-06]
scheduler_last_epoch: 169


Running epoch 1, step 1352, batch 304
Sampled inputs[:2]: tensor([[   0, 1295, 1178,  ..., 4808,  287,  996],
        [   0, 4385,  342,  ..., 3644,  775,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6995e-05, -4.2632e-06, -1.2471e-04,  ...,  6.5213e-06,
         -2.0376e-04, -7.1863e-05],
        [-1.4007e-06, -1.0803e-06,  9.4622e-07,  ..., -1.1772e-06,
         -8.4937e-07, -8.2701e-07],
        [-3.9935e-06, -3.1590e-06,  2.8312e-06,  ..., -3.3528e-06,
         -2.4587e-06, -2.3842e-06],
        [-3.9339e-06, -3.0100e-06,  2.7716e-06,  ..., -3.2932e-06,
         -2.3693e-06, -2.4140e-06],
        [-3.1292e-06, -2.5779e-06,  2.1905e-06,  ..., -2.6822e-06,
         -2.0713e-06, -1.7509e-06]], device='cuda:0')
Loss: 1.0035825967788696


Running epoch 1, step 1353, batch 305
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  271,  669,   12],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1360e-05, -3.4823e-05, -1.3128e-04,  ...,  1.7014e-04,
         -3.7584e-04, -2.2755e-04],
        [-2.7418e-06, -2.1085e-06,  1.8515e-06,  ..., -2.2873e-06,
         -1.6652e-06, -1.6801e-06],
        [-7.8082e-06, -6.1542e-06,  5.5730e-06,  ..., -6.4969e-06,
         -4.7833e-06, -4.7833e-06],
        [-7.7188e-06, -5.8562e-06,  5.4538e-06,  ..., -6.4224e-06,
         -4.6641e-06, -4.9323e-06],
        [-6.1691e-06, -5.0515e-06,  4.3213e-06,  ..., -5.2005e-06,
         -4.0531e-06, -3.5092e-06]], device='cuda:0')
Loss: 0.9562370181083679


Running epoch 1, step 1354, batch 306
Sampled inputs[:2]: tensor([[   0, 1688,  790,  ...,  546,  696,   12],
        [   0,  278, 5492,  ...,  328,  995,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4074e-04, -9.0272e-05, -1.1625e-04,  ...,  1.9240e-04,
         -3.5908e-04, -1.8307e-04],
        [-4.2021e-06, -3.1814e-06,  2.8126e-06,  ..., -3.4943e-06,
         -2.6003e-06, -2.5891e-06],
        [ 3.0331e-04,  3.6311e-04, -1.7004e-04,  ...,  2.0816e-04,
          2.7350e-04,  8.6134e-05],
        [-1.1742e-05, -8.8364e-06,  8.1956e-06,  ..., -9.7752e-06,
         -7.2718e-06, -7.5549e-06],
        [-9.4175e-06, -7.6592e-06,  6.5416e-06,  ..., -7.9423e-06,
         -6.3032e-06, -5.4315e-06]], device='cuda:0')
Loss: 1.0050314664840698


Running epoch 1, step 1355, batch 307
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  266,  720,  342],
        [   0, 9611,  278,  ...,  278,  638,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2293e-04, -1.1624e-04, -2.4083e-04,  ...,  3.6469e-04,
         -4.9425e-04, -1.4594e-04],
        [-5.7146e-06, -4.1947e-06,  3.7067e-06,  ..., -4.7684e-06,
         -3.5763e-06, -3.6396e-06],
        [ 2.9902e-04,  3.6005e-04, -1.6733e-04,  ...,  2.0454e-04,
          2.7068e-04,  8.3184e-05],
        [-1.5855e-05, -1.1608e-05,  1.0744e-05,  ..., -1.3292e-05,
         -1.0014e-05, -1.0550e-05],
        [-1.2860e-05, -1.0192e-05,  8.6874e-06,  ..., -1.0878e-05,
         -8.7023e-06, -7.6666e-06]], device='cuda:0')
Loss: 0.978493332862854


Running epoch 1, step 1356, batch 308
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   631,  2820,   344],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5491e-05, -1.2360e-04, -2.0387e-04,  ...,  4.6823e-04,
         -5.7643e-04, -6.4618e-05],
        [-7.1302e-06, -5.1931e-06,  4.6194e-06,  ..., -5.9530e-06,
         -4.4443e-06, -4.5747e-06],
        [ 2.9522e-04,  3.5722e-04, -1.6467e-04,  ...,  2.0135e-04,
          2.6831e-04,  8.0680e-05],
        [-1.9640e-05, -1.4275e-05,  1.3322e-05,  ..., -1.6496e-05,
         -1.2383e-05, -1.3173e-05],
        [-1.5944e-05, -1.2591e-05,  1.0788e-05,  ..., -1.3500e-05,
         -1.0788e-05, -9.5591e-06]], device='cuda:0')
Loss: 0.9600107073783875


Running epoch 1, step 1357, batch 309
Sampled inputs[:2]: tensor([[    0, 27754,  3807,  ...,  3370,  3809,   360],
        [    0,   432,   984,  ...,   287,   496,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1081e-05, -2.4944e-04, -4.7932e-04,  ...,  4.7348e-04,
         -3.9820e-04, -7.6016e-05],
        [-8.5831e-06, -6.1691e-06,  5.4501e-06,  ..., -7.2196e-06,
         -5.4277e-06, -5.5507e-06],
        [ 2.9123e-04,  3.5443e-04, -1.6220e-04,  ...,  1.9790e-04,
          2.6564e-04,  7.8028e-05],
        [-2.3603e-05, -1.6883e-05,  1.5676e-05,  ..., -1.9982e-05,
         -1.5080e-05, -1.5944e-05],
        [-1.9312e-05, -1.5035e-05,  1.2815e-05,  ..., -1.6451e-05,
         -1.3188e-05, -1.1660e-05]], device='cuda:0')
Loss: 1.0106208324432373


Running epoch 1, step 1358, batch 310
Sampled inputs[:2]: tensor([[    0,   587,   292,  ...,    12,   287,  2261],
        [    0,   902, 11331,  ...,  1795,   365,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4684e-05, -1.7279e-04, -4.8419e-04,  ...,  4.3884e-04,
         -4.3135e-04, -7.6016e-05],
        [-1.0051e-05, -7.1973e-06,  6.3144e-06,  ..., -8.4788e-06,
         -6.3553e-06, -6.5342e-06],
        [ 2.8723e-04,  3.5147e-04, -1.5967e-04,  ...,  1.9445e-04,
          2.6307e-04,  7.5375e-05],
        [-2.7567e-05, -1.9655e-05,  1.8105e-05,  ..., -2.3440e-05,
         -1.7658e-05, -1.8731e-05],
        [-2.2531e-05, -1.7509e-05,  1.4827e-05,  ..., -1.9267e-05,
         -1.5393e-05, -1.3672e-05]], device='cuda:0')
Loss: 0.954975426197052


Running epoch 1, step 1359, batch 311
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,   259,  6022,  ...,  1871,  1209,  1241]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7155e-05, -3.3600e-04, -6.0921e-04,  ...,  4.6448e-04,
         -4.7056e-04, -1.2667e-04],
        [-1.1481e-05, -8.2180e-06,  7.2904e-06,  ..., -9.6262e-06,
         -7.2010e-06, -7.4394e-06],
        [ 3.4003e-04,  4.0297e-04, -1.9604e-04,  ...,  3.0149e-04,
          3.3221e-04,  1.1209e-04],
        [-3.1620e-05, -2.2501e-05,  2.0981e-05,  ..., -2.6703e-05,
         -2.0057e-05, -2.1398e-05],
        [-2.5764e-05, -2.0012e-05,  1.7136e-05,  ..., -2.1920e-05,
         -1.7479e-05, -1.5594e-05]], device='cuda:0')
Loss: 0.9938340783119202
Graident accumulation at epoch 1, step 1359, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.2061e-05, -7.6813e-05, -6.8830e-05,  ...,  1.1264e-04,
          9.7233e-05,  1.6473e-04],
        [-9.5052e-06, -5.2101e-06,  5.6469e-06,  ..., -8.4847e-06,
         -4.8292e-06, -6.8871e-06],
        [ 5.3905e-05,  6.9535e-05, -3.4946e-05,  ...,  5.5616e-05,
          5.9027e-05,  1.1433e-05],
        [-1.6132e-05,  3.4787e-07,  7.5216e-06,  ..., -1.5114e-05,
         -1.7975e-06, -1.5064e-05],
        [-2.4237e-05, -1.8384e-05,  1.5520e-05,  ..., -2.0581e-05,
         -1.6893e-05, -1.6043e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3484e-08, 5.6443e-08, 4.2569e-08,  ..., 4.0134e-08, 1.1962e-07,
         5.2011e-08],
        [6.8473e-11, 5.4860e-11, 1.4808e-11,  ..., 5.0041e-11, 2.5182e-11,
         2.2386e-11],
        [3.9843e-09, 2.7149e-09, 1.2057e-09,  ..., 3.1037e-09, 1.1903e-09,
         9.9270e-10],
        [9.4934e-10, 8.9324e-10, 2.5008e-10,  ..., 7.8602e-10, 4.1621e-10,
         3.4097e-10],
        [3.5408e-10, 2.0199e-10, 7.1646e-11,  ..., 2.6592e-10, 7.2255e-11,
         9.9735e-11]], device='cuda:0')
optimizer state dict: 170.0
lr: [5.799290794242787e-06, 5.799290794242787e-06]
scheduler_last_epoch: 170


Running epoch 1, step 1360, batch 312
Sampled inputs[:2]: tensor([[    0,  4852,   266,  ...,  2523,  2080,  2632],
        [    0,   607, 27288,  ...,   445,  4712,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3269e-05,  1.3778e-04,  9.0110e-05,  ..., -3.7724e-05,
          0.0000e+00,  4.0514e-05],
        [-1.4305e-06, -1.0133e-06,  8.1956e-07,  ..., -1.2070e-06,
         -9.7603e-07, -9.9093e-07],
        [-4.1425e-06, -3.0398e-06,  2.5332e-06,  ..., -3.4422e-06,
         -2.7865e-06, -2.8312e-06],
        [-3.9935e-06, -2.7716e-06,  2.3544e-06,  ..., -3.3826e-06,
         -2.7418e-06, -2.8908e-06],
        [-3.3975e-06, -2.5630e-06,  2.0266e-06,  ..., -2.8610e-06,
         -2.3991e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9741916060447693


Running epoch 1, step 1361, batch 313
Sampled inputs[:2]: tensor([[    0,  1410,   271,  ...,   259, 27726,  9533],
        [    0,   413,    16,  ...,   493,  2104,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9997e-06,  3.7897e-04,  4.9340e-04,  ...,  9.4674e-05,
         -1.4398e-04, -9.7880e-05],
        [-2.9281e-06, -2.0266e-06,  1.5013e-06,  ..., -2.5257e-06,
         -1.9968e-06, -2.1607e-06],
        [-8.5235e-06, -6.1691e-06,  4.7237e-06,  ..., -7.2867e-06,
         -5.8860e-06, -6.2138e-06],
        [-8.0466e-06, -5.5134e-06,  4.2617e-06,  ..., -7.0035e-06,
         -5.6177e-06, -6.1244e-06],
        [-7.1824e-06, -5.2750e-06,  3.9116e-06,  ..., -6.1989e-06,
         -5.1558e-06, -4.9472e-06]], device='cuda:0')
Loss: 0.9332370758056641


Running epoch 1, step 1362, batch 314
Sampled inputs[:2]: tensor([[    0,  1371,   287,  ...,   689,   278, 12774],
        [    0,   266,  7264,  ...,  3211,   328,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1638e-06,  3.0127e-04,  4.3828e-04,  ...,  1.5391e-04,
         -3.6696e-04, -1.0424e-04],
        [-4.3958e-06, -3.0622e-06,  2.3693e-06,  ..., -3.7402e-06,
         -2.9355e-06, -3.1367e-06],
        [-1.2726e-05, -9.2536e-06,  7.3910e-06,  ..., -1.0744e-05,
         -8.5533e-06, -9.0003e-06],
        [-1.2100e-05, -8.3297e-06,  6.7651e-06,  ..., -1.0341e-05,
         -8.2105e-06, -8.9258e-06],
        [-1.0595e-05, -7.8678e-06,  6.0275e-06,  ..., -9.0450e-06,
         -7.4655e-06, -7.0632e-06]], device='cuda:0')
Loss: 0.9693670868873596


Running epoch 1, step 1363, batch 315
Sampled inputs[:2]: tensor([[    0,   714,    14,  ...,  1501, 11397, 31940],
        [    0,   344, 14017,  ...,    65,   298,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2259e-06,  5.2657e-04,  3.7440e-04,  ...,  1.3900e-04,
         -6.6204e-04, -4.1909e-04],
        [-5.7891e-06, -4.0382e-06,  3.2634e-06,  ..., -4.9025e-06,
         -3.8110e-06, -4.0419e-06],
        [-1.6570e-05, -1.2055e-05,  1.0043e-05,  ..., -1.3933e-05,
         -1.0967e-05, -1.1474e-05],
        [-1.5870e-05, -1.0923e-05,  9.2983e-06,  ..., -1.3500e-05,
         -1.0580e-05, -1.1459e-05],
        [-1.3709e-05, -1.0207e-05,  8.1286e-06,  ..., -1.1668e-05,
         -9.5516e-06, -8.9258e-06]], device='cuda:0')
Loss: 0.9593697786331177


Running epoch 1, step 1364, batch 316
Sampled inputs[:2]: tensor([[    0,  2906, 46441,  ..., 39156,   287, 11452],
        [    0,  9466,    36,  ...,  1795,   437,   874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5308e-05,  5.0125e-04,  3.9786e-04,  ...,  8.3831e-05,
         -5.3711e-04, -1.8493e-04],
        [-7.1898e-06, -5.0515e-06,  4.0866e-06,  ..., -6.1318e-06,
         -4.7795e-06, -5.0552e-06],
        [-2.0593e-05, -1.5095e-05,  1.2591e-05,  ..., -1.7449e-05,
         -1.3798e-05, -1.4320e-05],
        [-1.9833e-05, -1.3784e-05,  1.1712e-05,  ..., -1.7002e-05,
         -1.3396e-05, -1.4409e-05],
        [-1.7062e-05, -1.2785e-05,  1.0245e-05,  ..., -1.4618e-05,
         -1.2010e-05, -1.1161e-05]], device='cuda:0')
Loss: 0.9866045713424683


Running epoch 1, step 1365, batch 317
Sampled inputs[:2]: tensor([[    0, 26138,    17,  ...,   401,  1867,  4977],
        [    0,    12,  4957,  ...,   944,   278,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8339e-05,  5.9115e-04,  3.8401e-04,  ...,  4.7729e-05,
         -5.5894e-04, -1.9440e-04],
        [-8.6054e-06, -6.0424e-06,  4.9360e-06,  ..., -7.3463e-06,
         -5.7556e-06, -6.0163e-06],
        [-2.4557e-05, -1.8016e-05,  1.5125e-05,  ..., -2.0832e-05,
         -1.6540e-05, -1.6987e-05],
        [-2.3797e-05, -1.6525e-05,  1.4156e-05,  ..., -2.0429e-05,
         -1.6168e-05, -1.7196e-05],
        [-2.0400e-05, -1.5303e-05,  1.2361e-05,  ..., -1.7509e-05,
         -1.4424e-05, -1.3292e-05]], device='cuda:0')
Loss: 0.9837014079093933


Running epoch 1, step 1366, batch 318
Sampled inputs[:2]: tensor([[    0,  3577,    12,  ...,  4222,  2137, 31332],
        [    0,    12,  3454,  ...,   717,  1765, 14906]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1972e-04,  7.7689e-04,  4.8272e-04,  ...,  1.1372e-04,
         -7.7671e-04, -1.8710e-04],
        [-1.0014e-05, -7.0855e-06,  5.7854e-06,  ..., -8.5756e-06,
         -6.7018e-06, -6.9439e-06],
        [-2.8521e-05, -2.1055e-05,  1.7688e-05,  ..., -2.4244e-05,
         -1.9178e-05, -1.9550e-05],
        [-2.7671e-05, -1.9372e-05,  1.6585e-05,  ..., -2.3842e-05,
         -1.8790e-05, -1.9848e-05],
        [-2.3633e-05, -1.7852e-05,  1.4417e-05,  ..., -2.0340e-05,
         -1.6704e-05, -1.5259e-05]], device='cuda:0')
Loss: 0.9725436568260193


Running epoch 1, step 1367, batch 319
Sampled inputs[:2]: tensor([[    0,   413,    20,  ...,  2089,    12, 21064],
        [    0, 17694,    12,  ..., 12452,   446,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2266e-04,  1.0930e-03,  6.0546e-04,  ...,  1.0144e-04,
         -9.4205e-04, -2.7367e-04],
        [-1.1481e-05, -8.1211e-06,  6.5640e-06,  ..., -9.8348e-06,
         -7.7374e-06, -7.9647e-06],
        [-3.2693e-05, -2.4140e-05,  2.0042e-05,  ..., -2.7791e-05,
         -2.2158e-05, -2.2426e-05],
        [-3.1665e-05, -2.2173e-05,  1.8775e-05,  ..., -2.7284e-05,
         -2.1651e-05, -2.2739e-05],
        [-2.7180e-05, -2.0564e-05,  1.6399e-05,  ..., -2.3395e-05,
         -1.9372e-05, -1.7598e-05]], device='cuda:0')
Loss: 0.9613447189331055
Graident accumulation at epoch 1, step 1367, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.1589e-05,  4.0167e-05, -1.4010e-06,  ...,  1.1152e-04,
         -6.6954e-06,  1.2089e-04],
        [-9.7028e-06, -5.5012e-06,  5.7386e-06,  ..., -8.6197e-06,
         -5.1200e-06, -6.9949e-06],
        [ 4.5246e-05,  6.0167e-05, -2.9447e-05,  ...,  4.7275e-05,
          5.0908e-05,  8.0472e-06],
        [-1.7685e-05, -1.9042e-06,  8.6470e-06,  ..., -1.6331e-05,
         -3.7829e-06, -1.5831e-05],
        [-2.4532e-05, -1.8602e-05,  1.5608e-05,  ..., -2.0862e-05,
         -1.7141e-05, -1.6199e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3460e-08, 5.7581e-08, 4.2893e-08,  ..., 4.0104e-08, 1.2039e-07,
         5.2034e-08],
        [6.8536e-11, 5.4871e-11, 1.4836e-11,  ..., 5.0088e-11, 2.5216e-11,
         2.2427e-11],
        [3.9814e-09, 2.7127e-09, 1.2049e-09,  ..., 3.1013e-09, 1.1896e-09,
         9.9221e-10],
        [9.4939e-10, 8.9284e-10, 2.5019e-10,  ..., 7.8598e-10, 4.1626e-10,
         3.4114e-10],
        [3.5447e-10, 2.0221e-10, 7.1843e-11,  ..., 2.6620e-10, 7.2558e-11,
         9.9945e-11]], device='cuda:0')
optimizer state dict: 171.0
lr: [5.687433603747612e-06, 5.687433603747612e-06]
scheduler_last_epoch: 171
Epoch 1 | Batch 319/1048 | Training PPL: 2188.031918608275 | time 29.708434343338013
Saving checkpoint at epoch 1, step 1367, batch 319
Epoch 1 | Validation PPL: 6.9112591400901415 | Learning rate: 5.687433603747612e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1367, AFTER epoch 1, step 1367


Running epoch 1, step 1368, batch 320
Sampled inputs[:2]: tensor([[   0,  292,   33,  ...,  352,  266, 9129],
        [   0,   12,  344,  ..., 2337, 1122,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5645e-05,  2.7846e-04,  1.5548e-04,  ...,  9.6360e-05,
         -2.5904e-04,  2.5643e-05],
        [-1.4678e-06, -9.6112e-07,  8.4937e-07,  ..., -1.2740e-06,
         -9.4622e-07, -1.0282e-06],
        [-4.0829e-06, -2.7865e-06,  2.5183e-06,  ..., -3.5018e-06,
         -2.6375e-06, -2.7865e-06],
        [-3.9637e-06, -2.5481e-06,  2.3991e-06,  ..., -3.4720e-06,
         -2.5779e-06, -2.8461e-06],
        [-3.5614e-06, -2.4736e-06,  2.1160e-06,  ..., -3.0845e-06,
         -2.3991e-06, -2.3246e-06]], device='cuda:0')
Loss: 0.9523366093635559


Running epoch 1, step 1369, batch 321
Sampled inputs[:2]: tensor([[    0,  1250,  1797,  ...,   266,  1417,   367],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1959e-04,  3.1485e-04,  1.3086e-04,  ...,  9.1613e-05,
         -4.0997e-04,  3.7192e-05],
        [-2.8834e-06, -1.9446e-06,  1.7174e-06,  ..., -2.4736e-06,
         -1.8142e-06, -1.9819e-06],
        [-8.1956e-06, -5.7369e-06,  5.2005e-06,  ..., -6.9588e-06,
         -5.1558e-06, -5.5432e-06],
        [-7.9572e-06, -5.2750e-06,  4.9472e-06,  ..., -6.8396e-06,
         -5.0068e-06, -5.6177e-06],
        [-6.8247e-06, -4.9025e-06,  4.2170e-06,  ..., -5.8711e-06,
         -4.5449e-06, -4.3809e-06]], device='cuda:0')
Loss: 0.98116534948349


Running epoch 1, step 1370, batch 322
Sampled inputs[:2]: tensor([[    0,   328,   471,  ..., 11137,   679,  6585],
        [    0,   299,   292,  ...,   266,  2474,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2569e-05,  4.6729e-04,  1.3542e-04,  ...,  6.3151e-05,
         -2.7391e-04, -3.1287e-05],
        [-4.2990e-06, -2.9728e-06,  2.5630e-06,  ..., -3.7253e-06,
         -2.7828e-06, -2.9504e-06],
        [-1.2189e-05, -8.7321e-06,  7.7486e-06,  ..., -1.0490e-05,
         -7.9125e-06, -8.2701e-06],
        [-1.1891e-05, -8.0764e-06,  7.3910e-06,  ..., -1.0327e-05,
         -7.7188e-06, -8.4043e-06],
        [-1.0118e-05, -7.4357e-06,  6.2883e-06,  ..., -8.8066e-06,
         -6.9439e-06, -6.4969e-06]], device='cuda:0')
Loss: 0.9941127896308899


Running epoch 1, step 1371, batch 323
Sampled inputs[:2]: tensor([[    0,   344,   259,  ...,  6787, 10045,  9799],
        [    0,   367,  6267,  ...,     9,   287, 17056]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9356e-05,  5.6935e-04,  1.7318e-06,  ...,  1.4710e-04,
         -6.3609e-04, -2.3804e-04],
        [-5.6699e-06, -3.9265e-06,  3.4682e-06,  ..., -4.8950e-06,
         -3.6471e-06, -3.8445e-06],
        [-1.6123e-05, -1.1548e-05,  1.0505e-05,  ..., -1.3784e-05,
         -1.0371e-05, -1.0759e-05],
        [-1.5676e-05, -1.0639e-05,  1.0014e-05,  ..., -1.3530e-05,
         -1.0103e-05, -1.0952e-05],
        [-1.3232e-05, -9.7454e-06,  8.4341e-06,  ..., -1.1444e-05,
         -9.0152e-06, -8.3372e-06]], device='cuda:0')
Loss: 0.9614124298095703


Running epoch 1, step 1372, batch 324
Sampled inputs[:2]: tensor([[   0, 6538, 1805,  ...,  298,  271,  721],
        [   0, 1128, 3231,  ..., 8375,  199, 2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4401e-04,  6.3151e-04,  5.6789e-05,  ...,  1.9295e-04,
         -8.4149e-04, -2.5419e-04],
        [-7.1228e-06, -4.9323e-06,  4.2878e-06,  ..., -6.2063e-06,
         -4.6678e-06, -4.8876e-06],
        [-2.0295e-05, -1.4529e-05,  1.2979e-05,  ..., -1.7494e-05,
         -1.3277e-05, -1.3724e-05],
        [-1.9610e-05, -1.3307e-05,  1.2293e-05,  ..., -1.7077e-05,
         -1.2860e-05, -1.3843e-05],
        [-1.6809e-05, -1.2353e-05,  1.0520e-05,  ..., -1.4678e-05,
         -1.1638e-05, -1.0766e-05]], device='cuda:0')
Loss: 1.0006675720214844


Running epoch 1, step 1373, batch 325
Sampled inputs[:2]: tensor([[    0,    26,   874,  ...,    12, 21591,    12],
        [    0,  3592,   417,  ...,  4893,   328,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9730e-04,  6.5630e-04,  1.4943e-04,  ...,  2.0332e-04,
         -9.6278e-04, -2.5944e-04],
        [-8.5533e-06, -5.9307e-06,  5.2266e-06,  ..., -7.3761e-06,
         -5.5581e-06, -5.8077e-06],
        [-2.4527e-05, -1.7583e-05,  1.5900e-05,  ..., -2.0966e-05,
         -1.5929e-05, -1.6466e-05],
        [-2.3603e-05, -1.6063e-05,  1.5035e-05,  ..., -2.0355e-05,
         -1.5363e-05, -1.6525e-05],
        [-2.0042e-05, -1.4782e-05,  1.2711e-05,  ..., -1.7375e-05,
         -1.3813e-05, -1.2748e-05]], device='cuda:0')
Loss: 0.9957027435302734


Running epoch 1, step 1374, batch 326
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,  266, 2374,  ..., 1551,  518,  638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6153e-04,  6.1607e-04, -7.5304e-05,  ...,  7.0268e-05,
         -8.6108e-04, -3.8471e-04],
        [-9.9242e-06, -6.9216e-06,  6.1132e-06,  ..., -8.5682e-06,
         -6.5006e-06, -6.7689e-06],
        [-2.8312e-05, -2.0370e-05,  1.8507e-05,  ..., -2.4199e-05,
         -1.8477e-05, -1.9044e-05],
        [-2.7329e-05, -1.8686e-05,  1.7554e-05,  ..., -2.3589e-05,
         -1.7941e-05, -1.9252e-05],
        [-2.3142e-05, -1.7151e-05,  1.4812e-05,  ..., -2.0042e-05,
         -1.6019e-05, -1.4715e-05]], device='cuda:0')
Loss: 0.9545950889587402


Running epoch 1, step 1375, batch 327
Sampled inputs[:2]: tensor([[   0,   12, 2735,  ...,   12,  344, 1496],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5587e-04,  6.2115e-04, -9.9031e-05,  ...,  1.0878e-05,
         -6.7583e-04, -4.6968e-04],
        [-1.1392e-05, -7.8715e-06,  6.9253e-06,  ..., -9.8720e-06,
         -7.5139e-06, -7.8939e-06],
        [-3.2336e-05, -2.3082e-05,  2.0936e-05,  ..., -2.7716e-05,
         -2.1234e-05, -2.2054e-05],
        [-3.1292e-05, -2.1204e-05,  1.9819e-05,  ..., -2.7120e-05,
         -2.0698e-05, -2.2352e-05],
        [-2.6599e-05, -1.9535e-05,  1.6853e-05,  ..., -2.3082e-05,
         -1.8492e-05, -1.7144e-05]], device='cuda:0')
Loss: 0.9735814929008484
Graident accumulation at epoch 1, step 1375, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.2017e-05,  9.8265e-05, -1.1164e-05,  ...,  1.0146e-04,
         -7.3609e-05,  6.1834e-05],
        [-9.8717e-06, -5.7382e-06,  5.8573e-06,  ..., -8.7450e-06,
         -5.3594e-06, -7.0848e-06],
        [ 3.7487e-05,  5.1842e-05, -2.4409e-05,  ...,  3.9776e-05,
          4.3694e-05,  5.0371e-06],
        [-1.9046e-05, -3.8342e-06,  9.7641e-06,  ..., -1.7410e-05,
         -5.4744e-06, -1.6483e-05],
        [-2.4738e-05, -1.8695e-05,  1.5733e-05,  ..., -2.1084e-05,
         -1.7276e-05, -1.6293e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3452e-08, 5.7909e-08, 4.2860e-08,  ..., 4.0064e-08, 1.2072e-07,
         5.2203e-08],
        [6.8598e-11, 5.4878e-11, 1.4869e-11,  ..., 5.0136e-11, 2.5248e-11,
         2.2467e-11],
        [3.9785e-09, 2.7106e-09, 1.2042e-09,  ..., 3.0990e-09, 1.1889e-09,
         9.9170e-10],
        [9.4942e-10, 8.9240e-10, 2.5033e-10,  ..., 7.8593e-10, 4.1627e-10,
         3.4130e-10],
        [3.5482e-10, 2.0239e-10, 7.2055e-11,  ..., 2.6647e-10, 7.2827e-11,
         1.0014e-10]], device='cuda:0')
optimizer state dict: 172.0
lr: [5.576235411042707e-06, 5.576235411042707e-06]
scheduler_last_epoch: 172


Running epoch 1, step 1376, batch 328
Sampled inputs[:2]: tensor([[    0,  9577,  2789,  ...,  1042,  9086,   623],
        [    0,  1665,  6306,  ...,   300, 10204,   582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6105e-05, -6.8243e-05, -1.0681e-04,  ...,  1.3748e-04,
         -1.2904e-04,  8.4210e-05],
        [-1.4231e-06, -1.0133e-06,  9.8348e-07,  ..., -1.1772e-06,
         -9.6112e-07, -9.1270e-07],
        [ 3.1481e-04,  3.4406e-04, -2.3262e-04,  ...,  2.5579e-04,
          3.2402e-04,  1.9083e-04],
        [-3.9339e-06, -2.7418e-06,  2.8014e-06,  ..., -3.2485e-06,
         -2.6524e-06, -2.6375e-06],
        [-3.2634e-06, -2.4885e-06,  2.3097e-06,  ..., -2.7269e-06,
         -2.3246e-06, -1.9521e-06]], device='cuda:0')
Loss: 0.9963335990905762


Running epoch 1, step 1377, batch 329
Sampled inputs[:2]: tensor([[    0, 21540,   527,  ...,   824,    14,   381],
        [    0,  2974,   278,  ...,   365,  8758,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9373e-05, -2.6585e-06,  9.4439e-05,  ...,  1.7489e-04,
         -1.2904e-04,  1.2716e-04],
        [-2.8759e-06, -1.9222e-06,  1.8142e-06,  ..., -2.4289e-06,
         -1.8217e-06, -1.9483e-06],
        [ 3.1055e-04,  3.4126e-04, -2.3006e-04,  ...,  2.5212e-04,
          3.2149e-04,  1.8780e-04],
        [-7.9870e-06, -5.2452e-06,  5.1707e-06,  ..., -6.7800e-06,
         -5.0813e-06, -5.6326e-06],
        [-6.7651e-06, -4.8280e-06,  4.3660e-06,  ..., -5.7667e-06,
         -4.5002e-06, -4.3213e-06]], device='cuda:0')
Loss: 0.9580563902854919


Running epoch 1, step 1378, batch 330
Sampled inputs[:2]: tensor([[   0, 3087,  342,  ...,   14,  381, 1416],
        [   0, 1611,  266,  ...,  266, 2673, 6277]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4045e-06, -4.4157e-06,  3.9421e-05,  ...,  1.2240e-04,
         -2.2600e-04,  1.7573e-04],
        [-4.2617e-06, -2.9430e-06,  2.7753e-06,  ..., -3.5763e-06,
         -2.6785e-06, -2.8312e-06],
        [ 3.8207e-04,  4.0530e-04, -2.8465e-04,  ...,  3.3208e-04,
          3.3865e-04,  2.1705e-04],
        [-1.1921e-05, -8.1062e-06,  8.0019e-06,  ..., -1.0043e-05,
         -7.4953e-06, -8.2552e-06],
        [-9.8944e-06, -7.3016e-06,  6.5863e-06,  ..., -8.4192e-06,
         -6.5863e-06, -6.2287e-06]], device='cuda:0')
Loss: 0.9992752075195312


Running epoch 1, step 1379, batch 331
Sampled inputs[:2]: tensor([[    0,  1477,    12,  ..., 31038,   408,   298],
        [    0,   287,   221,  ...,  1871,  1482,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9891e-06,  1.0969e-04,  2.3562e-04,  ...,  1.0565e-04,
         -4.3677e-04,  1.7573e-04],
        [-5.6922e-06, -3.9935e-06,  3.6843e-06,  ..., -4.7982e-06,
         -3.6620e-06, -3.7663e-06],
        [ 3.7814e-04,  4.0226e-04, -2.8197e-04,  ...,  3.2872e-04,
          3.3591e-04,  2.1452e-04],
        [-1.5795e-05, -1.0967e-05,  1.0580e-05,  ..., -1.3396e-05,
         -1.0237e-05, -1.0923e-05],
        [-1.3098e-05, -9.8646e-06,  8.7470e-06,  ..., -1.1191e-05,
         -8.9407e-06, -8.1658e-06]], device='cuda:0')
Loss: 0.9548306465148926


Running epoch 1, step 1380, batch 332
Sampled inputs[:2]: tensor([[    0,   437,  1119,  ..., 32831,    83,   623],
        [    0,     9,  9925,  ...,   527, 23286,  6062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0952e-05,  1.4900e-04,  5.2824e-04,  ...,  1.4144e-04,
         -5.1788e-04,  1.4232e-04],
        [-7.1377e-06, -4.9993e-06,  4.5002e-06,  ..., -6.0424e-06,
         -4.6603e-06, -4.8392e-06],
        [ 3.7391e-04,  3.9919e-04, -2.7941e-04,  ...,  3.2511e-04,
          3.3300e-04,  2.1139e-04],
        [-1.9789e-05, -1.3724e-05,  1.2934e-05,  ..., -1.6853e-05,
         -1.3024e-05, -1.3992e-05],
        [-1.6585e-05, -1.2457e-05,  1.0833e-05,  ..., -1.4201e-05,
         -1.1459e-05, -1.0595e-05]], device='cuda:0')
Loss: 0.9548569917678833


Running epoch 1, step 1381, batch 333
Sampled inputs[:2]: tensor([[   0,  531,   20,  ...,   12, 1644,  680],
        [   0,   12, 2085,  ...,  287,  593, 4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1185e-04,  2.2639e-04,  6.0562e-04,  ...,  2.0834e-04,
         -7.8335e-04,  4.3137e-05],
        [-8.5160e-06, -5.9903e-06,  5.4911e-06,  ..., -7.1898e-06,
         -5.5023e-06, -5.7220e-06],
        [ 3.6994e-04,  3.9625e-04, -2.7643e-04,  ...,  3.2183e-04,
          3.3059e-04,  2.0886e-04],
        [-2.3589e-05, -1.6406e-05,  1.5780e-05,  ..., -2.0012e-05,
         -1.5348e-05, -1.6540e-05],
        [-1.9580e-05, -1.4782e-05,  1.3053e-05,  ..., -1.6734e-05,
         -1.3426e-05, -1.2390e-05]], device='cuda:0')
Loss: 0.9477200508117676


Running epoch 1, step 1382, batch 334
Sampled inputs[:2]: tensor([[    0,   352,  2284,  ..., 43204,    12,   709],
        [    0,  1197, 10640,  ...,  2405,   437,  5880]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9206e-04,  2.2849e-04,  6.4153e-04,  ...,  9.1067e-05,
         -9.3628e-04,  6.7257e-05],
        [-9.9167e-06, -6.9961e-06,  6.5118e-06,  ..., -8.3447e-06,
         -6.4000e-06, -6.6310e-06],
        [ 3.6595e-04,  3.9326e-04, -2.7339e-04,  ...,  3.1851e-04,
          3.2798e-04,  2.0625e-04],
        [-2.7493e-05, -1.9178e-05,  1.8746e-05,  ..., -2.3246e-05,
         -1.7866e-05, -1.9178e-05],
        [-2.2739e-05, -1.7270e-05,  1.5423e-05,  ..., -1.9416e-05,
         -1.5646e-05, -1.4327e-05]], device='cuda:0')
Loss: 0.9937384724617004


Running epoch 1, step 1383, batch 335
Sampled inputs[:2]: tensor([[    0, 42306,   278,  ...,  1110,  3427,  4224],
        [    0,   266,  4287,  ...,   367,  4428,  2118]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1681e-04,  3.2729e-04,  5.9238e-04,  ...,  1.1934e-04,
         -7.6459e-04,  1.2690e-04],
        [-1.1340e-05, -7.9460e-06,  7.3798e-06,  ..., -9.5293e-06,
         -7.2606e-06, -7.6964e-06],
        [ 3.6193e-04,  3.9050e-04, -2.7075e-04,  ...,  3.1522e-04,
          3.2557e-04,  2.0336e-04],
        [-3.1233e-05, -2.1636e-05,  2.1130e-05,  ..., -2.6360e-05,
         -2.0146e-05, -2.2009e-05],
        [-2.5943e-05, -1.9521e-05,  1.7479e-05,  ..., -2.2069e-05,
         -1.7688e-05, -1.6473e-05]], device='cuda:0')
Loss: 0.9477413296699524
Graident accumulation at epoch 1, step 1383, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.6497e-05,  1.2117e-04,  4.9190e-05,  ...,  1.0324e-04,
         -1.4271e-04,  6.8341e-05],
        [-1.0019e-05, -5.9590e-06,  6.0095e-06,  ..., -8.8234e-06,
         -5.5495e-06, -7.1459e-06],
        [ 6.9931e-05,  8.5708e-05, -4.9043e-05,  ...,  6.7320e-05,
          7.1881e-05,  2.4869e-05],
        [-2.0265e-05, -5.6145e-06,  1.0901e-05,  ..., -1.8305e-05,
         -6.9416e-06, -1.7036e-05],
        [-2.4859e-05, -1.8778e-05,  1.5907e-05,  ..., -2.1183e-05,
         -1.7317e-05, -1.6311e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3426e-08, 5.7958e-08, 4.3168e-08,  ..., 4.0038e-08, 1.2119e-07,
         5.2167e-08],
        [6.8658e-11, 5.4886e-11, 1.4909e-11,  ..., 5.0176e-11, 2.5275e-11,
         2.2504e-11],
        [4.1055e-09, 2.8603e-09, 1.2763e-09,  ..., 3.1953e-09, 1.2937e-09,
         1.0321e-09],
        [9.4945e-10, 8.9197e-10, 2.5053e-10,  ..., 7.8584e-10, 4.1626e-10,
         3.4145e-10],
        [3.5514e-10, 2.0257e-10, 7.2289e-11,  ..., 2.6669e-10, 7.3067e-11,
         1.0031e-10]], device='cuda:0')
optimizer state dict: 173.0
lr: [5.465713208182582e-06, 5.465713208182582e-06]
scheduler_last_epoch: 173


Running epoch 1, step 1384, batch 336
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,  5440,    13,  ...,  1878,   342,  2060]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8499e-05, -4.6827e-05, -3.3213e-05,  ..., -4.4310e-05,
          1.6874e-05, -1.1015e-05],
        [-1.3560e-06, -9.9093e-07,  9.0152e-07,  ..., -1.1548e-06,
         -9.4995e-07, -9.3132e-07],
        [-3.8445e-06, -2.9057e-06,  2.7120e-06,  ..., -3.2187e-06,
         -2.6673e-06, -2.5928e-06],
        [-3.6806e-06, -2.6524e-06,  2.5332e-06,  ..., -3.1292e-06,
         -2.5779e-06, -2.6226e-06],
        [-3.1441e-06, -2.4587e-06,  2.1756e-06,  ..., -2.6673e-06,
         -2.3097e-06, -1.9819e-06]], device='cuda:0')
Loss: 0.9797288775444031


Running epoch 1, step 1385, batch 337
Sampled inputs[:2]: tensor([[    0, 14867,   278,  ...,   674,   369,  4127],
        [    0,   298,   894,  ...,  7605,  3220,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9963e-05,  4.1695e-05,  3.3340e-05,  ..., -2.2838e-04,
          8.3793e-05,  6.9134e-05],
        [-2.8238e-06, -1.9595e-06,  1.8552e-06,  ..., -2.3991e-06,
         -1.8738e-06, -2.0042e-06],
        [-7.9870e-06, -5.7817e-06,  5.6028e-06,  ..., -6.7204e-06,
         -5.3346e-06, -5.6028e-06],
        [-7.5847e-06, -5.2303e-06,  5.2005e-06,  ..., -6.4820e-06,
         -5.1111e-06, -5.5730e-06],
        [-6.6310e-06, -4.9323e-06,  4.5151e-06,  ..., -5.6475e-06,
         -4.6641e-06, -4.3809e-06]], device='cuda:0')
Loss: 0.9599159359931946


Running epoch 1, step 1386, batch 338
Sampled inputs[:2]: tensor([[    0,  1477,  3205,  ...,  6441,  9363,   271],
        [    0, 10348,  2994,  ...,   266, 24089, 10607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8394e-04, -3.3801e-05, -9.0613e-05,  ..., -2.1436e-04,
         -3.9860e-05,  5.3625e-05],
        [-4.2170e-06, -3.0100e-06,  2.8685e-06,  ..., -3.5241e-06,
         -2.7940e-06, -2.8983e-06],
        [-1.2010e-05, -8.9407e-06,  8.6725e-06,  ..., -9.9838e-06,
         -8.0019e-06, -8.2105e-06],
        [-1.1429e-05, -8.0913e-06,  8.0913e-06,  ..., -9.5814e-06,
         -7.6443e-06, -8.1509e-06],
        [-9.7901e-06, -7.5251e-06,  6.8843e-06,  ..., -8.2552e-06,
         -6.9141e-06, -6.3032e-06]], device='cuda:0')
Loss: 0.9858667850494385


Running epoch 1, step 1387, batch 339
Sampled inputs[:2]: tensor([[   0, 1716,  271,  ...,  292,   78, 1365],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9162e-04, -1.0129e-04, -2.3206e-04,  ..., -2.4404e-04,
         -2.1389e-04,  4.6635e-05],
        [-5.6103e-06, -4.0010e-06,  3.8743e-06,  ..., -4.6864e-06,
         -3.6657e-06, -3.8333e-06],
        [-1.6063e-05, -1.1921e-05,  1.1742e-05,  ..., -1.3366e-05,
         -1.0550e-05, -1.0923e-05],
        [-1.5199e-05, -1.0729e-05,  1.0923e-05,  ..., -1.2740e-05,
         -9.9987e-06, -1.0774e-05],
        [-1.2919e-05, -9.9242e-06,  9.2089e-06,  ..., -1.0908e-05,
         -9.0152e-06, -8.2552e-06]], device='cuda:0')
Loss: 0.9839494824409485


Running epoch 1, step 1388, batch 340
Sampled inputs[:2]: tensor([[    0,   259,  1513,  ...,   275, 19511,  2350],
        [    0,   278,   634,  ...,   598,  1722,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7478e-04, -1.0700e-04, -1.5756e-04,  ..., -2.6615e-04,
         -1.4957e-04,  4.6635e-05],
        [-6.9812e-06, -4.9919e-06,  4.8056e-06,  ..., -5.8338e-06,
         -4.5151e-06, -4.7386e-06],
        [-2.0057e-05, -1.4886e-05,  1.4588e-05,  ..., -1.6704e-05,
         -1.3024e-05, -1.3575e-05],
        [-1.9133e-05, -1.3515e-05,  1.3709e-05,  ..., -1.6034e-05,
         -1.2428e-05, -1.3500e-05],
        [-1.6004e-05, -1.2323e-05,  1.1384e-05,  ..., -1.3545e-05,
         -1.1072e-05, -1.0177e-05]], device='cuda:0')
Loss: 0.9695950746536255


Running epoch 1, step 1389, batch 341
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,  4998,  4756,  5139],
        [    0,    12,  4567,  ...,  4154,  1799, 11883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5373e-04, -3.0194e-04, -3.6628e-04,  ..., -4.0201e-04,
          8.0302e-05,  1.0422e-04],
        [-8.2999e-06, -5.9828e-06,  5.7407e-06,  ..., -6.9365e-06,
         -5.3346e-06, -5.6215e-06],
        [-2.3931e-05, -1.7852e-05,  1.7524e-05,  ..., -1.9893e-05,
         -1.5363e-05, -1.6123e-05],
        [-2.2829e-05, -1.6227e-05,  1.6481e-05,  ..., -1.9103e-05,
         -1.4693e-05, -1.6078e-05],
        [-1.8999e-05, -1.4707e-05,  1.3605e-05,  ..., -1.6049e-05,
         -1.3039e-05, -1.2003e-05]], device='cuda:0')
Loss: 0.9483861923217773


Running epoch 1, step 1390, batch 342
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,  1125,   609,   292],
        [    0,   680,   401,  ...,  2872,   292, 23535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0185e-04, -1.9653e-04, -4.6629e-04,  ..., -3.5650e-04,
         -1.6200e-04, -7.1047e-05],
        [-9.6038e-06, -6.8806e-06,  6.6459e-06,  ..., -8.0317e-06,
         -6.1691e-06, -6.5342e-06],
        [-2.7463e-05, -2.0370e-05,  2.0191e-05,  ..., -2.2799e-05,
         -1.7613e-05, -1.8448e-05],
        [-2.6256e-05, -1.8522e-05,  1.9014e-05,  ..., -2.1979e-05,
         -1.6943e-05, -1.8567e-05],
        [-2.1875e-05, -1.6883e-05,  1.5736e-05,  ..., -1.8448e-05,
         -1.4991e-05, -1.3731e-05]], device='cuda:0')
Loss: 0.9063196778297424


Running epoch 1, step 1391, batch 343
Sampled inputs[:2]: tensor([[   0,  422,   14,  ...,  271, 1360,   12],
        [   0,  278,  266,  ...,  380, 4053,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7428e-04, -3.0690e-04, -4.9952e-04,  ..., -4.7104e-04,
         -1.6200e-04,  1.8023e-04],
        [-1.1094e-05, -7.8157e-06,  7.5363e-06,  ..., -9.3281e-06,
         -7.1526e-06, -7.6666e-06],
        [-3.1576e-05, -2.3067e-05,  2.2829e-05,  ..., -2.6315e-05,
         -2.0310e-05, -2.1473e-05],
        [-3.0190e-05, -2.0951e-05,  2.1473e-05,  ..., -2.5406e-05,
         -1.9550e-05, -2.1607e-05],
        [-2.5436e-05, -1.9252e-05,  1.7971e-05,  ..., -2.1547e-05,
         -1.7449e-05, -1.6220e-05]], device='cuda:0')
Loss: 0.9746834635734558
Graident accumulation at epoch 1, step 1391, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1527e-04,  7.8361e-05, -5.6808e-06,  ...,  4.5816e-05,
         -1.4464e-04,  7.9530e-05],
        [-1.0126e-05, -6.1447e-06,  6.1622e-06,  ..., -8.8739e-06,
         -5.7098e-06, -7.1980e-06],
        [ 5.9781e-05,  7.4831e-05, -4.1856e-05,  ...,  5.7956e-05,
          6.2662e-05,  2.0235e-05],
        [-2.1257e-05, -7.1481e-06,  1.1958e-05,  ..., -1.9015e-05,
         -8.2025e-06, -1.7493e-05],
        [-2.4917e-05, -1.8825e-05,  1.6114e-05,  ..., -2.1219e-05,
         -1.7330e-05, -1.6302e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3492e-08, 5.7995e-08, 4.3375e-08,  ..., 4.0220e-08, 1.2109e-07,
         5.2147e-08],
        [6.8712e-11, 5.4892e-11, 1.4950e-11,  ..., 5.0213e-11, 2.5301e-11,
         2.2540e-11],
        [4.1024e-09, 2.8580e-09, 1.2755e-09,  ..., 3.1928e-09, 1.2928e-09,
         1.0315e-09],
        [9.4941e-10, 8.9152e-10, 2.5074e-10,  ..., 7.8569e-10, 4.1623e-10,
         3.4157e-10],
        [3.5543e-10, 2.0274e-10, 7.2539e-11,  ..., 2.6689e-10, 7.3298e-11,
         1.0047e-10]], device='cuda:0')
optimizer state dict: 174.0
lr: [5.355883883924591e-06, 5.355883883924591e-06]
scheduler_last_epoch: 174


Running epoch 1, step 1392, batch 344
Sampled inputs[:2]: tensor([[    0,   935, 28368,  ...,   342,   259,  4600],
        [    0,  1497, 16170,  ...,  1888,  2350,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7515e-05, -8.5097e-05,  8.2935e-06,  ...,  1.1485e-06,
         -1.0028e-04,  6.6744e-05],
        [-1.4082e-06, -1.0431e-06,  1.0580e-06,  ..., -1.0952e-06,
         -8.1956e-07, -9.0525e-07],
        [-4.2319e-06, -3.2783e-06,  3.3379e-06,  ..., -3.3230e-06,
         -2.5034e-06, -2.7716e-06],
        [ 4.1722e-04,  4.8532e-04, -2.4475e-04,  ...,  3.8142e-04,
          3.9959e-04,  3.0821e-04],
        [-3.1292e-06, -2.5183e-06,  2.4140e-06,  ..., -2.5183e-06,
         -1.9968e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.9921739101409912


Running epoch 1, step 1393, batch 345
Sampled inputs[:2]: tensor([[    0,   494,   298,  ...,   408, 32859, 14550],
        [    0,  4889,  3593,  ..., 19787,   287, 22475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1838e-05, -2.2103e-04,  2.7601e-05,  ..., -2.1412e-04,
         -3.5290e-06, -6.2027e-05],
        [-2.9057e-06, -1.9595e-06,  1.9334e-06,  ..., -2.3767e-06,
         -1.8030e-06, -2.0750e-06],
        [-8.4341e-06, -5.9605e-06,  6.0201e-06,  ..., -6.8396e-06,
         -5.2154e-06, -5.9605e-06],
        [ 4.1326e-04,  4.8295e-04, -2.4230e-04,  ...,  3.7806e-04,
          3.9702e-04,  3.0507e-04],
        [-6.8247e-06, -4.9174e-06,  4.6939e-06,  ..., -5.6624e-06,
         -4.5300e-06, -4.5598e-06]], device='cuda:0')
Loss: 0.9708999991416931


Running epoch 1, step 1394, batch 346
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0, 49141,    14,  ...,   342,   259,  1943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1293e-04, -5.8367e-06,  2.9926e-04,  ..., -1.5807e-04,
          4.2489e-05, -4.3291e-05],
        [-4.2021e-06, -2.9653e-06,  2.8647e-06,  ..., -3.5241e-06,
         -2.7306e-06, -3.0138e-06],
        [-1.2279e-05, -9.0450e-06,  8.9407e-06,  ..., -1.0192e-05,
         -7.9572e-06, -8.6725e-06],
        [ 4.0962e-04,  4.8020e-04, -2.3956e-04,  ...,  3.7484e-04,
          3.9438e-04,  3.0232e-04],
        [-9.9093e-06, -7.4655e-06,  6.9886e-06,  ..., -8.3745e-06,
         -6.8396e-06, -6.5714e-06]], device='cuda:0')
Loss: 0.982520580291748


Running epoch 1, step 1395, batch 347
Sampled inputs[:2]: tensor([[   0,   12,  287,  ..., 4626,   27,  292],
        [   0,  287,  552,  ..., 7407, 2401,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5166e-05,  7.7616e-05,  3.0947e-04,  ..., -5.9343e-05,
         -8.3336e-05, -4.8268e-06],
        [-5.5432e-06, -3.9041e-06,  3.8557e-06,  ..., -4.6343e-06,
         -3.6061e-06, -3.9302e-06],
        [-1.5944e-05, -1.1742e-05,  1.1876e-05,  ..., -1.3217e-05,
         -1.0356e-05, -1.1116e-05],
        [ 4.0608e-04,  4.7772e-04, -2.3676e-04,  ...,  3.7187e-04,
          3.9201e-04,  2.9978e-04],
        [-1.2785e-05, -9.7007e-06,  9.2238e-06,  ..., -1.0774e-05,
         -8.8662e-06, -8.3223e-06]], device='cuda:0')
Loss: 0.9569815397262573


Running epoch 1, step 1396, batch 348
Sampled inputs[:2]: tensor([[    0,   278,  4575,  ...,  1220,   278,  4575],
        [    0,   266, 27347,  ...,   368,  3367,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4832e-04,  2.6262e-04,  3.1586e-04,  ..., -7.9118e-05,
         -4.7514e-05, -5.5198e-05],
        [ 5.1773e-05,  5.1312e-05, -4.5384e-05,  ...,  5.8865e-05,
          8.5159e-05,  1.6921e-05],
        [-1.9595e-05, -1.4231e-05,  1.4827e-05,  ..., -1.6242e-05,
         -1.2606e-05, -1.3635e-05],
        [ 4.0245e-04,  4.7540e-04, -2.3389e-04,  ...,  3.6883e-04,
          3.8974e-04,  2.9709e-04],
        [-1.5631e-05, -1.1727e-05,  1.1474e-05,  ..., -1.3158e-05,
         -1.0751e-05, -1.0110e-05]], device='cuda:0')
Loss: 0.9496713876724243


Running epoch 1, step 1397, batch 349
Sampled inputs[:2]: tensor([[   0,   17, 2736,  ...,  352,  422,   13],
        [   0, 3630, 2199,  ..., 4157,   27, 4765]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6443e-04,  1.6195e-04,  4.3804e-04,  ..., -1.4562e-04,
          1.1469e-04, -6.5000e-05],
        [ 5.0388e-05,  5.0384e-05, -4.4493e-05,  ...,  5.7628e-05,
          8.4190e-05,  1.5908e-05],
        [-2.3350e-05, -1.6853e-05,  1.7434e-05,  ..., -1.9521e-05,
         -1.5140e-05, -1.6332e-05],
        [ 3.9876e-04,  4.7298e-04, -2.3141e-04,  ...,  3.6554e-04,
          3.8718e-04,  2.9425e-04],
        [-1.8939e-05, -1.4111e-05,  1.3724e-05,  ..., -1.6078e-05,
         -1.3120e-05, -1.2301e-05]], device='cuda:0')
Loss: 0.9592336416244507


Running epoch 1, step 1398, batch 350
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  391,  721, 1576],
        [   0, 7963,   17,  ...,   50,   13,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1421e-05,  1.4735e-04,  4.1136e-04,  ..., -1.7836e-04,
         -5.8171e-05, -1.3656e-04],
        [ 4.8987e-05,  4.9438e-05, -4.3510e-05,  ...,  5.6511e-05,
          8.3319e-05,  1.4981e-05],
        [-2.7314e-05, -1.9684e-05,  2.0400e-05,  ..., -2.2724e-05,
         -1.7628e-05, -1.9014e-05],
        [ 3.9488e-04,  4.7039e-04, -2.2854e-04,  ...,  3.6242e-04,
          3.8477e-04,  2.9154e-04],
        [-2.2069e-05, -1.6481e-05,  1.6019e-05,  ..., -1.8656e-05,
         -1.5251e-05, -1.4253e-05]], device='cuda:0')
Loss: 0.9703649878501892


Running epoch 1, step 1399, batch 351
Sampled inputs[:2]: tensor([[   0,  391, 1351,  ...,   13,   40,    9],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5981e-04, -1.0169e-04,  4.3310e-04,  ..., -2.8764e-04,
          4.0335e-05, -4.9776e-05],
        [ 4.7594e-05,  4.8387e-05, -4.2579e-05,  ...,  5.5326e-05,
          8.2358e-05,  1.4057e-05],
        [-3.1218e-05, -2.2769e-05,  2.3186e-05,  ..., -2.6047e-05,
         -2.0340e-05, -2.1607e-05],
        [ 3.9116e-04,  4.6758e-04, -2.2591e-04,  ...,  3.5923e-04,
          3.8214e-04,  2.8894e-04],
        [-2.5243e-05, -1.9103e-05,  1.8269e-05,  ..., -2.1413e-05,
         -1.7621e-05, -1.6220e-05]], device='cuda:0')
Loss: 0.9857207536697388
Graident accumulation at epoch 1, step 1399, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0189],
        [ 0.0288, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.7766e-05,  6.0355e-05,  3.8197e-05,  ...,  1.2470e-05,
         -1.2614e-04,  6.6599e-05],
        [-4.3541e-06, -6.9150e-07,  1.2881e-06,  ..., -2.4539e-06,
          3.0969e-06, -5.0725e-06],
        [ 5.0681e-05,  6.5071e-05, -3.5352e-05,  ...,  4.9556e-05,
          5.4362e-05,  1.6051e-05],
        [ 1.9985e-05,  4.0324e-05, -1.1829e-05,  ...,  1.8810e-05,
          3.0832e-05,  1.3151e-05],
        [-2.4949e-05, -1.8853e-05,  1.6329e-05,  ..., -2.1238e-05,
         -1.7359e-05, -1.6294e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3444e-08, 5.7947e-08, 4.3519e-08,  ..., 4.0262e-08, 1.2097e-07,
         5.2097e-08],
        [7.0908e-11, 5.7179e-11, 1.6748e-11,  ..., 5.3224e-11, 3.2058e-11,
         2.2715e-11],
        [4.0993e-09, 2.8557e-09, 1.2748e-09,  ..., 3.1903e-09, 1.2919e-09,
         1.0309e-09],
        [1.1015e-09, 1.1093e-09, 3.0152e-10,  ..., 9.1396e-10, 5.6185e-10,
         4.2472e-10],
        [3.5571e-10, 2.0290e-10, 7.2801e-11,  ..., 2.6708e-10, 7.3536e-11,
         1.0064e-10]], device='cuda:0')
optimizer state dict: 175.0
lr: [5.246764221148206e-06, 5.246764221148206e-06]
scheduler_last_epoch: 175


Running epoch 1, step 1400, batch 352
Sampled inputs[:2]: tensor([[   0,  834,   89,  ..., 4030,   12, 6528],
        [   0,   12,  287,  ...,  278, 4697,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9475e-04,  1.2477e-05, -4.8271e-05,  ...,  1.5322e-05,
          3.6639e-05,  1.0513e-04],
        [-1.3113e-06, -8.7544e-07,  9.6858e-07,  ..., -1.1176e-06,
         -7.9349e-07, -9.0525e-07],
        [-3.6955e-06, -2.6077e-06,  2.9504e-06,  ..., -3.0994e-06,
         -2.2650e-06, -2.4587e-06],
        [-3.6210e-06, -2.4289e-06,  2.8610e-06,  ..., -3.0994e-06,
         -2.2352e-06, -2.5928e-06],
        [-2.8312e-06, -2.1160e-06,  2.2203e-06,  ..., -2.4140e-06,
         -1.8701e-06, -1.7434e-06]], device='cuda:0')
Loss: 0.9666926264762878


Running epoch 1, step 1401, batch 353
Sampled inputs[:2]: tensor([[   0,   73,   14,  ...,  650,   13, 3658],
        [   0,  300,  266,  ...,   13, 2920,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7731e-05,  5.6261e-05,  6.9446e-05,  ..., -4.2876e-05,
          1.9186e-04,  1.6806e-04],
        [-2.7046e-06, -1.8291e-06,  2.0191e-06,  ..., -2.2352e-06,
         -1.5907e-06, -1.8403e-06],
        [-7.7486e-06, -5.5134e-06,  6.1542e-06,  ..., -6.3628e-06,
         -4.6194e-06, -5.1707e-06],
        [-7.4357e-06, -5.0366e-06,  5.8711e-06,  ..., -6.1840e-06,
         -4.4405e-06, -5.2601e-06],
        [-5.9009e-06, -4.4256e-06,  4.5896e-06,  ..., -4.9174e-06,
         -3.8072e-06, -3.6508e-06]], device='cuda:0')
Loss: 0.9836269617080688


Running epoch 1, step 1402, batch 354
Sampled inputs[:2]: tensor([[    0,   266,   858,  ..., 11265,   607,  7455],
        [    0,    12,   344,  ...,    14,  2295,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9496e-05, -1.2535e-04, -6.2577e-05,  ..., -2.7108e-04,
          6.9993e-04,  4.0450e-04],
        [-4.0308e-06, -2.8275e-06,  2.9802e-06,  ..., -3.3751e-06,
         -2.5034e-06, -2.7083e-06],
        [-1.1533e-05, -8.4788e-06,  9.0599e-06,  ..., -9.6112e-06,
         -7.2122e-06, -7.6592e-06],
        [-1.1072e-05, -7.7337e-06,  8.6427e-06,  ..., -9.3430e-06,
         -6.9588e-06, -7.7933e-06],
        [-8.8811e-06, -6.8545e-06,  6.8396e-06,  ..., -7.5251e-06,
         -5.9828e-06, -5.4613e-06]], device='cuda:0')
Loss: 0.9893953800201416


Running epoch 1, step 1403, batch 355
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,   474, 38333,    87],
        [    0,  3059,  2013,  ...,   278,  1997,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8428e-05, -3.5447e-05, -6.2386e-05,  ..., -3.5014e-05,
          7.0951e-04,  1.3241e-04],
        [-5.4315e-06, -3.5986e-06,  3.8669e-06,  ..., -4.5449e-06,
         -3.3192e-06, -3.8259e-06],
        [-1.5408e-05, -1.0818e-05,  1.1772e-05,  ..., -1.2800e-05,
         -9.5069e-06, -1.0654e-05],
        [-1.4797e-05, -9.7901e-06,  1.1161e-05,  ..., -1.2502e-05,
         -9.2089e-06, -1.0863e-05],
        [-1.1966e-05, -8.7619e-06,  8.9258e-06,  ..., -1.0118e-05,
         -7.9349e-06, -7.6964e-06]], device='cuda:0')
Loss: 0.9603098034858704


Running epoch 1, step 1404, batch 356
Sampled inputs[:2]: tensor([[   0, 3235,  471,  ..., 1967, 4273, 2738],
        [   0, 8125, 5241,  ...,  328, 3227,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3927e-05, -2.5058e-05, -1.1015e-04,  ...,  2.9649e-06,
          6.6961e-04,  3.7179e-04],
        [-6.8024e-06, -4.4703e-06,  4.6678e-06,  ..., -5.7444e-06,
         -4.2878e-06, -4.8243e-06],
        [-1.9133e-05, -1.3351e-05,  1.4231e-05,  ..., -1.5989e-05,
         -1.2115e-05, -1.3247e-05],
        [-1.8492e-05, -1.2115e-05,  1.3426e-05,  ..., -1.5795e-05,
         -1.1921e-05, -1.3664e-05],
        [-1.5050e-05, -1.0923e-05,  1.0937e-05,  ..., -1.2770e-05,
         -1.0185e-05, -9.6634e-06]], device='cuda:0')
Loss: 0.9581501483917236


Running epoch 1, step 1405, batch 357
Sampled inputs[:2]: tensor([[    0,  9423,   298,  ...,  5274, 37902,   271],
        [    0,  1756,   271,  ...,   259, 48595, 19882]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0606e-04, -1.5290e-05, -2.6940e-04,  ..., -1.1457e-04,
          7.9276e-04,  3.0833e-04],
        [-8.2552e-06, -5.4054e-06,  5.6289e-06,  ..., -6.9439e-06,
         -5.1670e-06, -5.8524e-06],
        [-2.3156e-05, -1.6108e-05,  1.7107e-05,  ..., -1.9327e-05,
         -1.4618e-05, -1.6093e-05],
        [-2.2277e-05, -1.4544e-05,  1.6063e-05,  ..., -1.8939e-05,
         -1.4260e-05, -1.6406e-05],
        [-1.8433e-05, -1.3322e-05,  1.3292e-05,  ..., -1.5616e-05,
         -1.2435e-05, -1.1899e-05]], device='cuda:0')
Loss: 0.988753080368042


Running epoch 1, step 1406, batch 358
Sampled inputs[:2]: tensor([[    0,   494,   360,  ...,   391, 24104, 35211],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6957e-04, -1.3748e-04, -4.1334e-04,  ..., -2.6285e-04,
          1.0506e-03,  3.9225e-04],
        [-9.7826e-06, -6.2808e-06,  6.5118e-06,  ..., -8.1882e-06,
         -6.0610e-06, -7.0818e-06],
        [-2.7299e-05, -1.8671e-05,  1.9759e-05,  ..., -2.2680e-05,
         -1.7107e-05, -1.9372e-05],
        [-2.6211e-05, -1.6823e-05,  1.8477e-05,  ..., -2.2173e-05,
         -1.6645e-05, -1.9655e-05],
        [-2.1920e-05, -1.5527e-05,  1.5467e-05,  ..., -1.8477e-05,
         -1.4655e-05, -1.4476e-05]], device='cuda:0')
Loss: 0.9593091607093811


Running epoch 1, step 1407, batch 359
Sampled inputs[:2]: tensor([[    0, 15912,    14,  ..., 25535,    18,  3947],
        [    0,   278,  1620,  ...,   360,  1758,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2397e-05,  1.2736e-04, -2.3253e-04,  ..., -3.4481e-04,
          1.0739e-03,  3.9225e-04],
        [-1.1221e-05, -7.1712e-06,  7.4729e-06,  ..., -9.3356e-06,
         -6.8955e-06, -8.1398e-06],
        [-3.1263e-05, -2.1324e-05,  2.2635e-05,  ..., -2.5839e-05,
         -1.9461e-05, -2.2277e-05],
        [-2.9996e-05, -1.9178e-05,  2.1175e-05,  ..., -2.5243e-05,
         -1.8895e-05, -2.2516e-05],
        [-2.5213e-05, -1.7777e-05,  1.7792e-05,  ..., -2.1160e-05,
         -1.6741e-05, -1.6727e-05]], device='cuda:0')
Loss: 0.9162174463272095
Graident accumulation at epoch 1, step 1407, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0154,  0.0157, -0.0286,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.6229e-05,  6.7056e-05,  1.1124e-05,  ..., -2.3258e-05,
         -6.1326e-06,  9.9164e-05],
        [-5.0407e-06, -1.3395e-06,  1.9066e-06,  ..., -3.1420e-06,
          2.0977e-06, -5.3793e-06],
        [ 4.2486e-05,  5.6431e-05, -2.9553e-05,  ...,  4.2017e-05,
          4.6980e-05,  1.2218e-05],
        [ 1.4987e-05,  3.4374e-05, -8.5289e-06,  ...,  1.4405e-05,
          2.5860e-05,  9.5839e-06],
        [-2.4976e-05, -1.8746e-05,  1.6476e-05,  ..., -2.1230e-05,
         -1.7297e-05, -1.6337e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3376e-08, 5.7905e-08, 4.3529e-08,  ..., 4.0341e-08, 1.2201e-07,
         5.2199e-08],
        [7.0963e-11, 5.7173e-11, 1.6788e-11,  ..., 5.3258e-11, 3.2074e-11,
         2.2759e-11],
        [4.0961e-09, 2.8533e-09, 1.2740e-09,  ..., 3.1877e-09, 1.2910e-09,
         1.0304e-09],
        [1.1013e-09, 1.1085e-09, 3.0167e-10,  ..., 9.1368e-10, 5.6164e-10,
         4.2480e-10],
        [3.5599e-10, 2.0301e-10, 7.3044e-11,  ..., 2.6726e-10, 7.3742e-11,
         1.0081e-10]], device='cuda:0')
optimizer state dict: 176.0
lr: [5.1383708942904056e-06, 5.1383708942904056e-06]
scheduler_last_epoch: 176


Running epoch 1, step 1408, batch 360
Sampled inputs[:2]: tensor([[    0,  3398,  6361,  ..., 12942,   518,  4066],
        [    0,  3037,  4511,  ...,  1711,    12,  2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4819e-05,  9.9809e-05, -2.0253e-04,  ...,  3.6278e-05,
         -1.5106e-04, -1.0339e-04],
        [-1.3635e-06, -9.4622e-07,  1.0282e-06,  ..., -1.1474e-06,
         -8.3074e-07, -9.3505e-07],
        [-3.8445e-06, -2.7716e-06,  3.0845e-06,  ..., -3.1888e-06,
         -2.3395e-06, -2.5630e-06],
        [-3.7104e-06, -2.5630e-06,  2.9355e-06,  ..., -3.0994e-06,
         -2.2799e-06, -2.6077e-06],
        [-2.9802e-06, -2.2650e-06,  2.3693e-06,  ..., -2.5332e-06,
         -1.9819e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.971510112285614


Running epoch 1, step 1409, batch 361
Sampled inputs[:2]: tensor([[    0, 25228,  1168,  ...,  2728,    27,   298],
        [    0,   474,   513,  ...,   221,  2951,  7773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7025e-05,  1.1849e-04, -9.2040e-05,  ..., -2.5223e-05,
          2.0933e-04, -7.6553e-05],
        [-2.7344e-06, -1.8477e-06,  1.8068e-06,  ..., -2.3469e-06,
         -1.7732e-06, -2.0452e-06],
        [-7.5400e-06, -5.3644e-06,  5.4538e-06,  ..., -6.3628e-06,
         -4.9323e-06, -5.4091e-06],
        [-7.3314e-06, -4.9323e-06,  5.0962e-06,  ..., -6.3181e-06,
         -4.9025e-06, -5.6028e-06],
        [-6.1691e-06, -4.5151e-06,  4.3809e-06,  ..., -5.2899e-06,
         -4.3064e-06, -4.0829e-06]], device='cuda:0')
Loss: 0.9497168064117432


Running epoch 1, step 1410, batch 362
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,   565,  5539,  ...,    12,   516, 14426]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3954e-05, -2.3542e-04, -4.0526e-04,  ..., -9.6261e-06,
          2.6806e-04,  9.1337e-07],
        [-4.1276e-06, -2.6599e-06,  2.7902e-06,  ..., -3.5837e-06,
         -2.7642e-06, -3.1851e-06],
        [-1.1310e-05, -7.6890e-06,  8.3148e-06,  ..., -9.6262e-06,
         -7.5847e-06, -8.3596e-06],
        [-1.1042e-05, -7.0632e-06,  7.8529e-06,  ..., -9.5963e-06,
         -7.5400e-06, -8.6874e-06],
        [-9.3728e-06, -6.5267e-06,  6.7651e-06,  ..., -8.0764e-06,
         -6.6459e-06, -6.4224e-06]], device='cuda:0')
Loss: 0.9877176880836487


Running epoch 1, step 1411, batch 363
Sampled inputs[:2]: tensor([[    0,  1607,    12,  ...,   895,  1503,   369],
        [    0,   287,  2026,  ..., 16374,   266,  2236]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8456e-05, -3.1215e-04, -4.7125e-04,  ..., -3.2870e-05,
          1.7789e-04,  1.7536e-04],
        [-5.4836e-06, -3.5986e-06,  3.8631e-06,  ..., -4.6417e-06,
         -3.5278e-06, -4.0680e-06],
        [-1.5303e-05, -1.0654e-05,  1.1668e-05,  ..., -1.2755e-05,
         -9.9093e-06, -1.0967e-05],
        [-1.4752e-05, -9.6560e-06,  1.0967e-05,  ..., -1.2502e-05,
         -9.6858e-06, -1.1235e-05],
        [-1.2338e-05, -8.8364e-06,  9.2089e-06,  ..., -1.0446e-05,
         -8.5235e-06, -8.1956e-06]], device='cuda:0')
Loss: 0.9721864461898804


Running epoch 1, step 1412, batch 364
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,  1920,    19,  ...,  5232,   796,  1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5533e-05, -3.5265e-04, -6.3725e-04,  ...,  8.0037e-05,
          1.4107e-04,  1.3487e-04],
        [-6.8322e-06, -4.5523e-06,  4.8541e-06,  ..., -5.7593e-06,
         -4.3660e-06, -5.0217e-06],
        [-1.9237e-05, -1.3545e-05,  1.4737e-05,  ..., -1.5974e-05,
         -1.2323e-05, -1.3694e-05],
        [-1.8433e-05, -1.2234e-05,  1.3798e-05,  ..., -1.5557e-05,
         -1.1995e-05, -1.3962e-05],
        [-1.5378e-05, -1.1161e-05,  1.1548e-05,  ..., -1.2979e-05,
         -1.0535e-05, -1.0133e-05]], device='cuda:0')
Loss: 0.9529973864555359


Running epoch 1, step 1413, batch 365
Sampled inputs[:2]: tensor([[    0,   591,  1545,  ...,    71,   462,   221],
        [    0,   374,   298,  ..., 11183,    12,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6262e-04, -1.7867e-04, -6.7406e-04,  ...,  1.4090e-04,
         -7.0287e-05,  6.2125e-05],
        [-8.2627e-06, -5.4389e-06,  5.7966e-06,  ..., -6.9663e-06,
         -5.2564e-06, -6.0946e-06],
        [-2.3201e-05, -1.6153e-05,  1.7568e-05,  ..., -1.9282e-05,
         -1.4827e-05, -1.6585e-05],
        [-2.2158e-05, -1.4529e-05,  1.6391e-05,  ..., -1.8701e-05,
         -1.4350e-05, -1.6794e-05],
        [-1.8731e-05, -1.3396e-05,  1.3858e-05,  ..., -1.5825e-05,
         -1.2800e-05, -1.2457e-05]], device='cuda:0')
Loss: 0.92030930519104


Running epoch 1, step 1414, batch 366
Sampled inputs[:2]: tensor([[    0,   266,  1586,  ...,  1888,  2117,   328],
        [    0, 38816,   292,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0957e-04, -2.4094e-04, -9.0397e-04,  ...,  1.5872e-04,
         -5.6667e-05, -2.1293e-04],
        [-9.5069e-06, -6.2808e-06,  6.7726e-06,  ..., -8.0094e-06,
         -6.1058e-06, -7.0557e-06],
        [-2.6718e-05, -1.8567e-05,  2.0504e-05,  ..., -2.2113e-05,
         -1.7121e-05, -1.9148e-05],
        [-2.5541e-05, -1.6704e-05,  1.9178e-05,  ..., -2.1502e-05,
         -1.6659e-05, -1.9506e-05],
        [-2.1607e-05, -1.5453e-05,  1.6198e-05,  ..., -1.8194e-05,
         -1.4812e-05, -1.4365e-05]], device='cuda:0')
Loss: 0.9045721292495728


Running epoch 1, step 1415, batch 367
Sampled inputs[:2]: tensor([[    0,  5689,   271,  ...,   352,  9985,  3260],
        [    0,  6532,  6984,  ...,   271,  8212, 14409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5175e-04, -2.6326e-04, -1.0437e-03,  ...,  4.5868e-05,
         -1.2402e-05, -2.9928e-04],
        [-1.0811e-05, -7.1451e-06,  7.7076e-06,  ..., -9.1121e-06,
         -6.9737e-06, -8.1137e-06],
        [-3.0413e-05, -2.1115e-05,  2.3380e-05,  ..., -2.5168e-05,
         -1.9535e-05, -2.2009e-05],
        [-2.9072e-05, -1.8999e-05,  2.1845e-05,  ..., -2.4483e-05,
         -1.9029e-05, -2.2441e-05],
        [-2.4587e-05, -1.7568e-05,  1.8448e-05,  ..., -2.0698e-05,
         -1.6898e-05, -1.6481e-05]], device='cuda:0')
Loss: 0.9611920714378357
Graident accumulation at epoch 1, step 1415, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.2431e-05,  3.4024e-05, -9.4356e-05,  ..., -1.6345e-05,
         -6.7595e-06,  5.9320e-05],
        [-5.6177e-06, -1.9200e-06,  2.4867e-06,  ..., -3.7390e-06,
          1.1905e-06, -5.6527e-06],
        [ 3.5197e-05,  4.8677e-05, -2.4260e-05,  ...,  3.5298e-05,
          4.0328e-05,  8.7953e-06],
        [ 1.0581e-05,  2.9037e-05, -5.4915e-06,  ...,  1.0516e-05,
          2.1371e-05,  6.3814e-06],
        [-2.4937e-05, -1.8628e-05,  1.6673e-05,  ..., -2.1177e-05,
         -1.7257e-05, -1.6351e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3366e-08, 5.7917e-08, 4.4575e-08,  ..., 4.0303e-08, 1.2188e-07,
         5.2237e-08],
        [7.1009e-11, 5.7167e-11, 1.6830e-11,  ..., 5.3288e-11, 3.2091e-11,
         2.2802e-11],
        [4.0930e-09, 2.8509e-09, 1.2733e-09,  ..., 3.1852e-09, 1.2901e-09,
         1.0299e-09],
        [1.1010e-09, 1.1078e-09, 3.0185e-10,  ..., 9.1337e-10, 5.6144e-10,
         4.2488e-10],
        [3.5624e-10, 2.0312e-10, 7.3312e-11,  ..., 2.6742e-10, 7.3954e-11,
         1.0099e-10]], device='cuda:0')
optimizer state dict: 177.0
lr: [5.030720466797722e-06, 5.030720466797722e-06]
scheduler_last_epoch: 177


Running epoch 1, step 1416, batch 368
Sampled inputs[:2]: tensor([[    0,  2715, 10929,  ...,  4978,   287,   266],
        [    0,  1254,  1773,  ..., 19459,  2447,  2613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.3600e-05, -4.8705e-05,  2.1049e-04,  ..., -1.3383e-05,
          6.8660e-05,  9.2338e-05],
        [-1.3560e-06, -1.0431e-06,  9.6112e-07,  ..., -1.1623e-06,
         -1.0952e-06, -1.0356e-06],
        [-4.0233e-06, -3.2336e-06,  3.0398e-06,  ..., -3.4273e-06,
         -3.2634e-06, -3.0547e-06],
        [-3.8445e-06, -2.9206e-06,  2.8312e-06,  ..., -3.2932e-06,
         -3.1441e-06, -3.0398e-06],
        [-3.1590e-06, -2.6226e-06,  2.3544e-06,  ..., -2.7567e-06,
         -2.7120e-06, -2.2501e-06]], device='cuda:0')
Loss: 0.9622659683227539


Running epoch 1, step 1417, batch 369
Sampled inputs[:2]: tensor([[   0,   12,  298,  ..., 5125, 6654, 4925],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4008e-04, -1.0591e-04,  2.1714e-04,  ..., -3.7998e-05,
          1.3822e-04,  3.1518e-04],
        [-2.8163e-06, -2.0191e-06,  1.8366e-06,  ..., -2.4363e-06,
         -2.1309e-06, -2.1905e-06],
        [-8.1360e-06, -6.1840e-06,  5.7220e-06,  ..., -7.0035e-06,
         -6.2585e-06, -6.2585e-06],
        [-7.7188e-06, -5.4985e-06,  5.2452e-06,  ..., -6.7055e-06,
         -5.9754e-06, -6.2138e-06],
        [-6.8694e-06, -5.3197e-06,  4.7237e-06,  ..., -6.0052e-06,
         -5.5134e-06, -4.9770e-06]], device='cuda:0')
Loss: 0.9561456441879272


Running epoch 1, step 1418, batch 370
Sampled inputs[:2]: tensor([[    0, 16064, 10937,  ...,   346,   462,   221],
        [    0, 10565,  2677,  ...,   298,   292, 11188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6671e-04, -7.0981e-05,  1.0937e-04,  ..., -1.5736e-05,
         -4.4563e-06,  3.0513e-04],
        [-4.0904e-06, -2.9355e-06,  2.8275e-06,  ..., -3.5241e-06,
         -2.9840e-06, -3.1181e-06],
        [-1.1936e-05, -9.0152e-06,  8.8662e-06,  ..., -1.0222e-05,
         -8.8066e-06, -8.9407e-06],
        [-1.1280e-05, -8.0168e-06,  8.1807e-06,  ..., -9.7454e-06,
         -8.3894e-06, -8.9109e-06],
        [-9.8348e-06, -7.6145e-06,  7.1228e-06,  ..., -8.5533e-06,
         -7.6145e-06, -6.8992e-06]], device='cuda:0')
Loss: 0.9562864899635315


Running epoch 1, step 1419, batch 371
Sampled inputs[:2]: tensor([[    0,  1527, 21622,  ..., 14406,    13,  6182],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6770e-04, -1.3264e-04,  1.0187e-04,  ..., -1.2893e-04,
          2.8199e-04,  3.8218e-04],
        [-5.4017e-06, -3.9563e-06,  3.8333e-06,  ..., -4.6492e-06,
         -3.9116e-06, -4.0382e-06],
        [-1.5870e-05, -1.2204e-05,  1.2025e-05,  ..., -1.3590e-05,
         -1.1563e-05, -1.1712e-05],
        [-1.4991e-05, -1.0893e-05,  1.1116e-05,  ..., -1.2934e-05,
         -1.1042e-05, -1.1668e-05],
        [-1.2904e-05, -1.0192e-05,  9.5516e-06,  ..., -1.1221e-05,
         -9.9093e-06, -8.9109e-06]], device='cuda:0')
Loss: 0.993768036365509


Running epoch 1, step 1420, batch 372
Sampled inputs[:2]: tensor([[   0,  341, 2802,  ..., 1798,   12,  266],
        [   0, 1103,  271,  ...,  957,  756,  368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2758e-04, -1.1115e-04,  1.5354e-04,  ..., -1.3274e-04,
          1.8471e-04,  2.1330e-04],
        [-6.7651e-06, -4.9546e-06,  4.8764e-06,  ..., -5.7444e-06,
         -4.7684e-06, -4.9844e-06],
        [-1.9893e-05, -1.5303e-05,  1.5289e-05,  ..., -1.6838e-05,
         -1.4141e-05, -1.4529e-05],
        [-1.8805e-05, -1.3709e-05,  1.4201e-05,  ..., -1.6019e-05,
         -1.3486e-05, -1.4439e-05],
        [-1.6004e-05, -1.2696e-05,  1.2025e-05,  ..., -1.3784e-05,
         -1.2070e-05, -1.0937e-05]], device='cuda:0')
Loss: 0.9709692597389221


Running epoch 1, step 1421, batch 373
Sampled inputs[:2]: tensor([[   0,  287,  768,  ...,  221,  474,  221],
        [   0, 1083,  287,  ...,   12,  287, 2098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3449e-04, -6.5857e-05,  1.6483e-04,  ..., -7.7751e-05,
          1.5051e-04,  2.7574e-04],
        [-8.1509e-06, -5.9381e-06,  5.9493e-06,  ..., -6.8471e-06,
         -5.6848e-06, -5.9269e-06],
        [-2.3693e-05, -1.8150e-05,  1.8463e-05,  ..., -1.9878e-05,
         -1.6719e-05, -1.7121e-05],
        [-2.2620e-05, -1.6421e-05,  1.7345e-05,  ..., -1.9088e-05,
         -1.6093e-05, -1.7181e-05],
        [-1.8969e-05, -1.5035e-05,  1.4454e-05,  ..., -1.6198e-05,
         -1.4231e-05, -1.2808e-05]], device='cuda:0')
Loss: 0.9640958905220032


Running epoch 1, step 1422, batch 374
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  677, 9606,  ..., 9468, 9268,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5443e-04,  1.1293e-05,  1.8068e-04,  ..., -1.8955e-04,
          2.0102e-04,  3.0474e-04],
        [-9.5442e-06, -6.8061e-06,  6.9402e-06,  ..., -7.9721e-06,
         -6.5751e-06, -6.9551e-06],
        [-2.7567e-05, -2.0772e-05,  2.1428e-05,  ..., -2.3037e-05,
         -1.9282e-05, -1.9953e-05],
        [-2.6464e-05, -1.8835e-05,  2.0206e-05,  ..., -2.2233e-05,
         -1.8641e-05, -2.0117e-05],
        [-2.2173e-05, -1.7270e-05,  1.6853e-05,  ..., -1.8835e-05,
         -1.6451e-05, -1.4983e-05]], device='cuda:0')
Loss: 0.9588909149169922


Running epoch 1, step 1423, batch 375
Sampled inputs[:2]: tensor([[    0, 23230,    12,  ...,  5092,   741,   266],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9554e-04, -2.3305e-05,  8.7242e-05,  ..., -2.2745e-04,
          4.1440e-04,  3.6579e-04],
        [-1.0937e-05, -7.7747e-06,  8.0206e-06,  ..., -9.0972e-06,
         -7.4543e-06, -7.9051e-06],
        [-3.1531e-05, -2.3708e-05,  2.4721e-05,  ..., -2.6241e-05,
         -2.1830e-05, -2.2650e-05],
        [-3.0279e-05, -2.1487e-05,  2.3320e-05,  ..., -2.5332e-05,
         -2.1085e-05, -2.2829e-05],
        [-2.5243e-05, -1.9625e-05,  1.9342e-05,  ..., -2.1368e-05,
         -1.8582e-05, -1.6935e-05]], device='cuda:0')
Loss: 0.9848125576972961
Graident accumulation at epoch 1, step 1423, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7634e-05,  2.8291e-05, -7.6196e-05,  ..., -3.7456e-05,
          3.5356e-05,  8.9967e-05],
        [-6.1497e-06, -2.5055e-06,  3.0401e-06,  ..., -4.2749e-06,
          3.2605e-07, -5.8779e-06],
        [ 2.8524e-05,  4.1438e-05, -1.9362e-05,  ...,  2.9144e-05,
          3.4112e-05,  5.6508e-06],
        [ 6.4947e-06,  2.3984e-05, -2.6103e-06,  ...,  6.9312e-06,
          1.7125e-05,  3.4604e-06],
        [-2.4967e-05, -1.8728e-05,  1.6940e-05,  ..., -2.1196e-05,
         -1.7390e-05, -1.6410e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3380e-08, 5.7859e-08, 4.4538e-08,  ..., 4.0314e-08, 1.2193e-07,
         5.2318e-08],
        [7.1058e-11, 5.7170e-11, 1.6878e-11,  ..., 5.3317e-11, 3.2114e-11,
         2.2841e-11],
        [4.0899e-09, 2.8486e-09, 1.2726e-09,  ..., 3.1827e-09, 1.2893e-09,
         1.0293e-09],
        [1.1008e-09, 1.1071e-09, 3.0209e-10,  ..., 9.1310e-10, 5.6133e-10,
         4.2497e-10],
        [3.5652e-10, 2.0330e-10, 7.3612e-11,  ..., 2.6761e-10, 7.4226e-11,
         1.0117e-10]], device='cuda:0')
optimizer state dict: 178.0
lr: [4.9238293885951606e-06, 4.9238293885951606e-06]
scheduler_last_epoch: 178


Running epoch 1, step 1424, batch 376
Sampled inputs[:2]: tensor([[   0, 1016,  271,  ...,  461,  616,  993],
        [   0,   12,  342,  ..., 3458,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2709e-04,  7.0196e-05, -5.3639e-05,  ..., -7.8164e-05,
          9.6546e-05,  1.3806e-06],
        [-1.3784e-06, -9.4250e-07,  9.7603e-07,  ..., -1.1995e-06,
         -1.0133e-06, -9.0897e-07],
        [-3.8147e-06, -2.7120e-06,  2.8759e-06,  ..., -3.2783e-06,
         -2.7865e-06, -2.4736e-06],
        [-3.7849e-06, -2.5183e-06,  2.7865e-06,  ..., -3.2932e-06,
         -2.7716e-06, -2.5630e-06],
        [-3.2485e-06, -2.3544e-06,  2.4140e-06,  ..., -2.8163e-06,
         -2.4736e-06, -1.9521e-06]], device='cuda:0')
Loss: 0.9824881553649902


Running epoch 1, step 1425, batch 377
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,  1422, 30163,   578],
        [    0,  2834, 25800,  ...,    12,   367,  2870]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6607e-04,  3.8118e-05,  1.1143e-04,  ...,  9.5865e-05,
          1.2265e-04,  1.8359e-04],
        [-2.7642e-06, -1.8068e-06,  2.0042e-06,  ..., -2.3693e-06,
         -1.9148e-06, -1.9222e-06],
        [-7.8380e-06, -5.3048e-06,  6.0350e-06,  ..., -6.6310e-06,
         -5.3793e-06, -5.3495e-06],
        [-7.6890e-06, -4.9174e-06,  5.8115e-06,  ..., -6.6012e-06,
         -5.3346e-06, -5.5432e-06],
        [-6.4820e-06, -4.5300e-06,  4.9174e-06,  ..., -5.5581e-06,
         -4.6790e-06, -4.1127e-06]], device='cuda:0')
Loss: 1.0009351968765259


Running epoch 1, step 1426, batch 378
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0,    14,   417,  ...,    43,   503,    67]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2316e-04,  5.1104e-05,  1.7346e-04,  ...,  2.7651e-04,
         -2.1115e-04, -6.7386e-05],
        [-4.0829e-06, -2.7157e-06,  3.0845e-06,  ..., -3.4496e-06,
         -2.7046e-06, -2.7865e-06],
        [-1.1653e-05, -8.0466e-06,  9.3430e-06,  ..., -9.7752e-06,
         -7.7188e-06, -7.8529e-06],
        [-1.1384e-05, -7.4357e-06,  9.0003e-06,  ..., -9.6560e-06,
         -7.5847e-06, -8.0764e-06],
        [-9.3877e-06, -6.7055e-06,  7.3761e-06,  ..., -7.9870e-06,
         -6.5863e-06, -5.8711e-06]], device='cuda:0')
Loss: 0.9789956212043762


Running epoch 1, step 1427, batch 379
Sampled inputs[:2]: tensor([[    0,  2544,   394,  ...,    14,  1062,   516],
        [    0,    15, 14761,  ...,   278,  3218,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9516e-04, -6.3576e-05,  3.3290e-04,  ...,  3.1053e-04,
         -1.7472e-04,  2.4794e-05],
        [-5.4538e-06, -3.6843e-06,  4.0270e-06,  ..., -4.6045e-06,
         -3.6545e-06, -3.8072e-06],
        [-1.5736e-05, -1.1101e-05,  1.2323e-05,  ..., -1.3202e-05,
         -1.0565e-05, -1.0893e-05],
        [-1.5348e-05, -1.0237e-05,  1.1832e-05,  ..., -1.3039e-05,
         -1.0401e-05, -1.1161e-05],
        [-1.2681e-05, -9.2387e-06,  9.7305e-06,  ..., -1.0803e-05,
         -9.0152e-06, -8.1658e-06]], device='cuda:0')
Loss: 0.981407105922699


Running epoch 1, step 1428, batch 380
Sampled inputs[:2]: tensor([[    0,   446, 28686,  ...,    35,  2706, 19712],
        [    0,    14,   381,  ...,  7106,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9429e-04, -9.9150e-05,  3.8783e-04,  ...,  1.9938e-04,
          9.1086e-05, -3.5722e-05],
        [-6.7279e-06, -4.5970e-06,  5.0403e-06,  ..., -5.6997e-06,
         -4.5225e-06, -4.6827e-06],
        [-1.9535e-05, -1.3962e-05,  1.5512e-05,  ..., -1.6481e-05,
         -1.3143e-05, -1.3515e-05],
        [-1.8969e-05, -1.2815e-05,  1.4856e-05,  ..., -1.6168e-05,
         -1.2890e-05, -1.3813e-05],
        [-1.5616e-05, -1.1563e-05,  1.2144e-05,  ..., -1.3396e-05,
         -1.1146e-05, -1.0036e-05]], device='cuda:0')
Loss: 1.0091065168380737


Running epoch 1, step 1429, batch 381
Sampled inputs[:2]: tensor([[    0,    28,  2973,  ...,  8762,  2134,    27],
        [    0,    14, 49045,  ...,    12,   706,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8295e-04, -2.1920e-04,  3.3343e-04,  ...,  1.5934e-04,
          1.5671e-04,  1.2677e-05],
        [-8.1286e-06, -5.6326e-06,  6.1058e-06,  ..., -6.8545e-06,
         -5.4985e-06, -5.6513e-06],
        [-2.3529e-05, -1.7047e-05,  1.8775e-05,  ..., -1.9774e-05,
         -1.6004e-05, -1.6242e-05],
        [-2.2739e-05, -1.5616e-05,  1.7896e-05,  ..., -1.9297e-05,
         -1.5587e-05, -1.6525e-05],
        [-1.8775e-05, -1.4082e-05,  1.4663e-05,  ..., -1.6049e-05,
         -1.3545e-05, -1.2048e-05]], device='cuda:0')
Loss: 0.9995357990264893


Running epoch 1, step 1430, batch 382
Sampled inputs[:2]: tensor([[    0,   313,    66,  ...,   894,  2973, 25074],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5437e-04, -1.8303e-04,  4.3195e-04,  ...,  1.7247e-04,
          1.5621e-04, -5.8223e-06],
        [-9.4771e-06, -6.6608e-06,  7.1116e-06,  ..., -8.0019e-06,
         -6.4448e-06, -6.5491e-06],
        [-2.7522e-05, -2.0161e-05,  2.1875e-05,  ..., -2.3156e-05,
         -1.8775e-05, -1.8895e-05],
        [-2.6613e-05, -1.8507e-05,  2.0877e-05,  ..., -2.2605e-05,
         -1.8284e-05, -1.9222e-05],
        [-2.1949e-05, -1.6645e-05,  1.7092e-05,  ..., -1.8790e-05,
         -1.5870e-05, -1.4029e-05]], device='cuda:0')
Loss: 1.0069550275802612


Running epoch 1, step 1431, batch 383
Sampled inputs[:2]: tensor([[    0, 24781,   287,  ...,   266,  3873,  1400],
        [    0,    12,   638,  ...,   374,   221,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3977e-04, -1.8303e-04,  4.2065e-04,  ...,  2.2910e-04,
          5.0413e-05,  4.6243e-06],
        [-1.0811e-05, -7.5474e-06,  8.1845e-06,  ..., -9.0823e-06,
         -7.2308e-06, -7.4804e-06],
        [-3.1397e-05, -2.2829e-05,  2.5168e-05,  ..., -2.6271e-05,
         -2.1070e-05, -2.1532e-05],
        [-3.0354e-05, -2.0966e-05,  2.4050e-05,  ..., -2.5660e-05,
         -2.0534e-05, -2.1920e-05],
        [-2.4840e-05, -1.8716e-05,  1.9506e-05,  ..., -2.1160e-05,
         -1.7710e-05, -1.5862e-05]], device='cuda:0')
Loss: 0.9475906491279602
Graident accumulation at epoch 1, step 1431, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8107e-05,  7.1592e-06, -2.6512e-05,  ..., -1.0801e-05,
          3.6862e-05,  8.1433e-05],
        [-6.6158e-06, -3.0097e-06,  3.5545e-06,  ..., -4.7556e-06,
         -4.2964e-07, -6.0382e-06],
        [ 2.2532e-05,  3.5012e-05, -1.4909e-05,  ...,  2.3603e-05,
          2.8594e-05,  2.9325e-06],
        [ 2.8099e-06,  1.9489e-05,  5.5776e-08,  ...,  3.6721e-06,
          1.3359e-05,  9.2239e-07],
        [-2.4955e-05, -1.8726e-05,  1.7196e-05,  ..., -2.1193e-05,
         -1.7422e-05, -1.6355e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3500e-08, 5.7835e-08, 4.4671e-08,  ..., 4.0326e-08, 1.2182e-07,
         5.2266e-08],
        [7.1104e-11, 5.7170e-11, 1.6928e-11,  ..., 5.3346e-11, 3.2134e-11,
         2.2874e-11],
        [4.0868e-09, 2.8462e-09, 1.2720e-09,  ..., 3.1802e-09, 1.2885e-09,
         1.0288e-09],
        [1.1006e-09, 1.1065e-09, 3.0236e-10,  ..., 9.1284e-10, 5.6119e-10,
         4.2503e-10],
        [3.5678e-10, 2.0345e-10, 7.3919e-11,  ..., 2.6779e-10, 7.4465e-11,
         1.0132e-10]], device='cuda:0')
optimizer state dict: 179.0
lr: [4.817713993572543e-06, 4.817713993572543e-06]
scheduler_last_epoch: 179


Running epoch 1, step 1432, batch 384
Sampled inputs[:2]: tensor([[    0,  7185,   328,  ...,  1427,  1477,  1061],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6297e-05, -6.4173e-08,  5.2732e-05,  ..., -7.7214e-06,
         -1.5697e-05, -2.1793e-05],
        [-1.3039e-06, -9.2760e-07,  1.0952e-06,  ..., -1.0580e-06,
         -8.2329e-07, -8.1956e-07],
        [ 1.0499e-04,  1.5223e-04, -1.5871e-04,  ...,  1.0578e-04,
          9.8715e-05,  2.4037e-05],
        [-3.6955e-06, -2.6077e-06,  3.2336e-06,  ..., -3.0249e-06,
         -2.3544e-06, -2.4438e-06],
        [-2.9504e-06, -2.2799e-06,  2.5332e-06,  ..., -2.4587e-06,
         -2.0117e-06, -1.7509e-06]], device='cuda:0')
Loss: 1.0015273094177246


Running epoch 1, step 1433, batch 385
Sampled inputs[:2]: tensor([[   0, 2088, 5370,  ..., 1110, 3380,   12],
        [   0, 1167, 2667,  ..., 4769,   13, 5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4177e-05, -1.3615e-04,  1.4011e-04,  ..., -8.0660e-05,
         -4.8012e-05,  4.8552e-05],
        [-2.6375e-06, -1.9781e-06,  2.1979e-06,  ..., -2.1532e-06,
         -1.7472e-06, -1.6652e-06],
        [ 1.0109e-04,  1.4904e-04, -1.5536e-04,  ...,  1.0258e-04,
          9.6003e-05,  2.1534e-05],
        [-7.4655e-06, -5.5581e-06,  6.4671e-06,  ..., -6.1244e-06,
         -4.9770e-06, -4.9472e-06],
        [-5.9754e-06, -4.8429e-06,  5.0813e-06,  ..., -4.9919e-06,
         -4.2468e-06, -3.5614e-06]], device='cuda:0')
Loss: 1.0023599863052368


Running epoch 1, step 1434, batch 386
Sampled inputs[:2]: tensor([[   0, 3141,  311,  ...,  328, 7818,  408],
        [   0,   12, 1197,  ...,  516, 1136, 9774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0765e-05, -7.5641e-05,  1.2594e-04,  ..., -1.1626e-04,
          4.8484e-05,  8.6698e-05],
        [-3.9116e-06, -2.8461e-06,  3.0994e-06,  ..., -3.2634e-06,
         -2.6189e-06, -2.5257e-06],
        [ 9.7497e-05,  1.4645e-04, -1.5261e-04,  ...,  9.9452e-05,
          9.3545e-05,  1.9150e-05],
        [-1.1116e-05, -8.0466e-06,  9.1791e-06,  ..., -9.3728e-06,
         -7.5549e-06, -7.5996e-06],
        [-8.8960e-06, -7.0482e-06,  7.2569e-06,  ..., -7.5698e-06,
         -6.3777e-06, -5.3570e-06]], device='cuda:0')
Loss: 0.982896089553833


Running epoch 1, step 1435, batch 387
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0, 14576,  6617,  ...,    17,   367,  1608]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3815e-05, -1.4000e-05,  2.4512e-04,  ..., -1.1710e-04,
          8.0883e-05,  1.4820e-04],
        [-5.2378e-06, -3.7812e-06,  4.0680e-06,  ..., -4.3735e-06,
         -3.5204e-06, -3.3937e-06],
        [ 9.3697e-05,  1.4360e-04, -1.4968e-04,  ...,  9.6248e-05,
          9.0907e-05,  1.6646e-05],
        [-1.4916e-05, -1.0729e-05,  1.2085e-05,  ..., -1.2591e-05,
         -1.0222e-05, -1.0237e-05],
        [-1.1846e-05, -9.3579e-06,  9.4771e-06,  ..., -1.0103e-05,
         -8.5682e-06, -7.1526e-06]], device='cuda:0')
Loss: 0.9816434979438782


Running epoch 1, step 1436, batch 388
Sampled inputs[:2]: tensor([[   0,  894,  496,  ...,  266,  623,  587],
        [   0,  271, 3421,  ...,  306,  472,  346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8528e-04, -5.7542e-05,  1.8498e-04,  ...,  1.9904e-05,
          2.1090e-04,  1.5181e-04],
        [-6.7204e-06, -4.7050e-06,  4.9546e-06,  ..., -5.5879e-06,
         -4.4629e-06, -4.5039e-06],
        [ 8.9375e-05,  1.4077e-04, -1.4686e-04,  ...,  9.2731e-05,
          8.8136e-05,  1.3458e-05],
        [-1.8910e-05, -1.3188e-05,  1.4573e-05,  ..., -1.5900e-05,
         -1.2830e-05, -1.3322e-05],
        [-1.5557e-05, -1.1832e-05,  1.1832e-05,  ..., -1.3158e-05,
         -1.1072e-05, -9.7305e-06]], device='cuda:0')
Loss: 0.9263353943824768


Running epoch 1, step 1437, batch 389
Sampled inputs[:2]: tensor([[    0, 16847,  2027,  ...,     5,  1460,   496],
        [    0,  3408,   300,  ...,    14,  5870,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9140e-04,  1.0272e-04,  3.1042e-04,  ...,  7.2531e-05,
          9.4778e-05,  1.4682e-04],
        [-8.0913e-06, -5.5432e-06,  5.9307e-06,  ..., -6.6832e-06,
         -5.2601e-06, -5.4576e-06],
        [ 8.5442e-05,  1.3824e-04, -1.4388e-04,  ...,  8.9617e-05,
          8.5826e-05,  1.0761e-05],
        [-2.2754e-05, -1.5497e-05,  1.7434e-05,  ..., -1.8984e-05,
         -1.5110e-05, -1.6108e-05],
        [-1.8582e-05, -1.3858e-05,  1.4096e-05,  ..., -1.5602e-05,
         -1.2979e-05, -1.1683e-05]], device='cuda:0')
Loss: 0.9384157657623291


Running epoch 1, step 1438, batch 390
Sampled inputs[:2]: tensor([[   0,  369,  726,  ...,   83,  409,  729],
        [   0,  413,   16,  ...,  680,  401, 1407]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7453e-05,  2.6282e-04,  2.3708e-04,  ...,  7.6025e-05,
          6.1112e-05,  6.3999e-05],
        [-9.4175e-06, -6.4597e-06,  6.9141e-06,  ..., -7.8455e-06,
         -6.1430e-06, -6.4000e-06],
        [ 8.1537e-05,  1.3547e-04, -1.4081e-04,  ...,  8.6264e-05,
          8.3278e-05,  8.0485e-06],
        [-2.6464e-05, -1.8001e-05,  2.0310e-05,  ..., -2.2218e-05,
         -1.7568e-05, -1.8850e-05],
        [-2.1741e-05, -1.6153e-05,  1.6510e-05,  ..., -1.8343e-05,
         -1.5154e-05, -1.3724e-05]], device='cuda:0')
Loss: 0.9623109102249146


Running epoch 1, step 1439, batch 391
Sampled inputs[:2]: tensor([[    0,  2379,    13,  ...,   287,   259,  2193],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4122e-06,  2.6694e-04,  2.3706e-04,  ...,  1.1163e-05,
          1.6312e-04,  1.0619e-04],
        [-1.0714e-05, -7.3351e-06,  7.8529e-06,  ..., -8.9332e-06,
         -7.0557e-06, -7.3351e-06],
        [ 7.7693e-05,  1.3280e-04, -1.3786e-04,  ...,  8.3090e-05,
          8.0655e-05,  5.3663e-06],
        [-3.0160e-05, -2.0444e-05,  2.3082e-05,  ..., -2.5347e-05,
         -2.0191e-05, -2.1622e-05],
        [-2.4840e-05, -1.8388e-05,  1.8850e-05,  ..., -2.0936e-05,
         -1.7375e-05, -1.5736e-05]], device='cuda:0')
Loss: 0.9828221201896667
Graident accumulation at epoch 1, step 1439, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.5537e-05,  3.3137e-05, -1.5423e-07,  ..., -8.6043e-06,
          4.9488e-05,  8.3909e-05],
        [-7.0256e-06, -3.4422e-06,  3.9844e-06,  ..., -5.1734e-06,
         -1.0922e-06, -6.1679e-06],
        [ 2.8048e-05,  4.4790e-05, -2.7204e-05,  ...,  2.9551e-05,
          3.3800e-05,  3.1759e-06],
        [-4.8711e-07,  1.5496e-05,  2.3584e-06,  ...,  7.7022e-07,
          1.0004e-05, -1.3320e-06],
        [-2.4943e-05, -1.8693e-05,  1.7362e-05,  ..., -2.1167e-05,
         -1.7417e-05, -1.6293e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3427e-08, 5.7848e-08, 4.4682e-08,  ..., 4.0286e-08, 1.2172e-07,
         5.2225e-08],
        [7.1147e-11, 5.7167e-11, 1.6973e-11,  ..., 5.3373e-11, 3.2152e-11,
         2.2905e-11],
        [4.0887e-09, 2.8610e-09, 1.2897e-09,  ..., 3.1839e-09, 1.2937e-09,
         1.0278e-09],
        [1.1005e-09, 1.1058e-09, 3.0259e-10,  ..., 9.1257e-10, 5.6103e-10,
         4.2507e-10],
        [3.5704e-10, 2.0358e-10, 7.4201e-11,  ..., 2.6796e-10, 7.4692e-11,
         1.0147e-10]], device='cuda:0')
optimizer state dict: 180.0
lr: [4.712390497088522e-06, 4.712390497088522e-06]
scheduler_last_epoch: 180


Running epoch 1, step 1440, batch 392
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,  2366,  5036,  ...,  1477,   352,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2652e-05,  1.3273e-04, -3.3726e-05,  ...,  7.9073e-05,
         -1.0112e-04,  3.1597e-06],
        [-1.4082e-06, -7.9349e-07,  9.9093e-07,  ..., -1.1325e-06,
         -8.3447e-07, -1.0282e-06],
        [-4.1425e-06, -2.4587e-06,  3.0845e-06,  ..., -3.3081e-06,
         -2.4736e-06, -2.9802e-06],
        [-3.8445e-06, -2.1607e-06,  2.8163e-06,  ..., -3.1441e-06,
         -2.3544e-06, -2.9057e-06],
        [-3.3677e-06, -2.0415e-06,  2.4438e-06,  ..., -2.7120e-06,
         -2.1011e-06, -2.2650e-06]], device='cuda:0')
Loss: 0.9562900066375732


Running epoch 1, step 1441, batch 393
Sampled inputs[:2]: tensor([[    0,   270,   472,  ...,   292,    73,    14],
        [    0,   266, 15957,  ...,  1556, 45044,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5707e-05, -2.7004e-05, -1.7774e-04,  ..., -7.4788e-05,
         -1.9293e-04,  1.8069e-04],
        [-2.6301e-06, -1.7397e-06,  1.9893e-06,  ..., -2.1756e-06,
         -1.6503e-06, -1.8328e-06],
        [-7.8678e-06, -5.4091e-06,  6.2585e-06,  ..., -6.4522e-06,
         -4.9174e-06, -5.4240e-06],
        [-7.4804e-06, -4.9174e-06,  5.9009e-06,  ..., -6.2287e-06,
         -4.7535e-06, -5.4389e-06],
        [-6.2138e-06, -4.3809e-06,  4.8131e-06,  ..., -5.1409e-06,
         -4.0829e-06, -3.9786e-06]], device='cuda:0')
Loss: 0.9698038697242737


Running epoch 1, step 1442, batch 394
Sampled inputs[:2]: tensor([[    0,   767,  4478,  ...,   278,   266, 19201],
        [    0,  5506,   696,  ...,   607, 11129,   276]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5578e-05,  3.1352e-05, -1.4148e-04,  ..., -1.2450e-04,
         -2.1275e-04,  9.8280e-05],
        [-3.9265e-06, -2.6897e-06,  2.9653e-06,  ..., -3.2857e-06,
         -2.5108e-06, -2.6897e-06],
        [-1.1638e-05, -8.2999e-06,  9.2834e-06,  ..., -9.6858e-06,
         -7.4655e-06, -7.8678e-06],
        [-1.1191e-05, -7.6294e-06,  8.8513e-06,  ..., -9.4473e-06,
         -7.2718e-06, -8.0168e-06],
        [-9.2238e-06, -6.7651e-06,  7.1973e-06,  ..., -7.7486e-06,
         -6.2287e-06, -5.7891e-06]], device='cuda:0')
Loss: 0.981872022151947


Running epoch 1, step 1443, batch 395
Sampled inputs[:2]: tensor([[   0,   14,  496,  ...,  368,  259,  490],
        [   0, 2579,  278,  ...,   56,    9,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4655e-05, -3.1838e-05, -2.3225e-05,  ..., -2.8629e-05,
         -2.0397e-04,  1.2041e-04],
        [-5.2527e-06, -3.6247e-06,  3.9041e-06,  ..., -4.4182e-06,
         -3.4273e-06, -3.6024e-06],
        [-1.5542e-05, -1.1176e-05,  1.2249e-05,  ..., -1.2994e-05,
         -1.0177e-05, -1.0550e-05],
        [-1.4946e-05, -1.0222e-05,  1.1623e-05,  ..., -1.2666e-05,
         -9.8944e-06, -1.0714e-05],
        [-1.2428e-05, -9.2089e-06,  9.5665e-06,  ..., -1.0520e-05,
         -8.5682e-06, -7.8306e-06]], device='cuda:0')
Loss: 0.9446549415588379


Running epoch 1, step 1444, batch 396
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,  1451, 14349,  ...,   741,  2945,  7257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4156e-05, -4.2392e-05, -6.6170e-05,  ..., -3.7816e-05,
         -8.7704e-05,  1.2398e-04],
        [-6.6087e-06, -4.6082e-06,  4.8876e-06,  ..., -5.5432e-06,
         -4.3623e-06, -4.4778e-06],
        [-1.9446e-05, -1.4141e-05,  1.5244e-05,  ..., -1.6227e-05,
         -1.2890e-05, -1.3068e-05],
        [-1.8686e-05, -1.2919e-05,  1.4424e-05,  ..., -1.5780e-05,
         -1.2502e-05, -1.3247e-05],
        [-1.5572e-05, -1.1683e-05,  1.1936e-05,  ..., -1.3158e-05,
         -1.0863e-05, -9.7007e-06]], device='cuda:0')
Loss: 0.9771691560745239


Running epoch 1, step 1445, batch 397
Sampled inputs[:2]: tensor([[   0,  292, 3030,  ..., 1231, 2156,  266],
        [   0, 2663,  328,  ...,  342,  266, 1163]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0344e-04, -1.3142e-04, -1.1985e-04,  ..., -1.1065e-05,
         -2.4574e-05,  1.3559e-04],
        [-7.9274e-06, -5.6364e-06,  5.9456e-06,  ..., -6.6683e-06,
         -5.2825e-06, -5.3123e-06],
        [-2.3320e-05, -1.7315e-05,  1.8507e-05,  ..., -1.9580e-05,
         -1.5661e-05, -1.5542e-05],
        [-2.2471e-05, -1.5870e-05,  1.7583e-05,  ..., -1.9014e-05,
         -1.5184e-05, -1.5765e-05],
        [-1.8686e-05, -1.4320e-05,  1.4514e-05,  ..., -1.5885e-05,
         -1.3217e-05, -1.1563e-05]], device='cuda:0')
Loss: 1.0162733793258667


Running epoch 1, step 1446, batch 398
Sampled inputs[:2]: tensor([[    0,  3703,   278,  ...,  9807,    14, 10365],
        [    0,   259,  6887,  ...,  1400,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4044e-04, -1.7160e-04, -1.3360e-04,  ...,  2.1646e-05,
          1.0070e-04,  7.0013e-05],
        [-9.2387e-06, -6.6124e-06,  6.9737e-06,  ..., -7.7784e-06,
         -6.1542e-06, -6.0759e-06],
        [-2.7031e-05, -2.0191e-05,  2.1577e-05,  ..., -2.2724e-05,
         -1.8135e-05, -1.7673e-05],
        [-2.6122e-05, -1.8582e-05,  2.0579e-05,  ..., -2.2128e-05,
         -1.7643e-05, -1.8016e-05],
        [-2.1607e-05, -1.6659e-05,  1.6868e-05,  ..., -1.8388e-05,
         -1.5274e-05, -1.3120e-05]], device='cuda:0')
Loss: 0.9871609210968018


Running epoch 1, step 1447, batch 399
Sampled inputs[:2]: tensor([[   0, 3159,  278,  ...,  266, 2545,  863],
        [   0,   13, 6913,  ...,  278, 1317, 4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1743e-04, -9.4426e-05,  7.7644e-05,  ..., -4.7103e-05,
          6.9312e-05,  1.5513e-04],
        [-1.0476e-05, -7.4469e-06,  7.9200e-06,  ..., -8.8513e-06,
         -6.9849e-06, -6.9179e-06],
        [-3.0577e-05, -2.2650e-05,  2.4453e-05,  ..., -2.5749e-05,
         -2.0489e-05, -2.0012e-05],
        [-2.9594e-05, -2.0862e-05,  2.3350e-05,  ..., -2.5138e-05,
         -1.9997e-05, -2.0489e-05],
        [-2.4453e-05, -1.8716e-05,  1.9118e-05,  ..., -2.0832e-05,
         -1.7270e-05, -1.4849e-05]], device='cuda:0')
Loss: 0.9697389602661133
Graident accumulation at epoch 1, step 1447, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4727e-05,  2.0381e-05,  7.6256e-06,  ..., -1.2454e-05,
          5.1470e-05,  9.1031e-05],
        [-7.3706e-06, -3.8427e-06,  4.3779e-06,  ..., -5.5412e-06,
         -1.6815e-06, -6.2429e-06],
        [ 2.2185e-05,  3.8046e-05, -2.2039e-05,  ...,  2.4021e-05,
          2.8371e-05,  8.5706e-07],
        [-3.3978e-06,  1.1860e-05,  4.4576e-06,  ..., -1.8206e-06,
          7.0041e-06, -3.2477e-06],
        [-2.4894e-05, -1.8695e-05,  1.7537e-05,  ..., -2.1133e-05,
         -1.7403e-05, -1.6149e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3367e-08, 5.7799e-08, 4.4644e-08,  ..., 4.0248e-08, 1.2160e-07,
         5.2197e-08],
        [7.1186e-11, 5.7165e-11, 1.7018e-11,  ..., 5.3398e-11, 3.2168e-11,
         2.2930e-11],
        [4.0856e-09, 2.8587e-09, 1.2890e-09,  ..., 3.1814e-09, 1.2928e-09,
         1.0271e-09],
        [1.1002e-09, 1.1051e-09, 3.0284e-10,  ..., 9.1229e-10, 5.6087e-10,
         4.2507e-10],
        [3.5728e-10, 2.0373e-10, 7.4492e-11,  ..., 2.6813e-10, 7.4916e-11,
         1.0159e-10]], device='cuda:0')
optimizer state dict: 181.0
lr: [4.6078749934927426e-06, 4.6078749934927426e-06]
scheduler_last_epoch: 181


Running epoch 1, step 1448, batch 400
Sampled inputs[:2]: tensor([[   0,  806, 1255,  ...,  474,  221,  380],
        [   0, 1832,  292,  ..., 2176, 1345,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0146e-05,  3.1151e-05, -6.0041e-05,  ..., -6.8994e-05,
         -3.3244e-05, -9.1164e-05],
        [-1.3113e-06, -9.0897e-07,  9.6858e-07,  ..., -1.1101e-06,
         -8.5309e-07, -8.3074e-07],
        [-3.7253e-06, -2.6971e-06,  2.9355e-06,  ..., -3.1143e-06,
         -2.4140e-06, -2.3097e-06],
        [-3.6657e-06, -2.5034e-06,  2.8312e-06,  ..., -3.1143e-06,
         -2.4140e-06, -2.4289e-06],
        [-2.9802e-06, -2.2203e-06,  2.2948e-06,  ..., -2.5183e-06,
         -2.0266e-06, -1.6987e-06]], device='cuda:0')
Loss: 0.9729686975479126


Running epoch 1, step 1449, batch 401
Sampled inputs[:2]: tensor([[    0,   367,   925,  ..., 25491,   847,   328],
        [    0,   287,   516,  ...,  2386,  3492,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9767e-05,  4.3234e-05, -6.7075e-05,  ..., -2.7491e-05,
         -3.4599e-04, -2.3413e-04],
        [-2.6524e-06, -1.8552e-06,  2.0564e-06,  ..., -2.2277e-06,
         -1.7025e-06, -1.6242e-06],
        [-7.6592e-06, -5.5730e-06,  6.2585e-06,  ..., -6.3926e-06,
         -4.9174e-06, -4.6492e-06],
        [-7.4804e-06, -5.1558e-06,  6.0350e-06,  ..., -6.2883e-06,
         -4.8429e-06, -4.7982e-06],
        [-6.0052e-06, -4.5151e-06,  4.8131e-06,  ..., -5.0813e-06,
         -4.0829e-06, -3.3677e-06]], device='cuda:0')
Loss: 0.9817639589309692


Running epoch 1, step 1450, batch 402
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0,  221,  374,  ..., 2296,  365, 4579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0536e-05,  1.5703e-04,  1.5998e-04,  ...,  6.3561e-05,
         -4.4149e-04, -2.1977e-04],
        [-3.9488e-06, -2.8089e-06,  2.9430e-06,  ..., -3.4049e-06,
         -2.6338e-06, -2.5481e-06],
        [-1.1593e-05, -8.5384e-06,  9.1493e-06,  ..., -9.9242e-06,
         -7.7188e-06, -7.4059e-06],
        [-1.1206e-05, -7.8827e-06,  8.7023e-06,  ..., -9.7007e-06,
         -7.5549e-06, -7.5698e-06],
        [-9.2387e-06, -7.0035e-06,  7.1377e-06,  ..., -8.0168e-06,
         -6.5118e-06, -5.4836e-06]], device='cuda:0')
Loss: 0.9512494206428528


Running epoch 1, step 1451, batch 403
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  2192,  2534,   287],
        [    0,  2834,   266,  ..., 39474,    12, 15441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0866e-04,  2.4817e-04,  2.8403e-05,  ...,  3.6334e-05,
         -7.1502e-04, -5.7867e-04],
        [-5.2676e-06, -3.6545e-06,  3.8929e-06,  ..., -4.5449e-06,
         -3.4757e-06, -3.4310e-06],
        [-1.5348e-05, -1.1012e-05,  1.2040e-05,  ..., -1.3098e-05,
         -1.0088e-05, -9.8497e-06],
        [-1.4842e-05, -1.0148e-05,  1.1444e-05,  ..., -1.2830e-05,
         -9.8795e-06, -1.0088e-05],
        [-1.2323e-05, -9.1344e-06,  9.4622e-06,  ..., -1.0669e-05,
         -8.5831e-06, -7.3239e-06]], device='cuda:0')
Loss: 0.9511353373527527


Running epoch 1, step 1452, batch 404
Sampled inputs[:2]: tensor([[    0,   278,  1041,  ...,  2098,  1837,    12],
        [    0,   680,   993,  ...,   699, 11426,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9354e-04,  1.6490e-04,  5.5172e-05,  ...,  1.7022e-05,
         -6.3353e-04, -5.0154e-04],
        [-6.5044e-06, -4.6678e-06,  4.8466e-06,  ..., -5.6326e-06,
         -4.3698e-06, -4.2580e-06],
        [-1.9073e-05, -1.4156e-05,  1.5065e-05,  ..., -1.6361e-05,
         -1.2755e-05, -1.2338e-05],
        [-1.8418e-05, -1.3053e-05,  1.4350e-05,  ..., -1.5989e-05,
         -1.2457e-05, -1.2621e-05],
        [-1.5259e-05, -1.1683e-05,  1.1802e-05,  ..., -1.3277e-05,
         -1.0803e-05, -9.1493e-06]], device='cuda:0')
Loss: 0.956275463104248


Running epoch 1, step 1453, batch 405
Sampled inputs[:2]: tensor([[    0,  3412,  1707,  ..., 11114,    15,  1821],
        [    0, 38495, 36253,  ..., 11006,  5699,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7108e-04,  5.4150e-05, -1.2195e-04,  ...,  5.2641e-05,
         -7.1970e-04, -5.4038e-04],
        [-7.8529e-06, -5.5805e-06,  5.8822e-06,  ..., -6.7577e-06,
         -5.2452e-06, -5.0813e-06],
        [-2.2888e-05, -1.6898e-05,  1.8179e-05,  ..., -1.9580e-05,
         -1.5303e-05, -1.4678e-05],
        [-2.2113e-05, -1.5557e-05,  1.7315e-05,  ..., -1.9118e-05,
         -1.4931e-05, -1.4991e-05],
        [-1.8269e-05, -1.3947e-05,  1.4201e-05,  ..., -1.5855e-05,
         -1.2949e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.9968096017837524


Running epoch 1, step 1454, batch 406
Sampled inputs[:2]: tensor([[   0,  395, 4973,  ..., 5851,  409, 4370],
        [   0, 1862,   14,  ..., 2310, 2915, 4016]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2906e-04,  1.2973e-04,  1.0456e-05,  ...,  1.9734e-04,
         -7.9142e-04, -7.6666e-04],
        [-9.1121e-06, -6.4597e-06,  6.9179e-06,  ..., -7.7859e-06,
         -5.9977e-06, -5.8562e-06],
        [-2.6658e-05, -1.9640e-05,  2.1458e-05,  ..., -2.2665e-05,
         -1.7583e-05, -1.6987e-05],
        [-2.5854e-05, -1.8135e-05,  2.0534e-05,  ..., -2.2173e-05,
         -1.7181e-05, -1.7390e-05],
        [-2.1100e-05, -1.6093e-05,  1.6615e-05,  ..., -1.8209e-05,
         -1.4775e-05, -1.2450e-05]], device='cuda:0')
Loss: 0.9793452620506287


Running epoch 1, step 1455, batch 407
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   278,   266, 10995],
        [    0,   266,  1513,  ...,   367,  1941,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6406e-04,  8.3016e-05, -1.9573e-04,  ...,  1.3208e-04,
         -7.4421e-04, -7.4492e-04],
        [-1.0431e-05, -7.3984e-06,  7.9684e-06,  ..., -8.9034e-06,
         -6.8396e-06, -6.7055e-06],
        [-3.0532e-05, -2.2516e-05,  2.4691e-05,  ..., -2.5958e-05,
         -2.0087e-05, -1.9491e-05],
        [-2.9564e-05, -2.0757e-05,  2.3603e-05,  ..., -2.5332e-05,
         -1.9580e-05, -1.9893e-05],
        [-2.4065e-05, -1.8403e-05,  1.9029e-05,  ..., -2.0772e-05,
         -1.6846e-05, -1.4238e-05]], device='cuda:0')
Loss: 0.9652575850486755
Graident accumulation at epoch 1, step 1455, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0293, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8486e-06,  2.6644e-05, -1.2710e-05,  ...,  1.9991e-06,
         -2.8098e-05,  7.4364e-06],
        [-7.6766e-06, -4.1983e-06,  4.7370e-06,  ..., -5.8774e-06,
         -2.1973e-06, -6.2891e-06],
        [ 1.6914e-05,  3.1990e-05, -1.7366e-05,  ...,  1.9023e-05,
          2.3525e-05, -1.1777e-06],
        [-6.0144e-06,  8.5984e-06,  6.3721e-06,  ..., -4.1718e-06,
          4.3456e-06, -4.9123e-06],
        [-2.4811e-05, -1.8666e-05,  1.7686e-05,  ..., -2.1097e-05,
         -1.7347e-05, -1.5958e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3363e-08, 5.7749e-08, 4.4637e-08,  ..., 4.0225e-08, 1.2204e-07,
         5.2699e-08],
        [7.1224e-11, 5.7162e-11, 1.7065e-11,  ..., 5.3423e-11, 3.2183e-11,
         2.2952e-11],
        [4.0824e-09, 2.8563e-09, 1.2883e-09,  ..., 3.1789e-09, 1.2919e-09,
         1.0265e-09],
        [1.1000e-09, 1.1044e-09, 3.0309e-10,  ..., 9.1202e-10, 5.6069e-10,
         4.2504e-10],
        [3.5751e-10, 2.0387e-10, 7.4780e-11,  ..., 2.6829e-10, 7.5125e-11,
         1.0169e-10]], device='cuda:0')
optimizer state dict: 182.0
lr: [4.504183453666481e-06, 4.504183453666481e-06]
scheduler_last_epoch: 182


Running epoch 1, step 1456, batch 408
Sampled inputs[:2]: tensor([[    0,  1529,  5227,  ...,  1480,   367,   925],
        [    0,   508,  2322,  ...,   968,   266, 15123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5703e-05,  5.4828e-05, -8.3198e-06,  ...,  6.1842e-05,
          3.4596e-05,  1.4622e-04],
        [-1.3113e-06, -9.7603e-07,  9.6858e-07,  ..., -1.1027e-06,
         -8.7917e-07, -8.0466e-07],
        [-3.9637e-06, -3.0249e-06,  3.0845e-06,  ..., -3.3379e-06,
         -2.6524e-06, -2.4289e-06],
        [-3.8445e-06, -2.8014e-06,  2.9504e-06,  ..., -3.2336e-06,
         -2.5630e-06, -2.4587e-06],
        [-3.1739e-06, -2.5034e-06,  2.4140e-06,  ..., -2.7120e-06,
         -2.2501e-06, -1.8105e-06]], device='cuda:0')
Loss: 0.9863695502281189


Running epoch 1, step 1457, batch 409
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,   720,  1122,  ...,   656,   287, 14258]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9205e-05,  3.2890e-04,  1.5024e-04,  ...,  1.8838e-04,
         -1.0848e-04,  1.7501e-04],
        [-2.8163e-06, -1.8291e-06,  1.9819e-06,  ..., -2.2799e-06,
         -1.7509e-06, -1.8552e-06],
        [-8.1658e-06, -5.6475e-06,  6.1244e-06,  ..., -6.6757e-06,
         -5.2154e-06, -5.3644e-06],
        [-7.8976e-06, -5.1409e-06,  5.8413e-06,  ..., -6.4671e-06,
         -5.0217e-06, -5.3644e-06],
        [-6.7502e-06, -4.7386e-06,  4.8727e-06,  ..., -5.6177e-06,
         -4.5300e-06, -4.1947e-06]], device='cuda:0')
Loss: 0.893555760383606


Running epoch 1, step 1458, batch 410
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,  1340,  1049,  ...,  1441,  1211,  4165]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1847e-04,  2.2853e-04,  1.8544e-04,  ...,  1.1266e-04,
          1.3863e-05,  1.6940e-04],
        [-4.0978e-06, -2.8573e-06,  2.9355e-06,  ..., -3.4124e-06,
         -2.7046e-06, -2.6189e-06],
        [-1.1891e-05, -8.8066e-06,  9.0748e-06,  ..., -9.9987e-06,
         -8.0466e-06, -7.5847e-06],
        [-1.1474e-05, -8.0615e-06,  8.6427e-06,  ..., -9.6709e-06,
         -7.7486e-06, -7.6145e-06],
        [-9.7454e-06, -7.3463e-06,  7.1824e-06,  ..., -8.3148e-06,
         -6.9290e-06, -5.8562e-06]], device='cuda:0')
Loss: 1.0056272745132446


Running epoch 1, step 1459, batch 411
Sampled inputs[:2]: tensor([[   0, 5116, 4330,  ...,  925,  699, 1351],
        [   0, 2426,  699,  ...,  221, 1551,  720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2049e-04,  2.1068e-04,  3.7038e-04,  ...,  2.5612e-04,
         -3.9532e-05,  1.2039e-04],
        [ 9.4607e-05,  4.6895e-05, -4.7946e-05,  ...,  5.3767e-05,
          5.6423e-05,  8.6443e-05],
        [-1.6212e-05, -1.1548e-05,  1.1995e-05,  ..., -1.3530e-05,
         -1.0714e-05, -1.0788e-05],
        [-1.5497e-05, -1.0446e-05,  1.1265e-05,  ..., -1.2994e-05,
         -1.0252e-05, -1.0699e-05],
        [-1.3500e-05, -9.7752e-06,  9.6560e-06,  ..., -1.1429e-05,
         -9.3728e-06, -8.4788e-06]], device='cuda:0')
Loss: 0.9440219402313232


Running epoch 1, step 1460, batch 412
Sampled inputs[:2]: tensor([[    0,   298, 11712,  ...,   221,   273,   298],
        [    0,   775,   721,  ...,  5650,   518, 11548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8186e-04,  3.7574e-04,  6.1469e-04,  ...,  2.8178e-04,
         -3.6299e-04,  2.8175e-05],
        [ 9.3117e-05,  4.6020e-05, -4.7022e-05,  ...,  5.2552e-05,
          5.5633e-05,  8.5355e-05],
        [-2.0742e-05, -1.4335e-05,  1.4946e-05,  ..., -1.7226e-05,
         -1.3173e-05, -1.4096e-05],
        [-1.9461e-05, -1.2785e-05,  1.3828e-05,  ..., -1.6242e-05,
         -1.2383e-05, -1.3620e-05],
        [-1.7375e-05, -1.2085e-05,  1.2010e-05,  ..., -1.4633e-05,
         -1.1593e-05, -1.1250e-05]], device='cuda:0')
Loss: 0.8964213132858276


Running epoch 1, step 1461, batch 413
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0, 14026,  4137,  ..., 12292,  1553,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9126e-04,  4.3743e-04,  5.5620e-04,  ...,  3.5143e-04,
         -3.5224e-04,  1.8762e-05],
        [ 9.1791e-05,  4.5129e-05, -4.6053e-05,  ...,  5.1412e-05,
          5.4777e-05,  8.4473e-05],
        [-2.4542e-05, -1.6987e-05,  1.7911e-05,  ..., -2.0429e-05,
         -1.5572e-05, -1.6585e-05],
        [-2.3201e-05, -1.5244e-05,  1.6674e-05,  ..., -1.9446e-05,
         -1.4782e-05, -1.6212e-05],
        [-2.0549e-05, -1.4350e-05,  1.4395e-05,  ..., -1.7360e-05,
         -1.3724e-05, -1.3188e-05]], device='cuda:0')
Loss: 0.9268290996551514


Running epoch 1, step 1462, batch 414
Sampled inputs[:2]: tensor([[   0,  898,  266,  ...,   12, 3222, 8095],
        [   0, 6010,  829,  ...,  668, 1784,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7967e-05,  5.2843e-04,  3.9556e-04,  ...,  3.1370e-04,
         -3.7393e-04,  9.5360e-05],
        [ 9.0472e-05,  4.4205e-05, -4.5084e-05,  ...,  5.0302e-05,
          5.3916e-05,  8.3582e-05],
        [-2.8446e-05, -1.9833e-05,  2.0951e-05,  ..., -2.3723e-05,
         -1.8150e-05, -1.9208e-05],
        [-2.7001e-05, -1.7881e-05,  1.9580e-05,  ..., -2.2665e-05,
         -1.7300e-05, -1.8895e-05],
        [-2.3693e-05, -1.6704e-05,  1.6779e-05,  ..., -2.0042e-05,
         -1.5929e-05, -1.5169e-05]], device='cuda:0')
Loss: 0.989124596118927


Running epoch 1, step 1463, batch 415
Sampled inputs[:2]: tensor([[    0,  1085,  4878,  ...,   298,   894,   496],
        [    0,   287,  2269,  ..., 22413,   391,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4774e-05,  6.7465e-04,  4.2309e-04,  ...,  3.8937e-04,
         -4.3554e-04,  1.1818e-04],
        [ 8.9236e-05,  4.3315e-05, -4.4108e-05,  ...,  4.9237e-05,
          5.3134e-05,  8.2763e-05],
        [-3.2112e-05, -2.2575e-05,  2.4021e-05,  ..., -2.6882e-05,
         -2.0489e-05, -2.1607e-05],
        [-3.0652e-05, -2.0459e-05,  2.2605e-05,  ..., -2.5824e-05,
         -1.9625e-05, -2.1428e-05],
        [-2.6494e-05, -1.8880e-05,  1.9073e-05,  ..., -2.2486e-05,
         -1.7829e-05, -1.6861e-05]], device='cuda:0')
Loss: 0.9417873024940491
Graident accumulation at epoch 1, step 1463, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8863e-06,  9.1445e-05,  3.0870e-05,  ...,  4.0736e-05,
         -6.8843e-05,  1.8511e-05],
        [ 2.0146e-06,  5.5307e-07, -1.4757e-07,  ..., -3.6597e-07,
          3.3358e-06,  2.6160e-06],
        [ 1.2011e-05,  2.6534e-05, -1.3227e-05,  ...,  1.4433e-05,
          1.9124e-05, -3.2206e-06],
        [-8.4781e-06,  5.6927e-06,  7.9954e-06,  ..., -6.3370e-06,
          1.9486e-06, -6.5638e-06],
        [-2.4980e-05, -1.8687e-05,  1.7825e-05,  ..., -2.1236e-05,
         -1.7395e-05, -1.6048e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3290e-08, 5.8146e-08, 4.4772e-08,  ..., 4.0337e-08, 1.2210e-07,
         5.2661e-08],
        [7.9115e-11, 5.8982e-11, 1.8993e-11,  ..., 5.5794e-11, 3.4974e-11,
         2.9779e-11],
        [4.0794e-09, 2.8540e-09, 1.2876e-09,  ..., 3.1764e-09, 1.2910e-09,
         1.0259e-09],
        [1.0998e-09, 1.1037e-09, 3.0330e-10,  ..., 9.1177e-10, 5.6052e-10,
         4.2507e-10],
        [3.5785e-10, 2.0402e-10, 7.5069e-11,  ..., 2.6853e-10, 7.5368e-11,
         1.0187e-10]], device='cuda:0')
optimizer state dict: 183.0
lr: [4.401331722582158e-06, 4.401331722582158e-06]
scheduler_last_epoch: 183


Running epoch 1, step 1464, batch 416
Sampled inputs[:2]: tensor([[    0,   409,   729,  ...,   391,   266,   996],
        [    0,   292, 29800,  ...,  4144,   278,  1243]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5828e-05,  1.8361e-04, -4.1774e-05,  ...,  6.2717e-05,
         -1.5728e-04,  3.1058e-05],
        [-1.2815e-06, -8.8662e-07,  1.0207e-06,  ..., -1.1101e-06,
         -8.5682e-07, -8.5682e-07],
        [-3.6955e-06, -2.6971e-06,  3.1292e-06,  ..., -3.1888e-06,
         -2.4885e-06, -2.4438e-06],
        [-3.6508e-06, -2.5034e-06,  3.0547e-06,  ..., -3.1888e-06,
         -2.4736e-06, -2.5630e-06],
        [-3.0249e-06, -2.2650e-06,  2.4885e-06,  ..., -2.6375e-06,
         -2.1607e-06, -1.8552e-06]], device='cuda:0')
Loss: 0.9442176222801208


Running epoch 1, step 1465, batch 417
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0,  4601,   328,  ..., 10258,  2282,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0484e-05,  1.5541e-04, -1.4454e-04,  ...,  5.7413e-05,
         -7.8565e-05, -6.3017e-05],
        [-2.5705e-06, -1.8142e-06,  2.0340e-06,  ..., -2.2203e-06,
         -1.6429e-06, -1.6801e-06],
        [-7.4655e-06, -5.4985e-06,  6.2734e-06,  ..., -6.4075e-06,
         -4.7833e-06, -4.8131e-06],
        [-7.2718e-06, -5.0664e-06,  6.0350e-06,  ..., -6.3032e-06,
         -4.6939e-06, -4.9919e-06],
        [-5.9605e-06, -4.5151e-06,  4.8727e-06,  ..., -5.1707e-06,
         -4.0680e-06, -3.5614e-06]], device='cuda:0')
Loss: 0.9685729146003723


Running epoch 1, step 1466, batch 418
Sampled inputs[:2]: tensor([[    0,   266, 11692,  ...,   278, 14620, 12718],
        [    0,   409, 35049,  ...,    12,   699,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6810e-05,  1.8111e-04,  5.0995e-06,  ..., -2.4495e-06,
          9.0297e-06,  3.5493e-06],
        [-3.8520e-06, -2.7493e-06,  2.9467e-06,  ..., -3.3528e-06,
         -2.5034e-06, -2.5854e-06],
        [-1.1280e-05, -8.4192e-06,  9.1642e-06,  ..., -9.7752e-06,
         -7.3910e-06, -7.5102e-06],
        [-1.0893e-05, -7.6890e-06,  8.7172e-06,  ..., -9.5218e-06,
         -7.1526e-06, -7.6592e-06],
        [-9.0897e-06, -6.9588e-06,  7.1824e-06,  ..., -7.9572e-06,
         -6.3181e-06, -5.6326e-06]], device='cuda:0')
Loss: 0.9748272895812988


Running epoch 1, step 1467, batch 419
Sampled inputs[:2]: tensor([[    0,  7203,   271,  ...,    12,   275,  3338],
        [    0,  5750,   642,  ...,   221, 15441,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0328e-05,  1.6512e-04,  7.7054e-05,  ..., -1.6850e-04,
          1.5068e-04,  1.0079e-04],
        [-5.1260e-06, -3.6694e-06,  3.8743e-06,  ..., -4.4554e-06,
         -3.3639e-06, -3.4273e-06],
        [-1.5184e-05, -1.1295e-05,  1.2144e-05,  ..., -1.3113e-05,
         -9.9838e-06, -1.0043e-05],
        [-1.4544e-05, -1.0267e-05,  1.1459e-05,  ..., -1.2666e-05,
         -9.5963e-06, -1.0163e-05],
        [-1.2174e-05, -9.3132e-06,  9.5069e-06,  ..., -1.0639e-05,
         -8.4788e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.0001765489578247


Running epoch 1, step 1468, batch 420
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,  3070,  9719,  ...,   600,  4207,  4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9501e-05,  1.7054e-04, -7.2643e-05,  ..., -2.4774e-05,
          9.3077e-05,  1.9058e-04],
        [-6.4597e-06, -4.6454e-06,  4.9099e-06,  ..., -5.5507e-06,
         -4.2096e-06, -4.2431e-06],
        [ 7.0648e-05,  8.2344e-05, -4.2590e-05,  ...,  4.3392e-05,
          4.7395e-05,  1.4275e-05],
        [-1.8314e-05, -1.3024e-05,  1.4529e-05,  ..., -1.5765e-05,
         -1.2010e-05, -1.2591e-05],
        [-1.5303e-05, -1.1787e-05,  1.2025e-05,  ..., -1.3262e-05,
         -1.0625e-05, -9.2909e-06]], device='cuda:0')
Loss: 0.9922437071800232


Running epoch 1, step 1469, batch 421
Sampled inputs[:2]: tensor([[    0,  2377,   360,  ...,   266,  4745,   963],
        [    0, 10296,   809,  ..., 27683,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1632e-04,  3.4097e-04, -4.5067e-05,  ...,  4.6244e-05,
          8.1397e-05,  1.2571e-04],
        [-7.6741e-06, -5.4650e-06,  5.8934e-06,  ..., -6.5640e-06,
         -4.9435e-06, -5.0142e-06],
        [ 6.7116e-05,  7.9900e-05, -3.9565e-05,  ...,  4.0501e-05,
          4.5265e-05,  1.2130e-05],
        [-2.1830e-05, -1.5333e-05,  1.7554e-05,  ..., -1.8686e-05,
         -1.4156e-05, -1.4901e-05],
        [-1.8045e-05, -1.3769e-05,  1.4335e-05,  ..., -1.5542e-05,
         -1.2383e-05, -1.0826e-05]], device='cuda:0')
Loss: 0.9419768452644348


Running epoch 1, step 1470, batch 422
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,   221,   381,  ...,   360,  8978,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8857e-04,  2.2056e-04,  5.2032e-05,  ..., -8.3838e-06,
          4.2743e-04,  3.0881e-04],
        [-9.0078e-06, -6.4559e-06,  6.8471e-06,  ..., -7.7263e-06,
         -5.8338e-06, -5.9120e-06],
        [ 6.3182e-05,  7.6905e-05, -3.6570e-05,  ...,  3.7119e-05,
          4.2612e-05,  9.5069e-06],
        [-2.5541e-05, -1.8060e-05,  2.0325e-05,  ..., -2.1935e-05,
         -1.6674e-05, -1.7494e-05],
        [-2.1204e-05, -1.6257e-05,  1.6689e-05,  ..., -1.8328e-05,
         -1.4663e-05, -1.2808e-05]], device='cuda:0')
Loss: 0.9979583024978638


Running epoch 1, step 1471, batch 423
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7017e-04,  4.1032e-04,  1.7858e-04,  ...,  2.8025e-05,
          2.6661e-04,  1.4410e-04],
        [-1.0133e-05, -7.2606e-06,  7.7933e-06,  ..., -8.7321e-06,
         -6.6608e-06, -6.7353e-06],
        [ 5.9949e-05,  7.4491e-05, -3.3649e-05,  ...,  3.4317e-05,
          4.0243e-05,  7.2718e-06],
        [-2.8849e-05, -2.0385e-05,  2.3276e-05,  ..., -2.4870e-05,
         -1.9148e-05, -1.9982e-05],
        [-2.3857e-05, -1.8299e-05,  1.8999e-05,  ..., -2.0638e-05,
         -1.6689e-05, -1.4462e-05]], device='cuda:0')
Loss: 0.9290313124656677
Graident accumulation at epoch 1, step 1471, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4420e-05,  1.2333e-04,  4.5641e-05,  ...,  3.9465e-05,
         -3.5297e-05,  3.1070e-05],
        [ 7.9985e-07, -2.2830e-07,  6.4652e-07,  ..., -1.2026e-06,
          2.3361e-06,  1.6809e-06],
        [ 1.6805e-05,  3.1329e-05, -1.5269e-05,  ...,  1.6421e-05,
          2.1236e-05, -2.1714e-06],
        [-1.0515e-05,  3.0849e-06,  9.5235e-06,  ..., -8.1903e-06,
         -1.6106e-07, -7.9057e-06],
        [-2.4867e-05, -1.8648e-05,  1.7943e-05,  ..., -2.1176e-05,
         -1.7325e-05, -1.5889e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3246e-08, 5.8256e-08, 4.4759e-08,  ..., 4.0297e-08, 1.2205e-07,
         5.2629e-08],
        [7.9139e-11, 5.8975e-11, 1.9035e-11,  ..., 5.5815e-11, 3.4983e-11,
         2.9795e-11],
        [4.0789e-09, 2.8567e-09, 1.2875e-09,  ..., 3.1744e-09, 1.2914e-09,
         1.0250e-09],
        [1.0996e-09, 1.1030e-09, 3.0354e-10,  ..., 9.1148e-10, 5.6032e-10,
         4.2505e-10],
        [3.5806e-10, 2.0415e-10, 7.5355e-11,  ..., 2.6868e-10, 7.5571e-11,
         1.0198e-10]], device='cuda:0')
optimizer state dict: 184.0
lr: [4.299335516882092e-06, 4.299335516882092e-06]
scheduler_last_epoch: 184


Running epoch 1, step 1472, batch 424
Sampled inputs[:2]: tensor([[    0,  5522,  5662,  ...,   638,  1231,  1098],
        [    0,   271,   768,  ..., 15555,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.8505e-05, -1.5811e-05,  2.8997e-05,  ..., -7.2865e-05,
         -2.9005e-05,  6.5159e-06],
        [-1.3039e-06, -9.9093e-07,  1.0133e-06,  ..., -1.0803e-06,
         -8.2329e-07, -8.1584e-07],
        [-3.9637e-06, -3.1292e-06,  3.2187e-06,  ..., -3.2783e-06,
         -2.5332e-06, -2.4587e-06],
        [-3.8147e-06, -2.8610e-06,  3.0845e-06,  ..., -3.1441e-06,
         -2.4140e-06, -2.4885e-06],
        [-3.0696e-06, -2.5183e-06,  2.4736e-06,  ..., -2.5928e-06,
         -2.1011e-06, -1.7732e-06]], device='cuda:0')
Loss: 0.9610452651977539


Running epoch 1, step 1473, batch 425
Sampled inputs[:2]: tensor([[   0, 1932,  278,  ...,  609,  271,  266],
        [   0, 2440,  709,  ..., 4505, 1549, 4111]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4271e-05, -1.8327e-05,  2.7596e-05,  ...,  3.6340e-05,
          9.2174e-05,  1.2038e-05],
        [-2.6226e-06, -1.7546e-06,  1.9521e-06,  ..., -2.1681e-06,
         -1.6019e-06, -1.7919e-06],
        [-7.7784e-06, -5.4538e-06,  6.1393e-06,  ..., -6.3628e-06,
         -4.7833e-06, -5.1856e-06],
        [-7.4953e-06, -4.9174e-06,  5.8413e-06,  ..., -6.1840e-06,
         -4.6194e-06, -5.2899e-06],
        [-6.2585e-06, -4.5300e-06,  4.8727e-06,  ..., -5.2154e-06,
         -4.1127e-06, -3.8743e-06]], device='cuda:0')
Loss: 0.937626838684082


Running epoch 1, step 1474, batch 426
Sampled inputs[:2]: tensor([[   0,  278, 1099,  ...,  496,   14,  879],
        [   0,  278, 4452,  ...,   14,   18, 3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5210e-04, -1.6421e-04, -7.8136e-05,  ...,  1.0524e-04,
          2.6416e-04,  7.2098e-05],
        [-3.9935e-06, -2.6934e-06,  2.9653e-06,  ..., -3.3006e-06,
         -2.4438e-06, -2.7083e-06],
        [-1.1832e-05, -8.3745e-06,  9.2983e-06,  ..., -9.7156e-06,
         -7.3314e-06, -7.8231e-06],
        [-1.1280e-05, -7.5251e-06,  8.7470e-06,  ..., -9.3281e-06,
         -6.9886e-06, -7.8827e-06],
        [-9.5218e-06, -6.9588e-06,  7.3761e-06,  ..., -7.9572e-06,
         -6.3032e-06, -5.8860e-06]], device='cuda:0')
Loss: 0.9885392189025879


Running epoch 1, step 1475, batch 427
Sampled inputs[:2]: tensor([[   0, 3699, 3058,  ...,  820, 5327, 8055],
        [   0, 1217,    9,  ..., 1821,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0858e-04, -7.2336e-05, -4.9299e-05,  ...,  2.0340e-05,
          4.6130e-04,  5.2555e-05],
        [-5.2452e-06, -3.5837e-06,  3.7886e-06,  ..., -4.5002e-06,
         -3.3677e-06, -3.5763e-06],
        [-1.5527e-05, -1.1161e-05,  1.1921e-05,  ..., -1.3232e-05,
         -1.0088e-05, -1.0341e-05],
        [-1.4886e-05, -1.0118e-05,  1.1221e-05,  ..., -1.2845e-05,
         -9.7603e-06, -1.0520e-05],
        [-1.2636e-05, -9.3877e-06,  9.5665e-06,  ..., -1.0952e-05,
         -8.7321e-06, -7.8529e-06]], device='cuda:0')
Loss: 0.994230329990387


Running epoch 1, step 1476, batch 428
Sampled inputs[:2]: tensor([[   0, 9792, 3239,  ...,  699, 3636, 1761],
        [   0, 1802, 4165,  ...,  298,  445,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4410e-04, -7.9728e-05,  2.2336e-04,  ...,  3.4115e-06,
          5.8619e-04, -1.0506e-05],
        [-6.5714e-06, -4.4852e-06,  4.6976e-06,  ..., -5.6252e-06,
         -4.2059e-06, -4.4629e-06],
        [-1.9491e-05, -1.3992e-05,  1.4856e-05,  ..., -1.6585e-05,
         -1.2651e-05, -1.2964e-05],
        [-1.8641e-05, -1.2681e-05,  1.3933e-05,  ..., -1.6078e-05,
         -1.2204e-05, -1.3128e-05],
        [-1.5810e-05, -1.1712e-05,  1.1861e-05,  ..., -1.3664e-05,
         -1.0893e-05, -9.7901e-06]], device='cuda:0')
Loss: 0.997683048248291


Running epoch 1, step 1477, batch 429
Sampled inputs[:2]: tensor([[   0, 3605, 2572,  ...,  300,  259, 1513],
        [   0,   13, 8982,  ...,  462,  221,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1756e-04,  6.5803e-05,  2.0132e-04,  ...,  9.0009e-05,
          3.8569e-04, -2.4775e-04],
        [-7.7039e-06, -5.3719e-06,  5.6513e-06,  ..., -6.6310e-06,
         -4.9137e-06, -5.2080e-06],
        [-2.2829e-05, -1.6645e-05,  1.7896e-05,  ..., -1.9461e-05,
         -1.4707e-05, -1.5065e-05],
        [-2.1815e-05, -1.5110e-05,  1.6794e-05,  ..., -1.8895e-05,
         -1.4216e-05, -1.5318e-05],
        [-1.8403e-05, -1.3843e-05,  1.4141e-05,  ..., -1.5929e-05,
         -1.2621e-05, -1.1243e-05]], device='cuda:0')
Loss: 0.9289165735244751


Running epoch 1, step 1478, batch 430
Sampled inputs[:2]: tensor([[    0,  6112,   278,  ...,  4092,   490,  2774],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4926e-04, -2.2881e-05,  1.3860e-04,  ..., -7.0319e-05,
          4.0921e-04, -2.9144e-04],
        [-8.9258e-06, -6.2361e-06,  6.6347e-06,  ..., -7.7039e-06,
         -5.6773e-06, -6.0201e-06],
        [ 2.9667e-05,  3.9586e-05, -2.4587e-05,  ...,  3.4890e-05,
          4.1965e-05,  5.1311e-06],
        [-2.5392e-05, -1.7613e-05,  1.9819e-05,  ..., -2.2039e-05,
         -1.6451e-05, -1.7792e-05],
        [-2.1264e-05, -1.6004e-05,  1.6525e-05,  ..., -1.8448e-05,
         -1.4529e-05, -1.2957e-05]], device='cuda:0')
Loss: 0.961589515209198


Running epoch 1, step 1479, batch 431
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0,   266, 12080,  ...,   674,   369, 10956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8059e-04, -8.5285e-05,  1.7436e-04,  ..., -4.6585e-05,
          4.0702e-04, -2.2222e-04],
        [-1.0110e-05, -7.2122e-06,  7.6108e-06,  ..., -8.7395e-06,
         -6.5081e-06, -6.8024e-06],
        [ 2.6075e-05,  3.6546e-05, -2.1458e-05,  ...,  3.1761e-05,
          3.9432e-05,  2.7618e-06],
        [-2.8893e-05, -2.0489e-05,  2.2843e-05,  ..., -2.5108e-05,
         -1.8924e-05, -2.0221e-05],
        [-2.4065e-05, -1.8477e-05,  1.8924e-05,  ..., -2.0936e-05,
         -1.6645e-05, -1.4670e-05]], device='cuda:0')
Loss: 0.9915372133255005
Graident accumulation at epoch 1, step 1479, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5081e-05,  1.0247e-04,  5.8513e-05,  ...,  3.0860e-05,
          8.9346e-06,  5.7408e-06],
        [-2.9118e-07, -9.2668e-07,  1.3429e-06,  ..., -1.9563e-06,
          1.4517e-06,  8.3258e-07],
        [ 1.7732e-05,  3.1851e-05, -1.5888e-05,  ...,  1.7955e-05,
          2.3055e-05, -1.6781e-06],
        [-1.2353e-05,  7.2752e-07,  1.0855e-05,  ..., -9.8821e-06,
         -2.0374e-06, -9.1372e-06],
        [-2.4787e-05, -1.8631e-05,  1.8041e-05,  ..., -2.1152e-05,
         -1.7257e-05, -1.5767e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3251e-08, 5.8205e-08, 4.4744e-08,  ..., 4.0259e-08, 1.2210e-07,
         5.2625e-08],
        [7.9162e-11, 5.8968e-11, 1.9074e-11,  ..., 5.5835e-11, 3.4991e-11,
         2.9811e-11],
        [4.0755e-09, 2.8552e-09, 1.2866e-09,  ..., 3.1723e-09, 1.2916e-09,
         1.0239e-09],
        [1.0993e-09, 1.1024e-09, 3.0376e-10,  ..., 9.1120e-10, 5.6012e-10,
         4.2503e-10],
        [3.5828e-10, 2.0429e-10, 7.5637e-11,  ..., 2.6885e-10, 7.5772e-11,
         1.0209e-10]], device='cuda:0')
optimizer state dict: 185.0
lr: [4.19821042247685e-06, 4.19821042247685e-06]
scheduler_last_epoch: 185


Running epoch 1, step 1480, batch 432
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,   14,   69,  ...,  287,  259, 5158]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6603e-05,  1.1287e-04, -1.2119e-04,  ...,  0.0000e+00,
          9.6925e-05, -9.6682e-07],
        [-1.2666e-06, -9.0152e-07,  9.1642e-07,  ..., -1.0878e-06,
         -8.3819e-07, -9.0152e-07],
        [-3.9041e-06, -2.9206e-06,  3.0100e-06,  ..., -3.3677e-06,
         -2.6077e-06, -2.7865e-06],
        [-3.6508e-06, -2.5630e-06,  2.7567e-06,  ..., -3.1739e-06,
         -2.4587e-06, -2.7120e-06],
        [-3.0696e-06, -2.3842e-06,  2.3097e-06,  ..., -2.7120e-06,
         -2.2203e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9632707238197327


Running epoch 1, step 1481, batch 433
Sampled inputs[:2]: tensor([[   0, 7111,  409,  ..., 1908, 1260,  883],
        [   0, 1640, 1103,  ...,  685, 1478,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2194e-05,  1.8827e-04, -8.7949e-05,  ..., -2.6507e-05,
          2.4580e-04, -5.3627e-05],
        [-2.5257e-06, -1.7658e-06,  1.8217e-06,  ..., -2.1905e-06,
         -1.6242e-06, -1.7621e-06],
        [-7.5996e-06, -5.6475e-06,  5.9456e-06,  ..., -6.5863e-06,
         -4.9770e-06, -5.2601e-06],
        [-7.1824e-06, -5.0068e-06,  5.4687e-06,  ..., -6.3181e-06,
         -4.7535e-06, -5.2154e-06],
        [-6.0797e-06, -4.6343e-06,  4.6045e-06,  ..., -5.3644e-06,
         -4.2617e-06, -3.8892e-06]], device='cuda:0')
Loss: 0.9481003284454346


Running epoch 1, step 1482, batch 434
Sampled inputs[:2]: tensor([[    0,  1979,   352,  ...,   292,  1591,   446],
        [    0,    12, 12774,  ...,  1231,   278,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0581e-05,  2.7470e-04, -4.5543e-05,  ..., -6.7725e-06,
          2.9121e-04, -2.6998e-05],
        [-3.7253e-06, -2.6003e-06,  2.6897e-06,  ..., -3.2634e-06,
         -2.5183e-06, -2.6226e-06],
        [-1.1146e-05, -8.2254e-06,  8.6725e-06,  ..., -9.6560e-06,
         -7.5847e-06, -7.6741e-06],
        [-1.0639e-05, -7.3910e-06,  8.0913e-06,  ..., -9.3877e-06,
         -7.3612e-06, -7.7337e-06],
        [-9.0897e-06, -6.8992e-06,  6.8396e-06,  ..., -8.0019e-06,
         -6.5714e-06, -5.7742e-06]], device='cuda:0')
Loss: 0.9074697494506836


Running epoch 1, step 1483, batch 435
Sampled inputs[:2]: tensor([[    0,   471,    14,  ...,  1260,  2129,   367],
        [    0,   474,   221,  ..., 32291,   360,  2458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1623e-04,  3.8521e-04,  3.6338e-05,  ...,  1.8236e-04,
          1.8178e-04, -1.5174e-04],
        [-4.9844e-06, -3.4682e-06,  3.6582e-06,  ..., -4.3213e-06,
         -3.2559e-06, -3.4422e-06],
        [-1.4916e-05, -1.0997e-05,  1.1772e-05,  ..., -1.2845e-05,
         -9.8646e-06, -1.0133e-05],
        [-1.4290e-05, -9.8944e-06,  1.1057e-05,  ..., -1.2472e-05,
         -9.5516e-06, -1.0237e-05],
        [-1.1981e-05, -9.1046e-06,  9.1642e-06,  ..., -1.0490e-05,
         -8.4639e-06, -7.5102e-06]], device='cuda:0')
Loss: 0.9367361068725586


Running epoch 1, step 1484, batch 436
Sampled inputs[:2]: tensor([[   0,  437,  266,  ..., 5512,  822,   89],
        [   0, 2771,   13,  ..., 4169,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8105e-04,  4.9298e-04,  1.9448e-04,  ...,  1.6475e-04,
          3.3531e-04, -9.2086e-05],
        [-6.2063e-06, -4.4107e-06,  4.4964e-06,  ..., -5.4166e-06,
         -4.1798e-06, -4.2655e-06],
        [-1.8686e-05, -1.4052e-05,  1.4573e-05,  ..., -1.6198e-05,
         -1.2726e-05, -1.2621e-05],
        [-1.7717e-05, -1.2532e-05,  1.3530e-05,  ..., -1.5572e-05,
         -1.2219e-05, -1.2681e-05],
        [-1.4961e-05, -1.1578e-05,  1.1325e-05,  ..., -1.3173e-05,
         -1.0833e-05, -9.3207e-06]], device='cuda:0')
Loss: 0.9920452833175659


Running epoch 1, step 1485, batch 437
Sampled inputs[:2]: tensor([[    0,    12,   344,  ..., 10482,   950, 15744],
        [    0,   199,  1139,  ...,    13,  1303, 26330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1699e-04,  5.6772e-04,  1.5113e-04,  ...,  5.3606e-05,
          2.6908e-04, -2.6637e-04],
        [-7.4431e-06, -5.2936e-06,  5.4948e-06,  ..., -6.4448e-06,
         -4.9360e-06, -5.0478e-06],
        [-2.2352e-05, -1.6764e-05,  1.7717e-05,  ..., -1.9208e-05,
         -1.4961e-05, -1.4886e-05],
        [-2.1175e-05, -1.4961e-05,  1.6466e-05,  ..., -1.8433e-05,
         -1.4350e-05, -1.4961e-05],
        [-1.7807e-05, -1.3769e-05,  1.3724e-05,  ..., -1.5557e-05,
         -1.2703e-05, -1.0923e-05]], device='cuda:0')
Loss: 0.9670948386192322


Running epoch 1, step 1486, batch 438
Sampled inputs[:2]: tensor([[    0, 16803,   965,  ..., 36064,    12, 13769],
        [    0,   221,   380,  ...,   292,   334,   674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5584e-04,  7.5373e-04,  3.3518e-04,  ...,  1.0089e-04,
          9.1781e-05, -4.4326e-04],
        [-8.6874e-06, -6.1914e-06,  6.4075e-06,  ..., -7.5549e-06,
         -5.7518e-06, -5.8860e-06],
        [-2.6226e-05, -1.9714e-05,  2.0757e-05,  ..., -2.2635e-05,
         -1.7524e-05, -1.7464e-05],
        [-2.4796e-05, -1.7568e-05,  1.9252e-05,  ..., -2.1666e-05,
         -1.6749e-05, -1.7539e-05],
        [-2.0862e-05, -1.6168e-05,  1.6063e-05,  ..., -1.8299e-05,
         -1.4849e-05, -1.2800e-05]], device='cuda:0')
Loss: 0.9782671928405762


Running epoch 1, step 1487, batch 439
Sampled inputs[:2]: tensor([[   0, 4371, 4806,  ...,  685,  461,  654],
        [   0,  843, 3365,  ..., 1136, 1615,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5148e-04,  7.0085e-04,  1.2132e-04,  ...,  6.8052e-05,
          3.6633e-04, -5.2968e-04],
        [-9.9689e-06, -7.1526e-06,  7.3612e-06,  ..., -8.6427e-06,
         -6.5900e-06, -6.6869e-06],
        [-3.0160e-05, -2.2799e-05,  2.3872e-05,  ..., -2.5958e-05,
         -2.0102e-05, -1.9908e-05],
        [-2.8402e-05, -2.0266e-05,  2.2039e-05,  ..., -2.4751e-05,
         -1.9133e-05, -1.9878e-05],
        [-2.3842e-05, -1.8567e-05,  1.8358e-05,  ..., -2.0862e-05,
         -1.6920e-05, -1.4506e-05]], device='cuda:0')
Loss: 1.0011234283447266
Graident accumulation at epoch 1, step 1487, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.1575e-05,  1.6231e-04,  6.4793e-05,  ...,  3.4579e-05,
          4.4674e-05, -4.7802e-05],
        [-1.2589e-06, -1.5493e-06,  1.9448e-06,  ..., -2.6249e-06,
          6.4753e-07,  8.0629e-08],
        [ 1.2943e-05,  2.6386e-05, -1.1912e-05,  ...,  1.3564e-05,
          1.8740e-05, -3.5010e-06],
        [-1.3958e-05, -1.3718e-06,  1.1974e-05,  ..., -1.1369e-05,
         -3.7470e-06, -1.0211e-05],
        [-2.4693e-05, -1.8625e-05,  1.8072e-05,  ..., -2.1123e-05,
         -1.7223e-05, -1.5641e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3482e-08, 5.8638e-08, 4.4714e-08,  ..., 4.0223e-08, 1.2211e-07,
         5.2853e-08],
        [7.9182e-11, 5.8960e-11, 1.9109e-11,  ..., 5.5854e-11, 3.4999e-11,
         2.9826e-11],
        [4.0723e-09, 2.8528e-09, 1.2859e-09,  ..., 3.1698e-09, 1.2907e-09,
         1.0233e-09],
        [1.0990e-09, 1.1017e-09, 3.0394e-10,  ..., 9.1090e-10, 5.5993e-10,
         4.2500e-10],
        [3.5849e-10, 2.0443e-10, 7.5899e-11,  ..., 2.6902e-10, 7.5983e-11,
         1.0220e-10]], device='cuda:0')
optimizer state dict: 186.0
lr: [4.097971892163585e-06, 4.097971892163585e-06]
scheduler_last_epoch: 186


Running epoch 1, step 1488, batch 440
Sampled inputs[:2]: tensor([[   0,  759, 4585,  ...,  360,  300,  670],
        [   0,  462, 9202,  ...,   15, 3256,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0005e-04, -8.9447e-05, -1.0686e-05,  ...,  8.9576e-06,
          3.3994e-05,  1.1172e-04],
        [-1.2442e-06, -9.2760e-07,  9.9093e-07,  ..., -1.0058e-06,
         -7.7486e-07, -7.1153e-07],
        [-3.8147e-06, -2.9951e-06,  3.2485e-06,  ..., -3.1143e-06,
         -2.4289e-06, -2.2054e-06],
        [-3.5167e-06, -2.6077e-06,  2.9802e-06,  ..., -2.8610e-06,
         -2.2203e-06, -2.1458e-06],
        [-2.8461e-06, -2.3097e-06,  2.3544e-06,  ..., -2.3544e-06,
         -1.9222e-06, -1.4976e-06]], device='cuda:0')
Loss: 0.9855313897132874


Running epoch 1, step 1489, batch 441
Sampled inputs[:2]: tensor([[    0,  1032,   287,  ...,   266, 33161,  4728],
        [    0,   292, 21215,  ...,   266,   818,  1527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4233e-04,  8.8081e-05,  6.9667e-05,  ...,  8.7706e-05,
          6.9792e-05,  1.6462e-04],
        [-2.5108e-06, -1.8254e-06,  1.8515e-06,  ..., -2.1160e-06,
         -1.6093e-06, -1.5721e-06],
        [-7.6145e-06, -5.8264e-06,  6.0797e-06,  ..., -6.4224e-06,
         -4.9770e-06, -4.7535e-06],
        [-7.1377e-06, -5.1707e-06,  5.5879e-06,  ..., -6.0797e-06,
         -4.6790e-06, -4.7088e-06],
        [-5.7966e-06, -4.5747e-06,  4.5002e-06,  ..., -4.9770e-06,
         -4.0233e-06, -3.3230e-06]], device='cuda:0')
Loss: 0.9419840574264526


Running epoch 1, step 1490, batch 442
Sampled inputs[:2]: tensor([[    0, 48214,   287,  ...,   494,  8524,    12],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3954e-05,  9.8470e-06,  8.1557e-06,  ..., -1.0376e-05,
          2.5328e-05,  2.7437e-04],
        [-3.8520e-06, -2.6971e-06,  2.7828e-06,  ..., -3.2410e-06,
         -2.4773e-06, -2.5779e-06],
        [-1.1548e-05, -8.5086e-06,  9.0450e-06,  ..., -9.7007e-06,
         -7.5400e-06, -7.6145e-06],
        [-1.0729e-05, -7.4953e-06,  8.2403e-06,  ..., -9.1344e-06,
         -7.0482e-06, -7.4506e-06],
        [-9.1493e-06, -6.8843e-06,  6.9588e-06,  ..., -7.8231e-06,
         -6.3181e-06, -5.6028e-06]], device='cuda:0')
Loss: 0.9591368436813354


Running epoch 1, step 1491, batch 443
Sampled inputs[:2]: tensor([[    0,   287,   266,  ..., 10238,    12, 39004],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6151e-04, -6.1741e-05,  1.4784e-04,  ..., -7.9554e-05,
          6.4085e-05,  1.5537e-04],
        [-5.0887e-06, -3.6098e-06,  3.7439e-06,  ..., -4.2617e-06,
         -3.2559e-06, -3.4124e-06],
        [-1.5482e-05, -1.1533e-05,  1.2279e-05,  ..., -1.2949e-05,
         -1.0028e-05, -1.0267e-05],
        [-1.4439e-05, -1.0192e-05,  1.1250e-05,  ..., -1.2189e-05,
         -9.3877e-06, -1.0073e-05],
        [-1.2159e-05, -9.2983e-06,  9.3877e-06,  ..., -1.0356e-05,
         -8.3596e-06, -7.4729e-06]], device='cuda:0')
Loss: 0.9742648601531982


Running epoch 1, step 1492, batch 444
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,  6673,   298,  ...,  4391,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1009e-04, -1.6116e-04,  2.6992e-05,  ...,  7.2298e-05,
         -1.1182e-04,  1.5690e-04],
        [-6.3553e-06, -4.5002e-06,  4.7274e-06,  ..., -5.3123e-06,
         -4.0270e-06, -4.2729e-06],
        [-1.9327e-05, -1.4320e-05,  1.5482e-05,  ..., -1.6078e-05,
         -1.2383e-05, -1.2785e-05],
        [-1.8045e-05, -1.2696e-05,  1.4231e-05,  ..., -1.5169e-05,
         -1.1623e-05, -1.2636e-05],
        [-1.5140e-05, -1.1519e-05,  1.1832e-05,  ..., -1.2830e-05,
         -1.0312e-05, -9.2834e-06]], device='cuda:0')
Loss: 0.931065022945404


Running epoch 1, step 1493, batch 445
Sampled inputs[:2]: tensor([[    0,  1445,  3597,  ...,   281,    78,     9],
        [    0,   292,   380,  ...,   287, 10086,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4076e-04, -2.9342e-04, -1.1499e-05,  ...,  5.1047e-05,
          1.9893e-04,  3.6897e-04],
        [-7.6443e-06, -5.4054e-06,  5.6140e-06,  ..., -6.4671e-06,
         -4.8764e-06, -5.1484e-06],
        [-2.3171e-05, -1.7181e-05,  1.8358e-05,  ..., -1.9506e-05,
         -1.4991e-05, -1.5348e-05],
        [-2.1651e-05, -1.5214e-05,  1.6838e-05,  ..., -1.8418e-05,
         -1.4067e-05, -1.5199e-05],
        [-1.8224e-05, -1.3858e-05,  1.4082e-05,  ..., -1.5602e-05,
         -1.2517e-05, -1.1206e-05]], device='cuda:0')
Loss: 0.9759564399719238


Running epoch 1, step 1494, batch 446
Sampled inputs[:2]: tensor([[    0,  1978,   352,  ...,  2276,    12,   221],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4606e-05, -3.8035e-04, -1.1669e-04,  ...,  8.5447e-06,
          2.6644e-04,  2.3816e-04],
        [-8.8960e-06, -6.2995e-06,  6.5640e-06,  ..., -7.5176e-06,
         -5.6475e-06, -5.9418e-06],
        [-2.6852e-05, -1.9938e-05,  2.1368e-05,  ..., -2.2575e-05,
         -1.7300e-05, -1.7628e-05],
        [-2.5183e-05, -1.7717e-05,  1.9670e-05,  ..., -2.1383e-05,
         -1.6287e-05, -1.7539e-05],
        [-2.1189e-05, -1.6168e-05,  1.6436e-05,  ..., -1.8105e-05,
         -1.4499e-05, -1.2882e-05]], device='cuda:0')
Loss: 0.9673301577568054


Running epoch 1, step 1495, batch 447
Sampled inputs[:2]: tensor([[    0,   266,  1336,  ...,  1841,  9705,  1219],
        [    0,   278,  6653,  ...,  7524,   271, 28279]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4128e-04, -3.5555e-04, -1.4652e-04,  ..., -2.0626e-05,
          4.3781e-04,  2.9906e-04],
        [-1.0103e-05, -7.1712e-06,  7.4543e-06,  ..., -8.5682e-06,
         -6.4708e-06, -6.8136e-06],
        [-3.0443e-05, -2.2620e-05,  2.4259e-05,  ..., -2.5630e-05,
         -1.9774e-05, -2.0087e-05],
        [-2.8700e-05, -2.0236e-05,  2.2426e-05,  ..., -2.4438e-05,
         -1.8731e-05, -2.0131e-05],
        [-2.4095e-05, -1.8403e-05,  1.8731e-05,  ..., -2.0608e-05,
         -1.6615e-05, -1.4707e-05]], device='cuda:0')
Loss: 0.9663630127906799
Graident accumulation at epoch 1, step 1495, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.3290e-05,  1.1052e-04,  4.3662e-05,  ...,  2.9058e-05,
          8.3988e-05, -1.3116e-05],
        [-2.1434e-06, -2.1115e-06,  2.4957e-06,  ..., -3.2192e-06,
         -6.4303e-08, -6.0879e-07],
        [ 8.6041e-06,  2.1485e-05, -8.2949e-06,  ...,  9.6446e-06,
          1.4888e-05, -5.1596e-06],
        [-1.5432e-05, -3.2582e-06,  1.3019e-05,  ..., -1.2676e-05,
         -5.2454e-06, -1.1203e-05],
        [-2.4633e-05, -1.8603e-05,  1.8138e-05,  ..., -2.1072e-05,
         -1.7162e-05, -1.5548e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3429e-08, 5.8706e-08, 4.4691e-08,  ..., 4.0184e-08, 1.2218e-07,
         5.2890e-08],
        [7.9205e-11, 5.8953e-11, 1.9145e-11,  ..., 5.5872e-11, 3.5006e-11,
         2.9843e-11],
        [4.0692e-09, 2.8505e-09, 1.2852e-09,  ..., 3.1673e-09, 1.2898e-09,
         1.0227e-09],
        [1.0987e-09, 1.1010e-09, 3.0414e-10,  ..., 9.1059e-10, 5.5972e-10,
         4.2498e-10],
        [3.5872e-10, 2.0456e-10, 7.6174e-11,  ..., 2.6918e-10, 7.6183e-11,
         1.0231e-10]], device='cuda:0')
optimizer state dict: 187.0
lr: [3.998635243264737e-06, 3.998635243264737e-06]
scheduler_last_epoch: 187


Running epoch 1, step 1496, batch 448
Sampled inputs[:2]: tensor([[   0, 2667,  365,  ..., 9281, 1631, 9123],
        [   0, 1774, 1781,  ..., 4685,  409, 4614]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6180e-05,  1.7329e-05,  6.0584e-06,  ..., -3.1824e-05,
          1.3201e-04,  8.9322e-06],
        [-1.2070e-06, -8.8662e-07,  8.9779e-07,  ..., -1.0505e-06,
         -8.2329e-07, -8.4192e-07],
        [-3.6955e-06, -2.8014e-06,  2.9355e-06,  ..., -3.1739e-06,
         -2.4885e-06, -2.5481e-06],
        [-3.5465e-06, -2.5630e-06,  2.7716e-06,  ..., -3.0845e-06,
         -2.4140e-06, -2.5779e-06],
        [-2.9206e-06, -2.2948e-06,  2.2650e-06,  ..., -2.5630e-06,
         -2.0862e-06, -1.8626e-06]], device='cuda:0')
Loss: 1.0006974935531616


Running epoch 1, step 1497, batch 449
Sampled inputs[:2]: tensor([[    0, 18717,  2837,  ...,    48,    18,   292],
        [    0,  2356,   292,  ...,    12,   287,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7280e-04,  1.1716e-04,  9.3639e-05,  ..., -5.6692e-05,
          1.3201e-04,  2.9144e-05],
        [-2.3618e-06, -1.7658e-06,  1.6615e-06,  ..., -2.1607e-06,
         -1.7658e-06, -1.6801e-06],
        [-7.1377e-06, -5.4985e-06,  5.4389e-06,  ..., -6.3777e-06,
         -5.2303e-06, -4.9323e-06],
        [-6.8396e-06, -5.0068e-06,  5.0813e-06,  ..., -6.2436e-06,
         -5.1260e-06, -5.0962e-06],
        [-5.8264e-06, -4.6194e-06,  4.3362e-06,  ..., -5.2750e-06,
         -4.4703e-06, -3.6955e-06]], device='cuda:0')
Loss: 0.9629541039466858


Running epoch 1, step 1498, batch 450
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0,    16,    14,  ...,   300,  9283,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7485e-07,  1.6663e-04,  1.0205e-04,  ...,  3.8795e-05,
          3.2114e-04, -1.3140e-04],
        [-3.6061e-06, -2.6710e-06,  2.4289e-06,  ..., -3.2485e-06,
         -2.6971e-06, -2.5965e-06],
        [-1.0908e-05, -8.3745e-06,  8.0615e-06,  ..., -9.6262e-06,
         -8.0913e-06, -7.6443e-06],
        [-1.0356e-05, -7.5698e-06,  7.4059e-06,  ..., -9.3430e-06,
         -7.8678e-06, -7.8082e-06],
        [-8.8811e-06, -7.0333e-06,  6.4224e-06,  ..., -7.9572e-06,
         -6.9290e-06, -5.7220e-06]], device='cuda:0')
Loss: 0.9602984189987183


Running epoch 1, step 1499, batch 451
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,   278,  2305,  ...,  2529, 34181,  4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3071e-04,  4.6765e-04,  4.5212e-04,  ...,  4.1061e-05,
          5.7901e-04, -6.5044e-05],
        [-4.8429e-06, -3.5688e-06,  3.1516e-06,  ..., -4.3213e-06,
         -3.6471e-06, -3.4757e-06],
        [-1.4722e-05, -1.1265e-05,  1.0520e-05,  ..., -1.2904e-05,
         -1.1027e-05, -1.0312e-05],
        [-1.4022e-05, -1.0192e-05,  9.5963e-06,  ..., -1.2562e-05,
         -1.0818e-05, -1.0565e-05],
        [-1.1936e-05, -9.4026e-06,  8.3447e-06,  ..., -1.0625e-05,
         -9.3877e-06, -7.7039e-06]], device='cuda:0')
Loss: 0.9612802267074585


Running epoch 1, step 1500, batch 452
Sampled inputs[:2]: tensor([[    0,   278, 10875,  ...,   445,   267,    14],
        [    0,   272,   352,  ...,   590,  4361,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8953e-04,  3.2493e-04,  3.5267e-04,  ...,  4.3410e-05,
          9.2513e-04,  5.4110e-05],
        [-5.9679e-06, -4.4294e-06,  4.0010e-06,  ..., -5.3346e-06,
         -4.4405e-06, -4.2543e-06],
        [-1.8194e-05, -1.3977e-05,  1.3366e-05,  ..., -1.5974e-05,
         -1.3441e-05, -1.2651e-05],
        [-1.7256e-05, -1.2636e-05,  1.2189e-05,  ..., -1.5482e-05,
         -1.3143e-05, -1.2919e-05],
        [-1.4603e-05, -1.1578e-05,  1.0490e-05,  ..., -1.3009e-05,
         -1.1355e-05, -9.3356e-06]], device='cuda:0')
Loss: 0.9853555560112


Running epoch 1, step 1501, batch 453
Sampled inputs[:2]: tensor([[   0,   47,   12,  ..., 4367,  278,  471],
        [   0,  278,  638,  ...,  278,  266, 9387]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9339e-04,  3.6145e-04,  3.4941e-04,  ...,  9.8301e-06,
          9.1285e-04,  4.1853e-05],
        [-7.2122e-06, -5.2713e-06,  4.9360e-06,  ..., -6.3479e-06,
         -5.1931e-06, -5.1260e-06],
        [-2.2009e-05, -1.6689e-05,  1.6421e-05,  ..., -1.9059e-05,
         -1.5780e-05, -1.5259e-05],
        [-2.0862e-05, -1.5035e-05,  1.5035e-05,  ..., -1.8403e-05,
         -1.5333e-05, -1.5557e-05],
        [-1.7554e-05, -1.3769e-05,  1.2800e-05,  ..., -1.5438e-05,
         -1.3307e-05, -1.1191e-05]], device='cuda:0')
Loss: 0.966567873954773


Running epoch 1, step 1502, batch 454
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,  4494,    12,  ...,   341,  1619,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9588e-04,  3.4704e-04,  3.8997e-04,  ..., -5.4358e-05,
          1.1368e-03,  1.7537e-04],
        [-8.4266e-06, -6.1467e-06,  5.6811e-06,  ..., -7.4431e-06,
         -6.1281e-06, -6.0052e-06],
        [-2.5690e-05, -1.9401e-05,  1.8910e-05,  ..., -2.2277e-05,
         -1.8492e-05, -1.7807e-05],
        [-2.4408e-05, -1.7464e-05,  1.7315e-05,  ..., -2.1577e-05,
         -1.8060e-05, -1.8254e-05],
        [-2.0653e-05, -1.6108e-05,  1.4842e-05,  ..., -1.8179e-05,
         -1.5691e-05, -1.3143e-05]], device='cuda:0')
Loss: 0.9477183222770691


Running epoch 1, step 1503, batch 455
Sampled inputs[:2]: tensor([[    0,    13,  2549,  ...,   221,   382,   298],
        [    0,   342,  3001,  ...,   369, 11195,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9147e-04,  5.2320e-04,  6.2276e-04,  ..., -1.1872e-06,
          1.3164e-03,  2.0625e-04],
        [-9.6485e-06, -6.9626e-06,  6.5342e-06,  ..., -8.5384e-06,
         -6.9253e-06, -6.9253e-06],
        [-2.9370e-05, -2.1994e-05,  2.1711e-05,  ..., -2.5541e-05,
         -2.0936e-05, -2.0519e-05],
        [-2.7895e-05, -1.9744e-05,  1.9863e-05,  ..., -2.4721e-05,
         -2.0400e-05, -2.0966e-05],
        [-2.3574e-05, -1.8224e-05,  1.7002e-05,  ..., -2.0832e-05,
         -1.7762e-05, -1.5140e-05]], device='cuda:0')
Loss: 0.9388983845710754
Graident accumulation at epoch 1, step 1503, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0139, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8186e-05,  1.5179e-04,  1.0157e-04,  ...,  2.6034e-05,
          2.0723e-04,  8.8207e-06],
        [-2.8939e-06, -2.5966e-06,  2.8996e-06,  ..., -3.7512e-06,
         -7.5040e-07, -1.2404e-06],
        [ 4.8067e-06,  1.7137e-05, -5.2943e-06,  ...,  6.1261e-06,
          1.1306e-05, -6.6955e-06],
        [-1.6678e-05, -4.9068e-06,  1.3703e-05,  ..., -1.3880e-05,
         -6.7608e-06, -1.2180e-05],
        [-2.4527e-05, -1.8565e-05,  1.8025e-05,  ..., -2.1048e-05,
         -1.7222e-05, -1.5507e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3597e-08, 5.8921e-08, 4.5034e-08,  ..., 4.0143e-08, 1.2379e-07,
         5.2880e-08],
        [7.9219e-11, 5.8942e-11, 1.9169e-11,  ..., 5.5889e-11, 3.5019e-11,
         2.9861e-11],
        [4.0660e-09, 2.8481e-09, 1.2844e-09,  ..., 3.1647e-09, 1.2890e-09,
         1.0221e-09],
        [1.0984e-09, 1.1003e-09, 3.0423e-10,  ..., 9.1029e-10, 5.5958e-10,
         4.2500e-10],
        [3.5891e-10, 2.0469e-10, 7.6387e-11,  ..., 2.6934e-10, 7.6422e-11,
         1.0244e-10]], device='cuda:0')
optimizer state dict: 188.0
lr: [3.900215655287364e-06, 3.900215655287364e-06]
scheduler_last_epoch: 188


Running epoch 1, step 1504, batch 456
Sampled inputs[:2]: tensor([[    0,   292,    33,  ..., 32754,   300, 14476],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0069e-05, -1.3190e-05, -3.6318e-06,  ...,  9.6506e-06,
         -5.8813e-05,  2.9199e-06],
        [-1.2070e-06, -8.4192e-07,  8.8662e-07,  ..., -1.0282e-06,
         -7.6368e-07, -8.2329e-07],
        [-3.6061e-06, -2.6226e-06,  2.8908e-06,  ..., -3.0398e-06,
         -2.2799e-06, -2.4140e-06],
        [-3.4273e-06, -2.3693e-06,  2.6971e-06,  ..., -2.9057e-06,
         -2.2054e-06, -2.4587e-06],
        [-2.7716e-06, -2.1160e-06,  2.1607e-06,  ..., -2.3693e-06,
         -1.8924e-06, -1.6764e-06]], device='cuda:0')
Loss: 0.9553857445716858


Running epoch 1, step 1505, batch 457
Sampled inputs[:2]: tensor([[    0,    13,   711,  ...,   591,   953,   352],
        [    0,   221,   474,  ..., 10688,  7988, 25842]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2814e-05, -3.0498e-05, -7.3442e-06,  ..., -2.6479e-05,
         -1.9231e-04, -2.1947e-04],
        [-2.4140e-06, -1.7136e-06,  1.8552e-06,  ..., -2.0638e-06,
         -1.6503e-06, -1.6876e-06],
        [-7.2122e-06, -5.3346e-06,  5.9903e-06,  ..., -6.1095e-06,
         -4.9472e-06, -4.9025e-06],
        [-6.8098e-06, -4.7684e-06,  5.5581e-06,  ..., -5.8115e-06,
         -4.7386e-06, -4.9621e-06],
        [-5.6177e-06, -4.3362e-06,  4.5449e-06,  ..., -4.8280e-06,
         -4.1127e-06, -3.4794e-06]], device='cuda:0')
Loss: 0.9475914239883423


Running epoch 1, step 1506, batch 458
Sampled inputs[:2]: tensor([[    0, 12324,  7368,  ...,   365,   726,  3595],
        [    0,   560,   199,  ...,   292, 12605,  2096]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9362e-05, -1.8588e-04,  9.5216e-05,  ..., -9.6861e-05,
          6.1736e-05, -2.1371e-04],
        [-3.7923e-06, -2.5779e-06,  2.7493e-06,  ..., -3.1963e-06,
         -2.5593e-06, -2.7604e-06],
        [ 4.7034e-05,  1.1031e-04, -4.0747e-05,  ...,  5.6098e-05,
          7.9193e-05,  1.1451e-05],
        [-1.0610e-05, -7.1228e-06,  8.1509e-06,  ..., -8.9556e-06,
         -7.3016e-06, -8.0019e-06],
        [-9.0450e-06, -6.6459e-06,  6.8843e-06,  ..., -7.6741e-06,
         -6.5118e-06, -5.9381e-06]], device='cuda:0')
Loss: 0.9311524629592896


Running epoch 1, step 1507, batch 459
Sampled inputs[:2]: tensor([[    0,   334,   344,  ...,   266,  4141,   287],
        [    0,  4347,   638,  ...,  1345,   292, 15343]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2830e-04, -3.4594e-04, -7.1793e-05,  ..., -1.2959e-04,
          1.8765e-05, -3.6340e-04],
        [-5.0217e-06, -3.4571e-06,  3.6433e-06,  ..., -4.2468e-06,
         -3.4273e-06, -3.5800e-06],
        [ 4.3279e-05,  1.0758e-04, -3.7826e-05,  ...,  5.2939e-05,
          7.6600e-05,  9.0073e-06],
        [-1.4141e-05, -9.5516e-06,  1.0848e-05,  ..., -1.1951e-05,
         -9.7752e-06, -1.0461e-05],
        [-1.2100e-05, -8.9556e-06,  9.2089e-06,  ..., -1.0282e-05,
         -8.7470e-06, -7.7710e-06]], device='cuda:0')
Loss: 0.9713211059570312


Running epoch 1, step 1508, batch 460
Sampled inputs[:2]: tensor([[    0,   300,  7239,  ...,  2283,  4890,    14],
        [    0,   269,    12,  ..., 45645,    14,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6868e-04, -4.5554e-04, -3.0210e-04,  ..., -1.2742e-04,
          8.1433e-05, -4.7760e-04],
        [-6.4448e-06, -4.3400e-06,  4.5449e-06,  ..., -5.4017e-06,
         -4.2804e-06, -4.6231e-06],
        [ 3.9225e-05,  1.0489e-04, -3.4980e-05,  ...,  4.9675e-05,
          7.4082e-05,  6.1463e-06],
        [-1.7911e-05, -1.1891e-05,  1.3411e-05,  ..., -1.5035e-05,
         -1.2130e-05, -1.3247e-05],
        [-1.5602e-05, -1.1310e-05,  1.1548e-05,  ..., -1.3158e-05,
         -1.1057e-05, -1.0096e-05]], device='cuda:0')
Loss: 0.9439239501953125


Running epoch 1, step 1509, batch 461
Sampled inputs[:2]: tensor([[    0,   278,  1059,  ...,   300,  1877,    13],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6411e-04, -4.2126e-04, -3.1488e-04,  ..., -3.8829e-05,
          5.7887e-05, -4.8719e-04],
        [-7.7114e-06, -5.2862e-06,  5.4725e-06,  ..., -6.4448e-06,
         -5.1595e-06, -5.4538e-06],
        [ 3.5485e-05,  1.0194e-04, -3.2000e-05,  ...,  4.6591e-05,
          7.1399e-05,  3.7323e-06],
        [-2.1487e-05, -1.4573e-05,  1.6212e-05,  ..., -1.8001e-05,
         -1.4693e-05, -1.5706e-05],
        [-1.8507e-05, -1.3679e-05,  1.3813e-05,  ..., -1.5602e-05,
         -1.3262e-05, -1.1817e-05]], device='cuda:0')
Loss: 0.9571828246116638


Running epoch 1, step 1510, batch 462
Sampled inputs[:2]: tensor([[    0,   650,    14,  ...,  6330,   221,   494],
        [    0,   221,   474,  ..., 19245,   565,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4051e-04, -6.0929e-04, -1.4485e-04,  ..., -1.0448e-04,
          3.1753e-04, -4.0177e-04],
        [-9.0376e-06, -6.1393e-06,  6.2808e-06,  ..., -7.6443e-06,
         -6.1207e-06, -6.5118e-06],
        [ 3.1462e-05,  9.9269e-05, -2.9318e-05,  ...,  4.3029e-05,
          6.8538e-05,  6.6267e-07],
        [-2.5272e-05, -1.6972e-05,  1.8641e-05,  ..., -2.1443e-05,
         -1.7464e-05, -1.8790e-05],
        [-2.2054e-05, -1.6019e-05,  1.6093e-05,  ..., -1.8761e-05,
         -1.5840e-05, -1.4395e-05]], device='cuda:0')
Loss: 0.9722252488136292


Running epoch 1, step 1511, batch 463
Sampled inputs[:2]: tensor([[   0,  795, 1445,  ..., 6292,  287, 9782],
        [   0,  328,  266,  ...,  382,   17,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5768e-04, -7.3812e-04, -3.0862e-04,  ..., -1.0685e-04,
          3.6653e-04, -3.9759e-04],
        [-1.0349e-05, -7.0110e-06,  7.1190e-06,  ..., -8.7544e-06,
         -7.0147e-06, -7.4506e-06],
        [ 2.7528e-05,  9.6513e-05, -2.6561e-05,  ...,  3.9736e-05,
          6.5841e-05, -2.1239e-06],
        [-2.8953e-05, -1.9416e-05,  2.1145e-05,  ..., -2.4587e-05,
         -2.0027e-05, -2.1547e-05],
        [-2.5287e-05, -1.8343e-05,  1.8284e-05,  ..., -2.1532e-05,
         -1.8209e-05, -1.6525e-05]], device='cuda:0')
Loss: 0.9727628827095032
Graident accumulation at epoch 1, step 1511, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5994e-06,  6.2800e-05,  6.0553e-05,  ...,  1.2746e-05,
          2.2316e-04, -3.1820e-05],
        [-3.6394e-06, -3.0380e-06,  3.3215e-06,  ..., -4.2515e-06,
         -1.3768e-06, -1.8615e-06],
        [ 7.0788e-06,  2.5075e-05, -7.4210e-06,  ...,  9.4871e-06,
          1.6759e-05, -6.2384e-06],
        [-1.7906e-05, -6.3577e-06,  1.4448e-05,  ..., -1.4951e-05,
         -8.0874e-06, -1.3116e-05],
        [-2.4603e-05, -1.8543e-05,  1.8051e-05,  ..., -2.1096e-05,
         -1.7321e-05, -1.5609e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3548e-08, 5.9407e-08, 4.5084e-08,  ..., 4.0115e-08, 1.2380e-07,
         5.2985e-08],
        [7.9247e-11, 5.8933e-11, 1.9201e-11,  ..., 5.5910e-11, 3.5033e-11,
         2.9886e-11],
        [4.0627e-09, 2.8546e-09, 1.2838e-09,  ..., 3.1632e-09, 1.2920e-09,
         1.0211e-09],
        [1.0982e-09, 1.0995e-09, 3.0437e-10,  ..., 9.0998e-10, 5.5942e-10,
         4.2503e-10],
        [3.5919e-10, 2.0482e-10, 7.6644e-11,  ..., 2.6953e-10, 7.6677e-11,
         1.0261e-10]], device='cuda:0')
optimizer state dict: 189.0
lr: [3.80272816760364e-06, 3.80272816760364e-06]
scheduler_last_epoch: 189


Running epoch 1, step 1512, batch 464
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0, 4350,   14,  ...,  266, 9479,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1172e-06, -1.3704e-04, -3.3145e-05,  ..., -2.0198e-05,
         -5.5269e-05,  1.1145e-05],
        [-1.2517e-06, -8.8289e-07,  1.0356e-06,  ..., -1.0058e-06,
         -7.8604e-07, -8.5309e-07],
        [-3.9041e-06, -2.8163e-06,  3.3975e-06,  ..., -3.1441e-06,
         -2.4587e-06, -2.6375e-06],
        [-3.5912e-06, -2.5034e-06,  3.1292e-06,  ..., -2.8908e-06,
         -2.2650e-06, -2.5332e-06],
        [-2.8908e-06, -2.1905e-06,  2.4587e-06,  ..., -2.3842e-06,
         -1.9670e-06, -1.8179e-06]], device='cuda:0')
Loss: 0.9639421105384827


Running epoch 1, step 1513, batch 465
Sampled inputs[:2]: tensor([[    0, 21748,   792,  ...,   408,   266, 31879],
        [    0,   957,  1357,  ..., 26179,   287,  6458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3807e-04, -2.2144e-04, -1.7991e-04,  ..., -7.8379e-05,
          3.5155e-05,  2.2016e-04],
        [-2.5034e-06, -1.7360e-06,  1.8738e-06,  ..., -2.0936e-06,
         -1.7360e-06, -1.7993e-06],
        [ 6.6357e-05,  9.6218e-05, -6.6151e-05,  ...,  6.0320e-05,
          7.4312e-05,  4.2289e-05],
        [-7.3314e-06, -5.0068e-06,  5.7369e-06,  ..., -6.1542e-06,
         -5.1409e-06, -5.4687e-06],
        [-6.1393e-06, -4.5449e-06,  4.7535e-06,  ..., -5.2154e-06,
         -4.5151e-06, -4.0531e-06]], device='cuda:0')
Loss: 1.0041896104812622


Running epoch 1, step 1514, batch 466
Sampled inputs[:2]: tensor([[    0,   729,  3430,  ...,  9715,    13, 42383],
        [    0,  1062,   648,  ...,   266,  4939,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8243e-04, -2.7129e-04, -3.3560e-04,  ..., -3.1638e-05,
          8.7810e-06,  2.3247e-04],
        [-3.7178e-06, -2.5444e-06,  2.8275e-06,  ..., -3.1069e-06,
         -2.4922e-06, -2.7381e-06],
        [ 6.2646e-05,  9.3670e-05, -6.3021e-05,  ...,  5.7281e-05,
          7.2002e-05,  3.9517e-05],
        [-1.0788e-05, -7.2420e-06,  8.6129e-06,  ..., -9.0152e-06,
         -7.3016e-06, -8.2105e-06],
        [-8.9705e-06, -6.5714e-06,  7.0781e-06,  ..., -7.5698e-06,
         -6.4075e-06, -5.9754e-06]], device='cuda:0')
Loss: 0.9380664229393005


Running epoch 1, step 1515, batch 467
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0,  292,  474,  ..., 1085,  494, 2665]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3439e-04, -3.6679e-04, -4.6139e-04,  ..., -1.0053e-04,
          1.1280e-04,  2.1521e-04],
        [-4.9472e-06, -3.3639e-06,  3.6322e-06,  ..., -4.1723e-06,
         -3.2820e-06, -3.6694e-06],
        [ 5.8832e-05,  9.1003e-05, -6.0265e-05,  ...,  5.4047e-05,
          6.9574e-05,  3.6745e-05],
        [-1.4305e-05, -9.5218e-06,  1.1042e-05,  ..., -1.2070e-05,
         -9.5963e-06, -1.0967e-05],
        [-1.1966e-05, -8.7172e-06,  9.1940e-06,  ..., -1.0148e-05,
         -8.4341e-06, -7.9721e-06]], device='cuda:0')
Loss: 0.948153018951416


Running epoch 1, step 1516, batch 468
Sampled inputs[:2]: tensor([[    0,  2296,   446,  ...,  2937,   287,  2795],
        [    0,   278,   264,  ..., 21836,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3439e-04, -4.9727e-04, -4.9037e-04,  ..., -1.0911e-04,
          1.9225e-05,  2.6584e-04],
        [-6.1989e-06, -4.2208e-06,  4.6976e-06,  ..., -5.1707e-06,
         -4.0457e-06, -4.5486e-06],
        [ 5.5032e-05,  8.8261e-05, -5.6882e-05,  ...,  5.0992e-05,
          6.7204e-05,  3.4033e-05],
        [-1.7911e-05, -1.1966e-05,  1.4246e-05,  ..., -1.4946e-05,
         -1.1817e-05, -1.3620e-05],
        [-1.4842e-05, -1.0893e-05,  1.1697e-05,  ..., -1.2532e-05,
         -1.0386e-05, -9.8795e-06]], device='cuda:0')
Loss: 0.9584836959838867


Running epoch 1, step 1517, batch 469
Sampled inputs[:2]: tensor([[    0,  4890,  1528,  ...,   847,   328,  1703],
        [    0, 43788,    12,  ...,    12,  6288,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6163e-04, -5.9582e-04, -6.2841e-04,  ..., -1.1217e-04,
          1.8251e-04,  2.9461e-04],
        [-7.4357e-06, -5.0329e-06,  5.5544e-06,  ..., -6.2138e-06,
         -4.8801e-06, -5.4836e-06],
        [ 5.1247e-05,  8.5683e-05, -5.3991e-05,  ...,  4.7848e-05,
          6.4656e-05,  3.1292e-05],
        [-2.1443e-05, -1.4246e-05,  1.6853e-05,  ..., -1.7911e-05,
         -1.4231e-05, -1.6361e-05],
        [-1.7837e-05, -1.2994e-05,  1.3918e-05,  ..., -1.5050e-05,
         -1.2532e-05, -1.1876e-05]], device='cuda:0')
Loss: 0.9553775191307068


Running epoch 1, step 1518, batch 470
Sampled inputs[:2]: tensor([[   0,  377,  472,  ..., 9256, 3807, 5499],
        [   0,  328, 5180,  ...,  344, 2356,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3208e-04, -5.7929e-04, -8.0017e-04,  ...,  7.6303e-05,
         -9.0798e-05,  2.1703e-05],
        [-8.6874e-06, -5.8897e-06,  6.4410e-06,  ..., -7.2867e-06,
         -5.7258e-06, -6.3442e-06],
        [ 4.7552e-05,  8.3090e-05, -5.1130e-05,  ...,  4.4778e-05,
          6.2197e-05,  2.8863e-05],
        [-2.4945e-05, -1.6585e-05,  1.9476e-05,  ..., -2.0877e-05,
         -1.6600e-05, -1.8820e-05],
        [-2.0802e-05, -1.5154e-05,  1.6138e-05,  ..., -1.7554e-05,
         -1.4633e-05, -1.3642e-05]], device='cuda:0')
Loss: 0.953542172908783


Running epoch 1, step 1519, batch 471
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,    14,   298,  ...,   333,   199,   769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0679e-04, -6.6110e-04, -9.0936e-04,  ...,  1.8413e-04,
          2.0875e-04,  1.3321e-04],
        [-9.9838e-06, -6.6720e-06,  7.2494e-06,  ..., -8.4490e-06,
         -6.5975e-06, -7.3873e-06],
        [ 4.3707e-05,  8.0631e-05, -4.8463e-05,  ...,  4.1470e-05,
          5.9694e-05,  2.5972e-05],
        [-2.8566e-05, -1.8716e-05,  2.1830e-05,  ..., -2.4095e-05,
         -1.9029e-05, -2.1800e-05],
        [-2.4140e-05, -1.7256e-05,  1.8373e-05,  ..., -2.0459e-05,
         -1.6898e-05, -1.6011e-05]], device='cuda:0')
Loss: 0.9376960396766663
Graident accumulation at epoch 1, step 1519, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.9319e-05, -9.5902e-06, -3.6439e-05,  ...,  2.9884e-05,
          2.2172e-04, -1.5318e-05],
        [-4.2738e-06, -3.4014e-06,  3.7143e-06,  ..., -4.6712e-06,
         -1.8989e-06, -2.4140e-06],
        [ 1.0742e-05,  3.0631e-05, -1.1525e-05,  ...,  1.2685e-05,
          2.1053e-05, -3.0173e-06],
        [-1.8972e-05, -7.5935e-06,  1.5186e-05,  ..., -1.5865e-05,
         -9.1816e-06, -1.3985e-05],
        [-2.4557e-05, -1.8414e-05,  1.8083e-05,  ..., -2.1033e-05,
         -1.7279e-05, -1.5649e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3732e-08, 5.9785e-08, 4.5866e-08,  ..., 4.0108e-08, 1.2372e-07,
         5.2950e-08],
        [7.9267e-11, 5.8918e-11, 1.9234e-11,  ..., 5.5925e-11, 3.5042e-11,
         2.9911e-11],
        [4.0605e-09, 2.8582e-09, 1.2849e-09,  ..., 3.1617e-09, 1.2943e-09,
         1.0207e-09],
        [1.0979e-09, 1.0988e-09, 3.0454e-10,  ..., 9.0965e-10, 5.5922e-10,
         4.2508e-10],
        [3.5942e-10, 2.0491e-10, 7.6905e-11,  ..., 2.6968e-10, 7.6886e-11,
         1.0277e-10]], device='cuda:0')
optimizer state dict: 190.0
lr: [3.7061876771526483e-06, 3.7061876771526483e-06]
scheduler_last_epoch: 190


Running epoch 1, step 1520, batch 472
Sampled inputs[:2]: tensor([[    0,   300, 12579,  ...,  1722,   369,  5049],
        [    0,   300, 16683,  ...,  8709,    40,  9817]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0700e-04, -6.7601e-05, -6.6952e-05,  ...,  1.9967e-06,
          9.5793e-05, -2.5828e-05],
        [-1.3039e-06, -8.9407e-07,  9.2760e-07,  ..., -1.0431e-06,
         -8.6799e-07, -9.0525e-07],
        [-3.9637e-06, -2.8461e-06,  3.0547e-06,  ..., -3.1441e-06,
         -2.6524e-06, -2.7418e-06],
        [-3.6508e-06, -2.4885e-06,  2.7567e-06,  ..., -2.9206e-06,
         -2.4587e-06, -2.6375e-06],
        [-3.0100e-06, -2.2352e-06,  2.2501e-06,  ..., -2.4587e-06,
         -2.1458e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9647676348686218


Running epoch 1, step 1521, batch 473
Sampled inputs[:2]: tensor([[   0,  368, 2418,  ..., 3275, 1116, 5189],
        [   0, 3968,  446,  ...,   22,  722,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1553e-04, -7.5447e-05, -1.6279e-04,  ..., -1.0969e-04,
          1.8632e-04,  1.2029e-04],
        [-2.5108e-06, -1.8142e-06,  1.7919e-06,  ..., -2.0564e-06,
         -1.7807e-06, -1.7807e-06],
        [-7.7337e-06, -5.8860e-06,  5.9456e-06,  ..., -6.3181e-06,
         -5.5730e-06, -5.4538e-06],
        [-7.1377e-06, -5.1260e-06,  5.3793e-06,  ..., -5.8711e-06,
         -5.1558e-06, -5.2750e-06],
        [-5.9158e-06, -4.6641e-06,  4.4405e-06,  ..., -4.9621e-06,
         -4.5151e-06, -3.8743e-06]], device='cuda:0')
Loss: 0.9709376096725464


Running epoch 1, step 1522, batch 474
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    14, 10961,    12],
        [    0,  1682,   271,  ...,   367,  3210,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2897e-04,  1.0962e-04, -1.6965e-04,  ..., -8.5457e-05,
          3.2575e-05,  7.2433e-05],
        [-3.7998e-06, -2.7753e-06,  2.7381e-06,  ..., -3.1590e-06,
         -2.6673e-06, -2.6859e-06],
        [-1.1668e-05, -9.0152e-06,  9.0450e-06,  ..., -9.7305e-06,
         -8.4043e-06, -8.2403e-06],
        [-1.0684e-05, -7.7784e-06,  8.1062e-06,  ..., -8.9258e-06,
         -7.6741e-06, -7.8678e-06],
        [-9.0301e-06, -7.2271e-06,  6.8396e-06,  ..., -7.7188e-06,
         -6.8992e-06, -5.9307e-06]], device='cuda:0')
Loss: 0.9823085069656372


Running epoch 1, step 1523, batch 475
Sampled inputs[:2]: tensor([[    0,    12,   546,  ..., 24994, 31107,   266],
        [    0,    14,  8058,  ..., 10316,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7112e-04,  1.1903e-04, -3.3660e-04,  ...,  2.3204e-05,
         -1.3760e-04,  9.4864e-05],
        [-4.9919e-06, -3.5726e-06,  3.7290e-06,  ..., -4.1202e-06,
         -3.3826e-06, -3.5278e-06],
        [-1.5199e-05, -1.1489e-05,  1.2174e-05,  ..., -1.2562e-05,
         -1.0550e-05, -1.0654e-05],
        [-1.4082e-05, -1.0014e-05,  1.1101e-05,  ..., -1.1653e-05,
         -9.7454e-06, -1.0356e-05],
        [-1.1697e-05, -9.1940e-06,  9.1642e-06,  ..., -9.8944e-06,
         -8.6576e-06, -7.5996e-06]], device='cuda:0')
Loss: 0.943032443523407


Running epoch 1, step 1524, batch 476
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,  445,  749,  ...,  850, 1028,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1702e-04, -7.5289e-05, -3.3073e-04,  ...,  8.2284e-05,
          7.2574e-05,  4.0697e-04],
        [-6.1393e-06, -4.4666e-06,  4.5337e-06,  ..., -5.1260e-06,
         -4.2059e-06, -4.4331e-06],
        [-1.8939e-05, -1.4499e-05,  1.4976e-05,  ..., -1.5795e-05,
         -1.3188e-05, -1.3500e-05],
        [-1.7479e-05, -1.2636e-05,  1.3575e-05,  ..., -1.4663e-05,
         -1.2249e-05, -1.3173e-05],
        [-1.4573e-05, -1.1578e-05,  1.1295e-05,  ..., -1.2428e-05,
         -1.0803e-05, -9.6262e-06]], device='cuda:0')
Loss: 0.9600435495376587


Running epoch 1, step 1525, batch 477
Sampled inputs[:2]: tensor([[    0,   565,    27,  ...,    88,  4451,    14],
        [    0,    15,    72,  ...,   380, 22463,  2587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4056e-04, -2.1566e-04, -4.7073e-04,  ...,  1.1239e-04,
          1.6408e-04,  2.7869e-04],
        [-7.4059e-06, -5.3346e-06,  5.2564e-06,  ..., -6.2212e-06,
         -5.0999e-06, -5.4687e-06],
        [-2.2784e-05, -1.7211e-05,  1.7449e-05,  ..., -1.8999e-05,
         -1.5795e-05, -1.6481e-05],
        [-2.0966e-05, -1.4976e-05,  1.5646e-05,  ..., -1.7658e-05,
         -1.4707e-05, -1.6093e-05],
        [-1.7777e-05, -1.3843e-05,  1.3307e-05,  ..., -1.5125e-05,
         -1.3083e-05, -1.1921e-05]], device='cuda:0')
Loss: 0.9308200478553772


Running epoch 1, step 1526, batch 478
Sampled inputs[:2]: tensor([[    0,   271,  2862,  ...,   287,  5699,    18],
        [    0,    52, 26766,  ...,  4411,  4226,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1274e-04, -1.9536e-04, -3.3578e-04,  ...,  1.1239e-04,
          1.6764e-04,  4.0704e-04],
        [-8.6501e-06, -6.3702e-06,  6.1765e-06,  ..., -7.3239e-06,
         -6.1281e-06, -6.3404e-06],
        [-2.6777e-05, -2.0728e-05,  2.0579e-05,  ..., -2.2575e-05,
         -1.9178e-05, -1.9312e-05],
        [-2.4587e-05, -1.8030e-05,  1.8418e-05,  ..., -2.0906e-05,
         -1.7807e-05, -1.8746e-05],
        [-2.0877e-05, -1.6630e-05,  1.5691e-05,  ..., -1.7926e-05,
         -1.5810e-05, -1.3977e-05]], device='cuda:0')
Loss: 0.994757890701294


Running epoch 1, step 1527, batch 479
Sampled inputs[:2]: tensor([[   0,  361, 1224,  ..., 4401, 4261, 1663],
        [   0,   89, 2023,  ..., 3230,  328,  790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1603e-04, -1.7335e-04, -1.5608e-04,  ..., -5.6982e-05,
          3.4956e-04,  3.0195e-04],
        [-9.8348e-06, -7.2643e-06,  6.9886e-06,  ..., -8.3596e-06,
         -7.0520e-06, -7.1935e-06],
        [-3.0473e-05, -2.3603e-05,  2.3320e-05,  ..., -2.5749e-05,
         -2.2039e-05, -2.1920e-05],
        [ 1.5824e-04,  2.2439e-04, -1.4453e-04,  ...,  1.8021e-04,
          2.3233e-04,  9.2509e-05],
        [-2.3857e-05, -1.9014e-05,  1.7852e-05,  ..., -2.0534e-05,
         -1.8239e-05, -1.5885e-05]], device='cuda:0')
Loss: 0.9911404252052307
Graident accumulation at epoch 1, step 1527, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4990e-05, -2.5966e-05, -4.8403e-05,  ...,  2.1198e-05,
          2.3450e-04,  1.6409e-05],
        [-4.8299e-06, -3.7877e-06,  4.0417e-06,  ..., -5.0401e-06,
         -2.4142e-06, -2.8920e-06],
        [ 6.6202e-06,  2.5207e-05, -8.0406e-06,  ...,  8.8420e-06,
          1.6744e-05, -4.9076e-06],
        [-1.2504e-06,  1.5605e-05, -7.8552e-07,  ...,  3.7422e-06,
          1.4969e-05, -3.3354e-06],
        [-2.4487e-05, -1.8474e-05,  1.8060e-05,  ..., -2.0983e-05,
         -1.7375e-05, -1.5673e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3758e-08, 5.9755e-08, 4.5845e-08,  ..., 4.0072e-08, 1.2372e-07,
         5.2988e-08],
        [7.9285e-11, 5.8912e-11, 1.9263e-11,  ..., 5.5939e-11, 3.5057e-11,
         2.9933e-11],
        [4.0574e-09, 2.8559e-09, 1.2842e-09,  ..., 3.1592e-09, 1.2935e-09,
         1.0202e-09],
        [1.1218e-09, 1.1480e-09, 3.2513e-10,  ..., 9.4122e-10, 6.1264e-10,
         4.3322e-10],
        [3.5963e-10, 2.0507e-10, 7.7147e-11,  ..., 2.6984e-10, 7.7142e-11,
         1.0291e-10]], device='cuda:0')
optimizer state dict: 191.0
lr: [3.6106089361640563e-06, 3.6106089361640563e-06]
scheduler_last_epoch: 191
Epoch 1 | Batch 479/1048 | Training PPL: 2776.9914311216435 | time 47.94704508781433
Saving checkpoint at epoch 1, step 1527, batch 479
Epoch 1 | Validation PPL: 6.820766282573827 | Learning rate: 3.6106089361640563e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1527, AFTER epoch 1, step 1527


Running epoch 1, step 1528, batch 480
Sampled inputs[:2]: tensor([[    0,   391,  1866,  ...,  3711, 21119, 29613],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8980e-05, -1.1587e-04, -1.7476e-04,  ..., -4.5134e-05,
         -7.5879e-05,  2.0680e-04],
        [-1.3486e-06, -8.3819e-07,  7.1526e-07,  ..., -1.1772e-06,
         -9.8348e-07, -1.0878e-06],
        [-3.9041e-06, -2.5034e-06,  2.3395e-06,  ..., -3.2932e-06,
         -2.7418e-06, -3.0100e-06],
        [-3.7700e-06, -2.2352e-06,  2.0713e-06,  ..., -3.2932e-06,
         -2.7418e-06, -3.0994e-06],
        [-3.5018e-06, -2.2650e-06,  2.0564e-06,  ..., -2.9802e-06,
         -2.5481e-06, -2.5034e-06]], device='cuda:0')
Loss: 0.9685066342353821


Running epoch 1, step 1529, batch 481
Sampled inputs[:2]: tensor([[   0,  199,  769,  ...,  685, 1423,   13],
        [   0, 1042, 5738,  ...,   12,  287, 3643]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1580e-04, -1.8949e-04, -5.4777e-04,  ..., -1.2850e-04,
         -1.5777e-04, -4.7051e-05],
        [-2.5854e-06, -1.6540e-06,  1.6168e-06,  ..., -2.2352e-06,
         -1.8366e-06, -1.9930e-06],
        [-7.6145e-06, -5.0217e-06,  5.2601e-06,  ..., -6.3777e-06,
         -5.2601e-06, -5.6028e-06],
        [-7.2420e-06, -4.4554e-06,  4.7535e-06,  ..., -6.2436e-06,
         -5.1409e-06, -5.7369e-06],
        [-6.4373e-06, -4.3511e-06,  4.3213e-06,  ..., -5.4836e-06,
         -4.6790e-06, -4.3884e-06]], device='cuda:0')
Loss: 0.9286651015281677


Running epoch 1, step 1530, batch 482
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,  2629, 13422,  ...,  1042,  5301,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1987e-05, -2.6754e-04, -6.3525e-04,  ..., -1.1329e-04,
         -2.9378e-04, -4.9044e-05],
        [-3.9265e-06, -2.4810e-06,  2.5183e-06,  ..., -3.3453e-06,
         -2.7604e-06, -3.0212e-06],
        [-1.1817e-05, -7.7039e-06,  8.2850e-06,  ..., -9.8199e-06,
         -8.1211e-06, -8.7470e-06],
        [-1.0952e-05, -6.7055e-06,  7.3612e-06,  ..., -9.3132e-06,
         -7.6890e-06, -8.6427e-06],
        [-9.8944e-06, -6.6161e-06,  6.7353e-06,  ..., -8.3447e-06,
         -7.1526e-06, -6.8024e-06]], device='cuda:0')
Loss: 0.9424182772636414


Running epoch 1, step 1531, batch 483
Sampled inputs[:2]: tensor([[    0,  4988, 36842,  ...,  7630, 18362,    13],
        [    0,   328, 27958,  ...,   417,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1062e-05, -2.1977e-04, -5.6099e-04,  ..., -1.5680e-04,
         -3.4046e-04, -1.4591e-05],
        [-5.2229e-06, -3.4273e-06,  3.4720e-06,  ..., -4.4033e-06,
         -3.6657e-06, -3.9041e-06],
        [-1.5959e-05, -1.0848e-05,  1.1489e-05,  ..., -1.3202e-05,
         -1.1042e-05, -1.1593e-05],
        [-1.4752e-05, -9.4473e-06,  1.0282e-05,  ..., -1.2398e-05,
         -1.0327e-05, -1.1340e-05],
        [-1.3098e-05, -9.1344e-06,  9.1642e-06,  ..., -1.1012e-05,
         -9.5367e-06, -8.8736e-06]], device='cuda:0')
Loss: 1.005155324935913


Running epoch 1, step 1532, batch 484
Sampled inputs[:2]: tensor([[    0,    12,   496,  ...,   437,   266,  3767],
        [    0,   287, 19777,  ...,   266,  5061,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2300e-04, -2.2797e-04, -5.3502e-04,  ..., -1.8526e-04,
         -2.4120e-04,  1.1293e-04],
        [-6.5863e-06, -4.3586e-06,  4.2915e-06,  ..., -5.5358e-06,
         -4.6082e-06, -4.9397e-06],
        [-2.0310e-05, -1.4022e-05,  1.4350e-05,  ..., -1.6823e-05,
         -1.4111e-05, -1.4901e-05],
        [-1.8567e-05, -1.2085e-05,  1.2711e-05,  ..., -1.5616e-05,
         -1.3068e-05, -1.4380e-05],
        [-1.6615e-05, -1.1727e-05,  1.1399e-05,  ..., -1.3977e-05,
         -1.2130e-05, -1.1392e-05]], device='cuda:0')
Loss: 0.9642473459243774


Running epoch 1, step 1533, batch 485
Sampled inputs[:2]: tensor([[    0,  7712, 31756,  ...,   895,   360,   630],
        [    0,   944,   278,  ..., 17330,  1683,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2608e-05, -1.4546e-04, -5.0771e-04,  ..., -2.8314e-04,
         -2.8271e-04,  1.3731e-04],
        [-7.8604e-06, -5.2266e-06,  5.2676e-06,  ..., -6.5491e-06,
         -5.3942e-06, -5.7742e-06],
        [-2.4185e-05, -1.6838e-05,  1.7524e-05,  ..., -1.9923e-05,
         -1.6585e-05, -1.7449e-05],
        [-2.2262e-05, -1.4603e-05,  1.5736e-05,  ..., -1.8567e-05,
         -1.5408e-05, -1.6943e-05],
        [-1.9535e-05, -1.3977e-05,  1.3754e-05,  ..., -1.6391e-05,
         -1.4141e-05, -1.3180e-05]], device='cuda:0')
Loss: 0.9590168595314026


Running epoch 1, step 1534, batch 486
Sampled inputs[:2]: tensor([[    0,   342,   721,  ...,  2429,    14,   475],
        [    0, 20080, 11069,  ...,   300,  5768,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7534e-05, -2.0191e-04, -5.2741e-04,  ..., -2.5798e-04,
         -2.5784e-04,  8.8629e-05],
        [-9.1270e-06, -6.0126e-06,  6.2659e-06,  ..., -7.5325e-06,
         -6.0759e-06, -6.6534e-06],
        [-2.8029e-05, -1.9372e-05,  2.0742e-05,  ..., -2.2873e-05,
         -1.8656e-05, -2.0057e-05],
        [-2.5928e-05, -1.6838e-05,  1.8775e-05,  ..., -2.1398e-05,
         -1.7390e-05, -1.9595e-05],
        [-2.2396e-05, -1.5944e-05,  1.6078e-05,  ..., -1.8641e-05,
         -1.5803e-05, -1.4961e-05]], device='cuda:0')
Loss: 0.9277673959732056


Running epoch 1, step 1535, batch 487
Sampled inputs[:2]: tensor([[    0,   300,  1064,  ...,  6953,   944,   278],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1028e-04, -1.6900e-04, -5.7556e-04,  ..., -2.8003e-04,
         -3.6392e-04,  8.5706e-05],
        [-1.0520e-05, -6.9402e-06,  7.1116e-06,  ..., -8.6799e-06,
         -7.0184e-06, -7.6070e-06],
        [-3.2052e-05, -2.2262e-05,  2.3410e-05,  ..., -2.6196e-05,
         -2.1487e-05, -2.2784e-05],
        [-2.9683e-05, -1.9372e-05,  2.1189e-05,  ..., -2.4542e-05,
         -2.0042e-05, -2.2262e-05],
        [-2.5749e-05, -1.8433e-05,  1.8224e-05,  ..., -2.1473e-05,
         -1.8306e-05, -1.7107e-05]], device='cuda:0')
Loss: 0.9465599060058594
Graident accumulation at epoch 1, step 1535, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5463e-05, -4.0270e-05, -1.0112e-04,  ..., -8.9249e-06,
          1.7466e-04,  2.3339e-05],
        [-5.3989e-06, -4.1030e-06,  4.3487e-06,  ..., -5.4040e-06,
         -2.8746e-06, -3.3635e-06],
        [ 2.7529e-06,  2.0460e-05, -4.8956e-06,  ...,  5.3381e-06,
          1.2921e-05, -6.6952e-06],
        [-4.0937e-06,  1.2107e-05,  1.4120e-06,  ...,  9.1372e-07,
          1.1468e-05, -5.2281e-06],
        [-2.4613e-05, -1.8470e-05,  1.8076e-05,  ..., -2.1032e-05,
         -1.7468e-05, -1.5816e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3696e-08, 5.9724e-08, 4.6130e-08,  ..., 4.0110e-08, 1.2373e-07,
         5.2942e-08],
        [7.9316e-11, 5.8901e-11, 1.9295e-11,  ..., 5.5958e-11, 3.5071e-11,
         2.9961e-11],
        [4.0543e-09, 2.8536e-09, 1.2834e-09,  ..., 3.1567e-09, 1.2927e-09,
         1.0197e-09],
        [1.1216e-09, 1.1473e-09, 3.2525e-10,  ..., 9.4088e-10, 6.1243e-10,
         4.3328e-10],
        [3.5993e-10, 2.0520e-10, 7.7402e-11,  ..., 2.7003e-10, 7.7400e-11,
         1.0310e-10]], device='cuda:0')
optimizer state dict: 192.0
lr: [3.5160065499038043e-06, 3.5160065499038043e-06]
scheduler_last_epoch: 192


Running epoch 1, step 1536, batch 488
Sampled inputs[:2]: tensor([[   0,   14, 1266,  ..., 2288,  417,  199],
        [   0, 7094,  596,  ..., 4764, 9514,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6630e-05, -9.4477e-05,  1.1641e-04,  ...,  5.5580e-05,
          1.9166e-04,  5.3720e-05],
        [-1.3039e-06, -9.7603e-07,  8.5309e-07,  ..., -1.0952e-06,
         -9.7603e-07, -9.4995e-07],
        [-4.0829e-06, -3.2336e-06,  2.8610e-06,  ..., -3.4273e-06,
         -3.1292e-06, -2.9653e-06],
        [-3.8147e-06, -2.8610e-06,  2.6226e-06,  ..., -3.2037e-06,
         -2.9206e-06, -2.9057e-06],
        [-3.1888e-06, -2.6077e-06,  2.2054e-06,  ..., -2.7418e-06,
         -2.5779e-06, -2.1905e-06]], device='cuda:0')
Loss: 0.9805413484573364


Running epoch 1, step 1537, batch 489
Sampled inputs[:2]: tensor([[   0, 1106,  259,  ...,  271,  679,  382],
        [   0, 4258,  717,  ...,   34,  609, 1169]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2076e-05, -3.5286e-04,  1.2919e-04,  ..., -8.7423e-05,
          5.2490e-04,  2.4169e-04],
        [-2.6301e-06, -1.9148e-06,  1.6838e-06,  ..., -2.2501e-06,
         -1.9968e-06, -1.9260e-06],
        [-8.0168e-06, -6.1989e-06,  5.5581e-06,  ..., -6.8545e-06,
         -6.2138e-06, -5.7817e-06],
        [-7.4953e-06, -5.4836e-06,  5.0366e-06,  ..., -6.4373e-06,
         -5.8413e-06, -5.7220e-06],
        [-6.4820e-06, -5.1409e-06,  4.4256e-06,  ..., -5.6326e-06,
         -5.2601e-06, -4.3809e-06]], device='cuda:0')
Loss: 0.9899859428405762


Running epoch 1, step 1538, batch 490
Sampled inputs[:2]: tensor([[    0,  1070, 17816,  ...,  5547,  9966,   518],
        [    0,    14,  6707,  ..., 17771,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0730e-04, -3.3276e-04,  1.3859e-04,  ..., -4.1903e-05,
          6.3758e-04,  3.1969e-04],
        [-3.9786e-06, -2.8573e-06,  2.5257e-06,  ..., -3.4273e-06,
         -2.9877e-06, -2.8871e-06],
        [-1.2189e-05, -9.2536e-06,  8.3447e-06,  ..., -1.0476e-05,
         -9.2834e-06, -8.7172e-06],
        [-1.1295e-05, -8.1360e-06,  7.4953e-06,  ..., -9.7752e-06,
         -8.6725e-06, -8.5384e-06],
        [-9.8944e-06, -7.7039e-06,  6.6757e-06,  ..., -8.6278e-06,
         -7.8827e-06, -6.6310e-06]], device='cuda:0')
Loss: 1.002036213874817


Running epoch 1, step 1539, batch 491
Sampled inputs[:2]: tensor([[    0, 12923,  2489,  ...,   474,  3301,    54],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7684e-05, -2.0352e-04,  6.3508e-05,  ..., -8.3530e-05,
          4.7640e-04,  1.1056e-04],
        [-5.2899e-06, -3.7476e-06,  3.5241e-06,  ..., -4.4405e-06,
         -3.7998e-06, -3.7551e-06],
        [-1.6272e-05, -1.2144e-05,  1.1668e-05,  ..., -1.3649e-05,
         -1.1861e-05, -1.1429e-05],
        [-1.5020e-05, -1.0639e-05,  1.0505e-05,  ..., -1.2651e-05,
         -1.0997e-05, -1.1116e-05],
        [-1.2934e-05, -9.9540e-06,  9.0748e-06,  ..., -1.1042e-05,
         -9.9540e-06, -8.4862e-06]], device='cuda:0')
Loss: 0.9614900946617126


Running epoch 1, step 1540, batch 492
Sampled inputs[:2]: tensor([[    0,  9088,  7217,  ...,   199, 17822,   278],
        [    0,    12,  4856,  ...,   342,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2460e-04, -9.8550e-05,  3.8707e-05,  ..., -8.3530e-05,
          3.1290e-04,  1.5405e-04],
        [-6.6087e-06, -4.7311e-06,  4.5151e-06,  ..., -5.5283e-06,
         -4.7237e-06, -4.6603e-06],
        [ 5.5055e-05,  7.0669e-05, -3.4037e-05,  ...,  2.9800e-05,
          4.5730e-05,  3.2895e-06],
        [-1.8790e-05, -1.3441e-05,  1.3471e-05,  ..., -1.5765e-05,
         -1.3679e-05, -1.3828e-05],
        [-1.6123e-05, -1.2562e-05,  1.1548e-05,  ..., -1.3769e-05,
         -1.2383e-05, -1.0557e-05]], device='cuda:0')
Loss: 0.987891674041748


Running epoch 1, step 1541, batch 493
Sampled inputs[:2]: tensor([[   0,  394,  292,  ..., 1711,  365,  897],
        [   0,  300,  369,  ...,   12,  970,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1279e-04, -1.5011e-04, -2.0409e-05,  ..., -2.4371e-04,
          3.5612e-04,  1.9912e-04],
        [-7.9125e-06, -5.5991e-06,  5.4538e-06,  ..., -6.6236e-06,
         -5.6289e-06, -5.5805e-06],
        [ 5.1061e-05,  6.7838e-05, -3.0967e-05,  ...,  2.6417e-05,
          4.2914e-05,  4.5829e-07],
        [-2.2635e-05, -1.5959e-05,  1.6376e-05,  ..., -1.8984e-05,
         -1.6391e-05, -1.6674e-05],
        [-1.9208e-05, -1.4856e-05,  1.3873e-05,  ..., -1.6451e-05,
         -1.4693e-05, -1.2614e-05]], device='cuda:0')
Loss: 0.9731981158256531


Running epoch 1, step 1542, batch 494
Sampled inputs[:2]: tensor([[   0,    9,  278,  ...,  278,  298,  452],
        [   0,   14, 3921,  ...,  199, 2038, 1963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1201e-05, -1.2649e-04, -1.0744e-04,  ..., -1.7960e-04,
          3.8134e-04,  2.7337e-04],
        [-9.2089e-06, -6.4969e-06,  6.2771e-06,  ..., -7.7635e-06,
         -6.5900e-06, -6.5193e-06],
        [ 4.7068e-05,  6.4977e-05, -2.8255e-05,  ...,  2.2960e-05,
          3.9993e-05, -2.3729e-06],
        [-2.6390e-05, -1.8507e-05,  1.8835e-05,  ..., -2.2292e-05,
         -1.9208e-05, -1.9521e-05],
        [-2.2456e-05, -1.7256e-05,  1.6049e-05,  ..., -1.9312e-05,
         -1.7181e-05, -1.4775e-05]], device='cuda:0')
Loss: 0.989332914352417


Running epoch 1, step 1543, batch 495
Sampled inputs[:2]: tensor([[   0, 8158, 1416,  ...,  413,   29,  413],
        [   0, 1874,  300,  ...,   14, 5372,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2731e-04, -4.6275e-09,  1.9048e-05,  ..., -2.5304e-04,
          3.1073e-04,  2.6995e-04],
        [-1.0602e-05, -7.3425e-06,  7.0930e-06,  ..., -8.8885e-06,
         -7.5102e-06, -7.5027e-06],
        [ 4.2985e-05,  6.2280e-05, -2.5588e-05,  ...,  1.9652e-05,
          3.7192e-05, -5.1892e-06],
        [-3.0294e-05, -2.0862e-05,  2.1249e-05,  ..., -2.5496e-05,
         -2.1920e-05, -2.2352e-05],
        [-2.5779e-05, -1.9491e-05,  1.8150e-05,  ..., -2.2039e-05,
         -1.9565e-05, -1.6920e-05]], device='cuda:0')
Loss: 0.9361746311187744
Graident accumulation at epoch 1, step 1543, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0082,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6186e-05, -3.6244e-05, -8.9102e-05,  ..., -3.3337e-05,
          1.8827e-04,  4.8000e-05],
        [-5.9193e-06, -4.4269e-06,  4.6231e-06,  ..., -5.7525e-06,
         -3.3382e-06, -3.7774e-06],
        [ 6.7761e-06,  2.4642e-05, -6.9648e-06,  ...,  6.7695e-06,
          1.5348e-05, -6.5446e-06],
        [-6.7137e-06,  8.8101e-06,  3.3957e-06,  ..., -1.7272e-06,
          8.1295e-06, -6.9405e-06],
        [-2.4729e-05, -1.8572e-05,  1.8083e-05,  ..., -2.1132e-05,
         -1.7677e-05, -1.5926e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3639e-08, 5.9664e-08, 4.6084e-08,  ..., 4.0134e-08, 1.2370e-07,
         5.2962e-08],
        [7.9349e-11, 5.8896e-11, 1.9326e-11,  ..., 5.5981e-11, 3.5092e-11,
         2.9987e-11],
        [4.0521e-09, 2.8546e-09, 1.2828e-09,  ..., 3.1540e-09, 1.2928e-09,
         1.0187e-09],
        [1.1214e-09, 1.1466e-09, 3.2538e-10,  ..., 9.4059e-10, 6.1229e-10,
         4.3335e-10],
        [3.6023e-10, 2.0538e-10, 7.7654e-11,  ..., 2.7024e-10, 7.7705e-11,
         1.0329e-10]], device='cuda:0')
optimizer state dict: 193.0
lr: [3.422394974442298e-06, 3.422394974442298e-06]
scheduler_last_epoch: 193


Running epoch 1, step 1544, batch 496
Sampled inputs[:2]: tensor([[    0, 47831,   266,  ...,    66,    17, 20005],
        [    0,  4154,    12,  ...,    14,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2666e-05, -1.3936e-04, -8.9151e-05,  ..., -3.8518e-06,
         -9.3185e-05,  2.8993e-05],
        [-1.3113e-06, -9.2760e-07,  9.6112e-07,  ..., -1.0803e-06,
         -8.8289e-07, -8.8662e-07],
        [-4.0531e-06, -2.9504e-06,  3.1590e-06,  ..., -3.2932e-06,
         -2.7418e-06, -2.6822e-06],
        [-3.7849e-06, -2.6524e-06,  2.9057e-06,  ..., -3.0994e-06,
         -2.5481e-06, -2.6524e-06],
        [-3.1292e-06, -2.3693e-06,  2.3842e-06,  ..., -2.5779e-06,
         -2.2352e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9989000558853149


Running epoch 1, step 1545, batch 497
Sampled inputs[:2]: tensor([[    0,  3779,    12,  ...,    12, 12774, 14261],
        [    0,    12,  5820,  ...,   221,   380,   560]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2913e-05, -2.0858e-04, -1.2335e-04,  ..., -4.2368e-05,
          1.1770e-04,  6.2617e-05],
        [-2.6301e-06, -1.8887e-06,  1.7323e-06,  ..., -2.2277e-06,
         -1.9111e-06, -1.8701e-06],
        [-7.9572e-06, -5.9903e-06,  5.6773e-06,  ..., -6.6757e-06,
         -5.8264e-06, -5.5283e-06],
        [-7.4357e-06, -5.3495e-06,  5.1409e-06,  ..., -6.3330e-06,
         -5.4985e-06, -5.5134e-06],
        [-6.3926e-06, -4.9621e-06,  4.4554e-06,  ..., -5.4687e-06,
         -4.9323e-06, -4.1872e-06]], device='cuda:0')
Loss: 0.9725208878517151


Running epoch 1, step 1546, batch 498
Sampled inputs[:2]: tensor([[    0,  2348,   565,  ...,    12,   709,   266],
        [    0,     9,   300,  ...,  6838,   328, 18619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1011e-05, -4.1356e-06, -1.5537e-04,  ...,  2.1430e-05,
          3.4157e-05, -1.3725e-04],
        [-3.8892e-06, -2.7232e-06,  2.5816e-06,  ..., -3.2857e-06,
         -2.7493e-06, -2.8014e-06],
        [-1.1802e-05, -8.6427e-06,  8.4937e-06,  ..., -9.8497e-06,
         -8.3596e-06, -8.2850e-06],
        [-1.1072e-05, -7.6890e-06,  7.7039e-06,  ..., -9.3877e-06,
         -7.9423e-06, -8.3297e-06],
        [-9.3877e-06, -7.1079e-06,  6.6012e-06,  ..., -7.9721e-06,
         -7.0184e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9398224949836731


Running epoch 1, step 1547, batch 499
Sampled inputs[:2]: tensor([[    0,  1934,  2413,  ..., 19697,    13, 16325],
        [    0,   221,   334,  ...,   706,  2680,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9653e-05,  8.8618e-05, -1.8382e-04,  ...,  2.0317e-05,
         -9.2405e-05, -1.5631e-04],
        [-5.3346e-06, -3.4757e-06,  3.5651e-06,  ..., -4.4182e-06,
         -3.5912e-06, -3.9041e-06],
        [-1.6123e-05, -1.1057e-05,  1.1653e-05,  ..., -1.3232e-05,
         -1.0937e-05, -1.1519e-05],
        [-1.5035e-05, -9.7305e-06,  1.0505e-05,  ..., -1.2502e-05,
         -1.0297e-05, -1.1414e-05],
        [-1.2949e-05, -9.1046e-06,  9.1195e-06,  ..., -1.0774e-05,
         -9.2387e-06, -8.7023e-06]], device='cuda:0')
Loss: 0.9432654976844788


Running epoch 1, step 1548, batch 500
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 4298, 1231,  352],
        [   0,  596,  292,  ...,   13, 6673,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1661e-06,  1.2633e-04, -2.6816e-04,  ..., -3.5459e-05,
         -3.5769e-05, -1.2503e-04],
        [-6.6310e-06, -4.2468e-06,  4.2990e-06,  ..., -5.5283e-06,
         -4.5039e-06, -4.9621e-06],
        [-2.0057e-05, -1.3486e-05,  1.4141e-05,  ..., -1.6496e-05,
         -1.3635e-05, -1.4588e-05],
        [-1.8790e-05, -1.1876e-05,  1.2726e-05,  ..., -1.5721e-05,
         -1.2934e-05, -1.4558e-05],
        [-1.6212e-05, -1.1131e-05,  1.1146e-05,  ..., -1.3515e-05,
         -1.1548e-05, -1.1072e-05]], device='cuda:0')
Loss: 0.9400960803031921


Running epoch 1, step 1549, batch 501
Sampled inputs[:2]: tensor([[   0, 2280,  344,  ...,  287,  266, 3344],
        [   0, 3103, 2134,  ..., 6627,  275, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5371e-05,  1.6616e-04, -3.6912e-04,  ...,  2.1702e-05,
         -1.3540e-04, -8.7078e-05],
        [-7.9125e-06, -5.1409e-06,  5.2303e-06,  ..., -6.5863e-06,
         -5.4128e-06, -5.8562e-06],
        [-2.3931e-05, -1.6347e-05,  1.7151e-05,  ..., -1.9714e-05,
         -1.6466e-05, -1.7270e-05],
        [-2.2456e-05, -1.4424e-05,  1.5542e-05,  ..., -1.8761e-05,
         -1.5602e-05, -1.7226e-05],
        [-1.9252e-05, -1.3471e-05,  1.3456e-05,  ..., -1.6093e-05,
         -1.3903e-05, -1.3039e-05]], device='cuda:0')
Loss: 0.9550901651382446


Running epoch 1, step 1550, batch 502
Sampled inputs[:2]: tensor([[   0,  659,  278,  ..., 4032, 1109,  721],
        [   0, 2736, 2523,  ..., 4086, 4798, 7701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5273e-05,  1.3435e-04, -3.2042e-04,  ...,  1.4860e-05,
         -1.2245e-04,  2.1196e-06],
        [-9.2313e-06, -6.0722e-06,  6.1095e-06,  ..., -7.6815e-06,
         -6.3442e-06, -6.7540e-06],
        [-2.8014e-05, -1.9357e-05,  2.0027e-05,  ..., -2.3127e-05,
         -1.9372e-05, -2.0057e-05],
        [ 1.4413e-04,  1.1315e-04, -1.6567e-04,  ...,  2.4846e-04,
          1.9979e-04,  1.0506e-04],
        [-2.2501e-05, -1.5959e-05,  1.5706e-05,  ..., -1.8850e-05,
         -1.6332e-05, -1.5125e-05]], device='cuda:0')
Loss: 1.0148037672042847


Running epoch 1, step 1551, batch 503
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 8271,  365,  437],
        [   0,  694, 2326,  ...,  278, 1781, 9660]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1596e-04,  3.7059e-04, -1.7945e-05,  ...,  9.0266e-05,
         -3.6063e-04, -1.6247e-04],
        [-1.0394e-05, -6.9067e-06,  6.9961e-06,  ..., -8.6948e-06,
         -7.1824e-06, -7.5959e-06],
        [-3.1680e-05, -2.2054e-05,  2.3037e-05,  ..., -2.6271e-05,
         -2.1979e-05, -2.2635e-05],
        [ 1.4066e-04,  1.1069e-04, -1.6285e-04,  ...,  2.4544e-04,
          1.9725e-04,  1.0244e-04],
        [-2.5257e-05, -1.8060e-05,  1.7911e-05,  ..., -2.1249e-05,
         -1.8403e-05, -1.6928e-05]], device='cuda:0')
Loss: 0.9433168172836304
Graident accumulation at epoch 1, step 1551, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9972e-05,  4.4394e-06, -8.1987e-05,  ..., -2.0976e-05,
          1.3338e-04,  2.6953e-05],
        [-6.3667e-06, -4.6749e-06,  4.8604e-06,  ..., -6.0467e-06,
         -3.7226e-06, -4.1593e-06],
        [ 2.9305e-06,  1.9973e-05, -3.9646e-06,  ...,  3.4655e-06,
          1.1615e-05, -8.1536e-06],
        [ 8.0236e-06,  1.8998e-05, -1.3229e-05,  ...,  2.2989e-05,
          2.7042e-05,  3.9971e-06],
        [-2.4782e-05, -1.8521e-05,  1.8066e-05,  ..., -2.1144e-05,
         -1.7750e-05, -1.6027e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3578e-08, 5.9742e-08, 4.6039e-08,  ..., 4.0102e-08, 1.2370e-07,
         5.2936e-08],
        [7.9378e-11, 5.8885e-11, 1.9355e-11,  ..., 5.6001e-11, 3.5109e-11,
         3.0015e-11],
        [4.0491e-09, 2.8522e-09, 1.2820e-09,  ..., 3.1515e-09, 1.2920e-09,
         1.0182e-09],
        [1.1400e-09, 1.1577e-09, 3.5157e-10,  ..., 9.9989e-10, 6.5059e-10,
         4.4341e-10],
        [3.6051e-10, 2.0550e-10, 7.7897e-11,  ..., 2.7042e-10, 7.7966e-11,
         1.0347e-10]], device='cuda:0')
optimizer state dict: 194.0
lr: [3.32978851444543e-06, 3.32978851444543e-06]
scheduler_last_epoch: 194


Running epoch 1, step 1552, batch 504
Sampled inputs[:2]: tensor([[    0,   409, 22809,  ...,   342,   720,    14],
        [    0,   368,   729,  ...,   221,   380,  2830]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3577e-05,  1.1622e-04, -9.9956e-05,  ...,  1.5364e-05,
         -1.4241e-04, -6.3667e-05],
        [-1.2890e-06, -9.2760e-07,  9.6112e-07,  ..., -1.0207e-06,
         -8.0839e-07, -8.6427e-07],
        [-4.0233e-06, -3.0547e-06,  3.2037e-06,  ..., -3.1888e-06,
         -2.5630e-06, -2.6673e-06],
        [-3.7551e-06, -2.7269e-06,  2.9504e-06,  ..., -2.9951e-06,
         -2.3991e-06, -2.6673e-06],
        [-2.9653e-06, -2.3395e-06,  2.3246e-06,  ..., -2.3991e-06,
         -2.0266e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.9599100947380066


Running epoch 1, step 1553, batch 505
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2020e-05,  2.5295e-04, -1.2211e-04,  ..., -1.2607e-05,
         -1.1671e-04, -7.0893e-05],
        [-2.7120e-06, -1.7472e-06,  1.7732e-06,  ..., -2.1458e-06,
         -1.7360e-06, -1.9819e-06],
        [-8.3447e-06, -5.7071e-06,  5.9009e-06,  ..., -6.5714e-06,
         -5.3942e-06, -6.0350e-06],
        [-7.8380e-06, -5.0366e-06,  5.3644e-06,  ..., -6.2436e-06,
         -5.0962e-06, -5.9456e-06],
        [-6.5118e-06, -4.5449e-06,  4.4852e-06,  ..., -5.2452e-06,
         -4.4703e-06, -4.4554e-06]], device='cuda:0')
Loss: 0.9525734782218933


Running epoch 1, step 1554, batch 506
Sampled inputs[:2]: tensor([[    0,    14, 38914,  ...,   266,  5690,   278],
        [    0,   271, 21394,  ...,  1487,   287,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1535e-05,  4.5401e-04,  5.2713e-05,  ..., -9.7105e-06,
         -2.2930e-04, -7.0893e-05],
        [-4.0829e-06, -2.6822e-06,  2.5369e-06,  ..., -3.2485e-06,
         -2.7269e-06, -2.9877e-06],
        [-1.2755e-05, -8.8960e-06,  8.5235e-06,  ..., -1.0148e-05,
         -8.6576e-06, -9.2834e-06],
        [-1.1891e-05, -7.7784e-06,  7.6890e-06,  ..., -9.5218e-06,
         -8.0913e-06, -9.0450e-06],
        [-1.0148e-05, -7.1824e-06,  6.5863e-06,  ..., -8.2254e-06,
         -7.2420e-06, -6.9886e-06]], device='cuda:0')
Loss: 0.9555625915527344


Running epoch 1, step 1555, batch 507
Sampled inputs[:2]: tensor([[    0,   365,  1462,  ...,   518,  6104,   278],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8889e-04,  5.0395e-04, -9.0251e-05,  ..., -4.0323e-05,
         -4.0668e-04, -1.2485e-04],
        [-5.3048e-06, -3.5390e-06,  3.4422e-06,  ..., -4.2468e-06,
         -3.5204e-06, -3.8743e-06],
        [-1.6659e-05, -1.1712e-05,  1.1593e-05,  ..., -1.3322e-05,
         -1.1176e-05, -1.2100e-05],
        [-1.5557e-05, -1.0312e-05,  1.0535e-05,  ..., -1.2517e-05,
         -1.0461e-05, -1.1817e-05],
        [-1.3053e-05, -9.3430e-06,  8.8215e-06,  ..., -1.0625e-05,
         -9.2238e-06, -8.9407e-06]], device='cuda:0')
Loss: 0.9490928649902344


Running epoch 1, step 1556, batch 508
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   446,   747,  1193],
        [    0,   221,   451,  ...,   741, 25712,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3773e-04,  7.2838e-04,  7.9805e-05,  ..., -1.5810e-06,
         -6.9506e-04, -1.7627e-04],
        [-6.6236e-06, -4.3176e-06,  4.2059e-06,  ..., -5.4017e-06,
         -4.3884e-06, -4.9397e-06],
        [-2.0593e-05, -1.4171e-05,  1.4111e-05,  ..., -1.6704e-05,
         -1.3754e-05, -1.5154e-05],
        [-1.9491e-05, -1.2591e-05,  1.2904e-05,  ..., -1.5974e-05,
         -1.3083e-05, -1.5095e-05],
        [-1.6302e-05, -1.1384e-05,  1.0863e-05,  ..., -1.3426e-05,
         -1.1414e-05, -1.1310e-05]], device='cuda:0')
Loss: 0.9346235990524292


Running epoch 1, step 1557, batch 509
Sampled inputs[:2]: tensor([[   0,   14, 7870,  ...,  284,  830,  292],
        [   0, 7230,   13,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9476e-05,  9.9318e-04,  1.2078e-04,  ...,  1.4806e-05,
         -9.8098e-04, -3.3918e-04],
        [-7.8678e-06, -5.1111e-06,  5.0962e-06,  ..., -6.4448e-06,
         -5.2005e-06, -5.8487e-06],
        [-2.4334e-05, -1.6615e-05,  1.7002e-05,  ..., -1.9789e-05,
         -1.6168e-05, -1.7792e-05],
        [-2.3082e-05, -1.4797e-05,  1.5616e-05,  ..., -1.8969e-05,
         -1.5423e-05, -1.7807e-05],
        [-1.9178e-05, -1.3351e-05,  1.3039e-05,  ..., -1.5840e-05,
         -1.3381e-05, -1.3188e-05]], device='cuda:0')
Loss: 0.9187378883361816


Running epoch 1, step 1558, batch 510
Sampled inputs[:2]: tensor([[    0, 48705,   292,  ...,   266,  2548,  2697],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6773e-05,  1.0737e-03,  9.1835e-05,  ..., -9.0838e-05,
         -1.0053e-03, -2.4948e-04],
        [-9.2536e-06, -6.0536e-06,  6.0163e-06,  ..., -7.5772e-06,
         -6.1542e-06, -6.8247e-06],
        [ 3.3240e-04,  2.5096e-04, -3.5077e-04,  ...,  2.8225e-04,
          3.4410e-04,  2.1285e-04],
        [-2.7075e-05, -1.7524e-05,  1.8373e-05,  ..., -2.2247e-05,
         -1.8224e-05, -2.0728e-05],
        [-2.2560e-05, -1.5885e-05,  1.5378e-05,  ..., -1.8686e-05,
         -1.5900e-05, -1.5482e-05]], device='cuda:0')
Loss: 0.9687879681587219


Running epoch 1, step 1559, batch 511
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ..., 3988, 1049,  935],
        [   0,  446, 1845,  ...,  422,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3699e-05,  1.3003e-03,  4.5909e-04,  ..., -1.3959e-04,
         -1.2746e-03, -3.9207e-04],
        [-1.0692e-05, -6.9104e-06,  6.8061e-06,  ..., -8.7172e-06,
         -7.0371e-06, -7.9349e-06],
        [ 3.2817e-04,  2.4819e-04, -3.4819e-04,  ...,  2.7887e-04,
          3.4136e-04,  2.0963e-04],
        [-3.1009e-05, -1.9893e-05,  2.0623e-05,  ..., -2.5436e-05,
         -2.0772e-05, -2.3812e-05],
        [-2.6107e-05, -1.8165e-05,  1.7434e-05,  ..., -2.1562e-05,
         -1.8254e-05, -1.8030e-05]], device='cuda:0')
Loss: 0.8793213963508606
Graident accumulation at epoch 1, step 1559, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0041,  ..., -0.0100, -0.0030, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.8344e-05,  1.3403e-04, -2.7879e-05,  ..., -3.2838e-05,
         -7.4249e-06, -1.4949e-05],
        [-6.7992e-06, -4.8984e-06,  5.0550e-06,  ..., -6.3138e-06,
         -4.0541e-06, -4.5368e-06],
        [ 3.5455e-05,  4.2795e-05, -3.8387e-05,  ...,  3.1006e-05,
          4.4589e-05,  1.3625e-05],
        [ 4.1203e-06,  1.5109e-05, -9.8437e-06,  ...,  1.8147e-05,
          2.2261e-05,  1.2162e-06],
        [-2.4915e-05, -1.8485e-05,  1.8003e-05,  ..., -2.1186e-05,
         -1.7800e-05, -1.6227e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3505e-08, 6.1373e-08, 4.6203e-08,  ..., 4.0081e-08, 1.2521e-07,
         5.3036e-08],
        [7.9413e-11, 5.8874e-11, 1.9382e-11,  ..., 5.6021e-11, 3.5123e-11,
         3.0048e-11],
        [4.1527e-09, 2.9110e-09, 1.4020e-09,  ..., 3.2261e-09, 1.4072e-09,
         1.0611e-09],
        [1.1399e-09, 1.1569e-09, 3.5164e-10,  ..., 9.9954e-10, 6.5037e-10,
         4.4353e-10],
        [3.6083e-10, 2.0562e-10, 7.8123e-11,  ..., 2.7062e-10, 7.8222e-11,
         1.0369e-10]], device='cuda:0')
optimizer state dict: 195.0
lr: [3.2382013209886466e-06, 3.2382013209886466e-06]
scheduler_last_epoch: 195


Running epoch 1, step 1560, batch 512
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  6354, 12230,     9],
        [    0,   578,   221,  ...,   287,  1254,  4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5097e-04,  6.6195e-05,  1.8560e-04,  ...,  2.3335e-04,
         -3.1450e-04,  2.6274e-04],
        [-1.4082e-06, -8.0094e-07,  7.6741e-07,  ..., -1.1772e-06,
         -8.4192e-07, -1.1101e-06],
        [-4.4107e-06, -2.6971e-06,  2.5928e-06,  ..., -3.7104e-06,
         -2.7120e-06, -3.4571e-06],
        [-4.2021e-06, -2.3842e-06,  2.3544e-06,  ..., -3.5763e-06,
         -2.5928e-06, -3.4273e-06],
        [-3.5912e-06, -2.1756e-06,  2.0266e-06,  ..., -3.0547e-06,
         -2.2799e-06, -2.7120e-06]], device='cuda:0')
Loss: 0.9129325747489929


Running epoch 1, step 1561, batch 513
Sampled inputs[:2]: tensor([[    0,  7527,    15,  ...,  2677,   292, 30654],
        [    0,  1086,    14,  ...,   963,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7180e-04,  1.0634e-04,  2.3920e-04,  ...,  1.3688e-04,
         -6.6040e-04,  1.3147e-04],
        [-2.7716e-06, -1.7248e-06,  1.6466e-06,  ..., -2.3246e-06,
         -1.7695e-06, -2.1681e-06],
        [-8.6725e-06, -5.6922e-06,  5.5581e-06,  ..., -7.2420e-06,
         -5.5879e-06, -6.7055e-06],
        [-8.0764e-06, -4.9770e-06,  4.9770e-06,  ..., -6.8247e-06,
         -5.2452e-06, -6.5118e-06],
        [-7.1079e-06, -4.6343e-06,  4.3660e-06,  ..., -6.0052e-06,
         -4.7535e-06, -5.2154e-06]], device='cuda:0')
Loss: 0.9116348028182983


Running epoch 1, step 1562, batch 514
Sampled inputs[:2]: tensor([[    0,  2314,   516,  ...,  1871,    13,  1303],
        [    0, 20291,  1990,  ...,   298,   732,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5232e-04,  1.0145e-04,  1.2511e-04,  ...,  6.4742e-05,
         -7.6325e-04,  7.7660e-05],
        [-4.0010e-06, -2.5555e-06,  2.5891e-06,  ..., -3.3677e-06,
         -2.5928e-06, -3.0324e-06],
        [-1.2696e-05, -8.5533e-06,  8.8364e-06,  ..., -1.0654e-05,
         -8.2999e-06, -9.5069e-06],
        [-1.1846e-05, -7.5102e-06,  8.0168e-06,  ..., -1.0028e-05,
         -7.7635e-06, -9.2536e-06],
        [-1.0028e-05, -6.7800e-06,  6.6906e-06,  ..., -8.5086e-06,
         -6.8396e-06, -7.1153e-06]], device='cuda:0')
Loss: 0.9846497774124146


Running epoch 1, step 1563, batch 515
Sampled inputs[:2]: tensor([[    0,   271, 16084,  ...,   688,  1122,    12],
        [    0,   273,   298,  ..., 23554,    12,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6083e-04, -3.8525e-05,  4.1877e-06,  ...,  5.4286e-06,
         -6.0072e-04,  1.2141e-04],
        [-5.4389e-06, -3.5688e-06,  3.4757e-06,  ..., -4.5672e-06,
         -3.5763e-06, -4.0680e-06],
        [-1.7077e-05, -1.1861e-05,  1.1727e-05,  ..., -1.4380e-05,
         -1.1429e-05, -1.2696e-05],
        [-1.5870e-05, -1.0401e-05,  1.0625e-05,  ..., -1.3441e-05,
         -1.0625e-05, -1.2279e-05],
        [-1.3769e-05, -9.6560e-06,  9.0897e-06,  ..., -1.1727e-05,
         -9.6262e-06, -9.7081e-06]], device='cuda:0')
Loss: 0.9839529395103455


Running epoch 1, step 1564, batch 516
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0,  1110, 26330,  ...,  1558,   674,  2351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9664e-04,  2.3595e-05,  3.6939e-05,  ...,  8.1354e-05,
         -9.0424e-04, -1.4254e-04],
        [-6.6683e-06, -4.3511e-06,  4.2990e-06,  ..., -5.6103e-06,
         -4.4145e-06, -5.0105e-06],
        [-2.0862e-05, -1.4365e-05,  1.4469e-05,  ..., -1.7524e-05,
         -1.3933e-05, -1.5497e-05],
        [-1.9491e-05, -1.2651e-05,  1.3173e-05,  ..., -1.6496e-05,
         -1.3083e-05, -1.5154e-05],
        [-1.6794e-05, -1.1727e-05,  1.1221e-05,  ..., -1.4275e-05,
         -1.1757e-05, -1.1779e-05]], device='cuda:0')
Loss: 0.9238176345825195


Running epoch 1, step 1565, batch 517
Sampled inputs[:2]: tensor([[    0,  8588,  3937,  ...,   516,  1128,  2341],
        [    0,  1172,   365,  ...,  1119, 15573,  3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9664e-04,  1.1277e-04,  1.4290e-04,  ...,  4.6148e-05,
         -1.0073e-03, -2.5130e-04],
        [-7.9274e-06, -5.2676e-06,  5.1148e-06,  ..., -6.7055e-06,
         -5.3570e-06, -5.9418e-06],
        [-2.4796e-05, -1.7315e-05,  1.7241e-05,  ..., -2.0891e-05,
         -1.6853e-05, -1.8328e-05],
        [-2.3201e-05, -1.5303e-05,  1.5691e-05,  ..., -1.9729e-05,
         -1.5900e-05, -1.8001e-05],
        [-1.9893e-05, -1.4111e-05,  1.3351e-05,  ..., -1.6972e-05,
         -1.4171e-05, -1.3866e-05]], device='cuda:0')
Loss: 0.9612934589385986


Running epoch 1, step 1566, batch 518
Sampled inputs[:2]: tensor([[    0,  1101,   300,  ...,  6104,   367,   993],
        [    0, 18901,     5,  ...,  2253,   278, 17423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0651e-04,  1.2357e-04,  2.0914e-04,  ...,  8.6518e-05,
         -9.0279e-04, -1.3724e-04],
        [-9.2238e-06, -6.2585e-06,  5.9344e-06,  ..., -7.8008e-06,
         -6.3404e-06, -6.8285e-06],
        [-2.8819e-05, -2.0519e-05,  1.9953e-05,  ..., -2.4319e-05,
         -1.9953e-05, -2.1070e-05],
        [-2.7075e-05, -1.8254e-05,  1.8224e-05,  ..., -2.3022e-05,
         -1.8895e-05, -2.0772e-05],
        [-2.3112e-05, -1.6749e-05,  1.5482e-05,  ..., -1.9744e-05,
         -1.6749e-05, -1.5922e-05]], device='cuda:0')
Loss: 1.0020781755447388


Running epoch 1, step 1567, batch 519
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  984,   14,  759],
        [   0, 3036,  471,  ...,  287, 1906,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8798e-04,  1.1339e-04,  1.7369e-04,  ...,  6.6375e-05,
         -1.0630e-03, -2.6197e-04],
        [-1.0498e-05, -7.2196e-06,  6.8434e-06,  ..., -8.8140e-06,
         -7.1973e-06, -7.7076e-06],
        [-3.3021e-05, -2.3857e-05,  2.3112e-05,  ..., -2.7686e-05,
         -2.2858e-05, -2.3991e-05],
        [-3.0875e-05, -2.1130e-05,  2.1055e-05,  ..., -2.6062e-05,
         -2.1487e-05, -2.3484e-05],
        [-2.6226e-05, -1.9297e-05,  1.7777e-05,  ..., -2.2292e-05,
         -1.9014e-05, -1.7948e-05]], device='cuda:0')
Loss: 0.9704444408416748
Graident accumulation at epoch 1, step 1567, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0041,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.2877e-06,  1.3196e-04, -7.7219e-06,  ..., -2.2917e-05,
         -1.1298e-04, -3.9651e-05],
        [-7.1690e-06, -5.1306e-06,  5.2338e-06,  ..., -6.5638e-06,
         -4.3684e-06, -4.8539e-06],
        [ 2.8607e-05,  3.6129e-05, -3.2237e-05,  ...,  2.5137e-05,
          3.7844e-05,  9.8631e-06],
        [ 6.2076e-07,  1.1485e-05, -6.7538e-06,  ...,  1.3726e-05,
          1.7886e-05, -1.2539e-06],
        [-2.5046e-05, -1.8566e-05,  1.7980e-05,  ..., -2.1296e-05,
         -1.7922e-05, -1.6399e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3514e-08, 6.1324e-08, 4.6187e-08,  ..., 4.0046e-08, 1.2621e-07,
         5.3052e-08],
        [7.9444e-11, 5.8867e-11, 1.9410e-11,  ..., 5.6043e-11, 3.5140e-11,
         3.0077e-11],
        [4.1497e-09, 2.9086e-09, 1.4011e-09,  ..., 3.2237e-09, 1.4063e-09,
         1.0606e-09],
        [1.1397e-09, 1.1562e-09, 3.5174e-10,  ..., 9.9922e-10, 6.5018e-10,
         4.4364e-10],
        [3.6116e-10, 2.0579e-10, 7.8361e-11,  ..., 2.7084e-10, 7.8505e-11,
         1.0391e-10]], device='cuda:0')
optimizer state dict: 196.0
lr: [3.1476473893945937e-06, 3.1476473893945937e-06]
scheduler_last_epoch: 196


Running epoch 1, step 1568, batch 520
Sampled inputs[:2]: tensor([[   0,  493,  221,  ...,  259,  726, 2786],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4764e-04, -1.0714e-04, -1.6592e-04,  ..., -2.4837e-05,
         -2.5730e-04, -1.5239e-04],
        [-1.3858e-06, -8.6054e-07,  1.0207e-06,  ..., -1.1325e-06,
         -8.7172e-07, -9.8348e-07],
        [-4.3809e-06, -2.8610e-06,  3.3677e-06,  ..., -3.5763e-06,
         -2.7865e-06, -3.0696e-06],
        [-4.0531e-06, -2.5034e-06,  3.0994e-06,  ..., -3.2932e-06,
         -2.5481e-06, -2.9653e-06],
        [-3.3826e-06, -2.2799e-06,  2.5630e-06,  ..., -2.8163e-06,
         -2.3097e-06, -2.2501e-06]], device='cuda:0')
Loss: 0.9760165214538574


Running epoch 1, step 1569, batch 521
Sampled inputs[:2]: tensor([[   0, 1067,  271,  ...,  266,  940,  271],
        [   0,  300, 3808,  ...,  496,   14, 1364]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0299e-04, -2.5426e-04, -1.6732e-04,  ..., -4.3780e-05,
         -1.6974e-04, -5.2948e-05],
        [-2.6450e-06, -1.8217e-06,  1.8738e-06,  ..., -2.2054e-06,
         -1.7807e-06, -1.9334e-06],
        [-8.4937e-06, -6.1095e-06,  6.3628e-06,  ..., -7.0482e-06,
         -5.7220e-06, -6.1393e-06],
        [-7.9572e-06, -5.4538e-06,  5.8562e-06,  ..., -6.6310e-06,
         -5.3793e-06, -6.0201e-06],
        [-6.5565e-06, -4.8429e-06,  4.8280e-06,  ..., -5.5432e-06,
         -4.6790e-06, -4.4703e-06]], device='cuda:0')
Loss: 0.9797962307929993


Running epoch 1, step 1570, batch 522
Sampled inputs[:2]: tensor([[    0, 16187,   565,  ...,   586,  3196,   271],
        [    0, 13964,    13,  ...,    14,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4071e-04, -2.7887e-04, -1.1154e-04,  ..., -1.3841e-04,
         -1.6110e-04, -1.3786e-04],
        [-3.9861e-06, -2.7455e-06,  2.8275e-06,  ..., -3.3081e-06,
         -2.6524e-06, -2.8647e-06],
        [-1.2666e-05, -9.1940e-06,  9.5367e-06,  ..., -1.0520e-05,
         -8.5086e-06, -9.0748e-06],
        [-1.1802e-05, -8.1062e-06,  8.7321e-06,  ..., -9.8050e-06,
         -7.9274e-06, -8.8364e-06],
        [-9.7007e-06, -7.2420e-06,  7.1675e-06,  ..., -8.2105e-06,
         -6.9290e-06, -6.5416e-06]], device='cuda:0')
Loss: 0.9323351383209229


Running epoch 1, step 1571, batch 523
Sampled inputs[:2]: tensor([[    0,   623,    12,  ...,  4792,  6572,   300],
        [    0, 11661,    12,  ...,  1707,   394,   264]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8780e-04, -2.9273e-04, -9.7096e-05,  ..., -2.1738e-04,
         -1.9843e-04, -4.9718e-05],
        [-5.2676e-06, -3.6396e-06,  3.7886e-06,  ..., -4.3288e-06,
         -3.4720e-06, -3.7737e-06],
        [-1.6719e-05, -1.2144e-05,  1.2755e-05,  ..., -1.3754e-05,
         -1.1131e-05, -1.1936e-05],
        [-1.5616e-05, -1.0729e-05,  1.1742e-05,  ..., -1.2830e-05,
         -1.0371e-05, -1.1653e-05],
        [-1.2711e-05, -9.5069e-06,  9.5069e-06,  ..., -1.0669e-05,
         -9.0152e-06, -8.5384e-06]], device='cuda:0')
Loss: 0.9466057419776917


Running epoch 1, step 1572, batch 524
Sampled inputs[:2]: tensor([[    0,  1796,   342,  ...,   668,  2903,   518],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8560e-04, -3.5185e-04,  1.3275e-05,  ..., -2.9833e-04,
         -4.9117e-05,  2.0482e-04],
        [-6.6683e-06, -4.5784e-06,  4.6343e-06,  ..., -5.4985e-06,
         -4.3958e-06, -4.8168e-06],
        [-2.1040e-05, -1.5244e-05,  1.5602e-05,  ..., -1.7360e-05,
         -1.4082e-05, -1.5125e-05],
        [-1.9580e-05, -1.3396e-05,  1.4260e-05,  ..., -1.6168e-05,
         -1.3083e-05, -1.4678e-05],
        [-1.6287e-05, -1.2100e-05,  1.1787e-05,  ..., -1.3709e-05,
         -1.1548e-05, -1.1057e-05]], device='cuda:0')
Loss: 0.9531919360160828


Running epoch 1, step 1573, batch 525
Sampled inputs[:2]: tensor([[   0, 6909,  352,  ..., 1075,  706, 6909],
        [   0, 2029,   13,  ...,   12, 4536,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6638e-04, -3.6678e-04, -2.2818e-05,  ..., -3.5662e-04,
          2.0394e-04,  3.5785e-04],
        [-8.0243e-06, -5.4166e-06,  5.5246e-06,  ..., -6.6310e-06,
         -5.2266e-06, -5.7779e-06],
        [-2.5153e-05, -1.8001e-05,  1.8552e-05,  ..., -2.0802e-05,
         -1.6704e-05, -1.7986e-05],
        [-2.3454e-05, -1.5825e-05,  1.6958e-05,  ..., -1.9446e-05,
         -1.5572e-05, -1.7568e-05],
        [-1.9461e-05, -1.4305e-05,  1.4022e-05,  ..., -1.6406e-05,
         -1.3694e-05, -1.3128e-05]], device='cuda:0')
Loss: 0.9945275783538818


Running epoch 1, step 1574, batch 526
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,    13,   635,    13],
        [    0,  3984, 13077,  ...,   287,   650,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7061e-04, -4.4080e-04,  1.1523e-04,  ..., -3.1715e-04,
          3.0466e-04,  3.6331e-04],
        [-9.3505e-06, -6.2883e-06,  6.3851e-06,  ..., -7.7486e-06,
         -6.1803e-06, -6.8136e-06],
        [-2.9236e-05, -2.0787e-05,  2.1413e-05,  ..., -2.4199e-05,
         -1.9640e-05, -2.1055e-05],
        [-2.7359e-05, -1.8373e-05,  1.9625e-05,  ..., -2.2754e-05,
         -1.8448e-05, -2.0698e-05],
        [-2.2724e-05, -1.6585e-05,  1.6287e-05,  ..., -1.9148e-05,
         -1.6138e-05, -1.5423e-05]], device='cuda:0')
Loss: 0.9574529528617859


Running epoch 1, step 1575, batch 527
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0,  1549,  7052,  ...,  2529,  3958,    37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0448e-04, -4.7561e-04,  2.2457e-04,  ..., -2.6367e-04,
          4.0463e-04,  4.0582e-04],
        [-1.0617e-05, -7.2382e-06,  7.2680e-06,  ..., -8.8066e-06,
         -7.1190e-06, -7.7412e-06],
        [-3.3379e-05, -2.4065e-05,  2.4483e-05,  ..., -2.7686e-05,
         -2.2739e-05, -2.4095e-05],
        [-3.1203e-05, -2.1249e-05,  2.2411e-05,  ..., -2.6003e-05,
         -2.1368e-05, -2.3678e-05],
        [-2.5883e-05, -1.9163e-05,  1.8567e-05,  ..., -2.1845e-05,
         -1.8612e-05, -1.7583e-05]], device='cuda:0')
Loss: 1.0020781755447388
Graident accumulation at epoch 1, step 1575, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0041,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3407e-05,  7.1207e-05,  1.5507e-05,  ..., -4.6992e-05,
         -6.1221e-05,  4.8960e-06],
        [-7.5139e-06, -5.3413e-06,  5.4373e-06,  ..., -6.7881e-06,
         -4.6434e-06, -5.1426e-06],
        [ 2.2409e-05,  3.0110e-05, -2.6565e-05,  ...,  1.9854e-05,
          3.1786e-05,  6.4673e-06],
        [-2.5616e-06,  8.2119e-06, -3.8372e-06,  ...,  9.7529e-06,
          1.3960e-05, -3.4963e-06],
        [-2.5130e-05, -1.8626e-05,  1.8039e-05,  ..., -2.1351e-05,
         -1.7991e-05, -1.6518e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3452e-08, 6.1489e-08, 4.6192e-08,  ..., 4.0075e-08, 1.2625e-07,
         5.3164e-08],
        [7.9477e-11, 5.8861e-11, 1.9443e-11,  ..., 5.6064e-11, 3.5155e-11,
         3.0107e-11],
        [4.1466e-09, 2.9063e-09, 1.4003e-09,  ..., 3.2212e-09, 1.4054e-09,
         1.0602e-09],
        [1.1395e-09, 1.1555e-09, 3.5189e-10,  ..., 9.9889e-10, 6.4999e-10,
         4.4375e-10],
        [3.6147e-10, 2.0595e-10, 7.8628e-11,  ..., 2.7105e-10, 7.8773e-11,
         1.0412e-10]], device='cuda:0')
optimizer state dict: 197.0
lr: [3.058140557094472e-06, 3.058140557094472e-06]
scheduler_last_epoch: 197


Running epoch 1, step 1576, batch 528
Sampled inputs[:2]: tensor([[   0,   18,   14,  ...,  300,  275, 1184],
        [   0, 1765, 5370,  ..., 1711,  292,  380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3378e-04, -9.3469e-06, -2.8107e-05,  ..., -2.0229e-05,
          4.8193e-05,  4.8513e-05],
        [-1.2666e-06, -9.9093e-07,  9.8348e-07,  ..., -1.0356e-06,
         -8.7917e-07, -8.9779e-07],
        [-4.1127e-06, -3.3826e-06,  3.3826e-06,  ..., -3.3975e-06,
         -2.9057e-06, -2.9206e-06],
        [-3.8743e-06, -3.0398e-06,  3.1441e-06,  ..., -3.1888e-06,
         -2.7567e-06, -2.8908e-06],
        [-3.0100e-06, -2.5630e-06,  2.4140e-06,  ..., -2.5332e-06,
         -2.2501e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9864165186882019


Running epoch 1, step 1577, batch 529
Sampled inputs[:2]: tensor([[    0,   259,  2122,  ...,   554,   392, 10814],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6786e-04, -2.0246e-04, -3.1051e-05,  ..., -2.1246e-04,
         -1.4752e-04, -2.0910e-04],
        [-2.5183e-06, -1.8738e-06,  1.9446e-06,  ..., -2.0489e-06,
         -1.6950e-06, -1.7658e-06],
        [-8.0168e-06, -6.2883e-06,  6.5565e-06,  ..., -6.5714e-06,
         -5.4687e-06, -5.6624e-06],
        [-7.6294e-06, -5.6475e-06,  6.1542e-06,  ..., -6.2138e-06,
         -5.2005e-06, -5.6326e-06],
        [-5.9158e-06, -4.8280e-06,  4.7237e-06,  ..., -4.9621e-06,
         -4.3064e-06, -3.8818e-06]], device='cuda:0')
Loss: 0.9703301191329956


Running epoch 1, step 1578, batch 530
Sampled inputs[:2]: tensor([[    0,    34,     9,  ...,    19,    14, 45576],
        [    0,   342,  4781,  ...,   630,   940,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7118e-04, -1.4340e-04, -4.1725e-06,  ..., -1.5967e-04,
         -1.4091e-04, -2.0560e-04],
        [-3.8594e-06, -2.7455e-06,  2.7530e-06,  ..., -3.1739e-06,
         -2.6077e-06, -2.8461e-06],
        [-1.2368e-05, -9.1791e-06,  9.4175e-06,  ..., -1.0133e-05,
         -8.3596e-06, -9.0748e-06],
        [-1.1563e-05, -8.1360e-06,  8.5980e-06,  ..., -9.5069e-06,
         -7.9125e-06, -8.8662e-06],
        [-9.4324e-06, -7.1973e-06,  6.9737e-06,  ..., -7.8976e-06,
         -6.7651e-06, -6.4597e-06]], device='cuda:0')
Loss: 0.9454251527786255


Running epoch 1, step 1579, batch 531
Sampled inputs[:2]: tensor([[   0,  266, 1624,  ...,   14,   19,   14],
        [   0,  278,  668,  ..., 2743,  638,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6746e-04, -2.8615e-04, -9.1552e-05,  ..., -8.1889e-05,
         -1.4135e-04, -1.7285e-04],
        [-5.2378e-06, -3.6135e-06,  3.8110e-06,  ..., -4.2170e-06,
         -3.3751e-06, -3.8147e-06],
        [-1.6749e-05, -1.2100e-05,  1.2994e-05,  ..., -1.3471e-05,
         -1.0863e-05, -1.2159e-05],
        [-1.5587e-05, -1.0669e-05,  1.1846e-05,  ..., -1.2562e-05,
         -1.0177e-05, -1.1772e-05],
        [-1.2577e-05, -9.3728e-06,  9.4622e-06,  ..., -1.0341e-05,
         -8.6874e-06, -8.5309e-06]], device='cuda:0')
Loss: 0.9401735067367554


Running epoch 1, step 1580, batch 532
Sampled inputs[:2]: tensor([[    0,  3001,  3325,  ..., 16332,  2661,  1200],
        [    0,  2229,   352,  ...,  4988,    33,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0996e-04, -3.0970e-04, -2.2130e-04,  ..., -9.6081e-05,
         -9.9395e-05, -1.8483e-04],
        [-6.5267e-06, -4.5113e-06,  4.7944e-06,  ..., -5.2303e-06,
         -4.2357e-06, -4.7088e-06],
        [-2.0742e-05, -1.5035e-05,  1.6272e-05,  ..., -1.6615e-05,
         -1.3590e-05, -1.4871e-05],
        [-1.9357e-05, -1.3292e-05,  1.4886e-05,  ..., -1.5527e-05,
         -1.2755e-05, -1.4469e-05],
        [-1.5602e-05, -1.1668e-05,  1.1891e-05,  ..., -1.2755e-05,
         -1.0863e-05, -1.0431e-05]], device='cuda:0')
Loss: 0.9725784659385681


Running epoch 1, step 1581, batch 533
Sampled inputs[:2]: tensor([[    0,   352,   721,  ...,   634, 17642,   278],
        [    0,  1042,  2548,  ...,   328,   259,  2771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3716e-04, -3.0732e-04, -1.2673e-04,  ..., -1.3035e-04,
         -4.0953e-05, -1.4511e-04],
        [-7.8008e-06, -5.4613e-06,  5.7220e-06,  ..., -6.2808e-06,
         -5.1297e-06, -5.6177e-06],
        [-2.4945e-05, -1.8299e-05,  1.9521e-05,  ..., -2.0072e-05,
         -1.6570e-05, -1.7866e-05],
        [-2.3231e-05, -1.6168e-05,  1.7852e-05,  ..., -1.8716e-05,
         -1.5512e-05, -1.7360e-05],
        [-1.8701e-05, -1.4171e-05,  1.4231e-05,  ..., -1.5363e-05,
         -1.3202e-05, -1.2502e-05]], device='cuda:0')
Loss: 0.9778821468353271


Running epoch 1, step 1582, batch 534
Sampled inputs[:2]: tensor([[    0,  1883,  1090,  ...,   365,  1943,   298],
        [    0, 14949,    12,  ...,   669, 10168,  7166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7548e-04, -2.8334e-04, -2.2421e-04,  ...,  6.0505e-05,
         -3.1879e-04, -2.7725e-04],
        [-9.0748e-06, -6.2995e-06,  6.7651e-06,  ..., -7.3016e-06,
         -5.9493e-06, -6.5379e-06],
        [-2.8878e-05, -2.1011e-05,  2.2933e-05,  ..., -2.3201e-05,
         -1.9103e-05, -2.0653e-05],
        [-2.7046e-05, -1.8656e-05,  2.1145e-05,  ..., -2.1756e-05,
         -1.8001e-05, -2.0221e-05],
        [-2.1562e-05, -1.6242e-05,  1.6659e-05,  ..., -1.7688e-05,
         -1.5169e-05, -1.4357e-05]], device='cuda:0')
Loss: 0.9425307512283325


Running epoch 1, step 1583, batch 535
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0,  445,   18,  ..., 1478,  578,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6936e-04, -2.3840e-04, -1.7083e-04,  ...,  6.9814e-05,
         -3.8811e-04, -2.1648e-04],
        [-1.0476e-05, -7.2010e-06,  7.5698e-06,  ..., -8.4192e-06,
         -6.8955e-06, -7.5884e-06],
        [-3.3319e-05, -2.4065e-05,  2.5675e-05,  ..., -2.6777e-05,
         -2.2188e-05, -2.4006e-05],
        [-3.1158e-05, -2.1294e-05,  2.3589e-05,  ..., -2.5064e-05,
         -2.0817e-05, -2.3410e-05],
        [-2.5183e-05, -1.8775e-05,  1.8790e-05,  ..., -2.0668e-05,
         -1.7822e-05, -1.6950e-05]], device='cuda:0')
Loss: 0.9482671618461609
Graident accumulation at epoch 1, step 1583, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.9002e-05,  4.0247e-05, -3.1268e-06,  ..., -3.5311e-05,
         -9.3910e-05, -1.7242e-05],
        [-7.8100e-06, -5.5273e-06,  5.6505e-06,  ..., -6.9512e-06,
         -4.8686e-06, -5.3872e-06],
        [ 1.6836e-05,  2.4692e-05, -2.1341e-05,  ...,  1.5191e-05,
          2.6389e-05,  3.4200e-06],
        [-5.4213e-06,  5.2613e-06, -1.0947e-06,  ...,  6.2712e-06,
          1.0483e-05, -5.4876e-06],
        [-2.5135e-05, -1.8641e-05,  1.8114e-05,  ..., -2.1283e-05,
         -1.7974e-05, -1.6561e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3826e-08, 6.1484e-08, 4.6175e-08,  ..., 4.0040e-08, 1.2627e-07,
         5.3157e-08],
        [7.9507e-11, 5.8854e-11, 1.9481e-11,  ..., 5.6079e-11, 3.5168e-11,
         3.0134e-11],
        [4.1436e-09, 2.9040e-09, 1.3996e-09,  ..., 3.2187e-09, 1.4045e-09,
         1.0597e-09],
        [1.1393e-09, 1.1548e-09, 3.5209e-10,  ..., 9.9852e-10, 6.4977e-10,
         4.4386e-10],
        [3.6174e-10, 2.0610e-10, 7.8902e-11,  ..., 2.7121e-10, 7.9012e-11,
         1.0430e-10]], device='cuda:0')
optimizer state dict: 198.0
lr: [2.969694501513574e-06, 2.969694501513574e-06]
scheduler_last_epoch: 198


Running epoch 1, step 1584, batch 536
Sampled inputs[:2]: tensor([[   0, 1478,   14,  ...,  266, 9417, 9105],
        [   0,   17,  292,  ..., 8055,  365, 3125]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7953e-05,  6.6636e-06,  8.9166e-05,  ..., -1.0864e-04,
          1.9752e-04,  7.0635e-05],
        [-1.3039e-06, -9.4995e-07,  8.5309e-07,  ..., -1.1399e-06,
         -9.5367e-07, -9.9093e-07],
        [-4.0829e-06, -3.1888e-06,  2.9206e-06,  ..., -3.5912e-06,
         -3.0845e-06, -3.1143e-06],
        [-3.8147e-06, -2.8014e-06,  2.6375e-06,  ..., -3.3826e-06,
         -2.8908e-06, -3.0547e-06],
        [-3.2634e-06, -2.6226e-06,  2.2352e-06,  ..., -2.9355e-06,
         -2.6077e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.976824164390564


Running epoch 1, step 1585, batch 537
Sampled inputs[:2]: tensor([[   0,   24,   15,  ...,  221,  380,  417],
        [   0, 6847,  437,  ...,   17,   14,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8663e-05,  1.7024e-04,  3.0351e-04,  ..., -2.6298e-04,
          2.6741e-04, -4.9330e-06],
        [-2.7344e-06, -1.7844e-06,  1.6391e-06,  ..., -2.3171e-06,
         -1.9521e-06, -2.1458e-06],
        [-8.4043e-06, -5.9754e-06,  5.5581e-06,  ..., -7.1377e-06,
         -6.1989e-06, -6.5416e-06],
        [-7.7784e-06, -5.1558e-06,  4.9174e-06,  ..., -6.7055e-06,
         -5.7966e-06, -6.3032e-06],
        [-6.9290e-06, -4.9770e-06,  4.3809e-06,  ..., -6.0052e-06,
         -5.3346e-06, -5.0664e-06]], device='cuda:0')
Loss: 0.943516194820404


Running epoch 1, step 1586, batch 538
Sampled inputs[:2]: tensor([[   0, 6067, 1188,  ..., 5282,  756,  342],
        [   0,  266,  824,  ..., 1799,  287, 6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1735e-06,  7.1707e-05,  2.0034e-04,  ..., -3.9141e-04,
          3.8654e-04,  1.1009e-04],
        [-4.0233e-06, -2.7381e-06,  2.6450e-06,  ..., -3.3528e-06,
         -2.8461e-06, -3.0100e-06],
        [-1.2577e-05, -9.1940e-06,  8.9705e-06,  ..., -1.0505e-05,
         -9.1195e-06, -9.3430e-06],
        [-1.1563e-05, -7.9423e-06,  8.0019e-06,  ..., -9.7603e-06,
         -8.4490e-06, -8.9705e-06],
        [-9.9540e-06, -7.4059e-06,  6.8098e-06,  ..., -8.5086e-06,
         -7.5847e-06, -6.9663e-06]], device='cuda:0')
Loss: 0.9930137395858765


Running epoch 1, step 1587, batch 539
Sampled inputs[:2]: tensor([[   0, 3860,  694,  ..., 1027,  292,  221],
        [   0,  635,   13,  ...,   13, 4710, 1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4415e-04,  1.2019e-06,  1.7828e-04,  ..., -3.6769e-04,
          5.8169e-04,  1.7706e-04],
        [-5.3048e-06, -3.6247e-06,  3.5539e-06,  ..., -4.4554e-06,
         -3.7812e-06, -4.0233e-06],
        [-1.6451e-05, -1.2010e-05,  1.1936e-05,  ..., -1.3798e-05,
         -1.1936e-05, -1.2293e-05],
        [-1.5348e-05, -1.0535e-05,  1.0788e-05,  ..., -1.3024e-05,
         -1.1250e-05, -1.2055e-05],
        [-1.3053e-05, -9.6858e-06,  9.1344e-06,  ..., -1.1146e-05,
         -9.9093e-06, -9.1717e-06]], device='cuda:0')
Loss: 0.981915295124054


Running epoch 1, step 1588, batch 540
Sampled inputs[:2]: tensor([[    0,    14,   475,  ..., 44038,    12,   894],
        [    0,  5332,   391,  ...,   221,   334,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7777e-04,  5.2600e-05,  2.5638e-04,  ..., -4.7970e-04,
          6.8580e-04,  1.7696e-04],
        [-6.5267e-06, -4.6603e-06,  4.3288e-06,  ..., -5.5432e-06,
         -4.7721e-06, -4.9099e-06],
        [-2.0295e-05, -1.5438e-05,  1.4603e-05,  ..., -1.7211e-05,
         -1.5095e-05, -1.5035e-05],
        [-1.9059e-05, -1.3679e-05,  1.3247e-05,  ..., -1.6347e-05,
         -1.4335e-05, -1.4856e-05],
        [-1.6093e-05, -1.2428e-05,  1.1191e-05,  ..., -1.3888e-05,
         -1.2517e-05, -1.1198e-05]], device='cuda:0')
Loss: 0.9691342711448669


Running epoch 1, step 1589, batch 541
Sampled inputs[:2]: tensor([[    0, 23842,   342,  ...,   365,  4011, 10151],
        [    0,   591,   953,  ...,  4118,  5750,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1564e-04, -1.9395e-05,  1.9937e-04,  ..., -5.1029e-04,
          6.9214e-04,  1.2626e-04],
        [-7.7784e-06, -5.5619e-06,  5.1223e-06,  ..., -6.6608e-06,
         -5.7183e-06, -5.8301e-06],
        [-2.4199e-05, -1.8403e-05,  1.7315e-05,  ..., -2.0668e-05,
         -1.8060e-05, -1.7852e-05],
        [-2.2739e-05, -1.6332e-05,  1.5721e-05,  ..., -1.9655e-05,
         -1.7166e-05, -1.7688e-05],
        [-1.9208e-05, -1.4856e-05,  1.3322e-05,  ..., -1.6689e-05,
         -1.4991e-05, -1.3299e-05]], device='cuda:0')
Loss: 0.9715189933776855


Running epoch 1, step 1590, batch 542
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,  1597,   278,  ...,    20,    38,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9508e-05, -1.0050e-05,  2.9582e-04,  ..., -4.1506e-04,
          9.0865e-04,  1.6481e-04],
        [-9.0823e-06, -6.4448e-06,  5.9754e-06,  ..., -7.7486e-06,
         -6.6347e-06, -6.8806e-06],
        [-2.8372e-05, -2.1383e-05,  2.0251e-05,  ..., -2.4140e-05,
         -2.1026e-05, -2.1175e-05],
        [-2.6554e-05, -1.8895e-05,  1.8314e-05,  ..., -2.2873e-05,
         -1.9938e-05, -2.0906e-05],
        [-2.2486e-05, -1.7270e-05,  1.5572e-05,  ..., -1.9476e-05,
         -1.7434e-05, -1.5743e-05]], device='cuda:0')
Loss: 0.9501957893371582


Running epoch 1, step 1591, batch 543
Sampled inputs[:2]: tensor([[    0,   396,   298,  ...,    52,  5065,    13],
        [    0,  1340,   800,  ...,   259, 13583,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5886e-04, -1.7398e-04,  3.4088e-04,  ..., -5.3536e-04,
          1.1666e-03,  3.2728e-04],
        [-1.0364e-05, -7.3090e-06,  6.7838e-06,  ..., -8.8587e-06,
         -7.5325e-06, -7.9535e-06],
        [-3.2365e-05, -2.4185e-05,  2.3022e-05,  ..., -2.7537e-05,
         -2.3812e-05, -2.4408e-05],
        [-3.0324e-05, -2.1398e-05,  2.0817e-05,  ..., -2.6152e-05,
         -2.2635e-05, -2.4140e-05],
        [-2.5794e-05, -1.9580e-05,  1.7792e-05,  ..., -2.2292e-05,
         -1.9804e-05, -1.8217e-05]], device='cuda:0')
Loss: 0.9396977424621582
Graident accumulation at epoch 1, step 1591, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0062, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.5215e-05,  1.8824e-05,  3.1274e-05,  ..., -8.5316e-05,
          3.2137e-05,  1.7210e-05],
        [-8.0654e-06, -5.7055e-06,  5.7638e-06,  ..., -7.1419e-06,
         -5.1350e-06, -5.6438e-06],
        [ 1.1916e-05,  1.9805e-05, -1.6905e-05,  ...,  1.0918e-05,
          2.1369e-05,  6.3718e-07],
        [-7.9115e-06,  2.5954e-06,  1.0965e-06,  ...,  3.0290e-06,
          7.1709e-06, -7.3528e-06],
        [-2.5201e-05, -1.8735e-05,  1.8082e-05,  ..., -2.1384e-05,
         -1.8157e-05, -1.6726e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3778e-08, 6.1453e-08, 4.6245e-08,  ..., 4.0286e-08, 1.2751e-07,
         5.3211e-08],
        [7.9535e-11, 5.8848e-11, 1.9508e-11,  ..., 5.6101e-11, 3.5189e-11,
         3.0168e-11],
        [4.1405e-09, 2.9017e-09, 1.3987e-09,  ..., 3.2162e-09, 1.4037e-09,
         1.0592e-09],
        [1.1391e-09, 1.1541e-09, 3.5217e-10,  ..., 9.9821e-10, 6.4964e-10,
         4.4400e-10],
        [3.6205e-10, 2.0628e-10, 7.9140e-11,  ..., 2.7143e-10, 7.9325e-11,
         1.0453e-10]], device='cuda:0')
optimizer state dict: 199.0
lr: [2.882322737981248e-06, 2.882322737981248e-06]
scheduler_last_epoch: 199


Running epoch 1, step 1592, batch 544
Sampled inputs[:2]: tensor([[    0,   292, 23950,  ...,  9305,   287,  4401],
        [    0,  1549,   824,  ...,  3609,   720,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0941e-05,  7.1623e-05,  6.1103e-05,  ...,  1.3074e-05,
         -4.0464e-05,  3.5093e-05],
        [-1.2293e-06, -9.4622e-07,  8.8662e-07,  ..., -1.0431e-06,
         -8.9779e-07, -8.9407e-07],
        [-3.8743e-06, -3.1888e-06,  3.0100e-06,  ..., -3.3230e-06,
         -2.8759e-06, -2.8163e-06],
        [-3.7551e-06, -2.9355e-06,  2.8610e-06,  ..., -3.2336e-06,
         -2.8312e-06, -2.9057e-06],
        [-2.9057e-06, -2.4736e-06,  2.2054e-06,  ..., -2.5332e-06,
         -2.2799e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.9581247568130493


Running epoch 1, step 1593, batch 545
Sampled inputs[:2]: tensor([[    0,   591, 36195,  ...,  3359,   717,    12],
        [    0,    19,    14,  ...,    13,  6673,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3339e-04, -1.5083e-04, -3.6661e-05,  ...,  2.9371e-04,
          2.2965e-04,  2.6879e-04],
        [-2.5034e-06, -1.7807e-06,  1.7025e-06,  ..., -2.1756e-06,
         -1.7919e-06, -1.9670e-06],
        [-7.7188e-06, -5.8115e-06,  5.7817e-06,  ..., -6.6459e-06,
         -5.5581e-06, -5.8860e-06],
        [-7.4655e-06, -5.3495e-06,  5.4091e-06,  ..., -6.5267e-06,
         -5.4687e-06, -6.0499e-06],
        [-6.0499e-06, -4.5747e-06,  4.3809e-06,  ..., -5.2750e-06,
         -4.5598e-06, -4.3064e-06]], device='cuda:0')
Loss: 0.9387523531913757


Running epoch 1, step 1594, batch 546
Sampled inputs[:2]: tensor([[    0,    12, 17340,  ...,   408,  1550,  2415],
        [    0,   271,   266,  ...,   365,  2463,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7902e-04, -1.3551e-04,  1.5145e-04,  ...,  2.5626e-04,
          2.2956e-04,  3.1522e-04],
        [-3.7476e-06, -2.7344e-06,  2.6524e-06,  ..., -3.1888e-06,
         -2.6822e-06, -2.8275e-06],
        [-1.1712e-05, -9.0301e-06,  9.0152e-06,  ..., -9.9242e-06,
         -8.4937e-06, -8.6725e-06],
        [-1.1235e-05, -8.2254e-06,  8.4490e-06,  ..., -9.5963e-06,
         -8.2254e-06, -8.7917e-06],
        [-8.9705e-06, -7.0482e-06,  6.6906e-06,  ..., -7.7337e-06,
         -6.8545e-06, -6.2063e-06]], device='cuda:0')
Loss: 0.973491907119751


Running epoch 1, step 1595, batch 547
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0367e-04, -4.4312e-05,  1.6733e-04,  ...,  3.2432e-04,
          6.7585e-05,  2.3822e-04],
        [-4.9770e-06, -3.6433e-06,  3.5539e-06,  ..., -4.2468e-06,
         -3.5539e-06, -3.7141e-06],
        [-1.5557e-05, -1.2070e-05,  1.2115e-05,  ..., -1.3262e-05,
         -1.1325e-05, -1.1429e-05],
        [-1.4916e-05, -1.0967e-05,  1.1340e-05,  ..., -1.2800e-05,
         -1.0937e-05, -1.1593e-05],
        [-1.1861e-05, -9.4324e-06,  8.9556e-06,  ..., -1.0282e-05,
         -9.1046e-06, -8.1137e-06]], device='cuda:0')
Loss: 0.9610393047332764


Running epoch 1, step 1596, batch 548
Sampled inputs[:2]: tensor([[   0, 6957,  271,  ..., 9094,  266, 4320],
        [   0,  259, 5918,  ...,  508, 3433, 1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8882e-04,  6.3247e-05,  3.1783e-04,  ...,  1.8247e-04,
          2.8665e-04,  2.4362e-04],
        [-6.2287e-06, -4.4219e-06,  4.3213e-06,  ..., -5.3272e-06,
         -4.4368e-06, -4.7497e-06],
        [-1.9521e-05, -1.4648e-05,  1.4767e-05,  ..., -1.6615e-05,
         -1.4096e-05, -1.4618e-05],
        [-1.8641e-05, -1.3188e-05,  1.3694e-05,  ..., -1.6019e-05,
         -1.3575e-05, -1.4722e-05],
        [-1.5110e-05, -1.1578e-05,  1.1042e-05,  ..., -1.3083e-05,
         -1.1504e-05, -1.0557e-05]], device='cuda:0')
Loss: 0.9611141681671143


Running epoch 1, step 1597, batch 549
Sampled inputs[:2]: tensor([[   0, 1485,  271,  ..., 6359, 1799, 5442],
        [   0, 1921,  843,  ..., 9420,  352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1684e-04,  3.7209e-05,  2.2698e-04,  ...,  6.0101e-05,
          2.8665e-04,  1.3966e-04],
        [-7.4506e-06, -5.3123e-06,  5.3570e-06,  ..., -6.3330e-06,
         -5.2154e-06, -5.5842e-06],
        [-2.3395e-05, -1.7613e-05,  1.8254e-05,  ..., -1.9819e-05,
         -1.6615e-05, -1.7285e-05],
        [-2.2247e-05, -1.5810e-05,  1.6928e-05,  ..., -1.8984e-05,
         -1.5900e-05, -1.7330e-05],
        [-1.7866e-05, -1.3798e-05,  1.3471e-05,  ..., -1.5423e-05,
         -1.3441e-05, -1.2308e-05]], device='cuda:0')
Loss: 0.9485411047935486


Running epoch 1, step 1598, batch 550
Sampled inputs[:2]: tensor([[    0,  6660, 13165,  ...,   380,   333,   199],
        [    0,   600,   287,  ...,  1933,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0998e-04,  1.2133e-04,  4.4119e-04,  ..., -5.4146e-05,
          5.5886e-04,  1.7041e-04],
        [-8.6874e-06, -6.2510e-06,  6.1579e-06,  ..., -7.4133e-06,
         -6.1989e-06, -6.5081e-06],
        [-2.7359e-05, -2.0772e-05,  2.1026e-05,  ..., -2.3276e-05,
         -1.9789e-05, -2.0236e-05],
        [-2.6092e-05, -1.8701e-05,  1.9506e-05,  ..., -2.2382e-05,
         -1.9029e-05, -2.0355e-05],
        [-2.0996e-05, -1.6347e-05,  1.5602e-05,  ..., -1.8209e-05,
         -1.6063e-05, -1.4484e-05]], device='cuda:0')
Loss: 0.9870572090148926


Running epoch 1, step 1599, batch 551
Sampled inputs[:2]: tensor([[    0,   510,    13,  ...,  3454,   513,    13],
        [    0,   259,  2416,  ..., 14474,    12,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6611e-04, -2.4435e-05,  5.6353e-04,  ..., -1.3149e-04,
          6.1050e-04,  7.9486e-05],
        [-9.9316e-06, -7.0408e-06,  6.9290e-06,  ..., -8.4862e-06,
         -7.0930e-06, -7.4916e-06],
        [-3.1322e-05, -2.3425e-05,  2.3708e-05,  ..., -2.6643e-05,
         -2.2590e-05, -2.3291e-05],
        [-2.9936e-05, -2.1085e-05,  2.2009e-05,  ..., -2.5690e-05,
         -2.1785e-05, -2.3469e-05],
        [-2.4185e-05, -1.8522e-05,  1.7628e-05,  ..., -2.0966e-05,
         -1.8463e-05, -1.6764e-05]], device='cuda:0')
Loss: 0.9410408735275269
Graident accumulation at epoch 1, step 1599, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.3083e-05,  1.4498e-05,  8.4500e-05,  ..., -8.9934e-05,
          8.9973e-05,  2.3438e-05],
        [-8.2520e-06, -5.8390e-06,  5.8804e-06,  ..., -7.2764e-06,
         -5.3308e-06, -5.8286e-06],
        [ 7.5919e-06,  1.5482e-05, -1.2844e-05,  ...,  7.1622e-06,
          1.6973e-05, -1.7556e-06],
        [-1.0114e-05,  2.2735e-07,  3.1877e-06,  ...,  1.5711e-07,
          4.2752e-06, -8.9645e-06],
        [-2.5099e-05, -1.8714e-05,  1.8037e-05,  ..., -2.1342e-05,
         -1.8187e-05, -1.6730e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3732e-08, 6.1392e-08, 4.6516e-08,  ..., 4.0263e-08, 1.2775e-07,
         5.3164e-08],
        [7.9554e-11, 5.8839e-11, 1.9536e-11,  ..., 5.6117e-11, 3.5204e-11,
         3.0194e-11],
        [4.1373e-09, 2.8993e-09, 1.3979e-09,  ..., 3.2137e-09, 1.4028e-09,
         1.0587e-09],
        [1.1389e-09, 1.1534e-09, 3.5230e-10,  ..., 9.9787e-10, 6.4946e-10,
         4.4410e-10],
        [3.6227e-10, 2.0641e-10, 7.9371e-11,  ..., 2.7160e-10, 7.9586e-11,
         1.0470e-10]], device='cuda:0')
optimizer state dict: 200.0
lr: [2.796038617665642e-06, 2.796038617665642e-06]
scheduler_last_epoch: 200


Running epoch 1, step 1600, batch 552
Sampled inputs[:2]: tensor([[    0,    20,     9,  ...,    12,  2212, 24950],
        [    0,   360,  2063,  ..., 49105,   221,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0938e-05,  8.6537e-05, -1.1981e-04,  ...,  7.9039e-06,
         -5.0783e-05, -1.3081e-04],
        [-1.2442e-06, -8.3074e-07,  8.6799e-07,  ..., -1.0207e-06,
         -8.1211e-07, -9.9838e-07],
        [-3.9339e-06, -2.6971e-06,  3.0249e-06,  ..., -3.1441e-06,
         -2.5481e-06, -2.9802e-06],
        [-3.6359e-06, -2.3842e-06,  2.7120e-06,  ..., -2.9951e-06,
         -2.4289e-06, -2.9653e-06],
        [-2.9057e-06, -2.0266e-06,  2.1905e-06,  ..., -2.3693e-06,
         -1.9968e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.933651328086853


Running epoch 1, step 1601, batch 553
Sampled inputs[:2]: tensor([[   0,   15, 4291,  ..., 1685,  278, 2101],
        [   0,  259, 3022,  ...,  437, 5100, 1782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6440e-05,  1.1087e-04, -3.5690e-04,  ..., -1.9953e-05,
         -4.4257e-05, -2.4073e-04],
        [-2.4736e-06, -1.7770e-06,  1.7807e-06,  ..., -2.0787e-06,
         -1.6838e-06, -1.8924e-06],
        [-7.7784e-06, -5.7817e-06,  6.0797e-06,  ..., -6.4224e-06,
         -5.2750e-06, -5.7071e-06],
        [-7.2718e-06, -5.1707e-06,  5.5581e-06,  ..., -6.1244e-06,
         -5.0515e-06, -5.7220e-06],
        [-5.8413e-06, -4.4703e-06,  4.4703e-06,  ..., -4.9323e-06,
         -4.2021e-06, -3.9637e-06]], device='cuda:0')
Loss: 0.9664427042007446


Running epoch 1, step 1602, batch 554
Sampled inputs[:2]: tensor([[    0,   927,   259,  ...,   328,  9430,  2330],
        [    0,    13, 36961,  ...,  6671, 13711,  4568]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5158e-04,  1.3542e-04, -3.3783e-04,  ..., -1.1341e-04,
         -2.0063e-04, -4.4444e-04],
        [-3.6061e-06, -2.6040e-06,  2.5742e-06,  ..., -3.1441e-06,
         -2.5667e-06, -2.7679e-06],
        [-1.1280e-05, -8.4490e-06,  8.7619e-06,  ..., -9.6858e-06,
         -8.0019e-06, -8.3297e-06],
        [-1.0803e-05, -7.7039e-06,  8.1807e-06,  ..., -9.4473e-06,
         -7.8380e-06, -8.5682e-06],
        [-8.5980e-06, -6.6608e-06,  6.5267e-06,  ..., -7.5400e-06,
         -6.4820e-06, -5.8860e-06]], device='cuda:0')
Loss: 0.9555992484092712


Running epoch 1, step 1603, batch 555
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,   266,   283,  ...,   271, 48829,   580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7393e-05, -1.8415e-04, -4.9888e-04,  ..., -1.8251e-04,
          1.9125e-04, -2.2240e-04],
        [-4.8429e-06, -3.4869e-06,  3.2298e-06,  ..., -4.2915e-06,
         -3.5428e-06, -3.8408e-06],
        [-1.5095e-05, -1.1310e-05,  1.1161e-05,  ..., -1.3068e-05,
         -1.0982e-05, -1.1370e-05],
        [-1.4409e-05, -1.0252e-05,  1.0237e-05,  ..., -1.2770e-05,
         -1.0744e-05, -1.1683e-05],
        [-1.1757e-05, -8.9705e-06,  8.4490e-06,  ..., -1.0356e-05,
         -8.9854e-06, -8.2105e-06]], device='cuda:0')
Loss: 0.9880138635635376


Running epoch 1, step 1604, batch 556
Sampled inputs[:2]: tensor([[    0,  1456, 32380,  ...,    12,  1172, 12557],
        [    0,   677,  6499,  ...,  2738,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4012e-05, -2.4593e-05, -6.3822e-04,  ..., -1.1735e-04,
          5.6529e-05, -5.3082e-04],
        [-6.1244e-06, -4.3586e-06,  4.1984e-06,  ..., -5.3421e-06,
         -4.4405e-06, -4.7535e-06],
        [-1.8999e-05, -1.4141e-05,  1.4350e-05,  ..., -1.6302e-05,
         -1.3784e-05, -1.4171e-05],
        [-1.8284e-05, -1.2860e-05,  1.3337e-05,  ..., -1.5959e-05,
         -1.3500e-05, -1.4588e-05],
        [-1.4707e-05, -1.1221e-05,  1.0788e-05,  ..., -1.2845e-05,
         -1.1250e-05, -1.0163e-05]], device='cuda:0')
Loss: 0.9652032852172852


Running epoch 1, step 1605, batch 557
Sampled inputs[:2]: tensor([[   0,  221,  380,  ..., 1590,  997, 2239],
        [   0,  894,   16,  ...,  892,  300,  722]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4068e-05, -1.6221e-04, -6.4026e-04,  ..., -5.1454e-05,
          2.7036e-04, -5.7655e-04],
        [-7.3314e-06, -5.2117e-06,  5.0180e-06,  ..., -6.4224e-06,
         -5.3234e-06, -5.7146e-06],
        [-2.2680e-05, -1.6809e-05,  1.7136e-05,  ..., -1.9506e-05,
         -1.6466e-05, -1.6943e-05],
        [-2.1860e-05, -1.5333e-05,  1.5914e-05,  ..., -1.9148e-05,
         -1.6168e-05, -1.7494e-05],
        [-1.7628e-05, -1.3381e-05,  1.2919e-05,  ..., -1.5438e-05,
         -1.3500e-05, -1.2204e-05]], device='cuda:0')
Loss: 0.9179471731185913


Running epoch 1, step 1606, batch 558
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,    40, 25575,   292],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4379e-06, -2.9891e-04, -7.2364e-04,  ...,  6.1628e-05,
          2.3740e-04, -6.1834e-04],
        [-8.4713e-06, -6.0946e-06,  5.9344e-06,  ..., -7.3984e-06,
         -6.1244e-06, -6.5677e-06],
        [-2.6375e-05, -1.9774e-05,  2.0325e-05,  ..., -2.2650e-05,
         -1.9088e-05, -1.9640e-05],
        [-2.5302e-05, -1.7956e-05,  1.8850e-05,  ..., -2.2084e-05,
         -1.8612e-05, -2.0146e-05],
        [-2.0310e-05, -1.5616e-05,  1.5169e-05,  ..., -1.7777e-05,
         -1.5527e-05, -1.4015e-05]], device='cuda:0')
Loss: 0.9570911526679993


Running epoch 1, step 1607, batch 559
Sampled inputs[:2]: tensor([[    0,  1529,   354,  ...,   709,   271,   266],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0480e-05, -3.4000e-04, -7.7354e-04,  ..., -3.3041e-05,
          2.1815e-04, -7.1342e-04],
        [-9.7528e-06, -7.0147e-06,  6.8173e-06,  ..., -8.4937e-06,
         -7.0781e-06, -7.5288e-06],
        [-3.0458e-05, -2.2858e-05,  2.3365e-05,  ..., -2.6122e-05,
         -2.2158e-05, -2.2665e-05],
        [-2.9057e-05, -2.0638e-05,  2.1547e-05,  ..., -2.5317e-05,
         -2.1443e-05, -2.3067e-05],
        [-2.3469e-05, -1.8075e-05,  1.7449e-05,  ..., -2.0534e-05,
         -1.8045e-05, -1.6190e-05]], device='cuda:0')
Loss: 0.95484459400177
Graident accumulation at epoch 1, step 1607, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.7823e-05, -2.0952e-05, -1.3040e-06,  ..., -8.4245e-05,
          1.0279e-04, -5.0248e-05],
        [-8.4021e-06, -5.9566e-06,  5.9740e-06,  ..., -7.3981e-06,
         -5.5055e-06, -5.9986e-06],
        [ 3.7869e-06,  1.1648e-05, -9.2228e-06,  ...,  3.8338e-06,
          1.3060e-05, -3.8465e-06],
        [-1.2008e-05, -1.8592e-06,  5.0237e-06,  ..., -2.3903e-06,
          1.7034e-06, -1.0375e-05],
        [-2.4936e-05, -1.8650e-05,  1.7978e-05,  ..., -2.1261e-05,
         -1.8173e-05, -1.6676e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3664e-08, 6.1446e-08, 4.7068e-08,  ..., 4.0224e-08, 1.2767e-07,
         5.3620e-08],
        [7.9570e-11, 5.8829e-11, 1.9563e-11,  ..., 5.6133e-11, 3.5219e-11,
         3.0220e-11],
        [4.1341e-09, 2.8969e-09, 1.3970e-09,  ..., 3.2112e-09, 1.4019e-09,
         1.0581e-09],
        [1.1386e-09, 1.1527e-09, 3.5242e-10,  ..., 9.9751e-10, 6.4927e-10,
         4.4419e-10],
        [3.6246e-10, 2.0653e-10, 7.9596e-11,  ..., 2.7175e-10, 7.9832e-11,
         1.0486e-10]], device='cuda:0')
optimizer state dict: 201.0
lr: [2.7108553255335225e-06, 2.7108553255335225e-06]
scheduler_last_epoch: 201


Running epoch 1, step 1608, batch 560
Sampled inputs[:2]: tensor([[   0, 1064, 1042,  ...,   12,  259, 4754],
        [   0, 1635,  266,  ...,  437, 3302,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5518e-05, -5.2318e-05, -1.2886e-04,  ..., -2.8256e-05,
         -2.8204e-05,  5.3191e-05],
        [-1.2740e-06, -9.7603e-07,  9.8348e-07,  ..., -9.9838e-07,
         -8.6799e-07, -8.3074e-07],
        [-4.1425e-06, -3.3081e-06,  3.3677e-06,  ..., -3.2783e-06,
         -2.8908e-06, -2.7418e-06],
        [-3.9041e-06, -2.9802e-06,  3.1590e-06,  ..., -3.0547e-06,
         -2.6822e-06, -2.6524e-06],
        [-2.9802e-06, -2.4736e-06,  2.3842e-06,  ..., -2.4140e-06,
         -2.2352e-06, -1.8477e-06]], device='cuda:0')
Loss: 0.9957271814346313


Running epoch 1, step 1609, batch 561
Sampled inputs[:2]: tensor([[   0,  275, 2351,  ...,   14, 4520,   12],
        [   0, 3544,  417,  ...,  380,  381, 3794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6411e-06,  9.1845e-05, -1.9941e-04,  ..., -2.1502e-05,
         -3.3089e-04, -3.0543e-04],
        [-2.4885e-06, -1.7919e-06,  1.8477e-06,  ..., -2.0415e-06,
         -1.7174e-06, -1.7434e-06],
        [-7.8380e-06, -5.8860e-06,  6.2436e-06,  ..., -6.3628e-06,
         -5.4389e-06, -5.3793e-06],
        [-7.4655e-06, -5.3048e-06,  5.8413e-06,  ..., -6.0946e-06,
         -5.2005e-06, -5.3793e-06],
        [-5.8711e-06, -4.5449e-06,  4.5598e-06,  ..., -4.8578e-06,
         -4.3362e-06, -3.7253e-06]], device='cuda:0')
Loss: 0.9019718170166016


Running epoch 1, step 1610, batch 562
Sampled inputs[:2]: tensor([[    0,   266,  3536,  ...,   266,  1883,   266],
        [    0,   266, 30368,  ...,   950,   266,  1868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2775e-05,  3.8242e-05, -1.1166e-04,  ..., -1.6034e-04,
         -2.0347e-04, -3.1420e-04],
        [-3.7402e-06, -2.6226e-06,  2.7493e-06,  ..., -3.0845e-06,
         -2.5481e-06, -2.7120e-06],
        [-1.1772e-05, -8.6576e-06,  9.3132e-06,  ..., -9.6411e-06,
         -8.0764e-06, -8.4192e-06],
        [-1.1101e-05, -7.7039e-06,  8.5980e-06,  ..., -9.1493e-06,
         -7.6592e-06, -8.2850e-06],
        [-8.8215e-06, -6.6906e-06,  6.7651e-06,  ..., -7.3761e-06,
         -6.4373e-06, -5.8264e-06]], device='cuda:0')
Loss: 0.9469638466835022


Running epoch 1, step 1611, batch 563
Sampled inputs[:2]: tensor([[    0, 12305,  1179,  ...,  6321,   600,   271],
        [    0,   199,  3289,  ...,  2269,  6476,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8369e-05,  1.4040e-04, -1.9812e-04,  ..., -1.1892e-04,
         -1.5930e-04, -2.9269e-04],
        [-5.1931e-06, -3.4198e-06,  3.5577e-06,  ..., -4.2468e-06,
         -3.4794e-06, -3.8221e-06],
        [-1.6183e-05, -1.1340e-05,  1.2040e-05,  ..., -1.3202e-05,
         -1.1027e-05, -1.1817e-05],
        [-1.5184e-05, -9.9838e-06,  1.0967e-05,  ..., -1.2487e-05,
         -1.0386e-05, -1.1489e-05],
        [-1.2368e-05, -8.8215e-06,  8.8513e-06,  ..., -1.0297e-05,
         -8.9109e-06, -8.4192e-06]], device='cuda:0')
Loss: 0.9445803165435791


Running epoch 1, step 1612, batch 564
Sampled inputs[:2]: tensor([[    0,  1526,   422,  ..., 22454,   409, 31482],
        [    0,   259, 19567,  ...,   266,  3899,  2123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0675e-04,  1.2855e-04, -3.3957e-04,  ..., -1.1330e-04,
         -6.4939e-05, -2.5829e-04],
        [-6.4895e-06, -4.3437e-06,  4.5635e-06,  ..., -5.3495e-06,
         -4.3660e-06, -4.7535e-06],
        [-2.0325e-05, -1.4484e-05,  1.5453e-05,  ..., -1.6779e-05,
         -1.3977e-05, -1.4827e-05],
        [-1.9059e-05, -1.2755e-05,  1.4126e-05,  ..., -1.5795e-05,
         -1.3098e-05, -1.4380e-05],
        [-1.5453e-05, -1.1250e-05,  1.1325e-05,  ..., -1.3009e-05,
         -1.1250e-05, -1.0505e-05]], device='cuda:0')
Loss: 1.009812355041504


Running epoch 1, step 1613, batch 565
Sampled inputs[:2]: tensor([[    0,    61, 22315,  ..., 36901,    17,   360],
        [    0,    13,  4363,  ...,   271,  2462,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0421e-05,  2.0806e-04, -1.7073e-04,  ..., -2.5472e-04,
          3.2404e-06, -1.4721e-04],
        [-7.7412e-06, -5.2974e-06,  5.4054e-06,  ..., -6.4746e-06,
         -5.3495e-06, -5.6587e-06],
        [ 3.6499e-05,  5.0574e-05, -3.0912e-05,  ...,  4.6356e-05,
          7.1377e-05,  2.1193e-05],
        [-2.2814e-05, -1.5557e-05,  1.6779e-05,  ..., -1.9178e-05,
         -1.6078e-05, -1.7226e-05],
        [-1.8492e-05, -1.3754e-05,  1.3486e-05,  ..., -1.5780e-05,
         -1.3769e-05, -1.2547e-05]], device='cuda:0')
Loss: 0.980891764163971


Running epoch 1, step 1614, batch 566
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0,   300,  5201,  ...,  1997,  7423,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9480e-05,  1.3689e-04, -2.4720e-04,  ..., -2.2177e-04,
         -4.6851e-05, -1.2205e-04],
        [-8.9407e-06, -6.1244e-06,  6.4187e-06,  ..., -7.4431e-06,
         -6.0946e-06, -6.5006e-06],
        [ 3.2758e-05,  4.7832e-05, -2.7559e-05,  ...,  4.3301e-05,
          6.8992e-05,  1.8585e-05],
        [-2.6464e-05, -1.8075e-05,  2.0042e-05,  ..., -2.2143e-05,
         -1.8403e-05, -1.9908e-05],
        [-2.1145e-05, -1.5795e-05,  1.5810e-05,  ..., -1.7986e-05,
         -1.5587e-05, -1.4246e-05]], device='cuda:0')
Loss: 0.9705219864845276


Running epoch 1, step 1615, batch 567
Sampled inputs[:2]: tensor([[   0, 1356,   18,  ...,   31,  333,  199],
        [   0, 1823,   12,  ..., 1874,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5733e-05,  1.8492e-04, -2.9905e-04,  ..., -2.9095e-04,
          5.7684e-05, -1.0631e-04],
        [-1.0222e-05, -7.1302e-06,  7.3723e-06,  ..., -8.5309e-06,
         -6.9961e-06, -7.3388e-06],
        [ 2.8675e-05,  4.4494e-05, -2.4341e-05,  ...,  3.9829e-05,
          6.6087e-05,  1.5918e-05],
        [ 9.8242e-05,  1.7761e-04, -9.3445e-05,  ...,  5.6305e-05,
          1.0389e-04,  5.7925e-05],
        [-2.4229e-05, -1.8433e-05,  1.8209e-05,  ..., -2.0668e-05,
         -1.7926e-05, -1.6138e-05]], device='cuda:0')
Loss: 0.9740169644355774
Graident accumulation at epoch 1, step 1615, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.8614e-05, -3.6466e-07, -3.1078e-05,  ..., -1.0491e-04,
          9.8279e-05, -5.5854e-05],
        [-8.5841e-06, -6.0739e-06,  6.1139e-06,  ..., -7.5114e-06,
         -5.6546e-06, -6.1326e-06],
        [ 6.2758e-06,  1.4932e-05, -1.0735e-05,  ...,  7.4333e-06,
          1.8362e-05, -1.8701e-06],
        [-9.8330e-07,  1.6088e-05, -4.8232e-06,  ...,  3.4792e-06,
          1.1922e-05, -3.5448e-06],
        [-2.4866e-05, -1.8628e-05,  1.8001e-05,  ..., -2.1202e-05,
         -1.8148e-05, -1.6622e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3593e-08, 6.1419e-08, 4.7110e-08,  ..., 4.0269e-08, 1.2755e-07,
         5.3578e-08],
        [7.9595e-11, 5.8821e-11, 1.9598e-11,  ..., 5.6150e-11, 3.5233e-11,
         3.0244e-11],
        [4.1308e-09, 2.8960e-09, 1.3962e-09,  ..., 3.2096e-09, 1.4048e-09,
         1.0573e-09],
        [1.1471e-09, 1.1830e-09, 3.6080e-10,  ..., 9.9968e-10, 6.5941e-10,
         4.4710e-10],
        [3.6268e-10, 2.0667e-10, 7.9848e-11,  ..., 2.7191e-10, 8.0074e-11,
         1.0502e-10]], device='cuda:0')
optimizer state dict: 202.0
lr: [2.626785878335505e-06, 2.626785878335505e-06]
scheduler_last_epoch: 202


Running epoch 1, step 1616, batch 568
Sampled inputs[:2]: tensor([[   0,  642,  287,  ...,  800,   12, 3338],
        [   0,   14, 2787,  ..., 9674, 2491,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5510e-05, -6.1955e-05, -9.8945e-05,  ..., -6.0952e-06,
         -6.5425e-06, -1.7309e-04],
        [-1.2368e-06, -9.1642e-07,  1.0207e-06,  ..., -9.9838e-07,
         -8.0466e-07, -8.8289e-07],
        [-3.9637e-06, -3.0547e-06,  3.4273e-06,  ..., -3.2187e-06,
         -2.6375e-06, -2.8312e-06],
        [-3.7700e-06, -2.7716e-06,  3.2485e-06,  ..., -3.0547e-06,
         -2.5034e-06, -2.7865e-06],
        [-2.8759e-06, -2.3246e-06,  2.4438e-06,  ..., -2.3991e-06,
         -2.0713e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.954250156879425


Running epoch 1, step 1617, batch 569
Sampled inputs[:2]: tensor([[    0, 49018,   292,  ...,  8774,   642,   365],
        [    0,    13, 38195,  ...,   950,   298,   257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5329e-04, -1.7449e-04, -1.1986e-05,  ..., -2.2226e-04,
          2.4206e-04, -9.2045e-05],
        [-2.4289e-06, -1.8701e-06,  1.8217e-06,  ..., -2.1011e-06,
         -1.7881e-06, -1.7919e-06],
        [-7.6741e-06, -6.1840e-06,  6.1691e-06,  ..., -6.6310e-06,
         -5.7220e-06, -5.6624e-06],
        [-7.3016e-06, -5.6028e-06,  5.7667e-06,  ..., -6.3479e-06,
         -5.4836e-06, -5.6475e-06],
        [-5.8711e-06, -4.9174e-06,  4.5896e-06,  ..., -5.2005e-06,
         -4.6939e-06, -4.0084e-06]], device='cuda:0')
Loss: 0.9672893285751343


Running epoch 1, step 1618, batch 570
Sampled inputs[:2]: tensor([[    0,  6275,    12,  ...,  2027,  2887,   287],
        [    0, 41010,  6737,  ...,   963,   409,   382]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0051e-05, -2.7312e-04, -7.1365e-05,  ..., -2.9823e-04,
          4.2747e-04,  3.6459e-05],
        [-3.6582e-06, -2.7940e-06,  2.8200e-06,  ..., -3.0994e-06,
         -2.5779e-06, -2.6524e-06],
        [-1.1668e-05, -9.3281e-06,  9.5963e-06,  ..., -9.8795e-06,
         -8.2999e-06, -8.4639e-06],
        [-1.1012e-05, -8.4192e-06,  8.9556e-06,  ..., -9.3728e-06,
         -7.8976e-06, -8.3894e-06],
        [-8.7619e-06, -7.2867e-06,  7.0035e-06,  ..., -7.6145e-06,
         -6.7204e-06, -5.8934e-06]], device='cuda:0')
Loss: 0.9850397706031799


Running epoch 1, step 1619, batch 571
Sampled inputs[:2]: tensor([[   0, 2422,  300,  ...,  630,  729, 3400],
        [   0,  382,   17,  ..., 8733,   13, 9306]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1913e-04, -2.7888e-04, -9.4062e-05,  ..., -2.9140e-04,
          4.4361e-04, -2.0301e-05],
        [-4.9174e-06, -3.7178e-06,  3.6769e-06,  ..., -4.1798e-06,
         -3.5055e-06, -3.5912e-06],
        [-1.5602e-05, -1.2428e-05,  1.2532e-05,  ..., -1.3262e-05,
         -1.1295e-05, -1.1355e-05],
        [-1.4797e-05, -1.1221e-05,  1.1697e-05,  ..., -1.2666e-05,
         -1.0818e-05, -1.1355e-05],
        [-1.1653e-05, -9.6411e-06,  9.1195e-06,  ..., -1.0163e-05,
         -9.0450e-06, -7.8753e-06]], device='cuda:0')
Loss: 0.9480896592140198


Running epoch 1, step 1620, batch 572
Sampled inputs[:2]: tensor([[  0,  12, 358,  ..., 352, 266, 319],
        [  0, 328, 266,  ..., 271, 706,  13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8063e-04, -3.5595e-04, -1.6055e-04,  ..., -3.5072e-04,
          7.2351e-04,  1.7801e-04],
        [-6.1914e-06, -4.5747e-06,  4.5560e-06,  ..., -5.2825e-06,
         -4.3921e-06, -4.6045e-06],
        [-1.9684e-05, -1.5378e-05,  1.5631e-05,  ..., -1.6779e-05,
         -1.4186e-05, -1.4544e-05],
        [-1.8671e-05, -1.3828e-05,  1.4529e-05,  ..., -1.6049e-05,
         -1.3590e-05, -1.4484e-05],
        [-1.4648e-05, -1.1861e-05,  1.1325e-05,  ..., -1.2800e-05,
         -1.1295e-05, -1.0051e-05]], device='cuda:0')
Loss: 0.9455451965332031


Running epoch 1, step 1621, batch 573
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0, 2645,   12,  ...,    5, 1239, 7200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4659e-04, -3.1359e-04, -2.2416e-04,  ..., -3.8315e-04,
          8.4222e-04,  1.4902e-04],
        [-7.4506e-06, -5.6028e-06,  5.4687e-06,  ..., -6.3479e-06,
         -5.3421e-06, -5.4352e-06],
        [-2.3678e-05, -1.8865e-05,  1.8761e-05,  ..., -2.0221e-05,
         -1.7315e-05, -1.7211e-05],
        [-2.2411e-05, -1.6972e-05,  1.7419e-05,  ..., -1.9267e-05,
         -1.6525e-05, -1.7092e-05],
        [-1.7568e-05, -1.4529e-05,  1.3560e-05,  ..., -1.5378e-05,
         -1.3724e-05, -1.1861e-05]], device='cuda:0')
Loss: 0.980873167514801


Running epoch 1, step 1622, batch 574
Sampled inputs[:2]: tensor([[    0,    13, 23070,  ...,   266,   319,    13],
        [    0,   292,  2860,  ...,   266,  7000,  7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1330e-04, -3.1551e-04, -3.8856e-04,  ..., -4.5914e-04,
          1.1061e-03,  1.9733e-04],
        [-8.6501e-06, -6.5044e-06,  6.3814e-06,  ..., -7.4133e-06,
         -6.2287e-06, -6.3591e-06],
        [-2.7418e-05, -2.1756e-05,  2.1830e-05,  ..., -2.3484e-05,
         -2.0087e-05, -2.0012e-05],
        [-2.6107e-05, -1.9684e-05,  2.0385e-05,  ..., -2.2545e-05,
         -1.9297e-05, -2.0027e-05],
        [-2.0400e-05, -1.6794e-05,  1.5840e-05,  ..., -1.7911e-05,
         -1.5959e-05, -1.3828e-05]], device='cuda:0')
Loss: 0.9715613126754761


Running epoch 1, step 1623, batch 575
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,   278,  3358,  ...,    12,   287,  9612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1330e-04, -2.8925e-04, -3.6407e-04,  ..., -4.1177e-04,
          8.3891e-04,  8.1878e-05],
        [-9.9614e-06, -7.4171e-06,  7.2382e-06,  ..., -8.4713e-06,
         -7.1488e-06, -7.3053e-06],
        [-3.1710e-05, -2.4930e-05,  2.4885e-05,  ..., -2.6926e-05,
         -2.3127e-05, -2.3127e-05],
        [-3.0041e-05, -2.2411e-05,  2.3082e-05,  ..., -2.5734e-05,
         -2.2128e-05, -2.2992e-05],
        [-2.3544e-05, -1.9222e-05,  1.8016e-05,  ..., -2.0519e-05,
         -1.8343e-05, -1.5944e-05]], device='cuda:0')
Loss: 0.9566576480865479
Graident accumulation at epoch 1, step 1623, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.4229e-06, -2.9253e-05, -6.4378e-05,  ..., -1.3560e-04,
          1.7234e-04, -4.2080e-05],
        [-8.7218e-06, -6.2082e-06,  6.2263e-06,  ..., -7.6074e-06,
         -5.8040e-06, -6.2499e-06],
        [ 2.4772e-06,  1.0946e-05, -7.1726e-06,  ...,  3.9973e-06,
          1.4213e-05, -3.9957e-06],
        [-3.8890e-06,  1.2238e-05, -2.0327e-06,  ...,  5.5785e-07,
          8.5167e-06, -5.4896e-06],
        [-2.4733e-05, -1.8687e-05,  1.8002e-05,  ..., -2.1134e-05,
         -1.8168e-05, -1.6555e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3617e-08, 6.1441e-08, 4.7196e-08,  ..., 4.0398e-08, 1.2812e-07,
         5.3531e-08],
        [7.9614e-11, 5.8818e-11, 1.9631e-11,  ..., 5.6166e-11, 3.5249e-11,
         3.0267e-11],
        [4.1277e-09, 2.8937e-09, 1.3954e-09,  ..., 3.2071e-09, 1.4039e-09,
         1.0568e-09],
        [1.1469e-09, 1.1824e-09, 3.6097e-10,  ..., 9.9935e-10, 6.5924e-10,
         4.4719e-10],
        [3.6287e-10, 2.0683e-10, 8.0093e-11,  ..., 2.7206e-10, 8.0330e-11,
         1.0517e-10]], device='cuda:0')
optimizer state dict: 203.0
lr: [2.5438431226169712e-06, 2.5438431226169712e-06]
scheduler_last_epoch: 203


Running epoch 1, step 1624, batch 576
Sampled inputs[:2]: tensor([[   0,  767, 1811,  ..., 1441, 1428,  278],
        [   0,  747, 7890,  ...,  706, 8667,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1883e-05,  1.3927e-04,  5.7612e-05,  ...,  1.2562e-04,
         -1.3536e-05,  1.8305e-04],
        [-1.1921e-06, -7.9721e-07,  8.6427e-07,  ..., -1.0580e-06,
         -8.6799e-07, -9.7603e-07],
        [-3.6955e-06, -2.6375e-06,  2.9355e-06,  ..., -3.2485e-06,
         -2.7269e-06, -2.9802e-06],
        [-3.6359e-06, -2.3693e-06,  2.7418e-06,  ..., -3.2783e-06,
         -2.7716e-06, -3.1292e-06],
        [-2.8312e-06, -2.0713e-06,  2.1607e-06,  ..., -2.5332e-06,
         -2.1905e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9159594178199768


Running epoch 1, step 1625, batch 577
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,    13, 23904,  ...,   560,  8840,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1319e-04,  2.3683e-04, -1.2270e-04,  ...,  1.3108e-04,
         -1.0027e-04,  1.9280e-04],
        [-2.3842e-06, -1.7136e-06,  1.7695e-06,  ..., -2.0862e-06,
         -1.7881e-06, -1.8999e-06],
        [-7.5996e-06, -5.7518e-06,  6.0946e-06,  ..., -6.6012e-06,
         -5.7220e-06, -5.9605e-06],
        [-7.3016e-06, -5.1409e-06,  5.6773e-06,  ..., -6.4373e-06,
         -5.6326e-06, -6.0797e-06],
        [-5.7369e-06, -4.4554e-06,  4.4554e-06,  ..., -5.0664e-06,
         -4.5300e-06, -4.1574e-06]], device='cuda:0')
Loss: 0.9522523880004883


Running epoch 1, step 1626, batch 578
Sampled inputs[:2]: tensor([[   0,  277,  279,  ...,   12,  287,  259],
        [   0, 3440, 5745,  ...,  360, 4998,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0924e-04,  1.1932e-04, -2.9400e-04,  ...,  9.7816e-05,
         -4.5460e-05,  1.3445e-04],
        [-3.5986e-06, -2.6636e-06,  2.7679e-06,  ..., -3.0473e-06,
         -2.5779e-06, -2.7381e-06],
        [-1.1533e-05, -9.0152e-06,  9.5218e-06,  ..., -9.7603e-06,
         -8.3297e-06, -8.7172e-06],
        [-1.0952e-05, -7.9870e-06,  8.8364e-06,  ..., -9.3281e-06,
         -8.0168e-06, -8.7470e-06],
        [-8.4788e-06, -6.8396e-06,  6.7800e-06,  ..., -7.3314e-06,
         -6.4969e-06, -5.9307e-06]], device='cuda:0')
Loss: 0.9366096258163452


Running epoch 1, step 1627, batch 579
Sampled inputs[:2]: tensor([[    0,   342,   516,  ...,    12,   729,  3701],
        [    0, 40624,   266,  ..., 12236,   292,    41]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1924e-05,  6.2439e-06, -3.5193e-04,  ...,  2.0765e-05,
          1.0677e-04,  1.4461e-04],
        [-4.8131e-06, -3.6918e-06,  3.6396e-06,  ..., -4.1202e-06,
         -3.5465e-06, -3.5875e-06],
        [-1.5378e-05, -1.2428e-05,  1.2547e-05,  ..., -1.3173e-05,
         -1.1429e-05, -1.1414e-05],
        [-1.4529e-05, -1.1027e-05,  1.1563e-05,  ..., -1.2517e-05,
         -1.0937e-05, -1.1399e-05],
        [-1.1355e-05, -9.4622e-06,  8.9705e-06,  ..., -9.9093e-06,
         -8.9258e-06, -7.7784e-06]], device='cuda:0')
Loss: 0.9691296219825745


Running epoch 1, step 1628, batch 580
Sampled inputs[:2]: tensor([[    0,  1967,  6851,  ...,  1151,   809,   360],
        [    0, 38136,    12,  ...,   367, 12851,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3148e-05, -1.8575e-04, -3.1727e-04,  ...,  1.0449e-04,
          1.3643e-04,  3.1128e-04],
        [-6.1095e-06, -4.6343e-06,  4.6007e-06,  ..., -5.1633e-06,
         -4.3847e-06, -4.4554e-06],
        [-1.9431e-05, -1.5557e-05,  1.5751e-05,  ..., -1.6466e-05,
         -1.4141e-05, -1.4141e-05],
        [-1.8403e-05, -1.3843e-05,  1.4603e-05,  ..., -1.5646e-05,
         -1.3500e-05, -1.4141e-05],
        [-1.4290e-05, -1.1832e-05,  1.1250e-05,  ..., -1.2353e-05,
         -1.1027e-05, -9.6262e-06]], device='cuda:0')
Loss: 0.9627567529678345


Running epoch 1, step 1629, batch 581
Sampled inputs[:2]: tensor([[    0,  3217, 16714,  ...,   462,   221,   474],
        [    0,  2328,   271,  ...,   706,    13,  8961]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0597e-04, -2.9117e-04, -4.8563e-04,  ...,  3.6068e-05,
          2.2134e-04,  3.4151e-04],
        [-7.4133e-06, -5.5581e-06,  5.5842e-06,  ..., -6.2138e-06,
         -5.2527e-06, -5.3607e-06],
        [-2.3603e-05, -1.8686e-05,  1.9103e-05,  ..., -1.9863e-05,
         -1.7002e-05, -1.7077e-05],
        [-2.2277e-05, -1.6570e-05,  1.7658e-05,  ..., -1.8775e-05,
         -1.6108e-05, -1.6958e-05],
        [-1.7405e-05, -1.4260e-05,  1.3664e-05,  ..., -1.4931e-05,
         -1.3292e-05, -1.1668e-05]], device='cuda:0')
Loss: 0.9925020933151245


Running epoch 1, step 1630, batch 582
Sampled inputs[:2]: tensor([[    0,   677, 35427,  ..., 30465,  2783,     9],
        [    0,   607,   443,  ...,   259,  2646,  1597]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1174e-05, -3.2253e-04, -4.5913e-04,  ...,  1.1634e-04,
          2.8773e-04,  2.1565e-04],
        [-8.5756e-06, -6.4075e-06,  6.3777e-06,  ..., -7.2569e-06,
         -6.1691e-06, -6.2361e-06],
        [-2.7299e-05, -2.1473e-05,  2.1815e-05,  ..., -2.3112e-05,
         -1.9833e-05, -1.9789e-05],
        [-2.5913e-05, -1.9148e-05,  2.0236e-05,  ..., -2.2024e-05,
         -1.8969e-05, -1.9789e-05],
        [-2.0295e-05, -1.6510e-05,  1.5736e-05,  ..., -1.7524e-05,
         -1.5616e-05, -1.3635e-05]], device='cuda:0')
Loss: 0.9636518955230713


Running epoch 1, step 1631, batch 583
Sampled inputs[:2]: tensor([[    0,   292, 12522,  ...,   266,  1977,  8481],
        [    0,  9582,  3645,  ...,  1027,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4039e-04, -2.9220e-04, -4.7214e-04,  ...,  1.9459e-04,
          4.0390e-04,  3.9297e-04],
        [-9.7975e-06, -7.3910e-06,  7.2531e-06,  ..., -8.3745e-06,
         -7.1377e-06, -7.1265e-06],
        [-3.1263e-05, -2.4855e-05,  2.4870e-05,  ..., -2.6733e-05,
         -2.3022e-05, -2.2635e-05],
        [-2.9579e-05, -2.2113e-05,  2.3007e-05,  ..., -2.5406e-05,
         -2.1964e-05, -2.2590e-05],
        [-2.3291e-05, -1.9118e-05,  1.7986e-05,  ..., -2.0295e-05,
         -1.8135e-05, -1.5646e-05]], device='cuda:0')
Loss: 0.9506597518920898
Graident accumulation at epoch 1, step 1631, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0958e-05, -5.5547e-05, -1.0515e-04,  ..., -1.0258e-04,
          1.9550e-04,  1.4250e-06],
        [-8.8294e-06, -6.3265e-06,  6.3290e-06,  ..., -7.6841e-06,
         -5.9374e-06, -6.3376e-06],
        [-8.9677e-07,  7.3660e-06, -3.9684e-06,  ...,  9.2434e-07,
          1.0490e-05, -5.8596e-06],
        [-6.4580e-06,  8.8027e-06,  4.7131e-07,  ..., -2.0386e-06,
          5.4686e-06, -7.1996e-06],
        [-2.4589e-05, -1.8731e-05,  1.8001e-05,  ..., -2.1050e-05,
         -1.8165e-05, -1.6464e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3563e-08, 6.1465e-08, 4.7371e-08,  ..., 4.0395e-08, 1.2816e-07,
         5.3632e-08],
        [7.9631e-11, 5.8813e-11, 1.9664e-11,  ..., 5.6180e-11, 3.5264e-11,
         3.0287e-11],
        [4.1245e-09, 2.8915e-09, 1.3947e-09,  ..., 3.2046e-09, 1.4031e-09,
         1.0563e-09],
        [1.1466e-09, 1.1817e-09, 3.6114e-10,  ..., 9.9899e-10, 6.5907e-10,
         4.4725e-10],
        [3.6305e-10, 2.0699e-10, 8.0337e-11,  ..., 2.7220e-10, 8.0579e-11,
         1.0531e-10]], device='cuda:0')
optimizer state dict: 204.0
lr: [2.4620397327550194e-06, 2.4620397327550194e-06]
scheduler_last_epoch: 204


Running epoch 1, step 1632, batch 584
Sampled inputs[:2]: tensor([[    0,   266,  1403,  ...,  5145,   266,  3470],
        [    0,  1581, 11884,  ...,  7031,   689,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8788e-05, -1.9723e-04, -1.7227e-04,  ..., -1.7577e-04,
         -3.1659e-05, -7.7991e-05],
        [-1.1697e-06, -8.3819e-07,  9.2387e-07,  ..., -9.6858e-07,
         -7.7859e-07, -8.3074e-07],
        [-3.7700e-06, -2.7716e-06,  3.1590e-06,  ..., -3.0696e-06,
         -2.4736e-06, -2.6226e-06],
        [-3.6061e-06, -2.5034e-06,  3.0100e-06,  ..., -2.9653e-06,
         -2.3842e-06, -2.6524e-06],
        [-2.7269e-06, -2.0862e-06,  2.2352e-06,  ..., -2.2799e-06,
         -1.9222e-06, -1.7360e-06]], device='cuda:0')
Loss: 0.9338372349739075


Running epoch 1, step 1633, batch 585
Sampled inputs[:2]: tensor([[   0, 6491, 3667,  ..., 5042,   14, 2152],
        [   0,   14, 4746,  ...,  266, 1119, 1705]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2437e-04, -1.9723e-04, -1.6068e-05,  ..., -2.0574e-04,
          2.3563e-04, -2.4447e-05],
        [-2.3767e-06, -1.8291e-06,  1.7174e-06,  ..., -2.1011e-06,
         -1.8366e-06, -1.6950e-06],
        [-7.7039e-06, -6.1244e-06,  5.9307e-06,  ..., -6.7502e-06,
         -5.9456e-06, -5.4091e-06],
        [-7.3761e-06, -5.5730e-06,  5.5879e-06,  ..., -6.5267e-06,
         -5.7667e-06, -5.4836e-06],
        [-5.7518e-06, -4.7386e-06,  4.3213e-06,  ..., -5.1558e-06,
         -4.7088e-06, -3.7327e-06]], device='cuda:0')
Loss: 1.018725037574768


Running epoch 1, step 1634, batch 586
Sampled inputs[:2]: tensor([[    0,  1380,   342,  ...,  3904,   259,   624],
        [    0,   342, 43937,  ...,   298,   413,    29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0642e-04, -3.2037e-05,  1.3280e-04,  ..., -2.4334e-04,
          1.7709e-04, -8.9635e-05],
        [-3.6210e-06, -2.8126e-06,  2.5257e-06,  ..., -3.2485e-06,
         -2.8051e-06, -2.5742e-06],
        [-1.1638e-05, -9.4175e-06,  8.7023e-06,  ..., -1.0401e-05,
         -9.1493e-06, -8.1956e-06],
        [-1.1221e-05, -8.6278e-06,  8.1807e-06,  ..., -1.0148e-05,
         -8.9109e-06, -8.3297e-06],
        [-8.7768e-06, -7.3463e-06,  6.3926e-06,  ..., -8.0168e-06,
         -7.3016e-06, -5.7444e-06]], device='cuda:0')
Loss: 0.9467235207557678


Running epoch 1, step 1635, batch 587
Sampled inputs[:2]: tensor([[    0,  1619,   938,  ...,   292, 10026, 14367],
        [    0,    12,  3367,  ..., 16917, 12221, 12138]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3287e-05, -6.3620e-05, -3.3437e-05,  ..., -1.0319e-04,
          1.3666e-04, -1.8673e-04],
        [-4.8801e-06, -3.7737e-06,  3.4533e-06,  ..., -4.3288e-06,
         -3.7663e-06, -3.4608e-06],
        [-1.5602e-05, -1.2636e-05,  1.1817e-05,  ..., -1.3828e-05,
         -1.2234e-05, -1.0967e-05],
        [-1.4916e-05, -1.1459e-05,  1.1042e-05,  ..., -1.3351e-05,
         -1.1787e-05, -1.1057e-05],
        [-1.1742e-05, -9.8497e-06,  8.6725e-06,  ..., -1.0639e-05,
         -9.7454e-06, -7.6815e-06]], device='cuda:0')
Loss: 0.977342426776886


Running epoch 1, step 1636, batch 588
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0,  2165,  1323,  ...,   199,   677,  8376]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1315e-04, -2.1004e-04, -2.4020e-04,  ..., -8.7661e-05,
          1.4480e-05, -1.3525e-04],
        [-6.2138e-06, -4.7274e-06,  4.4145e-06,  ..., -5.4240e-06,
         -4.6343e-06, -4.3809e-06],
        [-1.9923e-05, -1.5870e-05,  1.5125e-05,  ..., -1.7375e-05,
         -1.5080e-05, -1.3947e-05],
        [-1.8790e-05, -1.4231e-05,  1.3977e-05,  ..., -1.6525e-05,
         -1.4335e-05, -1.3828e-05],
        [-1.4961e-05, -1.2338e-05,  1.1072e-05,  ..., -1.3337e-05,
         -1.2010e-05, -9.7677e-06]], device='cuda:0')
Loss: 0.9540087580680847


Running epoch 1, step 1637, batch 589
Sampled inputs[:2]: tensor([[    0,   409,   699,  ...,    12,   546,   696],
        [    0,   806,   352,  ...,  3493,   352, 49256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9976e-04, -2.9451e-04, -2.1873e-04,  ..., -8.2862e-05,
          6.7498e-05, -7.6438e-05],
        [-7.4431e-06, -5.6289e-06,  5.3160e-06,  ..., -6.5193e-06,
         -5.5879e-06, -5.2527e-06],
        [-2.3663e-05, -1.8746e-05,  1.8075e-05,  ..., -2.0683e-05,
         -1.7986e-05, -1.6570e-05],
        [-2.2411e-05, -1.6853e-05,  1.6764e-05,  ..., -1.9744e-05,
         -1.7166e-05, -1.6525e-05],
        [-1.7896e-05, -1.4693e-05,  1.3322e-05,  ..., -1.5989e-05,
         -1.4439e-05, -1.1660e-05]], device='cuda:0')
Loss: 0.9932956695556641


Running epoch 1, step 1638, batch 590
Sampled inputs[:2]: tensor([[    0,   335,   446,  ...,  5795,    12, 12433],
        [    0,   344,  8133,  ...,   368,  1119,  5539]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7954e-04, -2.2137e-04, -1.9228e-04,  ..., -1.0299e-04,
          9.1371e-06, -1.1404e-04],
        [-8.6799e-06, -6.5416e-06,  6.0834e-06,  ..., -7.6517e-06,
         -6.5193e-06, -6.2659e-06],
        [-2.7627e-05, -2.1726e-05,  2.0787e-05,  ..., -2.4214e-05,
         -2.0891e-05, -1.9714e-05],
        [-2.6062e-05, -1.9476e-05,  1.9163e-05,  ..., -2.3082e-05,
         -1.9908e-05, -1.9595e-05],
        [-2.1204e-05, -1.7181e-05,  1.5467e-05,  ..., -1.8984e-05,
         -1.6972e-05, -1.4119e-05]], device='cuda:0')
Loss: 0.9316191673278809


Running epoch 1, step 1639, batch 591
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,    80, 10802,  ...,   287, 28533, 25359]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6792e-04, -3.4131e-04, -2.3742e-04,  ..., -5.7637e-05,
          2.0085e-04, -2.7593e-04],
        [-9.9391e-06, -7.4804e-06,  6.9067e-06,  ..., -8.7842e-06,
         -7.5251e-06, -7.2271e-06],
        [-3.1471e-05, -2.4736e-05,  2.3559e-05,  ..., -2.7671e-05,
         -2.4065e-05, -2.2590e-05],
        [-2.9683e-05, -2.2218e-05,  2.1666e-05,  ..., -2.6375e-05,
         -2.2933e-05, -2.2471e-05],
        [-2.4319e-05, -1.9670e-05,  1.7688e-05,  ..., -2.1800e-05,
         -1.9640e-05, -1.6280e-05]], device='cuda:0')
Loss: 1.0066108703613281
Graident accumulation at epoch 1, step 1639, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0020,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0157, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.9298e-06, -8.4124e-05, -1.1838e-04,  ..., -9.8086e-05,
          1.9603e-04, -2.6310e-05],
        [-8.9404e-06, -6.4419e-06,  6.3868e-06,  ..., -7.7941e-06,
         -6.0962e-06, -6.4265e-06],
        [-3.9542e-06,  4.1559e-06, -1.2156e-06,  ..., -1.9352e-06,
          7.0344e-06, -7.5327e-06],
        [-8.7805e-06,  5.7007e-06,  2.5908e-06,  ..., -4.4722e-06,
          2.6285e-06, -8.7268e-06],
        [-2.4562e-05, -1.8824e-05,  1.7969e-05,  ..., -2.1125e-05,
         -1.8312e-05, -1.6445e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3518e-08, 6.1520e-08, 4.7380e-08,  ..., 4.0358e-08, 1.2807e-07,
         5.3654e-08],
        [7.9650e-11, 5.8811e-11, 1.9692e-11,  ..., 5.6201e-11, 3.5286e-11,
         3.0309e-11],
        [4.1214e-09, 2.8892e-09, 1.3938e-09,  ..., 3.2022e-09, 1.4023e-09,
         1.0557e-09],
        [1.1463e-09, 1.1810e-09, 3.6124e-10,  ..., 9.9869e-10, 6.5893e-10,
         4.4731e-10],
        [3.6328e-10, 2.0717e-10, 8.0569e-11,  ..., 2.7240e-10, 8.0884e-11,
         1.0546e-10]], device='cuda:0')
optimizer state dict: 205.0
lr: [2.381388209021682e-06, 2.381388209021682e-06]
scheduler_last_epoch: 205


Running epoch 1, step 1640, batch 592
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0, 18971,   278,  ...,  1934,  1916,  2612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3640e-05,  1.6281e-05, -4.6518e-05,  ...,  1.5020e-06,
         -1.1996e-04,  4.2956e-05],
        [-1.1995e-06, -8.5309e-07,  9.3505e-07,  ..., -1.0431e-06,
         -8.4564e-07, -8.8289e-07],
        [ 7.3952e-05,  6.8188e-05, -2.9450e-05,  ...,  3.0969e-05,
          7.3700e-05,  4.1485e-05],
        [-3.5465e-06, -2.5034e-06,  2.9057e-06,  ..., -3.0845e-06,
         -2.5630e-06, -2.6673e-06],
        [-2.7567e-06, -2.1607e-06,  2.2650e-06,  ..., -2.4289e-06,
         -2.1160e-06, -1.8254e-06]], device='cuda:0')
Loss: 0.9586855173110962


Running epoch 1, step 1641, batch 593
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0,  352,  644,  ..., 2928,  590, 3040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5978e-05,  8.9544e-05,  4.8416e-06,  ...,  3.6994e-07,
         -2.5720e-04, -6.0726e-05],
        [-2.3916e-06, -1.7099e-06,  1.9260e-06,  ..., -2.0713e-06,
         -1.6652e-06, -1.6913e-06],
        [ 7.0375e-05,  6.5476e-05, -2.6246e-05,  ...,  2.7884e-05,
          7.1212e-05,  3.9086e-05],
        [-7.1526e-06, -5.0515e-06,  6.1095e-06,  ..., -6.1840e-06,
         -5.0515e-06, -5.2601e-06],
        [-5.4389e-06, -4.2915e-06,  4.6045e-06,  ..., -4.7833e-06,
         -4.1127e-06, -3.4645e-06]], device='cuda:0')
Loss: 0.9507764577865601


Running epoch 1, step 1642, batch 594
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  298, 9855,  278],
        [   0,  496,   14,  ...,  266,  596,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9613e-04,  5.0345e-04,  2.2765e-04,  ...,  1.9028e-04,
         -6.8299e-04, -1.5992e-04],
        [-3.7402e-06, -2.5816e-06,  2.7828e-06,  ..., -3.2410e-06,
         -2.5742e-06, -2.7046e-06],
        [ 6.6263e-05,  6.2674e-05, -2.3400e-05,  ...,  2.4353e-05,
          6.8395e-05,  3.6061e-05],
        [-1.0937e-05, -7.4506e-06,  8.6278e-06,  ..., -9.4771e-06,
         -7.6592e-06, -8.1509e-06],
        [-8.7470e-06, -6.5863e-06,  6.8098e-06,  ..., -7.6592e-06,
         -6.4820e-06, -5.7295e-06]], device='cuda:0')
Loss: 0.9146672487258911


Running epoch 1, step 1643, batch 595
Sampled inputs[:2]: tensor([[    0,  3227,   278,  ...,  2950,    14, 15544],
        [    0,  7333,   342,  ...,    13,  1818,  6183]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0834e-04,  6.0672e-04,  1.3481e-04,  ...,  2.1736e-04,
         -6.9848e-04, -1.0915e-04],
        [-4.9546e-06, -3.4869e-06,  3.6694e-06,  ..., -4.3213e-06,
         -3.4459e-06, -3.5875e-06],
        [ 6.2269e-05,  5.9620e-05, -2.0285e-05,  ...,  2.0866e-05,
          6.5579e-05,  3.3244e-05],
        [-1.4618e-05, -1.0118e-05,  1.1444e-05,  ..., -1.2726e-05,
         -1.0297e-05, -1.0937e-05],
        [-1.1668e-05, -8.8960e-06,  9.0450e-06,  ..., -1.0252e-05,
         -8.6576e-06, -7.6294e-06]], device='cuda:0')
Loss: 0.944830596446991


Running epoch 1, step 1644, batch 596
Sampled inputs[:2]: tensor([[    0,   767,  9289,  ...,   494,   287,  8957],
        [    0,   287, 14752,  ...,   910, 26097,  1477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3567e-04,  4.8157e-04,  8.7675e-05,  ...,  8.4583e-05,
         -4.6415e-04, -2.1968e-04],
        [-6.1840e-06, -4.3884e-06,  4.5188e-06,  ..., -5.3942e-06,
         -4.3027e-06, -4.4666e-06],
        [ 1.1647e-04,  1.2267e-04, -6.8446e-05,  ...,  9.3822e-05,
          1.9630e-04,  9.6420e-05],
        [-1.8254e-05, -1.2815e-05,  1.4082e-05,  ..., -1.5944e-05,
         -1.2964e-05, -1.3620e-05],
        [-1.4544e-05, -1.1206e-05,  1.1161e-05,  ..., -1.2800e-05,
         -1.0848e-05, -9.5069e-06]], device='cuda:0')
Loss: 0.9732415676116943


Running epoch 1, step 1645, batch 597
Sampled inputs[:2]: tensor([[    0,  1624,  7437,  ...,    12, 16369,  5153],
        [    0,  1530,    17,  ...,   409,  1611,   895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1502e-04,  4.2715e-04,  8.7675e-05,  ...,  4.8179e-05,
         -5.5840e-04, -5.1219e-04],
        [-7.3612e-06, -5.3011e-06,  5.4464e-06,  ..., -6.4224e-06,
         -5.1148e-06, -5.3085e-06],
        [ 1.1284e-04,  1.1980e-04, -6.5376e-05,  ...,  9.0693e-05,
          1.9380e-04,  9.3901e-05],
        [-2.1756e-05, -1.5482e-05,  1.7002e-05,  ..., -1.9014e-05,
         -1.5408e-05, -1.6242e-05],
        [-1.7256e-05, -1.3441e-05,  1.3396e-05,  ..., -1.5184e-05,
         -1.2830e-05, -1.1243e-05]], device='cuda:0')
Loss: 0.9459012746810913


Running epoch 1, step 1646, batch 598
Sampled inputs[:2]: tensor([[    0,   677, 20206,  ...,   292,   334,  1550],
        [    0,  3261,  5866,  ...,   593,   360,  2502]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4445e-04,  4.0877e-04,  2.7401e-05,  ...,  1.5512e-05,
         -4.0954e-04, -5.3542e-04],
        [-8.5011e-06, -6.1393e-06,  6.2883e-06,  ..., -7.5027e-06,
         -5.9344e-06, -6.2063e-06],
        [ 1.0926e-04,  1.1713e-04, -6.2470e-05,  ...,  8.7414e-05,
          1.9129e-04,  9.1264e-05],
        [-2.5153e-05, -1.7911e-05,  1.9670e-05,  ..., -2.2218e-05,
         -1.7852e-05, -1.9014e-05],
        [-1.9968e-05, -1.5527e-05,  1.5542e-05,  ..., -1.7688e-05,
         -1.4827e-05, -1.3068e-05]], device='cuda:0')
Loss: 0.9657896161079407


Running epoch 1, step 1647, batch 599
Sampled inputs[:2]: tensor([[   0, 8754,   14,  ..., 6125,  394,  927],
        [   0, 2992,  352,  ...,  259, 2063, 6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5451e-04,  4.4450e-04,  9.6035e-05,  ..., -7.9330e-05,
         -5.5722e-04, -5.6847e-04],
        [-9.6634e-06, -7.0743e-06,  7.2122e-06,  ..., -8.5235e-06,
         -6.7763e-06, -7.0371e-06],
        [ 1.0552e-04,  1.1404e-04, -5.9282e-05,  ...,  8.4166e-05,
          1.8857e-04,  8.8656e-05],
        [-2.8715e-05, -2.0728e-05,  2.2665e-05,  ..., -2.5347e-05,
         -2.0459e-05, -2.1651e-05],
        [-2.2680e-05, -1.7852e-05,  1.7807e-05,  ..., -2.0087e-05,
         -1.6913e-05, -1.4804e-05]], device='cuda:0')
Loss: 0.9708216190338135
Graident accumulation at epoch 1, step 1647, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.2139e-06, -3.1261e-05, -9.6939e-05,  ..., -9.6211e-05,
          1.2071e-04, -8.0527e-05],
        [-9.0127e-06, -6.5051e-06,  6.4693e-06,  ..., -7.8670e-06,
         -6.1642e-06, -6.4876e-06],
        [ 6.9933e-06,  1.5145e-05, -7.0222e-06,  ...,  6.6749e-06,
          2.5188e-05,  2.0862e-06],
        [-1.0774e-05,  3.0579e-06,  4.5982e-06,  ..., -6.5597e-06,
          3.1969e-07, -1.0019e-05],
        [-2.4374e-05, -1.8727e-05,  1.7953e-05,  ..., -2.1021e-05,
         -1.8172e-05, -1.6281e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3468e-08, 6.1657e-08, 4.7342e-08,  ..., 4.0324e-08, 1.2825e-07,
         5.3924e-08],
        [7.9664e-11, 5.8802e-11, 1.9724e-11,  ..., 5.6217e-11, 3.5296e-11,
         3.0328e-11],
        [4.1284e-09, 2.8993e-09, 1.3959e-09,  ..., 3.2060e-09, 1.4364e-09,
         1.0625e-09],
        [1.1460e-09, 1.1802e-09, 3.6140e-10,  ..., 9.9833e-10, 6.5869e-10,
         4.4733e-10],
        [3.6343e-10, 2.0728e-10, 8.0806e-11,  ..., 2.7253e-10, 8.1089e-11,
         1.0558e-10]], device='cuda:0')
optimizer state dict: 206.0
lr: [2.3019008756738137e-06, 2.3019008756738137e-06]
scheduler_last_epoch: 206


Running epoch 1, step 1648, batch 600
Sampled inputs[:2]: tensor([[    0,    14,   747,  ..., 12545,    12, 15209],
        [    0,  9029,   634,  ...,  1424,  6872,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5442e-05,  1.0567e-06, -1.5781e-04,  ...,  7.0617e-05,
          2.4153e-05, -1.3358e-05],
        [-1.3113e-06, -7.9721e-07,  8.3819e-07,  ..., -1.1101e-06,
         -8.2701e-07, -1.0654e-06],
        [-4.1127e-06, -2.6375e-06,  2.8908e-06,  ..., -3.4273e-06,
         -2.5928e-06, -3.2634e-06],
        [-3.7402e-06, -2.2501e-06,  2.5034e-06,  ..., -3.1888e-06,
         -2.4140e-06, -3.0994e-06],
        [-3.2037e-06, -2.0862e-06,  2.1756e-06,  ..., -2.7120e-06,
         -2.1309e-06, -2.3544e-06]], device='cuda:0')
Loss: 0.918081521987915


Running epoch 1, step 1649, batch 601
Sampled inputs[:2]: tensor([[    0,  3386, 43625,  ...,    19,  2125,   271],
        [    0,  1034,   287,  ...,  9677,    13,  6687]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0161e-04, -3.4728e-05, -2.3492e-04,  ..., -3.4887e-05,
          8.0298e-05, -8.2023e-05],
        [-2.5034e-06, -1.6429e-06,  1.6913e-06,  ..., -2.1681e-06,
         -1.7136e-06, -1.9409e-06],
        [-7.8082e-06, -5.2899e-06,  5.7369e-06,  ..., -6.6161e-06,
         -5.2750e-06, -5.8711e-06],
        [-7.2867e-06, -4.6939e-06,  5.1856e-06,  ..., -6.3330e-06,
         -5.0664e-06, -5.8115e-06],
        [-6.0648e-06, -4.2021e-06,  4.3362e-06,  ..., -5.2154e-06,
         -4.2915e-06, -4.2096e-06]], device='cuda:0')
Loss: 0.957090437412262


Running epoch 1, step 1650, batch 602
Sampled inputs[:2]: tensor([[    0,   437,   266,  ...,   266, 16084,  1781],
        [    0,  8840,    26,  ...,    28,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4461e-05,  9.7106e-06, -2.3988e-04,  ...,  1.8285e-05,
          1.8818e-04, -8.2664e-05],
        [-3.7849e-06, -2.6636e-06,  2.5630e-06,  ..., -3.2932e-06,
         -2.7269e-06, -2.8834e-06],
        [-1.1832e-05, -8.6725e-06,  8.6874e-06,  ..., -1.0163e-05,
         -8.4937e-06, -8.8215e-06],
        [-1.1057e-05, -7.7039e-06,  7.8678e-06,  ..., -9.6709e-06,
         -8.1211e-06, -8.7172e-06],
        [-9.2387e-06, -6.9290e-06,  6.6012e-06,  ..., -8.0466e-06,
         -6.9439e-06, -6.4000e-06]], device='cuda:0')
Loss: 0.9985519051551819


Running epoch 1, step 1651, batch 603
Sampled inputs[:2]: tensor([[    0,   721,  1717,  ...,   278, 26029,    12],
        [    0,  7120,   344,  ...,  6273,    52, 22639]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0427e-06,  2.7990e-05, -1.6750e-04,  ...,  2.3224e-05,
          2.5894e-04, -1.5410e-04],
        [-5.0142e-06, -3.5502e-06,  3.4086e-06,  ..., -4.3958e-06,
         -3.6322e-06, -3.8072e-06],
        [-1.5765e-05, -1.1593e-05,  1.1578e-05,  ..., -1.3664e-05,
         -1.1370e-05, -1.1742e-05],
        [-1.4722e-05, -1.0282e-05,  1.0505e-05,  ..., -1.2949e-05,
         -1.0818e-05, -1.1578e-05],
        [-1.2279e-05, -9.2685e-06,  8.7768e-06,  ..., -1.0788e-05,
         -9.2834e-06, -8.5011e-06]], device='cuda:0')
Loss: 0.9854608178138733


Running epoch 1, step 1652, batch 604
Sampled inputs[:2]: tensor([[    0,   287,   271,  ...,  1039,  4186,    13],
        [    0,   638,  2708,  ..., 28492,  1814,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0427e-06,  2.5447e-05, -1.9084e-04,  ..., -2.4051e-05,
          2.7290e-04, -1.7817e-04],
        [-6.2436e-06, -4.5113e-06,  4.4145e-06,  ..., -5.4091e-06,
         -4.5300e-06, -4.6231e-06],
        [-1.9759e-05, -1.4842e-05,  1.4991e-05,  ..., -1.6972e-05,
         -1.4305e-05, -1.4439e-05],
        [-1.8507e-05, -1.3217e-05,  1.3739e-05,  ..., -1.6063e-05,
         -1.3590e-05, -1.4231e-05],
        [-1.5154e-05, -1.1712e-05,  1.1176e-05,  ..., -1.3232e-05,
         -1.1533e-05, -1.0312e-05]], device='cuda:0')
Loss: 1.0143362283706665


Running epoch 1, step 1653, batch 605
Sampled inputs[:2]: tensor([[   0,  792,   83,  ...,  300,  768,  932],
        [   0,  287, 1070,  ...,  292,  221,  374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4821e-06,  7.2953e-05, -3.1361e-04,  ..., -1.7125e-05,
          2.2092e-04, -2.0621e-04],
        [-7.5027e-06, -5.3048e-06,  5.2713e-06,  ..., -6.4895e-06,
         -5.4464e-06, -5.5358e-06],
        [-2.3603e-05, -1.7419e-05,  1.7837e-05,  ..., -2.0221e-05,
         -1.7092e-05, -1.7166e-05],
        [-2.2292e-05, -1.5557e-05,  1.6466e-05,  ..., -1.9312e-05,
         -1.6391e-05, -1.7077e-05],
        [-1.8075e-05, -1.3724e-05,  1.3277e-05,  ..., -1.5751e-05,
         -1.3754e-05, -1.2249e-05]], device='cuda:0')
Loss: 0.9580682516098022


Running epoch 1, step 1654, batch 606
Sampled inputs[:2]: tensor([[    0,   709,   630,  ...,  6263,   409,   508],
        [    0,  3231,   271,  ...,  9279,  8231, 28871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6234e-05,  1.3754e-04, -2.9127e-04,  ..., -1.7016e-04,
         -1.2710e-05, -4.3409e-04],
        [-8.7023e-06, -6.1914e-06,  6.2063e-06,  ..., -7.5251e-06,
         -6.3106e-06, -6.3926e-06],
        [-2.7388e-05, -2.0340e-05,  2.0981e-05,  ..., -2.3484e-05,
         -1.9848e-05, -1.9819e-05],
        [-2.5839e-05, -1.8135e-05,  1.9386e-05,  ..., -2.2367e-05,
         -1.8969e-05, -1.9699e-05],
        [-2.0906e-05, -1.5989e-05,  1.5572e-05,  ..., -1.8239e-05,
         -1.5944e-05, -1.4104e-05]], device='cuda:0')
Loss: 0.9269800186157227


Running epoch 1, step 1655, batch 607
Sampled inputs[:2]: tensor([[    0, 40995,  5863,  ...,    13,  9819,   609],
        [    0,    14,  3080,  ...,   910,   266,  5275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0859e-04,  2.7568e-04, -2.6776e-04,  ..., -1.0348e-04,
         -3.9338e-04, -4.5619e-04],
        [-1.0081e-05, -7.0482e-06,  7.0706e-06,  ..., -8.6427e-06,
         -7.2382e-06, -7.4953e-06],
        [-3.1769e-05, -2.3276e-05,  2.3961e-05,  ..., -2.7046e-05,
         -2.2903e-05, -2.3305e-05],
        [-2.9862e-05, -2.0623e-05,  2.2054e-05,  ..., -2.5660e-05,
         -2.1771e-05, -2.3007e-05],
        [-2.4438e-05, -1.8373e-05,  1.7866e-05,  ..., -2.1160e-05,
         -1.8522e-05, -1.6756e-05]], device='cuda:0')
Loss: 0.9360907673835754
Graident accumulation at epoch 1, step 1655, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.5663e-06, -5.6722e-07, -1.1402e-04,  ..., -9.6938e-05,
          6.9298e-05, -1.1809e-04],
        [-9.1195e-06, -6.5595e-06,  6.5294e-06,  ..., -7.9446e-06,
         -6.2716e-06, -6.5883e-06],
        [ 3.1170e-06,  1.1303e-05, -3.9239e-06,  ...,  3.3028e-06,
          2.0379e-05, -4.5296e-07],
        [-1.2683e-05,  6.8976e-07,  6.3437e-06,  ..., -8.4697e-06,
         -1.8893e-06, -1.1318e-05],
        [-2.4380e-05, -1.8692e-05,  1.7945e-05,  ..., -2.1035e-05,
         -1.8207e-05, -1.6329e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3407e-08, 6.1671e-08, 4.7367e-08,  ..., 4.0295e-08, 1.2828e-07,
         5.4078e-08],
        [7.9686e-11, 5.8793e-11, 1.9754e-11,  ..., 5.6235e-11, 3.5314e-11,
         3.0354e-11],
        [4.1253e-09, 2.8969e-09, 1.3951e-09,  ..., 3.2036e-09, 1.4355e-09,
         1.0620e-09],
        [1.1457e-09, 1.1795e-09, 3.6152e-10,  ..., 9.9799e-10, 6.5851e-10,
         4.4741e-10],
        [3.6367e-10, 2.0741e-10, 8.1044e-11,  ..., 2.7270e-10, 8.1351e-11,
         1.0575e-10]], device='cuda:0')
optimizer state dict: 207.0
lr: [2.223589879069793e-06, 2.223589879069793e-06]
scheduler_last_epoch: 207


Running epoch 1, step 1656, batch 608
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0,  5489,    80,  ...,   221,   380,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0756e-05,  1.1816e-04,  4.6532e-05,  ...,  1.1089e-04,
         -2.0237e-04, -1.6525e-05],
        [-1.1995e-06, -8.9407e-07,  8.9407e-07,  ..., -1.0952e-06,
         -9.0897e-07, -8.4192e-07],
        [-3.7402e-06, -2.8759e-06,  2.9653e-06,  ..., -3.3528e-06,
         -2.8014e-06, -2.5183e-06],
        [-3.5614e-06, -2.6077e-06,  2.7865e-06,  ..., -3.2336e-06,
         -2.7269e-06, -2.5779e-06],
        [-2.8163e-06, -2.2501e-06,  2.1905e-06,  ..., -2.5630e-06,
         -2.2203e-06, -1.7732e-06]], device='cuda:0')
Loss: 0.9616401195526123


Running epoch 1, step 1657, batch 609
Sampled inputs[:2]: tensor([[    0,   278,  5717,  ...,  5342,  5147,    14],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2846e-04, -6.7777e-06,  5.3987e-05,  ...,  2.1407e-04,
         -3.6964e-04, -4.8786e-05],
        [-2.4587e-06, -1.8552e-06,  1.8068e-06,  ..., -2.2054e-06,
         -1.7732e-06, -1.7323e-06],
        [-7.6145e-06, -5.9754e-06,  5.9903e-06,  ..., -6.7949e-06,
         -5.5730e-06, -5.2601e-06],
        [-7.2718e-06, -5.4538e-06,  5.6475e-06,  ..., -6.5267e-06,
         -5.3346e-06, -5.3197e-06],
        [-5.7071e-06, -4.6641e-06,  4.3958e-06,  ..., -5.1707e-06,
         -4.4256e-06, -3.6806e-06]], device='cuda:0')
Loss: 0.9789105653762817


Running epoch 1, step 1658, batch 610
Sampled inputs[:2]: tensor([[    0,  3529,   271,  ...,  1553,   365,  2714],
        [    0,   275, 11628,  ...,   408,  1296,  3796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3756e-05,  3.9615e-05,  6.4361e-05,  ...,  1.4889e-04,
         -4.0981e-04, -1.1360e-04],
        [-3.7178e-06, -2.8089e-06,  2.7157e-06,  ..., -3.3230e-06,
         -2.7083e-06, -2.6114e-06],
        [-1.1519e-05, -9.1344e-06,  9.0301e-06,  ..., -1.0282e-05,
         -8.5682e-06, -7.9572e-06],
        [-1.0982e-05, -8.2850e-06,  8.4937e-06,  ..., -9.8497e-06,
         -8.1807e-06, -8.0317e-06],
        [-8.5980e-06, -7.0930e-06,  6.6012e-06,  ..., -7.8082e-06,
         -6.7651e-06, -5.5581e-06]], device='cuda:0')
Loss: 0.9722172617912292


Running epoch 1, step 1659, batch 611
Sampled inputs[:2]: tensor([[    0,   221,   422,  ...,  2693,   733,   381],
        [    0,    12,  6426,  ...,  2629, 13422,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0616e-04,  1.8152e-04, -4.1627e-05,  ...,  1.0172e-04,
         -6.9535e-04, -7.3061e-05],
        [-4.9993e-06, -3.6508e-06,  3.5428e-06,  ..., -4.4629e-06,
         -3.6992e-06, -3.6173e-06],
        [-1.5453e-05, -1.1891e-05,  1.1757e-05,  ..., -1.3754e-05,
         -1.1653e-05, -1.0967e-05],
        [-1.4722e-05, -1.0699e-05,  1.1012e-05,  ..., -1.3188e-05,
         -1.1131e-05, -1.1042e-05],
        [-1.1787e-05, -9.3728e-06,  8.7321e-06,  ..., -1.0669e-05,
         -9.3877e-06, -7.8678e-06]], device='cuda:0')
Loss: 0.9273695349693298


Running epoch 1, step 1660, batch 612
Sampled inputs[:2]: tensor([[    0,  2320,    63,  ...,   858,    13, 40170],
        [    0,  3504,     9,  ...,  7166, 10945,  3119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5888e-05,  9.4037e-05,  4.8186e-05,  ...,  2.0338e-04,
         -6.4498e-04, -2.0792e-05],
        [-6.2957e-06, -4.5672e-06,  4.4554e-06,  ..., -5.5730e-06,
         -4.6156e-06, -4.5151e-06],
        [-1.9625e-05, -1.4976e-05,  1.4886e-05,  ..., -1.7330e-05,
         -1.4603e-05, -1.3888e-05],
        [-1.8537e-05, -1.3351e-05,  1.3813e-05,  ..., -1.6466e-05,
         -1.3843e-05, -1.3843e-05],
        [-1.4886e-05, -1.1757e-05,  1.0982e-05,  ..., -1.3381e-05,
         -1.1697e-05, -9.8795e-06]], device='cuda:0')
Loss: 0.9861597418785095


Running epoch 1, step 1661, batch 613
Sampled inputs[:2]: tensor([[    0,   266,  3727,  ...,  1143,   271,  5213],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7474e-04,  1.5254e-05,  3.3786e-05,  ...,  1.7400e-04,
         -5.0947e-04, -6.5299e-05],
        [-7.6294e-06, -5.5134e-06,  5.3421e-06,  ..., -6.7726e-06,
         -5.5842e-06, -5.4911e-06],
        [-2.3797e-05, -1.8105e-05,  1.7852e-05,  ..., -2.1100e-05,
         -1.7717e-05, -1.6972e-05],
        [ 7.3600e-05,  8.1937e-05, -2.9949e-05,  ...,  3.6426e-05,
          5.8788e-05,  2.2556e-05],
        [-1.8254e-05, -1.4320e-05,  1.3277e-05,  ..., -1.6466e-05,
         -1.4305e-05, -1.2249e-05]], device='cuda:0')
Loss: 0.9991835951805115


Running epoch 1, step 1662, batch 614
Sampled inputs[:2]: tensor([[    0,    20,  2637,  ..., 14044,     9,   292],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8433e-05,  5.4429e-05,  7.8867e-05,  ...,  2.4882e-04,
         -3.9152e-04,  1.8454e-04],
        [-8.9407e-06, -6.4299e-06,  6.1058e-06,  ..., -7.9870e-06,
         -6.5751e-06, -6.5193e-06],
        [-2.7910e-05, -2.1204e-05,  2.0519e-05,  ..., -2.4915e-05,
         -2.0921e-05, -2.0206e-05],
        [ 6.9666e-05,  7.9135e-05, -2.7535e-05,  ...,  3.2701e-05,
          5.5673e-05,  1.9352e-05],
        [-2.1473e-05, -1.6794e-05,  1.5303e-05,  ..., -1.9491e-05,
         -1.6898e-05, -1.4618e-05]], device='cuda:0')
Loss: 0.9397668242454529


Running epoch 1, step 1663, batch 615
Sampled inputs[:2]: tensor([[   0,    9,  292,  ...,  944,  278, 1758],
        [   0,  271, 8278,  ...,  271, 8278, 3560]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8107e-04,  9.2681e-05, -1.2280e-04,  ...,  2.4933e-04,
         -6.4679e-04, -3.4079e-05],
        [-1.0163e-05, -7.3500e-06,  7.1116e-06,  ..., -9.0376e-06,
         -7.3984e-06, -7.3351e-06],
        [-3.1710e-05, -2.4185e-05,  2.3797e-05,  ..., -2.8193e-05,
         -2.3499e-05, -2.2754e-05],
        [ 6.5941e-05,  7.6349e-05, -2.4331e-05,  ...,  2.9482e-05,
          5.3140e-05,  1.6730e-05],
        [-2.4199e-05, -1.9044e-05,  1.7613e-05,  ..., -2.1890e-05,
         -1.8880e-05, -1.6324e-05]], device='cuda:0')
Loss: 0.9705237746238708
Graident accumulation at epoch 1, step 1663, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.0416e-05,  8.7576e-06, -1.1490e-04,  ..., -6.2311e-05,
         -2.3106e-06, -1.0969e-04],
        [-9.2238e-06, -6.6385e-06,  6.5876e-06,  ..., -8.0539e-06,
         -6.3843e-06, -6.6630e-06],
        [-3.6564e-07,  7.7539e-06, -1.1518e-06,  ...,  1.5325e-07,
          1.5991e-05, -2.6831e-06],
        [-4.8204e-06,  8.2557e-06,  3.2763e-06,  ..., -4.6745e-06,
          3.6136e-06, -8.5133e-06],
        [-2.4362e-05, -1.8727e-05,  1.7911e-05,  ..., -2.1120e-05,
         -1.8274e-05, -1.6328e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3412e-08, 6.1618e-08, 4.7334e-08,  ..., 4.0317e-08, 1.2857e-07,
         5.4025e-08],
        [7.9709e-11, 5.8788e-11, 1.9785e-11,  ..., 5.6261e-11, 3.5333e-11,
         3.0378e-11],
        [4.1222e-09, 2.8946e-09, 1.3943e-09,  ..., 3.2012e-09, 1.4346e-09,
         1.0615e-09],
        [1.1489e-09, 1.1841e-09, 3.6175e-10,  ..., 9.9787e-10, 6.6067e-10,
         4.4724e-10],
        [3.6389e-10, 2.0757e-10, 8.1273e-11,  ..., 2.7291e-10, 8.1626e-11,
         1.0591e-10]], device='cuda:0')
optimizer state dict: 208.0
lr: [2.1464671858134968e-06, 2.1464671858134968e-06]
scheduler_last_epoch: 208


Running epoch 1, step 1664, batch 616
Sampled inputs[:2]: tensor([[   0, 2706,  292,  ...,   13, 8954,   13],
        [   0, 1875, 2117,  ..., 1422, 1059,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2256e-05,  2.1038e-04,  9.1336e-05,  ..., -1.5008e-04,
          8.4701e-05,  1.0856e-04],
        [-1.1995e-06, -8.5682e-07,  8.0839e-07,  ..., -1.1325e-06,
         -9.9838e-07, -9.0152e-07],
        [-3.5763e-06, -2.6673e-06,  2.5928e-06,  ..., -3.3081e-06,
         -2.9802e-06, -2.5779e-06],
        [ 1.0358e-04,  1.9428e-04, -1.2004e-04,  ...,  1.3673e-04,
          1.5027e-04,  8.7789e-05],
        [-2.8461e-06, -2.2054e-06,  2.0117e-06,  ..., -2.6673e-06,
         -2.4736e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.9493772387504578


Running epoch 1, step 1665, batch 617
Sampled inputs[:2]: tensor([[    0,   654,   300,  ..., 21762,  3597, 11117],
        [    0,  4834,   278,  ...,    13,  8382,   669]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0742e-04,  3.3981e-04,  1.8340e-04,  ..., -2.0976e-04,
          1.2953e-04,  1.5580e-04],
        [-2.5406e-06, -1.8030e-06,  1.6615e-06,  ..., -2.3022e-06,
         -1.9744e-06, -1.8328e-06],
        [-7.7486e-06, -5.7966e-06,  5.4538e-06,  ..., -6.9588e-06,
         -6.0648e-06, -5.4985e-06],
        [ 9.9679e-05,  1.9154e-04, -1.1746e-04,  ...,  1.3331e-04,
          1.4740e-04,  8.4973e-05],
        [-6.0648e-06, -4.6790e-06,  4.1574e-06,  ..., -5.5283e-06,
         -4.9919e-06, -4.0084e-06]], device='cuda:0')
Loss: 0.9576407670974731


Running epoch 1, step 1666, batch 618
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,  4684, 14107,   259],
        [    0,   271, 28279,  ...,   367,   806,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9957e-05,  3.3631e-04,  1.2242e-04,  ..., -3.6487e-04,
          1.0056e-04,  1.4765e-04],
        [-3.8669e-06, -2.7195e-06,  2.5816e-06,  ..., -3.4571e-06,
         -2.9244e-06, -2.7642e-06],
        [-1.1891e-05, -8.8215e-06,  8.5086e-06,  ..., -1.0580e-05,
         -9.0748e-06, -8.3894e-06],
        [ 9.5774e-05,  1.8884e-04, -1.1462e-04,  ...,  1.2989e-04,
          1.4454e-04,  8.2097e-05],
        [-9.2387e-06, -7.0930e-06,  6.4522e-06,  ..., -8.3596e-06,
         -7.4506e-06, -6.1095e-06]], device='cuda:0')
Loss: 0.9683434367179871


Running epoch 1, step 1667, batch 619
Sampled inputs[:2]: tensor([[    0,  1192, 11929,  ...,   266,  1551,  1860],
        [    0,   494,   825,  ...,   897,   328,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8144e-05,  4.2626e-04,  1.9536e-04,  ..., -3.7090e-04,
         -4.0940e-05,  1.6070e-04],
        [-5.1633e-06, -3.6731e-06,  3.5129e-06,  ..., -4.5821e-06,
         -3.8855e-06, -3.6545e-06],
        [-1.5974e-05, -1.1995e-05,  1.1623e-05,  ..., -1.4126e-05,
         -1.2144e-05, -1.1221e-05],
        [ 9.1930e-05,  1.8604e-04, -1.1174e-04,  ...,  1.2657e-04,
          1.4165e-04,  7.9310e-05],
        [-1.2323e-05, -9.5963e-06,  8.7619e-06,  ..., -1.1101e-05,
         -9.9093e-06, -8.1062e-06]], device='cuda:0')
Loss: 0.9824104309082031


Running epoch 1, step 1668, batch 620
Sampled inputs[:2]: tensor([[   0,    9,  870,  ..., 2671,  965, 3229],
        [   0,  898, 1427,  ...,  508, 1860,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7554e-05,  6.1460e-04,  3.0802e-04,  ..., -3.4668e-04,
         -1.1177e-05,  2.1140e-04],
        [-6.4373e-06, -4.5747e-06,  4.3772e-06,  ..., -5.6922e-06,
         -4.8466e-06, -4.5560e-06],
        [-1.9819e-05, -1.4946e-05,  1.4439e-05,  ..., -1.7524e-05,
         -1.5140e-05, -1.3977e-05],
        [ 8.8056e-05,  1.8325e-04, -1.0898e-04,  ...,  1.2313e-04,
          1.3861e-04,  7.6434e-05],
        [-1.5274e-05, -1.1951e-05,  1.0863e-05,  ..., -1.3769e-05,
         -1.2353e-05, -1.0088e-05]], device='cuda:0')
Loss: 0.9927091002464294


Running epoch 1, step 1669, batch 621
Sampled inputs[:2]: tensor([[   0,  772,  699,  ..., 1849,  287, 7134],
        [   0, 2013,   13,  ...,  271,  266,  908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6972e-04,  6.4202e-04,  2.9250e-04,  ..., -4.4493e-04,
         -2.5678e-04,  1.3875e-04],
        [-7.6666e-06, -5.5134e-06,  5.3532e-06,  ..., -6.7800e-06,
         -5.7630e-06, -5.3942e-06],
        [-2.3693e-05, -1.8060e-05,  1.7703e-05,  ..., -2.0996e-05,
         -1.8060e-05, -1.6674e-05],
        [ 8.4390e-05,  1.8047e-04, -1.0592e-04,  ...,  1.1987e-04,
          1.3585e-04,  7.3782e-05],
        [-1.8120e-05, -1.4320e-05,  1.3202e-05,  ..., -1.6361e-05,
         -1.4633e-05, -1.1936e-05]], device='cuda:0')
Loss: 0.9797939658164978


Running epoch 1, step 1670, batch 622
Sampled inputs[:2]: tensor([[    0,   598,   696,  ...,  4048,  1795,    14],
        [    0,   587,   300,  ...,  4325,   278, 12564]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6114e-04,  6.6013e-04,  2.6297e-04,  ..., -4.6149e-04,
         -4.6234e-04,  1.9216e-04],
        [-8.9630e-06, -6.4112e-06,  6.3814e-06,  ..., -7.8529e-06,
         -6.6459e-06, -6.2473e-06],
        [-2.7627e-05, -2.0981e-05,  2.1026e-05,  ..., -2.4304e-05,
         -2.0847e-05, -1.9282e-05],
        [ 8.0575e-05,  1.7780e-04, -1.0271e-04,  ...,  1.1668e-04,
          1.3317e-04,  7.1130e-05],
        [-2.0936e-05, -1.6510e-05,  1.5527e-05,  ..., -1.8775e-05,
         -1.6749e-05, -1.3687e-05]], device='cuda:0')
Loss: 0.9616695642471313


Running epoch 1, step 1671, batch 623
Sampled inputs[:2]: tensor([[    0,   527,  2811,  ...,   287,  1288,   352],
        [    0,  4092,  3517,  ..., 23070,    14,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8180e-04,  7.0416e-04,  3.4286e-04,  ..., -5.3526e-04,
         -3.6304e-04,  3.5941e-04],
        [-1.0170e-05, -7.2755e-06,  7.1824e-06,  ..., -8.9183e-06,
         -7.5847e-06, -7.1451e-06],
        [-3.1352e-05, -2.3842e-05,  2.3723e-05,  ..., -2.7612e-05,
         -2.3797e-05, -2.2069e-05],
        [ 7.6895e-05,  1.7516e-04, -1.0015e-04,  ...,  1.1337e-04,
          1.3022e-04,  6.8254e-05],
        [-2.3812e-05, -1.8790e-05,  1.7539e-05,  ..., -2.1368e-05,
         -1.9133e-05, -1.5683e-05]], device='cuda:0')
Loss: 0.9524617791175842
Graident accumulation at epoch 1, step 1671, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.5554e-05,  7.8298e-05, -6.9123e-05,  ..., -1.0961e-04,
         -3.8384e-05, -6.2781e-05],
        [-9.3184e-06, -6.7022e-06,  6.6471e-06,  ..., -8.1403e-06,
         -6.5043e-06, -6.7112e-06],
        [-3.4643e-06,  4.5943e-06,  1.3356e-06,  ..., -2.6233e-06,
          1.2012e-05, -4.6216e-06],
        [ 3.3511e-06,  2.4946e-05, -7.0662e-06,  ...,  7.1298e-06,
          1.6274e-05, -8.3657e-07],
        [-2.4307e-05, -1.8733e-05,  1.7874e-05,  ..., -2.1145e-05,
         -1.8360e-05, -1.6264e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3418e-08, 6.2052e-08, 4.7404e-08,  ..., 4.0563e-08, 1.2857e-07,
         5.4100e-08],
        [7.9733e-11, 5.8782e-11, 1.9817e-11,  ..., 5.6284e-11, 3.5355e-11,
         3.0398e-11],
        [4.1190e-09, 2.8923e-09, 1.3935e-09,  ..., 3.1987e-09, 1.4337e-09,
         1.0609e-09],
        [1.1537e-09, 1.2136e-09, 3.7142e-10,  ..., 1.0097e-09, 6.7697e-10,
         4.5145e-10],
        [3.6409e-10, 2.0771e-10, 8.1500e-11,  ..., 2.7310e-10, 8.1911e-11,
         1.0605e-10]], device='cuda:0')
optimizer state dict: 209.0
lr: [2.070544580925664e-06, 2.070544580925664e-06]
scheduler_last_epoch: 209


Running epoch 1, step 1672, batch 624
Sampled inputs[:2]: tensor([[    0,   259,  2283,  ...,   462,   221,   474],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0527e-04,  3.9617e-05, -7.1946e-05,  ..., -9.7180e-06,
         -2.2190e-04, -1.1744e-04],
        [-1.0729e-06, -7.4506e-07,  8.7917e-07,  ..., -1.0133e-06,
         -8.0094e-07, -8.2701e-07],
        [-3.2485e-06, -2.2799e-06,  2.8759e-06,  ..., -2.9504e-06,
         -2.3395e-06, -2.3246e-06],
        [-3.1739e-06, -2.1309e-06,  2.7865e-06,  ..., -2.9504e-06,
         -2.3544e-06, -2.5183e-06],
        [-2.4587e-06, -1.8105e-06,  2.1160e-06,  ..., -2.2501e-06,
         -1.8775e-06, -1.5944e-06]], device='cuda:0')
Loss: 0.9143418669700623


Running epoch 1, step 1673, batch 625
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,   287, 30256,  ...,   287,  8137, 13021]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2669e-04,  2.0870e-04, -7.9475e-05,  ...,  1.0563e-05,
         -3.9478e-04, -5.9369e-05],
        [-2.4140e-06, -1.5795e-06,  1.9670e-06,  ..., -2.1011e-06,
         -1.6429e-06, -1.7062e-06],
        [-7.3314e-06, -4.9919e-06,  6.3628e-06,  ..., -6.2883e-06,
         -5.0068e-06, -4.9770e-06],
        [-7.1675e-06, -4.6045e-06,  6.1691e-06,  ..., -6.1989e-06,
         -4.9174e-06, -5.2303e-06],
        [-5.3495e-06, -3.8520e-06,  4.5449e-06,  ..., -4.6790e-06,
         -3.9190e-06, -3.3528e-06]], device='cuda:0')
Loss: 0.9881593585014343


Running epoch 1, step 1674, batch 626
Sampled inputs[:2]: tensor([[   0,  518, 9048,  ..., 1354,  352,  266],
        [   0, 3951,   77,  ..., 7062,  278,  600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4004e-04, -4.9460e-05, -8.2795e-05,  ..., -7.5577e-05,
         -3.5651e-04,  2.3829e-05],
        [-3.5614e-06, -2.4587e-06,  2.8312e-06,  ..., -3.0994e-06,
         -2.4810e-06, -2.5705e-06],
        [-1.0937e-05, -7.8678e-06,  9.2834e-06,  ..., -9.4026e-06,
         -7.6145e-06, -7.6443e-06],
        [-1.0684e-05, -7.2569e-06,  8.9556e-06,  ..., -9.2685e-06,
         -7.5102e-06, -8.0168e-06],
        [-8.0615e-06, -6.0871e-06,  6.6906e-06,  ..., -7.0482e-06,
         -5.9903e-06, -5.1931e-06]], device='cuda:0')
Loss: 0.9556365013122559


Running epoch 1, step 1675, batch 627
Sampled inputs[:2]: tensor([[   0, 1057,   14,  ...,   14, 4735,   13],
        [   0, 1236, 6446,  ...,  300,  706, 3698]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5515e-04, -6.5055e-05, -6.4191e-05,  ..., -6.9981e-05,
         -3.8292e-04, -1.4806e-04],
        [-4.9472e-06, -3.3528e-06,  3.6806e-06,  ..., -4.3064e-06,
         -3.4943e-06, -3.5763e-06],
        [-1.5348e-05, -1.0848e-05,  1.2189e-05,  ..., -1.3217e-05,
         -1.0848e-05, -1.0848e-05],
        [-1.4707e-05, -9.8199e-06,  1.1519e-05,  ..., -1.2770e-05,
         -1.0476e-05, -1.1057e-05],
        [-1.1504e-05, -8.5011e-06,  8.8960e-06,  ..., -1.0088e-05,
         -8.6576e-06, -7.5623e-06]], device='cuda:0')
Loss: 0.978812038898468


Running epoch 1, step 1676, batch 628
Sampled inputs[:2]: tensor([[   0, 1481,  278,  ..., 3940, 4938,    5],
        [   0,   12,  266,  ...,  674,  369,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3655e-04,  3.0968e-05, -7.5658e-05,  ...,  2.4952e-05,
         -6.3266e-04, -2.0521e-04],
        [-6.1765e-06, -4.1872e-06,  4.5821e-06,  ..., -5.3495e-06,
         -4.2841e-06, -4.4368e-06],
        [-1.9193e-05, -1.3575e-05,  1.5229e-05,  ..., -1.6451e-05,
         -1.3292e-05, -1.3441e-05],
        [-1.8358e-05, -1.2264e-05,  1.4350e-05,  ..., -1.5885e-05,
         -1.2845e-05, -1.3739e-05],
        [-1.4231e-05, -1.0513e-05,  1.1012e-05,  ..., -1.2413e-05,
         -1.0498e-05, -9.2462e-06]], device='cuda:0')
Loss: 0.9391318559646606


Running epoch 1, step 1677, batch 629
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0, 1145,   35,  ...,  300, 5192,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4266e-04, -6.0118e-06, -3.6419e-05,  ..., -3.8876e-05,
         -6.0182e-04, -2.4967e-04],
        [-7.3388e-06, -5.0403e-06,  5.4762e-06,  ..., -6.4373e-06,
         -5.1558e-06, -5.3346e-06],
        [-2.2709e-05, -1.6198e-05,  1.8165e-05,  ..., -1.9670e-05,
         -1.5914e-05, -1.5989e-05],
        [-2.1741e-05, -1.4707e-05,  1.7107e-05,  ..., -1.9029e-05,
         -1.5423e-05, -1.6376e-05],
        [-1.6883e-05, -1.2539e-05,  1.3173e-05,  ..., -1.4842e-05,
         -1.2554e-05, -1.1019e-05]], device='cuda:0')
Loss: 0.9493163824081421


Running epoch 1, step 1678, batch 630
Sampled inputs[:2]: tensor([[    0, 28590,    12,  ...,   342, 29639,  1693],
        [    0,   287,  4579,  ...,   909,    12,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7651e-04,  4.3220e-05, -1.2927e-04,  ..., -3.2790e-05,
         -6.5804e-04, -3.3446e-04],
        [-8.5533e-06, -5.9083e-06,  6.4038e-06,  ..., -7.5176e-06,
         -6.0089e-06, -6.2138e-06],
        [-2.6479e-05, -1.8939e-05,  2.1234e-05,  ..., -2.2963e-05,
         -1.8522e-05, -1.8641e-05],
        [-2.5377e-05, -1.7226e-05,  2.0012e-05,  ..., -2.2233e-05,
         -1.7956e-05, -1.9103e-05],
        [-1.9655e-05, -1.4625e-05,  1.5363e-05,  ..., -1.7285e-05,
         -1.4566e-05, -1.2808e-05]], device='cuda:0')
Loss: 0.9540277123451233


Running epoch 1, step 1679, batch 631
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 8122, 1387,  616],
        [   0,    9,  391,  ...,  300, 2646, 1717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6340e-04, -1.3358e-04, -2.6636e-04,  ..., -1.1582e-04,
         -6.1886e-04, -2.4677e-04],
        [-9.8720e-06, -6.8024e-06,  7.3239e-06,  ..., -8.6278e-06,
         -6.8694e-06, -7.0892e-06],
        [-3.0324e-05, -2.1785e-05,  2.4170e-05,  ..., -2.6241e-05,
         -2.1145e-05, -2.1175e-05],
        [-2.9176e-05, -1.9848e-05,  2.2858e-05,  ..., -2.5481e-05,
         -2.0549e-05, -2.1785e-05],
        [-2.2501e-05, -1.6831e-05,  1.7479e-05,  ..., -1.9774e-05,
         -1.6652e-05, -1.4551e-05]], device='cuda:0')
Loss: 0.9801611304283142
Graident accumulation at epoch 1, step 1679, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0287,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.6339e-05,  5.7111e-05, -8.8847e-05,  ..., -1.1023e-04,
         -9.6431e-05, -8.1180e-05],
        [-9.3738e-06, -6.7122e-06,  6.7148e-06,  ..., -8.1891e-06,
         -6.5408e-06, -6.7490e-06],
        [-6.1502e-06,  1.9564e-06,  3.6190e-06,  ..., -4.9850e-06,
          8.6964e-06, -6.2769e-06],
        [ 9.8360e-08,  2.0467e-05, -4.0737e-06,  ...,  3.8687e-06,
          1.2592e-05, -2.9315e-06],
        [-2.4126e-05, -1.8543e-05,  1.7835e-05,  ..., -2.1008e-05,
         -1.8189e-05, -1.6092e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3372e-08, 6.2008e-08, 4.7428e-08,  ..., 4.0536e-08, 1.2883e-07,
         5.4107e-08],
        [7.9751e-11, 5.8770e-11, 1.9851e-11,  ..., 5.6302e-11, 3.5367e-11,
         3.0418e-11],
        [4.1158e-09, 2.8899e-09, 1.3927e-09,  ..., 3.1962e-09, 1.4328e-09,
         1.0603e-09],
        [1.1534e-09, 1.2128e-09, 3.7157e-10,  ..., 1.0094e-09, 6.7672e-10,
         4.5148e-10],
        [3.6423e-10, 2.0779e-10, 8.1724e-11,  ..., 2.7321e-10, 8.2106e-11,
         1.0616e-10]], device='cuda:0')
optimizer state dict: 210.0
lr: [1.995833666043061e-06, 1.995833666043061e-06]
scheduler_last_epoch: 210


Running epoch 1, step 1680, batch 632
Sampled inputs[:2]: tensor([[    0,    12,  1250,  ...,   381,  1524,  2204],
        [    0, 23070,   367,  ...,   287,   790,  3252]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7761e-05, -1.1358e-04, -2.7205e-05,  ...,  1.6695e-04,
          2.2078e-04,  2.0414e-04],
        [-1.2591e-06, -8.2329e-07,  8.6799e-07,  ..., -1.1176e-06,
         -8.1211e-07, -9.6112e-07],
        [-3.9339e-06, -2.6524e-06,  2.9057e-06,  ..., -3.4124e-06,
         -2.5332e-06, -2.8908e-06],
        [-3.7253e-06, -2.3693e-06,  2.6822e-06,  ..., -3.2783e-06,
         -2.3991e-06, -2.9057e-06],
        [-2.9206e-06, -2.0415e-06,  2.1160e-06,  ..., -2.5630e-06,
         -1.9968e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9659193754196167


Running epoch 1, step 1681, batch 633
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0, 18125, 16419,  ...,   278,   638, 11744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0970e-05, -1.7179e-04, -1.3144e-04,  ...,  1.4179e-04,
          2.5184e-04,  3.3589e-04],
        [-2.5034e-06, -1.6950e-06,  1.7919e-06,  ..., -2.2128e-06,
         -1.7062e-06, -1.8328e-06],
        [-7.7784e-06, -5.4687e-06,  5.9158e-06,  ..., -6.7949e-06,
         -5.3197e-06, -5.6028e-06],
        [-7.4953e-06, -4.9770e-06,  5.5879e-06,  ..., -6.5863e-06,
         -5.1111e-06, -5.6773e-06],
        [-5.8413e-06, -4.2766e-06,  4.3362e-06,  ..., -5.1856e-06,
         -4.2468e-06, -3.9190e-06]], device='cuda:0')
Loss: 0.9911829233169556


Running epoch 1, step 1682, batch 634
Sampled inputs[:2]: tensor([[   0,  266, 1790,  ...,  292,   78,  527],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0607e-04, -3.7091e-04, -5.1850e-05,  ...,  6.1531e-05,
          4.6624e-04,  4.0008e-04],
        [-3.8520e-06, -2.5928e-06,  2.7455e-06,  ..., -3.3304e-06,
         -2.5705e-06, -2.7679e-06],
        [-1.1981e-05, -8.4490e-06,  9.1195e-06,  ..., -1.0297e-05,
         -8.0615e-06, -8.5235e-06],
        [-1.1429e-05, -7.5996e-06,  8.5235e-06,  ..., -9.8646e-06,
         -7.6741e-06, -8.5384e-06],
        [-8.8662e-06, -6.5118e-06,  6.5863e-06,  ..., -7.7635e-06,
         -6.3479e-06, -5.8711e-06]], device='cuda:0')
Loss: 0.9642521739006042


Running epoch 1, step 1683, batch 635
Sampled inputs[:2]: tensor([[    0,  1171,   341,  ...,   278, 14713,    18],
        [    0,  2496, 10545,  ...,   287, 13978,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0799e-05, -4.1882e-04, -1.6353e-05,  ...,  5.8570e-05,
          3.9140e-04,  3.7548e-04],
        [-5.0962e-06, -3.5092e-06,  3.7365e-06,  ..., -4.3958e-06,
         -3.4496e-06, -3.5837e-06],
        [-1.5825e-05, -1.1429e-05,  1.2338e-05,  ..., -1.3620e-05,
         -1.0833e-05, -1.1042e-05],
        [-1.5125e-05, -1.0312e-05,  1.1608e-05,  ..., -1.3039e-05,
         -1.0312e-05, -1.1072e-05],
        [-1.1697e-05, -8.8066e-06,  8.9109e-06,  ..., -1.0252e-05,
         -8.5235e-06, -7.5996e-06]], device='cuda:0')
Loss: 0.9871698617935181


Running epoch 1, step 1684, batch 636
Sampled inputs[:2]: tensor([[    0, 10446,    14,  ...,   266,  1164,   287],
        [    0,   741,   266,  ...,   271,  5166,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6092e-05, -4.0358e-04, -8.7334e-05,  ...,  3.3098e-05,
          4.2561e-04,  4.9751e-04],
        [-6.3628e-06, -4.4182e-06,  4.6678e-06,  ..., -5.4613e-06,
         -4.2543e-06, -4.4182e-06],
        [-1.9759e-05, -1.4409e-05,  1.5438e-05,  ..., -1.6928e-05,
         -1.3381e-05, -1.3620e-05],
        [-1.8910e-05, -1.3039e-05,  1.4544e-05,  ..., -1.6257e-05,
         -1.2770e-05, -1.3694e-05],
        [-1.4499e-05, -1.1012e-05,  1.1072e-05,  ..., -1.2666e-05,
         -1.0446e-05, -9.3058e-06]], device='cuda:0')
Loss: 0.9691677689552307


Running epoch 1, step 1685, batch 637
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,    16,    14,  ...,  5148,   259,  1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5065e-05, -3.3415e-04, -1.8599e-04,  ...,  2.0033e-04,
          2.7100e-04,  4.0916e-04],
        [-7.5772e-06, -5.2452e-06,  5.6028e-06,  ..., -6.5491e-06,
         -5.0329e-06, -5.2527e-06],
        [-2.3410e-05, -1.6987e-05,  1.8448e-05,  ..., -2.0161e-05,
         -1.5721e-05, -1.6078e-05],
        [-2.2471e-05, -1.5408e-05,  1.7434e-05,  ..., -1.9431e-05,
         -1.5050e-05, -1.6242e-05],
        [-1.7196e-05, -1.2994e-05,  1.3232e-05,  ..., -1.5080e-05,
         -1.2293e-05, -1.0967e-05]], device='cuda:0')
Loss: 0.9476821422576904


Running epoch 1, step 1686, batch 638
Sampled inputs[:2]: tensor([[    0,  2626,    13,  ...,   300,   369,   259],
        [    0,   292, 17181,  ...,   634,  5039,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3810e-05, -3.4713e-04, -2.3265e-04,  ...,  1.7645e-04,
          2.0955e-04,  2.5795e-04],
        [-8.7768e-06, -6.1132e-06,  6.5118e-06,  ..., -7.5921e-06,
         -5.8487e-06, -6.1356e-06],
        [-2.7001e-05, -1.9774e-05,  2.1443e-05,  ..., -2.3305e-05,
         -1.8299e-05, -1.8641e-05],
        [-2.5958e-05, -1.7941e-05,  2.0266e-05,  ..., -2.2486e-05,
         -1.7539e-05, -1.8924e-05],
        [-1.9804e-05, -1.5125e-05,  1.5348e-05,  ..., -1.7405e-05,
         -1.4275e-05, -1.2673e-05]], device='cuda:0')
Loss: 0.9424946308135986


Running epoch 1, step 1687, batch 639
Sampled inputs[:2]: tensor([[    0,   301,   298,  ..., 10030,   300,  3780],
        [    0,  2314,   266,  ...,   342,  7299,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8822e-05, -4.1786e-04, -2.1663e-04,  ..., -2.7068e-06,
          1.4314e-04,  4.1467e-04],
        [-1.0148e-05, -7.0371e-06,  7.4282e-06,  ..., -8.7693e-06,
         -6.8396e-06, -7.0706e-06],
        [ 1.0882e-04,  1.3027e-04, -7.4541e-05,  ...,  8.7099e-05,
          9.7374e-05,  2.7322e-05],
        [-2.9862e-05, -2.0564e-05,  2.3007e-05,  ..., -2.5854e-05,
         -2.0415e-05, -2.1711e-05],
        [-2.2992e-05, -1.7509e-05,  1.7539e-05,  ..., -2.0236e-05,
         -1.6823e-05, -1.4760e-05]], device='cuda:0')
Loss: 0.9748597741127014
Graident accumulation at epoch 1, step 1687, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.9823e-05,  9.6138e-06, -1.0163e-04,  ..., -9.9475e-05,
         -7.2474e-05, -3.1595e-05],
        [-9.4512e-06, -6.7447e-06,  6.7861e-06,  ..., -8.2471e-06,
         -6.5707e-06, -6.7812e-06],
        [ 5.3472e-06,  1.4788e-05, -4.1969e-06,  ...,  4.2234e-06,
          1.7564e-05, -2.9171e-06],
        [-2.8977e-06,  1.6364e-05, -1.3656e-06,  ...,  8.9650e-07,
          9.2911e-06, -4.8094e-06],
        [-2.4013e-05, -1.8440e-05,  1.7805e-05,  ..., -2.0931e-05,
         -1.8053e-05, -1.5959e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3308e-08, 6.2120e-08, 4.7428e-08,  ..., 4.0495e-08, 1.2872e-07,
         5.4225e-08],
        [7.9774e-11, 5.8760e-11, 1.9886e-11,  ..., 5.6323e-11, 3.5378e-11,
         3.0438e-11],
        [4.1236e-09, 2.9040e-09, 1.3968e-09,  ..., 3.2006e-09, 1.4408e-09,
         1.0600e-09],
        [1.1531e-09, 1.2120e-09, 3.7173e-10,  ..., 1.0090e-09, 6.7646e-10,
         4.5150e-10],
        [3.6440e-10, 2.0788e-10, 8.1949e-11,  ..., 2.7335e-10, 8.2307e-11,
         1.0627e-10]], device='cuda:0')
optimizer state dict: 211.0
lr: [1.922345857645641e-06, 1.922345857645641e-06]
scheduler_last_epoch: 211
Epoch 1 | Batch 639/1048 | Training PPL: 2437.8656281167728 | time 66.19570088386536
Saving checkpoint at epoch 1, step 1687, batch 639
Epoch 1 | Validation PPL: 6.76921243440275 | Learning rate: 1.922345857645641e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1687, AFTER epoch 1, step 1687


Running epoch 1, step 1688, batch 640
Sampled inputs[:2]: tensor([[   0,  395, 5949,  ...,  341,   13,  635],
        [   0, 1197,  729,  ...,  674,  369, 8222]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9938e-05,  1.2829e-06,  2.6564e-05,  ..., -1.0426e-04,
         -2.0215e-05, -2.4162e-04],
        [-1.2442e-06, -8.9407e-07,  9.8348e-07,  ..., -1.0356e-06,
         -8.6799e-07, -8.2701e-07],
        [ 5.4529e-04,  4.7689e-04, -4.3681e-04,  ...,  4.1327e-04,
          4.3559e-04,  1.3418e-04],
        [-3.7402e-06, -2.6822e-06,  3.0994e-06,  ..., -3.1292e-06,
         -2.6524e-06, -2.6077e-06],
        [-2.8163e-06, -2.2650e-06,  2.3097e-06,  ..., -2.4289e-06,
         -2.1607e-06, -1.7732e-06]], device='cuda:0')
Loss: 0.9907683730125427


Running epoch 1, step 1689, batch 641
Sampled inputs[:2]: tensor([[    0,  1270,   413,  ...,   413,   711,    14],
        [    0,  1894,   317,  ...,  9920,    13, 19888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1518e-05,  6.0756e-05,  5.8641e-05,  ..., -1.8550e-04,
          9.4167e-05, -1.8119e-04],
        [-2.5854e-06, -1.8775e-06,  1.8738e-06,  ..., -2.2054e-06,
         -1.8366e-06, -1.7248e-06],
        [ 5.4115e-04,  4.7364e-04, -4.3386e-04,  ...,  4.0961e-04,
          4.3246e-04,  1.3136e-04],
        [-7.5847e-06, -5.5283e-06,  5.7817e-06,  ..., -6.4969e-06,
         -5.5134e-06, -5.3048e-06],
        [-5.9158e-06, -4.7982e-06,  4.4703e-06,  ..., -5.2154e-06,
         -4.6343e-06, -3.7402e-06]], device='cuda:0')
Loss: 0.9857916235923767


Running epoch 1, step 1690, batch 642
Sampled inputs[:2]: tensor([[    0,  6192,   266,  ...,  3318,  9872, 10931],
        [    0,   923,    13,  ...,   199,   677,  3826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9867e-04,  9.4151e-05,  2.1587e-04,  ..., -2.7867e-04,
          2.1475e-04, -2.4203e-04],
        [-3.8445e-06, -2.7530e-06,  2.7232e-06,  ..., -3.3304e-06,
         -2.7306e-06, -2.6152e-06],
        [ 5.3727e-04,  4.7077e-04, -4.3102e-04,  ...,  4.0615e-04,
          4.2965e-04,  1.2863e-04],
        [-1.1325e-05, -8.1360e-06,  8.4341e-06,  ..., -9.8795e-06,
         -8.2552e-06, -8.0764e-06],
        [-8.7917e-06, -7.0035e-06,  6.5267e-06,  ..., -7.8231e-06,
         -6.8396e-06, -5.6326e-06]], device='cuda:0')
Loss: 0.9659731984138489


Running epoch 1, step 1691, batch 643
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,  1253,   287,  ...,  2988,    14,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0189e-04,  1.1784e-04,  2.5934e-04,  ..., -3.3221e-04,
          1.9065e-04, -4.0377e-04],
        [-5.1707e-06, -3.5726e-06,  3.6173e-06,  ..., -4.4703e-06,
         -3.6582e-06, -3.6359e-06],
        [ 5.3328e-04,  4.6819e-04, -4.2810e-04,  ...,  4.0278e-04,
          4.2683e-04,  1.2570e-04],
        [-1.5110e-05, -1.0446e-05,  1.1116e-05,  ..., -1.3143e-05,
         -1.0952e-05, -1.1012e-05],
        [-1.1906e-05, -9.0748e-06,  8.7768e-06,  ..., -1.0505e-05,
         -9.1493e-06, -7.7784e-06]], device='cuda:0')
Loss: 0.9463005065917969


Running epoch 1, step 1692, batch 644
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,   342,  8514,  ...,   266, 46850,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5191e-04,  9.4139e-05,  3.6707e-04,  ..., -3.0090e-04,
          4.8761e-04, -1.9991e-04],
        [-6.4522e-06, -4.5002e-06,  4.5672e-06,  ..., -5.5507e-06,
         -4.5896e-06, -4.4927e-06],
        [ 5.2923e-04,  4.6508e-04, -4.2493e-04,  ...,  3.9933e-04,
          4.2382e-04,  1.2296e-04],
        [-1.8984e-05, -1.3247e-05,  1.4111e-05,  ..., -1.6421e-05,
         -1.3798e-05, -1.3724e-05],
        [-1.4856e-05, -1.1444e-05,  1.1042e-05,  ..., -1.3068e-05,
         -1.1459e-05, -9.6485e-06]], device='cuda:0')
Loss: 0.9986819624900818


Running epoch 1, step 1693, batch 645
Sampled inputs[:2]: tensor([[    0,   461,  4182,  ...,  7461,   292,  4895],
        [    0,  5151,   292,  ..., 13658,   401,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6144e-04,  2.8706e-04,  3.5399e-04,  ..., -3.3016e-04,
          5.1294e-04, -1.5869e-04],
        [-7.7561e-06, -5.3309e-06,  5.4501e-06,  ..., -6.7130e-06,
         -5.5023e-06, -5.4836e-06],
        [ 5.2547e-04,  4.6253e-04, -4.2219e-04,  ...,  3.9602e-04,
          4.2115e-04,  1.2018e-04],
        [-2.2665e-05, -1.5572e-05,  1.6719e-05,  ..., -1.9729e-05,
         -1.6451e-05, -1.6630e-05],
        [-1.7822e-05, -1.3545e-05,  1.3158e-05,  ..., -1.5736e-05,
         -1.3679e-05, -1.1690e-05]], device='cuda:0')
Loss: 0.9484775066375732


Running epoch 1, step 1694, batch 646
Sampled inputs[:2]: tensor([[   0, 2310,  292,  ...,  462,  508,  586],
        [   0, 1867,  300,  ...,  259, 3095, 1842]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9084e-04,  3.6350e-04,  3.9851e-04,  ..., -2.9576e-04,
          5.5197e-04, -2.1616e-04],
        [-8.9109e-06, -6.1654e-06,  6.2846e-06,  ..., -7.7859e-06,
         -6.4149e-06, -6.3330e-06],
        [ 5.2195e-04,  4.5987e-04, -4.1942e-04,  ...,  3.9278e-04,
          4.1841e-04,  1.1765e-04],
        [-2.6181e-05, -1.8090e-05,  1.9431e-05,  ..., -2.3007e-05,
         -1.9252e-05, -1.9327e-05],
        [-2.0519e-05, -1.5691e-05,  1.5244e-05,  ..., -1.8254e-05,
         -1.5914e-05, -1.3486e-05]], device='cuda:0')
Loss: 0.9597603678703308


Running epoch 1, step 1695, batch 647
Sampled inputs[:2]: tensor([[   0, 5902,  518,  ..., 3126,   12,  497],
        [   0,  953,  328,  ..., 2245,   12, 1253]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6254e-04,  4.3916e-04,  4.9383e-04,  ..., -2.9828e-04,
          3.7803e-04, -2.8244e-04],
        [-1.0200e-05, -7.0110e-06,  7.1935e-06,  ..., -8.9481e-06,
         -7.2680e-06, -7.2531e-06],
        [ 5.1802e-04,  4.5721e-04, -4.1647e-04,  ...,  3.8930e-04,
          4.1583e-04,  1.1494e-04],
        [-2.9832e-05, -2.0415e-05,  2.2113e-05,  ..., -2.6286e-05,
         -2.1666e-05, -2.2009e-05],
        [-2.3454e-05, -1.7777e-05,  1.7390e-05,  ..., -2.0906e-05,
         -1.7956e-05, -1.5371e-05]], device='cuda:0')
Loss: 0.9624429941177368
Graident accumulation at epoch 1, step 1695, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.1094e-05,  5.2568e-05, -4.2080e-05,  ..., -1.1936e-04,
         -2.7423e-05, -5.6679e-05],
        [-9.5260e-06, -6.7713e-06,  6.8269e-06,  ..., -8.3172e-06,
         -6.6404e-06, -6.8284e-06],
        [ 5.6615e-05,  5.9030e-05, -4.5424e-05,  ...,  4.2731e-05,
          5.7391e-05,  8.8685e-06],
        [-5.5911e-06,  1.2686e-05,  9.8228e-07,  ..., -1.8217e-06,
          6.1953e-06, -6.5294e-06],
        [-2.3957e-05, -1.8373e-05,  1.7763e-05,  ..., -2.0928e-05,
         -1.8043e-05, -1.5900e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3304e-08, 6.2251e-08, 4.7624e-08,  ..., 4.0544e-08, 1.2873e-07,
         5.4250e-08],
        [7.9798e-11, 5.8751e-11, 1.9918e-11,  ..., 5.6347e-11, 3.5396e-11,
         3.0460e-11],
        [4.3878e-09, 3.1101e-09, 1.5689e-09,  ..., 3.3490e-09, 1.6123e-09,
         1.0721e-09],
        [1.1529e-09, 1.2112e-09, 3.7185e-10,  ..., 1.0087e-09, 6.7625e-10,
         4.5153e-10],
        [3.6458e-10, 2.0799e-10, 8.2170e-11,  ..., 2.7351e-10, 8.2547e-11,
         1.0640e-10]], device='cuda:0')
optimizer state dict: 212.0
lr: [1.8500923853120123e-06, 1.8500923853120123e-06]
scheduler_last_epoch: 212


Running epoch 1, step 1696, batch 648
Sampled inputs[:2]: tensor([[    0,  1336,   287,  ..., 15920,    12,   287],
        [    0,   266,   923,  ...,    14,   298, 12230]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5419e-05, -2.8165e-05, -5.0683e-05,  ..., -1.6458e-04,
         -8.4181e-05, -5.6116e-05],
        [-1.2890e-06, -8.1211e-07,  9.9093e-07,  ..., -1.0729e-06,
         -7.9349e-07, -8.7544e-07],
        [-3.9339e-06, -2.6226e-06,  3.2037e-06,  ..., -3.2634e-06,
         -2.4587e-06, -2.6673e-06],
        [-3.7849e-06, -2.3544e-06,  3.0398e-06,  ..., -3.1292e-06,
         -2.3395e-06, -2.6822e-06],
        [-2.8312e-06, -1.9968e-06,  2.2501e-06,  ..., -2.4140e-06,
         -1.9222e-06, -1.7807e-06]], device='cuda:0')
Loss: 0.948723554611206


Running epoch 1, step 1697, batch 649
Sampled inputs[:2]: tensor([[    0,   266, 20604,  ...,   409, 13764,  6048],
        [    0,  4645,  7688,  ..., 26535,   471,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0894e-04, -1.3967e-04, -1.3740e-04,  ..., -1.8574e-04,
         -3.6975e-04, -2.4958e-04],
        [-2.6077e-06, -1.8030e-06,  1.9670e-06,  ..., -2.1979e-06,
         -1.6801e-06, -1.7025e-06],
        [-8.0168e-06, -5.8860e-06,  6.4224e-06,  ..., -6.7800e-06,
         -5.2899e-06, -5.2452e-06],
        [-7.5996e-06, -5.2601e-06,  6.0201e-06,  ..., -6.4075e-06,
         -4.9770e-06, -5.2005e-06],
        [-5.7220e-06, -4.4256e-06,  4.4852e-06,  ..., -4.9621e-06,
         -4.0829e-06, -3.4720e-06]], device='cuda:0')
Loss: 1.0040479898452759


Running epoch 1, step 1698, batch 650
Sampled inputs[:2]: tensor([[    0, 12987,   609,  ...,   699,  9863,  3227],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8919e-05, -1.0435e-04, -1.0134e-04,  ..., -2.8558e-04,
         -2.9733e-04, -1.3519e-04],
        [-3.8594e-06, -2.7157e-06,  2.9206e-06,  ..., -3.3006e-06,
         -2.6040e-06, -2.5555e-06],
        [-1.1832e-05, -8.8066e-06,  9.5218e-06,  ..., -1.0148e-05,
         -8.1509e-06, -7.8827e-06],
        [-1.1206e-05, -7.8678e-06,  8.9258e-06,  ..., -9.5814e-06,
         -7.6741e-06, -7.8082e-06],
        [-8.5086e-06, -6.6757e-06,  6.6906e-06,  ..., -7.4953e-06,
         -6.3181e-06, -5.2750e-06]], device='cuda:0')
Loss: 0.9542070031166077


Running epoch 1, step 1699, batch 651
Sampled inputs[:2]: tensor([[   0,   14, 6436,  ...,  271, 1211, 8917],
        [   0,  381, 1795,  ...,   12,  344,  593]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7458e-05,  4.7708e-06, -4.0795e-05,  ..., -3.2876e-04,
         -1.1453e-04, -7.9121e-05],
        [-5.1558e-06, -3.6210e-06,  3.8408e-06,  ..., -4.4331e-06,
         -3.5129e-06, -3.3714e-06],
        [-1.5855e-05, -1.1817e-05,  1.2562e-05,  ..., -1.3709e-05,
         -1.1042e-05, -1.0446e-05],
        [-1.5140e-05, -1.0610e-05,  1.1846e-05,  ..., -1.3024e-05,
         -1.0461e-05, -1.0401e-05],
        [-1.1474e-05, -8.9854e-06,  8.8811e-06,  ..., -1.0177e-05,
         -8.5980e-06, -7.0408e-06]], device='cuda:0')
Loss: 0.9991818070411682


Running epoch 1, step 1700, batch 652
Sampled inputs[:2]: tensor([[    0, 21891,     9,  ...,  5216,   717,   287],
        [    0,  1732,   699,  ...,   417,   199,  1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4207e-04, -5.3708e-05, -2.1642e-04,  ..., -3.6759e-04,
         -2.7092e-04, -2.1188e-04],
        [-6.3777e-06, -4.4592e-06,  4.8392e-06,  ..., -5.4911e-06,
         -4.2804e-06, -4.1947e-06],
        [ 6.1296e-05,  6.1397e-05, -4.9296e-05,  ...,  4.5951e-05,
          1.0846e-04,  2.1672e-05],
        [-1.8761e-05, -1.3083e-05,  1.4946e-05,  ..., -1.6168e-05,
         -1.2770e-05, -1.2964e-05],
        [-1.4141e-05, -1.1012e-05,  1.1131e-05,  ..., -1.2532e-05,
         -1.0431e-05, -8.6948e-06]], device='cuda:0')
Loss: 0.9683440327644348


Running epoch 1, step 1701, batch 653
Sampled inputs[:2]: tensor([[    0,   508, 12163,  ...,  4920,   344, 11003],
        [    0,   287, 17044,  ...,   496,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1047e-05, -1.1708e-04, -1.7965e-04,  ..., -3.7982e-04,
         -2.0378e-04, -1.1963e-05],
        [-7.6443e-06, -5.3495e-06,  5.7891e-06,  ..., -6.6161e-06,
         -5.1707e-06, -5.0850e-06],
        [ 5.7451e-05,  5.8536e-05, -4.6227e-05,  ...,  4.2523e-05,
          1.0571e-04,  1.8990e-05],
        [-2.2441e-05, -1.5676e-05,  1.7837e-05,  ..., -1.9476e-05,
         -1.5438e-05, -1.5691e-05],
        [-1.7017e-05, -1.3247e-05,  1.3381e-05,  ..., -1.5125e-05,
         -1.2606e-05, -1.0557e-05]], device='cuda:0')
Loss: 0.9850364327430725


Running epoch 1, step 1702, batch 654
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,   300,   259,  ...,   352, 12080,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0441e-05, -1.4292e-04, -5.3896e-05,  ..., -3.4133e-04,
          6.6368e-05,  1.5670e-04],
        [-8.9332e-06, -6.2399e-06,  6.6645e-06,  ..., -7.7784e-06,
         -6.0610e-06, -6.0052e-06],
        [ 5.3547e-05,  5.5764e-05, -4.3351e-05,  ...,  3.9066e-05,
          1.0303e-04,  1.6278e-05],
        [-2.6166e-05, -1.8209e-05,  2.0504e-05,  ..., -2.2814e-05,
         -1.8030e-05, -1.8448e-05],
        [-1.9997e-05, -1.5438e-05,  1.5542e-05,  ..., -1.7777e-05,
         -1.4767e-05, -1.2465e-05]], device='cuda:0')
Loss: 0.9634044170379639


Running epoch 1, step 1703, batch 655
Sampled inputs[:2]: tensor([[    0,   266,  3574,  ...,  7052,  3829,   292],
        [    0, 14161,  1241,  ..., 15255,   768,  4239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5058e-05, -2.5879e-04, -1.4773e-05,  ..., -5.4766e-04,
          2.7781e-04,  2.9871e-04],
        [-1.0274e-05, -7.0967e-06,  7.5996e-06,  ..., -8.8885e-06,
         -6.9067e-06, -7.0482e-06],
        [ 4.9375e-05,  5.3007e-05, -4.0281e-05,  ...,  3.5669e-05,
          1.0045e-04,  1.3089e-05],
        [-2.9832e-05, -2.0504e-05,  2.3156e-05,  ..., -2.5854e-05,
         -2.0340e-05, -2.1383e-05],
        [-2.3261e-05, -1.7658e-05,  1.7896e-05,  ..., -2.0489e-05,
         -1.6913e-05, -1.4819e-05]], device='cuda:0')
Loss: 0.9553824067115784
Graident accumulation at epoch 1, step 1703, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.4479e-05,  2.1432e-05, -3.9350e-05,  ..., -1.6219e-04,
          3.0997e-06, -2.1139e-05],
        [-9.6009e-06, -6.8039e-06,  6.9042e-06,  ..., -8.3743e-06,
         -6.6671e-06, -6.8504e-06],
        [ 5.5891e-05,  5.8428e-05, -4.4910e-05,  ...,  4.2025e-05,
          6.1697e-05,  9.2906e-06],
        [-8.0152e-06,  9.3670e-06,  3.1997e-06,  ..., -4.2249e-06,
          3.5418e-06, -8.0148e-06],
        [-2.3888e-05, -1.8302e-05,  1.7777e-05,  ..., -2.0884e-05,
         -1.7930e-05, -1.5792e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3239e-08, 6.2256e-08, 4.7577e-08,  ..., 4.0803e-08, 1.2868e-07,
         5.4285e-08],
        [7.9824e-11, 5.8742e-11, 1.9956e-11,  ..., 5.6369e-11, 3.5408e-11,
         3.0479e-11],
        [4.3858e-09, 3.1098e-09, 1.5689e-09,  ..., 3.3469e-09, 1.6208e-09,
         1.0712e-09],
        [1.1526e-09, 1.2104e-09, 3.7201e-10,  ..., 1.0084e-09, 6.7599e-10,
         4.5153e-10],
        [3.6476e-10, 2.0810e-10, 8.2408e-11,  ..., 2.7366e-10, 8.2751e-11,
         1.0652e-10]], device='cuda:0')
optimizer state dict: 213.0
lr: [1.7790842900034565e-06, 1.7790842900034565e-06]
scheduler_last_epoch: 213


Running epoch 1, step 1704, batch 656
Sampled inputs[:2]: tensor([[    0,    20, 13016,  ...,    14,  2743,   516],
        [    0,    12,   221,  ...,   292, 27729,  9837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1132e-05,  1.4754e-04,  3.5585e-05,  ...,  2.0500e-04,
         -1.5301e-04, -1.2118e-04],
        [-1.2368e-06, -7.1898e-07,  9.0525e-07,  ..., -1.1176e-06,
         -8.9034e-07, -9.3505e-07],
        [-3.6061e-06, -2.1905e-06,  2.8461e-06,  ..., -3.1888e-06,
         -2.5779e-06, -2.6077e-06],
        [-3.5614e-06, -2.0117e-06,  2.7567e-06,  ..., -3.2336e-06,
         -2.6077e-06, -2.7716e-06],
        [-2.8610e-06, -1.8179e-06,  2.2054e-06,  ..., -2.5630e-06,
         -2.1756e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9407370686531067


Running epoch 1, step 1705, batch 657
Sampled inputs[:2]: tensor([[   0,  474,  221,  ..., 2945,    9,  287],
        [   0,  822, 5085,  ...,  293, 1608,  391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4720e-04,  3.2548e-04,  3.2932e-04,  ...,  3.0328e-04,
         -3.4690e-04, -2.0071e-04],
        [-2.4959e-06, -1.4268e-06,  1.8962e-06,  ..., -2.1905e-06,
         -1.6429e-06, -1.7509e-06],
        [-7.3463e-06, -4.4405e-06,  5.9903e-06,  ..., -6.3628e-06,
         -4.8429e-06, -4.9919e-06],
        [-7.2420e-06, -4.0382e-06,  5.8115e-06,  ..., -6.3777e-06,
         -4.8131e-06, -5.2601e-06],
        [-5.5879e-06, -3.5688e-06,  4.4405e-06,  ..., -4.9174e-06,
         -3.9637e-06, -3.5167e-06]], device='cuda:0')
Loss: 0.9599418640136719


Running epoch 1, step 1706, batch 658
Sampled inputs[:2]: tensor([[   0,  825, 3066,  ..., 1184,  266, 7964],
        [   0,  515,  266,  ...,   18, 3770, 1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1092e-04,  2.4255e-04,  1.5768e-04,  ...,  2.4196e-04,
         -4.1641e-04,  4.0192e-05],
        [-3.8594e-06, -2.2799e-06,  2.9020e-06,  ..., -3.2932e-06,
         -2.4363e-06, -2.6710e-06],
        [-1.1608e-05, -7.2718e-06,  9.3281e-06,  ..., -9.8050e-06,
         -7.3761e-06, -7.8529e-06],
        [-1.1176e-05, -6.4522e-06,  8.8364e-06,  ..., -9.5516e-06,
         -7.1079e-06, -8.0168e-06],
        [-8.6576e-06, -5.6997e-06,  6.7800e-06,  ..., -7.4655e-06,
         -5.9307e-06, -5.4389e-06]], device='cuda:0')
Loss: 0.965366005897522


Running epoch 1, step 1707, batch 659
Sampled inputs[:2]: tensor([[  0,  12, 271,  ...,  12, 298, 273],
        [  0,  12, 221,  ..., 593, 360, 726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4404e-04,  2.5801e-04,  2.2846e-04,  ...,  3.9705e-04,
         -3.8724e-04,  1.4901e-04],
        [-5.2452e-06, -3.0547e-06,  3.8184e-06,  ..., -4.4331e-06,
         -3.3006e-06, -3.6843e-06],
        [-1.5840e-05, -9.8050e-06,  1.2383e-05,  ..., -1.3262e-05,
         -1.0073e-05, -1.0893e-05],
        [-1.5140e-05, -8.6278e-06,  1.1608e-05,  ..., -1.2815e-05,
         -9.6262e-06, -1.0982e-05],
        [-1.1861e-05, -7.6815e-06,  9.0003e-06,  ..., -1.0133e-05,
         -8.0913e-06, -7.5847e-06]], device='cuda:0')
Loss: 0.9244053363800049


Running epoch 1, step 1708, batch 660
Sampled inputs[:2]: tensor([[    0,   360,   259,  ...,    12,   358,    19],
        [    0,   417,   199,  ...,  9472, 15004,   511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3336e-04,  1.5784e-04,  2.9688e-04,  ...,  6.2980e-04,
         -6.5026e-04, -1.1324e-04],
        [-6.6310e-06, -3.9861e-06,  4.7460e-06,  ..., -5.6028e-06,
         -4.1835e-06, -4.5486e-06],
        [-1.9863e-05, -1.2785e-05,  1.5333e-05,  ..., -1.6719e-05,
         -1.2860e-05, -1.3411e-05],
        [-1.9044e-05, -1.1340e-05,  1.4395e-05,  ..., -1.6168e-05,
         -1.2279e-05, -1.3560e-05],
        [-1.4886e-05, -1.0066e-05,  1.1191e-05,  ..., -1.2800e-05,
         -1.0341e-05, -9.3505e-06]], device='cuda:0')
Loss: 0.9897545576095581


Running epoch 1, step 1709, batch 661
Sampled inputs[:2]: tensor([[   0,  292,  380,  ..., 9636,  417,  199],
        [   0,    9,  342,  ...,   12,  709,  857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4028e-04,  3.2112e-04,  1.1449e-04,  ...,  6.6142e-04,
         -8.2477e-04, -3.0826e-04],
        [-7.8455e-06, -4.8056e-06,  5.6103e-06,  ..., -6.7353e-06,
         -5.0403e-06, -5.4277e-06],
        [-2.3454e-05, -1.5333e-05,  1.8120e-05,  ..., -2.0027e-05,
         -1.5393e-05, -1.5914e-05],
        [-2.2590e-05, -1.3679e-05,  1.7062e-05,  ..., -1.9476e-05,
         -1.4812e-05, -1.6227e-05],
        [-1.7583e-05, -1.2077e-05,  1.3232e-05,  ..., -1.5303e-05,
         -1.2353e-05, -1.1079e-05]], device='cuda:0')
Loss: 0.9464298486709595


Running epoch 1, step 1710, batch 662
Sampled inputs[:2]: tensor([[    0,   287,  2997,  ...,   437,   266,  1040],
        [    0,   287,  1790,  ..., 11367,  9476,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2929e-04,  5.5562e-04,  3.8648e-04,  ...,  7.0637e-04,
         -7.8682e-04, -2.9431e-04],
        [-9.1270e-06, -5.8189e-06,  6.5118e-06,  ..., -7.8827e-06,
         -6.0387e-06, -6.2920e-06],
        [-2.7448e-05, -1.8656e-05,  2.1085e-05,  ..., -2.3633e-05,
         -1.8552e-05, -1.8612e-05],
        [-2.6315e-05, -1.6615e-05,  1.9774e-05,  ..., -2.2843e-05,
         -1.7747e-05, -1.8880e-05],
        [-2.0519e-05, -1.4611e-05,  1.5378e-05,  ..., -1.8001e-05,
         -1.4782e-05, -1.2934e-05]], device='cuda:0')
Loss: 0.9844348430633545


Running epoch 1, step 1711, batch 663
Sampled inputs[:2]: tensor([[   0,  221,  825,  ...,  616, 3661, 8052],
        [   0,  266, 6737,  ..., 2409,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5798e-04,  4.8642e-04,  4.2176e-04,  ...,  6.6747e-04,
         -6.1966e-04, -1.7768e-04],
        [-1.0394e-05, -6.7316e-06,  7.4543e-06,  ..., -8.9854e-06,
         -6.9253e-06, -7.1451e-06],
        [-3.1382e-05, -2.1607e-05,  2.4170e-05,  ..., -2.7031e-05,
         -2.1309e-05, -2.1264e-05],
        [-3.0100e-05, -1.9327e-05,  2.2709e-05,  ..., -2.6137e-05,
         -2.0415e-05, -2.1547e-05],
        [-2.3395e-05, -1.6876e-05,  1.7583e-05,  ..., -2.0549e-05,
         -1.6928e-05, -1.4745e-05]], device='cuda:0')
Loss: 0.9872570037841797
Graident accumulation at epoch 1, step 1711, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4483e-04,  6.7931e-05,  6.7610e-06,  ..., -7.9220e-05,
         -5.9176e-05, -3.6794e-05],
        [-9.6801e-06, -6.7966e-06,  6.9592e-06,  ..., -8.4354e-06,
         -6.6929e-06, -6.8798e-06],
        [ 4.7163e-05,  5.0424e-05, -3.8002e-05,  ...,  3.5119e-05,
          5.3396e-05,  6.2352e-06],
        [-1.0224e-05,  6.4976e-06,  5.1507e-06,  ..., -6.4161e-06,
          1.1462e-06, -9.3680e-06],
        [-2.3838e-05, -1.8159e-05,  1.7757e-05,  ..., -2.0851e-05,
         -1.7830e-05, -1.5687e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4084e-08, 6.2430e-08, 4.7707e-08,  ..., 4.1208e-08, 1.2894e-07,
         5.4263e-08],
        [7.9852e-11, 5.8729e-11, 1.9991e-11,  ..., 5.6394e-11, 3.5421e-11,
         3.0500e-11],
        [4.3824e-09, 3.1072e-09, 1.5679e-09,  ..., 3.3443e-09, 1.6196e-09,
         1.0706e-09],
        [1.1524e-09, 1.2096e-09, 3.7215e-10,  ..., 1.0080e-09, 6.7573e-10,
         4.5155e-10],
        [3.6494e-10, 2.0817e-10, 8.2635e-11,  ..., 2.7381e-10, 8.2954e-11,
         1.0663e-10]], device='cuda:0')
optimizer state dict: 214.0
lr: [1.7093324223767748e-06, 1.7093324223767748e-06]
scheduler_last_epoch: 214


Running epoch 1, step 1712, batch 664
Sampled inputs[:2]: tensor([[   0, 1064,  266,  ..., 2971,  292,  474],
        [   0, 1159,  278,  ...,    9,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9348e-05,  1.6455e-05,  2.8342e-05,  ..., -6.3388e-05,
         -1.3499e-05,  8.9033e-05],
        [-1.2442e-06, -9.0525e-07,  9.5367e-07,  ..., -1.0952e-06,
         -8.7172e-07, -8.2701e-07],
        [-3.9339e-06, -2.9802e-06,  3.1590e-06,  ..., -3.4422e-06,
         -2.7865e-06, -2.5928e-06],
        [-3.6657e-06, -2.6077e-06,  2.9206e-06,  ..., -3.2187e-06,
         -2.5779e-06, -2.5481e-06],
        [-2.8759e-06, -2.2799e-06,  2.2650e-06,  ..., -2.5779e-06,
         -2.1607e-06, -1.7732e-06]], device='cuda:0')
Loss: 0.9908580780029297


Running epoch 1, step 1713, batch 665
Sampled inputs[:2]: tensor([[    0,   452,    13,  ...,   358,    13, 12347],
        [    0, 13649,  7841,  ...,   287,  4713,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2840e-05,  3.7152e-05, -2.8754e-05,  ..., -1.3940e-04,
         -1.9977e-05,  1.9450e-04],
        [-2.5481e-06, -1.7397e-06,  1.8440e-06,  ..., -2.2724e-06,
         -1.7807e-06, -1.7621e-06],
        [-7.7784e-06, -5.6028e-06,  6.0052e-06,  ..., -6.9141e-06,
         -5.5134e-06, -5.3048e-06],
        [-7.3761e-06, -4.9472e-06,  5.6028e-06,  ..., -6.5863e-06,
         -5.2154e-06, -5.3346e-06],
        [-5.8413e-06, -4.3958e-06,  4.3958e-06,  ..., -5.3048e-06,
         -4.3809e-06, -3.7253e-06]], device='cuda:0')
Loss: 0.9652984738349915


Running epoch 1, step 1714, batch 666
Sampled inputs[:2]: tensor([[    0,  8416,   669,  ...,   298,   894,   496],
        [    0, 11325,   278,  ...,   446,  1869,   642]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6881e-04, -3.9267e-05, -6.0201e-05,  ..., -1.8644e-04,
          6.2826e-05,  1.9806e-04],
        [-3.7998e-06, -2.6301e-06,  2.7716e-06,  ..., -3.3602e-06,
         -2.6450e-06, -2.5928e-06],
        [-1.1772e-05, -8.5682e-06,  9.1195e-06,  ..., -1.0371e-05,
         -8.2701e-06, -7.9423e-06],
        [-1.1101e-05, -7.5549e-06,  8.4490e-06,  ..., -9.8348e-06,
         -7.7933e-06, -7.8827e-06],
        [-8.7172e-06, -6.6161e-06,  6.5714e-06,  ..., -7.8529e-06,
         -6.4969e-06, -5.5134e-06]], device='cuda:0')
Loss: 0.9889883995056152


Running epoch 1, step 1715, batch 667
Sampled inputs[:2]: tensor([[    0,  2823,   287,  ...,  3504,     9, 13910],
        [    0,  4665,   909,  ...,  3607,   259,  1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5007e-04, -8.3757e-05, -1.1283e-04,  ..., -1.4297e-04,
          1.0602e-04,  1.3044e-04],
        [-5.0962e-06, -3.4571e-06,  3.7402e-06,  ..., -4.5002e-06,
         -3.5465e-06, -3.4794e-06],
        [-1.5572e-05, -1.1116e-05,  1.2174e-05,  ..., -1.3649e-05,
         -1.0908e-05, -1.0446e-05],
        [-1.4797e-05, -9.8646e-06,  1.1325e-05,  ..., -1.3083e-05,
         -1.0386e-05, -1.0476e-05],
        [-1.1638e-05, -8.6278e-06,  8.8513e-06,  ..., -1.0416e-05,
         -8.6278e-06, -7.3016e-06]], device='cuda:0')
Loss: 0.9832012057304382


Running epoch 1, step 1716, batch 668
Sampled inputs[:2]: tensor([[    0,   677, 25912,  ...,  2337,   292,  4462],
        [    0,   792,   287,  ...,   706,  9751,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7268e-04, -5.6818e-05,  5.8934e-05,  ..., -4.0921e-05,
          3.0235e-04,  2.0170e-04],
        [-6.3404e-06, -4.3064e-06,  4.5747e-06,  ..., -5.6177e-06,
         -4.4405e-06, -4.2953e-06],
        [-1.9252e-05, -1.3888e-05,  1.4901e-05,  ..., -1.6987e-05,
         -1.3664e-05, -1.2860e-05],
        [-1.8358e-05, -1.2308e-05,  1.3828e-05,  ..., -1.6347e-05,
         -1.3083e-05, -1.2919e-05],
        [-1.4380e-05, -1.0759e-05,  1.0788e-05,  ..., -1.2964e-05,
         -1.0818e-05, -8.9630e-06]], device='cuda:0')
Loss: 0.9362524747848511


Running epoch 1, step 1717, batch 669
Sampled inputs[:2]: tensor([[   0,   22, 2577,  ..., 4970,    9, 3868],
        [   0,  957, 1231,  ...,  800,  342, 1398]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5302e-04,  6.6125e-05,  2.1750e-04,  ..., -1.2340e-04,
          5.3739e-04,  2.6808e-04],
        [-7.6666e-06, -5.1223e-06,  5.3681e-06,  ..., -6.8173e-06,
         -5.3607e-06, -5.3011e-06],
        [-2.3276e-05, -1.6496e-05,  1.7509e-05,  ..., -2.0579e-05,
         -1.6466e-05, -1.5825e-05],
        [-2.2173e-05, -1.4618e-05,  1.6198e-05,  ..., -1.9819e-05,
         -1.5780e-05, -1.5885e-05],
        [-1.7539e-05, -1.2830e-05,  1.2785e-05,  ..., -1.5810e-05,
         -1.3098e-05, -1.1154e-05]], device='cuda:0')
Loss: 0.9772988557815552


Running epoch 1, step 1718, batch 670
Sampled inputs[:2]: tensor([[   0,   12, 3570,  ...,  273,  298,  894],
        [   0, 7849,  278,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1514e-04,  5.2265e-05,  1.1336e-04,  ..., -2.1020e-04,
          8.5148e-04,  4.1908e-04],
        [-8.9407e-06, -5.9418e-06,  6.1393e-06,  ..., -7.9796e-06,
         -6.2883e-06, -6.2995e-06],
        [-2.6777e-05, -1.8939e-05,  1.9893e-05,  ..., -2.3723e-05,
         -1.9073e-05, -1.8403e-05],
        [-2.5660e-05, -1.6868e-05,  1.8448e-05,  ..., -2.3037e-05,
         -1.8433e-05, -1.8656e-05],
        [-2.0474e-05, -1.4916e-05,  1.4722e-05,  ..., -1.8463e-05,
         -1.5348e-05, -1.3150e-05]], device='cuda:0')
Loss: 0.8957542777061462


Running epoch 1, step 1719, batch 671
Sampled inputs[:2]: tensor([[    0,   300, 13523,  ..., 42438,   786,  1416],
        [    0,    14,   469,  ...,   367,  2564,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3114e-04,  9.0823e-05,  2.6866e-05,  ..., -2.0371e-04,
          6.7451e-04,  3.5563e-04],
        [-1.0192e-05, -6.7689e-06,  7.0259e-06,  ..., -9.0674e-06,
         -7.1079e-06, -7.1451e-06],
        [-3.0622e-05, -2.1636e-05,  2.2843e-05,  ..., -2.7046e-05,
         -2.1636e-05, -2.0951e-05],
        [-2.9266e-05, -1.9252e-05,  2.1175e-05,  ..., -2.6166e-05,
         -2.0832e-05, -2.1219e-05],
        [-2.3216e-05, -1.6928e-05,  1.6779e-05,  ..., -2.0877e-05,
         -1.7315e-05, -1.4827e-05]], device='cuda:0')
Loss: 0.9613615870475769
Graident accumulation at epoch 1, step 1719, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6346e-04,  7.0220e-05,  8.7714e-06,  ..., -9.1669e-05,
          1.4192e-05,  2.4486e-06],
        [-9.7314e-06, -6.7939e-06,  6.9658e-06,  ..., -8.4986e-06,
         -6.7344e-06, -6.9064e-06],
        [ 3.9385e-05,  4.3218e-05, -3.1917e-05,  ...,  2.8903e-05,
          4.5893e-05,  3.5165e-06],
        [-1.2128e-05,  3.9226e-06,  6.7531e-06,  ..., -8.3911e-06,
         -1.0516e-06, -1.0553e-05],
        [-2.3776e-05, -1.8036e-05,  1.7660e-05,  ..., -2.0853e-05,
         -1.7778e-05, -1.5601e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4119e-08, 6.2376e-08, 4.7660e-08,  ..., 4.1208e-08, 1.2926e-07,
         5.4335e-08],
        [7.9876e-11, 5.8716e-11, 2.0021e-11,  ..., 5.6420e-11, 3.5436e-11,
         3.0520e-11],
        [4.3790e-09, 3.1045e-09, 1.5669e-09,  ..., 3.3416e-09, 1.6184e-09,
         1.0700e-09],
        [1.1521e-09, 1.2088e-09, 3.7223e-10,  ..., 1.0077e-09, 6.7549e-10,
         4.5155e-10],
        [3.6512e-10, 2.0825e-10, 8.2834e-11,  ..., 2.7397e-10, 8.3171e-11,
         1.0674e-10]], device='cuda:0')
optimizer state dict: 215.0
lr: [1.6408474411262143e-06, 1.6408474411262143e-06]
scheduler_last_epoch: 215


Running epoch 1, step 1720, batch 672
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0,   360,   508,  ...,   259,   554,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0608e-04,  4.4191e-06, -1.2013e-04,  ...,  5.6880e-05,
         -1.5245e-05,  1.1089e-04],
        [-1.3039e-06, -8.6427e-07,  9.6858e-07,  ..., -1.1027e-06,
         -8.2701e-07, -7.8604e-07],
        [-3.8147e-06, -2.7418e-06,  3.0845e-06,  ..., -3.2783e-06,
         -2.5481e-06, -2.3246e-06],
        [-3.6657e-06, -2.4587e-06,  2.9206e-06,  ..., -3.1292e-06,
         -2.4140e-06, -2.3246e-06],
        [-2.7567e-06, -2.1011e-06,  2.1905e-06,  ..., -2.4289e-06,
         -2.0117e-06, -1.5572e-06]], device='cuda:0')
Loss: 0.9566420316696167


Running epoch 1, step 1721, batch 673
Sampled inputs[:2]: tensor([[    0,  7926,  6750,  ...,   259,  1524,  6257],
        [    0,   445,     8,  ...,    13, 25386,    17]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0430e-04, -2.1514e-04, -2.3152e-04,  ...,  1.4804e-04,
         -1.7077e-05,  8.3160e-05],
        [-2.5705e-06, -1.6689e-06,  1.9036e-06,  ..., -2.2277e-06,
         -1.6689e-06, -1.6727e-06],
        [-7.7486e-06, -5.3793e-06,  6.1989e-06,  ..., -6.7353e-06,
         -5.1707e-06, -5.0515e-06],
        [-7.4208e-06, -4.8131e-06,  5.8115e-06,  ..., -6.4671e-06,
         -4.9323e-06, -5.0664e-06],
        [-5.6773e-06, -4.1574e-06,  4.4554e-06,  ..., -5.0515e-06,
         -4.0829e-06, -3.4422e-06]], device='cuda:0')
Loss: 0.9571191668510437


Running epoch 1, step 1722, batch 674
Sampled inputs[:2]: tensor([[    0,  3611, 10765,  ...,   271,  4317,    13],
        [    0,  2612,   271,  ...,   369,  9862,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3703e-05, -2.3901e-04, -2.5637e-04,  ...,  1.2181e-04,
         -1.3231e-05,  3.2538e-05],
        [-3.8072e-06, -2.5369e-06,  2.7455e-06,  ..., -3.3900e-06,
         -2.5854e-06, -2.5816e-06],
        [-1.1459e-05, -8.0764e-06,  8.9407e-06,  ..., -1.0163e-05,
         -7.9274e-06, -7.7039e-06],
        [-1.1027e-05, -7.3016e-06,  8.3894e-06,  ..., -9.8497e-06,
         -7.6294e-06, -7.8231e-06],
        [-8.5980e-06, -6.3628e-06,  6.5863e-06,  ..., -7.8082e-06,
         -6.3777e-06, -5.3942e-06]], device='cuda:0')
Loss: 0.9492809176445007


Running epoch 1, step 1723, batch 675
Sampled inputs[:2]: tensor([[    0, 28559,  1357,  ...,  7720,  1398, 41925],
        [    0,    15,    19,  ...,    12,   287,  7897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1184e-05, -1.9052e-04, -2.5131e-04,  ...,  7.3464e-05,
         -3.4401e-05,  4.3951e-05],
        [-5.1036e-06, -3.3751e-06,  3.6769e-06,  ..., -4.5076e-06,
         -3.4869e-06, -3.4608e-06],
        [-1.5423e-05, -1.0833e-05,  1.1995e-05,  ..., -1.3590e-05,
         -1.0759e-05, -1.0401e-05],
        [-1.4842e-05, -9.7752e-06,  1.1280e-05,  ..., -1.3173e-05,
         -1.0356e-05, -1.0550e-05],
        [-1.1429e-05, -8.4341e-06,  8.7172e-06,  ..., -1.0297e-05,
         -8.5235e-06, -7.1749e-06]], device='cuda:0')
Loss: 0.9559392929077148


Running epoch 1, step 1724, batch 676
Sampled inputs[:2]: tensor([[   0,  266, 7407,  ...,  287,  365, 4371],
        [   0,  409, 3669,  ...,   12,  374,   20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3260e-04, -3.0511e-04, -2.5642e-04,  ..., -5.1358e-05,
          1.4288e-04,  2.2458e-04],
        [-6.3926e-06, -4.3139e-06,  4.6119e-06,  ..., -5.6550e-06,
         -4.3921e-06, -4.3586e-06],
        [-1.9386e-05, -1.3947e-05,  1.5080e-05,  ..., -1.7166e-05,
         -1.3664e-05, -1.3188e-05],
        [ 2.2964e-04,  3.7209e-04, -1.5901e-04,  ...,  2.5030e-04,
          2.9248e-04,  1.5658e-04],
        [-1.4365e-05, -1.0818e-05,  1.0952e-05,  ..., -1.2979e-05,
         -1.0803e-05, -9.0972e-06]], device='cuda:0')
Loss: 0.9868634343147278


Running epoch 1, step 1725, batch 677
Sampled inputs[:2]: tensor([[   0, 1268,  278,  ...,  461,  925,  630],
        [   0, 2383, 9843,  ...,  401, 3959,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3351e-04, -1.1718e-04, -2.2382e-04,  ..., -1.4305e-04,
          1.1470e-05,  1.6569e-04],
        [-7.6368e-06, -5.1446e-06,  5.6103e-06,  ..., -6.7279e-06,
         -5.1670e-06, -5.1484e-06],
        [-2.3291e-05, -1.6689e-05,  1.8403e-05,  ..., -2.0549e-05,
         -1.6153e-05, -1.5676e-05],
        [ 2.2590e-04,  3.6962e-04, -1.5587e-04,  ...,  2.4708e-04,
          2.9014e-04,  1.5410e-04],
        [-1.7062e-05, -1.2815e-05,  1.3188e-05,  ..., -1.5363e-05,
         -1.2659e-05, -1.0677e-05]], device='cuda:0')
Loss: 0.9656962752342224


Running epoch 1, step 1726, batch 678
Sampled inputs[:2]: tensor([[   0,  278, 1295,  ..., 4337,  271, 1268],
        [   0, 1603,   12,  ...,   12,  756,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8895e-06, -1.7140e-04, -2.2447e-04,  ...,  1.8315e-04,
         -1.0470e-04,  4.7171e-04],
        [-9.0301e-06, -5.9009e-06,  6.4634e-06,  ..., -7.9051e-06,
         -5.9828e-06, -6.1542e-06],
        [-2.7671e-05, -1.9252e-05,  2.1249e-05,  ..., -2.4289e-05,
         -1.8805e-05, -1.8850e-05],
        [ 2.2179e-04,  3.6738e-04, -1.5328e-04,  ...,  2.4355e-04,
          2.8765e-04,  1.5103e-04],
        [-2.0444e-05, -1.4812e-05,  1.5289e-05,  ..., -1.8284e-05,
         -1.4804e-05, -1.3016e-05]], device='cuda:0')
Loss: 0.9398520588874817


Running epoch 1, step 1727, batch 679
Sampled inputs[:2]: tensor([[   0,   12,  328,  ...,  578,   19,   40],
        [   0,  334,  287,  ..., 1348, 6139,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2763e-04, -1.1284e-04, -1.5796e-04,  ...,  2.0365e-04,
          4.0143e-05,  5.0313e-04],
        [-1.0304e-05, -6.7465e-06,  7.4059e-06,  ..., -9.0227e-06,
         -6.8769e-06, -6.9886e-06],
        [-3.1471e-05, -2.1905e-05,  2.4229e-05,  ..., -2.7627e-05,
         -2.1487e-05, -2.1309e-05],
        [ 2.1807e-04,  3.6494e-04, -1.5040e-04,  ...,  2.4027e-04,
          2.8500e-04,  1.4847e-04],
        [-2.3350e-05, -1.6943e-05,  1.7524e-05,  ..., -2.0877e-05,
         -1.6980e-05, -1.4767e-05]], device='cuda:0')
Loss: 0.9794949293136597
Graident accumulation at epoch 1, step 1727, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5988e-04,  5.1914e-05, -7.9019e-06,  ..., -6.2137e-05,
          1.6787e-05,  5.2517e-05],
        [-9.7886e-06, -6.7891e-06,  7.0098e-06,  ..., -8.5510e-06,
         -6.7486e-06, -6.9146e-06],
        [ 3.2299e-05,  3.6706e-05, -2.6302e-05,  ...,  2.3250e-05,
          3.9155e-05,  1.0340e-06],
        [ 1.0891e-05,  4.0024e-05, -8.9622e-06,  ...,  1.6475e-05,
          2.7553e-05,  5.3491e-06],
        [-2.3733e-05, -1.7927e-05,  1.7646e-05,  ..., -2.0856e-05,
         -1.7699e-05, -1.5518e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4062e-08, 6.2326e-08, 4.7637e-08,  ..., 4.1208e-08, 1.2914e-07,
         5.4534e-08],
        [7.9902e-11, 5.8703e-11, 2.0056e-11,  ..., 5.6445e-11, 3.5448e-11,
         3.0539e-11],
        [4.3756e-09, 3.1019e-09, 1.5659e-09,  ..., 3.3391e-09, 1.6173e-09,
         1.0693e-09],
        [1.1985e-09, 1.3407e-09, 3.9448e-10,  ..., 1.0644e-09, 7.5603e-10,
         4.7314e-10],
        [3.6530e-10, 2.0833e-10, 8.3058e-11,  ..., 2.7413e-10, 8.3376e-11,
         1.0685e-10]], device='cuda:0')
optimizer state dict: 216.0
lr: [1.5736398113547236e-06, 1.5736398113547236e-06]
scheduler_last_epoch: 216


Running epoch 1, step 1728, batch 680
Sampled inputs[:2]: tensor([[    0,   278, 30377,  ...,    13,    83,  2908],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4494e-04,  1.3009e-04,  2.8293e-04,  ..., -4.7534e-05,
          2.8357e-04,  2.3572e-04],
        [-1.3709e-06, -8.1956e-07,  6.5938e-07,  ..., -1.2293e-06,
         -1.0729e-06, -9.6858e-07],
        [-3.8743e-06, -2.5630e-06,  2.2054e-06,  ..., -3.4720e-06,
         -3.1143e-06, -2.6673e-06],
        [-3.8743e-06, -2.3097e-06,  1.9819e-06,  ..., -3.5316e-06,
         -3.1590e-06, -2.8014e-06],
        [-3.1292e-06, -2.0862e-06,  1.6987e-06,  ..., -2.8312e-06,
         -2.5779e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9282599091529846


Running epoch 1, step 1729, batch 681
Sampled inputs[:2]: tensor([[    0,  1266,  2257,  ..., 27146,  1141,  1196],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3080e-04,  8.4415e-05,  1.6391e-04,  ..., -3.5074e-05,
          2.4440e-04,  1.5583e-04],
        [-2.6152e-06, -1.7099e-06,  1.6354e-06,  ..., -2.3246e-06,
         -1.9558e-06, -1.7844e-06],
        [-7.6443e-06, -5.4240e-06,  5.3644e-06,  ..., -6.8098e-06,
         -5.8562e-06, -5.1409e-06],
        [-7.4506e-06, -4.8429e-06,  4.9323e-06,  ..., -6.7055e-06,
         -5.7518e-06, -5.2899e-06],
        [-5.8413e-06, -4.2617e-06,  3.9190e-06,  ..., -5.2899e-06,
         -4.6790e-06, -3.6433e-06]], device='cuda:0')
Loss: 0.9784071445465088


Running epoch 1, step 1730, batch 682
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,  1561,    14,  ...,  4433,   352,  1561]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1976e-05,  2.2551e-04,  1.1876e-04,  ..., -2.4838e-05,
         -3.1249e-05,  1.8956e-04],
        [-3.8967e-06, -2.5444e-06,  2.6189e-06,  ..., -3.4347e-06,
         -2.7865e-06, -2.6636e-06],
        [-1.1668e-05, -8.1211e-06,  8.6278e-06,  ..., -1.0252e-05,
         -8.4639e-06, -7.8529e-06],
        [-1.1235e-05, -7.2569e-06,  7.9721e-06,  ..., -9.9689e-06,
         -8.2105e-06, -7.9572e-06],
        [-8.7023e-06, -6.2585e-06,  6.1840e-06,  ..., -7.7933e-06,
         -6.6608e-06, -5.4389e-06]], device='cuda:0')
Loss: 0.955295205116272


Running epoch 1, step 1731, batch 683
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,  5722, 20126,  ...,  1500,   696,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9094e-05,  1.6353e-04,  1.8953e-04,  ..., -2.9677e-05,
          8.9446e-05,  2.5316e-04],
        [-5.1558e-06, -3.4012e-06,  3.5614e-06,  ..., -4.5598e-06,
         -3.6657e-06, -3.5502e-06],
        [-1.5512e-05, -1.0908e-05,  1.1727e-05,  ..., -1.3664e-05,
         -1.1176e-05, -1.0476e-05],
        [-1.4901e-05, -9.7454e-06,  1.0878e-05,  ..., -1.3262e-05,
         -1.0818e-05, -1.0654e-05],
        [-1.1444e-05, -8.3148e-06,  8.3297e-06,  ..., -1.0252e-05,
         -8.6874e-06, -7.1675e-06]], device='cuda:0')
Loss: 0.9334051012992859


Running epoch 1, step 1732, batch 684
Sampled inputs[:2]: tensor([[    0,   271,  4728,  ...,   344,   259,  1774],
        [    0,  2372,  1319,  ...,  1253,   292, 34166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3421e-04,  2.4059e-04,  1.9913e-04,  ..., -6.9339e-06,
          1.4930e-04,  3.1519e-04],
        [-6.4000e-06, -4.1723e-06,  4.5039e-06,  ..., -5.6252e-06,
         -4.4741e-06, -4.4368e-06],
        [-1.9312e-05, -1.3426e-05,  1.4842e-05,  ..., -1.6898e-05,
         -1.3679e-05, -1.3113e-05],
        [-1.8477e-05, -1.1951e-05,  1.3769e-05,  ..., -1.6332e-05,
         -1.3202e-05, -1.3292e-05],
        [-1.4171e-05, -1.0215e-05,  1.0505e-05,  ..., -1.2621e-05,
         -1.0625e-05, -8.9034e-06]], device='cuda:0')
Loss: 0.913793683052063


Running epoch 1, step 1733, batch 685
Sampled inputs[:2]: tensor([[    0,  1927,   287,  ...,  1027,   271,   266],
        [    0, 13555,    14,  ...,  1067,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2535e-05,  2.4420e-04,  2.3584e-04,  ..., -6.9350e-05,
          1.6895e-05,  2.7173e-04],
        [-7.6741e-06, -5.0552e-06,  5.3532e-06,  ..., -6.8098e-06,
         -5.4240e-06, -5.3793e-06],
        [-2.3276e-05, -1.6257e-05,  1.7688e-05,  ..., -2.0519e-05,
         -1.6600e-05, -1.5974e-05],
        [-2.2277e-05, -1.4529e-05,  1.6406e-05,  ..., -1.9848e-05,
         -1.6034e-05, -1.6183e-05],
        [-1.7256e-05, -1.2480e-05,  1.2651e-05,  ..., -1.5482e-05,
         -1.3024e-05, -1.1005e-05]], device='cuda:0')
Loss: 0.961453914642334


Running epoch 1, step 1734, batch 686
Sampled inputs[:2]: tensor([[   0,  790, 2816,  ...,   14, 1062,  668],
        [   0,  333,  199,  ...,  292,   48, 1792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1423e-04,  3.1089e-04,  1.9664e-04,  ...,  2.7842e-06,
         -6.9543e-05,  3.1851e-04],
        [-8.9705e-06, -5.9158e-06,  6.1467e-06,  ..., -7.9796e-06,
         -6.3181e-06, -6.3032e-06],
        [-2.7388e-05, -1.9193e-05,  2.0400e-05,  ..., -2.4229e-05,
         -1.9476e-05, -1.8895e-05],
        [-2.6181e-05, -1.7121e-05,  1.8850e-05,  ..., -2.3440e-05,
         -1.8820e-05, -1.9118e-05],
        [-2.0236e-05, -1.4685e-05,  1.4573e-05,  ..., -1.8224e-05,
         -1.5199e-05, -1.2986e-05]], device='cuda:0')
Loss: 0.9671770334243774


Running epoch 1, step 1735, batch 687
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,  1725,   271,   266],
        [    0,   669,  1528,  ..., 21826,   259,  5024]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2483e-05,  4.6880e-04,  2.7823e-04,  ..., -7.4411e-05,
         -3.6960e-04,  3.2728e-04],
        [-1.0207e-05, -6.7651e-06,  7.1228e-06,  ..., -9.0674e-06,
         -7.1526e-06, -7.1339e-06],
        [-3.1233e-05, -2.1979e-05,  2.3589e-05,  ..., -2.7627e-05,
         -2.2128e-05, -2.1517e-05],
        [-2.9907e-05, -1.9655e-05,  2.1920e-05,  ..., -2.6718e-05,
         -2.1338e-05, -2.1756e-05],
        [-2.3007e-05, -1.6816e-05,  1.6823e-05,  ..., -2.0757e-05,
         -1.7256e-05, -1.4760e-05]], device='cuda:0')
Loss: 0.9764524102210999
Graident accumulation at epoch 1, step 1735, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5314e-04,  9.3602e-05,  2.0711e-05,  ..., -6.3364e-05,
         -2.1852e-05,  7.9993e-05],
        [-9.8305e-06, -6.7867e-06,  7.0211e-06,  ..., -8.6027e-06,
         -6.7890e-06, -6.9365e-06],
        [ 2.5946e-05,  3.0837e-05, -2.1313e-05,  ...,  1.8162e-05,
          3.3026e-05, -1.2211e-06],
        [ 6.8116e-06,  3.4056e-05, -5.8740e-06,  ...,  1.2156e-05,
          2.2664e-05,  2.6386e-06],
        [-2.3661e-05, -1.7816e-05,  1.7564e-05,  ..., -2.0846e-05,
         -1.7654e-05, -1.5442e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3996e-08, 6.2484e-08, 4.7667e-08,  ..., 4.1173e-08, 1.2914e-07,
         5.4586e-08],
        [7.9927e-11, 5.8690e-11, 2.0086e-11,  ..., 5.6470e-11, 3.5463e-11,
         3.0559e-11],
        [4.3722e-09, 3.0993e-09, 1.5649e-09,  ..., 3.3365e-09, 1.6162e-09,
         1.0687e-09],
        [1.1982e-09, 1.3398e-09, 3.9456e-10,  ..., 1.0641e-09, 7.5573e-10,
         4.7314e-10],
        [3.6546e-10, 2.0841e-10, 8.3258e-11,  ..., 2.7429e-10, 8.3591e-11,
         1.0696e-10]], device='cuda:0')
optimizer state dict: 217.0
lr: [1.5077198029747941e-06, 1.5077198029747941e-06]
scheduler_last_epoch: 217


Running epoch 1, step 1736, batch 688
Sampled inputs[:2]: tensor([[    0,   221, 18844,  ...,   199, 10174,   259],
        [    0,    12,   287,  ...,  3359,  1751,  5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7342e-05, -1.8943e-04, -4.5978e-05,  ..., -3.3663e-05,
          6.2967e-05,  1.5749e-04],
        [-1.2815e-06, -7.6368e-07,  9.5367e-07,  ..., -1.0729e-06,
         -8.4564e-07, -9.3505e-07],
        [-3.9339e-06, -2.4885e-06,  3.1441e-06,  ..., -3.2932e-06,
         -2.6226e-06, -2.8461e-06],
        [-3.8743e-06, -2.2948e-06,  3.0547e-06,  ..., -3.2634e-06,
         -2.5928e-06, -2.9355e-06],
        [-2.9504e-06, -1.9521e-06,  2.2799e-06,  ..., -2.5183e-06,
         -2.1011e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9447946548461914


Running epoch 1, step 1737, batch 689
Sampled inputs[:2]: tensor([[   0, 1039,  259,  ...,  221,  685,  546],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7230e-05, -2.0209e-04,  2.6313e-06,  ...,  6.0697e-05,
          6.6815e-05,  4.1284e-04],
        [-2.6450e-06, -1.4789e-06,  1.7993e-06,  ..., -2.2054e-06,
         -1.5981e-06, -1.9409e-06],
        [-8.1956e-06, -4.9770e-06,  6.0201e-06,  ..., -6.8694e-06,
         -5.1260e-06, -5.9456e-06],
        [-7.8380e-06, -4.3809e-06,  5.6028e-06,  ..., -6.6012e-06,
         -4.8727e-06, -5.9307e-06],
        [-6.0499e-06, -3.8370e-06,  4.3213e-06,  ..., -5.1558e-06,
         -4.0382e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9512653946876526


Running epoch 1, step 1738, batch 690
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0,   995,    13,  ...,  3494,   367,  6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9263e-05, -1.4542e-04,  4.1878e-05,  ...,  3.6581e-05,
          2.8613e-04,  6.7313e-04],
        [-3.8892e-06, -2.4401e-06,  2.5630e-06,  ..., -3.3453e-06,
         -2.6487e-06, -2.8014e-06],
        [-1.2189e-05, -8.2254e-06,  8.6427e-06,  ..., -1.0550e-05,
         -8.5384e-06, -8.7172e-06],
        [-1.1683e-05, -7.3463e-06,  8.0615e-06,  ..., -1.0163e-05,
         -8.1807e-06, -8.7321e-06],
        [-9.1195e-06, -6.4000e-06,  6.2883e-06,  ..., -8.0317e-06,
         -6.7800e-06, -6.1244e-06]], device='cuda:0')
Loss: 0.9969429969787598


Running epoch 1, step 1739, batch 691
Sampled inputs[:2]: tensor([[    0,  1862,   674,  ...,   391,   266,  7688],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1622e-05, -1.7981e-04,  3.1065e-05,  ..., -6.0231e-05,
          4.2501e-04,  6.2036e-04],
        [-5.2229e-06, -3.2485e-06,  3.5390e-06,  ..., -4.4256e-06,
         -3.4384e-06, -3.7104e-06],
        [-1.6391e-05, -1.0937e-05,  1.1951e-05,  ..., -1.3962e-05,
         -1.1086e-05, -1.1593e-05],
        [-1.5616e-05, -9.7305e-06,  1.1101e-05,  ..., -1.3381e-05,
         -1.0550e-05, -1.1489e-05],
        [-1.2115e-05, -8.4117e-06,  8.5831e-06,  ..., -1.0550e-05,
         -8.7619e-06, -8.0466e-06]], device='cuda:0')
Loss: 0.9570289254188538


Running epoch 1, step 1740, batch 692
Sampled inputs[:2]: tensor([[    0,  6294,   367,  ...,   496,    14,    18],
        [    0,   300, 11040,  ...,   266,  1736,  3487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9171e-06, -1.5727e-04, -8.0312e-05,  ..., -1.1211e-04,
          3.3823e-04,  7.1649e-04],
        [-6.5118e-06, -4.0643e-06,  4.4592e-06,  ..., -5.5209e-06,
         -4.3213e-06, -4.5411e-06],
        [-2.0325e-05, -1.3575e-05,  1.4961e-05,  ..., -1.7285e-05,
         -1.3784e-05, -1.4052e-05],
        [-1.9297e-05, -1.2040e-05,  1.3858e-05,  ..., -1.6525e-05,
         -1.3098e-05, -1.3977e-05],
        [-1.4991e-05, -1.0423e-05,  1.0729e-05,  ..., -1.3024e-05,
         -1.0848e-05, -9.7156e-06]], device='cuda:0')
Loss: 0.9592406749725342


Running epoch 1, step 1741, batch 693
Sampled inputs[:2]: tensor([[    0,  6584,   278,  ...,  1039,   965,  1410],
        [    0, 10676,   328,  ...,     9,   360,  2583]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6217e-05, -1.3006e-04, -1.7953e-04,  ..., -6.7045e-05,
          3.8074e-04,  8.9301e-04],
        [-7.8753e-06, -4.9248e-06,  5.3421e-06,  ..., -6.7055e-06,
         -5.2415e-06, -5.5321e-06],
        [-2.4527e-05, -1.6436e-05,  1.7911e-05,  ..., -2.0921e-05,
         -1.6659e-05, -1.7017e-05],
        [-2.3291e-05, -1.4573e-05,  1.6570e-05,  ..., -1.9997e-05,
         -1.5825e-05, -1.6913e-05],
        [-1.8343e-05, -1.2688e-05,  1.2979e-05,  ..., -1.5959e-05,
         -1.3247e-05, -1.1966e-05]], device='cuda:0')
Loss: 0.9740289449691772


Running epoch 1, step 1742, batch 694
Sampled inputs[:2]: tensor([[   0,  278, 4191,  ...,  381, 3020,  352],
        [   0,  287,  271,  ..., 5090,  631, 3276]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4036e-05, -6.8384e-05, -1.8173e-04,  ..., -4.2395e-06,
          2.7701e-04,  8.2140e-04],
        [-9.1344e-06, -5.7220e-06,  6.3255e-06,  ..., -7.8231e-06,
         -6.0089e-06, -6.4000e-06],
        [-2.8402e-05, -1.8969e-05,  2.1130e-05,  ..., -2.4289e-05,
         -1.8984e-05, -1.9610e-05],
        [-2.7001e-05, -1.6883e-05,  1.9610e-05,  ..., -2.3276e-05,
         -1.8090e-05, -1.9550e-05],
        [-2.1175e-05, -1.4596e-05,  1.5259e-05,  ..., -1.8463e-05,
         -1.5073e-05, -1.3724e-05]], device='cuda:0')
Loss: 0.94392329454422


Running epoch 1, step 1743, batch 695
Sampled inputs[:2]: tensor([[    0,    19,   669,  ...,    14,  4053,   352],
        [    0, 16371,    12,  ...,  1296,   680,  1098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9718e-05, -2.6619e-04, -2.1740e-04,  ...,  1.3177e-05,
          5.6072e-04,  8.3245e-04],
        [-1.0401e-05, -6.5193e-06,  7.1451e-06,  ..., -8.9556e-06,
         -6.9141e-06, -7.3500e-06],
        [-3.2157e-05, -2.1487e-05,  2.3827e-05,  ..., -2.7582e-05,
         -2.1681e-05, -2.2337e-05],
        [-3.0622e-05, -1.9133e-05,  2.2084e-05,  ..., -2.6554e-05,
         -2.0742e-05, -2.2352e-05],
        [-2.4095e-05, -1.6592e-05,  1.7285e-05,  ..., -2.1070e-05,
         -1.7263e-05, -1.5691e-05]], device='cuda:0')
Loss: 0.9585899710655212
Graident accumulation at epoch 1, step 1743, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3285e-04,  5.7623e-05, -3.1007e-06,  ..., -5.5710e-05,
          3.6405e-05,  1.5524e-04],
        [-9.8876e-06, -6.7600e-06,  7.0335e-06,  ..., -8.6380e-06,
         -6.8015e-06, -6.9779e-06],
        [ 2.0136e-05,  2.5605e-05, -1.6799e-05,  ...,  1.3588e-05,
          2.7556e-05, -3.3327e-06],
        [ 3.0683e-06,  2.8737e-05, -3.0783e-06,  ...,  8.2851e-06,
          1.8323e-05,  1.3959e-07],
        [-2.3704e-05, -1.7693e-05,  1.7536e-05,  ..., -2.0868e-05,
         -1.7615e-05, -1.5467e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3925e-08, 6.2492e-08, 4.7667e-08,  ..., 4.1132e-08, 1.2933e-07,
         5.5225e-08],
        [7.9955e-11, 5.8674e-11, 2.0117e-11,  ..., 5.6494e-11, 3.5476e-11,
         3.0583e-11],
        [4.3689e-09, 3.0966e-09, 1.5639e-09,  ..., 3.3339e-09, 1.6150e-09,
         1.0682e-09],
        [1.1979e-09, 1.3388e-09, 3.9466e-10,  ..., 1.0637e-09, 7.5541e-10,
         4.7316e-10],
        [3.6568e-10, 2.0847e-10, 8.3473e-11,  ..., 2.7446e-10, 8.3805e-11,
         1.0710e-10]], device='cuda:0')
optimizer state dict: 218.0
lr: [1.4430974891391325e-06, 1.4430974891391325e-06]
scheduler_last_epoch: 218


Running epoch 1, step 1744, batch 696
Sampled inputs[:2]: tensor([[   0,   12,  328,  ...,  908, 1086,   12],
        [   0,   14,   20,  ...,  607, 8386,   88]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6030e-05,  8.5171e-05, -1.4988e-05,  ...,  7.1683e-05,
         -1.8595e-05, -3.9644e-06],
        [-1.2517e-06, -9.0897e-07,  9.7603e-07,  ..., -1.0952e-06,
         -8.7544e-07, -7.8604e-07],
        [-3.9339e-06, -2.9504e-06,  3.2187e-06,  ..., -3.4422e-06,
         -2.7567e-06, -2.4587e-06],
        [-3.7849e-06, -2.7120e-06,  3.0845e-06,  ..., -3.3230e-06,
         -2.6673e-06, -2.4885e-06],
        [-2.7716e-06, -2.1905e-06,  2.2203e-06,  ..., -2.4736e-06,
         -2.0713e-06, -1.6093e-06]], device='cuda:0')
Loss: 0.985008180141449


Running epoch 1, step 1745, batch 697
Sampled inputs[:2]: tensor([[    0,  1265,   328,  ...,  2282, 35414,    13],
        [    0,    14, 30840,  ...,   287,   932,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2940e-04,  1.0883e-04, -3.0561e-05,  ...,  1.8015e-04,
          7.7075e-06,  5.8556e-05],
        [-2.4661e-06, -1.7770e-06,  1.8589e-06,  ..., -2.1681e-06,
         -1.7509e-06, -1.6429e-06],
        [-7.6890e-06, -5.7369e-06,  6.1989e-06,  ..., -6.6757e-06,
         -5.4240e-06, -5.0068e-06],
        [-7.4357e-06, -5.2750e-06,  5.8860e-06,  ..., -6.4969e-06,
         -5.2899e-06, -5.1260e-06],
        [-5.5581e-06, -4.3064e-06,  4.3660e-06,  ..., -4.9025e-06,
         -4.1425e-06, -3.3453e-06]], device='cuda:0')
Loss: 0.979845404624939


Running epoch 1, step 1746, batch 698
Sampled inputs[:2]: tensor([[    0, 28684,   472,  ...,   317,     9,  1926],
        [    0,  3388,   278,  ...,  7203,   271,  1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9192e-05,  1.4264e-04, -1.2689e-04,  ...,  4.7182e-04,
         -1.0381e-04, -1.4793e-05],
        [-3.6955e-06, -2.5779e-06,  2.8051e-06,  ..., -3.2485e-06,
         -2.5891e-06, -2.4997e-06],
        [-1.1384e-05, -8.2999e-06,  9.2834e-06,  ..., -9.8795e-06,
         -7.9721e-06, -7.4804e-06],
        [-1.0982e-05, -7.5847e-06,  8.7768e-06,  ..., -9.6112e-06,
         -7.7635e-06, -7.6890e-06],
        [-8.1956e-06, -6.2287e-06,  6.5267e-06,  ..., -7.2271e-06,
         -6.0797e-06, -4.9621e-06]], device='cuda:0')
Loss: 0.9439690709114075


Running epoch 1, step 1747, batch 699
Sampled inputs[:2]: tensor([[    0,  3308,   259,  ...,    14,  6349,  1389],
        [    0, 16763,  1538,  ...,   631,  3299,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5636e-05,  5.9592e-04, -5.1653e-05,  ...,  5.1605e-04,
         -4.8048e-04, -1.3938e-04],
        [-4.9248e-06, -3.4161e-06,  3.6433e-06,  ..., -4.3586e-06,
         -3.5018e-06, -3.3714e-06],
        [-1.5154e-05, -1.1057e-05,  1.2070e-05,  ..., -1.3292e-05,
         -1.0863e-05, -1.0133e-05],
        [-1.4648e-05, -1.0073e-05,  1.1399e-05,  ..., -1.2979e-05,
         -1.0595e-05, -1.0401e-05],
        [-1.1042e-05, -8.3894e-06,  8.5533e-06,  ..., -9.8646e-06,
         -8.4043e-06, -6.8322e-06]], device='cuda:0')
Loss: 0.951456606388092


Running epoch 1, step 1748, batch 700
Sampled inputs[:2]: tensor([[   0,   21, 1304,  ..., 3577,   13, 2497],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8933e-04,  9.0590e-04,  4.0248e-04,  ...,  4.6142e-04,
         -6.1206e-04, -1.4337e-04],
        [-6.2063e-06, -4.2878e-06,  4.3772e-06,  ..., -5.5060e-06,
         -4.5151e-06, -4.3325e-06],
        [-1.9327e-05, -1.4067e-05,  1.4633e-05,  ..., -1.7047e-05,
         -1.4201e-05, -1.3262e-05],
        [-1.8701e-05, -1.2770e-05,  1.3709e-05,  ..., -1.6659e-05,
         -1.3903e-05, -1.3545e-05],
        [-1.4141e-05, -1.0654e-05,  1.0327e-05,  ..., -1.2726e-05,
         -1.1042e-05, -9.0674e-06]], device='cuda:0')
Loss: 0.9305270910263062


Running epoch 1, step 1749, batch 701
Sampled inputs[:2]: tensor([[   0,  275, 2101,  ..., 1145,  590, 1619],
        [   0,   21,   13,  ...,   14,  747,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2534e-04,  9.3128e-04,  3.8358e-04,  ...,  4.5491e-04,
         -7.2706e-04, -1.4634e-04],
        [-7.5176e-06, -5.1521e-06,  5.3160e-06,  ..., -6.6757e-06,
         -5.3458e-06, -5.1893e-06],
        [-2.3350e-05, -1.6898e-05,  1.7732e-05,  ..., -2.0638e-05,
         -1.6838e-05, -1.5855e-05],
        [-2.2501e-05, -1.5274e-05,  1.6570e-05,  ..., -2.0072e-05,
         -1.6391e-05, -1.6138e-05],
        [-1.7017e-05, -1.2785e-05,  1.2502e-05,  ..., -1.5348e-05,
         -1.3068e-05, -1.0781e-05]], device='cuda:0')
Loss: 0.9865556955337524


Running epoch 1, step 1750, batch 702
Sampled inputs[:2]: tensor([[    0,  3058,   292,  ...,  1387,  1236,   369],
        [    0,   346,   462,  ..., 37683,    14,  1500]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9678e-04,  9.7472e-04,  3.9331e-04,  ...,  7.5525e-04,
         -8.9028e-04, -5.9337e-04],
        [-8.8662e-06, -5.9456e-06,  6.1616e-06,  ..., -7.8902e-06,
         -6.2622e-06, -6.2101e-06],
        [-2.7433e-05, -1.9506e-05,  2.0549e-05,  ..., -2.4289e-05,
         -1.9640e-05, -1.8939e-05],
        [-2.6256e-05, -1.7479e-05,  1.9088e-05,  ..., -2.3454e-05,
         -1.8969e-05, -1.9088e-05],
        [-2.0310e-05, -1.4946e-05,  1.4663e-05,  ..., -1.8343e-05,
         -1.5467e-05, -1.3106e-05]], device='cuda:0')
Loss: 0.9158852696418762


Running epoch 1, step 1751, batch 703
Sampled inputs[:2]: tensor([[    0,   792,    83,  ...,   957, 13285,   271],
        [    0, 36122,  1085,  ...,  6231,     9,  7794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5684e-04,  8.9653e-04,  5.1156e-04,  ...,  8.6413e-04,
         -9.3526e-04, -6.0260e-04],
        [-1.0088e-05, -6.7316e-06,  7.0594e-06,  ..., -9.0152e-06,
         -7.1824e-06, -7.0967e-06],
        [-3.1143e-05, -2.2024e-05,  2.3484e-05,  ..., -2.7657e-05,
         -2.2396e-05, -2.1517e-05],
        [-2.9773e-05, -1.9714e-05,  2.1815e-05,  ..., -2.6718e-05,
         -2.1636e-05, -2.1726e-05],
        [-2.3127e-05, -1.6913e-05,  1.6838e-05,  ..., -2.0936e-05,
         -1.7658e-05, -1.4924e-05]], device='cuda:0')
Loss: 0.9874362945556641
Graident accumulation at epoch 1, step 1751, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6525e-04,  1.4151e-04,  4.8365e-05,  ...,  3.6274e-05,
         -6.0761e-05,  7.9456e-05],
        [-9.9076e-06, -6.7571e-06,  7.0361e-06,  ..., -8.6757e-06,
         -6.8396e-06, -6.9898e-06],
        [ 1.5008e-05,  2.0842e-05, -1.2771e-05,  ...,  9.4632e-06,
          2.2560e-05, -5.1511e-06],
        [-2.1580e-07,  2.3892e-05, -5.8891e-07,  ...,  4.7848e-06,
          1.4327e-05, -2.0470e-06],
        [-2.3647e-05, -1.7615e-05,  1.7466e-05,  ..., -2.0875e-05,
         -1.7619e-05, -1.5413e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4059e-08, 6.3233e-08, 4.7881e-08,  ..., 4.1837e-08, 1.3007e-07,
         5.5533e-08],
        [7.9977e-11, 5.8660e-11, 2.0147e-11,  ..., 5.6519e-11, 3.5492e-11,
         3.0602e-11],
        [4.3655e-09, 3.0940e-09, 1.5629e-09,  ..., 3.3313e-09, 1.6139e-09,
         1.0676e-09],
        [1.1976e-09, 1.3378e-09, 3.9474e-10,  ..., 1.0634e-09, 7.5512e-10,
         4.7316e-10],
        [3.6584e-10, 2.0855e-10, 8.3673e-11,  ..., 2.7462e-10, 8.4033e-11,
         1.0722e-10]], device='cuda:0')
optimizer state dict: 219.0
lr: [1.3797827447013867e-06, 1.3797827447013867e-06]
scheduler_last_epoch: 219


Running epoch 1, step 1752, batch 704
Sampled inputs[:2]: tensor([[   0, 1142,   87,  ..., 2273,  287,  829],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0360e-05, -6.9293e-05, -1.6698e-04,  ...,  1.4306e-04,
          2.3622e-05,  2.9690e-05],
        [-1.2070e-06, -8.1211e-07,  9.2015e-07,  ..., -1.0282e-06,
         -8.1584e-07, -7.8231e-07],
        [-3.7849e-06, -2.6673e-06,  3.0845e-06,  ..., -3.2037e-06,
         -2.5928e-06, -2.4289e-06],
        [-3.5912e-06, -2.3842e-06,  2.8908e-06,  ..., -3.0696e-06,
         -2.4587e-06, -2.4438e-06],
        [-2.7269e-06, -2.0117e-06,  2.1756e-06,  ..., -2.3544e-06,
         -1.9819e-06, -1.6019e-06]], device='cuda:0')
Loss: 0.9711571335792542


Running epoch 1, step 1753, batch 705
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,   292,    36,     9],
        [    0,    14,  3948,  ...,   571, 10097,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6545e-05, -1.2004e-04, -2.1455e-04,  ...,  1.9655e-04,
          4.5882e-05,  1.0110e-04],
        [-2.4587e-06, -1.6540e-06,  1.8217e-06,  ..., -2.1234e-06,
         -1.6503e-06, -1.6689e-06],
        [-7.5996e-06, -5.4091e-06,  6.0648e-06,  ..., -6.5565e-06,
         -5.1856e-06, -5.0962e-06],
        [-7.1824e-06, -4.7684e-06,  5.6177e-06,  ..., -6.2436e-06,
         -4.9025e-06, -5.0813e-06],
        [-5.5879e-06, -4.1276e-06,  4.3362e-06,  ..., -4.8876e-06,
         -4.0382e-06, -3.4422e-06]], device='cuda:0')
Loss: 0.9490215182304382


Running epoch 1, step 1754, batch 706
Sampled inputs[:2]: tensor([[   0,  759, 1128,  ...,  221,  474,  221],
        [   0,  287, 6932,  ..., 1549, 1480,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6264e-04,  4.1205e-05, -2.8727e-04,  ...,  2.3725e-04,
         -1.8642e-04, -1.6875e-05],
        [-3.6806e-06, -2.5444e-06,  2.8051e-06,  ..., -3.1516e-06,
         -2.4736e-06, -2.4550e-06],
        [-1.1444e-05, -8.3894e-06,  9.4175e-06,  ..., -9.8348e-06,
         -7.8380e-06, -7.5847e-06],
        [-1.0893e-05, -7.4655e-06,  8.8066e-06,  ..., -9.3877e-06,
         -7.4506e-06, -7.5996e-06],
        [-8.2105e-06, -6.2436e-06,  6.5565e-06,  ..., -7.1675e-06,
         -5.9754e-06, -4.9993e-06]], device='cuda:0')
Loss: 0.9413228631019592


Running epoch 1, step 1755, batch 707
Sampled inputs[:2]: tensor([[   0, 1746,   14,  ..., 3134, 5968,    9],
        [   0,  266, 2057,  ...,   88, 1801,   66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8374e-04,  2.8442e-06, -2.8858e-04,  ...,  1.4742e-04,
         -6.7128e-07,  4.3090e-05],
        [-4.9099e-06, -3.4124e-06,  3.7104e-06,  ..., -4.2319e-06,
         -3.3416e-06, -3.2857e-06],
        [-1.5259e-05, -1.1235e-05,  1.2457e-05,  ..., -1.3173e-05,
         -1.0595e-05, -1.0148e-05],
        [-1.4618e-05, -1.0073e-05,  1.1697e-05,  ..., -1.2651e-05,
         -1.0133e-05, -1.0222e-05],
        [-1.0982e-05, -8.3894e-06,  8.7023e-06,  ..., -9.6262e-06,
         -8.0913e-06, -6.7130e-06]], device='cuda:0')
Loss: 0.9701080322265625


Running epoch 1, step 1756, batch 708
Sampled inputs[:2]: tensor([[   0,  266, 1784,  ..., 1119, 1276,  292],
        [   0, 7110,  278,  ...,   66,   13, 9070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0820e-04, -1.5227e-04, -4.1614e-04,  ...,  8.4337e-05,
          1.6212e-04,  1.9860e-04],
        [-6.1542e-06, -4.2021e-06,  4.5635e-06,  ..., -5.3495e-06,
         -4.2208e-06, -4.2319e-06],
        [-1.9133e-05, -1.3828e-05,  1.5348e-05,  ..., -1.6570e-05,
         -1.3307e-05, -1.2949e-05],
        [-1.8194e-05, -1.2308e-05,  1.4290e-05,  ..., -1.5855e-05,
         -1.2696e-05, -1.3009e-05],
        [-1.3843e-05, -1.0371e-05,  1.0788e-05,  ..., -1.2174e-05,
         -1.0207e-05, -8.6203e-06]], device='cuda:0')
Loss: 0.9537155628204346


Running epoch 1, step 1757, batch 709
Sampled inputs[:2]: tensor([[    0, 21410, 13160,  ...,   292,    69,    14],
        [    0, 22390,   292,  ...,  3552,   278,   317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0272e-04, -1.1032e-04, -3.6249e-04,  ...,  1.0217e-04,
          4.2269e-04,  3.4657e-04],
        [-7.4208e-06, -5.1707e-06,  5.4501e-06,  ..., -6.5267e-06,
         -5.2340e-06, -5.0664e-06],
        [-2.3156e-05, -1.7047e-05,  1.8343e-05,  ..., -2.0310e-05,
         -1.6540e-05, -1.5602e-05],
        [-2.1949e-05, -1.5154e-05,  1.7047e-05,  ..., -1.9357e-05,
         -1.5706e-05, -1.5616e-05],
        [-1.6868e-05, -1.2860e-05,  1.2979e-05,  ..., -1.5020e-05,
         -1.2740e-05, -1.0476e-05]], device='cuda:0')
Loss: 0.9998548030853271


Running epoch 1, step 1758, batch 710
Sampled inputs[:2]: tensor([[    0,  2027,   365,  ...,   368,  1782,   394],
        [    0, 25939, 47777,  ...,    13,  3483,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0539e-05,  1.0684e-04, -2.0795e-04,  ...,  9.1068e-05,
          4.3153e-04,  3.2344e-04],
        [-8.7991e-06, -5.9791e-06,  6.2101e-06,  ..., -7.6964e-06,
         -6.1765e-06, -6.0946e-06],
        [-2.7537e-05, -1.9848e-05,  2.0981e-05,  ..., -2.4050e-05,
         -1.9655e-05, -1.8805e-05],
        [-2.5943e-05, -1.7479e-05,  1.9327e-05,  ..., -2.2814e-05,
         -1.8552e-05, -1.8656e-05],
        [-2.0266e-05, -1.5080e-05,  1.4931e-05,  ..., -1.7971e-05,
         -1.5289e-05, -1.2845e-05]], device='cuda:0')
Loss: 0.9062991142272949


Running epoch 1, step 1759, batch 711
Sampled inputs[:2]: tensor([[    0,  7240,   365,  ...,   630,   491, 10524],
        [    0,  6022,   644,  ..., 14834,  3554,   591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5839e-05,  2.6030e-04, -1.8552e-04,  ...,  3.9784e-05,
          4.5873e-04,  3.5839e-04],
        [-1.0073e-05, -6.8732e-06,  7.0408e-06,  ..., -8.8438e-06,
         -7.1041e-06, -6.9700e-06],
        [-3.1501e-05, -2.2873e-05,  2.3827e-05,  ..., -2.7657e-05,
         -2.2665e-05, -2.1532e-05],
        [-2.9653e-05, -2.0102e-05,  2.1875e-05,  ..., -2.6196e-05,
         -2.1338e-05, -2.1279e-05],
        [-2.3142e-05, -1.7315e-05,  1.6913e-05,  ..., -2.0623e-05,
         -1.7568e-05, -1.4693e-05]], device='cuda:0')
Loss: 0.9463493824005127
Graident accumulation at epoch 1, step 1759, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4314e-04,  1.5339e-04,  2.4977e-05,  ...,  3.6625e-05,
         -8.8121e-06,  1.0735e-04],
        [-9.9242e-06, -6.7687e-06,  7.0366e-06,  ..., -8.6925e-06,
         -6.8661e-06, -6.9878e-06],
        [ 1.0357e-05,  1.6471e-05, -9.1112e-06,  ...,  5.7512e-06,
          1.8038e-05, -6.7892e-06],
        [-3.1596e-06,  1.9493e-05,  1.6575e-06,  ...,  1.6867e-06,
          1.0761e-05, -3.9701e-06],
        [-2.3596e-05, -1.7585e-05,  1.7411e-05,  ..., -2.0850e-05,
         -1.7614e-05, -1.5341e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3988e-08, 6.3238e-08, 4.7867e-08,  ..., 4.1797e-08, 1.3015e-07,
         5.5606e-08],
        [7.9998e-11, 5.8649e-11, 2.0176e-11,  ..., 5.6541e-11, 3.5507e-11,
         3.0620e-11],
        [4.3621e-09, 3.0915e-09, 1.5619e-09,  ..., 3.3288e-09, 1.6128e-09,
         1.0670e-09],
        [1.1973e-09, 1.3369e-09, 3.9482e-10,  ..., 1.0630e-09, 7.5482e-10,
         4.7314e-10],
        [3.6601e-10, 2.0864e-10, 8.3876e-11,  ..., 2.7477e-10, 8.4258e-11,
         1.0733e-10]], device='cuda:0')
optimizer state dict: 220.0
lr: [1.3177852447071903e-06, 1.3177852447071903e-06]
scheduler_last_epoch: 220


Running epoch 1, step 1760, batch 712
Sampled inputs[:2]: tensor([[   0,  767, 1953,  ...,   14, 1364,   12],
        [   0, 4323, 8213,  ..., 1153,  278, 4258]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0032e-05, -6.1704e-06, -6.7498e-05,  ..., -9.0451e-05,
          3.0056e-04,  1.8424e-04],
        [-1.2740e-06, -7.9349e-07,  8.0094e-07,  ..., -1.1325e-06,
         -8.4192e-07, -8.5309e-07],
        [-3.8743e-06, -2.6375e-06,  2.7120e-06,  ..., -3.4571e-06,
         -2.6524e-06, -2.5779e-06],
        [-3.7253e-06, -2.3544e-06,  2.4885e-06,  ..., -3.3677e-06,
         -2.5928e-06, -2.6077e-06],
        [-2.8163e-06, -1.9968e-06,  1.9073e-06,  ..., -2.5630e-06,
         -2.0564e-06, -1.7211e-06]], device='cuda:0')
Loss: 0.936704158782959


Running epoch 1, step 1761, batch 713
Sampled inputs[:2]: tensor([[    0,  7294, 23782,  ...,   471, 11528,  3437],
        [    0, 20596,  2943,  ...,  5560,  2512,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0292e-05,  4.3986e-05, -5.6389e-05,  ..., -1.5838e-04,
          2.6002e-04,  2.3298e-04],
        [-2.5183e-06, -1.6578e-06,  1.7099e-06,  ..., -2.2426e-06,
         -1.7546e-06, -1.7174e-06],
        [-7.8678e-06, -5.5581e-06,  5.8413e-06,  ..., -7.0184e-06,
         -5.6177e-06, -5.3495e-06],
        [-7.4804e-06, -4.9323e-06,  5.3793e-06,  ..., -6.7204e-06,
         -5.3644e-06, -5.3197e-06],
        [-5.6922e-06, -4.1872e-06,  4.0978e-06,  ..., -5.1707e-06,
         -4.3064e-06, -3.5688e-06]], device='cuda:0')
Loss: 0.964246928691864


Running epoch 1, step 1762, batch 714
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4445e-06,  1.1036e-04, -2.7896e-05,  ..., -1.6906e-04,
          1.9104e-05,  5.1244e-05],
        [-3.7402e-06, -2.4475e-06,  2.6934e-06,  ..., -3.2932e-06,
         -2.5555e-06, -2.5555e-06],
        [-1.1668e-05, -8.1509e-06,  9.0897e-06,  ..., -1.0282e-05,
         -8.1509e-06, -7.9125e-06],
        [-1.1086e-05, -7.2271e-06,  8.4490e-06,  ..., -9.8199e-06,
         -7.7486e-06, -7.8827e-06],
        [-8.3447e-06, -6.1095e-06,  6.3181e-06,  ..., -7.5102e-06,
         -6.2287e-06, -5.2378e-06]], device='cuda:0')
Loss: 0.9517703652381897


Running epoch 1, step 1763, batch 715
Sampled inputs[:2]: tensor([[    0,    12,   696,  ..., 14275,  2661,  6129],
        [    0,    12,  3518,  ...,  1580,  2573,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9008e-05,  1.6930e-04, -9.4193e-06,  ..., -1.4963e-04,
          1.8432e-04,  1.3769e-04],
        [-5.0440e-06, -3.3192e-06,  3.5428e-06,  ..., -4.4405e-06,
         -3.4831e-06, -3.4943e-06],
        [-1.5810e-05, -1.1101e-05,  1.2010e-05,  ..., -1.3918e-05,
         -1.1101e-05, -1.0863e-05],
        [-1.4856e-05, -9.7305e-06,  1.1012e-05,  ..., -1.3188e-05,
         -1.0490e-05, -1.0699e-05],
        [-1.1444e-05, -8.3894e-06,  8.4490e-06,  ..., -1.0282e-05,
         -8.5682e-06, -7.2792e-06]], device='cuda:0')
Loss: 0.9545237421989441


Running epoch 1, step 1764, batch 716
Sampled inputs[:2]: tensor([[    0,  4073,  1548,  ...,   292,   221,   301],
        [    0, 13856,   278,  ...,    14,    69,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8538e-06,  1.2404e-04, -2.5254e-05,  ..., -1.0298e-04,
          1.2922e-04,  3.9652e-05],
        [-6.2808e-06, -4.1984e-06,  4.5486e-06,  ..., -5.5283e-06,
         -4.3288e-06, -4.3437e-06],
        [-1.9684e-05, -1.4022e-05,  1.5333e-05,  ..., -1.7330e-05,
         -1.3813e-05, -1.3486e-05],
        [-1.8507e-05, -1.2323e-05,  1.4126e-05,  ..., -1.6421e-05,
         -1.3039e-05, -1.3307e-05],
        [-1.4126e-05, -1.0520e-05,  1.0699e-05,  ..., -1.2711e-05,
         -1.0580e-05, -8.9779e-06]], device='cuda:0')
Loss: 0.9488555192947388


Running epoch 1, step 1765, batch 717
Sampled inputs[:2]: tensor([[    0,   631,   516,  ..., 13374,   898,   266],
        [    0,  6418,   446,  ...,   413,    29,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2879e-05,  3.1046e-04, -7.0770e-05,  ..., -8.9757e-05,
          1.5171e-04, -1.6373e-04],
        [-7.5400e-06, -5.1148e-06,  5.3532e-06,  ..., -6.6310e-06,
         -5.3421e-06, -5.2042e-06],
        [ 3.8550e-05,  3.9328e-05, -1.5788e-05,  ...,  3.0748e-05,
          5.2549e-05,  1.1199e-05],
        [-2.2441e-05, -1.5154e-05,  1.6630e-05,  ..., -1.9938e-05,
         -1.6361e-05, -1.6108e-05],
        [-1.7032e-05, -1.2830e-05,  1.2591e-05,  ..., -1.5348e-05,
         -1.3113e-05, -1.0870e-05]], device='cuda:0')
Loss: 0.9653688073158264


Running epoch 1, step 1766, batch 718
Sampled inputs[:2]: tensor([[   0,  726, 8241,  ...,  266, 5994,    9],
        [   0,   21,   66,  ..., 1377,  278, 1634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8277e-05,  2.9269e-04, -1.2251e-04,  ..., -2.1656e-05,
          3.2654e-04, -1.8257e-05],
        [-8.9630e-06, -5.9605e-06,  6.2250e-06,  ..., -7.8753e-06,
         -6.3479e-06, -6.2473e-06],
        [ 3.4050e-05,  3.6467e-05, -1.2852e-05,  ...,  2.6844e-05,
          4.9345e-05,  7.9055e-06],
        [-2.6464e-05, -1.7539e-05,  1.9178e-05,  ..., -2.3469e-05,
         -1.9237e-05, -1.9163e-05],
        [-2.0549e-05, -1.5095e-05,  1.4812e-05,  ..., -1.8477e-05,
         -1.5736e-05, -1.3329e-05]], device='cuda:0')
Loss: 0.9801518321037292


Running epoch 1, step 1767, batch 719
Sampled inputs[:2]: tensor([[    0, 13642, 14635,  ...,   367,  1040,  8580],
        [    0,  1304,   292,  ...,  2101,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1977e-04,  4.2284e-04, -2.8602e-04,  ..., -9.1629e-05,
          3.1878e-04, -1.4305e-04],
        [-1.0237e-05, -6.8657e-06,  7.0892e-06,  ..., -9.0227e-06,
         -7.3388e-06, -7.1488e-06],
        [ 3.0056e-05,  3.3486e-05, -9.9465e-06,  ...,  2.3268e-05,
          4.6246e-05,  5.1339e-06],
        [-3.0220e-05, -2.0161e-05,  2.1830e-05,  ..., -2.6852e-05,
         -2.2188e-05, -2.1935e-05],
        [-2.3499e-05, -1.7390e-05,  1.6913e-05,  ..., -2.1175e-05,
         -1.8165e-05, -1.5229e-05]], device='cuda:0')
Loss: 0.9575357437133789
Graident accumulation at epoch 1, step 1767, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5080e-04,  1.8034e-04, -6.1228e-06,  ...,  2.3799e-05,
          2.3947e-05,  8.2309e-05],
        [-9.9555e-06, -6.7784e-06,  7.0419e-06,  ..., -8.7255e-06,
         -6.9133e-06, -7.0039e-06],
        [ 1.2327e-05,  1.8172e-05, -9.1947e-06,  ...,  7.5029e-06,
          2.0859e-05, -5.5969e-06],
        [-5.8656e-06,  1.5527e-05,  3.6747e-06,  ..., -1.1671e-06,
          7.4660e-06, -5.7666e-06],
        [-2.3586e-05, -1.7566e-05,  1.7361e-05,  ..., -2.0882e-05,
         -1.7669e-05, -1.5329e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3963e-08, 6.3354e-08, 4.7901e-08,  ..., 4.1763e-08, 1.3013e-07,
         5.5570e-08],
        [8.0023e-11, 5.8637e-11, 2.0206e-11,  ..., 5.6565e-11, 3.5525e-11,
         3.0641e-11],
        [4.3586e-09, 3.0895e-09, 1.5604e-09,  ..., 3.3260e-09, 1.6133e-09,
         1.0659e-09],
        [1.1970e-09, 1.3360e-09, 3.9490e-10,  ..., 1.0627e-09, 7.5456e-10,
         4.7315e-10],
        [3.6620e-10, 2.0873e-10, 8.4078e-11,  ..., 2.7495e-10, 8.4503e-11,
         1.0745e-10]], device='cuda:0')
optimizer state dict: 221.0
lr: [1.2571144629157273e-06, 1.2571144629157273e-06]
scheduler_last_epoch: 221


Running epoch 1, step 1768, batch 720
Sampled inputs[:2]: tensor([[    0,   437, 38603,  ..., 37253, 10432,   278],
        [    0,  3804,   300,  ...,  5062,  9848,  3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4117e-05, -2.7125e-04, -1.6574e-04,  ..., -1.6452e-04,
          8.5001e-05,  1.5383e-04],
        [-1.2368e-06, -8.6799e-07,  9.2015e-07,  ..., -1.0729e-06,
         -8.9407e-07, -8.0466e-07],
        [-3.9339e-06, -2.9206e-06,  3.1143e-06,  ..., -3.4422e-06,
         -2.8759e-06, -2.5779e-06],
        [-3.7104e-06, -2.5928e-06,  2.8908e-06,  ..., -3.2336e-06,
         -2.6971e-06, -2.5481e-06],
        [-2.7716e-06, -2.1607e-06,  2.1458e-06,  ..., -2.4736e-06,
         -2.1458e-06, -1.6838e-06]], device='cuda:0')
Loss: 0.9968264698982239


Running epoch 1, step 1769, batch 721
Sampled inputs[:2]: tensor([[    0, 13706,  1862,  ...,   275,  1036, 42948],
        [    0,   278,   266,  ...,  5503,   259,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0562e-05, -4.8927e-04, -3.6748e-04,  ..., -1.1531e-04,
          2.5657e-04,  3.2421e-04],
        [-2.6301e-06, -1.6503e-06,  1.9260e-06,  ..., -2.2277e-06,
         -1.7360e-06, -1.8701e-06],
        [-8.2850e-06, -5.5432e-06,  6.4820e-06,  ..., -7.0333e-06,
         -5.5581e-06, -5.8115e-06],
        [-7.5847e-06, -4.7833e-06,  5.8711e-06,  ..., -6.4373e-06,
         -5.0664e-06, -5.5432e-06],
        [-6.0648e-06, -4.1425e-06,  4.5598e-06,  ..., -5.2303e-06,
         -4.2766e-06, -4.0233e-06]], device='cuda:0')
Loss: 0.9443502426147461


Running epoch 1, step 1770, batch 722
Sampled inputs[:2]: tensor([[   0,  271, 5738,  ...,   12,   21, 9023],
        [   0,   45, 6556,  ..., 1477,  352, 1611]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3695e-05, -5.6821e-04, -4.2656e-04,  ..., -1.3673e-04,
          4.2614e-04,  3.2056e-04],
        [-3.9786e-06, -2.5369e-06,  2.8051e-06,  ..., -3.4198e-06,
         -2.6897e-06, -2.8200e-06],
        [-1.2517e-05, -8.4937e-06,  9.4473e-06,  ..., -1.0774e-05,
         -8.6278e-06, -8.7917e-06],
        [-1.1548e-05, -7.3761e-06,  8.5682e-06,  ..., -9.9689e-06,
         -7.9274e-06, -8.4490e-06],
        [-9.2536e-06, -6.4373e-06,  6.7353e-06,  ..., -8.1211e-06,
         -6.7204e-06, -6.1542e-06]], device='cuda:0')
Loss: 0.985510528087616


Running epoch 1, step 1771, batch 723
Sampled inputs[:2]: tensor([[    0,   965,   300,  ...,   546,   857,  4350],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9166e-04, -5.7739e-04, -4.4606e-04,  ..., -1.7398e-04,
          4.5136e-04,  3.9816e-04],
        [-5.2154e-06, -3.4273e-06,  3.7029e-06,  ..., -4.5598e-06,
         -3.5651e-06, -3.7216e-06],
        [-1.6540e-05, -1.1548e-05,  1.2606e-05,  ..., -1.4484e-05,
         -1.1519e-05, -1.1712e-05],
        [-1.5214e-05, -1.0014e-05,  1.1370e-05,  ..., -1.3381e-05,
         -1.0565e-05, -1.1221e-05],
        [-1.2055e-05, -8.6427e-06,  8.8811e-06,  ..., -1.0744e-05,
         -8.8662e-06, -8.0392e-06]], device='cuda:0')
Loss: 0.9534890055656433


Running epoch 1, step 1772, batch 724
Sampled inputs[:2]: tensor([[  0,  25,  26,  ...,   9, 287, 298],
        [  0,  69, 462,  ..., 437, 266, 634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4614e-04, -4.8644e-04, -4.5586e-04,  ..., -1.7215e-04,
          5.8628e-04,  6.0270e-04],
        [-6.4969e-06, -4.2468e-06,  4.4890e-06,  ..., -5.7742e-06,
         -4.5411e-06, -4.6976e-06],
        [-2.0474e-05, -1.4186e-05,  1.5289e-05,  ..., -1.8120e-05,
         -1.4499e-05, -1.4573e-05],
        [-1.8954e-05, -1.2383e-05,  1.3784e-05,  ..., -1.6943e-05,
         -1.3486e-05, -1.4141e-05],
        [-1.5095e-05, -1.0714e-05,  1.0908e-05,  ..., -1.3590e-05,
         -1.1265e-05, -1.0096e-05]], device='cuda:0')
Loss: 0.9372747540473938


Running epoch 1, step 1773, batch 725
Sampled inputs[:2]: tensor([[    0,   287,  2503,  ...,   496,    14, 37791],
        [    0,    13,    19,  ..., 22111,  2489,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3313e-04, -3.2205e-04, -4.2587e-04,  ..., -1.3776e-04,
          5.8792e-04,  7.0216e-04],
        [-7.6890e-06, -5.1372e-06,  5.3681e-06,  ..., -6.8918e-06,
         -5.4464e-06, -5.5172e-06],
        [-2.4319e-05, -1.7241e-05,  1.8373e-05,  ..., -2.1741e-05,
         -1.7494e-05, -1.7211e-05],
        [-2.2456e-05, -1.5020e-05,  1.6525e-05,  ..., -2.0251e-05,
         -1.6212e-05, -1.6659e-05],
        [-1.7792e-05, -1.2934e-05,  1.3024e-05,  ..., -1.6168e-05,
         -1.3486e-05, -1.1802e-05]], device='cuda:0')
Loss: 0.9682778716087341


Running epoch 1, step 1774, batch 726
Sampled inputs[:2]: tensor([[    0, 12472,  1059,  ...,   642,   365,  6517],
        [    0,    11,   360,  ...,  4524,  1553,   401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8054e-04, -4.7971e-04, -4.6989e-04,  ..., -1.7611e-04,
          6.8911e-04,  7.1520e-04],
        [-8.9183e-06, -6.0610e-06,  6.3144e-06,  ..., -7.9721e-06,
         -6.3553e-06, -6.3181e-06],
        [-2.8282e-05, -2.0370e-05,  2.1607e-05,  ..., -2.5228e-05,
         -2.0444e-05, -1.9804e-05],
        [-2.6181e-05, -1.7822e-05,  1.9535e-05,  ..., -2.3529e-05,
         -1.8984e-05, -1.9208e-05],
        [-2.0593e-05, -1.5229e-05,  1.5259e-05,  ..., -1.8671e-05,
         -1.5676e-05, -1.3515e-05]], device='cuda:0')
Loss: 0.9782282710075378


Running epoch 1, step 1775, batch 727
Sampled inputs[:2]: tensor([[   0,  598,  278,  ...,  437,  266, 2388],
        [   0,  221,  474,  ...,  266, 2025,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8733e-04, -2.1139e-04, -4.3204e-04,  ..., -1.0432e-04,
          3.3876e-04,  6.2192e-04],
        [-1.0155e-05, -6.7838e-06,  7.2531e-06,  ..., -9.0376e-06,
         -7.0781e-06, -7.2010e-06],
        [-3.2157e-05, -2.2799e-05,  2.4766e-05,  ..., -2.8566e-05,
         -2.2769e-05, -2.2516e-05],
        [-2.9832e-05, -1.9923e-05,  2.2456e-05,  ..., -2.6703e-05,
         -2.1175e-05, -2.1905e-05],
        [-2.3216e-05, -1.6958e-05,  1.7345e-05,  ..., -2.0981e-05,
         -1.7375e-05, -1.5214e-05]], device='cuda:0')
Loss: 0.9107154011726379
Graident accumulation at epoch 1, step 1775, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1699e-04,  1.4116e-04, -4.8714e-05,  ...,  1.0987e-05,
          5.5428e-05,  1.3627e-04],
        [-9.9754e-06, -6.7790e-06,  7.0630e-06,  ..., -8.7567e-06,
         -6.9298e-06, -7.0236e-06],
        [ 7.8785e-06,  1.4075e-05, -5.7987e-06,  ...,  3.8960e-06,
          1.6496e-05, -7.2888e-06],
        [-8.2622e-06,  1.1982e-05,  5.5529e-06,  ..., -3.7207e-06,
          4.6019e-06, -7.3804e-06],
        [-2.3549e-05, -1.7505e-05,  1.7359e-05,  ..., -2.0892e-05,
         -1.7640e-05, -1.5318e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3924e-08, 6.3335e-08, 4.8040e-08,  ..., 4.1733e-08, 1.3011e-07,
         5.5902e-08],
        [8.0046e-11, 5.8625e-11, 2.0239e-11,  ..., 5.6591e-11, 3.5540e-11,
         3.0662e-11],
        [4.3553e-09, 3.0869e-09, 1.5595e-09,  ..., 3.3235e-09, 1.6122e-09,
         1.0654e-09],
        [1.1967e-09, 1.3350e-09, 3.9501e-10,  ..., 1.0623e-09, 7.5425e-10,
         4.7316e-10],
        [3.6637e-10, 2.0881e-10, 8.4295e-11,  ..., 2.7511e-10, 8.4721e-11,
         1.0757e-10]], device='cuda:0')
optimizer state dict: 222.0
lr: [1.1977796703520529e-06, 1.1977796703520529e-06]
scheduler_last_epoch: 222


Running epoch 1, step 1776, batch 728
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,  2734,  2703,  ...,  7851,   280,  1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4141e-05, -4.5604e-05,  5.4002e-05,  ..., -9.6047e-05,
          2.3239e-04,  4.4256e-05],
        [-1.2070e-06, -8.6799e-07,  7.4506e-07,  ..., -1.1101e-06,
         -9.9093e-07, -9.1642e-07],
        [-3.9041e-06, -2.8908e-06,  2.6971e-06,  ..., -3.4869e-06,
         -3.1590e-06, -2.8163e-06],
        [-3.6210e-06, -2.5332e-06,  2.3693e-06,  ..., -3.3081e-06,
         -2.9951e-06, -2.8014e-06],
        [-2.8610e-06, -2.1756e-06,  1.9372e-06,  ..., -2.6226e-06,
         -2.4587e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9618185758590698


Running epoch 1, step 1777, batch 729
Sampled inputs[:2]: tensor([[   0, 2546,  300,  ...,   14, 1075,  756],
        [   0,  266, 1658,  ...,  278, 1083, 5993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3573e-04, -4.5604e-05,  7.3903e-05,  ..., -1.4872e-04,
          3.7061e-04,  1.3363e-04],
        [-2.4810e-06, -1.7323e-06,  1.5572e-06,  ..., -2.2650e-06,
         -1.9148e-06, -1.8924e-06],
        [-7.8678e-06, -5.6773e-06,  5.4538e-06,  ..., -7.0035e-06,
         -5.9605e-06, -5.7667e-06],
        [-7.2867e-06, -4.9472e-06,  4.7982e-06,  ..., -6.6459e-06,
         -5.6624e-06, -5.6922e-06],
        [-5.8860e-06, -4.3511e-06,  3.9786e-06,  ..., -5.3495e-06,
         -4.7088e-06, -3.9786e-06]], device='cuda:0')
Loss: 0.9315738677978516


Running epoch 1, step 1778, batch 730
Sampled inputs[:2]: tensor([[   0,  741, 2985,  ...,  199,  769,  278],
        [   0,  944,  278,  ..., 2374,  699, 8867]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1711e-04,  6.0665e-05,  1.6722e-04,  ..., -2.3331e-04,
          4.9541e-04,  7.1577e-05],
        [-3.6806e-06, -2.5742e-06,  2.4550e-06,  ..., -3.3379e-06,
         -2.8461e-06, -2.7381e-06],
        [-1.1742e-05, -8.5533e-06,  8.5086e-06,  ..., -1.0461e-05,
         -8.9556e-06, -8.4937e-06],
        [-1.0923e-05, -7.4655e-06,  7.6294e-06,  ..., -9.8944e-06,
         -8.4937e-06, -8.3596e-06],
        [-8.6129e-06, -6.4671e-06,  6.0797e-06,  ..., -7.8380e-06,
         -6.9439e-06, -5.7817e-06]], device='cuda:0')
Loss: 0.9413354992866516


Running epoch 1, step 1779, batch 731
Sampled inputs[:2]: tensor([[   0, 9657,  300,  ...,   12,  271,  266],
        [   0, 1125,  278,  ..., 6447,  609,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7116e-04, -2.5046e-05,  1.2990e-04,  ..., -1.0335e-04,
          4.0058e-04,  1.8670e-04],
        [-4.9248e-06, -3.3490e-06,  3.4012e-06,  ..., -4.4033e-06,
         -3.6806e-06, -3.6024e-06],
        [-1.5825e-05, -1.1265e-05,  1.1832e-05,  ..., -1.3992e-05,
         -1.1772e-05, -1.1355e-05],
        [-1.4678e-05, -9.7752e-06,  1.0639e-05,  ..., -1.3128e-05,
         -1.1057e-05, -1.1086e-05],
        [-1.1444e-05, -8.4341e-06,  8.3148e-06,  ..., -1.0327e-05,
         -9.0301e-06, -7.6070e-06]], device='cuda:0')
Loss: 0.9597521424293518


Running epoch 1, step 1780, batch 732
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8489e-04,  4.0706e-05,  3.3889e-04,  ..., -7.5804e-05,
          1.7933e-04,  1.5161e-04],
        [-6.1393e-06, -4.2655e-06,  4.4145e-06,  ..., -5.4762e-06,
         -4.5970e-06, -4.3698e-06],
        [-1.9670e-05, -1.4335e-05,  1.5229e-05,  ..., -1.7434e-05,
         -1.4752e-05, -1.3843e-05],
        [-1.8403e-05, -1.2591e-05,  1.3933e-05,  ..., -1.6436e-05,
         -1.3933e-05, -1.3605e-05],
        [-1.4156e-05, -1.0699e-05,  1.0654e-05,  ..., -1.2800e-05,
         -1.1250e-05, -9.2462e-06]], device='cuda:0')
Loss: 0.9979531764984131


Running epoch 1, step 1781, batch 733
Sampled inputs[:2]: tensor([[    0,  1781,   659,  ...,    12,  1478,    14],
        [    0, 48545,    26,  ...,  1471,   266,   319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5522e-04,  8.5545e-05,  3.5757e-04,  ..., -7.2688e-05,
          6.4577e-06,  3.5762e-04],
        [-7.3984e-06, -4.9695e-06,  5.2787e-06,  ..., -6.6087e-06,
         -5.4985e-06, -5.3681e-06],
        [-2.3812e-05, -1.6764e-05,  1.8254e-05,  ..., -2.1070e-05,
         -1.7703e-05, -1.7017e-05],
        [-2.2277e-05, -1.4663e-05,  1.6674e-05,  ..., -1.9908e-05,
         -1.6734e-05, -1.6719e-05],
        [-1.7181e-05, -1.2532e-05,  1.2830e-05,  ..., -1.5497e-05,
         -1.3515e-05, -1.1422e-05]], device='cuda:0')
Loss: 0.9496455788612366


Running epoch 1, step 1782, batch 734
Sampled inputs[:2]: tensor([[   0,  360,  259,  ...,   14,  381, 1371],
        [   0, 3179,  221,  ...,  910,  706, 1102]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8596e-05,  1.1475e-04,  1.3911e-04,  ...,  1.2927e-04,
         -2.0842e-04,  4.7834e-04],
        [-8.7097e-06, -5.6960e-06,  6.2697e-06,  ..., -7.7263e-06,
         -6.3255e-06, -6.3069e-06],
        [-2.7716e-05, -1.9103e-05,  2.1443e-05,  ..., -2.4378e-05,
         -2.0191e-05, -1.9759e-05],
        [-2.6032e-05, -1.6764e-05,  1.9684e-05,  ..., -2.3097e-05,
         -1.9133e-05, -1.9491e-05],
        [-2.0087e-05, -1.4327e-05,  1.5125e-05,  ..., -1.7986e-05,
         -1.5467e-05, -1.3314e-05]], device='cuda:0')
Loss: 0.9565267562866211


Running epoch 1, step 1783, batch 735
Sampled inputs[:2]: tensor([[   0,  257,  221,  ..., 1474, 2044,  300],
        [   0,  285,   53,  ...,  259, 5012, 3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4941e-04,  3.0732e-05, -1.4606e-04,  ...,  2.6178e-04,
         -1.6979e-04,  4.3004e-04],
        [-1.0066e-05, -6.4149e-06,  7.1526e-06,  ..., -8.9258e-06,
         -7.1824e-06, -7.3723e-06],
        [-3.1829e-05, -2.1443e-05,  2.4378e-05,  ..., -2.7955e-05,
         -2.2799e-05, -2.2888e-05],
        [-2.9907e-05, -1.8805e-05,  2.2352e-05,  ..., -2.6524e-05,
         -2.1622e-05, -2.2605e-05],
        [-2.3276e-05, -1.6160e-05,  1.7330e-05,  ..., -2.0802e-05,
         -1.7583e-05, -1.5609e-05]], device='cuda:0')
Loss: 0.9407257437705994
Graident accumulation at epoch 1, step 1783, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2023e-04,  1.3012e-04, -5.8449e-05,  ...,  3.6066e-05,
          3.2906e-05,  1.6565e-04],
        [-9.9845e-06, -6.7426e-06,  7.0719e-06,  ..., -8.7736e-06,
         -6.9551e-06, -7.0585e-06],
        [ 3.9077e-06,  1.0523e-05, -2.7810e-06,  ...,  7.1099e-07,
          1.2567e-05, -8.8487e-06],
        [-1.0427e-05,  8.9035e-06,  7.2328e-06,  ..., -6.0011e-06,
          1.9796e-06, -8.9029e-06],
        [-2.3522e-05, -1.7370e-05,  1.7356e-05,  ..., -2.0883e-05,
         -1.7634e-05, -1.5347e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3872e-08, 6.3273e-08, 4.8013e-08,  ..., 4.1759e-08, 1.3001e-07,
         5.6031e-08],
        [8.0067e-11, 5.8607e-11, 2.0270e-11,  ..., 5.6614e-11, 3.5556e-11,
         3.0686e-11],
        [4.3520e-09, 3.0843e-09, 1.5585e-09,  ..., 3.3209e-09, 1.6111e-09,
         1.0648e-09],
        [1.1964e-09, 1.3341e-09, 3.9512e-10,  ..., 1.0619e-09, 7.5396e-10,
         4.7320e-10],
        [3.6655e-10, 2.0887e-10, 8.4511e-11,  ..., 2.7527e-10, 8.4945e-11,
         1.0771e-10]], device='cuda:0')
optimizer state dict: 223.0
lr: [1.1397899338904206e-06, 1.1397899338904206e-06]
scheduler_last_epoch: 223


Running epoch 1, step 1784, batch 736
Sampled inputs[:2]: tensor([[    0,  2849,  1173,  ...,  1481,    12,   287],
        [    0,    12, 32425,  ...,   389,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5321e-05,  1.5943e-04, -7.1915e-05,  ...,  6.1030e-05,
         -1.0855e-04, -2.2670e-04],
        [-1.2964e-06, -9.0152e-07,  9.4622e-07,  ..., -1.1176e-06,
         -8.6427e-07, -8.7172e-07],
        [-3.9339e-06, -2.9951e-06,  3.1590e-06,  ..., -3.4571e-06,
         -2.8163e-06, -2.6524e-06],
        [-3.7551e-06, -2.6673e-06,  2.9355e-06,  ..., -3.2932e-06,
         -2.6375e-06, -2.6524e-06],
        [-2.8312e-06, -2.2501e-06,  2.2203e-06,  ..., -2.5332e-06,
         -2.1756e-06, -1.7509e-06]], device='cuda:0')
Loss: 0.959686279296875


Running epoch 1, step 1785, batch 737
Sampled inputs[:2]: tensor([[   0,  266, 1422,  ...,  446, 1992,  586],
        [   0,  593,  300,  ...,  278, 4694,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9100e-04,  1.6189e-05, -1.3190e-04,  ..., -3.1239e-05,
         -8.7271e-05, -2.4953e-04],
        [-2.5108e-06, -1.8254e-06,  1.8813e-06,  ..., -2.2054e-06,
         -1.7472e-06, -1.6540e-06],
        [-7.7784e-06, -6.0946e-06,  6.3181e-06,  ..., -6.9290e-06,
         -5.6475e-06, -5.1409e-06],
        [-7.3314e-06, -5.3644e-06,  5.8264e-06,  ..., -6.5267e-06,
         -5.2601e-06, -5.0813e-06],
        [-5.5581e-06, -4.5449e-06,  4.3809e-06,  ..., -5.0366e-06,
         -4.3064e-06, -3.3826e-06]], device='cuda:0')
Loss: 0.9826761484146118


Running epoch 1, step 1786, batch 738
Sampled inputs[:2]: tensor([[   0, 5182,  446,  ...,  417,  199,   50],
        [   0,  298,  374,  ...,  298,  413,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7048e-04,  1.8451e-04,  1.6554e-04,  ...,  2.4418e-06,
         -4.8178e-05, -2.2193e-04],
        [-3.7998e-06, -2.6859e-06,  2.6897e-06,  ..., -3.3453e-06,
         -2.6561e-06, -2.6077e-06],
        [-1.1772e-05, -8.9407e-06,  9.1195e-06,  ..., -1.0446e-05,
         -8.5384e-06, -8.0317e-06],
        [-1.1042e-05, -7.8231e-06,  8.2850e-06,  ..., -9.8497e-06,
         -7.9721e-06, -7.9125e-06],
        [-8.4490e-06, -6.6757e-06,  6.3330e-06,  ..., -7.6443e-06,
         -6.5416e-06, -5.3197e-06]], device='cuda:0')
Loss: 0.9261257648468018


Running epoch 1, step 1787, batch 739
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0, 1420, 6319,  ...,  292, 4895, 4050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3886e-04,  3.0722e-04,  3.3687e-04,  ..., -8.4297e-06,
          3.6729e-06, -4.8099e-05],
        [-5.0515e-06, -3.5502e-06,  3.5651e-06,  ..., -4.4629e-06,
         -3.5278e-06, -3.4720e-06],
        [-1.5616e-05, -1.1787e-05,  1.2040e-05,  ..., -1.3873e-05,
         -1.1265e-05, -1.0669e-05],
        [-1.4722e-05, -1.0386e-05,  1.1012e-05,  ..., -1.3173e-05,
         -1.0595e-05, -1.0565e-05],
        [-1.1310e-05, -8.8364e-06,  8.4341e-06,  ..., -1.0237e-05,
         -8.6725e-06, -7.1675e-06]], device='cuda:0')
Loss: 0.9614960551261902


Running epoch 1, step 1788, batch 740
Sampled inputs[:2]: tensor([[    0,    13,  2615,  ..., 31594, 15867,  3484],
        [    0,   271,   266,  ..., 46357, 11101, 10621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2597e-04,  2.2636e-04,  3.7989e-04,  ..., -8.7143e-05,
         -7.8195e-05, -1.3446e-04],
        [-6.2287e-06, -4.3772e-06,  4.4778e-06,  ..., -5.5209e-06,
         -4.3213e-06, -4.2878e-06],
        [-1.9431e-05, -1.4618e-05,  1.5244e-05,  ..., -1.7285e-05,
         -1.3873e-05, -1.3277e-05],
        [-1.8254e-05, -1.2830e-05,  1.3933e-05,  ..., -1.6347e-05,
         -1.3009e-05, -1.3128e-05],
        [-1.3962e-05, -1.0908e-05,  1.0595e-05,  ..., -1.2651e-05,
         -1.0610e-05, -8.8289e-06]], device='cuda:0')
Loss: 0.9580217599868774


Running epoch 1, step 1789, batch 741
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0,   300,  2607,  ...,  1279,   368,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2150e-04,  1.9138e-04,  2.8099e-04,  ...,  8.4626e-05,
          7.4987e-05, -1.2643e-04],
        [-7.4580e-06, -5.2750e-06,  5.3644e-06,  ..., -6.6385e-06,
         -5.2974e-06, -5.1446e-06],
        [-2.3276e-05, -1.7524e-05,  1.8284e-05,  ..., -2.0728e-05,
         -1.6898e-05, -1.5855e-05],
        [-2.1905e-05, -1.5467e-05,  1.6734e-05,  ..., -1.9655e-05,
         -1.5929e-05, -1.5751e-05],
        [-1.6719e-05, -1.3068e-05,  1.2726e-05,  ..., -1.5154e-05,
         -1.2875e-05, -1.0520e-05]], device='cuda:0')
Loss: 0.9713976979255676


Running epoch 1, step 1790, batch 742
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0,   221,   380,  ..., 10022,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5047e-04,  4.5877e-04,  1.6604e-04,  ...,  1.4152e-04,
         -1.7748e-04, -1.6322e-04],
        [-8.7172e-06, -6.0834e-06,  6.3479e-06,  ..., -7.6890e-06,
         -6.0946e-06, -5.9754e-06],
        [-2.7299e-05, -2.0266e-05,  2.1651e-05,  ..., -2.4110e-05,
         -1.9491e-05, -1.8492e-05],
        [-2.5749e-05, -1.7911e-05,  1.9893e-05,  ..., -2.2873e-05,
         -1.8388e-05, -1.8388e-05],
        [-1.9401e-05, -1.4976e-05,  1.4916e-05,  ..., -1.7464e-05,
         -1.4722e-05, -1.2144e-05]], device='cuda:0')
Loss: 0.943261981010437


Running epoch 1, step 1791, batch 743
Sampled inputs[:2]: tensor([[    0,   287,   259,  ...,  5041,  1826,  5041],
        [    0, 28107,    14,  ...,   864,   298,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9397e-04,  6.6694e-04,  3.8685e-04,  ...,  6.4031e-05,
         -9.3762e-05, -2.2520e-04],
        [-9.9614e-06, -6.8992e-06,  7.0408e-06,  ..., -8.8215e-06,
         -7.0855e-06, -6.9141e-06],
        [-3.1143e-05, -2.2992e-05,  2.4080e-05,  ..., -2.7597e-05,
         -2.2620e-05, -2.1353e-05],
        [-2.9489e-05, -2.0340e-05,  2.2024e-05,  ..., -2.6330e-05,
         -2.1517e-05, -2.1294e-05],
        [-2.2307e-05, -1.7047e-05,  1.6674e-05,  ..., -2.0146e-05,
         -1.7196e-05, -1.4141e-05]], device='cuda:0')
Loss: 0.9045764803886414
Graident accumulation at epoch 1, step 1791, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.8813e-05,  1.8380e-04, -1.3919e-05,  ...,  3.8863e-05,
          2.0239e-05,  1.2656e-04],
        [-9.9822e-06, -6.7582e-06,  7.0688e-06,  ..., -8.7784e-06,
         -6.9681e-06, -7.0440e-06],
        [ 4.0262e-07,  7.1717e-06, -9.4838e-08,  ..., -2.1198e-06,
          9.0479e-06, -1.0099e-05],
        [-1.2333e-05,  5.9792e-06,  8.7119e-06,  ..., -8.0340e-06,
         -3.7013e-07, -1.0142e-05],
        [-2.3400e-05, -1.7338e-05,  1.7288e-05,  ..., -2.0810e-05,
         -1.7590e-05, -1.5226e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3954e-08, 6.3654e-08, 4.8115e-08,  ..., 4.1722e-08, 1.2989e-07,
         5.6025e-08],
        [8.0087e-11, 5.8596e-11, 2.0299e-11,  ..., 5.6635e-11, 3.5570e-11,
         3.0703e-11],
        [4.3486e-09, 3.0817e-09, 1.5575e-09,  ..., 3.3184e-09, 1.6100e-09,
         1.0642e-09],
        [1.1961e-09, 1.3331e-09, 3.9521e-10,  ..., 1.0616e-09, 7.5367e-10,
         4.7318e-10],
        [3.6668e-10, 2.0895e-10, 8.4704e-11,  ..., 2.7540e-10, 8.5156e-11,
         1.0780e-10]], device='cuda:0')
optimizer state dict: 224.0
lr: [1.083154114868752e-06, 1.083154114868752e-06]
scheduler_last_epoch: 224


Running epoch 1, step 1792, batch 744
Sampled inputs[:2]: tensor([[   0,  259, 2561,  ...,   77, 4830,  292],
        [   0, 3393, 3380,  ...,  292, 6502,  950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4446e-05, -1.0207e-04, -2.2307e-05,  ..., -1.2577e-04,
          1.7065e-04,  1.0817e-04],
        [-1.1176e-06, -8.1211e-07,  8.6054e-07,  ..., -1.0356e-06,
         -7.9349e-07, -8.2329e-07],
        [-3.4720e-06, -2.5928e-06,  2.8908e-06,  ..., -3.1292e-06,
         -2.3991e-06, -2.4140e-06],
        [-3.3081e-06, -2.3395e-06,  2.6822e-06,  ..., -3.0547e-06,
         -2.3544e-06, -2.5183e-06],
        [-2.4736e-06, -1.9372e-06,  1.9819e-06,  ..., -2.2650e-06,
         -1.8179e-06, -1.5572e-06]], device='cuda:0')
Loss: 0.9406895637512207


Running epoch 1, step 1793, batch 745
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,    14, 13078,  ...,  1994,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6801e-05, -2.4131e-05, -4.4099e-06,  ..., -1.6487e-04,
          4.2744e-04,  2.2008e-04],
        [-2.4587e-06, -1.5981e-06,  1.6540e-06,  ..., -2.2426e-06,
         -1.7062e-06, -1.8291e-06],
        [-7.4357e-06, -5.1856e-06,  5.5134e-06,  ..., -6.7055e-06,
         -5.2154e-06, -5.3495e-06],
        [-7.0184e-06, -4.5300e-06,  4.9621e-06,  ..., -6.4522e-06,
         -4.9919e-06, -5.3495e-06],
        [-5.5283e-06, -3.9786e-06,  3.9339e-06,  ..., -5.0813e-06,
         -4.0978e-06, -3.6582e-06]], device='cuda:0')
Loss: 0.9067720174789429


Running epoch 1, step 1794, batch 746
Sampled inputs[:2]: tensor([[   0,  292,  263,  ...,  342, 4575,  271],
        [   0,  328,  957,  ...,  298,  275, 8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5752e-04,  4.0421e-06, -1.0466e-04,  ..., -1.7918e-04,
          5.9062e-04,  2.0480e-04],
        [-3.6135e-06, -2.4773e-06,  2.5555e-06,  ..., -3.3602e-06,
         -2.6450e-06, -2.6375e-06],
        [-1.1012e-05, -8.0466e-06,  8.5235e-06,  ..., -1.0118e-05,
         -8.1360e-06, -7.7635e-06],
        [-1.0461e-05, -7.1526e-06,  7.8082e-06,  ..., -9.8050e-06,
         -7.8827e-06, -7.8827e-06],
        [-8.1360e-06, -6.1691e-06,  6.0946e-06,  ..., -7.6145e-06,
         -6.3330e-06, -5.2825e-06]], device='cuda:0')
Loss: 0.9972090721130371


Running epoch 1, step 1795, batch 747
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,  6408,   278,  1119],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2363e-04,  9.7273e-05, -3.9577e-04,  ...,  3.3570e-05,
          2.5421e-04,  2.2600e-04],
        [-4.8280e-06, -3.2559e-06,  3.4571e-06,  ..., -4.4778e-06,
         -3.4645e-06, -3.5092e-06],
        [-1.4886e-05, -1.0639e-05,  1.1623e-05,  ..., -1.3605e-05,
         -1.0714e-05, -1.0476e-05],
        [-1.4156e-05, -9.4771e-06,  1.0699e-05,  ..., -1.3202e-05,
         -1.0401e-05, -1.0639e-05],
        [-1.0848e-05, -8.0615e-06,  8.2254e-06,  ..., -1.0088e-05,
         -8.2552e-06, -7.0333e-06]], device='cuda:0')
Loss: 0.9508222937583923


Running epoch 1, step 1796, batch 748
Sampled inputs[:2]: tensor([[   0, 7061,  437,  ...,  278, 9500,   18],
        [   0,   14,  475,  ..., 4103,  278, 4190]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9866e-04,  2.6255e-04, -2.9295e-04,  ...,  2.6172e-04,
          3.3932e-04,  2.4573e-04],
        [-6.0797e-06, -4.1164e-06,  4.3064e-06,  ..., -5.5730e-06,
         -4.3474e-06, -4.4070e-06],
        [-1.8910e-05, -1.3560e-05,  1.4588e-05,  ..., -1.7092e-05,
         -1.3545e-05, -1.3292e-05],
        [-1.7881e-05, -1.2025e-05,  1.3366e-05,  ..., -1.6481e-05,
         -1.3053e-05, -1.3426e-05],
        [-1.3769e-05, -1.0252e-05,  1.0312e-05,  ..., -1.2651e-05,
         -1.0446e-05, -8.9183e-06]], device='cuda:0')
Loss: 0.9478212594985962


Running epoch 1, step 1797, batch 749
Sampled inputs[:2]: tensor([[    0,  3761,    12,  ...,  3476, 20966,   391],
        [    0,   870,   278,  ...,   478,   401,   897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0248e-04,  3.3398e-06, -5.9124e-04,  ...,  3.2436e-04,
          6.8193e-04,  3.4954e-04],
        [-7.1898e-06, -4.9509e-06,  5.1148e-06,  ..., -6.6310e-06,
         -5.2229e-06, -5.2638e-06],
        [-2.2426e-05, -1.6242e-05,  1.7375e-05,  ..., -2.0340e-05,
         -1.6257e-05, -1.5840e-05],
        [-2.1204e-05, -1.4424e-05,  1.5914e-05,  ..., -1.9625e-05,
         -1.5706e-05, -1.6049e-05],
        [-1.6317e-05, -1.2249e-05,  1.2279e-05,  ..., -1.5005e-05,
         -1.2487e-05, -1.0580e-05]], device='cuda:0')
Loss: 0.9693647623062134


Running epoch 1, step 1798, batch 750
Sampled inputs[:2]: tensor([[   0,   14, 4494,  ..., 4830,  368,  266],
        [   0,   12,  287,  ...,   12, 5576,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3454e-04, -1.7092e-04, -7.2498e-04,  ...,  5.2654e-04,
          1.0213e-03,  5.3851e-04],
        [-8.3372e-06, -5.8524e-06,  6.0014e-06,  ..., -7.7188e-06,
         -6.1467e-06, -6.0908e-06],
        [-2.6107e-05, -1.9193e-05,  2.0415e-05,  ..., -2.3782e-05,
         -1.9208e-05, -1.8418e-05],
        [-2.4661e-05, -1.7092e-05,  1.8731e-05,  ..., -2.2903e-05,
         -1.8507e-05, -1.8612e-05],
        [-1.8984e-05, -1.4454e-05,  1.4424e-05,  ..., -1.7524e-05,
         -1.4722e-05, -1.2301e-05]], device='cuda:0')
Loss: 0.9852995276451111


Running epoch 1, step 1799, batch 751
Sampled inputs[:2]: tensor([[   0,  756,  943,  ..., 4016,   12,  627],
        [   0, 2530,  634,  ...,   15, 8808,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4328e-04, -3.0997e-04, -8.8765e-04,  ...,  5.6012e-04,
          1.3030e-03,  6.3492e-04],
        [-9.5665e-06, -6.6571e-06,  6.8620e-06,  ..., -8.8364e-06,
         -7.0557e-06, -6.9849e-06],
        [-2.9877e-05, -2.1771e-05,  2.3350e-05,  ..., -2.7135e-05,
         -2.1964e-05, -2.1070e-05],
        [-2.8223e-05, -1.9372e-05,  2.1368e-05,  ..., -2.6152e-05,
         -2.1160e-05, -2.1264e-05],
        [-2.1830e-05, -1.6451e-05,  1.6555e-05,  ..., -2.0102e-05,
         -1.6913e-05, -1.4141e-05]], device='cuda:0')
Loss: 0.9722432494163513
Graident accumulation at epoch 1, step 1799, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3959e-06,  1.3442e-04, -1.0129e-04,  ...,  9.0988e-05,
          1.4852e-04,  1.7740e-04],
        [-9.9406e-06, -6.7481e-06,  7.0481e-06,  ..., -8.7842e-06,
         -6.9769e-06, -7.0381e-06],
        [-2.6253e-06,  4.2774e-06,  2.2497e-06,  ..., -4.6213e-06,
          5.9466e-06, -1.1196e-05],
        [-1.3922e-05,  3.4441e-06,  9.9775e-06,  ..., -9.8457e-06,
         -2.4491e-06, -1.1254e-05],
        [-2.3243e-05, -1.7249e-05,  1.7215e-05,  ..., -2.0739e-05,
         -1.7523e-05, -1.5118e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4293e-08, 6.3686e-08, 4.8855e-08,  ..., 4.1994e-08, 1.3146e-07,
         5.6372e-08],
        [8.0098e-11, 5.8582e-11, 2.0326e-11,  ..., 5.6656e-11, 3.5585e-11,
         3.0721e-11],
        [4.3451e-09, 3.0791e-09, 1.5565e-09,  ..., 3.3158e-09, 1.6089e-09,
         1.0636e-09],
        [1.1957e-09, 1.3322e-09, 3.9527e-10,  ..., 1.0612e-09, 7.5337e-10,
         4.7315e-10],
        [3.6679e-10, 2.0901e-10, 8.4894e-11,  ..., 2.7553e-10, 8.5357e-11,
         1.0789e-10]], device='cuda:0')
optimizer state dict: 225.0
lr: [1.027880867734583e-06, 1.027880867734583e-06]
scheduler_last_epoch: 225


Running epoch 1, step 1800, batch 752
Sampled inputs[:2]: tensor([[    0,  8450,   292,  ...,   352,   722, 37719],
        [    0,   775,   266,  ...,   409,   328,  5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2868e-05, -2.2108e-04, -3.2316e-04,  ...,  8.6979e-05,
         -6.9047e-05,  6.7136e-05],
        [-1.3113e-06, -7.4878e-07,  8.7172e-07,  ..., -1.1623e-06,
         -8.5309e-07, -1.0431e-06],
        [-4.0531e-06, -2.4289e-06,  2.9653e-06,  ..., -3.5316e-06,
         -2.6524e-06, -3.1143e-06],
        [-3.7104e-06, -2.1011e-06,  2.6077e-06,  ..., -3.2932e-06,
         -2.4587e-06, -3.0100e-06],
        [-3.1292e-06, -1.9073e-06,  2.2203e-06,  ..., -2.7567e-06,
         -2.1458e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9435044527053833


Running epoch 1, step 1801, batch 753
Sampled inputs[:2]: tensor([[   0,  365, 1110,  ..., 4130,  221,  199],
        [   0, 1171, 2926,  ...,  259, 4288,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1414e-05, -2.4390e-04, -3.1876e-04,  ..., -3.2906e-05,
         -1.2393e-04,  6.4279e-06],
        [-2.5779e-06, -1.5832e-06,  1.7546e-06,  ..., -2.3320e-06,
         -1.7285e-06, -1.9297e-06],
        [-7.9870e-06, -5.2005e-06,  5.9307e-06,  ..., -7.1377e-06,
         -5.4091e-06, -5.8562e-06],
        [-7.4506e-06, -4.5598e-06,  5.3495e-06,  ..., -6.7502e-06,
         -5.0813e-06, -5.7518e-06],
        [-6.0648e-06, -4.0680e-06,  4.3809e-06,  ..., -5.5134e-06,
         -4.3511e-06, -4.1351e-06]], device='cuda:0')
Loss: 0.9580205082893372


Running epoch 1, step 1802, batch 754
Sampled inputs[:2]: tensor([[    0,    13,  4831,  ...,   333,   199,  2038],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0932e-04, -3.1254e-04, -3.9908e-04,  ..., -1.3818e-04,
         -1.6012e-05, -1.7103e-04],
        [-3.8072e-06, -2.3954e-06,  2.7381e-06,  ..., -3.3900e-06,
         -2.5295e-06, -2.8275e-06],
        [-1.1861e-05, -7.9125e-06,  9.2685e-06,  ..., -1.0461e-05,
         -7.9721e-06, -8.6725e-06],
        [-1.1027e-05, -6.8992e-06,  8.3894e-06,  ..., -9.8348e-06,
         -7.4208e-06, -8.4490e-06],
        [-8.7917e-06, -6.0499e-06,  6.6310e-06,  ..., -7.9125e-06,
         -6.2883e-06, -5.9530e-06]], device='cuda:0')
Loss: 0.929174542427063


Running epoch 1, step 1803, batch 755
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,   668,  2474,  ...,   668,  4599,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0006e-04, -3.1841e-04, -3.7645e-04,  ..., -1.2563e-04,
         -1.3587e-04, -2.0499e-04],
        [-5.0142e-06, -3.2410e-06,  3.6433e-06,  ..., -4.4480e-06,
         -3.3677e-06, -3.6843e-06],
        [-1.5676e-05, -1.0654e-05,  1.2323e-05,  ..., -1.3739e-05,
         -1.0565e-05, -1.1295e-05],
        [-1.4663e-05, -9.3877e-06,  1.1250e-05,  ..., -1.2994e-05,
         -9.9391e-06, -1.1116e-05],
        [-1.1548e-05, -8.1062e-06,  8.7768e-06,  ..., -1.0327e-05,
         -8.2999e-06, -7.7039e-06]], device='cuda:0')
Loss: 0.942179262638092


Running epoch 1, step 1804, batch 756
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,  2319,    30,  ...,   508,  6703,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4536e-06, -2.6378e-04, -3.6310e-04,  ...,  3.7260e-05,
         -1.6502e-04, -2.4141e-04],
        [-6.2808e-06, -4.0382e-06,  4.6045e-06,  ..., -5.5656e-06,
         -4.2468e-06, -4.5560e-06],
        [-1.9640e-05, -1.3247e-05,  1.5527e-05,  ..., -1.7211e-05,
         -1.3337e-05, -1.3962e-05],
        [-1.8507e-05, -1.1757e-05,  1.4305e-05,  ..., -1.6391e-05,
         -1.2636e-05, -1.3828e-05],
        [-1.4365e-05, -1.0028e-05,  1.0997e-05,  ..., -1.2830e-05,
         -1.0371e-05, -9.4771e-06]], device='cuda:0')
Loss: 0.9697820544242859


Running epoch 1, step 1805, batch 757
Sampled inputs[:2]: tensor([[    0,   381, 13565,  ...,     9,   847,   300],
        [    0,    12,   287,  ...,   365,  1943,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2202e-05, -1.3143e-04, -1.3704e-04,  ...,  3.3082e-05,
          1.6507e-04, -5.8365e-05],
        [-7.4506e-06, -4.9546e-06,  5.3123e-06,  ..., -6.7055e-06,
         -5.3048e-06, -5.4128e-06],
        [-2.3335e-05, -1.6332e-05,  1.8016e-05,  ..., -2.0832e-05,
         -1.6749e-05, -1.6630e-05],
        [-2.2054e-05, -1.4573e-05,  1.6555e-05,  ..., -1.9938e-05,
         -1.6019e-05, -1.6555e-05],
        [-1.7211e-05, -1.2457e-05,  1.2867e-05,  ..., -1.5646e-05,
         -1.3083e-05, -1.1399e-05]], device='cuda:0')
Loss: 0.9547112584114075


Running epoch 1, step 1806, batch 758
Sampled inputs[:2]: tensor([[   0,  271, 4787,  ...,  292,  494,  221],
        [   0,  472,  346,  ...,  298,  527,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0582e-04, -1.3817e-04, -3.2890e-04,  ..., -1.9205e-04,
          3.6108e-04, -3.3132e-04],
        [-8.7023e-06, -5.8040e-06,  6.3181e-06,  ..., -7.7784e-06,
         -6.1467e-06, -6.2659e-06],
        [-2.7105e-05, -1.9088e-05,  2.1309e-05,  ..., -2.4065e-05,
         -1.9342e-05, -1.9148e-05],
        [-2.5645e-05, -1.7017e-05,  1.9640e-05,  ..., -2.3037e-05,
         -1.8477e-05, -1.9088e-05],
        [-1.9848e-05, -1.4484e-05,  1.5102e-05,  ..., -1.7956e-05,
         -1.5035e-05, -1.3009e-05]], device='cuda:0')
Loss: 0.9333329200744629


Running epoch 1, step 1807, batch 759
Sampled inputs[:2]: tensor([[   0, 6640,   13,  ...,  292,  221,  273],
        [   0,  278, 6046,  ..., 1671,  199,  395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0950e-04, -5.5858e-05, -3.8589e-04,  ..., -1.3682e-04,
          4.3903e-04, -2.6398e-04],
        [-9.9763e-06, -6.5416e-06,  7.2122e-06,  ..., -8.8811e-06,
         -6.9588e-06, -7.2494e-06],
        [-3.1129e-05, -2.1607e-05,  2.4393e-05,  ..., -2.7537e-05,
         -2.1964e-05, -2.2218e-05],
        [-2.9325e-05, -1.9133e-05,  2.2367e-05,  ..., -2.6256e-05,
         -2.0906e-05, -2.2009e-05],
        [-2.2694e-05, -1.6332e-05,  1.7203e-05,  ..., -2.0489e-05,
         -1.7032e-05, -1.5020e-05]], device='cuda:0')
Loss: 0.9070189595222473
Graident accumulation at epoch 1, step 1807, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.7941e-06,  1.1540e-04, -1.2975e-04,  ...,  6.8207e-05,
          1.7757e-04,  1.3326e-04],
        [-9.9442e-06, -6.7275e-06,  7.0645e-06,  ..., -8.7939e-06,
         -6.9751e-06, -7.0593e-06],
        [-5.4756e-06,  1.6890e-06,  4.4640e-06,  ..., -6.9129e-06,
          3.1555e-06, -1.2298e-05],
        [-1.5462e-05,  1.1864e-06,  1.1216e-05,  ..., -1.1487e-05,
         -4.2948e-06, -1.2330e-05],
        [-2.3189e-05, -1.7158e-05,  1.7214e-05,  ..., -2.0714e-05,
         -1.7474e-05, -1.5108e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4231e-08, 6.3626e-08, 4.8955e-08,  ..., 4.1970e-08, 1.3152e-07,
         5.6386e-08],
        [8.0117e-11, 5.8566e-11, 2.0358e-11,  ..., 5.6678e-11, 3.5598e-11,
         3.0743e-11],
        [4.3418e-09, 3.0765e-09, 1.5556e-09,  ..., 3.3132e-09, 1.6078e-09,
         1.0630e-09],
        [1.1953e-09, 1.3312e-09, 3.9538e-10,  ..., 1.0608e-09, 7.5305e-10,
         4.7317e-10],
        [3.6694e-10, 2.0907e-10, 8.5105e-11,  ..., 2.7567e-10, 8.5562e-11,
         1.0801e-10]], device='cuda:0')
optimizer state dict: 226.0
lr: [9.739786387225548e-07, 9.739786387225548e-07]
scheduler_last_epoch: 226


Running epoch 1, step 1808, batch 760
Sampled inputs[:2]: tensor([[   0, 1196, 2612,  ..., 2489,   14,  333],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2726e-05,  4.4366e-05,  1.3370e-05,  ...,  7.5675e-05,
         -1.2266e-04, -9.3756e-05],
        [-1.2144e-06, -8.6054e-07,  9.2387e-07,  ..., -1.0952e-06,
         -9.2387e-07, -8.3447e-07],
        [-3.8743e-06, -2.8908e-06,  3.1739e-06,  ..., -3.4720e-06,
         -2.9653e-06, -2.6226e-06],
        [-3.5614e-06, -2.4736e-06,  2.8759e-06,  ..., -3.2037e-06,
         -2.7418e-06, -2.5779e-06],
        [-2.7567e-06, -2.1607e-06,  2.1905e-06,  ..., -2.5183e-06,
         -2.2501e-06, -1.7136e-06]], device='cuda:0')
Loss: 0.9449523687362671


Running epoch 1, step 1809, batch 761
Sampled inputs[:2]: tensor([[    0, 13509,   472,  ...,  1805,    13, 27816],
        [    0, 15033,   278,  ...,   266,  2937,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7012e-06, -1.8419e-04, -1.3332e-04,  ..., -4.2856e-07,
         -4.0021e-06, -3.7174e-04],
        [-2.3246e-06, -1.5348e-06,  1.8626e-06,  ..., -2.1234e-06,
         -1.7807e-06, -1.6838e-06],
        [-7.1377e-06, -4.9621e-06,  6.1095e-06,  ..., -6.3926e-06,
         -5.3942e-06, -4.9025e-06],
        [-6.7800e-06, -4.3362e-06,  5.7369e-06,  ..., -6.1542e-06,
         -5.2154e-06, -5.0515e-06],
        [-5.2750e-06, -3.8669e-06,  4.3958e-06,  ..., -4.7982e-06,
         -4.2468e-06, -3.3230e-06]], device='cuda:0')
Loss: 0.8871092200279236


Running epoch 1, step 1810, batch 762
Sampled inputs[:2]: tensor([[   0, 1049,   12,  ...,  292, 3963,  755],
        [   0, 3152, 1385,  ..., 1403,  518, 2088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2474e-04, -4.3990e-04, -4.6872e-04,  ..., -1.4848e-05,
         -1.1803e-04, -5.7164e-04],
        [-3.5018e-06, -2.3283e-06,  2.8387e-06,  ..., -3.1590e-06,
         -2.5779e-06, -2.4810e-06],
        [-1.0818e-05, -7.4804e-06,  9.3579e-06,  ..., -9.5516e-06,
         -7.8529e-06, -7.2867e-06],
        [-1.0327e-05, -6.6310e-06,  8.8364e-06,  ..., -9.2238e-06,
         -7.5996e-06, -7.5102e-06],
        [-7.8380e-06, -5.7220e-06,  6.6012e-06,  ..., -7.0482e-06,
         -6.0871e-06, -4.8280e-06]], device='cuda:0')
Loss: 0.9690147042274475


Running epoch 1, step 1811, batch 763
Sampled inputs[:2]: tensor([[   0, 1615,  328,  ...,  266, 3133,  963],
        [   0, 9855,  278,  ...,  266, 3134,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1375e-04, -4.1205e-04, -5.1709e-04,  ..., -1.8754e-05,
         -1.2010e-04, -8.1136e-04],
        [-4.7460e-06, -3.1553e-06,  3.7104e-06,  ..., -4.2170e-06,
         -3.3863e-06, -3.3043e-06],
        [-1.4812e-05, -1.0267e-05,  1.2398e-05,  ..., -1.2934e-05,
         -1.0461e-05, -9.9242e-06],
        [-1.4141e-05, -9.1344e-06,  1.1683e-05,  ..., -1.2472e-05,
         -1.0088e-05, -1.0133e-05],
        [-1.0610e-05, -7.7337e-06,  8.6576e-06,  ..., -9.4473e-06,
         -8.0392e-06, -6.5044e-06]], device='cuda:0')
Loss: 0.947367787361145


Running epoch 1, step 1812, batch 764
Sampled inputs[:2]: tensor([[   0,  298, 8761,  ...,  271,  266,  298],
        [   0,  365,  925,  ...,  909,  598,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0669e-04, -6.0705e-04, -6.7462e-04,  ..., -5.2860e-05,
         -3.1427e-05, -8.9535e-04],
        [-6.3479e-06, -3.9153e-06,  4.5449e-06,  ..., -5.5134e-06,
         -4.2617e-06, -4.5709e-06],
        [-1.9670e-05, -1.2800e-05,  1.5169e-05,  ..., -1.6868e-05,
         -1.3247e-05, -1.3724e-05],
        [-1.8194e-05, -1.1086e-05,  1.3873e-05,  ..., -1.5780e-05,
         -1.2383e-05, -1.3381e-05],
        [-1.4603e-05, -9.8199e-06,  1.0863e-05,  ..., -1.2755e-05,
         -1.0423e-05, -9.4846e-06]], device='cuda:0')
Loss: 0.9298126101493835


Running epoch 1, step 1813, batch 765
Sampled inputs[:2]: tensor([[    0,   271,  4136,  ...,  5052, 14552,  3339],
        [    0, 11541,  4784,  ...,  2837, 38541,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7979e-05, -6.1832e-04, -6.3190e-04,  ..., -6.7299e-05,
          1.7915e-04, -8.6571e-04],
        [-7.6070e-06, -4.7721e-06,  5.3979e-06,  ..., -6.6534e-06,
         -5.2303e-06, -5.4389e-06],
        [-2.3633e-05, -1.5631e-05,  1.8075e-05,  ..., -2.0444e-05,
         -1.6272e-05, -1.6406e-05],
        [-2.1935e-05, -1.3605e-05,  1.6525e-05,  ..., -1.9178e-05,
         -1.5259e-05, -1.6049e-05],
        [-1.7583e-05, -1.2010e-05,  1.2994e-05,  ..., -1.5482e-05,
         -1.2808e-05, -1.1355e-05]], device='cuda:0')
Loss: 0.9829782843589783


Running epoch 1, step 1814, batch 766
Sampled inputs[:2]: tensor([[    0,   461,  1169,  ..., 14135,  2771,    13],
        [    0,  5646,    12,  ...,  1952,   287,  3088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0204e-05, -5.7081e-04, -6.7131e-04,  ..., -1.1073e-05,
          1.3903e-04, -8.5451e-04],
        [-8.8140e-06, -5.7220e-06,  6.4261e-06,  ..., -7.6815e-06,
         -6.0797e-06, -6.2399e-06],
        [-2.7567e-05, -1.8835e-05,  2.1622e-05,  ..., -2.3782e-05,
         -1.9059e-05, -1.8999e-05],
        [-2.5585e-05, -1.6451e-05,  1.9804e-05,  ..., -2.2277e-05,
         -1.7837e-05, -1.8582e-05],
        [-2.0221e-05, -1.4246e-05,  1.5303e-05,  ..., -1.7777e-05,
         -1.4804e-05, -1.2949e-05]], device='cuda:0')
Loss: 0.9491366744041443


Running epoch 1, step 1815, batch 767
Sampled inputs[:2]: tensor([[    0, 11657,   367,  ..., 31468,    26,   266],
        [    0,  1690,  2558,  ...,  2025,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2110e-04, -5.7039e-04, -7.6443e-04,  ..., -7.9219e-05,
          2.1354e-05, -8.8741e-04],
        [-1.0066e-05, -6.6347e-06,  7.3798e-06,  ..., -8.7693e-06,
         -6.9924e-06, -7.0482e-06],
        [-3.1680e-05, -2.2039e-05,  2.4974e-05,  ..., -2.7418e-05,
         -2.2203e-05, -2.1696e-05],
        [-2.9400e-05, -1.9267e-05,  2.2888e-05,  ..., -2.5630e-05,
         -2.0713e-05, -2.1175e-05],
        [-2.3022e-05, -1.6525e-05,  1.7524e-05,  ..., -2.0295e-05,
         -1.7069e-05, -1.4655e-05]], device='cuda:0')
Loss: 0.9952564239501953
Graident accumulation at epoch 1, step 1815, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4195e-05,  4.6818e-05, -1.9322e-04,  ...,  5.3464e-05,
          1.6195e-04,  3.1194e-05],
        [-9.9563e-06, -6.7182e-06,  7.0961e-06,  ..., -8.7914e-06,
         -6.9768e-06, -7.0582e-06],
        [-8.0961e-06, -6.8376e-07,  6.5150e-06,  ..., -8.9635e-06,
          6.1972e-07, -1.3238e-05],
        [-1.6856e-05, -8.5897e-07,  1.2384e-05,  ..., -1.2901e-05,
         -5.9366e-06, -1.3214e-05],
        [-2.3172e-05, -1.7094e-05,  1.7245e-05,  ..., -2.0672e-05,
         -1.7433e-05, -1.5063e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4206e-08, 6.3888e-08, 4.9490e-08,  ..., 4.1935e-08, 1.3139e-07,
         5.7117e-08],
        [8.0139e-11, 5.8552e-11, 2.0392e-11,  ..., 5.6699e-11, 3.5611e-11,
         3.0762e-11],
        [4.3384e-09, 3.0739e-09, 1.5546e-09,  ..., 3.3107e-09, 1.6067e-09,
         1.0624e-09],
        [1.1950e-09, 1.3303e-09, 3.9550e-10,  ..., 1.0604e-09, 7.5273e-10,
         4.7314e-10],
        [3.6710e-10, 2.0913e-10, 8.5327e-11,  ..., 2.7581e-10, 8.5767e-11,
         1.0812e-10]], device='cuda:0')
optimizer state dict: 227.0
lr: [9.214556645637851e-07, 9.214556645637851e-07]
scheduler_last_epoch: 227


Running epoch 1, step 1816, batch 768
Sampled inputs[:2]: tensor([[   0,   35, 3815,  ...,  278, 7097, 4601],
        [   0,  266, 4908,  ..., 1209,  328, 1603]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1420e-05,  1.2897e-05,  2.2078e-05,  ...,  6.3717e-05,
         -1.7724e-04, -3.3729e-06],
        [-1.2964e-06, -7.7114e-07,  9.2015e-07,  ..., -1.1474e-06,
         -8.7544e-07, -9.1270e-07],
        [-4.0233e-06, -2.6226e-06,  3.0845e-06,  ..., -3.5614e-06,
         -2.8312e-06, -2.7865e-06],
        [-3.6806e-06, -2.2054e-06,  2.7716e-06,  ..., -3.2783e-06,
         -2.5630e-06, -2.6971e-06],
        [-2.8908e-06, -1.9670e-06,  2.1458e-06,  ..., -2.6077e-06,
         -2.1607e-06, -1.8626e-06]], device='cuda:0')
Loss: 0.9323893785476685


Running epoch 1, step 1817, batch 769
Sampled inputs[:2]: tensor([[   0,  829,  874,  ...,  292,  380,  759],
        [   0, 2344,  271,  ..., 5415,   14, 1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2882e-05, -3.5514e-05, -1.6519e-04,  ..., -2.5593e-05,
         -2.4906e-04, -7.9019e-05],
        [-2.5332e-06, -1.6205e-06,  1.9036e-06,  ..., -2.1756e-06,
         -1.7248e-06, -1.7583e-06],
        [-7.8976e-06, -5.4389e-06,  6.3926e-06,  ..., -6.7800e-06,
         -5.5581e-06, -5.3793e-06],
        [-7.3463e-06, -4.6939e-06,  5.8711e-06,  ..., -6.3181e-06,
         -5.1111e-06, -5.2899e-06],
        [-5.5879e-06, -4.0233e-06,  4.3958e-06,  ..., -4.9025e-06,
         -4.1872e-06, -3.5241e-06]], device='cuda:0')
Loss: 0.9477176070213318


Running epoch 1, step 1818, batch 770
Sampled inputs[:2]: tensor([[    0,   275,  4452,  ...,    12,  3516,  5227],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2646e-04, -3.5514e-05, -2.1002e-04,  ...,  2.4711e-06,
         -4.6518e-04, -5.0884e-05],
        [-3.7774e-06, -2.4922e-06,  2.9020e-06,  ..., -3.1963e-06,
         -2.5295e-06, -2.5444e-06],
        [-1.2010e-05, -8.4788e-06,  9.9093e-06,  ..., -1.0163e-05,
         -8.2254e-06, -7.9721e-06],
        [-1.1146e-05, -7.3463e-06,  9.1046e-06,  ..., -9.4473e-06,
         -7.5698e-06, -7.7933e-06],
        [-8.3297e-06, -6.1542e-06,  6.6608e-06,  ..., -7.2271e-06,
         -6.1095e-06, -5.1335e-06]], device='cuda:0')
Loss: 0.9938545823097229


Running epoch 1, step 1819, batch 771
Sampled inputs[:2]: tensor([[    0,  2836,  3084,  ...,  3634,  6464,   271],
        [    0,   266,  1144,  ..., 21458,    12, 15890]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4049e-04,  2.3270e-05, -2.6651e-04,  ..., -1.7359e-04,
         -4.1205e-04, -1.0126e-04],
        [-5.0515e-06, -3.3490e-06,  3.8929e-06,  ..., -4.2692e-06,
         -3.3677e-06, -3.4086e-06],
        [-1.6093e-05, -1.1384e-05,  1.3292e-05,  ..., -1.3605e-05,
         -1.0937e-05, -1.0744e-05],
        [-1.4991e-05, -9.9093e-06,  1.2249e-05,  ..., -1.2711e-05,
         -1.0118e-05, -1.0505e-05],
        [-1.1221e-05, -8.2850e-06,  8.9854e-06,  ..., -9.7156e-06,
         -8.1658e-06, -6.9439e-06]], device='cuda:0')
Loss: 0.9384023547172546


Running epoch 1, step 1820, batch 772
Sampled inputs[:2]: tensor([[   0, 1503, 1785,  ...,  221,  380, 1869],
        [   0,   12,  266,  ...,  287, 2888, 4845]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6120e-04, -1.6300e-04, -3.1180e-04,  ..., -1.8076e-04,
         -5.3088e-04, -2.3273e-04],
        [-6.3255e-06, -4.1798e-06,  4.8913e-06,  ..., -5.2974e-06,
         -4.1462e-06, -4.2096e-06],
        [-2.0057e-05, -1.4156e-05,  1.6645e-05,  ..., -1.6823e-05,
         -1.3441e-05, -1.3188e-05],
        [-1.8835e-05, -1.2428e-05,  1.5467e-05,  ..., -1.5810e-05,
         -1.2532e-05, -1.3024e-05],
        [-1.3888e-05, -1.0237e-05,  1.1191e-05,  ..., -1.1936e-05,
         -9.9912e-06, -8.4564e-06]], device='cuda:0')
Loss: 0.9568424820899963


Running epoch 1, step 1821, batch 773
Sampled inputs[:2]: tensor([[   0,  292,   65,  ...,   12,  857,  344],
        [   0, 1471,  266,  ...,  525, 5202,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3959e-04, -1.5512e-04, -2.4910e-04,  ..., -1.0266e-04,
         -2.9993e-04, -1.2349e-04],
        [-7.5847e-06, -4.9658e-06,  5.8189e-06,  ..., -6.3777e-06,
         -4.9658e-06, -5.0962e-06],
        [-2.3991e-05, -1.6764e-05,  1.9774e-05,  ..., -2.0161e-05,
         -1.6019e-05, -1.5885e-05],
        [-2.2560e-05, -1.4722e-05,  1.8358e-05,  ..., -1.8999e-05,
         -1.4991e-05, -1.5721e-05],
        [-1.6630e-05, -1.2137e-05,  1.3307e-05,  ..., -1.4305e-05,
         -1.1913e-05, -1.0192e-05]], device='cuda:0')
Loss: 0.9428039193153381


Running epoch 1, step 1822, batch 774
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,   70,   27, 5311],
        [   0,  266, 5528,  ...,  685,  266, 1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5186e-04, -3.1264e-04, -3.6624e-04,  ..., -1.4253e-04,
         -1.1093e-04, -8.2520e-05],
        [-8.8587e-06, -5.8375e-06,  6.8843e-06,  ..., -7.4431e-06,
         -5.8003e-06, -5.9269e-06],
        [-2.8014e-05, -1.9714e-05,  2.3335e-05,  ..., -2.3559e-05,
         -1.8701e-05, -1.8522e-05],
        [-2.6345e-05, -1.7345e-05,  2.1681e-05,  ..., -2.2188e-05,
         -1.7524e-05, -1.8299e-05],
        [-1.9342e-05, -1.4208e-05,  1.5631e-05,  ..., -1.6645e-05,
         -1.3836e-05, -1.1832e-05]], device='cuda:0')
Loss: 0.9552075862884521


Running epoch 1, step 1823, batch 775
Sampled inputs[:2]: tensor([[   0, 2827, 5744,  ...,  365,  513,   13],
        [   0,  328, 1410,  ..., 7344,   12, 5067]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0125e-05, -6.2558e-04, -6.9344e-04,  ..., -1.8688e-04,
          2.5834e-04, -7.7776e-06],
        [-1.0096e-05, -6.7092e-06,  7.8529e-06,  ..., -8.5086e-06,
         -6.6310e-06, -6.7838e-06],
        [-3.2008e-05, -2.2620e-05,  2.6658e-05,  ..., -2.6956e-05,
         -2.1413e-05, -2.1219e-05],
        [-3.0056e-05, -1.9923e-05,  2.4736e-05,  ..., -2.5347e-05,
         -2.0057e-05, -2.0921e-05],
        [-2.2128e-05, -1.6324e-05,  1.7896e-05,  ..., -1.9059e-05,
         -1.5847e-05, -1.3582e-05]], device='cuda:0')
Loss: 0.9690631031990051
Graident accumulation at epoch 1, step 1823, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.7788e-05, -2.0422e-05, -2.4324e-04,  ...,  2.9430e-05,
          1.7159e-04,  2.7296e-05],
        [-9.9702e-06, -6.7173e-06,  7.1718e-06,  ..., -8.7632e-06,
         -6.9422e-06, -7.0307e-06],
        [-1.0487e-05, -2.8774e-06,  8.5294e-06,  ..., -1.0763e-05,
         -1.5836e-06, -1.4036e-05],
        [-1.8176e-05, -2.7654e-06,  1.3619e-05,  ..., -1.4146e-05,
         -7.3486e-06, -1.3985e-05],
        [-2.3068e-05, -1.7017e-05,  1.7310e-05,  ..., -2.0511e-05,
         -1.7275e-05, -1.4915e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4134e-08, 6.4215e-08, 4.9921e-08,  ..., 4.1928e-08, 1.3132e-07,
         5.7060e-08],
        [8.0160e-11, 5.8538e-11, 2.0433e-11,  ..., 5.6714e-11, 3.5619e-11,
         3.0777e-11],
        [4.3351e-09, 3.0714e-09, 1.5538e-09,  ..., 3.3081e-09, 1.6055e-09,
         1.0618e-09],
        [1.1947e-09, 1.3293e-09, 3.9572e-10,  ..., 1.0600e-09, 7.5238e-10,
         4.7311e-10],
        [3.6722e-10, 2.0919e-10, 8.5562e-11,  ..., 2.7590e-10, 8.5933e-11,
         1.0820e-10]], device='cuda:0')
optimizer state dict: 228.0
lr: [8.703199712272026e-07, 8.703199712272026e-07]
scheduler_last_epoch: 228


Running epoch 1, step 1824, batch 776
Sampled inputs[:2]: tensor([[   0, 5982, 9385,  ...,   26,  469,  446],
        [   0,  346,   14,  ...,  381,  535,  505]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0583e-05, -2.5692e-05,  1.7012e-04,  ..., -3.7691e-05,
         -1.6831e-05,  2.6768e-04],
        [-1.2442e-06, -8.6799e-07,  1.0133e-06,  ..., -1.0878e-06,
         -8.8289e-07, -7.9721e-07],
        [-3.9041e-06, -2.9355e-06,  3.3826e-06,  ..., -3.4273e-06,
         -2.8461e-06, -2.5034e-06],
        [-3.6806e-06, -2.5928e-06,  3.1739e-06,  ..., -3.2485e-06,
         -2.6971e-06, -2.5034e-06],
        [-2.6673e-06, -2.1011e-06,  2.2501e-06,  ..., -2.3842e-06,
         -2.0564e-06, -1.5795e-06]], device='cuda:0')
Loss: 0.9616857767105103


Running epoch 1, step 1825, batch 777
Sampled inputs[:2]: tensor([[    0,  2816,   292,  ...,  3662,   461,  2723],
        [    0,   607, 11059,  ...,  2081,  1194,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6435e-05, -2.4744e-05, -5.0831e-05,  ..., -2.4841e-05,
          2.4673e-05,  5.6587e-05],
        [-2.4065e-06, -1.6876e-06,  1.9483e-06,  ..., -2.1383e-06,
         -1.7472e-06, -1.6093e-06],
        [-7.5847e-06, -5.6326e-06,  6.5416e-06,  ..., -6.7204e-06,
         -5.5581e-06, -5.0217e-06],
        [-7.2122e-06, -5.0217e-06,  6.1691e-06,  ..., -6.4224e-06,
         -5.3197e-06, -5.0664e-06],
        [-5.2601e-06, -4.0978e-06,  4.4107e-06,  ..., -4.7386e-06,
         -4.0829e-06, -3.2112e-06]], device='cuda:0')
Loss: 0.9422213435173035


Running epoch 1, step 1826, batch 778
Sampled inputs[:2]: tensor([[    0,  2518,   437,  ...,    12,  1041,   283],
        [    0,  5129,  1245,  ...,   292, 24298,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5557e-05, -1.1079e-04, -1.3819e-04,  ..., -1.1968e-04,
         -5.7969e-05,  7.7579e-05],
        [-3.6731e-06, -2.4922e-06,  2.8685e-06,  ..., -3.2559e-06,
         -2.6152e-06, -2.4922e-06],
        [-1.1697e-05, -8.4341e-06,  9.7901e-06,  ..., -1.0356e-05,
         -8.4341e-06, -7.8827e-06],
        [-1.0967e-05, -7.3761e-06,  9.0599e-06,  ..., -9.7454e-06,
         -7.9125e-06, -7.7933e-06],
        [-8.1509e-06, -6.1542e-06,  6.6161e-06,  ..., -7.3463e-06,
         -6.2287e-06, -5.0664e-06]], device='cuda:0')
Loss: 0.9624967575073242


Running epoch 1, step 1827, batch 779
Sampled inputs[:2]: tensor([[    0, 25778,  3804,  ...,  2354,    12,   554],
        [    0,  3756,    13,  ...,  1704,   278,  5851]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9978e-05, -2.4655e-04, -2.1599e-04,  ..., -2.4982e-04,
          1.1332e-04,  1.0275e-04],
        [-4.9472e-06, -3.3937e-06,  3.7998e-06,  ..., -4.3660e-06,
         -3.5241e-06, -3.3267e-06],
        [-1.5721e-05, -1.1414e-05,  1.2934e-05,  ..., -1.3858e-05,
         -1.1355e-05, -1.0535e-05],
        [-1.4767e-05, -1.0043e-05,  1.1995e-05,  ..., -1.3053e-05,
         -1.0654e-05, -1.0431e-05],
        [-1.1027e-05, -8.3894e-06,  8.8066e-06,  ..., -9.9093e-06,
         -8.4490e-06, -6.8173e-06]], device='cuda:0')
Loss: 1.0053417682647705


Running epoch 1, step 1828, batch 780
Sampled inputs[:2]: tensor([[    0,   858,    13,  ...,  2253,   847,   300],
        [    0,   278,   266,  ..., 10639,   292,  4723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6413e-04, -3.7844e-04, -3.6862e-04,  ..., -1.9120e-04,
          3.4100e-04,  2.4546e-04],
        [-6.2361e-06, -4.3139e-06,  4.6529e-06,  ..., -5.5730e-06,
         -4.5151e-06, -4.2953e-06],
        [-1.9744e-05, -1.4424e-05,  1.5825e-05,  ..., -1.7583e-05,
         -1.4499e-05, -1.3515e-05],
        [-1.8463e-05, -1.2666e-05,  1.4558e-05,  ..., -1.6525e-05,
         -1.3560e-05, -1.3307e-05],
        [-1.4037e-05, -1.0714e-05,  1.0923e-05,  ..., -1.2755e-05,
         -1.0937e-05, -8.8885e-06]], device='cuda:0')
Loss: 0.9532558917999268


Running epoch 1, step 1829, batch 781
Sampled inputs[:2]: tensor([[    0,   422,    13,  ..., 14026,   368,  4999],
        [    0, 21930,    12,  ...,  2849,   863,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7320e-05, -3.0961e-04, -3.5813e-04,  ..., -2.6566e-04,
          3.2294e-04,  2.0665e-04],
        [-7.5474e-06, -5.2042e-06,  5.5991e-06,  ..., -6.6981e-06,
         -5.4985e-06, -5.2489e-06],
        [-2.3708e-05, -1.7300e-05,  1.8924e-05,  ..., -2.0996e-05,
         -1.7494e-05, -1.6391e-05],
        [-2.2277e-05, -1.5244e-05,  1.7464e-05,  ..., -1.9804e-05,
         -1.6451e-05, -1.6183e-05],
        [-1.7166e-05, -1.3053e-05,  1.3262e-05,  ..., -1.5512e-05,
         -1.3456e-05, -1.1034e-05]], device='cuda:0')
Loss: 0.9743422269821167


Running epoch 1, step 1830, batch 782
Sampled inputs[:2]: tensor([[   0, 7432,  287,  ...,   12,  461, 2652],
        [   0, 4868, 3106,  ..., 2637,  278,  521]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2335e-05, -4.0153e-04, -4.0531e-04,  ..., -2.5073e-04,
          3.3550e-04,  2.7159e-04],
        [-8.7842e-06, -6.0759e-06,  6.5826e-06,  ..., -7.7635e-06,
         -6.3255e-06, -6.0312e-06],
        [-2.7463e-05, -2.0117e-05,  2.2128e-05,  ..., -2.4244e-05,
         -2.0072e-05, -1.8746e-05],
        [-2.5943e-05, -1.7807e-05,  2.0564e-05,  ..., -2.2963e-05,
         -1.8939e-05, -1.8626e-05],
        [-1.9833e-05, -1.5169e-05,  1.5482e-05,  ..., -1.7866e-05,
         -1.5423e-05, -1.2562e-05]], device='cuda:0')
Loss: 0.9770893454551697


Running epoch 1, step 1831, batch 783
Sampled inputs[:2]: tensor([[   0,  874,  590,  ...,  300,  867,  638],
        [   0, 2202,  292,  ..., 2431, 2267, 3423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5244e-04, -5.0550e-04, -4.5146e-04,  ..., -3.5561e-04,
          6.5274e-04,  3.2111e-04],
        [-1.0028e-05, -6.9290e-06,  7.5176e-06,  ..., -8.8811e-06,
         -7.2680e-06, -6.8694e-06],
        [-3.1188e-05, -2.2843e-05,  2.5123e-05,  ..., -2.7597e-05,
         -2.2933e-05, -2.1219e-05],
        [-2.9564e-05, -2.0295e-05,  2.3440e-05,  ..., -2.6226e-05,
         -2.1741e-05, -2.1160e-05],
        [-2.2605e-05, -1.7300e-05,  1.7658e-05,  ..., -2.0415e-05,
         -1.7688e-05, -1.4268e-05]], device='cuda:0')
Loss: 0.9661574363708496
Graident accumulation at epoch 1, step 1831, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.1253e-05, -6.8930e-05, -2.6406e-04,  ..., -9.0743e-06,
          2.1970e-04,  5.6678e-05],
        [-9.9761e-06, -6.7385e-06,  7.2063e-06,  ..., -8.7749e-06,
         -6.9748e-06, -7.0146e-06],
        [-1.2557e-05, -4.8740e-06,  1.0189e-05,  ..., -1.2446e-05,
         -3.7185e-06, -1.4755e-05],
        [-1.9315e-05, -4.5184e-06,  1.4601e-05,  ..., -1.5354e-05,
         -8.7878e-06, -1.4702e-05],
        [-2.3021e-05, -1.7046e-05,  1.7345e-05,  ..., -2.0501e-05,
         -1.7316e-05, -1.4850e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4083e-08, 6.4406e-08, 5.0075e-08,  ..., 4.2012e-08, 1.3162e-07,
         5.7106e-08],
        [8.0181e-11, 5.8528e-11, 2.0469e-11,  ..., 5.6737e-11, 3.5636e-11,
         3.0793e-11],
        [4.3317e-09, 3.0688e-09, 1.5529e-09,  ..., 3.3056e-09, 1.6044e-09,
         1.0612e-09],
        [1.1944e-09, 1.3284e-09, 3.9587e-10,  ..., 1.0596e-09, 7.5210e-10,
         4.7308e-10],
        [3.6737e-10, 2.0928e-10, 8.5788e-11,  ..., 2.7604e-10, 8.6160e-11,
         1.0829e-10]], device='cuda:0')
optimizer state dict: 229.0
lr: [8.205793726931199e-07, 8.205793726931199e-07]
scheduler_last_epoch: 229


Running epoch 1, step 1832, batch 784
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0,   14, 3449,  ...,   12, 2665,    5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8610e-04,  1.2044e-04,  1.0769e-04,  ..., -6.8799e-05,
          8.7003e-05, -2.2301e-05],
        [-1.2219e-06, -8.7172e-07,  8.2329e-07,  ..., -1.0878e-06,
         -9.6112e-07, -8.7172e-07],
        [-3.8147e-06, -2.9653e-06,  2.8163e-06,  ..., -3.4571e-06,
         -3.1143e-06, -2.7120e-06],
        [-3.6210e-06, -2.6226e-06,  2.5481e-06,  ..., -3.2783e-06,
         -2.9951e-06, -2.6673e-06],
        [-2.8163e-06, -2.2203e-06,  2.0266e-06,  ..., -2.5779e-06,
         -2.3991e-06, -1.8626e-06]], device='cuda:0')
Loss: 0.9527314901351929


Running epoch 1, step 1833, batch 785
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,    27,   417,  ...,    18,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7575e-04,  1.2044e-04, -1.2189e-04,  ..., -3.3418e-05,
          1.6751e-04, -1.9408e-04],
        [-2.4587e-06, -1.7285e-06,  1.7397e-06,  ..., -2.1756e-06,
         -1.8440e-06, -1.7248e-06],
        [-7.9274e-06, -5.9456e-06,  6.0201e-06,  ..., -7.0632e-06,
         -6.0499e-06, -5.5432e-06],
        [-7.4357e-06, -5.2601e-06,  5.4985e-06,  ..., -6.6459e-06,
         -5.7369e-06, -5.4091e-06],
        [-5.6922e-06, -4.4107e-06,  4.2170e-06,  ..., -5.1558e-06,
         -4.5747e-06, -3.7178e-06]], device='cuda:0')
Loss: 0.9931629300117493


Running epoch 1, step 1834, batch 786
Sampled inputs[:2]: tensor([[   0,   18,  998,  ..., 5322,  504,  287],
        [   0,  199, 5990,  ...,  278,  638, 5513]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8086e-04,  4.9521e-05, -1.1569e-04,  ..., -1.0503e-04,
          1.8463e-04, -2.7432e-04],
        [-3.6955e-06, -2.6152e-06,  2.7008e-06,  ..., -3.3006e-06,
         -2.7344e-06, -2.5667e-06],
        [-1.1832e-05, -8.9407e-06,  9.2387e-06,  ..., -1.0639e-05,
         -8.9258e-06, -8.1956e-06],
        [-1.1191e-05, -7.9572e-06,  8.5682e-06,  ..., -1.0073e-05,
         -8.5086e-06, -8.1062e-06],
        [-8.4490e-06, -6.6310e-06,  6.4224e-06,  ..., -7.7337e-06,
         -6.7502e-06, -5.4613e-06]], device='cuda:0')
Loss: 0.9775199890136719


Running epoch 1, step 1835, batch 787
Sampled inputs[:2]: tensor([[    0,   292, 44809,  ...,   642,   437,  9038],
        [    0,    14,    71,  ...,   278, 14258, 12440]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5682e-05,  1.6854e-04, -2.7438e-04,  ..., -1.4183e-04,
         -5.1685e-05, -4.5936e-04],
        [-4.8876e-06, -3.4422e-06,  3.6545e-06,  ..., -4.3511e-06,
         -3.6024e-06, -3.3639e-06],
        [-1.5527e-05, -1.1697e-05,  1.2428e-05,  ..., -1.3918e-05,
         -1.1653e-05, -1.0654e-05],
        [-1.4871e-05, -1.0505e-05,  1.1683e-05,  ..., -1.3337e-05,
         -1.1250e-05, -1.0699e-05],
        [-1.1042e-05, -8.6427e-06,  8.5980e-06,  ..., -1.0073e-05,
         -8.7917e-06, -7.0408e-06]], device='cuda:0')
Loss: 0.9569928646087646


Running epoch 1, step 1836, batch 788
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ...,  278, 6607,   13],
        [   0,   20,   13,  ...,  496,   14, 1032]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1857e-04,  3.6623e-04, -2.5253e-04,  ..., -1.8324e-04,
         -8.7687e-05, -6.4227e-04],
        [-6.1691e-06, -4.2692e-06,  4.5300e-06,  ..., -5.5060e-06,
         -4.5486e-06, -4.3400e-06],
        [-1.9297e-05, -1.4335e-05,  1.5244e-05,  ..., -1.7315e-05,
         -1.4529e-05, -1.3486e-05],
        [-1.8537e-05, -1.2890e-05,  1.4320e-05,  ..., -1.6674e-05,
         -1.4022e-05, -1.3545e-05],
        [-1.3947e-05, -1.0714e-05,  1.0684e-05,  ..., -1.2755e-05,
         -1.1131e-05, -9.0823e-06]], device='cuda:0')
Loss: 0.9582975506782532


Running epoch 1, step 1837, batch 789
Sampled inputs[:2]: tensor([[   0, 2228, 1416,  ..., 3766,  266, 1136],
        [   0, 1713,  292,  ...,  596,  328, 1644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2060e-04,  5.1578e-04, -3.1546e-04,  ..., -1.5898e-04,
         -2.8493e-04, -6.2360e-04],
        [-7.5325e-06, -5.1111e-06,  5.4315e-06,  ..., -6.6981e-06,
         -5.4725e-06, -5.2936e-06],
        [-2.3559e-05, -1.7092e-05,  1.8254e-05,  ..., -2.1011e-05,
         -1.7405e-05, -1.6421e-05],
        [-2.2471e-05, -1.5289e-05,  1.7047e-05,  ..., -2.0117e-05,
         -1.6689e-05, -1.6361e-05],
        [-1.7166e-05, -1.2860e-05,  1.2890e-05,  ..., -1.5587e-05,
         -1.3426e-05, -1.1168e-05]], device='cuda:0')
Loss: 0.9504401683807373


Running epoch 1, step 1838, batch 790
Sampled inputs[:2]: tensor([[    0,   259,   587,  ...,    14,    71,   462],
        [    0,  3473,   278,  ..., 11743,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7088e-05,  4.7299e-04, -3.5591e-04,  ..., -3.2834e-04,
         -1.5888e-04, -5.5038e-04],
        [-8.7395e-06, -5.9158e-06,  6.4149e-06,  ..., -7.7486e-06,
         -6.2324e-06, -6.1132e-06],
        [-2.7344e-05, -1.9744e-05,  2.1532e-05,  ..., -2.4274e-05,
         -1.9804e-05, -1.8954e-05],
        [-2.6107e-05, -1.7673e-05,  2.0161e-05,  ..., -2.3246e-05,
         -1.8984e-05, -1.8910e-05],
        [-1.9804e-05, -1.4797e-05,  1.5110e-05,  ..., -1.7911e-05,
         -1.5236e-05, -1.2793e-05]], device='cuda:0')
Loss: 0.941439688205719


Running epoch 1, step 1839, batch 791
Sampled inputs[:2]: tensor([[   0,  221,  380,  ..., 3990,  717,   12],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8910e-05,  4.5436e-04, -6.0776e-04,  ..., -4.1959e-04,
         -4.0074e-04, -7.1302e-04],
        [-1.0021e-05, -6.7577e-06,  7.3649e-06,  ..., -8.8736e-06,
         -7.1004e-06, -7.0520e-06],
        [-3.1427e-05, -2.2516e-05,  2.4766e-05,  ..., -2.7776e-05,
         -2.2560e-05, -2.1860e-05],
        [-2.9862e-05, -2.0087e-05,  2.3097e-05,  ..., -2.6509e-05,
         -2.1532e-05, -2.1726e-05],
        [-2.2769e-05, -1.6898e-05,  1.7390e-05,  ..., -2.0519e-05,
         -1.7367e-05, -1.4745e-05]], device='cuda:0')
Loss: 0.9286960959434509
Graident accumulation at epoch 1, step 1839, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1237e-05, -1.6601e-05, -2.9843e-04,  ..., -5.0126e-05,
          1.5766e-04, -2.0292e-05],
        [-9.9806e-06, -6.7404e-06,  7.2222e-06,  ..., -8.7848e-06,
         -6.9874e-06, -7.0183e-06],
        [-1.4444e-05, -6.6382e-06,  1.1646e-05,  ..., -1.3979e-05,
         -5.6027e-06, -1.5465e-05],
        [-2.0370e-05, -6.0752e-06,  1.5450e-05,  ..., -1.6469e-05,
         -1.0062e-05, -1.5405e-05],
        [-2.2996e-05, -1.7031e-05,  1.7349e-05,  ..., -2.0503e-05,
         -1.7321e-05, -1.4840e-05]], device='cuda:0')
optimizer state dict: tensor([[7.4014e-08, 6.4548e-08, 5.0395e-08,  ..., 4.2146e-08, 1.3164e-07,
         5.7557e-08],
        [8.0201e-11, 5.8515e-11, 2.0503e-11,  ..., 5.6759e-11, 3.5651e-11,
         3.0812e-11],
        [4.3284e-09, 3.0663e-09, 1.5519e-09,  ..., 3.3030e-09, 1.6033e-09,
         1.0606e-09],
        [1.1941e-09, 1.3275e-09, 3.9601e-10,  ..., 1.0593e-09, 7.5181e-10,
         4.7308e-10],
        [3.6752e-10, 2.0935e-10, 8.6005e-11,  ..., 2.7618e-10, 8.6375e-11,
         1.0840e-10]], device='cuda:0')
optimizer state dict: 230.0
lr: [7.722414697591851e-07, 7.722414697591851e-07]
scheduler_last_epoch: 230


Running epoch 1, step 1840, batch 792
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0,     9,   287,  ...,   259,  8244,  1143]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9719e-05, -5.5630e-05, -2.9534e-04,  ...,  3.5508e-05,
         -2.0094e-04, -5.2641e-05],
        [-1.2368e-06, -8.1584e-07,  9.5367e-07,  ..., -1.1101e-06,
         -8.7172e-07, -8.6799e-07],
        [-3.9637e-06, -2.7418e-06,  3.2634e-06,  ..., -3.5316e-06,
         -2.7865e-06, -2.7567e-06],
        [-3.7700e-06, -2.4438e-06,  3.0547e-06,  ..., -3.3826e-06,
         -2.6673e-06, -2.7716e-06],
        [-2.7865e-06, -2.0266e-06,  2.2352e-06,  ..., -2.5332e-06,
         -2.0862e-06, -1.7956e-06]], device='cuda:0')
Loss: 0.9577996134757996


Running epoch 1, step 1841, batch 793
Sampled inputs[:2]: tensor([[    0,  1235,    14,  ...,  3301,   549,    14],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9777e-05, -8.6470e-05, -3.1684e-04,  ..., -2.4897e-05,
         -2.8325e-04, -1.8646e-04],
        [-2.4512e-06, -1.6615e-06,  1.8626e-06,  ..., -2.1979e-06,
         -1.7509e-06, -1.6838e-06],
        [-7.8082e-06, -5.5730e-06,  6.4075e-06,  ..., -6.9588e-06,
         -5.5879e-06, -5.2750e-06],
        [-7.3612e-06, -4.9323e-06,  5.9158e-06,  ..., -6.6161e-06,
         -5.3197e-06, -5.3197e-06],
        [-5.5432e-06, -4.1425e-06,  4.4107e-06,  ..., -5.0068e-06,
         -4.2021e-06, -3.4422e-06]], device='cuda:0')
Loss: 0.9646593928337097


Running epoch 1, step 1842, batch 794
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,  3448,   278,  ...,   380,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1751e-05, -2.1607e-04, -5.8167e-04,  ..., -7.0559e-05,
         -3.4579e-05, -1.0056e-04],
        [-3.7104e-06, -2.4587e-06,  2.7642e-06,  ..., -3.2708e-06,
         -2.5816e-06, -2.5555e-06],
        [-1.1742e-05, -8.1807e-06,  9.4771e-06,  ..., -1.0282e-05,
         -8.1658e-06, -7.9274e-06],
        [-1.1146e-05, -7.3016e-06,  8.7917e-06,  ..., -9.8646e-06,
         -7.8529e-06, -8.0466e-06],
        [-8.3894e-06, -6.1095e-06,  6.5416e-06,  ..., -7.4655e-06,
         -6.2287e-06, -5.2229e-06]], device='cuda:0')
Loss: 0.9723367094993591


Running epoch 1, step 1843, batch 795
Sampled inputs[:2]: tensor([[    0,   391,  9095,  ...,   417,   199,  2038],
        [    0, 21413,  1735,  ..., 10789, 12523,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9768e-05, -4.0464e-04, -7.5152e-04,  ..., -1.8610e-04,
          8.3906e-05, -1.0014e-04],
        [-4.9993e-06, -3.3751e-06,  3.7774e-06,  ..., -4.3884e-06,
         -3.4533e-06, -3.3900e-06],
        [-1.5885e-05, -1.1295e-05,  1.2904e-05,  ..., -1.3888e-05,
         -1.1012e-05, -1.0595e-05],
        [ 4.6268e-05,  9.0141e-05, -3.6483e-05,  ...,  6.1798e-05,
          8.5234e-05,  3.7570e-05],
        [-1.1340e-05, -8.4192e-06,  8.9258e-06,  ..., -1.0073e-05,
         -8.3894e-06, -6.9961e-06]], device='cuda:0')
Loss: 0.9814983010292053


Running epoch 1, step 1844, batch 796
Sampled inputs[:2]: tensor([[   0,  300, 5864,  ...,   12, 3667,  796],
        [   0, 1304, 1040,  ...,  287, 1665,  741]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9888e-05, -5.4857e-04, -7.3537e-04,  ..., -1.5594e-04,
          1.1841e-04, -1.4636e-04],
        [-6.2883e-06, -4.3660e-06,  4.6603e-06,  ..., -5.5507e-06,
         -4.4368e-06, -4.2766e-06],
        [-2.0057e-05, -1.4678e-05,  1.6004e-05,  ..., -1.7643e-05,
         -1.4260e-05, -1.3456e-05],
        [ 4.2483e-05,  8.7235e-05, -3.3741e-05,  ...,  5.8371e-05,
          8.2298e-05,  3.4858e-05],
        [-1.4305e-05, -1.0908e-05,  1.1086e-05,  ..., -1.2800e-05,
         -1.0833e-05, -8.8885e-06]], device='cuda:0')
Loss: 0.9794285893440247


Running epoch 1, step 1845, batch 797
Sampled inputs[:2]: tensor([[    0,  5775,   292,  ...,  8671,  1339,   642],
        [    0,  2258, 10315,  ...,  4185,  9433,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7104e-05, -5.9611e-04, -8.0204e-04,  ..., -1.9721e-04,
          3.6273e-05, -1.9485e-04],
        [-7.4878e-06, -5.1968e-06,  5.6438e-06,  ..., -6.6012e-06,
         -5.2825e-06, -5.0701e-06],
        [-2.3991e-05, -1.7539e-05,  1.9372e-05,  ..., -2.1100e-05,
         -1.7062e-05, -1.6063e-05],
        [ 3.8758e-05,  8.4687e-05, -3.0567e-05,  ...,  5.5122e-05,
          7.9661e-05,  3.2295e-05],
        [-1.6987e-05, -1.2934e-05,  1.3322e-05,  ..., -1.5184e-05,
         -1.2845e-05, -1.0550e-05]], device='cuda:0')
Loss: 0.9886478781700134


Running epoch 1, step 1846, batch 798
Sampled inputs[:2]: tensor([[    0,   591,   688,  ...,   271,  3390,    12],
        [    0,   792,    83,  ..., 29085, 15914,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1176e-06, -5.0326e-04, -7.1289e-04,  ..., -1.1566e-04,
         -2.6314e-04, -3.5084e-04],
        [-8.7023e-06, -6.0387e-06,  6.6496e-06,  ..., -7.6294e-06,
         -6.0871e-06, -5.8450e-06],
        [-2.7806e-05, -2.0340e-05,  2.2754e-05,  ..., -2.4378e-05,
         -1.9655e-05, -1.8522e-05],
        [ 3.5107e-05,  8.2169e-05, -2.7364e-05,  ...,  5.2023e-05,
          7.7217e-05,  2.9837e-05],
        [-1.9684e-05, -1.5020e-05,  1.5631e-05,  ..., -1.7539e-05,
         -1.4812e-05, -1.2152e-05]], device='cuda:0')
Loss: 0.970478355884552


Running epoch 1, step 1847, batch 799
Sampled inputs[:2]: tensor([[    0,   266, 10726,  ..., 13973, 22191, 15913],
        [    0,  8920, 24095,  ...,   278,  2025,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5577e-05, -6.5037e-04, -8.0992e-04,  ..., -2.2048e-04,
         -2.9902e-04, -2.6989e-04],
        [-9.9912e-06, -6.9253e-06,  7.7076e-06,  ..., -8.7023e-06,
         -6.9402e-06, -6.6981e-06],
        [-3.2097e-05, -2.3440e-05,  2.6420e-05,  ..., -2.7984e-05,
         -2.2545e-05, -2.1383e-05],
        [ 3.1352e-05,  7.9591e-05, -2.4160e-05,  ...,  4.8894e-05,
          7.4713e-05,  2.7274e-05],
        [-2.2590e-05, -1.7211e-05,  1.8060e-05,  ..., -2.0027e-05,
         -1.6898e-05, -1.3977e-05]], device='cuda:0')
Loss: 0.9739772081375122
Graident accumulation at epoch 1, step 1847, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4671e-05, -7.9978e-05, -3.4958e-04,  ..., -6.7162e-05,
          1.1199e-04, -4.5252e-05],
        [-9.9816e-06, -6.7589e-06,  7.2707e-06,  ..., -8.7766e-06,
         -6.9826e-06, -6.9863e-06],
        [-1.6210e-05, -8.3183e-06,  1.3124e-05,  ..., -1.5380e-05,
         -7.2969e-06, -1.6057e-05],
        [-1.5197e-05,  2.4914e-06,  1.1489e-05,  ..., -9.9329e-06,
         -1.5847e-06, -1.1137e-05],
        [-2.2955e-05, -1.7049e-05,  1.7420e-05,  ..., -2.0455e-05,
         -1.7279e-05, -1.4753e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3943e-08, 6.4907e-08, 5.1000e-08,  ..., 4.2153e-08, 1.3160e-07,
         5.7572e-08],
        [8.0221e-11, 5.8504e-11, 2.0542e-11,  ..., 5.6778e-11, 3.5664e-11,
         3.0826e-11],
        [4.3251e-09, 3.0637e-09, 1.5511e-09,  ..., 3.3005e-09, 1.6022e-09,
         1.0600e-09],
        [1.1939e-09, 1.3325e-09, 3.9620e-10,  ..., 1.0606e-09, 7.5664e-10,
         4.7335e-10],
        [3.6766e-10, 2.0944e-10, 8.6245e-11,  ..., 2.7631e-10, 8.6574e-11,
         1.0849e-10]], device='cuda:0')
optimizer state dict: 231.0
lr: [7.253136488789125e-07, 7.253136488789125e-07]
scheduler_last_epoch: 231
Epoch 1 | Batch 799/1048 | Training PPL: 2420.713632398106 | time 85.69057297706604
Saving checkpoint at epoch 1, step 1847, batch 799
Epoch 1 | Validation PPL: 6.743980161511573 | Learning rate: 7.253136488789125e-07
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1847, AFTER epoch 1, step 1847


Running epoch 1, step 1848, batch 800
Sampled inputs[:2]: tensor([[   0,  586,  940,  ..., 1471, 2612,  591],
        [   0, 7066, 2737,  ..., 2269,  271,  927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5927e-05,  3.8169e-05, -1.0317e-04,  ..., -2.9183e-05,
         -3.0831e-05,  0.0000e+00],
        [-1.3188e-06, -9.0152e-07,  1.0803e-06,  ..., -1.0654e-06,
         -8.0466e-07, -8.1956e-07],
        [-4.2021e-06, -3.0100e-06,  3.6210e-06,  ..., -3.4124e-06,
         -2.5928e-06, -2.6375e-06],
        [-3.9041e-06, -2.6375e-06,  3.3230e-06,  ..., -3.1292e-06,
         -2.3693e-06, -2.5183e-06],
        [-2.8461e-06, -2.1309e-06,  2.3842e-06,  ..., -2.3544e-06,
         -1.8775e-06, -1.6540e-06]], device='cuda:0')
Loss: 0.9858120679855347


Running epoch 1, step 1849, batch 801
Sampled inputs[:2]: tensor([[   0,   12,  895,  ...,   13, 2900,   14],
        [   0,  221,  527,  ...,  298,  335,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3052e-04,  5.6211e-05, -1.8110e-04,  ..., -4.2407e-05,
         -1.7845e-04, -3.0831e-05],
        [-2.6077e-06, -1.6876e-06,  2.1085e-06,  ..., -2.1681e-06,
         -1.5944e-06, -1.7360e-06],
        [-8.2552e-06, -5.5581e-06,  7.0632e-06,  ..., -6.8247e-06,
         -5.0664e-06, -5.4389e-06],
        [-7.6741e-06, -4.9025e-06,  6.4671e-06,  ..., -6.3479e-06,
         -4.6790e-06, -5.2601e-06],
        [-5.7220e-06, -4.0010e-06,  4.7386e-06,  ..., -4.8280e-06,
         -3.7476e-06, -3.5092e-06]], device='cuda:0')
Loss: 0.9492073059082031


Running epoch 1, step 1850, batch 802
Sampled inputs[:2]: tensor([[    0,    14,  1147,  ...,    19,    14, 42301],
        [    0,    15, 43895,  ...,   292,   380, 16795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1710e-04,  9.4738e-05, -1.8367e-04,  ...,  1.3302e-04,
         -2.3066e-04, -4.7170e-05],
        [-3.9116e-06, -2.5183e-06,  3.0622e-06,  ..., -3.3081e-06,
         -2.4512e-06, -2.6710e-06],
        [-1.2517e-05, -8.4341e-06,  1.0386e-05,  ..., -1.0565e-05,
         -7.9423e-06, -8.4490e-06],
        [-1.1519e-05, -7.3463e-06,  9.4324e-06,  ..., -9.7454e-06,
         -7.2718e-06, -8.1062e-06],
        [-8.7917e-06, -6.1318e-06,  7.0781e-06,  ..., -7.5698e-06,
         -5.9381e-06, -5.5656e-06]], device='cuda:0')
Loss: 0.9436807036399841


Running epoch 1, step 1851, batch 803
Sampled inputs[:2]: tensor([[    0,  6481,   298,  ...,  6145, 16858,   824],
        [    0,   298,  2587,  ...,   298,   894,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2053e-05,  1.1687e-04,  3.5151e-05,  ...,  2.3300e-05,
         -4.9213e-04,  4.0017e-05],
        [-5.2974e-06, -3.2373e-06,  3.9749e-06,  ..., -4.4331e-06,
         -3.2000e-06, -3.7067e-06],
        [-1.6898e-05, -1.0893e-05,  1.3500e-05,  ..., -1.4096e-05,
         -1.0401e-05, -1.1683e-05],
        [-1.5393e-05, -9.3132e-06,  1.2085e-05,  ..., -1.2890e-05,
         -9.4026e-06, -1.1027e-05],
        [-1.1966e-05, -7.9721e-06,  9.2536e-06,  ..., -1.0207e-05,
         -7.8604e-06, -7.7561e-06]], device='cuda:0')
Loss: 0.9052690863609314


Running epoch 1, step 1852, batch 804
Sampled inputs[:2]: tensor([[    0,   527,   496,  ...,    12,   795,  8296],
        [    0,  1855,    14,  ...,    12,   287, 16479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5085e-05,  1.3901e-04,  5.7523e-06,  ..., -1.0412e-04,
         -5.0270e-04, -1.0983e-04],
        [-6.5565e-06, -4.1313e-06,  4.8466e-06,  ..., -5.5432e-06,
         -4.0941e-06, -4.5672e-06],
        [-2.0862e-05, -1.3947e-05,  1.6496e-05,  ..., -1.7628e-05,
         -1.3292e-05, -1.4380e-05],
        [-1.9073e-05, -1.1966e-05,  1.4812e-05,  ..., -1.6183e-05,
         -1.2085e-05, -1.3664e-05],
        [-1.4871e-05, -1.0297e-05,  1.1384e-05,  ..., -1.2845e-05,
         -1.0110e-05, -9.5814e-06]], device='cuda:0')
Loss: 0.9508439302444458


Running epoch 1, step 1853, batch 805
Sampled inputs[:2]: tensor([[    0,  4014,    88,  ...,  1103,    14,  1771],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7341e-04,  1.6285e-04, -1.4553e-04,  ..., -6.7166e-05,
         -6.2730e-04, -1.0920e-04],
        [-7.8008e-06, -5.0329e-06,  5.8450e-06,  ..., -6.6236e-06,
         -4.9509e-06, -5.3793e-06],
        [ 6.5371e-05,  3.9449e-05, -4.2619e-05,  ...,  5.6282e-05,
          9.0546e-05,  2.6195e-05],
        [-2.2888e-05, -1.4707e-05,  1.8001e-05,  ..., -1.9506e-05,
         -1.4707e-05, -1.6257e-05],
        [-1.7598e-05, -1.2442e-05,  1.3620e-05,  ..., -1.5274e-05,
         -1.2122e-05, -1.1228e-05]], device='cuda:0')
Loss: 0.9805185198783875


Running epoch 1, step 1854, batch 806
Sampled inputs[:2]: tensor([[   0,  365, 8790,  ..., 1172, 8806,  266],
        [   0, 3115, 1640,  ...,  300,  266, 5453]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0516e-05,  2.1622e-04, -2.1421e-04,  ..., -3.0655e-05,
         -7.6667e-04, -1.4109e-04],
        [-9.0078e-06, -5.8785e-06,  6.7540e-06,  ..., -7.6890e-06,
         -5.8189e-06, -6.1914e-06],
        [ 6.1526e-05,  3.6693e-05, -3.9534e-05,  ...,  5.2959e-05,
          8.7805e-05,  2.3722e-05],
        [-2.6494e-05, -1.7166e-05,  2.0877e-05,  ..., -2.2680e-05,
         -1.7330e-05, -1.8790e-05],
        [-2.0280e-05, -1.4454e-05,  1.5736e-05,  ..., -1.7628e-05,
         -1.4149e-05, -1.2815e-05]], device='cuda:0')
Loss: 0.9447101354598999


Running epoch 1, step 1855, batch 807
Sampled inputs[:2]: tensor([[    0, 25409,   287,  ...,  1005,   344,  3493],
        [    0,   546,   360,  ...,    12,   461,  8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0516e-05,  2.6752e-04, -1.7834e-04,  ...,  2.6075e-05,
         -6.8579e-04, -1.6370e-04],
        [-1.0259e-05, -6.7204e-06,  7.7076e-06,  ..., -8.7842e-06,
         -6.6720e-06, -7.0333e-06],
        [ 5.7682e-05,  3.3966e-05, -3.6390e-05,  ...,  4.9591e-05,
          8.5122e-05,  2.1203e-05],
        [-3.0279e-05, -1.9714e-05,  2.3901e-05,  ..., -2.6032e-05,
         -1.9997e-05, -2.1413e-05],
        [-2.2948e-05, -1.6421e-05,  1.7852e-05,  ..., -1.9997e-05,
         -1.6116e-05, -1.4439e-05]], device='cuda:0')
Loss: 0.9708548784255981
Graident accumulation at epoch 1, step 1855, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4152e-05, -4.5228e-05, -3.3246e-04,  ..., -5.7838e-05,
          3.2213e-05, -5.7096e-05],
        [-1.0009e-05, -6.7550e-06,  7.3144e-06,  ..., -8.7773e-06,
         -6.9516e-06, -6.9910e-06],
        [-8.8204e-06, -4.0899e-06,  8.1724e-06,  ..., -8.8826e-06,
          1.9450e-06, -1.2331e-05],
        [-1.6706e-05,  2.7086e-07,  1.2731e-05,  ..., -1.1543e-05,
         -3.4260e-06, -1.2164e-05],
        [-2.2955e-05, -1.6986e-05,  1.7463e-05,  ..., -2.0409e-05,
         -1.7162e-05, -1.4722e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3876e-08, 6.4914e-08, 5.0981e-08,  ..., 4.2111e-08, 1.3194e-07,
         5.7542e-08],
        [8.0246e-11, 5.8491e-11, 2.0581e-11,  ..., 5.6798e-11, 3.5673e-11,
         3.0845e-11],
        [4.3241e-09, 3.0618e-09, 1.5509e-09,  ..., 3.2997e-09, 1.6079e-09,
         1.0594e-09],
        [1.1936e-09, 1.3315e-09, 3.9637e-10,  ..., 1.0602e-09, 7.5628e-10,
         4.7334e-10],
        [3.6782e-10, 2.0950e-10, 8.6477e-11,  ..., 2.7643e-10, 8.6748e-11,
         1.0859e-10]], device='cuda:0')
optimizer state dict: 232.0
lr: [6.798030810329725e-07, 6.798030810329725e-07]
scheduler_last_epoch: 232


Running epoch 1, step 1856, batch 808
Sampled inputs[:2]: tensor([[    0,    14,   747,  ...,   367,   300,   369],
        [    0, 13576,   431,  ...,    14,   475,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9999e-05,  9.7050e-05, -8.3100e-05,  ...,  1.1744e-05,
         -6.9262e-05, -8.9091e-05],
        [-1.2219e-06, -8.1584e-07,  9.4995e-07,  ..., -1.0356e-06,
         -7.8604e-07, -8.2329e-07],
        [-3.9935e-06, -2.7120e-06,  3.2932e-06,  ..., -3.3379e-06,
         -2.5332e-06, -2.6524e-06],
        [-3.7402e-06, -2.4289e-06,  3.0398e-06,  ..., -3.1441e-06,
         -2.3842e-06, -2.6077e-06],
        [-2.7567e-06, -1.9670e-06,  2.2203e-06,  ..., -2.3544e-06,
         -1.8850e-06, -1.6838e-06]], device='cuda:0')
Loss: 0.9811367392539978


Running epoch 1, step 1857, batch 809
Sampled inputs[:2]: tensor([[    0,   278, 14971,  ...,  2341,   266,   717],
        [    0,   923,    13,  ...,   300,  8262,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8093e-05,  2.1701e-04, -6.1020e-05,  ..., -5.7508e-05,
         -1.4982e-04, -1.8928e-04],
        [-2.4959e-06, -1.5758e-06,  1.8291e-06,  ..., -2.1383e-06,
         -1.6652e-06, -1.6987e-06],
        [-7.9870e-06, -5.2452e-06,  6.2734e-06,  ..., -6.7651e-06,
         -5.3197e-06, -5.3197e-06],
        [-7.6145e-06, -4.6939e-06,  5.8562e-06,  ..., -6.4820e-06,
         -5.0664e-06, -5.3197e-06],
        [-5.6326e-06, -3.8594e-06,  4.3064e-06,  ..., -4.8876e-06,
         -4.0457e-06, -3.4794e-06]], device='cuda:0')
Loss: 0.9340525269508362


Running epoch 1, step 1858, batch 810
Sampled inputs[:2]: tensor([[    0,  9342,   600,  ...,   199, 12095,   291],
        [    0,   328,   490,  ...,  6280,  4283,  4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2704e-05,  1.2452e-04, -3.4751e-05,  ..., -1.0538e-04,
         -1.5350e-04, -1.0182e-04],
        [-3.7774e-06, -2.4252e-06,  2.8573e-06,  ..., -3.1814e-06,
         -2.4065e-06, -2.5257e-06],
        [-1.2010e-05, -8.0913e-06,  9.7007e-06,  ..., -1.0073e-05,
         -7.7337e-06, -7.9274e-06],
        [-1.1489e-05, -7.2569e-06,  9.1046e-06,  ..., -9.6112e-06,
         -7.3314e-06, -7.9125e-06],
        [-8.2999e-06, -5.8413e-06,  6.5118e-06,  ..., -7.1228e-06,
         -5.7817e-06, -5.0738e-06]], device='cuda:0')
Loss: 0.9314969182014465


Running epoch 1, step 1859, batch 811
Sampled inputs[:2]: tensor([[    0,  3889,  4039,  ...,   616, 22910,   259],
        [    0,  6945,  2360,  ...,    30,   413,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5496e-07,  2.8569e-04, -1.7250e-04,  ..., -1.2381e-04,
         -1.2674e-04,  1.8374e-05],
        [-5.1782e-06, -3.2596e-06,  3.7439e-06,  ..., -4.3884e-06,
         -3.3267e-06, -3.5539e-06],
        [-1.6451e-05, -1.0967e-05,  1.2755e-05,  ..., -1.3947e-05,
         -1.0774e-05, -1.1221e-05],
        [-1.5542e-05, -9.6858e-06,  1.1787e-05,  ..., -1.3158e-05,
         -1.0073e-05, -1.0982e-05],
        [-1.1623e-05, -8.0615e-06,  8.7172e-06,  ..., -1.0088e-05,
         -8.1807e-06, -7.3984e-06]], device='cuda:0')
Loss: 0.9522533416748047


Running epoch 1, step 1860, batch 812
Sampled inputs[:2]: tensor([[   0,  726, 3979,  ...,   27, 2085,   12],
        [   0,  706, 6989,  ..., 6914,   15, 2537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4583e-05,  3.1459e-04, -1.0890e-05,  ..., -1.2325e-04,
          1.0538e-04,  1.3315e-04],
        [-6.4299e-06, -4.1798e-06,  4.6603e-06,  ..., -5.4911e-06,
         -4.2394e-06, -4.3847e-06],
        [-2.0415e-05, -1.4007e-05,  1.5855e-05,  ..., -1.7449e-05,
         -1.3679e-05, -1.3843e-05],
        [-1.9267e-05, -1.2398e-05,  1.4648e-05,  ..., -1.6451e-05,
         -1.2800e-05, -1.3575e-05],
        [-1.4499e-05, -1.0356e-05,  1.0908e-05,  ..., -1.2666e-05,
         -1.0401e-05, -9.1717e-06]], device='cuda:0')
Loss: 0.9789024591445923


Running epoch 1, step 1861, batch 813
Sampled inputs[:2]: tensor([[   0, 4845, 1521,  ...,  963,  292, 6414],
        [   0,   17, 4110,  ...,  287, 7115,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4390e-05,  3.4636e-04, -8.0496e-05,  ..., -1.2519e-04,
         -4.7328e-05,  1.2799e-04],
        [-7.7039e-06, -5.0887e-06,  5.7779e-06,  ..., -6.5193e-06,
         -5.0105e-06, -5.1819e-06],
        [-2.4498e-05, -1.7032e-05,  1.9580e-05,  ..., -2.0757e-05,
         -1.6183e-05, -1.6391e-05],
        [-2.3007e-05, -1.5050e-05,  1.8075e-05,  ..., -1.9461e-05,
         -1.5065e-05, -1.6004e-05],
        [-1.7151e-05, -1.2442e-05,  1.3292e-05,  ..., -1.4886e-05,
         -1.2182e-05, -1.0706e-05]], device='cuda:0')
Loss: 0.9666712880134583


Running epoch 1, step 1862, batch 814
Sampled inputs[:2]: tensor([[    0,   298,   301,  ...,    13, 10308,  2129],
        [    0,  2911,   287,  ...,  2178, 22788,  8645]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4405e-05,  5.4645e-04, -1.3326e-04,  ..., -1.1458e-04,
         -3.9227e-04,  1.3072e-04],
        [-9.0823e-06, -5.9418e-06,  6.7055e-06,  ..., -7.7188e-06,
         -5.9009e-06, -6.1579e-06],
        [-2.8878e-05, -1.9923e-05,  2.2739e-05,  ..., -2.4572e-05,
         -1.9103e-05, -1.9446e-05],
        [-2.6971e-05, -1.7509e-05,  2.0891e-05,  ..., -2.2948e-05,
         -1.7717e-05, -1.8835e-05],
        [-2.0385e-05, -1.4603e-05,  1.5512e-05,  ..., -1.7762e-05,
         -1.4432e-05, -1.2852e-05]], device='cuda:0')
Loss: 0.9357188940048218


Running epoch 1, step 1863, batch 815
Sampled inputs[:2]: tensor([[    0,   292,    17,  ...,  5760,  1345,   578],
        [    0,  1487,  2511,  ..., 27735,   760,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4361e-05,  9.0534e-04,  1.1661e-04,  ..., -1.3053e-04,
         -6.1557e-04,  1.4604e-04],
        [-1.0408e-05, -6.7353e-06,  7.4990e-06,  ..., -8.9407e-06,
         -6.8210e-06, -7.1563e-06],
        [-3.3140e-05, -2.2650e-05,  2.5555e-05,  ..., -2.8506e-05,
         -2.2113e-05, -2.2620e-05],
        [-3.0994e-05, -1.9908e-05,  2.3425e-05,  ..., -2.6703e-05,
         -2.0579e-05, -2.1964e-05],
        [-2.3648e-05, -1.6734e-05,  1.7598e-05,  ..., -2.0787e-05,
         -1.6846e-05, -1.5132e-05]], device='cuda:0')
Loss: 0.9267138838768005
Graident accumulation at epoch 1, step 1863, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3010e-06,  4.9829e-05, -2.8755e-04,  ..., -6.5107e-05,
         -3.2565e-05, -3.6783e-05],
        [-1.0049e-05, -6.7531e-06,  7.3329e-06,  ..., -8.7937e-06,
         -6.9385e-06, -7.0075e-06],
        [-1.1252e-05, -5.9459e-06,  9.9107e-06,  ..., -1.0845e-05,
         -4.6084e-07, -1.3360e-05],
        [-1.8134e-05, -1.7470e-06,  1.3800e-05,  ..., -1.3059e-05,
         -5.1412e-06, -1.3144e-05],
        [-2.3024e-05, -1.6961e-05,  1.7477e-05,  ..., -2.0447e-05,
         -1.7131e-05, -1.4763e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3807e-08, 6.5668e-08, 5.0944e-08,  ..., 4.2086e-08, 1.3219e-07,
         5.7505e-08],
        [8.0274e-11, 5.8478e-11, 2.0616e-11,  ..., 5.6821e-11, 3.5683e-11,
         3.0865e-11],
        [4.3209e-09, 3.0593e-09, 1.5500e-09,  ..., 3.2972e-09, 1.6068e-09,
         1.0589e-09],
        [1.1934e-09, 1.3306e-09, 3.9653e-10,  ..., 1.0599e-09, 7.5595e-10,
         4.7334e-10],
        [3.6801e-10, 2.0957e-10, 8.6700e-11,  ..., 2.7659e-10, 8.6945e-11,
         1.0871e-10]], device='cuda:0')
optimizer state dict: 233.0
lr: [6.357167206333992e-07, 6.357167206333992e-07]
scheduler_last_epoch: 233


Running epoch 1, step 1864, batch 816
Sampled inputs[:2]: tensor([[    0,  1074,  1593,  ...,   992,  1810,   300],
        [    0, 17442,  2416,  ...,  7244,    66, 16907]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0791e-04, -3.7624e-05, -1.1736e-05,  ..., -7.7145e-05,
         -1.9998e-04, -9.1594e-05],
        [-1.2442e-06, -8.4564e-07,  9.9838e-07,  ..., -1.0505e-06,
         -8.2329e-07, -8.0466e-07],
        [-3.9339e-06, -2.7865e-06,  3.3826e-06,  ..., -3.3081e-06,
         -2.6375e-06, -2.4885e-06],
        [-3.7402e-06, -2.5034e-06,  3.1739e-06,  ..., -3.1441e-06,
         -2.4885e-06, -2.4885e-06],
        [-2.6822e-06, -1.9819e-06,  2.2650e-06,  ..., -2.2948e-06,
         -1.9222e-06, -1.5721e-06]], device='cuda:0')
Loss: 0.9737088084220886


Running epoch 1, step 1865, batch 817
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,   346,   462,  ...,  2915,   275,  2565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0291e-04,  1.6231e-04,  2.0539e-04,  ..., -8.0701e-05,
         -4.8982e-04, -1.5049e-04],
        [-2.5630e-06, -1.4789e-06,  1.8515e-06,  ..., -2.1905e-06,
         -1.6317e-06, -1.8701e-06],
        [-7.8082e-06, -4.7833e-06,  6.1393e-06,  ..., -6.5714e-06,
         -5.0068e-06, -5.4538e-06],
        [-7.4804e-06, -4.2543e-06,  5.7220e-06,  ..., -6.3777e-06,
         -4.8131e-06, -5.4985e-06],
        [-5.6624e-06, -3.5390e-06,  4.3064e-06,  ..., -4.8280e-06,
         -3.8445e-06, -3.6880e-06]], device='cuda:0')
Loss: 0.8890833258628845


Running epoch 1, step 1866, batch 818
Sampled inputs[:2]: tensor([[    0, 11054,    12,  ...,   560,   199,   677],
        [    0,  1901, 11083,  ...,   360,  6055,  2374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5673e-06,  2.9784e-04,  9.1944e-05,  ..., -2.3690e-04,
         -5.8853e-04, -1.4163e-04],
        [-3.7700e-06, -2.4140e-06,  2.6412e-06,  ..., -3.3528e-06,
         -2.7120e-06, -2.7269e-06],
        [-1.1533e-05, -7.8231e-06,  8.8066e-06,  ..., -1.0148e-05,
         -8.4043e-06, -8.0764e-06],
        [-1.1072e-05, -7.0259e-06,  8.1956e-06,  ..., -9.8646e-06,
         -8.1211e-06, -8.1658e-06],
        [-8.5533e-06, -5.9679e-06,  6.3181e-06,  ..., -7.6443e-06,
         -6.5863e-06, -5.5879e-06]], device='cuda:0')
Loss: 0.9725556373596191


Running epoch 1, step 1867, batch 819
Sampled inputs[:2]: tensor([[   0,   14,  475,  ..., 6895, 5842, 2239],
        [   0,   19,   14,  ...,  278, 2588,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7912e-05,  4.0740e-04,  1.0236e-04,  ..., -1.5147e-04,
         -5.9684e-04, -1.4947e-04],
        [-4.9770e-06, -3.2894e-06,  3.5688e-06,  ..., -4.4182e-06,
         -3.5837e-06, -3.5167e-06],
        [-1.5408e-05, -1.0774e-05,  1.1981e-05,  ..., -1.3590e-05,
         -1.1265e-05, -1.0580e-05],
        [-1.4707e-05, -9.6336e-06,  1.1146e-05,  ..., -1.3068e-05,
         -1.0774e-05, -1.0625e-05],
        [-1.1265e-05, -8.1137e-06,  8.4788e-06,  ..., -1.0088e-05,
         -8.7023e-06, -7.2122e-06]], device='cuda:0')
Loss: 0.9661686420440674


Running epoch 1, step 1868, batch 820
Sampled inputs[:2]: tensor([[    0,   925,   271,  ...,   631,  3370,   940],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0917e-04,  3.4687e-04,  7.0469e-05,  ..., -1.1991e-04,
         -4.8546e-04, -1.0119e-04],
        [-6.3777e-06, -4.1462e-06,  4.5374e-06,  ..., -5.5581e-06,
         -4.4480e-06, -4.4256e-06],
        [-1.9670e-05, -1.3590e-05,  1.5154e-05,  ..., -1.7077e-05,
         -1.4022e-05, -1.3337e-05],
        [-1.8671e-05, -1.2092e-05,  1.4037e-05,  ..., -1.6317e-05,
         -1.3307e-05, -1.3277e-05],
        [-1.4335e-05, -1.0230e-05,  1.0714e-05,  ..., -1.2651e-05,
         -1.0818e-05, -9.0823e-06]], device='cuda:0')
Loss: 0.9564735293388367


Running epoch 1, step 1869, batch 821
Sampled inputs[:2]: tensor([[    0,  2352,  4275,  ..., 10518,   342,   266],
        [    0,    89,  2023,  ...,   271,    13,   704]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6927e-04,  3.5920e-04,  8.9885e-05,  ..., -1.7464e-04,
         -5.6950e-04, -1.3496e-05],
        [-7.5996e-06, -4.9993e-06,  5.4911e-06,  ..., -6.6310e-06,
         -5.3309e-06, -5.2266e-06],
        [-2.3544e-05, -1.6436e-05,  1.8373e-05,  ..., -2.0489e-05,
         -1.6853e-05, -1.5855e-05],
        [-2.2382e-05, -1.4655e-05,  1.7077e-05,  ..., -1.9580e-05,
         -1.6019e-05, -1.5825e-05],
        [-1.7077e-05, -1.2331e-05,  1.2934e-05,  ..., -1.5110e-05,
         -1.2934e-05, -1.0729e-05]], device='cuda:0')
Loss: 0.9884198307991028


Running epoch 1, step 1870, batch 822
Sampled inputs[:2]: tensor([[   0,   14,  381,  ..., 2195,  278,  266],
        [   0,  365, 5911,  ...,  925,  408,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4562e-04,  4.4809e-04, -4.9267e-05,  ..., -1.9009e-04,
         -3.3005e-04, -6.3352e-05],
        [-8.8215e-06, -5.8375e-06,  6.3479e-06,  ..., -7.7039e-06,
         -6.2063e-06, -6.1169e-06],
        [-2.7359e-05, -1.9133e-05,  2.1279e-05,  ..., -2.3782e-05,
         -1.9580e-05, -1.8507e-05],
        [-2.5958e-05, -1.7025e-05,  1.9699e-05,  ..., -2.2724e-05,
         -1.8597e-05, -1.8477e-05],
        [-1.9804e-05, -1.4313e-05,  1.4961e-05,  ..., -1.7494e-05,
         -1.4976e-05, -1.2465e-05]], device='cuda:0')
Loss: 0.951740562915802


Running epoch 1, step 1871, batch 823
Sampled inputs[:2]: tensor([[    0,  3761,    12,  ...,    14,    22,   287],
        [    0, 24063,   717,  ...,  2228,  1416,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5058e-04,  3.2372e-04, -2.6161e-04,  ..., -8.7467e-05,
         -1.9942e-04,  9.4834e-05],
        [-1.0103e-05, -6.6012e-06,  7.2420e-06,  ..., -8.8662e-06,
         -7.0445e-06, -7.1302e-06],
        [-3.1412e-05, -2.1636e-05,  2.4319e-05,  ..., -2.7344e-05,
         -2.2173e-05, -2.1547e-05],
        [-2.9624e-05, -1.9170e-05,  2.2382e-05,  ..., -2.6047e-05,
         -2.1011e-05, -2.1413e-05],
        [-2.2754e-05, -1.6190e-05,  1.7136e-05,  ..., -2.0117e-05,
         -1.6972e-05, -1.4506e-05]], device='cuda:0')
Loss: 0.944200336933136
Graident accumulation at epoch 1, step 1871, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.9829e-05,  7.7218e-05, -2.8496e-04,  ..., -6.7343e-05,
         -4.9251e-05, -2.3621e-05],
        [-1.0055e-05, -6.7379e-06,  7.3238e-06,  ..., -8.8009e-06,
         -6.9491e-06, -7.0198e-06],
        [-1.3268e-05, -7.5149e-06,  1.1351e-05,  ..., -1.2495e-05,
         -2.6321e-06, -1.4179e-05],
        [-1.9283e-05, -3.4894e-06,  1.4658e-05,  ..., -1.4358e-05,
         -6.7282e-06, -1.3971e-05],
        [-2.2997e-05, -1.6884e-05,  1.7443e-05,  ..., -2.0414e-05,
         -1.7115e-05, -1.4737e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3796e-08, 6.5707e-08, 5.0961e-08,  ..., 4.2052e-08, 1.3210e-07,
         5.7457e-08],
        [8.0296e-11, 5.8463e-11, 2.0648e-11,  ..., 5.6843e-11, 3.5697e-11,
         3.0885e-11],
        [4.3175e-09, 3.0567e-09, 1.5490e-09,  ..., 3.2946e-09, 1.6057e-09,
         1.0583e-09],
        [1.1930e-09, 1.3296e-09, 3.9663e-10,  ..., 1.0595e-09, 7.5563e-10,
         4.7333e-10],
        [3.6816e-10, 2.0962e-10, 8.6907e-11,  ..., 2.7671e-10, 8.7146e-11,
         1.0881e-10]], device='cuda:0')
optimizer state dict: 234.0
lr: [5.930613044608946e-07, 5.930613044608946e-07]
scheduler_last_epoch: 234


Running epoch 1, step 1872, batch 824
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ...,   298,   365,  4920],
        [    0, 39224,    34,  ...,   401,  1716,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8017e-05,  1.6457e-04,  3.8262e-05,  ..., -8.5114e-05,
          2.1813e-05, -3.1338e-05],
        [-1.2890e-06, -7.5623e-07,  7.5623e-07,  ..., -1.1697e-06,
         -8.9779e-07, -1.0133e-06],
        [-3.9041e-06, -2.4140e-06,  2.5481e-06,  ..., -3.4571e-06,
         -2.6822e-06, -2.9802e-06],
        [-3.6955e-06, -2.1160e-06,  2.2501e-06,  ..., -3.3528e-06,
         -2.5779e-06, -2.9653e-06],
        [-3.0994e-06, -1.9521e-06,  1.9222e-06,  ..., -2.7865e-06,
         -2.2352e-06, -2.1905e-06]], device='cuda:0')
Loss: 0.9257464408874512


Running epoch 1, step 1873, batch 825
Sampled inputs[:2]: tensor([[    0,   850,    13,  ..., 11823,    13, 30706],
        [    0,  1356,   634,  ...,  6604,   634,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6220e-04,  2.2736e-04,  4.0235e-05,  ..., -1.9433e-04,
          8.8828e-05, -1.8848e-04],
        [-2.5034e-06, -1.6280e-06,  1.7919e-06,  ..., -2.1830e-06,
         -1.6727e-06, -1.7807e-06],
        [-7.7784e-06, -5.3048e-06,  6.0052e-06,  ..., -6.7055e-06,
         -5.2005e-06, -5.3942e-06],
        [-7.3165e-06, -4.6790e-06,  5.4687e-06,  ..., -6.3777e-06,
         -4.9174e-06, -5.3495e-06],
        [-5.7667e-06, -4.0531e-06,  4.2468e-06,  ..., -5.0813e-06,
         -4.0978e-06, -3.7327e-06]], device='cuda:0')
Loss: 0.9512452483177185


Running epoch 1, step 1874, batch 826
Sampled inputs[:2]: tensor([[    0,    13, 11273,  ...,   292,  1057,    14],
        [    0,   546, 28676,  ...,   271,  1267,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5994e-04,  3.5933e-04,  2.6502e-04,  ..., -2.6419e-04,
          2.8398e-05, -2.6238e-04],
        [-3.7625e-06, -2.5295e-06,  2.6561e-06,  ..., -3.3081e-06,
         -2.6412e-06, -2.7418e-06],
        [-1.1742e-05, -8.2850e-06,  8.9407e-06,  ..., -1.0222e-05,
         -8.2850e-06, -8.4043e-06],
        [-1.1072e-05, -7.3165e-06,  8.1360e-06,  ..., -9.7305e-06,
         -7.8231e-06, -8.2850e-06],
        [-8.7619e-06, -6.3628e-06,  6.3926e-06,  ..., -7.8082e-06,
         -6.5565e-06, -5.8785e-06]], device='cuda:0')
Loss: 0.9573307037353516


Running epoch 1, step 1875, batch 827
Sampled inputs[:2]: tensor([[    0,   320,   472,  ...,  1345,    14,  1869],
        [    0,  3398,   271,  ...,    13,  1581, 13600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1102e-05,  2.1571e-04,  2.0420e-04,  ..., -2.8462e-04,
          1.8337e-04, -3.1221e-05],
        [-5.0515e-06, -3.3751e-06,  3.6843e-06,  ..., -4.3958e-06,
         -3.5092e-06, -3.5726e-06],
        [-1.5706e-05, -1.1086e-05,  1.2308e-05,  ..., -1.3575e-05,
         -1.0997e-05, -1.0923e-05],
        [-1.4916e-05, -9.8348e-06,  1.1370e-05,  ..., -1.2964e-05,
         -1.0446e-05, -1.0893e-05],
        [-1.1578e-05, -8.4490e-06,  8.7321e-06,  ..., -1.0237e-05,
         -8.5980e-06, -7.5325e-06]], device='cuda:0')
Loss: 0.9560163021087646


Running epoch 1, step 1876, batch 828
Sampled inputs[:2]: tensor([[   0,  199, 7513,  ...,  271,  259,  957],
        [   0,  221,  709,  ..., 3365, 3504,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0805e-04,  2.9141e-04,  1.9303e-04,  ..., -2.4260e-04,
          9.7138e-05,  8.4086e-05],
        [-6.4149e-06, -4.2208e-06,  4.7348e-06,  ..., -5.4762e-06,
         -4.3362e-06, -4.4592e-06],
        [-2.0087e-05, -1.3933e-05,  1.5870e-05,  ..., -1.7062e-05,
         -1.3694e-05, -1.3754e-05],
        [-1.8939e-05, -1.2308e-05,  1.4603e-05,  ..., -1.6153e-05,
         -1.2904e-05, -1.3590e-05],
        [-1.4529e-05, -1.0446e-05,  1.1086e-05,  ..., -1.2636e-05,
         -1.0535e-05, -9.3207e-06]], device='cuda:0')
Loss: 0.9551611542701721


Running epoch 1, step 1877, batch 829
Sampled inputs[:2]: tensor([[   0,  257,  298,  ..., 1878,  328,  259],
        [   0,  300,  344,  ...,   14, 5077, 2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3505e-04,  3.1731e-04,  1.7886e-04,  ..., -1.6163e-04,
          1.0359e-05,  1.2280e-04],
        [-7.8604e-06, -4.9695e-06,  5.6177e-06,  ..., -6.7055e-06,
         -5.1893e-06, -5.5842e-06],
        [-2.4557e-05, -1.6421e-05,  1.8835e-05,  ..., -2.0832e-05,
         -1.6376e-05, -1.7151e-05],
        [-2.2933e-05, -1.4365e-05,  1.7151e-05,  ..., -1.9565e-05,
         -1.5289e-05, -1.6704e-05],
        [-1.8060e-05, -1.2398e-05,  1.3322e-05,  ..., -1.5661e-05,
         -1.2755e-05, -1.1854e-05]], device='cuda:0')
Loss: 0.9038987755775452


Running epoch 1, step 1878, batch 830
Sampled inputs[:2]: tensor([[    0,   266,  1634,  ...,   310,  1372,   287],
        [    0,   367,  3399,  ..., 13481,   408,  6944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3206e-04,  2.1281e-04,  1.0165e-05,  ..., -9.1529e-05,
          4.3951e-05,  7.6928e-05],
        [-9.0674e-06, -5.8599e-06,  6.5081e-06,  ..., -7.7859e-06,
         -6.0499e-06, -6.4112e-06],
        [-2.8521e-05, -1.9446e-05,  2.1979e-05,  ..., -2.4319e-05,
         -1.9193e-05, -1.9789e-05],
        [-2.6628e-05, -1.7047e-05,  2.0042e-05,  ..., -2.2843e-05,
         -1.7941e-05, -1.9327e-05],
        [-2.0772e-05, -1.4558e-05,  1.5408e-05,  ..., -1.8105e-05,
         -1.4812e-05, -1.3523e-05]], device='cuda:0')
Loss: 0.9496727585792542


Running epoch 1, step 1879, batch 831
Sampled inputs[:2]: tensor([[    0,   292,    46,  ...,  1217,    17,   292],
        [    0, 47684,   292,  ...,   287, 49958, 22022]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6897e-04,  3.7933e-04,  9.6633e-05,  ..., -1.2455e-04,
         -2.9456e-05,  1.4474e-04],
        [-1.0237e-05, -6.6683e-06,  7.3612e-06,  ..., -8.8736e-06,
         -6.9588e-06, -7.2271e-06],
        [-3.2037e-05, -2.2039e-05,  2.4781e-05,  ..., -2.7567e-05,
         -2.1964e-05, -2.2143e-05],
        [-3.0190e-05, -1.9491e-05,  2.2799e-05,  ..., -2.6181e-05,
         -2.0802e-05, -2.1905e-05],
        [-2.3425e-05, -1.6585e-05,  1.7464e-05,  ..., -2.0593e-05,
         -1.7002e-05, -1.5169e-05]], device='cuda:0')
Loss: 0.9338622689247131
Graident accumulation at epoch 1, step 1879, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0170]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0051e-05,  1.0743e-04, -2.4680e-04,  ..., -7.3064e-05,
         -4.7271e-05, -6.7851e-06],
        [-1.0073e-05, -6.7309e-06,  7.3275e-06,  ..., -8.8082e-06,
         -6.9501e-06, -7.0405e-06],
        [-1.5145e-05, -8.9673e-06,  1.2694e-05,  ..., -1.4002e-05,
         -4.5653e-06, -1.4975e-05],
        [-2.0374e-05, -5.0895e-06,  1.5472e-05,  ..., -1.5540e-05,
         -8.1355e-06, -1.4765e-05],
        [-2.3040e-05, -1.6854e-05,  1.7445e-05,  ..., -2.0432e-05,
         -1.7104e-05, -1.4781e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3859e-08, 6.5786e-08, 5.0919e-08,  ..., 4.2025e-08, 1.3196e-07,
         5.7420e-08],
        [8.0320e-11, 5.8449e-11, 2.0682e-11,  ..., 5.6865e-11, 3.5710e-11,
         3.0907e-11],
        [4.3143e-09, 3.0541e-09, 1.5481e-09,  ..., 3.2921e-09, 1.6045e-09,
         1.0577e-09],
        [1.1928e-09, 1.3287e-09, 3.9675e-10,  ..., 1.0591e-09, 7.5531e-10,
         4.7334e-10],
        [3.6834e-10, 2.0969e-10, 8.7125e-11,  ..., 2.7686e-10, 8.7348e-11,
         1.0893e-10]], device='cuda:0')
optimizer state dict: 235.0
lr: [5.518433506354004e-07, 5.518433506354004e-07]
scheduler_last_epoch: 235


Running epoch 1, step 1880, batch 832
Sampled inputs[:2]: tensor([[    0,   266, 15794,  ...,  3128,  6479,  2626],
        [    0,  1041,    14,  ...,   360,   266, 14966]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9304e-05,  1.6794e-05,  2.4372e-05,  ...,  2.9624e-05,
          1.1748e-05,  3.5784e-05],
        [-1.2070e-06, -8.0839e-07,  8.3447e-07,  ..., -1.0878e-06,
         -8.7917e-07, -8.7917e-07],
        [-3.8445e-06, -2.6822e-06,  2.8610e-06,  ..., -3.3975e-06,
         -2.7716e-06, -2.7418e-06],
        [-3.7104e-06, -2.4140e-06,  2.6673e-06,  ..., -3.3379e-06,
         -2.7120e-06, -2.7716e-06],
        [-2.7865e-06, -2.0117e-06,  1.9968e-06,  ..., -2.5183e-06,
         -2.1458e-06, -1.8328e-06]], device='cuda:0')
Loss: 0.9658393859863281


Running epoch 1, step 1881, batch 833
Sampled inputs[:2]: tensor([[   0, 2355, 2728,  ...,  554, 9025,  368],
        [   0,  944,  278,  ..., 5755,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8476e-05,  9.2644e-05,  1.6236e-04,  ..., -1.6800e-06,
          1.2316e-04,  3.9884e-05],
        [-2.4587e-06, -1.7211e-06,  1.7062e-06,  ..., -2.2277e-06,
         -1.8254e-06, -1.7025e-06],
        [-7.8380e-06, -5.7667e-06,  5.8264e-06,  ..., -7.0632e-06,
         -5.8562e-06, -5.3793e-06],
        [-7.4506e-06, -5.1111e-06,  5.3793e-06,  ..., -6.7800e-06,
         -5.5879e-06, -5.3644e-06],
        [-5.6773e-06, -4.3064e-06,  4.0978e-06,  ..., -5.2154e-06,
         -4.4703e-06, -3.6284e-06]], device='cuda:0')
Loss: 0.9744893312454224


Running epoch 1, step 1882, batch 834
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 3164,   12,  ...,  984,  344, 3993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7186e-05,  1.6136e-04,  5.9763e-05,  ..., -4.4422e-05,
          2.6046e-04, -2.6513e-05],
        [-3.8892e-06, -2.4997e-06,  2.6338e-06,  ..., -3.4422e-06,
         -2.6934e-06, -2.7530e-06],
        [-1.2249e-05, -8.3148e-06,  8.8960e-06,  ..., -1.0774e-05,
         -8.5533e-06, -8.5235e-06],
        [-1.1444e-05, -7.2867e-06,  8.0913e-06,  ..., -1.0177e-05,
         -8.0466e-06, -8.3297e-06],
        [-9.1344e-06, -6.3181e-06,  6.4075e-06,  ..., -8.1509e-06,
         -6.6608e-06, -5.9530e-06]], device='cuda:0')
Loss: 0.965934157371521


Running epoch 1, step 1883, batch 835
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0,  1235,   368,  ..., 12152,  8498,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1285e-05,  7.2376e-05, -4.6777e-06,  ..., -1.3753e-05,
          2.8361e-04, -9.8573e-06],
        [ 5.2516e-05,  7.6655e-05, -4.7268e-05,  ...,  5.9585e-05,
          8.9962e-05,  4.4835e-05],
        [-1.6302e-05, -1.1235e-05,  1.2189e-05,  ..., -1.4216e-05,
         -1.1295e-05, -1.1012e-05],
        [-1.5348e-05, -9.9689e-06,  1.1221e-05,  ..., -1.3500e-05,
         -1.0699e-05, -1.0848e-05],
        [-1.2010e-05, -8.4788e-06,  8.6874e-06,  ..., -1.0639e-05,
         -8.7321e-06, -7.5921e-06]], device='cuda:0')
Loss: 1.0155378580093384


Running epoch 1, step 1884, batch 836
Sampled inputs[:2]: tensor([[   0, 1477, 5648,  ..., 4391, 1722,  369],
        [   0,  694,  266,  ..., 3007,  300, 5726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4200e-05,  1.2780e-04, -2.0387e-04,  ..., -3.4742e-05,
          1.7889e-04, -1.7696e-04],
        [ 5.1212e-05,  7.5758e-05, -4.6217e-05,  ...,  5.8497e-05,
          8.9127e-05,  4.4064e-05],
        [-2.0355e-05, -1.4171e-05,  1.5661e-05,  ..., -1.7628e-05,
         -1.3977e-05, -1.3441e-05],
        [-1.9282e-05, -1.2681e-05,  1.4558e-05,  ..., -1.6779e-05,
         -1.3247e-05, -1.3262e-05],
        [-1.4752e-05, -1.0565e-05,  1.0982e-05,  ..., -1.2994e-05,
         -1.0669e-05, -9.0972e-06]], device='cuda:0')
Loss: 0.9793562889099121


Running epoch 1, step 1885, batch 837
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,  6762,   689,  ...,  7061,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2729e-04,  1.2580e-04, -2.4437e-04,  ...,  3.8336e-05,
          1.3714e-04, -1.7524e-04],
        [ 4.9886e-05,  7.5020e-05, -4.5249e-05,  ...,  5.7431e-05,
          8.8304e-05,  4.3163e-05],
        [-2.4408e-05, -1.6600e-05,  1.8850e-05,  ..., -2.0862e-05,
         -1.6570e-05, -1.6138e-05],
        [-2.3216e-05, -1.4856e-05,  1.7554e-05,  ..., -1.9923e-05,
         -1.5736e-05, -1.5974e-05],
        [-1.7643e-05, -1.2368e-05,  1.3173e-05,  ..., -1.5363e-05,
         -1.2636e-05, -1.0885e-05]], device='cuda:0')
Loss: 0.949843168258667


Running epoch 1, step 1886, batch 838
Sampled inputs[:2]: tensor([[   0, 1184, 1451,  ...,  934,  352,  266],
        [   0, 7303,   12,  ..., 1085,  413,  711]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8525e-05,  8.2532e-05, -2.3344e-04,  ..., -3.2323e-05,
          3.0930e-05, -1.5284e-04],
        [ 4.8619e-05,  7.4156e-05, -4.4306e-05,  ...,  5.6321e-05,
          8.7399e-05,  4.2306e-05],
        [-2.8342e-05, -1.9446e-05,  2.2009e-05,  ..., -2.4289e-05,
         -1.9401e-05, -1.8790e-05],
        [-2.6911e-05, -1.7375e-05,  2.0444e-05,  ..., -2.3186e-05,
         -1.8403e-05, -1.8597e-05],
        [-2.0474e-05, -1.4499e-05,  1.5378e-05,  ..., -1.7881e-05,
         -1.4797e-05, -1.2636e-05]], device='cuda:0')
Loss: 0.9708988666534424


Running epoch 1, step 1887, batch 839
Sampled inputs[:2]: tensor([[    0,  3773, 23452,  ..., 14393,  1121,   304],
        [    0,  4113,   709,  ..., 22407,  3231,  1130]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3448e-05, -4.8139e-05, -2.0500e-04,  ..., -8.5395e-06,
         -6.8401e-06, -2.6958e-04],
        [ 4.7397e-05,  7.3262e-05, -4.3278e-05,  ...,  5.5256e-05,
          8.6538e-05,  4.1539e-05],
        [-3.2306e-05, -2.2441e-05,  2.5496e-05,  ..., -2.7731e-05,
         -2.2188e-05, -2.1309e-05],
        [-3.0652e-05, -2.0072e-05,  2.3708e-05,  ..., -2.6435e-05,
         -2.1026e-05, -2.1055e-05],
        [-2.3261e-05, -1.6689e-05,  1.7747e-05,  ..., -2.0340e-05,
         -1.6883e-05, -1.4268e-05]], device='cuda:0')
Loss: 0.9975924491882324
Graident accumulation at epoch 1, step 1887, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0391e-05,  9.1872e-05, -2.4262e-04,  ..., -6.6612e-05,
         -4.3228e-05, -3.3065e-05],
        [-4.3259e-06,  1.2683e-06,  2.2670e-06,  ..., -2.4018e-06,
          2.3987e-06, -2.1826e-06],
        [-1.6861e-05, -1.0315e-05,  1.3975e-05,  ..., -1.5375e-05,
         -6.3275e-06, -1.5608e-05],
        [-2.1402e-05, -6.5877e-06,  1.6296e-05,  ..., -1.6630e-05,
         -9.4245e-06, -1.5394e-05],
        [-2.3062e-05, -1.6837e-05,  1.7475e-05,  ..., -2.0423e-05,
         -1.7082e-05, -1.4729e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3785e-08, 6.5722e-08, 5.0911e-08,  ..., 4.1983e-08, 1.3183e-07,
         5.7436e-08],
        [8.2486e-11, 6.3758e-11, 2.2534e-11,  ..., 5.9861e-11, 4.3163e-11,
         3.2601e-11],
        [4.3110e-09, 3.0516e-09, 1.5472e-09,  ..., 3.2896e-09, 1.6034e-09,
         1.0571e-09],
        [1.1925e-09, 1.3278e-09, 3.9692e-10,  ..., 1.0588e-09, 7.5500e-10,
         4.7331e-10],
        [3.6851e-10, 2.0976e-10, 8.7353e-11,  ..., 2.7700e-10, 8.7545e-11,
         1.0902e-10]], device='cuda:0')
optimizer state dict: 236.0
lr: [5.120691576200498e-07, 5.120691576200498e-07]
scheduler_last_epoch: 236


Running epoch 1, step 1888, batch 840
Sampled inputs[:2]: tensor([[   0,  266, 4505,  ...,   12,  461,  806],
        [   0, 2973,   30,  ...,  408,  259, 1914]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2113e-05, -1.4186e-04, -1.0103e-04,  ..., -2.0853e-04,
         -8.6621e-05, -2.1960e-05],
        [-1.2890e-06, -9.0525e-07,  1.0058e-06,  ..., -1.1101e-06,
         -8.9779e-07, -8.4564e-07],
        [-4.0531e-06, -3.0249e-06,  3.3677e-06,  ..., -3.5316e-06,
         -2.8759e-06, -2.6971e-06],
        [-3.9339e-06, -2.7418e-06,  3.1888e-06,  ..., -3.3975e-06,
         -2.7418e-06, -2.6971e-06],
        [-2.9057e-06, -2.2352e-06,  2.3395e-06,  ..., -2.5928e-06,
         -2.1905e-06, -1.7956e-06]], device='cuda:0')
Loss: 0.9900985956192017


Running epoch 1, step 1889, batch 841
Sampled inputs[:2]: tensor([[    0,   266,  2623,  ...,     5, 10781,   287],
        [    0,   328,  6875,  ...,   369,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7455e-04, -1.8949e-04, -2.2568e-04,  ..., -1.9470e-04,
         -4.2706e-05, -2.8152e-04],
        [-2.5854e-06, -1.7360e-06,  1.9558e-06,  ..., -2.2426e-06,
         -1.7881e-06, -1.6764e-06],
        [-7.8976e-06, -5.6326e-06,  6.4373e-06,  ..., -6.8694e-06,
         -5.6028e-06, -5.0813e-06],
        [-7.7039e-06, -5.1558e-06,  6.1244e-06,  ..., -6.6906e-06,
         -5.4091e-06, -5.1856e-06],
        [-5.7369e-06, -4.2319e-06,  4.5449e-06,  ..., -5.0813e-06,
         -4.2915e-06, -3.3975e-06]], device='cuda:0')
Loss: 0.9505912065505981


Running epoch 1, step 1890, batch 842
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,  396,  221,  ..., 1279,  720,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2550e-04,  8.5919e-05, -1.1293e-04,  ..., -2.3385e-04,
         -2.0699e-04, -4.0628e-04],
        [-3.9190e-06, -2.5332e-06,  2.8498e-06,  ..., -3.4049e-06,
         -2.6748e-06, -2.6450e-06],
        [-1.1921e-05, -8.1956e-06,  9.3728e-06,  ..., -1.0327e-05,
         -8.2999e-06, -7.8976e-06],
        [-1.1519e-05, -7.4059e-06,  8.8215e-06,  ..., -1.0028e-05,
         -8.0019e-06, -8.0168e-06],
        [-8.8364e-06, -6.2287e-06,  6.7204e-06,  ..., -7.7784e-06,
         -6.4522e-06, -5.4240e-06]], device='cuda:0')
Loss: 0.8727386593818665


Running epoch 1, step 1891, batch 843
Sampled inputs[:2]: tensor([[   0,  271, 3403,  ..., 6168,  300, 2257],
        [   0, 2652,  271,  ...,  634, 1921,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5083e-04,  1.1621e-04, -1.2823e-04,  ..., -2.9003e-04,
         -1.6376e-04, -3.4276e-04],
        [-5.1931e-06, -3.4347e-06,  3.8855e-06,  ..., -4.4703e-06,
         -3.5986e-06, -3.4533e-06],
        [-1.5944e-05, -1.1250e-05,  1.2860e-05,  ..., -1.3754e-05,
         -1.1310e-05, -1.0461e-05],
        [-1.5289e-05, -1.0118e-05,  1.2055e-05,  ..., -1.3232e-05,
         -1.0818e-05, -1.0505e-05],
        [-1.1623e-05, -8.4341e-06,  9.0897e-06,  ..., -1.0207e-05,
         -8.6725e-06, -7.0855e-06]], device='cuda:0')
Loss: 0.9704321622848511


Running epoch 1, step 1892, batch 844
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,   341, 22766,  ...,   271,   266,  1176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0286e-04,  4.2380e-05, -1.3055e-04,  ..., -2.6173e-04,
         -1.0627e-04, -3.0962e-04],
        [-6.5342e-06, -4.3474e-06,  4.7609e-06,  ..., -5.6624e-06,
         -4.5896e-06, -4.4219e-06],
        [-2.0117e-05, -1.4260e-05,  1.5780e-05,  ..., -1.7479e-05,
         -1.4469e-05, -1.3471e-05],
        [-1.9088e-05, -1.2726e-05,  1.4648e-05,  ..., -1.6659e-05,
         -1.3724e-05, -1.3351e-05],
        [-1.4827e-05, -1.0803e-05,  1.1295e-05,  ..., -1.3128e-05,
         -1.1221e-05, -9.2760e-06]], device='cuda:0')
Loss: 0.9474283456802368


Running epoch 1, step 1893, batch 845
Sampled inputs[:2]: tensor([[    0,    41,     7,  ...,   496,    14,  4075],
        [    0,  4566,   300,  ...,   271,  1644, 16473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2126e-04,  2.0241e-04, -1.3448e-04,  ..., -3.3106e-04,
         -3.7369e-04, -5.2151e-04],
        [-7.8231e-06, -5.1409e-06,  5.8040e-06,  ..., -6.7502e-06,
         -5.3942e-06, -5.2154e-06],
        [-2.3961e-05, -1.6779e-05,  1.9103e-05,  ..., -2.0713e-05,
         -1.6913e-05, -1.5795e-05],
        [-2.2858e-05, -1.5050e-05,  1.7866e-05,  ..., -1.9833e-05,
         -1.6093e-05, -1.5736e-05],
        [-1.7539e-05, -1.2673e-05,  1.3575e-05,  ..., -1.5453e-05,
         -1.3068e-05, -1.0788e-05]], device='cuda:0')
Loss: 0.9694485664367676


Running epoch 1, step 1894, batch 846
Sampled inputs[:2]: tensor([[    0,  2386,  4012,  ...,   300, 15480,  1036],
        [    0, 16765,   367,  ..., 30192,  7038,  8135]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3238e-04,  3.8925e-04, -1.5257e-04,  ..., -2.8994e-04,
         -4.7073e-04, -4.1267e-04],
        [-9.0152e-06, -5.9828e-06,  6.8396e-06,  ..., -7.7635e-06,
         -6.1840e-06, -5.9940e-06],
        [-2.7731e-05, -1.9580e-05,  2.2545e-05,  ..., -2.3916e-05,
         -1.9446e-05, -1.8239e-05],
        [-2.6435e-05, -1.7568e-05,  2.1130e-05,  ..., -2.2873e-05,
         -1.8477e-05, -1.8165e-05],
        [-2.0087e-05, -1.4655e-05,  1.5855e-05,  ..., -1.7658e-05,
         -1.4901e-05, -1.2308e-05]], device='cuda:0')
Loss: 0.9334121346473694


Running epoch 1, step 1895, batch 847
Sampled inputs[:2]: tensor([[    0,    12,   496,  ..., 11354,  4856,  1109],
        [    0,    40,   568,  ...,  3750,   300,  3421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2344e-04,  5.1682e-04, -1.0005e-04,  ..., -3.1267e-04,
         -7.4594e-04, -6.1594e-04],
        [-1.0252e-05, -6.9067e-06,  7.8529e-06,  ..., -8.8289e-06,
         -7.0743e-06, -6.7689e-06],
        [ 1.4001e-04,  2.2059e-04, -8.1393e-05,  ...,  2.2779e-04,
          1.1965e-04,  1.5067e-04],
        [-3.0234e-05, -2.0400e-05,  2.4393e-05,  ..., -2.6152e-05,
         -2.1234e-05, -2.0668e-05],
        [-2.2888e-05, -1.6950e-05,  1.8224e-05,  ..., -2.0146e-05,
         -1.7062e-05, -1.3947e-05]], device='cuda:0')
Loss: 0.9738333225250244
Graident accumulation at epoch 1, step 1895, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.2993e-05,  1.3437e-04, -2.2836e-04,  ..., -9.1217e-05,
         -1.1350e-04, -9.1352e-05],
        [-4.9185e-06,  4.5083e-07,  2.8256e-06,  ..., -3.0445e-06,
          1.4514e-06, -2.6412e-06],
        [-1.1743e-06,  1.2775e-05,  4.4378e-06,  ...,  8.9419e-06,
          6.2698e-06,  1.0190e-06],
        [-2.2285e-05, -7.9689e-06,  1.7106e-05,  ..., -1.7582e-05,
         -1.0606e-05, -1.5921e-05],
        [-2.3045e-05, -1.6849e-05,  1.7550e-05,  ..., -2.0395e-05,
         -1.7080e-05, -1.4651e-05]], device='cuda:0')
optimizer state dict: tensor([[7.3816e-08, 6.5924e-08, 5.0870e-08,  ..., 4.2039e-08, 1.3226e-07,
         5.7758e-08],
        [8.2509e-11, 6.3742e-11, 2.2573e-11,  ..., 5.9879e-11, 4.3170e-11,
         3.2614e-11],
        [4.3263e-09, 3.0972e-09, 1.5522e-09,  ..., 3.3382e-09, 1.6161e-09,
         1.0787e-09],
        [1.1922e-09, 1.3269e-09, 3.9712e-10,  ..., 1.0584e-09, 7.5469e-10,
         4.7326e-10],
        [3.6867e-10, 2.0984e-10, 8.7598e-11,  ..., 2.7713e-10, 8.7749e-11,
         1.0911e-10]], device='cuda:0')
optimizer state dict: 237.0
lr: [4.737448032587366e-07, 4.737448032587366e-07]
scheduler_last_epoch: 237


Running epoch 1, step 1896, batch 848
Sampled inputs[:2]: tensor([[    0,   292,   221,  ...,   796, 12886,   694],
        [    0,   266,  1034,  ...,  6153,   263,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9416e-05,  8.7894e-05,  2.4357e-05,  ...,  2.5584e-05,
         -6.2665e-05,  5.3049e-06],
        [-1.3784e-06, -7.7114e-07,  9.5367e-07,  ..., -1.1474e-06,
         -8.3447e-07, -9.9093e-07],
        [-4.2319e-06, -2.5183e-06,  3.1441e-06,  ..., -3.5167e-06,
         -2.5779e-06, -3.0100e-06],
        [-3.9935e-06, -2.2203e-06,  2.8908e-06,  ..., -3.3379e-06,
         -2.4438e-06, -2.9653e-06],
        [-3.1590e-06, -1.9222e-06,  2.2352e-06,  ..., -2.6822e-06,
         -2.0564e-06, -2.1011e-06]], device='cuda:0')
Loss: 0.9203608632087708


Running epoch 1, step 1897, batch 849
Sampled inputs[:2]: tensor([[    0,    14,  8047,  ...,  3813,     9,  8237],
        [    0,   471,    14,  ..., 27104,     9,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0036e-06,  7.3278e-05, -9.6018e-05,  ...,  1.5089e-04,
         -1.4965e-04,  2.4272e-05],
        [-2.6077e-06, -1.6801e-06,  1.9372e-06,  ..., -2.1905e-06,
         -1.6615e-06, -1.7695e-06],
        [-8.1956e-06, -5.5879e-06,  6.5118e-06,  ..., -6.8992e-06,
         -5.2899e-06, -5.5283e-06],
        [-7.7337e-06, -4.9621e-06,  6.0350e-06,  ..., -6.5118e-06,
         -4.9770e-06, -5.4389e-06],
        [ 1.1421e-04,  1.4734e-04, -1.1576e-04,  ...,  1.2674e-04,
          5.0721e-05,  7.9535e-05]], device='cuda:0')
Loss: 0.9763647317886353


Running epoch 1, step 1898, batch 850
Sampled inputs[:2]: tensor([[   0,  221,  334,  ...,  271,  266, 7246],
        [   0, 6508, 4305,  ...,  806, 3888, 4431]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5166e-04,  1.4442e-04,  2.8722e-04,  ...,  2.1202e-04,
         -4.5553e-05, -1.4300e-05],
        [-4.1500e-06, -2.3916e-06,  2.6040e-06,  ..., -3.5092e-06,
         -2.6897e-06, -2.9765e-06],
        [-1.2845e-05, -7.9125e-06,  8.7172e-06,  ..., -1.0863e-05,
         -8.4639e-06, -9.1493e-06],
        [-1.1936e-05, -6.8843e-06,  7.9051e-06,  ..., -1.0133e-05,
         -7.8380e-06, -8.7619e-06],
        [ 1.1010e-04,  1.4531e-04, -1.1394e-04,  ...,  1.2321e-04,
          4.7860e-05,  7.6480e-05]], device='cuda:0')
Loss: 0.9410618543624878


Running epoch 1, step 1899, batch 851
Sampled inputs[:2]: tensor([[    0,    13, 26011,  ...,   342,  3873,   720],
        [    0,   328,   266,  ...,    14,  3352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2898e-04,  1.0506e-04,  2.8722e-04,  ...,  1.5341e-04,
         -4.2419e-06,  4.0241e-06],
        [-5.4836e-06, -3.2783e-06,  3.6098e-06,  ..., -4.5821e-06,
         -3.5129e-06, -3.7998e-06],
        [-1.6928e-05, -1.0774e-05,  1.1995e-05,  ..., -1.4156e-05,
         -1.1042e-05, -1.1683e-05],
        [-1.5810e-05, -9.4473e-06,  1.0990e-05,  ..., -1.3262e-05,
         -1.0267e-05, -1.1265e-05],
        [ 1.0723e-04,  1.4321e-04, -1.1170e-04,  ...,  1.2084e-04,
          4.5908e-05,  7.4841e-05]], device='cuda:0')
Loss: 0.9497022032737732


Running epoch 1, step 1900, batch 852
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5499e-04,  1.0411e-04,  3.2011e-04,  ...,  1.7395e-04,
          1.3139e-05,  4.1360e-05],
        [-6.6906e-06, -4.1761e-06,  4.6082e-06,  ..., -5.6401e-06,
         -4.3437e-06, -4.6082e-06],
        [-2.0802e-05, -1.3858e-05,  1.5423e-05,  ..., -1.7628e-05,
         -1.3813e-05, -1.4290e-05],
        [-1.9431e-05, -1.2174e-05,  1.4164e-05,  ..., -1.6481e-05,
         -1.2830e-05, -1.3828e-05],
        [ 1.0455e-04,  1.4100e-04, -1.0939e-04,  ...,  1.1841e-04,
          4.3882e-05,  7.3172e-05]], device='cuda:0')
Loss: 0.9645575284957886


Running epoch 1, step 1901, batch 853
Sampled inputs[:2]: tensor([[    0,   365,  1941,  ..., 38029,  1790, 44066],
        [    0,  4120,   278,  ...,   298,   273,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7883e-04,  1.0561e-04,  3.4694e-04,  ...,  7.5802e-05,
          2.9889e-04,  3.9723e-05],
        [-7.8529e-06, -5.0291e-06,  5.4501e-06,  ..., -6.7279e-06,
         -5.2154e-06, -5.4389e-06],
        [-2.4408e-05, -1.6585e-05,  1.8269e-05,  ..., -2.0951e-05,
         -1.6466e-05, -1.6794e-05],
        [-2.2858e-05, -1.4663e-05,  1.6786e-05,  ..., -1.9699e-05,
         -1.5423e-05, -1.6391e-05],
        [ 1.0190e-04,  1.3893e-04, -1.0737e-04,  ...,  1.1595e-04,
          4.1840e-05,  7.1488e-05]], device='cuda:0')
Loss: 0.9717774391174316


Running epoch 1, step 1902, batch 854
Sampled inputs[:2]: tensor([[    0,  1575,  4384,  ...,   328,   722,  6124],
        [    0,  5143,  3877,  ...,   292, 44003,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0662e-05, -5.5263e-05,  3.4689e-04,  ...,  6.3804e-05,
          6.2008e-04,  1.4167e-04],
        [-9.1717e-06, -5.8897e-06,  6.3106e-06,  ..., -7.8827e-06,
         -6.1207e-06, -6.4075e-06],
        [-2.8521e-05, -1.9431e-05,  2.1219e-05,  ..., -2.4527e-05,
         -1.9312e-05, -1.9774e-05],
        [-2.6599e-05, -1.7077e-05,  1.9364e-05,  ..., -2.2978e-05,
         -1.8016e-05, -1.9222e-05],
        [ 9.8785e-05,  1.3674e-04, -1.0522e-04,  ...,  1.1320e-04,
          3.9575e-05,  6.9387e-05]], device='cuda:0')
Loss: 0.9574470520019531


Running epoch 1, step 1903, batch 855
Sampled inputs[:2]: tensor([[    0,  1890,   278,  ...,  1400,   367,  1874],
        [    0,   456,    17,  ...,  1553, 29477,  2713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4759e-06,  4.2881e-05,  4.0962e-04,  ...,  1.0734e-04,
          6.3215e-04, -1.3815e-04],
        [-1.0461e-05, -6.6087e-06,  7.1190e-06,  ..., -9.0152e-06,
         -6.9886e-06, -7.3761e-06],
        [-3.2604e-05, -2.1800e-05,  2.4006e-05,  ..., -2.8059e-05,
         -2.2039e-05, -2.2784e-05],
        [-3.0473e-05, -1.9148e-05,  2.1882e-05,  ..., -2.6375e-05,
         -2.0638e-05, -2.2203e-05],
        [ 9.5701e-05,  1.3489e-04, -1.0323e-04,  ...,  1.1048e-04,
          3.7385e-05,  6.7286e-05]], device='cuda:0')
Loss: 0.9329625964164734
Graident accumulation at epoch 1, step 1903, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0346e-05,  1.2522e-04, -1.6456e-04,  ..., -7.1361e-05,
         -3.8934e-05, -9.6033e-05],
        [-5.4727e-06, -2.5512e-07,  3.2549e-06,  ..., -3.6416e-06,
          6.0740e-07, -3.1147e-06],
        [-4.3172e-06,  9.3179e-06,  6.3946e-06,  ...,  5.2418e-06,
          3.4389e-06, -1.3613e-06],
        [-2.3104e-05, -9.0868e-06,  1.7583e-05,  ..., -1.8461e-05,
         -1.1609e-05, -1.6549e-05],
        [-1.1170e-05, -1.6751e-06,  5.4725e-06,  ..., -7.3072e-06,
         -1.1633e-05, -6.4574e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3742e-08, 6.5859e-08, 5.0987e-08,  ..., 4.2009e-08, 1.3252e-07,
         5.7719e-08],
        [8.2536e-11, 6.3722e-11, 2.2601e-11,  ..., 5.9901e-11, 4.3176e-11,
         3.2636e-11],
        [4.3230e-09, 3.0945e-09, 1.5513e-09,  ..., 3.3356e-09, 1.6150e-09,
         1.0782e-09],
        [1.1920e-09, 1.3259e-09, 3.9720e-10,  ..., 1.0580e-09, 7.5437e-10,
         4.7328e-10],
        [3.7746e-10, 2.2782e-10, 9.8166e-11,  ..., 2.8906e-10, 8.9059e-11,
         1.1353e-10]], device='cuda:0')
optimizer state dict: 238.0
lr: [4.36876143847339e-07, 4.36876143847339e-07]
scheduler_last_epoch: 238


Running epoch 1, step 1904, batch 856
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0,  266, 1553,  ..., 8954,   21,  409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7897e-05, -8.0994e-05, -9.7366e-05,  ..., -7.5030e-06,
          6.5974e-05,  1.8834e-05],
        [-1.2070e-06, -7.8976e-07,  9.3132e-07,  ..., -1.0878e-06,
         -7.9721e-07, -8.6799e-07],
        [-3.8743e-06, -2.6822e-06,  3.1888e-06,  ..., -3.4720e-06,
         -2.5779e-06, -2.7418e-06],
        [-3.5614e-06, -2.3097e-06,  2.8908e-06,  ..., -3.2037e-06,
         -2.3693e-06, -2.6524e-06],
        [-2.6375e-06, -1.8924e-06,  2.1160e-06,  ..., -2.3991e-06,
         -1.8552e-06, -1.7285e-06]], device='cuda:0')
Loss: 0.9371028542518616


Running epoch 1, step 1905, batch 857
Sampled inputs[:2]: tensor([[   0, 1712,   12,  ..., 1255, 1688,  266],
        [   0,  259, 1329,  ...,  266,  706, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0113e-04, -9.1129e-05, -5.3188e-05,  ..., -8.7432e-05,
          5.0598e-07,  3.0083e-05],
        [-2.4587e-06, -1.6838e-06,  1.9222e-06,  ..., -2.1830e-06,
         -1.6876e-06, -1.7099e-06],
        [-7.8380e-06, -5.6326e-06,  6.5267e-06,  ..., -6.9588e-06,
         -5.4389e-06, -5.4091e-06],
        [-7.3016e-06, -4.9621e-06,  6.0201e-06,  ..., -6.4969e-06,
         -5.0664e-06, -5.2899e-06],
        [-5.4091e-06, -4.0680e-06,  4.3958e-06,  ..., -4.8876e-06,
         -4.0010e-06, -3.4645e-06]], device='cuda:0')
Loss: 0.9616787433624268


Running epoch 1, step 1906, batch 858
Sampled inputs[:2]: tensor([[   0,   13, 1320,  ..., 8686, 6851,   13],
        [   0,  266, 2086,  ..., 4283,  720,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3114e-04, -2.0147e-04,  8.3610e-05,  ..., -7.3977e-05,
          1.3122e-04,  5.1332e-06],
        [-3.6806e-06, -2.6114e-06,  2.9057e-06,  ..., -3.2559e-06,
         -2.5555e-06, -2.5332e-06],
        [-1.1832e-05, -8.7619e-06,  9.8944e-06,  ..., -1.0431e-05,
         -8.2701e-06, -8.0615e-06],
        [-1.1012e-05, -7.7337e-06,  9.1344e-06,  ..., -9.7305e-06,
         -7.7039e-06, -7.8976e-06],
        [-8.1658e-06, -6.3181e-06,  6.6757e-06,  ..., -7.3463e-06,
         -6.0871e-06, -5.1633e-06]], device='cuda:0')
Loss: 0.9713402986526489


Running epoch 1, step 1907, batch 859
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0, 11030,    72,  ...,   259, 16979,  9415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4569e-05, -1.3693e-04, -3.9435e-06,  ..., -1.5893e-04,
          2.5179e-04, -4.6616e-05],
        [-4.9397e-06, -3.4496e-06,  3.8445e-06,  ..., -4.3735e-06,
         -3.4533e-06, -3.4273e-06],
        [-1.5736e-05, -1.1504e-05,  1.3039e-05,  ..., -1.3888e-05,
         -1.1086e-05, -1.0788e-05],
        [-1.4663e-05, -1.0118e-05,  1.2010e-05,  ..., -1.2979e-05,
         -1.0341e-05, -1.0595e-05],
        [-1.1057e-05, -8.4192e-06,  8.9258e-06,  ..., -9.9391e-06,
         -8.2627e-06, -7.0408e-06]], device='cuda:0')
Loss: 0.9376741647720337


Running epoch 1, step 1908, batch 860
Sampled inputs[:2]: tensor([[    0,  2805,   391,  ...,    12,   259,  1420],
        [    0, 21801, 13084,  ...,  1738,  2946,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8889e-04,  2.3017e-04,  2.1700e-04,  ..., -1.3830e-04,
          4.4773e-05, -1.5879e-04],
        [-6.3628e-06, -4.2804e-06,  4.5821e-06,  ..., -5.5879e-06,
         -4.3772e-06, -4.4331e-06],
        [-2.0206e-05, -1.4350e-05,  1.5587e-05,  ..., -1.7732e-05,
         -1.4126e-05, -1.3933e-05],
        [-1.8746e-05, -1.2532e-05,  1.4201e-05,  ..., -1.6525e-05,
         -1.3128e-05, -1.3545e-05],
        [-1.4424e-05, -1.0595e-05,  1.0706e-05,  ..., -1.2904e-05,
         -1.0692e-05, -9.3058e-06]], device='cuda:0')
Loss: 0.9400709867477417


Running epoch 1, step 1909, batch 861
Sampled inputs[:2]: tensor([[    0,   259,  5112,  ...,  3520,   278,   298],
        [    0,   475,   668,  ..., 17680,   368,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6867e-04,  1.8053e-04,  2.6588e-04,  ..., -2.1698e-04,
          4.9406e-05, -1.7105e-04],
        [-7.7039e-06, -5.1707e-06,  5.4501e-06,  ..., -6.7577e-06,
         -5.3085e-06, -5.3942e-06],
        [-2.4498e-05, -1.7345e-05,  1.8612e-05,  ..., -2.1458e-05,
         -1.7136e-05, -1.6958e-05],
        [-2.2620e-05, -1.5080e-05,  1.6809e-05,  ..., -1.9938e-05,
         -1.5870e-05, -1.6391e-05],
        [-1.7568e-05, -1.2845e-05,  1.2867e-05,  ..., -1.5676e-05,
         -1.3016e-05, -1.1347e-05]], device='cuda:0')
Loss: 0.9516850709915161


Running epoch 1, step 1910, batch 862
Sampled inputs[:2]: tensor([[    0,   271, 16217,  ...,  6352,  4546,  2558],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5393e-04,  2.9552e-04,  3.9075e-04,  ..., -2.4819e-04,
          1.5573e-04, -2.6451e-04],
        [-8.9481e-06, -6.0573e-06,  6.3628e-06,  ..., -7.8604e-06,
         -6.2063e-06, -6.2287e-06],
        [-2.8461e-05, -2.0340e-05,  2.1711e-05,  ..., -2.4959e-05,
         -1.9982e-05, -1.9610e-05],
        [-2.6315e-05, -1.7703e-05,  1.9640e-05,  ..., -2.3216e-05,
         -1.8567e-05, -1.9014e-05],
        [-2.0429e-05, -1.5080e-05,  1.5028e-05,  ..., -1.8254e-05,
         -1.5207e-05, -1.3128e-05]], device='cuda:0')
Loss: 0.9925556182861328


Running epoch 1, step 1911, batch 863
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 5150, 1030,  ...,   14,  475, 1763]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3334e-04,  1.2504e-04,  3.6839e-04,  ..., -2.3661e-04,
          5.8637e-04, -2.7524e-04],
        [-1.0155e-05, -6.8024e-06,  7.1861e-06,  ..., -8.9705e-06,
         -7.0743e-06, -7.1302e-06],
        [-3.2231e-05, -2.2754e-05,  2.4483e-05,  ..., -2.8357e-05,
         -2.2665e-05, -2.2307e-05],
        [-3.0011e-05, -1.9923e-05,  2.2262e-05,  ..., -2.6628e-05,
         -2.1264e-05, -2.1860e-05],
        [-2.3231e-05, -1.6935e-05,  1.7054e-05,  ..., -2.0802e-05,
         -1.7293e-05, -1.5013e-05]], device='cuda:0')
Loss: 0.956415057182312
Graident accumulation at epoch 1, step 1911, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.9775e-06,  1.2520e-04, -1.1127e-04,  ..., -8.7886e-05,
          2.3596e-05, -1.1395e-04],
        [-5.9410e-06, -9.0985e-07,  3.6480e-06,  ..., -4.1745e-06,
         -1.6077e-07, -3.5163e-06],
        [-7.1086e-06,  6.1107e-06,  8.2034e-06,  ...,  1.8819e-06,
          8.2857e-07, -3.4559e-06],
        [-2.3795e-05, -1.0170e-05,  1.8051e-05,  ..., -1.9278e-05,
         -1.2574e-05, -1.7080e-05],
        [-1.2376e-05, -3.2011e-06,  6.6307e-06,  ..., -8.6567e-06,
         -1.2199e-05, -7.3129e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3686e-08, 6.5809e-08, 5.1071e-08,  ..., 4.2023e-08, 1.3274e-07,
         5.7737e-08],
        [8.2556e-11, 6.3704e-11, 2.2630e-11,  ..., 5.9921e-11, 4.3183e-11,
         3.2654e-11],
        [4.3197e-09, 3.0920e-09, 1.5503e-09,  ..., 3.3331e-09, 1.6139e-09,
         1.0776e-09],
        [1.1917e-09, 1.3250e-09, 3.9730e-10,  ..., 1.0577e-09, 7.5406e-10,
         4.7328e-10],
        [3.7762e-10, 2.2788e-10, 9.8359e-11,  ..., 2.8920e-10, 8.9269e-11,
         1.1364e-10]], device='cuda:0')
optimizer state dict: 239.0
lr: [4.014688132388511e-07, 4.014688132388511e-07]
scheduler_last_epoch: 239


Running epoch 1, step 1912, batch 864
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8930e-04,  3.3360e-04,  9.8948e-05,  ..., -8.7737e-05,
         -1.3138e-04, -2.5433e-05],
        [-1.4156e-06, -8.4564e-07,  8.9779e-07,  ..., -1.1846e-06,
         -8.9034e-07, -1.0356e-06],
        [-4.3213e-06, -2.8163e-06,  2.9802e-06,  ..., -3.6657e-06,
         -2.8163e-06, -3.1590e-06],
        [-3.9637e-06, -2.4289e-06,  2.6375e-06,  ..., -3.3975e-06,
         -2.6077e-06, -2.9802e-06],
        [-3.3379e-06, -2.2054e-06,  2.2203e-06,  ..., -2.8759e-06,
         -2.2799e-06, -2.2799e-06]], device='cuda:0')
Loss: 0.9237080216407776


Running epoch 1, step 1913, batch 865
Sampled inputs[:2]: tensor([[   0, 1824,   13,  ...,  266, 5940,   19],
        [   0, 1360,   14,  ...,  287, 2429, 2498]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1154e-04,  4.2594e-04,  2.5739e-04,  ..., -9.1055e-05,
         -2.0989e-04,  1.0317e-05],
        [-2.6748e-06, -1.7136e-06,  1.9334e-06,  ..., -2.2724e-06,
         -1.7583e-06, -1.8403e-06],
        [-8.2254e-06, -5.6773e-06,  6.3628e-06,  ..., -7.0930e-06,
         -5.6177e-06, -5.6773e-06],
        [-7.6741e-06, -4.9919e-06,  5.8413e-06,  ..., -6.6161e-06,
         -5.2154e-06, -5.4687e-06],
        [-6.0797e-06, -4.3064e-06,  4.5449e-06,  ..., -5.3197e-06,
         -4.3660e-06, -3.9190e-06]], device='cuda:0')
Loss: 0.9619356393814087


Running epoch 1, step 1914, batch 866
Sampled inputs[:2]: tensor([[   0, 4100,   12,  ...,   13, 4710, 1558],
        [   0, 5319,   14,  ..., 2372, 2356, 4093]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8379e-04,  5.3555e-04,  2.1784e-04,  ..., -7.0429e-05,
         -2.5613e-04, -9.6949e-05],
        [-3.7849e-06, -2.5332e-06,  2.8275e-06,  ..., -3.3155e-06,
         -2.6189e-06, -2.6487e-06],
        [-1.1638e-05, -8.1956e-06,  9.2983e-06,  ..., -1.0148e-05,
         -8.1509e-06, -7.9125e-06],
        [-1.0923e-05, -7.2867e-06,  8.5980e-06,  ..., -9.5963e-06,
         -7.7188e-06, -7.8380e-06],
        [-8.5980e-06, -6.2138e-06,  6.6757e-06,  ..., -7.5847e-06,
         -6.3032e-06, -5.4166e-06]], device='cuda:0')
Loss: 0.930995762348175


Running epoch 1, step 1915, batch 867
Sampled inputs[:2]: tensor([[    0,    33,    12,  ...,  1110,   467, 17467],
        [    0,  2919,  1482,  ...,   587, 20186,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0977e-04,  5.1325e-04,  1.1138e-04,  ..., -8.4087e-05,
         -1.0547e-04,  8.0732e-05],
        [-5.0813e-06, -3.4273e-06,  3.8929e-06,  ..., -4.4033e-06,
         -3.5204e-06, -3.4720e-06],
        [-1.5572e-05, -1.1086e-05,  1.2726e-05,  ..., -1.3471e-05,
         -1.0982e-05, -1.0401e-05],
        [-1.4707e-05, -9.9242e-06,  1.1861e-05,  ..., -1.2785e-05,
         -1.0416e-05, -1.0356e-05],
        [-1.1384e-05, -8.3745e-06,  9.0599e-06,  ..., -9.9987e-06,
         -8.4490e-06, -7.0706e-06]], device='cuda:0')
Loss: 0.9658206701278687


Running epoch 1, step 1916, batch 868
Sampled inputs[:2]: tensor([[    0,   591, 18622,  ...,   955,  6118,  9191],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9593e-04,  5.8022e-04,  1.0974e-04,  ...,  6.1881e-05,
         -3.1541e-04, -1.8099e-05],
        [-6.4299e-06, -4.3511e-06,  4.8131e-06,  ..., -5.4836e-06,
         -4.3735e-06, -4.2990e-06],
        [-1.9804e-05, -1.4201e-05,  1.5870e-05,  ..., -1.6913e-05,
         -1.3739e-05, -1.3024e-05],
        [-1.8671e-05, -1.2681e-05,  1.4737e-05,  ..., -1.5989e-05,
         -1.2979e-05, -1.2904e-05],
        [-1.4275e-05, -1.0565e-05,  1.1146e-05,  ..., -1.2398e-05,
         -1.0446e-05, -8.7395e-06]], device='cuda:0')
Loss: 0.95560222864151


Running epoch 1, step 1917, batch 869
Sampled inputs[:2]: tensor([[    0,  1978, 20360,  ...,   898,   699, 10262],
        [    0,   266,  9823,  ...,    14,  1062,  7676]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0378e-04,  6.0211e-04,  8.8912e-05,  ...,  1.8462e-05,
         -4.1211e-04,  1.9559e-05],
        [-7.6964e-06, -5.2676e-06,  5.8413e-06,  ..., -6.5565e-06,
         -5.2452e-06, -5.1036e-06],
        [-2.3827e-05, -1.7241e-05,  1.9252e-05,  ..., -2.0355e-05,
         -1.6555e-05, -1.5602e-05],
        [-2.2411e-05, -1.5363e-05,  1.7896e-05,  ..., -1.9163e-05,
         -1.5542e-05, -1.5393e-05],
        [-1.7121e-05, -1.2800e-05,  1.3486e-05,  ..., -1.4886e-05,
         -1.2562e-05, -1.0446e-05]], device='cuda:0')
Loss: 0.9675525426864624


Running epoch 1, step 1918, batch 870
Sampled inputs[:2]: tensor([[    0, 24440,  1918,  ...,   769,  1254,   596],
        [    0,  7692,    12,  ...,   266,  2042,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0378e-04,  5.3066e-04,  1.4099e-04,  ..., -9.2085e-07,
         -6.2728e-04, -3.5546e-05],
        [-8.9407e-06, -6.0797e-06,  6.8471e-06,  ..., -7.5921e-06,
         -6.0089e-06, -5.9120e-06],
        [-2.7761e-05, -1.9923e-05,  2.2620e-05,  ..., -2.3603e-05,
         -1.8954e-05, -1.8120e-05],
        [-2.6181e-05, -1.7762e-05,  2.1100e-05,  ..., -2.2277e-05,
         -1.7837e-05, -1.7956e-05],
        [-1.9804e-05, -1.4707e-05,  1.5721e-05,  ..., -1.7151e-05,
         -1.4320e-05, -1.2033e-05]], device='cuda:0')
Loss: 0.9316994547843933


Running epoch 1, step 1919, batch 871
Sampled inputs[:2]: tensor([[    0,    17,   590,  ...,  1412,    35,  5015],
        [    0, 19191,   266,  ...,   287,   843,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4406e-04,  5.4736e-04,  1.4541e-04,  ...,  2.9598e-05,
         -5.3820e-04, -3.5546e-05],
        [-1.0237e-05, -7.0073e-06,  7.8976e-06,  ..., -8.6576e-06,
         -6.8769e-06, -6.7167e-06],
        [-3.1814e-05, -2.3007e-05,  2.6092e-05,  ..., -2.6986e-05,
         -2.1756e-05, -2.0668e-05],
        [-2.9981e-05, -2.0519e-05,  2.4348e-05,  ..., -2.5421e-05,
         -2.0429e-05, -2.0415e-05],
        [-2.2590e-05, -1.6928e-05,  1.8045e-05,  ..., -1.9521e-05,
         -1.6361e-05, -1.3664e-05]], device='cuda:0')
Loss: 0.9700548648834229
Graident accumulation at epoch 1, step 1919, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8886e-05,  1.6742e-04, -8.5601e-05,  ..., -7.6138e-05,
         -3.2583e-05, -1.0611e-04],
        [-6.3706e-06, -1.5196e-06,  4.0730e-06,  ..., -4.6228e-06,
         -8.3238e-07, -3.8363e-06],
        [-9.5791e-06,  3.1989e-06,  9.9923e-06,  ..., -1.0049e-06,
         -1.4299e-06, -5.1771e-06],
        [-2.4413e-05, -1.1205e-05,  1.8681e-05,  ..., -1.9892e-05,
         -1.3360e-05, -1.7414e-05],
        [-1.3398e-05, -4.5738e-06,  7.7722e-06,  ..., -9.7431e-06,
         -1.2615e-05, -7.9481e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3809e-08, 6.6043e-08, 5.1041e-08,  ..., 4.1981e-08, 1.3289e-07,
         5.7681e-08],
        [8.2579e-11, 6.3690e-11, 2.2670e-11,  ..., 5.9936e-11, 4.3187e-11,
         3.2667e-11],
        [4.3164e-09, 3.0894e-09, 1.5494e-09,  ..., 3.3305e-09, 1.6128e-09,
         1.0769e-09],
        [1.1914e-09, 1.3241e-09, 3.9749e-10,  ..., 1.0573e-09, 7.5373e-10,
         4.7323e-10],
        [3.7775e-10, 2.2794e-10, 9.8586e-11,  ..., 2.8929e-10, 8.9447e-11,
         1.1371e-10]], device='cuda:0')
optimizer state dict: 240.0
lr: [3.6752822198246384e-07, 3.6752822198246384e-07]
scheduler_last_epoch: 240


Running epoch 1, step 1920, batch 872
Sampled inputs[:2]: tensor([[    0,   221,  1771,  ..., 14547,  1705,  1003],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5660e-05,  6.7546e-05,  1.7950e-04,  ...,  7.2760e-06,
          5.7567e-05,  1.7508e-04],
        [-1.3337e-06, -9.0152e-07,  9.0152e-07,  ..., -1.1772e-06,
         -9.4622e-07, -9.2387e-07],
        [-4.0531e-06, -2.8908e-06,  2.9504e-06,  ..., -3.5912e-06,
         -2.9355e-06, -2.7865e-06],
        [-3.8743e-06, -2.6077e-06,  2.7418e-06,  ..., -3.4422e-06,
         -2.8014e-06, -2.7865e-06],
        [-3.0547e-06, -2.2501e-06,  2.1756e-06,  ..., -2.7418e-06,
         -2.3097e-06, -1.9819e-06]], device='cuda:0')
Loss: 0.977849006652832


Running epoch 1, step 1921, batch 873
Sampled inputs[:2]: tensor([[   0,  409,  394,  ...,  475, 5458,  328],
        [   0,   12, 9248,  ..., 2673, 4239,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5207e-04,  8.9428e-05,  2.4147e-04,  ...,  7.9562e-05,
          2.0765e-04,  2.6184e-04],
        [-2.5555e-06, -1.7397e-06,  1.7099e-06,  ..., -2.2948e-06,
         -1.8850e-06, -1.8030e-06],
        [ 3.6237e-04,  2.7840e-04, -2.8778e-04,  ...,  2.5954e-04,
          2.9489e-04,  2.5535e-04],
        [-7.5847e-06, -5.1409e-06,  5.3048e-06,  ..., -6.8694e-06,
         -5.6922e-06, -5.5581e-06],
        [-5.9456e-06, -4.3660e-06,  4.1872e-06,  ..., -5.3942e-06,
         -4.5896e-06, -3.8818e-06]], device='cuda:0')
Loss: 0.9881634712219238


Running epoch 1, step 1922, batch 874
Sampled inputs[:2]: tensor([[    0,   266,  3382,  ...,   759,   631,   369],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8263e-05,  1.3537e-04,  2.9503e-04,  ...,  1.7715e-05,
          1.3518e-04,  3.4180e-04],
        [-3.8594e-06, -2.6338e-06,  2.7753e-06,  ..., -3.3304e-06,
         -2.6897e-06, -2.6338e-06],
        [ 3.5840e-04,  2.7551e-04, -2.8432e-04,  ...,  2.5638e-04,
          2.9236e-04,  2.5283e-04],
        [-1.1429e-05, -7.7784e-06,  8.5980e-06,  ..., -9.9093e-06,
         -8.1211e-06, -8.0913e-06],
        [-8.6725e-06, -6.4224e-06,  6.4969e-06,  ..., -7.6145e-06,
         -6.4373e-06, -5.4836e-06]], device='cuda:0')
Loss: 0.9318045377731323


Running epoch 1, step 1923, batch 875
Sampled inputs[:2]: tensor([[   0, 5054, 3945,  ...,  272,  278,  516],
        [   0, 2790,  266,  ...,  401, 1496,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0408e-04,  1.3788e-04,  4.8949e-04,  ..., -2.4684e-05,
         -1.0170e-04,  3.2476e-04],
        [-5.0813e-06, -3.4943e-06,  3.7439e-06,  ..., -4.3660e-06,
         -3.4794e-06, -3.4384e-06],
        [ 3.5460e-04,  2.7267e-04, -2.8107e-04,  ...,  2.5315e-04,
          2.8984e-04,  2.5036e-04],
        [-1.5125e-05, -1.0356e-05,  1.1727e-05,  ..., -1.3039e-05,
         -1.0550e-05, -1.0625e-05],
        [-1.1325e-05, -8.4788e-06,  8.7023e-06,  ..., -9.9093e-06,
         -8.2999e-06, -7.0781e-06]], device='cuda:0')
Loss: 0.9398273229598999


Running epoch 1, step 1924, batch 876
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0,  5340,   287,  ...,   912,  2837,  5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8992e-06,  5.4641e-05,  5.4853e-04,  ..., -1.5234e-04,
          2.3170e-04,  3.5829e-04],
        [-6.3777e-06, -4.2915e-06,  4.6119e-06,  ..., -5.5060e-06,
         -4.3437e-06, -4.3847e-06],
        [ 3.5070e-04,  2.7020e-04, -2.7824e-04,  ...,  2.4983e-04,
          2.8727e-04,  2.4768e-04],
        [-1.8850e-05, -1.2621e-05,  1.4335e-05,  ..., -1.6317e-05,
         -1.3083e-05, -1.3381e-05],
        [-1.4216e-05, -1.0364e-05,  1.0744e-05,  ..., -1.2398e-05,
         -1.0297e-05, -8.9034e-06]], device='cuda:0')
Loss: 0.9596989154815674


Running epoch 1, step 1925, batch 877
Sampled inputs[:2]: tensor([[   0,  635,   13,  ...,  292,   20,  445],
        [   0,  638, 1862,  ...,   14, 7869,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5995e-05,  1.9566e-04,  7.0629e-04,  ..., -1.4336e-04,
          3.3506e-04,  4.5119e-04],
        [-7.6592e-06, -5.0925e-06,  5.5470e-06,  ..., -6.5938e-06,
         -5.1782e-06, -5.2825e-06],
        [ 3.4680e-04,  2.6755e-04, -2.7514e-04,  ...,  2.4650e-04,
          2.8464e-04,  2.4498e-04],
        [-2.2545e-05, -1.4946e-05,  1.7181e-05,  ..., -1.9491e-05,
         -1.5587e-05, -1.6049e-05],
        [-1.6987e-05, -1.2301e-05,  1.2860e-05,  ..., -1.4797e-05,
         -1.2264e-05, -1.0677e-05]], device='cuda:0')
Loss: 0.9157294034957886


Running epoch 1, step 1926, batch 878
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,   949, 11135,  ...,   278,   772,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5131e-05,  2.5736e-04,  8.0970e-04,  ..., -1.4275e-04,
          2.3347e-04,  3.9568e-04],
        [-8.9109e-06, -6.0201e-06,  6.5379e-06,  ..., -7.6741e-06,
         -6.0499e-06, -6.0834e-06],
        [ 3.4274e-04,  2.6439e-04, -2.7177e-04,  ...,  2.4297e-04,
          2.8175e-04,  2.4236e-04],
        [-2.6390e-05, -1.7792e-05,  2.0370e-05,  ..., -2.2814e-05,
         -1.8299e-05, -1.8626e-05],
        [-1.9759e-05, -1.4551e-05,  1.5125e-05,  ..., -1.7256e-05,
         -1.4365e-05, -1.2353e-05]], device='cuda:0')
Loss: 0.9763420820236206


Running epoch 1, step 1927, batch 879
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  401, 1576,  271],
        [   0,  685,  344,  ...,  680,  401,  616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3375e-05,  2.7884e-04,  8.0624e-04,  ..., -7.9479e-05,
          1.2188e-04,  3.9568e-04],
        [-1.0155e-05, -6.8098e-06,  7.4878e-06,  ..., -8.7172e-06,
         -6.7987e-06, -6.9290e-06],
        [ 3.3887e-04,  2.6181e-04, -2.6858e-04,  ...,  2.3975e-04,
          2.7942e-04,  2.3976e-04],
        [-3.0085e-05, -2.0087e-05,  2.3350e-05,  ..., -2.5913e-05,
         -2.0519e-05, -2.1219e-05],
        [-2.2531e-05, -1.6488e-05,  1.7345e-05,  ..., -1.9625e-05,
         -1.6168e-05, -1.4059e-05]], device='cuda:0')
Loss: 0.9228655099868774
Graident accumulation at epoch 1, step 1927, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.5660e-05,  1.7856e-04,  3.5833e-06,  ..., -7.6472e-05,
         -1.7137e-05, -5.5933e-05],
        [-6.7490e-06, -2.0486e-06,  4.4145e-06,  ..., -5.0322e-06,
         -1.4290e-06, -4.1456e-06],
        [ 2.5266e-05,  2.9060e-05, -1.7865e-05,  ...,  2.3071e-05,
          2.6655e-05,  1.9317e-05],
        [-2.4980e-05, -1.2093e-05,  1.9148e-05,  ..., -2.0494e-05,
         -1.4076e-05, -1.7794e-05],
        [-1.4311e-05, -5.7652e-06,  8.7294e-06,  ..., -1.0731e-05,
         -1.2971e-05, -8.5592e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3743e-08, 6.6055e-08, 5.1640e-08,  ..., 4.1946e-08, 1.3278e-07,
         5.7779e-08],
        [8.2599e-11, 6.3672e-11, 2.2703e-11,  ..., 5.9952e-11, 4.3190e-11,
         3.2682e-11],
        [4.4269e-09, 3.1549e-09, 1.6200e-09,  ..., 3.3846e-09, 1.6892e-09,
         1.1334e-09],
        [1.1911e-09, 1.3231e-09, 3.9764e-10,  ..., 1.0569e-09, 7.5339e-10,
         4.7320e-10],
        [3.7788e-10, 2.2798e-10, 9.8788e-11,  ..., 2.8939e-10, 8.9619e-11,
         1.1380e-10]], device='cuda:0')
optimizer state dict: 241.0
lr: [3.3505955649678844e-07, 3.3505955649678844e-07]
scheduler_last_epoch: 241


Running epoch 1, step 1928, batch 880
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,   287, 20640,   292],
        [    0,  9677,   609,  ...,   199,  1919,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3018e-04,  2.5333e-05,  2.7004e-05,  ...,  2.6763e-04,
         -1.8780e-04, -1.7456e-05],
        [-1.2591e-06, -8.0839e-07,  9.9093e-07,  ..., -1.1101e-06,
         -8.2701e-07, -8.4564e-07],
        [-3.8743e-06, -2.6226e-06,  3.2336e-06,  ..., -3.3975e-06,
         -2.5928e-06, -2.5630e-06],
        [-3.6657e-06, -2.3246e-06,  3.0100e-06,  ..., -3.2485e-06,
         -2.4289e-06, -2.5481e-06],
        [-2.8014e-06, -1.9819e-06,  2.2948e-06,  ..., -2.5034e-06,
         -1.9968e-06, -1.7062e-06]], device='cuda:0')
Loss: 0.9551624655723572


Running epoch 1, step 1929, batch 881
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   266,  2105,  3925],
        [    0,    14, 21687,  ...,   943,  2153,  4089]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3214e-05,  6.8370e-06,  9.0393e-06,  ...,  2.9560e-04,
         -2.1548e-04, -3.1257e-05],
        [-2.5108e-06, -1.7919e-06,  2.0191e-06,  ..., -2.1979e-06,
         -1.7099e-06, -1.6280e-06],
        [-7.8678e-06, -5.8860e-06,  6.6757e-06,  ..., -6.8843e-06,
         -5.4389e-06, -5.0813e-06],
        [-7.3612e-06, -5.2303e-06,  6.1840e-06,  ..., -6.4671e-06,
         -5.0515e-06, -4.9472e-06],
        [-5.5730e-06, -4.3362e-06,  4.6194e-06,  ..., -4.9770e-06,
         -4.0978e-06, -3.3155e-06]], device='cuda:0')
Loss: 1.0058815479278564


Running epoch 1, step 1930, batch 882
Sampled inputs[:2]: tensor([[    0,    13,  1311,  ...,   271,   795,   957],
        [    0,   221,   380,  ...,   630,  3765, 19107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2003e-05, -6.1830e-05,  2.1926e-05,  ...,  1.9935e-04,
         -2.0252e-04,  5.7536e-05],
        [-3.7402e-06, -2.6785e-06,  2.9244e-06,  ..., -3.2783e-06,
         -2.5891e-06, -2.4438e-06],
        [-1.1772e-05, -8.8662e-06,  9.7603e-06,  ..., -1.0297e-05,
         -8.2701e-06, -7.6592e-06],
        [-1.1042e-05, -7.8976e-06,  9.0301e-06,  ..., -9.7156e-06,
         -7.7337e-06, -7.5251e-06],
        [-8.2999e-06, -6.5267e-06,  6.7204e-06,  ..., -7.4208e-06,
         -6.1989e-06, -4.9844e-06]], device='cuda:0')
Loss: 0.9655078053474426


Running epoch 1, step 1931, batch 883
Sampled inputs[:2]: tensor([[    0,  2973, 20362,  ...,   271, 43821, 11776],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6632e-06,  5.9130e-05,  4.4171e-06,  ...,  6.8867e-05,
         -3.3136e-04,  4.1405e-05],
        [-5.0366e-06, -3.5353e-06,  3.8333e-06,  ..., -4.3958e-06,
         -3.4980e-06, -3.3304e-06],
        [ 9.1747e-05,  2.3508e-05, -4.2094e-06,  ...,  6.1012e-05,
          3.6246e-05,  5.5019e-05],
        [-1.4856e-05, -1.0401e-05,  1.1846e-05,  ..., -1.3009e-05,
         -1.0476e-05, -1.0237e-05],
        [-1.1221e-05, -8.6725e-06,  8.8811e-06,  ..., -9.9838e-06,
         -8.4043e-06, -6.8247e-06]], device='cuda:0')
Loss: 0.9510524272918701


Running epoch 1, step 1932, batch 884
Sampled inputs[:2]: tensor([[   0, 2732,  413,  ...,  287,  266, 3668],
        [   0, 6541,  287,  ..., 1061, 4786,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4248e-05,  1.6264e-05, -7.9976e-05,  ..., -1.6698e-05,
         -2.1668e-04,  1.0780e-04],
        [-6.2734e-06, -4.3251e-06,  4.7162e-06,  ..., -5.4687e-06,
         -4.3511e-06, -4.2170e-06],
        [ 8.7843e-05,  2.0915e-05, -1.1695e-06,  ...,  5.7689e-05,
          3.3579e-05,  5.2307e-05],
        [-1.8433e-05, -1.2636e-05,  1.4529e-05,  ..., -1.6108e-05,
         -1.2979e-05, -1.2875e-05],
        [-1.4007e-05, -1.0580e-05,  1.0967e-05,  ..., -1.2398e-05,
         -1.0416e-05, -8.5831e-06]], device='cuda:0')
Loss: 0.9320458769798279


Running epoch 1, step 1933, batch 885
Sampled inputs[:2]: tensor([[    0,    14,   292,  ...,  1385,    12,   287],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4826e-05,  2.3041e-04, -1.0882e-04,  ...,  9.0403e-06,
         -3.3653e-04,  1.1384e-04],
        [-7.5921e-06, -5.1036e-06,  5.6289e-06,  ..., -6.6087e-06,
         -5.2080e-06, -5.1670e-06],
        [ 8.3700e-05,  1.8307e-05,  1.9448e-06,  ...,  5.4143e-05,
          3.0882e-05,  4.9386e-05],
        [-2.2337e-05, -1.4901e-05,  1.7345e-05,  ..., -1.9491e-05,
         -1.5542e-05, -1.5765e-05],
        [-1.7002e-05, -1.2517e-05,  1.3143e-05,  ..., -1.5005e-05,
         -1.2472e-05, -1.0550e-05]], device='cuda:0')
Loss: 0.9507536888122559


Running epoch 1, step 1934, batch 886
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  995,   13, 6507],
        [   0,  368,  275,  ..., 6389, 9102,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2266e-05,  3.2488e-04, -2.5383e-05,  ..., -3.3115e-05,
         -2.4480e-04,  3.5934e-04],
        [-8.7991e-06, -6.0126e-06,  6.4522e-06,  ..., -7.7039e-06,
         -6.1467e-06, -6.0089e-06],
        [ 7.9886e-05,  1.5297e-05,  4.7462e-06,  ...,  5.0686e-05,
          2.7901e-05,  4.6763e-05],
        [-2.6017e-05, -1.7673e-05,  1.9982e-05,  ..., -2.2873e-05,
         -1.8477e-05, -1.8477e-05],
        [-1.9819e-05, -1.4797e-05,  1.5169e-05,  ..., -1.7583e-05,
         -1.4767e-05, -1.2346e-05]], device='cuda:0')
Loss: 0.9604644775390625


Running epoch 1, step 1935, batch 887
Sampled inputs[:2]: tensor([[   0,  462,  221,  ...,   29,  413, 1801],
        [   0, 4441, 1821,  ...,  642, 2310,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1369e-04,  6.2490e-04,  1.5500e-04,  ...,  2.3450e-04,
         -6.0970e-04,  1.1301e-04],
        [-1.0125e-05, -6.7949e-06,  7.4357e-06,  ..., -8.7991e-06,
         -6.9737e-06, -6.9402e-06],
        [ 7.5713e-05,  1.2690e-05,  8.0394e-06,  ...,  4.7273e-05,
          2.5249e-05,  4.3902e-05],
        [-2.9892e-05, -1.9938e-05,  2.2978e-05,  ..., -2.6092e-05,
         -2.0951e-05, -2.1264e-05],
        [-2.2784e-05, -1.6719e-05,  1.7449e-05,  ..., -2.0057e-05,
         -1.6764e-05, -1.4246e-05]], device='cuda:0')
Loss: 0.943992555141449
Graident accumulation at epoch 1, step 1935, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.0725e-05,  2.2319e-04,  1.8725e-05,  ..., -4.5374e-05,
         -7.6394e-05, -3.9039e-05],
        [-7.0867e-06, -2.5232e-06,  4.7166e-06,  ..., -5.4089e-06,
         -1.9835e-06, -4.4251e-06],
        [ 3.0310e-05,  2.7423e-05, -1.5275e-05,  ...,  2.5491e-05,
          2.6515e-05,  2.1776e-05],
        [-2.5472e-05, -1.2878e-05,  1.9531e-05,  ..., -2.1054e-05,
         -1.4763e-05, -1.8141e-05],
        [-1.5158e-05, -6.8606e-06,  9.6014e-06,  ..., -1.1664e-05,
         -1.3350e-05, -9.1278e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3682e-08, 6.6379e-08, 5.1613e-08,  ..., 4.1959e-08, 1.3301e-07,
         5.7734e-08],
        [8.2619e-11, 6.3655e-11, 2.2736e-11,  ..., 5.9970e-11, 4.3195e-11,
         3.2698e-11],
        [4.4282e-09, 3.1519e-09, 1.6185e-09,  ..., 3.3835e-09, 1.6882e-09,
         1.1342e-09],
        [1.1908e-09, 1.3222e-09, 3.9777e-10,  ..., 1.0565e-09, 7.5308e-10,
         4.7318e-10],
        [3.7803e-10, 2.2803e-10, 9.8994e-11,  ..., 2.8950e-10, 8.9810e-11,
         1.1389e-10]], device='cuda:0')
optimizer state dict: 242.0
lr: [3.040677782773405e-07, 3.040677782773405e-07]
scheduler_last_epoch: 242


Running epoch 1, step 1936, batch 888
Sampled inputs[:2]: tensor([[    0,  4878,   607,  ...,    14, 17331,   287],
        [    0,  3561,   278,  ..., 37517,   278,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1904e-04,  1.4967e-04,  1.3801e-05,  ..., -1.5233e-05,
          9.6458e-05,  1.3755e-05],
        [-1.3784e-06, -8.5309e-07,  9.6858e-07,  ..., -1.1772e-06,
         -8.5309e-07, -1.0207e-06],
        [-4.2915e-06, -2.7716e-06,  3.2037e-06,  ..., -3.6061e-06,
         -2.6375e-06, -3.0845e-06],
        [-3.8743e-06, -2.3842e-06,  2.8461e-06,  ..., -3.3081e-06,
         -2.3991e-06, -2.9206e-06],
        [-3.2932e-06, -2.1458e-06,  2.3842e-06,  ..., -2.8014e-06,
         -2.1309e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9670105576515198


Running epoch 1, step 1937, batch 889
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,   266,  4411,  ...,   368,  6388,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6212e-04,  1.0512e-04, -3.2313e-04,  ...,  6.0488e-05,
          1.0907e-04, -2.2485e-04],
        [-2.5854e-06, -1.6429e-06,  1.8515e-06,  ..., -2.2501e-06,
         -1.6391e-06, -1.9073e-06],
        [-8.0317e-06, -5.2750e-06,  6.1691e-06,  ..., -6.8545e-06,
         -5.0515e-06, -5.7071e-06],
        [-7.2867e-06, -4.5598e-06,  5.4687e-06,  ..., -6.3181e-06,
         -4.6343e-06, -5.4985e-06],
        [-5.9754e-06, -4.0233e-06,  4.4703e-06,  ..., -5.1707e-06,
         -3.9786e-06, -3.9488e-06]], device='cuda:0')
Loss: 0.9524036645889282


Running epoch 1, step 1938, batch 890
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4697e-04,  1.8379e-04, -2.5364e-04,  ...,  9.1783e-05,
          8.3499e-05, -2.3183e-04],
        [-3.8445e-06, -2.5406e-06,  2.8722e-06,  ..., -3.3006e-06,
         -2.4624e-06, -2.7046e-06],
        [-1.1876e-05, -8.1509e-06,  9.4920e-06,  ..., -1.0058e-05,
         -7.6443e-06, -8.1360e-06],
        [-1.0982e-05, -7.1675e-06,  8.6278e-06,  ..., -9.4026e-06,
         -7.0781e-06, -7.9423e-06],
        [-8.7023e-06, -6.1840e-06,  6.7800e-06,  ..., -7.5102e-06,
         -5.9605e-06, -5.5581e-06]], device='cuda:0')
Loss: 0.9721116423606873


Running epoch 1, step 1939, batch 891
Sampled inputs[:2]: tensor([[    0,  1278,    69,  ...,    15,  7377, 20524],
        [    0,  6574,  1707,  ...,    14,  5077,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5371e-04,  3.0985e-04, -2.3245e-04,  ...,  1.6105e-04,
          4.1234e-05, -2.2886e-04],
        [-5.0887e-06, -3.3975e-06,  3.8333e-06,  ..., -4.3586e-06,
         -3.3192e-06, -3.4943e-06],
        [-1.5900e-05, -1.1072e-05,  1.2770e-05,  ..., -1.3486e-05,
         -1.0431e-05, -1.0684e-05],
        [-1.4767e-05, -9.7752e-06,  1.1683e-05,  ..., -1.2636e-05,
         -9.7007e-06, -1.0446e-05],
        [-1.1414e-05, -8.2254e-06,  8.9556e-06,  ..., -9.8646e-06,
         -7.9423e-06, -7.1451e-06]], device='cuda:0')
Loss: 0.9585450291633606


Running epoch 1, step 1940, batch 892
Sampled inputs[:2]: tensor([[   0,  292,  685,  ...,  278, 3281,  298],
        [   0,   21,  292,  ...,   13, 1861, 4254]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7344e-04,  5.3142e-04, -6.3943e-05,  ...,  2.2035e-04,
         -4.0452e-04, -3.5883e-04],
        [-6.4000e-06, -4.1574e-06,  4.5635e-06,  ..., -5.5134e-06,
         -4.2245e-06, -4.4778e-06],
        [-1.9982e-05, -1.3575e-05,  1.5333e-05,  ..., -1.7032e-05,
         -1.3247e-05, -1.3664e-05],
        [-1.8731e-05, -1.2025e-05,  1.4007e-05,  ..., -1.6138e-05,
         -1.2472e-05, -1.3486e-05],
        [-1.4499e-05, -1.0148e-05,  1.0781e-05,  ..., -1.2591e-05,
         -1.0192e-05, -9.2611e-06]], device='cuda:0')
Loss: 0.9063866138458252


Running epoch 1, step 1941, batch 893
Sampled inputs[:2]: tensor([[   0,    5, 7523,  ...,  199, 8871,  266],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9652e-04,  6.0910e-04,  7.1326e-05,  ...,  3.1470e-04,
         -3.7271e-04, -2.7205e-04],
        [-7.6592e-06, -5.0813e-06,  5.4426e-06,  ..., -6.6161e-06,
         -5.1372e-06, -5.2936e-06],
        [-2.4036e-05, -1.6734e-05,  1.8373e-05,  ..., -2.0608e-05,
         -1.6257e-05, -1.6317e-05],
        [-2.2486e-05, -1.4782e-05,  1.6764e-05,  ..., -1.9446e-05,
         -1.5259e-05, -1.6063e-05],
        [-1.7375e-05, -1.2487e-05,  1.2867e-05,  ..., -1.5184e-05,
         -1.2442e-05, -1.1005e-05]], device='cuda:0')
Loss: 0.9559720158576965


Running epoch 1, step 1942, batch 894
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,  9161,   300,  4460],
        [    0,   287, 21212,  ...,  3123,   944,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5408e-04,  7.7755e-04,  3.6609e-05,  ...,  3.3999e-04,
         -1.0724e-03, -5.5317e-04],
        [-8.9854e-06, -5.8748e-06,  6.5230e-06,  ..., -7.6517e-06,
         -5.9009e-06, -6.1132e-06],
        [-2.8029e-05, -1.9252e-05,  2.1800e-05,  ..., -2.3708e-05,
         -1.8582e-05, -1.8761e-05],
        [-2.6390e-05, -1.7092e-05,  2.0087e-05,  ..., -2.2486e-05,
         -1.7509e-05, -1.8567e-05],
        [-2.0161e-05, -1.4350e-05,  1.5207e-05,  ..., -1.7405e-05,
         -1.4201e-05, -1.2577e-05]], device='cuda:0')
Loss: 0.9513651728630066


Running epoch 1, step 1943, batch 895
Sampled inputs[:2]: tensor([[   0,  397, 1267,  ..., 1276,  292,  221],
        [   0, 2771, 2070,  ...,  221,  396,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6121e-04,  8.2516e-04, -6.8457e-05,  ...,  3.2631e-04,
         -1.0008e-03, -5.7238e-04],
        [-1.0207e-05, -6.7316e-06,  7.3873e-06,  ..., -8.7842e-06,
         -6.7763e-06, -6.9924e-06],
        [-3.1844e-05, -2.2054e-05,  2.4736e-05,  ..., -2.7224e-05,
         -2.1324e-05, -2.1458e-05],
        [ 5.4709e-05,  3.3082e-05, -1.1641e-05,  ...,  4.4555e-05,
          4.0872e-05,  4.5643e-05],
        [-2.2978e-05, -1.6466e-05,  1.7293e-05,  ..., -2.0027e-05,
         -1.6317e-05, -1.4417e-05]], device='cuda:0')
Loss: 0.9435274004936218
Graident accumulation at epoch 1, step 1943, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0163],
        [ 0.0063, -0.0141,  0.0019,  ..., -0.0019,  0.0238, -0.0188],
        [ 0.0287, -0.0083,  0.0042,  ..., -0.0100, -0.0031, -0.0345],
        [ 0.0338, -0.0097,  0.0403,  ...,  0.0225,  0.0063, -0.0018],
        [-0.0153,  0.0158, -0.0288,  ...,  0.0294, -0.0138, -0.0169]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.4773e-05,  2.8339e-04,  1.0007e-05,  ..., -8.2062e-06,
         -1.6883e-04, -9.2373e-05],
        [-7.3987e-06, -2.9441e-06,  4.9837e-06,  ..., -5.7464e-06,
         -2.4628e-06, -4.6818e-06],
        [ 2.4095e-05,  2.2475e-05, -1.1274e-05,  ...,  2.0220e-05,
          2.1731e-05,  1.7452e-05],
        [-1.7453e-05, -8.2818e-06,  1.6414e-05,  ..., -1.4493e-05,
         -9.1997e-06, -1.1763e-05],
        [-1.5940e-05, -7.8211e-06,  1.0371e-05,  ..., -1.2500e-05,
         -1.3647e-05, -9.6567e-06]], device='cuda:0')
optimizer state dict: tensor([[7.3923e-08, 6.6994e-08, 5.1566e-08,  ..., 4.2023e-08, 1.3388e-07,
         5.8004e-08],
        [8.2641e-11, 6.3636e-11, 2.2768e-11,  ..., 5.9987e-11, 4.3198e-11,
         3.2714e-11],
        [4.4248e-09, 3.1492e-09, 1.6175e-09,  ..., 3.3808e-09, 1.6869e-09,
         1.1335e-09],
        [1.1926e-09, 1.3220e-09, 3.9751e-10,  ..., 1.0574e-09, 7.5400e-10,
         4.7479e-10],
        [3.7818e-10, 2.2808e-10, 9.9194e-11,  ..., 2.8961e-10, 8.9987e-11,
         1.1398e-10]], device='cuda:0')
optimizer state dict: 243.0
lr: [2.745576231383573e-07, 2.745576231383573e-07]
scheduler_last_epoch: 243


Running epoch 1, step 1944, batch 896
Sampled inputs[:2]: tensor([[    0,   266,   298,  ...,   654,   271,  4483],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4105e-05, -4.7210e-05, -1.1086e-04,  ..., -1.0259e-04,
         -1.1286e-04, -9.4798e-05],
        [-1.2890e-06, -8.8662e-07,  9.7603e-07,  ..., -1.0878e-06,
         -8.8289e-07, -8.2701e-07],
        [-4.0233e-06, -2.9206e-06,  3.2485e-06,  ..., -3.4124e-06,
         -2.7716e-06, -2.5779e-06],
        [-3.7700e-06, -2.5779e-06,  2.9951e-06,  ..., -3.2037e-06,
         -2.6077e-06, -2.5332e-06],
        [-2.9802e-06, -2.2352e-06,  2.3246e-06,  ..., -2.5779e-06,
         -2.1756e-06, -1.7807e-06]], device='cuda:0')
Loss: 0.9644559025764465


Running epoch 1, step 1945, batch 897
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0,   369, 17432,  ...,   874,  2577,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4392e-05, -8.7580e-05, -1.9554e-04,  ..., -2.1849e-04,
         -8.3057e-05, -1.1532e-04],
        [-2.5108e-06, -1.7472e-06,  1.8999e-06,  ..., -2.1905e-06,
         -1.7807e-06, -1.6242e-06],
        [-7.7188e-06, -5.6624e-06,  6.2585e-06,  ..., -6.7204e-06,
         -5.4687e-06, -4.9621e-06],
        [-7.3314e-06, -5.0813e-06,  5.8562e-06,  ..., -6.4224e-06,
         -5.2601e-06, -4.9770e-06],
        [-5.6475e-06, -4.2915e-06,  4.4405e-06,  ..., -5.0068e-06,
         -4.2468e-06, -3.3528e-06]], device='cuda:0')
Loss: 0.9573236107826233


Running epoch 1, step 1946, batch 898
Sampled inputs[:2]: tensor([[    0,   278, 19142,  ...,   271,   266,   298],
        [    0,   266,   996,  ...,   709,   616,  9378]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2458e-05, -9.9368e-05, -1.2959e-04,  ..., -2.6780e-04,
         -6.7322e-05, -1.6619e-04],
        [-3.8892e-06, -2.6487e-06,  2.9355e-06,  ..., -3.3081e-06,
         -2.6152e-06, -2.5369e-06],
        [-1.1951e-05, -8.6576e-06,  9.7007e-06,  ..., -1.0207e-05,
         -8.1807e-06, -7.7784e-06],
        [-1.1206e-05, -7.6443e-06,  8.9556e-06,  ..., -9.5963e-06,
         -7.7039e-06, -7.6443e-06],
        [-8.6129e-06, -6.4969e-06,  6.7949e-06,  ..., -7.5102e-06,
         -6.2883e-06, -5.1856e-06]], device='cuda:0')
Loss: 0.9568151831626892


Running epoch 1, step 1947, batch 899
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,    21,    14,  ...,  1159,  1978, 33323]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5624e-04, -6.1951e-05, -2.4096e-04,  ..., -2.2873e-04,
         -2.4384e-04, -1.4681e-04],
        [-5.2229e-06, -3.5167e-06,  3.9265e-06,  ..., -4.4107e-06,
         -3.4422e-06, -3.4980e-06],
        [-1.6272e-05, -1.1593e-05,  1.3128e-05,  ..., -1.3769e-05,
         -1.0878e-05, -1.0863e-05],
        [-1.5110e-05, -1.0163e-05,  1.2010e-05,  ..., -1.2845e-05,
         -1.0163e-05, -1.0550e-05],
        [-1.1653e-05, -8.6278e-06,  9.1195e-06,  ..., -1.0073e-05,
         -8.3148e-06, -7.2122e-06]], device='cuda:0')
Loss: 0.9563610553741455
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:          Batch ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá
wandb:   End of epoch ‚ñÅ
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  Learning Rate ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: Validation PPL ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 799
wandb:   End of epoch 0
wandb:          Epoch 1
wandb:  Learning Rate 0.0
wandb:   Training PPL 2420.71363
wandb: Validation PPL 6.74398
wandb: 
wandb: üöÄ View run dulcet-forest-310 at: https://wandb.ai/kenotron/brainlessgpt/runs/ah77duc8
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_133252-ah77duc8/logs
