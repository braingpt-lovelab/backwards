nohup: ignoring input
4
wandb: Currently logged in as: kenotron. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /datadrive1/ken/projects/backwards/model_training/wandb/run-20250325_140117-y1iuvq6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run upbeat-breeze-312
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenotron/brainlessgpt
wandb: üöÄ View run at https://wandb.ai/kenotron/brainlessgpt/runs/y1iuvq6b
rank: 0
Load custom tokenizer from cache/gpt2_neuro_tokenizer
{'train': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 2095
}), 'validation': Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 476
})}
Loading 2095 samples for training
Loading 476 samples for validation
Train from scratch
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
Resuming training from checkpoint: exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_319
Resumed training from epoch 0, step 320
Start training


Running epoch 0, step 0, batch 0
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0, 5221, 7166,  ..., 4309,  342,  996]], device='cuda:0')
Skip epoch 0, step 0, batch 0


Running epoch 0, step 1, batch 1
Sampled inputs[:2]: tensor([[    0,  9423,   298,  ...,  5274, 37902,   271],
        [    0, 24414,  4865,  ...,  8720,   344,  1566]], device='cuda:0')
Skip epoch 0, step 1, batch 1


Running epoch 0, step 2, batch 2
Sampled inputs[:2]: tensor([[    0,   278, 14971,  ...,  2341,   266,   717],
        [    0,   714,    14,  ...,  1501, 11397, 31940]], device='cuda:0')
Skip epoch 0, step 2, batch 2


Running epoch 0, step 3, batch 3
Sampled inputs[:2]: tensor([[    0, 21930,    12,  ...,  2849,   863,   578],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
Skip epoch 0, step 3, batch 3


Running epoch 0, step 4, batch 4
Sampled inputs[:2]: tensor([[   0, 5603, 6598,  ..., 1692, 1713,  365],
        [   0,   76,   15,  ...,   14,  333,  199]], device='cuda:0')
Skip epoch 0, step 4, batch 4


Running epoch 0, step 5, batch 5
Sampled inputs[:2]: tensor([[    0,   292,  2908,  ..., 16658,  7440,   271],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
Skip epoch 0, step 5, batch 5


Running epoch 0, step 6, batch 6
Sampled inputs[:2]: tensor([[    0, 19350,   271,  ...,   445,  1841,   446],
        [    0,   659,   278,  ...,   769,  1728,   278]], device='cuda:0')
Skip epoch 0, step 6, batch 6


Running epoch 0, step 7, batch 7
Sampled inputs[:2]: tensor([[    0,   957,  1231,  ...,   800,   342,  1398],
        [    0,  1403,    12,  ...,  1062,  2283, 13614]], device='cuda:0')
Skip epoch 0, step 7, batch 7


Running epoch 0, step 8, batch 8
Sampled inputs[:2]: tensor([[    0,   365,  5392,  ...,    14,   333,   199],
        [    0,   565,  5539,  ...,    12,   516, 14426]], device='cuda:0')
Skip epoch 0, step 8, batch 8


Running epoch 0, step 9, batch 9
Sampled inputs[:2]: tensor([[    0,  1853,  3373,  ...,  3020,  6695,   300],
        [    0,  2919,  1482,  ...,   587, 20186,   275]], device='cuda:0')
Skip epoch 0, step 9, batch 9


Running epoch 0, step 10, batch 10
Sampled inputs[:2]: tensor([[   0,  413,   16,  ...,  680,  401, 1407],
        [   0, 2192, 3182,  ..., 1445, 1531,  300]], device='cuda:0')
Skip epoch 0, step 10, batch 10


Running epoch 0, step 11, batch 11
Sampled inputs[:2]: tensor([[    0,    12,  1808,  ...,   847,   300, 44349],
        [    0,    12,   696,  ..., 14275,  2661,  6129]], device='cuda:0')
Skip epoch 0, step 11, batch 11


Running epoch 0, step 12, batch 12
Sampled inputs[:2]: tensor([[    0,  1167,   278,  ...,   278,  1853,  1424],
        [    0,  1410,   271,  ...,   259, 27726,  9533]], device='cuda:0')
Skip epoch 0, step 12, batch 12


Running epoch 0, step 13, batch 13
Sampled inputs[:2]: tensor([[    0,    13, 15578,  ...,   221,   494,   221],
        [    0,    20,    13,  ...,   496,    14,  1032]], device='cuda:0')
Skip epoch 0, step 13, batch 13


Running epoch 0, step 14, batch 14
Sampled inputs[:2]: tensor([[    0,  4889,  3593,  ..., 19787,   287, 22475],
        [    0,   271,  4219,  ...,   644,    14,  3607]], device='cuda:0')
Skip epoch 0, step 14, batch 14


Running epoch 0, step 15, batch 15
Sampled inputs[:2]: tensor([[    0,  3933,  6394,  ...,  1364,   950,   847],
        [    0,   607, 32336,  ...,  4787,   367,  1255]], device='cuda:0')
Skip epoch 0, step 15, batch 15


Running epoch 0, step 16, batch 16
Sampled inputs[:2]: tensor([[    0,   300, 26138,  ...,  7856,    14, 17535],
        [    0, 11853,  1611,  ...,  4413,  4240,   278]], device='cuda:0')
Skip epoch 0, step 16, batch 16


Running epoch 0, step 17, batch 17
Sampled inputs[:2]: tensor([[    0,  3448,   278,  ...,   380,   333,   199],
        [    0,  1265,  1545,  ...,   292, 36667, 36197]], device='cuda:0')
Skip epoch 0, step 17, batch 17


Running epoch 0, step 18, batch 18
Sampled inputs[:2]: tensor([[    0,   437,  1119,  ..., 32831,    83,   623],
        [    0, 12987,   609,  ...,   699,  9863,  3227]], device='cuda:0')
Skip epoch 0, step 18, batch 18


Running epoch 0, step 19, batch 19
Sampled inputs[:2]: tensor([[    0, 18322,   287,  ...,   953,   271,   221],
        [    0,  1480,   518,  ...,   445,    28,   445]], device='cuda:0')
Skip epoch 0, step 19, batch 19


Running epoch 0, step 20, batch 20
Sampled inputs[:2]: tensor([[    0,   266,  1403,  ...,  5145,   266,  3470],
        [    0,    13,  1581,  ...,    13, 11628, 14876]], device='cuda:0')
Skip epoch 0, step 20, batch 20


Running epoch 0, step 21, batch 21
Sampled inputs[:2]: tensor([[    0, 15165,   287,  ..., 15049,   278,   266],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
Skip epoch 0, step 21, batch 21


Running epoch 0, step 22, batch 22
Sampled inputs[:2]: tensor([[   0,  278, 7524,  ..., 1288,  669,  352],
        [   0, 1101,  300,  ..., 6104,  367,  993]], device='cuda:0')
Skip epoch 0, step 22, batch 22


Running epoch 0, step 23, batch 23
Sampled inputs[:2]: tensor([[    0, 18971,   278,  ...,  1934,  1916,  2612],
        [    0,  4073,  1548,  ...,   292,   221,   301]], device='cuda:0')
Skip epoch 0, step 23, batch 23


Running epoch 0, step 24, batch 24
Sampled inputs[:2]: tensor([[    0,  1042,  5738,  ...,    12,   287,  3643],
        [    0,  1477,   591,  ...,  4111, 18012, 11991]], device='cuda:0')
Skip epoch 0, step 24, batch 24


Running epoch 0, step 25, batch 25
Sampled inputs[:2]: tensor([[    0,   300,  1064,  ...,  6953,   944,   278],
        [    0,   271,  8130,  ...,   609, 28676,   965]], device='cuda:0')
Skip epoch 0, step 25, batch 25


Running epoch 0, step 26, batch 26
Sampled inputs[:2]: tensor([[    0,    15,  4291,  ...,  1685,   278,  2101],
        [    0,  2823,   287,  ...,  3504,     9, 13910]], device='cuda:0')
Skip epoch 0, step 26, batch 26


Running epoch 0, step 27, batch 27
Sampled inputs[:2]: tensor([[   0,   73,   30,  ..., 4112,   12, 9416],
        [   0, 9116,  278,  ..., 6997, 3244, 1192]], device='cuda:0')
Skip epoch 0, step 27, batch 27


Running epoch 0, step 28, batch 28
Sampled inputs[:2]: tensor([[    0,    19, 18798,  ...,    13, 17982,    20],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
Skip epoch 0, step 28, batch 28


Running epoch 0, step 29, batch 29
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 14761,  9700,   298],
        [    0,  6143,   642,  ...,   199, 14300,    41]], device='cuda:0')
Skip epoch 0, step 29, batch 29


Running epoch 0, step 30, batch 30
Sampled inputs[:2]: tensor([[    0,   266, 11080,  ...,   413,  7308,   413],
        [    0,   292,   380,  ...,  9636,   417,   199]], device='cuda:0')
Skip epoch 0, step 30, batch 30


Running epoch 0, step 31, batch 31
Sampled inputs[:2]: tensor([[    0,   298, 21144,  ...,  7825, 19426,  3709],
        [    0,   493,   221,  ...,   259,   726,  2786]], device='cuda:0')
Skip epoch 0, step 31, batch 31


Running epoch 0, step 32, batch 32
Sampled inputs[:2]: tensor([[    0, 31309,    83,  ...,  2923,   391,   266],
        [    0,  1049,   292,  ...,   221,   380,   341]], device='cuda:0')
Skip epoch 0, step 32, batch 32


Running epoch 0, step 33, batch 33
Sampled inputs[:2]: tensor([[   0,  300,  369,  ...,   12,  970,   12],
        [   0, 2140,   12,  ...,  696,  688, 1998]], device='cuda:0')
Skip epoch 0, step 33, batch 33


Running epoch 0, step 34, batch 34
Sampled inputs[:2]: tensor([[    0,  1978, 20360,  ...,   898,   699, 10262],
        [    0,    14,   381,  ...,   278,   269, 10376]], device='cuda:0')
Skip epoch 0, step 34, batch 34


Running epoch 0, step 35, batch 35
Sampled inputs[:2]: tensor([[   0, 4485,  741,  ...,  292,  221,  341],
        [   0, 1197, 3025,  ...,   14,  747, 3739]], device='cuda:0')
Skip epoch 0, step 35, batch 35


Running epoch 0, step 36, batch 36
Sampled inputs[:2]: tensor([[    0, 47354,  5923,  ...,   266, 14679,  8137],
        [    0,    33,    12,  ...,  1110,   467, 17467]], device='cuda:0')
Skip epoch 0, step 36, batch 36


Running epoch 0, step 37, batch 37
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0,   275,  1184,  ...,   328, 46278,  2117]], device='cuda:0')
Skip epoch 0, step 37, batch 37


Running epoch 0, step 38, batch 38
Sampled inputs[:2]: tensor([[   0, 6132,  300,  ...,   37,  271,  259],
        [   0,   15,   19,  ...,  266, 6391, 1777]], device='cuda:0')
Skip epoch 0, step 38, batch 38


Running epoch 0, step 39, batch 39
Sampled inputs[:2]: tensor([[    0,  1486,   292,  ...,  7484,    15,  5357],
        [    0,   266, 12964,  ...,   300,  3979,  4706]], device='cuda:0')
Skip epoch 0, step 39, batch 39


Running epoch 0, step 40, batch 40
Sampled inputs[:2]: tensor([[    0,  2629, 13422,  ...,  1042,  5301,    12],
        [    0,   287,  2503,  ...,   496,    14, 37791]], device='cuda:0')
Skip epoch 0, step 40, batch 40


Running epoch 0, step 41, batch 41
Sampled inputs[:2]: tensor([[    0, 15152,  1106,  ...,   607,   266,  2529],
        [    0,   313,    66,  ...,   894,  2973, 25074]], device='cuda:0')
Skip epoch 0, step 41, batch 41


Running epoch 0, step 42, batch 42
Sampled inputs[:2]: tensor([[    0,  1016,  1387,  ..., 12156, 14838,  3550],
        [    0,  1075,   940,  ...,  3780,    13,  4467]], device='cuda:0')
Skip epoch 0, step 42, batch 42


Running epoch 0, step 43, batch 43
Sampled inputs[:2]: tensor([[    0,  1234,   408,  ...,   292, 17323,   221],
        [    0,   508, 22318,  ...,    13,  1107,  4093]], device='cuda:0')
Skip epoch 0, step 43, batch 43


Running epoch 0, step 44, batch 44
Sampled inputs[:2]: tensor([[    0,    12,   401,  ...,  7665,  4101, 10193],
        [    0,  1008,   266,  ...,  1941,   437,  1626]], device='cuda:0')
Skip epoch 0, step 44, batch 44


Running epoch 0, step 45, batch 45
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0, 16847,  2027,  ...,     5,  1460,   496]], device='cuda:0')
Skip epoch 0, step 45, batch 45


Running epoch 0, step 46, batch 46
Sampled inputs[:2]: tensor([[    0,   292,   685,  ...,   278,  3281,   298],
        [    0,  1163,  5728,  ..., 24586,   756,    14]], device='cuda:0')
Skip epoch 0, step 46, batch 46


Running epoch 0, step 47, batch 47
Sampled inputs[:2]: tensor([[   0,  857,  352,  ..., 3608,  271,  995],
        [   0, 2159,  271,  ..., 1268,  344,  259]], device='cuda:0')
Skip epoch 0, step 47, batch 47


Running epoch 0, step 48, batch 48
Sampled inputs[:2]: tensor([[    0,  6418,   446,  ...,   413,    29,   413],
        [    0,  1371,   287,  ...,   689,   278, 12774]], device='cuda:0')
Skip epoch 0, step 48, batch 48


Running epoch 0, step 49, batch 49
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,  287,  552,  ..., 7407, 2401,  287]], device='cuda:0')
Skip epoch 0, step 49, batch 49


Running epoch 0, step 50, batch 50
Sampled inputs[:2]: tensor([[    0,   413,    20,  ...,  2089,    12, 21064],
        [    0,  8353,  1842,  ...,    38,   643,   472]], device='cuda:0')
Skip epoch 0, step 50, batch 50


Running epoch 0, step 51, batch 51
Sampled inputs[:2]: tensor([[   0,  259, 2180,  ...,  638, 1615,  694],
        [   0,  300, 3808,  ...,  496,   14, 1364]], device='cuda:0')
Skip epoch 0, step 51, batch 51


Running epoch 0, step 52, batch 52
Sampled inputs[:2]: tensor([[   0,  278,  554,  ...,  365, 3125,  271],
        [   0,  334,  344,  ...,  266, 4141,  287]], device='cuda:0')
Skip epoch 0, step 52, batch 52


Running epoch 0, step 53, batch 53
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,   452,   298,  ...,   287,  1575,  7856]], device='cuda:0')
Skip epoch 0, step 53, batch 53


Running epoch 0, step 54, batch 54
Sampled inputs[:2]: tensor([[    0,   292,   263,  ...,   342,  4575,   271],
        [    0,   352,  2284,  ..., 43204,    12,   709]], device='cuda:0')
Skip epoch 0, step 54, batch 54


Running epoch 0, step 55, batch 55
Sampled inputs[:2]: tensor([[   0, 3115, 1640,  ...,  300,  266, 5453],
        [   0,  271,  266,  ...,   70,   27, 5311]], device='cuda:0')
Skip epoch 0, step 55, batch 55


Running epoch 0, step 56, batch 56
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ..., 3230,  328,  790],
        [   0, 4215, 1478,  ...,  644,  409, 3803]], device='cuda:0')
Skip epoch 0, step 56, batch 56


Running epoch 0, step 57, batch 57
Sampled inputs[:2]: tensor([[    0,   367,  3399,  ..., 13481,   408,  6944],
        [    0,   266,  5232,  ...,  2719,    13, 25385]], device='cuda:0')
Skip epoch 0, step 57, batch 57


Running epoch 0, step 58, batch 58
Sampled inputs[:2]: tensor([[    0,  1471,   266,  ...,   525,  5202,   292],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
Skip epoch 0, step 58, batch 58


Running epoch 0, step 59, batch 59
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 8271,  365,  437],
        [   0,  298,  894,  ...,  396,  298,  527]], device='cuda:0')
Skip epoch 0, step 59, batch 59


Running epoch 0, step 60, batch 60
Sampled inputs[:2]: tensor([[   0, 2344,  271,  ..., 5415,   14, 1075],
        [   0, 2356,  292,  ...,   12,  287,  300]], device='cuda:0')
Skip epoch 0, step 60, batch 60


Running epoch 0, step 61, batch 61
Sampled inputs[:2]: tensor([[    0,   767,  4478,  ...,   278,   266, 19201],
        [    0,  3577,    12,  ...,  4222,  2137, 31332]], device='cuda:0')
Skip epoch 0, step 61, batch 61


Running epoch 0, step 62, batch 62
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0, 41010,  6737,  ...,   963,   409,   382]], device='cuda:0')
Skip epoch 0, step 62, batch 62


Running epoch 0, step 63, batch 63
Sampled inputs[:2]: tensor([[   0, 1527,  292,  ..., 2122,  278, 1911],
        [   0, 9466,   36,  ..., 1795,  437,  874]], device='cuda:0')
Skip epoch 0, step 63, batch 63


Running epoch 0, step 64, batch 64
Sampled inputs[:2]: tensor([[    0,   456,    17,  ...,  1553, 29477,  2713],
        [    0,   394,   292,  ...,  1711,   365,   897]], device='cuda:0')
Skip epoch 0, step 64, batch 64


Running epoch 0, step 65, batch 65
Sampled inputs[:2]: tensor([[    0, 10215,   408,  ...,  6071,   360,  1317],
        [    0,   461,   654,  ...,  4145,  7600,  4142]], device='cuda:0')
Skip epoch 0, step 65, batch 65


Running epoch 0, step 66, batch 66
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0,  266, 4287,  ...,  367, 4428, 2118]], device='cuda:0')
Skip epoch 0, step 66, batch 66


Running epoch 0, step 67, batch 67
Sampled inputs[:2]: tensor([[    0,   508,  2322,  ...,   968,   266, 15123],
        [    0,   825,  1243,  ...,    15,    22,    42]], device='cuda:0')
Skip epoch 0, step 67, batch 67


Running epoch 0, step 68, batch 68
Sampled inputs[:2]: tensor([[    0,    14,    22,  ...,  1319,   271,   266],
        [    0, 13856,   278,  ...,    14,    69,   462]], device='cuda:0')
Skip epoch 0, step 68, batch 68


Running epoch 0, step 69, batch 69
Sampled inputs[:2]: tensor([[    0,   995,    13,  ...,  3494,   367,  6768],
        [    0,   278,  2305,  ...,  2529, 34181,  4555]], device='cuda:0')
Skip epoch 0, step 69, batch 69


Running epoch 0, step 70, batch 70
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,    14,  1364, 19388],
        [    0,    14,   469,  ...,   367,  2564,   368]], device='cuda:0')
Skip epoch 0, step 70, batch 70


Running epoch 0, step 71, batch 71
Sampled inputs[:2]: tensor([[    0, 38136,    12,  ...,   367, 12851,  1040],
        [    0,  1774,  1781,  ...,  4685,   409,  4614]], device='cuda:0')
Skip epoch 0, step 71, batch 71


Running epoch 0, step 72, batch 72
Sampled inputs[:2]: tensor([[    0, 14409, 45007,  ...,  1197,   266,   944],
        [    0, 13312,  9048,  ..., 33470,  8672,  3524]], device='cuda:0')
Skip epoch 0, step 72, batch 72


Running epoch 0, step 73, batch 73
Sampled inputs[:2]: tensor([[    0,  1832,   292,  ...,  2176,  1345,    14],
        [    0,   380,  3584,  ..., 24402,  2057,     9]], device='cuda:0')
Skip epoch 0, step 73, batch 73


Running epoch 0, step 74, batch 74
Sampled inputs[:2]: tensor([[   0,  287, 1410,  ..., 1255, 1699,  328],
        [   0, 1920,   19,  ..., 5232,  796, 1303]], device='cuda:0')
Skip epoch 0, step 74, batch 74


Running epoch 0, step 75, batch 75
Sampled inputs[:2]: tensor([[    0, 45050,   342,  ...,  3729,   287, 27888],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
Skip epoch 0, step 75, batch 75


Running epoch 0, step 76, batch 76
Sampled inputs[:2]: tensor([[    0,  1196,  2612,  ...,  2489,    14,   333],
        [    0,   292,   380,  ...,   287, 10086,   300]], device='cuda:0')
Skip epoch 0, step 76, batch 76


Running epoch 0, step 77, batch 77
Sampled inputs[:2]: tensor([[    0,   894,    73,  ...,  2323,   909,  4103],
        [    0,  4710,    12,  ...,  3969,     9, 11692]], device='cuda:0')
Skip epoch 0, step 77, batch 77


Running epoch 0, step 78, batch 78
Sampled inputs[:2]: tensor([[    0, 10084,    12,  ..., 24717,   365,  1616],
        [    0,   221,   334,  ...,  1698,    13, 24137]], device='cuda:0')
Skip epoch 0, step 78, batch 78


Running epoch 0, step 79, batch 79
Sampled inputs[:2]: tensor([[   0,  369,  726,  ...,   83,  409,  729],
        [   0, 7070,   86,  ...,  298, 4930,  518]], device='cuda:0')
Skip epoch 0, step 79, batch 79


Running epoch 0, step 80, batch 80
Sampled inputs[:2]: tensor([[   0,  341, 2802,  ..., 1798,   12,  266],
        [   0, 1304,  292,  ..., 2101,  292,  474]], device='cuda:0')
Skip epoch 0, step 80, batch 80


Running epoch 0, step 81, batch 81
Sampled inputs[:2]: tensor([[   0,  685, 2461,  ...,  287,  298, 7943],
        [   0,   17,  292,  ..., 8055,  365, 3125]], device='cuda:0')
Skip epoch 0, step 81, batch 81


Running epoch 0, step 82, batch 82
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0,   344,  2574,  ...,  2558,  2663,   328]], device='cuda:0')
Skip epoch 0, step 82, batch 82


Running epoch 0, step 83, batch 83
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,   292, 23242,  ...,  6494,  3560,  1528]], device='cuda:0')
Skip epoch 0, step 83, batch 83


Running epoch 0, step 84, batch 84
Sampled inputs[:2]: tensor([[   0, 1236, 6446,  ...,  300,  706, 3698],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
Skip epoch 0, step 84, batch 84


Running epoch 0, step 85, batch 85
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0,  3134,   278,  ...,  2462,   300, 11015]], device='cuda:0')
Skip epoch 0, step 85, batch 85


Running epoch 0, step 86, batch 86
Sampled inputs[:2]: tensor([[   0, 2853,  590,  ..., 1351, 2927,   12],
        [   0,   17,   14,  ...,  650, 1711,  897]], device='cuda:0')
Skip epoch 0, step 86, batch 86


Running epoch 0, step 87, batch 87
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0, 22568,   287,  ...,    12,   471,    12]], device='cuda:0')
Skip epoch 0, step 87, batch 87


Running epoch 0, step 88, batch 88
Sampled inputs[:2]: tensor([[    0,  1934,  2413,  ..., 19697,    13, 16325],
        [    0,  2348,   565,  ...,    12,   709,   266]], device='cuda:0')
Skip epoch 0, step 88, batch 88


Running epoch 0, step 89, batch 89
Sampled inputs[:2]: tensor([[    0,   278,   795,  ...,  1774, 14474,   367],
        [    0,    14, 38591,  ...,   955,   892,  1635]], device='cuda:0')
Skip epoch 0, step 89, batch 89


Running epoch 0, step 90, batch 90
Sampled inputs[:2]: tensor([[    0,  3019,   278,  ...,   365,  1770,    12],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
Skip epoch 0, step 90, batch 90


Running epoch 0, step 91, batch 91
Sampled inputs[:2]: tensor([[    0, 14094,    83,  ...,  1431,   221,   287],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
Skip epoch 0, step 91, batch 91


Running epoch 0, step 92, batch 92
Sampled inputs[:2]: tensor([[    0,   380, 26765,  ...,     9,   367,  6930],
        [    0,   472,   346,  ...,   266,   720,   342]], device='cuda:0')
Skip epoch 0, step 92, batch 92


Running epoch 0, step 93, batch 93
Sampled inputs[:2]: tensor([[    0,    12,   287,  ..., 12678,  2503,   401],
        [    0,    12,   266,  ...,   278,   266, 10995]], device='cuda:0')
Skip epoch 0, step 93, batch 93


Running epoch 0, step 94, batch 94
Sampled inputs[:2]: tensor([[    0, 21891,     9,  ...,  5216,   717,   287],
        [    0, 45589,    13,  ...,    23,  6873,     9]], device='cuda:0')
Skip epoch 0, step 94, batch 94


Running epoch 0, step 95, batch 95
Sampled inputs[:2]: tensor([[    0,   287,   298,  ..., 14121,  3121,   409],
        [    0,   346,   462,  ..., 37683,    14,  1500]], device='cuda:0')
Skip epoch 0, step 95, batch 95


Running epoch 0, step 96, batch 96
Sampled inputs[:2]: tensor([[    0,    18,    66,  ...,    65,    17,   287],
        [    0,  5506,   696,  ...,   607, 11129,   276]], device='cuda:0')
Skip epoch 0, step 96, batch 96


Running epoch 0, step 97, batch 97
Sampled inputs[:2]: tensor([[    0,    12,   461,  ...,  2525,   278, 23762],
        [    0, 44175,   744,  ..., 16394, 26528,    12]], device='cuda:0')
Skip epoch 0, step 97, batch 97


Running epoch 0, step 98, batch 98
Sampled inputs[:2]: tensor([[    0,   278,   668,  ...,  2743,   638,   609],
        [    0,  1171,   341,  ...,   278, 14713,    18]], device='cuda:0')
Skip epoch 0, step 98, batch 98


Running epoch 0, step 99, batch 99
Sampled inputs[:2]: tensor([[   0, 1074, 1593,  ...,  992, 1810,  300],
        [   0,  292,   33,  ...,  352,  266, 9129]], device='cuda:0')
Skip epoch 0, step 99, batch 99


Running epoch 0, step 100, batch 100
Sampled inputs[:2]: tensor([[    0,  5159,   292,  ...,   772,   271,  3728],
        [    0, 25845,  4034,  ...,   474,   221,   474]], device='cuda:0')
Skip epoch 0, step 100, batch 100


Running epoch 0, step 101, batch 101
Sampled inputs[:2]: tensor([[    0,  1005,   292,  ...,   266, 19171,  2474],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
Skip epoch 0, step 101, batch 101


Running epoch 0, step 102, batch 102
Sampled inputs[:2]: tensor([[   0,  508,  586,  ..., 6157, 3146, 7647],
        [   0, 1458,  365,  ..., 5399, 1110,  870]], device='cuda:0')
Skip epoch 0, step 102, batch 102


Running epoch 0, step 103, batch 103
Sampled inputs[:2]: tensor([[    0,  6416,   367,  ...,   496,    14,    20],
        [    0,   271,  4136,  ...,  5052, 14552,  3339]], device='cuda:0')
Skip epoch 0, step 103, batch 103


Running epoch 0, step 104, batch 104
Sampled inputs[:2]: tensor([[    0,  2973, 20362,  ...,   271, 43821, 11776],
        [    0,  5105,   271,  ...,   308,  3056,  3640]], device='cuda:0')
Skip epoch 0, step 104, batch 104


Running epoch 0, step 105, batch 105
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,  266, 1371, 4811],
        [   0,  380,  759,  ..., 1420, 1804,  490]], device='cuda:0')
Skip epoch 0, step 105, batch 105


Running epoch 0, step 106, batch 106
Sampled inputs[:2]: tensor([[   0, 1253,  287,  ..., 2988,   14,  417],
        [   0,  266, 2086,  ..., 4283,  720,   14]], device='cuda:0')
Skip epoch 0, step 106, batch 106


Running epoch 0, step 107, batch 107
Sampled inputs[:2]: tensor([[    0,    80, 10802,  ...,   287, 28533, 25359],
        [    0,  1159,   278,  ...,     9,   271,   266]], device='cuda:0')
Skip epoch 0, step 107, batch 107


Running epoch 0, step 108, batch 108
Sampled inputs[:2]: tensor([[   0,   14, 3080,  ...,  910,  266, 5275],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
Skip epoch 0, step 108, batch 108


Running epoch 0, step 109, batch 109
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 6088, 1172,  ...,  546,  401,  925]], device='cuda:0')
Skip epoch 0, step 109, batch 109


Running epoch 0, step 110, batch 110
Sampled inputs[:2]: tensor([[   0,  381, 1795,  ...,   12,  344,  593],
        [   0, 5024, 3846,  ..., 5880, 1377,   12]], device='cuda:0')
Skip epoch 0, step 110, batch 110


Running epoch 0, step 111, batch 111
Sampled inputs[:2]: tensor([[    0,    12,  4567,  ...,  4154,  1799, 11883],
        [    0,   257,   298,  ...,  1878,   328,   259]], device='cuda:0')
Skip epoch 0, step 111, batch 111


Running epoch 0, step 112, batch 112
Sampled inputs[:2]: tensor([[    0,   380,   981,  ...,   567,  5407,   472],
        [    0,   688,  2353,  ..., 20538, 10393,    12]], device='cuda:0')
Skip epoch 0, step 112, batch 112


Running epoch 0, step 113, batch 113
Sampled inputs[:2]: tensor([[    0,  2895,    26,  ..., 11645,  1535,  1558],
        [    0,  7963,    17,  ...,    50,    13,    18]], device='cuda:0')
Skip epoch 0, step 113, batch 113


Running epoch 0, step 114, batch 114
Sampled inputs[:2]: tensor([[    0,   756,   943,  ...,  4016,    12,   627],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
Skip epoch 0, step 114, batch 114


Running epoch 0, step 115, batch 115
Sampled inputs[:2]: tensor([[    0,   287,   221,  ...,  1871,  1482,    12],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
Skip epoch 0, step 115, batch 115


Running epoch 0, step 116, batch 116
Sampled inputs[:2]: tensor([[   0, 1874,  300,  ...,   14, 5372,   12],
        [   0, 1912, 3461,  ...,  446, 9337, 1345]], device='cuda:0')
Skip epoch 0, step 116, batch 116


Running epoch 0, step 117, batch 117
Sampled inputs[:2]: tensor([[    0,  1737,   278,  ...,  2604,   367,  2002],
        [    0,   300, 12579,  ...,  1722,   369,  5049]], device='cuda:0')
Skip epoch 0, step 117, batch 117


Running epoch 0, step 118, batch 118
Sampled inputs[:2]: tensor([[    0, 11694,   292,  ...,   328,  1654,   818],
        [    0,  5689,   271,  ...,   352,  9985,  3260]], device='cuda:0')
Skip epoch 0, step 118, batch 118


Running epoch 0, step 119, batch 119
Sampled inputs[:2]: tensor([[    0,    12,   297,  ...,  2980,  1145, 17207],
        [    0, 20202,   300,  ..., 15185,   287,  6573]], device='cuda:0')
Skip epoch 0, step 119, batch 119


Running epoch 0, step 120, batch 120
Sampled inputs[:2]: tensor([[    0,   516,  1424,  ...,  3473,   278,  2442],
        [    0,   271, 16217,  ...,  6352,  4546,  2558]], device='cuda:0')
Skip epoch 0, step 120, batch 120


Running epoch 0, step 121, batch 121
Sampled inputs[:2]: tensor([[   0,   12,  722,  ...,  674,  369,  897],
        [   0, 9582, 3645,  ..., 1027,   12,  461]], device='cuda:0')
Skip epoch 0, step 121, batch 121


Running epoch 0, step 122, batch 122
Sampled inputs[:2]: tensor([[   0, 2732,  413,  ...,  287,  266, 3668],
        [   0, 5260,  365,  ..., 7242,  471,  391]], device='cuda:0')
Skip epoch 0, step 122, batch 122


Running epoch 0, step 123, batch 123
Sampled inputs[:2]: tensor([[    0,    13,  1320,  ...,  8686,  6851,    13],
        [    0, 39004,   266,  ...,   287, 21972,   278]], device='cuda:0')
Skip epoch 0, step 123, batch 123


Running epoch 0, step 124, batch 124
Sampled inputs[:2]: tensor([[    0,   266, 30368,  ...,   950,   266,  1868],
        [    0,  2790,   266,  ...,   401,  1496,    14]], device='cuda:0')
Skip epoch 0, step 124, batch 124


Running epoch 0, step 125, batch 125
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,  1549,  7052,  ...,  2529,  3958,    37]], device='cuda:0')
Skip epoch 0, step 125, batch 125


Running epoch 0, step 126, batch 126
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0, 1034, 5599,  ...,  259,  586, 1403]], device='cuda:0')
Skip epoch 0, step 126, batch 126


Running epoch 0, step 127, batch 127
Sampled inputs[:2]: tensor([[    0,   631,  4013,  ...,   368, 20301,   874],
        [    0,     9,   342,  ...,    12,   709,   857]], device='cuda:0')
Skip epoch 0, step 127, batch 127


Running epoch 0, step 128, batch 128
Sampled inputs[:2]: tensor([[   0, 7111,  409,  ..., 1908, 1260,  883],
        [   0, 1295,  508,  ...,  829,  772,  278]], device='cuda:0')
Skip epoch 0, step 128, batch 128


Running epoch 0, step 129, batch 129
Sampled inputs[:2]: tensor([[    0,  1342,    14,  ...,  1236, 15667, 12931],
        [    0,  1125,   278,  ...,  6447,   609,    14]], device='cuda:0')
Skip epoch 0, step 129, batch 129


Running epoch 0, step 130, batch 130
Sampled inputs[:2]: tensor([[    0,  4538,   271,  ...,  1603,   591,   688],
        [    0,  6481,   298,  ...,  6145, 16858,   824]], device='cuda:0')
Skip epoch 0, step 130, batch 130


Running epoch 0, step 131, batch 131
Sampled inputs[:2]: tensor([[    0,   650,    14,  ...,  3687,   278, 26952],
        [    0,  3658,   271,  ...,   278,   970,    12]], device='cuda:0')
Skip epoch 0, step 131, batch 131


Running epoch 0, step 132, batch 132
Sampled inputs[:2]: tensor([[    0,   870,   278,  ...,   478,   401,   897],
        [    0,  7294, 23782,  ...,   471, 11528,  3437]], device='cuda:0')
Skip epoch 0, step 132, batch 132


Running epoch 0, step 133, batch 133
Sampled inputs[:2]: tensor([[   0,  767, 1953,  ...,   14, 1364,   12],
        [   0, 3543,  391,  ..., 3370, 2926, 8090]], device='cuda:0')
Skip epoch 0, step 133, batch 133


Running epoch 0, step 134, batch 134
Sampled inputs[:2]: tensor([[   0,  600,   14,  ...,  221, 8187, 1802],
        [   0, 2220, 1110,  ...,  382,   18,   13]], device='cuda:0')
Skip epoch 0, step 134, batch 134


Running epoch 0, step 135, batch 135
Sampled inputs[:2]: tensor([[    0,    15,    72,  ...,   380, 22463,  2587],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
Skip epoch 0, step 135, batch 135


Running epoch 0, step 136, batch 136
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 8122, 1387,  616],
        [   0, 1415,  300,  ..., 1497, 5715, 4555]], device='cuda:0')
Skip epoch 0, step 136, batch 136


Running epoch 0, step 137, batch 137
Sampled inputs[:2]: tensor([[    0,    14,  3741,  ...,   278, 12472, 10257],
        [    0, 30229,    12,  ...,   518,   717,   271]], device='cuda:0')
Skip epoch 0, step 137, batch 137


Running epoch 0, step 138, batch 138
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0,  2422,   300,  ...,   630,   729,  3400]], device='cuda:0')
Skip epoch 0, step 138, batch 138


Running epoch 0, step 139, batch 139
Sampled inputs[:2]: tensor([[    0,    14,    28,  ..., 16032,   694,  1441],
        [    0, 13081,   278,  ...,   368,   266,  1717]], device='cuda:0')
Skip epoch 0, step 139, batch 139


Running epoch 0, step 140, batch 140
Sampled inputs[:2]: tensor([[   0, 2228, 1416,  ..., 3766,  266, 1136],
        [   0,  278, 1059,  ...,  300, 1877,   13]], device='cuda:0')
Skip epoch 0, step 140, batch 140


Running epoch 0, step 141, batch 141
Sampled inputs[:2]: tensor([[    0,  3047,  4878,  ...,   352, 10854, 34025],
        [    0, 31571,    13,  ...,   367,  2177,   271]], device='cuda:0')
Skip epoch 0, step 141, batch 141


Running epoch 0, step 142, batch 142
Sampled inputs[:2]: tensor([[    0,   496,    14,  ...,   266,   596,    13],
        [    0, 10705,   401,  ...,   768,  2392,   368]], device='cuda:0')
Skip epoch 0, step 142, batch 142


Running epoch 0, step 143, batch 143
Sampled inputs[:2]: tensor([[    0,   894,    16,  ...,   892,   300,   722],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
Skip epoch 0, step 143, batch 143


Running epoch 0, step 144, batch 144
Sampled inputs[:2]: tensor([[   0, 3087,  342,  ...,   14,  381, 1416],
        [   0,  462,  221,  ...,   29,  413, 1801]], device='cuda:0')
Skip epoch 0, step 144, batch 144


Running epoch 0, step 145, batch 145
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,   527, 37357,    12],
        [    0,    21,    66,  ...,  1377,   278,  1634]], device='cuda:0')
Skip epoch 0, step 145, batch 145


Running epoch 0, step 146, batch 146
Sampled inputs[:2]: tensor([[    0,   266, 15957,  ...,  1556, 45044,   271],
        [    0,   328,  5180,  ...,   344,  2356,   409]], device='cuda:0')
Skip epoch 0, step 146, batch 146


Running epoch 0, step 147, batch 147
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,  3615,    16,  ...,  2140,  1098,   352]], device='cuda:0')
Skip epoch 0, step 147, batch 147


Running epoch 0, step 148, batch 148
Sampled inputs[:2]: tensor([[   0,  272,  352,  ...,  590, 4361,  446],
        [   0, 1943, 1837,  ...,  870,  287,  266]], device='cuda:0')
Skip epoch 0, step 148, batch 148


Running epoch 0, step 149, batch 149
Sampled inputs[:2]: tensor([[    0, 16187,   565,  ...,   586,  3196,   271],
        [    0,   266,  2967,  ...,   287,  4432,    13]], device='cuda:0')
Skip epoch 0, step 149, batch 149


Running epoch 0, step 150, batch 150
Sampled inputs[:2]: tensor([[    0,  5635,   328,  ...,   287, 27260,   271],
        [    0,   475,  2985,  ...,   292,  5273,     9]], device='cuda:0')
Skip epoch 0, step 150, batch 150


Running epoch 0, step 151, batch 151
Sampled inputs[:2]: tensor([[    0,  1477,    12,  ..., 31038,   408,   298],
        [    0,  2698,   221,  ...,  8352,  5680,   782]], device='cuda:0')
Skip epoch 0, step 151, batch 151


Running epoch 0, step 152, batch 152
Sampled inputs[:2]: tensor([[   0, 5841,  328,  ..., 2051,  266,  756],
        [   0, 2555,  984,  ..., 5900, 1576,  271]], device='cuda:0')
Skip epoch 0, step 152, batch 152


Running epoch 0, step 153, batch 153
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 2165, 1323,  ...,  199,  677, 8376]], device='cuda:0')
Skip epoch 0, step 153, batch 153


Running epoch 0, step 154, batch 154
Sampled inputs[:2]: tensor([[    0,  1624,   391,  ...,   391, 36249,   259],
        [    0,   594,    84,  ..., 24411, 14140, 12720]], device='cuda:0')
Skip epoch 0, step 154, batch 154


Running epoch 0, step 155, batch 155
Sampled inputs[:2]: tensor([[    0,    13,  1529,  ...,  8197,  2700,  9629],
        [    0,  1136,   944,  ...,   401, 13771,    12]], device='cuda:0')
Skip epoch 0, step 155, batch 155


Running epoch 0, step 156, batch 156
Sampled inputs[:2]: tensor([[   0,   34, 3881,  ..., 1027,  271,  266],
        [   0,  292,  380,  ..., 1725,  271,  266]], device='cuda:0')
Skip epoch 0, step 156, batch 156


Running epoch 0, step 157, batch 157
Sampled inputs[:2]: tensor([[   0,  266, 1211,  ..., 1336,  694,  516],
        [   0,   12,  287,  ..., 4626,   27,  292]], device='cuda:0')
Skip epoch 0, step 157, batch 157


Running epoch 0, step 158, batch 158
Sampled inputs[:2]: tensor([[    0,  1360,    14,  ..., 31575, 28569,   292],
        [    0,  3119,   278,  ...,   352,   674,   369]], device='cuda:0')
Skip epoch 0, step 158, batch 158


Running epoch 0, step 159, batch 159
Sampled inputs[:2]: tensor([[   0,  729, 3084,  ...,  381, 1445,  642],
        [   0, 3767, 2337,  ...,  950,  847,  300]], device='cuda:0')
Skip epoch 0, step 159, batch 159


Running epoch 0, step 160, batch 160
Sampled inputs[:2]: tensor([[   0,  298,  369,  ..., 5936,  968,  259],
        [   0,  756,  401,  ...,  271, 7272, 1663]], device='cuda:0')
Skip epoch 0, step 160, batch 160


Running epoch 0, step 161, batch 161
Sampled inputs[:2]: tensor([[    0, 38232,   446,  ...,   287,  2456, 29919],
        [    0,  5775,   292,  ...,  8671,  1339,   642]], device='cuda:0')
Skip epoch 0, step 161, batch 161


Running epoch 0, step 162, batch 162
Sampled inputs[:2]: tensor([[   0, 1184, 1451,  ...,  934,  352,  266],
        [   0,   12,  616,  ...,  278,  266, 2907]], device='cuda:0')
Skip epoch 0, step 162, batch 162


Running epoch 0, step 163, batch 163
Sampled inputs[:2]: tensor([[    0, 27366,   504,  ...,  1358,   365,  6883],
        [    0,  2652,   271,  ...,   634,  1921,   266]], device='cuda:0')
Skip epoch 0, step 163, batch 163


Running epoch 0, step 164, batch 164
Sampled inputs[:2]: tensor([[   0,  857,  344,  ..., 1529, 9106, 1447],
        [   0, 1552,  271,  ...,   13,  287,  995]], device='cuda:0')
Skip epoch 0, step 164, batch 164


Running epoch 0, step 165, batch 165
Sampled inputs[:2]: tensor([[    0,  1690, 16858,  ...,   199,   395,  3902],
        [    0,  7926,  6750,  ...,   259,  1524,  6257]], device='cuda:0')
Skip epoch 0, step 165, batch 165


Running epoch 0, step 166, batch 166
Sampled inputs[:2]: tensor([[   0,  401, 9370,  ...,    9,  287,  518],
        [   0,   14,  747,  ...,  259, 6027, 1889]], device='cuda:0')
Skip epoch 0, step 166, batch 166


Running epoch 0, step 167, batch 167
Sampled inputs[:2]: tensor([[   0,  352,  644,  ..., 2928,  590, 3040],
        [   0,  292,  380,  ..., 6156,  278,  266]], device='cuda:0')
Skip epoch 0, step 167, batch 167


Running epoch 0, step 168, batch 168
Sampled inputs[:2]: tensor([[    0,    12,  4957,  ...,   944,   278,   609],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
Skip epoch 0, step 168, batch 168


Running epoch 0, step 169, batch 169
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0, 23530,  6713,  ...,  2813,   518,   266]], device='cuda:0')
Skip epoch 0, step 169, batch 169


Running epoch 0, step 170, batch 170
Sampled inputs[:2]: tensor([[   0, 3408,  300,  ...,   14, 5870,   12],
        [   0, 6809,  344,  ...,   14, 1266,  795]], device='cuda:0')
Skip epoch 0, step 170, batch 170


Running epoch 0, step 171, batch 171
Sampled inputs[:2]: tensor([[    0,   927,   259,  ...,   328,  9430,  2330],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
Skip epoch 0, step 171, batch 171


Running epoch 0, step 172, batch 172
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 1853,   12,  709],
        [   0, 1552,  300,  ..., 1085,   12,  298]], device='cuda:0')
Skip epoch 0, step 172, batch 172


Running epoch 0, step 173, batch 173
Sampled inputs[:2]: tensor([[   0, 2973,   30,  ...,  408,  259, 1914],
        [   0,  642,  271,  ..., 5430, 2314, 6431]], device='cuda:0')
Skip epoch 0, step 173, batch 173


Running epoch 0, step 174, batch 174
Sampled inputs[:2]: tensor([[    0,   278, 30377,  ...,    13,    83,  2908],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
Skip epoch 0, step 174, batch 174


Running epoch 0, step 175, batch 175
Sampled inputs[:2]: tensor([[    0, 33315,   266,  ...,    12,  1126,    14],
        [    0,   772,   699,  ...,  1849,   287,  7134]], device='cuda:0')
Skip epoch 0, step 175, batch 175


Running epoch 0, step 176, batch 176
Sampled inputs[:2]: tensor([[    0,   759,  1184,  ...,   472,   346,    14],
        [    0,  8450,   292,  ...,   352,   722, 37719]], device='cuda:0')
Skip epoch 0, step 176, batch 176


Running epoch 0, step 177, batch 177
Sampled inputs[:2]: tensor([[   0, 2626,   13,  ...,  300,  369,  259],
        [   0,   12,  358,  ...,  352,  266,  319]], device='cuda:0')
Skip epoch 0, step 177, batch 177


Running epoch 0, step 178, batch 178
Sampled inputs[:2]: tensor([[    0,  2663,   328,  ...,   266,  1040,  1679],
        [    0,   352,   266,  ...,   490, 10112,  3804]], device='cuda:0')
Skip epoch 0, step 178, batch 178


Running epoch 0, step 179, batch 179
Sampled inputs[:2]: tensor([[   0, 2546,  300,  ...,   14, 1075,  756],
        [   0,  259, 3022,  ...,  437, 5100, 1782]], device='cuda:0')
Skip epoch 0, step 179, batch 179


Running epoch 0, step 180, batch 180
Sampled inputs[:2]: tensor([[    0,   413,    28,  ...,   328, 37605,  6499],
        [    0, 26138,    17,  ...,   401,  1867,  4977]], device='cuda:0')
Skip epoch 0, step 180, batch 180


Running epoch 0, step 181, batch 181
Sampled inputs[:2]: tensor([[    0,   266,   858,  ..., 11265,   607,  7455],
        [    0,   380,   341,  ...,   955,   644,   271]], device='cuda:0')
Skip epoch 0, step 181, batch 181


Running epoch 0, step 182, batch 182
Sampled inputs[:2]: tensor([[   0, 1477, 3205,  ..., 6441, 9363,  271],
        [   0,   11,  360,  ..., 4524, 1553,  401]], device='cuda:0')
Skip epoch 0, step 182, batch 182


Running epoch 0, step 183, batch 183
Sampled inputs[:2]: tensor([[    0, 12182,  6294,  ...,  1042,  1070,  2228],
        [    0,   328,  6379,  ...,   287,  1342,     9]], device='cuda:0')
Skip epoch 0, step 183, batch 183


Running epoch 0, step 184, batch 184
Sampled inputs[:2]: tensor([[    0, 18901,     5,  ...,  2253,   278, 17423],
        [    0,  2388,  6604,  ...,  5005,  1196,   717]], device='cuda:0')
Skip epoch 0, step 184, batch 184


Running epoch 0, step 185, batch 185
Sampled inputs[:2]: tensor([[    0, 11030,    72,  ...,   259, 16979,  9415],
        [    0,   259,  2416,  ..., 14474,    12,   259]], device='cuda:0')
Skip epoch 0, step 185, batch 185


Running epoch 0, step 186, batch 186
Sampled inputs[:2]: tensor([[    0,  3058,   292,  ...,  1387,  1236,   369],
        [    0, 15689,   278,  ..., 12016,   271,  4353]], device='cuda:0')
Skip epoch 0, step 186, batch 186


Running epoch 0, step 187, batch 187
Sampled inputs[:2]: tensor([[   0,   12, 1471,  ..., 1356,  600,   12],
        [   0,  677, 9606,  ..., 9468, 9268,  328]], device='cuda:0')
Skip epoch 0, step 187, batch 187


Running epoch 0, step 188, batch 188
Sampled inputs[:2]: tensor([[   0, 4998, 1921,  ...,  968,  266, 1136],
        [   0,  266, 4411,  ...,  368, 6388, 3484]], device='cuda:0')
Skip epoch 0, step 188, batch 188


Running epoch 0, step 189, batch 189
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   394,   360,  5911],
        [    0,  3386, 43625,  ...,    19,  2125,   271]], device='cuda:0')
Skip epoch 0, step 189, batch 189


Running epoch 0, step 190, batch 190
Sampled inputs[:2]: tensor([[    0,    83,   292,  ...,   445,    11, 16109],
        [    0,    13,   786,  ...,   275,  2623,    13]], device='cuda:0')
Skip epoch 0, step 190, batch 190


Running epoch 0, step 191, batch 191
Sampled inputs[:2]: tensor([[    0,   677, 35427,  ..., 30465,  2783,     9],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
Skip epoch 0, step 191, batch 191


Running epoch 0, step 192, batch 192
Sampled inputs[:2]: tensor([[    0,   287, 17044,  ...,   496,    14,  1841],
        [    0,  1603,   694,  ...,    36,    18,   298]], device='cuda:0')
Skip epoch 0, step 192, batch 192


Running epoch 0, step 193, batch 193
Sampled inputs[:2]: tensor([[   0, 5522, 5662,  ...,  638, 1231, 1098],
        [   0, 6067, 1188,  ..., 5282,  756,  342]], device='cuda:0')
Skip epoch 0, step 193, batch 193


Running epoch 0, step 194, batch 194
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,  1070,  5746,  ...,   278,   689,    14]], device='cuda:0')
Skip epoch 0, step 194, batch 194


Running epoch 0, step 195, batch 195
Sampled inputs[:2]: tensor([[    0,    21,    14,  ...,  1159,  1978, 33323],
        [    0,  2667,   365,  ...,  9281,  1631,  9123]], device='cuda:0')
Skip epoch 0, step 195, batch 195


Running epoch 0, step 196, batch 196
Sampled inputs[:2]: tensor([[   0,  374, 5195,  ...,  266, 5555,   14],
        [   0,   12, 3518,  ..., 1580, 2573,  409]], device='cuda:0')
Skip epoch 0, step 196, batch 196


Running epoch 0, step 197, batch 197
Sampled inputs[:2]: tensor([[    0, 11661,    12,  ...,  1707,   394,   264],
        [    0,   271,   768,  ..., 15555,   278,   266]], device='cuda:0')
Skip epoch 0, step 197, batch 197


Running epoch 0, step 198, batch 198
Sampled inputs[:2]: tensor([[   0, 1188,  278,  ...,  271, 8368,  292],
        [   0,  266, 1194,  ..., 2267,   15, 1224]], device='cuda:0')
Skip epoch 0, step 198, batch 198


Running epoch 0, step 199, batch 199
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  342,  266, 1163],
        [   0, 3036,  471,  ...,  287, 1906,   12]], device='cuda:0')
Skip epoch 0, step 199, batch 199


Running epoch 0, step 200, batch 200
Sampled inputs[:2]: tensor([[   0, 4371, 4806,  ...,  685,  461,  654],
        [   0,  515,  266,  ...,   18, 3770, 1345]], device='cuda:0')
Skip epoch 0, step 200, batch 200


Running epoch 0, step 201, batch 201
Sampled inputs[:2]: tensor([[    0,   221,  4070,  ...,  1061,  3189,    26],
        [    0, 18125, 16419,  ...,   278,   638, 11744]], device='cuda:0')
Skip epoch 0, step 201, batch 201


Running epoch 0, step 202, batch 202
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0,  5160,   278,  ...,   496,    14, 46919]], device='cuda:0')
Skip epoch 0, step 202, batch 202


Running epoch 0, step 203, batch 203
Sampled inputs[:2]: tensor([[   0,   14,   69,  ...,  287,  259, 5158],
        [   0, 1607,   12,  ...,  895, 1503,  369]], device='cuda:0')
Skip epoch 0, step 203, batch 203


Running epoch 0, step 204, batch 204
Sampled inputs[:2]: tensor([[    0,   560, 23501,  ...,   292,   494,   221],
        [    0,   850,    13,  ..., 11823,    13, 30706]], device='cuda:0')
Skip epoch 0, step 204, batch 204


Running epoch 0, step 205, batch 205
Sampled inputs[:2]: tensor([[    0,  4852,   266,  ...,  2523,  2080,  2632],
        [    0, 12305,  1179,  ...,  6321,   600,   271]], device='cuda:0')
Skip epoch 0, step 205, batch 205


Running epoch 0, step 206, batch 206
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,   266,  1176,  ...,   199, 17791,  3662]], device='cuda:0')
Skip epoch 0, step 206, batch 206


Running epoch 0, step 207, batch 207
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0, 17471,  4778,  ...,  2177,   271,   266]], device='cuda:0')
Skip epoch 0, step 207, batch 207


Running epoch 0, step 208, batch 208
Sampled inputs[:2]: tensor([[    0,    12,  2418,  ...,   446,   381,  2204],
        [    0,   287, 16974,  ...,   300,  2283,  4013]], device='cuda:0')
Skip epoch 0, step 208, batch 208


Running epoch 0, step 209, batch 209
Sampled inputs[:2]: tensor([[    0,  2379,    13,  ...,   287,   259,  2193],
        [    0,  2853, 21042,  ...,  4120,   607, 11176]], device='cuda:0')
Skip epoch 0, step 209, batch 209


Running epoch 0, step 210, batch 210
Sampled inputs[:2]: tensor([[    0, 38816,   292,  ...,   346,   462,   221],
        [    0,   344, 10706,  ...,  1184,   578,   825]], device='cuda:0')
Skip epoch 0, step 210, batch 210


Running epoch 0, step 211, batch 211
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  380,  560,  199],
        [   0,  278, 7914,  ..., 1194,  300, 4419]], device='cuda:0')
Skip epoch 0, step 211, batch 211


Running epoch 0, step 212, batch 212
Sampled inputs[:2]: tensor([[    0, 49570,   644,  ...,   461,   800,   266],
        [    0,   765,   292,  ...,   623,    12,  7117]], device='cuda:0')
Skip epoch 0, step 212, batch 212


Running epoch 0, step 213, batch 213
Sampled inputs[:2]: tensor([[   0, 7066, 2737,  ..., 2269,  271,  927],
        [   0, 1760,    9,  ...,  278, 6607,   13]], device='cuda:0')
Skip epoch 0, step 213, batch 213


Running epoch 0, step 214, batch 214
Sampled inputs[:2]: tensor([[    0,   389, 18984,  ...,   287,   768,  1070],
        [    0, 12456,    14,  ...,  1822,  1016,   365]], device='cuda:0')
Skip epoch 0, step 214, batch 214


Running epoch 0, step 215, batch 215
Sampled inputs[:2]: tensor([[    0,   496,    14,  ...,  1034,  4679,   278],
        [    0, 28684,   472,  ...,   317,     9,  1926]], device='cuda:0')
Skip epoch 0, step 215, batch 215


Running epoch 0, step 216, batch 216
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   274, 30228,   287],
        [    0,   287,  7763,  ...,   689,  2409,   699]], device='cuda:0')
Skip epoch 0, step 216, batch 216


Running epoch 0, step 217, batch 217
Sampled inputs[:2]: tensor([[    0,  4350,    14,  ...,   266,  9479,   944],
        [    0,    52, 26766,  ...,  4411,  4226,   278]], device='cuda:0')
Skip epoch 0, step 217, batch 217


Running epoch 0, step 218, batch 218
Sampled inputs[:2]: tensor([[    0,    12,  1197,  ...,   352,  2513,   266],
        [    0, 10334,    17,  ...,   391,  1566, 24837]], device='cuda:0')
Skip epoch 0, step 218, batch 218


Running epoch 0, step 219, batch 219
Sampled inputs[:2]: tensor([[    0,   342,  8514,  ...,   266, 46850,  2545],
        [    0,   271,   266,  ..., 23648,   292, 21424]], device='cuda:0')
Skip epoch 0, step 219, batch 219


Running epoch 0, step 220, batch 220
Sampled inputs[:2]: tensor([[   0, 7555, 3908,  ...,  259, 8477,  278],
        [   0,  694, 2326,  ...,  278, 1781, 9660]], device='cuda:0')
Skip epoch 0, step 220, batch 220


Running epoch 0, step 221, batch 221
Sampled inputs[:2]: tensor([[    0, 23070,   367,  ...,   287,   790,  3252],
        [    0,   278,  1478,  ...,   266,  1607,  1220]], device='cuda:0')
Skip epoch 0, step 221, batch 221


Running epoch 0, step 222, batch 222
Sampled inputs[:2]: tensor([[    0,  1416,   367,  ...,   555,   764,   367],
        [    0,   843,  2621,  ...,  4589,   278, 14266]], device='cuda:0')
Skip epoch 0, step 222, batch 222


Running epoch 0, step 223, batch 223
Sampled inputs[:2]: tensor([[    0,    14,  2729,  ...,   266,  1659, 14362],
        [    0,    14,   357,  ...,    30,   287,   839]], device='cuda:0')
Skip epoch 0, step 223, batch 223


Running epoch 0, step 224, batch 224
Sampled inputs[:2]: tensor([[   0, 4258,  717,  ...,   34,  609, 1169],
        [   0, 7094,  596,  ..., 4764, 9514,   14]], device='cuda:0')
Skip epoch 0, step 224, batch 224


Running epoch 0, step 225, batch 225
Sampled inputs[:2]: tensor([[    0,    12,   496,  ..., 11354,  4856,  1109],
        [    0,   221,   264,  ...,  3613,  3222, 14000]], device='cuda:0')
Skip epoch 0, step 225, batch 225


Running epoch 0, step 226, batch 226
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0, 25778,  3804,  ...,  2354,    12,   554]], device='cuda:0')
Skip epoch 0, step 226, batch 226


Running epoch 0, step 227, batch 227
Sampled inputs[:2]: tensor([[    0,    27,  5375,  ...,  5357, 14933, 10944],
        [    0,    12,   287,  ...,    17,   271,   266]], device='cuda:0')
Skip epoch 0, step 227, batch 227


Running epoch 0, step 228, batch 228
Sampled inputs[:2]: tensor([[   0, 4882,   12,  ...,   12, 9575,  287],
        [   0, 4110,  271,  ...,  944,  278, 3230]], device='cuda:0')
Skip epoch 0, step 228, batch 228


Running epoch 0, step 229, batch 229
Sampled inputs[:2]: tensor([[    0, 40624,   266,  ..., 12236,   292,    41],
        [    0,  4823,    12,  ...,  1756,  3406,   300]], device='cuda:0')
Skip epoch 0, step 229, batch 229


Running epoch 0, step 230, batch 230
Sampled inputs[:2]: tensor([[   0,  475,  266,  ...,  843,  287, 1119],
        [   0,  221,  825,  ...,  616, 3661, 8052]], device='cuda:0')
Skip epoch 0, step 230, batch 230


Running epoch 0, step 231, batch 231
Sampled inputs[:2]: tensor([[    0,   292,   474,  ...,   446, 14932,   365],
        [    0,  4672,   278,  ...,  7523,  2305,    13]], device='cuda:0')
Skip epoch 0, step 231, batch 231


Running epoch 0, step 232, batch 232
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   278,  4697,    14],
        [    0, 12440,   578,  ..., 25918,   287,   996]], device='cuda:0')
Skip epoch 0, step 232, batch 232


Running epoch 0, step 233, batch 233
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0,   287,  9430,  ...,  3121,   352,   360]], device='cuda:0')
Skip epoch 0, step 233, batch 233


Running epoch 0, step 234, batch 234
Sampled inputs[:2]: tensor([[    0,   278,  2097,  ...,  1754,   287,   631],
        [    0,   607, 27288,  ...,   445,  4712,   278]], device='cuda:0')
Skip epoch 0, step 234, batch 234


Running epoch 0, step 235, batch 235
Sampled inputs[:2]: tensor([[    0, 49831,    12,  ...,   912,   221,   609],
        [    0,  1270,   413,  ...,   413,   711,    14]], device='cuda:0')
Skip epoch 0, step 235, batch 235


Running epoch 0, step 236, batch 236
Sampled inputs[:2]: tensor([[    0,    20,  2637,  ..., 14044,     9,   292],
        [    0,  1619,   938,  ...,   292, 10026, 14367]], device='cuda:0')
Skip epoch 0, step 236, batch 236


Running epoch 0, step 237, batch 237
Sampled inputs[:2]: tensor([[    0,    12, 12774,  ...,  1231,   278,   266],
        [    0,    13,  1311,  ...,   271,   795,   957]], device='cuda:0')
Skip epoch 0, step 237, batch 237


Running epoch 0, step 238, batch 238
Sampled inputs[:2]: tensor([[   0,   17,   12,  ...,   12,  461,  806],
        [   0, 1103,  271,  ...,  957,  756,  368]], device='cuda:0')
Skip epoch 0, step 238, batch 238


Running epoch 0, step 239, batch 239
Sampled inputs[:2]: tensor([[   0,  706, 1005,  ...,  278,  266, 5590],
        [   0,  531, 9804,  ..., 1027,  360, 1576]], device='cuda:0')
Skip epoch 0, step 239, batch 239


Running epoch 0, step 240, batch 240
Sampled inputs[:2]: tensor([[   0, 1085, 4878,  ...,  298,  894,  496],
        [   0, 3605, 2572,  ...,  300,  259, 1513]], device='cuda:0')
Skip epoch 0, step 240, batch 240


Running epoch 0, step 241, batch 241
Sampled inputs[:2]: tensor([[   0, 1503,  369,  ..., 1336,  271, 8429],
        [   0,   19,    9,  ..., 4971,  367, 1675]], device='cuda:0')
Skip epoch 0, step 241, batch 241


Running epoch 0, step 242, batch 242
Sampled inputs[:2]: tensor([[    0,  3125,   271,  ...,  1041,  1032,    15],
        [    0, 42329,   472,  ...,   292,    33,  3092]], device='cuda:0')
Skip epoch 0, step 242, batch 242


Running epoch 0, step 243, batch 243
Sampled inputs[:2]: tensor([[   0,  795, 1445,  ..., 6292,  287, 9782],
        [   0, 1481,  278,  ..., 3940, 4938,    5]], device='cuda:0')
Skip epoch 0, step 243, batch 243


Running epoch 0, step 244, batch 244
Sampled inputs[:2]: tensor([[   0, 2974,  278,  ...,  365, 8758,  271],
        [   0, 2280,  344,  ...,  287,  266, 3344]], device='cuda:0')
Skip epoch 0, step 244, batch 244


Running epoch 0, step 245, batch 245
Sampled inputs[:2]: tensor([[   0,  894,  496,  ...,  266,  623,  587],
        [   0,   14,  333,  ...,  328, 5453, 4713]], device='cuda:0')
Skip epoch 0, step 245, batch 245


Running epoch 0, step 246, batch 246
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,   892,   271,  ...,   278,   266, 10237]], device='cuda:0')
Skip epoch 0, step 246, batch 246


Running epoch 0, step 247, batch 247
Sampled inputs[:2]: tensor([[    0,   271,  3403,  ...,  6168,   300,  2257],
        [    0,     9,   300,  ...,  6838,   328, 18619]], device='cuda:0')
Skip epoch 0, step 247, batch 247


Running epoch 0, step 248, batch 248
Sampled inputs[:2]: tensor([[    0,    14,  8058,  ..., 10316,   352,   266],
        [    0,   266,  1144,  ..., 21458,    12, 15890]], device='cuda:0')
Skip epoch 0, step 248, batch 248


Running epoch 0, step 249, batch 249
Sampled inputs[:2]: tensor([[   0,   14, 4494,  ..., 4830,  368,  266],
        [   0, 3352,  259,  ..., 3565,   12,  409]], device='cuda:0')
Skip epoch 0, step 249, batch 249


Running epoch 0, step 250, batch 250
Sampled inputs[:2]: tensor([[   0, 1590, 2140,  ...,  287, 5342, 1319],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
Skip epoch 0, step 250, batch 250


Running epoch 0, step 251, batch 251
Sampled inputs[:2]: tensor([[    0,   292, 41192,  ..., 34298,  8741,   271],
        [    0,   616,  4935,  ...,    89,  4448,   271]], device='cuda:0')
Skip epoch 0, step 251, batch 251


Running epoch 0, step 252, batch 252
Sampled inputs[:2]: tensor([[    0,  5281,  4452,  ...,    14,  3391,    12],
        [    0,  1943,   300,  ..., 43803,   368,  2400]], device='cuda:0')
Skip epoch 0, step 252, batch 252


Running epoch 0, step 253, batch 253
Sampled inputs[:2]: tensor([[   0, 2816,  292,  ..., 3662,  461, 2723],
        [   0,  285,  590,  ...,  199,  395, 3523]], device='cuda:0')
Skip epoch 0, step 253, batch 253


Running epoch 0, step 254, batch 254
Sampled inputs[:2]: tensor([[   0, 9430,  287,  ..., 1141, 2280,  408],
        [   0,  992,  409,  ..., 5843,  344,  259]], device='cuda:0')
Skip epoch 0, step 254, batch 254


Running epoch 0, step 255, batch 255
Sampled inputs[:2]: tensor([[   0, 4845, 1521,  ...,  963,  292, 6414],
        [   0,    9, 1471,  ...,  741,  266, 5821]], device='cuda:0')
Skip epoch 0, step 255, batch 255


Running epoch 0, step 256, batch 256
Sampled inputs[:2]: tensor([[    0,  1211, 11131,  ..., 31480,   565,   446],
        [    0,  1067,   271,  ...,   266,   940,   271]], device='cuda:0')
Skip epoch 0, step 256, batch 256


Running epoch 0, step 257, batch 257
Sampled inputs[:2]: tensor([[   0,   16,   52,  ...,   12,  298,  374],
        [   0,   12,  287,  ...,  298, 9855,  278]], device='cuda:0')
Skip epoch 0, step 257, batch 257


Running epoch 0, step 258, batch 258
Sampled inputs[:2]: tensor([[   0,   47,   12,  ..., 4367,  278,  471],
        [   0, 2579,  278,  ...,   56,    9,  271]], device='cuda:0')
Skip epoch 0, step 258, batch 258


Running epoch 0, step 259, batch 259
Sampled inputs[:2]: tensor([[   0,  368,  275,  ..., 6389, 9102,   12],
        [   0,  591,  688,  ...,  271, 3390,   12]], device='cuda:0')
Skip epoch 0, step 259, batch 259


Running epoch 0, step 260, batch 260
Sampled inputs[:2]: tensor([[    0,     9, 25368,  ...,   271,   266,  1136],
        [    0,  1340,   800,  ...,   259, 13583,   422]], device='cuda:0')
Skip epoch 0, step 260, batch 260


Running epoch 0, step 261, batch 261
Sampled inputs[:2]: tensor([[    0,   720,  1122,  ...,   656,   287, 14258],
        [    0, 39224,    34,  ...,   401,  1716,   271]], device='cuda:0')
Skip epoch 0, step 261, batch 261


Running epoch 0, step 262, batch 262
Sampled inputs[:2]: tensor([[    0,   792,    83,  ..., 29085, 15914,   365],
        [    0,  2258, 10315,  ...,  4185,  9433,   221]], device='cuda:0')
Skip epoch 0, step 262, batch 262


Running epoch 0, step 263, batch 263
Sampled inputs[:2]: tensor([[   0, 1485,  271,  ..., 6359, 1799, 5442],
        [   0, 2355, 2728,  ...,  554, 9025,  368]], device='cuda:0')
Skip epoch 0, step 263, batch 263


Running epoch 0, step 264, batch 264
Sampled inputs[:2]: tensor([[    0, 14165,    14,  ..., 34395, 31103,  6905],
        [    0,   461,  1169,  ..., 14135,  2771,    13]], device='cuda:0')
Skip epoch 0, step 264, batch 264


Running epoch 0, step 265, batch 265
Sampled inputs[:2]: tensor([[    0,   287,   271,  ...,  5090,   631,  3276],
        [    0,  4988, 36842,  ...,  7630, 18362,    13]], device='cuda:0')
Skip epoch 0, step 265, batch 265


Running epoch 0, step 266, batch 266
Sampled inputs[:2]: tensor([[    0,   376,   283,  ..., 29188,   292,  7627],
        [    0,   259,  1329,  ...,   266,   706,  1663]], device='cuda:0')
Skip epoch 0, step 266, batch 266


Running epoch 0, step 267, batch 267
Sampled inputs[:2]: tensor([[   0, 4929, 4214,  ..., 1172,  591, 4422],
        [   0, 1106,  259,  ...,  271,  679,  382]], device='cuda:0')
Skip epoch 0, step 267, batch 267


Running epoch 0, step 268, batch 268
Sampled inputs[:2]: tensor([[    0,   677, 20206,  ...,   292,   334,  1550],
        [    0,    12,   546,  ..., 24994, 31107,   266]], device='cuda:0')
Skip epoch 0, step 268, batch 268


Running epoch 0, step 269, batch 269
Sampled inputs[:2]: tensor([[    0, 10206,   342,  ...,  1336,  5046,   360],
        [    0,   790, 43134,  ...,   446,   381,  1034]], device='cuda:0')
Skip epoch 0, step 269, batch 269


Running epoch 0, step 270, batch 270
Sampled inputs[:2]: tensor([[   0,  266, 2057,  ...,   88, 1801,   66],
        [   0, 3699, 3058,  ...,  820, 5327, 8055]], device='cuda:0')
Skip epoch 0, step 270, batch 270


Running epoch 0, step 271, batch 271
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  6895,  5842,  2239],
        [    0,   634, 10095,  ...,   367, 24607,   287]], device='cuda:0')
Skip epoch 0, step 271, batch 271


Running epoch 0, step 272, batch 272
Sampled inputs[:2]: tensor([[    0,   923,  2583,  ..., 11385,    14,  1062],
        [    0,   298,  8761,  ...,   271,   266,   298]], device='cuda:0')
Skip epoch 0, step 272, batch 272


Running epoch 0, step 273, batch 273
Sampled inputs[:2]: tensor([[    0,   494,   298,  ...,   408, 32859, 14550],
        [    0, 38495, 36253,  ..., 11006,  5699,    19]], device='cuda:0')
Skip epoch 0, step 273, batch 273


Running epoch 0, step 274, batch 274
Sampled inputs[:2]: tensor([[    0,  3951,    77,  ...,  7062,   278,   600],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
Skip epoch 0, step 274, batch 274


Running epoch 0, step 275, batch 275
Sampled inputs[:2]: tensor([[    0,  6693,  1235,  ..., 10814,  1810,   367],
        [    0,  2615,    13,  ...,   940,  3661,  6837]], device='cuda:0')
Skip epoch 0, step 275, batch 275


Running epoch 0, step 276, batch 276
Sampled inputs[:2]: tensor([[   0,  278,  266,  ...,  292,  474,  221],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
Skip epoch 0, step 276, batch 276


Running epoch 0, step 277, batch 277
Sampled inputs[:2]: tensor([[    0,  1842,   360,  ..., 10251,    14,  1062],
        [    0,  1580,   271,  ...,   656,   943,  1883]], device='cuda:0')
Skip epoch 0, step 277, batch 277


Running epoch 0, step 278, batch 278
Sampled inputs[:2]: tensor([[    0,  1029,  6068,  ..., 18017,   300,   259],
        [    0,   266,  1553,  ...,  8954,    21,   409]], device='cuda:0')
Skip epoch 0, step 278, batch 278


Running epoch 0, step 279, batch 279
Sampled inputs[:2]: tensor([[   0, 4175,  437,  ..., 1700,   14,  381],
        [   0, 3227,  300,  ..., 1817, 5709,  300]], device='cuda:0')
Skip epoch 0, step 279, batch 279


Running epoch 0, step 280, batch 280
Sampled inputs[:2]: tensor([[    0,   527,  2811,  ...,   287,  1288,   352],
        [    0, 21748,   792,  ...,   408,   266, 31879]], device='cuda:0')
Skip epoch 0, step 280, batch 280


Running epoch 0, step 281, batch 281
Sampled inputs[:2]: tensor([[   0,  300, 7239,  ..., 2283, 4890,   14],
        [   0,  292,   46,  ..., 1217,   17,  292]], device='cuda:0')
Skip epoch 0, step 281, batch 281


Running epoch 0, step 282, batch 282
Sampled inputs[:2]: tensor([[    0,  4645,  7688,  ..., 26535,   471,   287],
        [    0,  2548,   720,  ...,  1795,  1109, 32948]], device='cuda:0')
Skip epoch 0, step 282, batch 282


Running epoch 0, step 283, batch 283
Sampled inputs[:2]: tensor([[   0, 1098,  259,  ..., 6572, 1477,   12],
        [   0,  825, 3066,  ..., 1184,  266, 7964]], device='cuda:0')
Skip epoch 0, step 283, batch 283


Running epoch 0, step 284, batch 284
Sampled inputs[:2]: tensor([[   0, 2771,   13,  ..., 1412,   35,   15],
        [   0,   89, 2023,  ...,  271,   13,  704]], device='cuda:0')
Skip epoch 0, step 284, batch 284


Running epoch 0, step 285, batch 285
Sampled inputs[:2]: tensor([[    0, 25241,   717,  ...,   413,    16,    14],
        [    0,  1688,   790,  ...,   546,   696,    12]], device='cuda:0')
Skip epoch 0, step 285, batch 285


Running epoch 0, step 286, batch 286
Sampled inputs[:2]: tensor([[    0,   634,   631,  ...,  3431,   287, 27947],
        [    0,  2440,  1458,  ...,  7650,   328,  2297]], device='cuda:0')
Skip epoch 0, step 286, batch 286


Running epoch 0, step 287, batch 287
Sampled inputs[:2]: tensor([[    0,   391,  1761,  ...,   346,    14,   292],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
Skip epoch 0, step 287, batch 287


Running epoch 0, step 288, batch 288
Sampled inputs[:2]: tensor([[    0,  2530,   634,  ...,    15,  8808,     9],
        [    0,  1894,   317,  ...,  9920,    13, 19888]], device='cuda:0')
Skip epoch 0, step 288, batch 288


Running epoch 0, step 289, batch 289
Sampled inputs[:2]: tensor([[    0,   287, 49722,  ...,  7551,   278,  5711],
        [    0,   266,  1790,  ...,   292,    78,   527]], device='cuda:0')
Skip epoch 0, step 289, batch 289


Running epoch 0, step 290, batch 290
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0,   21, 1304,  ..., 3577,   13, 2497]], device='cuda:0')
Skip epoch 0, step 290, batch 290


Running epoch 0, step 291, batch 291
Sampled inputs[:2]: tensor([[   0,  504,  546,  ...,  634,  328,  630],
        [   0,  367, 2063,  ..., 3022,  221,  733]], device='cuda:0')
Skip epoch 0, step 291, batch 291


Running epoch 0, step 292, batch 292
Sampled inputs[:2]: tensor([[    0,  1451, 14349,  ...,   741,  2945,  7257],
        [    0,   271,   957,  ...,  1597,  1276,   292]], device='cuda:0')
Skip epoch 0, step 292, batch 292


Running epoch 0, step 293, batch 293
Sampled inputs[:2]: tensor([[    0,  2286,  1085,  ...,  1387,  1184,  1802],
        [    0,   677, 25912,  ...,  2337,   292,  4462]], device='cuda:0')
Skip epoch 0, step 293, batch 293


Running epoch 0, step 294, batch 294
Sampled inputs[:2]: tensor([[    0,   452,    13,  ...,   358,    13, 12347],
        [    0,    12,   287,  ...,   381,  3513,  1501]], device='cuda:0')
Skip epoch 0, step 294, batch 294


Running epoch 0, step 295, batch 295
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0,  266, 1441,  ..., 1817, 1589,  278]], device='cuda:0')
Skip epoch 0, step 295, batch 295


Running epoch 0, step 296, batch 296
Sampled inputs[:2]: tensor([[    0, 49141,    14,  ...,   342,   259,  1943],
        [    0,   368,   266,  ...,   591,   767,   824]], device='cuda:0')
Skip epoch 0, step 296, batch 296


Running epoch 0, step 297, batch 297
Sampled inputs[:2]: tensor([[    0,   421,  6007,  ...,   408,  2105,   843],
        [    0,   300,  6263,  ..., 18488,  1665,  1640]], device='cuda:0')
Skip epoch 0, step 297, batch 297


Running epoch 0, step 298, batch 298
Sampled inputs[:2]: tensor([[    0, 21410, 13160,  ...,   292,    69,    14],
        [    0,    19,   669,  ...,    14,  4053,   352]], device='cuda:0')
Skip epoch 0, step 298, batch 298


Running epoch 0, step 299, batch 299
Sampled inputs[:2]: tensor([[   0, 5116, 4330,  ...,  925,  699, 1351],
        [   0, 3261, 5866,  ...,  593,  360, 2502]], device='cuda:0')
Skip epoch 0, step 299, batch 299


Running epoch 0, step 300, batch 300
Sampled inputs[:2]: tensor([[   0,  266, 1336,  ..., 1841, 9705, 1219],
        [   0, 1823,   12,  ..., 1874,  271,  266]], device='cuda:0')
Skip epoch 0, step 300, batch 300


Running epoch 0, step 301, batch 301
Sampled inputs[:2]: tensor([[    0,   278,   264,  ..., 21836,   344,   259],
        [    0,   445,    29,  ..., 20247,   272,   298]], device='cuda:0')
Skip epoch 0, step 301, batch 301


Running epoch 0, step 302, batch 302
Sampled inputs[:2]: tensor([[    0,   298,   452,  ..., 41263,     9,   367],
        [    0,   694,   266,  ...,  3007,   300,  5726]], device='cuda:0')
Skip epoch 0, step 302, batch 302


Running epoch 0, step 303, batch 303
Sampled inputs[:2]: tensor([[   0,  409, 3669,  ...,   12,  374,   20],
        [   0,  259, 2283,  ...,  462,  221,  474]], device='cuda:0')
Skip epoch 0, step 303, batch 303


Running epoch 0, step 304, batch 304
Sampled inputs[:2]: tensor([[   0,  587,  292,  ...,   12,  287, 2261],
        [   0, 8158, 1416,  ...,  413,   29,  413]], device='cuda:0')
Skip epoch 0, step 304, batch 304


Running epoch 0, step 305, batch 305
Sampled inputs[:2]: tensor([[   0,  221,  380,  ..., 3990,  717,   12],
        [   0, 3261, 1518,  ..., 5019,  287, 1906]], device='cuda:0')
Skip epoch 0, step 305, batch 305


Running epoch 0, step 306, batch 306
Sampled inputs[:2]: tensor([[    0, 17508,    65,  ...,  8848, 13900,   796],
        [    0, 10288,   300,  ...,  5365,    12,  3539]], device='cuda:0')
Skip epoch 0, step 306, batch 306


Running epoch 0, step 307, batch 307
Sampled inputs[:2]: tensor([[    0,  9855,   278,  ...,   266,  3134,   278],
        [    0,    14,  1147,  ...,    19,    14, 42301]], device='cuda:0')
Skip epoch 0, step 307, batch 307


Running epoch 0, step 308, batch 308
Sampled inputs[:2]: tensor([[    0,   360,  2374,  ...,   221,   474,   357],
        [    0,   409, 22809,  ...,   342,   720,    14]], device='cuda:0')
Skip epoch 0, step 308, batch 308


Running epoch 0, step 309, batch 309
Sampled inputs[:2]: tensor([[    0,    13, 23904,  ...,   560,  8840,    26],
        [    0,  2377,   271,  ...,   395,   394,    14]], device='cuda:0')
Skip epoch 0, step 309, batch 309


Running epoch 0, step 310, batch 310
Sampled inputs[:2]: tensor([[    0,   333,   199,  ...,   287,  4299, 31928],
        [    0,    89,  6893,  ...,  5254,   278,  4531]], device='cuda:0')
Skip epoch 0, step 310, batch 310


Running epoch 0, step 311, batch 311
Sampled inputs[:2]: tensor([[    0, 38717,  1679,  ...,   472,   346,   462],
        [    0, 39200,  1828,  ...,   300,  3067,  4443]], device='cuda:0')
Skip epoch 0, step 311, batch 311


Running epoch 0, step 312, batch 312
Sampled inputs[:2]: tensor([[    0,   298, 11712,  ...,   221,   273,   298],
        [    0,   266,  9823,  ...,    14,  1062,  7676]], device='cuda:0')
Skip epoch 0, step 312, batch 312


Running epoch 0, step 313, batch 313
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
Skip epoch 0, step 313, batch 313


Running epoch 0, step 314, batch 314
Sampled inputs[:2]: tensor([[    0,  1254,  1773,  ..., 19459,  2447,  2613],
        [    0,  6538,  1805,  ...,   298,   271,   721]], device='cuda:0')
Skip epoch 0, step 314, batch 314


Running epoch 0, step 315, batch 315
Sampled inputs[:2]: tensor([[   0,  381, 1659,  ..., 1403,  271, 6324],
        [   0, 9058, 4048,  ...,   14,  759, 1403]], device='cuda:0')
Skip epoch 0, step 315, batch 315


Running epoch 0, step 316, batch 316
Sampled inputs[:2]: tensor([[   0, 1575, 4384,  ...,  328,  722, 6124],
        [   0, 2202,  292,  ..., 2431, 2267, 3423]], device='cuda:0')
Skip epoch 0, step 316, batch 316


Running epoch 0, step 317, batch 317
Sampled inputs[:2]: tensor([[    0, 48545,    26,  ...,  1471,   266,   319],
        [    0,  3398,  6361,  ..., 12942,   518,  4066]], device='cuda:0')
Skip epoch 0, step 317, batch 317


Running epoch 0, step 318, batch 318
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,   271,   266,  ...,   275,  2576,  3588]], device='cuda:0')
Skip epoch 0, step 318, batch 318


Running epoch 0, step 319, batch 319
Sampled inputs[:2]: tensor([[    0,  1858,   499,  ...,    14,  1032,    14],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
Skip epoch 0, step 319, batch 319


Running epoch 0, step 320, batch 320
Sampled inputs[:2]: tensor([[   0, 1276,  292,  ...,   83, 1837,   13],
        [   0,  285,   53,  ...,  259, 5012, 3037]], device='cuda:0')
Step 320, before update, should be same as saved 319?
optimizer state dict: tensor([[-3.4876e-05, -2.7616e-06, -2.5923e-05,  ...,  2.2013e-05,
         -5.0308e-05,  2.5417e-06],
        [-2.4172e-05, -1.7586e-05,  4.1748e-06,  ..., -2.0720e-05,
         -4.2994e-06, -1.0423e-05],
        [ 8.2188e-05,  6.4152e-05, -1.6454e-05,  ...,  8.1570e-05,
          2.2983e-05,  3.9684e-05],
        [-2.7891e-05, -2.0092e-05,  4.8874e-06,  ..., -2.3686e-05,
         -5.0242e-06, -1.1877e-05],
        [-5.1871e-05, -3.7584e-05,  8.7132e-06,  ..., -4.3999e-05,
         -8.4437e-06, -2.1431e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2089e-08, 2.8132e-08, 3.5303e-08,  ..., 1.1639e-08, 8.8045e-08,
         1.1461e-08],
        [4.1346e-11, 2.2651e-11, 2.3671e-12,  ..., 2.6018e-11, 1.5174e-12,
         5.7554e-12],
        [5.1965e-10, 2.8143e-10, 1.8502e-11,  ..., 4.6614e-10, 2.9989e-11,
         1.1433e-10],
        [3.2067e-11, 1.5689e-11, 1.1982e-12,  ..., 2.2001e-11, 8.2038e-13,
         5.1327e-12],
        [1.7794e-10, 9.2507e-11, 6.8980e-12,  ..., 1.2502e-10, 6.4646e-12,
         2.8726e-11]], device='cuda:0')
optimizer state dict: 40.0
lr: [1.9221075944084176e-05, 1.9221075944084176e-05]
scheduler_last_epoch: 40
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2136e-05, -2.0227e-05, -4.4316e-05,  ...,  2.1132e-05,
          1.5176e-05,  4.6956e-06],
        [-3.0547e-06, -2.3842e-06,  7.1153e-07,  ..., -2.7418e-06,
         -8.2329e-07, -1.2740e-06],
        [-3.3081e-06, -2.5928e-06,  7.7114e-07,  ..., -2.9653e-06,
         -8.9034e-07, -1.3784e-06],
        [-3.2932e-06, -2.5630e-06,  7.6741e-07,  ..., -2.9504e-06,
         -8.8289e-07, -1.3709e-06],
        [-6.9737e-06, -5.4240e-06,  1.6168e-06,  ..., -6.2287e-06,
         -1.8701e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.1507784128189087


Running epoch 0, step 321, batch 321
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  631, 2820,  344],
        [   0,  292, 2860,  ...,  266, 7000, 7806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2756e-07, -2.6542e-06, -6.1035e-05,  ...,  3.1776e-05,
          2.3344e-06,  3.6268e-05],
        [-6.0946e-06, -4.7833e-06,  1.4082e-06,  ..., -5.5134e-06,
         -1.6876e-06, -2.5183e-06],
        [-6.6012e-06, -5.1856e-06,  1.5236e-06,  ..., -5.9605e-06,
         -1.8217e-06, -2.7269e-06],
        [-6.5416e-06, -5.1260e-06,  1.5087e-06,  ..., -5.9009e-06,
         -1.7993e-06, -2.6971e-06],
        [-1.3888e-05, -1.0878e-05,  3.1963e-06,  ..., -1.2517e-05,
         -3.8370e-06, -5.7220e-06]], device='cuda:0')
Loss: 1.1426258087158203


Running epoch 0, step 322, batch 322
Sampled inputs[:2]: tensor([[    0, 28107,    14,  ...,   864,   298,   413],
        [    0,   259,   587,  ...,    14,    71,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3483e-05,  3.0097e-05, -3.5454e-05,  ...,  1.9238e-05,
         -5.6947e-06,  4.3841e-05],
        [-9.1791e-06, -7.2122e-06,  2.1048e-06,  ..., -8.2701e-06,
         -2.5555e-06, -3.7849e-06],
        [-9.9391e-06, -7.8231e-06,  2.2799e-06,  ..., -8.9556e-06,
         -2.7604e-06, -4.0978e-06],
        [-9.8348e-06, -7.7188e-06,  2.2538e-06,  ..., -8.8364e-06,
         -2.7232e-06, -4.0457e-06],
        [-2.0921e-05, -1.6421e-05,  4.7833e-06,  ..., -1.8805e-05,
         -5.8189e-06, -8.5980e-06]], device='cuda:0')
Loss: 1.132506012916565


Running epoch 0, step 323, batch 323
Sampled inputs[:2]: tensor([[    0,   266,   996,  ...,   709,   616,  9378],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4701e-05,  1.9677e-05, -3.3086e-05,  ...,  6.0756e-06,
         -3.6811e-05,  6.5287e-05],
        [-1.2234e-05, -9.6411e-06,  2.8387e-06,  ..., -1.1012e-05,
         -3.4459e-06, -5.0589e-06],
        [-1.3247e-05, -1.0461e-05,  3.0771e-06,  ..., -1.1936e-05,
         -3.7216e-06, -5.4836e-06],
        [-1.3083e-05, -1.0297e-05,  3.0361e-06,  ..., -1.1757e-05,
         -3.6657e-06, -5.4017e-06],
        [-2.7895e-05, -2.1964e-05,  6.4597e-06,  ..., -2.5064e-05,
         -7.8455e-06, -1.1519e-05]], device='cuda:0')
Loss: 1.1530135869979858


Running epoch 0, step 324, batch 324
Sampled inputs[:2]: tensor([[    0,    12,  6426,  ...,  2629, 13422,    12],
        [    0,   635,    13,  ...,    13,  4710,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7666e-05,  4.2048e-05, -5.1320e-05,  ..., -1.6713e-05,
         -3.1086e-05,  1.2264e-04],
        [-1.5289e-05, -1.2070e-05,  3.5614e-06,  ..., -1.3784e-05,
         -4.3400e-06, -6.3106e-06],
        [-1.6570e-05, -1.3113e-05,  3.8669e-06,  ..., -1.4961e-05,
         -4.6976e-06, -6.8471e-06],
        [-1.6332e-05, -1.2890e-05,  3.8072e-06,  ..., -1.4707e-05,
         -4.6156e-06, -6.7353e-06],
        [-3.4839e-05, -2.7508e-05,  8.1062e-06,  ..., -3.1382e-05,
         -9.8869e-06, -1.4365e-05]], device='cuda:0')
Loss: 1.1520698070526123


Running epoch 0, step 325, batch 325
Sampled inputs[:2]: tensor([[   0,  368, 2418,  ..., 3275, 1116, 5189],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7666e-05,  4.7655e-05, -6.6694e-05,  ..., -2.8086e-05,
         -1.5678e-05,  1.5467e-04],
        [-1.8328e-05, -1.4499e-05,  4.2543e-06,  ..., -1.6525e-05,
         -5.2191e-06, -7.5996e-06],
        [-1.9908e-05, -1.5795e-05,  4.6268e-06,  ..., -1.7971e-05,
         -5.6662e-06, -8.2627e-06],
        [-1.9580e-05, -1.5497e-05,  4.5523e-06,  ..., -1.7643e-05,
         -5.5544e-06, -8.1137e-06],
        [-4.1813e-05, -3.3081e-05,  9.6932e-06,  ..., -3.7670e-05,
         -1.1899e-05, -1.7315e-05]], device='cuda:0')
Loss: 1.1634695529937744


Running epoch 0, step 326, batch 326
Sampled inputs[:2]: tensor([[    0,   287,   266,  ..., 10238,    12, 39004],
        [    0,    14,   560,  ...,  1248,  1398,  1268]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5693e-05,  4.6952e-05, -6.8742e-05,  ..., -3.0092e-05,
         -3.4258e-05,  1.7880e-04],
        [-2.1368e-05, -1.6898e-05,  4.9807e-06,  ..., -1.9252e-05,
         -6.0871e-06, -8.8885e-06],
        [-2.3276e-05, -1.8463e-05,  5.4315e-06,  ..., -2.0996e-05,
         -6.6273e-06, -9.6858e-06],
        [-2.2858e-05, -1.8105e-05,  5.3383e-06,  ..., -2.0593e-05,
         -6.4932e-06, -9.5069e-06],
        [-4.8786e-05, -3.8594e-05,  1.1355e-05,  ..., -4.3929e-05,
         -1.3895e-05, -2.0266e-05]], device='cuda:0')
Loss: 1.1446789503097534


Running epoch 0, step 327, batch 327
Sampled inputs[:2]: tensor([[   0, 2310,  292,  ...,  462,  508,  586],
        [   0,  908,   14,  ...,   19,   27,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7712e-05,  5.3513e-05, -6.7147e-05,  ..., -2.1692e-05,
         -1.9815e-05,  1.9665e-04],
        [-2.4393e-05, -1.9282e-05,  5.6997e-06,  ..., -2.1994e-05,
         -6.9588e-06, -1.0140e-05],
        [-2.6584e-05, -2.1070e-05,  6.2212e-06,  ..., -2.4006e-05,
         -7.5810e-06, -1.1057e-05],
        [-2.6077e-05, -2.0638e-05,  6.1058e-06,  ..., -2.3514e-05,
         -7.4208e-06, -1.0841e-05],
        [-5.5730e-05, -4.4078e-05,  1.3001e-05,  ..., -5.0217e-05,
         -1.5892e-05, -2.3142e-05]], device='cuda:0')
Loss: 1.1526713371276855
Graident accumulation at epoch 0, step 327, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0203],
        [ 0.0291, -0.0077,  0.0032,  ..., -0.0095, -0.0022, -0.0339],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0013],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.8160e-05,  2.8659e-06, -3.0045e-05,  ...,  1.7642e-05,
         -4.7259e-05,  2.1953e-05],
        [-2.4194e-05, -1.7756e-05,  4.3273e-06,  ..., -2.0847e-05,
         -4.5653e-06, -1.0395e-05],
        [ 7.1310e-05,  5.5630e-05, -1.4186e-05,  ...,  7.1012e-05,
          1.9927e-05,  3.4610e-05],
        [-2.7709e-05, -2.0147e-05,  5.0093e-06,  ..., -2.3669e-05,
         -5.2638e-06, -1.1774e-05],
        [-5.2257e-05, -3.8233e-05,  9.1420e-06,  ..., -4.4621e-05,
         -9.1885e-06, -2.1602e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2052e-08, 2.8107e-08, 3.5272e-08,  ..., 1.1628e-08, 8.7957e-08,
         1.1489e-08],
        [4.1900e-11, 2.3001e-11, 2.3972e-12,  ..., 2.6476e-11, 1.5643e-12,
         5.8524e-12],
        [5.1984e-10, 2.8159e-10, 1.8522e-11,  ..., 4.6625e-10, 3.0017e-11,
         1.1434e-10],
        [3.2715e-11, 1.6099e-11, 1.2342e-12,  ..., 2.2532e-11, 8.7463e-13,
         5.2451e-12],
        [1.8087e-10, 9.4357e-11, 7.0601e-12,  ..., 1.2741e-10, 6.7107e-12,
         2.9233e-11]], device='cuda:0')
optimizer state dict: 41.0
lr: [1.9172541214186228e-05, 1.9172541214186228e-05]
scheduler_last_epoch: 41


Running epoch 0, step 328, batch 328
Sampled inputs[:2]: tensor([[   0,  278, 1099,  ...,  496,   14,  879],
        [   0,  741, 4933,  ...,  932,  365,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5081e-06, -6.4039e-06, -1.5480e-05,  ...,  1.8273e-05,
          3.4765e-07,  1.6267e-06],
        [-3.0100e-06, -2.4289e-06,  7.5623e-07,  ..., -2.7269e-06,
         -8.1584e-07, -1.2517e-06],
        [-3.3081e-06, -2.6673e-06,  8.3074e-07,  ..., -3.0100e-06,
         -9.0152e-07, -1.3784e-06],
        [-3.2037e-06, -2.5779e-06,  8.0839e-07,  ..., -2.9057e-06,
         -8.6799e-07, -1.3337e-06],
        [-6.8843e-06, -5.5432e-06,  1.7285e-06,  ..., -6.2585e-06,
         -1.8775e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.1593455076217651


Running epoch 0, step 329, batch 329
Sampled inputs[:2]: tensor([[    0,    14,  3080,  ..., 14737,    13, 17982],
        [    0,  7185,   328,  ...,  1427,  1477,  1061]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7669e-05, -4.5594e-06, -1.4765e-05,  ...,  2.4080e-05,
          1.4407e-05, -7.0114e-06],
        [-5.9754e-06, -4.8429e-06,  1.5050e-06,  ..., -5.4538e-06,
         -1.6242e-06, -2.5034e-06],
        [-6.5714e-06, -5.3197e-06,  1.6540e-06,  ..., -6.0052e-06,
         -1.7919e-06, -2.7567e-06],
        [-6.3628e-06, -5.1558e-06,  1.6093e-06,  ..., -5.8115e-06,
         -1.7285e-06, -2.6748e-06],
        [-1.3709e-05, -1.1086e-05,  3.4496e-06,  ..., -1.2517e-05,
         -3.7402e-06, -5.7518e-06]], device='cuda:0')
Loss: 1.165090560913086


Running epoch 0, step 330, batch 330
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0,   292,   221,  ...,   796, 12886,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8481e-05,  3.5996e-06, -3.8530e-05,  ...,  1.5845e-05,
          1.4644e-05,  1.5347e-05],
        [-9.0003e-06, -7.2718e-06,  2.2538e-06,  ..., -8.1807e-06,
         -2.4736e-06, -3.7476e-06],
        [-9.8795e-06, -7.9870e-06,  2.4773e-06,  ..., -9.0003e-06,
         -2.7195e-06, -4.1202e-06],
        [-9.5814e-06, -7.7486e-06,  2.4103e-06,  ..., -8.7172e-06,
         -2.6301e-06, -4.0010e-06],
        [-2.0653e-05, -1.6659e-05,  5.1707e-06,  ..., -1.8775e-05,
         -5.6922e-06, -8.5980e-06]], device='cuda:0')
Loss: 1.1466789245605469


Running epoch 0, step 331, batch 331
Sampled inputs[:2]: tensor([[    0,    14, 15670,  ...,  2027,   417,   199],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0098e-05,  9.4325e-06, -4.4992e-05,  ...,  9.9221e-06,
          1.5424e-05,  1.7189e-05],
        [-1.1981e-05, -9.7156e-06,  3.0175e-06,  ..., -1.0937e-05,
         -3.3341e-06, -4.9919e-06],
        [-1.3158e-05, -1.0669e-05,  3.3192e-06,  ..., -1.2025e-05,
         -3.6620e-06, -5.4836e-06],
        [-1.2755e-05, -1.0356e-05,  3.2261e-06,  ..., -1.1653e-05,
         -3.5428e-06, -5.3272e-06],
        [-2.7478e-05, -2.2262e-05,  6.9216e-06,  ..., -2.5064e-05,
         -7.6592e-06, -1.1444e-05]], device='cuda:0')
Loss: 1.1563204526901245


Running epoch 0, step 332, batch 332
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,    12,   850,  4952],
        [    0,  1527, 21622,  ..., 14406,    13,  6182]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9111e-06, -3.1000e-05, -5.7717e-05,  ...,  1.8807e-05,
         -1.1168e-06, -1.0378e-05],
        [-1.5005e-05, -1.2159e-05,  3.7663e-06,  ..., -1.3649e-05,
         -4.1388e-06, -6.2361e-06],
        [-1.6481e-05, -1.3351e-05,  4.1425e-06,  ..., -1.5005e-05,
         -4.5449e-06, -6.8545e-06],
        [-1.6004e-05, -1.2979e-05,  4.0308e-06,  ..., -1.4558e-05,
         -4.4033e-06, -6.6608e-06],
        [-3.4392e-05, -2.7835e-05,  8.6278e-06,  ..., -3.1233e-05,
         -9.4920e-06, -1.4290e-05]], device='cuda:0')
Loss: 1.1372156143188477


Running epoch 0, step 333, batch 333
Sampled inputs[:2]: tensor([[    0,  2950,    13,  ..., 16513,   300,  2205],
        [    0,  3164,    12,  ...,   984,   344,  3993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4026e-06, -3.0620e-05, -8.9176e-05,  ..., -7.7721e-06,
          6.6970e-07, -6.4236e-06],
        [-1.8030e-05, -1.4588e-05,  4.5225e-06,  ..., -1.6376e-05,
         -4.9695e-06, -7.4953e-06],
        [-1.9804e-05, -1.6034e-05,  4.9770e-06,  ..., -1.8016e-05,
         -5.4576e-06, -8.2403e-06],
        [-1.9252e-05, -1.5587e-05,  4.8466e-06,  ..., -1.7479e-05,
         -5.2899e-06, -8.0094e-06],
        [-4.1276e-05, -3.3379e-05,  1.0349e-05,  ..., -3.7462e-05,
         -1.1384e-05, -1.7151e-05]], device='cuda:0')
Loss: 1.1447516679763794


Running epoch 0, step 334, batch 334
Sampled inputs[:2]: tensor([[   0,  472,  346,  ...,  298,  527,  496],
        [   0, 1806,  319,  ..., 3427,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9255e-05, -3.1794e-05, -1.2727e-04,  ..., -1.0178e-05,
         -5.6598e-06, -1.3014e-05],
        [-2.1011e-05, -1.7002e-05,  5.2787e-06,  ..., -1.9118e-05,
         -5.8115e-06, -8.7768e-06],
        [-2.3097e-05, -1.8716e-05,  5.8115e-06,  ..., -2.1055e-05,
         -6.3851e-06, -9.6560e-06],
        [-2.2426e-05, -1.8165e-05,  5.6513e-06,  ..., -2.0400e-05,
         -6.1840e-06, -9.3728e-06],
        [-4.8071e-05, -3.8892e-05,  1.2062e-05,  ..., -4.3720e-05,
         -1.3292e-05, -2.0072e-05]], device='cuda:0')
Loss: 1.1434177160263062


Running epoch 0, step 335, batch 335
Sampled inputs[:2]: tensor([[    0,    14,  6707,  ..., 17771,   300,   259],
        [    0, 11752,   280,  ..., 14814,  1128,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8409e-05,  2.1818e-06, -1.3681e-04,  ..., -2.0837e-05,
         -1.2263e-06, -3.2724e-05],
        [-2.3991e-05, -1.9401e-05,  6.0350e-06,  ..., -2.1830e-05,
         -6.6310e-06, -1.0058e-05],
        [-2.6375e-05, -2.1353e-05,  6.6459e-06,  ..., -2.4050e-05,
         -7.2904e-06, -1.1064e-05],
        [-2.5615e-05, -2.0728e-05,  6.4634e-06,  ..., -2.3305e-05,
         -7.0594e-06, -1.0744e-05],
        [-5.4926e-05, -4.4405e-05,  1.3798e-05,  ..., -4.9978e-05,
         -1.5184e-05, -2.3022e-05]], device='cuda:0')
Loss: 1.1583244800567627
Graident accumulation at epoch 0, step 335, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0031,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9185e-05,  2.7975e-06, -4.0722e-05,  ...,  1.3794e-05,
         -4.2655e-05,  1.6485e-05],
        [-2.4174e-05, -1.7921e-05,  4.4981e-06,  ..., -2.0946e-05,
         -4.7719e-06, -1.0361e-05],
        [ 6.1542e-05,  4.7931e-05, -1.2103e-05,  ...,  6.1506e-05,
          1.7205e-05,  3.0043e-05],
        [-2.7500e-05, -2.0205e-05,  5.1547e-06,  ..., -2.3632e-05,
         -5.4434e-06, -1.1671e-05],
        [-5.2524e-05, -3.8850e-05,  9.6076e-06,  ..., -4.5157e-05,
         -9.7881e-06, -2.1744e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2012e-08, 2.8079e-08, 3.5256e-08,  ..., 1.1617e-08, 8.7869e-08,
         1.1478e-08],
        [4.2434e-11, 2.3354e-11, 2.4312e-12,  ..., 2.6926e-11, 1.6067e-12,
         5.9477e-12],
        [5.2002e-10, 2.8177e-10, 1.8548e-11,  ..., 4.6636e-10, 3.0040e-11,
         1.1434e-10],
        [3.3338e-11, 1.6512e-11, 1.2748e-12,  ..., 2.3053e-11, 9.2359e-13,
         5.3553e-12],
        [1.8370e-10, 9.6234e-11, 7.2434e-12,  ..., 1.2978e-10, 6.9346e-12,
         2.9734e-11]], device='cuda:0')
optimizer state dict: 42.0
lr: [1.9122604839922505e-05, 1.9122604839922505e-05]
scheduler_last_epoch: 42


Running epoch 0, step 336, batch 336
Sampled inputs[:2]: tensor([[    0,   199,  3289,  ...,  2269,  6476,   271],
        [    0,   278, 38717,  ...,  9945,   367,  5430]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1801e-05, -3.1387e-05, -2.9357e-05,  ...,  1.2978e-05,
         -1.5447e-05, -1.6091e-05],
        [-2.9504e-06, -2.3991e-06,  7.5996e-07,  ..., -2.6822e-06,
         -7.8231e-07, -1.2591e-06],
        [ 8.3197e-05,  6.6537e-05, -2.5593e-05,  ...,  7.7520e-05,
          7.4208e-06,  3.0184e-05],
        [-3.1441e-06, -2.5630e-06,  8.1584e-07,  ..., -2.8759e-06,
         -8.3447e-07, -1.3411e-06],
        [-6.7651e-06, -5.5134e-06,  1.7434e-06,  ..., -6.1691e-06,
         -1.7956e-06, -2.8759e-06]], device='cuda:0')
Loss: 1.1515077352523804


Running epoch 0, step 337, batch 337
Sampled inputs[:2]: tensor([[    0, 16064, 10937,  ...,   346,   462,   221],
        [    0,   278,  4452,  ...,    14,    18,  3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7694e-05, -7.2987e-05, -3.4572e-05,  ...,  2.1806e-05,
         -4.2505e-05, -2.5325e-05],
        [-5.9158e-06, -4.7982e-06,  1.5534e-06,  ..., -5.3942e-06,
         -1.5646e-06, -2.5108e-06],
        [ 7.9919e-05,  6.3870e-05, -2.4717e-05,  ...,  7.4510e-05,
          6.5528e-06,  2.8799e-05],
        [-6.3181e-06, -5.1409e-06,  1.6652e-06,  ..., -5.7817e-06,
         -1.6727e-06, -2.6822e-06],
        [-1.3530e-05, -1.1027e-05,  3.5539e-06,  ..., -1.2368e-05,
         -3.5912e-06, -5.7369e-06]], device='cuda:0')
Loss: 1.1615506410598755


Running epoch 0, step 338, batch 338
Sampled inputs[:2]: tensor([[   0,  266, 3574,  ..., 7052, 3829,  292],
        [   0,  266,  824,  ..., 1799,  287, 6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6816e-05, -9.7667e-05, -7.7889e-05,  ..., -2.4415e-05,
         -7.6822e-05, -7.3103e-05],
        [-8.8215e-06, -7.2420e-06,  2.3171e-06,  ..., -8.0764e-06,
         -2.3246e-06, -3.7402e-06],
        [ 7.6700e-05,  6.1158e-05, -2.3868e-05,  ...,  7.1529e-05,
          5.7109e-06,  2.7435e-05],
        [-9.4324e-06, -7.7486e-06,  2.4848e-06,  ..., -8.6576e-06,
         -2.4848e-06, -3.9935e-06],
        [-2.0206e-05, -1.6600e-05,  5.3048e-06,  ..., -1.8507e-05,
         -5.3272e-06, -8.5533e-06]], device='cuda:0')
Loss: 1.1440343856811523


Running epoch 0, step 339, batch 339
Sampled inputs[:2]: tensor([[    0,  2849,  1173,  ...,  1481,    12,   287],
        [    0,    14,   747,  ..., 12545,    12, 15209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5958e-05, -1.0168e-04, -8.1877e-05,  ..., -4.1273e-07,
         -9.4761e-05, -3.0201e-05],
        [-1.1772e-05, -9.6560e-06,  3.1032e-06,  ..., -1.0774e-05,
         -3.0957e-06, -4.9993e-06],
        [ 7.3437e-05,  5.8491e-05, -2.3000e-05,  ...,  6.8549e-05,
          4.8578e-06,  2.6042e-05],
        [-1.2621e-05, -1.0356e-05,  3.3341e-06,  ..., -1.1578e-05,
         -3.3192e-06, -5.3570e-06],
        [-2.6941e-05, -2.2084e-05,  7.0930e-06,  ..., -2.4647e-05,
         -7.0855e-06, -1.1414e-05]], device='cuda:0')
Loss: 1.1329964399337769


Running epoch 0, step 340, batch 340
Sampled inputs[:2]: tensor([[   0,  995,   13,  ..., 2192, 2534,  287],
        [   0,  352,  266,  ..., 2416,  287,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6692e-05, -1.4055e-04, -8.9401e-05,  ..., -3.2760e-05,
         -1.0671e-04, -1.9974e-05],
        [-1.4737e-05, -1.2085e-05,  3.8818e-06,  ..., -1.3515e-05,
         -3.8780e-06, -6.2659e-06],
        [ 7.0173e-05,  5.5808e-05, -2.2139e-05,  ...,  6.5524e-05,
          4.0010e-06,  2.4649e-05],
        [-1.5780e-05, -1.2949e-05,  4.1686e-06,  ..., -1.4499e-05,
         -4.1500e-06, -6.7055e-06],
        [-3.3706e-05, -2.7657e-05,  8.8736e-06,  ..., -3.0905e-05,
         -8.8736e-06, -1.4305e-05]], device='cuda:0')
Loss: 1.1612557172775269


Running epoch 0, step 341, batch 341
Sampled inputs[:2]: tensor([[    0,    14, 13078,  ...,  1994,    12,   287],
        [    0,  2314,   266,  ...,   342,  7299,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9308e-05, -1.3734e-04, -1.0161e-04,  ..., -3.0557e-05,
         -9.5994e-05,  1.1346e-05],
        [-1.7703e-05, -1.4499e-05,  4.6492e-06,  ..., -1.6257e-05,
         -4.6864e-06, -7.5102e-06],
        [ 2.5204e-04,  1.9416e-04, -3.5103e-05,  ...,  2.1810e-04,
          4.3879e-05,  6.8724e-05],
        [-1.8924e-05, -1.5497e-05,  4.9807e-06,  ..., -1.7390e-05,
         -4.9993e-06, -8.0243e-06],
        [-4.0412e-05, -3.3110e-05,  1.0610e-05,  ..., -3.7104e-05,
         -1.0699e-05, -1.7121e-05]], device='cuda:0')
Loss: 1.1470273733139038


Running epoch 0, step 342, batch 342
Sampled inputs[:2]: tensor([[    0, 24063,   717,  ...,  2228,  1416,     9],
        [    0,   391,  4356,  ...,   287, 32873,  5362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3176e-05, -1.5818e-04, -1.3602e-04,  ..., -2.2945e-05,
         -1.1358e-04,  1.0480e-05],
        [-2.0638e-05, -1.6928e-05,  5.4426e-06,  ..., -1.8954e-05,
         -5.4836e-06, -8.7470e-06],
        [ 2.4878e-04,  1.9147e-04, -3.4224e-05,  ...,  2.1510e-04,
          4.2992e-05,  6.7346e-05],
        [-2.2069e-05, -1.8090e-05,  5.8301e-06,  ..., -2.0266e-05,
         -5.8487e-06, -9.3430e-06],
        [-4.7147e-05, -3.8654e-05,  1.2420e-05,  ..., -4.3273e-05,
         -1.2524e-05, -1.9953e-05]], device='cuda:0')
Loss: 1.1536933183670044


Running epoch 0, step 343, batch 343
Sampled inputs[:2]: tensor([[    0,  7994,    12,  ..., 13800,   278,   795],
        [    0,   445,    16,  ...,  7747,  5308,  6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3375e-04, -1.6364e-04, -1.4628e-04,  ..., -3.8819e-05,
         -1.4744e-04,  4.5487e-07],
        [-2.3574e-05, -1.9342e-05,  6.2361e-06,  ..., -2.1666e-05,
         -6.2846e-06, -9.9987e-06],
        [ 2.4554e-04,  1.8880e-04, -3.3349e-05,  ...,  2.1211e-04,
          4.2109e-05,  6.5967e-05],
        [-2.5213e-05, -2.0668e-05,  6.6794e-06,  ..., -2.3156e-05,
         -6.7018e-06, -1.0677e-05],
        [-5.3823e-05, -4.4137e-05,  1.4223e-05,  ..., -4.9442e-05,
         -1.4342e-05, -2.2784e-05]], device='cuda:0')
Loss: 1.159455418586731
Graident accumulation at epoch 0, step 343, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0145, -0.0271,  ...,  0.0281, -0.0158, -0.0185]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.8641e-05, -1.3846e-05, -5.1278e-05,  ...,  8.5328e-06,
         -5.3134e-05,  1.4882e-05],
        [-2.4114e-05, -1.8063e-05,  4.6719e-06,  ..., -2.1018e-05,
         -4.9232e-06, -1.0325e-05],
        [ 7.9942e-05,  6.2018e-05, -1.4228e-05,  ...,  7.6566e-05,
          1.9696e-05,  3.3635e-05],
        [-2.7271e-05, -2.0251e-05,  5.3071e-06,  ..., -2.3585e-05,
         -5.5692e-06, -1.1571e-05],
        [-5.2654e-05, -3.9379e-05,  1.0069e-05,  ..., -4.5585e-05,
         -1.0244e-05, -2.1848e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1988e-08, 2.8078e-08, 3.5242e-08,  ..., 1.1607e-08, 8.7803e-08,
         1.1467e-08],
        [4.2947e-11, 2.3705e-11, 2.4677e-12,  ..., 2.7369e-11, 1.6446e-12,
         6.0418e-12],
        [5.7979e-10, 3.1713e-10, 1.9642e-11,  ..., 5.1089e-10, 3.1783e-11,
         1.1858e-10],
        [3.3941e-11, 1.6923e-11, 1.3181e-12,  ..., 2.3566e-11, 9.6758e-13,
         5.4639e-12],
        [1.8642e-10, 9.8086e-11, 7.4385e-12,  ..., 1.3210e-10, 7.1334e-12,
         3.0223e-11]], device='cuda:0')
optimizer state dict: 43.0
lr: [1.90712744520069e-05, 1.90712744520069e-05]
scheduler_last_epoch: 43


Running epoch 0, step 344, batch 344
Sampled inputs[:2]: tensor([[   0,  382,   17,  ..., 8733,   13, 9306],
        [   0, 3646, 1340,  ...,   13, 7800, 2872]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5355e-05, -9.3697e-06,  0.0000e+00,  ..., -1.0642e-05,
         -2.6737e-05, -3.9722e-05],
        [-2.9206e-06, -2.4140e-06,  8.0466e-07,  ..., -2.6971e-06,
         -7.3016e-07, -1.2740e-06],
        [-3.2634e-06, -2.6971e-06,  8.9779e-07,  ..., -3.0100e-06,
         -8.1211e-07, -1.4156e-06],
        [-3.1143e-06, -2.5779e-06,  8.6054e-07,  ..., -2.8759e-06,
         -7.7486e-07, -1.3560e-06],
        [-6.6757e-06, -5.5134e-06,  1.8328e-06,  ..., -6.1691e-06,
         -1.6615e-06, -2.8908e-06]], device='cuda:0')
Loss: 1.1349341869354248


Running epoch 0, step 345, batch 345
Sampled inputs[:2]: tensor([[   0,  689, 2149,  ..., 4263,   14,  292],
        [   0, 3761,   12,  ...,   14,   22,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6393e-06, -1.4782e-05, -2.9792e-05,  ...,  3.9030e-07,
         -1.7948e-05, -3.1959e-05],
        [-5.8860e-06, -4.8280e-06,  1.6503e-06,  ..., -5.3942e-06,
         -1.4678e-06, -2.5779e-06],
        [-6.5565e-06, -5.3793e-06,  1.8366e-06,  ..., -6.0052e-06,
         -1.6317e-06, -2.8610e-06],
        [-6.2734e-06, -5.1409e-06,  1.7583e-06,  ..., -5.7369e-06,
         -1.5572e-06, -2.7344e-06],
        [-1.3411e-05, -1.0997e-05,  3.7551e-06,  ..., -1.2308e-05,
         -3.3379e-06, -5.8413e-06]], device='cuda:0')
Loss: 1.1411374807357788


Running epoch 0, step 346, batch 346
Sampled inputs[:2]: tensor([[    0,   266,  7264,  ...,  3211,   328,   275],
        [    0,  7314,    19,  ...,  8350,   365, 13801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7293e-06, -3.4624e-05, -2.5452e-05,  ..., -6.2148e-06,
         -2.9208e-05, -2.2683e-05],
        [-8.8215e-06, -7.2271e-06,  2.4736e-06,  ..., -8.1062e-06,
         -2.1756e-06, -3.8445e-06],
        [-9.8497e-06, -8.0615e-06,  2.7567e-06,  ..., -9.0450e-06,
         -2.4252e-06, -4.2766e-06],
        [-9.4026e-06, -7.7039e-06,  2.6375e-06,  ..., -8.6278e-06,
         -2.3097e-06, -4.0829e-06],
        [-2.0117e-05, -1.6481e-05,  5.6326e-06,  ..., -1.8507e-05,
         -4.9546e-06, -8.7321e-06]], device='cuda:0')
Loss: 1.1576988697052002


Running epoch 0, step 347, batch 347
Sampled inputs[:2]: tensor([[    0,  1530,    17,  ...,   409,  1611,   895],
        [    0,    12, 30621,  ...,   578,  3126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3798e-07, -3.9944e-05, -3.2181e-05,  ..., -1.9750e-05,
         -4.1483e-05, -4.0412e-05],
        [-1.1727e-05, -9.6262e-06,  3.2969e-06,  ..., -1.0759e-05,
         -2.8722e-06, -5.1036e-06],
        [ 8.2128e-05,  5.1269e-05, -1.6408e-05,  ...,  8.3778e-05,
          2.6462e-06,  2.3732e-05],
        [-1.2532e-05, -1.0297e-05,  3.5278e-06,  ..., -1.1489e-05,
         -3.0585e-06, -5.4389e-06],
        [-2.6822e-05, -2.1994e-05,  7.5325e-06,  ..., -2.4587e-05,
         -6.5565e-06, -1.1623e-05]], device='cuda:0')
Loss: 1.163879632949829


Running epoch 0, step 348, batch 348
Sampled inputs[:2]: tensor([[   0,    9,  278,  ...,  278,  298,  452],
        [   0, 1356,   18,  ...,   31,  333,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7111e-06, -3.6968e-05, -5.3640e-05,  ..., -1.9950e-05,
         -5.6747e-05, -5.8472e-05],
        [-1.4678e-05, -1.2055e-05,  4.1313e-06,  ..., -1.3530e-05,
         -3.5800e-06, -6.3851e-06],
        [ 7.8865e-05,  4.8587e-05, -1.5484e-05,  ...,  8.0724e-05,
          1.8639e-06,  2.2317e-05],
        [ 1.4947e-04,  1.1853e-04, -4.3680e-05,  ...,  1.5558e-04,
          3.9236e-05,  6.5337e-05],
        [-3.3528e-05, -2.7508e-05,  9.4250e-06,  ..., -3.0875e-05,
         -8.1733e-06, -1.4529e-05]], device='cuda:0')
Loss: 1.162066102027893


Running epoch 0, step 349, batch 349
Sampled inputs[:2]: tensor([[   0,  328,  843,  ...,  298,  292,   37],
        [   0,  756,  401,  ..., 8385, 1004,  775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2435e-05, -4.7017e-05, -5.1171e-05,  ..., -7.9331e-07,
         -5.6180e-05, -7.3014e-05],
        [-1.7598e-05, -1.4469e-05,  4.9509e-06,  ..., -1.6242e-05,
         -4.3102e-06, -7.6741e-06],
        [ 7.5602e-05,  4.5890e-05, -1.4571e-05,  ...,  7.7699e-05,
          1.0517e-06,  2.0879e-05],
        [ 1.4633e-04,  1.1592e-04, -4.2798e-05,  ...,  1.5266e-04,
          3.8454e-05,  6.3951e-05],
        [-4.0233e-05, -3.3051e-05,  1.1295e-05,  ..., -3.7074e-05,
         -9.8422e-06, -1.7479e-05]], device='cuda:0')
Loss: 1.1481105089187622


Running epoch 0, step 350, batch 350
Sampled inputs[:2]: tensor([[    0,  1615,   292,  ...,  4824,   292,  9936],
        [    0, 29368,    13,  ...,   376,    88,  3333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6161e-05, -3.8816e-05, -4.3819e-05,  ..., -1.1578e-05,
         -6.1751e-05, -5.9923e-05],
        [-2.0579e-05, -1.6928e-05,  5.7667e-06,  ..., -1.8984e-05,
         -5.0440e-06, -8.9705e-06],
        [ 7.2338e-05,  4.3207e-05, -1.3681e-05,  ...,  7.4689e-05,
          2.4708e-07,  1.9456e-05],
        [ 1.4321e-04,  1.1336e-04, -4.1944e-05,  ...,  1.4978e-04,
          3.7686e-05,  6.2595e-05],
        [-4.6998e-05, -3.8624e-05,  1.3135e-05,  ..., -4.3303e-05,
         -1.1504e-05, -2.0415e-05]], device='cuda:0')
Loss: 1.1803227663040161


Running epoch 0, step 351, batch 351
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,   380,   333,  ...,  8127,   504,   679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2159e-05, -4.0763e-05, -9.9444e-05,  ..., -1.2708e-05,
         -4.4061e-05, -7.8117e-05],
        [-2.3499e-05, -1.9297e-05,  6.6012e-06,  ..., -2.1711e-05,
         -5.7854e-06, -1.0259e-05],
        [ 6.9105e-05,  4.0600e-05, -1.2757e-05,  ...,  7.1679e-05,
         -5.7248e-07,  1.8033e-05],
        [ 1.4010e-04,  1.1084e-04, -4.1054e-05,  ...,  1.4689e-04,
          3.6900e-05,  6.1224e-05],
        [-5.3644e-05, -4.3988e-05,  1.5028e-05,  ..., -4.9472e-05,
         -1.3188e-05, -2.3335e-05]], device='cuda:0')
Loss: 1.1503546237945557
Graident accumulation at epoch 0, step 351, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0223, -0.0202],
        [ 0.0291, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0271,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7993e-05, -1.6538e-05, -5.6095e-05,  ...,  6.4087e-06,
         -5.2226e-05,  5.5820e-06],
        [-2.4052e-05, -1.8186e-05,  4.8648e-06,  ..., -2.1087e-05,
         -5.0094e-06, -1.0318e-05],
        [ 7.8858e-05,  5.9876e-05, -1.4081e-05,  ...,  7.6077e-05,
          1.7669e-05,  3.2075e-05],
        [-1.0534e-05, -7.1425e-06,  6.7102e-07,  ..., -6.5372e-06,
         -1.3223e-06, -4.2916e-06],
        [-5.2753e-05, -3.9840e-05,  1.0565e-05,  ..., -4.5974e-05,
         -1.0538e-05, -2.1997e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1948e-08, 2.8051e-08, 3.5216e-08,  ..., 1.1595e-08, 8.7717e-08,
         1.1461e-08],
        [4.3456e-11, 2.4053e-11, 2.5088e-12,  ..., 2.7813e-11, 1.6764e-12,
         6.1410e-12],
        [5.8398e-10, 3.1846e-10, 1.9785e-11,  ..., 5.1551e-10, 3.1752e-11,
         1.1879e-10],
        [5.3534e-11, 2.9191e-11, 3.0022e-12,  ..., 4.5119e-11, 2.3283e-12,
         9.2068e-12],
        [1.8911e-10, 9.9923e-11, 7.6569e-12,  ..., 1.3441e-10, 7.3001e-12,
         3.0737e-11]], device='cuda:0')
optimizer state dict: 44.0
lr: [1.9018557894170758e-05, 1.9018557894170758e-05]
scheduler_last_epoch: 44


Running epoch 0, step 352, batch 352
Sampled inputs[:2]: tensor([[    0, 18905,  2311,  ..., 10213,   908,   694],
        [    0,  1730,  2068,  ...,   445,  2704,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9236e-06,  9.2954e-06,  8.0112e-06,  ...,  2.2905e-05,
         -1.7466e-05,  4.8609e-05],
        [-2.9504e-06, -2.3842e-06,  8.9034e-07,  ..., -2.6971e-06,
         -6.6683e-07, -1.2890e-06],
        [-3.2783e-06, -2.6673e-06,  9.9093e-07,  ..., -3.0100e-06,
         -7.4506e-07, -1.4380e-06],
        [-3.1441e-06, -2.5481e-06,  9.4995e-07,  ..., -2.8759e-06,
         -7.0781e-07, -1.3709e-06],
        [-6.7353e-06, -5.4538e-06,  2.0266e-06,  ..., -6.1691e-06,
         -1.5274e-06, -2.9355e-06]], device='cuda:0')
Loss: 1.1552687883377075


Running epoch 0, step 353, batch 353
Sampled inputs[:2]: tensor([[    0,  3440,  5745,  ...,   360,  4998,   654],
        [    0, 18787, 27117,  ...,   287, 16139,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1926e-05,  1.4522e-05,  7.7937e-07,  ...,  4.7013e-05,
         -3.2359e-05,  6.2062e-05],
        [-5.8860e-06, -4.7386e-06,  1.7509e-06,  ..., -5.3644e-06,
         -1.3337e-06, -2.5928e-06],
        [-6.5714e-06, -5.3197e-06,  1.9595e-06,  ..., -6.0052e-06,
         -1.4938e-06, -2.9057e-06],
        [-6.3032e-06, -5.0962e-06,  1.8775e-06,  ..., -5.7369e-06,
         -1.4268e-06, -2.7716e-06],
        [-1.3441e-05, -1.0878e-05,  3.9935e-06,  ..., -1.2279e-05,
         -3.0622e-06, -5.9158e-06]], device='cuda:0')
Loss: 1.1294171810150146


Running epoch 0, step 354, batch 354
Sampled inputs[:2]: tensor([[    0,  7935,  6521,  ..., 41312,   365,   806],
        [    0,   271,   259,  ...,  1345,   352,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4308e-05,  8.0855e-07, -2.5257e-05,  ...,  4.8228e-05,
         -4.5573e-05,  6.9502e-05],
        [-8.8215e-06, -7.1079e-06,  2.6189e-06,  ..., -8.0466e-06,
         -2.0079e-06, -3.8892e-06],
        [-9.8199e-06, -7.9423e-06,  2.9206e-06,  ..., -8.9705e-06,
         -2.2389e-06, -4.3362e-06],
        [-9.4175e-06, -7.6145e-06,  2.8014e-06,  ..., -8.5831e-06,
         -2.1383e-06, -4.1500e-06],
        [-2.0146e-05, -1.6272e-05,  5.9754e-06,  ..., -1.8418e-05,
         -4.6045e-06, -8.8811e-06]], device='cuda:0')
Loss: 1.1195553541183472


Running epoch 0, step 355, batch 355
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0, 24440,  1918,  ...,   769,  1254,   596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4623e-05, -3.1977e-06, -6.5947e-05,  ...,  4.8228e-05,
         -6.8303e-05,  6.7007e-05],
        [-1.1742e-05, -9.4920e-06,  3.5018e-06,  ..., -1.0774e-05,
         -2.6785e-06, -5.2005e-06],
        [-1.3039e-05, -1.0580e-05,  3.8967e-06,  ..., -1.1995e-05,
         -2.9802e-06, -5.7891e-06],
        [-1.2517e-05, -1.0148e-05,  3.7402e-06,  ..., -1.1489e-05,
         -2.8498e-06, -5.5432e-06],
        [-2.6762e-05, -2.1666e-05,  7.9870e-06,  ..., -2.4617e-05,
         -6.1318e-06, -1.1861e-05]], device='cuda:0')
Loss: 1.151046872138977


Running epoch 0, step 356, batch 356
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,  3398,   271,  ...,    13,  1581, 13600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7248e-07, -3.2470e-05, -9.2225e-05,  ...,  5.3845e-05,
         -7.7385e-05,  8.1017e-05],
        [-1.4663e-05, -1.1876e-05,  4.3884e-06,  ..., -1.3441e-05,
         -3.3453e-06, -6.4820e-06],
        [ 7.3149e-05,  6.9799e-05,  1.9595e-06,  ...,  6.3981e-05,
          2.0512e-05,  3.9930e-05],
        [-1.5646e-05, -1.2711e-05,  4.6939e-06,  ..., -1.4365e-05,
         -3.5614e-06, -6.9141e-06],
        [-3.3498e-05, -2.7150e-05,  1.0028e-05,  ..., -3.0786e-05,
         -7.6666e-06, -1.4812e-05]], device='cuda:0')
Loss: 1.152438998222351


Running epoch 0, step 357, batch 357
Sampled inputs[:2]: tensor([[   0, 1824,   13,  ...,  266, 5940,   19],
        [   0,  259, 6887,  ..., 1400,  292,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9602e-06, -4.1188e-05, -1.0372e-04,  ...,  4.4579e-05,
         -8.3552e-05,  7.5049e-05],
        [-1.7598e-05, -1.4260e-05,  5.2676e-06,  ..., -1.6153e-05,
         -4.0121e-06, -7.7635e-06],
        [ 6.9916e-05,  6.7162e-05,  2.9355e-06,  ...,  6.0986e-05,
          1.9775e-05,  3.8514e-05],
        [-1.8775e-05, -1.5259e-05,  5.6326e-06,  ..., -1.7256e-05,
         -4.2729e-06, -8.2850e-06],
        [-4.0144e-05, -3.2574e-05,  1.2025e-05,  ..., -3.6925e-05,
         -9.1791e-06, -1.7717e-05]], device='cuda:0')
Loss: 1.151658296585083


Running epoch 0, step 358, batch 358
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  437, 1916,  ...,   13, 1303, 2708]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9254e-05, -2.9345e-05, -1.1207e-04,  ...,  4.4217e-05,
         -4.9214e-05,  1.0126e-04],
        [-2.0549e-05, -1.6659e-05,  6.1505e-06,  ..., -1.8880e-05,
         -4.6864e-06, -9.0599e-06],
        [ 6.6652e-05,  6.4524e-05,  3.9116e-06,  ...,  5.7976e-05,
          1.9033e-05,  3.7084e-05],
        [-2.1920e-05, -1.7807e-05,  6.5751e-06,  ..., -2.0146e-05,
         -4.9882e-06, -9.6560e-06],
        [-4.6879e-05, -3.8028e-05,  1.4037e-05,  ..., -4.3124e-05,
         -1.0714e-05, -2.0653e-05]], device='cuda:0')
Loss: 1.1523419618606567


Running epoch 0, step 359, batch 359
Sampled inputs[:2]: tensor([[    0, 22599,  1336,  ...,   729,   923,    13],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5673e-06, -4.0052e-05, -9.3652e-05,  ...,  3.0769e-05,
         -2.9337e-05,  1.2741e-04],
        [-2.3469e-05, -1.9029e-05,  7.0184e-06,  ..., -2.1607e-05,
         -5.3383e-06, -1.0386e-05],
        [ 6.3419e-05,  6.1917e-05,  4.8727e-06,  ...,  5.4966e-05,
          1.8315e-05,  3.5616e-05],
        [-2.5034e-05, -2.0325e-05,  7.5027e-06,  ..., -2.3037e-05,
         -5.6811e-06, -1.1072e-05],
        [-5.3585e-05, -4.3422e-05,  1.6019e-05,  ..., -4.9353e-05,
         -1.2204e-05, -2.3693e-05]], device='cuda:0')
Loss: 1.1673883199691772
Graident accumulation at epoch 0, step 359, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0150,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0158, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.3650e-05, -1.8889e-05, -5.9850e-05,  ...,  8.8447e-06,
         -4.9937e-05,  1.7765e-05],
        [-2.3994e-05, -1.8270e-05,  5.0802e-06,  ..., -2.1139e-05,
         -5.0423e-06, -1.0325e-05],
        [ 7.7314e-05,  6.0080e-05, -1.2185e-05,  ...,  7.3966e-05,
          1.7733e-05,  3.2429e-05],
        [-1.1984e-05, -8.4608e-06,  1.3542e-06,  ..., -8.1872e-06,
         -1.7581e-06, -4.9696e-06],
        [-5.2836e-05, -4.0198e-05,  1.1110e-05,  ..., -4.6312e-05,
         -1.0705e-05, -2.2167e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1906e-08, 2.8025e-08, 3.5190e-08,  ..., 1.1584e-08, 8.7630e-08,
         1.1466e-08],
        [4.3964e-11, 2.4391e-11, 2.5555e-12,  ..., 2.8252e-11, 1.7033e-12,
         6.2427e-12],
        [5.8742e-10, 3.2198e-10, 1.9789e-11,  ..., 5.1802e-10, 3.2055e-11,
         1.1994e-10],
        [5.4107e-11, 2.9575e-11, 3.0555e-12,  ..., 4.5605e-11, 2.3582e-12,
         9.3202e-12],
        [1.9179e-10, 1.0171e-10, 7.9058e-12,  ..., 1.3671e-10, 7.4418e-12,
         3.1268e-11]], device='cuda:0')
optimizer state dict: 45.0
lr: [1.896446322196428e-05, 1.896446322196428e-05]
scheduler_last_epoch: 45


Running epoch 0, step 360, batch 360
Sampled inputs[:2]: tensor([[    0, 14349,   278,  ...,   365,   847,   300],
        [    0,   221,   380,  ...,  1590,   997,  2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5957e-06,  1.5589e-05, -7.0524e-05,  ...,  3.6035e-05,
         -2.2891e-05, -2.5312e-05],
        [-2.8908e-06, -2.3246e-06,  9.0152e-07,  ..., -2.6822e-06,
         -5.9232e-07, -1.3411e-06],
        [-3.2187e-06, -2.5928e-06,  1.0058e-06,  ..., -2.9951e-06,
         -6.6310e-07, -1.4901e-06],
        [-3.1292e-06, -2.5183e-06,  9.7603e-07,  ..., -2.9057e-06,
         -6.4075e-07, -1.4454e-06],
        [-6.6161e-06, -5.3048e-06,  2.0564e-06,  ..., -6.1393e-06,
         -1.3560e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.1280587911605835


Running epoch 0, step 361, batch 361
Sampled inputs[:2]: tensor([[   0, 1871,  518,  ...,  271,  259, 1110],
        [   0,  768, 2351,  ..., 3768,  401, 2463]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8776e-05,  1.7368e-05, -1.0070e-04,  ..., -4.4208e-06,
         -9.5615e-06, -3.3987e-05],
        [-5.8115e-06, -4.6492e-06,  1.8030e-06,  ..., -5.3793e-06,
         -1.2107e-06, -2.6897e-06],
        [-6.4522e-06, -5.1558e-06,  1.9968e-06,  ..., -5.9754e-06,
         -1.3411e-06, -2.9802e-06],
        [-6.2585e-06, -5.0068e-06,  1.9446e-06,  ..., -5.7966e-06,
         -1.2964e-06, -2.8908e-06],
        [-1.3322e-05, -1.0610e-05,  4.1127e-06,  ..., -1.2308e-05,
         -2.7642e-06, -6.1393e-06]], device='cuda:0')
Loss: 1.1598474979400635


Running epoch 0, step 362, batch 362
Sampled inputs[:2]: tensor([[    0,  7120,   344,  ...,  6273,    52, 22639],
        [    0, 36122,  1085,  ...,  6231,     9,  7794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1487e-05,  2.5959e-05, -1.0375e-04,  ..., -2.0740e-06,
         -2.7905e-05, -3.3987e-05],
        [-8.6874e-06, -6.9439e-06,  2.7120e-06,  ..., -8.0615e-06,
         -1.8030e-06, -4.0084e-06],
        [-9.6560e-06, -7.7188e-06,  3.0175e-06,  ..., -8.9705e-06,
         -2.0005e-06, -4.4554e-06],
        [-9.3728e-06, -7.4953e-06,  2.9355e-06,  ..., -8.7023e-06,
         -1.9334e-06, -4.3213e-06],
        [-1.9908e-05, -1.5885e-05,  6.1989e-06,  ..., -1.8477e-05,
         -4.1202e-06, -9.1642e-06]], device='cuda:0')
Loss: 1.163915991783142


Running epoch 0, step 363, batch 363
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0,   360,   259,  ...,    14,   381,  1371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9578e-05,  3.2220e-05, -9.8254e-05,  ..., -1.6520e-05,
         -3.5982e-05, -4.1563e-05],
        [-1.1608e-05, -9.2685e-06,  3.6247e-06,  ..., -1.0744e-05,
         -2.3879e-06, -5.3048e-06],
        [-1.2904e-05, -1.0297e-05,  4.0308e-06,  ..., -1.1966e-05,
         -2.6487e-06, -5.9009e-06],
        [-1.2532e-05, -1.0014e-05,  3.9265e-06,  ..., -1.1608e-05,
         -2.5630e-06, -5.7295e-06],
        [-2.6554e-05, -2.1189e-05,  8.2701e-06,  ..., -2.4587e-05,
         -5.4464e-06, -1.2115e-05]], device='cuda:0')
Loss: 1.1533987522125244


Running epoch 0, step 364, batch 364
Sampled inputs[:2]: tensor([[    0,    13, 32291,  ...,  3740,  3616,  1274],
        [    0,  1356,   634,  ...,  6604,   634,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6941e-05,  3.0267e-05, -1.0133e-04,  ..., -3.2369e-05,
         -8.9982e-05, -6.6523e-05],
        [-1.4499e-05, -1.1593e-05,  4.5449e-06,  ..., -1.3411e-05,
         -2.9877e-06, -6.6236e-06],
        [-1.6138e-05, -1.2904e-05,  5.0589e-06,  ..., -1.4961e-05,
         -3.3192e-06, -7.3761e-06],
        [-1.5676e-05, -1.2547e-05,  4.9323e-06,  ..., -1.4514e-05,
         -3.2112e-06, -7.1675e-06],
        [-3.3170e-05, -2.6494e-05,  1.0371e-05,  ..., -3.0696e-05,
         -6.8173e-06, -1.5125e-05]], device='cuda:0')
Loss: 1.1343291997909546


Running epoch 0, step 365, batch 365
Sampled inputs[:2]: tensor([[   0,  271, 8278,  ...,  271, 8278, 3560],
        [   0, 1412,   35,  ..., 6077,  298, 1826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2922e-05,  2.4953e-05, -1.0953e-04,  ..., -5.5739e-05,
         -8.9982e-05, -4.4839e-05],
        [-1.7434e-05, -1.3977e-05,  5.4352e-06,  ..., -1.6123e-05,
         -3.6359e-06, -7.9721e-06],
        [-1.9401e-05, -1.5557e-05,  6.0499e-06,  ..., -1.7986e-05,
         -4.0382e-06, -8.8736e-06],
        [-1.8805e-05, -1.5080e-05,  5.8822e-06,  ..., -1.7405e-05,
         -3.8967e-06, -8.5980e-06],
        [-3.9846e-05, -3.1918e-05,  1.2398e-05,  ..., -3.6865e-05,
         -8.2850e-06, -1.8179e-05]], device='cuda:0')
Loss: 1.1395143270492554


Running epoch 0, step 366, batch 366
Sampled inputs[:2]: tensor([[   0,  391, 9095,  ...,  417,  199, 2038],
        [   0, 1927,  287,  ..., 1027,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7306e-05, -1.5167e-06, -1.2300e-04,  ..., -4.1647e-05,
         -1.0679e-04, -3.9029e-05],
        [-2.0415e-05, -1.6347e-05,  6.3255e-06,  ..., -1.8850e-05,
         -4.2170e-06, -9.3207e-06],
        [-2.2709e-05, -1.8194e-05,  7.0482e-06,  ..., -2.1026e-05,
         -4.6827e-06, -1.0371e-05],
        [ 7.0840e-05,  4.9249e-05, -1.9509e-05,  ...,  7.1802e-05,
          4.1762e-06,  3.8533e-05],
        [-4.6670e-05, -3.7342e-05,  1.4439e-05,  ..., -4.3124e-05,
         -9.6187e-06, -2.1264e-05]], device='cuda:0')
Loss: 1.1469579935073853


Running epoch 0, step 367, batch 367
Sampled inputs[:2]: tensor([[   0, 6203,  352,  ...,  266, 3437,  287],
        [   0,  344, 3693,  ..., 1782, 3679,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1325e-05, -1.7712e-05, -1.3203e-04,  ..., -2.4124e-05,
         -1.1955e-04, -3.4680e-05],
        [-2.3335e-05, -1.8686e-05,  7.2345e-06,  ..., -2.1502e-05,
         -4.7982e-06, -1.0647e-05],
        [-2.5958e-05, -2.0802e-05,  8.0615e-06,  ..., -2.3991e-05,
         -5.3309e-06, -1.1846e-05],
        [ 6.7696e-05,  4.6731e-05, -1.8526e-05,  ...,  6.8941e-05,
          3.5504e-06,  3.7103e-05],
        [-5.3346e-05, -4.2707e-05,  1.6510e-05,  ..., -4.9204e-05,
         -1.0952e-05, -2.4304e-05]], device='cuda:0')
Loss: 1.1467857360839844
Graident accumulation at epoch 0, step 367, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0051, -0.0149,  0.0036,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0077,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0166,  0.0146, -0.0272,  ...,  0.0281, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0418e-05, -1.8771e-05, -6.7068e-05,  ...,  5.5479e-06,
         -5.6898e-05,  1.2521e-05],
        [-2.3928e-05, -1.8312e-05,  5.2956e-06,  ..., -2.1175e-05,
         -5.0179e-06, -1.0357e-05],
        [ 6.6987e-05,  5.1992e-05, -1.0161e-05,  ...,  6.4170e-05,
          1.5427e-05,  2.8002e-05],
        [-4.0163e-06, -2.9416e-06, -6.3382e-07,  ..., -4.7441e-07,
         -1.2273e-06, -7.6237e-07],
        [-5.2887e-05, -4.0449e-05,  1.1650e-05,  ..., -4.6601e-05,
         -1.0729e-05, -2.2380e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1864e-08, 2.7997e-08, 3.5172e-08,  ..., 1.1573e-08, 8.7557e-08,
         1.1456e-08],
        [4.4464e-11, 2.4716e-11, 2.6053e-12,  ..., 2.8686e-11, 1.7246e-12,
         6.3498e-12],
        [5.8751e-10, 3.2209e-10, 1.9834e-11,  ..., 5.1808e-10, 3.2052e-11,
         1.1996e-10],
        [5.8636e-11, 3.1729e-11, 3.3957e-12,  ..., 5.0312e-11, 2.3684e-12,
         1.0688e-11],
        [1.9445e-10, 1.0343e-10, 8.1705e-12,  ..., 1.3900e-10, 7.5543e-12,
         3.1827e-11]], device='cuda:0')
optimizer state dict: 46.0
lr: [1.890899870152558e-05, 1.890899870152558e-05]
scheduler_last_epoch: 46


Running epoch 0, step 368, batch 368
Sampled inputs[:2]: tensor([[    0,  1188,    12,  ...,   292, 23032,   689],
        [    0,   380,   333,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0356e-05, -2.1332e-06,  1.7338e-05,  ..., -2.3819e-05,
         -1.0160e-05,  3.0053e-05],
        [-2.9355e-06, -2.2799e-06,  9.2015e-07,  ..., -2.6971e-06,
         -5.4389e-07, -1.3560e-06],
        [-3.2485e-06, -2.5332e-06,  1.0207e-06,  ..., -2.9951e-06,
         -6.0350e-07, -1.4976e-06],
        [-3.1292e-06, -2.4438e-06,  9.8348e-07,  ..., -2.8759e-06,
         -5.7742e-07, -1.4380e-06],
        [-6.6459e-06, -5.1856e-06,  2.0862e-06,  ..., -6.1095e-06,
         -1.2368e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.1495956182479858


Running epoch 0, step 369, batch 369
Sampled inputs[:2]: tensor([[    0,   521,   486,  ...,   278, 25182,   271],
        [    0,     5,  4413,  ...,  9205, 16744,   775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1042e-06,  2.4119e-05,  1.5989e-05,  ..., -4.0300e-05,
         -1.1086e-05,  4.2487e-05],
        [-5.8562e-06, -4.5449e-06,  1.8813e-06,  ..., -5.4091e-06,
         -1.1139e-06, -2.7418e-06],
        [-6.4969e-06, -5.0515e-06,  2.0862e-06,  ..., -6.0052e-06,
         -1.2368e-06, -3.0324e-06],
        [-6.2883e-06, -4.8876e-06,  2.0191e-06,  ..., -5.7966e-06,
         -1.1884e-06, -2.9281e-06],
        [-1.3292e-05, -1.0341e-05,  4.2617e-06,  ..., -1.2279e-05,
         -2.5332e-06, -6.1840e-06]], device='cuda:0')
Loss: 1.1504918336868286


Running epoch 0, step 370, batch 370
Sampled inputs[:2]: tensor([[   0,   17, 3737,  ...,  298,  396,  221],
        [   0,  266, 6737,  ..., 2409,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6997e-05,  6.4767e-06,  8.8134e-06,  ..., -4.8988e-05,
          1.0682e-05,  3.0837e-05],
        [-8.8066e-06, -6.8843e-06,  2.8312e-06,  ..., -8.1062e-06,
         -1.6540e-06, -4.1276e-06],
        [-9.7603e-06, -7.6443e-06,  3.1441e-06,  ..., -9.0003e-06,
         -1.8328e-06, -4.5747e-06],
        [-9.4622e-06, -7.3910e-06,  3.0398e-06,  ..., -8.7023e-06,
         -1.7658e-06, -4.4182e-06],
        [-1.9938e-05, -1.5616e-05,  6.4075e-06,  ..., -1.8388e-05,
         -3.7551e-06, -9.3281e-06]], device='cuda:0')
Loss: 1.1320183277130127


Running epoch 0, step 371, batch 371
Sampled inputs[:2]: tensor([[   0,  266, 5528,  ...,  685,  266, 1231],
        [   0, 1250, 1797,  ...,  266, 1417,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5471e-05, -7.7301e-06, -7.5358e-06,  ..., -4.8501e-05,
          1.0637e-05,  7.8823e-06],
        [-1.1772e-05, -9.1791e-06,  3.7849e-06,  ..., -1.0818e-05,
         -2.2240e-06, -5.5134e-06],
        [-1.3083e-05, -1.0222e-05,  4.2096e-06,  ..., -1.2040e-05,
         -2.4736e-06, -6.1318e-06],
        [-1.2666e-05, -9.8646e-06,  4.0680e-06,  ..., -1.1623e-05,
         -2.3805e-06, -5.9158e-06],
        [-2.6673e-05, -2.0802e-05,  8.5682e-06,  ..., -2.4527e-05,
         -5.0440e-06, -1.2472e-05]], device='cuda:0')
Loss: 1.133798599243164


Running epoch 0, step 372, batch 372
Sampled inputs[:2]: tensor([[   0,  271, 3616,  ...,   12, 1348, 5037],
        [   0,  278,  490,  ...,  434,  472,  346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0353e-05, -2.6346e-05, -1.9497e-05,  ..., -4.8107e-05,
          4.5241e-05,  1.1380e-05],
        [-1.4707e-05, -1.1459e-05,  4.7237e-06,  ..., -1.3530e-05,
         -2.7902e-06, -6.8918e-06],
        [-1.6332e-05, -1.2755e-05,  5.2452e-06,  ..., -1.5050e-05,
         -3.0994e-06, -7.6592e-06],
        [-1.5825e-05, -1.2323e-05,  5.0738e-06,  ..., -1.4544e-05,
         -2.9877e-06, -7.3984e-06],
        [-3.3349e-05, -2.5988e-05,  1.0699e-05,  ..., -3.0696e-05,
         -6.3330e-06, -1.5602e-05]], device='cuda:0')
Loss: 1.1627274751663208


Running epoch 0, step 373, batch 373
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,  6660, 13165,  ...,   380,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4129e-05, -2.2803e-05, -5.1738e-06,  ..., -7.5088e-05,
          4.8487e-05,  1.8958e-05],
        [-1.7658e-05, -1.3754e-05,  5.6773e-06,  ..., -1.6227e-05,
         -3.3416e-06, -8.2627e-06],
        [-1.9610e-05, -1.5303e-05,  6.3032e-06,  ..., -1.8045e-05,
         -3.7104e-06, -9.1791e-06],
        [-1.9014e-05, -1.4797e-05,  6.1095e-06,  ..., -1.7464e-05,
         -3.5837e-06, -8.8811e-06],
        [-4.0084e-05, -3.1203e-05,  1.2860e-05,  ..., -3.6836e-05,
         -7.5921e-06, -1.8716e-05]], device='cuda:0')
Loss: 1.1541346311569214


Running epoch 0, step 374, batch 374
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0,   199,   769,  ..., 12038, 15317,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3125e-05, -1.1084e-05, -2.4762e-05,  ..., -1.0990e-04,
          8.4398e-05, -1.6188e-05],
        [-2.0593e-05, -1.6049e-05,  6.6161e-06,  ..., -1.8939e-05,
         -3.8929e-06, -9.6187e-06],
        [-2.2888e-05, -1.7866e-05,  7.3537e-06,  ..., -2.1085e-05,
         -4.3213e-06, -1.0692e-05],
        [-2.2203e-05, -1.7270e-05,  7.1302e-06,  ..., -2.0400e-05,
         -4.1761e-06, -1.0349e-05],
        [-4.6730e-05, -3.6389e-05,  1.4991e-05,  ..., -4.2975e-05,
         -8.8364e-06, -2.1785e-05]], device='cuda:0')
Loss: 1.1542844772338867


Running epoch 0, step 375, batch 375
Sampled inputs[:2]: tensor([[    0, 27342,    17,  ...,  5125,  3244,   287],
        [    0,  2319,    30,  ...,   508,  6703,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0847e-04,  1.8970e-05, -3.5294e-05,  ..., -1.1902e-04,
          6.9484e-05,  2.8454e-05],
        [-2.3544e-05, -1.8358e-05,  7.5772e-06,  ..., -2.1681e-05,
         -4.4666e-06, -1.0990e-05],
        [-2.6152e-05, -2.0429e-05,  8.4192e-06,  ..., -2.4125e-05,
         -4.9546e-06, -1.2212e-05],
        [-2.5362e-05, -1.9744e-05,  8.1584e-06,  ..., -2.3335e-05,
         -4.7870e-06, -1.1817e-05],
        [-5.3376e-05, -4.1574e-05,  1.7151e-05,  ..., -4.9144e-05,
         -1.0125e-05, -2.4870e-05]], device='cuda:0')
Loss: 1.141329288482666
Graident accumulation at epoch 0, step 375, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0202],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7223e-05, -1.4997e-05, -6.3890e-05,  ..., -6.9086e-06,
         -4.4260e-05,  1.4114e-05],
        [-2.3890e-05, -1.8317e-05,  5.5238e-06,  ..., -2.1226e-05,
         -4.9628e-06, -1.0421e-05],
        [ 5.7673e-05,  4.4750e-05, -8.3026e-06,  ...,  5.5341e-05,
          1.3389e-05,  2.3980e-05],
        [-6.1509e-06, -4.6218e-06,  2.4540e-07,  ..., -2.7605e-06,
         -1.5833e-06, -1.8678e-06],
        [-5.2936e-05, -4.0561e-05,  1.2201e-05,  ..., -4.6855e-05,
         -1.0669e-05, -2.2629e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1834e-08, 2.7969e-08, 3.5138e-08,  ..., 1.1576e-08, 8.7474e-08,
         1.1445e-08],
        [4.4974e-11, 2.5029e-11, 2.6601e-12,  ..., 2.9127e-11, 1.7428e-12,
         6.4642e-12],
        [5.8760e-10, 3.2218e-10, 1.9885e-11,  ..., 5.1814e-10, 3.2044e-11,
         1.1999e-10],
        [5.9220e-11, 3.2087e-11, 3.4588e-12,  ..., 5.0806e-11, 2.3890e-12,
         1.0816e-11],
        [1.9710e-10, 1.0506e-10, 8.4565e-12,  ..., 1.4127e-10, 7.6493e-12,
         3.2414e-11]], device='cuda:0')
optimizer state dict: 47.0
lr: [1.885217280831754e-05, 1.885217280831754e-05]
scheduler_last_epoch: 47


Running epoch 0, step 376, batch 376
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  292,   86,   16],
        [   0,   12,  638,  ...,  374,  221,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4786e-05, -1.6506e-05, -1.0522e-05,  ...,  0.0000e+00,
          1.5536e-05,  2.5009e-05],
        [-2.9653e-06, -2.2352e-06,  9.5367e-07,  ..., -2.7269e-06,
         -5.2899e-07, -1.4231e-06],
        [-3.2932e-06, -2.4736e-06,  1.0654e-06,  ..., -3.0249e-06,
         -5.8860e-07, -1.5795e-06],
        [-3.1888e-06, -2.3991e-06,  1.0356e-06,  ..., -2.9355e-06,
         -5.6997e-07, -1.5348e-06],
        [-6.7055e-06, -5.0366e-06,  2.1607e-06,  ..., -6.1691e-06,
         -1.1995e-06, -3.2187e-06]], device='cuda:0')
Loss: 1.1334065198898315


Running epoch 0, step 377, batch 377
Sampled inputs[:2]: tensor([[   0, 1561,   14,  ..., 4433,  352, 1561],
        [   0,  298,  696,  ..., 3502,  287, 1047]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0626e-07,  1.3267e-05,  6.3044e-06,  ...,  4.4319e-06,
          1.3676e-05,  5.1796e-05],
        [-5.9456e-06, -4.4554e-06,  1.9297e-06,  ..., -5.4240e-06,
         -1.0729e-06, -2.8759e-06],
        [-6.6012e-06, -4.9472e-06,  2.1532e-06,  ..., -6.0350e-06,
         -1.1884e-06, -3.1963e-06],
        [-6.3777e-06, -4.7833e-06,  2.0787e-06,  ..., -5.8264e-06,
         -1.1474e-06, -3.0920e-06],
        [-1.3411e-05, -1.0043e-05,  4.3511e-06,  ..., -1.2249e-05,
         -2.4214e-06, -6.4969e-06]], device='cuda:0')
Loss: 1.1389902830123901


Running epoch 0, step 378, batch 378
Sampled inputs[:2]: tensor([[   0, 2241, 8274,  ...,  908, 1811,  278],
        [   0, 1336,  278,  ...,  266, 3269,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5966e-05,  5.4209e-06, -4.1228e-05,  ...,  1.2425e-05,
          4.8121e-05,  5.0192e-05],
        [-8.9258e-06, -6.6906e-06,  2.9132e-06,  ..., -8.1062e-06,
         -1.6131e-06, -4.2841e-06],
        [-9.9540e-06, -7.4655e-06,  3.2559e-06,  ..., -9.0450e-06,
         -1.7919e-06, -4.7758e-06],
        [-9.6262e-06, -7.2122e-06,  3.1516e-06,  ..., -8.7470e-06,
         -1.7285e-06, -4.6194e-06],
        [-2.0146e-05, -1.5080e-05,  6.5714e-06,  ..., -1.8299e-05,
         -3.6359e-06, -9.6560e-06]], device='cuda:0')
Loss: 1.1414624452590942


Running epoch 0, step 379, batch 379
Sampled inputs[:2]: tensor([[    0,  8023,  1309,  ...,  3370,   266, 14988],
        [    0, 29258,   765,  ...,  4196,    19,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8324e-05, -1.1598e-05, -6.0567e-05,  ..., -7.0415e-06,
          4.9398e-05,  5.5756e-05],
        [-1.1891e-05, -8.9556e-06,  3.8967e-06,  ..., -1.0803e-05,
         -2.0936e-06, -5.6699e-06],
        [-1.3262e-05, -9.9838e-06,  4.3511e-06,  ..., -1.2055e-05,
         -2.3283e-06, -6.3181e-06],
        [-1.2830e-05, -9.6560e-06,  4.2170e-06,  ..., -1.1653e-05,
         -2.2464e-06, -6.1095e-06],
        [-2.6822e-05, -2.0146e-05,  8.7768e-06,  ..., -2.4348e-05,
         -4.7162e-06, -1.2770e-05]], device='cuda:0')
Loss: 1.1304078102111816


Running epoch 0, step 380, batch 380
Sampled inputs[:2]: tensor([[   0,   14,  417,  ...,   43,  503,   67],
        [   0,  342,  516,  ...,   12,  729, 3701]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2985e-05, -3.1260e-05, -5.4508e-05,  ..., -4.4999e-06,
          6.2613e-05,  3.1609e-05],
        [-1.4842e-05, -1.1221e-05,  4.8578e-06,  ..., -1.3515e-05,
         -2.6152e-06, -7.0781e-06],
        [-1.6540e-05, -1.2487e-05,  5.4166e-06,  ..., -1.5050e-05,
         -2.9057e-06, -7.8827e-06],
        [-1.6019e-05, -1.2100e-05,  5.2601e-06,  ..., -1.4573e-05,
         -2.8089e-06, -7.6294e-06],
        [-3.3498e-05, -2.5243e-05,  1.0952e-05,  ..., -3.0428e-05,
         -5.8934e-06, -1.5944e-05]], device='cuda:0')
Loss: 1.1395909786224365


Running epoch 0, step 381, batch 381
Sampled inputs[:2]: tensor([[    0, 43788,    12,  ...,    12,  6288,   391],
        [    0, 35449,   824,  ...,   278, 30449,  3659]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1161e-05, -3.8984e-05, -6.7998e-05,  ...,  4.0408e-06,
          6.1762e-05,  2.6265e-05],
        [-1.7807e-05, -1.3471e-05,  5.8338e-06,  ..., -1.6198e-05,
         -3.1516e-06, -8.5160e-06],
        [-1.9848e-05, -1.4991e-05,  6.5044e-06,  ..., -1.8045e-05,
         -3.5018e-06, -9.4846e-06],
        [-1.9222e-05, -1.4529e-05,  6.3181e-06,  ..., -1.7479e-05,
         -3.3863e-06, -9.1791e-06],
        [-4.0203e-05, -3.0309e-05,  1.3158e-05,  ..., -3.6508e-05,
         -7.1004e-06, -1.9178e-05]], device='cuda:0')
Loss: 1.1378291845321655


Running epoch 0, step 382, batch 382
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ...,  287, 3121,  292],
        [   0, 1932,  278,  ...,  609,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5098e-06, -7.0310e-06, -7.0721e-05,  ...,  2.5931e-05,
          6.6793e-05,  1.6285e-05],
        [-2.0757e-05, -1.5706e-05,  6.8024e-06,  ..., -1.8910e-05,
         -3.6769e-06, -9.9540e-06],
        [-2.3112e-05, -1.7479e-05,  7.5847e-06,  ..., -2.1040e-05,
         -4.0829e-06, -1.1079e-05],
        [-2.2396e-05, -1.6943e-05,  7.3686e-06,  ..., -2.0385e-05,
         -3.9488e-06, -1.0721e-05],
        [-4.6819e-05, -3.5346e-05,  1.5348e-05,  ..., -4.2588e-05,
         -8.2776e-06, -2.2396e-05]], device='cuda:0')
Loss: 1.15341317653656


Running epoch 0, step 383, batch 383
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,  1197, 10640,  ...,  2405,   437,  5880]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9348e-05, -2.7252e-05, -6.7204e-05,  ...,  5.6391e-06,
          4.5254e-05,  1.1274e-05],
        [-2.3693e-05, -1.7941e-05,  7.7859e-06,  ..., -2.1592e-05,
         -4.1798e-06, -1.1340e-05],
        [-2.6390e-05, -1.9968e-05,  8.6799e-06,  ..., -2.4036e-05,
         -4.6417e-06, -1.2621e-05],
        [-2.5615e-05, -1.9386e-05,  8.4490e-06,  ..., -2.3320e-05,
         -4.4964e-06, -1.2234e-05],
        [-5.3465e-05, -4.0412e-05,  1.7583e-05,  ..., -4.8667e-05,
         -9.4175e-06, -2.5526e-05]], device='cuda:0')
Loss: 1.1265592575073242
Graident accumulation at epoch 0, step 383, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0030,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0095, -0.0022, -0.0340],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.8566e-05, -1.6223e-05, -6.4222e-05,  ..., -5.6539e-06,
         -3.5309e-05,  1.3830e-05],
        [-2.3870e-05, -1.8279e-05,  5.7500e-06,  ..., -2.1262e-05,
         -4.8845e-06, -1.0513e-05],
        [ 4.9267e-05,  3.8278e-05, -6.6043e-06,  ...,  4.7403e-05,
          1.1586e-05,  2.0320e-05],
        [-8.0973e-06, -6.0983e-06,  1.0658e-06,  ..., -4.8165e-06,
         -1.8746e-06, -2.9044e-06],
        [-5.2989e-05, -4.0546e-05,  1.2739e-05,  ..., -4.7036e-05,
         -1.0544e-05, -2.2919e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1793e-08, 2.7942e-08, 3.5108e-08,  ..., 1.1564e-08, 8.7389e-08,
         1.1434e-08],
        [4.5490e-11, 2.5325e-11, 2.7181e-12,  ..., 2.9564e-11, 1.7585e-12,
         6.5864e-12],
        [5.8771e-10, 3.2226e-10, 1.9940e-11,  ..., 5.1820e-10, 3.2034e-11,
         1.2003e-10],
        [5.9817e-11, 3.2431e-11, 3.5268e-12,  ..., 5.1299e-11, 2.4068e-12,
         1.0955e-11],
        [1.9976e-10, 1.0658e-10, 8.7572e-12,  ..., 1.4350e-10, 7.7303e-12,
         3.3033e-11]], device='cuda:0')
optimizer state dict: 48.0
lr: [1.8793994225832682e-05, 1.8793994225832682e-05]
scheduler_last_epoch: 48


Running epoch 0, step 384, batch 384
Sampled inputs[:2]: tensor([[    0,  1855,    14,  ...,    12,   287, 16479],
        [    0,  7230,    13,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7078e-05, -9.5736e-06,  9.9355e-06,  ..., -2.8378e-05,
         -4.3481e-06, -8.2224e-06],
        [-3.0100e-06, -2.2054e-06,  1.0207e-06,  ..., -2.6971e-06,
         -4.7311e-07, -1.4752e-06],
        [-3.3230e-06, -2.4289e-06,  1.1250e-06,  ..., -2.9653e-06,
         -5.1782e-07, -1.6242e-06],
        [-3.2485e-06, -2.3842e-06,  1.1027e-06,  ..., -2.9057e-06,
         -5.0664e-07, -1.5870e-06],
        [-6.6757e-06, -4.8876e-06,  2.2650e-06,  ..., -5.9605e-06,
         -1.0431e-06, -3.2634e-06]], device='cuda:0')
Loss: 1.1326478719711304


Running epoch 0, step 385, batch 385
Sampled inputs[:2]: tensor([[    0,  3211,   328,  ...,  2098,  1231, 35325],
        [    0,  2042,  2909,  ...,    14, 15061,  5742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8259e-06, -2.9027e-05,  3.2395e-05,  ..., -2.0095e-05,
          3.6543e-05,  3.0203e-05],
        [-6.0499e-06, -4.4256e-06,  2.0415e-06,  ..., -5.3942e-06,
         -9.8348e-07, -2.9579e-06],
        [ 8.4415e-05,  8.0045e-05, -2.3536e-05,  ...,  6.1198e-05,
          3.0201e-06,  4.2901e-05],
        [-6.5118e-06, -4.7833e-06,  2.2054e-06,  ..., -5.8115e-06,
         -1.0543e-06, -3.1814e-06],
        [-1.3471e-05, -9.8646e-06,  4.5449e-06,  ..., -1.1981e-05,
         -2.1905e-06, -6.5714e-06]], device='cuda:0')
Loss: 1.1381251811981201


Running epoch 0, step 386, batch 386
Sampled inputs[:2]: tensor([[    0,   341, 22766,  ...,   271,   266,  1176],
        [    0,    29,   413,  ...,  1527,  1503,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8689e-05,  9.4391e-07,  4.9396e-05,  ..., -2.8827e-05,
          4.2166e-05,  8.3772e-05],
        [-9.1046e-06, -6.6459e-06,  3.0622e-06,  ..., -8.1360e-06,
         -1.5497e-06, -4.4778e-06],
        [ 8.1062e-05,  7.7601e-05, -2.2411e-05,  ...,  5.8203e-05,
          2.4017e-06,  4.1232e-05],
        [-9.7454e-06, -7.1377e-06,  3.2932e-06,  ..., -8.7023e-06,
         -1.6503e-06, -4.7907e-06],
        [-2.0236e-05, -1.4782e-05,  6.8098e-06,  ..., -1.8030e-05,
         -3.4422e-06, -9.9242e-06]], device='cuda:0')
Loss: 1.1124848127365112


Running epoch 0, step 387, batch 387
Sampled inputs[:2]: tensor([[    0,    13,  3105,  ...,   496,    14,   879],
        [    0,   616,  2002,  ..., 19763,   642,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1890e-05,  2.7683e-06,  2.6420e-05,  ..., -1.7151e-05,
          3.9154e-05,  6.9534e-05],
        [-1.2085e-05, -8.8662e-06,  4.0755e-06,  ..., -1.0848e-05,
         -2.0340e-06, -5.9605e-06],
        [ 7.7769e-05,  7.5142e-05, -2.1293e-05,  ...,  5.5208e-05,
          1.8689e-06,  3.9593e-05],
        [-1.2964e-05, -9.5367e-06,  4.3884e-06,  ..., -1.1623e-05,
         -2.1681e-06, -6.3926e-06],
        [-2.6822e-05, -1.9699e-05,  9.0450e-06,  ..., -2.3991e-05,
         -4.5076e-06, -1.3188e-05]], device='cuda:0')
Loss: 1.1283535957336426


Running epoch 0, step 388, batch 388
Sampled inputs[:2]: tensor([[    0,  1795,   365,  ...,   266, 46932,   293],
        [    0,   221,   527,  ...,   298,   335,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0230e-05,  2.8803e-05,  1.9880e-05,  ..., -1.9106e-05,
          6.0429e-05,  5.9588e-05],
        [-1.5095e-05, -1.1072e-05,  5.0738e-06,  ..., -1.3575e-05,
         -2.5555e-06, -7.4431e-06],
        [ 7.4476e-05,  7.2713e-05, -2.0198e-05,  ...,  5.2212e-05,
          1.2990e-06,  3.7969e-05],
        [-1.6168e-05, -1.1891e-05,  5.4538e-06,  ..., -1.4529e-05,
         -2.7232e-06, -7.9721e-06],
        [-3.3468e-05, -2.4587e-05,  1.1250e-05,  ..., -3.0011e-05,
         -5.6624e-06, -1.6466e-05]], device='cuda:0')
Loss: 1.1447269916534424


Running epoch 0, step 389, batch 389
Sampled inputs[:2]: tensor([[    0,  1336, 10446,  ...,   409,   275, 12528],
        [    0,  3473,   278,  ..., 11743,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7809e-05,  1.3535e-05,  1.9880e-05,  ..., -3.9248e-05,
          1.0751e-04,  7.8182e-05],
        [-1.8090e-05, -1.3292e-05,  6.0871e-06,  ..., -1.6287e-05,
         -3.0287e-06, -8.9034e-06],
        [ 7.1153e-05,  7.0255e-05, -1.9073e-05,  ...,  4.9187e-05,
          7.7371e-07,  3.6345e-05],
        [-1.9386e-05, -1.4275e-05,  6.5491e-06,  ..., -1.7464e-05,
         -3.2298e-06, -9.5442e-06],
        [-4.0144e-05, -2.9534e-05,  1.3500e-05,  ..., -3.6091e-05,
         -6.7204e-06, -1.9714e-05]], device='cuda:0')
Loss: 1.145015001296997


Running epoch 0, step 390, batch 390
Sampled inputs[:2]: tensor([[    0,  3630,  2199,  ...,  4157,    27,  4765],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5414e-05, -3.8769e-06,  3.4987e-05,  ..., -2.7512e-05,
          8.2605e-05,  1.0922e-04],
        [-2.1115e-05, -1.5512e-05,  7.0855e-06,  ..., -1.8984e-05,
         -3.5316e-06, -1.0394e-05],
        [ 6.7830e-05,  6.7811e-05, -1.7970e-05,  ...,  4.6207e-05,
          2.2237e-07,  3.4706e-05],
        [-2.2605e-05, -1.6645e-05,  7.6145e-06,  ..., -2.0340e-05,
         -3.7588e-06, -1.1131e-05],
        [-4.6909e-05, -3.4511e-05,  1.5736e-05,  ..., -4.2140e-05,
         -7.8380e-06, -2.3037e-05]], device='cuda:0')
Loss: 1.1505504846572876


Running epoch 0, step 391, batch 391
Sampled inputs[:2]: tensor([[    0,  1172,   365,  ...,  1119, 15573,  3701],
        [    0,  1144,  2680,  ...,   963,     9,  1184]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1185e-05, -1.1860e-06, -4.7458e-06,  ..., -4.1908e-05,
          9.5772e-05,  8.3608e-05],
        [-2.4125e-05, -1.7717e-05,  8.0839e-06,  ..., -2.1711e-05,
         -4.0382e-06, -1.1854e-05],
        [ 6.4507e-05,  6.5367e-05, -1.6860e-05,  ...,  4.3182e-05,
         -3.3642e-07,  3.3089e-05],
        [-2.5839e-05, -1.9029e-05,  8.6948e-06,  ..., -2.3276e-05,
         -4.2990e-06, -1.2703e-05],
        [-5.3555e-05, -3.9399e-05,  1.7956e-05,  ..., -4.8190e-05,
         -8.9630e-06, -2.6271e-05]], device='cuda:0')
Loss: 1.1574463844299316
Graident accumulation at epoch 0, step 391, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0023, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0146, -0.0272,  ...,  0.0282, -0.0157, -0.0184]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.3828e-05, -1.4719e-05, -5.8274e-05,  ..., -9.2793e-06,
         -2.2201e-05,  2.0808e-05],
        [-2.3895e-05, -1.8223e-05,  5.9834e-06,  ..., -2.1307e-05,
         -4.7998e-06, -1.0647e-05],
        [ 5.0791e-05,  4.0987e-05, -7.6299e-06,  ...,  4.6981e-05,
          1.0394e-05,  2.1597e-05],
        [-9.8714e-06, -7.3913e-06,  1.8287e-06,  ..., -6.6624e-06,
         -2.1170e-06, -3.8843e-06],
        [-5.3045e-05, -4.0432e-05,  1.3261e-05,  ..., -4.7152e-05,
         -1.0386e-05, -2.3254e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1760e-08, 2.7914e-08, 3.5073e-08,  ..., 1.1555e-08, 8.7311e-08,
         1.1430e-08],
        [4.6027e-11, 2.5614e-11, 2.7807e-12,  ..., 3.0006e-11, 1.7731e-12,
         6.7203e-12],
        [5.9129e-10, 3.2621e-10, 2.0205e-11,  ..., 5.1955e-10, 3.2002e-11,
         1.2100e-10],
        [6.0425e-11, 3.2760e-11, 3.5988e-12,  ..., 5.1790e-11, 2.4229e-12,
         1.1106e-11],
        [2.0243e-10, 1.0803e-10, 9.0709e-12,  ..., 1.4568e-10, 7.8029e-12,
         3.3690e-11]], device='cuda:0')
optimizer state dict: 49.0
lr: [1.8734471844266252e-05, 1.8734471844266252e-05]
scheduler_last_epoch: 49


Running epoch 0, step 392, batch 392
Sampled inputs[:2]: tensor([[    0,   565,  1360,  ...,   278,  2722,  1683],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1815e-05, -2.5884e-05, -5.6856e-06,  ..., -6.6282e-06,
          2.4732e-05, -2.8320e-06],
        [-3.0696e-06, -2.1607e-06,  1.0431e-06,  ..., -2.7269e-06,
         -4.6752e-07, -1.5423e-06],
        [-3.3677e-06, -2.3693e-06,  1.1399e-06,  ..., -2.9802e-06,
         -5.1036e-07, -1.6913e-06],
        [-3.2634e-06, -2.2948e-06,  1.1101e-06,  ..., -2.8908e-06,
         -4.9174e-07, -1.6391e-06],
        [-6.7055e-06, -4.7088e-06,  2.2650e-06,  ..., -5.9307e-06,
         -1.0133e-06, -3.3677e-06]], device='cuda:0')
Loss: 1.1422642469406128


Running epoch 0, step 393, batch 393
Sampled inputs[:2]: tensor([[    0, 25939, 47777,  ...,    13,  3483,   278],
        [    0,  5182,   446,  ...,   417,   199,    50]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6029e-05, -4.4490e-05, -2.0346e-05,  ...,  1.2764e-05,
         -4.4684e-06,  8.9228e-06],
        [-6.0946e-06, -4.3213e-06,  2.0787e-06,  ..., -5.4389e-06,
         -9.3691e-07, -3.0696e-06],
        [-6.7204e-06, -4.7684e-06,  2.2873e-06,  ..., -5.9903e-06,
         -1.0282e-06, -3.3826e-06],
        [-6.4820e-06, -4.5896e-06,  2.2128e-06,  ..., -5.7817e-06,
         -9.8720e-07, -3.2634e-06],
        [-1.3322e-05, -9.4175e-06,  4.5300e-06,  ..., -1.1861e-05,
         -2.0415e-06, -6.7055e-06]], device='cuda:0')
Loss: 1.128661870956421


Running epoch 0, step 394, batch 394
Sampled inputs[:2]: tensor([[   0, 4890, 1528,  ...,  847,  328, 1703],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3150e-05, -5.2223e-05, -1.0851e-05,  ...,  1.1864e-05,
         -4.4684e-06, -1.9017e-05],
        [-9.1046e-06, -6.4820e-06,  3.1069e-06,  ..., -8.1509e-06,
         -1.4119e-06, -4.5821e-06],
        [-1.0058e-05, -7.1526e-06,  3.4273e-06,  ..., -9.0003e-06,
         -1.5534e-06, -5.0589e-06],
        [-9.7305e-06, -6.9141e-06,  3.3230e-06,  ..., -8.7023e-06,
         -1.4976e-06, -4.8876e-06],
        [-1.9997e-05, -1.4186e-05,  6.8098e-06,  ..., -1.7881e-05,
         -3.0994e-06, -1.0058e-05]], device='cuda:0')
Loss: 1.1328867673873901


Running epoch 0, step 395, batch 395
Sampled inputs[:2]: tensor([[    0,  9041,  8375,  ...,   221,   474, 43112],
        [    0,  5151,   292,  ..., 13658,   401,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1126e-05, -5.0162e-05, -4.1745e-05,  ...,  2.4865e-05,
         -8.5551e-06, -3.7193e-05],
        [-1.2204e-05, -8.6576e-06,  4.1798e-06,  ..., -1.0893e-05,
         -1.9334e-06, -6.1542e-06],
        [-1.3411e-05, -9.5218e-06,  4.5896e-06,  ..., -1.1981e-05,
         -2.1160e-06, -6.7577e-06],
        [-1.2994e-05, -9.2089e-06,  4.4480e-06,  ..., -1.1593e-05,
         -2.0415e-06, -6.5342e-06],
        [-2.6673e-05, -1.8895e-05,  9.1195e-06,  ..., -2.3812e-05,
         -4.2245e-06, -1.3441e-05]], device='cuda:0')
Loss: 1.1280310153961182


Running epoch 0, step 396, batch 396
Sampled inputs[:2]: tensor([[    0,  6022,   644,  ..., 14834,  3554,   591],
        [    0,    12,   221,  ...,   292, 27729,  9837]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4112e-05, -4.6681e-05, -4.8392e-05,  ...,  3.7748e-05,
         -1.1258e-05, -1.1646e-05],
        [-1.5244e-05, -1.0818e-05,  5.2452e-06,  ..., -1.3620e-05,
         -2.4512e-06, -7.6890e-06],
        [-1.6749e-05, -1.1891e-05,  5.7518e-06,  ..., -1.4961e-05,
         -2.6822e-06, -8.4415e-06],
        [-1.6227e-05, -1.1489e-05,  5.5730e-06,  ..., -1.4484e-05,
         -2.5854e-06, -8.1658e-06],
        [-3.3349e-05, -2.3603e-05,  1.1444e-05,  ..., -2.9773e-05,
         -5.3570e-06, -1.6809e-05]], device='cuda:0')
Loss: 1.1393921375274658


Running epoch 0, step 397, batch 397
Sampled inputs[:2]: tensor([[    0,  7779,    12,  ...,  1380, 10199,  1086],
        [    0,     9,   287,  ..., 16261,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9548e-05, -4.3884e-05, -5.7409e-05,  ...,  2.9790e-05,
         -2.4402e-05, -1.0002e-05],
        [-1.8314e-05, -1.3009e-05,  6.3181e-06,  ..., -1.6347e-05,
         -2.9430e-06, -9.2611e-06],
        [-2.0131e-05, -1.4305e-05,  6.9290e-06,  ..., -1.7956e-05,
         -3.2187e-06, -1.0170e-05],
        [-1.9491e-05, -1.3828e-05,  6.7130e-06,  ..., -1.7375e-05,
         -3.0994e-06, -9.8348e-06],
        [-4.0054e-05, -2.8402e-05,  1.3784e-05,  ..., -3.5703e-05,
         -6.4224e-06, -2.0236e-05]], device='cuda:0')
Loss: 1.141547441482544


Running epoch 0, step 398, batch 398
Sampled inputs[:2]: tensor([[   0,  221,  381,  ...,  360, 8978,   14],
        [   0,  474,  513,  ...,  221, 2951, 7773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1234e-04, -3.8528e-05, -4.2967e-05,  ...,  6.2796e-05,
         -1.9721e-05, -2.2787e-05],
        [-2.1353e-05, -1.5169e-05,  7.3314e-06,  ..., -1.9044e-05,
         -3.4496e-06, -1.0803e-05],
        [-2.3499e-05, -1.6704e-05,  8.0466e-06,  ..., -2.0936e-05,
         -3.7774e-06, -1.1869e-05],
        [-2.2680e-05, -1.6108e-05,  7.7784e-06,  ..., -2.0206e-05,
         -3.6284e-06, -1.1452e-05],
        [-4.6700e-05, -3.3110e-05,  1.5989e-05,  ..., -4.1604e-05,
         -7.5251e-06, -2.3589e-05]], device='cuda:0')
Loss: 1.1322519779205322


Running epoch 0, step 399, batch 399
Sampled inputs[:2]: tensor([[   0, 3441,  796,  ..., 7561, 1711,  857],
        [   0,   14,  747,  ...,  367,  300,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0773e-04, -3.5054e-05, -3.8727e-05,  ...,  6.1216e-05,
         -2.2539e-05, -5.1496e-05],
        [-2.4378e-05, -1.7315e-05,  8.3745e-06,  ..., -2.1741e-05,
         -3.9451e-06, -1.2338e-05],
        [-2.6822e-05, -1.9059e-05,  9.1866e-06,  ..., -2.3901e-05,
         -4.3176e-06, -1.3545e-05],
        [-2.5913e-05, -1.8388e-05,  8.8885e-06,  ..., -2.3097e-05,
         -4.1500e-06, -1.3083e-05],
        [-5.3346e-05, -3.7819e-05,  1.8269e-05,  ..., -4.7535e-05,
         -8.6054e-06, -2.6941e-05]], device='cuda:0')
Loss: 1.1564704179763794
Graident accumulation at epoch 0, step 399, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0023, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0147, -0.0273,  ...,  0.0282, -0.0157, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.0218e-05, -1.6752e-05, -5.6319e-05,  ..., -2.2297e-06,
         -2.2235e-05,  1.3577e-05],
        [-2.3944e-05, -1.8132e-05,  6.2225e-06,  ..., -2.1351e-05,
         -4.7144e-06, -1.0816e-05],
        [ 4.3030e-05,  3.4982e-05, -5.9482e-06,  ...,  3.9893e-05,
          8.9224e-06,  1.8083e-05],
        [-1.1476e-05, -8.4910e-06,  2.5347e-06,  ..., -8.3058e-06,
         -2.3203e-06, -4.8042e-06],
        [-5.3076e-05, -4.0170e-05,  1.3761e-05,  ..., -4.7190e-05,
         -1.0208e-05, -2.3623e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1730e-08, 2.7888e-08, 3.5039e-08,  ..., 1.1547e-08, 8.7224e-08,
         1.1421e-08],
        [4.6575e-11, 2.5888e-11, 2.8481e-12,  ..., 3.0449e-11, 1.7869e-12,
         6.8658e-12],
        [5.9141e-10, 3.2625e-10, 2.0269e-11,  ..., 5.1960e-10, 3.1988e-11,
         1.2106e-10],
        [6.1036e-11, 3.3066e-11, 3.6742e-12,  ..., 5.2271e-11, 2.4377e-12,
         1.1266e-11],
        [2.0507e-10, 1.0935e-10, 9.3956e-12,  ..., 1.4779e-10, 7.8692e-12,
         3.4383e-11]], device='cuda:0')
optimizer state dict: 50.0
lr: [1.8673614759157743e-05, 1.8673614759157743e-05]
scheduler_last_epoch: 50


Running epoch 0, step 400, batch 400
Sampled inputs[:2]: tensor([[    0,  9829,   292,  ...,  2928,  1029,   271],
        [    0,  4878,   607,  ...,    14, 17331,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0634e-06, -2.9294e-05, -2.1983e-05,  ...,  4.7246e-07,
          4.5399e-06, -1.5765e-05],
        [-3.0994e-06, -2.1756e-06,  1.0803e-06,  ..., -2.7865e-06,
         -4.9919e-07, -1.6466e-06],
        [-3.3975e-06, -2.3842e-06,  1.1846e-06,  ..., -3.0547e-06,
         -5.4762e-07, -1.8030e-06],
        [-3.2187e-06, -2.2501e-06,  1.1250e-06,  ..., -2.8908e-06,
         -5.1782e-07, -1.7062e-06],
        [-6.6757e-06, -4.6790e-06,  2.3246e-06,  ..., -5.9903e-06,
         -1.0729e-06, -3.5465e-06]], device='cuda:0')
Loss: 1.1606683731079102


Running epoch 0, step 401, batch 401
Sampled inputs[:2]: tensor([[    0,   396,  1821,  ...,  5984, 18362,   278],
        [    0,    14,   475,  ...,  2117,  2792, 12848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6217e-06, -7.2804e-06, -6.5132e-05,  ...,  1.3108e-05,
          2.0654e-05, -6.4827e-06],
        [-6.2287e-06, -4.3511e-06,  2.1979e-06,  ..., -5.5581e-06,
         -1.0170e-06, -3.2857e-06],
        [-6.8098e-06, -4.7535e-06,  2.3991e-06,  ..., -6.0797e-06,
         -1.1064e-06, -3.5837e-06],
        [-6.4820e-06, -4.5151e-06,  2.2873e-06,  ..., -5.7817e-06,
         -1.0505e-06, -3.4124e-06],
        [-1.3351e-05, -9.2983e-06,  4.6939e-06,  ..., -1.1891e-05,
         -2.1681e-06, -7.0333e-06]], device='cuda:0')
Loss: 1.133000373840332


Running epoch 0, step 402, batch 402
Sampled inputs[:2]: tensor([[    0,   365,  8790,  ...,  1172,  8806,   266],
        [    0,   342,   408,  ...,  5162, 25842,  4855]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4953e-05, -3.4940e-05, -1.0083e-04,  ...,  3.2475e-05,
          4.3784e-05,  1.3345e-06],
        [-9.3430e-06, -6.5118e-06,  3.2857e-06,  ..., -8.2999e-06,
         -1.5423e-06, -4.8876e-06],
        [-1.0222e-05, -7.1228e-06,  3.5912e-06,  ..., -9.0897e-06,
         -1.6801e-06, -5.3421e-06],
        [-9.7454e-06, -6.7651e-06,  3.4273e-06,  ..., -8.6576e-06,
         -1.5944e-06, -5.0887e-06],
        [-2.0087e-05, -1.3977e-05,  7.0482e-06,  ..., -1.7822e-05,
         -3.3006e-06, -1.0490e-05]], device='cuda:0')
Loss: 1.1417521238327026


Running epoch 0, step 403, batch 403
Sampled inputs[:2]: tensor([[   0, 3351,  352,  ...,   17,  287,  357],
        [   0, 1967, 6851,  ..., 1151,  809,  360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9001e-05, -3.6002e-05, -1.4243e-04,  ...,  2.3081e-05,
          4.0541e-05, -1.4651e-05],
        [-1.2457e-05, -8.6725e-06,  4.3958e-06,  ..., -1.1072e-05,
         -2.0713e-06, -6.5044e-06],
        [-1.3590e-05, -9.4622e-06,  4.7833e-06,  ..., -1.2085e-05,
         -2.2464e-06, -7.0781e-06],
        [-1.2994e-05, -9.0152e-06,  4.5821e-06,  ..., -1.1548e-05,
         -2.1383e-06, -6.7651e-06],
        [-2.6762e-05, -1.8597e-05,  9.4175e-06,  ..., -2.3723e-05,
         -4.4182e-06, -1.3933e-05]], device='cuda:0')
Loss: 1.1328413486480713


Running epoch 0, step 404, batch 404
Sampled inputs[:2]: tensor([[   0,   14,  747,  ..., 2039,  287, 8053],
        [   0,  300, 4402,  ..., 2013,   13, 6825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6319e-05, -4.2588e-05, -1.3933e-04,  ...,  4.8303e-06,
          3.9865e-05, -1.3054e-05],
        [-1.5542e-05, -1.0848e-05,  5.4985e-06,  ..., -1.3798e-05,
         -2.5257e-06, -8.0913e-06],
        [-1.7002e-05, -1.1861e-05,  5.9977e-06,  ..., -1.5095e-05,
         -2.7455e-06, -8.8289e-06],
        [-1.6257e-05, -1.1310e-05,  5.7518e-06,  ..., -1.4439e-05,
         -2.6152e-06, -8.4490e-06],
        [-3.3498e-05, -2.3335e-05,  1.1817e-05,  ..., -2.9653e-05,
         -5.4091e-06, -1.7390e-05]], device='cuda:0')
Loss: 1.1448252201080322


Running epoch 0, step 405, batch 405
Sampled inputs[:2]: tensor([[    0,   287,  4599,  ..., 11812,   266,  1036],
        [    0,  2834,   266,  ..., 39474,    12, 15441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7312e-05, -4.0380e-05, -1.3596e-04,  ..., -7.4709e-06,
          3.7537e-05, -1.0379e-05],
        [-1.8641e-05, -1.2994e-05,  6.5789e-06,  ..., -1.6555e-05,
         -3.0212e-06, -9.6709e-06],
        [-2.0400e-05, -1.4216e-05,  7.1824e-06,  ..., -1.8120e-05,
         -3.2820e-06, -1.0557e-05],
        [-1.9521e-05, -1.3560e-05,  6.8918e-06,  ..., -1.7345e-05,
         -3.1292e-06, -1.0103e-05],
        [-4.0203e-05, -2.7955e-05,  1.4141e-05,  ..., -3.5614e-05,
         -6.4746e-06, -2.0787e-05]], device='cuda:0')
Loss: 1.141719937324524


Running epoch 0, step 406, batch 406
Sampled inputs[:2]: tensor([[    0,   271,   259,  ...,  4511,    14,   333],
        [    0,   266, 10726,  ..., 13973, 22191, 15913]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4855e-05, -5.0714e-05, -1.4042e-04,  ..., -1.2731e-05,
          2.8612e-05, -9.6271e-06],
        [-2.1741e-05, -1.5169e-05,  7.7114e-06,  ..., -1.9297e-05,
         -3.5502e-06, -1.1288e-05],
        [-2.3812e-05, -1.6615e-05,  8.4266e-06,  ..., -2.1130e-05,
         -3.8631e-06, -1.2338e-05],
        [-2.2769e-05, -1.5840e-05,  8.0764e-06,  ..., -2.0221e-05,
         -3.6769e-06, -1.1794e-05],
        [-4.6849e-05, -3.2634e-05,  1.6555e-05,  ..., -4.1485e-05,
         -7.6070e-06, -2.4244e-05]], device='cuda:0')
Loss: 1.1338917016983032


Running epoch 0, step 407, batch 407
Sampled inputs[:2]: tensor([[   0,  591, 2036,  ...,  266, 1027,  278],
        [   0,  292,   65,  ...,   12,  857,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5895e-07, -5.3410e-05, -1.4022e-04,  ...,  6.3256e-06,
          2.0458e-05,  9.7030e-07],
        [-2.4825e-05, -1.7315e-05,  8.8140e-06,  ..., -2.2024e-05,
         -4.0308e-06, -1.2852e-05],
        [-2.7180e-05, -1.8954e-05,  9.6336e-06,  ..., -2.4095e-05,
         -4.3847e-06, -1.4052e-05],
        [-2.6047e-05, -1.8120e-05,  9.2462e-06,  ..., -2.3112e-05,
         -4.1798e-06, -1.3456e-05],
        [-5.3525e-05, -3.7283e-05,  1.8939e-05,  ..., -4.7356e-05,
         -8.6427e-06, -2.7627e-05]], device='cuda:0')
Loss: 1.1243820190429688
Graident accumulation at epoch 0, step 407, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0224, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0023, -0.0340],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0147, -0.0273,  ...,  0.0282, -0.0157, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5111e-05, -2.0418e-05, -6.4710e-05,  ..., -1.3742e-06,
         -1.7965e-05,  1.2317e-05],
        [-2.4032e-05, -1.8050e-05,  6.4816e-06,  ..., -2.1418e-05,
         -4.6460e-06, -1.1019e-05],
        [ 3.6009e-05,  2.9589e-05, -4.3900e-06,  ...,  3.3494e-05,
          7.5917e-06,  1.4869e-05],
        [-1.2933e-05, -9.4539e-06,  3.2058e-06,  ..., -9.7864e-06,
         -2.5063e-06, -5.6693e-06],
        [-5.3120e-05, -3.9882e-05,  1.4279e-05,  ..., -4.7207e-05,
         -1.0051e-05, -2.4023e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1688e-08, 2.7863e-08, 3.5024e-08,  ..., 1.1535e-08, 8.7137e-08,
         1.1409e-08],
        [4.7145e-11, 2.6162e-11, 2.9229e-12,  ..., 3.0903e-11, 1.8013e-12,
         7.0241e-12],
        [5.9156e-10, 3.2628e-10, 2.0341e-11,  ..., 5.1966e-10, 3.1976e-11,
         1.2114e-10],
        [6.1654e-11, 3.3361e-11, 3.7561e-12,  ..., 5.2753e-11, 2.4527e-12,
         1.1436e-11],
        [2.0773e-10, 1.1063e-10, 9.7449e-12,  ..., 1.4989e-10, 7.9360e-12,
         3.5111e-11]], device='cuda:0')
optimizer state dict: 51.0
lr: [1.8611432270000978e-05, 1.8611432270000978e-05]
scheduler_last_epoch: 51


Running epoch 0, step 408, batch 408
Sampled inputs[:2]: tensor([[    0,  6192,   266,  ...,  3318,  9872, 10931],
        [    0,    17,   292,  ...,  2269,  3887,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5761e-05,  4.7316e-06, -2.0562e-05,  ...,  5.9348e-07,
         -2.1897e-05,  2.5046e-06],
        [-3.1590e-06, -2.1160e-06,  1.1474e-06,  ..., -2.7418e-06,
         -5.2899e-07, -1.6764e-06],
        [-3.4422e-06, -2.3097e-06,  1.2517e-06,  ..., -2.9802e-06,
         -5.7369e-07, -1.8254e-06],
        [-3.2783e-06, -2.1905e-06,  1.1921e-06,  ..., -2.8312e-06,
         -5.4017e-07, -1.7285e-06],
        [-6.7055e-06, -4.5002e-06,  2.4289e-06,  ..., -5.8115e-06,
         -1.1101e-06, -3.5465e-06]], device='cuda:0')
Loss: 1.135046362876892


Running epoch 0, step 409, batch 409
Sampled inputs[:2]: tensor([[    0,  2670, 31283,  ...,    18,  9106,  1389],
        [    0,    12,   287,  ...,   696,   700,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0644e-05,  6.1204e-05,  1.8929e-05,  ..., -1.6644e-05,
         -1.8769e-05,  2.7172e-05],
        [-6.2883e-06, -4.2915e-06,  2.3171e-06,  ..., -5.4985e-06,
         -1.1325e-06, -3.3751e-06],
        [-6.8694e-06, -4.6939e-06,  2.5332e-06,  ..., -5.9903e-06,
         -1.2331e-06, -3.6806e-06],
        [-6.5118e-06, -4.4256e-06,  2.3991e-06,  ..., -5.6624e-06,
         -1.1586e-06, -3.4720e-06],
        [-1.3351e-05, -9.1195e-06,  4.9025e-06,  ..., -1.1623e-05,
         -2.3916e-06, -7.1377e-06]], device='cuda:0')
Loss: 1.1670228242874146


Running epoch 0, step 410, batch 410
Sampled inputs[:2]: tensor([[    0,    14,  5551,  ...,   668, 11988,  2538],
        [    0,   874,   590,  ...,   300,   867,   638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9542e-05,  6.8485e-05,  4.7565e-06,  ..., -1.1943e-05,
         -1.0427e-05,  5.4236e-05],
        [-9.4175e-06, -6.4224e-06,  3.4645e-06,  ..., -8.2552e-06,
         -1.6689e-06, -5.0291e-06],
        [-1.0267e-05, -7.0035e-06,  3.7774e-06,  ..., -8.9854e-06,
         -1.8142e-06, -5.4762e-06],
        [-9.7454e-06, -6.6161e-06,  3.5837e-06,  ..., -8.5235e-06,
         -1.7099e-06, -5.1782e-06],
        [-1.9938e-05, -1.3590e-05,  7.3165e-06,  ..., -1.7464e-05,
         -3.5241e-06, -1.0610e-05]], device='cuda:0')
Loss: 1.1389209032058716


Running epoch 0, step 411, batch 411
Sampled inputs[:2]: tensor([[   0,  417,  199,  ...,   13,   20, 6248],
        [   0,  278,  266,  ..., 5503,  259, 1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7782e-05,  9.5342e-05,  1.1479e-05,  ..., -2.7858e-05,
          2.2084e-05,  5.9110e-05],
        [-1.2562e-05, -8.5533e-06,  4.6194e-06,  ..., -1.1027e-05,
         -2.2575e-06, -6.7502e-06],
        [-1.3694e-05, -9.3281e-06,  5.0366e-06,  ..., -1.2010e-05,
         -2.4550e-06, -7.3537e-06],
        [-1.2979e-05, -8.8066e-06,  4.7684e-06,  ..., -1.1370e-05,
         -2.3097e-06, -6.9439e-06],
        [-2.6643e-05, -1.8120e-05,  9.7603e-06,  ..., -2.3365e-05,
         -4.7684e-06, -1.4260e-05]], device='cuda:0')
Loss: 1.1302993297576904


Running epoch 0, step 412, batch 412
Sampled inputs[:2]: tensor([[    0,  4154, 14296,  ...,   516,  1796, 18233],
        [    0,  2088,  5370,  ...,  1110,  3380,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4814e-05,  1.2628e-04,  1.8069e-05,  ..., -1.7905e-05,
          9.6019e-06,  4.6684e-05],
        [-1.5736e-05, -1.0699e-05,  5.7667e-06,  ..., -1.3828e-05,
         -2.8275e-06, -8.4415e-06],
        [-1.7151e-05, -1.1668e-05,  6.2883e-06,  ..., -1.5065e-05,
         -3.0734e-06, -9.1940e-06],
        [-1.6242e-05, -1.1012e-05,  5.9530e-06,  ..., -1.4260e-05,
         -2.8908e-06, -8.6799e-06],
        [-3.3379e-05, -2.2680e-05,  1.2189e-05,  ..., -2.9296e-05,
         -5.9679e-06, -1.7837e-05]], device='cuda:0')
Loss: 1.135805606842041


Running epoch 0, step 413, batch 413
Sampled inputs[:2]: tensor([[   0, 1197,  729,  ...,  674,  369, 8222],
        [   0,  365, 2714,  ...,  298,  273,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5454e-05,  1.2655e-04,  6.3039e-06,  ..., -2.8418e-05,
          5.3423e-05,  5.3026e-05],
        [-1.8895e-05, -1.2845e-05,  6.8992e-06,  ..., -1.6585e-05,
         -3.3677e-06, -1.0103e-05],
        [ 6.2025e-04,  4.4095e-04, -2.2453e-04,  ...,  5.4518e-04,
          9.7224e-05,  3.4295e-04],
        [-1.9521e-05, -1.3247e-05,  7.1377e-06,  ..., -1.7121e-05,
         -3.4496e-06, -1.0408e-05],
        [-4.0114e-05, -2.7269e-05,  1.4618e-05,  ..., -3.5167e-05,
         -7.1153e-06, -2.1368e-05]], device='cuda:0')
Loss: 1.1412670612335205


Running epoch 0, step 414, batch 414
Sampled inputs[:2]: tensor([[    0, 16765,   367,  ..., 30192,  7038,  8135],
        [    0,   591, 36195,  ...,  3359,   717,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0389e-05,  1.2600e-04,  3.6922e-05,  ..., -2.1352e-05,
          4.0203e-05,  5.6811e-05],
        [-2.2009e-05, -1.4991e-05,  8.0243e-06,  ..., -1.9327e-05,
         -3.8780e-06, -1.1750e-05],
        [ 6.1681e-04,  4.3858e-04, -2.2329e-04,  ...,  5.4214e-04,
          9.6665e-05,  3.4113e-04],
        [-2.2784e-05, -1.5497e-05,  8.3148e-06,  ..., -1.9997e-05,
         -3.9786e-06, -1.2130e-05],
        [-4.6760e-05, -3.1829e-05,  1.7002e-05,  ..., -4.1038e-05,
         -8.1956e-06, -2.4870e-05]], device='cuda:0')
Loss: 1.1256933212280273


Running epoch 0, step 415, batch 415
Sampled inputs[:2]: tensor([[    0,   935,  2613,  ...,   623,  4289,  6803],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1779e-04,  1.6327e-04,  3.0777e-05,  ...,  9.5503e-06,
          3.1527e-05,  6.1421e-05],
        [-2.5153e-05, -1.7121e-05,  9.1344e-06,  ..., -2.2054e-05,
         -4.4107e-06, -1.3418e-05],
        [ 6.1340e-04,  4.3627e-04, -2.2209e-04,  ...,  5.3917e-04,
          9.6088e-05,  3.3932e-04],
        [-2.6032e-05, -1.7703e-05,  9.4697e-06,  ..., -2.2829e-05,
         -4.5262e-06, -1.3851e-05],
        [-5.3465e-05, -3.6359e-05,  1.9372e-05,  ..., -4.6849e-05,
         -9.3281e-06, -2.8417e-05]], device='cuda:0')
Loss: 1.120205283164978
Graident accumulation at epoch 0, step 415, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0052, -0.0149,  0.0035,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0290, -0.0078,  0.0033,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0165,  0.0147, -0.0273,  ...,  0.0282, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.2379e-05, -2.0495e-06, -5.5161e-05,  ..., -2.8175e-07,
         -1.3016e-05,  1.7227e-05],
        [-2.4144e-05, -1.7957e-05,  6.7469e-06,  ..., -2.1482e-05,
         -4.6225e-06, -1.1259e-05],
        [ 9.3748e-05,  7.0257e-05, -2.6160e-05,  ...,  8.4062e-05,
          1.6441e-05,  4.7314e-05],
        [-1.4243e-05, -1.0279e-05,  3.8322e-06,  ..., -1.1091e-05,
         -2.7083e-06, -6.4875e-06],
        [-5.3155e-05, -3.9529e-05,  1.4788e-05,  ..., -4.7171e-05,
         -9.9789e-06, -2.4463e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1660e-08, 2.7861e-08, 3.4990e-08,  ..., 1.1524e-08, 8.7051e-08,
         1.1402e-08],
        [4.7730e-11, 2.6429e-11, 3.0034e-12,  ..., 3.1359e-11, 1.8190e-12,
         7.1972e-12],
        [9.6723e-10, 5.1628e-10, 6.9643e-11,  ..., 8.0985e-10, 4.1177e-11,
         2.3616e-10],
        [6.2270e-11, 3.3641e-11, 3.8420e-12,  ..., 5.3221e-11, 2.4708e-12,
         1.1616e-11],
        [2.1038e-10, 1.1184e-10, 1.0110e-11,  ..., 1.5193e-10, 8.0151e-12,
         3.5884e-11]], device='cuda:0')
optimizer state dict: 52.0
lr: [1.8547933878823103e-05, 1.8547933878823103e-05]
scheduler_last_epoch: 52


Running epoch 0, step 416, batch 416
Sampled inputs[:2]: tensor([[    0,   287, 11638,  ...,    17,   221,   733],
        [    0,    12,   287,  ...,  3359,  1751,  5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6322e-06, -5.2889e-06, -1.1346e-05,  ..., -2.9116e-05,
         -1.0069e-06,  2.4791e-05],
        [-3.1441e-06, -2.1011e-06,  1.1623e-06,  ..., -2.7120e-06,
         -5.8115e-07, -1.7136e-06],
        [-3.5018e-06, -2.3395e-06,  1.3039e-06,  ..., -3.0249e-06,
         -6.4448e-07, -1.9073e-06],
        [-3.2932e-06, -2.2054e-06,  1.2219e-06,  ..., -2.8461e-06,
         -6.0350e-07, -1.7956e-06],
        [-6.6161e-06, -4.4107e-06,  2.4438e-06,  ..., -5.7220e-06,
         -1.2219e-06, -3.5912e-06]], device='cuda:0')
Loss: 1.1217674016952515


Running epoch 0, step 417, batch 417
Sampled inputs[:2]: tensor([[   0, 1682,  271,  ...,  367, 3210,  271],
        [   0, 1976, 1329,  ...,  278, 9469,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6006e-05, -3.7667e-06, -1.0266e-05,  ..., -4.6479e-05,
          2.3609e-05,  5.7671e-05],
        [-6.3330e-06, -4.2319e-06,  2.3022e-06,  ..., -5.4687e-06,
         -1.1697e-06, -3.4198e-06],
        [-7.0333e-06, -4.7088e-06,  2.5630e-06,  ..., -6.0797e-06,
         -1.2964e-06, -3.7923e-06],
        [-6.5565e-06, -4.3809e-06,  2.3842e-06,  ..., -5.6624e-06,
         -1.1995e-06, -3.5316e-06],
        [-1.3381e-05, -8.9109e-06,  4.8578e-06,  ..., -1.1563e-05,
         -2.4661e-06, -7.1973e-06]], device='cuda:0')
Loss: 1.140257477760315


Running epoch 0, step 418, batch 418
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 19245,   565,    14],
        [    0,   266,  1234,  ...,   908,   328, 26300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8538e-05, -2.4562e-05, -5.9096e-05,  ..., -3.0718e-05,
          2.0225e-05,  5.1079e-06],
        [-9.4771e-06, -6.3628e-06,  3.4571e-06,  ..., -8.2105e-06,
         -1.7323e-06, -5.1409e-06],
        [ 2.7575e-04,  1.7049e-04, -8.9687e-05,  ...,  2.7205e-04,
          3.2091e-05,  1.6326e-04],
        [-9.8050e-06, -6.5863e-06,  3.5837e-06,  ..., -8.5086e-06,
         -1.7732e-06, -5.3048e-06],
        [-1.9968e-05, -1.3381e-05,  7.2867e-06,  ..., -1.7315e-05,
         -3.6359e-06, -1.0788e-05]], device='cuda:0')
Loss: 1.1473129987716675


Running epoch 0, step 419, batch 419
Sampled inputs[:2]: tensor([[    0,   342, 22510,  ..., 49108,   278, 25904],
        [    0,   275,   467,  ...,   298,   365,  2714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3421e-05,  6.4443e-06, -8.7905e-05,  ..., -1.5026e-05,
          2.3715e-06,  7.6441e-06],
        [-1.2651e-05, -8.4937e-06,  4.6268e-06,  ..., -1.0967e-05,
         -2.2948e-06, -6.8992e-06],
        [ 2.7225e-04,  1.6814e-04, -8.8398e-05,  ...,  2.6900e-04,
          3.1473e-05,  1.6132e-04],
        [-1.3053e-05, -8.7768e-06,  4.7833e-06,  ..., -1.1340e-05,
         -2.3432e-06, -7.1079e-06],
        [-2.6613e-05, -1.7852e-05,  9.7305e-06,  ..., -2.3127e-05,
         -4.8131e-06, -1.4484e-05]], device='cuda:0')
Loss: 1.141106367111206


Running epoch 0, step 420, batch 420
Sampled inputs[:2]: tensor([[   0,   12,  496,  ...,  437,  266, 3767],
        [   0,   13,   41,  ...,    5,  271, 2936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1969e-05,  1.6197e-05, -8.7384e-05,  ..., -1.4615e-05,
         -1.7450e-05,  3.5100e-05],
        [-1.5870e-05, -1.0625e-05,  5.8264e-06,  ..., -1.3724e-05,
         -2.8796e-06, -8.6650e-06],
        [ 2.6869e-04,  1.6577e-04, -8.7072e-05,  ...,  2.6596e-04,
          3.0828e-05,  1.5937e-04],
        [-1.6347e-05, -1.0967e-05,  6.0126e-06,  ..., -1.4156e-05,
         -2.9393e-06, -8.9109e-06],
        [-3.3379e-05, -2.2352e-05,  1.2249e-05,  ..., -2.8908e-05,
         -6.0424e-06, -1.8194e-05]], device='cuda:0')
Loss: 1.1443767547607422


Running epoch 0, step 421, batch 421
Sampled inputs[:2]: tensor([[   0, 6477,   12,  ..., 2931,  221,  445],
        [   0, 8588, 3937,  ...,  516, 1128, 2341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7257e-05,  1.9036e-05, -8.0997e-05,  ...,  1.4220e-05,
         -1.6985e-05,  2.8950e-05],
        [-1.9059e-05, -1.2770e-05,  6.9961e-06,  ..., -1.6481e-05,
         -3.4608e-06, -1.0408e-05],
        [ 2.6516e-04,  1.6340e-04, -8.5775e-05,  ...,  2.6291e-04,
          3.0187e-05,  1.5745e-04],
        [ 8.1675e-05,  3.6628e-05, -3.8142e-05,  ...,  5.7904e-05,
          1.8879e-05,  3.5669e-05],
        [-4.0144e-05, -2.6882e-05,  1.4722e-05,  ..., -3.4750e-05,
         -7.2718e-06, -2.1875e-05]], device='cuda:0')
Loss: 1.1388081312179565


Running epoch 0, step 422, batch 422
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   493,  2104,    14],
        [    0,   300,   259,  ...,   352, 12080,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6831e-05,  3.3546e-05, -7.9819e-05,  ...,  6.0316e-05,
          2.1138e-05,  3.3624e-05],
        [-2.2277e-05, -1.4931e-05,  8.1286e-06,  ..., -1.9252e-05,
         -4.0494e-06, -1.2137e-05],
        [ 2.6158e-04,  1.6100e-04, -8.4524e-05,  ...,  2.5984e-04,
          2.9536e-05,  1.5552e-04],
        [ 7.8382e-05,  3.4422e-05, -3.6987e-05,  ...,  5.5073e-05,
          1.8283e-05,  3.3904e-05],
        [-4.6968e-05, -3.1471e-05,  1.7121e-05,  ..., -4.0621e-05,
         -8.5160e-06, -2.5541e-05]], device='cuda:0')
Loss: 1.1256262063980103


Running epoch 0, step 423, batch 423
Sampled inputs[:2]: tensor([[   0, 9611,  278,  ...,  278,  638,  600],
        [   0,  266, 2653,  ...,   29,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0306e-05,  1.5834e-05, -7.8843e-05,  ...,  5.7688e-05,
          9.3412e-06,  2.4474e-05],
        [-2.5436e-05, -1.7062e-05,  9.2834e-06,  ..., -2.1994e-05,
         -4.6380e-06, -1.3903e-05],
        [ 2.5806e-04,  1.5863e-04, -8.3235e-05,  ...,  2.5678e-04,
          2.8880e-05,  1.5356e-04],
        [ 7.5149e-05,  3.2232e-05, -3.5795e-05,  ...,  5.2257e-05,
          1.7683e-05,  3.2093e-05],
        [-5.3644e-05, -3.5971e-05,  1.9565e-05,  ..., -4.6402e-05,
         -9.7603e-06, -2.9266e-05]], device='cuda:0')
Loss: 1.1207178831100464
Graident accumulation at epoch 0, step 423, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0035,  ..., -0.0029,  0.0225, -0.0201],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1172e-05, -2.6115e-07, -5.7529e-05,  ...,  5.5152e-06,
         -1.0780e-05,  1.7952e-05],
        [-2.4273e-05, -1.7868e-05,  7.0006e-06,  ..., -2.1533e-05,
         -4.6240e-06, -1.1524e-05],
        [ 1.1018e-04,  7.9094e-05, -3.1867e-05,  ...,  1.0133e-04,
          1.7685e-05,  5.7938e-05],
        [-5.3036e-06, -6.0277e-06, -1.3051e-07,  ..., -4.7559e-06,
         -6.6914e-07, -2.6294e-06],
        [-5.3204e-05, -3.9174e-05,  1.5266e-05,  ..., -4.7094e-05,
         -9.9570e-06, -2.4943e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1620e-08, 2.7834e-08, 3.4961e-08,  ..., 1.1516e-08, 8.6964e-08,
         1.1391e-08],
        [4.8330e-11, 2.6694e-11, 3.0866e-12,  ..., 3.1811e-11, 1.8387e-12,
         7.3832e-12],
        [1.0329e-09, 5.4093e-10, 7.6502e-11,  ..., 8.7497e-10, 4.1969e-11,
         2.5950e-10],
        [6.7855e-11, 3.4646e-11, 5.1194e-12,  ..., 5.5899e-11, 2.7810e-12,
         1.2634e-11],
        [2.1305e-10, 1.1303e-10, 1.0483e-11,  ..., 1.5394e-10, 8.1023e-12,
         3.6704e-11]], device='cuda:0')
optimizer state dict: 53.0
lr: [1.8483129288732575e-05, 1.8483129288732575e-05]
scheduler_last_epoch: 53


Running epoch 0, step 424, batch 424
Sampled inputs[:2]: tensor([[   0, 8125, 5241,  ...,  328, 3227,  278],
        [   0,   12,  287,  ...,  658,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4005e-06,  1.5918e-05, -4.3001e-05,  ..., -1.3237e-05,
          4.5492e-06,  7.7614e-06],
        [-3.1441e-06, -2.0862e-06,  1.1325e-06,  ..., -2.7418e-06,
         -6.7055e-07, -1.7732e-06],
        [-3.5763e-06, -2.3693e-06,  1.2890e-06,  ..., -3.1143e-06,
         -7.5623e-07, -2.0117e-06],
        [-3.2485e-06, -2.1607e-06,  1.1772e-06,  ..., -2.8312e-06,
         -6.8545e-07, -1.8328e-06],
        [-6.5863e-06, -4.3511e-06,  2.3693e-06,  ..., -5.7220e-06,
         -1.3933e-06, -3.7104e-06]], device='cuda:0')
Loss: 1.1331900358200073


Running epoch 0, step 425, batch 425
Sampled inputs[:2]: tensor([[    0,   292,    58,  ...,   319,   221,  1061],
        [    0,   381, 13565,  ...,     9,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7570e-05,  4.1626e-05, -5.9386e-05,  ...,  1.0548e-05,
         -2.7651e-06,  2.6479e-05],
        [-6.3032e-06, -4.2170e-06,  2.3097e-06,  ..., -5.4836e-06,
         -1.3225e-06, -3.5763e-06],
        [-7.1675e-06, -4.7833e-06,  2.6301e-06,  ..., -6.2287e-06,
         -1.4901e-06, -4.0531e-06],
        [-6.4820e-06, -4.3362e-06,  2.3842e-06,  ..., -5.6326e-06,
         -1.3448e-06, -3.6731e-06],
        [-1.3262e-05, -8.8513e-06,  4.8429e-06,  ..., -1.1504e-05,
         -2.7642e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1272810697555542


Running epoch 0, step 426, batch 426
Sampled inputs[:2]: tensor([[   0,  287, 5724,  ...,  298,  591, 2609],
        [   0, 4100,   12,  ...,   13, 4710, 1558]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5871e-05,  6.4645e-05, -1.1254e-04,  ..., -2.2495e-05,
          2.1778e-05,  3.7950e-05],
        [-9.4473e-06, -6.3181e-06,  3.4869e-06,  ..., -8.2403e-06,
         -1.9930e-06, -5.3793e-06],
        [-1.0714e-05, -7.1377e-06,  3.9563e-06,  ..., -9.3281e-06,
         -2.2426e-06, -6.0797e-06],
        [-9.7156e-06, -6.4820e-06,  3.5986e-06,  ..., -8.4639e-06,
         -2.0266e-06, -5.5209e-06],
        [-1.9848e-05, -1.3232e-05,  7.3165e-06,  ..., -1.7285e-05,
         -4.1574e-06, -1.1265e-05]], device='cuda:0')
Loss: 1.1518394947052002


Running epoch 0, step 427, batch 427
Sampled inputs[:2]: tensor([[    0, 47684,   292,  ...,   287, 49958, 22022],
        [    0,   957,   680,  ...,  2573,   669,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4422e-04,  6.6811e-05, -1.8157e-04,  ..., -3.2181e-05,
          1.4213e-05,  2.6734e-05],
        [-1.2577e-05, -8.3894e-06,  4.6641e-06,  ..., -1.1012e-05,
         -2.6375e-06, -7.1749e-06],
        [-1.4246e-05, -9.4771e-06,  5.2899e-06,  ..., -1.2457e-05,
         -2.9691e-06, -8.1062e-06],
        [-1.2964e-05, -8.6427e-06,  4.8280e-06,  ..., -1.1340e-05,
         -2.6934e-06, -7.3835e-06],
        [-2.6345e-05, -1.7524e-05,  9.7752e-06,  ..., -2.3037e-05,
         -5.4911e-06, -1.4991e-05]], device='cuda:0')
Loss: 1.1415501832962036


Running epoch 0, step 428, batch 428
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0, 1064,  266,  ..., 2971,  292,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2060e-04,  2.4283e-05, -2.0482e-04,  ..., -3.4069e-05,
          2.0244e-05,  1.9280e-05],
        [-1.5780e-05, -1.0490e-05,  5.8115e-06,  ..., -1.3754e-05,
         -3.2671e-06, -8.9630e-06],
        [-1.7852e-05, -1.1846e-05,  6.5789e-06,  ..., -1.5542e-05,
         -3.6731e-06, -1.0118e-05],
        [-1.6242e-05, -1.0788e-05,  5.9977e-06,  ..., -1.4141e-05,
         -3.3267e-06, -9.2089e-06],
        [-3.3021e-05, -2.1905e-05,  1.2159e-05,  ..., -2.8759e-05,
         -6.7949e-06, -1.8716e-05]], device='cuda:0')
Loss: 1.1225987672805786


Running epoch 0, step 429, batch 429
Sampled inputs[:2]: tensor([[    0,  4601,   328,  ..., 10258,  2282,    12],
        [    0,   822,  5085,  ...,   293,  1608,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1487e-04,  1.2754e-05, -2.0714e-04,  ..., -5.9735e-05,
          1.6962e-05, -2.3127e-05],
        [-1.8924e-05, -1.2577e-05,  6.9588e-06,  ..., -1.6496e-05,
         -3.8818e-06, -1.0751e-05],
        [-2.1428e-05, -1.4231e-05,  7.8976e-06,  ..., -1.8671e-05,
         -4.3698e-06, -1.2159e-05],
        [-1.9521e-05, -1.2964e-05,  7.1973e-06,  ..., -1.7002e-05,
         -3.9600e-06, -1.1064e-05],
        [-3.9667e-05, -2.6315e-05,  1.4588e-05,  ..., -3.4541e-05,
         -8.0839e-06, -2.2486e-05]], device='cuda:0')
Loss: 1.1314769983291626


Running epoch 0, step 430, batch 430
Sampled inputs[:2]: tensor([[    0,   287,  6761,  ...,  1918, 33351,    12],
        [    0,   266,  1254,  ...,   369,  2870,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2649e-04,  3.3388e-05, -2.0033e-04,  ..., -9.8551e-05,
          4.0437e-05, -1.4586e-05],
        [-2.2113e-05, -1.4678e-05,  8.0913e-06,  ..., -1.9282e-05,
         -4.5523e-06, -1.2584e-05],
        [-2.5034e-05, -1.6615e-05,  9.1866e-06,  ..., -2.1815e-05,
         -5.1260e-06, -1.4231e-05],
        [-2.2799e-05, -1.5125e-05,  8.3670e-06,  ..., -1.9863e-05,
         -4.6454e-06, -1.2942e-05],
        [-4.6402e-05, -3.0786e-05,  1.6987e-05,  ..., -4.0412e-05,
         -9.4995e-06, -2.6360e-05]], device='cuda:0')
Loss: 1.147593379020691


Running epoch 0, step 431, batch 431
Sampled inputs[:2]: tensor([[    0,   474,   706,  ...,    83, 38084,   475],
        [    0,  1086,  5564,  ..., 29319, 32982,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3909e-04,  1.0004e-04, -2.1257e-04,  ..., -8.9880e-05,
         -5.6598e-06,  2.7471e-05],
        [-2.5257e-05, -1.6809e-05,  9.2685e-06,  ..., -2.2039e-05,
         -5.2713e-06, -1.4424e-05],
        [-2.8580e-05, -1.9014e-05,  1.0513e-05,  ..., -2.4915e-05,
         -5.9307e-06, -1.6302e-05],
        [-2.6003e-05, -1.7285e-05,  9.5740e-06,  ..., -2.2665e-05,
         -5.3719e-06, -1.4812e-05],
        [-5.2959e-05, -3.5226e-05,  1.9446e-05,  ..., -4.6164e-05,
         -1.0997e-05, -3.0175e-05]], device='cuda:0')
Loss: 1.1151522397994995
Graident accumulation at epoch 0, step 431, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0029,  0.0225, -0.0200],
        [ 0.0290, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0273,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.9963e-05,  9.7690e-06, -7.3033e-05,  ..., -4.0243e-06,
         -1.0268e-05,  1.8904e-05],
        [-2.4372e-05, -1.7762e-05,  7.2274e-06,  ..., -2.1583e-05,
         -4.6887e-06, -1.1814e-05],
        [ 9.6303e-05,  6.9283e-05, -2.7629e-05,  ...,  8.8709e-05,
          1.5324e-05,  5.0514e-05],
        [-7.3735e-06, -7.1535e-06,  8.3994e-07,  ..., -6.5468e-06,
         -1.1394e-06, -3.8476e-06],
        [-5.3179e-05, -3.8779e-05,  1.5684e-05,  ..., -4.7001e-05,
         -1.0061e-05, -2.5466e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1598e-08, 2.7816e-08, 3.4971e-08,  ..., 1.1512e-08, 8.6877e-08,
         1.1380e-08],
        [4.8919e-11, 2.6950e-11, 3.1694e-12,  ..., 3.2265e-11, 1.8646e-12,
         7.5839e-12],
        [1.0326e-09, 5.4075e-10, 7.6536e-11,  ..., 8.7472e-10, 4.1963e-11,
         2.5951e-10],
        [6.8463e-11, 3.4911e-11, 5.2060e-12,  ..., 5.6357e-11, 2.8071e-12,
         1.2841e-11],
        [2.1564e-10, 1.1415e-10, 1.0851e-11,  ..., 1.5591e-10, 8.2151e-12,
         3.7578e-11]], device='cuda:0')
optimizer state dict: 54.0
lr: [1.8417028402436446e-05, 1.8417028402436446e-05]
scheduler_last_epoch: 54


Running epoch 0, step 432, batch 432
Sampled inputs[:2]: tensor([[    0,   352,   721,  ...,   634, 17642,   278],
        [    0,   335,   446,  ...,  5795,    12, 12433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4156e-06, -4.0589e-05, -1.0763e-05,  ...,  4.8754e-07,
         -1.4497e-05,  2.0985e-05],
        [-3.1292e-06, -2.1011e-06,  1.1846e-06,  ..., -2.6822e-06,
         -6.6683e-07, -1.7956e-06],
        [-3.6508e-06, -2.4587e-06,  1.3858e-06,  ..., -3.1292e-06,
         -7.7486e-07, -2.1011e-06],
        [-3.2932e-06, -2.2054e-06,  1.2442e-06,  ..., -2.8163e-06,
         -6.9663e-07, -1.8924e-06],
        [-6.6161e-06, -4.4405e-06,  2.5034e-06,  ..., -5.6624e-06,
         -1.4082e-06, -3.7998e-06]], device='cuda:0')
Loss: 1.1257776021957397


Running epoch 0, step 433, batch 433
Sampled inputs[:2]: tensor([[   0,   12, 3067,  ..., 1381,  278, 5011],
        [   0,  287, 1477,  ...,  997,  292, 4471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3734e-05, -5.4347e-05, -4.1840e-05,  ..., -2.4464e-05,
         -2.1051e-05,  5.4345e-05],
        [-6.2585e-06, -4.1723e-06,  2.3544e-06,  ..., -5.3644e-06,
         -1.3709e-06, -3.6061e-06],
        [-7.2867e-06, -4.8578e-06,  2.7418e-06,  ..., -6.2287e-06,
         -1.5870e-06, -4.2021e-06],
        [-6.5863e-06, -4.3809e-06,  2.4736e-06,  ..., -5.6326e-06,
         -1.4268e-06, -3.7923e-06],
        [-1.3232e-05, -8.7917e-06,  4.9770e-06,  ..., -1.1295e-05,
         -2.8834e-06, -7.6145e-06]], device='cuda:0')
Loss: 1.135914921760559


Running epoch 0, step 434, batch 434
Sampled inputs[:2]: tensor([[    0,  1057,    14,  ...,    14,  4735,    13],
        [    0, 12324,  7368,  ...,   365,   726,  3595]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0215e-05, -2.9840e-05, -4.2473e-05,  ..., -1.3357e-05,
         -2.3197e-05,  2.6605e-05],
        [-9.4026e-06, -6.2436e-06,  3.5241e-06,  ..., -8.0317e-06,
         -2.0973e-06, -5.4836e-06],
        [ 8.0451e-05,  4.2569e-05, -2.8014e-05,  ...,  7.5787e-05,
          2.7432e-05,  3.3600e-05],
        [-9.8646e-06, -6.5416e-06,  3.6880e-06,  ..., -8.4192e-06,
         -2.1718e-06, -5.7444e-06],
        [-1.9848e-05, -1.3113e-05,  7.4208e-06,  ..., -1.6898e-05,
         -4.4033e-06, -1.1548e-05]], device='cuda:0')
Loss: 1.1347591876983643


Running epoch 0, step 435, batch 435
Sampled inputs[:2]: tensor([[    0, 32444,    41,  ...,    14,    18,    59],
        [    0,  5625,  2558,  ...,   680,   292,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8082e-05, -7.4703e-05, -5.0937e-05,  ..., -1.3357e-05,
         -2.9568e-06,  3.9759e-05],
        [-1.2547e-05, -8.3297e-06,  4.7311e-06,  ..., -1.0699e-05,
         -2.8238e-06, -7.3314e-06],
        [ 7.6771e-05,  4.0125e-05, -2.6599e-05,  ...,  7.2673e-05,
          2.6582e-05,  3.1439e-05],
        [-1.3128e-05, -8.7023e-06,  4.9472e-06,  ..., -1.1176e-05,
         -2.9206e-06, -7.6666e-06],
        [-2.6494e-05, -1.7494e-05,  9.9689e-06,  ..., -2.2501e-05,
         -5.9307e-06, -1.5453e-05]], device='cuda:0')
Loss: 1.1297367811203003


Running epoch 0, step 436, batch 436
Sampled inputs[:2]: tensor([[    0,    20,     9,  ...,    12,  2212, 24950],
        [    0,  1941,   437,  ..., 16539,  4129,  4156]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1673e-05, -9.9828e-05, -7.5960e-05,  ...,  2.2747e-05,
         -2.6240e-05,  1.8044e-05],
        [-1.5676e-05, -1.0401e-05,  5.9083e-06,  ..., -1.3351e-05,
         -3.4980e-06, -9.1940e-06],
        [ 7.3120e-05,  3.7711e-05, -2.5228e-05,  ...,  6.9574e-05,
          2.5800e-05,  2.9264e-05],
        [-1.6406e-05, -1.0863e-05,  6.1765e-06,  ..., -1.3947e-05,
         -3.6210e-06, -9.6187e-06],
        [-3.3081e-05, -2.1845e-05,  1.2442e-05,  ..., -2.8104e-05,
         -7.3463e-06, -1.9386e-05]], device='cuda:0')
Loss: 1.1285988092422485


Running epoch 0, step 437, batch 437
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0,  278,  565,  ..., 1125, 5222,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0872e-05, -1.2058e-04, -1.3117e-04,  ...,  1.1047e-05,
          2.8247e-06, -8.5374e-07],
        [-1.8820e-05, -1.2487e-05,  7.0930e-06,  ..., -1.6049e-05,
         -4.2431e-06, -1.1057e-05],
        [ 6.9484e-05,  3.5297e-05, -2.3857e-05,  ...,  6.6444e-05,
          2.4943e-05,  2.7118e-05],
        [-1.9655e-05, -1.3009e-05,  7.4059e-06,  ..., -1.6749e-05,
         -4.3847e-06, -1.1541e-05],
        [-3.9607e-05, -2.6166e-05,  1.4916e-05,  ..., -3.3736e-05,
         -8.8885e-06, -2.3261e-05]], device='cuda:0')
Loss: 1.1309541463851929


Running epoch 0, step 438, batch 438
Sampled inputs[:2]: tensor([[   0, 5150, 1030,  ...,   14,  475, 1763],
        [   0,   14, 8383,  ...,  266, 1717,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7951e-05, -1.0004e-04, -1.6420e-04,  ...,  9.5206e-06,
          2.8916e-06,  1.2374e-05],
        [-2.1964e-05, -1.4529e-05,  8.2701e-06,  ..., -1.8746e-05,
         -4.9397e-06, -1.2912e-05],
        [ 6.5848e-05,  3.2928e-05, -2.2493e-05,  ...,  6.3330e-05,
          2.4138e-05,  2.4972e-05],
        [-2.2933e-05, -1.5140e-05,  8.6352e-06,  ..., -1.9565e-05,
         -5.1074e-06, -1.3478e-05],
        [-4.6164e-05, -3.0428e-05,  1.7375e-05,  ..., -3.9369e-05,
         -1.0341e-05, -2.7135e-05]], device='cuda:0')
Loss: 1.1272855997085571


Running epoch 0, step 439, batch 439
Sampled inputs[:2]: tensor([[    0,   287, 14752,  ...,   910, 26097,  1477],
        [    0,  4120,   278,  ...,   298,   273,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5256e-05, -1.2457e-04, -1.4588e-04,  ...,  2.5046e-05,
          8.7674e-06, -9.0485e-06],
        [-2.5108e-05, -1.6615e-05,  9.4250e-06,  ..., -2.1413e-05,
         -5.6364e-06, -1.4745e-05],
        [ 1.6841e-04,  7.3771e-05, -4.0786e-05,  ...,  1.6483e-04,
          5.6269e-05,  7.8043e-05],
        [-2.6211e-05, -1.7315e-05,  9.8422e-06,  ..., -2.2337e-05,
         -5.8301e-06, -1.5385e-05],
        [-5.2810e-05, -3.4839e-05,  1.9804e-05,  ..., -4.4972e-05,
         -1.1809e-05, -3.1009e-05]], device='cuda:0')
Loss: 1.1351209878921509
Graident accumulation at epoch 0, step 439, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0289, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0274,  ...,  0.0283, -0.0156, -0.0183]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.0492e-05, -3.6650e-06, -8.0317e-05,  ..., -1.1172e-06,
         -8.3647e-06,  1.6108e-05],
        [-2.4445e-05, -1.7647e-05,  7.4471e-06,  ..., -2.1566e-05,
         -4.7835e-06, -1.2107e-05],
        [ 1.0351e-04,  6.9732e-05, -2.8945e-05,  ...,  9.6321e-05,
          1.9418e-05,  5.3267e-05],
        [-9.2572e-06, -8.1696e-06,  1.7402e-06,  ..., -8.1258e-06,
         -1.6085e-06, -5.0014e-06],
        [-5.3142e-05, -3.8385e-05,  1.6096e-05,  ..., -4.6798e-05,
         -1.0236e-05, -2.6020e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1561e-08, 2.7804e-08, 3.4957e-08,  ..., 1.1501e-08, 8.6790e-08,
         1.1369e-08],
        [4.9501e-11, 2.7199e-11, 3.2551e-12,  ..., 3.2691e-11, 1.8945e-12,
         7.7937e-12],
        [1.0600e-09, 5.4565e-10, 7.8123e-11,  ..., 9.0101e-10, 4.5087e-11,
         2.6534e-10],
        [6.9082e-11, 3.5175e-11, 5.2976e-12,  ..., 5.6799e-11, 2.8382e-12,
         1.3065e-11],
        [2.1822e-10, 1.1525e-10, 1.1232e-11,  ..., 1.5778e-10, 8.3464e-12,
         3.8502e-11]], device='cuda:0')
optimizer state dict: 55.0
lr: [1.8349641320727145e-05, 1.8349641320727145e-05]
scheduler_last_epoch: 55


Running epoch 0, step 440, batch 440
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,  1380,   342,  ...,  3904,   259,   624]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1974e-05, -2.0197e-05,  2.5057e-05,  ..., -7.3345e-06,
          2.9710e-05, -8.2384e-06],
        [-3.1441e-06, -2.0117e-06,  1.1325e-06,  ..., -2.6673e-06,
         -8.0094e-07, -1.8999e-06],
        [-3.7253e-06, -2.3842e-06,  1.3486e-06,  ..., -3.1590e-06,
         -9.4622e-07, -2.2501e-06],
        [-3.2932e-06, -2.1160e-06,  1.1921e-06,  ..., -2.8014e-06,
         -8.3074e-07, -1.9968e-06],
        [-6.6459e-06, -4.2617e-06,  2.3991e-06,  ..., -5.6326e-06,
         -1.6913e-06, -4.0233e-06]], device='cuda:0')
Loss: 1.1226882934570312


Running epoch 0, step 441, batch 441
Sampled inputs[:2]: tensor([[    0,   266,  4908,  ...,  1209,   328,  1603],
        [    0,  9419,   221,  ...,    15, 22168,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6217e-05, -5.9839e-05, -1.8500e-05,  ..., -1.4568e-05,
          3.1996e-05, -6.0832e-05],
        [-6.2436e-06, -4.0382e-06,  2.2948e-06,  ..., -5.3346e-06,
         -1.5534e-06, -3.7700e-06],
        [-7.4357e-06, -4.8131e-06,  2.7418e-06,  ..., -6.3479e-06,
         -1.8440e-06, -4.4852e-06],
        [-6.5714e-06, -4.2766e-06,  2.4214e-06,  ..., -5.6326e-06,
         -1.6205e-06, -3.9786e-06],
        [-1.3173e-05, -8.5235e-06,  4.8429e-06,  ..., -1.1235e-05,
         -3.2708e-06, -7.9572e-06]], device='cuda:0')
Loss: 1.1305804252624512


Running epoch 0, step 442, batch 442
Sampled inputs[:2]: tensor([[    0, 26473,  2117,  ...,    13,  3292,   950],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5417e-05, -3.9167e-05, -6.3358e-05,  ..., -1.7065e-05,
         -5.7319e-06, -4.3581e-05],
        [-9.3281e-06, -6.0946e-06,  3.4794e-06,  ..., -7.9572e-06,
         -2.3916e-06, -5.6773e-06],
        [-1.1101e-05, -7.2718e-06,  4.1574e-06,  ..., -9.4771e-06,
         -2.8349e-06, -6.7651e-06],
        [-9.8199e-06, -6.4522e-06,  3.6731e-06,  ..., -8.4043e-06,
         -2.4959e-06, -5.9903e-06],
        [-1.9640e-05, -1.2845e-05,  7.3314e-06,  ..., -1.6749e-05,
         -5.0217e-06, -1.1951e-05]], device='cuda:0')
Loss: 1.1152527332305908


Running epoch 0, step 443, batch 443
Sampled inputs[:2]: tensor([[    0,  2386,  4012,  ...,   300, 15480,  1036],
        [    0,   471,    14,  ..., 27104,     9,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0566e-06, -4.5238e-05, -9.8789e-05,  ..., -6.4137e-06,
         -2.3640e-05, -2.9602e-05],
        [-1.2428e-05, -8.1509e-06,  4.6566e-06,  ..., -1.0550e-05,
         -3.1441e-06, -7.5325e-06],
        [-1.4812e-05, -9.7305e-06,  5.5656e-06,  ..., -1.2591e-05,
         -3.7290e-06, -8.9854e-06],
        [-1.3128e-05, -8.6427e-06,  4.9248e-06,  ..., -1.1176e-05,
         -3.2894e-06, -7.9572e-06],
        [ 1.3375e-04,  1.2485e-04, -3.9935e-05,  ...,  1.4339e-04,
          3.0852e-05,  1.1526e-04]], device='cuda:0')
Loss: 1.1177246570587158


Running epoch 0, step 444, batch 444
Sampled inputs[:2]: tensor([[    0, 10296,   809,  ..., 27683,    12,   287],
        [    0,    13,  2549,  ...,   221,   382,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8857e-05,  1.9078e-07, -8.7673e-05,  ..., -3.3210e-05,
          2.6820e-05, -4.9372e-05],
        [-1.5512e-05, -1.0207e-05,  5.8338e-06,  ..., -1.3202e-05,
         -3.9376e-06, -9.4399e-06],
        [-1.8418e-05, -1.2144e-05,  6.9439e-06,  ..., -1.5706e-05,
         -4.6566e-06, -1.1221e-05],
        [-1.6347e-05, -1.0788e-05,  6.1542e-06,  ..., -1.3947e-05,
         -4.1164e-06, -9.9540e-06],
        [ 1.2734e-04,  1.2059e-04, -3.7492e-05,  ...,  1.3788e-04,
          2.9198e-05,  1.1129e-04]], device='cuda:0')
Loss: 1.1247615814208984


Running epoch 0, step 445, batch 445
Sampled inputs[:2]: tensor([[    0,    13, 20773,  ..., 22463,  2587,   292],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6894e-05, -1.3781e-06, -1.1301e-04,  ..., -4.4414e-05,
          2.2986e-05, -2.8662e-05],
        [-1.8582e-05, -1.2249e-05,  6.9812e-06,  ..., -1.5825e-05,
         -4.6790e-06, -1.1288e-05],
        [ 1.5506e-04,  1.0599e-04, -4.1898e-05,  ...,  1.3898e-04,
          2.6184e-05,  9.0226e-05],
        [-1.9610e-05, -1.2949e-05,  7.3761e-06,  ..., -1.6734e-05,
         -4.8988e-06, -1.1906e-05],
        [ 1.2081e-04,  1.1627e-04, -3.5048e-05,  ...,  1.3231e-04,
          2.7626e-05,  1.0739e-04]], device='cuda:0')
Loss: 1.125667691230774


Running epoch 0, step 446, batch 446
Sampled inputs[:2]: tensor([[   0, 1594,  586,  ...,   13,  701,  308],
        [   0,   29,  413,  ..., 2001, 1027,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0042e-05,  8.3140e-05, -7.9811e-05,  ..., -4.2110e-05,
          1.8701e-05, -8.2490e-06],
        [-2.1741e-05, -1.4320e-05,  8.1137e-06,  ..., -1.8492e-05,
         -5.5693e-06, -1.3210e-05],
        [ 1.5130e-04,  1.0354e-04, -4.0549e-05,  ...,  1.3581e-04,
          2.5134e-05,  8.7946e-05],
        [-2.2903e-05, -1.5110e-05,  8.5607e-06,  ..., -1.9506e-05,
         -5.8152e-06, -1.3903e-05],
        [ 1.1414e-04,  1.1189e-04, -3.2664e-05,  ...,  1.2667e-04,
          2.5756e-05,  1.0334e-04]], device='cuda:0')
Loss: 1.1105611324310303


Running epoch 0, step 447, batch 447
Sampled inputs[:2]: tensor([[    0,   935,   508,  ...,   287, 41582,    12],
        [    0,   273,   298,  ...,  7437,  2767,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8349e-05,  9.4235e-05, -1.2424e-04,  ..., -7.2807e-05,
          2.3539e-05,  8.2862e-06],
        [-2.4825e-05, -1.6376e-05,  9.2760e-06,  ..., -2.1160e-05,
         -6.3404e-06, -1.5087e-05],
        [ 1.4765e-04,  1.0109e-04, -3.9179e-05,  ...,  1.3265e-04,
          2.4225e-05,  8.5726e-05],
        [-2.6152e-05, -1.7285e-05,  9.7826e-06,  ..., -2.2307e-05,
         -6.6161e-06, -1.5870e-05],
        [ 1.0767e-04,  1.0757e-04, -3.0235e-05,  ...,  1.2110e-04,
          2.4154e-05,  9.9403e-05]], device='cuda:0')
Loss: 1.128736972808838
Graident accumulation at epoch 0, step 447, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0289, -0.0078,  0.0034,  ..., -0.0096, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0147, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.9278e-05,  6.1250e-06, -8.4710e-05,  ..., -8.2861e-06,
         -5.1743e-06,  1.5326e-05],
        [-2.4483e-05, -1.7520e-05,  7.6300e-06,  ..., -2.1526e-05,
         -4.9392e-06, -1.2405e-05],
        [ 1.0793e-04,  7.2868e-05, -2.9968e-05,  ...,  9.9954e-05,
          1.9899e-05,  5.6513e-05],
        [-1.0947e-05, -9.0812e-06,  2.5444e-06,  ..., -9.5439e-06,
         -2.1092e-06, -6.0883e-06],
        [-3.7061e-05, -2.3790e-05,  1.1463e-05,  ..., -3.0008e-05,
         -6.7968e-06, -1.3478e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1521e-08, 2.7785e-08, 3.4938e-08,  ..., 1.1495e-08, 8.6704e-08,
         1.1358e-08],
        [5.0068e-11, 2.7440e-11, 3.3379e-12,  ..., 3.3106e-11, 1.9328e-12,
         8.0136e-12],
        [1.0807e-09, 5.5533e-10, 7.9580e-11,  ..., 9.1771e-10, 4.5629e-11,
         2.7242e-10],
        [6.9696e-11, 3.5439e-11, 5.3880e-12,  ..., 5.7240e-11, 2.8792e-12,
         1.3304e-11],
        [2.2959e-10, 1.2671e-10, 1.2135e-11,  ..., 1.7229e-10, 8.9214e-12,
         4.8345e-11]], device='cuda:0')
optimizer state dict: 56.0
lr: [1.828097834093899e-05, 1.828097834093899e-05]
scheduler_last_epoch: 56


Running epoch 0, step 448, batch 448
Sampled inputs[:2]: tensor([[   0,   12,  266,  ...,   13,  635,   13],
        [   0, 1890,  278,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9021e-06,  7.4113e-06, -5.1140e-05,  ...,  2.2321e-05,
         -1.7673e-05, -1.9399e-05],
        [-2.9951e-06, -2.0117e-06,  1.1474e-06,  ..., -2.5779e-06,
         -8.2701e-07, -1.8999e-06],
        [-3.7104e-06, -2.5034e-06,  1.4231e-06,  ..., -3.1888e-06,
         -1.0133e-06, -2.3395e-06],
        [-3.2783e-06, -2.2054e-06,  1.2591e-06,  ..., -2.8163e-06,
         -8.8662e-07, -2.0713e-06],
        [-6.3479e-06, -4.2617e-06,  2.4438e-06,  ..., -5.4538e-06,
         -1.7360e-06, -4.0233e-06]], device='cuda:0')
Loss: 1.1128854751586914


Running epoch 0, step 449, batch 449
Sampled inputs[:2]: tensor([[   0,   14,  292,  ..., 1385,   12,  287],
        [   0, 5982, 9385,  ...,   26,  469,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6257e-06,  2.1858e-05, -8.3830e-05,  ...,  3.1681e-05,
         -2.1831e-05, -3.1743e-06],
        [-6.0946e-06, -4.0382e-06,  2.2799e-06,  ..., -5.2005e-06,
         -1.7248e-06, -3.8520e-06],
        [-7.5251e-06, -5.0068e-06,  2.8163e-06,  ..., -6.4224e-06,
         -2.1160e-06, -4.7386e-06],
        [-6.5863e-06, -4.3660e-06,  2.4736e-06,  ..., -5.6177e-06,
         -1.8403e-06, -4.1574e-06],
        [-1.2904e-05, -8.5533e-06,  4.8429e-06,  ..., -1.0997e-05,
         -3.6284e-06, -8.1360e-06]], device='cuda:0')
Loss: 1.1319242715835571


Running epoch 0, step 450, batch 450
Sampled inputs[:2]: tensor([[    0,   369, 17432,  ...,   874,  2577,    14],
        [    0, 20291,  1990,  ...,   298,   732,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1082e-06, -4.2687e-06, -1.0154e-04,  ...,  3.4956e-05,
         -1.0483e-05,  1.5277e-05],
        [-9.1344e-06, -6.0350e-06,  3.4198e-06,  ..., -7.8082e-06,
         -2.5705e-06, -5.7369e-06],
        [-1.1280e-05, -7.4804e-06,  4.2245e-06,  ..., -9.6560e-06,
         -3.1590e-06, -7.0632e-06],
        [-9.8497e-06, -6.5118e-06,  3.6955e-06,  ..., -8.4192e-06,
         -2.7418e-06, -6.1691e-06],
        [-1.9431e-05, -1.2845e-05,  7.2867e-06,  ..., -1.6600e-05,
         -5.4389e-06, -1.2159e-05]], device='cuda:0')
Loss: 1.1503746509552002


Running epoch 0, step 451, batch 451
Sampled inputs[:2]: tensor([[    0,   767,  1345,  ...,   276,   327,   328],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5583e-06,  1.0902e-05, -7.1320e-05,  ...,  5.7648e-05,
         -2.1213e-05,  2.3481e-05],
        [-1.2189e-05, -8.0466e-06,  4.5300e-06,  ..., -1.0386e-05,
         -3.4869e-06, -7.6443e-06],
        [-1.5080e-05, -9.9838e-06,  5.6028e-06,  ..., -1.2860e-05,
         -4.2915e-06, -9.4324e-06],
        [-1.3113e-05, -8.6427e-06,  4.8801e-06,  ..., -1.1161e-05,
         -3.7104e-06, -8.1956e-06],
        [-2.5988e-05, -1.7136e-05,  9.6560e-06,  ..., -2.2113e-05,
         -7.3910e-06, -1.6242e-05]], device='cuda:0')
Loss: 1.1108250617980957


Running epoch 0, step 452, batch 452
Sampled inputs[:2]: tensor([[   0, 6508, 4305,  ...,  806, 3888, 4431],
        [   0, 1795,  650,  ...,  516, 2793, 1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7413e-05,  4.6258e-05, -8.0578e-05,  ...,  8.2866e-05,
         -4.9542e-05,  1.6577e-05],
        [-1.5244e-05, -1.0014e-05,  5.6103e-06,  ..., -1.2949e-05,
         -4.3474e-06, -9.5442e-06],
        [-1.8895e-05, -1.2457e-05,  6.9588e-06,  ..., -1.6078e-05,
         -5.3644e-06, -1.1817e-05],
        [-1.6406e-05, -1.0774e-05,  6.0424e-06,  ..., -1.3918e-05,
         -4.6268e-06, -1.0237e-05],
        [-3.2663e-05, -2.1458e-05,  1.2010e-05,  ..., -2.7686e-05,
         -9.2611e-06, -2.0385e-05]], device='cuda:0')
Loss: 1.0977801084518433


Running epoch 0, step 453, batch 453
Sampled inputs[:2]: tensor([[   0,  944,  278,  ..., 2374,  699, 8867],
        [   0, 9818,  347,  ...,  413, 7359,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8838e-05,  9.1317e-05, -7.8841e-05,  ...,  6.2084e-05,
         -5.1710e-05,  6.5085e-05],
        [-1.8328e-05, -1.2025e-05,  6.7279e-06,  ..., -1.5557e-05,
         -5.2340e-06, -1.1466e-05],
        [-2.2709e-05, -1.4946e-05,  8.3372e-06,  ..., -1.9297e-05,
         -6.4522e-06, -1.4201e-05],
        [-1.9684e-05, -1.2919e-05,  7.2345e-06,  ..., -1.6689e-05,
         -5.5581e-06, -1.2279e-05],
        [-3.9220e-05, -2.5719e-05,  1.4380e-05,  ..., -3.3200e-05,
         -1.1131e-05, -2.4468e-05]], device='cuda:0')
Loss: 1.13318932056427


Running epoch 0, step 454, batch 454
Sampled inputs[:2]: tensor([[    0,   768,  3227,  ...,  3487,    13, 31431],
        [    0,   591, 18622,  ...,   955,  6118,  9191]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5016e-05,  6.1307e-05, -7.3586e-05,  ...,  6.8079e-05,
         -7.9617e-05,  8.7893e-05],
        [-2.1398e-05, -1.4052e-05,  7.8604e-06,  ..., -1.8105e-05,
         -6.0536e-06, -1.3344e-05],
        [-2.6524e-05, -1.7464e-05,  9.7454e-06,  ..., -2.2471e-05,
         -7.4655e-06, -1.6540e-05],
        [-2.3022e-05, -1.5125e-05,  8.4639e-06,  ..., -1.9476e-05,
         -6.4448e-06, -1.4320e-05],
        [-4.5776e-05, -3.0011e-05,  1.6779e-05,  ..., -3.8624e-05,
         -1.2875e-05, -2.8461e-05]], device='cuda:0')
Loss: 1.1262009143829346


Running epoch 0, step 455, batch 455
Sampled inputs[:2]: tensor([[    0,  1526,   341,  ...,   271,  4401,  3341],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4580e-05,  8.7283e-05, -1.0561e-04,  ...,  1.0132e-04,
         -1.2239e-04,  1.2496e-04],
        [-2.4423e-05, -1.6093e-05,  9.0152e-06,  ..., -2.0683e-05,
         -6.9141e-06, -1.5207e-05],
        [-3.0309e-05, -2.0027e-05,  1.1191e-05,  ..., -2.5705e-05,
         -8.5384e-06, -1.8865e-05],
        [-2.6301e-05, -1.7345e-05,  9.7156e-06,  ..., -2.2277e-05,
         -7.3686e-06, -1.6332e-05],
        [-5.2273e-05, -3.4422e-05,  1.9252e-05,  ..., -4.4197e-05,
         -1.4715e-05, -3.2455e-05]], device='cuda:0')
Loss: 1.1113173961639404
Graident accumulation at epoch 0, step 455, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0225, -0.0200],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0012],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5892e-05,  1.4241e-05, -8.6800e-05,  ...,  2.6745e-06,
         -1.6896e-05,  2.6290e-05],
        [-2.4477e-05, -1.7377e-05,  7.7685e-06,  ..., -2.1441e-05,
         -5.1367e-06, -1.2685e-05],
        [ 9.4104e-05,  6.3579e-05, -2.5852e-05,  ...,  8.7388e-05,
          1.7055e-05,  4.8975e-05],
        [-1.2482e-05, -9.9076e-06,  3.2615e-06,  ..., -1.0817e-05,
         -2.6352e-06, -7.1126e-06],
        [-3.8582e-05, -2.4853e-05,  1.2242e-05,  ..., -3.1427e-05,
         -7.5887e-06, -1.5376e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1485e-08, 2.7765e-08, 3.4914e-08,  ..., 1.1494e-08, 8.6632e-08,
         1.1362e-08],
        [5.0614e-11, 2.7671e-11, 3.4158e-12,  ..., 3.3501e-11, 1.9787e-12,
         8.2368e-12],
        [1.0805e-09, 5.5517e-10, 7.9625e-11,  ..., 9.1745e-10, 4.5656e-11,
         2.7250e-10],
        [7.0318e-11, 3.5704e-11, 5.4770e-12,  ..., 5.7679e-11, 2.9306e-12,
         1.3557e-11],
        [2.3209e-10, 1.2777e-10, 1.2493e-11,  ..., 1.7407e-10, 9.1291e-12,
         4.9350e-11]], device='cuda:0')
optimizer state dict: 57.0
lr: [1.8211049955374658e-05, 1.8211049955374658e-05]
scheduler_last_epoch: 57


Running epoch 0, step 456, batch 456
Sampled inputs[:2]: tensor([[    0, 32878,   593,  ...,   437,  1329,   644],
        [    0,   328,  6875,  ...,   369,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4159e-05, -3.2332e-06, -3.9283e-06,  ..., -1.2191e-06,
          7.0816e-06,  1.3648e-05],
        [-2.9951e-06, -2.0117e-06,  1.0878e-06,  ..., -2.5183e-06,
         -9.0525e-07, -1.8775e-06],
        [-3.8445e-06, -2.5779e-06,  1.3933e-06,  ..., -3.2336e-06,
         -1.1548e-06, -2.3991e-06],
        [-3.3379e-06, -2.2352e-06,  1.2144e-06,  ..., -2.8163e-06,
         -9.9838e-07, -2.0862e-06],
        [-6.4373e-06, -4.3213e-06,  2.3395e-06,  ..., -5.4240e-06,
         -1.9372e-06, -4.0233e-06]], device='cuda:0')
Loss: 1.127325177192688


Running epoch 0, step 457, batch 457
Sampled inputs[:2]: tensor([[    0,   292, 44809,  ...,   642,   437,  9038],
        [    0, 25938,   359,  ...,    36, 15859,   504]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6142e-05,  5.0799e-05, -3.4789e-05,  ...,  1.2317e-05,
         -2.3328e-05, -2.3451e-05],
        [-5.9605e-06, -3.9935e-06,  2.1979e-06,  ..., -5.0217e-06,
         -1.8440e-06, -3.7998e-06],
        [-7.6294e-06, -5.1111e-06,  2.8163e-06,  ..., -6.4373e-06,
         -2.3469e-06, -4.8429e-06],
        [-6.6310e-06, -4.4405e-06,  2.4512e-06,  ..., -5.6028e-06,
         -2.0266e-06, -4.2170e-06],
        [-1.2755e-05, -8.5533e-06,  4.7088e-06,  ..., -1.0759e-05,
         -3.9190e-06, -8.1062e-06]], device='cuda:0')
Loss: 1.1159366369247437


Running epoch 0, step 458, batch 458
Sampled inputs[:2]: tensor([[    0,    14,  3948,  ...,   571, 10097,    12],
        [    0,   546,   360,  ...,    12,   461,  8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3996e-05,  5.4002e-05, -4.0976e-05,  ...,  3.7997e-05,
         -5.3007e-05, -5.9516e-05],
        [-8.9556e-06, -6.0052e-06,  3.2857e-06,  ..., -7.5698e-06,
         -2.7604e-06, -5.7071e-06],
        [-1.1444e-05, -7.6890e-06,  4.2096e-06,  ..., -9.7007e-06,
         -3.5167e-06, -7.2718e-06],
        [-9.9540e-06, -6.6757e-06,  3.6657e-06,  ..., -8.4490e-06,
         -3.0398e-06, -6.3330e-06],
        [-1.9133e-05, -1.2845e-05,  7.0333e-06,  ..., -1.6212e-05,
         -5.8711e-06, -1.2159e-05]], device='cuda:0')
Loss: 1.1238387823104858


Running epoch 0, step 459, batch 459
Sampled inputs[:2]: tensor([[   0,   14,   20,  ...,  607, 8386,   88],
        [   0, 2771,   13,  ..., 4169,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2912e-05,  1.0378e-04, -5.1379e-05,  ...,  3.0916e-05,
         -3.7833e-05, -5.4085e-05],
        [-1.1966e-05, -8.0168e-06,  4.3735e-06,  ..., -1.0118e-05,
         -3.6769e-06, -7.6592e-06],
        [-1.5289e-05, -1.0267e-05,  5.6028e-06,  ..., -1.2949e-05,
         -4.6790e-06, -9.7603e-06],
        [-1.3232e-05, -8.8662e-06,  4.8578e-06,  ..., -1.1221e-05,
         -4.0308e-06, -8.4490e-06],
        [-2.5511e-05, -1.7107e-05,  9.3430e-06,  ..., -2.1607e-05,
         -7.8082e-06, -1.6272e-05]], device='cuda:0')
Loss: 1.1502387523651123


Running epoch 0, step 460, batch 460
Sampled inputs[:2]: tensor([[    0,  4347,   638,  ...,  1345,   292, 15343],
        [    0,  9088,  7217,  ...,   199, 17822,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4492e-05,  1.1209e-04, -8.2347e-05,  ...,  1.6131e-05,
         -7.2051e-05, -4.8650e-05],
        [-1.4931e-05, -1.0028e-05,  5.4613e-06,  ..., -1.2651e-05,
         -4.5821e-06, -9.5367e-06],
        [-1.9059e-05, -1.2815e-05,  6.9812e-06,  ..., -1.6168e-05,
         -5.8189e-06, -1.2144e-05],
        [-1.6496e-05, -1.1086e-05,  6.0499e-06,  ..., -1.4022e-05,
         -5.0217e-06, -1.0505e-05],
        [-3.1918e-05, -2.1428e-05,  1.1683e-05,  ..., -2.7090e-05,
         -9.7454e-06, -2.0325e-05]], device='cuda:0')
Loss: 1.1524850130081177


Running epoch 0, step 461, batch 461
Sampled inputs[:2]: tensor([[    0,   741,   300,  ...,    83,  7111,   292],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2367e-05,  9.2976e-05, -1.0092e-04,  ...,  1.5422e-05,
         -7.0361e-05, -5.9778e-05],
        [-1.7911e-05, -1.2010e-05,  6.5416e-06,  ..., -1.5154e-05,
         -5.4576e-06, -1.1414e-05],
        [-2.2903e-05, -1.5393e-05,  8.3745e-06,  ..., -1.9401e-05,
         -6.9439e-06, -1.4573e-05],
        [-1.9819e-05, -1.3307e-05,  7.2569e-06,  ..., -1.6809e-05,
         -5.9903e-06, -1.2591e-05],
        [-3.8326e-05, -2.5719e-05,  1.4007e-05,  ..., -3.2485e-05,
         -1.1630e-05, -2.4378e-05]], device='cuda:0')
Loss: 1.114157795906067


Running epoch 0, step 462, batch 462
Sampled inputs[:2]: tensor([[    0,   462,  9202,  ...,    15,  3256,   271],
        [    0,  2485,    12,  ...,   293,   259, 14600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2261e-04,  6.0041e-05, -1.4414e-04,  ...,  7.1754e-06,
         -7.9288e-05, -4.8016e-05],
        [-2.0906e-05, -1.4007e-05,  7.6294e-06,  ..., -1.7658e-05,
         -6.3516e-06, -1.3314e-05],
        [-2.6777e-05, -1.7971e-05,  9.7752e-06,  ..., -2.2620e-05,
         -8.0913e-06, -1.7017e-05],
        [-2.3171e-05, -1.5542e-05,  8.4788e-06,  ..., -1.9610e-05,
         -6.9812e-06, -1.4707e-05],
        [-4.4763e-05, -3.0011e-05,  1.6347e-05,  ..., -3.7849e-05,
         -1.3538e-05, -2.8461e-05]], device='cuda:0')
Loss: 1.1211166381835938


Running epoch 0, step 463, batch 463
Sampled inputs[:2]: tensor([[    0,   508, 12163,  ...,  4920,   344, 11003],
        [    0,   278, 10875,  ...,   445,   267,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4613e-04,  2.5380e-05, -1.8329e-04,  ...,  2.4161e-05,
         -8.6981e-05, -5.6140e-05],
        [-2.3872e-05, -1.6004e-05,  8.7172e-06,  ..., -2.0176e-05,
         -7.2718e-06, -1.5207e-05],
        [-3.0592e-05, -2.0534e-05,  1.1183e-05,  ..., -2.5854e-05,
         -9.2685e-06, -1.9446e-05],
        [-2.6450e-05, -1.7747e-05,  9.6932e-06,  ..., -2.2396e-05,
         -7.9945e-06, -1.6809e-05],
        [-5.1111e-05, -3.4273e-05,  1.8686e-05,  ..., -4.3243e-05,
         -1.5505e-05, -3.2514e-05]], device='cuda:0')
Loss: 1.1232960224151611
Graident accumulation at epoch 0, step 463, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0034,  ..., -0.0028,  0.0226, -0.0200],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.6690e-05,  1.5355e-05, -9.6448e-05,  ...,  4.8232e-06,
         -2.3905e-05,  1.8047e-05],
        [-2.4417e-05, -1.7240e-05,  7.8634e-06,  ..., -2.1315e-05,
         -5.3502e-06, -1.2937e-05],
        [ 8.1634e-05,  5.5167e-05, -2.2149e-05,  ...,  7.6064e-05,
          1.4423e-05,  4.2133e-05],
        [-1.3879e-05, -1.0692e-05,  3.9047e-06,  ..., -1.1975e-05,
         -3.1711e-06, -8.0822e-06],
        [-3.9835e-05, -2.5795e-05,  1.2886e-05,  ..., -3.2609e-05,
         -8.3803e-06, -1.7090e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1465e-08, 2.7737e-08, 3.4913e-08,  ..., 1.1483e-08, 8.6553e-08,
         1.1354e-08],
        [5.1133e-11, 2.7900e-11, 3.4884e-12,  ..., 3.3874e-11, 2.0296e-12,
         8.4598e-12],
        [1.0804e-09, 5.5504e-10, 7.9671e-11,  ..., 9.1720e-10, 4.5696e-11,
         2.7261e-10],
        [7.0948e-11, 3.5984e-11, 5.5655e-12,  ..., 5.8123e-11, 2.9916e-12,
         1.3826e-11],
        [2.3447e-10, 1.2881e-10, 1.2830e-11,  ..., 1.7576e-10, 9.3603e-12,
         5.0357e-11]], device='cuda:0')
optimizer state dict: 58.0
lr: [1.8139866849701876e-05, 1.8139866849701876e-05]
scheduler_last_epoch: 58


Running epoch 0, step 464, batch 464
Sampled inputs[:2]: tensor([[    0,   266,   997,  ...,  2670,     5,   278],
        [    0, 13642, 14635,  ...,   367,  1040,  8580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3473e-05, -3.5825e-05, -7.6816e-05,  ...,  2.0160e-05,
         -5.9225e-06, -5.1745e-05],
        [-2.9206e-06, -1.9968e-06,  1.0356e-06,  ..., -2.4438e-06,
         -9.0152e-07, -1.8403e-06],
        [-3.9041e-06, -2.6673e-06,  1.3858e-06,  ..., -3.2783e-06,
         -1.1995e-06, -2.4587e-06],
        [-3.3230e-06, -2.2650e-06,  1.1772e-06,  ..., -2.7865e-06,
         -1.0133e-06, -2.0862e-06],
        [-6.3479e-06, -4.3213e-06,  2.2352e-06,  ..., -5.3048e-06,
         -1.9521e-06, -3.9935e-06]], device='cuda:0')
Loss: 1.1217637062072754


Running epoch 0, step 465, batch 465
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ...,   943,   259,  2646],
        [    0, 15411,  4286,  ...,  3337,   300,  2257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1206e-04, -6.6507e-05, -7.5251e-05,  ...,  4.0186e-05,
         -2.0840e-05, -6.8032e-05],
        [-5.8860e-06, -4.0084e-06,  2.0787e-06,  ..., -4.8727e-06,
         -1.8477e-06, -3.6880e-06],
        [-7.8678e-06, -5.3644e-06,  2.7940e-06,  ..., -6.5416e-06,
         -2.4661e-06, -4.9323e-06],
        [-6.6906e-06, -4.5449e-06,  2.3693e-06,  ..., -5.5432e-06,
         -2.0787e-06, -4.1872e-06],
        [-1.2785e-05, -8.6725e-06,  4.5002e-06,  ..., -1.0580e-05,
         -3.9935e-06, -8.0168e-06]], device='cuda:0')
Loss: 1.12240731716156


Running epoch 0, step 466, batch 466
Sampled inputs[:2]: tensor([[    0,  2099,  1718,  ..., 11271,   287,   300],
        [    0,   266, 10262,  ...,   271,  3437,  4392]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1442e-04, -3.3461e-05, -8.4009e-05,  ...,  4.5701e-05,
         -1.1245e-05, -6.2119e-05],
        [-8.8364e-06, -6.0052e-06,  3.1069e-06,  ..., -7.3165e-06,
         -2.8685e-06, -5.5656e-06],
        [-1.1802e-05, -8.0317e-06,  4.1649e-06,  ..., -9.7901e-06,
         -3.8296e-06, -7.4357e-06],
        [-9.9838e-06, -6.7800e-06,  3.5167e-06,  ..., -8.2701e-06,
         -3.2112e-06, -6.2883e-06],
        [-1.9222e-05, -1.3024e-05,  6.7353e-06,  ..., -1.5914e-05,
         -6.2287e-06, -1.2130e-05]], device='cuda:0')
Loss: 1.1170049905776978


Running epoch 0, step 467, batch 467
Sampled inputs[:2]: tensor([[    0,  7264, 14450,  ...,   367,   654,   300],
        [    0,  1167,  2667,  ...,  4769,    13,  5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0490e-04, -6.1030e-05, -9.5452e-05,  ...,  4.7584e-05,
         -1.5279e-05, -4.7945e-05],
        [-1.1742e-05, -8.0019e-06,  4.1202e-06,  ..., -9.7603e-06,
         -3.8147e-06, -7.4059e-06],
        [-1.5676e-05, -1.0699e-05,  5.5209e-06,  ..., -1.3053e-05,
         -5.0887e-06, -9.8944e-06],
        [-1.3307e-05, -9.0599e-06,  4.6715e-06,  ..., -1.1057e-05,
         -4.2766e-06, -8.3894e-06],
        [-2.5570e-05, -1.7375e-05,  8.9407e-06,  ..., -2.1249e-05,
         -8.2850e-06, -1.6153e-05]], device='cuda:0')
Loss: 1.1224530935287476


Running epoch 0, step 468, batch 468
Sampled inputs[:2]: tensor([[   0, 5319,   14,  ..., 2372, 2356, 4093],
        [   0,  591, 1545,  ...,   71,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1884e-04, -9.6228e-05, -1.3544e-04,  ...,  8.7207e-05,
         -1.8285e-05, -4.4822e-05],
        [-1.4648e-05, -9.9689e-06,  5.1260e-06,  ..., -1.2204e-05,
         -4.7274e-06, -9.1940e-06],
        [-1.9550e-05, -1.3337e-05,  6.8694e-06,  ..., -1.6332e-05,
         -6.2957e-06, -1.2293e-05],
        [-1.6645e-05, -1.1325e-05,  5.8338e-06,  ..., -1.3873e-05,
         -5.3123e-06, -1.0446e-05],
        [-3.1859e-05, -2.1636e-05,  1.1116e-05,  ..., -2.6554e-05,
         -1.0237e-05, -2.0027e-05]], device='cuda:0')
Loss: 1.1063731908798218


Running epoch 0, step 469, batch 469
Sampled inputs[:2]: tensor([[   0, 4902,  518,  ..., 5493, 3227,  278],
        [   0, 3406,  300,  ..., 1726, 3521, 4481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9320e-04, -8.4497e-05, -1.4093e-04,  ...,  1.2719e-04,
         -2.7444e-05, -2.8820e-05],
        [-1.7554e-05, -1.1921e-05,  6.1542e-06,  ..., -1.4663e-05,
         -5.6550e-06, -1.1027e-05],
        [-2.3454e-05, -1.5959e-05,  8.2552e-06,  ..., -1.9640e-05,
         -7.5251e-06, -1.4752e-05],
        [-1.9997e-05, -1.3590e-05,  7.0259e-06,  ..., -1.6719e-05,
         -6.3702e-06, -1.2562e-05],
        [-3.8236e-05, -2.5928e-05,  1.3381e-05,  ..., -3.1948e-05,
         -1.2249e-05, -2.4021e-05]], device='cuda:0')
Loss: 1.1141353845596313


Running epoch 0, step 470, batch 470
Sampled inputs[:2]: tensor([[   0,  792,  287,  ...,  706, 9751,  278],
        [   0,  409,  729,  ...,  391,  266,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0460e-04, -9.1304e-05, -1.5442e-04,  ...,  1.6896e-04,
         -3.0390e-05, -1.0224e-05],
        [-2.0459e-05, -1.3933e-05,  7.1973e-06,  ..., -1.7107e-05,
         -6.5677e-06, -1.2822e-05],
        [-2.7329e-05, -1.8656e-05,  9.6411e-06,  ..., -2.2903e-05,
         -8.7395e-06, -1.7151e-05],
        [-2.3320e-05, -1.5900e-05,  8.2180e-06,  ..., -1.9506e-05,
         -7.3984e-06, -1.4603e-05],
        [-4.4554e-05, -3.0309e-05,  1.5646e-05,  ..., -3.7253e-05,
         -1.4231e-05, -2.7925e-05]], device='cuda:0')
Loss: 1.1122034788131714


Running epoch 0, step 471, batch 471
Sampled inputs[:2]: tensor([[    0,   494,   825,  ...,   897,   328,   275],
        [    0,  3594,   950,  ...,  6517,   344, 15386]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3235e-04, -8.5725e-05, -1.7148e-04,  ...,  1.6627e-04,
         -4.1497e-05, -3.4157e-06],
        [-2.3380e-05, -1.5885e-05,  8.2329e-06,  ..., -1.9535e-05,
         -7.5214e-06, -1.4700e-05],
        [-3.1233e-05, -2.1264e-05,  1.1027e-05,  ..., -2.6152e-05,
         -1.0006e-05, -1.9655e-05],
        [-2.6658e-05, -1.8120e-05,  9.4026e-06,  ..., -2.2277e-05,
         -8.4713e-06, -1.6734e-05],
        [-5.0992e-05, -3.4600e-05,  1.7941e-05,  ..., -4.2617e-05,
         -1.6317e-05, -3.2067e-05]], device='cuda:0')
Loss: 1.1421430110931396
Graident accumulation at epoch 0, step 471, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0053, -0.0148,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0023, -0.0341],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0156, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.8582e-07,  5.2468e-06, -1.0395e-04,  ...,  2.0968e-05,
         -2.5664e-05,  1.5901e-05],
        [-2.4313e-05, -1.7105e-05,  7.9003e-06,  ..., -2.1137e-05,
         -5.5673e-06, -1.3114e-05],
        [ 7.0347e-05,  4.7524e-05, -1.8831e-05,  ...,  6.5842e-05,
          1.1980e-05,  3.5954e-05],
        [-1.5157e-05, -1.1434e-05,  4.4545e-06,  ..., -1.3005e-05,
         -3.7011e-06, -8.9474e-06],
        [-4.0951e-05, -2.6675e-05,  1.3392e-05,  ..., -3.3610e-05,
         -9.1739e-06, -1.8587e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1478e-08, 2.7717e-08, 3.4907e-08,  ..., 1.1499e-08, 8.6468e-08,
         1.1342e-08],
        [5.1629e-11, 2.8124e-11, 3.5527e-12,  ..., 3.4222e-11, 2.0842e-12,
         8.6674e-12],
        [1.0803e-09, 5.5494e-10, 7.9713e-11,  ..., 9.1697e-10, 4.5751e-11,
         2.7272e-10],
        [7.1587e-11, 3.6276e-11, 5.6484e-12,  ..., 5.8561e-11, 3.0603e-12,
         1.4092e-11],
        [2.3684e-10, 1.2988e-10, 1.3139e-11,  ..., 1.7740e-10, 9.6172e-12,
         5.1335e-11]], device='cuda:0')
optimizer state dict: 59.0
lr: [1.806743990132056e-05, 1.806743990132056e-05]
scheduler_last_epoch: 59


Running epoch 0, step 472, batch 472
Sampled inputs[:2]: tensor([[   0,  266, 3634,  ...,  694,  266, 1784],
        [   0, 6847,  437,  ...,   17,   14,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2542e-06, -2.9840e-05, -2.1164e-05,  ..., -2.4226e-05,
          1.4236e-05, -2.8514e-05],
        [-2.8610e-06, -1.9670e-06,  9.7603e-07,  ..., -2.4140e-06,
         -9.9093e-07, -1.7658e-06],
        [-3.9637e-06, -2.7269e-06,  1.3486e-06,  ..., -3.3528e-06,
         -1.3709e-06, -2.4438e-06],
        [-3.2932e-06, -2.2501e-06,  1.1176e-06,  ..., -2.7865e-06,
         -1.1325e-06, -2.0266e-06],
        [-6.3479e-06, -4.3511e-06,  2.1607e-06,  ..., -5.3942e-06,
         -2.2054e-06, -3.9339e-06]], device='cuda:0')
Loss: 1.143154501914978


Running epoch 0, step 473, batch 473
Sampled inputs[:2]: tensor([[   0,  278, 8608,  ...,  293, 1608,  391],
        [   0,   12, 4856,  ...,  342,  266, 1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1135e-05, -4.4001e-05, -4.3217e-05,  ...,  4.1367e-06,
          4.7587e-06, -5.1616e-05],
        [-5.6475e-06, -3.8669e-06,  1.9521e-06,  ..., -4.7684e-06,
         -1.8962e-06, -3.4645e-06],
        [ 8.4498e-05,  5.5373e-05, -2.5091e-05,  ...,  6.8861e-05,
          3.4392e-05,  4.9178e-05],
        [-6.6310e-06, -4.5300e-06,  2.2873e-06,  ..., -5.6028e-06,
         -2.1979e-06, -4.0531e-06],
        [-1.2606e-05, -8.6129e-06,  4.3511e-06,  ..., -1.0669e-05,
         -4.2170e-06, -7.7188e-06]], device='cuda:0')
Loss: 1.1128867864608765


Running epoch 0, step 474, batch 474
Sampled inputs[:2]: tensor([[    0,   300, 16683,  ...,  8709,    40,  9817],
        [    0,  2728,  3139,  ...,  2254,   221,   380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3683e-04, -8.3105e-06, -5.8542e-05,  ...,  1.8451e-05,
         -9.2065e-06, -6.2721e-05],
        [-8.5235e-06, -5.8189e-06,  2.9430e-06,  ..., -7.1377e-06,
         -2.8498e-06, -5.2303e-06],
        [ 8.0504e-05,  5.2661e-05, -2.3713e-05,  ...,  6.5568e-05,
          3.3081e-05,  4.6734e-05],
        [-9.9987e-06, -6.8247e-06,  3.4496e-06,  ..., -8.3745e-06,
         -3.3006e-06, -6.1095e-06],
        [-1.8954e-05, -1.2934e-05,  6.5416e-06,  ..., -1.5885e-05,
         -6.3032e-06, -1.1593e-05]], device='cuda:0')
Loss: 1.0965334177017212


Running epoch 0, step 475, batch 475
Sampled inputs[:2]: tensor([[    0,  1042,  2548,  ...,   328,   259,  2771],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4412e-04,  1.4010e-05, -7.1508e-05,  ..., -4.1359e-06,
         -3.3394e-05, -4.1408e-05],
        [-1.1370e-05, -7.7710e-06,  3.9488e-06,  ..., -9.5069e-06,
         -3.8482e-06, -6.9886e-06],
        [ 7.6541e-05,  4.9949e-05, -2.2312e-05,  ...,  6.2260e-05,
          3.1702e-05,  4.4290e-05],
        [-1.3307e-05, -9.0897e-06,  4.6194e-06,  ..., -1.1131e-05,
         -4.4480e-06, -8.1509e-06],
        [-2.5272e-05, -1.7256e-05,  8.7768e-06,  ..., -2.1160e-05,
         -8.5086e-06, -1.5497e-05]], device='cuda:0')
Loss: 1.1166235208511353


Running epoch 0, step 476, batch 476
Sampled inputs[:2]: tensor([[    0,  6574,  1707,  ...,    14,  5077,    12],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9768e-04, -6.1255e-06, -1.0204e-04,  ...,  4.9807e-05,
         -7.8587e-05, -3.5972e-05],
        [-1.4171e-05, -9.6932e-06,  4.9621e-06,  ..., -1.1846e-05,
         -4.7684e-06, -8.7246e-06],
        [ 7.2636e-05,  4.7267e-05, -2.0896e-05,  ...,  5.8996e-05,
          3.0421e-05,  4.1861e-05],
        [-1.6645e-05, -1.1384e-05,  5.8264e-06,  ..., -1.3918e-05,
         -5.5358e-06, -1.0222e-05],
        [-3.1561e-05, -2.1547e-05,  1.1042e-05,  ..., -2.6405e-05,
         -1.0565e-05, -1.9372e-05]], device='cuda:0')
Loss: 1.1186468601226807


Running epoch 0, step 477, batch 477
Sampled inputs[:2]: tensor([[   0, 4448,   12,  ..., 3183,  328, 9559],
        [   0, 7849,  278,  ...,  346,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2156e-04,  4.4004e-05, -1.6637e-04,  ...,  3.5350e-05,
         -7.6372e-05, -6.8639e-05],
        [-1.7002e-05, -1.1645e-05,  6.0126e-06,  ..., -1.4260e-05,
         -5.8487e-06, -1.0595e-05],
        [ 6.8822e-05,  4.4630e-05, -1.9473e-05,  ...,  5.5733e-05,
          2.8975e-05,  3.9343e-05],
        [-1.9848e-05, -1.3590e-05,  7.0184e-06,  ..., -1.6659e-05,
         -6.7428e-06, -1.2338e-05],
        [-3.7640e-05, -2.5749e-05,  1.3292e-05,  ..., -3.1590e-05,
         -1.2860e-05, -2.3395e-05]], device='cuda:0')
Loss: 1.1252723932266235


Running epoch 0, step 478, batch 478
Sampled inputs[:2]: tensor([[    0,  1854,   292,  ...,   328,  1360,    14],
        [    0,   259,  2122,  ...,   554,   392, 10814]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3270e-04,  9.3392e-05, -1.8013e-04,  ...,  7.0291e-06,
         -8.9138e-05, -1.2063e-04],
        [-1.9774e-05, -1.3582e-05,  7.0110e-06,  ..., -1.6615e-05,
         -6.7428e-06, -1.2338e-05],
        [ 6.5007e-05,  4.1948e-05, -1.8087e-05,  ...,  5.2470e-05,
          2.7753e-05,  3.6929e-05],
        [-2.3112e-05, -1.5870e-05,  8.1956e-06,  ..., -1.9431e-05,
         -7.7784e-06, -1.4380e-05],
        [-4.3720e-05, -2.9981e-05,  1.5482e-05,  ..., -3.6746e-05,
         -1.4812e-05, -2.7210e-05]], device='cuda:0')
Loss: 1.1176977157592773


Running epoch 0, step 479, batch 479
Sampled inputs[:2]: tensor([[   0, 1062,  648,  ...,  266, 4939,  278],
        [   0,   12,  221,  ...,  593,  360,  726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4788e-04,  9.1851e-05, -2.3469e-04,  ...,  2.6354e-05,
         -1.2634e-04, -1.4669e-04],
        [-2.2545e-05, -1.5490e-05,  8.0168e-06,  ..., -1.8984e-05,
         -7.6778e-06, -1.4089e-05],
        [ 6.1133e-05,  3.9265e-05, -1.6679e-05,  ...,  4.9162e-05,
          2.6450e-05,  3.4470e-05],
        [-2.6420e-05, -1.8150e-05,  9.3952e-06,  ..., -2.2247e-05,
         -8.8811e-06, -1.6466e-05],
        [-4.9859e-05, -3.4213e-05,  1.7703e-05,  ..., -4.1991e-05,
         -1.6868e-05, -3.1084e-05]], device='cuda:0')
Loss: 1.1149351596832275
Graident accumulation at epoch 0, step 479, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0034,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0164,  0.0148, -0.0274,  ...,  0.0283, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4080e-05,  1.3907e-05, -1.1702e-04,  ...,  2.1506e-05,
         -3.5732e-05, -3.5804e-07],
        [-2.4136e-05, -1.6943e-05,  7.9120e-06,  ..., -2.0922e-05,
         -5.7784e-06, -1.3211e-05],
        [ 6.9426e-05,  4.6698e-05, -1.8616e-05,  ...,  6.4174e-05,
          1.3427e-05,  3.5806e-05],
        [-1.6283e-05, -1.2106e-05,  4.9486e-06,  ..., -1.3930e-05,
         -4.2191e-06, -9.6992e-06],
        [-4.1842e-05, -2.7429e-05,  1.3823e-05,  ..., -3.4448e-05,
         -9.9433e-06, -1.9837e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1458e-08, 2.7698e-08, 3.4927e-08,  ..., 1.1488e-08, 8.6398e-08,
         1.1353e-08],
        [5.2085e-11, 2.8336e-11, 3.6134e-12,  ..., 3.4548e-11, 2.1410e-12,
         8.8573e-12],
        [1.0830e-09, 5.5592e-10, 7.9911e-11,  ..., 9.1847e-10, 4.6404e-11,
         2.7364e-10],
        [7.2214e-11, 3.6569e-11, 5.7310e-12,  ..., 5.8998e-11, 3.1362e-12,
         1.4349e-11],
        [2.3909e-10, 1.3092e-10, 1.3439e-11,  ..., 1.7899e-10, 9.8921e-12,
         5.2250e-11]], device='cuda:0')
optimizer state dict: 60.0
lr: [1.799378017770064e-05, 1.799378017770064e-05]
scheduler_last_epoch: 60
Epoch 0 | Batch 479/1048 | Training PPL: 7476.210145017841 | time 12.74379587173462
Saving checkpoint at epoch 0, step 479, batch 479
Epoch 0 | Validation PPL: 9.298642409383568 | Learning rate: 1.799378017770064e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_479, AFTER epoch 0, step 479


Running epoch 0, step 480, batch 480
Sampled inputs[:2]: tensor([[    0, 13649,  7841,  ...,   287,  4713,    14],
        [    0,    12,  2085,  ...,   287,   593,  4137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2005e-05, -2.9811e-05,  6.7765e-06,  ...,  1.5994e-05,
         -3.8719e-05,  9.9712e-06],
        [-2.6822e-06, -1.8403e-06,  9.3877e-07,  ..., -2.2650e-06,
         -8.7544e-07, -1.5795e-06],
        [-3.9339e-06, -2.6971e-06,  1.3709e-06,  ..., -3.3081e-06,
         -1.2666e-06, -2.3097e-06],
        [-3.3379e-06, -2.2799e-06,  1.1697e-06,  ..., -2.8163e-06,
         -1.0654e-06, -1.9521e-06],
        [-6.1989e-06, -4.2617e-06,  2.1756e-06,  ..., -5.2452e-06,
         -2.0117e-06, -3.6359e-06]], device='cuda:0')
Loss: 1.103645920753479


Running epoch 0, step 481, batch 481
Sampled inputs[:2]: tensor([[    0,   409, 35049,  ...,    12,   699,   394],
        [    0,   474,   221,  ..., 32291,   360,  2458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5182e-05,  1.4739e-05, -1.6106e-05,  ...,  5.4695e-05,
         -9.1988e-05,  1.0634e-05],
        [-5.3495e-06, -3.6806e-06,  1.8850e-06,  ..., -4.5300e-06,
         -1.7509e-06, -3.1814e-06],
        [-7.8380e-06, -5.3942e-06,  2.7567e-06,  ..., -6.6310e-06,
         -2.5406e-06, -4.6492e-06],
        [-6.6757e-06, -4.5896e-06,  2.3544e-06,  ..., -5.6624e-06,
         -2.1458e-06, -3.9488e-06],
        [-1.2368e-05, -8.5235e-06,  4.3660e-06,  ..., -1.0490e-05,
         -4.0233e-06, -7.3314e-06]], device='cuda:0')
Loss: 1.1012812852859497


Running epoch 0, step 482, batch 482
Sampled inputs[:2]: tensor([[    0,  2715,  1478,  ...,  1171,  4697, 41847],
        [    0,    14,  3228,  ..., 13747,   287, 20295]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2644e-05,  2.2601e-05, -4.0711e-05,  ...,  3.8819e-05,
         -8.4900e-05,  3.4981e-05],
        [-8.0466e-06, -5.5507e-06,  2.8610e-06,  ..., -6.7800e-06,
         -2.6822e-06, -4.8280e-06],
        [ 2.6800e-04,  1.9046e-04, -1.1402e-04,  ...,  1.9322e-04,
          1.0937e-04,  1.5397e-04],
        [-1.0028e-05, -6.9141e-06,  3.5688e-06,  ..., -8.4490e-06,
         -3.2857e-06, -5.9903e-06],
        [-1.8567e-05, -1.2815e-05,  6.6012e-06,  ..., -1.5646e-05,
         -6.1542e-06, -1.1101e-05]], device='cuda:0')
Loss: 1.1163204908370972


Running epoch 0, step 483, batch 483
Sampled inputs[:2]: tensor([[    0,   631,   516,  ..., 13374,   898,   266],
        [    0,  1869,   596,  ..., 13055, 17051,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2233e-05, -3.6879e-06, -5.2600e-05,  ...,  6.0703e-05,
         -1.1459e-04,  3.9687e-05],
        [-1.0729e-05, -7.4208e-06,  3.8296e-06,  ..., -9.0152e-06,
         -3.5688e-06, -6.4149e-06],
        [ 3.5412e-04,  2.4148e-04, -1.5098e-04,  ...,  2.6618e-04,
          1.4308e-04,  1.9919e-04],
        [-1.3396e-05, -9.2536e-06,  4.7833e-06,  ..., -1.1265e-05,
         -4.3809e-06, -7.9721e-06],
        [-2.4766e-05, -1.7136e-05,  8.8364e-06,  ..., -2.0802e-05,
         -8.1807e-06, -1.4752e-05]], device='cuda:0')
Loss: 1.1124171018600464


Running epoch 0, step 484, batch 484
Sampled inputs[:2]: tensor([[   0, 5896,  352,  ..., 1168,  767, 1390],
        [   0,  271,  266,  ...,  984,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3329e-05,  2.8839e-06, -8.5221e-05,  ...,  6.1861e-05,
         -1.2786e-04, -8.8516e-06],
        [-1.3396e-05, -9.2611e-06,  4.8056e-06,  ..., -1.1265e-05,
         -4.4815e-06, -7.9945e-06],
        [ 3.5022e-04,  2.3878e-04, -1.4954e-04,  ...,  2.6287e-04,
          1.4175e-04,  1.9686e-04],
        [-1.6719e-05, -1.1548e-05,  6.0052e-06,  ..., -1.4082e-05,
         -5.5060e-06, -9.9391e-06],
        [-3.0905e-05, -2.1398e-05,  1.1101e-05,  ..., -2.6017e-05,
         -1.0282e-05, -1.8403e-05]], device='cuda:0')
Loss: 1.1322083473205566


Running epoch 0, step 485, batch 485
Sampled inputs[:2]: tensor([[   0, 3103, 2134,  ..., 6627,  275, 1911],
        [   0, 7110,  437,  ...,  266, 6724, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2373e-05,  9.3290e-06, -9.0794e-05,  ...,  5.5554e-05,
         -1.5345e-04,  2.1340e-06],
        [-1.6108e-05, -1.1116e-05,  5.7966e-06,  ..., -1.3575e-05,
         -5.4352e-06, -9.6485e-06],
        [ 3.4635e-04,  2.3613e-04, -1.4812e-04,  ...,  2.5956e-04,
          1.4039e-04,  1.9451e-04],
        [-1.9997e-05, -1.3784e-05,  7.1973e-06,  ..., -1.6868e-05,
         -6.6459e-06, -1.1921e-05],
        [-3.7074e-05, -2.5600e-05,  1.3351e-05,  ..., -3.1263e-05,
         -1.2442e-05, -2.2143e-05]], device='cuda:0')
Loss: 1.1274770498275757


Running epoch 0, step 486, batch 486
Sampled inputs[:2]: tensor([[    0,   806,   300,  ...,   360,  4918,  1106],
        [    0,   437,   266,  ...,   266, 16084,  1781]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0254e-04,  4.0744e-05, -1.2066e-04,  ...,  5.4773e-05,
         -1.7532e-04, -3.4060e-05],
        [-1.8805e-05, -1.2994e-05,  6.7726e-06,  ..., -1.5870e-05,
         -6.3740e-06, -1.1243e-05],
        [ 3.4247e-04,  2.3342e-04, -1.4672e-04,  ...,  2.5626e-04,
          1.3905e-04,  1.9221e-04],
        [-2.3320e-05, -1.6108e-05,  8.4043e-06,  ..., -1.9699e-05,
         -7.7859e-06, -1.3888e-05],
        [-4.3243e-05, -2.9892e-05,  1.5587e-05,  ..., -3.6508e-05,
         -1.4573e-05, -2.5779e-05]], device='cuda:0')
Loss: 1.13318932056427


Running epoch 0, step 487, batch 487
Sampled inputs[:2]: tensor([[    0,    67,   695,  ...,   437,   266, 44563],
        [    0,   409,   394,  ...,   475,  5458,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6395e-05,  6.3331e-05, -1.0657e-04,  ...,  7.2956e-05,
         -2.3235e-04, -2.0392e-05],
        [-2.1487e-05, -1.4842e-05,  7.7225e-06,  ..., -1.8150e-05,
         -7.2680e-06, -1.2845e-05],
        [ 3.3857e-04,  2.3072e-04, -1.4533e-04,  ...,  2.5293e-04,
          1.3776e-04,  1.8988e-04],
        [-2.6628e-05, -1.8388e-05,  9.5740e-06,  ..., -2.2501e-05,
         -8.8736e-06, -1.5855e-05],
        [-4.9442e-05, -3.4153e-05,  1.7777e-05,  ..., -4.1753e-05,
         -1.6630e-05, -2.9460e-05]], device='cuda:0')
Loss: 1.1230723857879639
Graident accumulation at epoch 0, step 487, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0028,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0274,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.2312e-05,  1.8850e-05, -1.1598e-04,  ...,  2.6651e-05,
         -5.5394e-05, -2.3614e-06],
        [-2.3871e-05, -1.6733e-05,  7.8930e-06,  ..., -2.0644e-05,
         -5.9273e-06, -1.3174e-05],
        [ 9.6340e-05,  6.5101e-05, -3.1287e-05,  ...,  8.3050e-05,
          2.5860e-05,  5.1213e-05],
        [-1.7318e-05, -1.2734e-05,  5.4111e-06,  ..., -1.4787e-05,
         -4.6846e-06, -1.0315e-05],
        [-4.2602e-05, -2.8102e-05,  1.4218e-05,  ..., -3.5178e-05,
         -1.0612e-05, -2.0799e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1426e-08, 2.7674e-08, 3.4904e-08,  ..., 1.1482e-08, 8.6365e-08,
         1.1342e-08],
        [5.2495e-11, 2.8528e-11, 3.6694e-12,  ..., 3.4843e-11, 2.1917e-12,
         9.0134e-12],
        [1.1965e-09, 6.0860e-10, 1.0095e-10,  ..., 9.8153e-10, 6.5335e-11,
         3.0942e-10],
        [7.2851e-11, 3.6871e-11, 5.8169e-12,  ..., 5.9445e-11, 3.2118e-12,
         1.4586e-11],
        [2.4129e-10, 1.3196e-10, 1.3742e-11,  ..., 1.8055e-10, 1.0159e-11,
         5.3066e-11]], device='cuda:0')
optimizer state dict: 61.0
lr: [1.791889893469088e-05, 1.791889893469088e-05]
scheduler_last_epoch: 61


Running epoch 0, step 488, batch 488
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,   300,   344,  ...,    14,  5077,  2715]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5497e-05,  3.9062e-05, -2.9997e-05,  ...,  5.1685e-05,
         -3.5162e-05, -9.2818e-06],
        [-2.5481e-06, -1.7658e-06,  9.1642e-07,  ..., -2.1607e-06,
         -8.7172e-07, -1.4976e-06],
        [-3.9041e-06, -2.7120e-06,  1.4007e-06,  ..., -3.3081e-06,
         -1.3188e-06, -2.2799e-06],
        [-3.2783e-06, -2.2650e-06,  1.1772e-06,  ..., -2.7865e-06,
         -1.0952e-06, -1.9073e-06],
        [-6.0499e-06, -4.1723e-06,  2.1756e-06,  ..., -5.1260e-06,
         -2.0415e-06, -3.5316e-06]], device='cuda:0')
Loss: 1.1002625226974487


Running epoch 0, step 489, batch 489
Sampled inputs[:2]: tensor([[   0,  598,  278,  ...,  437,  266, 2388],
        [   0, 1171, 2926,  ...,  259, 4288,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3178e-05,  5.7130e-05, -1.0220e-04,  ...,  7.3157e-05,
         -5.7452e-05, -8.4904e-07],
        [-5.0813e-06, -3.5018e-06,  1.8142e-06,  ..., -4.3064e-06,
         -1.7099e-06, -2.9504e-06],
        [-7.7784e-06, -5.3644e-06,  2.7716e-06,  ..., -6.5863e-06,
         -2.5854e-06, -4.5002e-06],
        [-6.5863e-06, -4.5300e-06,  2.3544e-06,  ..., -5.6028e-06,
         -2.1681e-06, -3.7998e-06],
        [-1.2100e-05, -8.3148e-06,  4.3213e-06,  ..., -1.0282e-05,
         -4.0382e-06, -7.0035e-06]], device='cuda:0')
Loss: 1.1085538864135742


Running epoch 0, step 490, batch 490
Sampled inputs[:2]: tensor([[   0,   14, 6436,  ...,  271, 1211, 8917],
        [   0,  834,   89,  ..., 4030,   12, 6528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8401e-05,  5.7130e-05, -1.5743e-04,  ...,  7.2303e-05,
         -5.8395e-05,  3.0572e-05],
        [-7.6592e-06, -5.2601e-06,  2.7120e-06,  ..., -6.4969e-06,
         -2.5742e-06, -4.4256e-06],
        [-1.1742e-05, -8.0615e-06,  4.1500e-06,  ..., -9.9540e-06,
         -3.8967e-06, -6.7651e-06],
        [-9.8944e-06, -6.7800e-06,  3.5018e-06,  ..., -8.4192e-06,
         -3.2559e-06, -5.6773e-06],
        [-1.8299e-05, -1.2517e-05,  6.4671e-06,  ..., -1.5527e-05,
         -6.0946e-06, -1.0535e-05]], device='cuda:0')
Loss: 1.1284377574920654


Running epoch 0, step 491, batch 491
Sampled inputs[:2]: tensor([[    0,   292,    33,  ..., 32754,   300, 14476],
        [    0,   271, 10474,  ...,   298,  2286,    29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6060e-05,  8.8224e-05, -1.8817e-04,  ...,  7.4746e-05,
         -9.9889e-05, -1.4685e-06],
        [-1.0222e-05, -7.0110e-06,  3.6098e-06,  ..., -8.6725e-06,
         -3.3863e-06, -5.8487e-06],
        [-1.5616e-05, -1.0729e-05,  5.5134e-06,  ..., -1.3262e-05,
         -5.1185e-06, -8.9258e-06],
        [-1.3217e-05, -9.0599e-06,  4.6715e-06,  ..., -1.1235e-05,
         -4.2915e-06, -7.5176e-06],
        [-2.4378e-05, -1.6689e-05,  8.6129e-06,  ..., -2.0713e-05,
         -8.0168e-06, -1.3918e-05]], device='cuda:0')
Loss: 1.1243412494659424


Running epoch 0, step 492, batch 492
Sampled inputs[:2]: tensor([[    0,  1979,   352,  ...,   292,  1591,   446],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7620e-05,  1.1754e-04, -1.9197e-04,  ...,  4.3313e-05,
         -1.2088e-04, -3.0497e-05],
        [-1.2785e-05, -8.7544e-06,  4.5560e-06,  ..., -1.0893e-05,
         -4.3474e-06, -7.3835e-06],
        [-1.9491e-05, -1.3366e-05,  6.9365e-06,  ..., -1.6600e-05,
         -6.5565e-06, -1.1235e-05],
        [-1.6466e-05, -1.1265e-05,  5.8711e-06,  ..., -1.4037e-05,
         -5.4911e-06, -9.4548e-06],
        [-3.0428e-05, -2.0802e-05,  1.0848e-05,  ..., -2.5928e-05,
         -1.0267e-05, -1.7524e-05]], device='cuda:0')
Loss: 1.1201320886611938


Running epoch 0, step 493, batch 493
Sampled inputs[:2]: tensor([[    0,  2372,  1319,  ...,  1253,   292, 34166],
        [    0,  2706,   292,  ...,    13,  8954,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6265e-05,  1.3249e-04, -2.4339e-04,  ...,  7.7626e-05,
         -1.0807e-04,  5.4607e-06],
        [-1.5378e-05, -1.0513e-05,  5.4687e-06,  ..., -1.3098e-05,
         -5.2825e-06, -8.8811e-06],
        [-2.3454e-05, -1.6049e-05,  8.3297e-06,  ..., -1.9953e-05,
         -7.9647e-06, -1.3515e-05],
        [ 1.6599e-04,  1.0211e-04, -5.7161e-05,  ...,  1.5883e-04,
          5.2704e-05,  9.5895e-05],
        [-3.6627e-05, -2.5004e-05,  1.3024e-05,  ..., -3.1173e-05,
         -1.2487e-05, -2.1085e-05]], device='cuda:0')
Loss: 1.1187351942062378


Running epoch 0, step 494, batch 494
Sampled inputs[:2]: tensor([[    0,  9792,  3239,  ...,   699,  3636,  1761],
        [    0,   287, 19777,  ...,   266,  5061,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8659e-05,  1.5808e-04, -2.2429e-04,  ...,  8.2913e-05,
         -1.5966e-04,  2.9037e-05],
        [-1.8001e-05, -1.2293e-05,  6.3851e-06,  ..., -1.5289e-05,
         -6.2212e-06, -1.0386e-05],
        [-2.7448e-05, -1.8761e-05,  9.7305e-06,  ..., -2.3305e-05,
         -9.3803e-06, -1.5810e-05],
        [ 1.6269e-04,  9.9874e-05, -5.6006e-05,  ...,  1.5605e-04,
          5.1534e-05,  9.4002e-05],
        [-4.2886e-05, -2.9266e-05,  1.5229e-05,  ..., -3.6448e-05,
         -1.4722e-05, -2.4691e-05]], device='cuda:0')
Loss: 1.110344648361206


Running epoch 0, step 495, batch 495
Sampled inputs[:2]: tensor([[    0,    14,   759,  ...,  2540,  1323,    12],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0499e-05,  1.5788e-04, -2.1276e-04,  ...,  8.1742e-05,
         -1.3426e-04,  6.3593e-05],
        [-2.0593e-05, -1.4067e-05,  7.3053e-06,  ..., -1.7494e-05,
         -7.1004e-06, -1.1832e-05],
        [-3.1441e-05, -2.1502e-05,  1.1146e-05,  ..., -2.6703e-05,
         -1.0721e-05, -1.8030e-05],
        [ 1.5935e-04,  9.7595e-05, -5.4822e-05,  ...,  1.5322e-04,
          5.0424e-05,  9.2147e-05],
        [-4.9144e-05, -3.3528e-05,  1.7434e-05,  ..., -4.1723e-05,
         -1.6823e-05, -2.8163e-05]], device='cuda:0')
Loss: 1.112095594406128
Graident accumulation at epoch 0, step 495, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3031e-05,  3.2753e-05, -1.2566e-04,  ...,  3.2160e-05,
         -6.3281e-05,  4.2340e-06],
        [-2.3544e-05, -1.6466e-05,  7.8343e-06,  ..., -2.0329e-05,
         -6.0446e-06, -1.3040e-05],
        [ 8.3562e-05,  5.6440e-05, -2.7044e-05,  ...,  7.2075e-05,
          2.2202e-05,  4.4289e-05],
        [ 3.4965e-07, -1.7012e-06, -6.1217e-07,  ...,  2.0143e-06,
          8.2625e-07, -6.8596e-08],
        [-4.3256e-05, -2.8644e-05,  1.4540e-05,  ..., -3.5833e-05,
         -1.1233e-05, -2.1536e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1390e-08, 2.7671e-08, 3.4914e-08,  ..., 1.1477e-08, 8.6297e-08,
         1.1334e-08],
        [5.2867e-11, 2.8697e-11, 3.7191e-12,  ..., 3.5114e-11, 2.2399e-12,
         9.1444e-12],
        [1.1963e-09, 6.0845e-10, 1.0098e-10,  ..., 9.8126e-10, 6.5384e-11,
         3.0943e-10],
        [9.8172e-11, 4.6359e-11, 8.8165e-12,  ..., 8.2863e-11, 5.7511e-12,
         2.3063e-11],
        [2.4347e-10, 1.3295e-10, 1.4032e-11,  ..., 1.8211e-10, 1.0432e-11,
         5.3806e-11]], device='cuda:0')
optimizer state dict: 62.0
lr: [1.7842807614798848e-05, 1.7842807614798848e-05]
scheduler_last_epoch: 62


Running epoch 0, step 496, batch 496
Sampled inputs[:2]: tensor([[   0, 2426,  699,  ...,  221, 1551,  720],
        [   0,  266, 2374,  ..., 1551,  518,  638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2988e-05, -2.3725e-05, -2.9131e-05,  ...,  8.7936e-06,
          1.4591e-05, -9.8449e-06],
        [ 8.8959e-05,  5.0801e-05, -3.4809e-05,  ...,  5.6638e-05,
          3.9101e-05,  3.8518e-05],
        [-3.9339e-06, -2.6822e-06,  1.4156e-06,  ..., -3.3528e-06,
         -1.4156e-06, -2.1905e-06],
        [-3.2783e-06, -2.2352e-06,  1.1846e-06,  ..., -2.8014e-06,
         -1.1697e-06, -1.8254e-06],
        [-6.1691e-06, -4.2021e-06,  2.2203e-06,  ..., -5.2452e-06,
         -2.2203e-06, -3.4422e-06]], device='cuda:0')
Loss: 1.1102690696716309


Running epoch 0, step 497, batch 497
Sampled inputs[:2]: tensor([[    0,   298,   301,  ...,    13, 10308,  2129],
        [    0,    15, 43895,  ...,   292,   380, 16795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4716e-05,  8.4975e-05, -5.9034e-05,  ...,  2.8846e-05,
         -1.5078e-05,  5.9942e-05],
        [ 8.6456e-05,  4.9095e-05, -3.3937e-05,  ...,  5.4492e-05,
          3.8140e-05,  3.7095e-05],
        [-7.8976e-06, -5.3793e-06,  2.7940e-06,  ..., -6.7502e-06,
         -2.9132e-06, -4.4256e-06],
        [-6.5416e-06, -4.4554e-06,  2.3171e-06,  ..., -5.6028e-06,
         -2.3916e-06, -3.6657e-06],
        [-1.2398e-05, -8.4341e-06,  4.3809e-06,  ..., -1.0580e-05,
         -4.5747e-06, -6.9588e-06]], device='cuda:0')
Loss: 1.0940974950790405


Running epoch 0, step 498, batch 498
Sampled inputs[:2]: tensor([[   0,   18,   14,  ...,  380,  981,   12],
        [   0,  623,   12,  ..., 4792, 6572,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1133e-05,  1.3841e-04, -7.5913e-05,  ...,  1.1874e-05,
         -4.3166e-05,  6.9193e-05],
        [ 8.3997e-05,  4.7419e-05, -3.3077e-05,  ...,  5.2391e-05,
          3.7331e-05,  3.5746e-05],
        [-1.1802e-05, -8.0466e-06,  4.1574e-06,  ..., -1.0073e-05,
         -4.1723e-06, -6.5565e-06],
        [-9.8646e-06, -6.7353e-06,  3.4869e-06,  ..., -8.4341e-06,
         -3.4571e-06, -5.4762e-06],
        [-1.8507e-05, -1.2606e-05,  6.5267e-06,  ..., -1.5795e-05,
         -6.5565e-06, -1.0297e-05]], device='cuda:0')
Loss: 1.0983468294143677


Running epoch 0, step 499, batch 499
Sampled inputs[:2]: tensor([[    0,   685,   344,  ...,   680,   401,   616],
        [    0, 15912,    14,  ..., 25535,    18,  3947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3236e-04,  1.8635e-04, -7.5913e-05,  ...,  8.8893e-06,
         -7.6446e-05,  1.8731e-05],
        [ 8.1538e-05,  4.5727e-05, -3.2201e-05,  ...,  5.0319e-05,
          3.6582e-05,  3.4457e-05],
        [-1.5736e-05, -1.0744e-05,  5.5581e-06,  ..., -1.3381e-05,
         -5.3495e-06, -8.6129e-06],
        [-1.3202e-05, -9.0301e-06,  4.6790e-06,  ..., -1.1250e-05,
         -4.4480e-06, -7.2196e-06],
        [-2.4647e-05, -1.6838e-05,  8.7172e-06,  ..., -2.0981e-05,
         -8.4043e-06, -1.3515e-05]], device='cuda:0')
Loss: 1.0864876508712769


Running epoch 0, step 500, batch 500
Sampled inputs[:2]: tensor([[    0,   360,   259,  ...,  5710,   278,  2433],
        [    0,   949, 11135,  ...,   278,   772,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4975e-04,  2.0217e-04, -9.5252e-05,  ...,  4.2347e-05,
         -1.0936e-04,  5.4435e-05],
        [ 7.9050e-05,  4.4036e-05, -3.1296e-05,  ...,  4.8203e-05,
          3.5740e-05,  3.3101e-05],
        [-1.9670e-05, -1.3426e-05,  6.9961e-06,  ..., -1.6734e-05,
         -6.6757e-06, -1.0759e-05],
        [-1.6540e-05, -1.1295e-05,  5.8934e-06,  ..., -1.4082e-05,
         -5.5581e-06, -9.0301e-06],
        [-3.0845e-05, -2.1040e-05,  1.0967e-05,  ..., -2.6256e-05,
         -1.0490e-05, -1.6883e-05]], device='cuda:0')
Loss: 1.112297773361206


Running epoch 0, step 501, batch 501
Sampled inputs[:2]: tensor([[    0,  3256,   221,  ..., 18116,   292, 47989],
        [    0, 27754,  3807,  ...,  3370,  3809,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8222e-04,  1.6638e-04, -1.3043e-04,  ...,  4.3933e-05,
         -1.2815e-04,  2.3488e-05],
        [ 7.6591e-05,  4.2360e-05, -3.0417e-05,  ...,  4.6102e-05,
          3.4928e-05,  3.1768e-05],
        [-2.3574e-05, -1.6093e-05,  8.3968e-06,  ..., -2.0087e-05,
         -7.9572e-06, -1.2875e-05],
        [-1.9819e-05, -1.3530e-05,  7.0706e-06,  ..., -1.6883e-05,
         -6.6236e-06, -1.0803e-05],
        [-3.6925e-05, -2.5183e-05,  1.3158e-05,  ..., -3.1471e-05,
         -1.2487e-05, -2.0176e-05]], device='cuda:0')
Loss: 1.1114715337753296


Running epoch 0, step 502, batch 502
Sampled inputs[:2]: tensor([[    0,   515,   352,  ...,  2326,  3595,  6887],
        [    0, 13509,   472,  ...,  1805,    13, 27816]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8647e-04,  2.4930e-04, -2.3839e-04,  ...,  6.1420e-05,
         -2.0956e-04, -1.3358e-05],
        [ 7.4177e-05,  4.0691e-05, -2.9508e-05,  ...,  4.3986e-05,
          3.4016e-05,  3.0375e-05],
        [-2.7373e-05, -1.8716e-05,  9.8199e-06,  ..., -2.3410e-05,
         -9.3654e-06, -1.5050e-05],
        [-2.3022e-05, -1.5736e-05,  8.2776e-06,  ..., -1.9684e-05,
         -7.8008e-06, -1.2636e-05],
        [-4.2856e-05, -2.9266e-05,  1.5393e-05,  ..., -3.6657e-05,
         -1.4693e-05, -2.3574e-05]], device='cuda:0')
Loss: 1.1266589164733887


Running epoch 0, step 503, batch 503
Sampled inputs[:2]: tensor([[    0,    14, 30840,  ...,   287,   932,    14],
        [    0,   278,   266,  ..., 10639,   292,  4723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1490e-04,  2.7301e-04, -2.5955e-04,  ...,  8.3900e-05,
         -2.4092e-04,  7.9295e-05],
        [ 7.1703e-05,  3.8955e-05, -2.8633e-05,  ...,  4.1900e-05,
          3.3166e-05,  2.9019e-05],
        [-3.1307e-05, -2.1487e-05,  1.1213e-05,  ..., -2.6733e-05,
         -1.0699e-05, -1.7196e-05],
        [-2.6286e-05, -1.8030e-05,  9.4324e-06,  ..., -2.2426e-05,
         -8.8960e-06, -1.4409e-05],
        [-4.8995e-05, -3.3557e-05,  1.7568e-05,  ..., -4.1813e-05,
         -1.6779e-05, -2.6911e-05]], device='cuda:0')
Loss: 1.099636197090149
Graident accumulation at epoch 0, step 503, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0289, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.9763e-05,  5.6779e-05, -1.3905e-04,  ...,  3.7334e-05,
         -8.1045e-05,  1.1740e-05],
        [-1.4019e-05, -1.0924e-05,  4.1876e-06,  ..., -1.4106e-05,
         -2.1236e-06, -8.8343e-06],
        [ 7.2075e-05,  4.8648e-05, -2.3218e-05,  ...,  6.2194e-05,
          1.8912e-05,  3.8140e-05],
        [-2.3139e-06, -3.3342e-06,  3.9229e-07,  ..., -4.2976e-07,
         -1.4597e-07, -1.5027e-06],
        [-4.3830e-05, -2.9136e-05,  1.4843e-05,  ..., -3.6431e-05,
         -1.1788e-05, -2.2073e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1447e-08, 2.7718e-08, 3.4947e-08,  ..., 1.1473e-08, 8.6269e-08,
         1.1329e-08],
        [5.7955e-11, 3.0186e-11, 4.5352e-12,  ..., 3.6835e-11, 3.3377e-12,
         9.9773e-12],
        [1.1961e-09, 6.0831e-10, 1.0100e-10,  ..., 9.8099e-10, 6.5434e-11,
         3.0942e-10],
        [9.8765e-11, 4.6637e-11, 8.8967e-12,  ..., 8.3283e-11, 5.8245e-12,
         2.3247e-11],
        [2.4562e-10, 1.3394e-10, 1.4327e-11,  ..., 1.8368e-10, 1.0703e-11,
         5.4476e-11]], device='cuda:0')
optimizer state dict: 63.0
lr: [1.7765517845442444e-05, 1.7765517845442444e-05]
scheduler_last_epoch: 63


Running epoch 0, step 504, batch 504
Sampled inputs[:2]: tensor([[   0, 3393, 3380,  ...,  292, 6502,  950],
        [   0, 3253, 1573,  ...,  298,  358,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7413e-05,  4.1550e-05,  1.1751e-05,  ...,  2.2063e-06,
         -6.8001e-05, -4.3357e-05],
        [-2.4289e-06, -1.6242e-06,  8.5682e-07,  ..., -2.0415e-06,
         -8.0094e-07, -1.3039e-06],
        [-3.9339e-06, -2.6375e-06,  1.3933e-06,  ..., -3.3081e-06,
         -1.2815e-06, -2.1160e-06],
        [-3.2336e-06, -2.1607e-06,  1.1474e-06,  ..., -2.7269e-06,
         -1.0431e-06, -1.7285e-06],
        [ 1.7233e-04,  1.1394e-04, -5.9736e-05,  ...,  1.2076e-04,
          9.6810e-05,  1.0728e-04]], device='cuda:0')
Loss: 1.1117844581604004


Running epoch 0, step 505, batch 505
Sampled inputs[:2]: tensor([[    0,   266,  2109,  ...,  6730, 11558,   287],
        [    0,   328,   266,  ...,    14,  3352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0494e-04,  3.8830e-05, -5.1049e-05,  ..., -2.9845e-06,
         -7.1676e-05, -1.3412e-04],
        [-4.8131e-06, -3.2410e-06,  1.7099e-06,  ..., -4.0531e-06,
         -1.5385e-06, -2.5257e-06],
        [-7.8082e-06, -5.2601e-06,  2.7865e-06,  ..., -6.5863e-06,
         -2.4661e-06, -4.0978e-06],
        [-6.5267e-06, -4.3958e-06,  2.3320e-06,  ..., -5.5134e-06,
         -2.0415e-06, -3.4049e-06],
        [ 1.6619e-04,  1.0979e-04, -5.7530e-05,  ...,  1.1557e-04,
          9.4933e-05,  1.0415e-04]], device='cuda:0')
Loss: 1.1331936120986938


Running epoch 0, step 506, batch 506
Sampled inputs[:2]: tensor([[    0,   401,  3704,  ...,    14,  1062,  1804],
        [    0,   733,   560,  ...,  1172, 22808,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4363e-05,  3.8830e-05, -9.5906e-05,  ...,  3.0292e-05,
         -1.1809e-04, -1.3640e-04],
        [-7.2420e-06, -4.8429e-06,  2.5891e-06,  ..., -6.1244e-06,
         -2.3842e-06, -3.8520e-06],
        [-1.1683e-05, -7.8231e-06,  4.1872e-06,  ..., -9.9093e-06,
         -3.7849e-06, -6.1989e-06],
        [-9.8050e-06, -6.5714e-06,  3.5241e-06,  ..., -8.3297e-06,
         -3.1516e-06, -5.1856e-06],
        [ 1.6008e-04,  1.0577e-04, -5.5310e-05,  ...,  1.1033e-04,
          9.2846e-05,  1.0083e-04]], device='cuda:0')
Loss: 1.0994943380355835


Running epoch 0, step 507, batch 507
Sampled inputs[:2]: tensor([[    0, 15003, 19278,  ...,   287,   847,   328],
        [    0,  5136,   446,  ...,  1173,   300,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6055e-05,  6.9173e-06, -1.2373e-04,  ...,  4.1766e-05,
         -1.0901e-04, -8.8667e-05],
        [-9.7007e-06, -6.4895e-06,  3.4496e-06,  ..., -8.2105e-06,
         -3.1926e-06, -5.1558e-06],
        [-1.5646e-05, -1.0476e-05,  5.5730e-06,  ..., -1.3262e-05,
         -5.0738e-06, -8.2999e-06],
        [-1.3098e-05, -8.7768e-06,  4.6790e-06,  ..., -1.1116e-05,
         -4.2096e-06, -6.9290e-06],
        [ 1.5373e-04,  1.0154e-04, -5.3090e-05,  ...,  1.0496e-04,
          9.0775e-05,  9.7478e-05]], device='cuda:0')
Loss: 1.1018832921981812


Running epoch 0, step 508, batch 508
Sampled inputs[:2]: tensor([[   0, 1726, 3775,  ...,  300,  266, 1686],
        [   0,  266, 1513,  ...,  367, 1941,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2929e-04, -6.9500e-06, -1.1624e-04,  ..., -8.2667e-07,
         -1.0407e-04, -1.1956e-04],
        [-1.2159e-05, -8.1733e-06,  4.3251e-06,  ..., -1.0297e-05,
         -4.0308e-06, -6.4597e-06],
        [-1.9580e-05, -1.3173e-05,  6.9737e-06,  ..., -1.6615e-05,
         -6.4000e-06, -1.0386e-05],
        [-1.6347e-05, -1.0997e-05,  5.8413e-06,  ..., -1.3888e-05,
         -5.2974e-06, -8.6427e-06],
        [ 1.4741e-04,  9.7218e-05, -5.0840e-05,  ...,  9.9600e-05,
          8.8644e-05,  9.4140e-05]], device='cuda:0')
Loss: 1.1387205123901367


Running epoch 0, step 509, batch 509
Sampled inputs[:2]: tensor([[    0,  1901, 11083,  ...,   360,  6055,  2374],
        [    0,   344,   259,  ..., 47553,   287, 28978]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0235e-05,  4.6061e-05, -1.9975e-04,  ...,  2.1964e-05,
         -1.4067e-04, -1.8653e-04],
        [-1.4573e-05, -9.8497e-06,  5.2191e-06,  ..., -1.2353e-05,
         -4.8727e-06, -7.7486e-06],
        [-2.3454e-05, -1.5870e-05,  8.4192e-06,  ..., -1.9923e-05,
         -7.7337e-06, -1.2457e-05],
        [-1.9610e-05, -1.3277e-05,  7.0632e-06,  ..., -1.6689e-05,
         -6.4075e-06, -1.0386e-05],
        [ 1.4127e-04,  9.2926e-05, -4.8545e-05,  ...,  9.4354e-05,
          8.6513e-05,  9.0861e-05]], device='cuda:0')
Loss: 1.1156485080718994


Running epoch 0, step 510, batch 510
Sampled inputs[:2]: tensor([[    0,   266,  2552,  ...,    13, 16179,   800],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6416e-05,  4.4662e-05, -2.1499e-04,  ...,  4.0020e-05,
         -1.4100e-04, -2.1584e-04],
        [-1.7032e-05, -1.1504e-05,  6.1132e-06,  ..., -1.4395e-05,
         -5.7034e-06, -9.0674e-06],
        [-2.7418e-05, -1.8537e-05,  9.8646e-06,  ..., -2.3216e-05,
         -9.0599e-06, -1.4573e-05],
        [-2.2903e-05, -1.5497e-05,  8.2627e-06,  ..., -1.9416e-05,
         -7.4953e-06, -1.2144e-05],
        [ 1.3502e-04,  8.8724e-05, -4.6265e-05,  ...,  8.9169e-05,
          8.4412e-05,  8.7524e-05]], device='cuda:0')
Loss: 1.1237297058105469


Running epoch 0, step 511, batch 511
Sampled inputs[:2]: tensor([[   0, 8822, 1486,  ...,   12,  287, 6903],
        [   0,  342,  970,  ...,  401, 2907, 1657]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4755e-05,  1.1166e-04, -2.2195e-04,  ...,  3.6135e-05,
         -1.4349e-04, -1.3040e-04],
        [-1.9521e-05, -1.3150e-05,  6.9886e-06,  ..., -1.6466e-05,
         -6.5751e-06, -1.0341e-05],
        [-3.1471e-05, -2.1219e-05,  1.1288e-05,  ..., -2.6584e-05,
         -1.0461e-05, -1.6645e-05],
        [-2.6211e-05, -1.7688e-05,  9.4250e-06,  ..., -2.2173e-05,
         -8.6278e-06, -1.3828e-05],
        [ 1.2858e-04,  8.4462e-05, -4.4015e-05,  ...,  8.3834e-05,
          8.2192e-05,  8.4245e-05]], device='cuda:0')
Loss: 1.0899845361709595
Graident accumulation at epoch 0, step 511, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0182]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1311e-05,  6.2267e-05, -1.4734e-04,  ...,  3.7214e-05,
         -8.7289e-05, -2.4738e-06],
        [-1.4569e-05, -1.1147e-05,  4.4677e-06,  ..., -1.4342e-05,
         -2.5687e-06, -8.9850e-06],
        [ 6.1720e-05,  4.1661e-05, -1.9768e-05,  ...,  5.3316e-05,
          1.5974e-05,  3.2662e-05],
        [-4.7036e-06, -4.7695e-06,  1.2956e-06,  ..., -2.6041e-06,
         -9.9415e-07, -2.7352e-06],
        [-2.6589e-05, -1.7776e-05,  8.9569e-06,  ..., -2.4404e-05,
         -2.3897e-06, -1.1441e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1410e-08, 2.7703e-08, 3.4961e-08,  ..., 1.1463e-08, 8.6203e-08,
         1.1335e-08],
        [5.8278e-11, 3.0329e-11, 4.5795e-12,  ..., 3.7069e-11, 3.3776e-12,
         1.0074e-11],
        [1.1959e-09, 6.0815e-10, 1.0103e-10,  ..., 9.8072e-10, 6.5478e-11,
         3.0939e-10],
        [9.9353e-11, 4.6904e-11, 8.9766e-12,  ..., 8.3691e-11, 5.8931e-12,
         2.3415e-11],
        [2.6191e-10, 1.4094e-10, 1.6250e-11,  ..., 1.9052e-10, 1.7448e-11,
         6.1519e-11]], device='cuda:0')
optimizer state dict: 64.0
lr: [1.7687041437173095e-05, 1.7687041437173095e-05]
scheduler_last_epoch: 64


Running epoch 0, step 512, batch 512
Sampled inputs[:2]: tensor([[    0,   446,  1115,  ...,  1869,  4971,  1954],
        [    0, 13706,  1862,  ...,   275,  1036, 42948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7905e-05, -1.4273e-05, -3.3924e-05,  ..., -1.9730e-05,
         -3.2262e-05, -4.6121e-05],
        [-2.3991e-06, -1.6093e-06,  9.2015e-07,  ..., -2.0117e-06,
         -8.1956e-07, -1.2368e-06],
        [-3.9041e-06, -2.6375e-06,  1.5050e-06,  ..., -3.2783e-06,
         -1.3188e-06, -2.0117e-06],
        [-3.2634e-06, -2.2054e-06,  1.2591e-06,  ..., -2.7418e-06,
         -1.0878e-06, -1.6764e-06],
        [-6.2883e-06, -4.2319e-06,  2.4140e-06,  ..., -5.2750e-06,
         -2.1309e-06, -3.2336e-06]], device='cuda:0')
Loss: 1.1104772090911865


Running epoch 0, step 513, batch 513
Sampled inputs[:2]: tensor([[   0, 3468,  278,  ..., 2442,  292,  380],
        [   0,  843, 3365,  ..., 1136, 1615,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8207e-05,  3.9838e-06, -6.9979e-05,  ...,  2.7882e-05,
         -6.1147e-05, -6.4281e-05],
        [-4.8280e-06, -3.2112e-06,  1.8179e-06,  ..., -4.0233e-06,
         -1.6652e-06, -2.4736e-06],
        [-7.8380e-06, -5.2303e-06,  2.9579e-06,  ..., -6.5416e-06,
         -2.6673e-06, -4.0084e-06],
        [-6.5118e-06, -4.3511e-06,  2.4587e-06,  ..., -5.4389e-06,
         -2.1979e-06, -3.3230e-06],
        [-1.2636e-05, -8.4341e-06,  4.7684e-06,  ..., -1.0580e-05,
         -4.3213e-06, -6.4671e-06]], device='cuda:0')
Loss: 1.1322871446609497


Running epoch 0, step 514, batch 514
Sampled inputs[:2]: tensor([[    0,  2518,   437,  ...,    12,  1041,   283],
        [    0,    13,  2615,  ..., 31594, 15867,  3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.4098e-05,  2.9190e-05, -7.3738e-05,  ..., -2.2752e-07,
         -6.8298e-05, -6.2305e-05],
        [-7.2420e-06, -4.8429e-06,  2.7232e-06,  ..., -6.0499e-06,
         -2.5369e-06, -3.7178e-06],
        [-1.1772e-05, -7.8976e-06,  4.4331e-06,  ..., -9.8348e-06,
         -4.0680e-06, -6.0350e-06],
        [-9.7603e-06, -6.5565e-06,  3.6806e-06,  ..., -8.1658e-06,
         -3.3453e-06, -4.9844e-06],
        [-1.8954e-05, -1.2696e-05,  7.1377e-06,  ..., -1.5855e-05,
         -6.5714e-06, -9.7007e-06]], device='cuda:0')
Loss: 1.1023566722869873


Running epoch 0, step 515, batch 515
Sampled inputs[:2]: tensor([[   0,  287, 4579,  ...,  909,   12,  344],
        [   0, 7110,  278,  ...,   66,   13, 9070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1329e-05,  8.9343e-05, -8.1077e-05,  ...,  1.4448e-05,
         -5.1742e-05, -5.1949e-05],
        [-9.6262e-06, -6.4671e-06,  3.6173e-06,  ..., -8.0764e-06,
         -3.3788e-06, -4.9472e-06],
        [-1.5616e-05, -1.0520e-05,  5.8785e-06,  ..., -1.3098e-05,
         -5.3942e-06, -8.0019e-06],
        [-1.2964e-05, -8.7321e-06,  4.8801e-06,  ..., -1.0878e-05,
         -4.4331e-06, -6.6161e-06],
        [-2.5183e-05, -1.6928e-05,  9.4771e-06,  ..., -2.1130e-05,
         -8.7172e-06, -1.2875e-05]], device='cuda:0')
Loss: 1.1193593740463257


Running epoch 0, step 516, batch 516
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  5998,   591,  ...,  3126,    12,   358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7432e-05,  1.4289e-04, -9.7267e-05,  ..., -4.0960e-05,
         -7.4414e-05, -9.1932e-05],
        [-1.2010e-05, -8.0615e-06,  4.5188e-06,  ..., -1.0133e-05,
         -4.2468e-06, -6.1840e-06],
        [-1.9461e-05, -1.3083e-05,  7.3314e-06,  ..., -1.6391e-05,
         -6.7651e-06, -9.9838e-06],
        [-1.6183e-05, -1.0878e-05,  6.1020e-06,  ..., -1.3635e-05,
         -5.5730e-06, -8.2776e-06],
        [-3.1352e-05, -2.1040e-05,  1.1817e-05,  ..., -2.6435e-05,
         -1.0923e-05, -1.6063e-05]], device='cuda:0')
Loss: 1.11370050907135


Running epoch 0, step 517, batch 517
Sampled inputs[:2]: tensor([[    0, 12472,  1059,  ...,   642,   365,  6517],
        [    0,   292,    40,  ..., 26995,   278,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9349e-05,  1.9830e-04, -1.1329e-04,  ...,  1.6337e-05,
         -7.1894e-05, -8.3084e-05],
        [-1.4424e-05, -9.6411e-06,  5.4240e-06,  ..., -1.2174e-05,
         -5.1558e-06, -7.4953e-06],
        [-2.3305e-05, -1.5602e-05,  8.7768e-06,  ..., -1.9640e-05,
         -8.1882e-06, -1.2070e-05],
        [-1.9386e-05, -1.2979e-05,  7.3090e-06,  ..., -1.6347e-05,
         -6.7428e-06, -1.0014e-05],
        [-3.7551e-05, -2.5094e-05,  1.4141e-05,  ..., -3.1680e-05,
         -1.3232e-05, -1.9431e-05]], device='cuda:0')
Loss: 1.1150339841842651


Running epoch 0, step 518, batch 518
Sampled inputs[:2]: tensor([[    0,  1265,   328,  ...,  2282, 35414,    13],
        [    0,   275,  2101,  ...,  1145,   590,  1619]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9195e-05,  2.5993e-04, -2.2625e-04,  ...,  2.0283e-05,
         -9.2085e-05, -8.2470e-05],
        [-1.6868e-05, -1.1250e-05,  6.3442e-06,  ..., -1.4275e-05,
         -6.0573e-06, -8.8215e-06],
        [-2.7150e-05, -1.8135e-05,  1.0230e-05,  ..., -2.2963e-05,
         -9.5814e-06, -1.4141e-05],
        [-2.2605e-05, -1.5095e-05,  8.5235e-06,  ..., -1.9118e-05,
         -7.8976e-06, -1.1750e-05],
        [-4.3690e-05, -2.9147e-05,  1.6466e-05,  ..., -3.6985e-05,
         -1.5467e-05, -2.2739e-05]], device='cuda:0')
Loss: 1.1077619791030884


Running epoch 0, step 519, batch 519
Sampled inputs[:2]: tensor([[    0,  4209,   278,  ...,   287,  9971,   717],
        [    0, 11822,    12,  ...,   554,  3845,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3545e-04,  2.9859e-04, -2.6629e-04,  ...,  3.6163e-05,
         -7.2789e-05, -6.3585e-05],
        [-1.9327e-05, -1.2867e-05,  7.2755e-06,  ..., -1.6332e-05,
         -7.0110e-06, -1.0148e-05],
        [-3.1084e-05, -2.0728e-05,  1.1712e-05,  ..., -2.6241e-05,
         -1.1086e-05, -1.6242e-05],
        [-2.5839e-05, -1.7226e-05,  9.7454e-06,  ..., -2.1815e-05,
         -9.1195e-06, -1.3478e-05],
        [-5.0068e-05, -3.3349e-05,  1.8865e-05,  ..., -4.2289e-05,
         -1.7911e-05, -2.6152e-05]], device='cuda:0')
Loss: 1.0924326181411743
Graident accumulation at epoch 0, step 519, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0199],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3725e-05,  8.5900e-05, -1.5923e-04,  ...,  3.7109e-05,
         -8.5839e-05, -8.5850e-06],
        [-1.5045e-05, -1.1319e-05,  4.7485e-06,  ..., -1.4541e-05,
         -3.0129e-06, -9.1013e-06],
        [ 5.2440e-05,  3.5422e-05, -1.6620e-05,  ...,  4.5361e-05,
          1.3268e-05,  2.7771e-05],
        [-6.8171e-06, -6.0151e-06,  2.1405e-06,  ..., -4.5252e-06,
         -1.8067e-06, -3.8095e-06],
        [-2.8937e-05, -1.9333e-05,  9.9477e-06,  ..., -2.6193e-05,
         -3.9418e-06, -1.2912e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1387e-08, 2.7764e-08, 3.4997e-08,  ..., 1.1453e-08, 8.6122e-08,
         1.1328e-08],
        [5.8594e-11, 3.0464e-11, 4.6279e-12,  ..., 3.7299e-11, 3.4234e-12,
         1.0167e-11],
        [1.1956e-09, 6.0797e-10, 1.0106e-10,  ..., 9.8042e-10, 6.5535e-11,
         3.0934e-10],
        [9.9921e-11, 4.7153e-11, 9.0626e-12,  ..., 8.4084e-11, 5.9704e-12,
         2.3574e-11],
        [2.6416e-10, 1.4191e-10, 1.6590e-11,  ..., 1.9212e-10, 1.7751e-11,
         6.2142e-11]], device='cuda:0')
optimizer state dict: 65.0
lr: [1.7607390381871007e-05, 1.7607390381871007e-05]
scheduler_last_epoch: 65


Running epoch 0, step 520, batch 520
Sampled inputs[:2]: tensor([[    0,  1802,  4165,  ...,   298,   445,    28],
        [    0,    13, 26011,  ...,   342,  3873,   720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2203e-05,  5.4520e-06,  1.2140e-05,  ..., -3.8408e-05,
         -2.0493e-05, -2.5084e-05],
        [-2.4289e-06, -1.5944e-06,  9.5367e-07,  ..., -1.9819e-06,
         -9.0897e-07, -1.2517e-06],
        [-3.8445e-06, -2.5332e-06,  1.5050e-06,  ..., -3.1292e-06,
         -1.4082e-06, -1.9670e-06],
        [-3.2187e-06, -2.1160e-06,  1.2591e-06,  ..., -2.6226e-06,
         -1.1623e-06, -1.6466e-06],
        [-6.3479e-06, -4.1723e-06,  2.4885e-06,  ..., -5.1856e-06,
         -2.3395e-06, -3.2485e-06]], device='cuda:0')
Loss: 1.105115532875061


Running epoch 0, step 521, batch 521
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,   266, 15794,  ...,  3128,  6479,  2626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0151e-05,  1.1548e-05, -1.5375e-06,  ..., -3.2796e-05,
         -7.6338e-05, -5.7439e-05],
        [-4.8131e-06, -3.1814e-06,  1.9222e-06,  ..., -3.9786e-06,
         -1.8440e-06, -2.5034e-06],
        [-7.6294e-06, -5.0515e-06,  3.0398e-06,  ..., -6.3032e-06,
         -2.8610e-06, -3.9339e-06],
        [-6.4075e-06, -4.2319e-06,  2.5481e-06,  ..., -5.2899e-06,
         -2.3693e-06, -3.3006e-06],
        [-1.2487e-05, -8.2254e-06,  4.9770e-06,  ..., -1.0312e-05,
         -4.6939e-06, -6.4373e-06]], device='cuda:0')
Loss: 1.080968976020813


Running epoch 0, step 522, batch 522
Sampled inputs[:2]: tensor([[   0, 8754,   14,  ..., 6125,  394,  927],
        [   0,  287,  266,  ...,  333,  199, 3217]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3508e-05, -3.2358e-05, -2.7773e-05,  ..., -5.3137e-05,
         -8.2460e-05, -4.7782e-05],
        [-7.2122e-06, -4.7907e-06,  2.8759e-06,  ..., -5.9307e-06,
         -2.7120e-06, -3.7253e-06],
        [-1.1474e-05, -7.6294e-06,  4.5747e-06,  ..., -9.4324e-06,
         -4.2319e-06, -5.8860e-06],
        [-9.6560e-06, -6.3926e-06,  3.8370e-06,  ..., -7.9274e-06,
         -3.5092e-06, -4.9397e-06],
        [-1.8716e-05, -1.2398e-05,  7.4506e-06,  ..., -1.5378e-05,
         -6.9141e-06, -9.5814e-06]], device='cuda:0')
Loss: 1.1044310331344604


Running epoch 0, step 523, batch 523
Sampled inputs[:2]: tensor([[   0, 5440,   13,  ..., 1878,  342, 2060],
        [   0, 6909,  352,  ..., 1075,  706, 6909]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4069e-05, -3.7510e-05, -6.6996e-05,  ..., -3.0007e-05,
         -1.2494e-04, -4.3615e-05],
        [-9.5963e-06, -6.3404e-06,  3.7998e-06,  ..., -7.9125e-06,
         -3.5688e-06, -4.9546e-06],
        [-1.5259e-05, -1.0088e-05,  6.0424e-06,  ..., -1.2591e-05,
         -5.5656e-06, -7.8380e-06],
        [-1.2860e-05, -8.4788e-06,  5.0813e-06,  ..., -1.0595e-05,
         -4.6194e-06, -6.5863e-06],
        [-2.4885e-05, -1.6421e-05,  9.8497e-06,  ..., -2.0534e-05,
         -9.1046e-06, -1.2755e-05]], device='cuda:0')
Loss: 1.1028517484664917


Running epoch 0, step 524, batch 524
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    12,   259,  1220],
        [    0,   446, 23105,  ..., 11867,   824,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4845e-06,  1.2723e-05, -1.1195e-04,  ...,  4.6118e-05,
         -1.5420e-04, -2.0634e-06],
        [-1.2010e-05, -7.9125e-06,  4.7609e-06,  ..., -9.8944e-06,
         -4.4815e-06, -6.2212e-06],
        [-1.9059e-05, -1.2577e-05,  7.5549e-06,  ..., -1.5721e-05,
         -6.9663e-06, -9.8199e-06],
        [-1.6078e-05, -1.0580e-05,  6.3628e-06,  ..., -1.3247e-05,
         -5.7966e-06, -8.2627e-06],
        [-3.1143e-05, -2.0504e-05,  1.2338e-05,  ..., -2.5690e-05,
         -1.1429e-05, -1.6004e-05]], device='cuda:0')
Loss: 1.104015827178955


Running epoch 0, step 525, batch 525
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,   680,   401,  ...,  2872,   292, 23535]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4933e-06,  6.9660e-05, -1.6331e-04,  ...,  6.8389e-05,
         -1.7472e-04,  5.7636e-05],
        [-1.4424e-05, -9.4995e-06,  5.7444e-06,  ..., -1.1891e-05,
         -5.4799e-06, -7.4953e-06],
        [-2.2858e-05, -1.5080e-05,  9.1046e-06,  ..., -1.8865e-05,
         -8.5086e-06, -1.1817e-05],
        [-1.9252e-05, -1.2666e-05,  7.6592e-06,  ..., -1.5885e-05,
         -7.0781e-06, -9.9316e-06],
        [-3.7342e-05, -2.4587e-05,  1.4871e-05,  ..., -3.0816e-05,
         -1.3947e-05, -1.9267e-05]], device='cuda:0')
Loss: 1.119894027709961


Running epoch 0, step 526, batch 526
Sampled inputs[:2]: tensor([[    0,   298, 49038,  ...,   288,  1690,  2736],
        [    0,  1231,   278,  ...,    12,  2606,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4933e-06,  1.1956e-04, -1.8487e-04,  ...,  7.5208e-05,
         -1.4014e-04,  1.1181e-04],
        [-1.6838e-05, -1.1131e-05,  6.6757e-06,  ..., -1.3918e-05,
         -6.4410e-06, -8.7544e-06],
        [-2.6643e-05, -1.7643e-05,  1.0565e-05,  ..., -2.2054e-05,
         -9.9838e-06, -1.3784e-05],
        [-2.2396e-05, -1.4797e-05,  8.8736e-06,  ..., -1.8537e-05,
         -8.2925e-06, -1.1563e-05],
        [-4.3541e-05, -2.8789e-05,  1.7270e-05,  ..., -3.6031e-05,
         -1.6391e-05, -2.2486e-05]], device='cuda:0')
Loss: 1.1212602853775024


Running epoch 0, step 527, batch 527
Sampled inputs[:2]: tensor([[    0,   874,   445,  ...,    14, 16205,  8510],
        [    0,   266,  6071,  ...,  1061,  1107,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4539e-05,  1.7157e-04, -1.9175e-04,  ...,  9.1212e-05,
         -1.8602e-04,  1.3350e-04],
        [-1.9252e-05, -1.2703e-05,  7.6257e-06,  ..., -1.5914e-05,
         -7.3798e-06, -1.0006e-05],
        [-3.0458e-05, -2.0131e-05,  1.2070e-05,  ..., -2.5213e-05,
         -1.1437e-05, -1.5765e-05],
        [-2.5600e-05, -1.6883e-05,  1.0133e-05,  ..., -2.1189e-05,
         -9.4995e-06, -1.3217e-05],
        [-4.9740e-05, -3.2842e-05,  1.9714e-05,  ..., -4.1157e-05,
         -1.8761e-05, -2.5690e-05]], device='cuda:0')
Loss: 1.0952670574188232
Graident accumulation at epoch 0, step 527, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0033,  ..., -0.0027,  0.0226, -0.0198],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3807e-05,  9.4467e-05, -1.6248e-04,  ...,  4.2519e-05,
         -9.5858e-05,  5.6238e-06],
        [-1.5466e-05, -1.1457e-05,  5.0362e-06,  ..., -1.4679e-05,
         -3.4496e-06, -9.1917e-06],
        [ 4.4150e-05,  2.9867e-05, -1.3751e-05,  ...,  3.8303e-05,
          1.0798e-05,  2.3418e-05],
        [-8.6954e-06, -7.1019e-06,  2.9398e-06,  ..., -6.1916e-06,
         -2.5760e-06, -4.7503e-06],
        [-3.1017e-05, -2.0684e-05,  1.0924e-05,  ..., -2.7689e-05,
         -5.4237e-06, -1.4190e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1346e-08, 2.7766e-08, 3.4999e-08,  ..., 1.1450e-08, 8.6071e-08,
         1.1334e-08],
        [5.8906e-11, 3.0595e-11, 4.6814e-12,  ..., 3.7515e-11, 3.4744e-12,
         1.0257e-11],
        [1.1954e-09, 6.0777e-10, 1.0111e-10,  ..., 9.8008e-10, 6.5600e-11,
         3.0928e-10],
        [1.0048e-10, 4.7391e-11, 9.1562e-12,  ..., 8.4448e-11, 6.0546e-12,
         2.3725e-11],
        [2.6637e-10, 1.4285e-10, 1.6962e-11,  ..., 1.9362e-10, 1.8085e-11,
         6.2739e-11]], device='cuda:0')
optimizer state dict: 66.0
lr: [1.7526576850912724e-05, 1.7526576850912724e-05]
scheduler_last_epoch: 66


Running epoch 0, step 528, batch 528
Sampled inputs[:2]: tensor([[    0,  3101,   275,  ...,  2345,   609,   287],
        [    0,  2715, 10929,  ...,  4978,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4972e-05,  1.2463e-04, -2.5399e-06,  ...,  3.1125e-06,
         -4.1230e-05, -5.2398e-05],
        [-2.4438e-06, -1.5870e-06,  1.0505e-06,  ..., -1.9670e-06,
         -1.1325e-06, -1.3411e-06],
        [-3.7253e-06, -2.4289e-06,  1.6019e-06,  ..., -3.0100e-06,
         -1.6838e-06, -2.0266e-06],
        [-3.1143e-06, -2.0266e-06,  1.3411e-06,  ..., -2.5183e-06,
         -1.4007e-06, -1.6987e-06],
        [-6.2585e-06, -4.0531e-06,  2.6971e-06,  ..., -5.0366e-06,
         -2.8461e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.0922584533691406


Running epoch 0, step 529, batch 529
Sampled inputs[:2]: tensor([[    0,  2923,   391,  ...,    14,  5424,   298],
        [    0,    12,   287,  ...,    15, 35654,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0955e-04,  1.0731e-04, -2.2522e-05,  ..., -4.5097e-06,
          1.1851e-05, -3.0065e-05],
        [-4.9025e-06, -3.1739e-06,  2.1011e-06,  ..., -3.9637e-06,
         -2.2054e-06, -2.6748e-06],
        [-7.4506e-06, -4.8429e-06,  3.1963e-06,  ..., -6.0499e-06,
         -3.2783e-06, -4.0531e-06],
        [-6.2734e-06, -4.0680e-06,  2.6897e-06,  ..., -5.0962e-06,
         -2.7418e-06, -3.4049e-06],
        [-1.2457e-05, -8.0466e-06,  5.3495e-06,  ..., -1.0073e-05,
         -5.5134e-06, -6.7651e-06]], device='cuda:0')
Loss: 1.094222068786621


Running epoch 0, step 530, batch 530
Sampled inputs[:2]: tensor([[    0,  6532,  6984,  ...,   271,  8212, 14409],
        [    0,  1883,  1090,  ...,   365,  1943,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6393e-05,  9.3262e-05, -2.2522e-05,  ..., -1.4987e-05,
         -3.4248e-05, -6.2722e-05],
        [-7.2867e-06, -4.6864e-06,  3.1069e-06,  ..., -5.9307e-06,
         -3.2187e-06, -3.9712e-06],
        [-1.1116e-05, -7.1675e-06,  4.7386e-06,  ..., -9.0748e-06,
         -4.7833e-06, -6.0350e-06],
        [-9.3728e-06, -6.0350e-06,  3.9935e-06,  ..., -7.6592e-06,
         -3.9935e-06, -5.0664e-06],
        [-1.8597e-05, -1.1921e-05,  7.9274e-06,  ..., -1.5140e-05,
         -8.0615e-06, -1.0073e-05]], device='cuda:0')
Loss: 1.1143296957015991


Running epoch 0, step 531, batch 531
Sampled inputs[:2]: tensor([[   0,   14, 3609,  ...,  298,  413,   29],
        [   0,  199,  769,  ...,  380,  560,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6935e-05,  1.3523e-04,  1.0419e-05,  ..., -1.4986e-05,
         -1.0634e-05, -1.6533e-05],
        [-9.7454e-06, -6.2436e-06,  4.1202e-06,  ..., -7.9125e-06,
         -4.2915e-06, -5.2825e-06],
        [-1.4886e-05, -9.5665e-06,  6.2957e-06,  ..., -1.2115e-05,
         -6.3926e-06, -8.0466e-06],
        [-1.2502e-05, -8.0317e-06,  5.2899e-06,  ..., -1.0177e-05,
         -5.3123e-06, -6.7279e-06],
        [-2.4915e-05, -1.5914e-05,  1.0535e-05,  ..., -2.0206e-05,
         -1.0759e-05, -1.3426e-05]], device='cuda:0')
Loss: 1.1107226610183716


Running epoch 0, step 532, batch 532
Sampled inputs[:2]: tensor([[    0,   578, 26976,  ...,  1389,    14,  1742],
        [    0,  3634,  3444,  ...,   642,  2156,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5640e-05,  1.7877e-04,  2.9348e-05,  ...,  4.5582e-05,
         -9.8848e-06, -1.6533e-05],
        [-1.2159e-05, -7.8082e-06,  5.1558e-06,  ..., -9.8348e-06,
         -5.3570e-06, -6.6012e-06],
        [-1.8612e-05, -1.1981e-05,  7.9051e-06,  ..., -1.5095e-05,
         -8.0019e-06, -1.0073e-05],
        [-1.5631e-05, -1.0058e-05,  6.6385e-06,  ..., -1.2681e-05,
         -6.6385e-06, -8.4266e-06],
        [-3.1143e-05, -1.9938e-05,  1.3217e-05,  ..., -2.5183e-05,
         -1.3456e-05, -1.6809e-05]], device='cuda:0')
Loss: 1.0894702672958374


Running epoch 0, step 533, batch 533
Sampled inputs[:2]: tensor([[   0, 2270, 3279,  ...,  380,  475,  768],
        [   0,  278,  266,  ...,   13, 2853,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3209e-05,  1.7130e-04, -1.8927e-05,  ...,  6.4017e-06,
         -1.5859e-05, -7.9009e-05],
        [-1.4529e-05, -9.3281e-06,  6.1691e-06,  ..., -1.1832e-05,
         -6.3106e-06, -7.8753e-06],
        [-2.2262e-05, -1.4335e-05,  9.4697e-06,  ..., -1.8179e-05,
         -9.4399e-06, -1.2025e-05],
        [-1.8746e-05, -1.2070e-05,  7.9721e-06,  ..., -1.5318e-05,
         -7.8380e-06, -1.0088e-05],
        [-3.7193e-05, -2.3842e-05,  1.5810e-05,  ..., -3.0309e-05,
         -1.5855e-05, -2.0057e-05]], device='cuda:0')
Loss: 1.106931447982788


Running epoch 0, step 534, batch 534
Sampled inputs[:2]: tensor([[    0,     9,   870,  ...,  2671,   965,  3229],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4126e-06,  1.7209e-04, -1.0433e-05,  ...,  4.0366e-05,
         -6.8350e-05, -9.3042e-05],
        [-1.6913e-05, -1.0848e-05,  7.1749e-06,  ..., -1.3798e-05,
         -7.3388e-06, -9.1866e-06],
        [-2.5943e-05, -1.6689e-05,  1.1019e-05,  ..., -2.1219e-05,
         -1.0990e-05, -1.4037e-05],
        [-2.1875e-05, -1.4067e-05,  9.2909e-06,  ..., -1.7896e-05,
         -9.1419e-06, -1.1794e-05],
        [-4.3362e-05, -2.7776e-05,  1.8418e-05,  ..., -3.5405e-05,
         -1.8477e-05, -2.3440e-05]], device='cuda:0')
Loss: 1.1002871990203857


Running epoch 0, step 535, batch 535
Sampled inputs[:2]: tensor([[    0,  1796,   342,  ...,   668,  2903,   518],
        [    0,   843,    14,  ...,   659,   271, 10511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8298e-07,  1.2045e-04,  2.5533e-05,  ...,  2.6988e-06,
         -4.8164e-05, -4.1007e-06],
        [-1.9357e-05, -1.2428e-05,  8.2031e-06,  ..., -1.5780e-05,
         -8.3596e-06, -1.0446e-05],
        [-2.9713e-05, -1.9133e-05,  1.2606e-05,  ..., -2.4289e-05,
         -1.2547e-05, -1.5974e-05],
        [-2.5019e-05, -1.6108e-05,  1.0617e-05,  ..., -2.0459e-05,
         -1.0423e-05, -1.3411e-05],
        [-4.9710e-05, -3.1888e-05,  2.1100e-05,  ..., -4.0561e-05,
         -2.1115e-05, -2.6703e-05]], device='cuda:0')
Loss: 1.1158112287521362
Graident accumulation at epoch 0, step 535, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0148, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1444e-05,  9.7066e-05, -1.4368e-04,  ...,  3.8537e-05,
         -9.1088e-05,  4.6514e-06],
        [-1.5855e-05, -1.1554e-05,  5.3529e-06,  ..., -1.4789e-05,
         -3.9406e-06, -9.3171e-06],
        [ 3.6764e-05,  2.4967e-05, -1.1115e-05,  ...,  3.2044e-05,
          8.4633e-06,  1.9478e-05],
        [-1.0328e-05, -8.0025e-06,  3.7075e-06,  ..., -7.6184e-06,
         -3.3607e-06, -5.6164e-06],
        [-3.2887e-05, -2.1804e-05,  1.1942e-05,  ..., -2.8976e-05,
         -6.9928e-06, -1.5441e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1305e-08, 2.7753e-08, 3.4964e-08,  ..., 1.1438e-08, 8.5987e-08,
         1.1323e-08],
        [5.9221e-11, 3.0719e-11, 4.7440e-12,  ..., 3.7726e-11, 3.5408e-12,
         1.0356e-11],
        [1.1951e-09, 6.0753e-10, 1.0117e-10,  ..., 9.7969e-10, 6.5692e-11,
         3.0923e-10],
        [1.0100e-10, 4.7603e-11, 9.2598e-12,  ..., 8.4783e-11, 6.1572e-12,
         2.3881e-11],
        [2.6857e-10, 1.4373e-10, 1.7390e-11,  ..., 1.9508e-10, 1.8513e-11,
         6.3390e-11]], device='cuda:0')
optimizer state dict: 67.0
lr: [1.7444613193311206e-05, 1.7444613193311206e-05]
scheduler_last_epoch: 67


Running epoch 0, step 536, batch 536
Sampled inputs[:2]: tensor([[   0, 1234,  278,  ..., 1237, 1008,  417],
        [   0, 1231,  352,  ..., 8524,   14,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0451e-05, -1.9806e-05, -1.6544e-05,  ..., -2.2635e-05,
          9.5705e-08, -1.7500e-06],
        [-2.4140e-06, -1.4901e-06,  1.1027e-06,  ..., -1.9968e-06,
         -1.1846e-06, -1.3635e-06],
        [-3.5465e-06, -2.2054e-06,  1.6242e-06,  ..., -2.9504e-06,
         -1.6987e-06, -1.9968e-06],
        [-2.9802e-06, -1.8477e-06,  1.3635e-06,  ..., -2.4736e-06,
         -1.4082e-06, -1.6689e-06],
        [-5.9903e-06, -3.7104e-06,  2.7418e-06,  ..., -4.9770e-06,
         -2.8908e-06, -3.3528e-06]], device='cuda:0')
Loss: 1.0931528806686401


Running epoch 0, step 537, batch 537
Sampled inputs[:2]: tensor([[    0,   275,  2351,  ...,    14,  4520,    12],
        [    0,   292, 17181,  ...,   634,  5039,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4230e-05,  4.3109e-07, -1.6762e-04,  ...,  7.0796e-05,
         -7.7475e-05, -5.3088e-05],
        [-4.8727e-06, -3.0622e-06,  2.2426e-06,  ..., -3.9786e-06,
         -2.4065e-06, -2.7716e-06],
        [-7.1526e-06, -4.5151e-06,  3.2932e-06,  ..., -5.8562e-06,
         -3.4496e-06, -4.0382e-06],
        [-5.9903e-06, -3.7849e-06,  2.7567e-06,  ..., -4.9025e-06,
         -2.8536e-06, -3.3751e-06],
        [-1.2100e-05, -7.6145e-06,  5.5730e-06,  ..., -9.8944e-06,
         -5.8711e-06, -6.8247e-06]], device='cuda:0')
Loss: 1.0845041275024414


Running epoch 0, step 538, batch 538
Sampled inputs[:2]: tensor([[    0,    14,  3449,  ...,    12,  2665,     5],
        [    0,  7011,   650,  ..., 28839, 11610,  3222]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5595e-05, -1.0872e-05, -1.4807e-04,  ...,  6.3265e-05,
         -8.6708e-05, -3.9878e-05],
        [-7.3910e-06, -4.6492e-06,  3.3528e-06,  ..., -5.9605e-06,
         -3.6880e-06, -4.1872e-06],
        [-1.0833e-05, -6.8396e-06,  4.9174e-06,  ..., -8.7619e-06,
         -5.2825e-06, -6.1095e-06],
        [-9.0152e-06, -5.6922e-06,  4.0904e-06,  ..., -7.2867e-06,
         -4.3437e-06, -5.0738e-06],
        [-1.8388e-05, -1.1578e-05,  8.3596e-06,  ..., -1.4871e-05,
         -9.0152e-06, -1.0356e-05]], device='cuda:0')
Loss: 1.0909055471420288


Running epoch 0, step 539, batch 539
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,     7, 22455,  ...,    14,   747,  1501]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4400e-04, -3.0885e-05, -1.7506e-04,  ...,  8.3728e-05,
         -8.3366e-05, -2.9000e-05],
        [-9.7901e-06, -6.1989e-06,  4.4778e-06,  ..., -7.9125e-06,
         -4.8578e-06, -5.5432e-06],
        [-1.4439e-05, -9.1642e-06,  6.6012e-06,  ..., -1.1697e-05,
         -6.9961e-06, -8.1360e-06],
        [-1.2010e-05, -7.6294e-06,  5.4911e-06,  ..., -9.7156e-06,
         -5.7444e-06, -6.7502e-06],
        [-2.4527e-05, -1.5542e-05,  1.1235e-05,  ..., -1.9878e-05,
         -1.1966e-05, -1.3813e-05]], device='cuda:0')
Loss: 1.1063282489776611


Running epoch 0, step 540, batch 540
Sampled inputs[:2]: tensor([[   0, 2173,  292,  ...,  344, 8106,  344],
        [   0, 1420, 6319,  ...,  292, 4895, 4050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0260e-04, -7.0668e-05, -1.6497e-04,  ...,  1.4333e-05,
         -7.5813e-05,  3.8271e-07],
        [-1.2338e-05, -7.7635e-06,  5.6028e-06,  ..., -9.9242e-06,
         -6.1765e-06, -7.0408e-06],
        [-1.8120e-05, -1.1429e-05,  8.2254e-06,  ..., -1.4618e-05,
         -8.8587e-06, -1.0297e-05],
        [-1.5050e-05, -9.4995e-06,  6.8322e-06,  ..., -1.2130e-05,
         -7.2643e-06, -8.5309e-06],
        [-3.0816e-05, -1.9416e-05,  1.4022e-05,  ..., -2.4885e-05,
         -1.5169e-05, -1.7509e-05]], device='cuda:0')
Loss: 1.1007976531982422


Running epoch 0, step 541, batch 541
Sampled inputs[:2]: tensor([[    0,  1041,    14,  ...,   360,   266, 14966],
        [    0,   328, 27958,  ...,   417,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0143e-04, -6.8554e-05, -1.9930e-04,  ...,  4.9017e-05,
         -1.2489e-04, -3.5844e-05],
        [-1.4842e-05, -9.3207e-06,  6.7502e-06,  ..., -1.1861e-05,
         -7.4133e-06, -8.4564e-06],
        [-2.1860e-05, -1.3754e-05,  9.9391e-06,  ..., -1.7524e-05,
         -1.0662e-05, -1.2413e-05],
        [-1.8150e-05, -1.1437e-05,  8.2552e-06,  ..., -1.4544e-05,
         -8.7470e-06, -1.0282e-05],
        [-3.7163e-05, -2.3350e-05,  1.6928e-05,  ..., -2.9832e-05,
         -1.8254e-05, -2.1085e-05]], device='cuda:0')
Loss: 1.1024912595748901


Running epoch 0, step 542, batch 542
Sampled inputs[:2]: tensor([[    0,   266,   923,  ...,    14,   298, 12230],
        [    0,  4868,  3106,  ...,  2637,   278,   521]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3824e-04, -8.9415e-05, -3.1920e-04,  ...,  1.0450e-04,
         -1.2495e-04, -1.1158e-04],
        [-1.7166e-05, -1.0803e-05,  7.8380e-06,  ..., -1.3769e-05,
         -8.4862e-06, -9.7677e-06],
        [-2.5347e-05, -1.5989e-05,  1.1578e-05,  ..., -2.0400e-05,
         -1.2226e-05, -1.4365e-05],
        [-2.1160e-05, -1.3359e-05,  9.6634e-06,  ..., -1.7017e-05,
         -1.0073e-05, -1.1966e-05],
        [-4.3064e-05, -2.7105e-05,  1.9699e-05,  ..., -3.4690e-05,
         -2.0906e-05, -2.4393e-05]], device='cuda:0')
Loss: 1.0850752592086792


Running epoch 0, step 543, batch 543
Sampled inputs[:2]: tensor([[    0,   609,    12,  ...,   409, 11041,   292],
        [    0,  3804,   300,  ...,  5062,  9848,  3515]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8505e-04, -2.3735e-05, -2.6749e-04,  ...,  9.2011e-05,
         -1.7224e-04, -1.4615e-04],
        [-1.9550e-05, -1.2316e-05,  8.9109e-06,  ..., -1.5751e-05,
         -9.7379e-06, -1.1206e-05],
        [-2.8893e-05, -1.8239e-05,  1.3173e-05,  ..., -2.3320e-05,
         -1.4037e-05, -1.6481e-05],
        [-2.4110e-05, -1.5222e-05,  1.0990e-05,  ..., -1.9446e-05,
         -1.1563e-05, -1.3717e-05],
        [-4.9055e-05, -3.0890e-05,  2.2396e-05,  ..., -3.9637e-05,
         -2.3961e-05, -2.7955e-05]], device='cuda:0')
Loss: 1.102726697921753
Graident accumulation at epoch 0, step 543, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0147,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0342, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0149, -0.0275,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.2053e-06,  8.4986e-05, -1.5606e-04,  ...,  4.3885e-05,
         -9.9204e-05, -1.0429e-05],
        [-1.6224e-05, -1.1630e-05,  5.7087e-06,  ..., -1.4885e-05,
         -4.5203e-06, -9.5060e-06],
        [ 3.0198e-05,  2.0646e-05, -8.6863e-06,  ...,  2.6508e-05,
          6.2133e-06,  1.5883e-05],
        [-1.1706e-05, -8.7244e-06,  4.4357e-06,  ..., -8.8012e-06,
         -4.1810e-06, -6.4264e-06],
        [-3.4503e-05, -2.2713e-05,  1.2987e-05,  ..., -3.0042e-05,
         -8.6897e-06, -1.6693e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1345e-08, 2.7726e-08, 3.5001e-08,  ..., 1.1435e-08, 8.5931e-08,
         1.1333e-08],
        [5.9544e-11, 3.0840e-11, 4.8187e-12,  ..., 3.7937e-11, 3.6321e-12,
         1.0471e-11],
        [1.1947e-09, 6.0725e-10, 1.0124e-10,  ..., 9.7925e-10, 6.5823e-11,
         3.0919e-10],
        [1.0148e-10, 4.7787e-11, 9.3713e-12,  ..., 8.5076e-11, 6.2848e-12,
         2.4045e-11],
        [2.7071e-10, 1.4454e-10, 1.7874e-11,  ..., 1.9645e-10, 1.9069e-11,
         6.4108e-11]], device='cuda:0')
optimizer state dict: 68.0
lr: [1.73615119338288e-05, 1.73615119338288e-05]
scheduler_last_epoch: 68


Running epoch 0, step 544, batch 544
Sampled inputs[:2]: tensor([[    0,   221,  1771,  ..., 14547,  1705,  1003],
        [    0,   301,   298,  ...,   806,   352, 22105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1574e-06,  2.4785e-05,  1.0948e-05,  ..., -1.6295e-06,
          4.8782e-06,  1.4865e-04],
        [-2.4140e-06, -1.5795e-06,  1.1921e-06,  ..., -1.9819e-06,
         -1.3933e-06, -1.4603e-06],
        [-3.4571e-06, -2.2650e-06,  1.7062e-06,  ..., -2.8461e-06,
         -1.9372e-06, -2.0862e-06],
        [-2.8461e-06, -1.8626e-06,  1.4007e-06,  ..., -2.3395e-06,
         -1.5721e-06, -1.7062e-06],
        [-5.9605e-06, -3.9041e-06,  2.9355e-06,  ..., -4.9174e-06,
         -3.3677e-06, -3.5763e-06]], device='cuda:0')
Loss: 1.0966286659240723


Running epoch 0, step 545, batch 545
Sampled inputs[:2]: tensor([[    0,   409,  4146,  ...,     9,   360,   259],
        [    0,   292, 12522,  ...,   266,  1977,  8481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4322e-05, -9.7518e-06, -5.3690e-06,  ..., -5.0942e-06,
         -8.6221e-06,  1.4479e-04],
        [-4.8727e-06, -3.1441e-06,  2.3618e-06,  ..., -3.9935e-06,
         -2.7269e-06, -2.9281e-06],
        [-6.9737e-06, -4.5002e-06,  3.3826e-06,  ..., -5.7220e-06,
         -3.7849e-06, -4.1723e-06],
        [-5.7817e-06, -3.7327e-06,  2.8014e-06,  ..., -4.7535e-06,
         -3.0994e-06, -3.4422e-06],
        [-1.2070e-05, -7.7784e-06,  5.8413e-06,  ..., -9.8944e-06,
         -6.6012e-06, -7.1824e-06]], device='cuda:0')
Loss: 1.091975212097168


Running epoch 0, step 546, batch 546
Sampled inputs[:2]: tensor([[   0,  266, 1634,  ...,  310, 1372,  287],
        [   0,  898,  266,  ...,   12, 3222, 8095]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1864e-04, -5.0564e-05, -1.2317e-04,  ...,  6.0669e-06,
          2.3673e-05, -2.7930e-05],
        [-7.2420e-06, -4.6939e-06,  3.5390e-06,  ..., -5.9754e-06,
         -3.9488e-06, -4.3288e-06],
        [-1.0431e-05, -6.7651e-06,  5.0962e-06,  ..., -8.6129e-06,
         -5.5060e-06, -6.1840e-06],
        [-8.7023e-06, -5.6401e-06,  4.2468e-06,  ..., -7.1973e-06,
         -4.5300e-06, -5.1409e-06],
        [-1.8001e-05, -1.1653e-05,  8.7917e-06,  ..., -1.4842e-05,
         -9.5814e-06, -1.0639e-05]], device='cuda:0')
Loss: 1.1043250560760498


Running epoch 0, step 547, batch 547
Sampled inputs[:2]: tensor([[   0,  221,  422,  ..., 2693,  733,  381],
        [   0, 1064, 1042,  ...,   12,  259, 4754]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4238e-04, -7.4967e-05, -1.0979e-04,  ...,  4.6184e-05,
          1.7091e-05,  5.8662e-05],
        [-9.6858e-06, -6.2287e-06,  4.7237e-06,  ..., -7.9125e-06,
         -5.3123e-06, -5.7966e-06],
        [-1.3992e-05, -9.0152e-06,  6.8173e-06,  ..., -1.1444e-05,
         -7.4282e-06, -8.2999e-06],
        [-1.1623e-05, -7.4804e-06,  5.6550e-06,  ..., -9.5218e-06,
         -6.0871e-06, -6.8694e-06],
        [-2.4199e-05, -1.5557e-05,  1.1787e-05,  ..., -1.9759e-05,
         -1.2949e-05, -1.4320e-05]], device='cuda:0')
Loss: 1.1087017059326172


Running epoch 0, step 548, batch 548
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,  3968,   446,  ...,    22,   722,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2214e-04, -9.1183e-05, -2.0194e-04,  ...,  6.5555e-05,
         -1.0253e-04, -3.0100e-05],
        [-1.2055e-05, -7.7635e-06,  5.9009e-06,  ..., -9.8646e-06,
         -6.5491e-06, -7.2122e-06],
        [-1.7479e-05, -1.1280e-05,  8.5533e-06,  ..., -1.4305e-05,
         -9.1940e-06, -1.0371e-05],
        [-1.4573e-05, -9.3877e-06,  7.1228e-06,  ..., -1.1951e-05,
         -7.5698e-06, -8.6203e-06],
        [-3.0220e-05, -1.9431e-05,  1.4782e-05,  ..., -2.4706e-05,
         -1.6019e-05, -1.7881e-05]], device='cuda:0')
Loss: 1.0966732501983643


Running epoch 0, step 549, batch 549
Sampled inputs[:2]: tensor([[   0,   12, 1197,  ...,  516, 1136, 9774],
        [   0, 1978,  352,  ..., 2276,   12,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9638e-04,  2.3612e-05, -3.8080e-04,  ...,  1.3813e-04,
         -2.1949e-04, -1.2347e-04],
        [-1.4395e-05, -9.2611e-06,  7.0482e-06,  ..., -1.1846e-05,
         -7.8157e-06, -8.6278e-06],
        [-2.0862e-05, -1.3441e-05,  1.0215e-05,  ..., -1.7166e-05,
         -1.0945e-05, -1.2398e-05],
        [-1.7464e-05, -1.1243e-05,  8.5384e-06,  ..., -1.4409e-05,
         -9.0450e-06, -1.0349e-05],
        [-3.6001e-05, -2.3127e-05,  1.7613e-05,  ..., -2.9594e-05,
         -1.9029e-05, -2.1324e-05]], device='cuda:0')
Loss: 1.1022658348083496


Running epoch 0, step 550, batch 550
Sampled inputs[:2]: tensor([[    0,   380,   560,  ...,   287,  6769,   806],
        [    0,  1665,  6306,  ...,   300, 10204,   582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9939e-04, -3.8506e-05, -5.2974e-04,  ...,  2.2374e-04,
         -2.6502e-04, -1.7509e-04],
        [-1.6838e-05, -1.0818e-05,  8.2478e-06,  ..., -1.3813e-05,
         -9.1046e-06, -1.0051e-05],
        [-2.4378e-05, -1.5691e-05,  1.1936e-05,  ..., -2.0012e-05,
         -1.2733e-05, -1.4439e-05],
        [-2.0429e-05, -1.3135e-05,  9.9838e-06,  ..., -1.6809e-05,
         -1.0528e-05, -1.2055e-05],
        [-4.2081e-05, -2.7001e-05,  2.0579e-05,  ..., -3.4481e-05,
         -2.2128e-05, -2.4825e-05]], device='cuda:0')
Loss: 1.1166874170303345


Running epoch 0, step 551, batch 551
Sampled inputs[:2]: tensor([[    0, 19444,  6307,  ...,    13, 38005,  1447],
        [    0, 23809, 27646,  ...,   266,  3373,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4765e-04,  1.7958e-05, -6.1420e-04,  ...,  2.1294e-04,
         -2.6908e-04, -1.6904e-04],
        [-1.9297e-05, -1.2405e-05,  9.4175e-06,  ..., -1.5795e-05,
         -1.0379e-05, -1.1444e-05],
        [-2.7940e-05, -1.8001e-05,  1.3627e-05,  ..., -2.2903e-05,
         -1.4521e-05, -1.6451e-05],
        [-2.3410e-05, -1.5058e-05,  1.1399e-05,  ..., -1.9222e-05,
         -1.2003e-05, -1.3731e-05],
        [-4.8280e-05, -3.0994e-05,  2.3514e-05,  ..., -3.9488e-05,
         -2.5257e-05, -2.8312e-05]], device='cuda:0')
Loss: 1.0951052904129028
Graident accumulation at epoch 0, step 551, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0054, -0.0146,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0288, -0.0079,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0149, -0.0276,  ...,  0.0284, -0.0155, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.3049e-05,  7.8283e-05, -2.0188e-04,  ...,  6.0790e-05,
         -1.1619e-04, -2.6290e-05],
        [-1.6532e-05, -1.1708e-05,  6.0796e-06,  ..., -1.4976e-05,
         -5.1062e-06, -9.6998e-06],
        [ 2.4384e-05,  1.6781e-05, -6.4550e-06,  ...,  2.1567e-05,
          4.1399e-06,  1.2649e-05],
        [-1.2876e-05, -9.3578e-06,  5.1321e-06,  ..., -9.8433e-06,
         -4.9632e-06, -7.1569e-06],
        [-3.5881e-05, -2.3541e-05,  1.4040e-05,  ..., -3.0987e-05,
         -1.0346e-05, -1.7855e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1424e-08, 2.7698e-08, 3.5343e-08,  ..., 1.1469e-08, 8.5917e-08,
         1.1350e-08],
        [5.9857e-11, 3.0963e-11, 4.9025e-12,  ..., 3.8148e-11, 3.7362e-12,
         1.0592e-11],
        [1.1943e-09, 6.0697e-10, 1.0132e-10,  ..., 9.7880e-10, 6.5968e-11,
         3.0915e-10],
        [1.0193e-10, 4.7966e-11, 9.4918e-12,  ..., 8.5360e-11, 6.4226e-12,
         2.4210e-11],
        [2.7277e-10, 1.4535e-10, 1.8409e-11,  ..., 1.9782e-10, 1.9687e-11,
         6.4845e-11]], device='cuda:0')
optimizer state dict: 69.0
lr: [1.7277285771063362e-05, 1.7277285771063362e-05]
scheduler_last_epoch: 69


Running epoch 0, step 552, batch 552
Sampled inputs[:2]: tensor([[    0,  1756,   271,  ...,   259, 48595, 19882],
        [    0,   767,  1811,  ...,  1441,  1428,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3191e-05,  3.5742e-05, -1.0911e-04,  ...,  5.5329e-05,
         -1.8251e-05,  4.5884e-06],
        [-2.2650e-06, -1.5348e-06,  1.2144e-06,  ..., -1.9521e-06,
         -1.1474e-06, -1.3635e-06],
        [-3.2634e-06, -2.2203e-06,  1.7509e-06,  ..., -2.8312e-06,
         -1.5795e-06, -1.9521e-06],
        [-2.8312e-06, -1.9222e-06,  1.5050e-06,  ..., -2.4438e-06,
         -1.3411e-06, -1.6764e-06],
        [-5.6922e-06, -3.8445e-06,  3.0398e-06,  ..., -4.9174e-06,
         -2.7865e-06, -3.3826e-06]], device='cuda:0')
Loss: 1.085404396057129


Running epoch 0, step 553, batch 553
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,  1765,  5370,  ...,  1711,   292,   380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0699e-04,  2.4505e-05, -2.5102e-04,  ...,  7.1169e-05,
          2.5337e-06, -4.4863e-05],
        [-4.5300e-06, -3.0771e-06,  2.4363e-06,  ..., -3.9339e-06,
         -2.4661e-06, -2.8461e-06],
        [-6.4671e-06, -4.4107e-06,  3.4794e-06,  ..., -5.6177e-06,
         -3.3528e-06, -4.0233e-06],
        [-5.5581e-06, -3.7849e-06,  2.9728e-06,  ..., -4.8280e-06,
         -2.8312e-06, -3.4422e-06],
        [-1.1265e-05, -7.6443e-06,  6.0499e-06,  ..., -9.7752e-06,
         -5.9009e-06, -6.9737e-06]], device='cuda:0')
Loss: 1.0973635911941528


Running epoch 0, step 554, batch 554
Sampled inputs[:2]: tensor([[   0,  518, 9048,  ..., 1354,  352,  266],
        [   0,  346,   14,  ...,  381,  535,  505]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7422e-05, -4.0433e-05, -3.1375e-04,  ...,  6.0641e-05,
         -6.3113e-05, -7.5655e-05],
        [-6.7949e-06, -4.6343e-06,  3.6582e-06,  ..., -5.8860e-06,
         -3.6582e-06, -4.2394e-06],
        [-9.6709e-06, -6.6012e-06,  5.2005e-06,  ..., -8.3745e-06,
         -4.9695e-06, -5.9754e-06],
        [-8.3447e-06, -5.6922e-06,  4.4703e-06,  ..., -7.2271e-06,
         -4.2170e-06, -5.1409e-06],
        [-1.6868e-05, -1.1489e-05,  9.0748e-06,  ..., -1.4603e-05,
         -8.7470e-06, -1.0386e-05]], device='cuda:0')
Loss: 1.0749680995941162


Running epoch 0, step 555, batch 555
Sampled inputs[:2]: tensor([[    0, 21325, 16967,  ...,  5895,   344,   513],
        [    0,    14,   417,  ...,  8821,  6845,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7755e-05, -3.0663e-05, -3.0751e-04,  ...,  2.8333e-05,
         -4.5836e-05,  5.6253e-05],
        [-9.1195e-06, -6.1914e-06,  4.8280e-06,  ..., -7.8678e-06,
         -4.9546e-06, -5.6773e-06],
        [-1.2994e-05, -8.8215e-06,  6.8620e-06,  ..., -1.1206e-05,
         -6.7428e-06, -8.0168e-06],
        [-1.1146e-05, -7.5623e-06,  5.8711e-06,  ..., -9.6112e-06,
         -5.6773e-06, -6.8471e-06],
        [-2.2709e-05, -1.5393e-05,  1.2010e-05,  ..., -1.9580e-05,
         -1.1891e-05, -1.3962e-05]], device='cuda:0')
Loss: 1.0991053581237793


Running epoch 0, step 556, batch 556
Sampled inputs[:2]: tensor([[    0,   382,  9279,  ...,   445, 37790,     9],
        [    0,    14,   496,  ...,   368,   259,   490]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7223e-05,  4.8097e-05, -2.9689e-04,  ...,  1.9122e-05,
         -4.2634e-05,  8.5182e-05],
        [-1.1578e-05, -7.8008e-06,  6.0499e-06,  ..., -9.9093e-06,
         -6.3628e-06, -7.1600e-06],
        [-1.6451e-05, -1.1086e-05,  8.5756e-06,  ..., -1.4082e-05,
         -8.6650e-06, -1.0088e-05],
        [-1.3992e-05, -9.4250e-06,  7.2792e-06,  ..., -1.1981e-05,
         -7.2420e-06, -8.5533e-06],
        [-2.8849e-05, -1.9416e-05,  1.5065e-05,  ..., -2.4676e-05,
         -1.5333e-05, -1.7643e-05]], device='cuda:0')
Loss: 1.1252315044403076


Running epoch 0, step 557, batch 557
Sampled inputs[:2]: tensor([[   0,   14, 3921,  ...,  199, 2038, 1963],
        [   0,  278,  634,  ...,  598, 1722,  591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2743e-05,  1.1003e-05, -2.9417e-04,  ...,  2.5016e-05,
         -2.1792e-05,  8.3748e-05],
        [-1.3873e-05, -9.3654e-06,  7.2420e-06,  ..., -1.1876e-05,
         -7.4729e-06, -8.4713e-06],
        [-1.9819e-05, -1.3381e-05,  1.0334e-05,  ..., -1.6972e-05,
         -1.0230e-05, -1.1995e-05],
        [-1.6868e-05, -1.1392e-05,  8.7768e-06,  ..., -1.4454e-05,
         -8.5533e-06, -1.0185e-05],
        [-3.4750e-05, -2.3440e-05,  1.8150e-05,  ..., -2.9743e-05,
         -1.8090e-05, -2.0981e-05]], device='cuda:0')
Loss: 1.1012628078460693


Running epoch 0, step 558, batch 558
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2915,  275, 2565],
        [   0,   14, 7870,  ...,  284,  830,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0198e-05,  1.7190e-04, -3.4888e-04,  ...,  1.4511e-04,
         -1.8223e-04,  2.6895e-05],
        [-1.6123e-05, -1.0885e-05,  8.4415e-06,  ..., -1.3858e-05,
         -8.8811e-06, -9.9763e-06],
        [-2.2948e-05, -1.5482e-05,  1.2010e-05,  ..., -1.9729e-05,
         -1.2100e-05, -1.4067e-05],
        [-1.9550e-05, -1.3195e-05,  1.0215e-05,  ..., -1.6823e-05,
         -1.0140e-05, -1.1951e-05],
        [-4.0144e-05, -2.7075e-05,  2.1040e-05,  ..., -3.4511e-05,
         -2.1338e-05, -2.4542e-05]], device='cuda:0')
Loss: 1.0873287916183472


Running epoch 0, step 559, batch 559
Sampled inputs[:2]: tensor([[    0,   266,  1586,  ...,  1888,  2117,   328],
        [    0, 10565,  2677,  ...,   298,   292, 11188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6293e-05,  1.1532e-04, -4.6785e-04,  ...,  1.4085e-04,
         -1.5053e-04, -5.8476e-05],
        [-1.8373e-05, -1.2428e-05,  9.5814e-06,  ..., -1.5765e-05,
         -9.9763e-06, -1.1295e-05],
        [-2.6241e-05, -1.7747e-05,  1.3672e-05,  ..., -2.2531e-05,
         -1.3627e-05, -1.5974e-05],
        [-2.2352e-05, -1.5132e-05,  1.1630e-05,  ..., -1.9222e-05,
         -1.1414e-05, -1.3575e-05],
        [-4.5866e-05, -3.1009e-05,  2.3946e-05,  ..., -3.9399e-05,
         -2.4036e-05, -2.7865e-05]], device='cuda:0')
Loss: 1.085437297821045
Graident accumulation at epoch 0, step 559, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0027,  0.0227, -0.0198],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0149, -0.0276,  ...,  0.0284, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.1374e-05,  8.1987e-05, -2.2848e-04,  ...,  6.8796e-05,
         -1.1963e-04, -2.9509e-05],
        [-1.6716e-05, -1.1780e-05,  6.4298e-06,  ..., -1.5055e-05,
         -5.5932e-06, -9.8593e-06],
        [ 1.9322e-05,  1.3329e-05, -4.4423e-06,  ...,  1.7157e-05,
          2.3632e-06,  9.7869e-06],
        [-1.3824e-05, -9.9352e-06,  5.7819e-06,  ..., -1.0781e-05,
         -5.6083e-06, -7.7987e-06],
        [-3.6879e-05, -2.4288e-05,  1.5031e-05,  ..., -3.1828e-05,
         -1.1715e-05, -1.8856e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1384e-08, 2.7684e-08, 3.5527e-08,  ..., 1.1477e-08, 8.5854e-08,
         1.1342e-08],
        [6.0135e-11, 3.1086e-11, 4.9894e-12,  ..., 3.8359e-11, 3.8320e-12,
         1.0709e-11],
        [1.1938e-09, 6.0668e-10, 1.0141e-10,  ..., 9.7833e-10, 6.6088e-11,
         3.0910e-10],
        [1.0233e-10, 4.8147e-11, 9.6176e-12,  ..., 8.5644e-11, 6.5464e-12,
         2.4370e-11],
        [2.7460e-10, 1.4617e-10, 1.8964e-11,  ..., 1.9917e-10, 2.0245e-11,
         6.5557e-11]], device='cuda:0')
optimizer state dict: 70.0
lr: [1.7191947575507777e-05, 1.7191947575507777e-05]
scheduler_last_epoch: 70


Running epoch 0, step 560, batch 560
Sampled inputs[:2]: tensor([[    0,   397,  1267,  ...,  1276,   292,   221],
        [    0, 20241,  1244,  ...,  6232,  1004,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0692e-05,  3.5490e-05, -4.6694e-05,  ..., -5.3386e-05,
         -2.3937e-05, -1.8293e-05],
        [-2.1160e-06, -1.4901e-06,  1.1176e-06,  ..., -1.9521e-06,
         -1.0878e-06, -1.3337e-06],
        [-2.9355e-06, -2.0713e-06,  1.5572e-06,  ..., -2.6971e-06,
         -1.4082e-06, -1.8254e-06],
        [-2.6077e-06, -1.8254e-06,  1.3784e-06,  ..., -2.3991e-06,
         -1.2293e-06, -1.6168e-06],
        [-5.3048e-06, -3.7253e-06,  2.8163e-06,  ..., -4.8578e-06,
         -2.5630e-06, -3.2783e-06]], device='cuda:0')
Loss: 1.0800632238388062


Running epoch 0, step 561, batch 561
Sampled inputs[:2]: tensor([[    0,   531,    20,  ...,    12,  1644,   680],
        [    0,  1067,   292,  ..., 10792, 11280,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1836e-04,  1.1326e-04, -3.1913e-05,  ...,  5.2165e-06,
         -3.7556e-05,  3.3887e-05],
        [-4.3958e-06, -3.0845e-06,  2.3320e-06,  ..., -3.9339e-06,
         -2.4736e-06, -2.8610e-06],
        [-6.0648e-06, -4.2766e-06,  3.2261e-06,  ..., -5.4091e-06,
         -3.2112e-06, -3.8967e-06],
        [-5.2601e-06, -3.6806e-06,  2.7865e-06,  ..., -4.6939e-06,
         -2.7269e-06, -3.3602e-06],
        [-1.0908e-05, -7.6592e-06,  5.8115e-06,  ..., -9.7156e-06,
         -5.8115e-06, -6.9737e-06]], device='cuda:0')
Loss: 1.0612915754318237


Running epoch 0, step 562, batch 562
Sampled inputs[:2]: tensor([[    0,    13,  4363,  ...,   271,  2462,   709],
        [    0,    12,  3454,  ...,   717,  1765, 14906]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0307e-04,  2.1308e-04, -2.4807e-05,  ..., -6.5645e-07,
         -4.9109e-05,  9.2316e-05],
        [-6.6459e-06, -4.6715e-06,  3.5018e-06,  ..., -5.8860e-06,
         -3.5912e-06, -4.2170e-06],
        [-9.2387e-06, -6.5118e-06,  4.8801e-06,  ..., -8.1658e-06,
         -4.6939e-06, -5.7891e-06],
        [-8.0168e-06, -5.6177e-06,  4.2245e-06,  ..., -7.0930e-06,
         -3.9935e-06, -4.9993e-06],
        [-1.6600e-05, -1.1653e-05,  8.7768e-06,  ..., -1.4663e-05,
         -8.4937e-06, -1.0356e-05]], device='cuda:0')
Loss: 1.0896416902542114


Running epoch 0, step 563, batch 563
Sampled inputs[:2]: tensor([[   0,   12,  271,  ...,   12,  298,  273],
        [   0,  367, 1236,  ...,  344,  292,   20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4976e-04,  3.2228e-04, -9.3476e-05,  ...,  8.5193e-05,
         -1.5305e-04,  1.4943e-04],
        [-8.8811e-06, -6.2585e-06,  4.7386e-06,  ..., -7.8529e-06,
         -4.7833e-06, -5.6624e-06],
        [ 5.4697e-05,  5.5297e-05, -4.0575e-05,  ...,  6.3830e-05,
          2.3667e-06,  5.7630e-05],
        [-1.0714e-05, -7.5251e-06,  5.7146e-06,  ..., -9.4622e-06,
         -5.3346e-06, -6.7204e-06],
        [-2.2113e-05, -1.5557e-05,  1.1817e-05,  ..., -1.9491e-05,
         -1.1310e-05, -1.3873e-05]], device='cuda:0')
Loss: 1.0775412321090698


Running epoch 0, step 564, batch 564
Sampled inputs[:2]: tensor([[    0,   809,   367,  ...,   717,   287,  1548],
        [    0,   729,  3430,  ...,  9715,    13, 42383]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3669e-04,  3.4769e-04, -1.3386e-04,  ...,  1.4493e-04,
         -1.8096e-04,  1.7031e-04],
        [-1.1086e-05, -7.8231e-06,  5.9158e-06,  ..., -9.8050e-06,
         -5.9605e-06, -7.1079e-06],
        [ 5.1598e-05,  5.3107e-05, -3.8914e-05,  ...,  6.1088e-05,
          8.0958e-07,  5.5619e-05],
        [-1.3411e-05, -9.4250e-06,  7.1526e-06,  ..., -1.1846e-05,
         -6.6608e-06, -8.4564e-06],
        [-2.7627e-05, -1.9431e-05,  1.4767e-05,  ..., -2.4348e-05,
         -1.4096e-05, -1.7419e-05]], device='cuda:0')
Loss: 1.0615192651748657


Running epoch 0, step 565, batch 565
Sampled inputs[:2]: tensor([[   0,  396,  221,  ..., 1279,  720,  292],
        [   0,   12,  328,  ...,  908, 1086,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0437e-04,  3.6479e-04, -1.6452e-04,  ...,  1.4459e-04,
         -1.8888e-04,  2.2274e-04],
        [-1.3351e-05, -9.4026e-06,  7.1675e-06,  ..., -1.1787e-05,
         -7.1675e-06, -8.5682e-06],
        [ 4.8483e-05,  5.0931e-05, -3.7193e-05,  ...,  5.8376e-05,
         -7.5504e-07,  5.3637e-05],
        [-1.6108e-05, -1.1310e-05,  8.6427e-06,  ..., -1.4201e-05,
         -7.9870e-06, -1.0163e-05],
        [-3.3200e-05, -2.3305e-05,  1.7852e-05,  ..., -2.9206e-05,
         -1.6913e-05, -2.0936e-05]], device='cuda:0')
Loss: 1.0627830028533936


Running epoch 0, step 566, batch 566
Sampled inputs[:2]: tensor([[   0, 2377,  360,  ...,  266, 4745,  963],
        [   0,   14,   23,  ...,  278,  266, 1462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7210e-04,  2.8063e-04, -6.6644e-05,  ...,  4.7984e-05,
         -7.9024e-05,  2.0454e-04],
        [-1.5497e-05, -1.0930e-05,  8.3148e-06,  ..., -1.3694e-05,
         -8.0839e-06, -9.7901e-06],
        [ 4.5354e-05,  4.8696e-05, -3.5524e-05,  ...,  5.5589e-05,
         -2.0142e-06,  5.1871e-05],
        [-1.8850e-05, -1.3262e-05,  1.0103e-05,  ..., -1.6645e-05,
         -9.0599e-06, -1.1705e-05],
        [-3.8862e-05, -2.7329e-05,  2.0862e-05,  ..., -3.4243e-05,
         -1.9222e-05, -2.4110e-05]], device='cuda:0')
Loss: 1.097765564918518


Running epoch 0, step 567, batch 567
Sampled inputs[:2]: tensor([[    0,  4213,  1921,  ...,  1340,  1049,   292],
        [    0,   266, 27347,  ...,   368,  3367,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8541e-04,  6.7639e-05, -5.6527e-05,  ..., -6.4812e-05,
          1.9687e-05,  1.4688e-04],
        [ 5.7755e-05,  3.1046e-05, -4.1687e-05,  ...,  4.8558e-05,
          3.2447e-05,  3.5149e-05],
        [ 4.2270e-05,  4.6550e-05, -3.3832e-05,  ...,  5.2818e-05,
         -3.3255e-06,  5.0075e-05],
        [-2.1577e-05, -1.5162e-05,  1.1593e-05,  ..., -1.9103e-05,
         -1.0185e-05, -1.3284e-05],
        [-4.4346e-05, -3.1143e-05,  2.3872e-05,  ..., -3.9160e-05,
         -2.1592e-05, -2.7299e-05]], device='cuda:0')
Loss: 1.1064343452453613
Graident accumulation at epoch 0, step 567, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0097, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0163,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8696e-05,  8.0552e-05, -2.1128e-04,  ...,  5.5435e-05,
         -1.0569e-04, -1.1870e-05],
        [-9.2686e-06, -7.4973e-06,  1.6181e-06,  ..., -8.6936e-06,
         -1.7891e-06, -5.3585e-06],
        [ 2.1617e-05,  1.6651e-05, -7.3813e-06,  ...,  2.0723e-05,
          1.7943e-06,  1.3816e-05],
        [-1.4599e-05, -1.0458e-05,  6.3630e-06,  ..., -1.1613e-05,
         -6.0659e-06, -8.3473e-06],
        [-3.7626e-05, -2.4973e-05,  1.5915e-05,  ..., -3.2561e-05,
         -1.2703e-05, -1.9700e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1377e-08, 2.7661e-08, 3.5494e-08,  ..., 1.1470e-08, 8.5768e-08,
         1.1352e-08],
        [6.3410e-11, 3.2019e-11, 6.7222e-12,  ..., 4.0678e-11, 4.8810e-12,
         1.1933e-11],
        [1.1944e-09, 6.0824e-10, 1.0245e-10,  ..., 9.8014e-10, 6.6033e-11,
         3.1130e-10],
        [1.0269e-10, 4.8329e-11, 9.7424e-12,  ..., 8.5924e-11, 6.6436e-12,
         2.4522e-11],
        [2.7629e-10, 1.4699e-10, 1.9515e-11,  ..., 2.0050e-10, 2.0691e-11,
         6.6237e-11]], device='cuda:0')
optimizer state dict: 71.0
lr: [1.710551038758326e-05, 1.710551038758326e-05]
scheduler_last_epoch: 71


Running epoch 0, step 568, batch 568
Sampled inputs[:2]: tensor([[    0,   266,  3727,  ...,  1143,   271,  5213],
        [    0,   367,   925,  ..., 25491,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7310e-06, -1.3195e-04,  1.8707e-04,  ..., -7.9836e-05,
          1.6077e-04,  4.9232e-05],
        [-2.1905e-06, -1.5572e-06,  1.2144e-06,  ..., -2.0117e-06,
         -1.0133e-06, -1.3262e-06],
        [-3.0696e-06, -2.1905e-06,  1.7062e-06,  ..., -2.8312e-06,
         -1.3411e-06, -1.8403e-06],
        [ 7.0820e-05,  5.5526e-05, -5.0892e-05,  ...,  7.5275e-05,
          4.7735e-05,  5.2110e-05],
        [-5.6326e-06, -3.9935e-06,  3.1143e-06,  ..., -5.1856e-06,
         -2.4885e-06, -3.3677e-06]], device='cuda:0')
Loss: 1.1151267290115356


Running epoch 0, step 569, batch 569
Sampled inputs[:2]: tensor([[    0,  1336,   287,  ..., 15920,    12,   287],
        [    0,  6945,  2360,  ...,    30,   413,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5777e-05, -2.3442e-04,  2.6131e-04,  ..., -1.6485e-04,
          3.0631e-04,  1.0417e-04],
        [-4.3809e-06, -3.1516e-06,  2.4214e-06,  ..., -3.9935e-06,
         -2.0266e-06, -2.6599e-06],
        [-6.1393e-06, -4.4256e-06,  3.3975e-06,  ..., -5.6028e-06,
         -2.6673e-06, -3.6880e-06],
        [ 6.8167e-05,  5.3604e-05, -4.9432e-05,  ...,  7.2876e-05,
          4.6625e-05,  5.0515e-05],
        [-1.1295e-05, -8.1062e-06,  6.2436e-06,  ..., -1.0282e-05,
         -4.9770e-06, -6.7800e-06]], device='cuda:0')
Loss: 1.079920768737793


Running epoch 0, step 570, batch 570
Sampled inputs[:2]: tensor([[    0,   287,  4170,  ...,    27, 12612,    13],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2935e-04, -2.5357e-04,  2.9724e-04,  ..., -2.1229e-04,
          3.3002e-04,  2.0993e-04],
        [-6.5863e-06, -4.7684e-06,  3.6657e-06,  ..., -6.0350e-06,
         -3.1739e-06, -4.0904e-06],
        [-9.1791e-06, -6.6608e-06,  5.1185e-06,  ..., -8.4192e-06,
         -4.1574e-06, -5.6401e-06],
        [ 6.5544e-05,  5.1681e-05, -4.7949e-05,  ...,  7.0447e-05,
          4.5366e-05,  4.8832e-05],
        [-1.6749e-05, -1.2100e-05,  9.3281e-06,  ..., -1.5318e-05,
         -7.6741e-06, -1.0252e-05]], device='cuda:0')
Loss: 1.1007862091064453


Running epoch 0, step 571, batch 571
Sampled inputs[:2]: tensor([[    0,   721,  1119,  ...,   600,   328,  3363],
        [    0,   680,   271,  ..., 12942,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6782e-04, -4.8204e-04,  3.8004e-04,  ..., -4.1001e-04,
          5.5297e-04,  2.7894e-04],
        [-8.6427e-06, -6.3255e-06,  4.8429e-06,  ..., -7.9200e-06,
         -3.9339e-06, -5.2601e-06],
        [-1.2144e-05, -8.8960e-06,  6.8098e-06,  ..., -1.1131e-05,
         -5.1707e-06, -7.3016e-06],
        [ 6.2892e-05,  4.9685e-05, -4.6444e-05,  ...,  6.8018e-05,
          4.4494e-05,  4.7349e-05],
        [-2.2262e-05, -1.6272e-05,  1.2472e-05,  ..., -2.0355e-05,
         -9.6112e-06, -1.3351e-05]], device='cuda:0')
Loss: 1.118329644203186


Running epoch 0, step 572, batch 572
Sampled inputs[:2]: tensor([[   0,  365, 5911,  ...,  925,  408,  266],
        [   0, 1268,  278,  ...,  461,  925,  630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7252e-04, -4.8168e-04,  4.2110e-04,  ..., -4.6743e-04,
          6.9896e-04,  3.7147e-04],
        [-1.0774e-05, -7.8902e-06,  6.0201e-06,  ..., -9.9167e-06,
         -4.9770e-06, -6.5714e-06],
        [-1.5110e-05, -1.1072e-05,  8.4490e-06,  ..., -1.3903e-05,
         -6.5193e-06, -9.0972e-06],
        [ 6.0329e-05,  4.7807e-05, -4.5029e-05,  ...,  6.5619e-05,
          4.3354e-05,  4.5799e-05],
        [-2.7627e-05, -2.0176e-05,  1.5453e-05,  ..., -2.5362e-05,
         -1.2085e-05, -1.6585e-05]], device='cuda:0')
Loss: 1.0776480436325073


Running epoch 0, step 573, batch 573
Sampled inputs[:2]: tensor([[    0,  1431,   221,  ...,   756,   409,   275],
        [    0,  3504,     9,  ...,  7166, 10945,  3119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6033e-04, -5.9040e-04,  4.3652e-04,  ..., -5.1203e-04,
          6.7826e-04,  3.0896e-04],
        [-1.2815e-05, -9.4175e-06,  7.1973e-06,  ..., -1.1824e-05,
         -5.8487e-06, -7.8455e-06],
        [-1.8060e-05, -1.3277e-05,  1.0148e-05,  ..., -1.6659e-05,
         -7.6964e-06, -1.0923e-05],
        [ 5.7751e-05,  4.5870e-05, -4.3539e-05,  ...,  6.3205e-05,
          4.2356e-05,  4.4197e-05],
        [-3.2902e-05, -2.4110e-05,  1.8492e-05,  ..., -3.0249e-05,
         -1.4216e-05, -1.9833e-05]], device='cuda:0')
Loss: 1.0801275968551636


Running epoch 0, step 574, batch 574
Sampled inputs[:2]: tensor([[   0, 9657,  300,  ...,   12,  271,  266],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6090e-04, -6.1589e-04,  4.5457e-04,  ..., -5.1111e-04,
          6.7826e-04,  3.7481e-04],
        [-1.5020e-05, -1.1019e-05,  8.4192e-06,  ..., -1.3851e-05,
         -6.9812e-06, -9.2834e-06],
        [-2.1055e-05, -1.5467e-05,  1.1817e-05,  ..., -1.9431e-05,
         -9.1270e-06, -1.2860e-05],
        [ 5.5143e-05,  4.3977e-05, -4.2093e-05,  ...,  6.0791e-05,
          4.1149e-05,  4.2528e-05],
        [-3.8385e-05, -2.8104e-05,  2.1547e-05,  ..., -3.5286e-05,
         -1.6868e-05, -2.3350e-05]], device='cuda:0')
Loss: 1.0999730825424194


Running epoch 0, step 575, batch 575
Sampled inputs[:2]: tensor([[    0, 24781,   287,  ...,   266,  3873,  1400],
        [    0, 13466,    14,  ..., 11227,  1966,  4039]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0407e-04, -6.1275e-04,  5.0159e-04,  ..., -5.4145e-04,
          7.7695e-04,  4.8174e-04],
        [-1.7136e-05, -1.2644e-05,  9.6783e-06,  ..., -1.5832e-05,
         -8.0392e-06, -1.0662e-05],
        [-2.4036e-05, -1.7747e-05,  1.3575e-05,  ..., -2.2203e-05,
         -1.0505e-05, -1.4767e-05],
        [ 5.2580e-05,  4.2025e-05, -4.0581e-05,  ...,  5.8407e-05,
          3.9994e-05,  4.0889e-05],
        [-4.3750e-05, -3.2187e-05,  2.4736e-05,  ..., -4.0263e-05,
         -1.9401e-05, -2.6792e-05]], device='cuda:0')
Loss: 1.0823259353637695
Graident accumulation at epoch 0, step 575, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.3580e-05,  1.1222e-05, -1.3999e-04,  ..., -4.2534e-06,
         -1.7431e-05,  3.7491e-05],
        [-1.0055e-05, -8.0120e-06,  2.4241e-06,  ..., -9.4075e-06,
         -2.4141e-06, -5.8889e-06],
        [ 1.7051e-05,  1.3211e-05, -5.2857e-06,  ...,  1.6430e-05,
          5.6434e-07,  1.0957e-05],
        [-7.8812e-06, -5.2095e-06,  1.6686e-06,  ..., -4.6114e-06,
         -1.4599e-06, -3.4236e-06],
        [-3.8238e-05, -2.5695e-05,  1.6797e-05,  ..., -3.3332e-05,
         -1.3373e-05, -2.0409e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1499e-08, 2.8008e-08, 3.5710e-08,  ..., 1.1752e-08, 8.6286e-08,
         1.1573e-08],
        [6.3641e-11, 3.2147e-11, 6.8092e-12,  ..., 4.0888e-11, 4.9407e-12,
         1.2035e-11],
        [1.1938e-09, 6.0794e-10, 1.0253e-10,  ..., 9.7965e-10, 6.6078e-11,
         3.1120e-10],
        [1.0535e-10, 5.0047e-11, 1.1379e-11,  ..., 8.9249e-11, 8.2365e-12,
         2.6169e-11],
        [2.7793e-10, 1.4788e-10, 2.0107e-11,  ..., 2.0192e-10, 2.1047e-11,
         6.6888e-11]], device='cuda:0')
optimizer state dict: 72.0
lr: [1.7017987415646643e-05, 1.7017987415646643e-05]
scheduler_last_epoch: 72


Running epoch 0, step 576, batch 576
Sampled inputs[:2]: tensor([[   0, 1781,  659,  ...,   12, 1478,   14],
        [   0,  395, 4973,  ..., 5851,  409, 4370]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0684e-05, -3.8409e-05,  8.1558e-05,  ...,  2.7537e-05,
          1.4751e-05,  6.3293e-05],
        [-2.2650e-06, -1.6689e-06,  1.2964e-06,  ..., -2.0713e-06,
         -1.0580e-06, -1.3933e-06],
        [-3.0994e-06, -2.2799e-06,  1.7732e-06,  ..., -2.8163e-06,
         -1.3486e-06, -1.8775e-06],
        [-2.6673e-06, -1.9670e-06,  1.5274e-06,  ..., -2.4289e-06,
         -1.1250e-06, -1.6093e-06],
        [-5.6028e-06, -4.1425e-06,  3.2336e-06,  ..., -5.1260e-06,
         -2.4736e-06, -3.3975e-06]], device='cuda:0')
Loss: 1.0924012660980225


Running epoch 0, step 577, batch 577
Sampled inputs[:2]: tensor([[    0,    21,   292,  ...,    13,  1861,  4254],
        [    0, 25228,  1168,  ...,  2728,    27,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6713e-04,  2.0242e-04,  1.2383e-04,  ...,  1.1885e-04,
          8.3623e-05,  1.8780e-04],
        [-4.7237e-06, -3.4645e-06,  2.5928e-06,  ..., -4.2766e-06,
         -2.5854e-06, -3.1143e-06],
        [-6.1542e-06, -4.5002e-06,  3.3975e-06,  ..., -5.5432e-06,
         -3.1367e-06, -3.9786e-06],
        [-5.2005e-06, -3.7998e-06,  2.8685e-06,  ..., -4.6939e-06,
         -2.5779e-06, -3.3453e-06],
        [-1.1235e-05, -8.2254e-06,  6.2585e-06,  ..., -1.0163e-05,
         -5.7966e-06, -7.2718e-06]], device='cuda:0')
Loss: 1.06913423538208


Running epoch 0, step 578, batch 578
Sampled inputs[:2]: tensor([[    0, 48705,   292,  ...,   266,  2548,  2697],
        [    0,   437, 11670,  ...,   381, 11996,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0976e-04,  1.8042e-04,  1.0101e-04,  ...,  9.4224e-05,
          9.4334e-05,  2.6943e-04],
        [-7.0482e-06, -5.1484e-06,  3.8892e-06,  ..., -6.3926e-06,
         -3.6433e-06, -4.5747e-06],
        [-9.2834e-06, -6.7651e-06,  5.1409e-06,  ..., -8.3894e-06,
         -4.4778e-06, -5.9307e-06],
        [-7.8529e-06, -5.7220e-06,  4.3511e-06,  ..., -7.1079e-06,
         -3.6806e-06, -4.9993e-06],
        [-1.6958e-05, -1.2368e-05,  9.4622e-06,  ..., -1.5348e-05,
         -8.2701e-06, -1.0833e-05]], device='cuda:0')
Loss: 1.0694602727890015


Running epoch 0, step 579, batch 579
Sampled inputs[:2]: tensor([[   0, 2162,   73,  ...,  278,  266, 1059],
        [   0,  879,   27,  ..., 3958, 2875,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7638e-04,  1.9332e-04,  2.0215e-04,  ...,  6.8515e-05,
          7.4815e-05,  3.9688e-04],
        [-9.4175e-06, -6.8843e-06,  5.1335e-06,  ..., -8.5086e-06,
         -4.6492e-06, -5.9530e-06],
        [-1.2517e-05, -9.1493e-06,  6.8471e-06,  ..., -1.1295e-05,
         -5.7667e-06, -7.7933e-06],
        [-1.0580e-05, -7.7337e-06,  5.7966e-06,  ..., -9.5665e-06,
         -4.7386e-06, -6.5714e-06],
        [-2.2858e-05, -1.6719e-05,  1.2591e-05,  ..., -2.0623e-05,
         -1.0654e-05, -1.4231e-05]], device='cuda:0')
Loss: 1.1001605987548828


Running epoch 0, step 580, batch 580
Sampled inputs[:2]: tensor([[   0, 6762,  689,  ..., 7061,   14,  381],
        [   0,  278, 5798,  ...,  266,  729, 1798]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9941e-04,  1.8813e-04,  2.3166e-04,  ...,  7.7550e-05,
          1.2499e-04,  4.5619e-04],
        [-1.1578e-05, -8.5384e-06,  6.3479e-06,  ..., -1.0550e-05,
         -5.5693e-06, -7.3537e-06],
        [-1.5408e-05, -1.1370e-05,  8.4862e-06,  ..., -1.4022e-05,
         -6.8918e-06, -9.6411e-06],
        [-1.3143e-05, -9.6858e-06,  7.2494e-06,  ..., -1.1981e-05,
         -5.7071e-06, -8.2105e-06],
        [-2.8133e-05, -2.0742e-05,  1.5587e-05,  ..., -2.5600e-05,
         -1.2755e-05, -1.7598e-05]], device='cuda:0')
Loss: 1.0567032098770142


Running epoch 0, step 581, batch 581
Sampled inputs[:2]: tensor([[   0,  259, 5112,  ..., 3520,  278,  298],
        [   0,  292,  960,  ...,  271, 1356,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8439e-04,  2.3507e-04,  2.5623e-04,  ...,  7.2655e-05,
          1.3809e-04,  4.0174e-04],
        [-1.3873e-05, -1.0200e-05,  7.6145e-06,  ..., -1.2666e-05,
         -6.6496e-06, -8.8215e-06],
        [-1.8448e-05, -1.3575e-05,  1.0177e-05,  ..., -1.6823e-05,
         -8.2180e-06, -1.1563e-05],
        [-1.5765e-05, -1.1586e-05,  8.7097e-06,  ..., -1.4395e-05,
         -6.8247e-06, -9.8720e-06],
        [-3.3706e-05, -2.4766e-05,  1.8701e-05,  ..., -3.0726e-05,
         -1.5214e-05, -2.1115e-05]], device='cuda:0')
Loss: 1.0746757984161377


Running epoch 0, step 582, batch 582
Sampled inputs[:2]: tensor([[   0, 2383, 9843,  ...,  401, 3959,  300],
        [   0, 1531,   14,  ..., 6169,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0902e-04,  2.1373e-04,  3.3936e-04,  ...,  1.0726e-04,
          2.2474e-04,  5.1144e-04],
        [-1.6108e-05, -1.1861e-05,  8.8140e-06,  ..., -1.4722e-05,
         -7.5139e-06, -1.0125e-05],
        [-2.1517e-05, -1.5870e-05,  1.1839e-05,  ..., -1.9655e-05,
         -9.3207e-06, -1.3344e-05],
        [-1.8433e-05, -1.3568e-05,  1.0148e-05,  ..., -1.6853e-05,
         -7.7449e-06, -1.1414e-05],
        [-3.9488e-05, -2.9057e-05,  2.1830e-05,  ..., -3.6031e-05,
         -1.7330e-05, -2.4453e-05]], device='cuda:0')
Loss: 1.080575704574585


Running epoch 0, step 583, batch 583
Sampled inputs[:2]: tensor([[    0,   292, 23950,  ...,  9305,   287,  4401],
        [    0,  1555,    12,  ...,   809,   287,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0902e-04,  9.8198e-05,  4.0798e-04,  ...,  4.6645e-05,
          4.2812e-04,  6.1883e-04],
        [-1.8477e-05, -1.3649e-05,  1.0110e-05,  ..., -1.6809e-05,
         -8.5272e-06, -1.1541e-05],
        [-2.4736e-05, -1.8299e-05,  1.3597e-05,  ..., -2.2486e-05,
         -1.0617e-05, -1.5251e-05],
        [-2.1204e-05, -1.5654e-05,  1.1660e-05,  ..., -1.9297e-05,
         -8.8327e-06, -1.3053e-05],
        [-4.5508e-05, -3.3587e-05,  2.5123e-05,  ..., -4.1336e-05,
         -1.9789e-05, -2.7999e-05]], device='cuda:0')
Loss: 1.1064164638519287
Graident accumulation at epoch 0, step 583, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.2124e-05,  1.9920e-05, -8.5195e-05,  ...,  8.3644e-07,
          2.7124e-05,  9.5624e-05],
        [-1.0898e-05, -8.5757e-06,  3.1928e-06,  ..., -1.0148e-05,
         -3.0255e-06, -6.4541e-06],
        [ 1.2873e-05,  1.0060e-05, -3.3974e-06,  ...,  1.2539e-05,
         -5.5380e-07,  8.3366e-06],
        [-9.2136e-06, -6.2540e-06,  2.6678e-06,  ..., -6.0799e-06,
         -2.1972e-06, -4.3866e-06],
        [-3.8965e-05, -2.6484e-05,  1.7630e-05,  ..., -3.4132e-05,
         -1.4014e-05, -2.1168e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1501e-08, 2.7990e-08, 3.5841e-08,  ..., 1.1742e-08, 8.6383e-08,
         1.1944e-08],
        [6.3918e-11, 3.2301e-11, 6.9046e-12,  ..., 4.1130e-11, 5.0085e-12,
         1.2156e-11],
        [1.1932e-09, 6.0767e-10, 1.0262e-10,  ..., 9.7918e-10, 6.6124e-11,
         3.1112e-10],
        [1.0570e-10, 5.0242e-11, 1.1504e-11,  ..., 8.9532e-11, 8.3063e-12,
         2.6314e-11],
        [2.7972e-10, 1.4886e-10, 2.0718e-11,  ..., 2.0343e-10, 2.1418e-11,
         6.7605e-11]], device='cuda:0')
optimizer state dict: 73.0
lr: [1.6929392033972038e-05, 1.6929392033972038e-05]
scheduler_last_epoch: 73


Running epoch 0, step 584, batch 584
Sampled inputs[:2]: tensor([[   0, 4494,   12,  ...,  341, 1619,   12],
        [   0, 4994, 8429,  ...,   12,  795,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4577e-04,  1.4238e-04, -1.8845e-04,  ...,  1.1918e-04,
         -3.1500e-05, -2.5627e-05],
        [-2.3991e-06, -1.8254e-06,  1.3560e-06,  ..., -2.2799e-06,
         -1.2666e-06, -1.5795e-06],
        [-3.0249e-06, -2.2948e-06,  1.7136e-06,  ..., -2.8461e-06,
         -1.4603e-06, -1.9521e-06],
        [-2.6226e-06, -1.9819e-06,  1.4901e-06,  ..., -2.4885e-06,
         -1.2442e-06, -1.6987e-06],
        [-5.6326e-06, -4.2319e-06,  3.1888e-06,  ..., -5.2750e-06,
         -2.7269e-06, -3.6210e-06]], device='cuda:0')
Loss: 1.0831012725830078


Running epoch 0, step 585, batch 585
Sampled inputs[:2]: tensor([[    0,  5332,   266,  ...,   300,   259, 15369],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0902e-04,  3.0867e-05, -1.7943e-04,  ...,  9.0462e-05,
          2.2850e-05, -6.0827e-05],
        [-4.8578e-06, -3.6955e-06,  2.6599e-06,  ..., -4.4107e-06,
         -2.3395e-06, -3.0324e-06],
        [-6.3032e-06, -4.7833e-06,  3.4496e-06,  ..., -5.6773e-06,
         -2.8089e-06, -3.8743e-06],
        [-5.3346e-06, -4.0531e-06,  2.9281e-06,  ..., -4.8429e-06,
         -2.3320e-06, -3.2932e-06],
        [-1.1712e-05, -8.8513e-06,  6.4224e-06,  ..., -1.0520e-05,
         -5.2601e-06, -7.1675e-06]], device='cuda:0')
Loss: 1.087874412536621


Running epoch 0, step 586, batch 586
Sampled inputs[:2]: tensor([[   0,  292,   17,  ...,  265, 6943,   14],
        [   0,  634, 1621,  ...,  688,  586, 8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0073e-05,  8.1099e-05, -1.0529e-04,  ...,  1.1913e-04,
         -3.3255e-05,  5.8125e-05],
        [-7.3761e-06, -5.5730e-06,  3.9786e-06,  ..., -6.6906e-06,
         -3.5688e-06, -4.6045e-06],
        [-9.5516e-06, -7.1973e-06,  5.1558e-06,  ..., -8.6129e-06,
         -4.3064e-06, -5.8711e-06],
        [-8.0317e-06, -6.0648e-06,  4.3437e-06,  ..., -7.2867e-06,
         -3.5614e-06, -4.9546e-06],
        [-1.7732e-05, -1.3322e-05,  9.5814e-06,  ..., -1.5944e-05,
         -8.0615e-06, -1.0863e-05]], device='cuda:0')
Loss: 1.1029253005981445


Running epoch 0, step 587, batch 587
Sampled inputs[:2]: tensor([[    0,   221,   451,  ...,   741, 25712,   950],
        [    0,   266, 15324,  ...,   943,  1613,  7178]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.3766e-05,  9.5137e-05, -2.0480e-04,  ...,  2.4264e-04,
         -1.5121e-04, -3.6354e-07],
        [-9.8795e-06, -7.4059e-06,  5.2974e-06,  ..., -9.0450e-06,
         -5.0217e-06, -6.3851e-06],
        [-1.2666e-05, -9.4771e-06,  6.8247e-06,  ..., -1.1519e-05,
         -5.9605e-06, -8.0317e-06],
        [-1.0669e-05, -7.9870e-06,  5.7518e-06,  ..., -9.7454e-06,
         -4.9323e-06, -6.7800e-06],
        [-2.3395e-05, -1.7434e-05,  1.2621e-05,  ..., -2.1189e-05,
         -1.1057e-05, -1.4737e-05]], device='cuda:0')
Loss: 1.0845791101455688


Running epoch 0, step 588, batch 588
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0,    15,    83,  ...,  6030,    14, 14080]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9785e-05,  7.3661e-05, -2.2477e-04,  ...,  2.6538e-04,
         -1.7498e-04,  1.6969e-05],
        [-1.2368e-05, -9.2834e-06,  6.6310e-06,  ..., -1.1250e-05,
         -6.1542e-06, -7.8604e-06],
        [-1.5959e-05, -1.1981e-05,  8.5980e-06,  ..., -1.4439e-05,
         -7.3761e-06, -9.9689e-06],
        [-1.3426e-05, -1.0058e-05,  7.2271e-06,  ..., -1.2189e-05,
         -6.0871e-06, -8.3894e-06],
        [-2.9504e-05, -2.2054e-05,  1.5900e-05,  ..., -2.6584e-05,
         -1.3709e-05, -1.8299e-05]], device='cuda:0')
Loss: 1.1211001873016357


Running epoch 0, step 589, batch 589
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ..., 3988, 1049,  935],
        [   0, 2296,  446,  ..., 2937,  287, 2795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9137e-06,  8.4443e-05, -1.5923e-04,  ...,  2.5068e-04,
         -1.9165e-04, -3.8490e-05],
        [-1.4767e-05, -1.1124e-05,  7.9647e-06,  ..., -1.3381e-05,
         -7.2420e-06, -9.3356e-06],
        [-1.9148e-05, -1.4439e-05,  1.0386e-05,  ..., -1.7270e-05,
         -8.7470e-06, -1.1921e-05],
        [-1.6078e-05, -1.2085e-05,  8.7023e-06,  ..., -1.4544e-05,
         -7.1973e-06, -1.0006e-05],
        [-3.5375e-05, -2.6554e-05,  1.9193e-05,  ..., -3.1799e-05,
         -1.6257e-05, -2.1890e-05]], device='cuda:0')
Loss: 1.0852432250976562


Running epoch 0, step 590, batch 590
Sampled inputs[:2]: tensor([[    0,   221,   380,  ..., 10022,    12,   461],
        [    0, 33119,   391,  ...,   292,  4462,  2721]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6811e-05,  5.5403e-05, -1.9768e-04,  ...,  3.1326e-04,
         -1.4175e-04, -3.5813e-05],
        [-1.7151e-05, -1.2964e-05,  9.2462e-06,  ..., -1.5542e-05,
         -8.1845e-06, -1.0751e-05],
        [-2.2382e-05, -1.6943e-05,  1.2130e-05,  ..., -2.0206e-05,
         -9.9391e-06, -1.3828e-05],
        [-1.8805e-05, -1.4186e-05,  1.0163e-05,  ..., -1.7017e-05,
         -8.1733e-06, -1.1608e-05],
        [-4.1306e-05, -3.1114e-05,  2.2382e-05,  ..., -3.7163e-05,
         -1.8477e-05, -2.5362e-05]], device='cuda:0')
Loss: 1.0793166160583496


Running epoch 0, step 591, batch 591
Sampled inputs[:2]: tensor([[    0,   199,   769,  ...,   685,  1423,    13],
        [    0,   266, 20604,  ...,   409, 13764,  6048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1904e-05, -7.5043e-05, -1.9095e-04,  ...,  2.4959e-04,
         -2.7009e-05, -1.1242e-04],
        [-1.9550e-05, -1.4871e-05,  1.0550e-05,  ..., -1.7777e-05,
         -9.2424e-06, -1.2204e-05],
        [-2.5526e-05, -1.9431e-05,  1.3851e-05,  ..., -2.3142e-05,
         -1.1228e-05, -1.5713e-05],
        [-2.1487e-05, -1.6302e-05,  1.1623e-05,  ..., -1.9521e-05,
         -9.2462e-06, -1.3217e-05],
        [-4.7147e-05, -3.5733e-05,  2.5585e-05,  ..., -4.2588e-05,
         -2.0906e-05, -2.8864e-05]], device='cuda:0')
Loss: 1.1054105758666992
Graident accumulation at epoch 0, step 591, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0267,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0198],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0149, -0.0276,  ...,  0.0285, -0.0154, -0.0181]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5102e-05,  1.0423e-05, -9.5770e-05,  ...,  2.5712e-05,
          2.1711e-05,  7.4820e-05],
        [-1.1763e-05, -9.2053e-06,  3.9285e-06,  ..., -1.0911e-05,
         -3.6472e-06, -7.0291e-06],
        [ 9.0328e-06,  7.1109e-06, -1.6726e-06,  ...,  8.9707e-06,
         -1.6212e-06,  5.9316e-06],
        [-1.0441e-05, -7.2587e-06,  3.5633e-06,  ..., -7.4240e-06,
         -2.9021e-06, -5.2697e-06],
        [-3.9784e-05, -2.7409e-05,  1.8425e-05,  ..., -3.4978e-05,
         -1.4704e-05, -2.1938e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1464e-08, 2.7968e-08, 3.5842e-08,  ..., 1.1793e-08, 8.6298e-08,
         1.1945e-08],
        [6.4237e-11, 3.2490e-11, 7.0090e-12,  ..., 4.1405e-11, 5.0889e-12,
         1.2293e-11],
        [1.1926e-09, 6.0744e-10, 1.0270e-10,  ..., 9.7873e-10, 6.6184e-11,
         3.1106e-10],
        [1.0605e-10, 5.0457e-11, 1.1628e-11,  ..., 8.9824e-11, 8.3835e-12,
         2.6462e-11],
        [2.8167e-10, 1.4999e-10, 2.1352e-11,  ..., 2.0504e-10, 2.1833e-11,
         6.8371e-11]], device='cuda:0')
optimizer state dict: 74.0
lr: [1.6839737780707125e-05, 1.6839737780707125e-05]
scheduler_last_epoch: 74


Running epoch 0, step 592, batch 592
Sampled inputs[:2]: tensor([[   0,   13,  711,  ...,  591,  953,  352],
        [   0,   14,  381,  ..., 7106,  287,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9860e-05,  7.1167e-05, -2.7857e-05,  ...,  3.1719e-05,
         -9.3486e-05,  2.2863e-05],
        [-2.4736e-06, -1.9372e-06,  1.3262e-06,  ..., -2.2203e-06,
         -1.0803e-06, -1.5125e-06],
        [-3.2783e-06, -2.5630e-06,  1.7732e-06,  ..., -2.9355e-06,
         -1.3262e-06, -1.9819e-06],
        [-2.7269e-06, -2.1309e-06,  1.4752e-06,  ..., -2.4438e-06,
         -1.0803e-06, -1.6466e-06],
        [-6.1095e-06, -4.7684e-06,  3.3081e-06,  ..., -5.4836e-06,
         -2.5183e-06, -3.6806e-06]], device='cuda:0')
Loss: 1.0856707096099854


Running epoch 0, step 593, batch 593
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,   287, 15506, 14476],
        [    0,    13, 26335,  ...,     5,  2570, 34403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9768e-05,  7.4902e-05,  2.7720e-05,  ...,  1.2087e-05,
         -1.2755e-04,  3.5964e-05],
        [-5.1111e-06, -3.9190e-06,  2.6599e-06,  ..., -4.5300e-06,
         -2.3767e-06, -3.1292e-06],
        [-6.7055e-06, -5.1409e-06,  3.5092e-06,  ..., -5.9307e-06,
         -2.9281e-06, -4.0680e-06],
        [-5.4240e-06, -4.1574e-06,  2.8387e-06,  ..., -4.7982e-06,
         -2.3171e-06, -3.2932e-06],
        [-1.2428e-05, -9.5069e-06,  6.5118e-06,  ..., -1.0997e-05,
         -5.4836e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.1014593839645386


Running epoch 0, step 594, batch 594
Sampled inputs[:2]: tensor([[    0, 10026,   992,  ...,   273,  2831,  8716],
        [    0,  1032,   287,  ...,   266, 33161,  4728]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9353e-05,  2.3411e-04, -7.7949e-05,  ...,  1.2186e-04,
         -3.0911e-04,  4.6060e-06],
        [-7.7039e-06, -5.9009e-06,  4.0531e-06,  ..., -6.9141e-06,
         -3.7923e-06, -4.8429e-06],
        [-9.9689e-06, -7.6145e-06,  5.2750e-06,  ..., -8.9258e-06,
         -4.5896e-06, -6.1840e-06],
        [-8.0317e-06, -6.1393e-06,  4.2468e-06,  ..., -7.1973e-06,
         -3.6284e-06, -4.9919e-06],
        [-1.8358e-05, -1.4007e-05,  9.7305e-06,  ..., -1.6421e-05,
         -8.5235e-06, -1.1340e-05]], device='cuda:0')
Loss: 1.0717788934707642


Running epoch 0, step 595, batch 595
Sampled inputs[:2]: tensor([[    0,  2278,   292,  ..., 12060,  1319,   292],
        [    0,   199, 14973,  ...,   638,  1119,  1329]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7647e-05,  1.6742e-04, -1.6803e-04,  ...,  1.1802e-04,
         -3.4614e-04, -6.9035e-05],
        [-1.0267e-05, -7.9274e-06,  5.4389e-06,  ..., -9.1791e-06,
         -4.9770e-06, -6.4746e-06],
        [-1.3322e-05, -1.0252e-05,  7.0930e-06,  ..., -1.1891e-05,
         -6.0424e-06, -8.2999e-06],
        [-1.0759e-05, -8.2850e-06,  5.7220e-06,  ..., -9.6112e-06,
         -4.7833e-06, -6.7130e-06],
        [-2.4498e-05, -1.8835e-05,  1.3068e-05,  ..., -2.1845e-05,
         -1.1235e-05, -1.5214e-05]], device='cuda:0')
Loss: 1.0734748840332031


Running epoch 0, step 596, batch 596
Sampled inputs[:2]: tensor([[   0,  806, 1255,  ...,  474,  221,  380],
        [   0,  461, 4182,  ..., 7461,  292, 4895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7988e-05,  3.1272e-04, -2.1640e-04,  ...,  1.4296e-04,
         -4.0167e-04, -3.2321e-05],
        [-1.2830e-05, -9.8944e-06,  6.7726e-06,  ..., -1.1504e-05,
         -6.3032e-06, -8.0764e-06],
        [-1.6674e-05, -1.2815e-05,  8.8438e-06,  ..., -1.4916e-05,
         -7.6666e-06, -1.0356e-05],
        [-1.3486e-05, -1.0371e-05,  7.1451e-06,  ..., -1.2070e-05,
         -6.0722e-06, -8.3819e-06],
        [-3.0726e-05, -2.3574e-05,  1.6332e-05,  ..., -2.7448e-05,
         -1.4275e-05, -1.9029e-05]], device='cuda:0')
Loss: 1.090121865272522


Running epoch 0, step 597, batch 597
Sampled inputs[:2]: tensor([[    0, 38460,     9,  ...,   829,   870,    12],
        [    0,  1526,   422,  ..., 22454,   409, 31482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1647e-05,  3.5487e-04, -2.4174e-04,  ...,  1.6112e-04,
         -4.2079e-04, -1.1251e-05],
        [-1.5467e-05, -1.1891e-05,  8.1360e-06,  ..., -1.3888e-05,
         -7.7933e-06, -9.8199e-06],
        [-2.0012e-05, -1.5333e-05,  1.0572e-05,  ..., -1.7941e-05,
         -9.4548e-06, -1.2547e-05],
        [-1.6138e-05, -1.2368e-05,  8.5086e-06,  ..., -1.4454e-05,
         -7.4580e-06, -1.0110e-05],
        [-3.6836e-05, -2.8163e-05,  1.9491e-05,  ..., -3.2961e-05,
         -1.7554e-05, -2.2992e-05]], device='cuda:0')
Loss: 1.1046825647354126


Running epoch 0, step 598, batch 598
Sampled inputs[:2]: tensor([[   0, 2085,   12,  ...,  496,   14,  747],
        [   0,  292, 1820,  ...,  591, 6619, 1607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6926e-04,  4.1151e-04, -2.3290e-04,  ...,  1.8322e-04,
         -4.6838e-04,  1.2708e-05],
        [-1.8045e-05, -1.3843e-05,  9.4920e-06,  ..., -1.6212e-05,
         -9.1046e-06, -1.1414e-05],
        [-2.3276e-05, -1.7807e-05,  1.2308e-05,  ..., -2.0877e-05,
         -1.1012e-05, -1.4544e-05],
        [-1.8820e-05, -1.4395e-05,  9.9316e-06,  ..., -1.6883e-05,
         -8.7097e-06, -1.1757e-05],
        [-4.2975e-05, -3.2783e-05,  2.2754e-05,  ..., -3.8475e-05,
         -2.0489e-05, -2.6733e-05]], device='cuda:0')
Loss: 1.096868872642517


Running epoch 0, step 599, batch 599
Sampled inputs[:2]: tensor([[    0,  2165,  9311,  ..., 10570,   437,   266],
        [    0,   747,  7890,  ...,   706,  8667,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0783e-04,  4.9343e-04, -3.7186e-04,  ...,  3.0797e-04,
         -5.7929e-04,  3.0515e-05],
        [-2.0668e-05, -1.5840e-05,  1.0870e-05,  ..., -1.8552e-05,
         -1.0498e-05, -1.3143e-05],
        [-2.6658e-05, -2.0385e-05,  1.4096e-05,  ..., -2.3887e-05,
         -1.2703e-05, -1.6734e-05],
        [-2.1547e-05, -1.6466e-05,  1.1370e-05,  ..., -1.9312e-05,
         -1.0051e-05, -1.3523e-05],
        [-4.9144e-05, -3.7462e-05,  2.6017e-05,  ..., -4.3958e-05,
         -2.3589e-05, -3.0726e-05]], device='cuda:0')
Loss: 1.0728667974472046
Graident accumulation at epoch 0, step 599, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0071, -0.0011],
        [-0.0162,  0.0149, -0.0277,  ...,  0.0285, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.9809e-05,  5.8724e-05, -1.2338e-04,  ...,  5.3938e-05,
         -3.8389e-05,  7.0390e-05],
        [-1.2653e-05, -9.8687e-06,  4.6227e-06,  ..., -1.1675e-05,
         -4.3322e-06, -7.6404e-06],
        [ 5.4637e-06,  4.3613e-06, -9.5671e-08,  ...,  5.6850e-06,
         -2.7294e-06,  3.6650e-06],
        [-1.1552e-05, -8.1795e-06,  4.3439e-06,  ..., -8.6128e-06,
         -3.6170e-06, -6.0950e-06],
        [-4.0720e-05, -2.8414e-05,  1.9184e-05,  ..., -3.5876e-05,
         -1.5592e-05, -2.2817e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1466e-08, 2.8183e-08, 3.5944e-08,  ..., 1.1876e-08, 8.6547e-08,
         1.1934e-08],
        [6.4600e-11, 3.2708e-11, 7.1201e-12,  ..., 4.1708e-11, 5.1940e-12,
         1.2454e-11],
        [1.1922e-09, 6.0725e-10, 1.0280e-10,  ..., 9.7833e-10, 6.6279e-11,
         3.1103e-10],
        [1.0641e-10, 5.0678e-11, 1.1745e-11,  ..., 9.0107e-11, 8.4761e-12,
         2.6618e-11],
        [2.8380e-10, 1.5124e-10, 2.2008e-11,  ..., 2.0677e-10, 2.2368e-11,
         6.9246e-11]], device='cuda:0')
optimizer state dict: 75.0
lr: [1.67490383558044e-05, 1.67490383558044e-05]
scheduler_last_epoch: 75


Running epoch 0, step 600, batch 600
Sampled inputs[:2]: tensor([[    0,  5007,  7551,  ...,     9,  2095,   300],
        [    0,  1253,  3197,  ...,   271,   266, 27896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0807e-04,  7.5289e-05, -2.3047e-04,  ...,  6.3879e-05,
         -9.7791e-05, -1.6159e-04],
        [-2.5779e-06, -2.0266e-06,  1.4156e-06,  ..., -2.3395e-06,
         -1.2368e-06, -1.7211e-06],
        [-3.3528e-06, -2.6226e-06,  1.8626e-06,  ..., -3.0249e-06,
         -1.4827e-06, -2.2054e-06],
        [-2.7120e-06, -2.1309e-06,  1.5050e-06,  ..., -2.4587e-06,
         -1.1772e-06, -1.7956e-06],
        [-6.1393e-06, -4.7982e-06,  3.4273e-06,  ..., -5.5432e-06,
         -2.7418e-06, -4.0233e-06]], device='cuda:0')
Loss: 1.0852608680725098


Running epoch 0, step 601, batch 601
Sampled inputs[:2]: tensor([[   0,  422,   14,  ...,  271, 1360,   12],
        [   0,  365, 2849,  ...,    9, 3365, 5027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9478e-05,  1.5735e-04, -2.7687e-04,  ...,  5.7791e-05,
         -6.7336e-05, -7.8565e-05],
        [-5.2303e-06, -4.0382e-06,  2.7791e-06,  ..., -4.8131e-06,
         -2.8685e-06, -3.6433e-06],
        [-6.7055e-06, -5.1558e-06,  3.6135e-06,  ..., -6.1244e-06,
         -3.4049e-06, -4.5747e-06],
        [-5.3048e-06, -4.0829e-06,  2.8461e-06,  ..., -4.8578e-06,
         -2.6524e-06, -3.6284e-06],
        [-1.2279e-05, -9.4175e-06,  6.6459e-06,  ..., -1.1206e-05,
         -6.2734e-06, -8.3447e-06]], device='cuda:0')
Loss: 1.1023789644241333


Running epoch 0, step 602, batch 602
Sampled inputs[:2]: tensor([[   0, 2736, 2523,  ..., 4086, 4798, 7701],
        [   0,   19,   14,  ...,  278, 2588,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1817e-04,  1.0360e-04, -2.9321e-04,  ...,  8.2511e-05,
         -5.4656e-05, -7.3845e-05],
        [-7.8827e-06, -6.0648e-06,  4.1202e-06,  ..., -7.1824e-06,
         -4.1351e-06, -5.3495e-06],
        [-1.0237e-05, -7.8529e-06,  5.4017e-06,  ..., -9.2685e-06,
         -5.0068e-06, -6.8247e-06],
        [ 2.2389e-04,  1.5496e-04, -2.0701e-04,  ...,  2.3750e-04,
          1.3145e-04,  1.1888e-04],
        [-1.8775e-05, -1.4395e-05,  9.9540e-06,  ..., -1.6987e-05,
         -9.2536e-06, -1.2487e-05]], device='cuda:0')
Loss: 1.1147379875183105


Running epoch 0, step 603, batch 603
Sampled inputs[:2]: tensor([[    0, 31318,    14,  ...,  1682,  1501,  1548],
        [    0,   445,     8,  ...,    13, 25386,    17]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3266e-04,  8.8609e-05, -2.9043e-04,  ...,  1.5585e-04,
         -2.4345e-04, -1.9356e-04],
        [-1.0476e-05, -8.0913e-06,  5.4836e-06,  ..., -9.5069e-06,
         -5.4017e-06, -7.1079e-06],
        [-1.3694e-05, -1.0565e-05,  7.2420e-06,  ..., -1.2383e-05,
         -6.5938e-06, -9.1493e-06],
        [ 2.2121e-04,  1.5287e-04, -2.0559e-04,  ...,  2.3509e-04,
          1.3025e-04,  1.1709e-04],
        [-2.5064e-05, -1.9312e-05,  1.3307e-05,  ..., -2.2650e-05,
         -1.2159e-05, -1.6689e-05]], device='cuda:0')
Loss: 1.0897377729415894


Running epoch 0, step 604, batch 604
Sampled inputs[:2]: tensor([[    0,  6904,  6069,  ..., 17196,   471,   221],
        [    0,   259, 19567,  ...,   266,  3899,  2123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4707e-04,  2.0435e-04, -2.8070e-04,  ...,  2.8680e-04,
         -3.4635e-04, -2.1409e-04],
        [-1.3053e-05, -1.0103e-05,  6.8098e-06,  ..., -1.1876e-05,
         -6.6683e-06, -8.8364e-06],
        [-1.7107e-05, -1.3232e-05,  9.0152e-06,  ..., -1.5512e-05,
         -8.1658e-06, -1.1414e-05],
        [ 2.1850e-04,  1.5077e-04, -2.0419e-04,  ...,  2.3262e-04,
          1.2902e-04,  1.1529e-04],
        [-3.1322e-05, -2.4199e-05,  1.6570e-05,  ..., -2.8372e-05,
         -1.5065e-05, -2.0832e-05]], device='cuda:0')
Loss: 1.0919114351272583


Running epoch 0, step 605, batch 605
Sampled inputs[:2]: tensor([[   0, 2440,  709,  ..., 4505, 1549, 4111],
        [   0,   12, 5820,  ...,  221,  380,  560]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6195e-04,  3.8886e-04, -3.0444e-04,  ...,  3.6670e-04,
         -4.0828e-04, -2.2521e-04],
        [-1.5616e-05, -1.2055e-05,  8.1062e-06,  ..., -1.4275e-05,
         -8.2776e-06, -1.0774e-05],
        [-2.0310e-05, -1.5661e-05,  1.0662e-05,  ..., -1.8492e-05,
         -1.0021e-05, -1.3784e-05],
        [ 2.1592e-04,  1.4883e-04, -2.0287e-04,  ...,  2.3022e-04,
          1.2756e-04,  1.1338e-04],
        [-3.7134e-05, -2.8580e-05,  1.9580e-05,  ..., -3.3736e-05,
         -1.8433e-05, -2.5094e-05]], device='cuda:0')
Loss: 1.0771214962005615


Running epoch 0, step 606, batch 606
Sampled inputs[:2]: tensor([[   0,   14,  475,  ..., 4103,  278, 4190],
        [   0,  775,  266,  ...,  409,  328, 5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6994e-04,  3.7227e-04, -4.0535e-04,  ...,  4.9531e-04,
         -4.7608e-04, -2.5560e-04],
        [-1.8194e-05, -1.4096e-05,  9.4473e-06,  ..., -1.6570e-05,
         -9.5293e-06, -1.2510e-05],
        [-2.3723e-05, -1.8358e-05,  1.2457e-05,  ..., -2.1517e-05,
         -1.1563e-05, -1.6049e-05],
        [ 2.1325e-04,  1.4673e-04, -2.0147e-04,  ...,  2.2785e-04,
          1.2638e-04,  1.1161e-04],
        [-4.3362e-05, -3.3468e-05,  2.2858e-05,  ..., -3.9250e-05,
         -2.1249e-05, -2.9206e-05]], device='cuda:0')
Loss: 1.0651105642318726


Running epoch 0, step 607, batch 607
Sampled inputs[:2]: tensor([[    0,   669,  1528,  ..., 21826,   259,  5024],
        [    0,  9058,  5481,  ...,   508, 15074,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7205e-04,  4.1948e-04, -3.2574e-04,  ...,  5.5515e-04,
         -4.4067e-04, -2.1665e-04],
        [-2.0832e-05, -1.6183e-05,  1.0788e-05,  ..., -1.8895e-05,
         -1.0796e-05, -1.4246e-05],
        [-2.7195e-05, -2.1085e-05,  1.4231e-05,  ..., -2.4572e-05,
         -1.3128e-05, -1.8314e-05],
        [ 2.1053e-04,  1.4460e-04, -2.0008e-04,  ...,  2.2545e-04,
          1.2518e-04,  1.0983e-04],
        [-4.9740e-05, -3.8445e-05,  2.6122e-05,  ..., -4.4823e-05,
         -2.4125e-05, -3.3319e-05]], device='cuda:0')
Loss: 1.096944808959961
Graident accumulation at epoch 0, step 607, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0149, -0.0277,  ...,  0.0285, -0.0154, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9377e-05,  9.4800e-05, -1.4362e-04,  ...,  1.0406e-04,
         -7.8617e-05,  4.1686e-05],
        [-1.3471e-05, -1.0500e-05,  5.2392e-06,  ..., -1.2397e-05,
         -4.9786e-06, -8.3009e-06],
        [ 2.1979e-06,  1.8167e-06,  1.3370e-06,  ...,  2.6593e-06,
         -3.7693e-06,  1.4672e-06],
        [ 1.0656e-05,  7.0984e-06, -1.6098e-05,  ...,  1.4793e-05,
          9.2626e-06,  5.4975e-06],
        [-4.1622e-05, -2.9417e-05,  1.9878e-05,  ..., -3.6770e-05,
         -1.6445e-05, -2.3867e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1563e-08, 2.8331e-08, 3.6014e-08,  ..., 1.2172e-08, 8.6655e-08,
         1.1969e-08],
        [6.4969e-11, 3.2937e-11, 7.2294e-12,  ..., 4.2023e-11, 5.3054e-12,
         1.2644e-11],
        [1.1917e-09, 6.0708e-10, 1.0290e-10,  ..., 9.7795e-10, 6.6385e-11,
         3.1105e-10],
        [1.5062e-10, 7.1536e-11, 5.1765e-11,  ..., 1.4084e-10, 2.4137e-11,
         3.8654e-11],
        [2.8599e-10, 1.5257e-10, 2.2668e-11,  ..., 2.0857e-10, 2.2928e-11,
         7.0287e-11]], device='cuda:0')
optimizer state dict: 76.0
lr: [1.6657307618927726e-05, 1.6657307618927726e-05]
scheduler_last_epoch: 76


Running epoch 0, step 608, batch 608
Sampled inputs[:2]: tensor([[    0,   278,  1295,  ...,  4337,   271,  1268],
        [    0, 28559,  1357,  ...,  7720,  1398, 41925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3778e-06,  1.5690e-04, -8.7507e-05,  ...,  1.4302e-04,
         -2.8976e-05, -5.9489e-05],
        [-2.4438e-06, -1.9073e-06,  1.2890e-06,  ..., -2.2650e-06,
         -1.0356e-06, -1.6689e-06],
        [-3.4422e-06, -2.6971e-06,  1.8403e-06,  ..., -3.2037e-06,
         -1.3486e-06, -2.3395e-06],
        [-2.7716e-06, -2.1607e-06,  1.4752e-06,  ..., -2.5779e-06,
         -1.0580e-06, -1.8775e-06],
        [-6.1691e-06, -4.8280e-06,  3.2932e-06,  ..., -5.7220e-06,
         -2.4587e-06, -4.1425e-06]], device='cuda:0')
Loss: 1.0822618007659912


Running epoch 0, step 609, batch 609
Sampled inputs[:2]: tensor([[   0, 3353,   17,  ...,  596,   12,  461],
        [   0,  287, 3284,  ...,  221,  493,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4131e-05,  1.6487e-04, -1.4733e-04,  ...,  1.8102e-04,
         -1.0171e-04, -1.3899e-04],
        [-4.9621e-06, -3.8743e-06,  2.5555e-06,  ..., -4.5598e-06,
         -2.0936e-06, -3.2932e-06],
        [ 7.8839e-05,  6.4193e-05, -4.4753e-05,  ...,  7.8326e-05,
          4.3236e-05,  5.1526e-05],
        [-5.6028e-06, -4.3660e-06,  2.9057e-06,  ..., -5.1558e-06,
         -2.1458e-06, -3.6880e-06],
        [-1.2696e-05, -9.8944e-06,  6.6012e-06,  ..., -1.1653e-05,
         -5.0515e-06, -8.2850e-06]], device='cuda:0')
Loss: 1.0927963256835938


Running epoch 0, step 610, batch 610
Sampled inputs[:2]: tensor([[    0,  1871,   401,  ...,    14,  4797,    12],
        [    0,    14,  1062,  ..., 10417,    13, 30579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3839e-05,  2.9880e-04, -2.6691e-04,  ...,  2.7221e-04,
         -2.3288e-04, -2.2305e-04],
        [-7.3314e-06, -5.8115e-06,  3.8370e-06,  ..., -6.7502e-06,
         -3.1292e-06, -4.8950e-06],
        [ 1.8848e-04,  1.3098e-04, -1.0258e-04,  ...,  1.6261e-04,
          7.4492e-05,  1.0356e-04],
        [-8.3447e-06, -6.6012e-06,  4.3958e-06,  ..., -7.6890e-06,
         -3.2261e-06, -5.5209e-06],
        [-1.8954e-05, -1.4961e-05,  1.0014e-05,  ..., -1.7375e-05,
         -7.5847e-06, -1.2398e-05]], device='cuda:0')
Loss: 1.074949026107788


Running epoch 0, step 611, batch 611
Sampled inputs[:2]: tensor([[    0,   221,   467,  ..., 21991,   630,  3990],
        [    0,  2297,   287,  ..., 10826, 13886,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3306e-05,  3.9604e-04, -4.0259e-04,  ...,  3.0588e-04,
         -2.3288e-04, -2.4675e-04],
        [-9.8050e-06, -7.7933e-06,  5.1036e-06,  ..., -9.0897e-06,
         -4.3213e-06, -6.6310e-06],
        [ 1.8511e-04,  1.2828e-04, -1.0083e-04,  ...,  1.5946e-04,
          7.2994e-05,  1.0123e-04],
        [-1.1072e-05, -8.7917e-06,  5.8040e-06,  ..., -1.0252e-05,
         -4.4182e-06, -7.4059e-06],
        [-2.5004e-05, -1.9819e-05,  1.3173e-05,  ..., -2.3037e-05,
         -1.0297e-05, -1.6540e-05]], device='cuda:0')
Loss: 1.0931735038757324


Running epoch 0, step 612, batch 612
Sampled inputs[:2]: tensor([[    0,   266, 11692,  ...,   278, 14620, 12718],
        [    0,    12,   344,  ...,  2337,  1122,   408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7406e-04,  4.1139e-04, -4.5658e-04,  ...,  3.4711e-04,
         -3.3871e-04, -3.9564e-04],
        [-1.2219e-05, -9.7454e-06,  6.3479e-06,  ..., -1.1325e-05,
         -5.3570e-06, -8.2478e-06],
        [ 1.8171e-04,  1.2554e-04, -9.9075e-05,  ...,  1.5633e-04,
          7.1638e-05,  9.8996e-05],
        [-1.3784e-05, -1.0982e-05,  7.2122e-06,  ..., -1.2770e-05,
         -5.4762e-06, -9.2089e-06],
        [-3.1143e-05, -2.4766e-05,  1.6361e-05,  ..., -2.8700e-05,
         -1.2770e-05, -2.0564e-05]], device='cuda:0')
Loss: 1.0876500606536865


Running epoch 0, step 613, batch 613
Sampled inputs[:2]: tensor([[    0,  1500,   367,  ...,   344,  4250,   287],
        [    0,   391,  1866,  ...,  3711, 21119, 29613]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2149e-04,  5.5397e-04, -4.4433e-04,  ...,  4.0600e-04,
         -3.2306e-04, -3.9761e-04],
        [-1.4663e-05, -1.1712e-05,  7.5921e-06,  ..., -1.3635e-05,
         -6.7651e-06, -1.0006e-05],
        [ 1.7842e-04,  1.2290e-04, -9.7369e-05,  ...,  1.5324e-04,
          6.9902e-05,  9.6686e-05],
        [-1.6391e-05, -1.3068e-05,  8.5607e-06,  ..., -1.5229e-05,
         -6.8247e-06, -1.1049e-05],
        [-3.7014e-05, -2.9445e-05,  1.9431e-05,  ..., -3.4183e-05,
         -1.5855e-05, -2.4647e-05]], device='cuda:0')
Loss: 1.0809246301651


Running epoch 0, step 614, batch 614
Sampled inputs[:2]: tensor([[   0,  221,  709,  ..., 3365, 3504,  278],
        [   0,  346,  462,  ..., 2208,   12, 1901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6753e-04,  5.3909e-04, -6.7176e-04,  ...,  6.4228e-04,
         -4.3410e-04, -5.3642e-04],
        [-1.7002e-05, -1.3679e-05,  8.8364e-06,  ..., -1.5870e-05,
         -7.8529e-06, -1.1668e-05],
        [ 1.7507e-04,  1.2011e-04, -9.5558e-05,  ...,  1.5007e-04,
          6.8472e-05,  9.4347e-05],
        [-1.9029e-05, -1.5259e-05,  9.9838e-06,  ..., -1.7732e-05,
         -7.9274e-06, -1.2897e-05],
        [-4.3005e-05, -3.4422e-05,  2.2680e-05,  ..., -3.9846e-05,
         -1.8433e-05, -2.8789e-05]], device='cuda:0')
Loss: 1.078127145767212


Running epoch 0, step 615, batch 615
Sampled inputs[:2]: tensor([[    0,   287,  3609,  ...,  3661,  5944,   838],
        [    0,    13,  1529,  ..., 15682,  1355,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0185e-05,  6.7155e-04, -7.7577e-04,  ...,  6.6471e-04,
         -5.3549e-04, -5.1289e-04],
        [-1.9431e-05, -1.5751e-05,  1.0133e-05,  ..., -1.8179e-05,
         -9.1866e-06, -1.3448e-05],
        [ 1.7174e-04,  1.1730e-04, -9.3770e-05,  ...,  1.4694e-04,
          6.6781e-05,  9.1948e-05],
        [-2.1622e-05, -1.7449e-05,  1.1377e-05,  ..., -2.0176e-05,
         -9.2313e-06, -1.4767e-05],
        [-4.8906e-05, -3.9399e-05,  2.5868e-05,  ..., -4.5389e-05,
         -2.1458e-05, -3.3021e-05]], device='cuda:0')
Loss: 1.08633553981781
Graident accumulation at epoch 0, step 615, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0227, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0149, -0.0277,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.6458e-05,  1.5248e-04, -2.0683e-04,  ...,  1.6012e-04,
         -1.2430e-04, -1.3772e-05],
        [-1.4067e-05, -1.1025e-05,  5.7286e-06,  ..., -1.2975e-05,
         -5.3994e-06, -8.8157e-06],
        [ 1.9152e-05,  1.3365e-05, -8.1738e-06,  ...,  1.7087e-05,
          3.2857e-06,  1.0515e-05],
        [ 7.4284e-06,  4.6437e-06, -1.3351e-05,  ...,  1.1296e-05,
          7.4132e-06,  3.4710e-06],
        [-4.2350e-05, -3.0415e-05,  2.0477e-05,  ..., -3.7632e-05,
         -1.6947e-05, -2.4782e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1530e-08, 2.8754e-08, 3.6580e-08,  ..., 1.2602e-08, 8.6855e-08,
         1.2220e-08],
        [6.5282e-11, 3.3153e-11, 7.3249e-12,  ..., 4.2311e-11, 5.3845e-12,
         1.2812e-11],
        [1.2200e-09, 6.2024e-10, 1.1159e-10,  ..., 9.9856e-10, 7.0779e-11,
         3.1920e-10],
        [1.5094e-10, 7.1769e-11, 5.1843e-11,  ..., 1.4111e-10, 2.4198e-11,
         3.8834e-11],
        [2.8810e-10, 1.5397e-10, 2.3315e-11,  ..., 2.1042e-10, 2.3365e-11,
         7.1307e-11]], device='cuda:0')
optimizer state dict: 77.0
lr: [1.656455958733442e-05, 1.656455958733442e-05]
scheduler_last_epoch: 77


Running epoch 0, step 616, batch 616
Sampled inputs[:2]: tensor([[    0,   300,  5864,  ...,    12,  3667,   796],
        [    0, 10251,   278,  ...,   278,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4114e-05, -1.6443e-05, -3.0720e-06,  ...,  7.2513e-05,
          2.2344e-05, -3.5203e-05],
        [-2.2948e-06, -1.9670e-06,  1.2219e-06,  ..., -2.1905e-06,
         -1.0580e-06, -1.6317e-06],
        [-3.3677e-06, -2.8759e-06,  1.8105e-06,  ..., -3.1888e-06,
         -1.4454e-06, -2.3693e-06],
        [-2.6673e-06, -2.2650e-06,  1.4305e-06,  ..., -2.5332e-06,
         -1.1176e-06, -1.8701e-06],
        [-6.0499e-06, -5.1260e-06,  3.2634e-06,  ..., -5.7220e-06,
         -2.6226e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.0920004844665527


Running epoch 0, step 617, batch 617
Sampled inputs[:2]: tensor([[   0, 2914,  352,  ...,  897,  328, 1679],
        [   0, 1603,   12,  ...,   12,  756,  437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0328e-05,  2.0405e-04, -1.7502e-04,  ...,  2.7846e-04,
         -1.4168e-04, -3.8436e-05],
        [-4.6194e-06, -3.9041e-06,  2.5183e-06,  ..., -4.3958e-06,
         -2.3171e-06, -3.3900e-06],
        [-6.7800e-06, -5.7220e-06,  3.7476e-06,  ..., -6.3926e-06,
         -3.1739e-06, -4.8876e-06],
        [-5.2601e-06, -4.4256e-06,  2.9057e-06,  ..., -4.9770e-06,
         -2.3991e-06, -3.7923e-06],
        [-1.2040e-05, -1.0103e-05,  6.6757e-06,  ..., -1.1325e-05,
         -5.6475e-06, -8.5831e-06]], device='cuda:0')
Loss: 1.0777512788772583


Running epoch 0, step 618, batch 618
Sampled inputs[:2]: tensor([[    0, 13245,  1503,  ...,    14,  5605,    12],
        [    0, 22387,   292,  ...,   352,  3097,   996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3219e-04,  3.8014e-04, -2.4699e-04,  ...,  3.3572e-04,
         -2.0321e-04, -1.0132e-05],
        [-6.9737e-06, -5.9009e-06,  3.7700e-06,  ..., -6.6310e-06,
         -3.4422e-06, -5.1260e-06],
        [-1.0163e-05, -8.5831e-06,  5.5805e-06,  ..., -9.5665e-06,
         -4.6566e-06, -7.3314e-06],
        [-7.9572e-06, -6.7055e-06,  4.3586e-06,  ..., -7.5251e-06,
         -3.5614e-06, -5.7444e-06],
        [-1.8060e-05, -1.5169e-05,  9.9391e-06,  ..., -1.6958e-05,
         -8.3148e-06, -1.2904e-05]], device='cuda:0')
Loss: 1.0846072435379028


Running epoch 0, step 619, batch 619
Sampled inputs[:2]: tensor([[   0,  668, 2474,  ...,  668, 4599,  360],
        [   0, 2328,  271,  ...,  706,   13, 8961]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0019e-04,  3.6297e-04, -2.1575e-04,  ...,  3.0603e-04,
         -2.2119e-04, -9.6828e-05],
        [-9.2536e-06, -7.8976e-06,  5.0366e-06,  ..., -8.7768e-06,
         -4.3884e-06, -6.7353e-06],
        [-1.3545e-05, -1.1519e-05,  7.4878e-06,  ..., -1.2711e-05,
         -5.9456e-06, -9.6858e-06],
        [-1.0625e-05, -9.0152e-06,  5.8636e-06,  ..., -1.0028e-05,
         -4.5523e-06, -7.6145e-06],
        [-2.4080e-05, -2.0385e-05,  1.3322e-05,  ..., -2.2560e-05,
         -1.0639e-05, -1.7047e-05]], device='cuda:0')
Loss: 1.089599609375


Running epoch 0, step 620, batch 620
Sampled inputs[:2]: tensor([[    0,  8920, 24095,  ...,   278,  2025,   437],
        [    0,   344,  8260,  ..., 16020, 18216, 11348]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6790e-04,  3.6027e-04, -2.9365e-04,  ...,  3.4721e-04,
         -2.8436e-04, -1.2192e-04],
        [-1.1533e-05, -9.7826e-06,  6.2659e-06,  ..., -1.0908e-05,
         -5.2899e-06, -8.3447e-06],
        [-1.6972e-05, -1.4350e-05,  9.3654e-06,  ..., -1.5914e-05,
         -7.2122e-06, -1.2085e-05],
        [-1.3381e-05, -1.1295e-05,  7.3761e-06,  ..., -1.2621e-05,
         -5.5432e-06, -9.5516e-06],
        [ 5.3334e-05,  3.1794e-05, -3.2131e-05,  ...,  4.9386e-05,
          2.9298e-05,  1.6995e-05]], device='cuda:0')
Loss: 1.0830638408660889


Running epoch 0, step 621, batch 621
Sampled inputs[:2]: tensor([[    0, 17471,  7279,  ...,   328,  6179,   287],
        [    0,   968,   266,  ...,   287,  2143, 15228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5789e-04,  4.2671e-04, -4.2264e-04,  ...,  3.3954e-04,
         -4.1755e-04, -1.5446e-04],
        [-1.3813e-05, -1.1675e-05,  7.5027e-06,  ..., -1.3113e-05,
         -6.4000e-06, -1.0051e-05],
        [-2.0295e-05, -1.7092e-05,  1.1206e-05,  ..., -1.9088e-05,
         -8.7023e-06, -1.4529e-05],
        [-1.6049e-05, -1.3500e-05,  8.8513e-06,  ..., -1.5184e-05,
         -6.7204e-06, -1.1519e-05],
        [ 4.7314e-05,  2.6846e-05, -2.8793e-05,  ...,  4.3664e-05,
          2.6571e-05,  1.2614e-05]], device='cuda:0')
Loss: 1.0961307287216187


Running epoch 0, step 622, batch 622
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,  4298,  1231,   352],
        [    0, 24062, 11234,  ...,  4252,   300,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1491e-04,  6.2173e-04, -4.5502e-04,  ...,  3.5308e-04,
         -4.6242e-04, -1.7035e-04],
        [-1.6034e-05, -1.3657e-05,  8.6874e-06,  ..., -1.5303e-05,
         -7.4953e-06, -1.1779e-05],
        [-2.3514e-05, -1.9923e-05,  1.2971e-05,  ..., -2.2203e-05,
         -1.0125e-05, -1.6958e-05],
        [-1.8612e-05, -1.5751e-05,  1.0252e-05,  ..., -1.7673e-05,
         -7.8455e-06, -1.3471e-05],
        [ 4.1652e-05,  2.1899e-05, -2.5664e-05,  ...,  3.8210e-05,
          2.4053e-05,  8.3816e-06]], device='cuda:0')
Loss: 1.0853267908096313


Running epoch 0, step 623, batch 623
Sampled inputs[:2]: tensor([[    0,  5055,   409,  ..., 32452, 24103,   472],
        [    0,   829,   874,  ...,   292,   380,   759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7779e-04,  7.0216e-04, -5.5234e-04,  ...,  4.1310e-04,
         -4.5084e-04, -1.9706e-04],
        [-1.8328e-05, -1.5609e-05,  9.9167e-06,  ..., -1.7494e-05,
         -8.5607e-06, -1.3471e-05],
        [-2.6837e-05, -2.2724e-05,  1.4782e-05,  ..., -2.5332e-05,
         -1.1556e-05, -1.9357e-05],
        [-2.1264e-05, -1.7986e-05,  1.1697e-05,  ..., -2.0191e-05,
         -8.9779e-06, -1.5408e-05],
        [ 3.5721e-05,  1.6922e-05, -2.2430e-05,  ...,  3.2637e-05,
          2.1475e-05,  4.1497e-06]], device='cuda:0')
Loss: 1.0903996229171753
Graident accumulation at epoch 0, step 623, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0032,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.3967e-05,  2.0744e-04, -2.4138e-04,  ...,  1.8542e-04,
         -1.5696e-04, -3.2101e-05],
        [-1.4493e-05, -1.1484e-05,  6.1474e-06,  ..., -1.3427e-05,
         -5.7155e-06, -9.2812e-06],
        [ 1.4553e-05,  9.7559e-06, -5.8782e-06,  ...,  1.2845e-05,
          1.8016e-06,  7.5280e-06],
        [ 4.5592e-06,  2.3807e-06, -1.0846e-05,  ...,  8.1476e-06,
          5.7741e-06,  1.5832e-06],
        [-3.4543e-05, -2.5682e-05,  1.6186e-05,  ..., -3.0605e-05,
         -1.3104e-05, -2.1889e-05]], device='cuda:0')
optimizer state dict: tensor([[4.1822e-08, 2.9218e-08, 3.6849e-08,  ..., 1.2760e-08, 8.6971e-08,
         1.2247e-08],
        [6.5552e-11, 3.3363e-11, 7.4159e-12,  ..., 4.2575e-11, 5.4524e-12,
         1.2981e-11],
        [1.2195e-09, 6.2013e-10, 1.1170e-10,  ..., 9.9821e-10, 7.0841e-11,
         3.1925e-10],
        [1.5124e-10, 7.2021e-11, 5.1928e-11,  ..., 1.4138e-10, 2.4255e-11,
         3.9032e-11],
        [2.8908e-10, 1.5410e-10, 2.3794e-11,  ..., 2.1128e-10, 2.3803e-11,
         7.1253e-11]], device='cuda:0')
optimizer state dict: 78.0
lr: [1.6470808433733317e-05, 1.6470808433733317e-05]
scheduler_last_epoch: 78


Running epoch 0, step 624, batch 624
Sampled inputs[:2]: tensor([[    0,   266,  7407,  ...,   287,   365,  4371],
        [    0,    71,    14,  ...,  1770,   391, 39516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9717e-05, -5.4105e-05,  9.5602e-05,  ...,  2.5776e-06,
          1.3331e-04,  3.1141e-05],
        [-2.0713e-06, -1.8924e-06,  1.2293e-06,  ..., -2.1160e-06,
         -1.0431e-06, -1.7136e-06],
        [-3.0994e-06, -2.8014e-06,  1.8775e-06,  ..., -3.0994e-06,
         -1.4156e-06, -2.4885e-06],
        [ 2.0585e-04,  3.2169e-04, -1.6929e-04,  ...,  2.8368e-04,
          2.3399e-04,  2.7172e-04],
        [-5.5432e-06, -4.9770e-06,  3.3677e-06,  ..., -5.5134e-06,
         -2.5481e-06, -4.3809e-06]], device='cuda:0')
Loss: 1.1043707132339478


Running epoch 0, step 625, batch 625
Sampled inputs[:2]: tensor([[   0, 4359, 5768,  ..., 4402,  292,   69],
        [   0,  741, 2985,  ...,  199,  769,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2863e-05, -6.3611e-05,  1.1178e-04,  ..., -5.5052e-05,
          1.6735e-04, -2.4350e-05],
        [-4.0680e-06, -3.6210e-06,  2.4512e-06,  ..., -4.1872e-06,
         -1.7546e-06, -3.3155e-06],
        [-6.1840e-06, -5.4836e-06,  3.7849e-06,  ..., -6.2883e-06,
         -2.4289e-06, -4.9472e-06],
        [ 2.0325e-04,  3.1946e-04, -1.6768e-04,  ...,  2.8099e-04,
          2.3317e-04,  2.6964e-04],
        [-1.0997e-05, -9.6858e-06,  6.7353e-06,  ..., -1.1116e-05,
         -4.3735e-06, -8.6427e-06]], device='cuda:0')
Loss: 1.0414371490478516


Running epoch 0, step 626, batch 626
Sampled inputs[:2]: tensor([[   0,  199, 7513,  ...,  271,  259,  957],
        [   0, 1034,  287,  ..., 9677,   13, 6687]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1907e-05, -1.0882e-05, -5.5230e-05,  ...,  1.9089e-05,
          1.4040e-05, -7.6537e-05],
        [-6.0946e-06, -5.3942e-06,  3.6433e-06,  ..., -6.2883e-06,
         -2.6710e-06, -4.9919e-06],
        [-9.3430e-06, -8.2105e-06,  5.6922e-06,  ..., -9.4920e-06,
         -3.6955e-06, -7.4655e-06],
        [ 2.0069e-04,  3.1724e-04, -1.6612e-04,  ...,  2.7836e-04,
          2.3214e-04,  2.6757e-04],
        [-1.6451e-05, -1.4365e-05,  1.0058e-05,  ..., -1.6630e-05,
         -6.5789e-06, -1.2934e-05]], device='cuda:0')
Loss: 1.0609015226364136


Running epoch 0, step 627, batch 627
Sampled inputs[:2]: tensor([[    0,   365,  1941,  ..., 38029,  1790, 44066],
        [    0, 15033,   278,  ...,   266,  2937,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0683e-04,  1.2415e-04, -1.6164e-04,  ...,  9.4956e-05,
         -1.3358e-05, -1.1371e-04],
        [-8.1062e-06, -7.1600e-06,  4.8354e-06,  ..., -8.4639e-06,
         -3.8333e-06, -6.7279e-06],
        [-1.2308e-05, -1.0774e-05,  7.4953e-06,  ..., -1.2621e-05,
         -5.2154e-06, -9.9242e-06],
        [ 1.9822e-04,  3.1509e-04, -1.6462e-04,  ...,  2.7572e-04,
          2.3087e-04,  2.6549e-04],
        [-2.1636e-05, -1.8835e-05,  1.3247e-05,  ..., -2.2084e-05,
         -9.2462e-06, -1.7196e-05]], device='cuda:0')
Loss: 1.0766412019729614


Running epoch 0, step 628, batch 628
Sampled inputs[:2]: tensor([[    0,  9010,    17,  ...,  3813,  1147,   199],
        [    0,  3829,   278,  ..., 11978,     9,   968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3048e-04,  1.3633e-04, -6.5350e-05,  ...,  3.3755e-05,
          1.2030e-04, -8.3510e-05],
        [-1.0267e-05, -9.0152e-06,  6.0871e-06,  ..., -1.0699e-05,
         -4.9584e-06, -8.5235e-06],
        [-1.5467e-05, -1.3456e-05,  9.3803e-06,  ..., -1.5825e-05,
         -6.6906e-06, -1.2457e-05],
        [ 1.9562e-04,  3.1289e-04, -1.6307e-04,  ...,  2.7305e-04,
          2.2965e-04,  2.6337e-04],
        [-2.7180e-05, -2.3574e-05,  1.6585e-05,  ..., -2.7716e-05,
         -1.1869e-05, -2.1607e-05]], device='cuda:0')
Loss: 1.1050997972488403


Running epoch 0, step 629, batch 629
Sampled inputs[:2]: tensor([[   0, 4304, 7406,  ...,  957, 7366,  328],
        [   0,   12, 9328,  ...,   20,  408,  790]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5406e-04,  1.7850e-04, -8.8084e-05,  ..., -4.0655e-06,
          1.4131e-04, -1.7167e-04],
        [-1.2368e-05, -1.0937e-05,  7.2867e-06,  ..., -1.2860e-05,
         -6.0759e-06, -1.0304e-05],
        [-1.8612e-05, -1.6272e-05,  1.1228e-05,  ..., -1.8984e-05,
         -8.1807e-06, -1.5005e-05],
        [ 1.9308e-04,  3.1061e-04, -1.6158e-04,  ...,  2.7047e-04,
          2.2845e-04,  2.6129e-04],
        [-3.2634e-05, -2.8402e-05,  1.9804e-05,  ..., -3.3170e-05,
         -1.4432e-05, -2.5958e-05]], device='cuda:0')
Loss: 1.085399866104126


Running epoch 0, step 630, batch 630
Sampled inputs[:2]: tensor([[    0,  1503,  1785,  ...,   221,   380,  1869],
        [    0,   609,   271,  ...,  4684, 14107,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6183e-04,  2.6394e-04, -1.3702e-04,  ...,  4.4253e-05,
          1.0603e-04, -2.7054e-04],
        [-1.4469e-05, -1.2822e-05,  8.4639e-06,  ..., -1.4976e-05,
         -6.9253e-06, -1.1906e-05],
        [-2.1875e-05, -1.9133e-05,  1.3098e-05,  ..., -2.2203e-05,
         -9.3430e-06, -1.7390e-05],
        [ 1.9045e-04,  3.0831e-04, -1.6009e-04,  ...,  2.6788e-04,
          2.2752e-04,  2.5935e-04],
        [-3.8385e-05, -3.3408e-05,  2.3097e-05,  ..., -3.8803e-05,
         -1.6488e-05, -3.0100e-05]], device='cuda:0')
Loss: 1.0682767629623413


Running epoch 0, step 631, batch 631
Sampled inputs[:2]: tensor([[    0,   320,  4886,  ...,    14,   333,   199],
        [    0, 43071,   278,  ...,   266, 21576,  5936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7250e-04,  2.5436e-05, -1.1052e-04,  ..., -1.8440e-05,
          3.2889e-04, -2.3116e-04],
        [-1.6406e-05, -1.4566e-05,  9.6336e-06,  ..., -1.7136e-05,
         -8.1919e-06, -1.3813e-05],
        [-2.4766e-05, -2.1681e-05,  1.4901e-05,  ..., -2.5287e-05,
         -1.0982e-05, -2.0087e-05],
        [ 1.8811e-04,  3.0626e-04, -1.5864e-04,  ...,  2.6536e-04,
          2.2620e-04,  2.5714e-04],
        [-4.3362e-05, -3.7760e-05,  2.6211e-05,  ..., -4.4048e-05,
         -1.9275e-05, -3.4600e-05]], device='cuda:0')
Loss: 1.0762547254562378
Graident accumulation at epoch 0, step 631, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0285, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.7820e-05,  1.8924e-04, -2.2830e-04,  ...,  1.6504e-04,
         -1.0837e-04, -5.2007e-05],
        [-1.4685e-05, -1.1792e-05,  6.4960e-06,  ..., -1.3798e-05,
         -5.9632e-06, -9.7344e-06],
        [ 1.0621e-05,  6.6122e-06, -3.8003e-06,  ...,  9.0320e-06,
          5.2318e-07,  4.7666e-06],
        [ 2.2915e-05,  3.2768e-05, -2.5625e-05,  ...,  3.3869e-05,
          2.7817e-05,  2.7139e-05],
        [-3.5425e-05, -2.6889e-05,  1.7189e-05,  ..., -3.1950e-05,
         -1.3722e-05, -2.3160e-05]], device='cuda:0')
optimizer state dict: tensor([[4.2108e-08, 2.9189e-08, 3.6824e-08,  ..., 1.2747e-08, 8.6992e-08,
         1.2288e-08],
        [6.5756e-11, 3.3542e-11, 7.5013e-12,  ..., 4.2826e-11, 5.5140e-12,
         1.3159e-11],
        [1.2189e-09, 6.1998e-10, 1.1181e-10,  ..., 9.9785e-10, 7.0891e-11,
         3.1934e-10],
        [1.8648e-10, 1.6574e-10, 7.7041e-11,  ..., 2.1165e-10, 7.5397e-11,
         1.0512e-10],
        [2.9067e-10, 1.5537e-10, 2.4458e-11,  ..., 2.1301e-10, 2.4151e-11,
         7.2379e-11]], device='cuda:0')
optimizer state dict: 79.0
lr: [1.6376068484119055e-05, 1.6376068484119055e-05]
scheduler_last_epoch: 79


Running epoch 0, step 632, batch 632
Sampled inputs[:2]: tensor([[    0,  5699,    20,  ...,  3502,  2051,   266],
        [    0,  1581, 11884,  ...,  7031,   689,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5042e-05, -3.6790e-05, -1.7067e-04,  ..., -2.2162e-05,
         -9.5824e-05, -7.5331e-05],
        [-1.7881e-06, -1.6764e-06,  1.2293e-06,  ..., -2.1160e-06,
         -1.3411e-06, -1.9372e-06],
        [-2.6077e-06, -2.4140e-06,  1.8328e-06,  ..., -2.9653e-06,
         -1.6913e-06, -2.6673e-06],
        [-2.2799e-06, -2.1160e-06,  1.6168e-06,  ..., -2.6226e-06,
         -1.4976e-06, -2.3842e-06],
        [-4.4107e-06, -4.0531e-06,  3.0994e-06,  ..., -4.9770e-06,
         -2.8759e-06, -4.4107e-06]], device='cuda:0')
Loss: 1.073965311050415


Running epoch 0, step 633, batch 633
Sampled inputs[:2]: tensor([[   0, 1713,  292,  ...,  596,  328, 1644],
        [   0,   12,  344,  ...,  824,   12,  968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8680e-04, -2.3492e-04,  1.0306e-04,  ..., -2.4331e-04,
          1.9966e-04,  5.9301e-05],
        [-3.7551e-06, -3.4720e-06,  2.3693e-06,  ..., -4.2319e-06,
         -2.8014e-06, -3.9339e-06],
        [-5.4687e-06, -4.9919e-06,  3.5465e-06,  ..., -5.9456e-06,
         -3.5688e-06, -5.4389e-06],
        [-4.6492e-06, -4.2468e-06,  3.0398e-06,  ..., -5.1111e-06,
         -3.0398e-06, -4.7088e-06],
        [-9.3579e-06, -8.4937e-06,  6.0946e-06,  ..., -1.0103e-05,
         -6.0946e-06, -9.0897e-06]], device='cuda:0')
Loss: 1.0846083164215088


Running epoch 0, step 634, batch 634
Sampled inputs[:2]: tensor([[    0,   266, 12080,  ...,   674,   369, 10956],
        [    0,  1921,   843,  ...,  9420,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9107e-04, -2.7688e-04,  1.2018e-04,  ..., -1.5653e-04,
          3.0538e-04, -5.7835e-05],
        [-5.6401e-06, -5.1558e-06,  3.5539e-06,  ..., -6.2436e-06,
         -3.7327e-06, -5.6103e-06],
        [-8.3596e-06, -7.5549e-06,  5.4240e-06,  ..., -8.9705e-06,
         -4.8131e-06, -7.9125e-06],
        [-7.0632e-06, -6.3777e-06,  4.6045e-06,  ..., -7.6592e-06,
         -4.0755e-06, -6.8098e-06],
        [-1.4424e-05, -1.2964e-05,  9.3877e-06,  ..., -1.5378e-05,
         -8.3148e-06, -1.3351e-05]], device='cuda:0')
Loss: 1.0656312704086304


Running epoch 0, step 635, batch 635
Sampled inputs[:2]: tensor([[    0,    12,  3367,  ..., 16917, 12221, 12138],
        [    0,   287,  2199,  ...,   266,  1241,  3139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9351e-04, -3.5728e-04,  1.5096e-04,  ..., -2.3867e-04,
          3.8488e-04, -7.2797e-05],
        [-7.5921e-06, -6.9365e-06,  4.8205e-06,  ..., -8.2403e-06,
         -4.6678e-06, -7.3612e-06],
        [-1.1355e-05, -1.0267e-05,  7.3910e-06,  ..., -1.1981e-05,
         -6.1095e-06, -1.0520e-05],
        [-9.5516e-06, -8.6427e-06,  6.2510e-06,  ..., -1.0192e-05,
         -5.1409e-06, -9.0152e-06],
        [-1.9610e-05, -1.7643e-05,  1.2815e-05,  ..., -2.0593e-05,
         -1.0625e-05, -1.7822e-05]], device='cuda:0')
Loss: 1.063877820968628


Running epoch 0, step 636, batch 636
Sampled inputs[:2]: tensor([[    0,   396,   298,  ...,    52,  5065,    13],
        [    0,   271, 36770,  ...,   278,  1398,  4555]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1989e-04, -1.0478e-03,  5.9070e-04,  ..., -7.2229e-04,
          1.0041e-03,  1.1037e-04],
        [-9.4324e-06, -8.6948e-06,  5.9232e-06,  ..., -1.0386e-05,
         -6.2101e-06, -9.3877e-06],
        [-1.4082e-05, -1.2845e-05,  9.1195e-06,  ..., -1.5020e-05,
         -8.0615e-06, -1.3322e-05],
        [-1.1757e-05, -1.0729e-05,  7.6443e-06,  ..., -1.2681e-05,
         -6.7279e-06, -1.1325e-05],
        [-2.4259e-05, -2.2024e-05,  1.5780e-05,  ..., -2.5690e-05,
         -1.3888e-05, -2.2441e-05]], device='cuda:0')
Loss: 1.0780318975448608


Running epoch 0, step 637, batch 637
Sampled inputs[:2]: tensor([[   0,   25,    5,  ..., 3935,   14,   16],
        [   0,   12, 5820,  ...,    5, 2122,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0241e-03, -1.1474e-03,  4.4078e-04,  ..., -6.8420e-04,
          9.0487e-04,  1.2037e-04],
        [-1.1332e-05, -1.0453e-05,  7.1302e-06,  ..., -1.2591e-05,
         -7.6778e-06, -1.1414e-05],
        [-1.6734e-05, -1.5259e-05,  1.0870e-05,  ..., -1.7956e-05,
         -9.8422e-06, -1.5974e-05],
        [-1.4096e-05, -1.2860e-05,  9.1940e-06,  ..., -1.5333e-05,
         -8.3297e-06, -1.3739e-05],
        [-2.8938e-05, -2.6226e-05,  1.8865e-05,  ..., -3.0816e-05,
         -1.7032e-05, -2.7001e-05]], device='cuda:0')
Loss: 1.0771191120147705


Running epoch 0, step 638, batch 638
Sampled inputs[:2]: tensor([[    0,  2270,   278,  ..., 36325,  5892,  3558],
        [    0,   341,   298,  ...,   298,  1304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1131e-03, -1.1879e-03,  4.9169e-04,  ..., -6.3179e-04,
          9.5262e-04,  1.3567e-04],
        [-1.3225e-05, -1.2212e-05,  8.2701e-06,  ..., -1.4707e-05,
         -9.0338e-06, -1.3314e-05],
        [-1.9550e-05, -1.7852e-05,  1.2629e-05,  ..., -2.0996e-05,
         -1.1623e-05, -1.8671e-05],
        [-1.6361e-05, -1.4931e-05,  1.0617e-05,  ..., -1.7792e-05,
         -9.7305e-06, -1.5929e-05],
        [-3.3736e-05, -3.0607e-05,  2.1890e-05,  ..., -3.5942e-05,
         -2.0027e-05, -3.1471e-05]], device='cuda:0')
Loss: 1.055769681930542


Running epoch 0, step 639, batch 639
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  6328,    12,  ...,   417,   199,  1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2170e-03, -1.2626e-03,  4.8924e-04,  ..., -7.0948e-04,
          9.0755e-04,  1.4687e-04],
        [-1.5162e-05, -1.3970e-05,  9.4771e-06,  ..., -1.6764e-05,
         -9.9726e-06, -1.5028e-05],
        [ 9.3059e-05,  4.1385e-05, -3.5933e-05,  ...,  5.3456e-05,
          2.3091e-05,  4.2996e-05],
        [-1.8865e-05, -1.7181e-05,  1.2219e-05,  ..., -2.0429e-05,
         -1.0811e-05, -1.8120e-05],
        [-3.8803e-05, -3.5137e-05,  2.5123e-05,  ..., -4.1217e-05,
         -2.2292e-05, -3.5763e-05]], device='cuda:0')
Loss: 1.0789457559585571
Graident accumulation at epoch 0, step 639, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0146,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0162,  0.0150, -0.0277,  ...,  0.0286, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.2659e-05,  4.4061e-05, -1.5654e-04,  ...,  7.7584e-05,
         -6.7806e-06, -3.2119e-05],
        [-1.4732e-05, -1.2010e-05,  6.7941e-06,  ..., -1.4094e-05,
         -6.3641e-06, -1.0264e-05],
        [ 1.8865e-05,  1.0089e-05, -7.0135e-06,  ...,  1.3474e-05,
          2.7799e-06,  8.5895e-06],
        [ 1.8737e-05,  2.7773e-05, -2.1841e-05,  ...,  2.8439e-05,
          2.3954e-05,  2.2613e-05],
        [-3.5763e-05, -2.7714e-05,  1.7982e-05,  ..., -3.2876e-05,
         -1.4579e-05, -2.4420e-05]], device='cuda:0')
optimizer state dict: tensor([[4.3547e-08, 3.0754e-08, 3.7027e-08,  ..., 1.3238e-08, 8.7729e-08,
         1.2297e-08],
        [6.5920e-11, 3.3703e-11, 7.5836e-12,  ..., 4.3064e-11, 5.6079e-12,
         1.3371e-11],
        [1.2263e-09, 6.2108e-10, 1.1299e-10,  ..., 9.9971e-10, 7.1353e-11,
         3.2087e-10],
        [1.8665e-10, 1.6587e-10, 7.7113e-11,  ..., 2.1186e-10, 7.5439e-11,
         1.0534e-10],
        [2.9189e-10, 1.5645e-10, 2.5064e-11,  ..., 2.1449e-10, 2.4623e-11,
         7.3586e-11]], device='cuda:0')
optimizer state dict: 80.0
lr: [1.628035421558293e-05, 1.628035421558293e-05]
scheduler_last_epoch: 80
Epoch 0 | Batch 639/1048 | Training PPL: 5605.8504111763405 | time 31.013324737548828
Saving checkpoint at epoch 0, step 639, batch 639
Epoch 0 | Validation PPL: 8.485576937040502 | Learning rate: 1.628035421558293e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_639, AFTER epoch 0, step 639


Running epoch 0, step 640, batch 640
Sampled inputs[:2]: tensor([[    0,   365,   925,  ...,   909,   598,   328],
        [    0,  1526,  3502,  ..., 11727,  3736,  1661]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3252e-04, -5.0074e-04,  2.3945e-04,  ..., -3.0855e-04,
          1.1451e-04,  1.8476e-04],
        [-2.0117e-06, -1.7583e-06,  1.1176e-06,  ..., -2.1607e-06,
         -1.6689e-06, -2.0862e-06],
        [-2.9355e-06, -2.5481e-06,  1.7360e-06,  ..., -3.0249e-06,
         -2.1160e-06, -2.8461e-06],
        [-2.3693e-06, -2.0564e-06,  1.4007e-06,  ..., -2.4736e-06,
         -1.7509e-06, -2.3693e-06],
        [-4.9472e-06, -4.3213e-06,  2.9802e-06,  ..., -5.0664e-06,
         -3.5912e-06, -4.6790e-06]], device='cuda:0')
Loss: 1.0617733001708984


Running epoch 0, step 641, batch 641
Sampled inputs[:2]: tensor([[   0,   25,   26,  ...,    9,  287,  298],
        [   0,  287,  259,  ..., 5041, 1826, 5041]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3870e-04, -6.9344e-04,  2.9004e-04,  ..., -5.3168e-04,
          4.7206e-04,  2.9500e-04],
        [-3.6731e-06, -3.3677e-06,  2.2799e-06,  ..., -4.2468e-06,
         -3.1516e-06, -4.0233e-06],
        [-5.4985e-06, -4.9174e-06,  3.5912e-06,  ..., -5.9754e-06,
         -3.9861e-06, -5.5432e-06],
        [-4.5449e-06, -4.0829e-06,  2.9877e-06,  ..., -5.0366e-06,
         -3.3826e-06, -4.7237e-06],
        [-9.2387e-06, -8.2552e-06,  6.1244e-06,  ..., -9.9540e-06,
         -6.7055e-06, -9.0301e-06]], device='cuda:0')
Loss: 1.0756205320358276


Running epoch 0, step 642, batch 642
Sampled inputs[:2]: tensor([[    0,   266,   298,  ...,   266,   818,   278],
        [    0,   417,   199,  ...,  9472, 15004,   511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4052e-04, -7.1578e-04,  2.3910e-04,  ..., -5.5145e-04,
          4.1927e-04,  1.2823e-04],
        [-5.6401e-06, -5.0217e-06,  3.3528e-06,  ..., -6.3777e-06,
         -4.6790e-06, -6.0499e-06],
        [-8.4937e-06, -7.4059e-06,  5.3123e-06,  ..., -9.0897e-06,
         -6.0424e-06, -8.4490e-06],
        [-6.8992e-06, -6.0350e-06,  4.3437e-06,  ..., -7.4953e-06,
         -4.9770e-06, -7.0333e-06],
        [-1.4365e-05, -1.2487e-05,  9.1046e-06,  ..., -1.5229e-05,
         -1.0177e-05, -1.3858e-05]], device='cuda:0')
Loss: 1.0677682161331177


Running epoch 0, step 643, batch 643
Sampled inputs[:2]: tensor([[    0,  2336,    26,  ...,  2564,   271,  1422],
        [    0, 14949,    12,  ...,   669, 10168,  7166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2033e-04, -6.9058e-04,  3.1412e-04,  ..., -4.9524e-04,
          4.4630e-04,  2.4066e-04],
        [-7.4729e-06, -6.7055e-06,  4.5449e-06,  ..., -8.4043e-06,
         -5.9307e-06, -7.8678e-06],
        [-1.1250e-05, -9.8944e-06,  7.1898e-06,  ..., -1.2010e-05,
         -7.6815e-06, -1.1027e-05],
        [-9.2685e-06, -8.1807e-06,  5.9605e-06,  ..., -1.0028e-05,
         -6.3926e-06, -9.2983e-06],
        [-1.9163e-05, -1.6749e-05,  1.2368e-05,  ..., -2.0236e-05,
         -1.3053e-05, -1.8209e-05]], device='cuda:0')
Loss: 1.039996862411499


Running epoch 0, step 644, batch 644
Sampled inputs[:2]: tensor([[    0,  3773, 23452,  ..., 14393,  1121,   304],
        [    0,    14,  1266,  ...,  2288,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6117e-04, -9.4183e-04,  6.3278e-04,  ..., -7.3892e-04,
          8.7460e-04,  3.5881e-04],
        [-9.4846e-06, -8.5235e-06,  5.7444e-06,  ..., -1.0476e-05,
         -7.3388e-06, -9.7007e-06],
        [-1.4305e-05, -1.2636e-05,  9.0972e-06,  ..., -1.5050e-05,
         -9.5889e-06, -1.3664e-05],
        [-1.1697e-05, -1.0371e-05,  7.4804e-06,  ..., -1.2487e-05,
         -7.9349e-06, -1.1459e-05],
        [-2.4378e-05, -2.1398e-05,  1.5646e-05,  ..., -2.5421e-05,
         -1.6332e-05, -2.2620e-05]], device='cuda:0')
Loss: 1.103775978088379


Running epoch 0, step 645, batch 645
Sampled inputs[:2]: tensor([[    0,   516,   689,  ...,   278,   516,  6137],
        [    0, 31539,  1156,  ...,     9,   287, 26127]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6111e-04, -8.9124e-04,  5.9076e-04,  ..., -7.3215e-04,
          7.7762e-04,  2.9814e-04],
        [-1.1235e-05, -1.0140e-05,  6.9365e-06,  ..., -1.2457e-05,
         -8.5160e-06, -1.1556e-05],
        [-1.6928e-05, -1.5005e-05,  1.0923e-05,  ..., -1.7837e-05,
         -1.1072e-05, -1.6212e-05],
        [-1.4022e-05, -1.2472e-05,  9.1195e-06,  ..., -1.5035e-05,
         -9.2983e-06, -1.3828e-05],
        [-2.8878e-05, -2.5451e-05,  1.8790e-05,  ..., -3.0160e-05,
         -1.8895e-05, -2.6882e-05]], device='cuda:0')
Loss: 1.058536171913147


Running epoch 0, step 646, batch 646
Sampled inputs[:2]: tensor([[   0, 6803, 6298,  ...,  490, 1781,   12],
        [   0,  287,  516,  ..., 2386, 3492, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5686e-04, -8.7496e-04,  6.4277e-04,  ..., -6.6678e-04,
          7.8090e-04,  3.0783e-04],
        [-1.3188e-05, -1.1884e-05,  8.2403e-06,  ..., -1.4484e-05,
         -9.5218e-06, -1.3225e-05],
        [-1.9923e-05, -1.7658e-05,  1.2949e-05,  ..., -2.0877e-05,
         -1.2457e-05, -1.8686e-05],
        [-1.6570e-05, -1.4722e-05,  1.0855e-05,  ..., -1.7643e-05,
         -1.0476e-05, -1.5974e-05],
        [-3.4004e-05, -2.9981e-05,  2.2277e-05,  ..., -3.5346e-05,
         -2.1309e-05, -3.1054e-05]], device='cuda:0')
Loss: 1.0583853721618652


Running epoch 0, step 647, batch 647
Sampled inputs[:2]: tensor([[   0,  408, 1782,  ...,  271,  729, 1692],
        [   0, 1099, 2851,  ...,  518,  496,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8706e-04, -9.1429e-04,  7.5735e-04,  ..., -6.4328e-04,
          8.0276e-04,  3.2219e-04],
        [-1.5259e-05, -1.3687e-05,  9.4771e-06,  ..., -1.6510e-05,
         -1.0647e-05, -1.4938e-05],
        [-2.3142e-05, -2.0429e-05,  1.4931e-05,  ..., -2.3961e-05,
         -1.4059e-05, -2.1249e-05],
        [-1.9163e-05, -1.6958e-05,  1.2457e-05,  ..., -2.0161e-05,
         -1.1764e-05, -1.8075e-05],
        [-3.9518e-05, -3.4720e-05,  2.5690e-05,  ..., -4.0621e-05,
         -2.4110e-05, -3.5375e-05]], device='cuda:0')
Loss: 1.0657364130020142
Graident accumulation at epoch 0, step 647, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0145,  0.0031,  ..., -0.0026,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2710e-04, -5.1775e-05, -6.5153e-05,  ...,  5.4983e-06,
          7.4174e-05,  3.3117e-06],
        [-1.4785e-05, -1.2177e-05,  7.0624e-06,  ..., -1.4336e-05,
         -6.7924e-06, -1.0731e-05],
        [ 1.4665e-05,  7.0376e-06, -4.8191e-06,  ...,  9.7308e-06,
          1.0960e-06,  5.6057e-06],
        [ 1.4947e-05,  2.3300e-05, -1.8411e-05,  ...,  2.3579e-05,
          2.0382e-05,  1.8544e-05],
        [-3.6138e-05, -2.8415e-05,  1.8753e-05,  ..., -3.3651e-05,
         -1.5532e-05, -2.5516e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4290e-08, 3.1559e-08, 3.7563e-08,  ..., 1.3639e-08, 8.8286e-08,
         1.2389e-08],
        [6.6087e-11, 3.3857e-11, 7.6658e-12,  ..., 4.3294e-11, 5.7157e-12,
         1.3581e-11],
        [1.2256e-09, 6.2087e-10, 1.1310e-10,  ..., 9.9928e-10, 7.1480e-11,
         3.2100e-10],
        [1.8683e-10, 1.6599e-10, 7.7191e-11,  ..., 2.1205e-10, 7.5502e-11,
         1.0556e-10],
        [2.9316e-10, 1.5750e-10, 2.5699e-11,  ..., 2.1593e-10, 2.5180e-11,
         7.4764e-11]], device='cuda:0')
optimizer state dict: 81.0
lr: [1.6183680254100683e-05, 1.6183680254100683e-05]
scheduler_last_epoch: 81


Running epoch 0, step 648, batch 648
Sampled inputs[:2]: tensor([[   0,  328, 1410,  ..., 7344,   12, 5067],
        [   0, 4294,  278,  ...,   13, 2759, 5160]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4376e-04,  2.3094e-04, -1.8013e-04,  ..., -3.3425e-05,
          3.3393e-05, -1.2917e-04],
        [-2.0564e-06, -1.6987e-06,  1.2517e-06,  ..., -2.0117e-06,
         -1.0952e-06, -1.5199e-06],
        [-3.3081e-06, -2.6971e-06,  2.0862e-06,  ..., -3.1143e-06,
         -1.5572e-06, -2.2948e-06],
        [-2.6673e-06, -2.1756e-06,  1.6838e-06,  ..., -2.5630e-06,
         -1.2740e-06, -1.9222e-06],
        [-5.5134e-06, -4.4703e-06,  3.5018e-06,  ..., -5.1558e-06,
         -2.6077e-06, -3.7551e-06]], device='cuda:0')
Loss: 1.0657451152801514


Running epoch 0, step 649, batch 649
Sampled inputs[:2]: tensor([[    0,  1445,  3597,  ...,   281,    78,     9],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0272e-04,  4.7577e-04, -2.9955e-04,  ...,  1.5943e-04,
         -2.5173e-04, -1.7777e-04],
        [-4.1574e-06, -3.3826e-06,  2.6003e-06,  ..., -4.0233e-06,
         -2.0787e-06, -3.0771e-06],
        [-6.6757e-06, -5.3793e-06,  4.2766e-06,  ..., -6.2734e-06,
         -2.9579e-06, -4.6790e-06],
        [-5.4091e-06, -4.3511e-06,  3.4794e-06,  ..., -5.1558e-06,
         -2.4214e-06, -3.9190e-06],
        [-1.1086e-05, -8.9109e-06,  7.1377e-06,  ..., -1.0341e-05,
         -4.9770e-06, -7.6294e-06]], device='cuda:0')
Loss: 1.066776156425476


Running epoch 0, step 650, batch 650
Sampled inputs[:2]: tensor([[    0,   360,  5323,  ..., 29974,    25,    27],
        [    0,    13,  4596,  ...,   408,   689,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0451e-04,  5.4948e-04, -4.0456e-04,  ...,  1.8914e-04,
         -2.9848e-04, -2.4573e-04],
        [-6.2287e-06, -5.1036e-06,  3.8520e-06,  ..., -6.0350e-06,
         -3.1814e-06, -4.6864e-06],
        [-9.9391e-06, -8.0466e-06,  6.3181e-06,  ..., -9.3281e-06,
         -4.4629e-06, -7.0482e-06],
        [-8.1211e-06, -6.5565e-06,  5.1782e-06,  ..., -7.7188e-06,
         -3.6880e-06, -5.9456e-06],
        [-1.6630e-05, -1.3381e-05,  1.0610e-05,  ..., -1.5467e-05,
         -7.5251e-06, -1.1533e-05]], device='cuda:0')
Loss: 1.0649569034576416


Running epoch 0, step 651, batch 651
Sampled inputs[:2]: tensor([[   0, 6112,  278,  ..., 4092,  490, 2774],
        [   0, 3388,  278,  ..., 7203,  271, 1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5685e-04,  7.5064e-04, -4.8216e-04,  ...,  2.8889e-04,
         -3.3572e-04, -3.5575e-04],
        [-8.2403e-06, -6.7875e-06,  5.0962e-06,  ..., -7.9870e-06,
         -4.1798e-06, -6.1914e-06],
        [ 4.3136e-05,  4.8519e-05, -4.6057e-05,  ...,  7.0655e-05,
          5.8882e-05,  7.0498e-05],
        [-1.0818e-05, -8.7917e-06,  6.9067e-06,  ..., -1.0297e-05,
         -4.8801e-06, -7.9125e-06],
        [-2.2084e-05, -1.7911e-05,  1.4111e-05,  ..., -2.0593e-05,
         -9.9689e-06, -1.5333e-05]], device='cuda:0')
Loss: 1.0675078630447388


Running epoch 0, step 652, batch 652
Sampled inputs[:2]: tensor([[   0,  266, 2604,  ...,  278, 4035, 4165],
        [   0,   18,   14,  ...,  300,  275, 1184]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9436e-04,  6.2534e-04, -4.3940e-04,  ...,  8.6393e-05,
         -2.0789e-04, -3.9645e-04],
        [-1.0416e-05, -8.5905e-06,  6.4299e-06,  ..., -9.9987e-06,
         -5.0999e-06, -7.6964e-06],
        [ 3.9620e-05,  4.5614e-05, -4.3866e-05,  ...,  6.7451e-05,
          5.7504e-05,  6.8129e-05],
        [-1.3649e-05, -1.1131e-05,  8.6725e-06,  ..., -1.2890e-05,
         -5.9828e-06, -9.8497e-06],
        [-2.8074e-05, -2.2829e-05,  1.7852e-05,  ..., -2.6047e-05,
         -1.2368e-05, -1.9297e-05]], device='cuda:0')
Loss: 1.0828057527542114


Running epoch 0, step 653, batch 653
Sampled inputs[:2]: tensor([[    0, 42306,   278,  ...,  1110,  3427,  4224],
        [    0,   292, 16983,  ...,   221,   474,  4800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5802e-04,  8.2193e-04, -4.6155e-04,  ...,  1.0408e-04,
         -1.7434e-04, -3.2753e-04],
        [-1.2502e-05, -1.0334e-05,  7.6070e-06,  ..., -1.2010e-05,
         -6.1952e-06, -9.2387e-06],
        [ 3.6252e-05,  4.2857e-05, -4.1870e-05,  ...,  6.4367e-05,
          5.5962e-05,  6.5805e-05],
        [-1.6302e-05, -1.3307e-05,  1.0230e-05,  ..., -1.5363e-05,
         -7.2196e-06, -1.1735e-05],
        [-3.3796e-05, -2.7448e-05,  2.1249e-05,  ..., -3.1233e-05,
         -1.4961e-05, -2.3112e-05]], device='cuda:0')
Loss: 1.0781035423278809


Running epoch 0, step 654, batch 654
Sampled inputs[:2]: tensor([[    0,    40,   568,  ...,  3750,   300,  3421],
        [    0,   221,   527,  ...,   417,   199, 30714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4660e-04,  8.1184e-04, -4.3039e-04,  ...,  5.6849e-05,
         -2.0836e-04, -3.4470e-04],
        [-1.4484e-05, -1.1995e-05,  8.8438e-06,  ..., -1.3992e-05,
         -7.2010e-06, -1.0803e-05],
        [ 3.3078e-05,  4.0249e-05, -3.9828e-05,  ...,  6.1342e-05,
          5.4576e-05,  6.3465e-05],
        [-1.8969e-05, -1.5497e-05,  1.1951e-05,  ..., -1.7956e-05,
         -8.4043e-06, -1.3776e-05],
        [-3.9101e-05, -3.1769e-05,  2.4661e-05,  ..., -3.6240e-05,
         -1.7285e-05, -2.6911e-05]], device='cuda:0')
Loss: 1.0590837001800537


Running epoch 0, step 655, batch 655
Sampled inputs[:2]: tensor([[    0,   271, 21394,  ...,  1487,   287,   292],
        [    0,  2720,    14,  ...,   300, 15867,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1017e-04,  1.0539e-03, -6.0043e-04,  ...,  1.5069e-04,
         -4.3294e-04, -3.4470e-04],
        [-1.6540e-05, -1.3694e-05,  1.0051e-05,  ..., -1.6004e-05,
         -8.4303e-06, -1.2405e-05],
        [ 2.9785e-05,  3.7567e-05, -3.7831e-05,  ...,  5.8257e-05,
          5.2847e-05,  6.1051e-05],
        [-2.1636e-05, -1.7673e-05,  1.3575e-05,  ..., -2.0489e-05,
         -9.8273e-06, -1.5803e-05],
        [-4.4644e-05, -3.6299e-05,  2.8059e-05,  ..., -4.1395e-05,
         -2.0221e-05, -3.0875e-05]], device='cuda:0')
Loss: 1.0724647045135498
Graident accumulation at epoch 0, step 655, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0055, -0.0145,  0.0031,  ..., -0.0025,  0.0228, -0.0197],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0343, -0.0088,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0153, -0.0180]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.3372e-05,  5.8793e-05, -1.1868e-04,  ...,  2.0018e-05,
          2.3463e-05, -3.1489e-05],
        [-1.4961e-05, -1.2329e-05,  7.3613e-06,  ..., -1.4503e-05,
         -6.9562e-06, -1.0899e-05],
        [ 1.6177e-05,  1.0091e-05, -8.1203e-06,  ...,  1.4584e-05,
          6.2711e-06,  1.1150e-05],
        [ 1.1288e-05,  1.9203e-05, -1.5212e-05,  ...,  1.9172e-05,
          1.7361e-05,  1.5110e-05],
        [-3.6989e-05, -2.9203e-05,  1.9684e-05,  ..., -3.4425e-05,
         -1.6001e-05, -2.6052e-05]], device='cuda:0')
optimizer state dict: tensor([[4.4750e-08, 3.2639e-08, 3.7886e-08,  ..., 1.3648e-08, 8.8385e-08,
         1.2495e-08],
        [6.6294e-11, 3.4011e-11, 7.7592e-12,  ..., 4.3507e-11, 5.7811e-12,
         1.3721e-11],
        [1.2253e-09, 6.2166e-10, 1.1441e-10,  ..., 1.0017e-09, 7.4201e-11,
         3.2440e-10],
        [1.8711e-10, 1.6614e-10, 7.7299e-11,  ..., 2.1226e-10, 7.5523e-11,
         1.0570e-10],
        [2.9486e-10, 1.5866e-10, 2.6461e-11,  ..., 2.1743e-10, 2.5564e-11,
         7.5642e-11]], device='cuda:0')
optimizer state dict: 82.0
lr: [1.6086061372297498e-05, 1.6086061372297498e-05]
scheduler_last_epoch: 82


Running epoch 0, step 656, batch 656
Sampled inputs[:2]: tensor([[    0, 21448,   344,  ...,   365,  1501,   271],
        [    0,  9017,   600,  ...,  6133,  1098,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1683e-05,  7.2269e-05, -1.1028e-04,  ...,  5.3395e-05,
         -9.2944e-05, -4.4647e-05],
        [-2.2650e-06, -1.7360e-06,  1.3188e-06,  ..., -1.9521e-06,
         -9.4250e-07, -1.3709e-06],
        [-3.6657e-06, -2.8163e-06,  2.1756e-06,  ..., -3.1292e-06,
         -1.4231e-06, -2.1607e-06],
        [-3.0100e-06, -2.3097e-06,  1.7956e-06,  ..., -2.5779e-06,
         -1.1548e-06, -1.8179e-06],
        [-6.1095e-06, -4.7088e-06,  3.6210e-06,  ..., -5.2154e-06,
         -2.4289e-06, -3.5465e-06]], device='cuda:0')
Loss: 1.0635496377944946


Running epoch 0, step 657, batch 657
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 10688,  7988, 25842],
        [    0,     9,   287,  ...,   369,  2968,  8347]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3476e-04,  2.3549e-04, -3.1956e-04,  ...,  2.4630e-04,
         -3.5906e-04, -2.3843e-05],
        [-4.3511e-06, -3.3006e-06,  2.5183e-06,  ..., -4.0233e-06,
         -2.5295e-06, -3.1292e-06],
        [-7.0035e-06, -5.3197e-06,  4.1574e-06,  ..., -6.3032e-06,
         -3.6880e-06, -4.7833e-06],
        [-5.5879e-06, -4.2319e-06,  3.3528e-06,  ..., -5.0962e-06,
         -2.9728e-06, -3.9488e-06],
        [-1.1504e-05, -8.7619e-06,  6.8247e-06,  ..., -1.0312e-05,
         -6.0648e-06, -7.6592e-06]], device='cuda:0')
Loss: 1.059747338294983


Running epoch 0, step 658, batch 658
Sampled inputs[:2]: tensor([[    0,  5685,   565,  ..., 23968,    14,   381],
        [    0,   367,  3675,  ...,    22,  3180,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0838e-04,  3.4327e-04, -3.4223e-04,  ...,  3.1086e-04,
         -4.2399e-04, -2.6131e-05],
        [-6.5714e-06, -5.0217e-06,  3.7476e-06,  ..., -6.0350e-06,
         -3.7514e-06, -4.5970e-06],
        [-1.0565e-05, -8.0615e-06,  6.1989e-06,  ..., -9.4324e-06,
         -5.4687e-06, -7.0184e-06],
        [-8.4788e-06, -6.4373e-06,  4.9993e-06,  ..., -7.6443e-06,
         -4.4405e-06, -5.8115e-06],
        [-1.7375e-05, -1.3262e-05,  1.0207e-05,  ..., -1.5438e-05,
         -9.0152e-06, -1.1250e-05]], device='cuda:0')
Loss: 1.0833429098129272


Running epoch 0, step 659, batch 659
Sampled inputs[:2]: tensor([[   0, 4323, 2377,  ..., 3878, 4044,   14],
        [   0, 1067,  408,  ..., 4657, 1016,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2969e-04,  5.8492e-04, -5.0196e-04,  ...,  4.3587e-04,
         -6.1646e-04,  5.1868e-05],
        [-8.8364e-06, -6.6906e-06,  4.9099e-06,  ..., -8.0615e-06,
         -4.9062e-06, -6.0350e-06],
        [-1.4201e-05, -1.0729e-05,  8.1360e-06,  ..., -1.2606e-05,
         -7.1898e-06, -9.2238e-06],
        [-1.1384e-05, -8.5682e-06,  6.5416e-06,  ..., -1.0207e-05,
         -5.8264e-06, -7.6219e-06],
        [-2.3425e-05, -1.7703e-05,  1.3441e-05,  ..., -2.0713e-05,
         -1.1891e-05, -1.4842e-05]], device='cuda:0')
Loss: 1.040786862373352


Running epoch 0, step 660, batch 660
Sampled inputs[:2]: tensor([[    0,  1862,   674,  ...,   391,   266,  7688],
        [    0, 16028,   669,  ...,   292,  6502,  7050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6375e-04,  8.7870e-04, -6.1040e-04,  ...,  6.2073e-04,
         -6.7009e-04, -3.4871e-05],
        [-1.1131e-05, -8.4713e-06,  6.0648e-06,  ..., -1.0058e-05,
         -6.1356e-06, -7.5772e-06],
        [-1.7911e-05, -1.3605e-05,  1.0088e-05,  ..., -1.5765e-05,
         -9.0301e-06, -1.1608e-05],
        [-1.4246e-05, -1.0774e-05,  8.0392e-06,  ..., -1.2651e-05,
         -7.2271e-06, -9.4995e-06],
        [-2.9564e-05, -2.2441e-05,  1.6674e-05,  ..., -2.5928e-05,
         -1.4931e-05, -1.8716e-05]], device='cuda:0')
Loss: 1.0403189659118652


Running epoch 0, step 661, batch 661
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   674,   369,   259],
        [    0,   301,   298,  ..., 10030,   300,  3780]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0280e-03,  1.2475e-03, -7.8632e-04,  ...,  8.0531e-04,
         -9.8961e-04, -8.2547e-05],
        [-1.3486e-05, -1.0200e-05,  7.2718e-06,  ..., -1.2070e-05,
         -7.3351e-06, -9.0599e-06],
        [-2.1726e-05, -1.6376e-05,  1.2130e-05,  ..., -1.8924e-05,
         -1.0796e-05, -1.3873e-05],
        [-1.7226e-05, -1.2934e-05,  9.6187e-06,  ..., -1.5140e-05,
         -8.6203e-06, -1.1317e-05],
        [-3.5942e-05, -2.7061e-05,  2.0102e-05,  ..., -3.1173e-05,
         -1.7896e-05, -2.2411e-05]], device='cuda:0')
Loss: 1.0600076913833618


Running epoch 0, step 662, batch 662
Sampled inputs[:2]: tensor([[    0,  6640,    13,  ...,   292,   221,   273],
        [    0,   607, 11059,  ...,  2081,  1194,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0603e-03,  1.4558e-03, -8.7690e-04,  ...,  8.7792e-04,
         -1.1245e-03, -5.5100e-05],
        [-1.5691e-05, -1.2003e-05,  8.4937e-06,  ..., -1.4082e-05,
         -8.4974e-06, -1.0535e-05],
        [-2.5272e-05, -1.9252e-05,  1.4156e-05,  ..., -2.2084e-05,
         -1.2487e-05, -1.6138e-05],
        [-2.0042e-05, -1.5214e-05,  1.1228e-05,  ..., -1.7673e-05,
         -9.9763e-06, -1.3165e-05],
        [-4.1783e-05, -3.1769e-05,  2.3440e-05,  ..., -3.6329e-05,
         -2.0698e-05, -2.6032e-05]], device='cuda:0')
Loss: 1.0449628829956055


Running epoch 0, step 663, batch 663
Sampled inputs[:2]: tensor([[   0,  342, 4014,  ...,  368,  408, 2105],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0842e-03,  1.5295e-03, -8.5981e-04,  ...,  8.0064e-04,
         -1.1616e-03, -9.0121e-06],
        [-1.7911e-05, -1.3731e-05,  9.6634e-06,  ..., -1.6063e-05,
         -9.7938e-06, -1.2085e-05],
        [-2.8834e-05, -2.2009e-05,  1.6108e-05,  ..., -2.5153e-05,
         -1.4372e-05, -1.8507e-05],
        [-2.2814e-05, -1.7360e-05,  1.2755e-05,  ..., -2.0102e-05,
         -1.1459e-05, -1.5065e-05],
        [-4.7803e-05, -3.6389e-05,  2.6748e-05,  ..., -4.1485e-05,
         -2.3901e-05, -2.9936e-05]], device='cuda:0')
Loss: 1.0665315389633179
Graident accumulation at epoch 0, step 663, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0031,  ..., -0.0025,  0.0228, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0153, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9382e-05,  2.0587e-04, -1.9279e-04,  ...,  9.8080e-05,
         -9.5040e-05, -2.9242e-05],
        [-1.5256e-05, -1.2469e-05,  7.5915e-06,  ..., -1.4659e-05,
         -7.2399e-06, -1.1017e-05],
        [ 1.1676e-05,  6.8806e-06, -5.6975e-06,  ...,  1.0610e-05,
          4.2068e-06,  8.1845e-06],
        [ 7.8782e-06,  1.5547e-05, -1.2415e-05,  ...,  1.5245e-05,
          1.4479e-05,  1.2092e-05],
        [-3.8070e-05, -2.9922e-05,  2.0390e-05,  ..., -3.5131e-05,
         -1.6791e-05, -2.6440e-05]], device='cuda:0')
optimizer state dict: tensor([[4.5881e-08, 3.4945e-08, 3.8587e-08,  ..., 1.4275e-08, 8.9646e-08,
         1.2483e-08],
        [6.6549e-11, 3.4165e-11, 7.8448e-12,  ..., 4.3721e-11, 5.8712e-12,
         1.3854e-11],
        [1.2249e-09, 6.2152e-10, 1.1456e-10,  ..., 1.0013e-09, 7.4333e-11,
         3.2442e-10],
        [1.8744e-10, 1.6627e-10, 7.7384e-11,  ..., 2.1245e-10, 7.5578e-11,
         1.0583e-10],
        [2.9685e-10, 1.5983e-10, 2.7150e-11,  ..., 2.1893e-10, 2.6109e-11,
         7.6463e-11]], device='cuda:0')
optimizer state dict: 83.0
lr: [1.5987512487190642e-05, 1.5987512487190642e-05]
scheduler_last_epoch: 83


Running epoch 0, step 664, batch 664
Sampled inputs[:2]: tensor([[   0,  257,  221,  ..., 1474, 2044,  300],
        [   0, 6010,  829,  ...,  668, 1784,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1053e-04, -9.1142e-04,  2.7659e-04,  ..., -5.0714e-04,
          9.7909e-04,  5.2374e-04],
        [-1.9670e-06, -1.3635e-06,  9.0525e-07,  ..., -2.0564e-06,
         -1.8030e-06, -1.9073e-06],
        [-3.1441e-06, -2.2054e-06,  1.6093e-06,  ..., -3.0249e-06,
         -2.4289e-06, -2.6971e-06],
        [-2.3991e-06, -1.6615e-06,  1.2070e-06,  ..., -2.3842e-06,
         -1.9521e-06, -2.1905e-06],
        [-4.9472e-06, -3.5018e-06,  2.5630e-06,  ..., -4.6492e-06,
         -3.6806e-06, -3.9935e-06]], device='cuda:0')
Loss: 1.059298038482666


Running epoch 0, step 665, batch 665
Sampled inputs[:2]: tensor([[    0,  1644,  1742,  ...,   287,  1704,  2044],
        [    0,    13, 30044,  ...,   381, 22105,    11]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2519e-04, -8.7093e-04,  2.0237e-04,  ..., -5.0949e-04,
          9.1351e-04,  6.0268e-04],
        [-4.1723e-06, -3.0175e-06,  2.0377e-06,  ..., -3.9414e-06,
         -2.9355e-06, -3.3230e-06],
        [-6.8098e-06, -4.9621e-06,  3.5763e-06,  ..., -6.0797e-06,
         -4.1649e-06, -4.9472e-06],
        [-5.2154e-06, -3.7625e-06,  2.7120e-06,  ..., -4.7684e-06,
         -3.3006e-06, -3.9786e-06],
        [-1.0818e-05, -7.9125e-06,  5.7220e-06,  ..., -9.5367e-06,
         -6.4820e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.0595307350158691


Running epoch 0, step 666, batch 666
Sampled inputs[:2]: tensor([[    0,  1626,     5,  ..., 10536,  1763,   292],
        [    0,  3167,   300,  ...,  1109,   490,  1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5465e-04, -5.3489e-04,  5.3338e-05,  ..., -3.2943e-04,
          6.4130e-04,  5.9330e-04],
        [-6.5565e-06, -4.7311e-06,  3.1926e-06,  ..., -6.0275e-06,
         -4.3586e-06, -5.0366e-06],
        [-1.0595e-05, -7.7039e-06,  5.4985e-06,  ..., -9.2983e-06,
         -6.2659e-06, -7.5251e-06],
        [-8.0913e-06, -5.8189e-06,  4.1649e-06,  ..., -7.2569e-06,
         -4.9099e-06, -6.0052e-06],
        [-1.6987e-05, -1.2383e-05,  8.8513e-06,  ..., -1.4752e-05,
         -9.9093e-06, -1.1578e-05]], device='cuda:0')
Loss: 1.0539863109588623


Running epoch 0, step 667, batch 667
Sampled inputs[:2]: tensor([[   0, 1596, 2700,  ...,  943,  266, 4086],
        [   0,  271,  266,  ..., 3795,  908,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5465e-04, -2.0157e-04, -2.6709e-04,  ..., -1.0679e-04,
          2.1469e-04,  6.5808e-04],
        [-8.9258e-06, -6.4000e-06,  4.2878e-06,  ..., -8.0094e-06,
         -5.7146e-06, -6.6534e-06],
        [-1.4529e-05, -1.0490e-05,  7.4208e-06,  ..., -1.2502e-05,
         -8.3670e-06, -1.0088e-05],
        [-1.0952e-05, -7.8306e-06,  5.5507e-06,  ..., -9.6112e-06,
         -6.4448e-06, -7.9274e-06],
        [-2.3216e-05, -1.6794e-05,  1.1921e-05,  ..., -1.9789e-05,
         -1.3232e-05, -1.5542e-05]], device='cuda:0')
Loss: 1.0503180027008057


Running epoch 0, step 668, batch 668
Sampled inputs[:2]: tensor([[    0,    18,   271,  ...,  4868,   963,   271],
        [    0,    13, 23070,  ...,   266,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4068e-04, -8.1282e-05, -4.3217e-04,  ..., -7.4122e-05,
          2.9326e-05,  6.3942e-04],
        [-1.1012e-05, -7.9721e-06,  5.3607e-06,  ..., -1.0021e-05,
         -7.1451e-06, -8.2925e-06],
        [-1.7837e-05, -1.2934e-05,  9.2462e-06,  ..., -1.5482e-05,
         -1.0349e-05, -1.2442e-05],
        [-1.3605e-05, -9.7826e-06,  7.0035e-06,  ..., -1.2070e-05,
         -8.0913e-06, -9.9093e-06],
        [-2.8521e-05, -2.0668e-05,  1.4871e-05,  ..., -2.4527e-05,
         -1.6376e-05, -1.9163e-05]], device='cuda:0')
Loss: 1.0583009719848633


Running epoch 0, step 669, batch 669
Sampled inputs[:2]: tensor([[   0,    9,  287,  ...,  259, 8244, 1143],
        [   0,  292,   41,  ...,  271, 9536,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0013e-04,  2.6101e-04, -6.5879e-04,  ...,  8.5858e-06,
          2.9326e-05,  6.3520e-04],
        [-1.3366e-05, -9.7379e-06,  6.5155e-06,  ..., -1.2048e-05,
         -8.6650e-06, -9.9912e-06],
        [-2.1636e-05, -1.5780e-05,  1.1198e-05,  ..., -1.8641e-05,
         -1.2599e-05, -1.5035e-05],
        [-1.6436e-05, -1.1884e-05,  8.4490e-06,  ..., -1.4469e-05,
         -9.7826e-06, -1.1906e-05],
        [-3.4600e-05, -2.5198e-05,  1.8016e-05,  ..., -2.9564e-05,
         -1.9968e-05, -2.3216e-05]], device='cuda:0')
Loss: 1.0487951040267944


Running epoch 0, step 670, batch 670
Sampled inputs[:2]: tensor([[   0,  298, 2587,  ...,  298,  894,  496],
        [   0,   14, 1845,  ...,  806,  352,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2368e-04,  4.5947e-04, -7.0177e-04,  ...,  2.2780e-06,
         -1.3944e-04,  6.9464e-04],
        [-1.5721e-05, -1.1481e-05,  7.6406e-06,  ..., -1.4029e-05,
         -1.0006e-05, -1.1638e-05],
        [-2.5541e-05, -1.8686e-05,  1.3150e-05,  ..., -2.1875e-05,
         -1.4700e-05, -1.7643e-05],
        [-1.9282e-05, -1.3985e-05,  9.8646e-06,  ..., -1.6838e-05,
         -1.1310e-05, -1.3843e-05],
        [-4.0829e-05, -2.9817e-05,  2.1175e-05,  ..., -3.4720e-05,
         -2.3335e-05, -2.7269e-05]], device='cuda:0')
Loss: 1.047898530960083


Running epoch 0, step 671, batch 671
Sampled inputs[:2]: tensor([[   0,  342,  726,  ...,   12,  895,  367],
        [   0, 1811,  278,  ...,  278,  259, 4617]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4608e-04,  6.2406e-04, -8.6647e-04,  ...,  1.3455e-04,
         -3.5897e-04,  4.2775e-04],
        [-1.7717e-05, -1.3016e-05,  8.6986e-06,  ..., -1.6041e-05,
         -1.1556e-05, -1.3381e-05],
        [-2.8700e-05, -2.1085e-05,  1.4953e-05,  ..., -2.4855e-05,
         -1.6831e-05, -2.0102e-05],
        [-2.1815e-05, -1.5907e-05,  1.1303e-05,  ..., -1.9297e-05,
         -1.3091e-05, -1.5944e-05],
        [-4.5896e-05, -3.3662e-05,  2.4095e-05,  ..., -3.9428e-05,
         -2.6718e-05, -3.1069e-05]], device='cuda:0')
Loss: 1.0548101663589478
Graident accumulation at epoch 0, step 671, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0031,  ..., -0.0025,  0.0228, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0153, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1705e-04,  2.4769e-04, -2.6016e-04,  ...,  1.0173e-04,
         -1.2143e-04,  1.6457e-05],
        [-1.5502e-05, -1.2524e-05,  7.7022e-06,  ..., -1.4797e-05,
         -7.6715e-06, -1.1254e-05],
        [ 7.6380e-06,  4.0840e-06, -3.6324e-06,  ...,  7.0633e-06,
          2.1030e-06,  5.3559e-06],
        [ 4.9089e-06,  1.2401e-05, -1.0044e-05,  ...,  1.1791e-05,
          1.1722e-05,  9.2886e-06],
        [-3.8853e-05, -3.0296e-05,  2.0761e-05,  ..., -3.5561e-05,
         -1.7783e-05, -2.6903e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6133e-08, 3.5300e-08, 3.9300e-08,  ..., 1.4279e-08, 8.9685e-08,
         1.2653e-08],
        [6.6796e-11, 3.4301e-11, 7.9126e-12,  ..., 4.3935e-11, 5.9989e-12,
         1.4019e-11],
        [1.2245e-09, 6.2135e-10, 1.1467e-10,  ..., 1.0009e-09, 7.4542e-11,
         3.2450e-10],
        [1.8773e-10, 1.6636e-10, 7.7434e-11,  ..., 2.1261e-10, 7.5674e-11,
         1.0597e-10],
        [2.9866e-10, 1.6080e-10, 2.7703e-11,  ..., 2.2026e-10, 2.6797e-11,
         7.7352e-11]], device='cuda:0')
optimizer state dict: 84.0
lr: [1.5888048657910018e-05, 1.5888048657910018e-05]
scheduler_last_epoch: 84


Running epoch 0, step 672, batch 672
Sampled inputs[:2]: tensor([[    0,   270,   472,  ...,   292,    73,    14],
        [    0,  4672,   278,  ...,    13,   265, 49987]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5566e-04, -1.2384e-04,  8.2580e-05,  ..., -3.6438e-05,
          1.0884e-04,  1.5604e-04],
        [-2.3246e-06, -1.6913e-06,  1.1176e-06,  ..., -1.8701e-06,
         -1.2442e-06, -1.5944e-06],
        [-3.9041e-06, -2.8610e-06,  1.9372e-06,  ..., -3.0845e-06,
         -1.9670e-06, -2.5630e-06],
        [-2.9951e-06, -2.1607e-06,  1.4901e-06,  ..., -2.3991e-06,
         -1.5125e-06, -2.0415e-06],
        [-6.0499e-06, -4.4405e-06,  3.0100e-06,  ..., -4.7684e-06,
         -3.0845e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.0832377672195435


Running epoch 0, step 673, batch 673
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   382,    17,    13],
        [    0,   266, 28695,  ...,   278,   266,  6087]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3209e-04, -2.2247e-04,  1.8110e-05,  ..., -6.8457e-05,
          1.1641e-04,  1.2915e-04],
        [-4.5598e-06, -3.2634e-06,  2.1756e-06,  ..., -3.8669e-06,
         -2.5928e-06, -3.2485e-06],
        [-7.5549e-06, -5.4240e-06,  3.7402e-06,  ..., -6.2287e-06,
         -3.9786e-06, -5.0962e-06],
        [-5.7817e-06, -4.0978e-06,  2.8610e-06,  ..., -4.8429e-06,
         -3.0696e-06, -4.0531e-06],
        [-1.1772e-05, -8.4639e-06,  5.8413e-06,  ..., -9.6560e-06,
         -6.2585e-06, -7.6890e-06]], device='cuda:0')
Loss: 1.072514295578003


Running epoch 0, step 674, batch 674
Sampled inputs[:2]: tensor([[   0,  342,  266,  ..., 4998, 4756, 5139],
        [   0,   16,   14,  ...,  300, 9283,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3179e-04,  5.7092e-05, -1.4048e-04,  ...,  1.6886e-04,
         -3.0919e-04,  4.2397e-05],
        [-6.7502e-06, -4.8056e-06,  3.0957e-06,  ..., -5.7891e-06,
         -4.0084e-06, -4.9919e-06],
        [-1.1146e-05, -7.9721e-06,  5.4091e-06,  ..., -9.2238e-06,
         -6.0499e-06, -7.7188e-06],
        [-8.5533e-06, -6.0499e-06,  4.1351e-06,  ..., -7.2271e-06,
         -4.7237e-06, -6.1989e-06],
        [-1.7226e-05, -1.2368e-05,  8.4043e-06,  ..., -1.4186e-05,
         -9.4324e-06, -1.1504e-05]], device='cuda:0')
Loss: 1.0400638580322266


Running epoch 0, step 675, batch 675
Sampled inputs[:2]: tensor([[    0,   273,   298,  ..., 23554,    12,  1530],
        [    0,   361,  1224,  ...,  4401,  4261,  1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9425e-04,  8.5798e-05,  6.8538e-05,  ...,  1.3136e-04,
         -1.9198e-04,  1.3026e-04],
        [-9.1791e-06, -6.5044e-06,  4.1015e-06,  ..., -7.7710e-06,
         -5.5581e-06, -6.7875e-06],
        [-1.5199e-05, -1.0848e-05,  7.1898e-06,  ..., -1.2472e-05,
         -8.5086e-06, -1.0595e-05],
        [ 2.3371e-04,  1.8502e-04, -1.7493e-04,  ...,  2.3132e-04,
          1.2883e-04,  1.7249e-04],
        [-2.3454e-05, -1.6809e-05,  1.1176e-05,  ..., -1.9163e-05,
         -1.3217e-05, -1.5765e-05]], device='cuda:0')
Loss: 1.0863252878189087


Running epoch 0, step 676, batch 676
Sampled inputs[:2]: tensor([[    0, 22390,   292,  ...,  3552,   278,   317],
        [    0,   273,    14,  ...,   271,   266, 25408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1700e-04, -2.3174e-05,  1.5836e-04,  ..., -3.5998e-06,
          6.0214e-05,  2.6944e-04],
        [-1.1548e-05, -8.2403e-06,  5.2117e-06,  ..., -9.7677e-06,
         -6.8471e-06, -8.4192e-06],
        [-1.9103e-05, -1.3724e-05,  9.1270e-06,  ..., -1.5691e-05,
         -1.0490e-05, -1.3143e-05],
        [ 2.3082e-04,  1.8291e-04, -1.7351e-04,  ...,  2.2893e-04,
          1.2736e-04,  1.7054e-04],
        [-2.9624e-05, -2.1368e-05,  1.4231e-05,  ..., -2.4229e-05,
         -1.6376e-05, -1.9670e-05]], device='cuda:0')
Loss: 1.0845463275909424


Running epoch 0, step 677, batch 677
Sampled inputs[:2]: tensor([[    0,   266,  2623,  ...,     5, 10781,   287],
        [    0,   292,   474,  ...,  1085,   494,  2665]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7949e-04, -2.9257e-04,  2.0520e-04,  ..., -1.3409e-04,
          3.0136e-04,  3.6826e-04],
        [-1.3545e-05, -9.5293e-06,  6.1430e-06,  ..., -1.1720e-05,
         -8.5533e-06, -1.0356e-05],
        [-2.2322e-05, -1.5870e-05,  1.0774e-05,  ..., -1.8626e-05,
         -1.2904e-05, -1.5914e-05],
        [ 2.2832e-04,  1.8127e-04, -1.7223e-04,  ...,  2.2656e-04,
          1.2536e-04,  1.6822e-04],
        [-3.4511e-05, -2.4691e-05,  1.6734e-05,  ..., -2.8610e-05,
         -1.9997e-05, -2.3663e-05]], device='cuda:0')
Loss: 1.038150429725647


Running epoch 0, step 678, batch 678
Sampled inputs[:2]: tensor([[    0,  7303,    12,  ...,  1085,   413,   711],
        [    0, 23842,   342,  ...,   365,  4011, 10151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2954e-04, -2.1698e-04,  2.2720e-04,  ..., -1.0750e-04,
          3.3942e-04,  3.8687e-04],
        [-1.5706e-05, -1.1064e-05,  7.1488e-06,  ..., -1.3702e-05,
         -1.0028e-05, -1.2092e-05],
        [-2.5794e-05, -1.8358e-05,  1.2502e-05,  ..., -2.1636e-05,
         -1.5035e-05, -1.8463e-05],
        [ 2.2561e-04,  1.7937e-04, -1.7089e-04,  ...,  2.2414e-04,
          1.2365e-04,  1.6612e-04],
        [-3.9965e-05, -2.8595e-05,  1.9461e-05,  ..., -3.3319e-05,
         -2.3350e-05, -2.7508e-05]], device='cuda:0')
Loss: 1.0688419342041016


Running epoch 0, step 679, batch 679
Sampled inputs[:2]: tensor([[    0,  5583,   598,  ...,   199,   395,  6551],
        [    0,   970,    13,  ..., 13798,    14,  1841]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5807e-04, -2.2004e-04,  2.2720e-04,  ...,  1.4388e-05,
          1.3251e-04,  3.0702e-04],
        [-1.8030e-05, -1.2748e-05,  8.3037e-06,  ..., -1.5572e-05,
         -1.1213e-05, -1.3672e-05],
        [-2.9668e-05, -2.1175e-05,  1.4499e-05,  ..., -2.4691e-05,
         -1.6883e-05, -2.0981e-05],
        [ 2.2264e-04,  1.7724e-04, -1.6936e-04,  ...,  2.2177e-04,
          1.2223e-04,  1.6412e-04],
        [-4.5985e-05, -3.2976e-05,  2.2545e-05,  ..., -3.8058e-05,
         -2.6271e-05, -3.1307e-05]], device='cuda:0')
Loss: 1.0617715120315552
Graident accumulation at epoch 0, step 679, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0031,  ..., -0.0025,  0.0228, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.5397e-06,  2.0091e-04, -2.1142e-04,  ...,  9.2993e-05,
         -9.6039e-05,  4.5514e-05],
        [-1.5755e-05, -1.2546e-05,  7.7623e-06,  ..., -1.4875e-05,
         -8.0257e-06, -1.1495e-05],
        [ 3.9074e-06,  1.5581e-06, -1.8193e-06,  ...,  3.8879e-06,
          2.0444e-07,  2.7222e-06],
        [ 2.6682e-05,  2.8885e-05, -2.5975e-05,  ...,  3.2789e-05,
          2.2773e-05,  2.4772e-05],
        [-3.9566e-05, -3.0564e-05,  2.0939e-05,  ..., -3.5811e-05,
         -1.8632e-05, -2.7344e-05]], device='cuda:0')
optimizer state dict: tensor([[4.7005e-08, 3.5313e-08, 3.9312e-08,  ..., 1.4265e-08, 8.9613e-08,
         1.2735e-08],
        [6.7055e-11, 3.4429e-11, 7.9737e-12,  ..., 4.4133e-11, 6.1186e-12,
         1.4192e-11],
        [1.2242e-09, 6.2118e-10, 1.1476e-10,  ..., 1.0005e-09, 7.4753e-11,
         3.2462e-10],
        [2.3711e-10, 1.9761e-10, 1.0604e-10,  ..., 2.6158e-10, 9.0538e-11,
         1.3280e-10],
        [3.0048e-10, 1.6173e-10, 2.8184e-11,  ..., 2.2149e-10, 2.7460e-11,
         7.8254e-11]], device='cuda:0')
optimizer state dict: 85.0
lr: [1.5787685083396957e-05, 1.5787685083396957e-05]
scheduler_last_epoch: 85


Running epoch 0, step 680, batch 680
Sampled inputs[:2]: tensor([[    0,    12, 32425,  ...,   389,   221,   494],
        [    0,   494,   360,  ...,   391, 24104, 35211]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1816e-04,  2.2363e-04, -1.8989e-05,  ...,  5.8788e-05,
         -2.1905e-04,  4.9735e-06],
        [-2.3693e-06, -1.6019e-06,  1.0505e-06,  ..., -1.8850e-06,
         -1.1921e-06, -1.8328e-06],
        [-4.0531e-06, -2.7567e-06,  1.9073e-06,  ..., -3.1292e-06,
         -1.9222e-06, -2.9206e-06],
        [-2.9355e-06, -1.9819e-06,  1.3784e-06,  ..., -2.2948e-06,
         -1.3933e-06, -2.2203e-06],
        [-6.0201e-06, -4.1425e-06,  2.8461e-06,  ..., -4.6492e-06,
         -2.8908e-06, -4.1425e-06]], device='cuda:0')
Loss: 1.039974331855774


Running epoch 0, step 681, batch 681
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0,   638,  2708,  ..., 28492,  1814,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2696e-06,  1.9306e-04, -8.0872e-05,  ...,  4.8319e-05,
         -3.3195e-04,  4.6688e-06],
        [-4.6790e-06, -3.2187e-06,  2.2203e-06,  ..., -3.6880e-06,
         -2.2352e-06, -3.3602e-06],
        [-7.9572e-06, -5.5581e-06,  3.9786e-06,  ..., -6.1691e-06,
         -3.6359e-06, -5.4091e-06],
        [-5.9605e-06, -4.1127e-06,  2.9802e-06,  ..., -4.6492e-06,
         -2.6971e-06, -4.2170e-06],
        [-1.2070e-05, -8.4937e-06,  6.0350e-06,  ..., -9.3579e-06,
         -5.6177e-06, -7.8678e-06]], device='cuda:0')
Loss: 1.0738496780395508


Running epoch 0, step 682, batch 682
Sampled inputs[:2]: tensor([[    0,    17,   590,  ...,  1412,    35,  5015],
        [    0,  1371, 10516,  ...,  2456,    13,  6469]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4419e-05,  2.0383e-04, -3.5758e-05,  ..., -8.0968e-06,
         -2.3962e-04,  2.0927e-05],
        [-6.9737e-06, -4.8578e-06,  3.2559e-06,  ..., -5.5879e-06,
         -3.5167e-06, -5.0291e-06],
        [-1.1891e-05, -8.3596e-06,  5.8785e-06,  ..., -9.2983e-06,
         -5.6475e-06, -8.0317e-06],
        [-8.8960e-06, -6.1840e-06,  4.3809e-06,  ..., -7.0333e-06,
         -4.2245e-06, -6.2734e-06],
        [-1.8179e-05, -1.2875e-05,  9.0152e-06,  ..., -1.4216e-05,
         -8.7768e-06, -1.1802e-05]], device='cuda:0')
Loss: 1.0575953722000122


Running epoch 0, step 683, batch 683
Sampled inputs[:2]: tensor([[    0,    12,  2212,  ..., 12415,  2131,   287],
        [    0,  1529,   354,  ...,   709,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5534e-05,  3.6939e-04, -8.1525e-05,  ...,  8.5405e-05,
         -3.1159e-04,  1.2507e-06],
        [-9.1493e-06, -6.3553e-06,  4.3064e-06,  ..., -7.3835e-06,
         -4.6790e-06, -6.6906e-06],
        [-1.5616e-05, -1.0967e-05,  7.8008e-06,  ..., -1.2279e-05,
         -7.5027e-06, -1.0669e-05],
        [-1.1712e-05, -8.1360e-06,  5.8264e-06,  ..., -9.3281e-06,
         -5.6475e-06, -8.3745e-06],
        [-2.3842e-05, -1.6838e-05,  1.1936e-05,  ..., -1.8716e-05,
         -1.1638e-05, -1.5646e-05]], device='cuda:0')
Loss: 1.0554012060165405


Running epoch 0, step 684, batch 684
Sampled inputs[:2]: tensor([[    0,  4137,   300,  ...,  2579,   278,   266],
        [    0,   508,  1548,  ...,   494, 10792,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9400e-05,  2.4793e-04, -2.0705e-05,  ...,  3.0785e-05,
         -1.8820e-04,  9.4974e-05],
        [-1.1176e-05, -7.7412e-06,  5.1558e-06,  ..., -9.2089e-06,
         -5.9083e-06, -8.3968e-06],
        [-1.9148e-05, -1.3366e-05,  9.4846e-06,  ..., -1.5214e-05,
         -9.3356e-06, -1.3262e-05],
        [-1.4409e-05, -9.9465e-06,  7.0781e-06,  ..., -1.1638e-05,
         -7.1004e-06, -1.0505e-05],
        [-2.9266e-05, -2.0519e-05,  1.4558e-05,  ..., -2.3127e-05,
         -1.4439e-05, -1.9386e-05]], device='cuda:0')
Loss: 1.043903112411499


Running epoch 0, step 685, batch 685
Sampled inputs[:2]: tensor([[    0,  3306,  4057,  ...,   287,   266,  1692],
        [    0,  3611, 10765,  ...,   271,  4317,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0191e-05,  3.7670e-04,  3.8519e-06,  ...,  1.6943e-04,
         -1.6520e-04,  1.1323e-04],
        [-1.3500e-05, -9.3654e-06,  6.2659e-06,  ..., -1.1116e-05,
         -7.0408e-06, -1.0021e-05],
        [-2.2992e-05, -1.6093e-05,  1.1437e-05,  ..., -1.8328e-05,
         -1.1146e-05, -1.5810e-05],
        [-1.7419e-05, -1.2048e-05,  8.5980e-06,  ..., -1.4096e-05,
         -8.5011e-06, -1.2577e-05],
        [-3.5286e-05, -2.4810e-05,  1.7628e-05,  ..., -2.8014e-05,
         -1.7330e-05, -2.3231e-05]], device='cuda:0')
Loss: 1.0550220012664795


Running epoch 0, step 686, batch 686
Sampled inputs[:2]: tensor([[    0,    14,  3445,  ...,   298,   527,  2732],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2662e-05,  3.7506e-04,  4.0010e-05,  ...,  2.2764e-04,
         -2.5507e-04,  6.3855e-05],
        [-1.5557e-05, -1.0893e-05,  7.2792e-06,  ..., -1.2957e-05,
         -8.2627e-06, -1.1683e-05],
        [-2.6509e-05, -1.8686e-05,  1.3277e-05,  ..., -2.1309e-05,
         -1.3031e-05, -1.8403e-05],
        [-2.0087e-05, -1.4015e-05,  9.9987e-06,  ..., -1.6436e-05,
         -9.9689e-06, -1.4678e-05],
        [-4.0591e-05, -2.8744e-05,  2.0400e-05,  ..., -3.2485e-05,
         -2.0191e-05, -2.6956e-05]], device='cuda:0')
Loss: 1.0550904273986816


Running epoch 0, step 687, batch 687
Sampled inputs[:2]: tensor([[   0, 1690, 2558,  ..., 2025,   12,  266],
        [   0, 4868, 1027,  ...,  409, 3047, 2953]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3300e-05,  2.4317e-04,  7.8643e-05,  ...,  1.8700e-04,
         -9.2433e-05,  2.0095e-04],
        [-1.7926e-05, -1.2591e-05,  8.3968e-06,  ..., -1.4804e-05,
         -9.3728e-06, -1.3255e-05],
        [-3.0562e-05, -2.1622e-05,  1.5259e-05,  ..., -2.4438e-05,
         -1.4879e-05, -2.0996e-05],
        [-2.3142e-05, -1.6205e-05,  1.1489e-05,  ..., -1.8805e-05,
         -1.1340e-05, -1.6689e-05],
        [-4.6790e-05, -3.3244e-05,  2.3440e-05,  ..., -3.7313e-05,
         -2.3097e-05, -3.0801e-05]], device='cuda:0')
Loss: 1.0836234092712402
Graident accumulation at epoch 0, step 687, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0031,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0070, -0.0011],
        [-0.0161,  0.0150, -0.0278,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.4427e-07,  2.0514e-04, -1.8242e-04,  ...,  1.0239e-04,
         -9.5678e-05,  6.1057e-05],
        [-1.5972e-05, -1.2551e-05,  7.8258e-06,  ..., -1.4868e-05,
         -8.1604e-06, -1.1671e-05],
        [ 4.6041e-07, -7.5983e-07, -1.1146e-07,  ...,  1.0553e-06,
         -1.3039e-06,  3.5040e-07],
        [ 2.1700e-05,  2.4376e-05, -2.2229e-05,  ...,  2.7630e-05,
          1.9361e-05,  2.0625e-05],
        [-4.0288e-05, -3.0832e-05,  2.1189e-05,  ..., -3.5961e-05,
         -1.9079e-05, -2.7689e-05]], device='cuda:0')
optimizer state dict: tensor([[4.6967e-08, 3.5337e-08, 3.9279e-08,  ..., 1.4286e-08, 8.9532e-08,
         1.2763e-08],
        [6.7309e-11, 3.4553e-11, 8.0362e-12,  ..., 4.4308e-11, 6.2003e-12,
         1.4353e-11],
        [1.2239e-09, 6.2102e-10, 1.1488e-10,  ..., 1.0001e-09, 7.4900e-11,
         3.2473e-10],
        [2.3741e-10, 1.9767e-10, 1.0607e-10,  ..., 2.6168e-10, 9.0576e-11,
         1.3295e-10],
        [3.0236e-10, 1.6267e-10, 2.8705e-11,  ..., 2.2266e-10, 2.7966e-11,
         7.9125e-11]], device='cuda:0')
optimizer state dict: 86.0
lr: [1.5686437100081734e-05, 1.5686437100081734e-05]
scheduler_last_epoch: 86


Running epoch 0, step 688, batch 688
Sampled inputs[:2]: tensor([[   0,   12,  287,  ...,  266, 2105, 3925],
        [   0, 8416,  669,  ...,  298,  894,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1259e-04,  2.3698e-06,  1.1455e-04,  ..., -4.2061e-05,
          4.7045e-05,  3.0418e-05],
        [-2.3395e-06, -1.6317e-06,  1.1474e-06,  ..., -1.7807e-06,
         -1.0654e-06, -1.5274e-06],
        [-4.0531e-06, -2.8312e-06,  2.0415e-06,  ..., -3.0249e-06,
         -1.7732e-06, -2.5332e-06],
        [-3.1143e-06, -2.1607e-06,  1.5870e-06,  ..., -2.3395e-06,
         -1.3486e-06, -2.0415e-06],
        [-6.3181e-06, -4.4405e-06,  3.1739e-06,  ..., -4.7386e-06,
         -2.8163e-06, -3.7700e-06]], device='cuda:0')
Loss: 1.0699466466903687


Running epoch 0, step 689, batch 689
Sampled inputs[:2]: tensor([[   0,  342,  266,  ...,  586, 1944,  271],
        [   0,   12,  344,  ...,   14, 2295,  516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3418e-04,  2.2405e-05,  7.6228e-05,  ..., -2.7301e-06,
          5.6821e-05,  5.4063e-05],
        [-4.5151e-06, -3.1292e-06,  2.3022e-06,  ..., -3.5614e-06,
         -2.0489e-06, -3.1292e-06],
        [-7.7933e-06, -5.4538e-06,  4.1276e-06,  ..., -6.0201e-06,
         -3.3826e-06, -5.1260e-06],
        [-6.1542e-06, -4.2468e-06,  3.2783e-06,  ..., -4.8131e-06,
         -2.6524e-06, -4.2766e-06],
        [-1.2130e-05, -8.5235e-06,  6.3926e-06,  ..., -9.3877e-06,
         -5.3793e-06, -7.6145e-06]], device='cuda:0')
Loss: 1.054115891456604


Running epoch 0, step 690, batch 690
Sampled inputs[:2]: tensor([[    0,  2827,  5744,  ...,   365,   513,    13],
        [    0, 16286,  5356,  ...,   590,  2161,     5]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4266e-04, -1.0618e-04,  1.0708e-04,  ..., -1.2289e-05,
          1.9549e-04,  1.4930e-04],
        [-6.8098e-06, -4.7013e-06,  3.4347e-06,  ..., -5.3495e-06,
         -3.0994e-06, -4.7311e-06],
        [-1.1727e-05, -8.1956e-06,  6.1542e-06,  ..., -9.0599e-06,
         -5.1484e-06, -7.7486e-06],
        [-9.2536e-06, -6.3628e-06,  4.8801e-06,  ..., -7.2122e-06,
         -4.0084e-06, -6.4224e-06],
        [-1.8299e-05, -1.2845e-05,  9.5516e-06,  ..., -1.4156e-05,
         -8.2105e-06, -1.1548e-05]], device='cuda:0')
Loss: 1.0761929750442505


Running epoch 0, step 691, batch 691
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ...,   298,   365,  4920],
        [    0,   902, 11331,  ...,  1795,   365,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1578e-04,  1.0408e-05,  7.2562e-05,  ...,  1.2770e-05,
          3.6467e-05,  1.2561e-04],
        [-9.0301e-06, -6.1765e-06,  4.4405e-06,  ..., -7.1973e-06,
         -4.2766e-06, -6.3851e-06],
        [-1.5438e-05, -1.0714e-05,  7.9721e-06,  ..., -1.2055e-05,
         -6.9812e-06, -1.0297e-05],
        [-1.2293e-05, -8.3894e-06,  6.3553e-06,  ..., -9.7007e-06,
         -5.5283e-06, -8.6427e-06],
        [-2.4229e-05, -1.6898e-05,  1.2442e-05,  ..., -1.8895e-05,
         -1.1176e-05, -1.5423e-05]], device='cuda:0')
Loss: 1.0483965873718262


Running epoch 0, step 692, batch 692
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0,   266,  1034,  ...,  6153,   263,   472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8236e-04, -2.0306e-05,  8.6011e-05,  ...,  2.1275e-05,
          6.5163e-05,  1.5547e-05],
        [-1.1310e-05, -7.7188e-06,  5.6401e-06,  ..., -8.9929e-06,
         -5.2974e-06, -7.9721e-06],
        [-1.9282e-05, -1.3381e-05,  1.0073e-05,  ..., -1.5035e-05,
         -8.6427e-06, -1.2800e-05],
        [-1.5348e-05, -1.0461e-05,  8.0317e-06,  ..., -1.2100e-05,
         -6.8247e-06, -1.0759e-05],
        [-3.0279e-05, -2.1100e-05,  1.5706e-05,  ..., -2.3574e-05,
         -1.3843e-05, -1.9178e-05]], device='cuda:0')
Loss: 1.0705071687698364


Running epoch 0, step 693, batch 693
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,  1145,    13,  ...,   721,  1119,  3495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6282e-04,  4.5907e-05, -6.9008e-05,  ...,  5.9878e-05,
         -1.2940e-04, -9.9994e-05],
        [-1.3575e-05, -9.2536e-06,  6.6981e-06,  ..., -1.0766e-05,
         -6.4448e-06, -9.6485e-06],
        [-2.3156e-05, -1.6063e-05,  1.1981e-05,  ..., -1.8016e-05,
         -1.0520e-05, -1.5482e-05],
        [-1.8299e-05, -1.2457e-05,  9.4697e-06,  ..., -1.4380e-05,
         -8.2627e-06, -1.2904e-05],
        [-3.6359e-05, -2.5332e-05,  1.8686e-05,  ..., -2.8223e-05,
         -1.6838e-05, -2.3201e-05]], device='cuda:0')
Loss: 1.0564558506011963


Running epoch 0, step 694, batch 694
Sampled inputs[:2]: tensor([[   0, 3152, 1385,  ..., 1403,  518, 2088],
        [   0,  300, 5631,  ..., 2278, 2669, 3011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3448e-04, -6.6776e-05, -1.4385e-05,  ...,  6.4459e-05,
         -2.0435e-04, -1.4095e-04],
        [-1.5795e-05, -1.0751e-05,  7.8753e-06,  ..., -1.2495e-05,
         -7.4655e-06, -1.1213e-05],
        [-2.6956e-05, -1.8686e-05,  1.4082e-05,  ..., -2.0951e-05,
         -1.2226e-05, -1.8045e-05],
        [-2.1398e-05, -1.4558e-05,  1.1183e-05,  ..., -1.6794e-05,
         -9.6336e-06, -1.5095e-05],
        [-4.2379e-05, -2.9504e-05,  2.1994e-05,  ..., -3.2872e-05,
         -1.9640e-05, -2.7075e-05]], device='cuda:0')
Loss: 1.0520853996276855


Running epoch 0, step 695, batch 695
Sampled inputs[:2]: tensor([[   0, 2771, 2070,  ...,  221,  396,  298],
        [   0,  369,  726,  ...,  292,  221,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1118e-03, -2.1196e-04,  1.0064e-05,  ..., -5.7100e-05,
         -3.1938e-04,  1.2433e-04],
        [-1.7986e-05, -1.2189e-05,  9.0301e-06,  ..., -1.4216e-05,
         -8.5384e-06, -1.2934e-05],
        [-3.0711e-05, -2.1219e-05,  1.6138e-05,  ..., -2.3827e-05,
         -1.3970e-05, -2.0787e-05],
        [ 5.2622e-05,  4.6656e-05, -2.7463e-05,  ...,  6.2821e-05,
          1.8027e-05,  4.1998e-05],
        [-4.8220e-05, -3.3468e-05,  2.5168e-05,  ..., -3.7342e-05,
         -2.2396e-05, -3.1099e-05]], device='cuda:0')
Loss: 1.059178352355957
Graident accumulation at epoch 0, step 695, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0231,  0.0069, -0.0011],
        [-0.0161,  0.0151, -0.0279,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1185e-04,  1.6343e-04, -1.6317e-04,  ...,  8.6444e-05,
         -1.1805e-04,  6.7385e-05],
        [-1.6173e-05, -1.2515e-05,  7.9462e-06,  ..., -1.4802e-05,
         -8.1982e-06, -1.1798e-05],
        [-2.6568e-06, -2.8058e-06,  1.5135e-06,  ..., -1.4329e-06,
         -2.5705e-06, -1.7634e-06],
        [ 2.4792e-05,  2.6604e-05, -2.2752e-05,  ...,  3.1149e-05,
          1.9228e-05,  2.2763e-05],
        [-4.1081e-05, -3.1095e-05,  2.1587e-05,  ..., -3.6099e-05,
         -1.9410e-05, -2.8030e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8156e-08, 3.5347e-08, 3.9240e-08,  ..., 1.4275e-08, 8.9544e-08,
         1.2765e-08],
        [6.7565e-11, 3.4667e-11, 8.1097e-12,  ..., 4.4466e-11, 6.2670e-12,
         1.4506e-11],
        [1.2236e-09, 6.2085e-10, 1.1503e-10,  ..., 9.9970e-10, 7.5020e-11,
         3.2484e-10],
        [2.3994e-10, 1.9965e-10, 1.0671e-10,  ..., 2.6536e-10, 9.0810e-11,
         1.3458e-10],
        [3.0439e-10, 1.6363e-10, 2.9310e-11,  ..., 2.2384e-10, 2.8440e-11,
         8.0013e-11]], device='cuda:0')
optimizer state dict: 87.0
lr: [1.5584320179540008e-05, 1.5584320179540008e-05]
scheduler_last_epoch: 87


Running epoch 0, step 696, batch 696
Sampled inputs[:2]: tensor([[   0, 5902,  518,  ..., 3126,   12,  497],
        [   0,  342,  721,  ..., 2429,   14,  475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6309e-04, -6.0541e-05,  8.5619e-05,  ..., -1.8480e-04,
          2.6141e-04,  7.5691e-05],
        [-1.9819e-06, -1.3933e-06,  1.1325e-06,  ..., -1.7509e-06,
         -1.0356e-06, -1.6093e-06],
        [-3.3826e-06, -2.4289e-06,  2.0266e-06,  ..., -2.8461e-06,
         -1.6242e-06, -2.4438e-06],
        [-2.8163e-06, -1.9670e-06,  1.7136e-06,  ..., -2.4438e-06,
         -1.3709e-06, -2.2501e-06],
        [-5.3346e-06, -3.8445e-06,  3.1441e-06,  ..., -4.4405e-06,
         -2.6077e-06, -3.5763e-06]], device='cuda:0')
Loss: 1.0329439640045166


Running epoch 0, step 697, batch 697
Sampled inputs[:2]: tensor([[    0,   515,   352,  ..., 21190,  1871,   950],
        [    0,   259,  1380,  ...,   287, 10221,   280]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5358e-04, -1.0731e-04,  1.1732e-04,  ..., -3.7567e-04,
          4.8215e-04,  6.4962e-05],
        [-4.1574e-06, -2.8983e-06,  2.2426e-06,  ..., -3.5092e-06,
         -1.9893e-06, -3.0547e-06],
        [ 9.9360e-05,  4.0619e-05, -2.8775e-05,  ...,  7.1795e-05,
          7.5131e-05,  4.4968e-05],
        [-5.9307e-06, -4.1127e-06,  3.3751e-06,  ..., -4.9323e-06,
         -2.6748e-06, -4.2915e-06],
        [-1.1355e-05, -8.1062e-06,  6.3628e-06,  ..., -9.1493e-06,
         -5.2154e-06, -7.0482e-06]], device='cuda:0')
Loss: 1.0473700761795044


Running epoch 0, step 698, batch 698
Sampled inputs[:2]: tensor([[   0,  395, 5949,  ...,  341,   13,  635],
        [   0, 6957,  271,  ..., 9094,  266, 4320]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4515e-04, -9.1056e-05,  1.9721e-05,  ..., -3.8218e-04,
          4.6055e-04, -1.7633e-04],
        [-6.3926e-06, -4.4405e-06,  3.3751e-06,  ..., -5.2527e-06,
         -2.9504e-06, -4.4778e-06],
        [ 9.5515e-05,  3.7937e-05, -2.6763e-05,  ...,  6.8830e-05,
          7.3529e-05,  4.2673e-05],
        [-9.0599e-06, -6.2585e-06,  5.0217e-06,  ..., -7.3612e-06,
         -3.9637e-06, -6.2734e-06],
        [-1.7613e-05, -1.2517e-05,  9.6411e-06,  ..., -1.3977e-05,
         -7.9125e-06, -1.0639e-05]], device='cuda:0')
Loss: 1.0645910501480103


Running epoch 0, step 699, batch 699
Sampled inputs[:2]: tensor([[   0,  767, 9289,  ...,  494,  287, 8957],
        [   0,  259, 5918,  ...,  508, 3433, 1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4226e-04, -4.7570e-05,  2.1239e-04,  ..., -6.5529e-04,
          4.4522e-04,  5.2322e-05],
        [-8.5086e-06, -5.9083e-06,  4.4033e-06,  ..., -6.9663e-06,
         -3.9935e-06, -6.0648e-06],
        [ 9.1641e-05,  3.5225e-05, -2.4767e-05,  ...,  6.5820e-05,
          7.1726e-05,  4.0036e-05],
        [-1.2025e-05, -8.2999e-06,  6.5416e-06,  ..., -9.7305e-06,
         -5.3421e-06, -8.4341e-06],
        [-2.3752e-05, -1.6838e-05,  1.2830e-05,  ..., -1.8716e-05,
         -1.0803e-05, -1.4603e-05]], device='cuda:0')
Loss: 1.0404268503189087


Running epoch 0, step 700, batch 700
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,  271, 4111, 1188],
        [   0, 2025,  287,  ...,  381, 1487, 3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6961e-04, -7.2541e-05,  2.5062e-04,  ..., -6.5343e-04,
          4.3916e-04,  1.2752e-04],
        [-1.0833e-05, -7.5996e-06,  5.5730e-06,  ..., -8.8438e-06,
         -5.1409e-06, -7.4655e-06],
        [ 8.7558e-05,  3.2214e-05, -2.2665e-05,  ...,  6.2586e-05,
          6.9774e-05,  3.7696e-05],
        [-1.5214e-05, -1.0625e-05,  8.1956e-06,  ..., -1.2293e-05,
         -6.8545e-06, -1.0371e-05],
        [-3.0339e-05, -2.1726e-05,  1.6212e-05,  ..., -2.3991e-05,
         -1.4037e-05, -1.8254e-05]], device='cuda:0')
Loss: 1.097095251083374


Running epoch 0, step 701, batch 701
Sampled inputs[:2]: tensor([[    0, 26396,    83,  ...,   292,    18,   590],
        [    0,    12,   287,  ...,    12,  5576,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2935e-04,  1.4400e-04,  2.4831e-04,  ..., -4.8707e-04,
          1.5971e-04,  1.7574e-05],
        [-1.2711e-05, -8.8811e-06,  6.6087e-06,  ..., -1.0632e-05,
         -6.5342e-06, -9.2313e-06],
        [ 8.4295e-05,  2.9920e-05, -2.0728e-05,  ...,  5.9710e-05,
          6.7614e-05,  3.5044e-05],
        [-1.8030e-05, -1.2524e-05,  9.8869e-06,  ..., -1.4931e-05,
         -8.8513e-06, -1.2979e-05],
        [-3.5465e-05, -2.5377e-05,  1.9237e-05,  ..., -2.8431e-05,
         -1.7434e-05, -2.2098e-05]], device='cuda:0')
Loss: 1.0516294240951538


Running epoch 0, step 702, batch 702
Sampled inputs[:2]: tensor([[   0, 3308,  259,  ...,   14, 6349, 1389],
        [   0, 1142,   87,  ..., 2273,  287,  829]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2885e-04,  1.6016e-04,  3.4165e-04,  ..., -4.6193e-04,
          1.5406e-04,  1.9032e-04],
        [-1.4842e-05, -1.0341e-05,  7.6741e-06,  ..., -1.2450e-05,
         -7.7412e-06, -1.0923e-05],
        [ 8.0555e-05,  2.7297e-05, -1.8776e-05,  ...,  5.6656e-05,
          6.5617e-05,  3.2347e-05],
        [-2.1026e-05, -1.4581e-05,  1.1481e-05,  ..., -1.7479e-05,
         -1.0490e-05, -1.5363e-05],
        [-4.1336e-05, -2.9519e-05,  2.2277e-05,  ..., -3.3170e-05,
         -2.0593e-05, -2.6062e-05]], device='cuda:0')
Loss: 1.0654674768447876


Running epoch 0, step 703, batch 703
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0,   266,  1422,  ...,   446,  1992,   586]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0623e-04,  1.9519e-04,  2.6166e-04,  ..., -4.2462e-04,
          3.3863e-05,  1.5453e-04],
        [-1.7017e-05, -1.1943e-05,  8.7991e-06,  ..., -1.4238e-05,
         -8.8587e-06, -1.2428e-05],
        [ 7.6755e-05,  2.4481e-05, -1.6735e-05,  ...,  5.3616e-05,
          6.3777e-05,  2.9918e-05],
        [-2.4065e-05, -1.6786e-05,  1.3113e-05,  ..., -1.9953e-05,
         -1.1973e-05, -1.7449e-05],
        [-4.7356e-05, -3.4019e-05,  2.5496e-05,  ..., -3.7968e-05,
         -2.3574e-05, -2.9713e-05]], device='cuda:0')
Loss: 1.0505754947662354
Graident accumulation at epoch 0, step 703, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0161,  0.0151, -0.0279,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4129e-04,  1.6661e-04, -1.2069e-04,  ...,  3.5338e-05,
         -1.0286e-04,  7.6099e-05],
        [-1.6258e-05, -1.2458e-05,  8.0315e-06,  ..., -1.4746e-05,
         -8.2643e-06, -1.1861e-05],
        [ 5.2844e-06, -7.7115e-08, -3.1135e-07,  ...,  4.0720e-06,
          4.0642e-06,  1.4048e-06],
        [ 1.9906e-05,  2.2265e-05, -1.9166e-05,  ...,  2.6039e-05,
          1.6108e-05,  1.8742e-05],
        [-4.1709e-05, -3.1388e-05,  2.1978e-05,  ..., -3.6286e-05,
         -1.9827e-05, -2.8199e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8273e-08, 3.5349e-08, 3.9269e-08,  ..., 1.4441e-08, 8.9456e-08,
         1.2776e-08],
        [6.7787e-11, 3.4775e-11, 8.1790e-12,  ..., 4.4624e-11, 6.3392e-12,
         1.4646e-11],
        [1.2283e-09, 6.2083e-10, 1.1519e-10,  ..., 1.0016e-09, 7.9012e-11,
         3.2541e-10],
        [2.4028e-10, 1.9973e-10, 1.0678e-10,  ..., 2.6549e-10, 9.0863e-11,
         1.3475e-10],
        [3.0633e-10, 1.6462e-10, 2.9931e-11,  ..., 2.2505e-10, 2.8967e-11,
         8.0816e-11]], device='cuda:0')
optimizer state dict: 88.0
lr: [1.5481349926128634e-05, 1.5481349926128634e-05]
scheduler_last_epoch: 88


Running epoch 0, step 704, batch 704
Sampled inputs[:2]: tensor([[    0,   650,    14,  ...,  6330,   221,   494],
        [    0,  7712, 31756,  ...,   895,   360,   630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1030e-04, -4.5051e-05,  4.0877e-04,  ..., -3.1207e-04,
          4.8651e-04,  4.8171e-04],
        [-2.1458e-06, -1.3411e-06,  1.0505e-06,  ..., -1.8179e-06,
         -1.1250e-06, -1.5646e-06],
        [-3.8445e-06, -2.4587e-06,  1.9670e-06,  ..., -3.1590e-06,
         -1.9222e-06, -2.5779e-06],
        [-3.1143e-06, -1.9222e-06,  1.6019e-06,  ..., -2.6077e-06,
         -1.5348e-06, -2.2352e-06],
        [-6.2287e-06, -4.0233e-06,  3.1888e-06,  ..., -5.0366e-06,
         -3.1143e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.0526639223098755


Running epoch 0, step 705, batch 705
Sampled inputs[:2]: tensor([[    0, 21801, 13084,  ...,  1738,  2946,    12],
        [    0,  9029,   634,  ...,  1424,  6872,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5470e-05,  4.3544e-04,  4.1723e-04,  ..., -1.5547e-04,
          1.8462e-04,  5.4047e-04],
        [-4.5002e-06, -2.8461e-06,  2.0564e-06,  ..., -3.6359e-06,
         -2.3618e-06, -3.2708e-06],
        [-8.1360e-06, -5.2750e-06,  3.8743e-06,  ..., -6.3777e-06,
         -4.0978e-06, -5.4389e-06],
        [-6.3628e-06, -3.9935e-06,  3.0473e-06,  ..., -5.0962e-06,
         -3.1963e-06, -4.5747e-06],
        [-1.3053e-05, -8.5235e-06,  6.2138e-06,  ..., -1.0103e-05,
         -6.5863e-06, -8.1658e-06]], device='cuda:0')
Loss: 1.0303170680999756


Running epoch 0, step 706, batch 706
Sampled inputs[:2]: tensor([[    0,   292,   494,  ...,   259, 14134, 11544],
        [    0,    12,   342,  ...,  3458,   271,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5382e-04,  6.7503e-04,  4.1522e-04,  ..., -9.8485e-05,
          8.6142e-05,  5.4047e-04],
        [-6.7502e-06, -4.3586e-06,  3.1888e-06,  ..., -5.5879e-06,
         -3.6061e-06, -4.7386e-06],
        [-1.1906e-05, -7.8827e-06,  5.8562e-06,  ..., -9.5516e-06,
         -6.1095e-06, -7.7039e-06],
        [-9.5069e-06, -6.1095e-06,  4.7088e-06,  ..., -7.8082e-06,
         -4.8727e-06, -6.6459e-06],
        [-1.9372e-05, -1.2934e-05,  9.5069e-06,  ..., -1.5408e-05,
         -1.0028e-05, -1.1757e-05]], device='cuda:0')
Loss: 1.0690776109695435


Running epoch 0, step 707, batch 707
Sampled inputs[:2]: tensor([[   0,  344, 8133,  ...,  368, 1119, 5539],
        [   0, 2700, 5221,  ...,  298,  259,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4523e-04,  7.6280e-04,  5.4186e-04,  ..., -1.3279e-04,
          1.6932e-04,  7.4906e-04],
        [-9.0152e-06, -5.9456e-06,  4.1723e-06,  ..., -7.4059e-06,
         -4.7535e-06, -6.3181e-06],
        [-1.5929e-05, -1.0744e-05,  7.7188e-06,  ..., -1.2681e-05,
         -8.0615e-06, -1.0312e-05],
        [-1.2651e-05, -8.2999e-06,  6.1467e-06,  ..., -1.0312e-05,
         -6.4000e-06, -8.8364e-06],
        [-2.5928e-05, -1.7583e-05,  1.2532e-05,  ..., -2.0444e-05,
         -1.3262e-05, -1.5751e-05]], device='cuda:0')
Loss: 1.0501599311828613


Running epoch 0, step 708, batch 708
Sampled inputs[:2]: tensor([[   0, 1615,  287,  ...,  259,  623,   12],
        [   0, 7203,  271,  ...,   12,  275, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7666e-04,  7.6418e-04,  5.4409e-04,  ..., -2.0035e-04,
          1.2208e-04,  7.9168e-04],
        [-1.1086e-05, -7.3910e-06,  5.3123e-06,  ..., -9.2536e-06,
         -5.8487e-06, -7.7859e-06],
        [-1.9506e-05, -1.3292e-05,  9.7603e-06,  ..., -1.5751e-05,
         -9.8571e-06, -1.2606e-05],
        [-1.5721e-05, -1.0431e-05,  7.9125e-06,  ..., -1.3009e-05,
         -7.9498e-06, -1.1012e-05],
        [-3.1769e-05, -2.1756e-05,  1.5825e-05,  ..., -2.5421e-05,
         -1.6242e-05, -1.9297e-05]], device='cuda:0')
Loss: 1.0532795190811157


Running epoch 0, step 709, batch 709
Sampled inputs[:2]: tensor([[   0, 7692,   12,  ...,  266, 2042,  278],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2055e-04,  1.0768e-03,  3.0630e-04,  ..., -6.6119e-05,
         -2.8350e-04,  4.2741e-04],
        [-1.3158e-05, -8.7544e-06,  6.4969e-06,  ..., -1.1034e-05,
         -6.8918e-06, -9.3654e-06],
        [-2.3052e-05, -1.5691e-05,  1.1876e-05,  ..., -1.8686e-05,
         -1.1563e-05, -1.5050e-05],
        [-1.8880e-05, -1.2487e-05,  9.8348e-06,  ..., -1.5721e-05,
         -9.4771e-06, -1.3441e-05],
        [-3.7432e-05, -2.5600e-05,  1.9163e-05,  ..., -3.0071e-05,
         -1.9029e-05, -2.2948e-05]], device='cuda:0')
Loss: 1.024111032485962


Running epoch 0, step 710, batch 710
Sampled inputs[:2]: tensor([[   0, 2302,  287,  ..., 1522, 1666,  300],
        [   0, 1549,  824,  ..., 3609,  720,  417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7108e-04,  1.2374e-03,  7.9089e-05,  ..., -8.4373e-05,
         -5.8865e-04,  3.2895e-04],
        [-1.5303e-05, -1.0245e-05,  7.6219e-06,  ..., -1.2785e-05,
         -7.8455e-06, -1.0774e-05],
        [-2.6628e-05, -1.8239e-05,  1.3828e-05,  ..., -2.1547e-05,
         -1.3106e-05, -1.7196e-05],
        [-2.1994e-05, -1.4648e-05,  1.1556e-05,  ..., -1.8254e-05,
         -1.0803e-05, -1.5497e-05],
        [-4.3213e-05, -2.9743e-05,  2.2307e-05,  ..., -3.4690e-05,
         -2.1622e-05, -2.6226e-05]], device='cuda:0')
Loss: 1.031349778175354


Running epoch 0, step 711, batch 711
Sampled inputs[:2]: tensor([[    0,   721,  1717,  ...,   278, 26029,    12],
        [    0,  2341,  7956,  ...,  2355,   413,    72]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1193e-04,  1.1968e-03,  1.5121e-04,  ..., -1.5875e-04,
         -4.7866e-04,  3.5164e-04],
        [-1.7643e-05, -1.1794e-05,  8.6948e-06,  ..., -1.4640e-05,
         -9.0003e-06, -1.2204e-05],
        [-3.0681e-05, -2.0996e-05,  1.5751e-05,  ..., -2.4721e-05,
         -1.5058e-05, -1.9535e-05],
        [ 8.9128e-05,  5.7355e-05, -2.1006e-05,  ...,  3.1943e-05,
          2.7140e-05,  4.3047e-06],
        [-4.9949e-05, -3.4332e-05,  2.5481e-05,  ..., -3.9935e-05,
         -2.4945e-05, -2.9922e-05]], device='cuda:0')
Loss: 1.0722144842147827
Graident accumulation at epoch 0, step 711, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0196],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0161,  0.0151, -0.0279,  ...,  0.0286, -0.0152, -0.0179]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.5965e-05,  2.6962e-04, -9.3497e-05,  ...,  1.5929e-05,
         -1.4044e-04,  1.0365e-04],
        [-1.6396e-05, -1.2391e-05,  8.0978e-06,  ..., -1.4735e-05,
         -8.3379e-06, -1.1895e-05],
        [ 1.6878e-06, -2.1690e-06,  1.2948e-06,  ...,  1.1927e-06,
          2.1520e-06, -6.8924e-07],
        [ 2.6828e-05,  2.5774e-05, -1.9350e-05,  ...,  2.6629e-05,
          1.7211e-05,  1.7298e-05],
        [-4.2533e-05, -3.1682e-05,  2.2328e-05,  ..., -3.6651e-05,
         -2.0339e-05, -2.8371e-05]], device='cuda:0')
optimizer state dict: tensor([[4.8731e-08, 3.6746e-08, 3.9252e-08,  ..., 1.4451e-08, 8.9595e-08,
         1.2887e-08],
        [6.8031e-11, 3.4879e-11, 8.2464e-12,  ..., 4.4794e-11, 6.4139e-12,
         1.4781e-11],
        [1.2280e-09, 6.2065e-10, 1.1533e-10,  ..., 1.0012e-09, 7.9160e-11,
         3.2547e-10],
        [2.4799e-10, 2.0282e-10, 1.0711e-10,  ..., 2.6625e-10, 9.1509e-11,
         1.3463e-10],
        [3.0851e-10, 1.6564e-10, 3.0550e-11,  ..., 2.2642e-10, 2.9561e-11,
         8.1630e-11]], device='cuda:0')
optimizer state dict: 89.0
lr: [1.537754207460116e-05, 1.537754207460116e-05]
scheduler_last_epoch: 89


Running epoch 0, step 712, batch 712
Sampled inputs[:2]: tensor([[    0,   298,   669,  ...,   287, 19731,    13],
        [    0, 18774,  4916,  ..., 35093,    19,    50]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7570e-04,  2.2471e-05,  2.1144e-04,  ...,  4.7454e-06,
          1.4238e-04,  1.5613e-04],
        [-2.2650e-06, -1.5125e-06,  1.0282e-06,  ..., -1.8701e-06,
         -1.1846e-06, -1.5423e-06],
        [-3.8445e-06, -2.6226e-06,  1.8403e-06,  ..., -3.1143e-06,
         -1.9670e-06, -2.4289e-06],
        [-3.2634e-06, -2.1756e-06,  1.5646e-06,  ..., -2.6822e-06,
         -1.6391e-06, -2.2352e-06],
        [-6.3181e-06, -4.3809e-06,  3.0100e-06,  ..., -5.0962e-06,
         -3.3081e-06, -3.7551e-06]], device='cuda:0')
Loss: 1.0494893789291382


Running epoch 0, step 713, batch 713
Sampled inputs[:2]: tensor([[    0,    13,  1107,  ...,   287, 25185,    14],
        [    0,   792,    83,  ...,   300,   768,   932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9432e-04, -1.4681e-04,  2.7726e-04,  ..., -6.3481e-05,
          2.0286e-04,  2.0253e-04],
        [-4.4554e-06, -2.9877e-06,  2.1309e-06,  ..., -3.6731e-06,
         -2.2203e-06, -2.9206e-06],
        [-7.4357e-06, -5.0813e-06,  3.7104e-06,  ..., -6.0350e-06,
         -3.6582e-06, -4.5598e-06],
        [-6.4969e-06, -4.3362e-06,  3.2708e-06,  ..., -5.3346e-06,
         -3.1069e-06, -4.3064e-06],
        [-1.2279e-05, -8.5235e-06,  6.0946e-06,  ..., -9.9540e-06,
         -6.2287e-06, -7.0930e-06]], device='cuda:0')
Loss: 1.0589467287063599


Running epoch 0, step 714, batch 714
Sampled inputs[:2]: tensor([[   0,  298,  894,  ...,  266, 2904, 1679],
        [   0,  709,  630,  ..., 6263,  409,  508]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8069e-04,  3.2590e-05,  2.7314e-04,  ..., -8.2648e-05,
          1.2503e-04,  1.5303e-04],
        [-6.7949e-06, -4.4405e-06,  3.1814e-06,  ..., -5.4911e-06,
         -3.3006e-06, -4.5225e-06],
        [-1.1459e-05, -7.6890e-06,  5.6028e-06,  ..., -9.1493e-06,
         -5.5432e-06, -7.1675e-06],
        [-9.8050e-06, -6.3926e-06,  4.8354e-06,  ..., -7.9125e-06,
         -4.5970e-06, -6.5863e-06],
        [-1.8656e-05, -1.2696e-05,  9.0748e-06,  ..., -1.4871e-05,
         -9.2834e-06, -1.0997e-05]], device='cuda:0')
Loss: 1.0242133140563965


Running epoch 0, step 715, batch 715
Sampled inputs[:2]: tensor([[    0,   790,  2816,  ...,    14,  1062,   668],
        [    0, 13576,   431,  ...,    14,   475,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8002e-04, -7.4197e-05,  2.2401e-04,  ..., -5.6981e-05,
          1.7311e-04,  9.5336e-05],
        [-8.9854e-06, -5.9679e-06,  4.2990e-06,  ..., -7.3463e-06,
         -4.3064e-06, -5.8264e-06],
        [-1.5020e-05, -1.0222e-05,  7.4729e-06,  ..., -1.2085e-05,
         -7.1228e-06, -9.1195e-06],
        [-1.3039e-05, -8.6278e-06,  6.5491e-06,  ..., -1.0625e-05,
         -6.0052e-06, -8.5682e-06],
        [-2.4647e-05, -1.6987e-05,  1.2174e-05,  ..., -1.9789e-05,
         -1.2025e-05, -1.4052e-05]], device='cuda:0')
Loss: 1.0750930309295654


Running epoch 0, step 716, batch 716
Sampled inputs[:2]: tensor([[   0, 1550,  685,  ...,  943, 1239,  996],
        [   0, 3037, 4511,  ..., 1711,   12, 2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5913e-04, -1.4105e-04,  4.7991e-05,  ..., -5.7842e-05,
          3.7208e-05,  7.1145e-05],
        [-1.1265e-05, -7.4804e-06,  5.4538e-06,  ..., -9.2015e-06,
         -5.3048e-06, -7.2122e-06],
        [-1.8641e-05, -1.2696e-05,  9.3952e-06,  ..., -1.4991e-05,
         -8.7097e-06, -1.1176e-05],
        [-1.6302e-05, -1.0788e-05,  8.2850e-06,  ..., -1.3277e-05,
         -7.3910e-06, -1.0595e-05],
        [-3.0637e-05, -2.1130e-05,  1.5318e-05,  ..., -2.4617e-05,
         -1.4767e-05, -1.7226e-05]], device='cuda:0')
Loss: 1.0366290807724


Running epoch 0, step 717, batch 717
Sampled inputs[:2]: tensor([[   0,  596,  292,  ...,   13, 6673,  298],
        [   0,   81, 1619,  ..., 2442,   13, 1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3288e-04, -3.3281e-04, -3.4318e-05,  ..., -5.7990e-05,
          2.2805e-04,  2.3501e-04],
        [-1.3515e-05, -8.9705e-06,  6.5416e-06,  ..., -1.1042e-05,
         -6.4522e-06, -8.7246e-06],
        [-2.2486e-05, -1.5303e-05,  1.1317e-05,  ..., -1.8075e-05,
         -1.0610e-05, -1.3560e-05],
        [-1.9580e-05, -1.2934e-05,  9.9316e-06,  ..., -1.5929e-05,
         -8.9854e-06, -1.2815e-05],
        [-3.6925e-05, -2.5421e-05,  1.8418e-05,  ..., -2.9624e-05,
         -1.7971e-05, -2.0877e-05]], device='cuda:0')
Loss: 1.0445349216461182


Running epoch 0, step 718, batch 718
Sampled inputs[:2]: tensor([[    0,  1266,  2257,  ..., 27146,  1141,  1196],
        [    0, 29073,   916,  ...,    12,   287,   850]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8918e-04, -6.7467e-04, -1.9642e-04,  ..., -1.5109e-04,
          6.9000e-04,  3.2672e-04],
        [-1.5602e-05, -1.0423e-05,  7.5921e-06,  ..., -1.2867e-05,
         -7.6294e-06, -1.0252e-05],
        [-2.5943e-05, -1.7792e-05,  1.3143e-05,  ..., -2.1026e-05,
         -1.2517e-05, -1.5840e-05],
        [-2.2545e-05, -1.4991e-05,  1.1526e-05,  ..., -1.8537e-05,
         -1.0632e-05, -1.5050e-05],
        [-4.2617e-05, -2.9564e-05,  2.1383e-05,  ..., -3.4422e-05,
         -2.1175e-05, -2.4319e-05]], device='cuda:0')
Loss: 1.0517826080322266


Running epoch 0, step 719, batch 719
Sampled inputs[:2]: tensor([[   0, 2229,  352,  ..., 4988,   33,   13],
        [   0, 3059, 2013,  ...,  278, 1997,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6475e-04, -5.5526e-04, -3.0301e-04,  ..., -2.0898e-05,
          6.3133e-04,  1.9359e-04],
        [-1.7792e-05, -1.1846e-05,  8.6352e-06,  ..., -1.4685e-05,
         -8.7321e-06, -1.1824e-05],
        [-2.9489e-05, -2.0191e-05,  1.4924e-05,  ..., -2.3901e-05,
         -1.4260e-05, -1.8150e-05],
        [-2.5645e-05, -1.7002e-05,  1.3098e-05,  ..., -2.1115e-05,
         -1.2159e-05, -1.7285e-05],
        [-4.8578e-05, -3.3647e-05,  2.4363e-05,  ..., -3.9220e-05,
         -2.4199e-05, -2.7925e-05]], device='cuda:0')
Loss: 1.0482909679412842
Graident accumulation at epoch 0, step 719, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0161,  0.0151, -0.0279,  ...,  0.0287, -0.0152, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3684e-04,  1.8713e-04, -1.1445e-04,  ...,  1.2247e-05,
         -6.3262e-05,  1.1265e-04],
        [-1.6536e-05, -1.2337e-05,  8.1516e-06,  ..., -1.4730e-05,
         -8.3773e-06, -1.1888e-05],
        [-1.4299e-06, -3.9712e-06,  2.6577e-06,  ..., -1.3168e-06,
          5.1079e-07, -2.4353e-06],
        [ 2.1581e-05,  2.1496e-05, -1.6105e-05,  ...,  2.1855e-05,
          1.4274e-05,  1.3840e-05],
        [-4.3137e-05, -3.1879e-05,  2.2532e-05,  ..., -3.6908e-05,
         -2.0725e-05, -2.8326e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9430e-08, 3.7018e-08, 3.9305e-08,  ..., 1.4437e-08, 8.9904e-08,
         1.2912e-08],
        [6.8279e-11, 3.4985e-11, 8.3127e-12,  ..., 4.4965e-11, 6.4837e-12,
         1.4906e-11],
        [1.2276e-09, 6.2044e-10, 1.1543e-10,  ..., 1.0008e-09, 7.9284e-11,
         3.2547e-10],
        [2.4840e-10, 2.0291e-10, 1.0718e-10,  ..., 2.6643e-10, 9.1565e-11,
         1.3480e-10],
        [3.1056e-10, 1.6660e-10, 3.1113e-11,  ..., 2.2773e-10, 3.0117e-11,
         8.2328e-11]], device='cuda:0')
optimizer state dict: 90.0
lr: [1.5272912487703465e-05, 1.5272912487703465e-05]
scheduler_last_epoch: 90


Running epoch 0, step 720, batch 720
Sampled inputs[:2]: tensor([[    0,  3703,   278,  ...,  9807,    14, 10365],
        [    0,   706,  6989,  ...,  6914,    15,  2537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1244e-07, -1.4486e-04,  6.7369e-05,  ...,  4.6852e-05,
         -4.1116e-05, -4.1136e-05],
        [-2.4885e-06, -1.6615e-06,  1.1623e-06,  ..., -1.9670e-06,
         -9.5367e-07, -1.3411e-06],
        [-3.8743e-06, -2.6524e-06,  1.8626e-06,  ..., -3.0547e-06,
         -1.4976e-06, -1.9819e-06],
        [-3.5316e-06, -2.3246e-06,  1.7211e-06,  ..., -2.8014e-06,
         -1.3039e-06, -1.9819e-06],
        [-6.4075e-06, -4.4405e-06,  3.0398e-06,  ..., -5.0366e-06,
         -2.5630e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.0841342210769653


Running epoch 0, step 721, batch 721
Sampled inputs[:2]: tensor([[    0,    69,   462,  ...,   437,   266,   634],
        [    0, 10766,  8311,  ...,   328,   957,  1231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0266e-04, -1.9228e-04,  1.5550e-04,  ...,  6.5123e-05,
          3.3397e-05,  5.7733e-05],
        [-4.7982e-06, -3.1590e-06,  2.1756e-06,  ..., -3.9190e-06,
         -2.1458e-06, -2.9504e-06],
        [-7.5996e-06, -5.1707e-06,  3.5912e-06,  ..., -6.1095e-06,
         -3.3826e-06, -4.3511e-06],
        [-6.7800e-06, -4.4107e-06,  3.2261e-06,  ..., -5.5432e-06,
         -2.9355e-06, -4.3064e-06],
        [-1.2457e-05, -8.5831e-06,  5.7966e-06,  ..., -9.9540e-06,
         -5.6922e-06, -6.6310e-06]], device='cuda:0')
Loss: 1.0314120054244995


Running epoch 0, step 722, batch 722
Sampled inputs[:2]: tensor([[    0, 12686, 18519,  ...,   328,   912,  3978],
        [    0,    12,   287,  ...,   365,  1943,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6370e-04, -1.8887e-04,  4.0543e-04,  ..., -1.7040e-05,
          2.0935e-04,  2.6223e-04],
        [-7.3761e-06, -4.9099e-06,  3.2187e-06,  ..., -5.8711e-06,
         -3.3677e-06, -4.3511e-06],
        [-1.1802e-05, -8.0764e-06,  5.3495e-06,  ..., -9.2685e-06,
         -5.3644e-06, -6.5118e-06],
        [-1.0297e-05, -6.7800e-06,  4.7088e-06,  ..., -8.1956e-06,
         -4.5523e-06, -6.2436e-06],
        [-1.9491e-05, -1.3500e-05,  8.7172e-06,  ..., -1.5259e-05,
         -9.1493e-06, -1.0073e-05]], device='cuda:0')
Loss: 1.0546631813049316


Running epoch 0, step 723, batch 723
Sampled inputs[:2]: tensor([[    0,   287, 21212,  ...,  3123,   944,   278],
        [    0,    22,  2577,  ...,  4970,     9,  3868]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6683e-04, -2.3921e-04,  2.7044e-04,  ..., -4.7515e-05,
          2.0437e-04,  3.0055e-04],
        [-9.8646e-06, -6.5416e-06,  4.3139e-06,  ..., -7.7933e-06,
         -4.3884e-06, -5.7220e-06],
        [-1.5736e-05, -1.0729e-05,  7.1302e-06,  ..., -1.2308e-05,
         -7.0035e-06, -8.5980e-06],
        [-1.3798e-05, -9.0599e-06,  6.3106e-06,  ..., -1.0908e-05,
         -5.9381e-06, -8.2403e-06],
        [-2.5898e-05, -1.7852e-05,  1.1578e-05,  ..., -2.0236e-05,
         -1.1921e-05, -1.3277e-05]], device='cuda:0')
Loss: 1.0724371671676636


Running epoch 0, step 724, batch 724
Sampled inputs[:2]: tensor([[    0,  5722, 20126,  ...,  1500,   696,   259],
        [    0,    12,   689,  ...,  1110,  1712,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6595e-04, -2.8200e-04,  3.9246e-04,  ..., -4.4988e-05,
          2.4321e-04,  3.2280e-04],
        [-1.2264e-05, -8.1435e-06,  5.3719e-06,  ..., -9.7007e-06,
         -5.3309e-06, -7.0706e-06],
        [-1.9491e-05, -1.3322e-05,  8.8438e-06,  ..., -1.5303e-05,
         -8.5011e-06, -1.0595e-05],
        [-1.7270e-05, -1.1370e-05,  7.9125e-06,  ..., -1.3694e-05,
         -7.2643e-06, -1.0252e-05],
        [-3.2157e-05, -2.2203e-05,  1.4409e-05,  ..., -2.5243e-05,
         -1.4529e-05, -1.6376e-05]], device='cuda:0')
Loss: 1.0419392585754395


Running epoch 0, step 725, batch 725
Sampled inputs[:2]: tensor([[    0, 11325,   278,  ...,   446,  1869,   642],
        [    0,   578,   221,  ...,   287,  1254,  4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2267e-04,  9.6648e-05,  4.3192e-04,  ...,  3.3377e-04,
         -1.9005e-04,  3.8541e-04],
        [-1.4767e-05, -9.7156e-06,  6.4075e-06,  ..., -1.1638e-05,
         -6.3069e-06, -8.6501e-06],
        [-2.3752e-05, -1.6063e-05,  1.0662e-05,  ..., -1.8552e-05,
         -1.0163e-05, -1.3173e-05],
        [-2.0713e-05, -1.3530e-05,  9.3877e-06,  ..., -1.6361e-05,
         -8.5756e-06, -1.2442e-05],
        [-3.8803e-05, -2.6554e-05,  1.7241e-05,  ..., -3.0309e-05,
         -1.7181e-05, -2.0161e-05]], device='cuda:0')
Loss: 1.0534785985946655


Running epoch 0, step 726, batch 726
Sampled inputs[:2]: tensor([[    0,   287,  2421,  ...,  6612,   352,   344],
        [    0,   266,  6079,  ...,   437,   266, 44526]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5790e-04, -1.3932e-05,  4.5382e-04,  ...,  2.4130e-04,
          9.8366e-05,  6.3940e-04],
        [-1.7166e-05, -1.1273e-05,  7.3388e-06,  ..., -1.3530e-05,
         -7.3723e-06, -1.0192e-05],
        [-2.7835e-05, -1.8746e-05,  1.2301e-05,  ..., -2.1696e-05,
         -1.1921e-05, -1.5676e-05],
        [ 2.2424e-04,  1.1478e-04, -8.9492e-05,  ...,  1.8609e-04,
          8.0009e-05,  9.0078e-05],
        [-4.5329e-05, -3.0875e-05,  1.9848e-05,  ..., -3.5316e-05,
         -2.0072e-05, -2.3916e-05]], device='cuda:0')
Loss: 1.0555006265640259


Running epoch 0, step 727, batch 727
Sampled inputs[:2]: tensor([[   0,  365, 1462,  ...,  518, 6104,  278],
        [   0, 1615,  328,  ...,  266, 3133,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5675e-04, -1.0644e-04,  4.7220e-04,  ...,  2.4028e-04,
          1.0089e-04,  5.6343e-04],
        [-1.9431e-05, -1.2718e-05,  8.3521e-06,  ..., -1.5326e-05,
         -8.1882e-06, -1.1519e-05],
        [-3.1441e-05, -2.1100e-05,  1.3985e-05,  ..., -2.4527e-05,
         -1.3225e-05, -1.7673e-05],
        [ 2.2083e-04,  1.1262e-04, -8.7883e-05,  ...,  1.8339e-04,
          7.8847e-05,  8.8006e-05],
        [-5.1230e-05, -3.4720e-05,  2.2560e-05,  ..., -3.9905e-05,
         -2.2307e-05, -2.6971e-05]], device='cuda:0')
Loss: 1.0508030652999878
Graident accumulation at epoch 0, step 727, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0145,  0.0030,  ..., -0.0025,  0.0229, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0279,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5883e-04,  1.5778e-04, -5.5783e-05,  ...,  3.5050e-05,
         -4.6846e-05,  1.5773e-04],
        [-1.6825e-05, -1.2375e-05,  8.1716e-06,  ..., -1.4790e-05,
         -8.3584e-06, -1.1851e-05],
        [-4.4311e-06, -5.6841e-06,  3.7904e-06,  ..., -3.6378e-06,
         -8.6277e-07, -3.9590e-06],
        [ 4.1506e-05,  3.0609e-05, -2.3283e-05,  ...,  3.8009e-05,
          2.0731e-05,  2.1256e-05],
        [-4.3947e-05, -3.2163e-05,  2.2535e-05,  ..., -3.7207e-05,
         -2.0883e-05, -2.8191e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9508e-08, 3.6992e-08, 3.9489e-08,  ..., 1.4481e-08, 8.9825e-08,
         1.3216e-08],
        [6.8588e-11, 3.5111e-11, 8.3742e-12,  ..., 4.5155e-11, 6.5443e-12,
         1.5023e-11],
        [1.2274e-09, 6.2026e-10, 1.1551e-10,  ..., 1.0004e-09, 7.9380e-11,
         3.2546e-10],
        [2.9691e-10, 2.1539e-10, 1.1479e-10,  ..., 2.9979e-10, 9.7690e-11,
         1.4241e-10],
        [3.1288e-10, 1.6764e-10, 3.1591e-11,  ..., 2.2910e-10, 3.0584e-11,
         8.2973e-11]], device='cuda:0')
optimizer state dict: 91.0
lr: [1.5167477153749745e-05, 1.5167477153749745e-05]
scheduler_last_epoch: 91


Running epoch 0, step 728, batch 728
Sampled inputs[:2]: tensor([[   0, 1099,  644,  ..., 5481,   14, 8782],
        [   0,  510,   13,  ..., 3454,  513,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 0.0000e+00,  1.4361e-04,  2.2904e-04,  ...,  8.9035e-05,
         -1.1534e-04,  3.4702e-05],
        [-2.4587e-06, -1.5274e-06,  9.4250e-07,  ..., -1.9222e-06,
         -1.0729e-06, -1.5646e-06],
        [-4.0531e-06, -2.5928e-06,  1.6242e-06,  ..., -3.1143e-06,
         -1.7360e-06, -2.4140e-06],
        [-3.4571e-06, -2.1160e-06,  1.3784e-06,  ..., -2.6971e-06,
         -1.4529e-06, -2.2501e-06],
        [-6.5267e-06, -4.2319e-06,  2.6226e-06,  ..., -5.0366e-06,
         -2.8908e-06, -3.6657e-06]], device='cuda:0')
Loss: 1.0445526838302612


Running epoch 0, step 729, batch 729
Sampled inputs[:2]: tensor([[    0,  3978,  2697,  ...,   461,  5955,  3792],
        [    0,  3889,  4039,  ...,   616, 22910,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4089e-05,  1.1273e-04,  3.3165e-04,  ...,  1.0456e-04,
          7.8466e-05,  2.3238e-05],
        [-4.9174e-06, -2.9653e-06,  1.9111e-06,  ..., -3.8445e-06,
         -2.1607e-06, -3.2336e-06],
        [-8.1956e-06, -5.0813e-06,  3.3230e-06,  ..., -6.2734e-06,
         -3.5390e-06, -5.0515e-06],
        [-6.8992e-06, -4.0978e-06,  2.7865e-06,  ..., -5.3644e-06,
         -2.9206e-06, -4.6343e-06],
        [-1.3173e-05, -8.2850e-06,  5.3346e-06,  ..., -1.0103e-05,
         -5.8711e-06, -7.6592e-06]], device='cuda:0')
Loss: 1.0548056364059448


Running epoch 0, step 730, batch 730
Sampled inputs[:2]: tensor([[    0,  1083,   287,  ...,    12,   287,  2098],
        [    0,     8,    19,  ..., 13359, 12377,   938]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2022e-04, -1.0609e-04,  3.4783e-04,  ...,  9.4236e-05,
          7.0421e-05,  1.2944e-04],
        [-7.3761e-06, -4.5523e-06,  2.9542e-06,  ..., -5.7667e-06,
         -3.1665e-06, -4.6343e-06],
        [-1.2070e-05, -7.6592e-06,  5.0217e-06,  ..., -9.2685e-06,
         -5.1335e-06, -7.1228e-06],
        [-1.0327e-05, -6.2883e-06,  4.2990e-06,  ..., -8.0466e-06,
         -4.2766e-06, -6.6757e-06],
        [-1.9431e-05, -1.2517e-05,  8.0615e-06,  ..., -1.4991e-05,
         -8.5682e-06, -1.0803e-05]], device='cuda:0')
Loss: 1.059065818786621


Running epoch 0, step 731, batch 731
Sampled inputs[:2]: tensor([[    0,   287,   768,  ...,   221,   474,   221],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6068e-05, -1.9762e-04,  4.9821e-04,  ...,  1.2688e-04,
          2.7548e-04,  2.0273e-04],
        [-9.8348e-06, -6.1765e-06,  3.9600e-06,  ..., -7.7188e-06,
         -4.1872e-06, -6.1244e-06],
        [-1.5914e-05, -1.0267e-05,  6.6757e-06,  ..., -1.2264e-05,
         -6.7279e-06, -9.2685e-06],
        [-1.3798e-05, -8.5533e-06,  5.7742e-06,  ..., -1.0803e-05,
         -5.6773e-06, -8.8513e-06],
        [-2.5779e-05, -1.6838e-05,  1.0774e-05,  ..., -1.9908e-05,
         -1.1280e-05, -1.4096e-05]], device='cuda:0')
Loss: 1.0642168521881104


Running epoch 0, step 732, batch 732
Sampled inputs[:2]: tensor([[   0, 9677,  609,  ...,  199, 1919,  298],
        [   0, 2645,   12,  ...,    5, 1239, 7200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0927e-04, -3.4677e-04,  5.5771e-04,  ...,  2.1260e-04,
          3.4038e-04,  3.7705e-04],
        [-1.2293e-05, -7.7114e-06,  4.9882e-06,  ..., -9.7007e-06,
         -5.2378e-06, -7.6368e-06],
        [-1.9848e-05, -1.2800e-05,  8.3745e-06,  ..., -1.5408e-05,
         -8.4341e-06, -1.1548e-05],
        [-1.7241e-05, -1.0684e-05,  7.2792e-06,  ..., -1.3575e-05,
         -7.1153e-06, -1.1027e-05],
        [-3.2127e-05, -2.1011e-05,  1.3500e-05,  ..., -2.5034e-05,
         -1.4171e-05, -1.7539e-05]], device='cuda:0')
Loss: 1.0648061037063599


Running epoch 0, step 733, batch 733
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8563e-04, -2.7853e-04,  4.7974e-04,  ...,  1.2054e-04,
          2.6140e-04,  2.1935e-04],
        [-1.4871e-05, -9.3356e-06,  5.9381e-06,  ..., -1.1601e-05,
         -6.2436e-06, -9.1493e-06],
        [-2.3991e-05, -1.5497e-05,  9.9614e-06,  ..., -1.8463e-05,
         -1.0081e-05, -1.3873e-05],
        [-2.0772e-05, -1.2904e-05,  8.6278e-06,  ..., -1.6183e-05,
         -8.4713e-06, -1.3173e-05],
        [-3.8922e-05, -2.5481e-05,  1.6078e-05,  ..., -3.0041e-05,
         -1.6958e-05, -2.1145e-05]], device='cuda:0')
Loss: 1.0593839883804321


Running epoch 0, step 734, batch 734
Sampled inputs[:2]: tensor([[    0,   199,  1139,  ...,    13,  1303, 26330],
        [    0,   292, 15087,  ...,  2675,  1663,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9112e-04, -9.3848e-05,  1.3861e-04,  ...,  1.5778e-04,
         -3.4557e-04, -2.7567e-04],
        [-1.7047e-05, -1.0505e-05,  6.9439e-06,  ..., -1.3448e-05,
         -7.1786e-06, -1.0699e-05],
        [-2.7388e-05, -1.7419e-05,  1.1601e-05,  ..., -2.1249e-05,
         -1.1519e-05, -1.6063e-05],
        [-2.4006e-05, -1.4588e-05,  1.0207e-05,  ..., -1.8924e-05,
         -9.8199e-06, -1.5572e-05],
        [-4.4435e-05, -2.8685e-05,  1.8701e-05,  ..., -3.4511e-05,
         -1.9372e-05, -2.4423e-05]], device='cuda:0')
Loss: 1.015861988067627


Running epoch 0, step 735, batch 735
Sampled inputs[:2]: tensor([[    0,  5332,   391,  ...,   221,   334,  1530],
        [    0,  1278,    69,  ...,    15,  7377, 20524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5360e-04, -8.2379e-05,  1.2349e-04,  ...,  9.4101e-05,
         -4.2238e-04, -3.6805e-04],
        [-1.9446e-05, -1.1995e-05,  7.8790e-06,  ..., -1.5356e-05,
         -8.1472e-06, -1.2033e-05],
        [-3.1158e-05, -1.9833e-05,  1.3135e-05,  ..., -2.4229e-05,
         -1.3061e-05, -1.8016e-05],
        [-2.7344e-05, -1.6630e-05,  1.1563e-05,  ..., -2.1592e-05,
         -1.1131e-05, -1.7494e-05],
        [-5.0604e-05, -3.2678e-05,  2.1204e-05,  ..., -3.9399e-05,
         -2.1979e-05, -2.7433e-05]], device='cuda:0')
Loss: 1.0415456295013428
Graident accumulation at epoch 0, step 735, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0144,  0.0030,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6831e-04,  1.3376e-04, -3.7856e-05,  ...,  4.0955e-05,
         -8.4400e-05,  1.0515e-04],
        [-1.7087e-05, -1.2337e-05,  8.1424e-06,  ..., -1.4846e-05,
         -8.3373e-06, -1.1869e-05],
        [-7.1038e-06, -7.0990e-06,  4.7249e-06,  ..., -5.6970e-06,
         -2.0826e-06, -5.3647e-06],
        [ 3.4621e-05,  2.5885e-05, -1.9798e-05,  ...,  3.2049e-05,
          1.7545e-05,  1.7381e-05],
        [-4.4612e-05, -3.2214e-05,  2.2402e-05,  ..., -3.7427e-05,
         -2.0992e-05, -2.8115e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9523e-08, 3.6962e-08, 3.9464e-08,  ..., 1.4475e-08, 8.9913e-08,
         1.3339e-08],
        [6.8898e-11, 3.5220e-11, 8.4279e-12,  ..., 4.5346e-11, 6.6041e-12,
         1.5153e-11],
        [1.2271e-09, 6.2003e-10, 1.1557e-10,  ..., 9.9994e-10, 7.9471e-11,
         3.2546e-10],
        [2.9736e-10, 2.1545e-10, 1.1481e-10,  ..., 2.9996e-10, 9.7716e-11,
         1.4257e-10],
        [3.1513e-10, 1.6854e-10, 3.2009e-11,  ..., 2.3042e-10, 3.1037e-11,
         8.3643e-11]], device='cuda:0')
optimizer state dict: 92.0
lr: [1.5061252184179384e-05, 1.5061252184179384e-05]
scheduler_last_epoch: 92


Running epoch 0, step 736, batch 736
Sampled inputs[:2]: tensor([[    0,  3806,    13,  ..., 11786,  2254,   221],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8811e-05, -6.2980e-05,  1.3677e-04,  ...,  7.5534e-05,
         -1.4368e-04, -2.6740e-05],
        [-2.4140e-06, -1.4529e-06,  1.0580e-06,  ..., -1.8775e-06,
         -8.9034e-07, -1.2815e-06],
        [-3.8147e-06, -2.3693e-06,  1.7360e-06,  ..., -2.9504e-06,
         -1.4156e-06, -1.9222e-06],
        [-3.4422e-06, -2.0266e-06,  1.5721e-06,  ..., -2.6822e-06,
         -1.2219e-06, -1.9222e-06],
        [-6.2883e-06, -3.9339e-06,  2.8312e-06,  ..., -4.8578e-06,
         -2.4438e-06, -2.9504e-06]], device='cuda:0')
Loss: 1.0598649978637695


Running epoch 0, step 737, batch 737
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,    14, 21687,  ...,   943,  2153,  4089]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9787e-04, -1.8302e-04,  1.0860e-04,  ...,  7.7765e-05,
          7.4750e-05, -4.4556e-05],
        [-4.8727e-06, -3.0845e-06,  2.1458e-06,  ..., -3.7998e-06,
         -1.8366e-06, -2.6524e-06],
        [-7.6294e-06, -4.9770e-06,  3.4794e-06,  ..., -5.9158e-06,
         -2.9057e-06, -3.9488e-06],
        [-6.8396e-06, -4.2617e-06,  3.1441e-06,  ..., -5.3495e-06,
         -2.4736e-06, -3.9339e-06],
        [-1.2457e-05, -8.1956e-06,  5.6028e-06,  ..., -9.6560e-06,
         -4.9621e-06, -6.0052e-06]], device='cuda:0')
Loss: 1.078639030456543


Running epoch 0, step 738, batch 738
Sampled inputs[:2]: tensor([[   0,  795, 3185,  ...,   14, 1671,  199],
        [   0,  925,  271,  ...,  631, 3370,  940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6404e-04, -2.6138e-04,  1.3885e-04,  ...,  1.1830e-04,
          3.4286e-05, -5.0027e-05],
        [-7.3612e-06, -4.6566e-06,  3.1590e-06,  ..., -5.6624e-06,
         -2.7269e-06, -4.1127e-06],
        [-1.1563e-05, -7.5400e-06,  5.1558e-06,  ..., -8.8662e-06,
         -4.3437e-06, -6.1393e-06],
        [-1.0282e-05, -6.4075e-06,  4.6045e-06,  ..., -7.9274e-06,
         -3.6582e-06, -6.0201e-06],
        [-1.8775e-05, -1.2398e-05,  8.2701e-06,  ..., -1.4424e-05,
         -7.4208e-06, -9.2983e-06]], device='cuda:0')
Loss: 1.0215457677841187


Running epoch 0, step 739, batch 739
Sampled inputs[:2]: tensor([[   0,  199, 5990,  ...,  278,  638, 5513],
        [   0,  895, 4110,  ..., 1578, 1245,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1999e-04, -4.0288e-04,  1.2526e-04,  ...,  8.0715e-05,
          2.2941e-04,  9.0318e-05],
        [-9.8646e-06, -6.2063e-06,  4.2617e-06,  ..., -7.5847e-06,
         -3.6247e-06, -5.5358e-06],
        [-1.5408e-05, -9.9987e-06,  6.8918e-06,  ..., -1.1832e-05,
         -5.7518e-06, -8.2403e-06],
        [-1.3858e-05, -8.5980e-06,  6.2436e-06,  ..., -1.0669e-05,
         -4.8876e-06, -8.1509e-06],
        [-2.5094e-05, -1.6481e-05,  1.1072e-05,  ..., -1.9282e-05,
         -9.8646e-06, -1.2517e-05]], device='cuda:0')
Loss: 1.0721148252487183


Running epoch 0, step 740, batch 740
Sampled inputs[:2]: tensor([[   0, 4263, 4865,  ..., 1878,  278, 4450],
        [   0, 5340,  287,  ...,  912, 2837, 5340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3322e-04, -4.7979e-04,  4.3118e-04,  ...,  1.2832e-04,
          4.2476e-04,  2.8450e-04],
        [-1.2264e-05, -7.7263e-06,  5.1446e-06,  ..., -9.4250e-06,
         -4.5784e-06, -6.9961e-06],
        [-1.9372e-05, -1.2591e-05,  8.4564e-06,  ..., -1.4812e-05,
         -7.3239e-06, -1.0476e-05],
        [-1.7196e-05, -1.0684e-05,  7.5251e-06,  ..., -1.3232e-05,
         -6.1765e-06, -1.0267e-05],
        [-3.1561e-05, -2.0742e-05,  1.3620e-05,  ..., -2.4110e-05,
         -1.2502e-05, -1.5885e-05]], device='cuda:0')
Loss: 1.0640727281570435


Running epoch 0, step 741, batch 741
Sampled inputs[:2]: tensor([[    0, 12165,    12,  ...,  2860, 10718,   278],
        [    0,   445,   749,  ...,   850,  1028,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4336e-04, -5.3090e-04,  4.8592e-04,  ...,  9.7579e-05,
          4.7001e-04,  1.9967e-04],
        [-1.4752e-05, -9.3579e-06,  6.2026e-06,  ..., -1.1265e-05,
         -5.4538e-06, -8.3372e-06],
        [-2.3305e-05, -1.5229e-05,  1.0170e-05,  ..., -1.7717e-05,
         -8.7246e-06, -1.2502e-05],
        [-2.0623e-05, -1.2904e-05,  9.0599e-06,  ..., -1.5780e-05,
         -7.3239e-06, -1.2219e-05],
        [-3.7909e-05, -2.5064e-05,  1.6347e-05,  ..., -2.8819e-05,
         -1.4901e-05, -1.8939e-05]], device='cuda:0')
Loss: 1.0590331554412842


Running epoch 0, step 742, batch 742
Sampled inputs[:2]: tensor([[   0,  437,  266,  ..., 5512,  822,   89],
        [   0, 3544,  417,  ...,  380,  381, 3794]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1716e-04, -1.6383e-04,  1.5456e-04,  ...,  1.8123e-04,
         -4.6625e-04, -2.0696e-04],
        [-1.6913e-05, -1.0602e-05,  7.1526e-06,  ..., -1.3135e-05,
         -6.5267e-06, -9.8497e-06],
        [-2.6748e-05, -1.7300e-05,  1.1757e-05,  ..., -2.0608e-05,
         -1.0408e-05, -1.4678e-05],
        [-2.3797e-05, -1.4640e-05,  1.0587e-05,  ..., -1.8612e-05,
         -8.8885e-06, -1.4663e-05],
        [-4.3571e-05, -2.8551e-05,  1.8910e-05,  ..., -3.3498e-05,
         -1.7747e-05, -2.2188e-05]], device='cuda:0')
Loss: 1.0220638513565063


Running epoch 0, step 743, batch 743
Sampled inputs[:2]: tensor([[    0,  2587,    27,  ...,   259,  2462,  1220],
        [    0,   365,  1410,  ...,    12,  1478, 16062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1912e-05, -6.3411e-05, -1.5917e-04,  ...,  3.2397e-04,
         -5.1456e-04, -7.9819e-05],
        [-1.9297e-05, -1.2018e-05,  8.1211e-06,  ..., -1.5028e-05,
         -7.5102e-06, -1.1541e-05],
        [-3.0831e-05, -1.9789e-05,  1.3493e-05,  ..., -2.3738e-05,
         -1.2055e-05, -1.7345e-05],
        [-2.7120e-05, -1.6563e-05,  1.2018e-05,  ..., -2.1249e-05,
         -1.0207e-05, -1.7136e-05],
        [-4.9800e-05, -3.2425e-05,  2.1547e-05,  ..., -3.8236e-05,
         -2.0340e-05, -2.5958e-05]], device='cuda:0')
Loss: 1.0314946174621582
Graident accumulation at epoch 0, step 743, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0056, -0.0144,  0.0030,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5467e-04,  1.1404e-04, -4.9988e-05,  ...,  6.9257e-05,
         -1.2742e-04,  8.6651e-05],
        [-1.7308e-05, -1.2305e-05,  8.1402e-06,  ..., -1.4865e-05,
         -8.2545e-06, -1.1836e-05],
        [-9.4765e-06, -8.3680e-06,  5.6017e-06,  ..., -7.5010e-06,
         -3.0798e-06, -6.5627e-06],
        [ 2.8447e-05,  2.1640e-05, -1.6617e-05,  ...,  2.6719e-05,
          1.4770e-05,  1.3929e-05],
        [-4.5131e-05, -3.2235e-05,  2.2316e-05,  ..., -3.7508e-05,
         -2.0927e-05, -2.7899e-05]], device='cuda:0')
optimizer state dict: tensor([[4.9474e-08, 3.6929e-08, 3.9450e-08,  ..., 1.4565e-08, 9.0088e-08,
         1.3332e-08],
        [6.9201e-11, 3.5329e-11, 8.4854e-12,  ..., 4.5526e-11, 6.6539e-12,
         1.5271e-11],
        [1.2268e-09, 6.1981e-10, 1.1564e-10,  ..., 9.9950e-10, 7.9537e-11,
         3.2543e-10],
        [2.9780e-10, 2.1551e-10, 1.1484e-10,  ..., 3.0011e-10, 9.7723e-11,
         1.4272e-10],
        [3.1729e-10, 1.6942e-10, 3.2441e-11,  ..., 2.3165e-10, 3.1419e-11,
         8.4233e-11]], device='cuda:0')
optimizer state dict: 93.0
lr: [1.4954253811094988e-05, 1.4954253811094988e-05]
scheduler_last_epoch: 93


Running epoch 0, step 744, batch 744
Sampled inputs[:2]: tensor([[    0,   278,  4191,  ...,   381,  3020,   352],
        [    0,  1075, 14981,  ...,   221,   380,  1075]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4853e-06, -3.4018e-05, -8.3326e-05,  ...,  2.3810e-05,
          7.7307e-05,  8.4840e-06],
        [-2.2799e-06, -1.4454e-06,  9.8348e-07,  ..., -1.8179e-06,
         -9.2015e-07, -1.2964e-06],
        [-3.7402e-06, -2.4289e-06,  1.6838e-06,  ..., -2.9355e-06,
         -1.5050e-06, -1.9670e-06],
        [-3.2634e-06, -2.0117e-06,  1.4678e-06,  ..., -2.5928e-06,
         -1.2368e-06, -1.9521e-06],
        [-6.1691e-06, -4.0531e-06,  2.7567e-06,  ..., -4.8280e-06,
         -2.5928e-06, -2.9951e-06]], device='cuda:0')
Loss: 1.051993489265442


Running epoch 0, step 745, batch 745
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,    13,  6913,  ...,   278,  1317,  4470]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7428e-05, -5.3629e-05, -1.0287e-04,  ...,  8.9285e-05,
          7.9042e-05, -2.2572e-05],
        [-4.4107e-06, -2.7418e-06,  1.9073e-06,  ..., -3.5986e-06,
         -1.7621e-06, -2.4959e-06],
        [-7.1973e-06, -4.6045e-06,  3.2634e-06,  ..., -5.7518e-06,
         -2.8610e-06, -3.7551e-06],
        [-6.3628e-06, -3.8370e-06,  2.8685e-06,  ..., -5.2005e-06,
         -2.4065e-06, -3.8296e-06],
        [-1.1861e-05, -7.6890e-06,  5.3346e-06,  ..., -9.4175e-06,
         -4.9323e-06, -5.6773e-06]], device='cuda:0')
Loss: 1.0444542169570923


Running epoch 0, step 746, batch 746
Sampled inputs[:2]: tensor([[    0, 14576,  6617,  ...,    17,   367,  1608],
        [    0,  5379,  6922,  ...,  1115, 43884,  2843]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3496e-05, -3.0715e-05, -2.1084e-06,  ...,  1.1781e-04,
          3.0549e-04, -3.4303e-05],
        [-6.8545e-06, -4.3139e-06,  2.8908e-06,  ..., -5.4613e-06,
         -2.6897e-06, -3.8221e-06],
        [-1.1131e-05, -7.2122e-06,  4.9099e-06,  ..., -8.7172e-06,
         -4.3660e-06, -5.7518e-06],
        [-9.7454e-06, -5.9679e-06,  4.2915e-06,  ..., -7.7784e-06,
         -3.6210e-06, -5.7817e-06],
        [-1.8209e-05, -1.1981e-05,  7.9870e-06,  ..., -1.4246e-05,
         -7.5102e-06, -8.6725e-06]], device='cuda:0')
Loss: 1.050264835357666


Running epoch 0, step 747, batch 747
Sampled inputs[:2]: tensor([[   0,  266, 6449,  ...,  474,  221,  474],
        [   0,   17, 4110,  ...,  287, 7115,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3877e-04, -1.5481e-04,  2.2212e-05,  ...,  9.1117e-05,
          3.8874e-04, -9.5128e-06],
        [-9.2536e-06, -5.8562e-06,  3.9712e-06,  ..., -7.2792e-06,
         -3.5502e-06, -5.0142e-06],
        [-1.5035e-05, -9.7752e-06,  6.7130e-06,  ..., -1.1668e-05,
         -5.7891e-06, -7.5847e-06],
        [-1.3068e-05, -8.0690e-06,  5.8562e-06,  ..., -1.0297e-05,
         -4.7535e-06, -7.5251e-06],
        [-2.4498e-05, -1.6183e-05,  1.0863e-05,  ..., -1.9044e-05,
         -9.9540e-06, -1.1429e-05]], device='cuda:0')
Loss: 1.0551365613937378


Running epoch 0, step 748, batch 748
Sampled inputs[:2]: tensor([[   0,  471,   14,  ..., 1260, 2129,  367],
        [   0,  898, 1427,  ...,  508, 1860,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9751e-04, -9.6283e-05, -4.4424e-05,  ...,  1.1704e-04,
          3.0628e-04, -2.7296e-04],
        [-1.1727e-05, -7.3984e-06,  5.0142e-06,  ..., -9.1419e-06,
         -4.4480e-06, -6.3255e-06],
        [-1.9088e-05, -1.2368e-05,  8.4639e-06,  ..., -1.4722e-05,
         -7.2867e-06, -9.6411e-06],
        [-1.6585e-05, -1.0215e-05,  7.3835e-06,  ..., -1.2949e-05,
         -5.9679e-06, -9.4920e-06],
        [-3.1054e-05, -2.0444e-05,  1.3664e-05,  ..., -2.4021e-05,
         -1.2487e-05, -1.4544e-05]], device='cuda:0')
Loss: 1.0478417873382568


Running epoch 0, step 749, batch 749
Sampled inputs[:2]: tensor([[    0,   266,  1890,  ...,   287, 38242,    13],
        [    0, 14652,    12,  ..., 17330,   996,  3294]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0843e-04, -1.3895e-04, -2.0817e-04,  ...,  2.2629e-04,
          4.5324e-04, -3.7802e-04],
        [-1.3977e-05, -8.7917e-06,  6.0648e-06,  ..., -1.0885e-05,
         -5.2378e-06, -7.7039e-06],
        [-2.2843e-05, -1.4767e-05,  1.0252e-05,  ..., -1.7554e-05,
         -8.6278e-06, -1.1742e-05],
        [-1.9804e-05, -1.2152e-05,  8.9929e-06,  ..., -1.5482e-05,
         -7.0333e-06, -1.1638e-05],
        [-3.7163e-05, -2.4408e-05,  1.6540e-05,  ..., -2.8640e-05,
         -1.4812e-05, -1.7688e-05]], device='cuda:0')
Loss: 1.0448611974716187


Running epoch 0, step 750, batch 750
Sampled inputs[:2]: tensor([[    0,   409,   699,  ...,    12,   546,   696],
        [    0,  4653, 21419,  ...,  7845,   300,   565]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7198e-04, -1.0659e-04, -2.1391e-04,  ...,  1.6416e-04,
          4.1162e-04, -4.1751e-04],
        [-1.6406e-05, -1.0371e-05,  7.1526e-06,  ..., -1.2793e-05,
         -6.2063e-06, -9.0078e-06],
        [-2.6807e-05, -1.7434e-05,  1.2085e-05,  ..., -2.0638e-05,
         -1.0230e-05, -1.3739e-05],
        [ 3.4828e-04,  2.8207e-04, -1.5370e-04,  ...,  2.8805e-04,
          1.9031e-04,  9.6671e-05],
        [-4.3660e-05, -2.8849e-05,  1.9535e-05,  ..., -3.3736e-05,
         -1.7568e-05, -2.0742e-05]], device='cuda:0')
Loss: 1.058361291885376


Running epoch 0, step 751, batch 751
Sampled inputs[:2]: tensor([[    0,  2088,  1745,  ...,   293, 16489,    12],
        [    0,   266,  1784,  ...,  1119,  1276,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8115e-04, -3.8534e-04, -5.3562e-05,  ...,  1.3047e-04,
          9.7973e-04, -7.8837e-05],
        [-1.8820e-05, -1.1921e-05,  8.1211e-06,  ..., -1.4670e-05,
         -7.1824e-06, -1.0416e-05],
        [-3.0652e-05, -1.9997e-05,  1.3709e-05,  ..., -2.3603e-05,
         -1.1802e-05, -1.5825e-05],
        [ 3.4506e-04,  2.8006e-04, -1.5236e-04,  ...,  2.8553e-04,
          1.8905e-04,  9.4675e-05],
        [-5.0008e-05, -3.3140e-05,  2.2173e-05,  ..., -3.8624e-05,
         -2.0280e-05, -2.3916e-05]], device='cuda:0')
Loss: 1.0509220361709595
Graident accumulation at epoch 0, step 751, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0030,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.2732e-04,  6.4106e-05, -5.0345e-05,  ...,  7.5377e-05,
         -1.6701e-05,  7.0103e-05],
        [-1.7459e-05, -1.2267e-05,  8.1383e-06,  ..., -1.4845e-05,
         -8.1473e-06, -1.1694e-05],
        [-1.1594e-05, -9.5309e-06,  6.4125e-06,  ..., -9.1113e-06,
         -3.9520e-06, -7.4889e-06],
        [ 6.0108e-05,  4.7482e-05, -3.0191e-05,  ...,  5.2600e-05,
          3.2198e-05,  2.2004e-05],
        [-4.5619e-05, -3.2326e-05,  2.2302e-05,  ..., -3.7619e-05,
         -2.0863e-05, -2.7501e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0201e-08, 3.7041e-08, 3.9414e-08,  ..., 1.4568e-08, 9.0958e-08,
         1.3325e-08],
        [6.9486e-11, 3.5436e-11, 8.5429e-12,  ..., 4.5696e-11, 6.6989e-12,
         1.5364e-11],
        [1.2266e-09, 6.1959e-10, 1.1571e-10,  ..., 9.9906e-10, 7.9597e-11,
         3.2536e-10],
        [4.1657e-10, 2.9373e-10, 1.3794e-10,  ..., 3.8134e-10, 1.3336e-10,
         1.5154e-10],
        [3.1948e-10, 1.7035e-10, 3.2900e-11,  ..., 2.3291e-10, 3.1799e-11,
         8.4721e-11]], device='cuda:0')
optimizer state dict: 94.0
lr: [1.4846498384781962e-05, 1.4846498384781962e-05]
scheduler_last_epoch: 94


Running epoch 0, step 752, batch 752
Sampled inputs[:2]: tensor([[    0,   374,   298,  ..., 11183,    12,   654],
        [    0,   346,   462,  ..., 35247,  2547,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6047e-04,  2.5263e-05, -1.1449e-04,  ...,  9.7116e-05,
         -5.3896e-06, -1.8317e-04],
        [-2.2799e-06, -1.3411e-06,  1.0058e-06,  ..., -1.7807e-06,
         -1.0058e-06, -1.5423e-06],
        [-3.9637e-06, -2.4289e-06,  1.8179e-06,  ..., -3.0398e-06,
         -1.7434e-06, -2.5034e-06],
        [-3.1590e-06, -1.8105e-06,  1.4827e-06,  ..., -2.4885e-06,
         -1.3635e-06, -2.2799e-06],
        [-6.3479e-06, -3.9935e-06,  2.9057e-06,  ..., -4.8578e-06,
         -2.9057e-06, -3.6955e-06]], device='cuda:0')
Loss: 1.0213760137557983


Running epoch 0, step 753, batch 753
Sampled inputs[:2]: tensor([[    0, 22340,   574,  ...,   494,   221,   334],
        [    0,   590,    16,  ...,    13,    35,  1151]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7119e-04, -1.3901e-05, -8.9110e-05,  ...,  1.2757e-04,
          1.1863e-04, -1.9808e-04],
        [-4.6343e-06, -2.8461e-06,  2.0638e-06,  ..., -3.6284e-06,
         -1.8217e-06, -2.7195e-06],
        [-7.8380e-06, -4.9770e-06,  3.6210e-06,  ..., -6.0648e-06,
         -3.1069e-06, -4.3288e-06],
        [-6.4373e-06, -3.8669e-06,  2.9951e-06,  ..., -5.0664e-06,
         -2.4512e-06, -4.0159e-06],
        [ 4.8070e-05,  5.4575e-05, -3.7472e-05,  ...,  9.1562e-05,
          2.5824e-05,  4.1096e-05]], device='cuda:0')
Loss: 1.0565119981765747


Running epoch 0, step 754, batch 754
Sampled inputs[:2]: tensor([[    0,   775,   721,  ...,  5650,   518, 11548],
        [    0,    13, 41550,  ...,    12,   546,  1996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2531e-04, -4.5075e-05, -9.8352e-05,  ...,  1.7813e-04,
          1.3289e-04, -2.5326e-04],
        [-6.9886e-06, -4.4182e-06,  3.0845e-06,  ..., -5.4017e-06,
         -2.6636e-06, -3.7700e-06],
        [-1.1772e-05, -7.6592e-06,  5.3719e-06,  ..., -9.0450e-06,
         -4.5523e-06, -6.0275e-06],
        [-9.7305e-06, -6.0275e-06,  4.4778e-06,  ..., -7.5549e-06,
         -3.5763e-06, -5.5730e-06],
        [ 4.1841e-05,  5.0284e-05, -3.4745e-05,  ...,  8.6794e-05,
          2.3410e-05,  3.8593e-05]], device='cuda:0')
Loss: 1.0541938543319702


Running epoch 0, step 755, batch 755
Sampled inputs[:2]: tensor([[    0, 19191,   266,  ...,   287,   843,  1528],
        [    0,  3941,   257,  ...,    50,   699, 13374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6056e-04, -6.2916e-05, -1.8870e-04,  ...,  2.6801e-04,
          1.3356e-04, -2.1911e-04],
        [-9.2536e-06, -5.9083e-06,  4.1947e-06,  ..., -7.2122e-06,
         -3.5353e-06, -4.8205e-06],
        [-1.5423e-05, -1.0103e-05,  7.1973e-06,  ..., -1.1966e-05,
         -5.9754e-06, -7.6517e-06],
        [-1.2904e-05, -8.0839e-06,  6.0871e-06,  ..., -1.0088e-05,
         -4.7386e-06, -7.1302e-06],
        [ 3.5941e-05,  4.6290e-05, -3.1824e-05,  ...,  8.2055e-05,
          2.0996e-05,  3.6164e-05]], device='cuda:0')
Loss: 1.0407805442810059


Running epoch 0, step 756, batch 756
Sampled inputs[:2]: tensor([[    0,   368, 46614,  ...,  1070,   278,  1028],
        [    0,    26,   874,  ...,    12, 21591,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8933e-04, -1.4399e-05, -4.9240e-05,  ...,  3.5872e-04,
          2.5552e-04, -9.0013e-06],
        [-1.1623e-05, -7.4580e-06,  5.1633e-06,  ..., -9.0450e-06,
         -4.4778e-06, -6.0126e-06],
        [-1.9476e-05, -1.2800e-05,  8.9332e-06,  ..., -1.5065e-05,
         -7.5921e-06, -9.5740e-06],
        [-1.6168e-05, -1.0185e-05,  7.4804e-06,  ..., -1.2606e-05,
         -5.9754e-06, -8.8662e-06],
        [ 2.9473e-05,  4.1939e-05, -2.9067e-05,  ...,  7.7108e-05,
          1.8299e-05,  3.3333e-05]], device='cuda:0')
Loss: 1.0588887929916382


Running epoch 0, step 757, batch 757
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,    14,   221,  ...,   298,   408,  1849]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3737e-04,  1.9744e-04,  7.4721e-05,  ...,  4.4941e-04,
          2.9694e-04,  2.5104e-04],
        [-1.3947e-05, -8.8140e-06,  6.0610e-06,  ..., -1.0855e-05,
         -5.4836e-06, -7.5474e-06],
        [-2.3678e-05, -1.5348e-05,  1.0647e-05,  ..., -1.8284e-05,
         -9.3952e-06, -1.2197e-05],
        [-1.9401e-05, -1.2018e-05,  8.7768e-06,  ..., -1.5125e-05,
         -7.3165e-06, -1.1131e-05],
        [ 2.2947e-05,  3.7916e-05, -2.6415e-05,  ...,  7.2161e-05,
          1.5408e-05,  2.9593e-05]], device='cuda:0')
Loss: 1.0338438749313354


Running epoch 0, step 758, batch 758
Sampled inputs[:2]: tensor([[    0,  4092,  3517,  ..., 23070,    14,   475],
        [    0,   360,   259,  ...,    12,   358,    19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9696e-04,  2.4775e-04,  1.3009e-04,  ...,  6.0693e-04,
          3.0646e-04,  3.8331e-04],
        [-1.6421e-05, -1.0356e-05,  7.0818e-06,  ..., -1.2659e-05,
         -6.3516e-06, -8.7991e-06],
        [-2.7731e-05, -1.7971e-05,  1.2383e-05,  ..., -2.1264e-05,
         -1.0893e-05, -1.4134e-05],
        [-2.2739e-05, -1.4089e-05,  1.0215e-05,  ..., -1.7568e-05,
         -8.4564e-06, -1.2904e-05],
        [ 1.6450e-05,  3.3654e-05, -2.3643e-05,  ...,  6.7363e-05,
          1.2845e-05,  2.6732e-05]], device='cuda:0')
Loss: 1.0413177013397217


Running epoch 0, step 759, batch 759
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0,  292,   17,  ..., 5760, 1345,  578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9820e-04,  2.4796e-04,  1.9198e-04,  ...,  5.9375e-04,
          4.2392e-04,  6.0538e-04],
        [-1.8820e-05, -1.1846e-05,  8.0876e-06,  ..., -1.4491e-05,
         -7.2867e-06, -1.0252e-05],
        [-3.1695e-05, -2.0519e-05,  1.4134e-05,  ..., -2.4259e-05,
         -1.2457e-05, -1.6369e-05],
        [-2.6047e-05, -1.6116e-05,  1.1675e-05,  ..., -2.0102e-05,
         -9.7007e-06, -1.5020e-05],
        [ 9.9828e-06,  2.9482e-05, -2.0797e-05,  ...,  6.2505e-05,
          1.0177e-05,  2.3394e-05]], device='cuda:0')
Loss: 1.0025874376296997
Graident accumulation at epoch 0, step 759, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0031, -0.0266,  ..., -0.0108, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0342, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6477e-04,  8.2491e-05, -2.6113e-05,  ...,  1.2722e-04,
          2.7361e-05,  1.2363e-04],
        [-1.7596e-05, -1.2225e-05,  8.1333e-06,  ..., -1.4810e-05,
         -8.0613e-06, -1.1550e-05],
        [-1.3604e-05, -1.0630e-05,  7.1846e-06,  ..., -1.0626e-05,
         -4.8025e-06, -8.3769e-06],
        [ 5.1493e-05,  4.1122e-05, -2.6004e-05,  ...,  4.5330e-05,
          2.8008e-05,  1.8302e-05],
        [-4.0059e-05, -2.6145e-05,  1.7992e-05,  ..., -2.7607e-05,
         -1.7759e-05, -2.2411e-05]], device='cuda:0')
optimizer state dict: tensor([[5.0310e-08, 3.7065e-08, 3.9411e-08,  ..., 1.4906e-08, 9.1047e-08,
         1.3678e-08],
        [6.9771e-11, 3.5541e-11, 8.5998e-12,  ..., 4.5860e-11, 6.7453e-12,
         1.5454e-11],
        [1.2263e-09, 6.1939e-10, 1.1579e-10,  ..., 9.9865e-10, 7.9672e-11,
         3.2530e-10],
        [4.1683e-10, 2.9369e-10, 1.3794e-10,  ..., 3.8136e-10, 1.3333e-10,
         1.5162e-10],
        [3.1926e-10, 1.7105e-10, 3.3300e-11,  ..., 2.3659e-10, 3.1871e-11,
         8.5184e-11]], device='cuda:0')
optimizer state dict: 95.0
lr: [1.4738002371210062e-05, 1.4738002371210062e-05]
scheduler_last_epoch: 95


Running epoch 0, step 760, batch 760
Sampled inputs[:2]: tensor([[    0,   281,   221,  ...,  2236, 15064,  1458],
        [    0,  6294,   367,  ...,   496,    14,    18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1781e-04,  5.2209e-05, -7.7142e-05,  ..., -3.2298e-05,
         -6.6607e-05, -1.7838e-04],
        [-2.2501e-06, -1.2219e-06,  9.0525e-07,  ..., -1.7509e-06,
         -9.4250e-07, -1.3635e-06],
        [-3.9041e-06, -2.2203e-06,  1.6540e-06,  ..., -2.9951e-06,
         -1.6242e-06, -2.2352e-06],
        [-3.2932e-06, -1.7211e-06,  1.3784e-06,  ..., -2.5928e-06,
         -1.3337e-06, -2.1607e-06],
        [-6.2287e-06, -3.6061e-06,  2.6375e-06,  ..., -4.7386e-06,
         -2.6822e-06, -3.3081e-06]], device='cuda:0')
Loss: 1.0198249816894531


Running epoch 0, step 761, batch 761
Sampled inputs[:2]: tensor([[    0,   287,  2926,  ...,   266, 40854,   287],
        [    0,    18,   998,  ...,  5322,   504,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8226e-04,  9.1790e-05, -2.3855e-05,  ...,  3.1665e-05,
         -1.5865e-04, -1.4759e-04],
        [-4.5300e-06, -2.7046e-06,  1.9260e-06,  ..., -3.5316e-06,
         -1.8775e-06, -2.4587e-06],
        [-7.9274e-06, -4.9025e-06,  3.5018e-06,  ..., -6.1542e-06,
         -3.2932e-06, -4.1202e-06],
        [-6.6459e-06, -3.8520e-06,  2.9355e-06,  ..., -5.2303e-06,
         -2.6599e-06, -3.8892e-06],
        [-1.2606e-05, -7.9274e-06,  5.5283e-06,  ..., -9.7752e-06,
         -5.4389e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.0478487014770508


Running epoch 0, step 762, batch 762
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,  2805,   607, 10848],
        [    0,  2027,   365,  ...,   368,  1782,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1868e-04,  2.1598e-04, -2.0537e-04,  ..., -5.9721e-06,
         -4.7455e-04, -3.0778e-04],
        [-6.8992e-06, -4.1425e-06,  2.8200e-06,  ..., -5.2452e-06,
         -2.9057e-06, -3.8743e-06],
        [-1.2308e-05, -7.6294e-06,  5.1856e-06,  ..., -9.2834e-06,
         -5.2303e-06, -6.6534e-06],
        [-9.9838e-06, -5.8189e-06,  4.2468e-06,  ..., -7.6592e-06,
         -4.0606e-06, -5.9754e-06],
        [-1.9550e-05, -1.2308e-05,  8.2105e-06,  ..., -1.4782e-05,
         -8.5980e-06, -9.8944e-06]], device='cuda:0')
Loss: 0.9989923238754272


Running epoch 0, step 763, batch 763
Sampled inputs[:2]: tensor([[    0,  6541,   287,  ...,  1061,  4786,   292],
        [    0,  4566,   300,  ...,   271,  1644, 16473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.2604e-04,  3.6450e-04, -4.5473e-04,  ...,  1.2910e-04,
         -7.6272e-04, -4.7149e-04],
        [-8.9556e-06, -5.3644e-06,  3.7551e-06,  ..., -6.9439e-06,
         -3.7998e-06, -4.9844e-06],
        [-1.5885e-05, -9.8199e-06,  6.8769e-06,  ..., -1.2204e-05,
         -6.7875e-06, -8.4788e-06],
        [-1.3053e-05, -7.5921e-06,  5.7071e-06,  ..., -1.0207e-05,
         -5.3495e-06, -7.7412e-06],
        [-2.5243e-05, -1.5840e-05,  1.0878e-05,  ..., -1.9431e-05,
         -1.1176e-05, -1.2562e-05]], device='cuda:0')
Loss: 1.0381120443344116


Running epoch 0, step 764, batch 764
Sampled inputs[:2]: tensor([[    0,  2906, 46441,  ..., 39156,   287, 11452],
        [    0,    35,  3815,  ...,   278,  7097,  4601]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0606e-03,  4.2069e-04, -4.1449e-04,  ...,  1.9531e-04,
         -7.5941e-04, -4.4101e-04],
        [-1.1340e-05, -6.8024e-06,  4.7237e-06,  ..., -8.7097e-06,
         -4.8801e-06, -6.3106e-06],
        [-2.0206e-05, -1.2517e-05,  8.7172e-06,  ..., -1.5408e-05,
         -8.7842e-06, -1.0759e-05],
        [-1.6361e-05, -9.5591e-06,  7.0930e-06,  ..., -1.2666e-05,
         -6.8322e-06, -9.6634e-06],
        [-3.2067e-05, -2.0161e-05,  1.3769e-05,  ..., -2.4498e-05,
         -1.4409e-05, -1.5944e-05]], device='cuda:0')
Loss: 1.052409291267395


Running epoch 0, step 765, batch 765
Sampled inputs[:2]: tensor([[   0, 6795, 1728,  ...,  578,   19,   14],
        [   0,  278, 5717,  ..., 5342, 5147,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0542e-03,  4.4462e-04, -2.9370e-04,  ...,  2.5640e-04,
         -6.6587e-04, -3.7459e-04],
        [-1.3575e-05, -8.2105e-06,  5.6550e-06,  ..., -1.0394e-05,
         -5.7630e-06, -7.4655e-06],
        [-2.4199e-05, -1.5095e-05,  1.0453e-05,  ..., -1.8388e-05,
         -1.0379e-05, -1.2755e-05],
        [-1.9714e-05, -1.1630e-05,  8.5309e-06,  ..., -1.5184e-05,
         -8.0988e-06, -1.1496e-05],
        [-3.8505e-05, -2.4334e-05,  1.6540e-05,  ..., -2.9296e-05,
         -1.7062e-05, -1.8954e-05]], device='cuda:0')
Loss: 1.0499571561813354


Running epoch 0, step 766, batch 766
Sampled inputs[:2]: tensor([[   0,  278, 4575,  ..., 1220,  278, 4575],
        [   0,  271,  266,  ..., 1034, 1928,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3257e-03,  6.1950e-04, -5.4054e-04,  ...,  3.8002e-04,
         -1.1262e-03, -8.6182e-04],
        [-1.5751e-05, -9.5740e-06,  6.5863e-06,  ..., -1.2107e-05,
         -6.6534e-06, -8.5384e-06],
        [ 4.8919e-05,  5.0830e-05, -3.5740e-05,  ...,  6.7920e-05,
          4.5411e-05,  1.8828e-05],
        [-2.2948e-05, -1.3612e-05,  9.9763e-06,  ..., -1.7747e-05,
         -9.3728e-06, -1.3232e-05],
        [-4.4733e-05, -2.8387e-05,  1.9282e-05,  ..., -3.4183e-05,
         -1.9714e-05, -2.1696e-05]], device='cuda:0')
Loss: 1.0426445007324219


Running epoch 0, step 767, batch 767
Sampled inputs[:2]: tensor([[   0,   12,  287,  ..., 2336,  221,  334],
        [   0, 1295, 1178,  ..., 4808,  287,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4053e-03,  5.9863e-04, -5.6286e-04,  ...,  5.2827e-04,
         -1.0486e-03, -7.0880e-04],
        [-1.8001e-05, -1.1064e-05,  7.5363e-06,  ..., -1.3895e-05,
         -7.6592e-06, -9.6560e-06],
        [ 4.5074e-05,  4.8237e-05, -3.4027e-05,  ...,  6.4881e-05,
          4.3690e-05,  1.7003e-05],
        [-2.6107e-05, -1.5669e-05,  1.1355e-05,  ..., -2.0251e-05,
         -1.0729e-05, -1.4901e-05],
        [-5.0902e-05, -3.2589e-05,  2.2039e-05,  ..., -3.9041e-05,
         -2.2575e-05, -2.4423e-05]], device='cuda:0')
Loss: 1.0430363416671753
Graident accumulation at epoch 0, step 767, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0151, -0.0280,  ...,  0.0287, -0.0151, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.7596e-06,  1.3411e-04, -7.9787e-05,  ...,  1.6732e-04,
         -8.0232e-05,  4.0388e-05],
        [-1.7636e-05, -1.2109e-05,  8.0736e-06,  ..., -1.4718e-05,
         -8.0211e-06, -1.1361e-05],
        [-7.7362e-06, -4.7431e-06,  3.0634e-06,  ..., -3.0754e-06,
          4.6727e-08, -5.8389e-06],
        [ 4.3733e-05,  3.5443e-05, -2.2268e-05,  ...,  3.8772e-05,
          2.4134e-05,  1.4981e-05],
        [-4.1143e-05, -2.6790e-05,  1.8397e-05,  ..., -2.8750e-05,
         -1.8240e-05, -2.2613e-05]], device='cuda:0')
optimizer state dict: tensor([[5.2234e-08, 3.7386e-08, 3.9689e-08,  ..., 1.5170e-08, 9.2055e-08,
         1.4166e-08],
        [7.0025e-11, 3.5628e-11, 8.6480e-12,  ..., 4.6007e-11, 6.7972e-12,
         1.5532e-11],
        [1.2271e-09, 6.2109e-10, 1.1684e-10,  ..., 1.0019e-09, 8.1501e-11,
         3.2526e-10],
        [4.1710e-10, 2.9364e-10, 1.3793e-10,  ..., 3.8139e-10, 1.3331e-10,
         1.5169e-10],
        [3.2153e-10, 1.7194e-10, 3.3752e-11,  ..., 2.3788e-10, 3.2349e-11,
         8.5695e-11]], device='cuda:0')
optimizer state dict: 96.0
lr: [1.4628782349517233e-05, 1.4628782349517233e-05]
scheduler_last_epoch: 96


Running epoch 0, step 768, batch 768
Sampled inputs[:2]: tensor([[    0, 23230,    12,  ...,  5092,   741,   266],
        [    0,    14,  1075,  ..., 22182,  5948,  8401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8365e-05,  1.6555e-04,  5.9540e-05,  ...,  1.9120e-04,
         -7.2827e-05,  2.3825e-05],
        [-2.1309e-06, -1.3933e-06,  9.8348e-07,  ..., -1.6689e-06,
         -8.8289e-07, -1.1548e-06],
        [-3.7700e-06, -2.5481e-06,  1.7956e-06,  ..., -2.9504e-06,
         -1.5944e-06, -1.9670e-06],
        [-3.1590e-06, -2.0415e-06,  1.5274e-06,  ..., -2.4736e-06,
         -1.2591e-06, -1.8328e-06],
        [-5.8413e-06, -4.0233e-06,  2.7567e-06,  ..., -4.6194e-06,
         -2.5928e-06, -2.8312e-06]], device='cuda:0')
Loss: 1.0150014162063599


Running epoch 0, step 769, batch 769
Sampled inputs[:2]: tensor([[    0,   565,    27,  ...,    88,  4451,    14],
        [    0,   292,    48,  ...,   199, 19047,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1854e-04,  2.8270e-04, -2.9018e-05,  ...,  1.9120e-04,
         -2.9564e-04, -2.2299e-04],
        [-4.3511e-06, -2.7642e-06,  1.8701e-06,  ..., -3.3900e-06,
         -1.8589e-06, -2.4438e-06],
        [-7.7635e-06, -5.0664e-06,  3.4496e-06,  ..., -6.0201e-06,
         -3.3602e-06, -4.1872e-06],
        [-6.3181e-06, -3.9414e-06,  2.8238e-06,  ..., -4.9174e-06,
         -2.6003e-06, -3.7551e-06],
        [-1.2159e-05, -8.0764e-06,  5.3793e-06,  ..., -9.5069e-06,
         -5.4836e-06, -6.1542e-06]], device='cuda:0')
Loss: 1.0288184881210327


Running epoch 0, step 770, batch 770
Sampled inputs[:2]: tensor([[    0,  1192, 11929,  ...,   266,  1551,  1860],
        [    0,    14,   298,  ...,   333,   199,   769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3581e-04,  3.2522e-04, -1.3772e-04,  ...,  1.7600e-04,
         -4.9988e-04, -4.1934e-04],
        [-6.5267e-06, -3.9563e-06,  2.7232e-06,  ..., -5.0813e-06,
         -2.9318e-06, -3.8967e-06],
        [-1.1876e-05, -7.4059e-06,  5.1558e-06,  ..., -9.1493e-06,
         -5.3421e-06, -6.7651e-06],
        [-9.5367e-06, -5.6326e-06,  4.1202e-06,  ..., -7.4208e-06,
         -4.1351e-06, -6.0499e-06],
        [-1.8597e-05, -1.1832e-05,  8.0913e-06,  ..., -1.4395e-05,
         -8.6576e-06, -9.9242e-06]], device='cuda:0')
Loss: 1.0047203302383423


Running epoch 0, step 771, batch 771
Sampled inputs[:2]: tensor([[    0,    13,    19,  ..., 22111,  2489,    14],
        [    0,    73,    14,  ...,   650,    13,  3658]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7591e-04,  4.1064e-04,  5.3626e-05,  ...,  2.0352e-04,
         -4.7051e-04, -3.3863e-04],
        [-8.7917e-06, -5.4315e-06,  3.7141e-06,  ..., -6.8024e-06,
         -3.9078e-06, -5.0217e-06],
        [-1.6019e-05, -1.0163e-05,  7.0035e-06,  ..., -1.2308e-05,
         -7.1526e-06, -8.7768e-06],
        [-1.2800e-05, -7.7188e-06,  5.5954e-06,  ..., -9.8944e-06,
         -5.4836e-06, -7.7635e-06],
        [-2.5064e-05, -1.6183e-05,  1.0937e-05,  ..., -1.9372e-05,
         -1.1608e-05, -1.2890e-05]], device='cuda:0')
Loss: 1.0593682527542114


Running epoch 0, step 772, batch 772
Sampled inputs[:2]: tensor([[    0,   300,  2607,  ...,  1279,   368,   266],
        [    0,   824,   278,  ..., 10513,  6909,  4077]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4316e-04,  4.1064e-04,  1.5833e-04,  ...,  2.6252e-04,
         -4.0306e-04, -1.1952e-04],
        [-1.0952e-05, -6.8024e-06,  4.6007e-06,  ..., -8.5160e-06,
         -4.8578e-06, -6.1616e-06],
        [-1.9863e-05, -1.2651e-05,  8.6576e-06,  ..., -1.5348e-05,
         -8.8662e-06, -1.0729e-05],
        [-1.5974e-05, -9.6858e-06,  6.9514e-06,  ..., -1.2413e-05,
         -6.8173e-06, -9.5516e-06],
        [-3.1054e-05, -2.0117e-05,  1.3515e-05,  ..., -2.4080e-05,
         -1.4365e-05, -1.5691e-05]], device='cuda:0')
Loss: 1.049998164176941


Running epoch 0, step 773, batch 773
Sampled inputs[:2]: tensor([[    0, 13964,    13,  ...,    14,   560,   199],
        [    0,  1716,   271,  ...,   292,    78,  1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0815e-04,  4.3801e-04,  1.4719e-05,  ...,  2.8201e-04,
         -5.4978e-04, -3.9260e-04],
        [-1.3128e-05, -8.2254e-06,  5.5507e-06,  ..., -1.0230e-05,
         -5.8189e-06, -7.1749e-06],
        [-2.3767e-05, -1.5289e-05,  1.0416e-05,  ..., -1.8448e-05,
         -1.0617e-05, -1.2502e-05],
        [-1.9133e-05, -1.1727e-05,  8.3745e-06,  ..., -1.4901e-05,
         -8.1658e-06, -1.1116e-05],
        [-3.7193e-05, -2.4289e-05,  1.6242e-05,  ..., -2.8938e-05,
         -1.7211e-05, -1.8299e-05]], device='cuda:0')
Loss: 1.0423115491867065


Running epoch 0, step 774, batch 774
Sampled inputs[:2]: tensor([[    0,   638,  1862,  ...,    14,  7869,    14],
        [    0,  6702, 18279,  ...,    14, 47571,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.5681e-04,  7.9203e-04, -2.2692e-04,  ...,  2.5807e-04,
         -5.9778e-04, -2.8675e-04],
        [-1.5244e-05, -9.5367e-06,  6.3218e-06,  ..., -1.1876e-05,
         -6.8620e-06, -8.5607e-06],
        [-2.7642e-05, -1.7762e-05,  1.1906e-05,  ..., -2.1398e-05,
         -1.2554e-05, -1.4871e-05],
        [-2.2158e-05, -1.3568e-05,  9.5144e-06,  ..., -1.7270e-05,
         -9.6560e-06, -1.3188e-05],
        [-4.3303e-05, -2.8282e-05,  1.8641e-05,  ..., -3.3617e-05,
         -2.0370e-05, -2.1756e-05]], device='cuda:0')
Loss: 0.9913907647132874


Running epoch 0, step 775, batch 775
Sampled inputs[:2]: tensor([[    0,    21,    13,  ...,    14,   747,   806],
        [    0,   278, 39533,  ...,   277,  1395, 47607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0110e-03,  7.5487e-04, -2.2725e-04,  ...,  3.1636e-04,
         -6.2855e-04, -2.7025e-04],
        [-1.7494e-05, -1.1019e-05,  7.2643e-06,  ..., -1.3627e-05,
         -7.8045e-06, -9.6411e-06],
        [-3.1635e-05, -2.0444e-05,  1.3612e-05,  ..., -2.4498e-05,
         -1.4238e-05, -1.6741e-05],
        [-2.5392e-05, -1.5654e-05,  1.0908e-05,  ..., -1.9774e-05,
         -1.0945e-05, -1.4834e-05],
        [-4.9740e-05, -3.2663e-05,  2.1368e-05,  ..., -3.8654e-05,
         -2.3201e-05, -2.4602e-05]], device='cuda:0')
Loss: 1.0521255731582642
Graident accumulation at epoch 0, step 775, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0230, -0.0195],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0160,  0.0152, -0.0280,  ...,  0.0287, -0.0150, -0.0178]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.4112e-05,  1.9618e-04, -9.4534e-05,  ...,  1.8222e-04,
         -1.3506e-04,  9.3235e-06],
        [-1.7622e-05, -1.2000e-05,  7.9926e-06,  ..., -1.4609e-05,
         -7.9994e-06, -1.1189e-05],
        [-1.0126e-05, -6.3132e-06,  4.1183e-06,  ..., -5.2176e-06,
         -1.3818e-06, -6.9292e-06],
        [ 3.6820e-05,  3.0333e-05, -1.8951e-05,  ...,  3.2917e-05,
          2.0626e-05,  1.2000e-05],
        [-4.2003e-05, -2.7377e-05,  1.8694e-05,  ..., -2.9740e-05,
         -1.8736e-05, -2.2812e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3204e-08, 3.7919e-08, 3.9701e-08,  ..., 1.5255e-08, 9.2358e-08,
         1.4225e-08],
        [7.0261e-11, 3.5714e-11, 8.6921e-12,  ..., 4.6147e-11, 6.8513e-12,
         1.5609e-11],
        [1.2269e-09, 6.2089e-10, 1.1690e-10,  ..., 1.0015e-09, 8.1623e-11,
         3.2522e-10],
        [4.1733e-10, 2.9360e-10, 1.3791e-10,  ..., 3.8140e-10, 1.3329e-10,
         1.5176e-10],
        [3.2368e-10, 1.7284e-10, 3.4175e-11,  ..., 2.3913e-10, 3.2855e-11,
         8.6214e-11]], device='cuda:0')
optimizer state dict: 97.0
lr: [1.4518855009476186e-05, 1.4518855009476186e-05]
scheduler_last_epoch: 97


Running epoch 0, step 776, batch 776
Sampled inputs[:2]: tensor([[    0,   221, 18844,  ...,   199, 10174,   259],
        [    0,   391,  1351,  ...,    13,    40,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8589e-05, -3.0144e-05,  1.1160e-04,  ...,  3.8550e-05,
          1.9224e-05,  1.8570e-04],
        [-2.1160e-06, -1.2666e-06,  8.9779e-07,  ..., -1.6093e-06,
         -9.9838e-07, -1.3486e-06],
        [-3.8743e-06, -2.3842e-06,  1.7211e-06,  ..., -2.9206e-06,
         -1.8328e-06, -2.3693e-06],
        [-3.1888e-06, -1.8477e-06,  1.4007e-06,  ..., -2.4140e-06,
         -1.4529e-06, -2.1607e-06],
        [-6.1691e-06, -3.9041e-06,  2.7269e-06,  ..., -4.6790e-06,
         -3.0398e-06, -3.5316e-06]], device='cuda:0')
Loss: 1.0388462543487549


Running epoch 0, step 777, batch 777
Sampled inputs[:2]: tensor([[   0,  266, 8802,  ..., 8401,    9,  287],
        [   0, 1016,  271,  ...,  461,  616,  993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2768e-05, -4.3247e-05,  7.5144e-05,  ..., -4.0265e-05,
          1.2562e-04,  1.9152e-04],
        [-4.3511e-06, -2.7269e-06,  1.8440e-06,  ..., -3.3602e-06,
         -2.1905e-06, -2.6152e-06],
        [-7.9274e-06, -5.0962e-06,  3.5092e-06,  ..., -6.0797e-06,
         -4.0233e-06, -4.5598e-06],
        [-6.3777e-06, -3.8743e-06,  2.7865e-06,  ..., -4.9025e-06,
         -3.1218e-06, -4.0531e-06],
        [-1.2636e-05, -8.3447e-06,  5.5879e-06,  ..., -9.7454e-06,
         -6.6459e-06, -6.8247e-06]], device='cuda:0')
Loss: 1.059500813484192


Running epoch 0, step 778, batch 778
Sampled inputs[:2]: tensor([[    0,   607,   259,  ...,   271,   669,    12],
        [    0,  3984, 13077,  ...,   287,   650,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0480e-04,  1.6726e-05,  1.6174e-04,  ...,  1.4969e-04,
          3.3259e-04,  2.5763e-04],
        [-6.4671e-06, -4.1276e-06,  2.7753e-06,  ..., -5.0366e-06,
         -3.2485e-06, -3.8147e-06],
        [-1.1802e-05, -7.7337e-06,  5.2750e-06,  ..., -9.1642e-06,
         -5.9903e-06, -6.7055e-06],
        [-9.5367e-06, -5.9307e-06,  4.2319e-06,  ..., -7.4059e-06,
         -4.6566e-06, -5.9381e-06],
        [-1.8865e-05, -1.2666e-05,  8.4192e-06,  ..., -1.4722e-05,
         -9.9540e-06, -1.0058e-05]], device='cuda:0')
Loss: 1.0609403848648071


Running epoch 0, step 779, batch 779
Sampled inputs[:2]: tensor([[   0,  221, 6872,  ...,  806,  518,  266],
        [   0,  508,  927,  ..., 1390,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7433e-04, -6.5960e-05,  3.5008e-04,  ...,  1.9571e-04,
          6.1764e-04,  4.0484e-04],
        [-8.2925e-06, -5.2229e-06,  3.7253e-06,  ..., -6.5193e-06,
         -4.1090e-06, -5.1260e-06],
        [-1.5065e-05, -9.7901e-06,  7.0408e-06,  ..., -1.1757e-05,
         -7.5549e-06, -8.8662e-06],
        [-1.2383e-05, -7.5325e-06,  5.8562e-06,  ..., -9.7752e-06,
         -5.9679e-06, -8.2329e-06],
        [-2.4021e-05, -1.6004e-05,  1.1161e-05,  ..., -1.8805e-05,
         -1.2547e-05, -1.3143e-05]], device='cuda:0')
Loss: 1.0023564100265503


Running epoch 0, step 780, batch 780
Sampled inputs[:2]: tensor([[   0, 1358,  367,  ..., 1758, 2921,   12],
        [   0,  560,  199,  ...,   29,  445,   16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1867e-04, -1.8880e-04,  7.5058e-04,  ...,  8.2603e-05,
          1.3935e-03,  1.1231e-03],
        [-1.0572e-05, -6.5118e-06,  4.5747e-06,  ..., -8.1658e-06,
         -5.2191e-06, -6.6981e-06],
        [-1.9416e-05, -1.2338e-05,  8.7321e-06,  ..., -1.4886e-05,
         -9.7007e-06, -1.1757e-05],
        [-1.5706e-05, -9.3430e-06,  7.1377e-06,  ..., -1.2159e-05,
         -7.5325e-06, -1.0647e-05],
        [-3.0786e-05, -2.0087e-05,  1.3784e-05,  ..., -2.3723e-05,
         -1.6049e-05, -1.7345e-05]], device='cuda:0')
Loss: 1.0277599096298218


Running epoch 0, step 781, batch 781
Sampled inputs[:2]: tensor([[    0,  7432,   287,  ...,    12,   461,  2652],
        [    0,   680,   993,  ...,   699, 11426,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0955e-04, -3.4816e-04,  7.7125e-04,  ...,  1.8617e-04,
          1.3783e-03,  1.1828e-03],
        [-1.2688e-05, -7.9498e-06,  5.5507e-06,  ..., -9.8199e-06,
         -6.2771e-06, -7.9125e-06],
        [-2.3201e-05, -1.4991e-05,  1.0528e-05,  ..., -1.7837e-05,
         -1.1608e-05, -1.3843e-05],
        [-1.8835e-05, -1.1414e-05,  8.6501e-06,  ..., -1.4618e-05,
         -9.0450e-06, -1.2614e-05],
        [-3.6776e-05, -2.4348e-05,  1.6585e-05,  ..., -2.8431e-05,
         -1.9208e-05, -2.0429e-05]], device='cuda:0')
Loss: 1.0503320693969727


Running epoch 0, step 782, batch 782
Sampled inputs[:2]: tensor([[   0, 2734, 2703,  ..., 7851,  280, 1713],
        [   0,   12,  266,  ..., 1125,  609,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3160e-04, -4.4887e-04,  7.5251e-04,  ...,  2.7817e-04,
          1.3162e-03,  1.1866e-03],
        [-1.4745e-05, -9.2760e-06,  6.4559e-06,  ..., -1.1429e-05,
         -7.2978e-06, -9.0972e-06],
        [-2.6882e-05, -1.7464e-05,  1.2219e-05,  ..., -2.0713e-05,
         -1.3471e-05, -1.5855e-05],
        [-2.1830e-05, -1.3299e-05,  1.0021e-05,  ..., -1.6987e-05,
         -1.0505e-05, -1.4469e-05],
        [-4.2647e-05, -2.8372e-05,  1.9297e-05,  ..., -3.3021e-05,
         -2.2307e-05, -2.3425e-05]], device='cuda:0')
Loss: 1.0328456163406372


Running epoch 0, step 783, batch 783
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,   360,   508,  ...,   259,   554,  1319]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0868e-04, -6.2519e-04,  5.3768e-04,  ...,  2.6194e-04,
          1.2666e-03,  1.1747e-03],
        [-1.6667e-05, -1.0394e-05,  7.4543e-06,  ..., -1.2971e-05,
         -8.2292e-06, -1.0341e-05],
        [-3.0234e-05, -1.9535e-05,  1.4029e-05,  ..., -2.3410e-05,
         -1.5154e-05, -1.7881e-05],
        [-2.4840e-05, -1.4976e-05,  1.1727e-05,  ..., -1.9446e-05,
         -1.1943e-05, -1.6615e-05],
        [-4.8012e-05, -3.1769e-05,  2.2143e-05,  ..., -3.7372e-05,
         -2.5153e-05, -2.6375e-05]], device='cuda:0')
Loss: 1.0132750272750854
Graident accumulation at epoch 0, step 783, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0230, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0160,  0.0152, -0.0280,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3833e-05,  1.1404e-04, -3.1312e-05,  ...,  1.9020e-04,
          5.1020e-06,  1.2586e-04],
        [-1.7526e-05, -1.1839e-05,  7.9388e-06,  ..., -1.4445e-05,
         -8.0224e-06, -1.1104e-05],
        [-1.2137e-05, -7.6354e-06,  5.1094e-06,  ..., -7.0368e-06,
         -2.7590e-06, -8.0244e-06],
        [ 3.0654e-05,  2.5802e-05, -1.5883e-05,  ...,  2.7681e-05,
          1.7369e-05,  9.1383e-06],
        [-4.2604e-05, -2.7816e-05,  1.9039e-05,  ..., -3.0504e-05,
         -1.9378e-05, -2.3168e-05]], device='cuda:0')
optimizer state dict: tensor([[5.3653e-08, 3.8272e-08, 3.9950e-08,  ..., 1.5308e-08, 9.3870e-08,
         1.5591e-08],
        [7.0469e-11, 3.5786e-11, 8.7389e-12,  ..., 4.6269e-11, 6.9122e-12,
         1.5701e-11],
        [1.2266e-09, 6.2065e-10, 1.1698e-10,  ..., 1.0010e-09, 8.1771e-11,
         3.2521e-10],
        [4.1753e-10, 2.9353e-10, 1.3791e-10,  ..., 3.8140e-10, 1.3330e-10,
         1.5188e-10],
        [3.2566e-10, 1.7367e-10, 3.4631e-11,  ..., 2.4029e-10, 3.3455e-11,
         8.6824e-11]], device='cuda:0')
optimizer state dict: 98.0
lr: [1.4408237148944047e-05, 1.4408237148944047e-05]
scheduler_last_epoch: 98


Running epoch 0, step 784, batch 784
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0,  199, 2834,  ..., 1236,  768, 4316]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1472e-04, -5.0616e-04,  1.6814e-04,  ..., -8.9757e-05,
          6.7538e-04, -2.7635e-05],
        [-1.7136e-06, -9.4250e-07,  9.2760e-07,  ..., -1.4752e-06,
         -1.1250e-06, -1.4231e-06],
        [-3.1441e-06, -1.8775e-06,  1.7807e-06,  ..., -2.6226e-06,
         -2.0266e-06, -2.3544e-06],
        [-2.7269e-06, -1.3411e-06,  1.6838e-06,  ..., -2.4438e-06,
         -1.7881e-06, -2.6226e-06],
        [-5.1260e-06, -3.2037e-06,  2.8312e-06,  ..., -4.2319e-06,
         -3.4124e-06, -3.4124e-06]], device='cuda:0')
Loss: 0.9843761920928955


Running epoch 0, step 785, batch 785
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2707e-04, -6.0472e-04,  2.8426e-04,  ..., -1.2092e-04,
          8.8541e-04,  7.6631e-05],
        [-3.6955e-06, -2.2091e-06,  1.8105e-06,  ..., -3.0547e-06,
         -2.3022e-06, -2.7269e-06],
        [-6.7055e-06, -4.2617e-06,  3.4496e-06,  ..., -5.4687e-06,
         -4.1723e-06, -4.6194e-06],
        [-5.7220e-06, -3.2112e-06,  3.0845e-06,  ..., -4.8578e-06,
         -3.5465e-06, -4.7237e-06],
        [-1.0729e-05, -7.0184e-06,  5.4538e-06,  ..., -8.7321e-06,
         -6.9141e-06, -6.7353e-06]], device='cuda:0')
Loss: 1.0413285493850708


Running epoch 0, step 786, batch 786
Sampled inputs[:2]: tensor([[   0, 4599, 9005,  ...,  809,   13, 1875],
        [   0, 1732,  699,  ...,  417,  199, 1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8368e-04, -6.4276e-04,  4.3763e-04,  ..., -5.2279e-05,
          1.1283e-03,  1.3686e-04],
        [-5.5134e-06, -3.2820e-06,  2.7493e-06,  ..., -4.5970e-06,
         -3.2187e-06, -3.9563e-06],
        [-9.9987e-06, -6.3181e-06,  5.2005e-06,  ..., -8.2552e-06,
         -5.8636e-06, -6.7353e-06],
        [-8.6427e-06, -4.8503e-06,  4.6939e-06,  ..., -7.3761e-06,
         -4.9770e-06, -6.8843e-06],
        [-1.5914e-05, -1.0356e-05,  8.1956e-06,  ..., -1.3143e-05,
         -9.7305e-06, -9.8050e-06]], device='cuda:0')
Loss: 1.0232807397842407


Running epoch 0, step 787, batch 787
Sampled inputs[:2]: tensor([[    0,    30,  1869,  ...,  4998, 44266,    12],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3830e-04, -6.7687e-04,  7.0327e-04,  ..., -7.7881e-05,
          1.4025e-03,  6.0321e-04],
        [-7.6145e-06, -4.6603e-06,  3.7029e-06,  ..., -6.2436e-06,
         -4.3288e-06, -5.2750e-06],
        [-1.3798e-05, -8.8960e-06,  6.9886e-06,  ..., -1.1235e-05,
         -7.9349e-06, -9.0450e-06],
        [-1.1742e-05, -6.8471e-06,  6.1765e-06,  ..., -9.8050e-06,
         -6.5863e-06, -8.9407e-06],
        [-2.1845e-05, -1.4499e-05,  1.0982e-05,  ..., -1.7852e-05,
         -1.3113e-05, -1.3188e-05]], device='cuda:0')
Loss: 1.0545843839645386


Running epoch 0, step 788, batch 788
Sampled inputs[:2]: tensor([[    0, 14161,  1241,  ..., 15255,   768,  4239],
        [    0,   278,  5492,  ...,   328,   995,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7770e-04, -7.8160e-04,  7.1480e-04,  ..., -3.2875e-05,
          1.4726e-03,  7.4573e-04],
        [-9.7305e-06, -5.9642e-06,  4.6268e-06,  ..., -7.8306e-06,
         -5.4538e-06, -6.6757e-06],
        [-1.7792e-05, -1.1414e-05,  8.7619e-06,  ..., -1.4216e-05,
         -1.0066e-05, -1.1623e-05],
        [-1.4856e-05, -8.7023e-06,  7.5847e-06,  ..., -1.2144e-05,
         -8.1882e-06, -1.1116e-05],
        [-2.8133e-05, -1.8552e-05,  1.3754e-05,  ..., -2.2590e-05,
         -1.6600e-05, -1.7032e-05]], device='cuda:0')
Loss: 1.0530025959014893


Running epoch 0, step 789, batch 789
Sampled inputs[:2]: tensor([[   0,  858,   13,  ..., 2253,  847,  300],
        [   0,   13, 5005,  ...,  654,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0421e-03, -7.5848e-04,  8.1090e-04,  ..., -1.7307e-04,
          1.6688e-03,  8.3018e-04],
        [-1.1742e-05, -7.2904e-06,  5.5507e-06,  ..., -9.4473e-06,
         -6.6310e-06, -8.0019e-06],
        [-2.1368e-05, -1.3843e-05,  1.0461e-05,  ..., -1.7077e-05,
         -1.2182e-05, -1.3903e-05],
        [-1.7807e-05, -1.0580e-05,  9.0078e-06,  ..., -1.4529e-05,
         -9.8720e-06, -1.3232e-05],
        [-3.3885e-05, -2.2545e-05,  1.6451e-05,  ..., -2.7210e-05,
         -2.0117e-05, -2.0415e-05]], device='cuda:0')
Loss: 1.0372403860092163


Running epoch 0, step 790, batch 790
Sampled inputs[:2]: tensor([[   0, 3825, 1626,  ..., 5096, 3775,  266],
        [   0,  257,   13,  ...,  328,  630, 1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1184e-03, -7.9585e-04,  9.0607e-04,  ..., -2.2940e-04,
          1.7444e-03,  9.7347e-04],
        [-1.3769e-05, -8.6464e-06,  6.5044e-06,  ..., -1.1064e-05,
         -7.6964e-06, -9.1791e-06],
        [-2.5094e-05, -1.6406e-05,  1.2249e-05,  ..., -2.0042e-05,
         -1.4149e-05, -1.6034e-05],
        [-2.0832e-05, -1.2532e-05,  1.0490e-05,  ..., -1.6943e-05,
         -1.1414e-05, -1.5125e-05],
        [-3.9726e-05, -2.6628e-05,  1.9222e-05,  ..., -3.1918e-05,
         -2.3335e-05, -2.3529e-05]], device='cuda:0')
Loss: 1.052347183227539


Running epoch 0, step 791, batch 791
Sampled inputs[:2]: tensor([[    0, 29883,   680,  ...,  3363,  1049,   292],
        [    0,   334,   287,  ...,  1348,  6139,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1520e-03, -7.9324e-04,  7.6763e-04,  ..., -2.2929e-04,
          1.7926e-03,  8.9355e-04],
        [-1.5676e-05, -9.8832e-06,  7.4320e-06,  ..., -1.2666e-05,
         -8.7544e-06, -1.0319e-05],
        [-2.8551e-05, -1.8731e-05,  1.3992e-05,  ..., -2.2948e-05,
         -1.6086e-05, -1.8016e-05],
        [-2.3797e-05, -1.4387e-05,  1.2033e-05,  ..., -1.9476e-05,
         -1.3031e-05, -1.7077e-05],
        [-4.5329e-05, -3.0503e-05,  2.2039e-05,  ..., -3.6657e-05,
         -2.6628e-05, -2.6524e-05]], device='cuda:0')
Loss: 1.0377410650253296
Graident accumulation at epoch 0, step 791, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0230, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0160,  0.0152, -0.0281,  ...,  0.0287, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0275e-04,  2.3316e-05,  4.8582e-05,  ...,  1.4825e-04,
          1.8386e-04,  2.0263e-04],
        [-1.7341e-05, -1.1643e-05,  7.8881e-06,  ..., -1.4267e-05,
         -8.0956e-06, -1.1025e-05],
        [-1.3778e-05, -8.7450e-06,  5.9977e-06,  ..., -8.6279e-06,
         -4.0917e-06, -9.0235e-06],
        [ 2.5209e-05,  2.1783e-05, -1.3091e-05,  ...,  2.2965e-05,
          1.4329e-05,  6.5168e-06],
        [-4.2876e-05, -2.8085e-05,  1.9339e-05,  ..., -3.1119e-05,
         -2.0103e-05, -2.3504e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4927e-08, 3.8863e-08, 4.0499e-08,  ..., 1.5346e-08, 9.6990e-08,
         1.6374e-08],
        [7.0644e-11, 3.5848e-11, 8.7854e-12,  ..., 4.6383e-11, 6.9819e-12,
         1.5791e-11],
        [1.2262e-09, 6.2038e-10, 1.1706e-10,  ..., 1.0005e-09, 8.1948e-11,
         3.2521e-10],
        [4.1767e-10, 2.9344e-10, 1.3792e-10,  ..., 3.8140e-10, 1.3334e-10,
         1.5202e-10],
        [3.2739e-10, 1.7443e-10, 3.5082e-11,  ..., 2.4139e-10, 3.4130e-11,
         8.7440e-11]], device='cuda:0')
optimizer state dict: 99.0
lr: [1.4296945671295503e-05, 1.4296945671295503e-05]
scheduler_last_epoch: 99


Running epoch 0, step 792, batch 792
Sampled inputs[:2]: tensor([[    0, 10348,  2994,  ...,   266, 24089, 10607],
        [    0, 24674,   513,  ...,  6099,    12,  4863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2285e-05, -1.3130e-04, -3.9414e-05,  ..., -2.8083e-05,
         -1.3352e-04, -1.1877e-04],
        [-2.0266e-06, -1.4007e-06,  9.6112e-07,  ..., -1.6391e-06,
         -1.1027e-06, -1.0505e-06],
        [-3.7551e-06, -2.6375e-06,  1.8254e-06,  ..., -3.0249e-06,
         -2.0415e-06, -1.9222e-06],
        [-3.0100e-06, -2.0266e-06,  1.4678e-06,  ..., -2.4140e-06,
         -1.5721e-06, -1.6540e-06],
        [-5.7817e-06, -4.1425e-06,  2.8014e-06,  ..., -4.7088e-06,
         -3.2783e-06, -2.8014e-06]], device='cuda:0')
Loss: 1.0321553945541382


Running epoch 0, step 793, batch 793
Sampled inputs[:2]: tensor([[   0, 3665, 1419,  ...,  600,  847,  328],
        [   0,  221,  380,  ...,  508, 1853,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6851e-05, -1.3130e-04, -1.1041e-04,  ...,  8.6897e-05,
         -2.3243e-04, -1.4199e-04],
        [-3.8892e-06, -2.5257e-06,  1.8291e-06,  ..., -3.2410e-06,
         -2.2203e-06, -2.2501e-06],
        [-7.1526e-06, -4.7833e-06,  3.4869e-06,  ..., -5.9009e-06,
         -4.0829e-06, -4.0084e-06],
        [-5.7817e-06, -3.6210e-06,  2.8238e-06,  ..., -4.7982e-06,
         -3.1888e-06, -3.5763e-06],
        [-1.1176e-05, -7.6592e-06,  5.4389e-06,  ..., -9.2983e-06,
         -6.6459e-06, -5.8860e-06]], device='cuda:0')
Loss: 1.009507179260254


Running epoch 0, step 794, batch 794
Sampled inputs[:2]: tensor([[    0,   984,    13,  ...,    13, 37385,   490],
        [    0,   300,  1635,  ...,   437,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2453e-05, -1.6878e-04, -1.4081e-04,  ...,  6.0622e-05,
         -2.1597e-04,  7.9671e-05],
        [-5.9456e-06, -3.7178e-06,  2.7977e-06,  ..., -4.8950e-06,
         -3.3155e-06, -3.6284e-06],
        [-1.0937e-05, -7.0482e-06,  5.3421e-06,  ..., -8.9258e-06,
         -6.1244e-06, -6.4522e-06],
        [-8.7917e-06, -5.3197e-06,  4.2915e-06,  ..., -7.2271e-06,
         -4.7609e-06, -5.6922e-06],
        [-1.7077e-05, -1.1295e-05,  8.3148e-06,  ..., -1.4037e-05,
         -9.9689e-06, -9.4622e-06]], device='cuda:0')
Loss: 1.02676522731781


Running epoch 0, step 795, batch 795
Sampled inputs[:2]: tensor([[    0,   380,  2114,  ...,   456, 28979,   472],
        [    0,   278, 19142,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8702e-05, -1.4410e-04, -1.9218e-04,  ...,  3.0921e-05,
         -9.7846e-05,  6.2568e-05],
        [-7.7561e-06, -4.8131e-06,  3.6582e-06,  ..., -6.4820e-06,
         -4.3735e-06, -4.9695e-06],
        [-1.4171e-05, -9.1195e-06,  6.9737e-06,  ..., -1.1712e-05,
         -8.0317e-06, -8.6576e-06],
        [-1.1444e-05, -6.8545e-06,  5.6699e-06,  ..., -9.6262e-06,
         -6.3330e-06, -7.8678e-06],
        [-2.2322e-05, -1.4767e-05,  1.0967e-05,  ..., -1.8567e-05,
         -1.3188e-05, -1.2740e-05]], device='cuda:0')
Loss: 0.9983914494514465


Running epoch 0, step 796, batch 796
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0,   515,   352,  ...,    40, 25575,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1325e-05, -1.8418e-04, -4.1679e-04,  ...,  1.4453e-04,
         -2.1228e-04,  2.8335e-05],
        [-9.5516e-06, -5.9977e-06,  4.5821e-06,  ..., -8.1062e-06,
         -5.3868e-06, -6.0424e-06],
        [-1.7405e-05, -1.1325e-05,  8.7097e-06,  ..., -1.4603e-05,
         -9.8646e-06, -1.0490e-05],
        [-1.4186e-05, -8.5831e-06,  7.1824e-06,  ..., -1.2130e-05,
         -7.8380e-06, -9.6858e-06],
        [-2.7478e-05, -1.8358e-05,  1.3694e-05,  ..., -2.3186e-05,
         -1.6227e-05, -1.5408e-05]], device='cuda:0')
Loss: 1.0339460372924805


Running epoch 0, step 797, batch 797
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0, 23749, 27341,  ..., 34110,   342,  9672]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6666e-05, -2.6073e-04, -2.4053e-04,  ...,  1.1952e-04,
         -1.3890e-04,  1.1867e-04],
        [-1.1578e-05, -7.3910e-06,  5.5879e-06,  ..., -9.7975e-06,
         -6.5640e-06, -7.1302e-06],
        [-2.0981e-05, -1.3873e-05,  1.0543e-05,  ..., -1.7613e-05,
         -1.1981e-05, -1.2361e-05],
        [-1.7196e-05, -1.0625e-05,  8.7395e-06,  ..., -1.4663e-05,
         -9.5665e-06, -1.1399e-05],
        [-3.3200e-05, -2.2531e-05,  1.6615e-05,  ..., -2.8044e-05,
         -1.9759e-05, -1.8224e-05]], device='cuda:0')
Loss: 1.063604474067688


Running epoch 0, step 798, batch 798
Sampled inputs[:2]: tensor([[    0, 10446,    14,  ...,   266,  1164,   287],
        [    0,  3412,  1707,  ..., 11114,    15,  1821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4554e-05, -2.9649e-04, -2.2965e-04,  ...,  1.9507e-04,
          5.1247e-05,  2.6897e-04],
        [-1.3486e-05, -8.6576e-06,  6.5491e-06,  ..., -1.1452e-05,
         -7.5996e-06, -8.1807e-06],
        [-2.4363e-05, -1.6198e-05,  1.2308e-05,  ..., -2.0549e-05,
         -1.3851e-05, -1.4149e-05],
        [-2.0027e-05, -1.2465e-05,  1.0230e-05,  ..., -1.7136e-05,
         -1.1086e-05, -1.3061e-05],
        [-3.8534e-05, -2.6256e-05,  1.9386e-05,  ..., -3.2663e-05,
         -2.2814e-05, -2.0847e-05]], device='cuda:0')
Loss: 1.0507409572601318


Running epoch 0, step 799, batch 799
Sampled inputs[:2]: tensor([[   0,  280, 5656,  ..., 7369, 2276,   12],
        [   0, 1049,   12,  ...,  292, 3963,  755]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7341e-05, -4.5484e-04, -1.0618e-04,  ...,  2.5016e-04,
          2.2763e-04,  2.7375e-04],
        [-1.5348e-05, -9.8348e-06,  7.5325e-06,  ..., -1.3053e-05,
         -8.6352e-06, -9.3952e-06],
        [-2.7761e-05, -1.8418e-05,  1.4171e-05,  ..., -2.3425e-05,
         -1.5743e-05, -1.6235e-05],
        [-2.2784e-05, -1.4134e-05,  1.1787e-05,  ..., -1.9535e-05,
         -1.2591e-05, -1.5013e-05],
        [-4.3809e-05, -2.9802e-05,  2.2247e-05,  ..., -3.7134e-05,
         -2.5868e-05, -2.3827e-05]], device='cuda:0')
Loss: 1.0409847497940063
Graident accumulation at epoch 0, step 799, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0165],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.6212e-05, -2.4499e-05,  3.3105e-05,  ...,  1.5844e-04,
          1.8823e-04,  2.0974e-04],
        [-1.7142e-05, -1.1463e-05,  7.8526e-06,  ..., -1.4146e-05,
         -8.1495e-06, -1.0862e-05],
        [-1.5177e-05, -9.7122e-06,  6.8150e-06,  ..., -1.0108e-05,
         -5.2568e-06, -9.7447e-06],
        [ 2.0410e-05,  1.8192e-05, -1.0604e-05,  ...,  1.8715e-05,
          1.1637e-05,  4.3638e-06],
        [-4.2970e-05, -2.8257e-05,  1.9630e-05,  ..., -3.1720e-05,
         -2.0680e-05, -2.3536e-05]], device='cuda:0')
optimizer state dict: tensor([[5.4873e-08, 3.9031e-08, 4.0470e-08,  ..., 1.5393e-08, 9.6945e-08,
         1.6432e-08],
        [7.0809e-11, 3.5909e-11, 8.8334e-12,  ..., 4.6507e-11, 7.0495e-12,
         1.5864e-11],
        [1.2257e-09, 6.2010e-10, 1.1715e-10,  ..., 1.0001e-09, 8.2113e-11,
         3.2515e-10],
        [4.1778e-10, 2.9335e-10, 1.3792e-10,  ..., 3.8140e-10, 1.3336e-10,
         1.5209e-10],
        [3.2898e-10, 1.7514e-10, 3.5542e-11,  ..., 2.4253e-10, 3.4765e-11,
         8.7921e-11]], device='cuda:0')
optimizer state dict: 100.0
lr: [1.418499758283982e-05, 1.418499758283982e-05]
scheduler_last_epoch: 100
Epoch 0 | Batch 799/1048 | Training PPL: 4137.628176218959 | time 49.27988600730896
Saving checkpoint at epoch 0, step 799, batch 799
Epoch 0 | Validation PPL: 7.77656186426735 | Learning rate: 1.418499758283982e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_799, AFTER epoch 0, step 799


Running epoch 0, step 800, batch 800
Sampled inputs[:2]: tensor([[    0,   342,  3001,  ...,   369, 11195,   367],
        [    0,  1760,   446,  ...,   329,  1405,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8547e-05,  2.0094e-04, -2.5464e-06,  ...,  4.7809e-05,
         -1.9834e-04,  3.4377e-04],
        [-1.9372e-06, -1.1697e-06,  9.4995e-07,  ..., -1.6093e-06,
         -1.0952e-06, -1.3560e-06],
        [-3.6359e-06, -2.2799e-06,  1.8403e-06,  ..., -2.9951e-06,
         -2.0713e-06, -2.4587e-06],
        [-2.8163e-06, -1.6391e-06,  1.4454e-06,  ..., -2.3544e-06,
         -1.5572e-06, -2.1011e-06],
        [-5.6922e-06, -3.6657e-06,  2.8461e-06,  ..., -4.6790e-06,
         -3.3528e-06, -3.6061e-06]], device='cuda:0')
Loss: 1.0198596715927124


Running epoch 0, step 801, batch 801
Sampled inputs[:2]: tensor([[   0,  278, 1620,  ...,  360, 1758,  278],
        [   0,  689, 3953,  ...,  461,  943,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8592e-04,  1.3079e-04,  1.7250e-05,  ...,  1.2336e-04,
         -2.7709e-04,  6.0681e-04],
        [-3.9339e-06, -2.4736e-06,  1.9185e-06,  ..., -3.2410e-06,
         -2.2203e-06, -2.6003e-06],
        [-7.3016e-06, -4.7386e-06,  3.6731e-06,  ..., -5.9903e-06,
         -4.1425e-06, -4.6641e-06],
        [-5.6773e-06, -3.4571e-06,  2.8908e-06,  ..., -4.6939e-06,
         -3.1143e-06, -3.9637e-06],
        [-1.1295e-05, -7.5102e-06,  5.6475e-06,  ..., -9.2685e-06,
         -6.6310e-06, -6.7502e-06]], device='cuda:0')
Loss: 1.0131831169128418


Running epoch 0, step 802, batch 802
Sampled inputs[:2]: tensor([[    0,   300, 13523,  ..., 42438,   786,  1416],
        [    0,   446, 21112,  ..., 22092,    22,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6402e-04,  1.2111e-04,  3.6773e-05,  ...,  1.1811e-04,
         -1.7729e-04,  5.7847e-04],
        [-5.9903e-06, -3.8072e-06,  2.9244e-06,  ..., -4.9248e-06,
         -3.3379e-06, -3.6210e-06],
        [-1.1086e-05, -7.2569e-06,  5.5805e-06,  ..., -9.0897e-06,
         -6.2138e-06, -6.4969e-06],
        [-8.6427e-06, -5.3421e-06,  4.3958e-06,  ..., -7.1377e-06,
         -4.6939e-06, -5.5283e-06],
        [-1.7136e-05, -1.1474e-05,  8.5831e-06,  ..., -1.4096e-05,
         -9.9689e-06, -9.4324e-06]], device='cuda:0')
Loss: 1.0617108345031738


Running epoch 0, step 803, batch 803
Sampled inputs[:2]: tensor([[    0,   607,   443,  ...,   259,  2646,  1597],
        [    0,   792,   342,  ..., 12152,  9904,  1239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4750e-04,  1.2111e-04, -1.5272e-04,  ...,  1.5020e-04,
         -6.1042e-04,  3.2210e-04],
        [-7.9274e-06, -5.1856e-06,  3.9153e-06,  ..., -6.5714e-06,
         -4.5225e-06, -4.7535e-06],
        [-1.4678e-05, -9.8497e-06,  7.4804e-06,  ..., -1.2115e-05,
         -8.4043e-06, -8.5384e-06],
        [-1.1474e-05, -7.2941e-06,  5.9158e-06,  ..., -9.5665e-06,
         -6.4075e-06, -7.3090e-06],
        [-2.2680e-05, -1.5527e-05,  1.1489e-05,  ..., -1.8775e-05,
         -1.3441e-05, -1.2368e-05]], device='cuda:0')
Loss: 1.0464105606079102


Running epoch 0, step 804, batch 804
Sampled inputs[:2]: tensor([[   0, 2029,   13,  ...,   12, 4536,   12],
        [   0,  471,   12,  ...,   13, 9909, 2673]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6752e-04,  1.7907e-04, -5.2752e-05,  ...,  1.5587e-04,
         -4.1914e-04,  4.8299e-04],
        [-9.9689e-06, -6.5789e-06,  4.9137e-06,  ..., -8.2552e-06,
         -5.6699e-06, -5.8487e-06],
        [-1.8433e-05, -1.2487e-05,  9.3877e-06,  ..., -1.5229e-05,
         -1.0550e-05, -1.0505e-05],
        [-1.4395e-05, -9.2611e-06,  7.3910e-06,  ..., -1.2010e-05,
         -8.0466e-06, -8.9705e-06],
        [-2.8461e-05, -1.9670e-05,  1.4424e-05,  ..., -2.3603e-05,
         -1.6853e-05, -1.5229e-05]], device='cuda:0')
Loss: 1.0715422630310059


Running epoch 0, step 805, batch 805
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0,    13, 20054,  ...,    19,     9,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8713e-04,  3.0227e-04, -5.5940e-05,  ...,  1.3415e-04,
         -4.3373e-04,  5.9230e-04],
        [-1.1869e-05, -7.6741e-06,  5.8152e-06,  ..., -9.8795e-06,
         -6.7949e-06, -7.0781e-06],
        [-2.1994e-05, -1.4618e-05,  1.1154e-05,  ..., -1.8209e-05,
         -1.2651e-05, -1.2681e-05],
        [-1.7136e-05, -1.0774e-05,  8.7768e-06,  ..., -1.4409e-05,
         -9.6709e-06, -1.0878e-05],
        [-3.4034e-05, -2.3097e-05,  1.7196e-05,  ..., -2.8253e-05,
         -2.0236e-05, -1.8403e-05]], device='cuda:0')
Loss: 1.048927664756775


Running epoch 0, step 806, batch 806
Sampled inputs[:2]: tensor([[    0,  1760,     9,  ...,  5996,    71,    19],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7678e-04,  3.4769e-04, -1.7232e-04,  ...,  8.7219e-05,
         -4.7638e-04,  6.1047e-04],
        [-1.3940e-05, -9.0450e-06,  6.8359e-06,  ..., -1.1548e-05,
         -7.9647e-06, -8.2180e-06],
        [-2.5764e-05, -1.7196e-05,  1.3061e-05,  ..., -2.1249e-05,
         -1.4797e-05, -1.4693e-05],
        [-2.0161e-05, -1.2726e-05,  1.0312e-05,  ..., -1.6838e-05,
         -1.1347e-05, -1.2659e-05],
        [-3.9876e-05, -2.7180e-05,  2.0146e-05,  ..., -3.2991e-05,
         -2.3663e-05, -2.1338e-05]], device='cuda:0')
Loss: 1.0315665006637573


Running epoch 0, step 807, batch 807
Sampled inputs[:2]: tensor([[   0, 5646,   12,  ..., 1952,  287, 3088],
        [   0, 1086,   14,  ...,  963,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5358e-04,  4.4723e-04, -1.7766e-04,  ..., -3.7825e-06,
         -6.3344e-04,  5.8107e-04],
        [-1.6101e-05, -1.0483e-05,  7.8194e-06,  ..., -1.3284e-05,
         -9.2760e-06, -9.5740e-06],
        [-2.9817e-05, -1.9953e-05,  1.4968e-05,  ..., -2.4498e-05,
         -1.7256e-05, -1.7181e-05],
        [-2.3082e-05, -1.4633e-05,  1.1683e-05,  ..., -1.9208e-05,
         -1.3091e-05, -1.4581e-05],
        [-4.6074e-05, -3.1441e-05,  2.3082e-05,  ..., -3.7968e-05,
         -2.7478e-05, -2.4945e-05]], device='cuda:0')
Loss: 1.0058355331420898
Graident accumulation at epoch 0, step 807, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1233e-05,  2.2674e-05,  1.2029e-05,  ...,  1.4222e-04,
          1.0607e-04,  2.4687e-04],
        [-1.7038e-05, -1.1365e-05,  7.8492e-06,  ..., -1.4060e-05,
         -8.2622e-06, -1.0734e-05],
        [-1.6641e-05, -1.0736e-05,  7.6304e-06,  ..., -1.1547e-05,
         -6.4567e-06, -1.0488e-05],
        [ 1.6061e-05,  1.4909e-05, -8.3749e-06,  ...,  1.4923e-05,
          9.1644e-06,  2.4693e-06],
        [-4.3280e-05, -2.8575e-05,  1.9975e-05,  ..., -3.2345e-05,
         -2.1359e-05, -2.3677e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5125e-08, 3.9192e-08, 4.0461e-08,  ..., 1.5377e-08, 9.7249e-08,
         1.6754e-08],
        [7.0998e-11, 3.5983e-11, 8.8857e-12,  ..., 4.6637e-11, 7.1285e-12,
         1.5940e-11],
        [1.2254e-09, 6.1988e-10, 1.1725e-10,  ..., 9.9968e-10, 8.2329e-11,
         3.2512e-10],
        [4.1789e-10, 2.9327e-10, 1.3792e-10,  ..., 3.8139e-10, 1.3340e-10,
         1.5215e-10],
        [3.3078e-10, 1.7596e-10, 3.6040e-11,  ..., 2.4373e-10, 3.5486e-11,
         8.8455e-11]], device='cuda:0')
optimizer state dict: 101.0
lr: [1.4072409990222116e-05, 1.4072409990222116e-05]
scheduler_last_epoch: 101


Running epoch 0, step 808, batch 808
Sampled inputs[:2]: tensor([[   0, 4323, 8213,  ..., 1153,  278, 4258],
        [   0,  287,  358,  ...,  328, 1704, 3227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7080e-05,  1.1011e-04, -7.1703e-05,  ..., -5.8066e-05,
          4.1689e-05,  2.3765e-04],
        [-2.0415e-06, -1.3709e-06,  1.0878e-06,  ..., -1.7136e-06,
         -1.1325e-06, -1.0654e-06],
        [-3.6359e-06, -2.5183e-06,  1.9819e-06,  ..., -3.0398e-06,
         -2.0266e-06, -1.8552e-06],
        [-2.8014e-06, -1.8328e-06,  1.5274e-06,  ..., -2.3544e-06,
         -1.5125e-06, -1.5572e-06],
        [-5.5134e-06, -3.8743e-06,  2.9802e-06,  ..., -4.6194e-06,
         -3.1739e-06, -2.6226e-06]], device='cuda:0')
Loss: 1.0331611633300781


Running epoch 0, step 809, batch 809
Sampled inputs[:2]: tensor([[    0,   287,  6015,  ...,    14,   333,   199],
        [    0, 21540,   527,  ...,   824,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5437e-05,  3.7872e-04,  9.1385e-05,  ..., -8.6829e-05,
         -5.9698e-05,  4.5622e-04],
        [-4.2766e-06, -2.7716e-06,  2.1309e-06,  ..., -3.4496e-06,
         -2.4065e-06, -2.5481e-06],
        [-7.9572e-06, -5.3048e-06,  4.0531e-06,  ..., -6.3926e-06,
         -4.5002e-06, -4.6864e-06],
        [-5.8711e-06, -3.7253e-06,  2.9877e-06,  ..., -4.7386e-06,
         -3.2336e-06, -3.6433e-06],
        [-1.1832e-05, -8.0168e-06,  6.0201e-06,  ..., -9.5367e-06,
         -6.8545e-06, -6.5565e-06]], device='cuda:0')
Loss: 1.033409595489502


Running epoch 0, step 810, batch 810
Sampled inputs[:2]: tensor([[   0,  749,    9,  ..., 2756,   14, 1062],
        [   0, 1487,  409,  ..., 6979, 1273,  496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9613e-05,  4.2610e-04,  2.4333e-04,  ..., -1.2172e-04,
          1.4863e-04,  5.7409e-04],
        [-6.4224e-06, -4.2990e-06,  3.2410e-06,  ..., -5.2080e-06,
         -3.6284e-06, -3.6731e-06],
        [-1.1921e-05, -8.1658e-06,  6.1393e-06,  ..., -9.6112e-06,
         -6.7353e-06, -6.7279e-06],
        [-8.8364e-06, -5.7817e-06,  4.5523e-06,  ..., -7.1675e-06,
         -4.8801e-06, -5.2899e-06],
        [-1.7822e-05, -1.2428e-05,  9.1791e-06,  ..., -1.4454e-05,
         -1.0356e-05, -9.4920e-06]], device='cuda:0')
Loss: 1.076935052871704


Running epoch 0, step 811, batch 811
Sampled inputs[:2]: tensor([[   0,  199,  677,  ..., 2792,  271, 2386],
        [   0,  221,  474,  ...,  287,  271, 2540]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5651e-04,  7.6854e-04, -1.1814e-04,  ..., -1.5356e-04,
         -4.8052e-04,  2.1622e-05],
        [-8.1658e-06, -5.3197e-06,  4.3064e-06,  ..., -6.7577e-06,
         -4.6864e-06, -4.8056e-06],
        [-1.5035e-05, -1.0073e-05,  8.1062e-06,  ..., -1.2308e-05,
         -8.5905e-06, -8.5905e-06],
        [-1.1429e-05, -7.1973e-06,  6.2883e-06,  ..., -9.5516e-06,
         -6.4522e-06, -7.1824e-06],
        [-2.2620e-05, -1.5482e-05,  1.2174e-05,  ..., -1.8567e-05,
         -1.3322e-05, -1.2085e-05]], device='cuda:0')
Loss: 0.9846526980400085


Running epoch 0, step 812, batch 812
Sampled inputs[:2]: tensor([[    0,   923,    13,  ...,   199,   677,  3826],
        [    0,  2588, 25531,  ...,  1977,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4084e-05,  1.0174e-03,  8.9541e-05,  ..., -2.0367e-04,
         -3.8478e-04,  1.2668e-04],
        [-1.0237e-05, -6.6906e-06,  5.2974e-06,  ..., -8.4415e-06,
         -5.8487e-06, -6.1169e-06],
        [-1.8790e-05, -1.2636e-05,  9.9689e-06,  ..., -1.5348e-05,
         -1.0706e-05, -1.0885e-05],
        [-1.4290e-05, -9.0599e-06,  7.6964e-06,  ..., -1.1891e-05,
         -8.0317e-06, -9.0823e-06],
        [-2.8312e-05, -1.9446e-05,  1.5005e-05,  ..., -2.3186e-05,
         -1.6615e-05, -1.5348e-05]], device='cuda:0')
Loss: 1.0275373458862305


Running epoch 0, step 813, batch 813
Sampled inputs[:2]: tensor([[   0,  259, 1143,  ...,  593,  360,  278],
        [   0, 3408,  300,  ..., 3868,  300, 2932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1575e-05,  1.0327e-03,  1.0557e-04,  ..., -2.1999e-04,
         -3.0618e-04,  1.8882e-04],
        [-1.2279e-05, -7.9721e-06,  6.3702e-06,  ..., -1.0118e-05,
         -6.8843e-06, -7.1153e-06],
        [-2.2382e-05, -1.4961e-05,  1.1921e-05,  ..., -1.8314e-05,
         -1.2547e-05, -1.2606e-05],
        [-1.7211e-05, -1.0841e-05,  9.2909e-06,  ..., -1.4290e-05,
         -9.4548e-06, -1.0602e-05],
        [-3.3855e-05, -2.3127e-05,  1.8001e-05,  ..., -2.7776e-05,
         -1.9580e-05, -1.7852e-05]], device='cuda:0')
Loss: 1.0409141778945923


Running epoch 0, step 814, batch 814
Sampled inputs[:2]: tensor([[    0,     9,   391,  ...,   300,  2646,  1717],
        [    0,   266, 15258,  ...,  2366,   368,  3988]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9185e-05,  1.0411e-03,  1.0624e-04,  ..., -2.5620e-04,
         -1.5771e-04,  3.5420e-04],
        [-1.4335e-05, -9.3579e-06,  7.4357e-06,  ..., -1.1861e-05,
         -8.0317e-06, -8.2105e-06],
        [-2.5883e-05, -1.7390e-05,  1.3798e-05,  ..., -2.1279e-05,
         -1.4529e-05, -1.4395e-05],
        [-2.0057e-05, -1.2711e-05,  1.0803e-05,  ..., -1.6734e-05,
         -1.1034e-05, -1.2212e-05],
        [-3.9339e-05, -2.7031e-05,  2.0966e-05,  ..., -3.2425e-05,
         -2.2784e-05, -2.0459e-05]], device='cuda:0')
Loss: 1.026955008506775


Running epoch 0, step 815, batch 815
Sampled inputs[:2]: tensor([[    0,  6408,   391,  ...,   870,   278,   266],
        [    0,  3001,  3325,  ..., 16332,  2661,  1200]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7702e-05,  1.0700e-03,  5.3930e-05,  ..., -3.0063e-04,
         -1.7059e-04,  3.7901e-04],
        [-1.6376e-05, -1.0729e-05,  8.4862e-06,  ..., -1.3568e-05,
         -9.1344e-06, -9.2909e-06],
        [-2.9579e-05, -1.9953e-05,  1.5780e-05,  ..., -2.4363e-05,
         -1.6540e-05, -1.6280e-05],
        [-2.2858e-05, -1.4536e-05,  1.2316e-05,  ..., -1.9103e-05,
         -1.2532e-05, -1.3813e-05],
        [-4.4972e-05, -3.1024e-05,  2.3991e-05,  ..., -3.7134e-05,
         -2.5943e-05, -2.3142e-05]], device='cuda:0')
Loss: 1.0229744911193848
Graident accumulation at epoch 0, step 815, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0057, -0.0144,  0.0029,  ..., -0.0024,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0150, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.8339e-05,  1.2741e-04,  1.6219e-05,  ...,  9.7932e-05,
          7.8401e-05,  2.6009e-04],
        [-1.6972e-05, -1.1301e-05,  7.9129e-06,  ..., -1.4011e-05,
         -8.3494e-06, -1.0589e-05],
        [-1.7934e-05, -1.1658e-05,  8.4453e-06,  ..., -1.2828e-05,
         -7.4651e-06, -1.1067e-05],
        [ 1.2169e-05,  1.1965e-05, -6.3058e-06,  ...,  1.1520e-05,
          6.9948e-06,  8.4106e-07],
        [-4.3449e-05, -2.8820e-05,  2.0376e-05,  ..., -3.2824e-05,
         -2.1818e-05, -2.3623e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5079e-08, 4.0297e-08, 4.0424e-08,  ..., 1.5452e-08, 9.7181e-08,
         1.6880e-08],
        [7.1195e-11, 3.6062e-11, 8.9488e-12,  ..., 4.6775e-11, 7.2048e-12,
         1.6010e-11],
        [1.2251e-09, 6.1966e-10, 1.1738e-10,  ..., 9.9927e-10, 8.2520e-11,
         3.2506e-10],
        [4.1800e-10, 2.9319e-10, 1.3793e-10,  ..., 3.8137e-10, 1.3343e-10,
         1.5219e-10],
        [3.3247e-10, 1.7674e-10, 3.6579e-11,  ..., 2.4486e-10, 3.6123e-11,
         8.8902e-11]], device='cuda:0')
optimizer state dict: 102.0
lr: [1.3959200097809337e-05, 1.3959200097809337e-05]
scheduler_last_epoch: 102


Running epoch 0, step 816, batch 816
Sampled inputs[:2]: tensor([[    0,    12,   895,  ...,    13,  2900,    14],
        [    0,  1487,  2511,  ..., 27735,   760,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2292e-05,  2.7076e-05,  2.7547e-04,  ...,  4.9726e-05,
          6.1238e-05,  7.5480e-05],
        [-2.0862e-06, -1.2964e-06,  1.2293e-06,  ..., -1.6391e-06,
         -9.8348e-07, -1.3560e-06],
        [-3.9339e-06, -2.5183e-06,  2.3544e-06,  ..., -3.0547e-06,
         -1.8552e-06, -2.5034e-06],
        [-2.9057e-06, -1.7360e-06,  1.7881e-06,  ..., -2.2948e-06,
         -1.3039e-06, -2.0266e-06],
        [-5.7518e-06, -3.7849e-06,  3.4422e-06,  ..., -4.5002e-06,
         -2.8461e-06, -3.4124e-06]], device='cuda:0')
Loss: 1.0308334827423096


Running epoch 0, step 817, batch 817
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0,  328,  266,  ...,  271,  706,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2576e-05,  1.6843e-04,  2.5364e-04,  ...,  5.0756e-05,
          2.4805e-04,  4.7909e-04],
        [-4.1872e-06, -2.5854e-06,  2.3469e-06,  ..., -3.3230e-06,
         -2.0340e-06, -2.7716e-06],
        [-7.6443e-06, -4.9025e-06,  4.4256e-06,  ..., -6.0201e-06,
         -3.7551e-06, -4.9025e-06],
        [-5.6922e-06, -3.4124e-06,  3.3528e-06,  ..., -4.5449e-06,
         -2.6748e-06, -3.9786e-06],
        [-1.1265e-05, -7.4208e-06,  6.5267e-06,  ..., -8.9109e-06,
         -5.8115e-06, -6.6906e-06]], device='cuda:0')
Loss: 0.9882494807243347


Running epoch 0, step 818, batch 818
Sampled inputs[:2]: tensor([[   0,  271, 4787,  ...,  292,  494,  221],
        [   0,   26, 4044,  ..., 9531,  365,  993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.1428e-05,  1.9832e-04,  3.5683e-04,  ...,  1.2753e-04,
          3.0737e-04,  5.2602e-04],
        [-6.2436e-06, -3.9563e-06,  3.4794e-06,  ..., -4.9993e-06,
         -3.2187e-06, -3.9786e-06],
        [-1.1280e-05, -7.4059e-06,  6.4969e-06,  ..., -8.9556e-06,
         -5.8413e-06, -6.9439e-06],
        [-8.5533e-06, -5.2378e-06,  4.9695e-06,  ..., -6.8843e-06,
         -4.2915e-06, -5.7742e-06],
        [-1.6838e-05, -1.1325e-05,  9.6858e-06,  ..., -1.3441e-05,
         -9.1344e-06, -9.6262e-06]], device='cuda:0')
Loss: 1.0424399375915527


Running epoch 0, step 819, batch 819
Sampled inputs[:2]: tensor([[    0,   199, 11296,  ...,   266, 10463,  8256],
        [    0,   271,  2862,  ...,   287,  5699,    18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3796e-04,  1.2219e-04,  6.1421e-04,  ...,  6.0616e-05,
          7.4572e-04,  8.0139e-04],
        [-8.4341e-06, -5.4389e-06,  4.6790e-06,  ..., -6.7726e-06,
         -4.4778e-06, -5.2229e-06],
        [-1.5184e-05, -1.0148e-05,  8.7172e-06,  ..., -1.2130e-05,
         -8.1360e-06, -9.1046e-06],
        [-1.1504e-05, -7.2047e-06,  6.6385e-06,  ..., -9.2834e-06,
         -5.9754e-06, -7.5400e-06],
        [-2.2709e-05, -1.5557e-05,  1.3024e-05,  ..., -1.8269e-05,
         -1.2726e-05, -1.2681e-05]], device='cuda:0')
Loss: 1.0406485795974731


Running epoch 0, step 820, batch 820
Sampled inputs[:2]: tensor([[   0,  278, 5210,  ..., 1968, 2002,  923],
        [   0, 1235,   14,  ..., 3301,  549,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3984e-04,  2.7181e-04,  7.1860e-04,  ..., -4.5042e-05,
          7.6504e-04,  7.7279e-04],
        [-1.0304e-05, -6.6087e-06,  5.7146e-06,  ..., -8.3819e-06,
         -5.4985e-06, -6.3479e-06],
        [-1.8492e-05, -1.2308e-05,  1.0654e-05,  ..., -1.4931e-05,
         -9.9167e-06, -1.0975e-05],
        [-1.4186e-05, -8.7768e-06,  8.1956e-06,  ..., -1.1623e-05,
         -7.4059e-06, -9.3058e-06],
        [-2.7746e-05, -1.8954e-05,  1.5974e-05,  ..., -2.2531e-05,
         -1.5572e-05, -1.5274e-05]], device='cuda:0')
Loss: 1.0064588785171509


Running epoch 0, step 821, batch 821
Sampled inputs[:2]: tensor([[    0,   292, 17190,  ...,  3078,     9,   287],
        [    0,   259,  2561,  ...,    77,  4830,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1913e-05,  3.0502e-04,  9.0923e-04,  ..., -2.0808e-04,
          1.1400e-03,  1.0952e-03],
        [-1.2390e-05, -7.8753e-06,  6.7875e-06,  ..., -1.0066e-05,
         -6.6459e-06, -7.8231e-06],
        [-2.2277e-05, -1.4707e-05,  1.2681e-05,  ..., -1.7956e-05,
         -1.2003e-05, -1.3582e-05],
        [-1.6943e-05, -1.0401e-05,  9.6560e-06,  ..., -1.3858e-05,
         -8.8885e-06, -1.1377e-05],
        [-3.3408e-05, -2.2680e-05,  1.8984e-05,  ..., -2.7061e-05,
         -1.8820e-05, -1.8895e-05]], device='cuda:0')
Loss: 1.0339760780334473


Running epoch 0, step 822, batch 822
Sampled inputs[:2]: tensor([[    0, 12449,    12,  ...,   292,  2178,   413],
        [    0,   471,   590,  ...,  5007,    13,  2920]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1985e-04,  8.2663e-04,  1.2749e-03,  ..., -2.9483e-04,
          1.2420e-03,  1.7042e-03],
        [-1.4909e-05, -9.4026e-06,  7.7933e-06,  ..., -1.1846e-05,
         -8.1286e-06, -9.7156e-06],
        [-2.6718e-05, -1.7568e-05,  1.4588e-05,  ..., -2.1085e-05,
         -1.4685e-05, -1.6786e-05],
        [-1.9923e-05, -1.2204e-05,  1.0908e-05,  ..., -1.5989e-05,
         -1.0684e-05, -1.3687e-05],
        [-4.0114e-05, -2.7150e-05,  2.1964e-05,  ..., -3.1829e-05,
         -2.2992e-05, -2.3425e-05]], device='cuda:0')
Loss: 0.965456485748291


Running epoch 0, step 823, batch 823
Sampled inputs[:2]: tensor([[    0, 23487,   273,  ...,   368,   259,   422],
        [    0,  4014,    88,  ...,  1103,    14,  1771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4779e-05,  8.3757e-04,  1.4903e-03,  ..., -4.4777e-04,
          1.6251e-03,  2.0712e-03],
        [-1.7144e-05, -1.0885e-05,  9.0227e-06,  ..., -1.3582e-05,
         -9.3058e-06, -1.1057e-05],
        [ 6.1653e-05,  4.8935e-05, -7.2160e-06,  ...,  3.7324e-05,
          5.2072e-05,  1.2932e-05],
        [-2.2992e-05, -1.4186e-05,  1.2636e-05,  ..., -1.8358e-05,
         -1.2241e-05, -1.5609e-05],
        [-4.6104e-05, -3.1292e-05,  2.5332e-05,  ..., -3.6508e-05,
         -2.6286e-05, -2.6762e-05]], device='cuda:0')
Loss: 1.0626417398452759
Graident accumulation at epoch 0, step 823, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0057, -0.0143,  0.0029,  ..., -0.0023,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0149, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0027e-05,  1.9842e-04,  1.6363e-04,  ...,  4.3362e-05,
          2.3307e-04,  4.4120e-04],
        [-1.6989e-05, -1.1259e-05,  8.0239e-06,  ..., -1.3968e-05,
         -8.4450e-06, -1.0636e-05],
        [-9.9757e-06, -5.5987e-06,  6.8792e-06,  ..., -7.8131e-06,
         -1.5114e-06, -8.6675e-06],
        [ 8.6526e-06,  9.3497e-06, -4.4116e-06,  ...,  8.5325e-06,
          5.0712e-06, -8.0394e-07],
        [-4.3715e-05, -2.9067e-05,  2.0872e-05,  ..., -3.3192e-05,
         -2.2265e-05, -2.3937e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5028e-08, 4.0958e-08, 4.2604e-08,  ..., 1.5637e-08, 9.9725e-08,
         2.1153e-08],
        [7.1417e-11, 3.6144e-11, 9.0213e-12,  ..., 4.6912e-11, 7.2842e-12,
         1.6116e-11],
        [1.2276e-09, 6.2143e-10, 1.1732e-10,  ..., 9.9967e-10, 8.5149e-11,
         3.2490e-10],
        [4.1811e-10, 2.9309e-10, 1.3795e-10,  ..., 3.8132e-10, 1.3344e-10,
         1.5228e-10],
        [3.3426e-10, 1.7755e-10, 3.7184e-11,  ..., 2.4595e-10, 3.6778e-11,
         8.9530e-11]], device='cuda:0')
optimizer state dict: 103.0
lr: [1.3845385205061268e-05, 1.3845385205061268e-05]
scheduler_last_epoch: 103


Running epoch 0, step 824, batch 824
Sampled inputs[:2]: tensor([[    0,  2738,   278,  ...,   292,    35,  2147],
        [    0,  2286,    29,  ...,   518,  1307, 16881]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4508e-05,  1.8078e-05,  1.0914e-04,  ...,  1.5367e-04,
          1.0185e-04, -7.7773e-05],
        [-2.2948e-06, -1.6466e-06,  1.2442e-06,  ..., -1.7509e-06,
         -1.3262e-06, -1.3709e-06],
        [ 1.8344e-04,  1.3941e-04, -6.2346e-05,  ...,  1.2836e-04,
          1.2951e-04,  5.7179e-05],
        [-2.8759e-06, -2.0415e-06,  1.6019e-06,  ..., -2.2054e-06,
         -1.6317e-06, -1.8179e-06],
        [-6.2585e-06, -4.7088e-06,  3.4571e-06,  ..., -4.8280e-06,
         -3.7700e-06, -3.5316e-06]], device='cuda:0')
Loss: 1.0446691513061523


Running epoch 0, step 825, batch 825
Sampled inputs[:2]: tensor([[   0, 6184, 1412,  ...,   12,  266,  944],
        [   0,  298,  374,  ...,  298,  413,   28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6460e-05,  9.5181e-05,  3.1285e-04,  ...,  2.1992e-04,
          1.7688e-04, -1.9425e-05],
        [-4.5449e-06, -3.0175e-06,  2.2948e-06,  ..., -3.4124e-06,
         -2.5928e-06, -3.0324e-06],
        [ 1.7929e-04,  1.3681e-04, -6.0305e-05,  ...,  1.2535e-04,
          1.2717e-04,  5.4228e-05],
        [-5.7518e-06, -3.7402e-06,  3.0026e-06,  ..., -4.3213e-06,
         -3.2187e-06, -4.0382e-06],
        [-1.2249e-05, -8.6129e-06,  6.4671e-06,  ..., -9.2089e-06,
         -7.3016e-06, -7.4953e-06]], device='cuda:0')
Loss: 1.0007779598236084


Running epoch 0, step 826, batch 826
Sampled inputs[:2]: tensor([[    0,   554,  1034,  ...,  3313,   365,   654],
        [    0, 10386,  6404,  ...,   292,   325, 12071]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9394e-04,  1.1805e-04,  6.1544e-04,  ...,  1.0686e-04,
          3.8828e-04,  2.3809e-04],
        [-6.7055e-06, -4.4927e-06,  3.5390e-06,  ..., -5.1409e-06,
         -3.7327e-06, -4.3437e-06],
        [ 1.7560e-04,  1.3420e-04, -5.8129e-05,  ...,  1.2240e-04,
          1.2517e-04,  5.2067e-05],
        [-8.5384e-06, -5.6028e-06,  4.6641e-06,  ..., -6.5565e-06,
         -4.6641e-06, -5.8040e-06],
        [-1.7673e-05, -1.2577e-05,  9.6709e-06,  ..., -1.3590e-05,
         -1.0416e-05, -1.0446e-05]], device='cuda:0')
Loss: 1.0395362377166748


Running epoch 0, step 827, batch 827
Sampled inputs[:2]: tensor([[   0,   13, 4831,  ...,  333,  199, 2038],
        [   0,  377,  472,  ..., 9256, 3807, 5499]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2896e-04,  3.7950e-05,  4.6695e-04,  ...,  1.8906e-04,
          2.9166e-04,  3.2512e-04],
        [-8.5980e-06, -5.6624e-06,  4.7609e-06,  ..., -6.7204e-06,
         -4.7982e-06, -5.8413e-06],
        [ 1.7225e-04,  1.3201e-04, -5.5894e-05,  ...,  1.1969e-04,
          1.2328e-04,  4.9639e-05],
        [-1.1027e-05, -7.0333e-06,  6.4746e-06,  ..., -8.6725e-06,
         -6.0424e-06, -7.9945e-06],
        [-2.2680e-05, -1.5974e-05,  1.2904e-05,  ..., -1.7613e-05,
         -1.3381e-05, -1.3664e-05]], device='cuda:0')
Loss: 0.9813152551651001


Running epoch 0, step 828, batch 828
Sampled inputs[:2]: tensor([[    0,    61, 22315,  ..., 36901,    17,   360],
        [    0,  4014,    88,  ...,    14, 11961,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3670e-04,  9.1105e-05,  4.6955e-04,  ...,  1.5082e-04,
          2.7298e-04,  2.6332e-04],
        [-1.0833e-05, -7.2196e-06,  6.0797e-06,  ..., -8.4937e-06,
         -6.0499e-06, -7.0706e-06],
        [ 2.6405e-04,  1.9939e-04, -1.0051e-04,  ...,  1.9405e-04,
          2.2046e-04,  8.5923e-05],
        [-1.4067e-05, -9.1046e-06,  8.3223e-06,  ..., -1.1086e-05,
         -7.6964e-06, -9.7752e-06],
        [-2.8431e-05, -2.0176e-05,  1.6361e-05,  ..., -2.2233e-05,
         -1.6749e-05, -1.6645e-05]], device='cuda:0')
Loss: 1.0493419170379639


Running epoch 0, step 829, batch 829
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,   278,   266,  ...,   352, 10572,   345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6438e-04,  8.2210e-05,  4.0505e-04,  ...,  1.0535e-04,
          2.3085e-04,  1.4544e-04],
        [-1.3039e-05, -8.5086e-06,  7.1898e-06,  ..., -1.0163e-05,
         -7.1973e-06, -8.6576e-06],
        [ 2.5991e-04,  1.9687e-04, -9.8337e-05,  ...,  1.9095e-04,
          2.1831e-04,  8.3077e-05],
        [-1.6883e-05, -1.0669e-05,  9.8050e-06,  ..., -1.3217e-05,
         -9.1121e-06, -1.1921e-05],
        [-3.4511e-05, -2.4050e-05,  1.9580e-05,  ..., -2.6822e-05,
         -2.0102e-05, -2.0549e-05]], device='cuda:0')
Loss: 0.9921571612358093


Running epoch 0, step 830, batch 830
Sampled inputs[:2]: tensor([[    0,   756,    12,  ..., 29374,    12,  2726],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7263e-04, -5.0709e-05,  4.9666e-04,  ...,  1.3194e-04,
          4.3898e-04,  3.1351e-04],
        [-1.5244e-05, -1.0081e-05,  8.3968e-06,  ..., -1.1928e-05,
         -8.4788e-06, -9.9018e-06],
        [ 2.5614e-04,  1.9409e-04, -9.6192e-05,  ...,  1.8794e-04,
          2.1609e-04,  8.1006e-05],
        [-1.9670e-05, -1.2621e-05,  1.1384e-05,  ..., -1.5453e-05,
         -1.0714e-05, -1.3575e-05],
        [-4.0203e-05, -2.8372e-05,  2.2843e-05,  ..., -3.1412e-05,
         -2.3633e-05, -2.3454e-05]], device='cuda:0')
Loss: 1.0556120872497559


Running epoch 0, step 831, batch 831
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0,   635,    13,  ...,   292,    20,   445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8931e-04,  5.2767e-05,  5.9794e-04,  ...,  1.6505e-04,
          3.5103e-04,  4.5979e-04],
        [-1.7375e-05, -1.1496e-05,  9.7603e-06,  ..., -1.3620e-05,
         -9.5889e-06, -1.1072e-05],
        [ 2.5243e-04,  1.9155e-04, -9.3748e-05,  ...,  1.8499e-04,
          2.1412e-04,  7.9024e-05],
        [-2.2545e-05, -1.4484e-05,  1.3307e-05,  ..., -1.7747e-05,
         -1.2174e-05, -1.5259e-05],
        [-4.5657e-05, -3.2216e-05,  2.6420e-05,  ..., -3.5793e-05,
         -2.6673e-05, -2.6166e-05]], device='cuda:0')
Loss: 1.0281925201416016
Graident accumulation at epoch 0, step 831, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0057, -0.0143,  0.0028,  ..., -0.0023,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0281,  ...,  0.0288, -0.0149, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.7956e-05,  1.8386e-04,  2.0706e-04,  ...,  5.5531e-05,
          2.4487e-04,  4.4306e-04],
        [-1.7028e-05, -1.1283e-05,  8.1975e-06,  ..., -1.3933e-05,
         -8.5594e-06, -1.0680e-05],
        [ 1.6265e-05,  1.4116e-05, -3.1835e-06,  ...,  1.1467e-05,
          2.0052e-05,  1.0163e-07],
        [ 5.5328e-06,  6.9663e-06, -2.6398e-06,  ...,  5.9045e-06,
          3.3466e-06, -2.2494e-06],
        [-4.3909e-05, -2.9382e-05,  2.1427e-05,  ..., -3.3452e-05,
         -2.2705e-05, -2.4160e-05]], device='cuda:0')
optimizer state dict: tensor([[5.5125e-08, 4.0920e-08, 4.2919e-08,  ..., 1.5649e-08, 9.9748e-08,
         2.1344e-08],
        [7.1648e-11, 3.6240e-11, 9.1075e-12,  ..., 4.7051e-11, 7.3688e-12,
         1.6223e-11],
        [1.2901e-09, 6.5750e-10, 1.2599e-10,  ..., 1.0329e-09, 1.3091e-10,
         3.3082e-10],
        [4.1820e-10, 2.9301e-10, 1.3799e-10,  ..., 3.8126e-10, 1.3346e-10,
         1.5236e-10],
        [3.3601e-10, 1.7841e-10, 3.7845e-11,  ..., 2.4699e-10, 3.7453e-11,
         9.0125e-11]], device='cuda:0')
optimizer state dict: 104.0
lr: [1.3730982703887026e-05, 1.3730982703887026e-05]
scheduler_last_epoch: 104


Running epoch 0, step 832, batch 832
Sampled inputs[:2]: tensor([[    0, 13555,    14,  ...,  1067,   271,   266],
        [    0,   328,  9424,  ...,    13, 24635,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2612e-06, -5.3434e-05,  5.4135e-05,  ..., -6.1034e-05,
          8.7091e-05,  1.7600e-05],
        [-2.2352e-06, -1.5274e-06,  1.2070e-06,  ..., -1.7285e-06,
         -1.3635e-06, -1.5199e-06],
        [-4.1127e-06, -2.8759e-06,  2.3097e-06,  ..., -3.1739e-06,
         -2.4885e-06, -2.7418e-06],
        [-2.9355e-06, -1.9670e-06,  1.6317e-06,  ..., -2.2650e-06,
         -1.7509e-06, -2.0713e-06],
        [-6.0797e-06, -4.3213e-06,  3.4422e-06,  ..., -4.7088e-06,
         -3.7998e-06, -3.8147e-06]], device='cuda:0')
Loss: 1.0501320362091064


Running epoch 0, step 833, batch 833
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,    47,  1838,  ...,   792,    83, 42612]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9077e-05,  8.9019e-05, -1.6769e-04,  ..., -2.5344e-04,
         -2.0796e-04, -2.8578e-04],
        [-4.4107e-06, -2.9206e-06,  2.3842e-06,  ..., -3.4496e-06,
         -2.6003e-06, -2.8536e-06],
        [-8.0168e-06, -5.4538e-06,  4.4852e-06,  ..., -6.2436e-06,
         -4.7088e-06, -5.0962e-06],
        [-5.7966e-06, -3.7476e-06,  3.2261e-06,  ..., -4.5300e-06,
         -3.3453e-06, -3.9265e-06],
        [-1.1712e-05, -8.1360e-06,  6.5565e-06,  ..., -9.1791e-06,
         -7.1377e-06, -6.9886e-06]], device='cuda:0')
Loss: 1.020870327949524


Running epoch 0, step 834, batch 834
Sampled inputs[:2]: tensor([[    0,   269,    12,  ..., 45645,    14,   298],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5463e-04,  5.7912e-05, -3.7330e-04,  ..., -2.0106e-04,
         -2.9070e-04, -3.3933e-04],
        [-6.6161e-06, -4.2543e-06,  3.5316e-06,  ..., -5.1409e-06,
         -3.9265e-06, -4.5672e-06],
        [-1.2010e-05, -7.9572e-06,  6.6459e-06,  ..., -9.2685e-06,
         -7.1228e-06, -8.0615e-06],
        [-8.5980e-06, -5.3942e-06,  4.8056e-06,  ..., -6.6906e-06,
         -4.9993e-06, -6.1914e-06],
        [-1.7643e-05, -1.1981e-05,  9.7901e-06,  ..., -1.3679e-05,
         -1.0848e-05, -1.1101e-05]], device='cuda:0')
Loss: 1.0013427734375


Running epoch 0, step 835, batch 835
Sampled inputs[:2]: tensor([[   0, 1875, 2117,  ..., 1422, 1059,  963],
        [   0,  278, 6318,  ...,  458,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5391e-05,  4.6362e-05, -5.8702e-04,  ..., -2.0345e-04,
         -2.4948e-04, -4.9027e-04],
        [-8.5235e-06, -5.4836e-06,  4.7460e-06,  ..., -6.7726e-06,
         -5.1260e-06, -5.8785e-06],
        [-1.5244e-05, -1.0148e-05,  8.7917e-06,  ..., -1.1981e-05,
         -9.1493e-06, -1.0118e-05],
        [-1.1086e-05, -6.9216e-06,  6.5714e-06,  ..., -8.8811e-06,
         -6.5789e-06, -8.0913e-06],
        [-2.2531e-05, -1.5408e-05,  1.2979e-05,  ..., -1.7732e-05,
         -1.4022e-05, -1.3858e-05]], device='cuda:0')
Loss: 0.9850950837135315


Running epoch 0, step 836, batch 836
Sampled inputs[:2]: tensor([[    0,   266,   944,  ..., 14981,  1952,   271],
        [    0,   320,   472,  ...,  1345,    14,  1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4921e-04, -5.0967e-04,  6.3519e-05,  ..., -2.4121e-04,
          6.0964e-04,  2.5977e-04],
        [-1.0759e-05, -6.7800e-06,  5.9828e-06,  ..., -8.5086e-06,
         -6.5193e-06, -7.5400e-06],
        [-1.9237e-05, -1.2562e-05,  1.1072e-05,  ..., -1.5065e-05,
         -1.1638e-05, -1.3009e-05],
        [-1.3962e-05, -8.5011e-06,  8.2403e-06,  ..., -1.1101e-05,
         -8.3297e-06, -1.0356e-05],
        [-2.8402e-05, -1.9118e-05,  1.6332e-05,  ..., -2.2262e-05,
         -1.7807e-05, -1.7822e-05]], device='cuda:0')
Loss: 1.031095027923584


Running epoch 0, step 837, batch 837
Sampled inputs[:2]: tensor([[   0,  221,  380,  ..., 5543,  768, 6375],
        [   0,  298,  894,  ..., 7605, 3220,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8508e-04, -6.0385e-04,  3.3818e-05,  ..., -2.6488e-04,
          4.7972e-04,  2.8520e-06],
        [-1.2994e-05, -8.1062e-06,  7.1749e-06,  ..., -1.0222e-05,
         -7.7337e-06, -9.1642e-06],
        [-2.3320e-05, -1.5080e-05,  1.3322e-05,  ..., -1.8165e-05,
         -1.3873e-05, -1.5944e-05],
        [-1.6868e-05, -1.0163e-05,  9.8497e-06,  ..., -1.3337e-05,
         -9.8720e-06, -1.2547e-05],
        [-3.4362e-05, -2.2933e-05,  1.9625e-05,  ..., -2.6822e-05,
         -2.1234e-05, -2.1815e-05]], device='cuda:0')
Loss: 0.9926014542579651


Running epoch 0, step 838, batch 838
Sampled inputs[:2]: tensor([[   0, 1176,   13,  ..., 1919,  221,  380],
        [   0, 5775,   12,  ...,   12, 1034, 9257]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1145e-04, -5.8674e-04, -8.1247e-05,  ..., -2.6285e-04,
          4.9809e-04, -2.3849e-05],
        [-1.5140e-05, -9.5665e-06,  8.4192e-06,  ..., -1.1899e-05,
         -9.0152e-06, -1.0513e-05],
        [-2.7016e-05, -1.7673e-05,  1.5557e-05,  ..., -2.1040e-05,
         -1.6078e-05, -1.8179e-05],
        [-1.9714e-05, -1.2048e-05,  1.1630e-05,  ..., -1.5587e-05,
         -1.1556e-05, -1.4469e-05],
        [-3.9816e-05, -2.6867e-05,  2.2918e-05,  ..., -3.1084e-05,
         -2.4632e-05, -2.4900e-05]], device='cuda:0')
Loss: 0.9638494253158569


Running epoch 0, step 839, batch 839
Sampled inputs[:2]: tensor([[    0,  2467, 18011,  ...,  5913,  9281,    12],
        [    0,  1890,   278,  ...,   578,    72,   815]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1110e-03, -9.1398e-04, -3.9077e-05,  ..., -1.7902e-04,
          6.8731e-04,  1.5185e-05],
        [-1.7121e-05, -1.0803e-05,  9.6560e-06,  ..., -1.3523e-05,
         -1.0140e-05, -1.1861e-05],
        [-3.0443e-05, -1.9908e-05,  1.7792e-05,  ..., -2.3812e-05,
         -1.8030e-05, -2.0400e-05],
        [-2.2382e-05, -1.3642e-05,  1.3433e-05,  ..., -1.7792e-05,
         -1.3046e-05, -1.6451e-05],
        [-4.4763e-05, -3.0234e-05,  2.6107e-05,  ..., -3.5107e-05,
         -2.7597e-05, -2.7820e-05]], device='cuda:0')
Loss: 0.9970163106918335
Graident accumulation at epoch 0, step 839, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0282,  ...,  0.0288, -0.0149, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.5426e-04,  7.4073e-05,  1.8245e-04,  ...,  3.2075e-05,
          2.8911e-04,  4.0027e-04],
        [-1.7037e-05, -1.1235e-05,  8.3434e-06,  ..., -1.3892e-05,
         -8.7175e-06, -1.0798e-05],
        [ 1.1594e-05,  1.0714e-05, -1.0859e-06,  ...,  7.9389e-06,
          1.6244e-05, -1.9485e-06],
        [ 2.7414e-06,  4.9055e-06, -1.0325e-06,  ...,  3.5349e-06,
          1.7074e-06, -3.6696e-06],
        [-4.3994e-05, -2.9467e-05,  2.1895e-05,  ..., -3.3618e-05,
         -2.3195e-05, -2.4526e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6304e-08, 4.1715e-08, 4.2878e-08,  ..., 1.5665e-08, 1.0012e-07,
         2.1323e-08],
        [7.1869e-11, 3.6321e-11, 9.1917e-12,  ..., 4.7187e-11, 7.4643e-12,
         1.6347e-11],
        [1.2898e-09, 6.5724e-10, 1.2618e-10,  ..., 1.0324e-09, 1.3111e-10,
         3.3091e-10],
        [4.1828e-10, 2.9290e-10, 1.3803e-10,  ..., 3.8119e-10, 1.3349e-10,
         1.5248e-10],
        [3.3768e-10, 1.7914e-10, 3.8489e-11,  ..., 2.4797e-10, 3.8177e-11,
         9.0809e-11]], device='cuda:0')
optimizer state dict: 105.0
lr: [1.3616010075987416e-05, 1.3616010075987416e-05]
scheduler_last_epoch: 105


Running epoch 0, step 840, batch 840
Sampled inputs[:2]: tensor([[    0,  2561,  4994,  ..., 10407,   287,  1339],
        [    0,  5885,   271,  ...,   278,  1049,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0600e-05, -8.5200e-05, -3.3314e-04,  ...,  5.6098e-05,
         -3.2434e-04, -3.9891e-04],
        [-2.1011e-06, -1.4678e-06,  1.2293e-06,  ..., -1.7434e-06,
         -1.2517e-06, -1.2442e-06],
        [-3.7253e-06, -2.6524e-06,  2.2501e-06,  ..., -3.0696e-06,
         -2.2054e-06, -2.1607e-06],
        [-2.8759e-06, -1.9670e-06,  1.7583e-06,  ..., -2.3991e-06,
         -1.6838e-06, -1.8179e-06],
        [-5.2452e-06, -3.8445e-06,  3.1739e-06,  ..., -4.3511e-06,
         -3.2336e-06, -2.8610e-06]], device='cuda:0')
Loss: 1.0322431325912476


Running epoch 0, step 841, batch 841
Sampled inputs[:2]: tensor([[    0, 15931,    14,  ...,  2645,   699,   266],
        [    0,   593,   300,  ...,   278,  4694,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7313e-04,  8.1629e-05, -6.7851e-04,  ...,  2.5117e-05,
         -6.0145e-04, -7.8871e-04],
        [-4.2021e-06, -2.9802e-06,  2.5034e-06,  ..., -3.4720e-06,
         -2.4885e-06, -2.3842e-06],
        [-7.4655e-06, -5.3942e-06,  4.5747e-06,  ..., -6.1542e-06,
         -4.3958e-06, -4.1723e-06],
        [-5.7369e-06, -3.9786e-06,  3.5539e-06,  ..., -4.7535e-06,
         -3.3379e-06, -3.4794e-06],
        [-1.0610e-05, -7.8380e-06,  6.4820e-06,  ..., -8.7917e-06,
         -6.4820e-06, -5.5730e-06]], device='cuda:0')
Loss: 1.0342082977294922


Running epoch 0, step 842, batch 842
Sampled inputs[:2]: tensor([[    0,  1751,   287,  ...,  6079,  1059,   287],
        [    0, 10205,   342,  ...,  6354, 12230,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3987e-04,  4.9918e-06, -8.5709e-04,  ...,  8.0506e-05,
         -7.8876e-04, -9.6933e-04],
        [-6.4224e-06, -4.6045e-06,  3.7476e-06,  ..., -5.2452e-06,
         -3.7849e-06, -3.6061e-06],
        [-1.1429e-05, -8.3447e-06,  6.8545e-06,  ..., -9.3430e-06,
         -6.7204e-06, -6.3628e-06],
        [-8.6576e-06, -6.0946e-06,  5.2303e-06,  ..., -7.0930e-06,
         -5.0068e-06, -5.1707e-06],
        [-1.6391e-05, -1.2219e-05,  9.7901e-06,  ..., -1.3471e-05,
         -1.0014e-05, -8.5980e-06]], device='cuda:0')
Loss: 1.0218843221664429


Running epoch 0, step 843, batch 843
Sampled inputs[:2]: tensor([[    0,  2958,   298,  ...,    12,   709,   616],
        [    0, 17301,   300,  ...,   278,   546,  1576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4745e-05, -4.4880e-05, -9.9275e-04,  ...,  2.0657e-04,
         -1.2479e-03, -1.2325e-03],
        [-8.5533e-06, -5.8040e-06,  4.8429e-06,  ..., -6.9961e-06,
         -5.1260e-06, -5.3421e-06],
        [-1.5453e-05, -1.0699e-05,  9.0152e-06,  ..., -1.2606e-05,
         -9.2089e-06, -9.5069e-06],
        [-1.1578e-05, -7.6443e-06,  6.7726e-06,  ..., -9.4920e-06,
         -6.8247e-06, -7.6443e-06],
        [-2.2113e-05, -1.5706e-05,  1.2875e-05,  ..., -1.8090e-05,
         -1.3635e-05, -1.2830e-05]], device='cuda:0')
Loss: 1.0272753238677979


Running epoch 0, step 844, batch 844
Sampled inputs[:2]: tensor([[    0,    14,  4746,  ...,   266,  1119,  1705],
        [    0,    69, 27768,  ...,  1869,  1566,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7103e-06,  1.4153e-04, -9.5736e-04,  ...,  2.2324e-04,
         -1.3698e-03, -1.3954e-03],
        [-1.0744e-05, -7.3835e-06,  6.0126e-06,  ..., -8.7917e-06,
         -6.4522e-06, -6.6310e-06],
        [-1.9416e-05, -1.3620e-05,  1.1221e-05,  ..., -1.5840e-05,
         -1.1578e-05, -1.1817e-05],
        [-1.4484e-05, -9.7007e-06,  8.3745e-06,  ..., -1.1876e-05,
         -8.5533e-06, -9.4473e-06],
        [-2.7686e-05, -1.9908e-05,  1.5974e-05,  ..., -2.2650e-05,
         -1.7092e-05, -1.5885e-05]], device='cuda:0')
Loss: 1.0126547813415527


Running epoch 0, step 845, batch 845
Sampled inputs[:2]: tensor([[    0, 47831,   266,  ...,    66,    17, 20005],
        [    0,  3860,   694,  ...,  1027,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0518e-04,  2.1969e-04, -1.1117e-03,  ...,  2.8989e-04,
         -1.6232e-03, -1.3626e-03],
        [-1.2845e-05, -8.6948e-06,  7.1600e-06,  ..., -1.0550e-05,
         -7.7635e-06, -8.0094e-06],
        [-2.3201e-05, -1.6049e-05,  1.3381e-05,  ..., -1.8969e-05,
         -1.3888e-05, -1.4216e-05],
        [-1.7345e-05, -1.1444e-05,  9.9838e-06,  ..., -1.4275e-05,
         -1.0304e-05, -1.1399e-05],
        [-3.3110e-05, -2.3484e-05,  1.9103e-05,  ..., -2.7090e-05,
         -2.0474e-05, -1.9103e-05]], device='cuda:0')
Loss: 1.0394330024719238


Running epoch 0, step 846, batch 846
Sampled inputs[:2]: tensor([[   0,  598,  696,  ..., 4048, 1795,   14],
        [   0, 5054, 3945,  ...,  272,  278,  516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2930e-05,  1.2120e-04, -1.1018e-03,  ...,  2.2026e-04,
         -1.6426e-03, -1.5514e-03],
        [-1.5080e-05, -1.0289e-05,  8.4788e-06,  ..., -1.2308e-05,
         -9.0599e-06, -9.2611e-06],
        [-2.7165e-05, -1.8954e-05,  1.5765e-05,  ..., -2.2113e-05,
         -1.6227e-05, -1.6436e-05],
        [-2.0415e-05, -1.3620e-05,  1.1854e-05,  ..., -1.6704e-05,
         -1.2070e-05, -1.3202e-05],
        [-3.8743e-05, -2.7716e-05,  2.2501e-05,  ..., -3.1620e-05,
         -2.3946e-05, -2.2113e-05]], device='cuda:0')
Loss: 1.0130759477615356


Running epoch 0, step 847, batch 847
Sampled inputs[:2]: tensor([[    0,    14, 49045,  ...,    12,   706,   409],
        [    0,   299,   292,  ...,   266,  2474,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8162e-05,  1.0771e-04, -1.2068e-03,  ...,  2.5214e-04,
         -1.6700e-03, -1.7708e-03],
        [-1.7285e-05, -1.1876e-05,  9.6709e-06,  ..., -1.4111e-05,
         -1.0461e-05, -1.0632e-05],
        [-3.1158e-05, -2.1860e-05,  1.8001e-05,  ..., -2.5362e-05,
         -1.8761e-05, -1.8865e-05],
        [-2.3305e-05, -1.5646e-05,  1.3471e-05,  ..., -1.9059e-05,
         -1.3866e-05, -1.5065e-05],
        [-4.4495e-05, -3.1978e-05,  2.5734e-05,  ..., -3.6329e-05,
         -2.7731e-05, -2.5421e-05]], device='cuda:0')
Loss: 1.0164515972137451
Graident accumulation at epoch 0, step 847, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0231, -0.0194],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0152, -0.0282,  ...,  0.0288, -0.0149, -0.0177]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.3102e-04,  7.7437e-05,  4.3523e-05,  ...,  5.4082e-05,
          9.3206e-05,  1.8316e-04],
        [-1.7062e-05, -1.1299e-05,  8.4761e-06,  ..., -1.3914e-05,
         -8.8918e-06, -1.0781e-05],
        [ 7.3188e-06,  7.4566e-06,  8.2271e-07,  ...,  4.6088e-06,
          1.2744e-05, -3.6401e-06],
        [ 1.3670e-07,  2.8503e-06,  4.1783e-07,  ...,  1.2755e-06,
          1.5008e-07, -4.8091e-06],
        [-4.4044e-05, -2.9718e-05,  2.2279e-05,  ..., -3.3889e-05,
         -2.3648e-05, -2.4616e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6254e-08, 4.1685e-08, 4.4291e-08,  ..., 1.5713e-08, 1.0281e-07,
         2.4437e-08],
        [7.2096e-11, 3.6426e-11, 9.2760e-12,  ..., 4.7339e-11, 7.5663e-12,
         1.6444e-11],
        [1.2894e-09, 6.5706e-10, 1.2638e-10,  ..., 1.0320e-09, 1.3133e-10,
         3.3093e-10],
        [4.1840e-10, 2.9286e-10, 1.3808e-10,  ..., 3.8118e-10, 1.3355e-10,
         1.5256e-10],
        [3.3932e-10, 1.7999e-10, 3.9113e-11,  ..., 2.4905e-10, 3.8908e-11,
         9.1364e-11]], device='cuda:0')
optimizer state dict: 106.0
lr: [1.3500484890183603e-05, 1.3500484890183603e-05]
scheduler_last_epoch: 106


Running epoch 0, step 848, batch 848
Sampled inputs[:2]: tensor([[    0,   281,    82,  ...,  2485,   417,   199],
        [    0,   555,   764,  ...,   932,   709, 18731]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7624e-04,  1.1284e-04, -2.0039e-05,  ...,  4.9273e-05,
         -2.4313e-04, -2.2340e-04],
        [-2.1011e-06, -1.4678e-06,  1.2070e-06,  ..., -1.7211e-06,
         -1.2740e-06, -1.3113e-06],
        [-3.8445e-06, -2.7418e-06,  2.2799e-06,  ..., -3.1143e-06,
         -2.2650e-06, -2.3246e-06],
        [-2.9057e-06, -1.9670e-06,  1.7062e-06,  ..., -2.3693e-06,
         -1.7211e-06, -1.8924e-06],
        [-5.3942e-06, -3.9041e-06,  3.2037e-06,  ..., -4.3809e-06,
         -3.2634e-06, -3.0845e-06]], device='cuda:0')
Loss: 1.003366231918335


Running epoch 0, step 849, batch 849
Sampled inputs[:2]: tensor([[    0,  3141,   311,  ...,   328,  7818,   408],
        [    0, 28590,    12,  ...,   342, 29639,  1693]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9780e-04,  2.5594e-04, -4.0132e-05,  ..., -1.0455e-06,
         -3.2987e-04, -2.6434e-04],
        [-4.2468e-06, -3.0920e-06,  2.4363e-06,  ..., -3.4943e-06,
         -2.6673e-06, -2.6003e-06],
        [-7.7188e-06, -5.7369e-06,  4.5747e-06,  ..., -6.3032e-06,
         -4.7535e-06, -4.6045e-06],
        [-5.8860e-06, -4.1723e-06,  3.4347e-06,  ..., -4.8429e-06,
         -3.6433e-06, -3.7774e-06],
        [-1.0848e-05, -8.1360e-06,  6.4224e-06,  ..., -8.8513e-06,
         -6.8098e-06, -6.1393e-06]], device='cuda:0')
Loss: 1.0228010416030884


Running epoch 0, step 850, batch 850
Sampled inputs[:2]: tensor([[   0,  446, 1845,  ...,  422,  221,  474],
        [   0,  278,  266,  ...,  380, 4053,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9780e-04,  3.7364e-04, -9.9150e-05,  ..., -6.6181e-05,
         -5.8416e-04, -2.6434e-04],
        [-6.4969e-06, -4.5523e-06,  3.5837e-06,  ..., -5.2229e-06,
         -3.9488e-06, -4.1574e-06],
        [-1.1832e-05, -8.4937e-06,  6.7651e-06,  ..., -9.4771e-06,
         -7.1377e-06, -7.4059e-06],
        [-8.8960e-06, -6.1095e-06,  5.0440e-06,  ..., -7.1675e-06,
         -5.3570e-06, -5.8934e-06],
        [-1.6600e-05, -1.2070e-05,  9.5069e-06,  ..., -1.3322e-05,
         -1.0222e-05, -9.8646e-06]], device='cuda:0')
Loss: 0.9769048094749451


Running epoch 0, step 851, batch 851
Sampled inputs[:2]: tensor([[   0,   12, 3570,  ...,  273,  298,  894],
        [   0,    5, 7523,  ...,  199, 8871,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5371e-04,  5.5358e-04,  1.2821e-05,  ..., -1.1951e-04,
         -6.1118e-04, -3.4942e-04],
        [-8.7768e-06, -6.1616e-06,  4.7386e-06,  ..., -7.0781e-06,
         -5.4017e-06, -5.6773e-06],
        [-1.5944e-05, -1.1489e-05,  8.9407e-06,  ..., -1.2845e-05,
         -9.7752e-06, -1.0103e-05],
        [-1.1891e-05, -8.2105e-06,  6.6087e-06,  ..., -9.6411e-06,
         -7.2792e-06, -7.9647e-06],
        [-2.2590e-05, -1.6510e-05,  1.2681e-05,  ..., -1.8209e-05,
         -1.4156e-05, -1.3575e-05]], device='cuda:0')
Loss: 1.020772933959961


Running epoch 0, step 852, batch 852
Sampled inputs[:2]: tensor([[    0,   689,    13,  ...,   756,   271, 31773],
        [    0,   669,    14,  ...,   596,   292,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5115e-04,  7.1325e-04, -2.2379e-04,  ...,  3.0110e-05,
         -1.0254e-03, -5.7873e-04],
        [-1.0833e-05, -7.5027e-06,  5.8934e-06,  ..., -8.7470e-06,
         -6.5789e-06, -7.0035e-06],
        [-1.9819e-05, -1.4052e-05,  1.1191e-05,  ..., -1.5944e-05,
         -1.1951e-05, -1.2532e-05],
        [-1.4871e-05, -1.0096e-05,  8.3297e-06,  ..., -1.2055e-05,
         -8.9407e-06, -9.9614e-06],
        [-2.8014e-05, -2.0221e-05,  1.5855e-05,  ..., -2.2590e-05,
         -1.7315e-05, -1.6794e-05]], device='cuda:0')
Loss: 1.015577793121338


Running epoch 0, step 853, batch 853
Sampled inputs[:2]: tensor([[    0,   591,   953,  ...,  4118,  5750,   292],
        [    0,  7952,   266,  ..., 10864, 24825,   927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4321e-04,  7.5618e-04, -3.8017e-04,  ...,  1.2670e-05,
         -1.4048e-03, -7.1977e-04],
        [-1.2934e-05, -8.9109e-06,  7.1004e-06,  ..., -1.0476e-05,
         -7.9125e-06, -8.4043e-06],
        [-2.3693e-05, -1.6704e-05,  1.3486e-05,  ..., -1.9103e-05,
         -1.4380e-05, -1.5050e-05],
        [-1.7822e-05, -1.2018e-05,  1.0066e-05,  ..., -1.4484e-05,
         -1.0781e-05, -1.2003e-05],
        [-3.3498e-05, -2.4095e-05,  1.9133e-05,  ..., -2.7090e-05,
         -2.0877e-05, -2.0176e-05]], device='cuda:0')
Loss: 1.0165302753448486


Running epoch 0, step 854, batch 854
Sampled inputs[:2]: tensor([[   0,   12, 1631,  ..., 1143,  271,  266],
        [   0,  996, 2226,  ..., 5322,  287,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6904e-04,  9.6927e-04, -5.4087e-04,  ..., -1.7657e-06,
         -1.6251e-03, -5.8926e-04],
        [-1.5095e-05, -1.0453e-05,  8.2329e-06,  ..., -1.2189e-05,
         -9.3058e-06, -9.8571e-06],
        [-2.7746e-05, -1.9640e-05,  1.5676e-05,  ..., -2.2292e-05,
         -1.6943e-05, -1.7717e-05],
        [-2.0757e-05, -1.4074e-05,  1.1630e-05,  ..., -1.6809e-05,
         -1.2636e-05, -1.4029e-05],
        [-3.9190e-05, -2.8297e-05,  2.2233e-05,  ..., -3.1561e-05,
         -2.4557e-05, -2.3738e-05]], device='cuda:0')
Loss: 1.0068798065185547


Running epoch 0, step 855, batch 855
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5558e-04,  8.9336e-04, -7.5038e-04,  ..., -5.0524e-07,
         -1.6702e-03, -9.0133e-04],
        [-1.6898e-05, -1.1735e-05,  9.4995e-06,  ..., -1.3776e-05,
         -1.0520e-05, -1.1116e-05],
        [-3.1024e-05, -2.2039e-05,  1.8045e-05,  ..., -2.5108e-05,
         -1.9103e-05, -1.9893e-05],
        [-2.3350e-05, -1.5818e-05,  1.3627e-05,  ..., -1.9133e-05,
         -1.4372e-05, -1.6011e-05],
        [-4.3869e-05, -3.1829e-05,  2.5541e-05,  ..., -3.5524e-05,
         -2.7731e-05, -2.6554e-05]], device='cuda:0')
Loss: 0.9948215484619141
Graident accumulation at epoch 0, step 855, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0153, -0.0282,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.2356e-05,  1.5903e-04, -3.5868e-05,  ...,  4.8623e-05,
         -8.3137e-05,  7.4709e-05],
        [-1.7045e-05, -1.1343e-05,  8.5785e-06,  ..., -1.3900e-05,
         -9.0547e-06, -1.0815e-05],
        [ 3.4845e-06,  4.5070e-06,  2.5450e-06,  ...,  1.6371e-06,
          9.5589e-06, -5.2654e-06],
        [-2.2120e-06,  9.8352e-07,  1.7388e-06,  ..., -7.6533e-07,
         -1.3021e-06, -5.9293e-06],
        [-4.4027e-05, -2.9929e-05,  2.2605e-05,  ..., -3.4053e-05,
         -2.4056e-05, -2.4809e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6930e-08, 4.2441e-08, 4.4810e-08,  ..., 1.5698e-08, 1.0550e-07,
         2.5225e-08],
        [7.2310e-11, 3.6527e-11, 9.3570e-12,  ..., 4.7481e-11, 7.6694e-12,
         1.6551e-11],
        [1.2891e-09, 6.5689e-10, 1.2658e-10,  ..., 1.0316e-09, 1.3156e-10,
         3.3100e-10],
        [4.1853e-10, 2.9281e-10, 1.3813e-10,  ..., 3.8116e-10, 1.3363e-10,
         1.5266e-10],
        [3.4091e-10, 1.8082e-10, 3.9726e-11,  ..., 2.5006e-10, 3.9638e-11,
         9.1978e-11]], device='cuda:0')
optimizer state dict: 107.0
lr: [1.3384424799732402e-05, 1.3384424799732402e-05]
scheduler_last_epoch: 107


Running epoch 0, step 856, batch 856
Sampled inputs[:2]: tensor([[    0,  1478,    14,  ...,   266,  9417,  9105],
        [    0, 33792,   352,  ...,   278,   546, 30495]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2537e-05,  8.0194e-06,  7.2115e-05,  ..., -1.2853e-04,
         -4.5556e-05,  9.0606e-05],
        [-1.9819e-06, -1.4529e-06,  1.2964e-06,  ..., -1.6093e-06,
         -1.0580e-06, -1.1846e-06],
        [-3.6657e-06, -2.7567e-06,  2.4736e-06,  ..., -2.9802e-06,
         -1.9670e-06, -2.1905e-06],
        [-2.8759e-06, -2.0713e-06,  1.9521e-06,  ..., -2.3395e-06,
         -1.5050e-06, -1.7956e-06],
        [-4.9770e-06, -3.8147e-06,  3.3379e-06,  ..., -4.0829e-06,
         -2.8163e-06, -2.7865e-06]], device='cuda:0')
Loss: 1.0219395160675049


Running epoch 0, step 857, batch 857
Sampled inputs[:2]: tensor([[    0,  8405,  4142,  ..., 18796,     9,   699],
        [    0,   271,  3421,  ...,   306,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4442e-04,  3.9549e-05,  3.6257e-04,  ..., -1.4672e-04,
          2.6031e-04,  3.9188e-04],
        [-3.9637e-06, -2.9132e-06,  2.5481e-06,  ..., -3.2261e-06,
         -2.2203e-06, -2.4363e-06],
        [-7.2718e-06, -5.4836e-06,  4.8429e-06,  ..., -5.9009e-06,
         -4.0680e-06, -4.3958e-06],
        [-5.7817e-06, -4.1574e-06,  3.8743e-06,  ..., -4.7237e-06,
         -3.1888e-06, -3.7476e-06],
        [-1.0014e-05, -7.7486e-06,  6.6608e-06,  ..., -8.1956e-06,
         -5.8711e-06, -5.6475e-06]], device='cuda:0')
Loss: 1.031005859375


Running epoch 0, step 858, batch 858
Sampled inputs[:2]: tensor([[   0, 1234,  278,  ..., 8635,  271,  546],
        [   0, 1611,  266,  ...,  266, 2673, 6277]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2005e-04, -6.8732e-06,  3.0903e-04,  ..., -2.1306e-04,
          2.3293e-04,  4.4502e-04],
        [-5.9158e-06, -4.3660e-06,  3.8370e-06,  ..., -4.8280e-06,
         -3.2037e-06, -3.4869e-06],
        [ 8.5461e-05,  2.4053e-05, -4.3027e-05,  ...,  6.5464e-05,
          2.0273e-05,  1.0803e-05],
        [-8.7321e-06, -6.3181e-06,  5.8711e-06,  ..., -7.1377e-06,
         -4.6119e-06, -5.4091e-06],
        [-1.4782e-05, -1.1444e-05,  9.8646e-06,  ..., -1.2159e-05,
         -8.4043e-06, -8.0615e-06]], device='cuda:0')
Loss: 1.0471134185791016


Running epoch 0, step 859, batch 859
Sampled inputs[:2]: tensor([[    0,  9342,   600,  ...,   199, 12095,   291],
        [    0, 20080, 11069,  ...,   300,  5768,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5650e-04,  1.9694e-05,  3.1604e-04,  ..., -2.1723e-04,
          2.9133e-04,  3.5233e-04],
        [-7.7859e-06, -5.6624e-06,  5.1111e-06,  ..., -6.3777e-06,
         -4.0531e-06, -4.5747e-06],
        [ 8.2257e-05,  2.1773e-05, -4.0777e-05,  ...,  6.2811e-05,
          1.8820e-05,  8.9854e-06],
        [-1.1578e-05, -8.2403e-06,  7.8827e-06,  ..., -9.4920e-06,
         -5.8636e-06, -7.1526e-06],
        [-1.9133e-05, -1.4633e-05,  1.2904e-05,  ..., -1.5765e-05,
         -1.0505e-05, -1.0327e-05]], device='cuda:0')
Loss: 0.9749742746353149


Running epoch 0, step 860, batch 860
Sampled inputs[:2]: tensor([[    0,    12,   344,  ..., 10482,   950, 15744],
        [    0,   287,  2997,  ...,   437,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8434e-04,  1.2397e-04,  2.5699e-04,  ..., -1.9751e-04,
          1.3928e-04,  2.7340e-04],
        [-9.7975e-06, -7.2569e-06,  6.4149e-06,  ..., -8.0764e-06,
         -5.2825e-06, -5.7220e-06],
        [ 7.8561e-05,  1.8808e-05, -3.8333e-05,  ...,  5.9697e-05,
          1.6570e-05,  6.8694e-06],
        [-1.4454e-05, -1.0476e-05,  9.7826e-06,  ..., -1.1906e-05,
         -7.5921e-06, -8.8662e-06],
        [-2.4229e-05, -1.8835e-05,  1.6302e-05,  ..., -2.0117e-05,
         -1.3754e-05, -1.3083e-05]], device='cuda:0')
Loss: 1.0479620695114136


Running epoch 0, step 861, batch 861
Sampled inputs[:2]: tensor([[    0,   401,  3408,  ...,   287, 19892,   328],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8223e-04,  1.7476e-04,  2.2683e-04,  ..., -1.7451e-04,
          6.0862e-05,  2.6658e-04],
        [-1.1764e-05, -8.7768e-06,  7.7039e-06,  ..., -9.7752e-06,
         -6.4000e-06, -6.8247e-06],
        [ 7.4940e-05,  1.5977e-05, -3.5919e-05,  ...,  5.6597e-05,
          1.4544e-05,  4.8727e-06],
        [-1.7419e-05, -1.2711e-05,  1.1779e-05,  ..., -1.4454e-05,
         -9.2313e-06, -1.0602e-05],
        [-2.9176e-05, -2.2769e-05,  1.9595e-05,  ..., -2.4378e-05,
         -1.6630e-05, -1.5646e-05]], device='cuda:0')
Loss: 1.0369499921798706


Running epoch 0, step 862, batch 862
Sampled inputs[:2]: tensor([[   0,  474,  221,  ..., 2945,    9,  287],
        [   0, 7117,  278,  ...,  287,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0970e-04,  6.6376e-04,  1.9634e-05,  ..., -8.5900e-05,
         -4.2275e-04, -1.3669e-05],
        [-1.3664e-05, -1.0014e-05,  8.8811e-06,  ..., -1.1414e-05,
         -7.6145e-06, -8.1882e-06],
        [ 7.1543e-05,  1.3667e-05, -3.3699e-05,  ...,  5.3707e-05,
          1.2383e-05,  2.5630e-06],
        [-2.0176e-05, -1.4439e-05,  1.3575e-05,  ..., -1.6853e-05,
         -1.0975e-05, -1.2644e-05],
        [-3.4034e-05, -2.6181e-05,  2.2769e-05,  ..., -2.8491e-05,
         -1.9833e-05, -1.8716e-05]], device='cuda:0')
Loss: 0.9905836582183838


Running epoch 0, step 863, batch 863
Sampled inputs[:2]: tensor([[    0,   437, 38603,  ..., 37253, 10432,   278],
        [    0,   278,  1041,  ...,  2098,  1837,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1376e-05,  5.7785e-04, -9.4982e-06,  ..., -8.1662e-05,
         -4.2293e-04, -2.3120e-06],
        [-1.5661e-05, -1.1556e-05,  1.0148e-05,  ..., -1.3031e-05,
         -8.8289e-06, -9.3654e-06],
        [ 6.7818e-05,  1.0761e-05, -3.1300e-05,  ...,  5.0697e-05,
          1.0133e-05,  3.7253e-07],
        [-2.3097e-05, -1.6645e-05,  1.5460e-05,  ..., -1.9208e-05,
         -1.2703e-05, -1.4447e-05],
        [-3.9101e-05, -3.0234e-05,  2.6047e-05,  ..., -3.2604e-05,
         -2.3022e-05, -2.1517e-05]], device='cuda:0')
Loss: 1.0335499048233032
Graident accumulation at epoch 0, step 863, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0153, -0.0282,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.2258e-05,  2.0091e-04, -3.3231e-05,  ...,  3.5594e-05,
         -1.1712e-04,  6.7007e-05],
        [-1.6907e-05, -1.1364e-05,  8.7354e-06,  ..., -1.3813e-05,
         -9.0321e-06, -1.0670e-05],
        [ 9.9178e-06,  5.1324e-06, -8.3953e-07,  ...,  6.5430e-06,
          9.6163e-06, -4.7016e-06],
        [-4.3005e-06, -7.7929e-07,  3.1109e-06,  ..., -2.6096e-06,
         -2.4423e-06, -6.7811e-06],
        [-4.3534e-05, -2.9960e-05,  2.2949e-05,  ..., -3.3908e-05,
         -2.3953e-05, -2.4480e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6874e-08, 4.2733e-08, 4.4765e-08,  ..., 1.5689e-08, 1.0557e-07,
         2.5200e-08],
        [7.2483e-11, 3.6624e-11, 9.4506e-12,  ..., 4.7603e-11, 7.7396e-12,
         1.6622e-11],
        [1.2924e-09, 6.5635e-10, 1.2743e-10,  ..., 1.0332e-09, 1.3153e-10,
         3.3067e-10],
        [4.1865e-10, 2.9280e-10, 1.3823e-10,  ..., 3.8115e-10, 1.3365e-10,
         1.5272e-10],
        [3.4209e-10, 1.8155e-10, 4.0364e-11,  ..., 2.5087e-10, 4.0128e-11,
         9.2349e-11]], device='cuda:0')
optimizer state dict: 108.0
lr: [1.3267847539628745e-05, 1.3267847539628745e-05]
scheduler_last_epoch: 108


Running epoch 0, step 864, batch 864
Sampled inputs[:2]: tensor([[   0,  443,   40,  ...,  346,  462,  221],
        [   0,  741,  266,  ...,  271, 5166,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2686e-06, -8.0403e-05,  1.1197e-04,  ..., -4.1389e-05,
          5.2280e-06,  1.9171e-04],
        [-1.7360e-06, -1.3262e-06,  1.2517e-06,  ..., -1.4827e-06,
         -9.9093e-07, -1.1325e-06],
        [-3.2634e-06, -2.5183e-06,  2.3991e-06,  ..., -2.7120e-06,
         -1.8030e-06, -2.0266e-06],
        [-2.6971e-06, -1.9819e-06,  2.0862e-06,  ..., -2.2948e-06,
         -1.4827e-06, -1.8701e-06],
        [-4.2617e-06, -3.3975e-06,  3.1143e-06,  ..., -3.5763e-06,
         -2.5034e-06, -2.3991e-06]], device='cuda:0')
Loss: 0.9957941174507141


Running epoch 0, step 865, batch 865
Sampled inputs[:2]: tensor([[   0, 3756,   13,  ..., 1704,  278, 5851],
        [   0, 3159,  278,  ...,  266, 2545,  863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7483e-05, -2.2705e-04,  4.4232e-04,  ..., -1.5817e-04,
          2.4388e-04,  4.5776e-04],
        [-3.5688e-06, -2.6897e-06,  2.5406e-06,  ..., -2.9430e-06,
         -2.0489e-06, -2.3469e-06],
        [-6.6906e-06, -5.1260e-06,  4.9025e-06,  ..., -5.4389e-06,
         -3.7700e-06, -4.2468e-06],
        [-5.5283e-06, -4.0531e-06,  4.1872e-06,  ..., -4.5449e-06,
         -3.0920e-06, -3.8370e-06],
        [-8.9407e-06, -7.0632e-06,  6.5118e-06,  ..., -7.3463e-06,
         -5.3495e-06, -5.2154e-06]], device='cuda:0')
Loss: 1.0503789186477661


Running epoch 0, step 866, batch 866
Sampled inputs[:2]: tensor([[    0,  2013,    13,  ...,   271,   266,   908],
        [    0, 18837,   394,  ...,   271,  1398,  1871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3292e-04, -1.5542e-04,  4.9904e-04,  ..., -1.8099e-04,
          4.8751e-04,  8.1360e-04],
        [-5.3272e-06, -3.9563e-06,  3.7625e-06,  ..., -4.4554e-06,
         -3.0100e-06, -3.6731e-06],
        [-1.0014e-05, -7.5400e-06,  7.3165e-06,  ..., -8.2105e-06,
         -5.5209e-06, -6.5863e-06],
        [-8.2701e-06, -5.9530e-06,  6.1989e-06,  ..., -6.8694e-06,
         -4.5374e-06, -5.9381e-06],
        [-1.3381e-05, -1.0356e-05,  9.7305e-06,  ..., -1.1042e-05,
         -7.8082e-06, -8.0615e-06]], device='cuda:0')
Loss: 0.9991301894187927


Running epoch 0, step 867, batch 867
Sampled inputs[:2]: tensor([[    0, 41638,  4573,  ...,   259,   790,  1416],
        [    0,   259,  1513,  ...,   275, 19511,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6513e-04, -6.0639e-05,  7.4609e-04,  ..., -2.5334e-04,
          6.1098e-04,  1.0088e-03],
        [-7.1600e-06, -5.2676e-06,  4.9695e-06,  ..., -6.0350e-06,
         -4.1127e-06, -4.9472e-06],
        [-1.3307e-05, -9.9540e-06,  9.5963e-06,  ..., -1.1012e-05,
         -7.4729e-06, -8.7917e-06],
        [-1.1057e-05, -7.9200e-06,  8.1062e-06,  ..., -9.2685e-06,
         -6.1914e-06, -7.9200e-06],
        [-1.7911e-05, -1.3769e-05,  1.2890e-05,  ..., -1.4916e-05,
         -1.0625e-05, -1.0893e-05]], device='cuda:0')
Loss: 1.011246919631958


Running epoch 0, step 868, batch 868
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,   266,  1658,  ...,   278,  1083,  5993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8099e-04, -5.9860e-05,  7.4609e-04,  ..., -3.4987e-04,
          7.6081e-04,  1.2339e-03],
        [-9.1121e-06, -6.7428e-06,  6.2659e-06,  ..., -7.5996e-06,
         -5.2229e-06, -6.2287e-06],
        [-1.6868e-05, -1.2711e-05,  1.2070e-05,  ..., -1.3858e-05,
         -9.4846e-06, -1.1072e-05],
        [-1.3947e-05, -1.0066e-05,  1.0118e-05,  ..., -1.1578e-05,
         -7.8008e-06, -9.8869e-06],
        [-2.2680e-05, -1.7554e-05,  1.6198e-05,  ..., -1.8761e-05,
         -1.3471e-05, -1.3724e-05]], device='cuda:0')
Loss: 1.0083250999450684


Running epoch 0, step 869, batch 869
Sampled inputs[:2]: tensor([[   0,  437,  638,  ..., 4514,   14,  333],
        [   0,  266, 4616,  ..., 1906, 7256,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6316e-04, -3.5260e-04,  6.8189e-04,  ..., -3.6658e-04,
          8.5054e-04,  1.4030e-03],
        [-1.0990e-05, -8.0913e-06,  7.5474e-06,  ..., -9.1270e-06,
         -6.2510e-06, -7.3835e-06],
        [-2.0280e-05, -1.5214e-05,  1.4484e-05,  ..., -1.6615e-05,
         -1.1347e-05, -1.3128e-05],
        [-1.6853e-05, -1.2107e-05,  1.2189e-05,  ..., -1.3947e-05,
         -9.3654e-06, -1.1764e-05],
        [-2.7329e-05, -2.1055e-05,  1.9491e-05,  ..., -2.2560e-05,
         -1.6153e-05, -1.6332e-05]], device='cuda:0')
Loss: 1.019895315170288


Running epoch 0, step 870, batch 870
Sampled inputs[:2]: tensor([[    0,   298, 39056,  ...,   221,  1061,  2165],
        [    0, 10511,  3887,  ...,  3504,   298,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0206e-05, -6.2048e-05,  7.8374e-04,  ..., -4.2987e-04,
          8.6235e-04,  1.7183e-03],
        [-1.2971e-05, -9.3430e-06,  8.6650e-06,  ..., -1.0692e-05,
         -7.5847e-06, -9.2387e-06],
        [-2.4244e-05, -1.7777e-05,  1.6823e-05,  ..., -1.9684e-05,
         -1.3925e-05, -1.6734e-05],
        [-1.9640e-05, -1.3821e-05,  1.3836e-05,  ..., -1.6138e-05,
         -1.1198e-05, -1.4447e-05],
        [-3.2693e-05, -2.4632e-05,  2.2680e-05,  ..., -2.6733e-05,
         -1.9729e-05, -2.0891e-05]], device='cuda:0')
Loss: 0.9988504648208618


Running epoch 0, step 871, batch 871
Sampled inputs[:2]: tensor([[    0,   360,  2063,  ..., 49105,   221,  1868],
        [    0,   271,  4728,  ...,   344,   259,  1774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5791e-05, -5.5670e-05,  9.2886e-04,  ..., -5.1214e-04,
          1.1039e-03,  1.9193e-03],
        [-1.4685e-05, -1.0565e-05,  9.8124e-06,  ..., -1.2152e-05,
         -8.6054e-06, -1.0550e-05],
        [-2.7478e-05, -2.0102e-05,  1.9118e-05,  ..., -2.2352e-05,
         -1.5810e-05, -1.9029e-05],
        [-2.2218e-05, -1.5579e-05,  1.5713e-05,  ..., -1.8314e-05,
         -1.2703e-05, -1.6488e-05],
        [-3.7074e-05, -2.7850e-05,  2.5764e-05,  ..., -3.0354e-05,
         -2.2411e-05, -2.3693e-05]], device='cuda:0')
Loss: 0.9678713083267212
Graident accumulation at epoch 0, step 871, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0159,  0.0153, -0.0282,  ...,  0.0288, -0.0149, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.3453e-05,  1.7525e-04,  6.2978e-05,  ..., -1.9179e-05,
          4.9894e-06,  2.5224e-04],
        [-1.6685e-05, -1.1284e-05,  8.8431e-06,  ..., -1.3647e-05,
         -8.9894e-06, -1.0658e-05],
        [ 6.1783e-06,  2.6090e-06,  1.1562e-06,  ...,  3.6536e-06,
          7.0736e-06, -6.1344e-06],
        [-6.0922e-06, -2.2593e-06,  4.3711e-06,  ..., -4.1800e-06,
         -3.4684e-06, -7.7518e-06],
        [-4.2888e-05, -2.9749e-05,  2.3231e-05,  ..., -3.3552e-05,
         -2.3799e-05, -2.4401e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6820e-08, 4.2693e-08, 4.5583e-08,  ..., 1.5935e-08, 1.0668e-07,
         2.8858e-08],
        [7.2626e-11, 3.6699e-11, 9.5374e-12,  ..., 4.7704e-11, 7.8060e-12,
         1.6717e-11],
        [1.2919e-09, 6.5610e-10, 1.2767e-10,  ..., 1.0326e-09, 1.3165e-10,
         3.3070e-10],
        [4.1872e-10, 2.9275e-10, 1.3833e-10,  ..., 3.8110e-10, 1.3368e-10,
         1.5284e-10],
        [3.4313e-10, 1.8215e-10, 4.0988e-11,  ..., 2.5154e-10, 4.0590e-11,
         9.2818e-11]], device='cuda:0')
optimizer state dict: 109.0
lr: [1.3150770923895586e-05, 1.3150770923895586e-05]
scheduler_last_epoch: 109


Running epoch 0, step 872, batch 872
Sampled inputs[:2]: tensor([[    0,  7377, 30662,  ...,   287,   694, 13403],
        [    0,  7333,   342,  ...,    13,  1818,  6183]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4694e-05,  3.8430e-06,  1.2026e-04,  ..., -7.9492e-05,
          2.9826e-05,  1.3023e-04],
        [-1.9222e-06, -1.3113e-06,  1.1995e-06,  ..., -1.5423e-06,
         -1.1921e-06, -1.4454e-06],
        [-3.7998e-06, -2.6673e-06,  2.5034e-06,  ..., -2.9951e-06,
         -2.2948e-06, -2.7418e-06],
        [-2.9504e-06, -1.9670e-06,  1.9372e-06,  ..., -2.3693e-06,
         -1.8105e-06, -2.2799e-06],
        [-5.0962e-06, -3.6657e-06,  3.3528e-06,  ..., -4.0233e-06,
         -3.1888e-06, -3.4273e-06]], device='cuda:0')
Loss: 0.9912868738174438


Running epoch 0, step 873, batch 873
Sampled inputs[:2]: tensor([[    0,    16,    14,  ...,  5148,   259,  1951],
        [    0,    14,   475,  ..., 44038,    12,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5376e-05,  6.1598e-05,  1.2990e-04,  ...,  7.7188e-06,
          1.7441e-04,  1.3764e-04],
        [-3.8072e-06, -2.7791e-06,  2.4810e-06,  ..., -3.1069e-06,
         -2.3022e-06, -2.6450e-06],
        [-7.3612e-06, -5.5134e-06,  5.0068e-06,  ..., -5.9456e-06,
         -4.3809e-06, -4.9770e-06],
        [-5.9158e-06, -4.2617e-06,  4.0233e-06,  ..., -4.8429e-06,
         -3.5465e-06, -4.2468e-06],
        [-9.7454e-06, -7.4506e-06,  6.6161e-06,  ..., -7.8976e-06,
         -6.0201e-06, -6.1542e-06]], device='cuda:0')
Loss: 1.022024154663086


Running epoch 0, step 874, batch 874
Sampled inputs[:2]: tensor([[    0, 13595,  3803,  ...,  1992,  4770,   818],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2039e-04,  1.1849e-04,  2.4109e-04,  ...,  3.8118e-05,
          3.2583e-04,  2.8059e-04],
        [-5.7891e-06, -4.2766e-06,  3.8594e-06,  ..., -4.7088e-06,
         -3.5390e-06, -3.9190e-06],
        [-1.1101e-05, -8.4192e-06,  7.6890e-06,  ..., -8.9854e-06,
         -6.7204e-06, -7.3761e-06],
        [-9.0301e-06, -6.6012e-06,  6.2585e-06,  ..., -7.3612e-06,
         -5.4836e-06, -6.3330e-06],
        [-1.4722e-05, -1.1414e-05,  1.0177e-05,  ..., -1.1981e-05,
         -9.2834e-06, -9.1791e-06]], device='cuda:0')
Loss: 1.0487486124038696


Running epoch 0, step 875, batch 875
Sampled inputs[:2]: tensor([[   0,  368,  729,  ...,  221,  380, 2830],
        [   0, 4441, 1821,  ...,  642, 2310,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.3658e-05,  1.1589e-04,  2.6560e-04,  ...,  1.2396e-04,
          4.2082e-04,  4.8832e-04],
        [-7.7561e-06, -5.6252e-06,  5.0738e-06,  ..., -6.2287e-06,
         -4.6045e-06, -5.3048e-06],
        [-1.4901e-05, -1.1116e-05,  1.0133e-05,  ..., -1.1906e-05,
         -8.7768e-06, -9.9987e-06],
        [-1.2055e-05, -8.6427e-06,  8.1956e-06,  ..., -9.7007e-06,
         -7.1004e-06, -8.5533e-06],
        [-1.9550e-05, -1.4946e-05,  1.3307e-05,  ..., -1.5736e-05,
         -1.2040e-05, -1.2308e-05]], device='cuda:0')
Loss: 1.005530834197998


Running epoch 0, step 876, batch 876
Sampled inputs[:2]: tensor([[    0,    12,  2735,  ...,    12,   344,  1496],
        [    0,  2346, 17886,  ...,   287,  6769,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5713e-05,  1.2551e-04,  4.4588e-04,  ...,  1.4346e-04,
          5.4310e-04,  6.6230e-04],
        [-9.6783e-06, -6.9886e-06,  6.3255e-06,  ..., -7.7710e-06,
         -5.7966e-06, -6.6236e-06],
        [-1.8477e-05, -1.3724e-05,  1.2547e-05,  ..., -1.4767e-05,
         -1.0967e-05, -1.2398e-05],
        [-1.5065e-05, -1.0729e-05,  1.0222e-05,  ..., -1.2115e-05,
         -8.9258e-06, -1.0684e-05],
        [-2.4289e-05, -1.8492e-05,  1.6510e-05,  ..., -1.9550e-05,
         -1.5095e-05, -1.5289e-05]], device='cuda:0')
Loss: 1.0483314990997314


Running epoch 0, step 877, batch 877
Sampled inputs[:2]: tensor([[    0,  8290,   391,  ...,   298,  1253,     7],
        [    0,   508,  3282,  ...,   334,   287, 31884]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5286e-04, -9.1958e-05,  7.1880e-04,  ...,  1.4346e-04,
          9.7983e-04,  8.5599e-04],
        [-1.1630e-05, -8.1956e-06,  7.5102e-06,  ..., -9.3207e-06,
         -6.8918e-06, -8.1733e-06],
        [-2.2069e-05, -1.6063e-05,  1.4886e-05,  ..., -1.7613e-05,
         -1.2994e-05, -1.5140e-05],
        [-1.7881e-05, -1.2435e-05,  1.2062e-05,  ..., -1.4350e-05,
         -1.0498e-05, -1.2994e-05],
        [-2.9027e-05, -2.1681e-05,  1.9595e-05,  ..., -2.3305e-05,
         -1.7881e-05, -1.8612e-05]], device='cuda:0')
Loss: 1.002442717552185


Running epoch 0, step 878, batch 878
Sampled inputs[:2]: tensor([[    0,   380,  8157,  ...,   943,   352,  2278],
        [    0,    34,     9,  ...,    19,    14, 45576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8024e-04, -9.1958e-05,  7.3030e-04,  ...,  2.7029e-04,
          1.1251e-03,  8.7662e-04],
        [-1.3486e-05, -9.4175e-06,  8.7470e-06,  ..., -1.0781e-05,
         -7.9721e-06, -9.6336e-06],
        [-2.5779e-05, -1.8552e-05,  1.7405e-05,  ..., -2.0474e-05,
         -1.5095e-05, -1.7956e-05],
        [-2.0713e-05, -1.4253e-05,  1.4104e-05,  ..., -1.6585e-05,
         -1.2122e-05, -1.5348e-05],
        [-3.3945e-05, -2.5094e-05,  2.2918e-05,  ..., -2.7120e-05,
         -2.0817e-05, -2.2098e-05]], device='cuda:0')
Loss: 1.0030056238174438


Running epoch 0, step 879, batch 879
Sampled inputs[:2]: tensor([[   0, 7219,  591,  ...,  278,  266, 5908],
        [   0, 1086,  292,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0212e-04,  4.3712e-05,  5.1023e-04,  ...,  3.0810e-04,
          9.9743e-04,  8.6769e-04],
        [-1.5453e-05, -1.0766e-05,  1.0103e-05,  ..., -1.2375e-05,
         -9.0152e-06, -1.0803e-05],
        [-2.9311e-05, -2.1040e-05,  1.9908e-05,  ..., -2.3350e-05,
         -1.6995e-05, -2.0057e-05],
        [-2.3812e-05, -1.6339e-05,  1.6324e-05,  ..., -1.9103e-05,
         -1.3731e-05, -1.7270e-05],
        [-3.8505e-05, -2.8431e-05,  2.6152e-05,  ..., -3.0890e-05,
         -2.3454e-05, -2.4602e-05]], device='cuda:0')
Loss: 1.022733211517334
Graident accumulation at epoch 0, step 879, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0229,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0288, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.1319e-05,  1.6210e-04,  1.0770e-04,  ...,  1.3549e-05,
          1.0423e-04,  3.1378e-04],
        [-1.6562e-05, -1.1232e-05,  8.9691e-06,  ..., -1.3520e-05,
         -8.9920e-06, -1.0672e-05],
        [ 2.6294e-06,  2.4408e-07,  3.0314e-06,  ...,  9.5320e-07,
          4.6668e-06, -7.5266e-06],
        [-7.8642e-06, -3.6673e-06,  5.5664e-06,  ..., -5.6723e-06,
         -4.4947e-06, -8.7036e-06],
        [-4.2450e-05, -2.9617e-05,  2.3523e-05,  ..., -3.3286e-05,
         -2.3764e-05, -2.4422e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6773e-08, 4.2652e-08, 4.5798e-08,  ..., 1.6014e-08, 1.0757e-07,
         2.9582e-08],
        [7.2792e-11, 3.6778e-11, 9.6299e-12,  ..., 4.7809e-11, 7.8794e-12,
         1.6817e-11],
        [1.2915e-09, 6.5588e-10, 1.2794e-10,  ..., 1.0321e-09, 1.3181e-10,
         3.3077e-10],
        [4.1887e-10, 2.9272e-10, 1.3846e-10,  ..., 3.8109e-10, 1.3374e-10,
         1.5298e-10],
        [3.4427e-10, 1.8277e-10, 4.1631e-11,  ..., 2.5224e-10, 4.1100e-11,
         9.3330e-11]], device='cuda:0')
optimizer state dict: 110.0
lr: [1.3033212842861785e-05, 1.3033212842861785e-05]
scheduler_last_epoch: 110


Running epoch 0, step 880, batch 880
Sampled inputs[:2]: tensor([[    0,    55,  2258,  ..., 32764,    75,   338],
        [    0,    12,   298,  ...,   292,    36,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5272e-05, -1.3333e-04, -6.8867e-05,  ...,  3.2516e-05,
          5.7758e-05, -9.9086e-05],
        [-2.0266e-06, -1.3113e-06,  1.2293e-06,  ..., -1.5497e-06,
         -1.2368e-06, -1.6168e-06],
        [-4.0233e-06, -2.6673e-06,  2.5481e-06,  ..., -3.0547e-06,
         -2.3991e-06, -3.1292e-06],
        [-3.1143e-06, -1.9670e-06,  1.9670e-06,  ..., -2.3693e-06,
         -1.8701e-06, -2.5630e-06],
        [-5.1856e-06, -3.5316e-06,  3.2932e-06,  ..., -3.9637e-06,
         -3.1888e-06, -3.7700e-06]], device='cuda:0')
Loss: 0.9994445443153381


Running epoch 0, step 881, batch 881
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  292,  334,  674],
        [   0, 4385,  342,  ..., 3644,  775,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.7648e-05, -4.9043e-05, -8.7913e-05,  ...,  5.6459e-05,
         -1.9399e-04, -1.6423e-04],
        [-3.9041e-06, -2.6077e-06,  2.5034e-06,  ..., -3.1069e-06,
         -2.3767e-06, -2.8685e-06],
        [-7.5996e-06, -5.2005e-06,  5.0515e-06,  ..., -5.9903e-06,
         -4.5449e-06, -5.4538e-06],
        [-6.1989e-06, -4.0531e-06,  4.1276e-06,  ..., -4.9174e-06,
         -3.7253e-06, -4.6939e-06],
        [-9.7454e-06, -6.8843e-06,  6.5118e-06,  ..., -7.7486e-06,
         -6.0350e-06, -6.5714e-06]], device='cuda:0')
Loss: 1.0290626287460327


Running epoch 0, step 882, batch 882
Sampled inputs[:2]: tensor([[    0,  2496, 10545,  ...,   287, 13978,   408],
        [    0,  5143,  3877,  ...,   292, 44003,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6624e-05, -1.0445e-05, -1.4271e-04,  ...,  1.0026e-04,
         -2.4792e-04, -2.2480e-04],
        [-5.8413e-06, -3.9935e-06,  3.7998e-06,  ..., -4.6417e-06,
         -3.5837e-06, -4.1872e-06],
        [-1.1310e-05, -7.9125e-06,  7.6145e-06,  ..., -8.9109e-06,
         -6.8247e-06, -7.9274e-06],
        [-9.3281e-06, -6.2436e-06,  6.2883e-06,  ..., -7.3761e-06,
         -5.6475e-06, -6.8843e-06],
        [-1.4544e-05, -1.0476e-05,  9.8348e-06,  ..., -1.1563e-05,
         -9.0897e-06, -9.5665e-06]], device='cuda:0')
Loss: 1.021942138671875


Running epoch 0, step 883, batch 883
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   630,  3765, 19107],
        [    0,  5750,   642,  ...,   221, 15441,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0792e-05, -1.4120e-05, -1.5225e-04,  ...,  1.6933e-04,
         -3.1046e-04, -2.9047e-04],
        [-7.8529e-06, -5.4538e-06,  5.1185e-06,  ..., -6.2138e-06,
         -4.7684e-06, -5.4538e-06],
        [-1.5154e-05, -1.0744e-05,  1.0192e-05,  ..., -1.1891e-05,
         -9.0450e-06, -1.0312e-05],
        [-1.2547e-05, -8.5384e-06,  8.4341e-06,  ..., -9.8795e-06,
         -7.5176e-06, -8.9705e-06],
        [-1.9401e-05, -1.4141e-05,  1.3098e-05,  ..., -1.5363e-05,
         -1.2025e-05, -1.2428e-05]], device='cuda:0')
Loss: 1.0406445264816284


Running epoch 0, step 884, batch 884
Sampled inputs[:2]: tensor([[   0,  586,  940,  ..., 1471, 2612,  591],
        [   0,  287,  271,  ..., 1039, 4186,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1016e-05, -6.5341e-05, -7.1541e-05,  ...,  1.7777e-04,
         -2.7964e-04, -3.8401e-04],
        [-9.9093e-06, -6.9439e-06,  6.4969e-06,  ..., -7.7784e-06,
         -5.9232e-06, -6.7279e-06],
        [-1.9148e-05, -1.3664e-05,  1.2904e-05,  ..., -1.4931e-05,
         -1.1280e-05, -1.2785e-05],
        [-1.5870e-05, -1.0908e-05,  1.0714e-05,  ..., -1.2413e-05,
         -9.3505e-06, -1.1101e-05],
        [-2.4408e-05, -1.7911e-05,  1.6496e-05,  ..., -1.9208e-05,
         -1.4946e-05, -1.5393e-05]], device='cuda:0')
Loss: 1.05224609375


Running epoch 0, step 885, batch 885
Sampled inputs[:2]: tensor([[   0,  328,  490,  ..., 6280, 4283, 4582],
        [   0,  266, 3382,  ...,  759,  631,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0873e-05, -6.9394e-05, -2.9547e-04,  ...,  1.7605e-04,
         -4.9849e-04, -7.1059e-04],
        [-1.1921e-05, -8.3223e-06,  7.8306e-06,  ..., -9.2983e-06,
         -7.0333e-06, -8.0541e-06],
        [-2.2948e-05, -1.6317e-05,  1.5497e-05,  ..., -1.7777e-05,
         -1.3351e-05, -1.5244e-05],
        [-1.9178e-05, -1.3128e-05,  1.3024e-05,  ..., -1.4901e-05,
         -1.1154e-05, -1.3381e-05],
        [-2.9176e-05, -2.1368e-05,  1.9759e-05,  ..., -2.2843e-05,
         -1.7717e-05, -1.8328e-05]], device='cuda:0')
Loss: 0.9987895488739014


Running epoch 0, step 886, batch 886
Sampled inputs[:2]: tensor([[    0, 15666,   609,  ...,   527,  4486,     9],
        [    0,   607,  2697,  ...,   391, 14410, 14997]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7469e-05, -1.6482e-04, -3.2336e-04,  ...,  1.7605e-04,
         -3.5878e-04, -6.3642e-04],
        [ 6.1435e-05,  7.2019e-05, -5.2278e-05,  ...,  5.2324e-05,
          7.7925e-05,  5.9890e-05],
        [-2.6494e-05, -1.8910e-05,  1.8016e-05,  ..., -2.0623e-05,
         -1.5706e-05, -1.7628e-05],
        [-2.2262e-05, -1.5289e-05,  1.5259e-05,  ..., -1.7434e-05,
         -1.3284e-05, -1.5661e-05],
        [-3.3796e-05, -2.4855e-05,  2.3052e-05,  ..., -2.6599e-05,
         -2.0906e-05, -2.1219e-05]], device='cuda:0')
Loss: 1.0354053974151611


Running epoch 0, step 887, batch 887
Sampled inputs[:2]: tensor([[    0,    15, 14761,  ...,   278,  3218,   287],
        [    0,   600,  9092,  ...,   554,  1485,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7222e-05, -9.5392e-05, -3.2456e-04,  ...,  1.2677e-04,
         -3.9350e-04, -5.9375e-04],
        [ 5.9394e-05,  7.0611e-05, -5.0974e-05,  ...,  5.0744e-05,
          7.6688e-05,  5.8348e-05],
        [-3.0369e-05, -2.1666e-05,  2.0593e-05,  ..., -2.3618e-05,
         -1.8016e-05, -2.0534e-05],
        [-2.5421e-05, -1.7419e-05,  1.7330e-05,  ..., -1.9878e-05,
         -1.5162e-05, -1.8135e-05],
        [-3.8743e-05, -2.8491e-05,  2.6345e-05,  ..., -3.0443e-05,
         -2.3991e-05, -2.4706e-05]], device='cuda:0')
Loss: 1.0097925662994385
Graident accumulation at epoch 0, step 887, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0282,  ...,  0.0289, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.0465e-05,  1.3635e-04,  6.4477e-05,  ...,  2.4871e-05,
          5.4460e-05,  2.2303e-04],
        [-8.9660e-06, -3.0481e-06,  2.9747e-06,  ..., -7.0935e-06,
         -4.2397e-07, -3.7703e-06],
        [-6.7041e-07, -1.9470e-06,  4.7876e-06,  ..., -1.5040e-06,
          2.3986e-06, -8.8273e-06],
        [-9.6199e-06, -5.0425e-06,  6.7428e-06,  ..., -7.0929e-06,
         -5.5614e-06, -9.6468e-06],
        [-4.2079e-05, -2.9505e-05,  2.3805e-05,  ..., -3.3002e-05,
         -2.3787e-05, -2.4450e-05]], device='cuda:0')
optimizer state dict: tensor([[5.6723e-08, 4.2619e-08, 4.5858e-08,  ..., 1.6014e-08, 1.0762e-07,
         2.9905e-08],
        [7.6247e-11, 4.1727e-11, 1.2219e-11,  ..., 5.0336e-11, 1.3753e-11,
         2.0205e-11],
        [1.2911e-09, 6.5570e-10, 1.2823e-10,  ..., 1.0317e-09, 1.3200e-10,
         3.3086e-10],
        [4.1910e-10, 2.9273e-10, 1.3862e-10,  ..., 3.8110e-10, 1.3383e-10,
         1.5316e-10],
        [3.4542e-10, 1.8340e-10, 4.2283e-11,  ..., 2.5292e-10, 4.1634e-11,
         9.3847e-11]], device='cuda:0')
optimizer state dict: 111.0
lr: [1.2915191260428308e-05, 1.2915191260428308e-05]
scheduler_last_epoch: 111


Running epoch 0, step 888, batch 888
Sampled inputs[:2]: tensor([[   0, 2577,  995,  ..., 6104,   14, 2032],
        [   0,  221,  334,  ...,  271,  266, 7246]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5492e-05,  1.3639e-04, -8.2028e-05,  ...,  2.9923e-06,
         -2.4797e-04, -1.0151e-04],
        [-2.0862e-06, -1.4380e-06,  1.2591e-06,  ..., -1.6913e-06,
         -1.4156e-06, -1.6242e-06],
        [-4.0531e-06, -2.8610e-06,  2.5183e-06,  ..., -3.2336e-06,
         -2.6822e-06, -3.0845e-06],
        [-3.1739e-06, -2.1458e-06,  1.9670e-06,  ..., -2.5481e-06,
         -2.1160e-06, -2.5332e-06],
        [-5.3346e-06, -3.8445e-06,  3.3379e-06,  ..., -4.2915e-06,
         -3.6359e-06, -3.8743e-06]], device='cuda:0')
Loss: 1.0396262407302856


Running epoch 0, step 889, batch 889
Sampled inputs[:2]: tensor([[   0, 1428,  266,  ..., 3169, 3058,  278],
        [   0,  333,  199,  ...,  292,   48, 1792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6558e-05,  2.5644e-04, -1.7856e-04,  ...,  1.1164e-06,
         -3.7690e-04, -1.4222e-04],
        [-4.2021e-06, -2.9951e-06,  2.4959e-06,  ..., -3.3826e-06,
         -2.8610e-06, -3.1963e-06],
        [-8.1658e-06, -5.9456e-06,  4.9621e-06,  ..., -6.4969e-06,
         -5.4538e-06, -6.1095e-06],
        [-6.3628e-06, -4.4703e-06,  3.8669e-06,  ..., -5.0962e-06,
         -4.2915e-06, -4.9621e-06],
        [-1.0520e-05, -7.8082e-06,  6.4522e-06,  ..., -8.4341e-06,
         -7.2122e-06, -7.5400e-06]], device='cuda:0')
Loss: 1.022591471672058


Running epoch 0, step 890, batch 890
Sampled inputs[:2]: tensor([[    0,    14, 38914,  ...,   266,  5690,   278],
        [    0,    13, 36961,  ...,  6671, 13711,  4568]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9688e-04,  4.1663e-04, -1.3048e-05,  ...,  2.0533e-05,
         -3.5494e-04, -1.3606e-04],
        [-6.2883e-06, -4.4927e-06,  3.6806e-06,  ..., -5.0291e-06,
         -4.3586e-06, -4.8578e-06],
        [-1.2279e-05, -8.9556e-06,  7.3463e-06,  ..., -9.7305e-06,
         -8.4043e-06, -9.3728e-06],
        [-9.5367e-06, -6.7353e-06,  5.7295e-06,  ..., -7.6294e-06,
         -6.5863e-06, -7.5549e-06],
        [-1.5974e-05, -1.1891e-05,  9.6560e-06,  ..., -1.2785e-05,
         -1.1206e-05, -1.1712e-05]], device='cuda:0')
Loss: 1.0108001232147217


Running epoch 0, step 891, batch 891
Sampled inputs[:2]: tensor([[    0,   346,   462,  ...,   474, 38333,    87],
        [    0,   300,  5201,  ...,  1997,  7423,   417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7015e-04,  5.5593e-04, -3.9408e-04,  ...,  2.5634e-04,
         -8.1481e-04, -3.6413e-04],
        [-8.3148e-06, -5.8562e-06,  5.0142e-06,  ..., -6.6310e-06,
         -5.4985e-06, -6.1616e-06],
        [-1.5989e-05, -1.1519e-05,  9.8795e-06,  ..., -1.2651e-05,
         -1.0476e-05, -1.1712e-05],
        [-1.2800e-05, -8.8811e-06,  7.9796e-06,  ..., -1.0237e-05,
         -8.4266e-06, -9.7603e-06],
        [-2.0593e-05, -1.5169e-05,  1.2800e-05,  ..., -1.6451e-05,
         -1.3888e-05, -1.4469e-05]], device='cuda:0')
Loss: 1.0296746492385864


Running epoch 0, step 892, batch 892
Sampled inputs[:2]: tensor([[    0,  1682,   271,  ...,   300,   266, 10935],
        [    0,  3592,   417,  ...,  4893,   328,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2010e-04,  5.0385e-04, -4.9860e-04,  ...,  1.1276e-04,
         -9.0887e-04, -4.5655e-04],
        [-1.0371e-05, -7.3537e-06,  6.3926e-06,  ..., -8.2105e-06,
         -6.7726e-06, -7.5400e-06],
        [-1.9982e-05, -1.4469e-05,  1.2606e-05,  ..., -1.5706e-05,
         -1.2904e-05, -1.4365e-05],
        [-1.6138e-05, -1.1280e-05,  1.0289e-05,  ..., -1.2785e-05,
         -1.0468e-05, -1.2085e-05],
        [-2.5570e-05, -1.8939e-05,  1.6198e-05,  ..., -2.0295e-05,
         -1.7047e-05, -1.7613e-05]], device='cuda:0')
Loss: 1.0202133655548096


Running epoch 0, step 893, batch 893
Sampled inputs[:2]: tensor([[   0,  342, 4781,  ...,  630,  940,  271],
        [   0, 1911,  679,  ...,   19, 3737,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9291e-04,  6.2520e-04, -5.9435e-04,  ...,  1.1292e-04,
         -9.3737e-04, -5.0668e-04],
        [-1.2353e-05, -8.8438e-06,  7.6145e-06,  ..., -9.8869e-06,
         -8.1360e-06, -8.9481e-06],
        [-2.3782e-05, -1.7360e-05,  1.5050e-05,  ..., -1.8865e-05,
         -1.5423e-05, -1.6943e-05],
        [-1.9267e-05, -1.3560e-05,  1.2271e-05,  ..., -1.5408e-05,
         -1.2584e-05, -1.4365e-05],
        [-3.0518e-05, -2.2754e-05,  1.9431e-05,  ..., -2.4408e-05,
         -2.0385e-05, -2.0772e-05]], device='cuda:0')
Loss: 1.008665680885315


Running epoch 0, step 894, batch 894
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,  3761,   527,  ..., 24518,   391,   638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9920e-04,  7.2163e-04, -7.8815e-04,  ...,  1.6898e-04,
         -1.2838e-03, -4.9774e-04],
        [-1.4484e-05, -1.0267e-05,  8.8438e-06,  ..., -1.1519e-05,
         -9.5889e-06, -1.0699e-05],
        [-2.7865e-05, -2.0191e-05,  1.7524e-05,  ..., -2.1994e-05,
         -1.8224e-05, -2.0251e-05],
        [-2.2382e-05, -1.5631e-05,  1.4193e-05,  ..., -1.7822e-05,
         -1.4760e-05, -1.7017e-05],
        [-3.5971e-05, -2.6599e-05,  2.2739e-05,  ..., -2.8610e-05,
         -2.4185e-05, -2.4974e-05]], device='cuda:0')
Loss: 0.9661146998405457


Running epoch 0, step 895, batch 895
Sampled inputs[:2]: tensor([[    0, 11541,  4784,  ...,  2837, 38541,    12],
        [    0,   367,  6267,  ...,     9,   287, 17056]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4990e-04,  6.2961e-04, -7.1798e-04,  ...,  1.8258e-04,
         -1.2838e-03, -5.0705e-04],
        [-1.6570e-05, -1.1839e-05,  1.0163e-05,  ..., -1.3195e-05,
         -1.0967e-05, -1.2062e-05],
        [-3.1859e-05, -2.3246e-05,  2.0117e-05,  ..., -2.5183e-05,
         -2.0817e-05, -2.2843e-05],
        [-2.5615e-05, -1.8045e-05,  1.6280e-05,  ..., -2.0415e-05,
         -1.6876e-05, -1.9193e-05],
        [-4.1097e-05, -3.0622e-05,  2.6092e-05,  ..., -3.2753e-05,
         -2.7627e-05, -2.8178e-05]], device='cuda:0')
Loss: 1.0617928504943848
Graident accumulation at epoch 0, step 895, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0024, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6572e-05,  1.8568e-04, -1.3769e-05,  ...,  4.0641e-05,
         -7.9367e-05,  1.5002e-04],
        [-9.7264e-06, -3.9272e-06,  3.6935e-06,  ..., -7.7037e-06,
         -1.4783e-06, -4.5996e-06],
        [-3.7892e-06, -4.0768e-06,  6.3205e-06,  ..., -3.8719e-06,
          7.7003e-08, -1.0229e-05],
        [-1.1219e-05, -6.3428e-06,  7.6965e-06,  ..., -8.4250e-06,
         -6.6928e-06, -1.0601e-05],
        [-4.1981e-05, -2.9616e-05,  2.4034e-05,  ..., -3.2977e-05,
         -2.4171e-05, -2.4823e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7088e-08, 4.2972e-08, 4.6327e-08,  ..., 1.6032e-08, 1.0916e-07,
         3.0133e-08],
        [7.6445e-11, 4.1826e-11, 1.2310e-11,  ..., 5.0460e-11, 1.3859e-11,
         2.0330e-11],
        [1.2908e-09, 6.5558e-10, 1.2851e-10,  ..., 1.0313e-09, 1.3230e-10,
         3.3105e-10],
        [4.1933e-10, 2.9276e-10, 1.3875e-10,  ..., 3.8114e-10, 1.3398e-10,
         1.5337e-10],
        [3.4677e-10, 1.8416e-10, 4.2922e-11,  ..., 2.5374e-10, 4.2356e-11,
         9.4547e-11]], device='cuda:0')
optimizer state dict: 112.0
lr: [1.2796724211323173e-05, 1.2796724211323173e-05]
scheduler_last_epoch: 112


Running epoch 0, step 896, batch 896
Sampled inputs[:2]: tensor([[    0,   491, 10524,  ...,  2218,  5627,  4199],
        [    0,   342, 43937,  ...,   298,   413,    29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8772e-04,  1.3136e-04,  1.8083e-04,  ...,  7.3277e-05,
         -1.0990e-04,  1.1591e-05],
        [-2.2054e-06, -1.6689e-06,  1.2666e-06,  ..., -1.7658e-06,
         -1.5125e-06, -1.5721e-06],
        [-4.2319e-06, -3.3081e-06,  2.5034e-06,  ..., -3.3975e-06,
         -2.9355e-06, -2.9951e-06],
        [-3.3528e-06, -2.5332e-06,  1.9670e-06,  ..., -2.6971e-06,
         -2.3395e-06, -2.4587e-06],
        [-5.5134e-06, -4.3809e-06,  3.2932e-06,  ..., -4.4703e-06,
         -3.9041e-06, -3.7700e-06]], device='cuda:0')
Loss: 1.0183024406433105


Running epoch 0, step 897, batch 897
Sampled inputs[:2]: tensor([[    0,   278,  3358,  ...,    12,   287,  9612],
        [    0,   843, 17111,  ...,    12,   461,  6176]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9233e-04,  1.5803e-04, -5.8876e-05,  ...,  1.0579e-04,
         -2.4398e-04, -2.4140e-04],
        [-4.4107e-06, -3.3602e-06,  2.5705e-06,  ..., -3.5167e-06,
         -2.9355e-06, -2.9281e-06],
        [-8.3447e-06, -6.5416e-06,  5.0217e-06,  ..., -6.6757e-06,
         -5.5730e-06, -5.5134e-06],
        [-6.7055e-06, -5.0813e-06,  3.9935e-06,  ..., -5.3644e-06,
         -4.5151e-06, -4.5896e-06],
        [-1.0669e-05, -8.5235e-06,  6.4671e-06,  ..., -8.6129e-06,
         -7.3016e-06, -6.7949e-06]], device='cuda:0')
Loss: 1.0265929698944092


Running epoch 0, step 898, batch 898
Sampled inputs[:2]: tensor([[    0,    13,  4467,  ...,  2390, 47857,   287],
        [    0,  1850,   311,  ...,  3655,  3133,  9000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1803e-04,  4.1956e-04, -4.9655e-05,  ...,  1.0172e-04,
         -3.4551e-04, -1.5566e-04],
        [-6.5565e-06, -4.9472e-06,  3.8221e-06,  ..., -5.2005e-06,
         -4.4256e-06, -4.5225e-06],
        [ 3.5420e-04,  3.2284e-04, -2.4325e-04,  ...,  2.1850e-04,
          2.3193e-04,  9.4628e-05],
        [-1.0043e-05, -7.4953e-06,  6.0052e-06,  ..., -7.9721e-06,
         -6.8098e-06, -7.1079e-06],
        [-1.5944e-05, -1.2547e-05,  9.7007e-06,  ..., -1.2726e-05,
         -1.0982e-05, -1.0446e-05]], device='cuda:0')
Loss: 1.0383036136627197


Running epoch 0, step 899, batch 899
Sampled inputs[:2]: tensor([[    0,  1184,   271,  ...,  7225,   292,   474],
        [    0,    14,   475,  ...,  7903,   266, 27772]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4768e-04,  5.0424e-04, -9.4835e-05,  ...,  1.0172e-04,
         -4.4173e-04, -3.0005e-04],
        [-8.6725e-06, -6.4746e-06,  5.0887e-06,  ..., -6.8396e-06,
         -5.7593e-06, -5.9977e-06],
        [ 3.5006e-04,  3.1979e-04, -2.4070e-04,  ...,  2.1532e-04,
          2.2940e-04,  9.1782e-05],
        [-1.3337e-05, -9.8348e-06,  8.0317e-06,  ..., -1.0505e-05,
         -8.8513e-06, -9.4771e-06],
        [-2.1130e-05, -1.6451e-05,  1.2919e-05,  ..., -1.6749e-05,
         -1.4275e-05, -1.3843e-05]], device='cuda:0')
Loss: 1.018768548965454


Running epoch 0, step 900, batch 900
Sampled inputs[:2]: tensor([[    0,   792,    83,  ...,   957, 13285,   271],
        [    0,  2785,  1061,  ...,  1194,   692,  4339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3391e-04,  5.0520e-04, -2.7725e-04,  ...,  1.8279e-04,
         -5.6956e-04, -4.2857e-04],
        [-1.0684e-05, -7.7859e-06,  6.2659e-06,  ..., -8.4490e-06,
         -7.0184e-06, -7.6070e-06],
        [ 3.4622e-04,  3.1726e-04, -2.3835e-04,  ...,  2.1231e-04,
          2.2709e-04,  8.8831e-05],
        [-1.6436e-05, -1.1787e-05,  9.9167e-06,  ..., -1.2964e-05,
         -1.0751e-05, -1.2025e-05],
        [-2.6017e-05, -1.9759e-05,  1.5959e-05,  ..., -2.0593e-05,
         -1.7285e-05, -1.7375e-05]], device='cuda:0')
Loss: 0.9940600991249084


Running epoch 0, step 901, batch 901
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,  6408,   278,  1119],
        [    0, 26074,   486,  ...,  2314,   266,  1090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6483e-04,  5.9172e-04, -7.8150e-04,  ...,  1.4728e-04,
         -9.0520e-04, -8.8228e-04],
        [-1.2755e-05, -9.2611e-06,  7.5996e-06,  ..., -1.0043e-05,
         -8.2850e-06, -8.9407e-06],
        [ 3.4237e-04,  3.1449e-04, -2.3582e-04,  ...,  2.0941e-04,
          2.2482e-04,  8.6402e-05],
        [-1.9714e-05, -1.4096e-05,  1.2107e-05,  ..., -1.5482e-05,
         -1.2718e-05, -1.4231e-05],
        [-3.0845e-05, -2.3335e-05,  1.9178e-05,  ..., -2.4304e-05,
         -2.0251e-05, -2.0280e-05]], device='cuda:0')
Loss: 1.005568027496338


Running epoch 0, step 902, batch 902
Sampled inputs[:2]: tensor([[   0, 2612,  271,  ...,  369, 9862,  287],
        [   0,  266, 2511,  ..., 3220, 4164, 1173]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5014e-04,  7.0155e-04, -7.8856e-04,  ...,  1.2077e-04,
         -9.4860e-04, -1.1247e-03],
        [-1.4797e-05, -1.0692e-05,  8.7917e-06,  ..., -1.1615e-05,
         -9.5963e-06, -1.0356e-05],
        [ 3.3850e-04,  3.1173e-04, -2.3348e-04,  ...,  2.0646e-04,
          2.2240e-04,  8.3810e-05],
        [-2.2933e-05, -1.6302e-05,  1.4074e-05,  ..., -1.7956e-05,
         -1.4775e-05, -1.6540e-05],
        [-3.5733e-05, -2.6911e-05,  2.2158e-05,  ..., -2.8074e-05,
         -2.3425e-05, -2.3395e-05]], device='cuda:0')
Loss: 0.989137589931488


Running epoch 0, step 903, batch 903
Sampled inputs[:2]: tensor([[    0,   367,  3704,  ...,  1746,    14,   759],
        [    0,   278,  1253,  ...,   266,  1274, 22300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7073e-04,  7.4962e-04, -7.6943e-04,  ...,  1.8854e-04,
         -1.0241e-03, -1.0691e-03],
        [-1.6883e-05, -1.2316e-05,  1.0088e-05,  ..., -1.3255e-05,
         -1.0841e-05, -1.1705e-05],
        [ 3.3462e-04,  3.0867e-04, -2.3100e-04,  ...,  2.0343e-04,
          2.2012e-04,  8.1336e-05],
        [-2.6256e-05, -1.8835e-05,  1.6175e-05,  ..., -2.0549e-05,
         -1.6727e-05, -1.8746e-05],
        [-4.0561e-05, -3.0786e-05,  2.5243e-05,  ..., -3.1874e-05,
         -2.6375e-05, -2.6330e-05]], device='cuda:0')
Loss: 1.0128318071365356
Graident accumulation at epoch 0, step 903, batch 903
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0028,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.8988e-05,  2.4207e-04, -8.9335e-05,  ...,  5.5431e-05,
         -1.7384e-04,  2.8107e-05],
        [-1.0442e-05, -4.7660e-06,  4.3330e-06,  ..., -8.2588e-06,
         -2.4145e-06, -5.3101e-06],
        [ 3.0052e-05,  2.7198e-05, -1.7412e-05,  ...,  1.6859e-05,
          2.2081e-05, -1.0725e-06],
        [-1.2723e-05, -7.5920e-06,  8.5443e-06,  ..., -9.6374e-06,
         -7.6962e-06, -1.1416e-05],
        [-4.1839e-05, -2.9733e-05,  2.4155e-05,  ..., -3.2867e-05,
         -2.4391e-05, -2.4974e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7169e-08, 4.3491e-08, 4.6873e-08,  ..., 1.6051e-08, 1.1010e-07,
         3.1246e-08],
        [7.6654e-11, 4.1935e-11, 1.2399e-11,  ..., 5.0585e-11, 1.3963e-11,
         2.0447e-11],
        [1.4015e-09, 7.5021e-10, 1.8175e-10,  ..., 1.0716e-09, 1.8062e-10,
         3.3734e-10],
        [4.1960e-10, 2.9283e-10, 1.3887e-10,  ..., 3.8118e-10, 1.3413e-10,
         1.5357e-10],
        [3.4806e-10, 1.8492e-10, 4.3516e-11,  ..., 2.5450e-10, 4.3009e-11,
         9.5146e-11]], device='cuda:0')
optimizer state dict: 113.0
lr: [1.2677829798345599e-05, 1.2677829798345599e-05]
scheduler_last_epoch: 113


Running epoch 0, step 904, batch 904
Sampled inputs[:2]: tensor([[    0,   474,   221,  ...,   287, 20640,   292],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7145e-05,  1.7705e-04, -1.6032e-04,  ...,  1.3295e-04,
         -3.5766e-04, -2.0278e-04],
        [-2.0266e-06, -1.4305e-06,  1.2293e-06,  ..., -1.6838e-06,
         -1.3784e-06, -1.4454e-06],
        [-3.7551e-06, -2.6971e-06,  2.3693e-06,  ..., -3.0547e-06,
         -2.4736e-06, -2.5928e-06],
        [-3.0994e-06, -2.1309e-06,  1.9372e-06,  ..., -2.5630e-06,
         -2.0713e-06, -2.2650e-06],
        [-4.9770e-06, -3.6657e-06,  3.1441e-06,  ..., -4.0531e-06,
         -3.3826e-06, -3.2634e-06]], device='cuda:0')
Loss: 0.9953353404998779


Running epoch 0, step 905, batch 905
Sampled inputs[:2]: tensor([[    0,   266,   283,  ...,   271, 48829,   580],
        [    0,  2018,  4798,  ...,   292,  1919,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5253e-05,  1.6484e-04, -1.3801e-04,  ...,  1.6190e-04,
         -4.6877e-04, -1.6253e-04],
        [-4.0680e-06, -2.9802e-06,  2.5257e-06,  ..., -3.2932e-06,
         -2.6748e-06, -2.7567e-06],
        [-7.5996e-06, -5.6624e-06,  4.9025e-06,  ..., -6.0648e-06,
         -4.8876e-06, -4.9919e-06],
        [-6.3479e-06, -4.5449e-06,  4.0978e-06,  ..., -5.1260e-06,
         -4.1425e-06, -4.4405e-06],
        [-9.7752e-06, -7.4357e-06,  6.3032e-06,  ..., -7.8231e-06,
         -6.4820e-06, -6.0797e-06]], device='cuda:0')
Loss: 1.0306710004806519


Running epoch 0, step 906, batch 906
Sampled inputs[:2]: tensor([[    0,  5301,   792,  ..., 27135, 34090,   292],
        [    0,  2366,  5036,  ...,  1477,   352,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7050e-05,  2.3561e-04, -2.7573e-04,  ...,  1.7955e-04,
         -5.9623e-04, -6.3507e-05],
        [-6.1989e-06, -4.4554e-06,  3.6582e-06,  ..., -4.9695e-06,
         -4.0308e-06, -4.4331e-06],
        [-1.1712e-05, -8.5831e-06,  7.2122e-06,  ..., -9.2685e-06,
         -7.4655e-06, -8.1360e-06],
        [-9.5516e-06, -6.7353e-06,  5.8562e-06,  ..., -7.6294e-06,
         -6.1691e-06, -6.9737e-06],
        [-1.5050e-05, -1.1221e-05,  9.2983e-06,  ..., -1.1936e-05,
         -9.8646e-06, -9.8944e-06]], device='cuda:0')
Loss: 1.0126535892486572


Running epoch 0, step 907, batch 907
Sampled inputs[:2]: tensor([[    0,   600,   518,  ...,  3134,   278, 37342],
        [    0,  2992,   352,  ...,   259,  2063,  6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3575e-05,  1.6939e-04, -4.1232e-04,  ...,  1.4486e-04,
         -8.3692e-04, -2.9687e-04],
        [-8.1360e-06, -5.8636e-06,  4.8876e-06,  ..., -6.5342e-06,
         -5.2452e-06, -5.6699e-06],
        [-1.5289e-05, -1.1221e-05,  9.5665e-06,  ..., -1.2115e-05,
         -9.6411e-06, -1.0356e-05],
        [-1.2711e-05, -8.9407e-06,  7.9274e-06,  ..., -1.0163e-05,
         -8.1211e-06, -9.0599e-06],
        [-1.9401e-05, -1.4499e-05,  1.2189e-05,  ..., -1.5408e-05,
         -1.2591e-05, -1.2428e-05]], device='cuda:0')
Loss: 0.993061363697052


Running epoch 0, step 908, batch 908
Sampled inputs[:2]: tensor([[   0,  593, 1387,  ...,  508, 8222, 1415],
        [   0,  607,  259,  ...,  995,   13, 6507]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6516e-06,  3.2482e-04, -4.4250e-04,  ...,  1.8231e-04,
         -9.2235e-04, -2.0381e-04],
        [-1.0267e-05, -7.4059e-06,  6.1467e-06,  ..., -8.1956e-06,
         -6.5789e-06, -7.0184e-06],
        [-1.9193e-05, -1.4141e-05,  1.1966e-05,  ..., -1.5169e-05,
         -1.2100e-05, -1.2800e-05],
        [-1.6034e-05, -1.1340e-05,  9.9540e-06,  ..., -1.2770e-05,
         -1.0222e-05, -1.1235e-05],
        [-2.4348e-05, -1.8299e-05,  1.5244e-05,  ..., -1.9312e-05,
         -1.5840e-05, -1.5393e-05]], device='cuda:0')
Loss: 0.9995962381362915


Running epoch 0, step 909, batch 909
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,  4032,  1109,   721],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6278e-05,  3.6931e-04, -3.9137e-04,  ...,  2.4989e-04,
         -1.0291e-03, -2.7747e-04],
        [-1.2338e-05, -8.9034e-06,  7.4208e-06,  ..., -9.8273e-06,
         -7.8082e-06, -8.2850e-06],
        [-2.2963e-05, -1.6928e-05,  1.4365e-05,  ..., -1.8135e-05,
         -1.4305e-05, -1.5080e-05],
        [-1.9267e-05, -1.3649e-05,  1.2010e-05,  ..., -1.5318e-05,
         -1.2130e-05, -1.3292e-05],
        [-2.8998e-05, -2.1830e-05,  1.8224e-05,  ..., -2.3007e-05,
         -1.8671e-05, -1.8060e-05]], device='cuda:0')
Loss: 1.0287054777145386


Running epoch 0, step 910, batch 910
Sampled inputs[:2]: tensor([[    0,  2523, 10780,  ...,  1041,    26, 13745],
        [    0,   654,   300,  ..., 21762,  3597, 11117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8133e-05,  4.8644e-04, -1.9764e-04,  ...,  2.4381e-04,
         -9.9723e-04, -1.1187e-04],
        [-1.4380e-05, -1.0476e-05,  8.6576e-06,  ..., -1.1489e-05,
         -9.1866e-06, -9.6187e-06],
        [-2.6807e-05, -1.9923e-05,  1.6779e-05,  ..., -2.1234e-05,
         -1.6838e-05, -1.7554e-05],
        [-2.2456e-05, -1.6063e-05,  1.3992e-05,  ..., -1.7896e-05,
         -1.4246e-05, -1.5438e-05],
        [-3.3855e-05, -2.5705e-05,  2.1279e-05,  ..., -2.6941e-05,
         -2.1979e-05, -2.1026e-05]], device='cuda:0')
Loss: 1.0334538221359253


Running epoch 0, step 911, batch 911
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,    12,   287,  ...,  2381, 12046,  2231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3104e-04,  4.0446e-04, -2.7085e-04,  ...,  3.1854e-04,
         -1.1088e-03, -1.7343e-05],
        [-1.6451e-05, -1.2025e-05,  9.9540e-06,  ..., -1.3106e-05,
         -1.0446e-05, -1.0923e-05],
        [-3.0711e-05, -2.2888e-05,  1.9267e-05,  ..., -2.4274e-05,
         -1.9178e-05, -1.9997e-05],
        [-2.5764e-05, -1.8507e-05,  1.6108e-05,  ..., -2.0459e-05,
         -1.6227e-05, -1.7598e-05],
        [-3.8743e-05, -2.9519e-05,  2.4408e-05,  ..., -3.0816e-05,
         -2.5049e-05, -2.3961e-05]], device='cuda:0')
Loss: 1.015276312828064
Graident accumulation at epoch 0, step 911, batch 911
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.4193e-05,  2.5831e-04, -1.0749e-04,  ...,  8.1742e-05,
         -2.6734e-04,  2.3562e-05],
        [-1.1043e-05, -5.4919e-06,  4.8951e-06,  ..., -8.7434e-06,
         -3.2176e-06, -5.8713e-06],
        [ 2.3976e-05,  2.2190e-05, -1.3744e-05,  ...,  1.2745e-05,
          1.7955e-05, -2.9649e-06],
        [-1.4027e-05, -8.6835e-06,  9.3007e-06,  ..., -1.0720e-05,
         -8.5493e-06, -1.2034e-05],
        [-4.1529e-05, -2.9712e-05,  2.4180e-05,  ..., -3.2661e-05,
         -2.4457e-05, -2.4872e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7129e-08, 4.3611e-08, 4.6900e-08,  ..., 1.6137e-08, 1.1122e-07,
         3.1215e-08],
        [7.6848e-11, 4.2038e-11, 1.2486e-11,  ..., 5.0706e-11, 1.4058e-11,
         2.0545e-11],
        [1.4010e-09, 7.4998e-10, 1.8193e-10,  ..., 1.0711e-09, 1.8081e-10,
         3.3740e-10],
        [4.1985e-10, 2.9288e-10, 1.3899e-10,  ..., 3.8121e-10, 1.3426e-10,
         1.5373e-10],
        [3.4922e-10, 1.8561e-10, 4.4068e-11,  ..., 2.5520e-10, 4.3593e-11,
         9.5625e-11]], device='cuda:0')
optimizer state dict: 114.0
lr: [1.255852618959973e-05, 1.255852618959973e-05]
scheduler_last_epoch: 114


Running epoch 0, step 912, batch 912
Sampled inputs[:2]: tensor([[    0,    15,  2537,  ...,    14,  3544,   417],
        [    0,  2734,  2338,  ...,  3977,   970, 10537]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0553e-05, -4.6238e-05, -2.8687e-05,  ..., -4.1729e-05,
          4.3315e-05, -1.2465e-04],
        [-1.9819e-06, -1.5870e-06,  1.2815e-06,  ..., -1.5944e-06,
         -1.3262e-06, -1.1921e-06],
        [-3.6955e-06, -3.0249e-06,  2.4736e-06,  ..., -2.9653e-06,
         -2.4289e-06, -2.2054e-06],
        [-3.0994e-06, -2.4587e-06,  2.0713e-06,  ..., -2.5034e-06,
         -2.0713e-06, -1.9521e-06],
        [-4.5896e-06, -3.8445e-06,  3.0845e-06,  ..., -3.7253e-06,
         -3.1441e-06, -2.5928e-06]], device='cuda:0')
Loss: 1.039971947669983


Running epoch 0, step 913, batch 913
Sampled inputs[:2]: tensor([[    0, 17694,    12,  ..., 12452,   446,   475],
        [    0, 40995,  5863,  ...,    13,  9819,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1830e-04, -2.7354e-05, -1.1729e-04,  ..., -4.3159e-05,
         -3.8655e-07, -2.1757e-04],
        [-3.9339e-06, -2.9802e-06,  2.4885e-06,  ..., -3.1590e-06,
         -2.6003e-06, -2.4363e-06],
        [-7.3016e-06, -5.6773e-06,  4.8280e-06,  ..., -5.8264e-06,
         -4.7237e-06, -4.4405e-06],
        [-6.2585e-06, -4.6790e-06,  4.1127e-06,  ..., -5.0366e-06,
         -4.1127e-06, -4.0382e-06],
        [-9.1195e-06, -7.2718e-06,  6.0797e-06,  ..., -7.3761e-06,
         -6.1542e-06, -5.2303e-06]], device='cuda:0')
Loss: 1.0115381479263306


Running epoch 0, step 914, batch 914
Sampled inputs[:2]: tensor([[    0,   278, 11554,  ...,  4713,  1039, 17088],
        [    0,  4191,   368,  ...,   367,  4182,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1532e-05,  5.6744e-05, -1.0624e-04,  ...,  6.6434e-07,
          2.6179e-05, -1.3915e-04],
        [-6.0201e-06, -4.4703e-06,  3.6657e-06,  ..., -4.7162e-06,
         -3.8818e-06, -3.8445e-06],
        [-1.1295e-05, -8.6278e-06,  7.1824e-06,  ..., -8.8215e-06,
         -7.1675e-06, -7.1079e-06],
        [-9.4175e-06, -6.8992e-06,  5.9456e-06,  ..., -7.3910e-06,
         -6.0201e-06, -6.2287e-06],
        [-1.3918e-05, -1.0923e-05,  8.9258e-06,  ..., -1.1027e-05,
         -9.2089e-06, -8.2701e-06]], device='cuda:0')
Loss: 1.0051997900009155


Running epoch 0, step 915, batch 915
Sampled inputs[:2]: tensor([[    0,  6518,   681,  ...,   401,  9748,   391],
        [    0,  3227,   278,  ...,  2950,    14, 15544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1069e-05,  1.1363e-04, -7.9289e-05,  ..., -1.6811e-04,
          1.5156e-04, -1.3406e-04],
        [-8.0913e-06, -5.9605e-06,  4.8503e-06,  ..., -6.3181e-06,
         -5.2005e-06, -5.2974e-06],
        [-1.5289e-05, -1.1578e-05,  9.5367e-06,  ..., -1.1876e-05,
         -9.6709e-06, -9.8646e-06],
        [-1.2621e-05, -9.1642e-06,  7.8231e-06,  ..., -9.8497e-06,
         -8.0168e-06, -8.5384e-06],
        [-1.8895e-05, -1.4693e-05,  1.1891e-05,  ..., -1.4901e-05,
         -1.2472e-05, -1.1548e-05]], device='cuda:0')
Loss: 1.024015188217163


Running epoch 0, step 916, batch 916
Sampled inputs[:2]: tensor([[    0,   328,   266,  ...,   352, 13107,  4302],
        [    0,  3532,   300,  ...,    12,   461,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8797e-05,  9.7808e-05, -2.7641e-04,  ..., -2.2150e-04,
         -3.1407e-05, -2.2315e-04],
        [-1.0088e-05, -7.3910e-06,  6.0275e-06,  ..., -7.8976e-06,
         -6.3479e-06, -6.4746e-06],
        [-1.8969e-05, -1.4260e-05,  1.1802e-05,  ..., -1.4767e-05,
         -1.1742e-05, -1.2010e-05],
        [-1.5825e-05, -1.1399e-05,  9.7752e-06,  ..., -1.2368e-05,
         -9.8124e-06, -1.0520e-05],
        [-2.3365e-05, -1.8045e-05,  1.4648e-05,  ..., -1.8448e-05,
         -1.5110e-05, -1.3992e-05]], device='cuda:0')
Loss: 0.9959996342658997


Running epoch 0, step 917, batch 917
Sampled inputs[:2]: tensor([[    0,   266,  9076,  ...,   490,   437, 41298],
        [    0,  8840,    26,  ...,    28,    16,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0297e-05,  6.9254e-05, -3.9644e-04,  ..., -1.9475e-04,
         -1.2433e-04, -4.1872e-04],
        [-1.2040e-05, -8.8289e-06,  7.1824e-06,  ..., -9.4697e-06,
         -7.6592e-06, -7.8231e-06],
        [-2.2665e-05, -1.7077e-05,  1.4082e-05,  ..., -1.7717e-05,
         -1.4171e-05, -1.4514e-05],
        [-1.8910e-05, -1.3635e-05,  1.1660e-05,  ..., -1.4856e-05,
         -1.1869e-05, -1.2726e-05],
        [-2.8014e-05, -2.1681e-05,  1.7554e-05,  ..., -2.2203e-05,
         -1.8299e-05, -1.6958e-05]], device='cuda:0')
Loss: 1.0171579122543335


Running epoch 0, step 918, batch 918
Sampled inputs[:2]: tensor([[    0,  1295,   898,  ...,   298, 38754,    66],
        [    0,   380,  1075,  ..., 16948,   266,  1751]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2062e-05,  1.7151e-04, -3.9137e-04,  ..., -2.1230e-04,
         -4.6129e-05, -4.5604e-04],
        [-1.3888e-05, -1.0200e-05,  8.2925e-06,  ..., -1.1049e-05,
         -8.8736e-06, -9.0748e-06],
        [-2.6137e-05, -1.9670e-05,  1.6302e-05,  ..., -2.0593e-05,
         -1.6332e-05, -1.6734e-05],
        [-2.1815e-05, -1.5736e-05,  1.3478e-05,  ..., -1.7300e-05,
         -1.3731e-05, -1.4722e-05],
        [-3.2276e-05, -2.4945e-05,  2.0325e-05,  ..., -2.5749e-05,
         -2.1070e-05, -1.9476e-05]], device='cuda:0')
Loss: 1.0034661293029785


Running epoch 0, step 919, batch 919
Sampled inputs[:2]: tensor([[    0,     9,  9925,  ...,   527, 23286,  6062],
        [    0,  1640,  1103,  ...,   685,  1478,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5717e-05,  4.5449e-04, -3.3179e-04,  ..., -2.9492e-04,
          2.3546e-05, -3.4906e-04],
        [-1.5870e-05, -1.1630e-05,  9.3803e-06,  ..., -1.2659e-05,
         -1.0200e-05, -1.0580e-05],
        [-2.9817e-05, -2.2426e-05,  1.8477e-05,  ..., -2.3544e-05,
         -1.8761e-05, -1.9446e-05],
        [-2.4870e-05, -1.7926e-05,  1.5244e-05,  ..., -1.9789e-05,
         -1.5773e-05, -1.7107e-05],
        [-3.6985e-05, -2.8536e-05,  2.3142e-05,  ..., -2.9549e-05,
         -2.4274e-05, -2.2739e-05]], device='cuda:0')
Loss: 0.9785957932472229
Graident accumulation at epoch 0, step 919, batch 919
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0068, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0148, -0.0176]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.6202e-05,  2.7793e-04, -1.2992e-04,  ...,  4.4076e-05,
         -2.3825e-04, -1.3701e-05],
        [-1.1526e-05, -6.1058e-06,  5.3436e-06,  ..., -9.1349e-06,
         -3.9159e-06, -6.3422e-06],
        [ 1.8596e-05,  1.7728e-05, -1.0522e-05,  ...,  9.1164e-06,
          1.4283e-05, -4.6131e-06],
        [-1.5111e-05, -9.6078e-06,  9.8950e-06,  ..., -1.1627e-05,
         -9.2717e-06, -1.2541e-05],
        [-4.1075e-05, -2.9594e-05,  2.4076e-05,  ..., -3.2350e-05,
         -2.4439e-05, -2.4659e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7081e-08, 4.3774e-08, 4.6963e-08,  ..., 1.6207e-08, 1.1111e-07,
         3.1305e-08],
        [7.7023e-11, 4.2131e-11, 1.2561e-11,  ..., 5.0816e-11, 1.4148e-11,
         2.0637e-11],
        [1.4005e-09, 7.4973e-10, 1.8209e-10,  ..., 1.0706e-09, 1.8098e-10,
         3.3744e-10],
        [4.2005e-10, 2.9290e-10, 1.3909e-10,  ..., 3.8122e-10, 1.3437e-10,
         1.5387e-10],
        [3.5024e-10, 1.8624e-10, 4.4560e-11,  ..., 2.5581e-10, 4.4139e-11,
         9.6047e-11]], device='cuda:0')
optimizer state dict: 115.0
lr: [1.24388316157184e-05, 1.24388316157184e-05]
scheduler_last_epoch: 115


Running epoch 0, step 920, batch 920
Sampled inputs[:2]: tensor([[    0,   516,   596,  ...,  3109,   287,   394],
        [    0,  3502,   527,  ..., 21301, 22248,  1773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0289e-05, -5.1640e-05,  7.1886e-05,  ..., -8.6103e-05,
          1.8731e-04,  1.3362e-04],
        [-2.0415e-06, -1.6466e-06,  1.1697e-06,  ..., -1.6317e-06,
         -1.4082e-06, -1.4007e-06],
        [-3.7998e-06, -3.1739e-06,  2.2799e-06,  ..., -3.0547e-06,
         -2.6226e-06, -2.6077e-06],
        [-3.0696e-06, -2.4736e-06,  1.8105e-06,  ..., -2.4736e-06,
         -2.1160e-06, -2.2054e-06],
        [-4.8578e-06, -4.1425e-06,  2.9355e-06,  ..., -3.9637e-06,
         -3.4720e-06, -3.1739e-06]], device='cuda:0')
Loss: 1.0187053680419922


Running epoch 0, step 921, batch 921
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0,  1867,   300,  ...,   259,  3095,  1842]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1388e-04, -1.0149e-04,  8.3543e-05,  ..., -9.2631e-05,
          4.2271e-04,  1.5363e-04],
        [-3.8892e-06, -3.0845e-06,  2.2724e-06,  ..., -3.1516e-06,
         -2.5854e-06, -2.5630e-06],
        [-7.1973e-06, -5.8711e-06,  4.4405e-06,  ..., -5.8115e-06,
         -4.7237e-06, -4.7237e-06],
        [-6.0350e-06, -4.7386e-06,  3.6582e-06,  ..., -4.9025e-06,
         -3.9786e-06, -4.1574e-06],
        [-9.1791e-06, -7.6890e-06,  5.6922e-06,  ..., -7.5102e-06,
         -6.3032e-06, -5.6773e-06]], device='cuda:0')
Loss: 1.0112366676330566


Running epoch 0, step 922, batch 922
Sampled inputs[:2]: tensor([[    0,    14,    71,  ...,   278, 14258, 12440],
        [    0,   445,    18,  ...,  1478,   578,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8756e-04, -2.4400e-04,  1.4468e-04,  ..., -1.1603e-04,
          6.7332e-04,  4.3127e-04],
        [-5.9605e-06, -4.5449e-06,  3.2932e-06,  ..., -4.7311e-06,
         -3.8967e-06, -4.1202e-06],
        [-1.1280e-05, -8.8811e-06,  6.5416e-06,  ..., -8.9556e-06,
         -7.3463e-06, -7.8529e-06],
        [-9.1940e-06, -6.9737e-06,  5.3123e-06,  ..., -7.3165e-06,
         -5.9605e-06, -6.6012e-06],
        [-1.4424e-05, -1.1653e-05,  8.3894e-06,  ..., -1.1623e-05,
         -9.8497e-06, -9.5516e-06]], device='cuda:0')
Loss: 1.016838550567627


Running epoch 0, step 923, batch 923
Sampled inputs[:2]: tensor([[    0,   271,  5738,  ...,    12,    21,  9023],
        [    0,   944,   278,  ..., 17330,  1683,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6942e-04, -3.6499e-04,  1.6886e-04,  ..., -6.6245e-05,
          6.8331e-04,  3.5688e-04],
        [-7.8827e-06, -6.0052e-06,  4.4852e-06,  ..., -6.2510e-06,
         -5.0664e-06, -5.2974e-06],
        [-1.4856e-05, -1.1683e-05,  8.8513e-06,  ..., -1.1787e-05,
         -9.5218e-06, -1.0028e-05],
        [-1.2323e-05, -9.3281e-06,  7.3239e-06,  ..., -9.7901e-06,
         -7.8604e-06, -8.5831e-06],
        [-1.8895e-05, -1.5274e-05,  1.1295e-05,  ..., -1.5229e-05,
         -1.2711e-05, -1.2100e-05]], device='cuda:0')
Loss: 1.0149421691894531


Running epoch 0, step 924, batch 924
Sampled inputs[:2]: tensor([[    0, 25409,   287,  ...,  1005,   344,  3493],
        [    0,   221,   374,  ...,  2296,   365,  4579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0541e-04, -6.1670e-04,  1.3461e-04,  ..., -1.0335e-05,
          1.1268e-03,  5.6646e-04],
        [-9.8348e-06, -7.5325e-06,  5.4836e-06,  ..., -7.8902e-06,
         -6.4820e-06, -6.7502e-06],
        [-1.8597e-05, -1.4678e-05,  1.0937e-05,  ..., -1.4886e-05,
         -1.2159e-05, -1.2711e-05],
        [-1.5289e-05, -1.1623e-05,  8.9332e-06,  ..., -1.2293e-05,
         -1.0021e-05, -1.0833e-05],
        [-2.3663e-05, -1.9178e-05,  1.4007e-05,  ..., -1.9222e-05,
         -1.6198e-05, -1.5318e-05]], device='cuda:0')
Loss: 1.0183651447296143


Running epoch 0, step 925, batch 925
Sampled inputs[:2]: tensor([[    0, 16763,  1538,  ...,   631,  3299,   437],
        [    0,    14,  2787,  ...,  9674,  2491,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5261e-04, -5.8159e-04, -2.7746e-04,  ...,  1.7964e-04,
          9.2137e-04,  3.6899e-04],
        [-1.1772e-05, -8.8438e-06,  6.6906e-06,  ..., -9.3654e-06,
         -7.5921e-06, -7.9721e-06],
        [-2.2158e-05, -1.7166e-05,  1.3277e-05,  ..., -1.7598e-05,
         -1.4186e-05, -1.4931e-05],
        [-1.8388e-05, -1.3694e-05,  1.0975e-05,  ..., -1.4648e-05,
         -1.1772e-05, -1.2890e-05],
        [-2.8074e-05, -2.2382e-05,  1.6913e-05,  ..., -2.2650e-05,
         -1.8880e-05, -1.7896e-05]], device='cuda:0')
Loss: 0.9992531538009644


Running epoch 0, step 926, batch 926
Sampled inputs[:2]: tensor([[   0,  494,  221,  ...,  437,  266, 2143],
        [   0, 2680,  271,  ..., 4971,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0645e-04, -1.0958e-03, -5.7532e-04,  ...,  1.4779e-04,
          1.4336e-03,  4.6256e-04],
        [-1.3672e-05, -1.0066e-05,  7.7710e-06,  ..., -1.0915e-05,
         -8.9258e-06, -9.5814e-06],
        [-2.5839e-05, -1.9610e-05,  1.5497e-05,  ..., -2.0519e-05,
         -1.6674e-05, -1.7926e-05],
        [-2.1219e-05, -1.5482e-05,  1.2711e-05,  ..., -1.6943e-05,
         -1.3754e-05, -1.5318e-05],
        [-3.2902e-05, -2.5705e-05,  1.9848e-05,  ..., -2.6524e-05,
         -2.2262e-05, -2.1562e-05]], device='cuda:0')
Loss: 0.9954448342323303


Running epoch 0, step 927, batch 927
Sampled inputs[:2]: tensor([[    0, 17561,    12,  ...,   741,   496,    14],
        [    0,  3779,    12,  ...,    12, 12774, 14261]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7871e-04, -1.0901e-03, -6.0562e-04,  ...,  1.7875e-04,
          1.5145e-03,  4.1866e-04],
        [-1.5609e-05, -1.1563e-05,  8.9183e-06,  ..., -1.2480e-05,
         -1.0170e-05, -1.0803e-05],
        [-2.9445e-05, -2.2471e-05,  1.7762e-05,  ..., -2.3425e-05,
         -1.8969e-05, -2.0161e-05],
        [-2.4170e-05, -1.7747e-05,  1.4551e-05,  ..., -1.9342e-05,
         -1.5661e-05, -1.7256e-05],
        [-3.7283e-05, -2.9296e-05,  2.2635e-05,  ..., -3.0115e-05,
         -2.5213e-05, -2.4095e-05]], device='cuda:0')
Loss: 0.9962348341941833
Graident accumulation at epoch 0, step 927, batch 927
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0023,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.1711e-05,  1.4113e-04, -1.7749e-04,  ...,  5.7543e-05,
         -6.2976e-05,  2.9535e-05],
        [-1.1934e-05, -6.6515e-06,  5.7011e-06,  ..., -9.4694e-06,
         -4.5413e-06, -6.7883e-06],
        [ 1.3792e-05,  1.3708e-05, -7.6934e-06,  ...,  5.8623e-06,
          1.0958e-05, -6.1679e-06],
        [-1.6017e-05, -1.0422e-05,  1.0361e-05,  ..., -1.2398e-05,
         -9.9106e-06, -1.3013e-05],
        [-4.0696e-05, -2.9564e-05,  2.3932e-05,  ..., -3.2127e-05,
         -2.4516e-05, -2.4603e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7253e-08, 4.4919e-08, 4.7283e-08,  ..., 1.6223e-08, 1.1329e-07,
         3.1449e-08],
        [7.7189e-11, 4.2223e-11, 1.2628e-11,  ..., 5.0921e-11, 1.4237e-11,
         2.0733e-11],
        [1.4000e-09, 7.4949e-10, 1.8223e-10,  ..., 1.0701e-09, 1.8116e-10,
         3.3751e-10],
        [4.2021e-10, 2.9293e-10, 1.3916e-10,  ..., 3.8122e-10, 1.3448e-10,
         1.5401e-10],
        [3.5128e-10, 1.8691e-10, 4.5028e-11,  ..., 2.5647e-10, 4.4731e-11,
         9.6531e-11]], device='cuda:0')
optimizer state dict: 116.0
lr: [1.2318764367077325e-05, 1.2318764367077325e-05]
scheduler_last_epoch: 116


Running epoch 0, step 928, batch 928
Sampled inputs[:2]: tensor([[    0, 10205,   342,  ...,  2523,  4729, 13753],
        [    0,   287,   266,  ...,   998,   342, 17709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.1058e-06, -1.9563e-04, -7.4105e-05,  ...,  0.0000e+00,
          7.2454e-05,  2.9120e-05],
        [-1.9521e-06, -1.5870e-06,  1.1176e-06,  ..., -1.5646e-06,
         -1.2964e-06, -1.2070e-06],
        [-3.7104e-06, -3.1143e-06,  2.2054e-06,  ..., -2.9802e-06,
         -2.4438e-06, -2.2799e-06],
        [-3.0994e-06, -2.5332e-06,  1.8254e-06,  ..., -2.5034e-06,
         -2.0862e-06, -2.0117e-06],
        [-4.7386e-06, -4.0829e-06,  2.8461e-06,  ..., -3.8445e-06,
         -3.2783e-06, -2.7418e-06]], device='cuda:0')
Loss: 1.0483304262161255


Running epoch 0, step 929, batch 929
Sampled inputs[:2]: tensor([[   0, 4108,   85,  ...,   40,   12, 1530],
        [   0,  275, 4452,  ...,   12, 3516, 5227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0272e-06, -1.9393e-04, -1.4200e-04,  ...,  5.8460e-05,
         -3.0025e-05,  9.1096e-06],
        [-3.8892e-06, -3.0994e-06,  2.2203e-06,  ..., -3.1367e-06,
         -2.4959e-06, -2.3767e-06],
        [-7.3910e-06, -6.0499e-06,  4.4107e-06,  ..., -5.9456e-06,
         -4.6790e-06, -4.4703e-06],
        [-6.1691e-06, -4.9025e-06,  3.6508e-06,  ..., -4.9919e-06,
         -3.9712e-06, -3.9339e-06],
        [-9.2387e-06, -7.7933e-06,  5.5879e-06,  ..., -7.5549e-06,
         -6.1989e-06, -5.2750e-06]], device='cuda:0')
Loss: 1.0369552373886108


Running epoch 0, step 930, batch 930
Sampled inputs[:2]: tensor([[   0, 2544,  394,  ...,   14, 1062,  516],
        [   0, 4995,  287,  ...,  300, 4531, 4729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2996e-04, -2.9236e-04, -1.5060e-04,  ...,  5.5598e-05,
          2.3242e-04,  8.5848e-05],
        [-5.8264e-06, -4.6119e-06,  3.2559e-06,  ..., -4.7162e-06,
         -3.8669e-06, -3.7104e-06],
        [-1.1176e-05, -9.0748e-06,  6.5565e-06,  ..., -9.0003e-06,
         -7.3016e-06, -7.0333e-06],
        [-9.1344e-06, -7.1824e-06,  5.3197e-06,  ..., -7.4059e-06,
         -6.0573e-06, -6.0648e-06],
        [-1.4096e-05, -1.1727e-05,  8.3447e-06,  ..., -1.1519e-05,
         -9.6858e-06, -8.3596e-06]], device='cuda:0')
Loss: 1.0254095792770386


Running epoch 0, step 931, batch 931
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,    19,     9,  ..., 11504,   446,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4661e-04, -4.2033e-04, -1.5711e-04,  ..., -4.7490e-05,
          4.5044e-04,  1.4720e-04],
        [-7.8529e-06, -6.1467e-06,  4.3437e-06,  ..., -6.3255e-06,
         -5.0962e-06, -5.0366e-06],
        [-1.4991e-05, -1.2055e-05,  8.7172e-06,  ..., -1.2040e-05,
         -9.6112e-06, -9.5218e-06],
        [-1.2234e-05, -9.5069e-06,  7.0482e-06,  ..., -9.8795e-06,
         -7.9274e-06, -8.1658e-06],
        [-1.8984e-05, -1.5631e-05,  1.1131e-05,  ..., -1.5453e-05,
         -1.2800e-05, -1.1370e-05]], device='cuda:0')
Loss: 1.0188387632369995


Running epoch 0, step 932, batch 932
Sampled inputs[:2]: tensor([[    0, 34525,  2008,  ...,  1194,   300, 11120],
        [    0,   292, 21215,  ...,   266,   818,  1527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6805e-05, -2.4344e-04, -3.8178e-04,  ...,  3.2378e-05,
          4.1212e-04,  1.0795e-04],
        [-9.7379e-06, -7.5102e-06,  5.3719e-06,  ..., -7.8529e-06,
         -6.2659e-06, -6.2734e-06],
        [-1.8567e-05, -1.4693e-05,  1.0803e-05,  ..., -1.4886e-05,
         -1.1802e-05, -1.1787e-05],
        [-1.5244e-05, -1.1638e-05,  8.8140e-06,  ..., -1.2308e-05,
         -9.7901e-06, -1.0207e-05],
        [-2.3544e-05, -1.9088e-05,  1.3828e-05,  ..., -1.9118e-05,
         -1.5706e-05, -1.4052e-05]], device='cuda:0')
Loss: 0.9942504167556763


Running epoch 0, step 933, batch 933
Sampled inputs[:2]: tensor([[   0,   12,  630,  ..., 5049,   14, 2371],
        [   0, 7428, 1566,  ...,  199, 1726, 5647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1700e-04, -2.9035e-04, -5.1991e-04,  ...,  7.0542e-05,
          4.3088e-04,  9.7575e-05],
        [-1.1601e-05, -8.9258e-06,  6.4820e-06,  ..., -9.3803e-06,
         -7.4133e-06, -7.3761e-06],
        [ 4.8158e-05,  5.8297e-05, -2.5690e-05,  ...,  5.2974e-05,
          1.0083e-04,  3.1831e-05],
        [-1.8209e-05, -1.3828e-05,  1.0654e-05,  ..., -1.4707e-05,
         -1.1586e-05, -1.2025e-05],
        [-2.8014e-05, -2.2605e-05,  1.6645e-05,  ..., -2.2754e-05,
         -1.8507e-05, -1.6466e-05]], device='cuda:0')
Loss: 1.0170390605926514


Running epoch 0, step 934, batch 934
Sampled inputs[:2]: tensor([[    0,   287,  1790,  ..., 11367,  9476,  2545],
        [    0, 10676,   328,  ...,     9,   360,  2583]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1698e-05, -3.7442e-04, -3.8191e-04,  ...,  7.4757e-05,
          5.9181e-04,  2.1257e-04],
        [-1.3523e-05, -1.0557e-05,  7.5921e-06,  ..., -1.0960e-05,
         -8.6948e-06, -8.5160e-06],
        [ 4.4358e-05,  5.5004e-05, -2.3411e-05,  ...,  4.9860e-05,
          9.8324e-05,  2.9566e-05],
        [-2.1175e-05, -1.6332e-05,  1.2420e-05,  ..., -1.7136e-05,
         -1.3553e-05, -1.3858e-05],
        [-3.2723e-05, -2.6807e-05,  1.9491e-05,  ..., -2.6688e-05,
         -2.1756e-05, -1.9118e-05]], device='cuda:0')
Loss: 1.0329372882843018


Running epoch 0, step 935, batch 935
Sampled inputs[:2]: tensor([[    0,  2836,  3084,  ...,  3634,  6464,   271],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1390e-04, -4.0698e-04, -2.1276e-04,  ...,  1.0774e-04,
          7.3575e-04,  1.9380e-04],
        [-1.5460e-05, -1.2070e-05,  8.6874e-06,  ..., -1.2547e-05,
         -9.9465e-06, -9.6560e-06],
        [ 4.0662e-05,  5.2039e-05, -2.1205e-05,  ...,  4.6835e-05,
          9.5969e-05,  2.7406e-05],
        [-2.4125e-05, -1.8612e-05,  1.4156e-05,  ..., -1.9565e-05,
         -1.5445e-05, -1.5676e-05],
        [-3.7342e-05, -3.0622e-05,  2.2277e-05,  ..., -3.0532e-05,
         -2.4840e-05, -2.1681e-05]], device='cuda:0')
Loss: 1.019545316696167
Graident accumulation at epoch 0, step 935, batch 935
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0022,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0158,  0.0153, -0.0283,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.1929e-05,  8.6319e-05, -1.8102e-04,  ...,  6.2563e-05,
          1.6896e-05,  4.5962e-05],
        [-1.2287e-05, -7.1934e-06,  5.9997e-06,  ..., -9.7772e-06,
         -5.0818e-06, -7.0751e-06],
        [ 1.6479e-05,  1.7541e-05, -9.0446e-06,  ...,  9.9595e-06,
          1.9459e-05, -2.8105e-06],
        [-1.6828e-05, -1.1241e-05,  1.0740e-05,  ..., -1.3115e-05,
         -1.0464e-05, -1.3279e-05],
        [-4.0360e-05, -2.9670e-05,  2.3766e-05,  ..., -3.1967e-05,
         -2.4549e-05, -2.4310e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7208e-08, 4.5040e-08, 4.7280e-08,  ..., 1.6219e-08, 1.1372e-07,
         3.1455e-08],
        [7.7351e-11, 4.2326e-11, 1.2691e-11,  ..., 5.1027e-11, 1.4322e-11,
         2.0805e-11],
        [1.4002e-09, 7.5145e-10, 1.8249e-10,  ..., 1.0712e-09, 1.9019e-10,
         3.3792e-10],
        [4.2037e-10, 2.9298e-10, 1.3922e-10,  ..., 3.8122e-10, 1.3459e-10,
         1.5410e-10],
        [3.5232e-10, 1.8766e-10, 4.5479e-11,  ..., 2.5714e-10, 4.5303e-11,
         9.6905e-11]], device='cuda:0')
optimizer state dict: 117.0
lr: [1.219834279100018e-05, 1.219834279100018e-05]
scheduler_last_epoch: 117


Running epoch 0, step 936, batch 936
Sampled inputs[:2]: tensor([[    0, 11054,    12,  ...,   560,   199,   677],
        [    0, 19720,    12,  ...,  1239,    12, 22324]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1758e-06,  1.4815e-04, -8.5235e-05,  ..., -8.0725e-05,
         -2.3796e-05,  7.1903e-05],
        [-1.8403e-06, -1.4231e-06,  9.6858e-07,  ..., -1.5721e-06,
         -1.2517e-06, -1.1548e-06],
        [-3.5167e-06, -2.8014e-06,  1.9521e-06,  ..., -3.0100e-06,
         -2.3544e-06, -2.2054e-06],
        [-2.9355e-06, -2.2501e-06,  1.5944e-06,  ..., -2.5332e-06,
         -1.9968e-06, -1.9222e-06],
        [-4.5002e-06, -3.6955e-06,  2.5183e-06,  ..., -3.9041e-06,
         -3.1888e-06, -2.6673e-06]], device='cuda:0')
Loss: 0.990984320640564


Running epoch 0, step 937, batch 937
Sampled inputs[:2]: tensor([[    0,  5597, 11929,  ...,   271,   275,   955],
        [    0, 16803,   965,  ..., 36064,    12, 13769]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4789e-05,  1.0644e-04, -1.4604e-04,  ..., -2.7796e-05,
         -8.5100e-05,  9.4249e-06],
        [-3.8221e-06, -2.8461e-06,  2.0415e-06,  ..., -3.1367e-06,
         -2.3693e-06, -2.3246e-06],
        [-7.3612e-06, -5.6624e-06,  4.1276e-06,  ..., -6.0648e-06,
         -4.5449e-06, -4.4852e-06],
        [-6.1691e-06, -4.5598e-06,  3.4049e-06,  ..., -5.0813e-06,
         -3.7998e-06, -3.8892e-06],
        [-9.2983e-06, -7.3910e-06,  5.2601e-06,  ..., -7.8082e-06,
         -6.1244e-06, -5.3793e-06]], device='cuda:0')
Loss: 1.0254846811294556


Running epoch 0, step 938, batch 938
Sampled inputs[:2]: tensor([[    0,  4337,  2057,  ...,  3020,  1722,   369],
        [    0,   278,  6653,  ...,  7524,   271, 28279]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0381e-05, -2.6064e-05, -1.3391e-04,  ..., -1.4004e-05,
         -9.2380e-05, -4.1259e-05],
        [-5.5581e-06, -4.0978e-06,  3.0920e-06,  ..., -4.6417e-06,
         -3.5763e-06, -3.5092e-06],
        [-1.0654e-05, -8.0913e-06,  6.2585e-06,  ..., -8.8811e-06,
         -6.7800e-06, -6.6459e-06],
        [-9.0003e-06, -6.5416e-06,  5.2825e-06,  ..., -7.5400e-06,
         -5.7667e-06, -5.9009e-06],
        [-1.3530e-05, -1.0625e-05,  8.0168e-06,  ..., -1.1444e-05,
         -9.1195e-06, -7.9572e-06]], device='cuda:0')
Loss: 0.988310694694519


Running epoch 0, step 939, batch 939
Sampled inputs[:2]: tensor([[   0,  365, 1110,  ..., 4130,  221,  199],
        [   0,  969,  258,  ...,  726, 5303, 6512]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4102e-05,  8.1746e-05, -1.2656e-05,  ..., -1.7018e-05,
         -9.0830e-05, -5.9353e-05],
        [-7.4133e-06, -5.5209e-06,  4.0680e-06,  ..., -6.2138e-06,
         -4.8131e-06, -4.7460e-06],
        [-1.4365e-05, -1.0982e-05,  8.2850e-06,  ..., -1.1995e-05,
         -9.2089e-06, -9.1046e-06],
        [-1.1966e-05, -8.7768e-06,  6.8843e-06,  ..., -1.0043e-05,
         -7.7039e-06, -7.9423e-06],
        [-1.8269e-05, -1.4409e-05,  1.0610e-05,  ..., -1.5467e-05,
         -1.2368e-05, -1.0937e-05]], device='cuda:0')
Loss: 1.0314757823944092


Running epoch 0, step 940, batch 940
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,   14,  333,  199],
        [   0,  278, 6046,  ..., 1671,  199,  395]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6421e-04,  1.3107e-04, -2.4602e-04,  ...,  5.5966e-05,
         -2.7265e-04, -2.4670e-04],
        [-9.2462e-06, -6.6683e-06,  5.0589e-06,  ..., -7.6890e-06,
         -5.8487e-06, -5.9232e-06],
        [-1.7986e-05, -1.3322e-05,  1.0356e-05,  ..., -1.4886e-05,
         -1.1235e-05, -1.1384e-05],
        [-1.4976e-05, -1.0617e-05,  8.5905e-06,  ..., -1.2457e-05,
         -9.3803e-06, -9.9391e-06],
        [-2.2560e-05, -1.7300e-05,  1.3098e-05,  ..., -1.8954e-05,
         -1.4901e-05, -1.3456e-05]], device='cuda:0')
Loss: 0.9734036326408386


Running epoch 0, step 941, batch 941
Sampled inputs[:2]: tensor([[    0, 14700,   717,  ..., 10570,   292,   221],
        [    0,   446, 28686,  ...,    35,  2706, 19712]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9233e-04,  1.0943e-04, -3.1842e-04,  ...,  2.5932e-06,
         -1.7089e-04, -2.6354e-04],
        [-1.1086e-05, -7.9721e-06,  5.8822e-06,  ..., -9.3281e-06,
         -7.1749e-06, -7.3463e-06],
        [-2.1473e-05, -1.5914e-05,  1.2115e-05,  ..., -1.7941e-05,
         -1.3679e-05, -1.3962e-05],
        [-1.7762e-05, -1.2569e-05,  9.9018e-06,  ..., -1.4961e-05,
         -1.1437e-05, -1.2159e-05],
        [-2.7210e-05, -2.0802e-05,  1.5482e-05,  ..., -2.3007e-05,
         -1.8209e-05, -1.6659e-05]], device='cuda:0')
Loss: 0.9864738583564758


Running epoch 0, step 942, batch 942
Sampled inputs[:2]: tensor([[    0, 18981,    13,  ...,   365,  2714,   408],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0176e-04,  1.5094e-04, -3.6873e-04,  ..., -1.2129e-06,
         -1.0706e-04, -1.4445e-04],
        [-1.3039e-05, -9.3877e-06,  6.8247e-06,  ..., -1.0908e-05,
         -8.4490e-06, -8.6650e-06],
        [-2.5317e-05, -1.8761e-05,  1.4067e-05,  ..., -2.1040e-05,
         -1.6138e-05, -1.6585e-05],
        [-2.0772e-05, -1.4745e-05,  1.1414e-05,  ..., -1.7390e-05,
         -1.3374e-05, -1.4246e-05],
        [-3.2067e-05, -2.4498e-05,  1.7971e-05,  ..., -2.6971e-05,
         -2.1473e-05, -1.9804e-05]], device='cuda:0')
Loss: 0.998777449131012


Running epoch 0, step 943, batch 943
Sampled inputs[:2]: tensor([[    0,  2793,   271,  ...,   374,   298,   527],
        [    0,   870,   278,  ...,  1274, 10112,  3269]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7026e-04,  8.5873e-05, -5.2153e-04,  ..., -1.8065e-06,
         -4.1118e-05, -9.6192e-06],
        [-1.4789e-05, -1.0647e-05,  7.7523e-06,  ..., -1.2442e-05,
         -9.6262e-06, -9.8497e-06],
        [-2.8670e-05, -2.1189e-05,  1.5989e-05,  ..., -2.3887e-05,
         -1.8269e-05, -1.8731e-05],
        [-2.3618e-05, -1.6712e-05,  1.2994e-05,  ..., -1.9848e-05,
         -1.5229e-05, -1.6212e-05],
        [-3.6389e-05, -2.7686e-05,  2.0504e-05,  ..., -3.0652e-05,
         -2.4289e-05, -2.2367e-05]], device='cuda:0')
Loss: 1.0241080522537231
Graident accumulation at epoch 0, step 943, batch 943
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0022,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0158,  0.0154, -0.0283,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6762e-05,  8.6274e-05, -2.1507e-04,  ...,  5.6126e-05,
          1.1095e-05,  4.0404e-05],
        [-1.2537e-05, -7.5387e-06,  6.1750e-06,  ..., -1.0044e-05,
         -5.5362e-06, -7.3525e-06],
        [ 1.1964e-05,  1.3668e-05, -6.5412e-06,  ...,  6.5749e-06,
          1.5687e-05, -4.4025e-06],
        [-1.7507e-05, -1.1788e-05,  1.0966e-05,  ..., -1.3788e-05,
         -1.0941e-05, -1.3572e-05],
        [-3.9963e-05, -2.9472e-05,  2.3440e-05,  ..., -3.1836e-05,
         -2.4523e-05, -2.4116e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7288e-08, 4.5002e-08, 4.7505e-08,  ..., 1.6202e-08, 1.1361e-07,
         3.1424e-08],
        [7.7493e-11, 4.2397e-11, 1.2739e-11,  ..., 5.1131e-11, 1.4400e-11,
         2.0882e-11],
        [1.3997e-09, 7.5114e-10, 1.8257e-10,  ..., 1.0707e-09, 1.9033e-10,
         3.3793e-10],
        [4.2051e-10, 2.9297e-10, 1.3925e-10,  ..., 3.8123e-10, 1.3469e-10,
         1.5421e-10],
        [3.5329e-10, 1.8824e-10, 4.5854e-11,  ..., 2.5782e-10, 4.5848e-11,
         9.7308e-11]], device='cuda:0')
optimizer state dict: 118.0
lr: [1.2077585288954968e-05, 1.2077585288954968e-05]
scheduler_last_epoch: 118


Running epoch 0, step 944, batch 944
Sampled inputs[:2]: tensor([[   0, 6491, 3667,  ..., 5042,   14, 2152],
        [   0, 6668,  565,  ...,  360,  259, 8166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1491e-05,  9.1079e-05,  9.5135e-05,  ..., -5.1371e-05,
          1.3649e-05, -2.0818e-04],
        [-1.8328e-06, -1.3933e-06,  9.3877e-07,  ..., -1.6019e-06,
         -1.2890e-06, -1.1250e-06],
        [-3.6955e-06, -2.8610e-06,  1.9819e-06,  ..., -3.2037e-06,
         -2.5481e-06, -2.2650e-06],
        [-3.0249e-06, -2.2650e-06,  1.5870e-06,  ..., -2.6226e-06,
         -2.1160e-06, -1.9372e-06],
        [-4.6194e-06, -3.6806e-06,  2.5034e-06,  ..., -4.0531e-06,
         -3.3379e-06, -2.6822e-06]], device='cuda:0')
Loss: 1.0471160411834717


Running epoch 0, step 945, batch 945
Sampled inputs[:2]: tensor([[   0,  759, 4585,  ...,  360,  300,  670],
        [   0, 7180,  266,  ..., 1805,   12,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3048e-04,  1.5613e-04,  1.6939e-04,  ..., -1.1636e-04,
         -3.6904e-05, -2.2301e-04],
        [-3.6731e-06, -2.5332e-06,  1.8701e-06,  ..., -3.1292e-06,
         -2.3842e-06, -2.3618e-06],
        [-7.2867e-06, -5.1409e-06,  3.9339e-06,  ..., -6.1393e-06,
         -4.6492e-06, -4.6045e-06],
        [-5.9605e-06, -4.0382e-06,  3.1665e-06,  ..., -5.0366e-06,
         -3.8445e-06, -3.9488e-06],
        [-9.0599e-06, -6.5863e-06,  4.9472e-06,  ..., -7.7188e-06,
         -6.0499e-06, -5.4240e-06]], device='cuda:0')
Loss: 1.0116113424301147


Running epoch 0, step 946, batch 946
Sampled inputs[:2]: tensor([[   0,  298, 2230,  ..., 2300, 3698, 4764],
        [   0,   24,   15,  ...,  221,  380,  417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6655e-04,  4.0629e-04,  2.3883e-04,  ..., -2.1599e-04,
         -2.7053e-04, -1.9438e-04],
        [-5.6103e-06, -3.6880e-06,  2.6301e-06,  ..., -4.7684e-06,
         -3.6582e-06, -3.9786e-06],
        [-1.1131e-05, -7.5549e-06,  5.6028e-06,  ..., -9.3579e-06,
         -7.1675e-06, -7.7337e-06],
        [-8.7917e-06, -5.7146e-06,  4.3437e-06,  ..., -7.4357e-06,
         -5.7295e-06, -6.3628e-06],
        [-1.4096e-05, -9.8348e-06,  7.1824e-06,  ..., -1.1981e-05,
         -9.4324e-06, -9.3281e-06]], device='cuda:0')
Loss: 0.9522446990013123


Running epoch 0, step 947, batch 947
Sampled inputs[:2]: tensor([[    0,  1716,  1773,  ...,  5014,    12,   847],
        [    0,   685,  3482,  ..., 23113,    12,  6481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2930e-04,  4.0035e-04,  2.8141e-04,  ..., -2.7744e-04,
         -2.8977e-04, -4.0655e-04],
        [-7.5027e-06, -4.9695e-06,  3.5986e-06,  ..., -6.4075e-06,
         -4.8950e-06, -5.1484e-06],
        [-1.4693e-05, -1.0043e-05,  7.5251e-06,  ..., -1.2413e-05,
         -9.4622e-06, -9.8795e-06],
        [-1.1802e-05, -7.7114e-06,  5.9381e-06,  ..., -1.0043e-05,
         -7.6964e-06, -8.2850e-06],
        [-1.8626e-05, -1.3113e-05,  9.6709e-06,  ..., -1.5914e-05,
         -1.2487e-05, -1.1906e-05]], device='cuda:0')
Loss: 1.009064793586731


Running epoch 0, step 948, batch 948
Sampled inputs[:2]: tensor([[   0,   18,   14,  ...,  446,  747, 1193],
        [   0,  472,  346,  ..., 9161,  300, 4460]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3279e-04,  8.5623e-04,  2.6823e-04,  ..., -1.9320e-04,
         -7.3843e-04, -7.7939e-04],
        [-9.3505e-06, -6.1914e-06,  4.5896e-06,  ..., -7.9349e-06,
         -5.9903e-06, -6.2734e-06],
        [-1.8269e-05, -1.2487e-05,  9.5665e-06,  ..., -1.5348e-05,
         -1.1548e-05, -1.2025e-05],
        [-1.4871e-05, -9.6783e-06,  7.6741e-06,  ..., -1.2591e-05,
         -9.5144e-06, -1.0237e-05],
        [-2.3127e-05, -1.6257e-05,  1.2234e-05,  ..., -1.9625e-05,
         -1.5214e-05, -1.4439e-05]], device='cuda:0')
Loss: 0.9842077493667603


Running epoch 0, step 949, batch 949
Sampled inputs[:2]: tensor([[    0,  1624,  7437,  ...,    12, 16369,  5153],
        [    0,    12, 17340,  ...,   408,  1550,  2415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.1037e-04,  8.4814e-04,  1.2853e-04,  ..., -1.3846e-04,
         -8.9468e-04, -1.1285e-03],
        [-1.1079e-05, -7.4804e-06,  5.5432e-06,  ..., -9.4548e-06,
         -7.1451e-06, -7.3388e-06],
        [-2.1607e-05, -1.5035e-05,  1.1533e-05,  ..., -1.8254e-05,
         -1.3724e-05, -1.4037e-05],
        [-1.7703e-05, -1.1720e-05,  9.3654e-06,  ..., -1.5095e-05,
         -1.1422e-05, -1.2092e-05],
        [-2.7329e-05, -1.9550e-05,  1.4707e-05,  ..., -2.3291e-05,
         -1.8060e-05, -1.6794e-05]], device='cuda:0')
Loss: 0.9778054356575012


Running epoch 0, step 950, batch 950
Sampled inputs[:2]: tensor([[    0,    14,   381,  ...,  2195,   278,   266],
        [    0,  5129,  1245,  ...,   292, 24298,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9244e-04,  9.8728e-04,  4.1105e-05,  ..., -1.2042e-04,
         -1.0745e-03, -1.3046e-03],
        [-1.2800e-05, -8.7097e-06,  6.4597e-06,  ..., -1.0945e-05,
         -8.2701e-06, -8.4415e-06],
        [-2.5019e-05, -1.7509e-05,  1.3456e-05,  ..., -2.1160e-05,
         -1.5870e-05, -1.6168e-05],
        [-2.0549e-05, -1.3672e-05,  1.0945e-05,  ..., -1.7539e-05,
         -1.3240e-05, -1.3992e-05],
        [-3.1561e-05, -2.2694e-05,  1.7107e-05,  ..., -2.6926e-05,
         -2.0832e-05, -1.9252e-05]], device='cuda:0')
Loss: 1.0100773572921753


Running epoch 0, step 951, batch 951
Sampled inputs[:2]: tensor([[    0,   726,  8241,  ...,   266,  5994,     9],
        [    0,   365,   984,  ..., 18562,  4237, 31813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4955e-04,  1.0791e-03,  1.2268e-04,  ..., -1.5224e-04,
         -1.0696e-03, -1.2995e-03],
        [-1.4693e-05, -1.0066e-05,  7.4431e-06,  ..., -1.2524e-05,
         -9.4324e-06, -9.5963e-06],
        [ 2.2883e-04,  1.8396e-04, -1.3935e-04,  ...,  1.8940e-04,
          2.3410e-04,  1.1812e-05],
        [-2.3603e-05, -1.5847e-05,  1.2577e-05,  ..., -2.0102e-05,
         -1.5125e-05, -1.5914e-05],
        [-3.6061e-05, -2.6107e-05,  1.9550e-05,  ..., -3.0726e-05,
         -2.3723e-05, -2.1860e-05]], device='cuda:0')
Loss: 1.0165787935256958
Graident accumulation at epoch 0, step 951, batch 951
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0022,  0.0232, -0.0193],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0012],
        [-0.0158,  0.0154, -0.0284,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2604e-04,  1.8556e-04, -1.8129e-04,  ...,  3.5289e-05,
         -9.6978e-05, -9.3586e-05],
        [-1.2752e-05, -7.7914e-06,  6.3018e-06,  ..., -1.0292e-05,
         -5.9259e-06, -7.5769e-06],
        [ 3.3651e-05,  3.0697e-05, -1.9822e-05,  ...,  2.4858e-05,
          3.7528e-05, -2.7811e-06],
        [-1.8117e-05, -1.2194e-05,  1.1127e-05,  ..., -1.4419e-05,
         -1.1359e-05, -1.3807e-05],
        [-3.9573e-05, -2.9135e-05,  2.3051e-05,  ..., -3.1725e-05,
         -2.4443e-05, -2.3890e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7793e-08, 4.6121e-08, 4.7473e-08,  ..., 1.6209e-08, 1.1464e-07,
         3.3081e-08],
        [7.7631e-11, 4.2456e-11, 1.2781e-11,  ..., 5.1237e-11, 1.4475e-11,
         2.0953e-11],
        [1.4506e-09, 7.8423e-10, 2.0180e-10,  ..., 1.1055e-09, 2.4494e-10,
         3.3774e-10],
        [4.2065e-10, 2.9292e-10, 1.3927e-10,  ..., 3.8125e-10, 1.3478e-10,
         1.5431e-10],
        [3.5424e-10, 1.8873e-10, 4.6190e-11,  ..., 2.5851e-10, 4.6365e-11,
         9.7689e-11]], device='cuda:0')
optimizer state dict: 119.0
lr: [1.1956510313742102e-05, 1.1956510313742102e-05]
scheduler_last_epoch: 119


Running epoch 0, step 952, batch 952
Sampled inputs[:2]: tensor([[   0,   12,  328,  ...,  578,   19,   40],
        [   0,  266,  298,  ...,  654,  271, 4483]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8850e-05, -7.3313e-06, -4.6998e-05,  ..., -1.2523e-04,
         -7.7434e-05,  2.7297e-05],
        [-1.8179e-06, -1.3188e-06,  9.5367e-07,  ..., -1.6242e-06,
         -1.3411e-06, -1.1846e-06],
        [-3.6359e-06, -2.7120e-06,  1.9968e-06,  ..., -3.2336e-06,
         -2.6375e-06, -2.3246e-06],
        [-2.9653e-06, -2.1160e-06,  1.5944e-06,  ..., -2.6673e-06,
         -2.2054e-06, -2.0117e-06],
        [-4.7982e-06, -3.6657e-06,  2.6673e-06,  ..., -4.2915e-06,
         -3.5763e-06, -2.9206e-06]], device='cuda:0')
Loss: 1.0265440940856934


Running epoch 0, step 953, batch 953
Sampled inputs[:2]: tensor([[   0,  677, 6499,  ..., 2738,   12,  287],
        [   0, 9577, 2789,  ..., 1042, 9086,  623]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8859e-05, -6.5535e-05, -3.5333e-05,  ..., -1.2492e-04,
         -1.5051e-04,  5.8157e-05],
        [-3.5986e-06, -2.6152e-06,  1.9595e-06,  ..., -3.1665e-06,
         -2.5183e-06, -2.2575e-06],
        [ 3.4050e-04,  3.2407e-04, -1.7302e-04,  ...,  2.5662e-04,
          2.6926e-04,  1.5485e-04],
        [-5.9903e-06, -4.2915e-06,  3.3602e-06,  ..., -5.2899e-06,
         -4.1872e-06, -3.9116e-06],
        [-9.2685e-06, -7.1079e-06,  5.3048e-06,  ..., -8.1956e-06,
         -6.6310e-06, -5.4687e-06]], device='cuda:0')
Loss: 1.0228368043899536


Running epoch 0, step 954, batch 954
Sampled inputs[:2]: tensor([[    0,     8,    39,  ...,  7406,    13, 10896],
        [    0,  6673,   298,  ...,  4391,   292,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6810e-05, -7.9745e-05,  1.5872e-05,  ..., -1.8475e-04,
         -2.6803e-04,  2.8992e-05],
        [-5.3421e-06, -3.8147e-06,  2.9132e-06,  ..., -4.7386e-06,
         -3.7327e-06, -3.4496e-06],
        [ 3.3699e-04,  3.2160e-04, -1.7098e-04,  ...,  2.5353e-04,
          2.6690e-04,  1.5253e-04],
        [-8.8960e-06, -6.2287e-06,  5.0142e-06,  ..., -7.8976e-06,
         -6.2138e-06, -5.9530e-06],
        [-1.3679e-05, -1.0297e-05,  7.8976e-06,  ..., -1.2100e-05,
         -9.7007e-06, -8.2403e-06]], device='cuda:0')
Loss: 0.9991045594215393


Running epoch 0, step 955, batch 955
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  278, 2354,  ..., 4974, 7757,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0388e-05, -4.7320e-05, -1.1858e-04,  ..., -1.0211e-04,
         -3.6267e-04, -5.5383e-05],
        [-6.9439e-06, -5.0515e-06,  3.8408e-06,  ..., -6.1914e-06,
         -4.8503e-06, -4.4480e-06],
        [ 3.3379e-04,  3.1911e-04, -1.6903e-04,  ...,  2.5067e-04,
          2.6472e-04,  1.5056e-04],
        [-1.1593e-05, -8.2403e-06,  6.6608e-06,  ..., -1.0341e-05,
         -8.0913e-06, -7.7263e-06],
        [-1.7732e-05, -1.3530e-05,  1.0371e-05,  ..., -1.5751e-05,
         -1.2562e-05, -1.0550e-05]], device='cuda:0')
Loss: 1.0015605688095093


Running epoch 0, step 956, batch 956
Sampled inputs[:2]: tensor([[    0,  3179,   221,  ...,   910,   706,  1102],
        [    0, 37312,    12,  ...,   278,   795, 40854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3032e-05, -7.5971e-05, -1.8401e-04,  ..., -4.6909e-05,
         -3.7637e-04,  7.2842e-06],
        [-8.6799e-06, -6.1542e-06,  4.7721e-06,  ..., -7.7188e-06,
         -6.0499e-06, -5.7146e-06],
        [ 3.3030e-04,  3.1683e-04, -1.6705e-04,  ...,  2.4764e-04,
          2.6238e-04,  1.4809e-04],
        [-1.4454e-05, -1.0006e-05,  8.2552e-06,  ..., -1.2845e-05,
         -1.0043e-05, -9.8571e-06],
        [-2.2173e-05, -1.6525e-05,  1.2919e-05,  ..., -1.9595e-05,
         -1.5646e-05, -1.3486e-05]], device='cuda:0')
Loss: 1.0022258758544922


Running epoch 0, step 957, batch 957
Sampled inputs[:2]: tensor([[   0,  265, 1781,  ...,  334,  344,  984],
        [   0, 2805,  391,  ...,   12,  259, 1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6375e-06,  6.9494e-05, -2.5215e-04,  ..., -1.0261e-04,
         -4.9858e-04,  2.7762e-05],
        [-1.0476e-05, -7.3016e-06,  5.6252e-06,  ..., -9.3058e-06,
         -7.2569e-06, -7.0706e-06],
        [ 3.2665e-04,  3.1441e-04, -1.6520e-04,  ...,  2.4444e-04,
          2.5997e-04,  1.4539e-04],
        [-1.7241e-05, -1.1779e-05,  9.6485e-06,  ..., -1.5303e-05,
         -1.1906e-05, -1.1988e-05],
        [-2.6822e-05, -1.9670e-05,  1.5318e-05,  ..., -2.3678e-05,
         -1.8835e-05, -1.6779e-05]], device='cuda:0')
Loss: 1.0162036418914795


Running epoch 0, step 958, batch 958
Sampled inputs[:2]: tensor([[   0,   27, 3961,  ...,  462,  221,  474],
        [   0,  726, 3979,  ...,   27, 2085,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1418e-05, -6.9604e-06, -2.2966e-04,  ..., -1.9158e-04,
         -4.2593e-04,  1.3882e-04],
        [-1.2226e-05, -8.5980e-06,  6.5640e-06,  ..., -1.0841e-05,
         -8.4788e-06, -8.2031e-06],
        [ 3.2323e-04,  3.1181e-04, -1.6326e-04,  ...,  2.4145e-04,
          2.5760e-04,  1.4317e-04],
        [-2.0131e-05, -1.3895e-05,  1.1258e-05,  ..., -1.7837e-05,
         -1.3903e-05, -1.3940e-05],
        [-3.1143e-05, -2.3052e-05,  1.7762e-05,  ..., -2.7522e-05,
         -2.1949e-05, -1.9446e-05]], device='cuda:0')
Loss: 1.0112226009368896


Running epoch 0, step 959, batch 959
Sampled inputs[:2]: tensor([[   0,  965,  300,  ...,  546,  857, 4350],
        [   0,  352,  927,  ..., 1521, 3513,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4002e-05,  3.2116e-05, -2.2742e-04,  ..., -1.5257e-04,
         -3.3622e-04,  1.4541e-04],
        [-1.3918e-05, -9.7528e-06,  7.5251e-06,  ..., -1.2338e-05,
         -9.5963e-06, -9.2760e-06],
        [ 3.1983e-04,  3.0941e-04, -1.6124e-04,  ...,  2.3845e-04,
          2.5538e-04,  1.4103e-04],
        [-2.3022e-05, -1.5818e-05,  1.2994e-05,  ..., -2.0400e-05,
         -1.5795e-05, -1.5862e-05],
        [-3.5316e-05, -2.6092e-05,  2.0266e-05,  ..., -3.1248e-05,
         -2.4810e-05, -2.1905e-05]], device='cuda:0')
Loss: 1.010492205619812
Graident accumulation at epoch 0, step 959, batch 959
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0058, -0.0143,  0.0027,  ..., -0.0022,  0.0232, -0.0192],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0402,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0158,  0.0154, -0.0284,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0904e-04,  1.7021e-04, -1.8591e-04,  ...,  1.6503e-05,
         -1.2090e-04, -6.9686e-05],
        [-1.2869e-05, -7.9876e-06,  6.4241e-06,  ..., -1.0496e-05,
         -6.2929e-06, -7.7468e-06],
        [ 6.2269e-05,  5.8568e-05, -3.3964e-05,  ...,  4.6217e-05,
          5.9313e-05,  1.1600e-05],
        [-1.8607e-05, -1.2556e-05,  1.1313e-05,  ..., -1.5017e-05,
         -1.1803e-05, -1.4012e-05],
        [-3.9147e-05, -2.8831e-05,  2.2773e-05,  ..., -3.1677e-05,
         -2.4479e-05, -2.3692e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7737e-08, 4.6076e-08, 4.7477e-08,  ..., 1.6216e-08, 1.1463e-07,
         3.3069e-08],
        [7.7747e-11, 4.2509e-11, 1.2825e-11,  ..., 5.1338e-11, 1.4552e-11,
         2.1018e-11],
        [1.5515e-09, 8.7918e-10, 2.2760e-10,  ..., 1.1613e-09, 3.0992e-10,
         3.5729e-10],
        [4.2076e-10, 2.9288e-10, 1.3930e-10,  ..., 3.8129e-10, 1.3489e-10,
         1.5441e-10],
        [3.5513e-10, 1.8922e-10, 4.6555e-11,  ..., 2.5923e-10, 4.6934e-11,
         9.8071e-11]], device='cuda:0')
optimizer state dict: 120.0
lr: [1.1835136366674677e-05, 1.1835136366674677e-05]
scheduler_last_epoch: 120
Epoch 0 | Batch 959/1048 | Training PPL: 3241.973878218276 | time 67.61367726325989
Saving checkpoint at epoch 0, step 959, batch 959
Epoch 0 | Validation PPL: 7.376877026058996 | Learning rate: 1.1835136366674677e-05
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.0_959, AFTER epoch 0, step 959


Running epoch 0, step 960, batch 960
Sampled inputs[:2]: tensor([[    0,  1238,    14,  ...,   368,   940,   437],
        [    0,  3217, 16714,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5241e-05,  3.1947e-06,  1.2840e-04,  ...,  1.4326e-04,
         -1.2855e-04,  3.3245e-05],
        [-1.5646e-06, -1.1399e-06,  9.5367e-07,  ..., -1.4380e-06,
         -1.0654e-06, -1.0058e-06],
        [-3.2336e-06, -2.4140e-06,  2.0564e-06,  ..., -2.9504e-06,
         -2.1458e-06, -2.0862e-06],
        [-2.6822e-06, -1.9222e-06,  1.6913e-06,  ..., -2.4736e-06,
         -1.8030e-06, -1.8030e-06],
        [-3.9041e-06, -2.9951e-06,  2.4736e-06,  ..., -3.6061e-06,
         -2.7418e-06, -2.3395e-06]], device='cuda:0')
Loss: 1.0134336948394775


Running epoch 0, step 961, batch 961
Sampled inputs[:2]: tensor([[    0,   806,   352,  ...,  3493,   352, 49256],
        [    0,  1985,   278,  ...,   677, 12292, 17956]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8952e-05, -5.6048e-05,  6.6135e-05,  ...,  1.0944e-04,
         -2.6956e-04, -2.0483e-04],
        [-3.1590e-06, -2.2650e-06,  1.9372e-06,  ..., -2.9653e-06,
         -2.2948e-06, -2.0340e-06],
        [-6.3777e-06, -4.6790e-06,  4.0978e-06,  ..., -5.9009e-06,
         -4.5002e-06, -4.0680e-06],
        [-5.3644e-06, -3.7476e-06,  3.4273e-06,  ..., -5.0366e-06,
         -3.8743e-06, -3.6135e-06],
        [-8.0466e-06, -6.0648e-06,  5.1707e-06,  ..., -7.5102e-06,
         -5.9605e-06, -4.7684e-06]], device='cuda:0')
Loss: 1.0276503562927246


Running epoch 0, step 962, batch 962
Sampled inputs[:2]: tensor([[    0, 10064,   768,  ...,   266,  2816,   278],
        [    0,  3087,   401,  ...,  1875,  4122,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1329e-05, -2.4152e-04, -5.1158e-05,  ...,  1.8103e-04,
         -2.5616e-04, -3.2492e-04],
        [-4.6864e-06, -3.4422e-06,  2.8498e-06,  ..., -4.4107e-06,
         -3.4273e-06, -3.0100e-06],
        [-9.5516e-06, -7.1228e-06,  6.1095e-06,  ..., -8.8215e-06,
         -6.7353e-06, -6.0350e-06],
        [-8.0764e-06, -5.7742e-06,  5.1036e-06,  ..., -7.5847e-06,
         -5.8413e-06, -5.4166e-06],
        [-1.1981e-05, -9.1642e-06,  7.7039e-06,  ..., -1.1146e-05,
         -8.8513e-06, -7.0333e-06]], device='cuda:0')
Loss: 1.0380126237869263


Running epoch 0, step 963, batch 963
Sampled inputs[:2]: tensor([[   0,  504,  409,  ..., 5863, 2621,  824],
        [   0,   17, 3978,  ..., 3988,  598,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4442e-04, -1.2871e-04, -1.2963e-04,  ...,  1.9545e-04,
         -4.7184e-04, -5.3173e-04],
        [-6.3553e-06, -4.6864e-06,  3.8110e-06,  ..., -5.9828e-06,
         -4.6268e-06, -4.1053e-06],
        [-1.2934e-05, -9.7007e-06,  8.1658e-06,  ..., -1.1981e-05,
         -9.1046e-06, -8.2403e-06],
        [-1.0863e-05, -7.8157e-06,  6.7577e-06,  ..., -1.0222e-05,
         -7.8231e-06, -7.3239e-06],
        [-1.6242e-05, -1.2502e-05,  1.0297e-05,  ..., -1.5169e-05,
         -1.1966e-05, -9.6411e-06]], device='cuda:0')
Loss: 1.0077773332595825


Running epoch 0, step 964, batch 964
Sampled inputs[:2]: tensor([[   0,  461,  654,  ..., 6548, 7171,   14],
        [   0, 2380, 2667,  ...,   14,  381, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1066e-04, -2.2932e-04, -2.3848e-04,  ...,  2.0105e-04,
         -5.0702e-04, -6.1043e-04],
        [-7.9796e-06, -5.9679e-06,  4.8466e-06,  ..., -7.4729e-06,
         -5.7891e-06, -5.0366e-06],
        [-1.6242e-05, -1.2353e-05,  1.0356e-05,  ..., -1.5005e-05,
         -1.1429e-05, -1.0148e-05],
        [-1.3679e-05, -1.0006e-05,  8.6129e-06,  ..., -1.2800e-05,
         -9.8199e-06, -9.0078e-06],
        [-2.0385e-05, -1.5929e-05,  1.3024e-05,  ..., -1.9014e-05,
         -1.5020e-05, -1.1891e-05]], device='cuda:0')
Loss: 1.0576667785644531


Running epoch 0, step 965, batch 965
Sampled inputs[:2]: tensor([[    0,  1040,   287,  ...,    14, 10209,    12],
        [    0,    12,  1250,  ...,   381,  1524,  2204]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3371e-04, -1.5303e-04, -1.1188e-04,  ...,  2.6463e-04,
         -5.7869e-04, -4.6893e-04],
        [-9.7454e-06, -7.3463e-06,  5.8375e-06,  ..., -9.0599e-06,
         -7.0408e-06, -6.1318e-06],
        [-1.9759e-05, -1.5229e-05,  1.2442e-05,  ..., -1.8194e-05,
         -1.3977e-05, -1.2353e-05],
        [-1.6540e-05, -1.2256e-05,  1.0289e-05,  ..., -1.5408e-05,
         -1.1891e-05, -1.0878e-05],
        [-2.4796e-05, -1.9640e-05,  1.5646e-05,  ..., -2.3067e-05,
         -1.8373e-05, -1.4499e-05]], device='cuda:0')
Loss: 1.0154060125350952


Running epoch 0, step 966, batch 966
Sampled inputs[:2]: tensor([[   0,  432,  984,  ...,  287,  496,   14],
        [   0, 1217,    9,  ..., 1821,    5,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5579e-04, -1.4348e-04, -2.2858e-04,  ...,  1.1504e-04,
         -6.3068e-04, -3.6422e-04],
        [-1.1384e-05, -8.4713e-06,  6.7092e-06,  ..., -1.0684e-05,
         -8.3372e-06, -7.3090e-06],
        [-2.3007e-05, -1.7524e-05,  1.4320e-05,  ..., -2.1324e-05,
         -1.6436e-05, -1.4618e-05],
        [-1.9252e-05, -1.4044e-05,  1.1794e-05,  ..., -1.8090e-05,
         -1.4037e-05, -1.2919e-05],
        [-2.9176e-05, -2.2829e-05,  1.8194e-05,  ..., -2.7299e-05,
         -2.1800e-05, -1.7345e-05]], device='cuda:0')
Loss: 1.043575406074524


Running epoch 0, step 967, batch 967
Sampled inputs[:2]: tensor([[    0,  2352,  4275,  ..., 10518,   342,   266],
        [    0,  2923,   266,  ...,  7763,   360,  1255]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9230e-04, -1.1948e-04, -1.6045e-04,  ...,  1.3887e-04,
         -6.0757e-04, -4.7841e-04],
        [-1.3024e-05, -9.7603e-06,  7.5921e-06,  ..., -1.2212e-05,
         -9.4995e-06, -8.3670e-06],
        [-2.6345e-05, -2.0206e-05,  1.6242e-05,  ..., -2.4408e-05,
         -1.8761e-05, -1.6749e-05],
        [-2.2084e-05, -1.6250e-05,  1.3374e-05,  ..., -2.0742e-05,
         -1.6049e-05, -1.4827e-05],
        [-3.3289e-05, -2.6226e-05,  2.0593e-05,  ..., -3.1143e-05,
         -2.4796e-05, -1.9804e-05]], device='cuda:0')
Loss: 1.0258746147155762
Graident accumulation at epoch 0, step 967, batch 967
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0232, -0.0192],
        [ 0.0288, -0.0080,  0.0035,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0341, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3736e-04,  1.4124e-04, -1.8336e-04,  ...,  2.8739e-05,
         -1.6957e-04, -1.1056e-04],
        [-1.2884e-05, -8.1648e-06,  6.5409e-06,  ..., -1.0668e-05,
         -6.6136e-06, -7.8088e-06],
        [ 5.3408e-05,  5.0691e-05, -2.8943e-05,  ...,  3.9154e-05,
          5.1506e-05,  8.7647e-06],
        [-1.8955e-05, -1.2925e-05,  1.1519e-05,  ..., -1.5590e-05,
         -1.2227e-05, -1.4094e-05],
        [-3.8561e-05, -2.8570e-05,  2.2555e-05,  ..., -3.1624e-05,
         -2.4511e-05, -2.3303e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7833e-08, 4.6044e-08, 4.7455e-08,  ..., 1.6219e-08, 1.1489e-07,
         3.3265e-08],
        [7.7839e-11, 4.2562e-11, 1.2870e-11,  ..., 5.1436e-11, 1.4628e-11,
         2.1067e-11],
        [1.5506e-09, 8.7871e-10, 2.2764e-10,  ..., 1.1607e-09, 3.0996e-10,
         3.5721e-10],
        [4.2082e-10, 2.9285e-10, 1.3934e-10,  ..., 3.8134e-10, 1.3502e-10,
         1.5447e-10],
        [3.5588e-10, 1.8972e-10, 4.6932e-11,  ..., 2.5994e-10, 4.7502e-11,
         9.8365e-11]], device='cuda:0')
optimizer state dict: 121.0
lr: [1.1713481994751294e-05, 1.1713481994751294e-05]
scheduler_last_epoch: 121


Running epoch 0, step 968, batch 968
Sampled inputs[:2]: tensor([[    0, 48007,   417,  ...,   944,   278,  2903],
        [    0,    19,    14,  ...,    13,  6673,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7515e-05, -1.0784e-05, -1.7298e-05,  ...,  2.1374e-04,
         -2.0653e-04,  1.2401e-04],
        [-1.6317e-06, -1.0282e-06,  7.8604e-07,  ..., -1.4827e-06,
         -1.1399e-06, -1.4305e-06],
        [-3.5465e-06, -2.2948e-06,  1.8924e-06,  ..., -3.1143e-06,
         -2.3544e-06, -2.9653e-06],
        [-2.7716e-06, -1.6838e-06,  1.4231e-06,  ..., -2.4736e-06,
         -1.8850e-06, -2.4438e-06],
        [-4.4405e-06, -2.9206e-06,  2.4289e-06,  ..., -3.9041e-06,
         -3.0249e-06, -3.4422e-06]], device='cuda:0')
Loss: 0.9713314771652222


Running epoch 0, step 969, batch 969
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    14, 10961,    12],
        [    0,  3908,  4274,  ...,   298,  7998, 11109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1620e-06,  2.6986e-04, -7.1583e-05,  ...,  3.2575e-04,
         -1.9143e-04,  2.3222e-04],
        [-3.3379e-06, -2.1309e-06,  1.7174e-06,  ..., -2.9504e-06,
         -2.2873e-06, -2.7493e-06],
        [-7.1824e-06, -4.7386e-06,  3.9935e-06,  ..., -6.2138e-06,
         -4.7684e-06, -5.7518e-06],
        [-5.5879e-06, -3.4869e-06,  3.0324e-06,  ..., -4.9025e-06,
         -3.7923e-06, -4.7088e-06],
        [-8.9109e-06, -6.0499e-06,  5.0217e-06,  ..., -7.7784e-06,
         -6.1393e-06, -6.7055e-06]], device='cuda:0')
Loss: 0.9840599894523621


Running epoch 0, step 970, batch 970
Sampled inputs[:2]: tensor([[   0, 3235,  471,  ..., 1967, 4273, 2738],
        [   0, 4665,  909,  ..., 3607,  259, 1108]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0613e-05,  1.9375e-04, -1.4139e-04,  ...,  3.7589e-04,
         -1.2486e-04,  2.3082e-04],
        [-5.0142e-06, -3.3975e-06,  2.6785e-06,  ..., -4.5002e-06,
         -3.5539e-06, -3.8594e-06],
        [-1.0520e-05, -7.3463e-06,  6.0350e-06,  ..., -9.2834e-06,
         -7.2718e-06, -7.9572e-06],
        [-8.4043e-06, -5.6028e-06,  4.7088e-06,  ..., -7.5251e-06,
         -5.9530e-06, -6.6608e-06],
        [-1.3173e-05, -9.4622e-06,  7.6443e-06,  ..., -1.1742e-05,
         -9.4473e-06, -9.3281e-06]], device='cuda:0')
Loss: 1.0179600715637207


Running epoch 0, step 971, batch 971
Sampled inputs[:2]: tensor([[    0,   380,  6119,  ..., 11823,   287,  6797],
        [    0,   298, 22296,  ...,   287,  6494,   644]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2515e-04,  1.3137e-04,  9.0549e-05,  ...,  3.0068e-04,
          1.1487e-05,  2.8128e-04],
        [-6.6161e-06, -4.5151e-06,  3.5614e-06,  ..., -6.0052e-06,
         -4.7386e-06, -5.0962e-06],
        [-1.3843e-05, -9.7603e-06,  8.0019e-06,  ..., -1.2353e-05,
         -9.6709e-06, -1.0476e-05],
        [-1.0982e-05, -7.3612e-06,  6.2436e-06,  ..., -9.9689e-06,
         -7.8902e-06, -8.7321e-06],
        [-1.7554e-05, -1.2740e-05,  1.0237e-05,  ..., -1.5825e-05,
         -1.2726e-05, -1.2428e-05]], device='cuda:0')
Loss: 0.989695131778717


Running epoch 0, step 972, batch 972
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,  3529,   271,  ...,  1553,   365,  2714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6972e-04,  1.1036e-04,  7.3690e-05,  ...,  2.2188e-04,
          1.0633e-04,  3.1916e-04],
        [-8.2552e-06, -5.7146e-06,  4.5598e-06,  ..., -7.5474e-06,
         -5.9158e-06, -6.1542e-06],
        [-1.7136e-05, -1.2279e-05,  1.0133e-05,  ..., -1.5467e-05,
         -1.2085e-05, -1.2591e-05],
        [-1.3754e-05, -9.3728e-06,  8.0094e-06,  ..., -1.2591e-05,
         -9.9167e-06, -1.0587e-05],
        [-2.1666e-05, -1.6004e-05,  1.2890e-05,  ..., -1.9759e-05,
         -1.5870e-05, -1.4916e-05]], device='cuda:0')
Loss: 1.022412657737732


Running epoch 0, step 973, batch 973
Sampled inputs[:2]: tensor([[    0, 25009,   407,  ..., 13076,    13,  5226],
        [    0,  1550,  2013,  ...,  9970,   638,  6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6706e-04,  2.8405e-05,  6.7611e-05,  ...,  1.3638e-04,
          1.2074e-04,  4.2733e-04],
        [-9.8124e-06, -6.8173e-06,  5.4948e-06,  ..., -8.9779e-06,
         -7.0184e-06, -7.3090e-06],
        [-2.0355e-05, -1.4603e-05,  1.2189e-05,  ..., -1.8373e-05,
         -1.4275e-05, -1.4946e-05],
        [-1.6466e-05, -1.1221e-05,  9.7230e-06,  ..., -1.5065e-05,
         -1.1802e-05, -1.2688e-05],
        [-2.5719e-05, -1.9029e-05,  1.5497e-05,  ..., -2.3454e-05,
         -1.8761e-05, -1.7658e-05]], device='cuda:0')
Loss: 0.9911783933639526


Running epoch 0, step 974, batch 974
Sampled inputs[:2]: tensor([[   0, 3380, 1197,  ...,  631,  369, 3123],
        [   0,  546,  360,  ..., 9107, 2772, 4496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3844e-04, -2.2763e-05,  1.0586e-07,  ...,  9.2791e-05,
          1.2074e-04,  3.0690e-04],
        [-1.1496e-05, -8.1286e-06,  6.5528e-06,  ..., -1.0483e-05,
         -8.1211e-06, -8.2999e-06],
        [-2.3797e-05, -1.7330e-05,  1.4454e-05,  ..., -2.1458e-05,
         -1.6510e-05, -1.7002e-05],
        [-1.9312e-05, -1.3426e-05,  1.1586e-05,  ..., -1.7613e-05,
         -1.3649e-05, -1.4439e-05],
        [-2.9892e-05, -2.2471e-05,  1.8254e-05,  ..., -2.7269e-05,
         -2.1666e-05, -1.9982e-05]], device='cuda:0')
Loss: 1.0141119956970215


Running epoch 0, step 975, batch 975
Sampled inputs[:2]: tensor([[   0, 7036,  278,  ...,  221,  290,  446],
        [   0,   27,  417,  ...,   18,  365,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3843e-04, -4.8626e-05, -9.7022e-05,  ...,  3.4200e-05,
         -6.9050e-06,  3.9714e-04],
        [-1.3009e-05, -9.3803e-06,  7.4245e-06,  ..., -1.1966e-05,
         -9.3132e-06, -9.3952e-06],
        [-2.7090e-05, -2.0102e-05,  1.6466e-05,  ..., -2.4632e-05,
         -1.9014e-05, -1.9357e-05],
        [-2.1994e-05, -1.5602e-05,  1.3195e-05,  ..., -2.0236e-05,
         -1.5736e-05, -1.6466e-05],
        [-3.4094e-05, -2.6107e-05,  2.0832e-05,  ..., -3.1352e-05,
         -2.5004e-05, -2.2799e-05]], device='cuda:0')
Loss: 1.02171790599823
Graident accumulation at epoch 0, step 975, batch 975
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0289, -0.0147, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0978e-04,  1.2226e-04, -1.7473e-04,  ...,  2.9285e-05,
         -1.5330e-04, -5.9789e-05],
        [-1.2897e-05, -8.2864e-06,  6.6293e-06,  ..., -1.0798e-05,
         -6.8835e-06, -7.9675e-06],
        [ 4.5358e-05,  4.3611e-05, -2.4402e-05,  ...,  3.2776e-05,
          4.4454e-05,  5.9526e-06],
        [-1.9259e-05, -1.3193e-05,  1.1687e-05,  ..., -1.6055e-05,
         -1.2578e-05, -1.4331e-05],
        [-3.8115e-05, -2.8324e-05,  2.2382e-05,  ..., -3.1597e-05,
         -2.4560e-05, -2.3253e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7795e-08, 4.6001e-08, 4.7417e-08,  ..., 1.6204e-08, 1.1477e-07,
         3.3390e-08],
        [7.7930e-11, 4.2607e-11, 1.2912e-11,  ..., 5.1527e-11, 1.4700e-11,
         2.1134e-11],
        [1.5498e-09, 8.7824e-10, 2.2768e-10,  ..., 1.1602e-09, 3.1001e-10,
         3.5723e-10],
        [4.2089e-10, 2.9280e-10, 1.3937e-10,  ..., 3.8137e-10, 1.3513e-10,
         1.5459e-10],
        [3.5669e-10, 1.9021e-10, 4.7319e-11,  ..., 2.6066e-10, 4.8079e-11,
         9.8786e-11]], device='cuda:0')
optimizer state dict: 122.0
lr: [1.1591565787821919e-05, 1.1591565787821919e-05]
scheduler_last_epoch: 122


Running epoch 0, step 976, batch 976
Sampled inputs[:2]: tensor([[   0,  266,  452,  ..., 1725, 2200,  342],
        [   0,  271,  266,  ..., 5933,   35, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0403e-05, -1.0308e-04, -1.4090e-04,  ..., -5.8108e-05,
         -1.1890e-04, -2.3963e-04],
        [-1.4976e-06, -1.1176e-06,  1.0133e-06,  ..., -1.3858e-06,
         -9.9838e-07, -9.6112e-07],
        [-3.0994e-06, -2.3693e-06,  2.2203e-06,  ..., -2.8610e-06,
         -2.0266e-06, -1.9819e-06],
        [-2.7120e-06, -1.9670e-06,  1.9222e-06,  ..., -2.5183e-06,
         -1.7956e-06, -1.8403e-06],
        [-3.7700e-06, -3.0249e-06,  2.6971e-06,  ..., -3.5316e-06,
         -2.6375e-06, -2.2352e-06]], device='cuda:0')
Loss: 1.0163410902023315


Running epoch 0, step 977, batch 977
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,  1597,   278,  ...,    20,    38,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2828e-05, -2.2642e-05, -1.4474e-04,  ..., -3.0546e-06,
          6.8148e-05,  5.8055e-05],
        [-3.0100e-06, -2.3097e-06,  1.9073e-06,  ..., -2.8759e-06,
         -2.2352e-06, -2.1458e-06],
        [-6.2734e-06, -4.9621e-06,  4.2021e-06,  ..., -5.9456e-06,
         -4.5598e-06, -4.4554e-06],
        [-5.3048e-06, -3.9935e-06,  3.5241e-06,  ..., -5.0813e-06,
         -3.9265e-06, -3.9712e-06],
        [-7.8231e-06, -6.4522e-06,  5.2452e-06,  ..., -7.5251e-06,
         -6.0052e-06, -5.2005e-06]], device='cuda:0')
Loss: 0.9986265897750854


Running epoch 0, step 978, batch 978
Sampled inputs[:2]: tensor([[   0, 5489,   80,  ...,  221,  380,  333],
        [   0,   43,  527,  ..., 4309,   14, 8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3278e-05, -4.0221e-04, -2.7939e-04,  ..., -4.1889e-05,
          2.8951e-04,  6.9643e-05],
        [-4.4554e-06, -3.3453e-06,  2.7828e-06,  ..., -4.2692e-06,
         -3.3304e-06, -3.2261e-06],
        [-9.3430e-06, -7.2420e-06,  6.2138e-06,  ..., -8.8215e-06,
         -6.7949e-06, -6.7055e-06],
        [-7.9274e-06, -5.8115e-06,  5.2676e-06,  ..., -7.6145e-06,
         -5.9083e-06, -6.0424e-06],
        [-1.1668e-05, -9.4175e-06,  7.7635e-06,  ..., -1.1161e-05,
         -8.9407e-06, -7.8082e-06]], device='cuda:0')
Loss: 0.9726094603538513


Running epoch 0, step 979, batch 979
Sampled inputs[:2]: tensor([[    0,   897,   328,  ...,   908,   696,   688],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9963e-06, -3.8389e-04, -3.2334e-04,  ...,  6.0756e-05,
          1.9223e-04,  8.2372e-05],
        [-6.0722e-06, -4.5896e-06,  3.8184e-06,  ..., -5.7444e-06,
         -4.5374e-06, -4.3288e-06],
        [-1.2666e-05, -9.8795e-06,  8.4788e-06,  ..., -1.1846e-05,
         -9.2685e-06, -8.9854e-06],
        [-1.0744e-05, -7.9572e-06,  7.1898e-06,  ..., -1.0207e-05,
         -8.0392e-06, -8.0839e-06],
        [-1.5840e-05, -1.2860e-05,  1.0580e-05,  ..., -1.5035e-05,
         -1.2219e-05, -1.0461e-05]], device='cuda:0')
Loss: 0.9970037341117859


Running epoch 0, step 980, batch 980
Sampled inputs[:2]: tensor([[    0,   259,  6022,  ...,  1871,  1209,  1241],
        [    0,   475,   668,  ..., 17680,   368,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5680e-05, -6.3105e-04, -3.0568e-04,  ..., -2.1418e-05,
          1.8528e-04,  1.7137e-04],
        [-7.7635e-06, -5.7891e-06,  4.8392e-06,  ..., -7.2271e-06,
         -5.6475e-06, -5.5656e-06],
        [ 3.2931e-05,  6.6414e-05, -5.1969e-05,  ...,  9.4926e-05,
          6.9777e-05,  3.1824e-05],
        [-1.3620e-05, -9.9838e-06,  9.0152e-06,  ..., -1.2740e-05,
         -9.9391e-06, -1.0259e-05],
        [-2.0280e-05, -1.6257e-05,  1.3396e-05,  ..., -1.9029e-05,
         -1.5363e-05, -1.3560e-05]], device='cuda:0')
Loss: 1.0282219648361206


Running epoch 0, step 981, batch 981
Sampled inputs[:2]: tensor([[    0,  1176, 33084,  ...,   266,  2269,  1209],
        [    0,  6584,   278,  ...,  1039,   965,  1410]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5383e-05, -6.7840e-04, -4.9959e-04,  ...,  2.6266e-05,
          4.4475e-04,  4.9345e-04],
        [-9.3430e-06, -6.8545e-06,  5.6960e-06,  ..., -8.7693e-06,
         -6.8694e-06, -6.9290e-06],
        [ 2.9638e-05,  6.4105e-05, -5.0017e-05,  ...,  9.1797e-05,
          6.7318e-05,  2.9082e-05],
        [-1.6287e-05, -1.1757e-05,  1.0535e-05,  ..., -1.5333e-05,
         -1.2010e-05, -1.2614e-05],
        [-2.4661e-05, -1.9401e-05,  1.6004e-05,  ..., -2.3201e-05,
         -1.8746e-05, -1.6958e-05]], device='cuda:0')
Loss: 1.0099395513534546


Running epoch 0, step 982, batch 982
Sampled inputs[:2]: tensor([[    0,  1304,  1040,  ...,   287,  1665,   741],
        [    0, 14867,   278,  ...,   674,   369,  4127]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0819e-05, -6.5576e-04, -5.3426e-04,  ...,  3.6037e-05,
          5.0133e-04,  5.6227e-04],
        [-1.0937e-05, -8.1509e-06,  6.6869e-06,  ..., -1.0297e-05,
         -8.0615e-06, -8.0690e-06],
        [ 2.6359e-05,  6.1348e-05, -4.7841e-05,  ...,  8.8668e-05,
          6.4874e-05,  2.6727e-05],
        [-1.9014e-05, -1.3962e-05,  1.2338e-05,  ..., -1.7971e-05,
         -1.4067e-05, -1.4670e-05],
        [-2.8774e-05, -2.3007e-05,  1.8746e-05,  ..., -2.7224e-05,
         -2.2009e-05, -1.9729e-05]], device='cuda:0')
Loss: 1.0253334045410156


Running epoch 0, step 983, batch 983
Sampled inputs[:2]: tensor([[    0,   300, 11040,  ...,   266,  1736,  3487],
        [    0, 20596,  2943,  ...,  5560,  2512,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6361e-05, -8.4106e-04, -7.5552e-04,  ..., -4.8614e-06,
          4.3094e-04,  6.1629e-04],
        [-1.2495e-05, -9.2387e-06,  7.6331e-06,  ..., -1.1735e-05,
         -9.1717e-06, -9.1717e-06],
        [ 2.3141e-05,  5.9053e-05, -4.5770e-05,  ...,  8.5732e-05,
          6.2639e-05,  2.4477e-05],
        [-2.1696e-05, -1.5780e-05,  1.4052e-05,  ..., -2.0444e-05,
         -1.5952e-05, -1.6652e-05],
        [-3.2797e-05, -2.6017e-05,  2.1338e-05,  ..., -3.0950e-05,
         -2.4959e-05, -2.2352e-05]], device='cuda:0')
Loss: 0.9913924932479858
Graident accumulation at epoch 0, step 983, batch 983
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0146, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0444e-04,  2.5924e-05, -2.3280e-04,  ...,  2.5871e-05,
         -9.4879e-05,  7.8190e-06],
        [-1.2857e-05, -8.3816e-06,  6.7297e-06,  ..., -1.0891e-05,
         -7.1123e-06, -8.0879e-06],
        [ 4.3136e-05,  4.5156e-05, -2.6539e-05,  ...,  3.8071e-05,
          4.6272e-05,  7.8051e-06],
        [-1.9503e-05, -1.3452e-05,  1.1923e-05,  ..., -1.6494e-05,
         -1.2915e-05, -1.4563e-05],
        [-3.7583e-05, -2.8093e-05,  2.2278e-05,  ..., -3.1532e-05,
         -2.4600e-05, -2.3163e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7740e-08, 4.6662e-08, 4.7941e-08,  ..., 1.6188e-08, 1.1485e-07,
         3.3736e-08],
        [7.8008e-11, 4.2650e-11, 1.2958e-11,  ..., 5.1614e-11, 1.4770e-11,
         2.1197e-11],
        [1.5488e-09, 8.8085e-10, 2.2955e-10,  ..., 1.1664e-09, 3.1362e-10,
         3.5747e-10],
        [4.2094e-10, 2.9276e-10, 1.3943e-10,  ..., 3.8140e-10, 1.3525e-10,
         1.5471e-10],
        [3.5741e-10, 1.9070e-10, 4.7727e-11,  ..., 2.6136e-10, 4.8654e-11,
         9.9187e-11]], device='cuda:0')
optimizer state dict: 123.0
lr: [1.1469406375747185e-05, 1.1469406375747185e-05]
scheduler_last_epoch: 123


Running epoch 0, step 984, batch 984
Sampled inputs[:2]: tensor([[   0,  278, 6481,  ...,   13, 8970,   12],
        [   0,  266, 4505,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0236e-04, -1.0874e-04, -1.3739e-04,  ..., -2.4314e-04,
          3.0820e-04,  8.5907e-05],
        [-1.5423e-06, -1.0654e-06,  9.6858e-07,  ..., -1.4007e-06,
         -1.0207e-06, -1.1399e-06],
        [-3.2485e-06, -2.3395e-06,  2.1607e-06,  ..., -2.9206e-06,
         -2.1309e-06, -2.3842e-06],
        [-2.7269e-06, -1.8552e-06,  1.8179e-06,  ..., -2.4736e-06,
         -1.8179e-06, -2.1160e-06],
        [-4.0233e-06, -3.0398e-06,  2.6822e-06,  ..., -3.7104e-06,
         -2.8163e-06, -2.7716e-06]], device='cuda:0')
Loss: 0.9939947128295898


Running epoch 0, step 985, batch 985
Sampled inputs[:2]: tensor([[   0, 6275,   12,  ..., 2027, 2887,  287],
        [   0,   45, 6556,  ..., 1477,  352, 1611]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6865e-04, -5.9043e-05, -2.0707e-04,  ..., -3.5782e-04,
          6.0377e-04,  3.1261e-04],
        [-3.1814e-06, -2.2128e-06,  1.8924e-06,  ..., -2.9132e-06,
         -2.0787e-06, -2.4363e-06],
        [-6.8396e-06, -4.9323e-06,  4.3064e-06,  ..., -6.1989e-06,
         -4.3958e-06, -5.2005e-06],
        [-5.5879e-06, -3.8072e-06,  3.4869e-06,  ..., -5.1111e-06,
         -3.6433e-06, -4.4554e-06],
        [-8.5533e-06, -6.4224e-06,  5.3942e-06,  ..., -7.9125e-06,
         -5.8413e-06, -6.1095e-06]], device='cuda:0')
Loss: 1.0143494606018066


Running epoch 0, step 986, batch 986
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0,   431, 19346,  ...,    14,  3237, 18548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3092e-04, -8.2689e-05, -2.0382e-04,  ..., -2.9809e-04,
          6.6902e-04,  4.3394e-04],
        [-4.7609e-06, -3.4794e-06,  2.7306e-06,  ..., -4.4331e-06,
         -3.2037e-06, -3.6880e-06],
        [-1.0148e-05, -7.6890e-06,  6.2883e-06,  ..., -9.3430e-06,
         -6.7651e-06, -7.7337e-06],
        [-8.2403e-06, -5.9530e-06,  5.0366e-06,  ..., -7.7039e-06,
         -5.6103e-06, -6.6459e-06],
        [-1.2755e-05, -1.0028e-05,  7.9572e-06,  ..., -1.1936e-05,
         -9.0003e-06, -9.0748e-06]], device='cuda:0')
Loss: 0.9953662157058716


Running epoch 0, step 987, batch 987
Sampled inputs[:2]: tensor([[    0,   287, 39084,  ...,   266,  1817,  1589],
        [    0,   266,  1624,  ...,    14,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7613e-04, -1.2506e-04, -1.6501e-04,  ..., -2.5878e-04,
          7.3755e-04,  3.9693e-04],
        [-6.3628e-06, -4.5821e-06,  3.7588e-06,  ..., -5.8934e-06,
         -4.1723e-06, -4.7982e-06],
        [-1.3530e-05, -1.0088e-05,  8.5831e-06,  ..., -1.2383e-05,
         -8.7768e-06, -1.0043e-05],
        [-1.1027e-05, -7.8455e-06,  6.9216e-06,  ..., -1.0252e-05,
         -7.2867e-06, -8.6576e-06],
        [-1.6749e-05, -1.2964e-05,  1.0654e-05,  ..., -1.5572e-05,
         -1.1533e-05, -1.1623e-05]], device='cuda:0')
Loss: 0.9917455315589905


Running epoch 0, step 988, batch 988
Sampled inputs[:2]: tensor([[    0,   344,  8133,  ...,   278,  1603,   674],
        [    0, 15372, 10123,  ...,  1782,    12,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0877e-04, -1.2618e-04, -1.9359e-04,  ..., -3.9310e-04,
          9.6355e-04,  3.3360e-04],
        [-7.9796e-06, -5.8338e-06,  4.6715e-06,  ..., -7.4282e-06,
         -5.3793e-06, -5.9828e-06],
        [-1.7017e-05, -1.2860e-05,  1.0654e-05,  ..., -1.5661e-05,
         -1.1325e-05, -1.2591e-05],
        [ 1.0361e-04,  1.9873e-04, -3.8033e-05,  ...,  7.7469e-05,
          1.2837e-04,  7.3027e-05],
        [-2.1189e-05, -1.6615e-05,  1.3322e-05,  ..., -1.9804e-05,
         -1.4946e-05, -1.4678e-05]], device='cuda:0')
Loss: 1.0127617120742798


Running epoch 0, step 989, batch 989
Sampled inputs[:2]: tensor([[    0,  1235,   368,  ..., 12152,  8498,   287],
        [    0,    12,   401,  ...,   504,   565,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1130e-04, -4.4536e-04, -4.3319e-04,  ..., -3.6884e-04,
          8.7992e-04,  2.2482e-04],
        [-9.5516e-06, -6.9886e-06,  5.6997e-06,  ..., -8.8513e-06,
         -6.4000e-06, -7.0333e-06],
        [-2.0310e-05, -1.5333e-05,  1.2919e-05,  ..., -1.8626e-05,
         -1.3411e-05, -1.4797e-05],
        [ 1.0075e-04,  1.9669e-04, -3.6081e-05,  ...,  7.4892e-05,
          1.2656e-04,  7.1030e-05],
        [-2.5213e-05, -1.9759e-05,  1.6049e-05,  ..., -2.3484e-05,
         -1.7658e-05, -1.7181e-05]], device='cuda:0')
Loss: 1.047318696975708


Running epoch 0, step 990, batch 990
Sampled inputs[:2]: tensor([[    0,   401,  3740,  ...,  5980,   271,   266],
        [    0, 21178,  1952,  ..., 14930,     9,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9209e-04, -4.3176e-04, -3.9586e-04,  ..., -2.7511e-04,
          7.7318e-04,  2.1832e-04],
        [-1.1072e-05, -8.1137e-06,  6.7353e-06,  ..., -1.0252e-05,
         -7.3537e-06, -8.0988e-06],
        [-2.3693e-05, -1.7852e-05,  1.5303e-05,  ..., -2.1696e-05,
         -1.5467e-05, -1.7166e-05],
        [ 9.8006e-05,  1.9474e-04, -3.4144e-05,  ...,  7.2403e-05,
          1.2490e-04,  6.9018e-05],
        [-2.9147e-05, -2.2814e-05,  1.8820e-05,  ..., -2.7120e-05,
         -2.0206e-05, -1.9759e-05]], device='cuda:0')
Loss: 0.9931896924972534


Running epoch 0, step 991, batch 991
Sampled inputs[:2]: tensor([[    0,   266,  1916,  ...,   292, 12946,     9],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1552e-04, -4.9036e-04, -4.9488e-04,  ..., -5.1294e-04,
          8.4043e-04,  1.3598e-04],
        [-1.2666e-05, -9.2760e-06,  7.7188e-06,  ..., -1.1697e-05,
         -8.4341e-06, -9.2387e-06],
        [ 5.4366e-05,  4.7773e-05, -2.4386e-05,  ...,  5.0091e-05,
          3.8736e-05,  1.2221e-05],
        [ 9.5279e-05,  1.9277e-04, -3.2385e-05,  ...,  6.9929e-05,
          1.2305e-04,  6.6992e-05],
        [-3.3289e-05, -2.6107e-05,  2.1562e-05,  ..., -3.0965e-05,
         -2.3231e-05, -2.2531e-05]], device='cuda:0')
Loss: 0.9949396252632141
Graident accumulation at epoch 0, step 991, batch 991
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0025, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0146, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2446e-05, -2.5705e-05, -2.5901e-04,  ..., -2.8011e-05,
         -1.3475e-06,  2.0635e-05],
        [-1.2838e-05, -8.4710e-06,  6.8286e-06,  ..., -1.0972e-05,
         -7.2445e-06, -8.2030e-06],
        [ 4.4259e-05,  4.5417e-05, -2.6324e-05,  ...,  3.9273e-05,
          4.5519e-05,  8.2466e-06],
        [-8.0244e-06,  7.1706e-06,  7.4926e-06,  ..., -7.8512e-06,
          6.8088e-07, -6.4075e-06],
        [-3.7154e-05, -2.7895e-05,  2.2206e-05,  ..., -3.1475e-05,
         -2.4463e-05, -2.3099e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7782e-08, 4.6856e-08, 4.8138e-08,  ..., 1.6435e-08, 1.1544e-07,
         3.3721e-08],
        [7.8091e-11, 4.2693e-11, 1.3004e-11,  ..., 5.1699e-11, 1.4826e-11,
         2.1261e-11],
        [1.5502e-09, 8.8225e-10, 2.2991e-10,  ..., 1.1677e-09, 3.1481e-10,
         3.5726e-10],
        [4.2959e-10, 3.2963e-10, 1.4034e-10,  ..., 3.8591e-10, 1.5025e-10,
         1.5904e-10],
        [3.5816e-10, 1.9119e-10, 4.8144e-11,  ..., 2.6206e-10, 4.9145e-11,
         9.9595e-11]], device='cuda:0')
optimizer state dict: 124.0
lr: [1.1347022425551613e-05, 1.1347022425551613e-05]
scheduler_last_epoch: 124


Running epoch 0, step 992, batch 992
Sampled inputs[:2]: tensor([[   0, 1477, 5648,  ..., 4391, 1722,  369],
        [   0,  344, 2183,  ...,   14,  759,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3615e-04, -6.2923e-05, -2.5429e-05,  ..., -8.2592e-05,
         -1.0845e-05,  8.8734e-05],
        [-1.6987e-06, -1.1176e-06,  9.9093e-07,  ..., -1.5497e-06,
         -1.0058e-06, -1.2517e-06],
        [-3.5763e-06, -2.4438e-06,  2.2054e-06,  ..., -3.2485e-06,
         -2.1160e-06, -2.6226e-06],
        [-2.9057e-06, -1.8999e-06,  1.7807e-06,  ..., -2.6524e-06,
         -1.7136e-06, -2.1905e-06],
        [-4.4107e-06, -3.1292e-06,  2.7120e-06,  ..., -4.0829e-06,
         -2.7865e-06, -3.0845e-06]], device='cuda:0')
Loss: 1.0124931335449219


Running epoch 0, step 993, batch 993
Sampled inputs[:2]: tensor([[    0,   437,  1690,  ...,  1274, 10695, 10762],
        [    0,   344,   259,  ...,  6787, 10045,  9799]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0164e-04, -1.1015e-04, -2.0341e-04,  ..., -1.3619e-04,
         -7.3702e-05, -8.7202e-05],
        [-3.2261e-06, -2.2277e-06,  2.0191e-06,  ..., -2.9430e-06,
         -1.9968e-06, -2.2724e-06],
        [-6.9439e-06, -4.9621e-06,  4.5598e-06,  ..., -6.2585e-06,
         -4.2617e-06, -4.8131e-06],
        [-5.6475e-06, -3.8072e-06,  3.7327e-06,  ..., -5.1260e-06,
         -3.4645e-06, -4.0978e-06],
        [-8.4341e-06, -6.2585e-06,  5.5283e-06,  ..., -7.7337e-06,
         -5.4836e-06, -5.5432e-06]], device='cuda:0')
Loss: 1.0064879655838013


Running epoch 0, step 994, batch 994
Sampled inputs[:2]: tensor([[    0, 17900,   554,  ...,   266,  7912,    26],
        [    0,  1732,   292,  ...,  3440,  4010,  1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2586e-04, -2.4373e-04, -3.0869e-04,  ..., -1.1943e-04,
         -5.2861e-05, -1.8143e-04],
        [-4.9099e-06, -3.4645e-06,  3.0994e-06,  ..., -4.3958e-06,
         -2.9653e-06, -3.3230e-06],
        [-1.0595e-05, -7.7337e-06,  7.0035e-06,  ..., -9.4324e-06,
         -6.3628e-06, -7.1079e-06],
        [-8.6874e-06, -6.0126e-06,  5.7444e-06,  ..., -7.7486e-06,
         -5.2005e-06, -6.0648e-06],
        [-1.2726e-05, -9.6411e-06,  8.3745e-06,  ..., -1.1548e-05,
         -8.1211e-06, -8.0764e-06]], device='cuda:0')
Loss: 1.0344713926315308


Running epoch 0, step 995, batch 995
Sampled inputs[:2]: tensor([[    0,   344, 14017,  ...,    65,   298,   634],
        [    0,   957,  1357,  ..., 26179,   287,  6458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5985e-04, -2.9669e-04, -3.2455e-04,  ..., -3.0765e-04,
          7.2323e-05, -2.9951e-04],
        [-6.4895e-06, -4.6045e-06,  4.1425e-06,  ..., -5.7891e-06,
         -3.8818e-06, -4.3362e-06],
        [-1.3947e-05, -1.0207e-05,  9.3132e-06,  ..., -1.2338e-05,
         -8.2403e-06, -9.2685e-06],
        [-1.1519e-05, -8.0094e-06,  7.6964e-06,  ..., -1.0222e-05,
         -6.7949e-06, -7.9721e-06],
        [-1.6779e-05, -1.2755e-05,  1.1146e-05,  ..., -1.5140e-05,
         -1.0580e-05, -1.0490e-05]], device='cuda:0')
Loss: 1.006528377532959


Running epoch 0, step 996, batch 996
Sampled inputs[:2]: tensor([[   0,  300,  266,  ...,   13, 2920,  609],
        [   0,  996, 2226,  ...,  516, 3470,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3948e-04, -3.3054e-04, -3.9081e-04,  ..., -3.1221e-04,
          3.2780e-05, -2.8160e-04],
        [-8.1360e-06, -5.7742e-06,  5.2601e-06,  ..., -7.2569e-06,
         -4.8317e-06, -5.3346e-06],
        [-1.7434e-05, -1.2755e-05,  1.1772e-05,  ..., -1.5467e-05,
         -1.0252e-05, -1.1414e-05],
        [-1.4454e-05, -1.0081e-05,  9.7677e-06,  ..., -1.2845e-05,
         -8.4713e-06, -9.8273e-06],
        [-2.0772e-05, -1.5810e-05,  1.3947e-05,  ..., -1.8790e-05,
         -1.3053e-05, -1.2800e-05]], device='cuda:0')
Loss: 1.0074924230575562


Running epoch 0, step 997, batch 997
Sampled inputs[:2]: tensor([[   0, 1746,   14,  ..., 3134, 5968,    9],
        [   0,   17, 2736,  ...,  352,  422,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3948e-04, -2.5002e-04, -3.0234e-04,  ..., -3.7420e-04,
          1.3267e-04, -3.0352e-04],
        [-9.6709e-06, -6.8843e-06,  6.1989e-06,  ..., -8.7023e-06,
         -5.8673e-06, -6.4448e-06],
        [-2.0653e-05, -1.5154e-05,  1.3903e-05,  ..., -1.8418e-05,
         -1.2308e-05, -1.3679e-05],
        [-1.7166e-05, -1.1973e-05,  1.1504e-05,  ..., -1.5378e-05,
         -1.0259e-05, -1.1869e-05],
        [-2.4766e-05, -1.8910e-05,  1.6585e-05,  ..., -2.2501e-05,
         -1.5765e-05, -1.5393e-05]], device='cuda:0')
Loss: 0.983765721321106


Running epoch 0, step 998, batch 998
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,  1456, 32380,  ...,    12,  1172, 12557]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3907e-04, -2.6205e-04, -2.5425e-04,  ..., -3.0316e-04,
          9.9837e-05, -3.2227e-04],
        [-1.1228e-05, -7.8902e-06,  7.0632e-06,  ..., -1.0177e-05,
         -6.8508e-06, -7.7114e-06],
        [-2.3916e-05, -1.7330e-05,  1.5900e-05,  ..., -2.1428e-05,
         -1.4305e-05, -1.6227e-05],
        [-1.9923e-05, -1.3717e-05,  1.3143e-05,  ..., -1.7971e-05,
         -1.1981e-05, -1.4149e-05],
        [-2.8700e-05, -2.1607e-05,  1.9029e-05,  ..., -2.6137e-05,
         -1.8284e-05, -1.8224e-05]], device='cuda:0')
Loss: 0.984102189540863


Running epoch 0, step 999, batch 999
Sampled inputs[:2]: tensor([[    0,    13, 38195,  ...,   950,   298,   257],
        [    0,   527,   496,  ...,    12,   795,  8296]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0963e-06, -1.6303e-04, -2.4218e-04,  ..., -5.2164e-04,
          1.8309e-04, -2.7079e-04],
        [-1.2860e-05, -9.1866e-06,  7.9609e-06,  ..., -1.1697e-05,
         -7.9535e-06, -8.9183e-06],
        [-2.7508e-05, -2.0280e-05,  1.8001e-05,  ..., -2.4751e-05,
         -1.6719e-05, -1.8880e-05],
        [-2.2709e-05, -1.5907e-05,  1.4745e-05,  ..., -2.0564e-05,
         -1.3851e-05, -1.6294e-05],
        [-3.3259e-05, -2.5451e-05,  2.1696e-05,  ..., -3.0428e-05,
         -2.1502e-05, -2.1413e-05]], device='cuda:0')
Loss: 1.0016895532608032
Graident accumulation at epoch 0, step 999, batch 999
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0284,  ...,  0.0290, -0.0146, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.5892e-05, -3.9437e-05, -2.5733e-04,  ..., -7.7374e-05,
          1.7096e-05, -8.5072e-06],
        [-1.2840e-05, -8.5426e-06,  6.9418e-06,  ..., -1.1045e-05,
         -7.3154e-06, -8.2745e-06],
        [ 3.7082e-05,  3.8848e-05, -2.1891e-05,  ...,  3.2871e-05,
          3.9295e-05,  5.5340e-06],
        [-9.4929e-06,  4.8628e-06,  8.2178e-06,  ..., -9.1225e-06,
         -7.7227e-07, -7.3962e-06],
        [-3.6764e-05, -2.7650e-05,  2.2155e-05,  ..., -3.1370e-05,
         -2.4167e-05, -2.2931e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7724e-08, 4.6836e-08, 4.8148e-08,  ..., 1.6691e-08, 1.1535e-07,
         3.3760e-08],
        [7.8178e-11, 4.2735e-11, 1.3055e-11,  ..., 5.1784e-11, 1.4874e-11,
         2.1319e-11],
        [1.5494e-09, 8.8178e-10, 2.3001e-10,  ..., 1.1671e-09, 3.1478e-10,
         3.5726e-10],
        [4.2968e-10, 3.2955e-10, 1.4042e-10,  ..., 3.8595e-10, 1.5030e-10,
         1.5915e-10],
        [3.5891e-10, 1.9165e-10, 4.8567e-11,  ..., 2.6272e-10, 4.9558e-11,
         9.9954e-11]], device='cuda:0')
optimizer state dict: 125.0
lr: [1.1224432638571088e-05, 1.1224432638571088e-05]
scheduler_last_epoch: 125


Running epoch 0, step 1000, batch 1000
Sampled inputs[:2]: tensor([[    0,    76,   472,  ..., 21215,   472,   346],
        [    0,  7527,    15,  ...,  2677,   292, 30654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5896e-05,  1.7620e-04, -2.2740e-04,  ...,  2.6119e-04,
         -5.7051e-04, -6.3799e-04],
        [-1.6168e-06, -1.1176e-06,  1.0729e-06,  ..., -1.4231e-06,
         -8.9407e-07, -1.0505e-06],
        [-3.4869e-06, -2.4885e-06,  2.4438e-06,  ..., -3.0547e-06,
         -1.9222e-06, -2.2352e-06],
        [-2.9355e-06, -1.9819e-06,  2.0713e-06,  ..., -2.5779e-06,
         -1.6168e-06, -1.9968e-06],
        [-3.9637e-06, -2.9504e-06,  2.7567e-06,  ..., -3.5167e-06,
         -2.3395e-06, -2.3693e-06]], device='cuda:0')
Loss: 0.9777418375015259


Running epoch 0, step 1001, batch 1001
Sampled inputs[:2]: tensor([[    0,   292, 15156,  ...,    35,  3815,  1422],
        [    0,   587,   300,  ...,  4325,   278, 12564]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9501e-05,  1.9722e-04, -1.7554e-04,  ...,  2.4430e-04,
         -6.8286e-04, -7.4276e-04],
        [-3.2410e-06, -2.3097e-06,  2.0564e-06,  ..., -2.9206e-06,
         -1.9148e-06, -2.1458e-06],
        [-7.0930e-06, -5.1856e-06,  4.7386e-06,  ..., -6.3330e-06,
         -4.1276e-06, -4.6492e-06],
        [-5.7817e-06, -4.0233e-06,  3.8669e-06,  ..., -5.1856e-06,
         -3.3751e-06, -3.9935e-06],
        [-8.1658e-06, -6.1840e-06,  5.4240e-06,  ..., -7.3612e-06,
         -5.0366e-06, -5.0068e-06]], device='cuda:0')
Loss: 0.9936935901641846


Running epoch 0, step 1002, batch 1002
Sampled inputs[:2]: tensor([[   0,  275, 1911,  ..., 1371, 5151, 2813],
        [   0,  925,  271,  ...,  391,  721, 1576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4164e-05,  3.1987e-04, -4.2189e-05,  ...,  1.9878e-04,
         -7.3556e-04, -8.0572e-04],
        [-4.9844e-06, -3.5316e-06,  3.0920e-06,  ..., -4.3809e-06,
         -2.8014e-06, -3.1888e-06],
        [-1.0848e-05, -7.9274e-06,  7.0781e-06,  ..., -9.4920e-06,
         -6.0499e-06, -6.9141e-06],
        [-8.8960e-06, -6.1989e-06,  5.7891e-06,  ..., -7.7933e-06,
         -4.9546e-06, -5.9307e-06],
        [-1.2368e-05, -9.3728e-06,  8.0317e-06,  ..., -1.0982e-05,
         -7.3612e-06, -7.4059e-06]], device='cuda:0')
Loss: 0.9858298301696777


Running epoch 0, step 1003, batch 1003
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0,  2261,     9,  ..., 15008,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7821e-05,  2.7089e-04, -2.0457e-05,  ...,  1.2313e-04,
         -7.6719e-04, -8.8956e-04],
        [-6.5789e-06, -4.7311e-06,  4.0382e-06,  ..., -5.8189e-06,
         -3.7625e-06, -4.2468e-06],
        [ 9.5748e-06,  6.1353e-05, -2.4870e-05,  ...,  6.4291e-05,
          6.1734e-05,  2.3925e-05],
        [-1.1802e-05, -8.3148e-06,  7.5772e-06,  ..., -1.0386e-05,
         -6.6757e-06, -7.9125e-06],
        [-1.6421e-05, -1.2532e-05,  1.0580e-05,  ..., -1.4603e-05,
         -9.8348e-06, -9.8348e-06]], device='cuda:0')
Loss: 0.9999648928642273


Running epoch 0, step 1004, batch 1004
Sampled inputs[:2]: tensor([[    0,  2911,   287,  ...,  2178, 22788,  8645],
        [    0,   391,  7750,  ...,  4133,   271,   668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2619e-05,  3.4237e-04,  3.8798e-05,  ...,  2.1457e-05,
         -1.0334e-03, -1.0227e-03],
        [-8.2403e-06, -5.8711e-06,  5.0217e-06,  ..., -7.2718e-06,
         -4.7460e-06, -5.3421e-06],
        [ 5.8495e-06,  5.8730e-05, -2.2575e-05,  ...,  6.1073e-05,
          5.9558e-05,  2.1481e-05],
        [-1.4782e-05, -1.0297e-05,  9.3952e-06,  ..., -1.2964e-05,
         -8.4043e-06, -9.9540e-06],
        [-2.0713e-05, -1.5661e-05,  1.3202e-05,  ..., -1.8373e-05,
         -1.2487e-05, -1.2487e-05]], device='cuda:0')
Loss: 1.0032670497894287


Running epoch 0, step 1005, batch 1005
Sampled inputs[:2]: tensor([[    0,  5862,    13,  ..., 12497,   287,  3570],
        [    0, 24086,   266,  ..., 18814,    19,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9659e-05,  2.1237e-04, -4.1070e-05,  ..., -6.9150e-05,
         -9.4989e-04, -9.6676e-04],
        [-9.8050e-06, -7.0706e-06,  6.0573e-06,  ..., -8.6874e-06,
         -5.7146e-06, -6.4373e-06],
        [ 2.3477e-06,  5.6033e-05, -2.0161e-05,  ...,  5.7973e-05,
          5.7472e-05,  1.9067e-05],
        [-1.7643e-05, -1.2442e-05,  1.1347e-05,  ..., -1.5527e-05,
         -1.0125e-05, -1.2010e-05],
        [-2.4796e-05, -1.8910e-05,  1.6019e-05,  ..., -2.2039e-05,
         -1.5065e-05, -1.5125e-05]], device='cuda:0')
Loss: 1.009286880493164


Running epoch 0, step 1006, batch 1006
Sampled inputs[:2]: tensor([[    0,  7061,   437,  ...,   278,  9500,    18],
        [    0, 11657,   367,  ..., 31468,    26,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2670e-04,  3.4608e-04,  2.4943e-05,  ...,  2.3696e-07,
         -1.0153e-03, -9.7859e-04],
        [-1.1511e-05, -8.3447e-06,  7.0408e-06,  ..., -1.0170e-05,
         -6.6832e-06, -7.5176e-06],
        [-1.4372e-06,  5.3157e-05, -1.7866e-05,  ...,  5.4695e-05,
          5.5341e-05,  1.6683e-05],
        [-2.0698e-05, -1.4693e-05,  1.3180e-05,  ..., -1.8165e-05,
         -1.1817e-05, -1.3992e-05],
        [-2.9176e-05, -2.2337e-05,  1.8671e-05,  ..., -2.5883e-05,
         -1.7703e-05, -1.7732e-05]], device='cuda:0')
Loss: 1.0178533792495728


Running epoch 0, step 1007, batch 1007
Sampled inputs[:2]: tensor([[    0,  1932,    15,  ...,   344,   984,   344],
        [    0,   328, 16219,  ..., 14559,   351,   587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4817e-05,  3.2372e-04, -1.8021e-05,  ...,  2.4906e-05,
         -1.1151e-03, -1.0237e-03],
        [-1.3128e-05, -9.6112e-06,  8.0541e-06,  ..., -1.1675e-05,
         -7.7114e-06, -8.5756e-06],
        [-4.8197e-06,  5.0430e-05, -1.5602e-05,  ...,  5.1581e-05,
          5.3240e-05,  1.4493e-05],
        [-2.3559e-05, -1.6898e-05,  1.5080e-05,  ..., -2.0832e-05,
         -1.3635e-05, -1.5974e-05],
        [-3.3259e-05, -2.5734e-05,  2.1368e-05,  ..., -2.9683e-05,
         -2.0385e-05, -2.0176e-05]], device='cuda:0')
Loss: 1.0027426481246948
Graident accumulation at epoch 0, step 1007, batch 1007
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0266,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0285,  ...,  0.0290, -0.0146, -0.0175]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.5821e-05, -3.1218e-06, -2.3340e-04,  ..., -6.7146e-05,
         -9.6124e-05, -1.1002e-04],
        [-1.2869e-05, -8.6495e-06,  7.0530e-06,  ..., -1.1108e-05,
         -7.3550e-06, -8.3046e-06],
        [ 3.2892e-05,  4.0006e-05, -2.1262e-05,  ...,  3.4742e-05,
          4.0689e-05,  6.4298e-06],
        [-1.0899e-05,  2.6868e-06,  8.9040e-06,  ..., -1.0293e-05,
         -2.0585e-06, -8.2539e-06],
        [-3.6414e-05, -2.7459e-05,  2.2077e-05,  ..., -3.1202e-05,
         -2.3789e-05, -2.2655e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7668e-08, 4.6894e-08, 4.8100e-08,  ..., 1.6675e-08, 1.1648e-07,
         3.4774e-08],
        [7.8272e-11, 4.2785e-11, 1.3106e-11,  ..., 5.1868e-11, 1.4919e-11,
         2.1372e-11],
        [1.5479e-09, 8.8344e-10, 2.3002e-10,  ..., 1.1686e-09, 3.1730e-10,
         3.5711e-10],
        [4.2980e-10, 3.2951e-10, 1.4051e-10,  ..., 3.8600e-10, 1.5033e-10,
         1.5925e-10],
        [3.5966e-10, 1.9212e-10, 4.8975e-11,  ..., 2.6334e-10, 4.9924e-11,
         1.0026e-10]], device='cuda:0')
optimizer state dict: 126.0
lr: [1.1101655747595168e-05, 1.1101655747595168e-05]
scheduler_last_epoch: 126


Running epoch 0, step 1008, batch 1008
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0, 49018,   292,  ...,  8774,   642,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7838e-05, -4.2975e-06,  7.5539e-05,  ...,  4.8278e-05,
          1.9650e-05,  8.0467e-05],
        [-1.6615e-06, -1.3188e-06,  9.7603e-07,  ..., -1.4752e-06,
         -1.0282e-06, -1.0878e-06],
        [-3.7402e-06, -3.0547e-06,  2.2948e-06,  ..., -3.3081e-06,
         -2.2799e-06, -2.4438e-06],
        [-2.9653e-06, -2.3246e-06,  1.7881e-06,  ..., -2.6226e-06,
         -1.8030e-06, -2.0117e-06],
        [-4.3809e-06, -3.6657e-06,  2.6673e-06,  ..., -3.9041e-06,
         -2.8014e-06, -2.6971e-06]], device='cuda:0')
Loss: 0.9971799254417419


Running epoch 0, step 1009, batch 1009
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2675e-05, -4.5005e-05,  2.3358e-05,  ..., -3.7317e-05,
          1.6040e-04,  5.4517e-05],
        [-3.3379e-06, -2.6673e-06,  2.0638e-06,  ..., -2.9430e-06,
         -2.0117e-06, -2.0787e-06],
        [-7.5102e-06, -6.1691e-06,  4.8429e-06,  ..., -6.6012e-06,
         -4.4852e-06, -4.6939e-06],
        [-5.9307e-06, -4.7088e-06,  3.7849e-06,  ..., -5.2154e-06,
         -3.5390e-06, -3.8445e-06],
        [-8.6427e-06, -7.3165e-06,  5.5432e-06,  ..., -7.7188e-06,
         -5.4836e-06, -5.0962e-06]], device='cuda:0')
Loss: 1.0133576393127441


Running epoch 0, step 1010, batch 1010
Sampled inputs[:2]: tensor([[    0,    12,   298,  ...,  5125,  6654,  4925],
        [    0,   271,   266,  ..., 46357, 11101, 10621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0592e-04, -5.0577e-05,  1.4887e-04,  ..., -5.9892e-05,
          1.4282e-04, -3.3228e-05],
        [-5.0217e-06, -3.8967e-06,  3.0696e-06,  ..., -4.4405e-06,
         -2.9653e-06, -3.1888e-06],
        [-1.1325e-05, -9.0599e-06,  7.2122e-06,  ..., -9.9838e-06,
         -6.6310e-06, -7.1824e-06],
        [-8.8662e-06, -6.8098e-06,  5.5954e-06,  ..., -7.8231e-06,
         -5.1856e-06, -5.8562e-06],
        [-1.3024e-05, -1.0774e-05,  8.2701e-06,  ..., -1.1683e-05,
         -8.1211e-06, -7.8082e-06]], device='cuda:0')
Loss: 1.001949667930603


Running epoch 0, step 1011, batch 1011
Sampled inputs[:2]: tensor([[   0, 2314,  516,  ..., 1871,   13, 1303],
        [   0,  659,  278,  ...,  593, 2177,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8558e-04, -6.3334e-07,  8.8746e-05,  ..., -5.9892e-05,
          3.4708e-06, -2.2116e-04],
        [-6.5789e-06, -4.9472e-06,  4.1202e-06,  ..., -5.8264e-06,
         -3.7625e-06, -4.1276e-06],
        [-1.4856e-05, -1.1474e-05,  9.7156e-06,  ..., -1.3083e-05,
         -8.3745e-06, -9.2834e-06],
        [-1.1876e-05, -8.7619e-06,  7.7114e-06,  ..., -1.0476e-05,
         -6.6683e-06, -7.7337e-06],
        [-1.6823e-05, -1.3471e-05,  1.0923e-05,  ..., -1.5050e-05,
         -1.0133e-05, -9.9242e-06]], device='cuda:0')
Loss: 0.9799245595932007


Running epoch 0, step 1012, batch 1012
Sampled inputs[:2]: tensor([[   0,  287,  298,  ...,   14, 1147,  199],
        [   0,  669,  292,  ..., 4032,  271, 4442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0967e-04,  3.4656e-04,  1.5144e-04,  ..., -4.2786e-05,
          3.1382e-05, -2.0627e-04],
        [-8.2925e-06, -6.1318e-06,  5.0962e-06,  ..., -7.3537e-06,
         -4.7833e-06, -5.4017e-06],
        [-1.8641e-05, -1.4201e-05,  1.2025e-05,  ..., -1.6421e-05,
         -1.0610e-05, -1.2055e-05],
        [-1.4827e-05, -1.0774e-05,  9.4697e-06,  ..., -1.3113e-05,
         -8.4341e-06, -9.9987e-06],
        [-2.1353e-05, -1.6809e-05,  1.3679e-05,  ..., -1.9073e-05,
         -1.2934e-05, -1.3053e-05]], device='cuda:0')
Loss: 0.953606903553009


Running epoch 0, step 1013, batch 1013
Sampled inputs[:2]: tensor([[    0,  4154,    12,  ...,    14,   560,   199],
        [    0, 18717,  2837,  ...,    48,    18,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6885e-04,  4.1091e-04,  2.5554e-04,  ..., -1.0109e-04,
          9.8970e-05, -1.7286e-04],
        [-9.9242e-06, -7.4506e-06,  6.0648e-06,  ..., -8.8811e-06,
         -5.8040e-06, -6.4597e-06],
        [-2.2203e-05, -1.7121e-05,  1.4275e-05,  ..., -1.9699e-05,
         -1.2785e-05, -1.4335e-05],
        [-1.7703e-05, -1.3053e-05,  1.1235e-05,  ..., -1.5780e-05,
         -1.0207e-05, -1.1936e-05],
        [-2.5555e-05, -2.0340e-05,  1.6332e-05,  ..., -2.2978e-05,
         -1.5646e-05, -1.5557e-05]], device='cuda:0')
Loss: 1.0389600992202759


Running epoch 0, step 1014, batch 1014
Sampled inputs[:2]: tensor([[   0,  586, 1016,  ..., 7151, 8280,  300],
        [   0,  292, 3030,  ..., 1231, 2156,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4176e-04,  4.1157e-04,  1.3593e-04,  ..., -1.6988e-04,
          2.2542e-04, -2.7831e-04],
        [-1.1601e-05, -8.7693e-06,  7.1228e-06,  ..., -1.0349e-05,
         -6.7875e-06, -7.5027e-06],
        [-2.5854e-05, -2.0087e-05,  1.6704e-05,  ..., -2.2888e-05,
         -1.4931e-05, -1.6600e-05],
        [-2.0653e-05, -1.5363e-05,  1.3188e-05,  ..., -1.8358e-05,
         -1.1943e-05, -1.3843e-05],
        [-2.9758e-05, -2.3901e-05,  1.9118e-05,  ..., -2.6762e-05,
         -1.8328e-05, -1.8030e-05]], device='cuda:0')
Loss: 1.0095018148422241


Running epoch 0, step 1015, batch 1015
Sampled inputs[:2]: tensor([[   0,  278, 9939,  ..., 1238,   14,  445],
        [   0, 7018,   14,  ..., 8288,   12, 1250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6468e-04,  5.5711e-04,  2.8508e-04,  ..., -2.5453e-04,
          2.8559e-04, -2.1592e-04],
        [-1.3344e-05, -1.0058e-05,  8.1509e-06,  ..., -1.1884e-05,
         -7.8380e-06, -8.7246e-06],
        [-2.9817e-05, -2.3097e-05,  1.9133e-05,  ..., -2.6360e-05,
         -1.7315e-05, -1.9372e-05],
        [-2.3589e-05, -1.7524e-05,  1.4976e-05,  ..., -2.0936e-05,
         -1.3709e-05, -1.5959e-05],
        [-3.4347e-05, -2.7508e-05,  2.1935e-05,  ..., -3.0875e-05,
         -2.1249e-05, -2.1100e-05]], device='cuda:0')
Loss: 0.9886635541915894
Graident accumulation at epoch 0, step 1015, batch 1015
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0027,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0285,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4771e-05,  5.2901e-05, -1.8155e-04,  ..., -8.5884e-05,
         -5.7952e-05, -1.2061e-04],
        [-1.2916e-05, -8.7903e-06,  7.1628e-06,  ..., -1.1185e-05,
         -7.4033e-06, -8.3466e-06],
        [ 2.6621e-05,  3.3696e-05, -1.7223e-05,  ...,  2.8632e-05,
          3.4889e-05,  3.8497e-06],
        [-1.2168e-05,  6.6572e-07,  9.5112e-06,  ..., -1.1358e-05,
         -3.2236e-06, -9.0245e-06],
        [-3.6207e-05, -2.7464e-05,  2.2062e-05,  ..., -3.1169e-05,
         -2.3535e-05, -2.2500e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7638e-08, 4.7157e-08, 4.8133e-08,  ..., 1.6723e-08, 1.1645e-07,
         3.4786e-08],
        [7.8372e-11, 4.2843e-11, 1.3160e-11,  ..., 5.1958e-11, 1.4966e-11,
         2.1426e-11],
        [1.5472e-09, 8.8309e-10, 2.3015e-10,  ..., 1.1682e-09, 3.1728e-10,
         3.5713e-10],
        [4.2993e-10, 3.2949e-10, 1.4059e-10,  ..., 3.8605e-10, 1.5037e-10,
         1.5934e-10],
        [3.6048e-10, 1.9268e-10, 4.9407e-11,  ..., 2.6403e-10, 5.0326e-11,
         1.0061e-10]], device='cuda:0')
optimizer state dict: 127.0
lr: [1.0978710514004527e-05, 1.0978710514004527e-05]
scheduler_last_epoch: 127


Running epoch 0, step 1016, batch 1016
Sampled inputs[:2]: tensor([[    0,     9,   292,  ...,   944,   278,  1758],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6167e-05,  4.5649e-05, -1.1880e-04,  ...,  6.9815e-05,
         -2.2107e-04, -1.3543e-04],
        [-1.6093e-06, -1.2666e-06,  1.0952e-06,  ..., -1.4082e-06,
         -8.9779e-07, -9.3877e-07],
        [-3.5167e-06, -2.8461e-06,  2.5034e-06,  ..., -3.0547e-06,
         -1.9372e-06, -2.0415e-06],
        [-2.9057e-06, -2.2501e-06,  2.0713e-06,  ..., -2.5183e-06,
         -1.6019e-06, -1.7732e-06],
        [-3.9339e-06, -3.3081e-06,  2.7716e-06,  ..., -3.4869e-06,
         -2.3395e-06, -2.1458e-06]], device='cuda:0')
Loss: 1.0063363313674927


Running epoch 0, step 1017, batch 1017
Sampled inputs[:2]: tensor([[    0,  2939,    14,  ...,  1702,  1481,   278],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7829e-04,  6.6766e-05, -7.5414e-05,  ...,  1.1741e-04,
         -2.2151e-04, -1.2083e-05],
        [-3.2112e-06, -2.4289e-06,  2.0564e-06,  ..., -2.8685e-06,
         -1.8589e-06, -2.0862e-06],
        [-7.1079e-06, -5.4240e-06,  4.8131e-06,  ..., -6.1989e-06,
         -3.9637e-06, -4.5151e-06],
        [-5.8413e-06, -4.2915e-06,  3.9265e-06,  ..., -5.1409e-06,
         -3.3006e-06, -3.9190e-06],
        [-8.0466e-06, -6.3330e-06,  5.4091e-06,  ..., -7.1228e-06,
         -4.7982e-06, -4.7684e-06]], device='cuda:0')
Loss: 1.0065916776657104


Running epoch 0, step 1018, batch 1018
Sampled inputs[:2]: tensor([[    0,   546, 28676,  ...,   271,  1267,   328],
        [    0,  1241,  2098,  ...,  1862,   631,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3779e-04,  1.8099e-05, -7.7903e-05,  ...,  2.1168e-05,
         -1.7385e-04, -9.6340e-05],
        [-4.8876e-06, -3.7476e-06,  3.2037e-06,  ..., -4.3213e-06,
         -2.7753e-06, -3.0771e-06],
        [-1.0923e-05, -8.4639e-06,  7.5102e-06,  ..., -9.4771e-06,
         -6.0052e-06, -6.7651e-06],
        [-8.8662e-06, -6.6161e-06,  6.0573e-06,  ..., -7.7337e-06,
         -4.9025e-06, -5.7593e-06],
        [-1.2279e-05, -9.8348e-06,  8.3745e-06,  ..., -1.0848e-05,
         -7.2569e-06, -7.1228e-06]], device='cuda:0')
Loss: 1.0244401693344116


Running epoch 0, step 1019, batch 1019
Sampled inputs[:2]: tensor([[    0,   287,  1070,  ...,   292,   221,   374],
        [    0,   328,   471,  ..., 11137,   679,  6585]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0125e-04,  1.4387e-04,  1.2774e-05,  ..., -1.3060e-05,
         -1.6260e-05,  3.3389e-06],
        [-6.4373e-06, -4.9770e-06,  4.1388e-06,  ..., -5.7966e-06,
         -3.8110e-06, -4.1500e-06],
        [-1.4365e-05, -1.1235e-05,  9.7156e-06,  ..., -1.2696e-05,
         -8.2403e-06, -9.1046e-06],
        [-1.1668e-05, -8.7917e-06,  7.8231e-06,  ..., -1.0416e-05,
         -6.7726e-06, -7.7710e-06],
        [-1.6361e-05, -1.3188e-05,  1.0982e-05,  ..., -1.4693e-05,
         -9.9987e-06, -9.7156e-06]], device='cuda:0')
Loss: 1.0152854919433594


Running epoch 0, step 1020, batch 1020
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   287,  2888,  4845],
        [    0,    13, 37178,  ...,  1692,  3287, 10652]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8709e-04,  1.4387e-04,  5.0150e-05,  ..., -4.1592e-06,
         -1.5392e-05,  8.9248e-05],
        [-8.1137e-06, -6.3032e-06,  5.2117e-06,  ..., -7.3016e-06,
         -4.8839e-06, -5.0701e-06],
        [-1.7941e-05, -1.4156e-05,  1.2100e-05,  ..., -1.5900e-05,
         -1.0505e-05, -1.1057e-05],
        [-1.4707e-05, -1.1176e-05,  9.8348e-06,  ..., -1.3143e-05,
         -8.7097e-06, -9.5144e-06],
        [-2.0534e-05, -1.6764e-05,  1.3754e-05,  ..., -1.8537e-05,
         -1.2875e-05, -1.1876e-05]], device='cuda:0')
Loss: 1.046583890914917


Running epoch 0, step 1021, batch 1021
Sampled inputs[:2]: tensor([[   0,  221,  334,  ...,  706, 2680,  365],
        [   0,  278, 2088,  ...,   69,   14,   71]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1700e-04,  3.0474e-04,  1.2624e-04,  ...,  1.1769e-04,
         -2.3045e-04,  3.2134e-04],
        [-9.9242e-06, -7.4431e-06,  6.3218e-06,  ..., -8.7470e-06,
         -5.7556e-06, -6.2920e-06],
        [-2.2054e-05, -1.6823e-05,  1.4722e-05,  ..., -1.9148e-05,
         -1.2472e-05, -1.3798e-05],
        [-1.7807e-05, -1.3098e-05,  1.1802e-05,  ..., -1.5602e-05,
         -1.0192e-05, -1.1660e-05],
        [-2.5064e-05, -1.9804e-05,  1.6630e-05,  ..., -2.2203e-05,
         -1.5214e-05, -1.4767e-05]], device='cuda:0')
Loss: 0.9618523716926575


Running epoch 0, step 1022, batch 1022
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   266,  2025,   287],
        [    0,    14, 49601,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1422e-04,  6.5040e-04,  1.5434e-04,  ...,  2.6625e-04,
         -5.6488e-04,  2.2358e-04],
        [-1.1586e-05, -8.6352e-06,  7.3649e-06,  ..., -1.0215e-05,
         -6.6720e-06, -7.2829e-06],
        [-2.5645e-05, -1.9521e-05,  1.7107e-05,  ..., -2.2307e-05,
         -1.4454e-05, -1.5929e-05],
        [-2.0742e-05, -1.5169e-05,  1.3724e-05,  ..., -1.8209e-05,
         -1.1817e-05, -1.3478e-05],
        [-2.9087e-05, -2.2963e-05,  1.9297e-05,  ..., -2.5839e-05,
         -1.7628e-05, -1.6987e-05]], device='cuda:0')
Loss: 0.9608862400054932


Running epoch 0, step 1023, batch 1023
Sampled inputs[:2]: tensor([[   0,  368, 2035,  ...,  266, 1122,  587],
        [   0,   14,  560,  ...,   12, 8593,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3608e-04,  7.9955e-04,  2.6323e-04,  ...,  3.1801e-04,
         -8.5764e-04, -5.1319e-05],
        [-1.3322e-05, -9.9316e-06,  8.3335e-06,  ..., -1.1735e-05,
         -7.6406e-06, -8.4825e-06],
        [-2.9638e-05, -2.2560e-05,  1.9461e-05,  ..., -2.5749e-05,
         -1.6615e-05, -1.8656e-05],
        [-2.3752e-05, -1.7375e-05,  1.5482e-05,  ..., -2.0817e-05,
         -1.3448e-05, -1.5624e-05],
        [-3.3617e-05, -2.6524e-05,  2.1964e-05,  ..., -2.9832e-05,
         -2.0280e-05, -1.9938e-05]], device='cuda:0')
Loss: 0.9806077480316162
Graident accumulation at epoch 0, step 1023, batch 1023
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0285,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.5902e-05,  1.2757e-04, -1.3707e-04,  ..., -4.5495e-05,
         -1.3792e-04, -1.1368e-04],
        [-1.2957e-05, -8.9045e-06,  7.2799e-06,  ..., -1.1240e-05,
         -7.4270e-06, -8.3602e-06],
        [ 2.0995e-05,  2.8070e-05, -1.3554e-05,  ...,  2.3194e-05,
          2.9738e-05,  1.5991e-06],
        [-1.3327e-05, -1.1383e-06,  1.0108e-05,  ..., -1.2304e-05,
         -4.2460e-06, -9.6844e-06],
        [-3.5948e-05, -2.7370e-05,  2.2053e-05,  ..., -3.1035e-05,
         -2.3210e-05, -2.2244e-05]], device='cuda:0')
optimizer state dict: tensor([[5.7985e-08, 4.7749e-08, 4.8155e-08,  ..., 1.6807e-08, 1.1707e-07,
         3.4754e-08],
        [7.8471e-11, 4.2899e-11, 1.3216e-11,  ..., 5.2043e-11, 1.5009e-11,
         2.1477e-11],
        [1.5465e-09, 8.8271e-10, 2.3030e-10,  ..., 1.1677e-09, 3.1724e-10,
         3.5712e-10],
        [4.3006e-10, 3.2946e-10, 1.4069e-10,  ..., 3.8610e-10, 1.5040e-10,
         1.5943e-10],
        [3.6125e-10, 1.9319e-10, 4.9840e-11,  ..., 2.6465e-10, 5.0687e-11,
         1.0090e-10]], device='cuda:0')
optimizer state dict: 128.0
lr: [1.085561572490406e-05, 1.085561572490406e-05]
scheduler_last_epoch: 128


Running epoch 0, step 1024, batch 1024
Sampled inputs[:2]: tensor([[   0,   13, 8982,  ...,  462,  221,  494],
        [   0, 3070, 9719,  ...,  600, 4207, 4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5567e-05, -1.2851e-05, -2.0142e-04,  ...,  3.0821e-05,
         -1.9452e-04, -5.3271e-05],
        [-1.5870e-06, -1.2964e-06,  1.0505e-06,  ..., -1.4156e-06,
         -9.9093e-07, -9.7603e-07],
        [-3.5614e-06, -2.9355e-06,  2.4736e-06,  ..., -3.0994e-06,
         -2.1458e-06, -2.1756e-06],
        [-2.8163e-06, -2.2352e-06,  1.9968e-06,  ..., -2.4736e-06,
         -1.7285e-06, -1.8105e-06],
        [-3.9041e-06, -3.3528e-06,  2.7120e-06,  ..., -3.5018e-06,
         -2.5630e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9749679565429688


Running epoch 0, step 1025, batch 1025
Sampled inputs[:2]: tensor([[    0,    15,    19,  ...,    12,   287,  7897],
        [    0, 10893, 10997,  ...,   367,   616,  7903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5409e-04,  1.4400e-04, -4.3989e-04,  ..., -1.1327e-05,
         -3.4822e-04, -1.8661e-04],
        [-3.2037e-06, -2.5332e-06,  2.1532e-06,  ..., -2.8312e-06,
         -2.0638e-06, -1.9595e-06],
        [-7.1079e-06, -5.7369e-06,  5.0217e-06,  ..., -6.1542e-06,
         -4.4852e-06, -4.3213e-06],
        [-5.6773e-06, -4.3511e-06,  4.0829e-06,  ..., -4.9621e-06,
         -3.6135e-06, -3.6433e-06],
        [-7.8678e-06, -6.6012e-06,  5.5134e-06,  ..., -6.9886e-06,
         -5.3346e-06, -4.4554e-06]], device='cuda:0')
Loss: 0.9580137729644775


Running epoch 0, step 1026, batch 1026
Sampled inputs[:2]: tensor([[    0,   266,  2555,  ...,   587,    14, 14947],
        [    0,  1635,   266,  ...,   437,  3302,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6194e-04,  1.3986e-04, -3.0052e-04,  ...,  1.0846e-05,
         -3.8722e-04, -3.5172e-04],
        [-4.9621e-06, -3.8594e-06,  3.2037e-06,  ..., -4.3139e-06,
         -3.0473e-06, -3.0324e-06],
        [-1.1042e-05, -8.7768e-06,  7.4804e-06,  ..., -9.4324e-06,
         -6.6757e-06, -6.7204e-06],
        [-8.6278e-06, -6.5565e-06,  5.9232e-06,  ..., -7.4208e-06,
         -5.2452e-06, -5.4911e-06],
        [-1.2338e-05, -1.0177e-05,  8.2999e-06,  ..., -1.0803e-05,
         -8.0317e-06, -7.0333e-06]], device='cuda:0')
Loss: 0.9838619828224182


Running epoch 0, step 1027, batch 1027
Sampled inputs[:2]: tensor([[   0, 1529, 5227,  ..., 1480,  367,  925],
        [   0,   45,   17,  ...,  278, 4112,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8879e-04,  1.8242e-04, -2.7312e-04,  ...,  3.2637e-05,
         -4.4106e-04, -5.3373e-04],
        [-6.5267e-06, -5.1111e-06,  4.2543e-06,  ..., -5.6922e-06,
         -3.9935e-06, -3.9302e-06],
        [-1.4618e-05, -1.1668e-05,  9.9689e-06,  ..., -1.2517e-05,
         -8.7470e-06, -8.7470e-06],
        [-1.1504e-05, -8.7768e-06,  7.9200e-06,  ..., -9.9093e-06,
         -6.9216e-06, -7.2122e-06],
        [-1.6302e-05, -1.3500e-05,  1.1057e-05,  ..., -1.4305e-05,
         -1.0520e-05, -9.1195e-06]], device='cuda:0')
Loss: 0.9925201535224915


Running epoch 0, step 1028, batch 1028
Sampled inputs[:2]: tensor([[    0,  1497, 16170,  ...,  1888,  2350,   578],
        [    0,   494,   221,  ...,   298,  1062,  4923]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5507e-04,  1.4947e-04, -2.4189e-04,  ..., -2.6313e-05,
         -8.8406e-04, -1.0790e-03],
        [-8.2180e-06, -6.3181e-06,  5.3793e-06,  ..., -7.1898e-06,
         -4.9770e-06, -4.9733e-06],
        [-1.8433e-05, -1.4499e-05,  1.2621e-05,  ..., -1.5855e-05,
         -1.0937e-05, -1.1072e-05],
        [-1.4544e-05, -1.0908e-05,  1.0006e-05,  ..., -1.2591e-05,
         -8.6725e-06, -9.1493e-06],
        [-2.0534e-05, -1.6764e-05,  1.3977e-05,  ..., -1.8075e-05,
         -1.3113e-05, -1.1548e-05]], device='cuda:0')
Loss: 1.014643669128418


Running epoch 0, step 1029, batch 1029
Sampled inputs[:2]: tensor([[    0,   271, 28279,  ...,   367,   806,   271],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5616e-04,  2.5022e-04, -2.3383e-04,  ..., -3.7193e-05,
         -7.5718e-04, -9.9109e-04],
        [-9.9465e-06, -7.7337e-06,  6.4820e-06,  ..., -8.7321e-06,
         -6.1020e-06, -6.0014e-06],
        [-2.2337e-05, -1.7792e-05,  1.5214e-05,  ..., -1.9312e-05,
         -1.3456e-05, -1.3411e-05],
        [-1.7524e-05, -1.3337e-05,  1.1973e-05,  ..., -1.5229e-05,
         -1.0595e-05, -1.0997e-05],
        [-2.5034e-05, -2.0698e-05,  1.6958e-05,  ..., -2.2158e-05,
         -1.6242e-05, -1.4111e-05]], device='cuda:0')
Loss: 1.014181137084961


Running epoch 0, step 1030, batch 1030
Sampled inputs[:2]: tensor([[    0,  3086,   504,  ...,    14,   759,   935],
        [    0,  3231,   271,  ...,  9279,  8231, 28871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8185e-04,  3.9408e-04, -2.7557e-04,  ..., -1.1455e-05,
         -9.1090e-04, -1.1752e-03],
        [-1.1466e-05, -9.0525e-06,  7.5847e-06,  ..., -1.0140e-05,
         -7.1973e-06, -6.9328e-06],
        [-2.5705e-05, -2.0742e-05,  1.7762e-05,  ..., -2.2382e-05,
         -1.5825e-05, -1.5467e-05],
        [-2.0221e-05, -1.5616e-05,  1.4044e-05,  ..., -1.7703e-05,
         -1.2502e-05, -1.2718e-05],
        [-2.8908e-05, -2.4214e-05,  1.9863e-05,  ..., -2.5779e-05,
         -1.9148e-05, -1.6317e-05]], device='cuda:0')
Loss: 0.9699332118034363


Running epoch 0, step 1031, batch 1031
Sampled inputs[:2]: tensor([[    0,  4356, 12286,  ...,  3352,   275,  2879],
        [    0,    20, 13016,  ...,    14,  2743,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5611e-04,  3.5513e-04, -3.5142e-04,  ...,  3.4315e-07,
         -9.8714e-04, -1.1246e-03],
        [-1.3106e-05, -1.0274e-05,  8.6427e-06,  ..., -1.1571e-05,
         -8.2254e-06, -7.9609e-06],
        [-2.9400e-05, -2.3544e-05,  2.0280e-05,  ..., -2.5541e-05,
         -1.8075e-05, -1.7732e-05],
        [-2.3186e-05, -1.7747e-05,  1.6026e-05,  ..., -2.0251e-05,
         -1.4320e-05, -1.4625e-05],
        [-3.3081e-05, -2.7493e-05,  2.2694e-05,  ..., -2.9415e-05,
         -2.1845e-05, -1.8716e-05]], device='cuda:0')
Loss: 0.9962577819824219
Graident accumulation at epoch 0, step 1031, batch 1031
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0285,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3292e-04,  1.5032e-04, -1.5851e-04,  ..., -4.0911e-05,
         -2.2284e-04, -2.1477e-04],
        [-1.2972e-05, -9.0415e-06,  7.4162e-06,  ..., -1.1273e-05,
         -7.5069e-06, -8.3203e-06],
        [ 1.5956e-05,  2.2909e-05, -1.0171e-05,  ...,  1.8320e-05,
          2.4957e-05, -3.3403e-07],
        [-1.4313e-05, -2.7992e-06,  1.0700e-05,  ..., -1.3098e-05,
         -5.2534e-06, -1.0179e-05],
        [-3.5661e-05, -2.7382e-05,  2.2117e-05,  ..., -3.0873e-05,
         -2.3073e-05, -2.1891e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8236e-08, 4.7828e-08, 4.8230e-08,  ..., 1.6790e-08, 1.1792e-07,
         3.5984e-08],
        [7.8565e-11, 4.2962e-11, 1.3277e-11,  ..., 5.2125e-11, 1.5062e-11,
         2.1519e-11],
        [1.5459e-09, 8.8239e-10, 2.3048e-10,  ..., 1.1671e-09, 3.1725e-10,
         3.5708e-10],
        [4.3017e-10, 3.2944e-10, 1.4080e-10,  ..., 3.8612e-10, 1.5045e-10,
         1.5948e-10],
        [3.6198e-10, 1.9376e-10, 5.0305e-11,  ..., 2.6525e-10, 5.1114e-11,
         1.0115e-10]], device='cuda:0')
optimizer state dict: 129.0
lr: [1.0732390190252052e-05, 1.0732390190252052e-05]
scheduler_last_epoch: 129


Running epoch 0, step 1032, batch 1032
Sampled inputs[:2]: tensor([[    0, 21413,  1735,  ..., 10789, 12523,    12],
        [    0,  4834,   278,  ...,    13,  8382,   669]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0488e-05,  1.0077e-06, -8.3542e-05,  ..., -1.4485e-04,
          2.1376e-04,  1.1131e-04],
        [-1.7658e-06, -1.4603e-06,  1.1325e-06,  ..., -1.5199e-06,
         -1.1399e-06, -1.0356e-06],
        [-3.8445e-06, -3.2932e-06,  2.5928e-06,  ..., -3.2932e-06,
         -2.4587e-06, -2.2799e-06],
        [-2.9802e-06, -2.4438e-06,  1.9819e-06,  ..., -2.5481e-06,
         -1.8999e-06, -1.8403e-06],
        [-4.4107e-06, -3.9339e-06,  2.9802e-06,  ..., -3.9041e-06,
         -3.0547e-06, -2.4438e-06]], device='cuda:0')
Loss: 0.996437132358551


Running epoch 0, step 1033, batch 1033
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,   266,  3536,  ...,   266,  1883,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1638e-05, -4.7925e-05, -1.9097e-04,  ..., -2.0598e-04,
          1.7229e-04, -6.2800e-05],
        [-3.3230e-06, -2.6822e-06,  2.2426e-06,  ..., -2.9281e-06,
         -2.1085e-06, -1.9819e-06],
        [-7.2420e-06, -6.0052e-06,  5.1409e-06,  ..., -6.3181e-06,
         -4.5151e-06, -4.3511e-06],
        [-5.7966e-06, -4.5896e-06,  4.1127e-06,  ..., -5.0664e-06,
         -3.6210e-06, -3.6582e-06],
        [-8.1658e-06, -7.0930e-06,  5.7667e-06,  ..., -7.3463e-06,
         -5.5432e-06, -4.5598e-06]], device='cuda:0')
Loss: 0.9986984729766846


Running epoch 0, step 1034, batch 1034
Sampled inputs[:2]: tensor([[    0, 28926,   266,  ...,  1061,  2615,    13],
        [    0,   953,   328,  ...,  2245,    12,  1253]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1377e-04, -6.5476e-05, -1.7771e-04,  ..., -2.6814e-04,
          1.3760e-04, -1.8810e-04],
        [-4.9546e-06, -3.9190e-06,  3.3379e-06,  ..., -4.3884e-06,
         -3.0696e-06, -2.9579e-06],
        [-1.0833e-05, -8.7917e-06,  7.6741e-06,  ..., -9.4920e-06,
         -6.5863e-06, -6.4969e-06],
        [-8.6427e-06, -6.6906e-06,  6.1095e-06,  ..., -7.5996e-06,
         -5.2750e-06, -5.4538e-06],
        [-1.2130e-05, -1.0327e-05,  8.5384e-06,  ..., -1.0952e-05,
         -8.0317e-06, -6.7353e-06]], device='cuda:0')
Loss: 1.0091596841812134


Running epoch 0, step 1035, batch 1035
Sampled inputs[:2]: tensor([[   0,  759, 1128,  ...,  221,  474,  221],
        [   0,  413,   29,  ...,  818,  278,  970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6351e-05, -1.5066e-04, -9.8764e-05,  ..., -2.9507e-04,
          1.0633e-04, -4.0855e-04],
        [-6.6534e-06, -5.3197e-06,  4.3437e-06,  ..., -5.8785e-06,
         -4.2468e-06, -4.0978e-06],
        [-1.4678e-05, -1.2040e-05,  1.0073e-05,  ..., -1.2845e-05,
         -9.2089e-06, -9.0897e-06],
        [-1.1548e-05, -9.0450e-06,  7.9051e-06,  ..., -1.0133e-05,
         -7.2569e-06, -7.4655e-06],
        [-1.6600e-05, -1.4231e-05,  1.1295e-05,  ..., -1.4946e-05,
         -1.1310e-05, -9.5814e-06]], device='cuda:0')
Loss: 0.9671380519866943


Running epoch 0, step 1036, batch 1036
Sampled inputs[:2]: tensor([[   0,  328, 2097,  ...,  365, 1941,  607],
        [   0,  271,  266,  ...,  401, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2235e-04, -1.4760e-04, -1.2401e-04,  ..., -4.0668e-04,
          1.1767e-04, -3.2294e-04],
        [-8.2180e-06, -6.6012e-06,  5.3719e-06,  ..., -7.3239e-06,
         -5.2899e-06, -5.0738e-06],
        [-1.8150e-05, -1.4931e-05,  1.2487e-05,  ..., -1.5974e-05,
         -1.1459e-05, -1.1221e-05],
        [-1.4320e-05, -1.1235e-05,  9.8273e-06,  ..., -1.2666e-05,
         -9.0748e-06, -9.2760e-06],
        [-2.0653e-05, -1.7777e-05,  1.4096e-05,  ..., -1.8701e-05,
         -1.4141e-05, -1.1876e-05]], device='cuda:0')
Loss: 0.9861593842506409


Running epoch 0, step 1037, batch 1037
Sampled inputs[:2]: tensor([[    0,   400, 27972,  ..., 22726,  1871,    14],
        [    0,    28,  2973,  ...,  8762,  2134,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0684e-05, -3.0786e-04, -2.4791e-04,  ..., -4.3701e-04,
          1.9687e-04, -3.1228e-04],
        [-9.8720e-06, -7.9647e-06,  6.5193e-06,  ..., -8.7395e-06,
         -6.3404e-06, -6.0052e-06],
        [-2.1905e-05, -1.8060e-05,  1.5199e-05,  ..., -1.9163e-05,
         -1.3813e-05, -1.3351e-05],
        [-1.7211e-05, -1.3575e-05,  1.1913e-05,  ..., -1.5125e-05,
         -1.0878e-05, -1.0982e-05],
        [-2.4676e-05, -2.1279e-05,  1.6972e-05,  ..., -2.2218e-05,
         -1.6868e-05, -1.3992e-05]], device='cuda:0')
Loss: 1.0273622274398804


Running epoch 0, step 1038, batch 1038
Sampled inputs[:2]: tensor([[    0,  3084,   278,  ..., 10981,  3589,    12],
        [    0,  1927,   863,  ...,  1163,    13,  1888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2383e-05, -5.2382e-04, -2.5311e-04,  ..., -4.3638e-04,
          2.3426e-04, -3.7920e-04],
        [-1.1511e-05, -9.3281e-06,  7.6517e-06,  ..., -1.0163e-05,
         -7.3239e-06, -6.8918e-06],
        [-2.5690e-05, -2.1249e-05,  1.7896e-05,  ..., -2.2411e-05,
         -1.6034e-05, -1.5423e-05],
        [-2.0176e-05, -1.5989e-05,  1.4015e-05,  ..., -1.7658e-05,
         -1.2606e-05, -1.2666e-05],
        [-2.8759e-05, -2.4870e-05,  1.9863e-05,  ..., -2.5839e-05,
         -1.9491e-05, -1.6078e-05]], device='cuda:0')
Loss: 1.0103777647018433


Running epoch 0, step 1039, batch 1039
Sampled inputs[:2]: tensor([[    0,  1039,   259,  ...,   221,   685,   546],
        [    0,  7879,  5435,  ...,  1586, 12115,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6632e-04, -3.6687e-04, -1.7416e-04,  ..., -4.3268e-04,
          1.7464e-04, -9.5333e-05],
        [-1.3240e-05, -1.0572e-05,  8.6427e-06,  ..., -1.1690e-05,
         -8.3894e-06, -8.2180e-06],
        [-2.9653e-05, -2.4244e-05,  2.0325e-05,  ..., -2.5898e-05,
         -1.8477e-05, -1.8463e-05],
        [-2.3052e-05, -1.8045e-05,  1.5751e-05,  ..., -2.0206e-05,
         -1.4395e-05, -1.4961e-05],
        [-3.3289e-05, -2.8446e-05,  2.2635e-05,  ..., -2.9922e-05,
         -2.2516e-05, -1.9357e-05]], device='cuda:0')
Loss: 0.9885589480400085
Graident accumulation at epoch 0, step 1039, batch 1039
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0288, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0154, -0.0285,  ...,  0.0290, -0.0146, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0300e-04,  9.8603e-05, -1.6007e-04,  ..., -8.0088e-05,
         -1.8309e-04, -2.0283e-04],
        [-1.2998e-05, -9.1946e-06,  7.5388e-06,  ..., -1.1315e-05,
         -7.5951e-06, -8.3101e-06],
        [ 1.1395e-05,  1.8193e-05, -7.1212e-06,  ...,  1.3898e-05,
          2.0614e-05, -2.1469e-06],
        [-1.5187e-05, -4.3238e-06,  1.1205e-05,  ..., -1.3809e-05,
         -6.1675e-06, -1.0657e-05],
        [-3.5424e-05, -2.7488e-05,  2.2169e-05,  ..., -3.0778e-05,
         -2.3017e-05, -2.1637e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8205e-08, 4.7914e-08, 4.8212e-08,  ..., 1.6961e-08, 1.1784e-07,
         3.5957e-08],
        [7.8661e-11, 4.3030e-11, 1.3339e-11,  ..., 5.2210e-11, 1.5117e-11,
         2.1565e-11],
        [1.5452e-09, 8.8209e-10, 2.3067e-10,  ..., 1.1666e-09, 3.1727e-10,
         3.5706e-10],
        [4.3027e-10, 3.2944e-10, 1.4091e-10,  ..., 3.8614e-10, 1.5051e-10,
         1.5955e-10],
        [3.6272e-10, 1.9437e-10, 5.0767e-11,  ..., 2.6588e-10, 5.1569e-11,
         1.0143e-10]], device='cuda:0')
optimizer state dict: 130.0
lr: [1.060905273998585e-05, 1.060905273998585e-05]
scheduler_last_epoch: 130


Running epoch 0, step 1040, batch 1040
Sampled inputs[:2]: tensor([[   0,  638, 1276,  ..., 1589, 2432,  292],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.5154e-05, -1.2765e-04, -1.6512e-04,  ..., -6.4885e-05,
          1.7064e-05,  4.6724e-05],
        [-1.5572e-06, -1.2517e-06,  1.1176e-06,  ..., -1.3933e-06,
         -1.0207e-06, -9.4622e-07],
        [-3.4422e-06, -2.7716e-06,  2.5928e-06,  ..., -3.0100e-06,
         -2.1607e-06, -2.0862e-06],
        [-2.7716e-06, -2.1309e-06,  2.0862e-06,  ..., -2.4438e-06,
         -1.7583e-06, -1.7956e-06],
        [-3.8743e-06, -3.2932e-06,  2.8908e-06,  ..., -3.5167e-06,
         -2.6375e-06, -2.1458e-06]], device='cuda:0')
Loss: 0.9849448800086975


Running epoch 0, step 1041, batch 1041
Sampled inputs[:2]: tensor([[    0,    12,  9248,  ...,  2673,  4239,   292],
        [    0,  6976, 16084,  ...,    19,  9955,  3854]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9634e-05, -1.6058e-04, -1.9167e-05,  ..., -2.8887e-05,
          2.1718e-04,  4.2471e-05],
        [-3.1516e-06, -2.6450e-06,  2.2873e-06,  ..., -2.8461e-06,
         -2.1383e-06, -1.8291e-06],
        [ 3.5839e-04,  3.6293e-04, -2.8120e-04,  ...,  3.2957e-04,
          4.0378e-04,  2.0628e-04],
        [-5.5879e-06, -4.5598e-06,  4.2319e-06,  ..., -4.9770e-06,
         -3.6955e-06, -3.4496e-06],
        [-7.8082e-06, -6.9737e-06,  5.8711e-06,  ..., -7.2122e-06,
         -5.6028e-06, -4.2021e-06]], device='cuda:0')
Loss: 1.0378952026367188


Running epoch 0, step 1042, batch 1042
Sampled inputs[:2]: tensor([[   0,  401,  266,  ...,  266, 2236, 1458],
        [   0,   13, 7805,  ..., 2733,   12,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3577e-04, -2.2713e-04,  1.6827e-04,  ..., -1.6271e-04,
          3.8913e-04,  3.6842e-04],
        [-4.7982e-06, -4.0382e-06,  3.4645e-06,  ..., -4.2841e-06,
         -3.1739e-06, -2.7344e-06],
        [ 3.5474e-04,  3.5976e-04, -2.7847e-04,  ...,  3.2637e-04,
          4.0149e-04,  2.0423e-04],
        [-8.4192e-06, -6.9439e-06,  6.3330e-06,  ..., -7.4357e-06,
         -5.4538e-06, -5.0887e-06],
        [-1.1832e-05, -1.0639e-05,  8.8513e-06,  ..., -1.0863e-05,
         -8.3596e-06, -6.3181e-06]], device='cuda:0')
Loss: 1.035575270652771


Running epoch 0, step 1043, batch 1043
Sampled inputs[:2]: tensor([[   0,  944,  278,  ..., 5755,  292,  221],
        [   0,  287, 6932,  ..., 1549, 1480,  518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8177e-04, -7.4528e-05,  2.1100e-04,  ..., -2.4094e-04,
          3.3227e-04,  3.6842e-04],
        [-6.4000e-06, -5.3570e-06,  4.5896e-06,  ..., -5.7444e-06,
         -4.2245e-06, -3.6731e-06],
        [ 3.5127e-04,  3.5683e-04, -2.7589e-04,  ...,  3.2322e-04,
          3.9921e-04,  2.0217e-04],
        [-1.1235e-05, -9.2387e-06,  8.4192e-06,  ..., -9.9987e-06,
         -7.3016e-06, -6.8396e-06],
        [-1.5676e-05, -1.4052e-05,  1.1653e-05,  ..., -1.4454e-05,
         -1.1086e-05, -8.4192e-06]], device='cuda:0')
Loss: 0.986664891242981


Running epoch 0, step 1044, batch 1044
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  365, 2463,  391],
        [   0,  923,   13,  ...,  300, 8262,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5952e-04, -1.7518e-04,  1.5180e-04,  ..., -3.2441e-04,
          4.1107e-04,  4.0910e-04],
        [-8.0615e-06, -6.6832e-06,  5.7891e-06,  ..., -7.1749e-06,
         -5.2601e-06, -4.5896e-06],
        [ 3.4766e-04,  3.5383e-04, -2.7316e-04,  ...,  3.2009e-04,
          3.9694e-04,  2.0013e-04],
        [-1.4126e-05, -1.1533e-05,  1.0595e-05,  ..., -1.2472e-05,
         -9.0897e-06, -8.5160e-06],
        [-1.9580e-05, -1.7479e-05,  1.4573e-05,  ..., -1.7971e-05,
         -1.3798e-05, -1.0461e-05]], device='cuda:0')
Loss: 1.0127394199371338


Running epoch 0, step 1045, batch 1045
Sampled inputs[:2]: tensor([[    0,  1607, 26394,  ...,    19,   471,    14],
        [    0,  1712,    12,  ...,  1255,  1688,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1002e-04, -3.7724e-04,  8.6832e-05,  ..., -4.3182e-04,
          4.5500e-04,  3.1428e-04],
        [-9.6336e-06, -7.9274e-06,  6.9439e-06,  ..., -8.6203e-06,
         -6.3404e-06, -5.5321e-06],
        [ 3.4427e-04,  3.5109e-04, -2.7054e-04,  ...,  3.1701e-04,
          3.9466e-04,  1.9810e-04],
        [-1.6868e-05, -1.3664e-05,  1.2711e-05,  ..., -1.4991e-05,
         -1.0960e-05, -1.0252e-05],
        [-2.3335e-05, -2.0683e-05,  1.7464e-05,  ..., -2.1473e-05,
         -1.6525e-05, -1.2517e-05]], device='cuda:0')
Loss: 0.9920231699943542


Running epoch 0, step 1046, batch 1046
Sampled inputs[:2]: tensor([[    0,   277,   279,  ...,    12,   287,   259],
        [    0,   292, 49760,  ...,   275,  4474,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5253e-04, -6.6647e-04, -1.6345e-04,  ..., -4.1218e-04,
          4.9424e-04,  1.7321e-04],
        [-1.1317e-05, -9.1493e-06,  8.1286e-06,  ..., -1.0021e-05,
         -7.3090e-06, -6.4261e-06],
        [ 3.4066e-04,  3.4841e-04, -2.6789e-04,  ...,  3.1403e-04,
          3.9262e-04,  1.9615e-04],
        [-1.9744e-05, -1.5706e-05,  1.4827e-05,  ..., -1.7345e-05,
         -1.2569e-05, -1.1869e-05],
        [-2.7239e-05, -2.3767e-05,  2.0295e-05,  ..., -2.4825e-05,
         -1.8999e-05, -1.4439e-05]], device='cuda:0')
Loss: 0.9686842560768127


Running epoch 0, step 1047, batch 1047
Sampled inputs[:2]: tensor([[   0,  642,  287,  ...,  800,   12, 3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0652e-04, -1.0471e-03, -1.6036e-04,  ..., -3.9767e-04,
          4.0668e-04,  1.6302e-04],
        [-1.2971e-05, -1.0498e-05,  9.3207e-06,  ..., -1.1407e-05,
         -8.3297e-06, -7.3686e-06],
        [ 3.3702e-04,  3.4540e-04, -2.6519e-04,  ...,  3.1099e-04,
          3.9041e-04,  1.9405e-04],
        [-2.2680e-05, -1.8045e-05,  1.7002e-05,  ..., -1.9774e-05,
         -1.4335e-05, -1.3605e-05],
        [-3.1352e-05, -2.7314e-05,  2.3305e-05,  ..., -2.8372e-05,
         -2.1711e-05, -1.6659e-05]], device='cuda:0')
Loss: 1.009345293045044
Graident accumulation at epoch 0, step 1047, batch 1047
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0287, -0.0080,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.2048e-05, -1.5963e-05, -1.6010e-04,  ..., -1.1185e-04,
         -1.2412e-04, -1.6624e-04],
        [-1.2996e-05, -9.3249e-06,  7.7170e-06,  ..., -1.1324e-05,
         -7.6686e-06, -8.2159e-06],
        [ 4.3958e-05,  5.0914e-05, -3.2928e-05,  ...,  4.3608e-05,
          5.7594e-05,  1.7473e-05],
        [-1.5936e-05, -5.6960e-06,  1.1785e-05,  ..., -1.4406e-05,
         -6.9843e-06, -1.0952e-05],
        [-3.5017e-05, -2.7471e-05,  2.2282e-05,  ..., -3.0537e-05,
         -2.2887e-05, -2.1140e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8241e-08, 4.8963e-08, 4.8190e-08,  ..., 1.7102e-08, 1.1788e-07,
         3.5948e-08],
        [7.8751e-11, 4.3098e-11, 1.3412e-11,  ..., 5.2288e-11, 1.5171e-11,
         2.1598e-11],
        [1.6572e-09, 1.0005e-09, 3.0076e-10,  ..., 1.2622e-09, 4.6938e-10,
         3.9436e-10],
        [4.3036e-10, 3.2944e-10, 1.4106e-10,  ..., 3.8615e-10, 1.5057e-10,
         1.5957e-10],
        [3.6335e-10, 1.9492e-10, 5.1260e-11,  ..., 2.6642e-10, 5.1989e-11,
         1.0160e-10]], device='cuda:0')
optimizer state dict: 131.0
lr: [1.0485622221144482e-05, 1.0485622221144482e-05]
scheduler_last_epoch: 131
End of epoch 0 | Validation PPL: 7.239719256593545 | Learning rate: 1.0485622221144482e-05
[2025-03-25 14:02:49,457] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
End of epoch checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/end_of_epoch_checkpoint.0, AFTER epoch 0


Running epoch 1, step 1048, batch 0
Sampled inputs[:2]: tensor([[   0,   29,  413,  ..., 2001, 1027,  292],
        [   0,  659,  278,  ...,  769, 1728,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0348e-04,  4.6558e-04,  3.1077e-04,  ...,  2.9158e-05,
         -3.1655e-04, -1.9839e-04],
        [-1.6615e-06, -1.3337e-06,  8.7917e-07,  ..., -1.5050e-06,
         -1.2293e-06, -1.2070e-06],
        [-3.7551e-06, -3.1143e-06,  2.1011e-06,  ..., -3.3826e-06,
         -2.7716e-06, -2.7567e-06],
        [-2.7567e-06, -2.1905e-06,  1.5348e-06,  ..., -2.5034e-06,
         -2.0564e-06, -2.1011e-06],
        [-4.4405e-06, -3.7551e-06,  2.4438e-06,  ..., -4.0829e-06,
         -3.4422e-06, -3.0398e-06]], device='cuda:0')
Loss: 0.9371059536933899


Running epoch 1, step 1049, batch 1
Sampled inputs[:2]: tensor([[   0, 2379,   13,  ...,  287,  259, 2193],
        [   0,  221,  380,  ..., 3990,  717,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2306e-04,  5.7544e-04,  3.2612e-04,  ..., -1.6384e-05,
         -3.9412e-04, -2.5548e-04],
        [-3.2857e-06, -2.4885e-06,  1.9819e-06,  ..., -2.9206e-06,
         -2.2799e-06, -2.2575e-06],
        [-7.3314e-06, -5.6922e-06,  4.6641e-06,  ..., -6.4373e-06,
         -5.0068e-06, -5.0515e-06],
        [-5.4836e-06, -4.0606e-06,  3.5018e-06,  ..., -4.8727e-06,
         -3.7849e-06, -3.9488e-06],
        [-8.4937e-06, -6.8247e-06,  5.2899e-06,  ..., -7.6443e-06,
         -6.1840e-06, -5.4240e-06]], device='cuda:0')
Loss: 0.9739344716072083


Running epoch 1, step 1050, batch 2
Sampled inputs[:2]: tensor([[    0,  4823,    12,  ...,  1756,  3406,   300],
        [    0,   271, 12472,  ...,   374,    29,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1161e-04,  4.5509e-04,  3.2153e-04,  ..., -9.0909e-05,
         -2.6378e-04,  1.3100e-05],
        [-4.8354e-06, -3.7625e-06,  3.0771e-06,  ..., -4.3213e-06,
         -3.3602e-06, -3.1367e-06],
        [ 1.8206e-04,  2.5339e-04, -1.9717e-04,  ...,  1.4104e-04,
          1.7619e-04,  3.8872e-05],
        [-8.1956e-06, -6.2361e-06,  5.4985e-06,  ..., -7.3016e-06,
         -5.6326e-06, -5.5656e-06],
        [-1.2338e-05, -1.0207e-05,  8.0913e-06,  ..., -1.1191e-05,
         -9.0301e-06, -7.4506e-06]], device='cuda:0')
Loss: 1.0198256969451904


Running epoch 1, step 1051, batch 3
Sampled inputs[:2]: tensor([[    0,   445,   749,  ...,   850,  1028,    13],
        [    0,  1265,  1545,  ...,   292, 36667, 36197]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8248e-05,  2.1403e-04,  1.6364e-04,  ..., -8.7441e-05,
         -1.4336e-04,  1.0409e-04],
        [-6.4149e-06, -5.0664e-06,  4.1649e-06,  ..., -5.7220e-06,
         -4.3884e-06, -4.0643e-06],
        [ 1.7860e-04,  2.5046e-04, -1.9468e-04,  ...,  1.3800e-04,
          1.7399e-04,  3.6845e-05],
        [-1.0833e-05, -8.3819e-06,  7.3984e-06,  ..., -9.6262e-06,
         -7.3388e-06, -7.2196e-06],
        [-1.6138e-05, -1.3560e-05,  1.0774e-05,  ..., -1.4633e-05,
         -1.1638e-05, -9.4771e-06]], device='cuda:0')
Loss: 0.9960887432098389


Running epoch 1, step 1052, batch 4
Sampled inputs[:2]: tensor([[    0,   287, 30256,  ...,   287,  8137, 13021],
        [    0,   259,  2416,  ..., 14474,    12,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3288e-04,  1.2578e-04,  1.4442e-04,  ..., -1.8839e-04,
         -9.0096e-05, -8.0863e-05],
        [-8.0466e-06, -6.1989e-06,  5.4091e-06,  ..., -7.0781e-06,
         -5.3123e-06, -4.9546e-06],
        [ 1.7505e-04,  2.4794e-04, -1.9186e-04,  ...,  1.3509e-04,
          1.7202e-04,  3.4923e-05],
        [-1.3798e-05, -1.0379e-05,  9.7528e-06,  ..., -1.2055e-05,
         -8.9705e-06, -8.9109e-06],
        [-1.9908e-05, -1.6391e-05,  1.3739e-05,  ..., -1.7837e-05,
         -1.3947e-05, -1.1347e-05]], device='cuda:0')
Loss: 0.9976223111152649


Running epoch 1, step 1053, batch 5
Sampled inputs[:2]: tensor([[   0,  508,  586,  ...,  445,   29,  445],
        [   0, 1064, 1042,  ...,   12,  259, 4754]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2667e-04,  3.8939e-05,  2.5083e-04,  ..., -3.4992e-04,
          4.8654e-04,  6.6067e-04],
        [-9.6038e-06, -7.4580e-06,  6.4448e-06,  ..., -8.4713e-06,
         -6.4000e-06, -6.0275e-06],
        [ 1.7146e-04,  2.4499e-04, -1.8936e-04,  ...,  1.3196e-04,
          1.6962e-04,  3.2509e-05],
        [-1.6585e-05, -1.2554e-05,  1.1660e-05,  ..., -1.4514e-05,
         -1.0863e-05, -1.0908e-05],
        [-2.3901e-05, -1.9804e-05,  1.6510e-05,  ..., -2.1413e-05,
         -1.6823e-05, -1.3866e-05]], device='cuda:0')
Loss: 1.0223392248153687


Running epoch 1, step 1054, batch 6
Sampled inputs[:2]: tensor([[    0,   352, 13159,  ...,  3111,   394,    14],
        [    0,   298,   374,  ...,   298,   413,    28]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8441e-04,  1.9488e-04,  4.2635e-04,  ..., -2.8181e-04,
          6.3533e-04,  7.9629e-04],
        [-1.1213e-05, -8.7097e-06,  7.4580e-06,  ..., -9.9018e-06,
         -7.4580e-06, -7.0855e-06],
        [ 1.6799e-04,  2.4220e-04, -1.8702e-04,  ...,  1.2891e-04,
          1.6735e-04,  3.0259e-05],
        [-1.9312e-05, -1.4640e-05,  1.3500e-05,  ..., -1.6913e-05,
         -1.2629e-05, -1.2740e-05],
        [-2.7865e-05, -2.3127e-05,  1.9148e-05,  ..., -2.4989e-05,
         -1.9640e-05, -1.6235e-05]], device='cuda:0')
Loss: 0.9709994792938232


Running epoch 1, step 1055, batch 7
Sampled inputs[:2]: tensor([[    0,   352,   721,  ...,   634, 17642,   278],
        [    0,  7110,   437,  ...,   266,  6724,  2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1349e-04,  1.6253e-04,  5.5904e-04,  ..., -4.6320e-04,
          7.7547e-04,  9.5082e-04],
        [-1.2793e-05, -1.0028e-05,  8.6203e-06,  ..., -1.1340e-05,
         -8.4415e-06, -8.0317e-06],
        [ 1.6446e-04,  2.3919e-04, -1.8432e-04,  ...,  1.2573e-04,
          1.6518e-04,  2.8143e-05],
        [-2.2084e-05, -1.6920e-05,  1.5616e-05,  ..., -1.9416e-05,
         -1.4327e-05, -1.4476e-05],
        [-3.1769e-05, -2.6628e-05,  2.2098e-05,  ..., -2.8610e-05,
         -2.2262e-05, -1.8425e-05]], device='cuda:0')
Loss: 1.0072706937789917
Graident accumulation at epoch 1, step 1055, batch 7
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4494e-05,  1.8866e-06, -8.8187e-05,  ..., -1.4698e-04,
         -3.4159e-05, -5.4536e-05],
        [-1.2975e-05, -9.3952e-06,  7.8073e-06,  ..., -1.1326e-05,
         -7.7459e-06, -8.1975e-06],
        [ 5.6008e-05,  6.9741e-05, -4.8067e-05,  ...,  5.1820e-05,
          6.8352e-05,  1.8540e-05],
        [-1.6551e-05, -6.8184e-06,  1.2168e-05,  ..., -1.4907e-05,
         -7.7186e-06, -1.1304e-05],
        [-3.4692e-05, -2.7387e-05,  2.2264e-05,  ..., -3.0345e-05,
         -2.2824e-05, -2.0868e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8281e-08, 4.8940e-08, 4.8454e-08,  ..., 1.7299e-08, 1.1837e-07,
         3.6816e-08],
        [7.8836e-11, 4.3155e-11, 1.3473e-11,  ..., 5.2364e-11, 1.5227e-11,
         2.1641e-11],
        [1.6826e-09, 1.0567e-09, 3.3444e-10,  ..., 1.2767e-09, 4.9619e-10,
         3.9476e-10],
        [4.3042e-10, 3.2939e-10, 1.4116e-10,  ..., 3.8614e-10, 1.5062e-10,
         1.5962e-10],
        [3.6399e-10, 1.9544e-10, 5.1697e-11,  ..., 2.6698e-10, 5.2433e-11,
         1.0184e-10]], device='cuda:0')
optimizer state dict: 132.0
lr: [1.0362117494988668e-05, 1.0362117494988668e-05]
scheduler_last_epoch: 132


Running epoch 1, step 1056, batch 8
Sampled inputs[:2]: tensor([[   0, 1927,  863,  ..., 1163,   13, 1888],
        [   0,  795, 3185,  ...,   14, 1671,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0729e-06, -4.5327e-05,  1.2417e-04,  ..., -5.8655e-05,
         -5.4534e-05, -1.7893e-05],
        [-1.5795e-06, -1.2591e-06,  1.1325e-06,  ..., -1.3411e-06,
         -9.2015e-07, -8.6799e-07],
        [-3.5167e-06, -2.8461e-06,  2.6375e-06,  ..., -2.9802e-06,
         -2.0117e-06, -1.9521e-06],
        [-2.8014e-06, -2.1905e-06,  2.1011e-06,  ..., -2.3544e-06,
         -1.5870e-06, -1.6317e-06],
        [-3.7104e-06, -3.1739e-06,  2.7418e-06,  ..., -3.2485e-06,
         -2.3544e-06, -1.8999e-06]], device='cuda:0')
Loss: 0.98435378074646


Running epoch 1, step 1057, batch 9
Sampled inputs[:2]: tensor([[   0, 1477, 3205,  ..., 6441, 9363,  271],
        [   0,  445,   18,  ..., 1478,  578,  494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8563e-05,  1.0568e-04, -2.0523e-07,  ..., -7.9947e-05,
         -1.6279e-04,  1.8611e-04],
        [-3.3006e-06, -2.5555e-06,  2.1756e-06,  ..., -2.7642e-06,
         -2.0377e-06, -2.0303e-06],
        [-7.4208e-06, -5.9456e-06,  5.1111e-06,  ..., -6.2287e-06,
         -4.5896e-06, -4.6492e-06],
        [-5.6773e-06, -4.3511e-06,  3.9190e-06,  ..., -4.7237e-06,
         -3.4496e-06, -3.6582e-06],
        [-8.1509e-06, -6.8247e-06,  5.4985e-06,  ..., -7.0632e-06,
         -5.4985e-06, -4.8205e-06]], device='cuda:0')
Loss: 0.9951649308204651


Running epoch 1, step 1058, batch 10
Sampled inputs[:2]: tensor([[    0, 10705,   401,  ...,   768,  2392,   368],
        [    0,  1008,   266,  ...,  1941,   437,  1626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1826e-04,  9.4372e-05,  2.4587e-05,  ..., -1.0995e-04,
         -4.0161e-05,  2.1342e-04],
        [-4.9770e-06, -3.7178e-06,  3.3006e-06,  ..., -4.1723e-06,
         -3.0436e-06, -3.0734e-06],
        [-1.1116e-05, -8.5980e-06,  7.7188e-06,  ..., -9.3281e-06,
         -6.8098e-06, -6.9588e-06],
        [-8.5086e-06, -6.2883e-06,  5.9158e-06,  ..., -7.0781e-06,
         -5.1409e-06, -5.5060e-06],
        [-1.2144e-05, -9.8497e-06,  8.2850e-06,  ..., -1.0520e-05,
         -8.1211e-06, -7.1600e-06]], device='cuda:0')
Loss: 0.982420802116394


Running epoch 1, step 1059, batch 11
Sampled inputs[:2]: tensor([[    0,  1869,   596,  ..., 13055, 17051,   578],
        [    0,    18,    14,  ...,   380,   981,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7373e-04, -3.2306e-06,  1.3773e-04,  ..., -8.8731e-05,
          1.0311e-05,  3.5514e-04],
        [-6.6236e-06, -4.8578e-06,  4.4182e-06,  ..., -5.5432e-06,
         -4.0717e-06, -4.0643e-06],
        [-1.4707e-05, -1.1161e-05,  1.0297e-05,  ..., -1.2279e-05,
         -9.0450e-06, -9.0748e-06],
        [-1.1295e-05, -8.1733e-06,  7.9125e-06,  ..., -9.3877e-06,
         -6.8843e-06, -7.2643e-06],
        [-1.6049e-05, -1.2785e-05,  1.1057e-05,  ..., -1.3828e-05,
         -1.0744e-05, -9.2611e-06]], device='cuda:0')
Loss: 0.9827629923820496


Running epoch 1, step 1060, batch 12
Sampled inputs[:2]: tensor([[    0,   266,  3536,  ...,   266,  1883,   266],
        [    0,    67,   695,  ...,   437,   266, 44563]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4855e-04, -5.9863e-05,  1.9117e-04,  ..., -1.6963e-04,
          1.0276e-04,  2.3583e-04],
        [-8.2627e-06, -6.0424e-06,  5.5730e-06,  ..., -6.9216e-06,
         -5.0217e-06, -4.9695e-06],
        [-1.8284e-05, -1.3813e-05,  1.2919e-05,  ..., -1.5303e-05,
         -1.1101e-05, -1.1086e-05],
        [-1.4111e-05, -1.0170e-05,  9.9838e-06,  ..., -1.1742e-05,
         -8.4788e-06, -8.9034e-06],
        [-1.9804e-05, -1.5706e-05,  1.3769e-05,  ..., -1.7092e-05,
         -1.3098e-05, -1.1213e-05]], device='cuda:0')
Loss: 1.006822943687439


Running epoch 1, step 1061, batch 13
Sampled inputs[:2]: tensor([[    0,  1890,   278,  ...,   578,    72,   815],
        [    0,  2372,  1319,  ...,  1253,   292, 34166]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5116e-04,  4.0144e-05,  2.0317e-05,  ..., -1.0816e-04,
          2.7652e-04,  3.4208e-04],
        [-9.8869e-06, -7.0259e-06,  6.7353e-06,  ..., -8.2403e-06,
         -5.8971e-06, -5.8822e-06],
        [-2.1696e-05, -1.5959e-05,  1.5497e-05,  ..., -1.8045e-05,
         -1.2904e-05, -1.2979e-05],
        [-1.6898e-05, -1.1817e-05,  1.2115e-05,  ..., -1.3992e-05,
         -9.9540e-06, -1.0572e-05],
        [-2.3440e-05, -1.8179e-05,  1.6466e-05,  ..., -2.0117e-05,
         -1.5244e-05, -1.3024e-05]], device='cuda:0')
Loss: 0.9571160078048706


Running epoch 1, step 1062, batch 14
Sampled inputs[:2]: tensor([[   0,  341, 2802,  ..., 1798,   12,  266],
        [   0,   12,  287,  ..., 2336,  221,  334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7755e-05,  9.7919e-05,  1.1861e-05,  ..., -1.0511e-04,
          5.1083e-04,  2.4640e-04],
        [-1.1414e-05, -8.2627e-06,  7.6890e-06,  ..., -9.6634e-06,
         -6.9700e-06, -6.8955e-06],
        [-2.5049e-05, -1.8671e-05,  1.7732e-05,  ..., -2.1055e-05,
         -1.5154e-05, -1.5140e-05],
        [-1.9461e-05, -1.3843e-05,  1.3791e-05,  ..., -1.6347e-05,
         -1.1742e-05, -1.2353e-05],
        [-2.7210e-05, -2.1383e-05,  1.8969e-05,  ..., -2.3589e-05,
         -1.7971e-05, -1.5229e-05]], device='cuda:0')
Loss: 0.985434353351593


Running epoch 1, step 1063, batch 15
Sampled inputs[:2]: tensor([[   0,   14,  381,  ..., 7106,  287,  266],
        [   0, 3353,   17,  ...,  596,   12,  461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0284e-04,  1.3367e-04,  2.5448e-05,  ..., -1.5634e-04,
          5.3319e-04,  2.7371e-04],
        [-1.3031e-05, -9.4548e-06,  8.8662e-06,  ..., -1.1034e-05,
         -7.9460e-06, -7.7710e-06],
        [ 5.3688e-05,  3.5450e-05, -4.2611e-06,  ...,  4.4539e-05,
          3.5867e-05,  2.4457e-05],
        [-2.2322e-05, -1.5900e-05,  1.5952e-05,  ..., -1.8746e-05,
         -1.3418e-05, -1.4000e-05],
        [-3.1054e-05, -2.4468e-05,  2.1815e-05,  ..., -2.6956e-05,
         -2.0474e-05, -1.7166e-05]], device='cuda:0')
Loss: 1.026032567024231
Graident accumulation at epoch 1, step 1063, batch 15
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0022,  0.0233, -0.0192],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.7608e-06,  1.5065e-05, -7.6824e-05,  ..., -1.4792e-04,
          2.2576e-05, -2.1711e-05],
        [-1.2981e-05, -9.4012e-06,  7.9132e-06,  ..., -1.1297e-05,
         -7.7659e-06, -8.1548e-06],
        [ 5.5776e-05,  6.6312e-05, -4.3687e-05,  ...,  5.1092e-05,
          6.5104e-05,  1.9131e-05],
        [-1.7128e-05, -7.7265e-06,  1.2546e-05,  ..., -1.5291e-05,
         -8.2886e-06, -1.1574e-05],
        [-3.4328e-05, -2.7095e-05,  2.2219e-05,  ..., -3.0006e-05,
         -2.2589e-05, -2.0498e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8264e-08, 4.8909e-08, 4.8406e-08,  ..., 1.7307e-08, 1.1853e-07,
         3.6854e-08],
        [7.8927e-11, 4.3201e-11, 1.3538e-11,  ..., 5.2433e-11, 1.5275e-11,
         2.1679e-11],
        [1.6838e-09, 1.0569e-09, 3.3412e-10,  ..., 1.2774e-09, 4.9698e-10,
         3.9496e-10],
        [4.3048e-10, 3.2932e-10, 1.4128e-10,  ..., 3.8610e-10, 1.5065e-10,
         1.5966e-10],
        [3.6459e-10, 1.9584e-10, 5.2121e-11,  ..., 2.6744e-10, 5.2800e-11,
         1.0203e-10]], device='cuda:0')
optimizer state dict: 133.0
lr: [1.023855743411865e-05, 1.023855743411865e-05]
scheduler_last_epoch: 133


Running epoch 1, step 1064, batch 16
Sampled inputs[:2]: tensor([[    0,    14,  1075,  ..., 22182,  5948,  8401],
        [    0, 11694,   292,  ...,   328,  1654,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2485e-05,  4.4575e-04,  3.7977e-05,  ..., -4.6737e-05,
          8.5827e-05,  1.6543e-04],
        [-1.5646e-06, -1.2368e-06,  1.0058e-06,  ..., -1.3933e-06,
         -1.1027e-06, -9.5367e-07],
        [-3.5167e-06, -2.8312e-06,  2.3544e-06,  ..., -3.0845e-06,
         -2.4289e-06, -2.1458e-06],
        [-2.7716e-06, -2.1458e-06,  1.8403e-06,  ..., -2.4289e-06,
         -1.9222e-06, -1.7807e-06],
        [-3.7253e-06, -3.1292e-06,  2.4438e-06,  ..., -3.3677e-06,
         -2.7567e-06, -2.1309e-06]], device='cuda:0')
Loss: 0.9728071689605713


Running epoch 1, step 1065, batch 17
Sampled inputs[:2]: tensor([[    0, 12324,  7368,  ...,   365,   726,  3595],
        [    0,  2523, 10780,  ...,  1041,    26, 13745]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1450e-04,  4.6225e-04,  3.7977e-05,  ..., -5.0602e-05,
          2.2351e-04,  2.4519e-04],
        [-3.1888e-06, -2.4214e-06,  2.0787e-06,  ..., -2.8238e-06,
         -2.1532e-06, -1.8962e-06],
        [ 3.5112e-05,  8.9891e-05, -9.6313e-06,  ...,  3.9037e-05,
          7.1092e-05,  7.9404e-06],
        [-5.5283e-06, -4.1127e-06,  3.7402e-06,  ..., -4.8429e-06,
         -3.6731e-06, -3.4720e-06],
        [-7.5698e-06, -6.1393e-06,  5.0813e-06,  ..., -6.8247e-06,
         -5.3942e-06, -4.1872e-06]], device='cuda:0')
Loss: 1.0154248476028442


Running epoch 1, step 1066, batch 18
Sampled inputs[:2]: tensor([[    0,  1380,   342,  ...,  3904,   259,   624],
        [    0,  1665,  6306,  ...,   300, 10204,   582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0062e-04,  2.8785e-04,  5.2895e-05,  ..., -6.1360e-05,
          4.4508e-04,  3.0484e-04],
        [-4.8578e-06, -3.5688e-06,  3.2037e-06,  ..., -4.2170e-06,
         -3.1665e-06, -2.8089e-06],
        [ 3.1491e-05,  8.7358e-05, -7.0534e-06,  ...,  3.6072e-05,
          6.8961e-05,  5.9734e-06],
        [-8.3745e-06, -6.0126e-06,  5.7518e-06,  ..., -7.1824e-06,
         -5.3719e-06, -5.1260e-06],
        [-1.1474e-05, -9.0152e-06,  7.8082e-06,  ..., -1.0133e-05,
         -7.8976e-06, -6.1244e-06]], device='cuda:0')
Loss: 1.003694772720337


Running epoch 1, step 1067, batch 19
Sampled inputs[:2]: tensor([[    0,    30,  1869,  ...,  4998, 44266,    12],
        [    0,   292, 17190,  ...,  3078,     9,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9964e-04,  3.8558e-04,  1.8313e-04,  ..., -8.7333e-05,
          3.7008e-04,  4.9915e-04],
        [-6.5863e-06, -4.7684e-06,  4.3288e-06,  ..., -5.6997e-06,
         -4.2394e-06, -3.8594e-06],
        [ 2.7691e-05,  8.4602e-05, -4.4755e-06,  ...,  3.2838e-05,
          6.6607e-05,  3.6637e-06],
        [-1.1265e-05, -8.0094e-06,  7.7039e-06,  ..., -9.6560e-06,
         -7.1675e-06, -6.9514e-06],
        [-1.5646e-05, -1.2189e-05,  1.0610e-05,  ..., -1.3798e-05,
         -1.0684e-05, -8.5235e-06]], device='cuda:0')
Loss: 1.0181504487991333


Running epoch 1, step 1068, batch 20
Sampled inputs[:2]: tensor([[   0, 7692,   12,  ...,  266, 2042,  278],
        [   0, 1550, 2013,  ..., 9970,  638, 6482]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7170e-05,  5.3115e-04,  2.5846e-04,  ..., -1.1161e-04,
          3.7008e-04,  4.9194e-04],
        [-8.1211e-06, -5.7593e-06,  5.4762e-06,  ..., -6.9812e-06,
         -5.1036e-06, -4.7348e-06],
        [ 2.4383e-05,  8.2411e-05, -1.8827e-06,  ...,  3.0111e-05,
          6.4781e-05,  1.7862e-06],
        [-1.3977e-05, -9.6858e-06,  9.8646e-06,  ..., -1.1906e-05,
         -8.6725e-06, -8.6054e-06],
        [-1.9148e-05, -1.4648e-05,  1.3307e-05,  ..., -1.6749e-05,
         -1.2785e-05, -1.0319e-05]], device='cuda:0')
Loss: 0.9482479095458984


Running epoch 1, step 1069, batch 21
Sampled inputs[:2]: tensor([[    0,   300,  7239,  ...,  2283,  4890,    14],
        [    0,   587,   300,  ...,  4325,   278, 12564]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2062e-04,  5.7164e-04,  2.8415e-04,  ..., -1.2948e-04,
          2.2573e-04,  2.9993e-04],
        [-9.7305e-06, -6.8247e-06,  6.6310e-06,  ..., -8.2999e-06,
         -6.0536e-06, -5.6140e-06],
        [ 2.0822e-05,  8.0012e-05,  7.8461e-07,  ...,  2.7220e-05,
          6.2710e-05, -1.6588e-07],
        [-1.6779e-05, -1.1459e-05,  1.1981e-05,  ..., -1.4171e-05,
         -1.0297e-05, -1.0222e-05],
        [-2.2843e-05, -1.7285e-05,  1.6034e-05,  ..., -1.9833e-05,
         -1.5110e-05, -1.2167e-05]], device='cuda:0')
Loss: 0.9854531288146973


Running epoch 1, step 1070, batch 22
Sampled inputs[:2]: tensor([[   0, 1806,  319,  ..., 3427,  278,  266],
        [   0, 4599, 9005,  ...,  809,   13, 1875]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5281e-05,  5.7054e-04,  2.9641e-04,  ..., -1.0423e-04,
          2.3467e-04,  4.6052e-04],
        [-1.1362e-05, -7.9647e-06,  7.7337e-06,  ..., -9.6560e-06,
         -6.9961e-06, -6.5602e-06],
        [ 1.7156e-05,  7.7389e-05,  3.3923e-06,  ...,  2.4195e-05,
          6.0624e-05, -2.2967e-06],
        [-1.9550e-05, -1.3351e-05,  1.3947e-05,  ..., -1.6451e-05,
         -1.1869e-05, -1.1921e-05],
        [-2.6688e-05, -2.0176e-05,  1.8731e-05,  ..., -2.3112e-05,
         -1.7494e-05, -1.4208e-05]], device='cuda:0')
Loss: 0.9855656027793884


Running epoch 1, step 1071, batch 23
Sampled inputs[:2]: tensor([[    0,   266,  1176,  ...,   199, 17791,  3662],
        [    0,  2734,  2703,  ...,  7851,   280,  1713]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.5361e-06,  4.5995e-04,  2.7917e-04,  ..., -8.4657e-05,
          2.0929e-04,  3.3955e-04],
        [-1.2994e-05, -9.1270e-06,  8.8140e-06,  ..., -1.1034e-05,
         -8.0019e-06, -7.5586e-06],
        [ 1.3401e-05,  7.4677e-05,  6.0000e-06,  ...,  2.1066e-05,
          5.8359e-05, -4.5617e-06],
        [-2.2337e-05, -1.5303e-05,  1.5870e-05,  ..., -1.8790e-05,
         -1.3553e-05, -1.3687e-05],
        [-3.0652e-05, -2.3142e-05,  2.1473e-05,  ..., -2.6509e-05,
         -2.0072e-05, -1.6414e-05]], device='cuda:0')
Loss: 0.9951270818710327
Graident accumulation at epoch 1, step 1071, batch 23
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0233, -0.0192],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0157,  0.0155, -0.0285,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4383e-06,  5.9553e-05, -4.1225e-05,  ..., -1.4159e-04,
          4.1248e-05,  1.4415e-05],
        [-1.2982e-05, -9.3738e-06,  8.0033e-06,  ..., -1.1270e-05,
         -7.7895e-06, -8.0952e-06],
        [ 5.1538e-05,  6.7149e-05, -3.8718e-05,  ...,  4.8089e-05,
          6.4429e-05,  1.6762e-05],
        [-1.7649e-05, -8.4842e-06,  1.2879e-05,  ..., -1.5641e-05,
         -8.8150e-06, -1.1785e-05],
        [-3.3961e-05, -2.6699e-05,  2.2144e-05,  ..., -2.9656e-05,
         -2.2338e-05, -2.0090e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8206e-08, 4.9072e-08, 4.8436e-08,  ..., 1.7296e-08, 1.1846e-07,
         3.6932e-08],
        [7.9017e-11, 4.3241e-11, 1.3603e-11,  ..., 5.2503e-11, 1.5324e-11,
         2.1715e-11],
        [1.6823e-09, 1.0614e-09, 3.3382e-10,  ..., 1.2766e-09, 4.9989e-10,
         3.9459e-10],
        [4.3055e-10, 3.2922e-10, 1.4139e-10,  ..., 3.8607e-10, 1.5068e-10,
         1.5969e-10],
        [3.6517e-10, 1.9618e-10, 5.2530e-11,  ..., 2.6787e-10, 5.3150e-11,
         1.0220e-10]], device='cuda:0')
optimizer state dict: 134.0
lr: [1.01149609195903e-05, 1.01149609195903e-05]
scheduler_last_epoch: 134


Running epoch 1, step 1072, batch 24
Sampled inputs[:2]: tensor([[    0,  4494,    12,  ...,   341,  1619,    12],
        [    0,  1040,   287,  ...,    14, 10209,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5815e-04,  1.1571e-04,  1.5355e-04,  ..., -2.4981e-05,
          7.2435e-05,  1.8842e-04],
        [-1.6168e-06, -1.1474e-06,  9.9838e-07,  ..., -1.4007e-06,
         -1.0580e-06, -9.6858e-07],
        [-3.4720e-06, -2.5183e-06,  2.2799e-06,  ..., -2.9504e-06,
         -2.2352e-06, -2.0266e-06],
        [-2.7120e-06, -1.8477e-06,  1.7732e-06,  ..., -2.3246e-06,
         -1.7732e-06, -1.7136e-06],
        [-3.7700e-06, -2.8759e-06,  2.4438e-06,  ..., -3.2932e-06,
         -2.5928e-06, -2.0117e-06]], device='cuda:0')
Loss: 0.9737356901168823


Running epoch 1, step 1073, batch 25
Sampled inputs[:2]: tensor([[   0,  471,   12,  ...,   13, 9909, 2673],
        [   0,  472,  346,  ..., 9161,  300, 4460]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1064e-04,  3.3437e-04,  5.8727e-05,  ...,  1.0896e-04,
         -2.8582e-04,  2.1843e-04],
        [-3.3230e-06, -2.2277e-06,  2.1309e-06,  ..., -2.7791e-06,
         -2.0042e-06, -1.8962e-06],
        [-7.1377e-06, -4.9323e-06,  4.8280e-06,  ..., -5.9009e-06,
         -4.2766e-06, -4.0084e-06],
        [-5.6624e-06, -3.6433e-06,  3.8147e-06,  ..., -4.6790e-06,
         -3.3826e-06, -3.3975e-06],
        [-7.7039e-06, -5.6326e-06,  5.1409e-06,  ..., -6.5416e-06,
         -4.9770e-06, -3.9786e-06]], device='cuda:0')
Loss: 1.0054285526275635


Running epoch 1, step 1074, batch 26
Sampled inputs[:2]: tensor([[    0,   344,  8133,  ...,   368,  1119,  5539],
        [    0, 23749, 27341,  ..., 34110,   342,  9672]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8413e-04,  3.8721e-04,  4.6560e-05,  ...,  1.1583e-04,
         -4.9627e-04,  2.5209e-04],
        [-5.0068e-06, -3.3081e-06,  3.1516e-06,  ..., -4.1798e-06,
         -2.9653e-06, -3.0212e-06],
        [-1.0952e-05, -7.4059e-06,  7.2569e-06,  ..., -9.0152e-06,
         -6.3926e-06, -6.5118e-06],
        [-8.5235e-06, -5.4166e-06,  5.6326e-06,  ..., -7.0184e-06,
         -4.9695e-06, -5.3495e-06],
        [-1.1846e-05, -8.4192e-06,  7.7337e-06,  ..., -1.0014e-05,
         -7.4506e-06, -6.5416e-06]], device='cuda:0')
Loss: 0.9861706495285034


Running epoch 1, step 1075, batch 27
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,   287, 10086,   300],
        [    0,  2029,    13,  ...,    12,  4536,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9912e-04,  4.1174e-04,  5.1602e-05,  ..., -4.6620e-05,
         -3.6148e-05,  5.0595e-04],
        [-6.6906e-06, -4.4182e-06,  4.1574e-06,  ..., -5.6699e-06,
         -4.0233e-06, -4.0270e-06],
        [-1.4588e-05, -9.8944e-06,  9.5814e-06,  ..., -1.2189e-05,
         -8.6874e-06, -8.6278e-06],
        [-1.1295e-05, -7.2047e-06,  7.3537e-06,  ..., -9.4622e-06,
         -6.7055e-06, -7.0632e-06],
        [-1.5751e-05, -1.1206e-05,  1.0222e-05,  ..., -1.3500e-05,
         -1.0058e-05, -8.6576e-06]], device='cuda:0')
Loss: 1.0193651914596558


Running epoch 1, step 1076, batch 28
Sampled inputs[:2]: tensor([[    0,  3605,  2572,  ...,   300,   259,  1513],
        [    0, 15033,   278,  ...,   266,  2937,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.6254e-04,  5.7500e-04,  1.2330e-04,  ..., -6.2540e-05,
         -1.4797e-04,  3.7237e-04],
        [-8.2329e-06, -5.3942e-06,  5.2601e-06,  ..., -7.0184e-06,
         -4.9621e-06, -4.9062e-06],
        [-1.7911e-05, -1.2040e-05,  1.2085e-05,  ..., -1.5020e-05,
         -1.0625e-05, -1.0483e-05],
        [-1.3918e-05, -8.7619e-06,  9.3356e-06,  ..., -1.1712e-05,
         -8.2552e-06, -8.6427e-06],
        [-1.9267e-05, -1.3649e-05,  1.2815e-05,  ..., -1.6600e-05,
         -1.2308e-05, -1.0431e-05]], device='cuda:0')
Loss: 0.9554893970489502


Running epoch 1, step 1077, batch 29
Sampled inputs[:2]: tensor([[   0, 3308,  259,  ...,   14, 6349, 1389],
        [   0,  285,   53,  ...,  259, 5012, 3037]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4894e-04,  7.2102e-04,  3.2082e-04,  ..., -1.0812e-04,
         -2.8601e-05,  4.8594e-04],
        [-9.9242e-06, -6.6087e-06,  6.2808e-06,  ..., -8.4564e-06,
         -6.0797e-06, -5.9642e-06],
        [-2.1666e-05, -1.4842e-05,  1.4469e-05,  ..., -1.8209e-05,
         -1.3098e-05, -1.2808e-05],
        [-1.6734e-05, -1.0759e-05,  1.1094e-05,  ..., -1.4111e-05,
         -1.0125e-05, -1.0483e-05],
        [-2.3320e-05, -1.6823e-05,  1.5378e-05,  ..., -2.0146e-05,
         -1.5169e-05, -1.2800e-05]], device='cuda:0')
Loss: 0.9951419830322266


Running epoch 1, step 1078, batch 30
Sampled inputs[:2]: tensor([[    0,   287, 11638,  ...,    17,   221,   733],
        [    0,  3235,   471,  ...,  1967,  4273,  2738]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5199e-04,  8.6552e-04,  3.9159e-04,  ..., -9.0835e-05,
          2.9354e-05,  6.3056e-04],
        [-1.1556e-05, -7.7188e-06,  7.2420e-06,  ..., -9.8571e-06,
         -7.0855e-06, -6.8843e-06],
        [-2.5213e-05, -1.7345e-05,  1.6704e-05,  ..., -2.1234e-05,
         -1.5289e-05, -1.4775e-05],
        [-1.9580e-05, -1.2666e-05,  1.2845e-05,  ..., -1.6540e-05,
         -1.1884e-05, -1.2159e-05],
        [-2.6956e-05, -1.9521e-05,  1.7658e-05,  ..., -2.3335e-05,
         -1.7568e-05, -1.4670e-05]], device='cuda:0')
Loss: 0.9996837973594666


Running epoch 1, step 1079, batch 31
Sampled inputs[:2]: tensor([[    0,  3984, 13077,  ...,   287,   650,   413],
        [    0,  1042,  5738,  ...,    12,   287,  3643]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5519e-04,  9.2543e-04,  5.6253e-04,  ..., -7.2216e-05,
          2.7100e-04,  6.6823e-04],
        [-1.3188e-05, -8.7991e-06,  8.2627e-06,  ..., -1.1258e-05,
         -8.1509e-06, -7.8827e-06],
        [-2.8789e-05, -1.9789e-05,  1.9088e-05,  ..., -2.4244e-05,
         -1.7568e-05, -1.6935e-05],
        [-2.2367e-05, -1.4447e-05,  1.4678e-05,  ..., -1.8910e-05,
         -1.3679e-05, -1.3955e-05],
        [-3.0831e-05, -2.2307e-05,  2.0206e-05,  ..., -2.6688e-05,
         -2.0221e-05, -1.6816e-05]], device='cuda:0')
Loss: 0.983727753162384
Graident accumulation at epoch 1, step 1079, batch 31
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7713e-05,  1.4614e-04,  1.9150e-05,  ..., -1.3465e-04,
          6.4223e-05,  7.9796e-05],
        [-1.3003e-05, -9.3163e-06,  8.0292e-06,  ..., -1.1269e-05,
         -7.8256e-06, -8.0740e-06],
        [ 4.3506e-05,  5.8455e-05, -3.2938e-05,  ...,  4.0856e-05,
          5.6230e-05,  1.3392e-05],
        [-1.8121e-05, -9.0805e-06,  1.3059e-05,  ..., -1.5967e-05,
         -9.3014e-06, -1.2002e-05],
        [-3.3648e-05, -2.6260e-05,  2.1951e-05,  ..., -2.9359e-05,
         -2.2126e-05, -1.9762e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8355e-08, 4.9879e-08, 4.8704e-08,  ..., 1.7284e-08, 1.1841e-07,
         3.7342e-08],
        [7.9112e-11, 4.3276e-11, 1.3657e-11,  ..., 5.2577e-11, 1.5375e-11,
         2.1755e-11],
        [1.6815e-09, 1.0608e-09, 3.3385e-10,  ..., 1.2759e-09, 4.9970e-10,
         3.9448e-10],
        [4.3062e-10, 3.2910e-10, 1.4146e-10,  ..., 3.8604e-10, 1.5072e-10,
         1.5972e-10],
        [3.6575e-10, 1.9648e-10, 5.2886e-11,  ..., 2.6832e-10, 5.3505e-11,
         1.0238e-10]], device='cuda:0')
optimizer state dict: 135.0
lr: [9.99134683802993e-06, 9.99134683802993e-06]
scheduler_last_epoch: 135


Running epoch 1, step 1080, batch 32
Sampled inputs[:2]: tensor([[    0,   768,  3227,  ...,  3487,    13, 31431],
        [    0,   365,  5392,  ...,    14,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8521e-05,  9.9287e-05,  3.4062e-05,  ...,  4.6336e-05,
         -1.0811e-04, -6.4183e-05],
        [-1.6466e-06, -1.0282e-06,  1.0952e-06,  ..., -1.2964e-06,
         -9.3132e-07, -8.8662e-07],
        [-3.6508e-06, -2.3395e-06,  2.5481e-06,  ..., -2.8610e-06,
         -2.0415e-06, -1.9670e-06],
        [-2.8759e-06, -1.7434e-06,  1.9968e-06,  ..., -2.2352e-06,
         -1.5870e-06, -1.6391e-06],
        [-3.6657e-06, -2.5034e-06,  2.5183e-06,  ..., -2.9653e-06,
         -2.2352e-06, -1.8179e-06]], device='cuda:0')
Loss: 0.9886917471885681


Running epoch 1, step 1081, batch 33
Sampled inputs[:2]: tensor([[   0,  292,   17,  ..., 5760, 1345,  578],
        [   0,  346,  462,  ..., 2208,   12, 1901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7304e-05,  4.9729e-04, -2.2580e-05,  ...,  2.1343e-04,
         -5.8830e-04, -4.3847e-04],
        [-3.3081e-06, -2.0936e-06,  2.0787e-06,  ..., -2.6971e-06,
         -2.0042e-06, -2.0415e-06],
        [-7.2271e-06, -4.7088e-06,  4.8280e-06,  ..., -5.8264e-06,
         -4.3213e-06, -4.4107e-06],
        [-5.6028e-06, -3.4273e-06,  3.7178e-06,  ..., -4.5002e-06,
         -3.3304e-06, -3.6061e-06],
        [-7.5996e-06, -5.2452e-06,  4.9770e-06,  ..., -6.3330e-06,
         -4.9472e-06, -4.3362e-06]], device='cuda:0')
Loss: 0.9382748007774353


Running epoch 1, step 1082, batch 34
Sampled inputs[:2]: tensor([[    0,   596,   292,  ...,    13,  6673,   298],
        [    0,   266, 15794,  ...,  3128,  6479,  2626]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0492e-04,  4.9391e-04, -8.4884e-05,  ...,  2.5851e-04,
         -6.2292e-04, -4.0150e-04],
        [-4.9248e-06, -3.1069e-06,  3.0473e-06,  ..., -4.0680e-06,
         -3.0547e-06, -3.0920e-06],
        [-1.0833e-05, -7.0333e-06,  7.1228e-06,  ..., -8.8364e-06,
         -6.5863e-06, -6.7204e-06],
        [-8.3297e-06, -5.0738e-06,  5.4166e-06,  ..., -6.7949e-06,
         -5.0664e-06, -5.4687e-06],
        [-1.1325e-05, -7.7784e-06,  7.3165e-06,  ..., -9.5367e-06,
         -7.4655e-06, -6.5416e-06]], device='cuda:0')
Loss: 0.9676452875137329


Running epoch 1, step 1083, batch 35
Sampled inputs[:2]: tensor([[    0,  7110,   278,  ...,    66,    13,  9070],
        [    0,   278, 39533,  ...,   277,  1395, 47607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.4576e-05,  5.5323e-04, -8.9175e-05,  ...,  4.1656e-04,
         -5.6315e-04, -4.2119e-04],
        [-6.6161e-06, -4.2692e-06,  4.0457e-06,  ..., -5.4687e-06,
         -4.1649e-06, -4.1351e-06],
        [-1.4469e-05, -9.6262e-06,  9.4175e-06,  ..., -1.1817e-05,
         -8.9407e-06, -8.9407e-06],
        [-1.1057e-05, -6.9141e-06,  7.0930e-06,  ..., -9.0450e-06,
         -6.8471e-06, -7.2494e-06],
        [-1.5199e-05, -1.0669e-05,  9.7454e-06,  ..., -1.2815e-05,
         -1.0148e-05, -8.7172e-06]], device='cuda:0')
Loss: 0.9992842674255371


Running epoch 1, step 1084, batch 36
Sampled inputs[:2]: tensor([[    0,  4672,   278,  ...,  7523,  2305,    13],
        [    0,   409, 15720,  ...,    12,   287,  2350]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7363e-04,  6.6300e-04, -9.6932e-05,  ...,  2.5062e-04,
         -5.6287e-04, -4.6526e-04],
        [-8.2850e-06, -5.4389e-06,  5.0217e-06,  ..., -6.8992e-06,
         -5.3272e-06, -5.1782e-06],
        [-1.8120e-05, -1.2249e-05,  1.1668e-05,  ..., -1.4931e-05,
         -1.1459e-05, -1.1235e-05],
        [-1.3858e-05, -8.8364e-06,  8.7842e-06,  ..., -1.1444e-05,
         -8.7991e-06, -9.0897e-06],
        [ 1.3075e-04,  1.1244e-04, -8.5321e-05,  ...,  9.2105e-05,
          9.7835e-05,  5.7096e-05]], device='cuda:0')
Loss: 1.0349878072738647


Running epoch 1, step 1085, batch 37
Sampled inputs[:2]: tensor([[    0,   395,  5949,  ...,   341,    13,   635],
        [    0, 17734,    12,  ...,   278,  2421,   940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1400e-04,  6.9952e-04, -1.3574e-04,  ...,  3.0483e-04,
         -7.5618e-04, -6.0739e-04],
        [-9.9316e-06, -6.6534e-06,  6.1542e-06,  ..., -8.2180e-06,
         -6.3628e-06, -6.1132e-06],
        [-2.1830e-05, -1.5035e-05,  1.4290e-05,  ..., -1.7911e-05,
         -1.3769e-05, -1.3366e-05],
        [-1.6674e-05, -1.0878e-05,  1.0766e-05,  ..., -1.3679e-05,
         -1.0528e-05, -1.0759e-05],
        [ 1.2693e-04,  1.0942e-04, -8.2639e-05,  ...,  8.8931e-05,
          9.5272e-05,  5.5025e-05]], device='cuda:0')
Loss: 1.0167237520217896


Running epoch 1, step 1086, batch 38
Sampled inputs[:2]: tensor([[    0, 47684,   292,  ...,   287, 49958, 22022],
        [    0,  6957,   271,  ...,  9094,   266,  4320]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3312e-05,  7.1074e-04, -1.6894e-04,  ...,  3.1789e-04,
         -7.9659e-04, -7.6860e-04],
        [-1.1489e-05, -7.5810e-06,  7.2047e-06,  ..., -9.5218e-06,
         -7.3388e-06, -7.1116e-06],
        [-2.5213e-05, -1.7107e-05,  1.6704e-05,  ..., -2.0668e-05,
         -1.5810e-05, -1.5453e-05],
        [-1.9357e-05, -1.2375e-05,  1.2688e-05,  ..., -1.5885e-05,
         -1.2197e-05, -1.2554e-05],
        [ 1.2337e-04,  1.0708e-04, -8.0120e-05,  ...,  8.5935e-05,
          9.2962e-05,  5.3028e-05]], device='cuda:0')
Loss: 0.9737932085990906


Running epoch 1, step 1087, batch 39
Sampled inputs[:2]: tensor([[   0,  759, 4585,  ...,  360,  300,  670],
        [   0,  475, 2985,  ...,  292, 5273,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2860e-05,  6.9297e-04, -8.2907e-05,  ...,  2.9672e-04,
         -7.7652e-04, -9.2673e-04],
        [ 2.3841e-05,  7.9288e-05, -6.8749e-05,  ...,  5.2952e-05,
          4.9057e-05,  3.2885e-05],
        [-2.8566e-05, -1.9506e-05,  1.9088e-05,  ..., -2.3380e-05,
         -1.8016e-05, -1.7352e-05],
        [-2.1905e-05, -1.4089e-05,  1.4499e-05,  ..., -1.7971e-05,
         -1.3925e-05, -1.4134e-05],
        [ 1.1985e-04,  1.0444e-04, -7.7662e-05,  ...,  8.3015e-05,
          9.0518e-05,  5.1210e-05]], device='cuda:0')
Loss: 0.97411048412323
Graident accumulation at epoch 1, step 1087, batch 39
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.6228e-05,  2.0082e-04,  8.9446e-06,  ..., -9.1517e-05,
         -1.9851e-05, -2.0856e-05],
        [-9.3184e-06, -4.5589e-07,  3.5139e-07,  ..., -4.8470e-06,
         -2.1374e-06, -3.9781e-06],
        [ 3.6299e-05,  5.0659e-05, -2.7735e-05,  ...,  3.4432e-05,
          4.8805e-05,  1.0318e-05],
        [-1.8499e-05, -9.5813e-06,  1.3203e-05,  ..., -1.6168e-05,
         -9.7638e-06, -1.2215e-05],
        [-1.8297e-05, -1.3190e-05,  1.1989e-05,  ..., -1.8122e-05,
         -1.0861e-05, -1.2665e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8298e-08, 5.0309e-08, 4.8662e-08,  ..., 1.7355e-08, 1.1890e-07,
         3.8163e-08],
        [7.9601e-11, 4.9519e-11, 1.8370e-11,  ..., 5.5328e-11, 1.7766e-11,
         2.2815e-11],
        [1.6806e-09, 1.0601e-09, 3.3388e-10,  ..., 1.2752e-09, 4.9952e-10,
         3.9439e-10],
        [4.3067e-10, 3.2897e-10, 1.4153e-10,  ..., 3.8598e-10, 1.5076e-10,
         1.5976e-10],
        [3.7975e-10, 2.0719e-10, 5.8864e-11,  ..., 2.7494e-10, 6.1645e-11,
         1.0490e-10]], device='cuda:0')
optimizer state dict: 136.0
lr: [9.867734078748245e-06, 9.867734078748245e-06]
scheduler_last_epoch: 136


Running epoch 1, step 1088, batch 40
Sampled inputs[:2]: tensor([[   0,  266, 4287,  ...,  367, 4428, 2118],
        [   0, 3167,  300,  ..., 1109,  490, 1985]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9481e-05,  7.5552e-05, -2.3144e-05,  ...,  5.4240e-05,
          3.8009e-05,  1.3090e-04],
        [-1.8477e-06, -1.0207e-06,  9.9838e-07,  ..., -1.4752e-06,
         -1.1623e-06, -1.5050e-06],
        [-3.9637e-06, -2.3693e-06,  2.3395e-06,  ..., -3.1292e-06,
         -2.5034e-06, -3.1590e-06],
        [-2.8610e-06, -1.5572e-06,  1.6242e-06,  ..., -2.2650e-06,
         -1.8105e-06, -2.3693e-06],
        [-4.3809e-06, -2.7567e-06,  2.5183e-06,  ..., -3.6061e-06,
         -2.9951e-06, -3.3230e-06]], device='cuda:0')
Loss: 0.9508432745933533


Running epoch 1, step 1089, batch 41
Sampled inputs[:2]: tensor([[    0,  4566,   300,  ...,   271,  1644, 16473],
        [    0,   996,  2226,  ...,  5322,   287,   452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1397e-04,  6.1859e-05, -1.6142e-04,  ...,  8.4636e-05,
          5.3080e-05, -1.4910e-05],
        [-3.4720e-06, -2.0489e-06,  2.0266e-06,  ..., -2.8312e-06,
         -2.2799e-06, -2.5406e-06],
        [-7.5251e-06, -4.7088e-06,  4.7386e-06,  ..., -6.0648e-06,
         -4.9025e-06, -5.4091e-06],
        [-5.5283e-06, -3.1814e-06,  3.3975e-06,  ..., -4.4703e-06,
         -3.6284e-06, -4.1649e-06],
        [-8.0913e-06, -5.3793e-06,  4.9770e-06,  ..., -6.7651e-06,
         -5.6922e-06, -5.4687e-06]], device='cuda:0')
Loss: 1.003932237625122


Running epoch 1, step 1090, batch 42
Sampled inputs[:2]: tensor([[    0,  1921,   843,  ...,  9420,   352,   266],
        [    0,   422,    13,  ..., 14026,   368,  4999]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4210e-04,  1.4325e-05, -1.9044e-04,  ...,  6.3944e-05,
          2.6574e-05, -4.8862e-05],
        [-5.1782e-06, -3.1516e-06,  3.0771e-06,  ..., -4.2543e-06,
         -3.4273e-06, -3.7253e-06],
        [-1.1235e-05, -7.1824e-06,  7.1675e-06,  ..., -9.1195e-06,
         -7.3463e-06, -7.9721e-06],
        [-8.2701e-06, -4.9025e-06,  5.1558e-06,  ..., -6.7204e-06,
         -5.4389e-06, -6.1616e-06],
        [-1.2144e-05, -8.2403e-06,  7.5698e-06,  ..., -1.0222e-05,
         -8.5831e-06, -8.1211e-06]], device='cuda:0')
Loss: 0.9702026844024658


Running epoch 1, step 1091, batch 43
Sampled inputs[:2]: tensor([[    0,  1712,    12,  ...,  1255,  1688,   266],
        [    0,   271, 21394,  ...,  1487,   287,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3574e-04, -1.7801e-04, -1.9364e-04,  ...,  4.7679e-05,
         -6.3806e-05, -4.6474e-05],
        [-6.7726e-06, -4.2841e-06,  4.0829e-06,  ..., -5.5879e-06,
         -4.5821e-06, -4.7907e-06],
        [-1.4797e-05, -9.7752e-06,  9.5516e-06,  ..., -1.2055e-05,
         -9.8795e-06, -1.0356e-05],
        [-1.0967e-05, -6.7651e-06,  6.9365e-06,  ..., -8.9407e-06,
         -7.3761e-06, -8.0615e-06],
        [-1.5870e-05, -1.1116e-05,  1.0014e-05,  ..., -1.3411e-05,
         -1.1414e-05, -1.0431e-05]], device='cuda:0')
Loss: 0.9862396717071533


Running epoch 1, step 1092, batch 44
Sampled inputs[:2]: tensor([[    0,  1067,   292,  ..., 10792, 11280,    14],
        [    0,   609,    12,  ...,   409, 11041,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4465e-04, -2.1906e-04, -2.9206e-04,  ...,  3.9330e-05,
          1.1682e-04,  2.3378e-04],
        [-8.4713e-06, -5.2676e-06,  4.8950e-06,  ..., -7.0781e-06,
         -5.8115e-06, -6.2585e-06],
        [-1.8418e-05, -1.2010e-05,  1.1474e-05,  ..., -1.5199e-05,
         -1.2502e-05, -1.3411e-05],
        [-1.3605e-05, -8.2776e-06,  8.2627e-06,  ..., -1.1265e-05,
         -9.3281e-06, -1.0401e-05],
        [-2.0131e-05, -1.3813e-05,  1.2264e-05,  ..., -1.7166e-05,
         -1.4603e-05, -1.3798e-05]], device='cuda:0')
Loss: 0.9342015981674194


Running epoch 1, step 1093, batch 45
Sampled inputs[:2]: tensor([[   0, 2645,   12,  ...,    5, 1239, 7200],
        [   0,  494,  825,  ...,  897,  328,  275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8962e-04, -1.9924e-04, -2.4170e-04,  ...,  3.9334e-05,
          5.7037e-05,  3.1475e-04],
        [-1.0215e-05, -6.5714e-06,  5.9456e-06,  ..., -8.5160e-06,
         -7.0557e-06, -7.3314e-06],
        [-2.2233e-05, -1.5005e-05,  1.3903e-05,  ..., -1.8358e-05,
         -1.5274e-05, -1.5795e-05],
        [-1.6421e-05, -1.0394e-05,  1.0036e-05,  ..., -1.3575e-05,
         -1.1370e-05, -1.2234e-05],
        [-2.4155e-05, -1.7151e-05,  1.4812e-05,  ..., -2.0623e-05,
         -1.7747e-05, -1.6153e-05]], device='cuda:0')
Loss: 1.0322967767715454


Running epoch 1, step 1094, batch 46
Sampled inputs[:2]: tensor([[    0,   607, 32336,  ...,  4787,   367,  1255],
        [    0,   401,  9370,  ...,     9,   287,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3368e-04, -1.8035e-04, -4.1039e-04,  ...,  3.4609e-05,
         -3.9984e-05,  1.7542e-04],
        [-1.1928e-05, -7.5772e-06,  7.0706e-06,  ..., -9.8199e-06,
         -8.1286e-06, -8.4490e-06],
        [-2.6047e-05, -1.7330e-05,  1.6540e-05,  ..., -2.1249e-05,
         -1.7628e-05, -1.8284e-05],
        [-1.9282e-05, -1.2018e-05,  1.1988e-05,  ..., -1.5721e-05,
         -1.3143e-05, -1.4186e-05],
        [-2.7955e-05, -1.9625e-05,  1.7405e-05,  ..., -2.3603e-05,
         -2.0280e-05, -1.8448e-05]], device='cuda:0')
Loss: 0.9725384712219238


Running epoch 1, step 1095, batch 47
Sampled inputs[:2]: tensor([[    0,    47,  1838,  ...,   792,    83, 42612],
        [    0,   417,   199,  ...,    13,    20,  6248]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8161e-04, -1.5745e-04, -2.6755e-04,  ...,  2.6043e-05,
          1.3646e-04,  1.2304e-04],
        [-1.3635e-05, -8.7321e-06,  8.1286e-06,  ..., -1.1198e-05,
         -9.3058e-06, -9.5442e-06],
        [-2.9817e-05, -1.9968e-05,  1.8984e-05,  ..., -2.4304e-05,
         -2.0221e-05, -2.0713e-05],
        [-2.2128e-05, -1.3903e-05,  1.3813e-05,  ..., -1.8016e-05,
         -1.5095e-05, -1.6108e-05],
        [-3.1918e-05, -2.2575e-05,  1.9938e-05,  ..., -2.6941e-05,
         -2.3216e-05, -2.0832e-05]], device='cuda:0')
Loss: 1.0172606706619263
Graident accumulation at epoch 1, step 1095, batch 47
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0026, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0290, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.4437e-06,  1.6500e-04, -1.8705e-05,  ..., -7.9761e-05,
         -4.2200e-06, -6.4663e-06],
        [-9.7500e-06, -1.2835e-06,  1.1291e-06,  ..., -5.4821e-06,
         -2.8542e-06, -4.5347e-06],
        [ 2.9687e-05,  4.3596e-05, -2.3063e-05,  ...,  2.8559e-05,
          4.1902e-05,  7.2148e-06],
        [-1.8862e-05, -1.0013e-05,  1.3264e-05,  ..., -1.6353e-05,
         -1.0297e-05, -1.2604e-05],
        [-1.9660e-05, -1.4129e-05,  1.2784e-05,  ..., -1.9004e-05,
         -1.2097e-05, -1.3482e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8385e-08, 5.0284e-08, 4.8685e-08,  ..., 1.7338e-08, 1.1880e-07,
         3.8140e-08],
        [7.9707e-11, 4.9546e-11, 1.8418e-11,  ..., 5.5398e-11, 1.7835e-11,
         2.2883e-11],
        [1.6798e-09, 1.0594e-09, 3.3391e-10,  ..., 1.2745e-09, 4.9943e-10,
         3.9442e-10],
        [4.3073e-10, 3.2883e-10, 1.4158e-10,  ..., 3.8592e-10, 1.5084e-10,
         1.5986e-10],
        [3.8039e-10, 2.0750e-10, 5.9203e-11,  ..., 2.7539e-10, 6.2123e-11,
         1.0523e-10]], device='cuda:0')
optimizer state dict: 137.0
lr: [9.744141530853894e-06, 9.744141530853894e-06]
scheduler_last_epoch: 137


Running epoch 1, step 1096, batch 48
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  984,   14,  759],
        [   0, 6184, 1412,  ...,   12,  266,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8361e-05, -4.3011e-05, -4.7710e-05,  ...,  4.7405e-05,
         -1.3946e-04, -1.4421e-04],
        [-1.6838e-06, -1.1995e-06,  1.0505e-06,  ..., -1.3411e-06,
         -1.1548e-06, -1.1325e-06],
        [-3.8147e-06, -2.8014e-06,  2.5183e-06,  ..., -3.0249e-06,
         -2.5928e-06, -2.5779e-06],
        [-2.8014e-06, -1.9521e-06,  1.8179e-06,  ..., -2.2054e-06,
         -1.9073e-06, -1.9968e-06],
        [-3.9339e-06, -3.0398e-06,  2.5630e-06,  ..., -3.2336e-06,
         -2.8759e-06, -2.4736e-06]], device='cuda:0')
Loss: 0.9980195760726929


Running epoch 1, step 1097, batch 49
Sampled inputs[:2]: tensor([[    0,  1976,  1329,  ...,   278,  9469,   292],
        [    0,   508,  3282,  ...,   334,   287, 31884]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8249e-05, -1.2838e-04, -1.2586e-06,  ...,  7.1467e-06,
         -4.6739e-05, -2.0611e-04],
        [-3.4422e-06, -2.3395e-06,  2.0340e-06,  ..., -2.6822e-06,
         -2.3022e-06, -2.3469e-06],
        [-7.6592e-06, -5.4538e-06,  4.8429e-06,  ..., -5.9754e-06,
         -5.1558e-06, -5.2154e-06],
        [-5.5432e-06, -3.7178e-06,  3.4347e-06,  ..., -4.3064e-06,
         -3.7178e-06, -3.9786e-06],
        [-7.9572e-06, -5.9605e-06,  4.9621e-06,  ..., -6.4224e-06,
         -5.7369e-06, -5.0366e-06]], device='cuda:0')
Loss: 0.9822133183479309


Running epoch 1, step 1098, batch 50
Sampled inputs[:2]: tensor([[    0,   609,   271,  ...,  4684, 14107,   259],
        [    0, 12987,   609,  ...,   699,  9863,  3227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3898e-04, -1.8132e-04,  2.1147e-05,  ..., -1.1684e-04,
         -8.9567e-05, -2.5884e-04],
        [-5.1036e-06, -3.5763e-06,  3.0100e-06,  ..., -4.0904e-06,
         -3.5390e-06, -3.4571e-06],
        [-1.1355e-05, -8.2403e-06,  7.1228e-06,  ..., -9.0748e-06,
         -7.8529e-06, -7.7039e-06],
        [-8.2403e-06, -5.6550e-06,  5.0664e-06,  ..., -6.5714e-06,
         -5.6997e-06, -5.8860e-06],
        [-1.1861e-05, -9.0897e-06,  7.3463e-06,  ..., -9.8199e-06,
         -8.7768e-06, -7.5102e-06]], device='cuda:0')
Loss: 1.0015989542007446


Running epoch 1, step 1099, batch 51
Sampled inputs[:2]: tensor([[    0,   586,  1016,  ...,  7151,  8280,   300],
        [    0, 49570,   644,  ...,   461,   800,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2826e-04, -1.8132e-04, -7.2906e-05,  ..., -8.7389e-05,
         -9.2530e-05, -2.9563e-04],
        [-6.7130e-06, -4.6641e-06,  4.0531e-06,  ..., -5.3644e-06,
         -4.5896e-06, -4.5598e-06],
        [-1.4901e-05, -1.0744e-05,  9.5814e-06,  ..., -1.1891e-05,
         -1.0177e-05, -1.0163e-05],
        [-1.0908e-05, -7.4431e-06,  6.8918e-06,  ..., -8.6725e-06,
         -7.4357e-06, -7.8082e-06],
        [-1.5497e-05, -1.1846e-05,  9.8199e-06,  ..., -1.2830e-05,
         -1.1384e-05, -9.8497e-06]], device='cuda:0')
Loss: 0.9704946875572205


Running epoch 1, step 1100, batch 52
Sampled inputs[:2]: tensor([[    0,   292,  1820,  ...,   591,  6619,  1607],
        [    0,   344, 10706,  ...,  1184,   578,   825]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5163e-05, -2.4991e-04, -1.7074e-04,  ..., -1.5931e-04,
         -1.9636e-04, -2.5710e-04],
        [-8.3968e-06, -5.8413e-06,  5.1036e-06,  ..., -6.7204e-06,
         -5.7369e-06, -5.7071e-06],
        [-1.8552e-05, -1.3426e-05,  1.2055e-05,  ..., -1.4827e-05,
         -1.2681e-05, -1.2636e-05],
        [-1.3694e-05, -9.3803e-06,  8.7321e-06,  ..., -1.0908e-05,
         -9.3430e-06, -9.7901e-06],
        [-1.9297e-05, -1.4827e-05,  1.2353e-05,  ..., -1.6019e-05,
         -1.4216e-05, -1.2234e-05]], device='cuda:0')
Loss: 0.9869776368141174


Running epoch 1, step 1101, batch 53
Sampled inputs[:2]: tensor([[    0,  3594,   950,  ...,  6517,   344, 15386],
        [    0,   391,  1761,  ...,   346,    14,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1544e-05, -4.2787e-04, -2.1817e-04,  ..., -2.2775e-04,
         -1.0388e-04, -1.1339e-04],
        [-9.9912e-06, -6.8769e-06,  6.1169e-06,  ..., -8.0019e-06,
         -6.8322e-06, -6.8024e-06],
        [-2.2084e-05, -1.5780e-05,  1.4424e-05,  ..., -1.7628e-05,
         -1.5020e-05, -1.5050e-05],
        [-1.6391e-05, -1.1072e-05,  1.0513e-05,  ..., -1.3053e-05,
         -1.1161e-05, -1.1757e-05],
        [-2.2978e-05, -1.7449e-05,  1.4797e-05,  ..., -1.9059e-05,
         -1.6883e-05, -1.4573e-05]], device='cuda:0')
Loss: 1.0123258829116821


Running epoch 1, step 1102, batch 54
Sampled inputs[:2]: tensor([[    0,   342,   266,  ...,   199,   395, 11578],
        [    0,  2377,   360,  ...,   266,  4745,   963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2014e-04, -5.8736e-04, -4.3678e-04,  ..., -6.8160e-05,
         -1.1919e-04, -2.5018e-04],
        [-1.1720e-05, -7.8082e-06,  7.2196e-06,  ..., -9.3281e-06,
         -7.8902e-06, -7.9572e-06],
        [-2.5868e-05, -1.7911e-05,  1.6972e-05,  ..., -2.0504e-05,
         -1.7285e-05, -1.7539e-05],
        [-1.9312e-05, -1.2584e-05,  1.2465e-05,  ..., -1.5274e-05,
         -1.2927e-05, -1.3798e-05],
        [-2.6911e-05, -1.9833e-05,  1.7419e-05,  ..., -2.2158e-05,
         -1.9446e-05, -1.6987e-05]], device='cuda:0')
Loss: 0.9913196563720703


Running epoch 1, step 1103, batch 55
Sampled inputs[:2]: tensor([[    0,  3164,    12,  ...,   984,   344,  3993],
        [    0, 15931,    14,  ...,  2645,   699,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5314e-04, -6.4736e-04, -6.5704e-04,  ...,  3.3775e-06,
         -1.2398e-04, -3.3869e-04],
        [-1.3374e-05, -8.8960e-06,  8.2925e-06,  ..., -1.0647e-05,
         -8.9779e-06, -9.0525e-06],
        [-2.9549e-05, -2.0400e-05,  1.9491e-05,  ..., -2.3425e-05,
         -1.9670e-05, -1.9953e-05],
        [-2.2069e-05, -1.4335e-05,  1.4335e-05,  ..., -1.7464e-05,
         -1.4737e-05, -1.5736e-05],
        [-3.0681e-05, -2.2560e-05,  1.9968e-05,  ..., -2.5257e-05,
         -2.2069e-05, -1.9267e-05]], device='cuda:0')
Loss: 0.9889048337936401
Graident accumulation at epoch 1, step 1103, batch 55
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2214e-05,  8.3760e-05, -8.2538e-05,  ..., -7.1447e-05,
         -1.6196e-05, -3.9689e-05],
        [-1.0112e-05, -2.0448e-06,  1.8454e-06,  ..., -5.9986e-06,
         -3.4666e-06, -4.9865e-06],
        [ 2.3763e-05,  3.7197e-05, -1.8808e-05,  ...,  2.3360e-05,
          3.5745e-05,  4.4981e-06],
        [-1.9183e-05, -1.0446e-05,  1.3371e-05,  ..., -1.6464e-05,
         -1.0741e-05, -1.2918e-05],
        [-2.0762e-05, -1.4972e-05,  1.3503e-05,  ..., -1.9629e-05,
         -1.3094e-05, -1.4060e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8350e-08, 5.0653e-08, 4.9068e-08,  ..., 1.7321e-08, 1.1870e-07,
         3.8217e-08],
        [7.9806e-11, 4.9575e-11, 1.8468e-11,  ..., 5.5456e-11, 1.7898e-11,
         2.2942e-11],
        [1.6790e-09, 1.0588e-09, 3.3396e-10,  ..., 1.2738e-09, 4.9932e-10,
         3.9442e-10],
        [4.3079e-10, 3.2871e-10, 1.4164e-10,  ..., 3.8584e-10, 1.5091e-10,
         1.5995e-10],
        [3.8095e-10, 2.0780e-10, 5.9542e-11,  ..., 2.7575e-10, 6.2548e-11,
         1.0550e-10]], device='cuda:0')
optimizer state dict: 138.0
lr: [9.620588080367043e-06, 9.620588080367043e-06]
scheduler_last_epoch: 138


Running epoch 1, step 1104, batch 56
Sampled inputs[:2]: tensor([[   0, 7185,  328,  ..., 1427, 1477, 1061],
        [   0, 4371, 4806,  ...,  685,  461,  654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2251e-04, -1.7064e-04, -1.7597e-04,  ...,  1.3456e-05,
          8.9742e-08, -1.1313e-04],
        [-1.6242e-06, -1.1697e-06,  1.0505e-06,  ..., -1.3262e-06,
         -1.0654e-06, -1.0878e-06],
        [-3.7253e-06, -2.7418e-06,  2.5332e-06,  ..., -3.0249e-06,
         -2.3991e-06, -2.5183e-06],
        [-2.6971e-06, -1.8850e-06,  1.8105e-06,  ..., -2.1756e-06,
         -1.7285e-06, -1.8999e-06],
        [-3.7551e-06, -2.9355e-06,  2.5034e-06,  ..., -3.1739e-06,
         -2.6226e-06, -2.3693e-06]], device='cuda:0')
Loss: 1.0105698108673096


Running epoch 1, step 1105, batch 57
Sampled inputs[:2]: tensor([[    0, 20202,   300,  ..., 15185,   287,  6573],
        [    0,   292,   380,  ...,  9636,   417,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.1654e-05, -1.3432e-04, -3.2044e-04,  ..., -5.3644e-05,
          2.3441e-05, -5.8309e-05],
        [-3.2261e-06, -2.2128e-06,  2.0042e-06,  ..., -2.6673e-06,
         -2.1532e-06, -2.3171e-06],
        [-7.3463e-06, -5.1409e-06,  4.8727e-06,  ..., -5.9754e-06,
         -4.7833e-06, -5.2452e-06],
        [-5.3346e-06, -3.5316e-06,  3.4571e-06,  ..., -4.3511e-06,
         -3.5167e-06, -4.0159e-06],
        [-7.5102e-06, -5.6028e-06,  4.9025e-06,  ..., -6.3479e-06,
         -5.3048e-06, -4.9472e-06]], device='cuda:0')
Loss: 0.9656431078910828


Running epoch 1, step 1106, batch 58
Sampled inputs[:2]: tensor([[   0, 2366, 5036,  ..., 1477,  352,  631],
        [   0, 2336,   26,  ..., 2564,  271, 1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.8085e-05, -1.9104e-04, -3.0862e-04,  ..., -1.9290e-05,
         -5.8913e-05, -1.0245e-04],
        [-4.9844e-06, -3.2187e-06,  2.9728e-06,  ..., -4.0084e-06,
         -3.2261e-06, -3.8296e-06],
        [-1.1429e-05, -7.5996e-06,  7.2718e-06,  ..., -9.0599e-06,
         -7.2569e-06, -8.7321e-06],
        [-8.1360e-06, -5.1036e-06,  5.0589e-06,  ..., -6.4671e-06,
         -5.2154e-06, -6.5044e-06],
        [-1.1951e-05, -8.4043e-06,  7.4804e-06,  ..., -9.8199e-06,
         -8.2105e-06, -8.5235e-06]], device='cuda:0')
Loss: 0.9500251412391663


Running epoch 1, step 1107, batch 59
Sampled inputs[:2]: tensor([[    0,   292, 12376,  ...,   380, 20878, 13900],
        [    0, 28590,    12,  ...,   342, 29639,  1693]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0505e-04, -2.5738e-04, -4.1723e-04,  ..., -4.0426e-05,
          1.1115e-04, -1.8574e-04],
        [-6.5491e-06, -4.2841e-06,  3.9339e-06,  ..., -5.3048e-06,
         -4.3362e-06, -4.9323e-06],
        [-1.4931e-05, -1.0073e-05,  9.5814e-06,  ..., -1.1906e-05,
         -9.7007e-06, -1.1161e-05],
        [-1.0729e-05, -6.8098e-06,  6.7279e-06,  ..., -8.5980e-06,
         -7.0781e-06, -8.4415e-06],
        [-1.5572e-05, -1.1101e-05,  9.8497e-06,  ..., -1.2845e-05,
         -1.0908e-05, -1.0833e-05]], device='cuda:0')
Loss: 0.9856113195419312


Running epoch 1, step 1108, batch 60
Sampled inputs[:2]: tensor([[   0, 4665,  909,  ..., 3607,  259, 1108],
        [   0,  287,  266,  ...,  333,  199, 3217]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4300e-05, -4.4059e-04, -4.3177e-04,  ..., -4.6235e-05,
          1.3927e-04, -1.1814e-04],
        [-8.2701e-06, -5.5060e-06,  5.0291e-06,  ..., -6.6087e-06,
         -5.4389e-06, -6.0499e-06],
        [-1.8865e-05, -1.2979e-05,  1.2204e-05,  ..., -1.4886e-05,
         -1.2234e-05, -1.3739e-05],
        [-1.3486e-05, -8.7470e-06,  8.5458e-06,  ..., -1.0669e-05,
         -8.8438e-06, -1.0319e-05],
        [-1.9625e-05, -1.4260e-05,  1.2532e-05,  ..., -1.6049e-05,
         -1.3724e-05, -1.3322e-05]], device='cuda:0')
Loss: 1.0011521577835083


Running epoch 1, step 1109, batch 61
Sampled inputs[:2]: tensor([[   0,   14,  759,  ..., 2540, 1323,   12],
        [   0, 3408,  300,  ...,   14, 5870,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9152e-05, -5.3313e-04, -3.9742e-04,  ..., -1.6413e-05,
          1.1201e-04, -1.6018e-04],
        [-1.0014e-05, -6.6459e-06,  6.0722e-06,  ..., -7.9125e-06,
         -6.4969e-06, -7.2122e-06],
        [-2.2918e-05, -1.5765e-05,  1.4782e-05,  ..., -1.7941e-05,
         -1.4722e-05, -1.6481e-05],
        [-1.6347e-05, -1.0587e-05,  1.0341e-05,  ..., -1.2785e-05,
         -1.0565e-05, -1.2331e-05],
        [-2.3618e-05, -1.7211e-05,  1.5050e-05,  ..., -1.9193e-05,
         -1.6421e-05, -1.5855e-05]], device='cuda:0')
Loss: 0.9658806920051575


Running epoch 1, step 1110, batch 62
Sampled inputs[:2]: tensor([[    0,  1099,   644,  ...,  5481,    14,  8782],
        [    0,   874,   445,  ...,    14, 16205,  8510]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4234e-05, -7.0068e-04, -4.0886e-04,  ..., -7.8359e-05,
          1.8069e-04, -1.3412e-04],
        [-1.1601e-05, -7.7039e-06,  7.0408e-06,  ..., -9.2238e-06,
         -7.6145e-06, -8.4266e-06],
        [-2.6464e-05, -1.8209e-05,  1.7136e-05,  ..., -2.0802e-05,
         -1.7151e-05, -1.9148e-05],
        [-1.8939e-05, -1.2279e-05,  1.2018e-05,  ..., -1.4931e-05,
         -1.2405e-05, -1.4432e-05],
        [-2.7418e-05, -1.9997e-05,  1.7554e-05,  ..., -2.2382e-05,
         -1.9252e-05, -1.8477e-05]], device='cuda:0')
Loss: 0.9812129139900208


Running epoch 1, step 1111, batch 63
Sampled inputs[:2]: tensor([[    0,   591, 36195,  ...,  3359,   717,    12],
        [    0,  1145,    35,  ...,   300,  5192,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4384e-04, -7.1099e-04, -4.7476e-04,  ..., -7.8009e-05,
          1.7127e-04, -3.9328e-04],
        [-1.3173e-05, -8.8215e-06,  8.0913e-06,  ..., -1.0498e-05,
         -8.6501e-06, -9.4473e-06],
        [-2.9981e-05, -2.0787e-05,  1.9625e-05,  ..., -2.3648e-05,
         -1.9461e-05, -2.1458e-05],
        [-2.1607e-05, -1.4149e-05,  1.3888e-05,  ..., -1.7077e-05,
         -1.4149e-05, -1.6250e-05],
        [-3.1024e-05, -2.2799e-05,  2.0072e-05,  ..., -2.5392e-05,
         -2.1830e-05, -2.0668e-05]], device='cuda:0')
Loss: 0.9963236451148987
Graident accumulation at epoch 1, step 1111, batch 63
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3391e-05,  4.2854e-06, -1.2176e-04,  ..., -7.2103e-05,
          2.5501e-06, -7.5048e-05],
        [-1.0418e-05, -2.7224e-06,  2.4700e-06,  ..., -6.4485e-06,
         -3.9849e-06, -5.4326e-06],
        [ 1.8389e-05,  3.1398e-05, -1.4964e-05,  ...,  1.8660e-05,
          3.0225e-05,  1.9025e-06],
        [-1.9425e-05, -1.0816e-05,  1.3423e-05,  ..., -1.6525e-05,
         -1.1082e-05, -1.3251e-05],
        [-2.1788e-05, -1.5755e-05,  1.4159e-05,  ..., -2.0206e-05,
         -1.3968e-05, -1.4721e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8351e-08, 5.1108e-08, 4.9244e-08,  ..., 1.7310e-08, 1.1861e-07,
         3.8333e-08],
        [7.9900e-11, 4.9603e-11, 1.8515e-11,  ..., 5.5511e-11, 1.7955e-11,
         2.3008e-11],
        [1.6782e-09, 1.0582e-09, 3.3401e-10,  ..., 1.2731e-09, 4.9920e-10,
         3.9449e-10],
        [4.3082e-10, 3.2858e-10, 1.4169e-10,  ..., 3.8574e-10, 1.5096e-10,
         1.6005e-10],
        [3.8153e-10, 2.0811e-10, 5.9886e-11,  ..., 2.7612e-10, 6.2962e-11,
         1.0582e-10]], device='cuda:0')
optimizer state dict: 139.0
lr: [9.497092607333449e-06, 9.497092607333449e-06]
scheduler_last_epoch: 139


Running epoch 1, step 1112, batch 64
Sampled inputs[:2]: tensor([[    0,    14, 30840,  ...,   287,   932,    14],
        [    0,    13,  1529,  ...,   943,   266,  9479]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1456e-05, -7.2205e-05,  2.4748e-05,  ...,  2.3906e-05,
         -6.8920e-05, -1.2444e-04],
        [-1.6168e-06, -1.1325e-06,  1.0431e-06,  ..., -1.2591e-06,
         -1.0356e-06, -1.0654e-06],
        [-3.7402e-06, -2.6673e-06,  2.5183e-06,  ..., -2.8908e-06,
         -2.3544e-06, -2.5183e-06],
        [-2.7120e-06, -1.8403e-06,  1.8179e-06,  ..., -2.0713e-06,
         -1.6913e-06, -1.8775e-06],
        [-3.9041e-06, -2.9653e-06,  2.6077e-06,  ..., -3.1590e-06,
         -2.7120e-06, -2.4736e-06]], device='cuda:0')
Loss: 1.0191999673843384


Running epoch 1, step 1113, batch 65
Sampled inputs[:2]: tensor([[    0,    14,   475,  ...,  7903,   266, 27772],
        [    0,  7094,   596,  ...,  4764,  9514,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2471e-04, -8.3006e-05,  9.1076e-06,  ...,  1.4406e-05,
         -2.0899e-05, -1.6449e-04],
        [-3.2559e-06, -2.3469e-06,  2.0713e-06,  ..., -2.5481e-06,
         -2.0862e-06, -2.2277e-06],
        [-7.5847e-06, -5.6177e-06,  5.0664e-06,  ..., -5.9158e-06,
         -4.8131e-06, -5.2750e-06],
        [-5.5283e-06, -3.8669e-06,  3.6582e-06,  ..., -4.2468e-06,
         -3.4571e-06, -3.9786e-06],
        [-7.8678e-06, -6.1691e-06,  5.1856e-06,  ..., -6.3926e-06,
         -5.4538e-06, -5.1260e-06]], device='cuda:0')
Loss: 1.0029305219650269


Running epoch 1, step 1114, batch 66
Sampled inputs[:2]: tensor([[    0,   668,  2474,  ...,   668,  4599,   360],
        [    0,    12,   344,  ..., 10482,   950, 15744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5235e-05, -1.7163e-04, -1.3832e-05,  ...,  1.0372e-04,
         -9.3762e-05, -1.4548e-04],
        [-4.8801e-06, -3.5465e-06,  3.1739e-06,  ..., -3.7700e-06,
         -3.0398e-06, -3.2932e-06],
        [-1.1459e-05, -8.5235e-06,  7.8082e-06,  ..., -8.8066e-06,
         -7.0632e-06, -7.8529e-06],
        [-8.2403e-06, -5.8040e-06,  5.5656e-06,  ..., -6.2436e-06,
         -4.9993e-06, -5.8413e-06],
        [-1.1712e-05, -9.2089e-06,  7.8529e-06,  ..., -9.3728e-06,
         -7.8976e-06, -7.4804e-06]], device='cuda:0')
Loss: 1.0044419765472412


Running epoch 1, step 1115, batch 67
Sampled inputs[:2]: tensor([[   0,  395, 4973,  ..., 5851,  409, 4370],
        [   0, 4995,  287,  ...,  300, 4531, 4729]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7295e-05, -1.7052e-04, -2.7843e-05,  ...,  1.2854e-04,
         -1.9966e-04, -2.2248e-04],
        [-6.4969e-06, -4.6566e-06,  4.1723e-06,  ..., -5.0291e-06,
         -4.0904e-06, -4.4405e-06],
        [-1.5199e-05, -1.1176e-05,  1.0282e-05,  ..., -1.1697e-05,
         -9.4771e-06, -1.0505e-05],
        [-1.0923e-05, -7.5847e-06,  7.3090e-06,  ..., -8.2999e-06,
         -6.7279e-06, -7.8380e-06],
        [-1.5587e-05, -1.2115e-05,  1.0371e-05,  ..., -1.2472e-05,
         -1.0595e-05, -1.0028e-05]], device='cuda:0')
Loss: 0.9829598069190979


Running epoch 1, step 1116, batch 68
Sampled inputs[:2]: tensor([[    0,   680,   401,  ...,  2872,   292, 23535],
        [    0,  9010,    17,  ...,  3813,  1147,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.3933e-05, -1.3866e-04, -1.0775e-04,  ...,  1.1790e-05,
         -2.2173e-05, -2.2248e-04],
        [-8.0541e-06, -5.7518e-06,  5.1148e-06,  ..., -6.2808e-06,
         -5.1633e-06, -5.5730e-06],
        [-1.8641e-05, -1.3679e-05,  1.2547e-05,  ..., -1.4424e-05,
         -1.1802e-05, -1.2949e-05],
        [-1.3441e-05, -9.2983e-06,  8.9556e-06,  ..., -1.0312e-05,
         -8.4788e-06, -9.7901e-06],
        [-1.9386e-05, -1.5050e-05,  1.2845e-05,  ..., -1.5587e-05,
         -1.3351e-05, -1.2517e-05]], device='cuda:0')
Loss: 0.9641397595405579


Running epoch 1, step 1117, batch 69
Sampled inputs[:2]: tensor([[   0,  300,  344,  ...,   14, 5077, 2715],
        [   0,  368,  729,  ...,  221,  380, 2830]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6297e-05, -2.1865e-04, -2.5414e-04,  ..., -1.1053e-05,
          7.0235e-05, -2.3417e-04],
        [-9.7081e-06, -6.8471e-06,  6.0648e-06,  ..., -7.5325e-06,
         -6.1467e-06, -6.7651e-06],
        [-2.2575e-05, -1.6391e-05,  1.4976e-05,  ..., -1.7375e-05,
         -1.4096e-05, -1.5795e-05],
        [-1.6168e-05, -1.1064e-05,  1.0595e-05,  ..., -1.2338e-05,
         -1.0066e-05, -1.1861e-05],
        [-2.3380e-05, -1.7941e-05,  1.5274e-05,  ..., -1.8701e-05,
         -1.5914e-05, -1.5154e-05]], device='cuda:0')
Loss: 0.9643964171409607


Running epoch 1, step 1118, batch 70
Sampled inputs[:2]: tensor([[   0,  957,  680,  ..., 2573,  669,   12],
        [   0,  333,  199,  ...,  292,   48, 1792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0611e-04, -3.5122e-04, -4.1961e-04,  ..., -5.8300e-05,
          5.1720e-06, -7.6301e-05],
        [-1.1250e-05, -7.8380e-06,  6.8694e-06,  ..., -8.8289e-06,
         -7.2643e-06, -8.0392e-06],
        [-2.6166e-05, -1.8850e-05,  1.7032e-05,  ..., -2.0355e-05,
         -1.6659e-05, -1.8701e-05],
        [-1.8790e-05, -1.2703e-05,  1.2025e-05,  ..., -1.4558e-05,
         -1.2003e-05, -1.4141e-05],
        [-2.7120e-05, -2.0668e-05,  1.7419e-05,  ..., -2.1890e-05,
         -1.8746e-05, -1.7956e-05]], device='cuda:0')
Loss: 0.9915726780891418


Running epoch 1, step 1119, batch 71
Sampled inputs[:2]: tensor([[    0, 13509,   472,  ...,  1805,    13, 27816],
        [    0,   265,  1781,  ...,   334,   344,   984]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8420e-04, -5.3066e-04, -6.4080e-04,  ..., -6.1268e-04,
          5.5972e-04,  1.1810e-05],
        [-1.2785e-05, -8.6352e-06,  7.7933e-06,  ..., -1.0125e-05,
         -8.3521e-06, -9.4324e-06],
        [-2.9579e-05, -2.0705e-05,  1.9252e-05,  ..., -2.3112e-05,
         -1.8939e-05, -2.1622e-05],
        [-2.1219e-05, -1.3873e-05,  1.3620e-05,  ..., -1.6585e-05,
         -1.3731e-05, -1.6406e-05],
        [-3.1143e-05, -2.3037e-05,  2.0027e-05,  ..., -2.5243e-05,
         -2.1622e-05, -2.1130e-05]], device='cuda:0')
Loss: 0.9354920983314514
Graident accumulation at epoch 1, step 1119, batch 71
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0059, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0036,  ..., -0.0098, -0.0027, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.0472e-05, -4.9209e-05, -1.7366e-04,  ..., -1.2616e-04,
          5.8267e-05, -6.6362e-05],
        [-1.0655e-05, -3.3137e-06,  3.0024e-06,  ..., -6.8162e-06,
         -4.4217e-06, -5.8326e-06],
        [ 1.3592e-05,  2.6188e-05, -1.1543e-05,  ...,  1.4482e-05,
          2.5308e-05, -4.4992e-07],
        [-1.9604e-05, -1.1122e-05,  1.3442e-05,  ..., -1.6531e-05,
         -1.1347e-05, -1.3566e-05],
        [-2.2724e-05, -1.6483e-05,  1.4746e-05,  ..., -2.0709e-05,
         -1.4733e-05, -1.5362e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8327e-08, 5.1338e-08, 4.9605e-08,  ..., 1.7668e-08, 1.1880e-07,
         3.8295e-08],
        [7.9984e-11, 4.9628e-11, 1.8557e-11,  ..., 5.5558e-11, 1.8007e-11,
         2.3074e-11],
        [1.6774e-09, 1.0575e-09, 3.3404e-10,  ..., 1.2723e-09, 4.9906e-10,
         3.9456e-10],
        [4.3084e-10, 3.2845e-10, 1.4174e-10,  ..., 3.8563e-10, 1.5099e-10,
         1.6016e-10],
        [3.8212e-10, 2.0843e-10, 6.0227e-11,  ..., 2.7648e-10, 6.3366e-11,
         1.0616e-10]], device='cuda:0')
optimizer state dict: 140.0
lr: [9.373673982939395e-06, 9.373673982939395e-06]
scheduler_last_epoch: 140


Running epoch 1, step 1120, batch 72
Sampled inputs[:2]: tensor([[    0, 11030,    72,  ...,   259, 16979,  9415],
        [    0,   292,   494,  ...,   259, 14134, 11544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1995e-05,  7.7491e-05, -6.8174e-05,  ..., -9.0353e-05,
          5.7811e-05, -1.2513e-04],
        [-1.6242e-06, -1.1697e-06,  1.0133e-06,  ..., -1.3262e-06,
         -1.0803e-06, -1.1325e-06],
        [-3.7402e-06, -2.7716e-06,  2.5034e-06,  ..., -3.0100e-06,
         -2.4587e-06, -2.5928e-06],
        [-2.6375e-06, -1.8403e-06,  1.7360e-06,  ..., -2.1309e-06,
         -1.7509e-06, -1.9372e-06],
        [-4.0829e-06, -3.1888e-06,  2.6971e-06,  ..., -3.3826e-06,
         -2.8610e-06, -2.6226e-06]], device='cuda:0')
Loss: 0.9896472692489624


Running epoch 1, step 1121, batch 73
Sampled inputs[:2]: tensor([[    0,    14,   292,  ...,  1385,    12,   287],
        [    0, 47831,   266,  ...,    66,    17, 20005]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4647e-04,  7.7491e-05, -1.1951e-04,  ..., -5.9506e-05,
          2.5107e-05, -1.2688e-04],
        [-3.3155e-06, -2.2277e-06,  1.9893e-06,  ..., -2.6450e-06,
         -2.0936e-06, -2.3246e-06],
        [-7.6741e-06, -5.3197e-06,  4.9472e-06,  ..., -6.0350e-06,
         -4.7535e-06, -5.3197e-06],
        [-5.4389e-06, -3.5167e-06,  3.4273e-06,  ..., -4.2915e-06,
         -3.4124e-06, -4.0084e-06],
        [-8.1658e-06, -5.9903e-06,  5.2005e-06,  ..., -6.6310e-06,
         -5.4538e-06, -5.2750e-06]], device='cuda:0')
Loss: 0.9955546259880066


Running epoch 1, step 1122, batch 74
Sampled inputs[:2]: tensor([[    0,   271,   266,  ..., 23648,   292, 21424],
        [    0,   346,    14,  ...,   381,   535,   505]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8584e-04,  1.0766e-04, -2.6902e-04,  ..., -9.7002e-05,
          2.4087e-05, -1.2688e-04],
        [-4.9323e-06, -3.3826e-06,  3.0547e-06,  ..., -3.9265e-06,
         -3.1143e-06, -3.4049e-06],
        [-1.1370e-05, -8.0764e-06,  7.5102e-06,  ..., -8.9556e-06,
         -7.0482e-06, -7.7784e-06],
        [-8.1062e-06, -5.3942e-06,  5.2452e-06,  ..., -6.4075e-06,
         -5.1111e-06, -5.9083e-06],
        [-1.2070e-05, -9.0450e-06,  7.8529e-06,  ..., -9.8050e-06,
         -8.0615e-06, -7.6741e-06]], device='cuda:0')
Loss: 0.9954724311828613


Running epoch 1, step 1123, batch 75
Sampled inputs[:2]: tensor([[    0,   287, 39084,  ...,   266,  1817,  1589],
        [    0,   300,  1635,  ...,   437,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1071e-04,  1.9404e-04, -2.9373e-04,  ..., -7.4252e-05,
         -1.9981e-05, -3.9615e-05],
        [-6.5863e-06, -4.3437e-06,  4.0531e-06,  ..., -5.2378e-06,
         -4.0494e-06, -4.6417e-06],
        [-1.5169e-05, -1.0371e-05,  9.9838e-06,  ..., -1.1921e-05,
         -9.1642e-06, -1.0520e-05],
        [-1.0818e-05, -6.9141e-06,  6.9588e-06,  ..., -8.5384e-06,
         -6.6236e-06, -8.0094e-06],
        [-1.6093e-05, -1.1578e-05,  1.0431e-05,  ..., -1.3024e-05,
         -1.0476e-05, -1.0401e-05]], device='cuda:0')
Loss: 0.9751019477844238


Running epoch 1, step 1124, batch 76
Sampled inputs[:2]: tensor([[    0,   300, 26138,  ...,  7856,    14, 17535],
        [    0,   685,  2461,  ...,   287,   298,  7943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4199e-04,  1.2464e-04, -3.2229e-04,  ..., -7.8841e-05,
         -1.3495e-05, -3.3983e-05],
        [-8.2776e-06, -5.4762e-06,  5.0962e-06,  ..., -6.5491e-06,
         -5.0925e-06, -5.8338e-06],
        [ 3.5214e-05,  6.2299e-05, -5.9185e-05,  ...,  2.9190e-05,
          5.7960e-05,  2.9165e-05],
        [-1.3605e-05, -8.7321e-06,  8.7321e-06,  ..., -1.0669e-05,
         -8.3074e-06, -1.0066e-05],
        [-2.0236e-05, -1.4603e-05,  1.3083e-05,  ..., -1.6347e-05,
         -1.3217e-05, -1.3173e-05]], device='cuda:0')
Loss: 1.0067061185836792


Running epoch 1, step 1125, batch 77
Sampled inputs[:2]: tensor([[   0,  266,  996,  ...,  709,  616, 9378],
        [   0, 1927,  287,  ..., 1027,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7911e-04, -1.8132e-04, -3.7535e-04,  ..., -2.1414e-04,
         -4.3577e-05, -1.1866e-04],
        [-9.9540e-06, -6.7577e-06,  6.1840e-06,  ..., -7.8604e-06,
         -6.1058e-06, -6.9141e-06],
        [ 3.1221e-05,  5.9184e-05, -5.6473e-05,  ...,  2.6090e-05,
          5.5590e-05,  2.6528e-05],
        [-1.6376e-05, -1.0803e-05,  1.0587e-05,  ..., -1.2800e-05,
         -9.9391e-06, -1.1951e-05],
        [-2.4319e-05, -1.7971e-05,  1.5825e-05,  ..., -1.9655e-05,
         -1.5870e-05, -1.5691e-05]], device='cuda:0')
Loss: 0.9951367378234863


Running epoch 1, step 1126, batch 78
Sampled inputs[:2]: tensor([[   0,  352,  927,  ..., 1521, 3513,  292],
        [   0,   14, 3921,  ...,  199, 2038, 1963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4146e-04, -1.2179e-04, -3.7410e-04,  ..., -2.2252e-04,
         -6.0950e-05, -1.2064e-04],
        [-1.1556e-05, -7.8604e-06,  7.2122e-06,  ..., -9.1121e-06,
         -7.0371e-06, -7.9647e-06],
        [ 2.7481e-05,  5.6562e-05, -5.3939e-05,  ...,  2.3199e-05,
          5.3460e-05,  2.4069e-05],
        [-1.9088e-05, -1.2591e-05,  1.2405e-05,  ..., -1.4901e-05,
         -1.1489e-05, -1.3851e-05],
        [-2.8163e-05, -2.0847e-05,  1.8418e-05,  ..., -2.2739e-05,
         -1.8254e-05, -1.8030e-05]], device='cuda:0')
Loss: 0.9939791560173035


Running epoch 1, step 1127, batch 79
Sampled inputs[:2]: tensor([[    0,   677, 25912,  ...,  2337,   292,  4462],
        [    0, 19444,  6307,  ...,    13, 38005,  1447]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9271e-04,  2.6544e-04, -1.0684e-04,  ..., -2.2199e-04,
          1.0242e-04, -1.2693e-05],
        [-1.3195e-05, -9.1419e-06,  8.0131e-06,  ..., -1.0476e-05,
         -8.2441e-06, -9.1717e-06],
        [ 2.3740e-05,  5.3477e-05, -5.1958e-05,  ...,  2.0100e-05,
          5.0688e-05,  2.1327e-05],
        [-2.1711e-05, -1.4633e-05,  1.3717e-05,  ..., -1.7092e-05,
         -1.3486e-05, -1.5892e-05],
        [-3.2395e-05, -2.4483e-05,  2.0623e-05,  ..., -2.6345e-05,
         -2.1577e-05, -2.0936e-05]], device='cuda:0')
Loss: 0.9878728985786438
Graident accumulation at epoch 1, step 1127, batch 79
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0027, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0145, -0.0174]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.6696e-05, -1.7745e-05, -1.6698e-04,  ..., -1.3574e-04,
          6.2682e-05, -6.0995e-05],
        [-1.0909e-05, -3.8965e-06,  3.5034e-06,  ..., -7.1821e-06,
         -4.8039e-06, -6.1665e-06],
        [ 1.4607e-05,  2.8917e-05, -1.5584e-05,  ...,  1.5044e-05,
          2.7846e-05,  1.7278e-06],
        [-1.9815e-05, -1.1473e-05,  1.3470e-05,  ..., -1.6587e-05,
         -1.1561e-05, -1.3799e-05],
        [-2.3691e-05, -1.7283e-05,  1.5334e-05,  ..., -2.1273e-05,
         -1.5417e-05, -1.5919e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8354e-08, 5.1357e-08, 4.9567e-08,  ..., 1.7700e-08, 1.1869e-07,
         3.8257e-08],
        [8.0078e-11, 4.9662e-11, 1.8603e-11,  ..., 5.5612e-11, 1.8057e-11,
         2.3135e-11],
        [1.6763e-09, 1.0593e-09, 3.3641e-10,  ..., 1.2715e-09, 5.0113e-10,
         3.9462e-10],
        [4.3088e-10, 3.2833e-10, 1.4178e-10,  ..., 3.8554e-10, 1.5102e-10,
         1.6025e-10],
        [3.8279e-10, 2.0882e-10, 6.0592e-11,  ..., 2.7690e-10, 6.3768e-11,
         1.0649e-10]], device='cuda:0')
optimizer state dict: 141.0
lr: [9.250351066628025e-06, 9.250351066628025e-06]
scheduler_last_epoch: 141


Running epoch 1, step 1128, batch 80
Sampled inputs[:2]: tensor([[    0,   287,  6761,  ...,  1918, 33351,    12],
        [    0,   271,   266,  ...,  4298,  1231,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1288e-04,  6.3225e-05,  8.1077e-05,  ...,  1.2242e-05,
          1.0411e-04,  1.3088e-04],
        [-1.6466e-06, -1.1846e-06,  8.6054e-07,  ..., -1.3858e-06,
         -1.1250e-06, -1.2890e-06],
        [-3.7700e-06, -2.7716e-06,  2.1607e-06,  ..., -3.0845e-06,
         -2.4885e-06, -2.8461e-06],
        [-2.6673e-06, -1.8403e-06,  1.4380e-06,  ..., -2.2203e-06,
         -1.7956e-06, -2.1607e-06],
        [-4.3213e-06, -3.2783e-06,  2.4736e-06,  ..., -3.6210e-06,
         -3.0249e-06, -3.0696e-06]], device='cuda:0')
Loss: 1.0147877931594849


Running epoch 1, step 1129, batch 81
Sampled inputs[:2]: tensor([[    0,   729,  3084,  ...,   381,  1445,   642],
        [    0, 17694,    12,  ..., 12452,   446,   475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2532e-04,  6.5024e-05,  6.7637e-05,  ..., -1.0563e-06,
          4.8952e-05,  2.8313e-05],
        [-3.3230e-06, -2.3022e-06,  1.9409e-06,  ..., -2.6748e-06,
         -2.0377e-06, -2.3544e-06],
        [ 1.3626e-04,  1.5045e-04, -6.2222e-05,  ...,  1.0170e-04,
          1.2010e-04,  7.7757e-05],
        [-5.4687e-06, -3.6433e-06,  3.3155e-06,  ..., -4.3362e-06,
         -3.2857e-06, -4.0308e-06],
        [-8.3447e-06, -6.2287e-06,  5.2005e-06,  ..., -6.7949e-06,
         -5.3942e-06, -5.4389e-06]], device='cuda:0')
Loss: 1.0101969242095947


Running epoch 1, step 1130, batch 82
Sampled inputs[:2]: tensor([[    0,  4337,  2057,  ...,  3020,  1722,   369],
        [    0, 12472,  1059,  ...,   642,   365,  6517]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5767e-04,  5.4172e-05,  7.4883e-05,  ...,  9.8853e-05,
         -1.6877e-04, -3.1226e-05],
        [-4.8578e-06, -3.3453e-06,  2.9989e-06,  ..., -3.8818e-06,
         -2.9355e-06, -3.3975e-06],
        [ 1.3263e-04,  1.4794e-04, -5.9585e-05,  ...,  9.8886e-05,
          1.1805e-04,  7.5328e-05],
        [-8.1211e-06, -5.3793e-06,  5.2378e-06,  ..., -6.3926e-06,
         -4.8056e-06, -5.9158e-06],
        [-1.2130e-05, -9.0003e-06,  7.8976e-06,  ..., -9.8050e-06,
         -7.7188e-06, -7.7784e-06]], device='cuda:0')
Loss: 0.9627712368965149


Running epoch 1, step 1131, batch 83
Sampled inputs[:2]: tensor([[    0,   292,   380,  ...,   527, 37357,    12],
        [    0,  2706,   292,  ...,    13,  8954,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5767e-04,  1.6752e-04,  2.5518e-04,  ...,  2.0222e-05,
          6.4629e-05, -6.7577e-05],
        [-6.3777e-06, -4.5002e-06,  3.8520e-06,  ..., -5.1856e-06,
         -4.0084e-06, -4.5598e-06],
        [ 1.2911e-04,  1.4519e-04, -5.7409e-05,  ...,  9.5920e-05,
          1.1559e-04,  7.2750e-05],
        [ 9.2782e-05,  1.9343e-04, -9.0260e-05,  ...,  1.2859e-04,
          1.1493e-04,  7.3310e-05],
        [-1.6034e-05, -1.2159e-05,  1.0312e-05,  ..., -1.3173e-05,
         -1.0595e-05, -1.0446e-05]], device='cuda:0')
Loss: 1.0001577138900757


Running epoch 1, step 1132, batch 84
Sampled inputs[:2]: tensor([[    0, 29368,    13,  ...,   376,    88,  3333],
        [    0,   278,  1059,  ...,   300,  1877,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7064e-04,  2.6242e-04,  2.3448e-04,  ...,  8.0987e-05,
          1.8627e-06, -6.6076e-05],
        [-7.9870e-06, -5.6699e-06,  4.8354e-06,  ..., -6.4671e-06,
         -5.0142e-06, -5.6103e-06],
        [ 1.2544e-04,  1.4247e-04, -5.5025e-05,  ...,  9.3029e-05,
          1.1334e-04,  7.0380e-05],
        [ 9.0145e-05,  1.9157e-04, -8.8576e-05,  ...,  1.2652e-04,
          1.1332e-04,  7.1507e-05],
        [-1.9997e-05, -1.5259e-05,  1.2860e-05,  ..., -1.6391e-05,
         -1.3217e-05, -1.2845e-05]], device='cuda:0')
Loss: 1.006748914718628


Running epoch 1, step 1133, batch 85
Sampled inputs[:2]: tensor([[    0,   968,   266,  ...,   287,  2143, 15228],
        [    0,  1234,   408,  ...,   292, 17323,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7630e-04,  2.0811e-04,  1.1644e-04,  ...,  2.9574e-05,
         -8.8886e-05, -4.5909e-05],
        [-9.6411e-06, -6.7949e-06,  5.9158e-06,  ..., -7.7039e-06,
         -5.9307e-06, -6.6981e-06],
        [ 1.2154e-04,  1.3975e-04, -5.2358e-05,  ...,  9.0139e-05,
          1.1124e-04,  6.7832e-05],
        [ 8.7328e-05,  1.8972e-04, -8.6676e-05,  ...,  1.2445e-04,
          1.1180e-04,  6.9570e-05],
        [-2.4140e-05, -1.8299e-05,  1.5661e-05,  ..., -1.9580e-05,
         -1.5646e-05, -1.5363e-05]], device='cuda:0')
Loss: 0.9697474837303162


Running epoch 1, step 1134, batch 86
Sampled inputs[:2]: tensor([[    0,   638,  1862,  ...,    14,  7869,    14],
        [    0, 18197,  1340,  ...,   360,   266,  1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8847e-04,  3.8148e-04,  1.5518e-04,  ..., -4.4949e-06,
         -1.9168e-04,  7.3949e-05],
        [-1.1295e-05, -7.8678e-06,  6.9216e-06,  ..., -8.9556e-06,
         -6.8992e-06, -7.8529e-06],
        [ 1.1775e-04,  1.3715e-04, -4.9899e-05,  ...,  8.7278e-05,
          1.0900e-04,  6.5210e-05],
        [ 8.4646e-05,  1.8800e-04, -8.4970e-05,  ...,  1.2242e-04,
          1.1020e-04,  6.7618e-05],
        [-2.8074e-05, -2.1160e-05,  1.8194e-05,  ..., -2.2665e-05,
         -1.8165e-05, -1.7911e-05]], device='cuda:0')
Loss: 0.9572157263755798


Running epoch 1, step 1135, batch 87
Sampled inputs[:2]: tensor([[    0,  6124,  1209,  ...,  1176,  3164,   271],
        [    0, 12923,  2489,  ...,   474,  3301,    54]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2017e-06,  3.7235e-04,  1.0961e-04,  ...,  3.7813e-05,
         -2.2755e-04, -1.8394e-04],
        [-1.2912e-05, -8.9854e-06,  7.9796e-06,  ..., -1.0155e-05,
         -7.6853e-06, -8.8140e-06],
        [ 1.1391e-04,  1.3444e-04, -4.7232e-05,  ...,  8.4446e-05,
          1.0718e-04,  6.2900e-05],
        [ 8.1934e-05,  1.8618e-04, -8.3115e-05,  ...,  1.2044e-04,
          1.0893e-04,  6.5904e-05],
        [-3.1859e-05, -2.3991e-05,  2.0772e-05,  ..., -2.5570e-05,
         -2.0161e-05, -1.9997e-05]], device='cuda:0')
Loss: 0.970183253288269
Graident accumulation at epoch 1, step 1135, batch 87
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0098, -0.0027, -0.0342],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0145, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.1546e-05,  2.1264e-05, -1.3932e-04,  ..., -1.1839e-04,
          3.3660e-05, -7.3290e-05],
        [-1.1109e-05, -4.4054e-06,  3.9511e-06,  ..., -7.4794e-06,
         -5.0920e-06, -6.4312e-06],
        [ 2.4537e-05,  3.9469e-05, -1.8749e-05,  ...,  2.1984e-05,
          3.5779e-05,  7.8450e-06],
        [-9.6401e-06,  8.2922e-06,  3.8112e-06,  ..., -2.8842e-06,
          4.8834e-07, -5.8286e-06],
        [-2.4507e-05, -1.7954e-05,  1.5878e-05,  ..., -2.1703e-05,
         -1.5892e-05, -1.6327e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8296e-08, 5.1444e-08, 4.9530e-08,  ..., 1.7683e-08, 1.1863e-07,
         3.8253e-08],
        [8.0164e-11, 4.9693e-11, 1.8648e-11,  ..., 5.5660e-11, 1.8098e-11,
         2.3190e-11],
        [1.6876e-09, 1.0763e-09, 3.3830e-10,  ..., 1.2773e-09, 5.1212e-10,
         3.9819e-10],
        [4.3716e-10, 3.6267e-10, 1.4855e-10,  ..., 3.9966e-10, 1.6274e-10,
         1.6444e-10],
        [3.8342e-10, 2.0919e-10, 6.0963e-11,  ..., 2.7728e-10, 6.4111e-11,
         1.0678e-10]], device='cuda:0')
optimizer state dict: 142.0
lr: [9.12714270321745e-06, 9.12714270321745e-06]
scheduler_last_epoch: 142


Running epoch 1, step 1136, batch 88
Sampled inputs[:2]: tensor([[   0, 6518,  681,  ...,  401, 9748,  391],
        [   0,   25,   26,  ...,    9,  287,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1369e-05, -8.6289e-05, -5.3785e-06,  ...,  1.9490e-05,
          1.5574e-04,  1.3580e-04],
        [-1.6913e-06, -1.1176e-06,  1.0207e-06,  ..., -1.3933e-06,
         -1.0431e-06, -1.3113e-06],
        [-4.0531e-06, -2.7269e-06,  2.6077e-06,  ..., -3.2336e-06,
         -2.3842e-06, -3.0547e-06],
        [-2.6971e-06, -1.7136e-06,  1.6689e-06,  ..., -2.1905e-06,
         -1.6168e-06, -2.1458e-06],
        [-4.4107e-06, -3.0845e-06,  2.8163e-06,  ..., -3.6359e-06,
         -2.8014e-06, -3.1143e-06]], device='cuda:0')
Loss: 1.0064815282821655


Running epoch 1, step 1137, batch 89
Sampled inputs[:2]: tensor([[    0,     9,   287,  ...,   259,  8244,  1143],
        [    0, 10251,   278,  ...,   278,   319,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8886e-05, -1.0134e-04, -9.1176e-05,  ...,  2.5206e-05,
          1.8564e-04,  1.8513e-04],
        [-3.3453e-06, -2.3320e-06,  2.1160e-06,  ..., -2.6748e-06,
         -2.0042e-06, -2.3991e-06],
        [-7.9274e-06, -5.6624e-06,  5.3048e-06,  ..., -6.2138e-06,
         -4.6194e-06, -5.5879e-06],
        [-5.4389e-06, -3.6806e-06,  3.5465e-06,  ..., -4.2915e-06,
         -3.2037e-06, -4.0382e-06],
        [-8.4937e-06, -6.3181e-06,  5.6177e-06,  ..., -6.8694e-06,
         -5.3346e-06, -5.6028e-06]], device='cuda:0')
Loss: 0.9809109568595886


Running epoch 1, step 1138, batch 90
Sampled inputs[:2]: tensor([[    0,   616,  4935,  ...,    89,  4448,   271],
        [    0,  2715,  1478,  ...,  1171,  4697, 41847]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2244e-05, -1.3838e-04, -4.3643e-05,  ...,  1.3434e-05,
          2.2120e-04,  1.8011e-04],
        [-4.9621e-06, -3.6284e-06,  3.1665e-06,  ..., -3.9488e-06,
         -3.0324e-06, -3.4794e-06],
        [-1.1772e-05, -8.7917e-06,  7.9423e-06,  ..., -9.2238e-06,
         -7.0184e-06, -8.1658e-06],
        [-8.1062e-06, -5.7667e-06,  5.3570e-06,  ..., -6.3628e-06,
         -4.8801e-06, -5.9307e-06],
        [-1.2577e-05, -9.7901e-06,  8.3596e-06,  ..., -1.0148e-05,
         -8.0615e-06, -8.1360e-06]], device='cuda:0')
Loss: 0.9895243644714355


Running epoch 1, step 1139, batch 91
Sampled inputs[:2]: tensor([[    0, 13642, 14635,  ...,   367,  1040,  8580],
        [    0, 48598,  3313,  ...,  3482,    12,  1099]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0673e-05, -1.1144e-04, -4.3643e-05,  ...,  1.2501e-05,
          1.4575e-04,  1.8275e-04],
        [-6.6459e-06, -4.8876e-06,  4.2766e-06,  ..., -5.2080e-06,
         -3.9488e-06, -4.5300e-06],
        [-1.5736e-05, -1.1802e-05,  1.0684e-05,  ..., -1.2159e-05,
         -9.1195e-06, -1.0639e-05],
        [-1.0863e-05, -7.7635e-06,  7.2345e-06,  ..., -8.3894e-06,
         -6.3330e-06, -7.7486e-06],
        [-1.6689e-05, -1.3053e-05,  1.1161e-05,  ..., -1.3277e-05,
         -1.0431e-05, -1.0505e-05]], device='cuda:0')
Loss: 0.9955227971076965


Running epoch 1, step 1140, batch 92
Sampled inputs[:2]: tensor([[   0,  287, 6015,  ...,   14,  333,  199],
        [   0,  365, 2714,  ...,  298,  273,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9042e-05,  4.6349e-05, -5.4865e-05,  ...,  2.8056e-06,
          1.4575e-04,  1.9460e-04],
        [-8.3148e-06, -6.1244e-06,  5.2750e-06,  ..., -6.5342e-06,
         -4.9621e-06, -5.6475e-06],
        [-1.9610e-05, -1.4797e-05,  1.3173e-05,  ..., -1.5229e-05,
         -1.1474e-05, -1.3232e-05],
        [-1.3590e-05, -9.7752e-06,  8.9332e-06,  ..., -1.0550e-05,
         -8.0019e-06, -9.6709e-06],
        [-2.0772e-05, -1.6332e-05,  1.3769e-05,  ..., -1.6600e-05,
         -1.3083e-05, -1.3053e-05]], device='cuda:0')
Loss: 0.9900873303413391


Running epoch 1, step 1141, batch 93
Sampled inputs[:2]: tensor([[    0, 17442,  2416,  ...,  7244,    66, 16907],
        [    0,  3441,   796,  ...,  7561,  1711,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0816e-06,  9.9438e-05, -1.6514e-04,  ...,  3.6128e-05,
          5.8384e-05,  2.0382e-04],
        [-1.0021e-05, -7.2867e-06,  6.3702e-06,  ..., -7.8008e-06,
         -5.8636e-06, -6.7130e-06],
        [-2.3633e-05, -1.7628e-05,  1.5914e-05,  ..., -1.8194e-05,
         -1.3590e-05, -1.5751e-05],
        [-1.6451e-05, -1.1683e-05,  1.0833e-05,  ..., -1.2651e-05,
         -9.4920e-06, -1.1548e-05],
        [-2.4885e-05, -1.9357e-05,  1.6540e-05,  ..., -1.9729e-05,
         -1.5438e-05, -1.5438e-05]], device='cuda:0')
Loss: 1.010846495628357


Running epoch 1, step 1142, batch 94
Sampled inputs[:2]: tensor([[   0, 1481,  278,  ..., 3940, 4938,    5],
        [   0, 4108,   85,  ...,   40,   12, 1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0382e-05,  1.9936e-04, -7.4330e-05,  ...,  1.0907e-04,
          3.7164e-05,  2.5087e-04],
        [-1.1638e-05, -8.5086e-06,  7.3761e-06,  ..., -9.0674e-06,
         -6.8545e-06, -7.7337e-06],
        [-2.7478e-05, -2.0623e-05,  1.8463e-05,  ..., -2.1189e-05,
         -1.5914e-05, -1.8150e-05],
        [-1.9103e-05, -1.3649e-05,  1.2547e-05,  ..., -1.4722e-05,
         -1.1116e-05, -1.3322e-05],
        [-2.8908e-05, -2.2665e-05,  1.9193e-05,  ..., -2.2978e-05,
         -1.8075e-05, -1.7777e-05]], device='cuda:0')
Loss: 0.9957872033119202


Running epoch 1, step 1143, batch 95
Sampled inputs[:2]: tensor([[    0,    12, 20722,  ...,   266,  1916,  5341],
        [    0,    14, 49045,  ...,    12,   706,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3321e-05,  2.6900e-04, -1.3232e-04,  ...,  1.3710e-04,
          5.2695e-05,  3.0222e-04],
        [-1.3299e-05, -9.7901e-06,  8.4639e-06,  ..., -1.0349e-05,
         -7.8976e-06, -8.8215e-06],
        [ 4.3854e-05,  5.1742e-05,  8.6341e-06,  ...,  2.3903e-05,
          2.6839e-05,  2.1701e-05],
        [-2.1845e-05, -1.5736e-05,  1.4424e-05,  ..., -1.6809e-05,
         -1.2808e-05, -1.5207e-05],
        [-3.3081e-05, -2.6122e-05,  2.2069e-05,  ..., -2.6286e-05,
         -2.0891e-05, -2.0325e-05]], device='cuda:0')
Loss: 1.005335807800293
Graident accumulation at epoch 1, step 1143, batch 95
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7724e-05,  4.6038e-05, -1.3862e-04,  ..., -9.2840e-05,
          3.5563e-05, -3.5739e-05],
        [-1.1328e-05, -4.9439e-06,  4.4023e-06,  ..., -7.7664e-06,
         -5.3726e-06, -6.6703e-06],
        [ 2.6469e-05,  4.0696e-05, -1.6011e-05,  ...,  2.2176e-05,
          3.4885e-05,  9.2306e-06],
        [-1.0861e-05,  5.8895e-06,  4.8725e-06,  ..., -4.2766e-06,
         -8.4125e-07, -6.7664e-06],
        [-2.5365e-05, -1.8770e-05,  1.6497e-05,  ..., -2.2161e-05,
         -1.6392e-05, -1.6727e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8238e-08, 5.1465e-08, 4.9498e-08,  ..., 1.7684e-08, 1.1851e-07,
         3.8306e-08],
        [8.0261e-11, 4.9740e-11, 1.8701e-11,  ..., 5.5711e-11, 1.8142e-11,
         2.3245e-11],
        [1.6878e-09, 1.0779e-09, 3.3804e-10,  ..., 1.2766e-09, 5.1232e-10,
         3.9826e-10],
        [4.3720e-10, 3.6255e-10, 1.4861e-10,  ..., 3.9954e-10, 1.6274e-10,
         1.6450e-10],
        [3.8413e-10, 2.0966e-10, 6.1389e-11,  ..., 2.7769e-10, 6.4483e-11,
         1.0709e-10]], device='cuda:0')
optimizer state dict: 143.0
lr: [9.004067720021106e-06, 9.004067720021106e-06]
scheduler_last_epoch: 143


Running epoch 1, step 1144, batch 96
Sampled inputs[:2]: tensor([[   0,   14, 1266,  ..., 2288,  417,  199],
        [   0,   12,  630,  ..., 5049,   14, 2371]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5172e-05, -1.2147e-05, -7.2308e-06,  ...,  1.0052e-05,
          1.5359e-04,  1.1190e-04],
        [-1.7062e-06, -1.3039e-06,  1.0878e-06,  ..., -1.3337e-06,
         -1.0878e-06, -1.1176e-06],
        [ 7.0399e-05,  5.8037e-05, -4.9353e-05,  ...,  4.7766e-05,
          9.6738e-05,  2.8913e-05],
        [-2.7716e-06, -2.1011e-06,  1.8328e-06,  ..., -2.1607e-06,
         -1.7658e-06, -1.8999e-06],
        [-4.3511e-06, -3.5763e-06,  2.8908e-06,  ..., -3.5018e-06,
         -2.9653e-06, -2.6822e-06]], device='cuda:0')
Loss: 1.0260167121887207


Running epoch 1, step 1145, batch 97
Sampled inputs[:2]: tensor([[   0,   41,    7,  ...,  496,   14, 4075],
        [   0, 1526,  341,  ...,  271, 4401, 3341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.7678e-05,  7.5034e-05,  8.6371e-06,  ...,  1.1528e-04,
          6.5146e-05,  1.0704e-04],
        [-3.4571e-06, -2.4438e-06,  2.1905e-06,  ..., -2.6524e-06,
         -2.0787e-06, -2.3991e-06],
        [ 6.6226e-05,  5.5235e-05, -4.6566e-05,  ...,  4.4681e-05,
          9.4398e-05,  2.5888e-05],
        [-5.5879e-06, -3.9116e-06,  3.6880e-06,  ..., -4.2617e-06,
         -3.3602e-06, -4.0308e-06],
        [-8.6725e-06, -6.6161e-06,  5.7667e-06,  ..., -6.8247e-06,
         -5.5879e-06, -5.6028e-06]], device='cuda:0')
Loss: 0.9358940124511719


Running epoch 1, step 1146, batch 98
Sampled inputs[:2]: tensor([[    0,   352,  2284,  ..., 43204,    12,   709],
        [    0,  3037,  4511,  ...,  1711,    12,  2655]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0950e-04,  1.3249e-04, -5.1543e-05,  ...,  2.1223e-04,
         -1.2930e-04,  5.7504e-05],
        [-5.1185e-06, -3.5688e-06,  3.3528e-06,  ..., -3.8967e-06,
         -2.9169e-06, -3.4571e-06],
        [ 6.2233e-05,  5.2479e-05, -4.3646e-05,  ...,  4.1716e-05,
          9.2401e-05,  2.3340e-05],
        [-8.4043e-06, -5.7667e-06,  5.7295e-06,  ..., -6.3479e-06,
         -4.7535e-06, -5.9158e-06],
        [-1.2696e-05, -9.5814e-06,  8.6874e-06,  ..., -9.9242e-06,
         -7.8082e-06, -7.9870e-06]], device='cuda:0')
Loss: 0.9927608370780945


Running epoch 1, step 1147, batch 99
Sampled inputs[:2]: tensor([[   0, 6574, 1707,  ...,   14, 5077,   12],
        [   0,  669,  292,  ..., 4032,  271, 4442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0579e-04,  1.7586e-04, -1.3384e-05,  ...,  3.0624e-04,
         -1.8861e-04,  6.5655e-06],
        [-6.7800e-06, -4.7535e-06,  4.3437e-06,  ..., -5.2378e-06,
         -3.9302e-06, -4.6417e-06],
        [ 5.8329e-05,  4.9588e-05, -4.1157e-05,  ...,  3.8631e-05,
          9.0077e-05,  2.0628e-05],
        [-1.1206e-05, -7.7337e-06,  7.4506e-06,  ..., -8.5980e-06,
         -6.4597e-06, -8.0019e-06],
        [-1.6868e-05, -1.2785e-05,  1.1340e-05,  ..., -1.3307e-05,
         -1.0461e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9843944311141968


Running epoch 1, step 1148, batch 100
Sampled inputs[:2]: tensor([[   0, 1358,  367,  ..., 1758, 2921,   12],
        [   0,  266,  452,  ..., 1725, 2200,  342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1192e-04,  1.3669e-04, -1.5689e-04,  ...,  2.8886e-04,
         -1.9119e-04, -1.0857e-04],
        [-8.4490e-06, -5.8860e-06,  5.4017e-06,  ..., -6.5565e-06,
         -4.9286e-06, -5.8711e-06],
        [ 5.4395e-05,  4.6876e-05, -3.8505e-05,  ...,  3.5577e-05,
          8.7767e-05,  1.7752e-05],
        [-1.3903e-05, -9.5069e-06,  9.2313e-06,  ..., -1.0714e-05,
         -8.0466e-06, -1.0073e-05],
        [-2.1070e-05, -1.5855e-05,  1.4126e-05,  ..., -1.6689e-05,
         -1.3143e-05, -1.3560e-05]], device='cuda:0')
Loss: 0.9843423962593079


Running epoch 1, step 1149, batch 101
Sampled inputs[:2]: tensor([[    0, 19720,    12,  ...,  1239,    12, 22324],
        [    0,   721,  1119,  ...,   600,   328,  3363]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3742e-04,  1.2853e-04, -3.4448e-04,  ...,  2.6465e-04,
         -2.8931e-04, -8.1024e-05],
        [-1.0125e-05, -7.1451e-06,  6.6161e-06,  ..., -7.7859e-06,
         -5.8971e-06, -6.8843e-06],
        [ 5.0342e-05,  4.3791e-05, -3.5480e-05,  ...,  3.2641e-05,
          8.5457e-05,  1.5264e-05],
        [-1.6749e-05, -1.1593e-05,  1.1362e-05,  ..., -1.2770e-05,
         -9.6485e-06, -1.1884e-05],
        [-2.5183e-05, -1.9148e-05,  1.7166e-05,  ..., -1.9789e-05,
         -1.5676e-05, -1.5914e-05]], device='cuda:0')
Loss: 0.9853352308273315


Running epoch 1, step 1150, batch 102
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  342,  266, 1163],
        [   0, 2440, 1458,  ..., 7650,  328, 2297]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0407e-04,  7.9807e-05, -3.1826e-04,  ...,  2.7537e-04,
         -1.9130e-04, -9.3673e-05],
        [-1.1846e-05, -8.4862e-06,  7.8231e-06,  ..., -9.0972e-06,
         -6.9104e-06, -7.8902e-06],
        [ 4.6259e-05,  4.0543e-05, -3.2529e-05,  ...,  2.9527e-05,
          8.3073e-05,  1.2835e-05],
        [-1.9595e-05, -1.3769e-05,  1.3404e-05,  ..., -1.4916e-05,
         -1.1288e-05, -1.3635e-05],
        [-2.9504e-05, -2.2754e-05,  2.0266e-05,  ..., -2.3201e-05,
         -1.8388e-05, -1.8328e-05]], device='cuda:0')
Loss: 1.0331079959869385


Running epoch 1, step 1151, batch 103
Sampled inputs[:2]: tensor([[    0,  3779,    12,  ...,    12, 12774, 14261],
        [    0,  7036,   278,  ...,   221,   290,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8124e-04,  1.6551e-04, -4.8202e-05,  ...,  4.1812e-04,
         -1.4737e-04,  8.7754e-05],
        [-1.3418e-05, -9.7230e-06,  8.8066e-06,  ..., -1.0349e-05,
         -7.9162e-06, -8.9929e-06],
        [ 4.2489e-05,  3.7503e-05, -3.0026e-05,  ...,  2.6562e-05,
          8.0704e-05,  1.0212e-05],
        [-2.2218e-05, -1.5810e-05,  1.5095e-05,  ..., -1.7002e-05,
         -1.2979e-05, -1.5572e-05],
        [-3.3468e-05, -2.6107e-05,  2.2888e-05,  ..., -2.6435e-05,
         -2.1070e-05, -2.0906e-05]], device='cuda:0')
Loss: 0.9869469404220581
Graident accumulation at epoch 1, step 1151, batch 103
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.4827e-05,  5.7985e-05, -1.2958e-04,  ..., -4.1744e-05,
          1.7269e-05, -2.3390e-05],
        [-1.1537e-05, -5.4218e-06,  4.8428e-06,  ..., -8.0246e-06,
         -5.6270e-06, -6.9025e-06],
        [ 2.8071e-05,  4.0377e-05, -1.7412e-05,  ...,  2.2615e-05,
          3.9467e-05,  9.3288e-06],
        [-1.1996e-05,  3.7195e-06,  5.8948e-06,  ..., -5.5492e-06,
         -2.0550e-06, -7.6469e-06],
        [-2.6175e-05, -1.9504e-05,  1.7136e-05,  ..., -2.2588e-05,
         -1.6860e-05, -1.7145e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8212e-08, 5.1441e-08, 4.9450e-08,  ..., 1.7842e-08, 1.1841e-07,
         3.8275e-08],
        [8.0361e-11, 4.9784e-11, 1.8760e-11,  ..., 5.5763e-11, 1.8186e-11,
         2.3302e-11],
        [1.6879e-09, 1.0783e-09, 3.3860e-10,  ..., 1.2760e-09, 5.1833e-10,
         3.9796e-10],
        [4.3726e-10, 3.6244e-10, 1.4869e-10,  ..., 3.9943e-10, 1.6275e-10,
         1.6458e-10],
        [3.8487e-10, 2.1013e-10, 6.1851e-11,  ..., 2.7811e-10, 6.4863e-11,
         1.0742e-10]], device='cuda:0')
optimizer state dict: 144.0
lr: [8.881144923970756e-06, 8.881144923970756e-06]
scheduler_last_epoch: 144


Running epoch 1, step 1152, batch 104
Sampled inputs[:2]: tensor([[    0,    14,    71,  ...,   278, 14258, 12440],
        [    0,   806,   352,  ...,  3493,   352, 49256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8566e-05,  1.2888e-05,  5.7106e-05,  ...,  7.2350e-05,
         -7.4813e-05, -1.3875e-05],
        [-1.6540e-06, -1.1474e-06,  1.1399e-06,  ..., -1.3188e-06,
         -9.9838e-07, -1.0729e-06],
        [-3.8743e-06, -2.7418e-06,  2.8014e-06,  ..., -3.0547e-06,
         -2.2799e-06, -2.5183e-06],
        [-2.7865e-06, -1.8775e-06,  1.9968e-06,  ..., -2.1905e-06,
         -1.6391e-06, -1.8999e-06],
        [-4.0829e-06, -3.0547e-06,  2.9057e-06,  ..., -3.3379e-06,
         -2.6375e-06, -2.4885e-06]], device='cuda:0')
Loss: 1.011460781097412


Running epoch 1, step 1153, batch 105
Sampled inputs[:2]: tensor([[    0,     9,  8720,  ...,  1657,  1090, 27975],
        [    0, 13856,   278,  ...,    14,    69,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0196e-04, -4.2522e-05,  4.5777e-05,  ..., -1.2926e-05,
         -4.4123e-05, -1.3875e-05],
        [-3.3230e-06, -2.4438e-06,  2.2873e-06,  ..., -2.6226e-06,
         -2.0191e-06, -2.1309e-06],
        [-7.8678e-06, -5.9009e-06,  5.6624e-06,  ..., -6.1691e-06,
         -4.7088e-06, -5.0813e-06],
        [-5.5432e-06, -3.9786e-06,  3.9637e-06,  ..., -4.3213e-06,
         -3.2932e-06, -3.7402e-06],
        [-8.2552e-06, -6.5416e-06,  5.8711e-06,  ..., -6.7055e-06,
         -5.3793e-06, -5.0068e-06]], device='cuda:0')
Loss: 1.0126110315322876


Running epoch 1, step 1154, batch 106
Sampled inputs[:2]: tensor([[   0,  825, 3066,  ..., 1184,  266, 7964],
        [   0,  360,  259,  ..., 5710,  278, 2433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1014e-04,  1.2484e-04,  5.5549e-05,  ...,  5.6124e-05,
         -2.6389e-04,  4.8422e-05],
        [-5.0664e-06, -3.4571e-06,  3.4645e-06,  ..., -3.8967e-06,
         -2.8722e-06, -3.3304e-06],
        [-1.2010e-05, -8.4341e-06,  8.5831e-06,  ..., -9.1642e-06,
         -6.7353e-06, -7.9274e-06],
        [-8.4639e-06, -5.6401e-06,  5.9903e-06,  ..., -6.4373e-06,
         -4.7013e-06, -5.8264e-06],
        [-1.2457e-05, -9.2685e-06,  8.8066e-06,  ..., -9.8795e-06,
         -7.6443e-06, -7.7039e-06]], device='cuda:0')
Loss: 0.9817508459091187


Running epoch 1, step 1155, batch 107
Sampled inputs[:2]: tensor([[    0,  9458,   278,  ...,    15,  5251, 27858],
        [    0,  3059,  2013,  ...,   278,  1997,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1014e-04,  2.8015e-04,  1.5173e-04,  ...,  1.1877e-04,
         -3.5642e-04,  5.9166e-05],
        [-6.7502e-06, -4.5076e-06,  4.5225e-06,  ..., -5.1707e-06,
         -3.8147e-06, -4.6119e-06],
        [-1.6034e-05, -1.1057e-05,  1.1310e-05,  ..., -1.2174e-05,
         -8.9556e-06, -1.0937e-05],
        [-1.1206e-05, -7.2941e-06,  7.7933e-06,  ..., -8.4937e-06,
         -6.2212e-06, -8.0019e-06],
        [-1.6630e-05, -1.2115e-05,  1.1608e-05,  ..., -1.3113e-05,
         -1.0148e-05, -1.0625e-05]], device='cuda:0')
Loss: 0.9712711572647095


Running epoch 1, step 1156, batch 108
Sampled inputs[:2]: tensor([[    0,   747,  7890,  ...,   706,  8667,   271],
        [    0,   271,   266,  ..., 14308,   278,  9452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3960e-04,  3.8795e-04,  2.2444e-04,  ...,  3.2310e-04,
         -3.9732e-04,  2.2528e-04],
        [-8.4341e-06, -5.6624e-06,  5.4911e-06,  ..., -6.5416e-06,
         -4.9174e-06, -5.9158e-06],
        [-1.9968e-05, -1.3873e-05,  1.3754e-05,  ..., -1.5318e-05,
         -1.1504e-05, -1.3962e-05],
        [-1.3933e-05, -9.1195e-06,  9.4175e-06,  ..., -1.0714e-05,
         -8.0317e-06, -1.0237e-05],
        [-2.0862e-05, -1.5303e-05,  1.4216e-05,  ..., -1.6600e-05,
         -1.3083e-05, -1.3649e-05]], device='cuda:0')
Loss: 0.984093427658081


Running epoch 1, step 1157, batch 109
Sampled inputs[:2]: tensor([[    0,    14,   333,  ...,   328,  5453,  4713],
        [    0, 22599,  1336,  ...,   729,   923,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3909e-04,  4.7551e-04,  2.6192e-04,  ...,  3.3632e-04,
         -6.1263e-04,  5.9863e-05],
        [-1.0133e-05, -6.9216e-06,  6.6385e-06,  ..., -7.8380e-06,
         -5.9083e-06, -6.9961e-06],
        [-2.3931e-05, -1.6883e-05,  1.6585e-05,  ..., -1.8328e-05,
         -1.3769e-05, -1.6525e-05],
        [-1.6779e-05, -1.1191e-05,  1.1414e-05,  ..., -1.2860e-05,
         -9.6560e-06, -1.2159e-05],
        [-2.4974e-05, -1.8567e-05,  1.7092e-05,  ..., -1.9819e-05,
         -1.5631e-05, -1.6093e-05]], device='cuda:0')
Loss: 1.0021880865097046


Running epoch 1, step 1158, batch 110
Sampled inputs[:2]: tensor([[    0,  2626,    13,  ...,   300,   369,   259],
        [    0,    12, 17340,  ...,   408,  1550,  2415]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8743e-04,  6.0537e-04,  2.0503e-04,  ...,  2.7768e-04,
         -5.9165e-04,  3.6093e-05],
        [-1.1779e-05, -8.1658e-06,  7.7263e-06,  ..., -9.0823e-06,
         -6.9067e-06, -8.0913e-06],
        [-2.7806e-05, -1.9923e-05,  1.9297e-05,  ..., -2.1264e-05,
         -1.6138e-05, -1.9103e-05],
        [-1.9580e-05, -1.3277e-05,  1.3337e-05,  ..., -1.4961e-05,
         -1.1362e-05, -1.4126e-05],
        [-2.9027e-05, -2.1905e-05,  1.9908e-05,  ..., -2.2978e-05,
         -1.8299e-05, -1.8582e-05]], device='cuda:0')
Loss: 0.9884634613990784


Running epoch 1, step 1159, batch 111
Sampled inputs[:2]: tensor([[    0, 11325,   278,  ...,   446,  1869,   642],
        [    0,   775,   266,  ...,   409,   328,  5768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9182e-04,  3.5811e-04,  1.5328e-04,  ...,  2.6282e-04,
         -4.9480e-04,  6.2015e-05],
        [-1.3448e-05, -9.3579e-06,  8.8289e-06,  ..., -1.0386e-05,
         -7.8902e-06, -9.1866e-06],
        [-3.1859e-05, -2.2888e-05,  2.2128e-05,  ..., -2.4408e-05,
         -1.8477e-05, -2.1771e-05],
        [-2.2352e-05, -1.5214e-05,  1.5236e-05,  ..., -1.7107e-05,
         -1.2964e-05, -1.6049e-05],
        [-3.3170e-05, -2.5094e-05,  2.2754e-05,  ..., -2.6286e-05,
         -2.0877e-05, -2.1130e-05]], device='cuda:0')
Loss: 1.014866828918457
Graident accumulation at epoch 1, step 1159, batch 111
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6838e-05,  8.7997e-05, -1.0129e-04,  ..., -1.1287e-05,
         -3.3937e-05, -1.4849e-05],
        [-1.1728e-05, -5.8154e-06,  5.2414e-06,  ..., -8.2608e-06,
         -5.8533e-06, -7.1309e-06],
        [ 2.2078e-05,  3.4050e-05, -1.3458e-05,  ...,  1.7912e-05,
          3.3673e-05,  6.2189e-06],
        [-1.3032e-05,  1.8261e-06,  6.8289e-06,  ..., -6.7049e-06,
         -3.1459e-06, -8.4871e-06],
        [-2.6875e-05, -2.0063e-05,  1.7698e-05,  ..., -2.2958e-05,
         -1.7261e-05, -1.7543e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8307e-08, 5.1518e-08, 4.9425e-08,  ..., 1.7893e-08, 1.1854e-07,
         3.8241e-08],
        [8.0461e-11, 4.9822e-11, 1.8819e-11,  ..., 5.5815e-11, 1.8230e-11,
         2.3363e-11],
        [1.6873e-09, 1.0777e-09, 3.3875e-10,  ..., 1.2754e-09, 5.1815e-10,
         3.9804e-10],
        [4.3732e-10, 3.6231e-10, 1.4877e-10,  ..., 3.9932e-10, 1.6275e-10,
         1.6467e-10],
        [3.8558e-10, 2.1055e-10, 6.2307e-11,  ..., 2.7852e-10, 6.5234e-11,
         1.0776e-10]], device='cuda:0')
optimizer state dict: 145.0
lr: [8.758393098742647e-06, 8.758393098742647e-06]
scheduler_last_epoch: 145


Running epoch 1, step 1160, batch 112
Sampled inputs[:2]: tensor([[    0,    14, 15670,  ...,  2027,   417,   199],
        [    0, 12449,    12,  ...,   292,  2178,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2120e-05,  2.5239e-05,  2.3045e-04,  ...,  7.5278e-05,
          6.0557e-05,  1.5715e-04],
        [-1.7807e-06, -1.1623e-06,  9.4995e-07,  ..., -1.4454e-06,
         -1.2368e-06, -1.3486e-06],
        [-3.8743e-06, -2.7269e-06,  2.3246e-06,  ..., -3.1441e-06,
         -2.7120e-06, -2.8759e-06],
        [-2.8163e-06, -1.8254e-06,  1.5795e-06,  ..., -2.2948e-06,
         -2.0117e-06, -2.2203e-06],
        [-4.3511e-06, -3.1590e-06,  2.5779e-06,  ..., -3.6359e-06,
         -3.2187e-06, -3.0100e-06]], device='cuda:0')
Loss: 0.9326043128967285


Running epoch 1, step 1161, batch 113
Sampled inputs[:2]: tensor([[   0,   13, 7805,  ..., 2733,   12,  287],
        [   0, 7432,  287,  ...,   12,  461, 2652]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3437e-04,  7.9925e-05,  2.6672e-04,  ...,  8.0949e-05,
          1.1971e-04,  3.0343e-04],
        [-3.4943e-06, -2.4736e-06,  2.0824e-06,  ..., -2.7716e-06,
         -2.2799e-06, -2.4438e-06],
        [-7.8678e-06, -5.8711e-06,  5.0962e-06,  ..., -6.2436e-06,
         -5.1558e-06, -5.4687e-06],
        [-5.6326e-06, -3.9563e-06,  3.5018e-06,  ..., -4.4703e-06,
         -3.7327e-06, -4.1425e-06],
        [-8.5831e-06, -6.6608e-06,  5.4985e-06,  ..., -7.0333e-06,
         -6.0052e-06, -5.5730e-06]], device='cuda:0')
Loss: 1.0217993259429932


Running epoch 1, step 1162, batch 114
Sampled inputs[:2]: tensor([[    0,   278, 30377,  ...,    13,    83,  2908],
        [    0,   266,   923,  ...,    14,   298, 12230]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0621e-04,  6.7448e-05,  3.0258e-04,  ...,  1.5635e-06,
          1.4768e-04,  2.4949e-04],
        [-5.1782e-06, -3.6359e-06,  3.1702e-06,  ..., -4.0829e-06,
         -3.3304e-06, -3.6359e-06],
        [-1.1802e-05, -8.6278e-06,  7.7784e-06,  ..., -9.2387e-06,
         -7.5400e-06, -8.2105e-06],
        [-8.3596e-06, -5.7742e-06,  5.3272e-06,  ..., -6.5565e-06,
         -5.3942e-06, -6.1542e-06],
        [-1.2726e-05, -9.7454e-06,  8.2999e-06,  ..., -1.0312e-05,
         -8.7172e-06, -8.2403e-06]], device='cuda:0')
Loss: 0.982452929019928


Running epoch 1, step 1163, batch 115
Sampled inputs[:2]: tensor([[    0,  2085,    12,  ...,   496,    14,   747],
        [    0,    12,   401,  ...,  7665,  4101, 10193]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9966e-05,  8.8468e-05,  3.9082e-04,  ...,  4.4565e-05,
          1.2743e-04,  1.5761e-04],
        [-6.7875e-06, -4.7237e-06,  4.3251e-06,  ..., -5.3272e-06,
         -4.2990e-06, -4.7237e-06],
        [-1.5467e-05, -1.1131e-05,  1.0535e-05,  ..., -1.1995e-05,
         -9.6709e-06, -1.0654e-05],
        [-1.1027e-05, -7.5027e-06,  7.3239e-06,  ..., -8.5831e-06,
         -6.9737e-06, -8.0615e-06],
        [-1.6659e-05, -1.2591e-05,  1.1206e-05,  ..., -1.3366e-05,
         -1.1191e-05, -1.0639e-05]], device='cuda:0')
Loss: 0.9780778288841248


Running epoch 1, step 1164, batch 116
Sampled inputs[:2]: tensor([[   0,  895, 4110,  ..., 1578, 1245,   13],
        [   0,  266,  298,  ...,  654,  271, 4483]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4618e-05, -1.0298e-04,  2.4761e-04,  ..., -1.1629e-04,
          1.0474e-04,  3.0298e-05],
        [-8.5309e-06, -5.9977e-06,  5.4650e-06,  ..., -6.6757e-06,
         -5.4240e-06, -5.9381e-06],
        [-1.9580e-05, -1.4246e-05,  1.3381e-05,  ..., -1.5154e-05,
         -1.2293e-05, -1.3530e-05],
        [-1.3873e-05, -9.5442e-06,  9.2462e-06,  ..., -1.0774e-05,
         -8.8140e-06, -1.0163e-05],
        [-2.1160e-05, -1.6138e-05,  1.4275e-05,  ..., -1.6928e-05,
         -1.4231e-05, -1.3560e-05]], device='cuda:0')
Loss: 1.008519172668457


Running epoch 1, step 1165, batch 117
Sampled inputs[:2]: tensor([[   0,  287, 6932,  ..., 1549, 1480,  518],
        [   0,  287,  358,  ...,  328, 1704, 3227]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7645e-04,  5.0938e-06,  3.6324e-04,  ..., -1.9072e-04,
          4.3639e-05,  3.8998e-05],
        [-1.0192e-05, -7.2420e-06,  6.5826e-06,  ..., -7.9945e-06,
         -6.4820e-06, -7.0333e-06],
        [-2.3514e-05, -1.7315e-05,  1.6183e-05,  ..., -1.8254e-05,
         -1.4797e-05, -1.6153e-05],
        [-1.6659e-05, -1.1615e-05,  1.1198e-05,  ..., -1.2964e-05,
         -1.0587e-05, -1.2115e-05],
        [-2.5302e-05, -1.9535e-05,  1.7181e-05,  ..., -2.0310e-05,
         -1.7077e-05, -1.6138e-05]], device='cuda:0')
Loss: 0.9961714744567871


Running epoch 1, step 1166, batch 118
Sampled inputs[:2]: tensor([[    0,    17,   292,  ...,  8055,   365,  3125],
        [    0,  2467, 18011,  ...,  5913,  9281,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7446e-04,  8.4055e-05,  4.6524e-04,  ..., -1.3021e-04,
          1.3762e-04,  4.8829e-05],
        [-1.1899e-05, -8.4639e-06,  7.6480e-06,  ..., -9.4175e-06,
         -7.6219e-06, -8.2776e-06],
        [-2.7359e-05, -2.0176e-05,  1.8761e-05,  ..., -2.1428e-05,
         -1.7360e-05, -1.8939e-05],
        [-1.9401e-05, -1.3538e-05,  1.2986e-05,  ..., -1.5244e-05,
         -1.2435e-05, -1.4231e-05],
        [-2.9564e-05, -2.2903e-05,  1.9997e-05,  ..., -2.3946e-05,
         -2.0146e-05, -1.9029e-05]], device='cuda:0')
Loss: 0.9972792267799377


Running epoch 1, step 1167, batch 119
Sampled inputs[:2]: tensor([[   0,  278, 1099,  ...,  496,   14,  879],
        [   0, 6294,  367,  ...,  496,   14,   18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3911e-05, -3.2104e-05,  3.8927e-04,  ..., -1.1272e-05,
          1.0757e-04,  2.1757e-04],
        [-1.3672e-05, -9.6485e-06,  8.6986e-06,  ..., -1.0788e-05,
         -8.6948e-06, -9.5293e-06],
        [-3.1382e-05, -2.2992e-05,  2.1353e-05,  ..., -2.4498e-05,
         -1.9759e-05, -2.1741e-05],
        [-2.2233e-05, -1.5408e-05,  1.4737e-05,  ..., -1.7419e-05,
         -1.4156e-05, -1.6332e-05],
        [-3.3945e-05, -2.6137e-05,  2.2829e-05,  ..., -2.7403e-05,
         -2.2963e-05, -2.1875e-05]], device='cuda:0')
Loss: 0.983443021774292
Graident accumulation at epoch 1, step 1167, batch 119
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.0545e-05,  7.5987e-05, -5.2238e-05,  ..., -1.1286e-05,
         -1.9787e-05,  8.3922e-06],
        [-1.1923e-05, -6.1987e-06,  5.5871e-06,  ..., -8.5135e-06,
         -6.1374e-06, -7.3708e-06],
        [ 1.6732e-05,  2.8346e-05, -9.9770e-06,  ...,  1.3671e-05,
          2.8330e-05,  3.4229e-06],
        [-1.3952e-05,  1.0274e-07,  7.6198e-06,  ..., -7.7763e-06,
         -4.2469e-06, -9.2715e-06],
        [-2.7582e-05, -2.0670e-05,  1.8211e-05,  ..., -2.3403e-05,
         -1.7831e-05, -1.7976e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8252e-08, 5.1468e-08, 4.9527e-08,  ..., 1.7875e-08, 1.1843e-07,
         3.8250e-08],
        [8.0568e-11, 4.9865e-11, 1.8876e-11,  ..., 5.5875e-11, 1.8288e-11,
         2.3431e-11],
        [1.6866e-09, 1.0772e-09, 3.3887e-10,  ..., 1.2747e-09, 5.1802e-10,
         3.9812e-10],
        [4.3738e-10, 3.6218e-10, 1.4884e-10,  ..., 3.9923e-10, 1.6279e-10,
         1.6478e-10],
        [3.8635e-10, 2.1103e-10, 6.2766e-11,  ..., 2.7900e-10, 6.5696e-11,
         1.0813e-10]], device='cuda:0')
optimizer state dict: 146.0
lr: [8.635831001887192e-06, 8.635831001887192e-06]
scheduler_last_epoch: 146


Running epoch 1, step 1168, batch 120
Sampled inputs[:2]: tensor([[    0, 18905,  2311,  ..., 10213,   908,   694],
        [    0,  1760,   446,  ...,   329,  1405,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3283e-05, -2.0397e-06, -2.1231e-05,  ...,  6.6285e-05,
          2.6928e-05,  9.2041e-05],
        [-1.7062e-06, -1.2070e-06,  1.1325e-06,  ..., -1.3337e-06,
         -1.0803e-06, -1.2442e-06],
        [-4.0829e-06, -2.9951e-06,  2.8610e-06,  ..., -3.1441e-06,
         -2.5481e-06, -2.9802e-06],
        [-2.8014e-06, -1.9372e-06,  1.9073e-06,  ..., -2.1607e-06,
         -1.7583e-06, -2.1309e-06],
        [-4.5002e-06, -3.4273e-06,  3.0994e-06,  ..., -3.5912e-06,
         -3.0249e-06, -3.0547e-06]], device='cuda:0')
Loss: 1.0004900693893433


Running epoch 1, step 1169, batch 121
Sampled inputs[:2]: tensor([[    0, 18787, 27117,  ...,   287, 16139,    13],
        [    0,   266,   997,  ...,  2670,     5,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1096e-05,  3.7924e-05, -1.8011e-04,  ...,  6.1351e-05,
         -2.5093e-05,  2.4557e-04],
        [-3.3751e-06, -2.2873e-06,  2.1309e-06,  ..., -2.6822e-06,
         -2.1681e-06, -2.5332e-06],
        [-8.0168e-06, -5.6326e-06,  5.3793e-06,  ..., -6.2436e-06,
         -5.0515e-06, -5.9456e-06],
        [-5.5432e-06, -3.6433e-06,  3.5986e-06,  ..., -4.3511e-06,
         -3.5316e-06, -4.3362e-06],
        [-8.7023e-06, -6.4075e-06,  5.7518e-06,  ..., -7.0482e-06,
         -5.9158e-06, -6.0499e-06]], device='cuda:0')
Loss: 0.9557731747627258


Running epoch 1, step 1170, batch 122
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,   29,  445,   16],
        [   0, 3408,  300,  ..., 3868,  300, 2932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0937e-04,  3.9845e-05, -1.7742e-04,  ...,  1.0162e-04,
          1.1927e-04,  1.5391e-04],
        [-5.1036e-06, -3.3826e-06,  3.1739e-06,  ..., -4.0159e-06,
         -3.2261e-06, -3.7476e-06],
        [-1.2040e-05, -8.3447e-06,  8.0168e-06,  ..., -9.3281e-06,
         -7.5102e-06, -8.7619e-06],
        [-8.4192e-06, -5.4315e-06,  5.4091e-06,  ..., -6.5565e-06,
         -5.2974e-06, -6.4522e-06],
        [-1.2964e-05, -9.4473e-06,  8.5086e-06,  ..., -1.0446e-05,
         -8.7172e-06, -8.8066e-06]], device='cuda:0')
Loss: 0.9742087125778198


Running epoch 1, step 1171, batch 123
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,   287,   271,  2540],
        [    0,    14,  3080,  ..., 14737,    13, 17982]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3853e-04,  2.4771e-04, -1.2972e-04,  ...,  2.0507e-04,
         -3.1438e-05,  1.0729e-04],
        [-6.7651e-06, -4.4405e-06,  4.2915e-06,  ..., -5.3421e-06,
         -4.2841e-06, -5.0366e-06],
        [-1.5914e-05, -1.0923e-05,  1.0788e-05,  ..., -1.2353e-05,
         -9.9391e-06, -1.1712e-05],
        [-1.1146e-05, -7.0930e-06,  7.3314e-06,  ..., -8.7023e-06,
         -7.0110e-06, -8.6427e-06],
        [-1.7166e-05, -1.2398e-05,  1.1474e-05,  ..., -1.3858e-05,
         -1.1548e-05, -1.1802e-05]], device='cuda:0')
Loss: 0.9610204100608826


Running epoch 1, step 1172, batch 124
Sampled inputs[:2]: tensor([[    0,   266,  1234,  ...,   908,   328, 26300],
        [    0,   417,   199,  ...,  1853,    12,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6229e-04,  1.4672e-04, -5.0881e-04,  ...,  7.6157e-05,
         -1.1990e-04, -7.4226e-05],
        [-8.4564e-06, -5.6550e-06,  5.4166e-06,  ..., -6.6534e-06,
         -5.2974e-06, -6.1467e-06],
        [ 1.6048e-04,  1.7587e-04, -1.3112e-04,  ...,  2.0530e-04,
          1.0282e-04,  9.8147e-05],
        [-1.3947e-05, -9.0599e-06,  9.2685e-06,  ..., -1.0848e-05,
         -8.6650e-06, -1.0610e-05],
        [-2.1309e-05, -1.5631e-05,  1.4350e-05,  ..., -1.7166e-05,
         -1.4216e-05, -1.4335e-05]], device='cuda:0')
Loss: 1.006631851196289


Running epoch 1, step 1173, batch 125
Sampled inputs[:2]: tensor([[    0,  5054,  3945,  ...,   272,   278,   516],
        [    0,    19, 18798,  ...,    13, 17982,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6899e-04,  1.8093e-04, -4.5246e-04,  ..., -1.9693e-05,
         -9.1640e-05, -7.0350e-05],
        [-1.0140e-05, -6.8024e-06,  6.4895e-06,  ..., -8.0019e-06,
         -6.3479e-06, -7.3910e-06],
        [ 1.5663e-04,  1.7308e-04, -1.2849e-04,  ...,  2.0225e-04,
          1.0042e-04,  9.5316e-05],
        [-1.6794e-05, -1.0967e-05,  1.1161e-05,  ..., -1.3098e-05,
         -1.0423e-05, -1.2785e-05],
        [-2.5481e-05, -1.8761e-05,  1.7151e-05,  ..., -2.0579e-05,
         -1.6987e-05, -1.7211e-05]], device='cuda:0')
Loss: 0.9780397415161133


Running epoch 1, step 1174, batch 126
Sampled inputs[:2]: tensor([[   0,   14,  298,  ...,  333,  199,  769],
        [   0, 3159,  278,  ...,  266, 2545,  863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8713e-04, -1.2475e-06, -4.5903e-04,  ..., -7.1349e-05,
          1.1249e-04,  2.2882e-06],
        [-1.1809e-05, -7.7710e-06,  7.4394e-06,  ..., -9.4250e-06,
         -7.4431e-06, -8.8289e-06],
        [ 1.5286e-04,  1.7079e-04, -1.2610e-04,  ...,  1.9916e-04,
          9.8052e-05,  9.2231e-05],
        [-1.9476e-05, -1.2450e-05,  1.2740e-05,  ..., -1.5363e-05,
         -1.2189e-05, -1.5184e-05],
        [-2.9862e-05, -2.1517e-05,  1.9908e-05,  ..., -2.4244e-05,
         -1.9878e-05, -2.0534e-05]], device='cuda:0')
Loss: 0.9624677300453186


Running epoch 1, step 1175, batch 127
Sampled inputs[:2]: tensor([[    0,  3829,   278,  ..., 11978,     9,   968],
        [    0,  4890,  1528,  ...,   847,   328,  1703]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2010e-04, -1.4990e-04, -5.4493e-04,  ..., -8.1314e-05,
          2.3901e-04,  2.1230e-06],
        [-1.3411e-05, -8.8736e-06,  8.5793e-06,  ..., -1.0677e-05,
         -8.4043e-06, -9.9018e-06],
        [ 1.4908e-04,  1.6812e-04, -1.2323e-04,  ...,  1.9624e-04,
          9.5831e-05,  8.9713e-05],
        [-2.2218e-05, -1.4283e-05,  1.4782e-05,  ..., -1.7479e-05,
         -1.3813e-05, -1.7136e-05],
        [-3.3736e-05, -2.4423e-05,  2.2829e-05,  ..., -2.7344e-05,
         -2.2352e-05, -2.2888e-05]], device='cuda:0')
Loss: 0.9925970435142517
Graident accumulation at epoch 1, step 1175, batch 127
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0142,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.0500e-05,  5.3398e-05, -1.0151e-04,  ..., -1.8288e-05,
          6.0930e-06,  7.7653e-06],
        [-1.2072e-05, -6.4662e-06,  5.8863e-06,  ..., -8.7299e-06,
         -6.3641e-06, -7.6239e-06],
        [ 2.9967e-05,  4.2323e-05, -2.1302e-05,  ...,  3.1929e-05,
          3.5080e-05,  1.2052e-05],
        [-1.4779e-05, -1.3358e-06,  8.3360e-06,  ..., -8.7466e-06,
         -5.2036e-06, -1.0058e-05],
        [-2.8197e-05, -2.1046e-05,  1.8673e-05,  ..., -2.3797e-05,
         -1.8283e-05, -1.8468e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8296e-08, 5.1439e-08, 4.9774e-08,  ..., 1.7864e-08, 1.1837e-07,
         3.8212e-08],
        [8.0667e-11, 4.9894e-11, 1.8931e-11,  ..., 5.5933e-11, 1.8340e-11,
         2.3505e-11],
        [1.7071e-09, 1.1044e-09, 3.5372e-10,  ..., 1.3119e-09, 5.2669e-10,
         4.0577e-10],
        [4.3744e-10, 3.6202e-10, 1.4891e-10,  ..., 3.9914e-10, 1.6282e-10,
         1.6491e-10],
        [3.8710e-10, 2.1141e-10, 6.3225e-11,  ..., 2.7946e-10, 6.6130e-11,
         1.0855e-10]], device='cuda:0')
optimizer state dict: 147.0
lr: [8.513477361962645e-06, 8.513477361962645e-06]
scheduler_last_epoch: 147


Running epoch 1, step 1176, batch 128
Sampled inputs[:2]: tensor([[    0,  2836,  3084,  ...,  3634,  6464,   271],
        [    0,    61, 22315,  ..., 36901,    17,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8019e-05, -2.7819e-05,  1.8453e-04,  ..., -3.9998e-05,
          8.4066e-06, -7.9670e-05],
        [-1.6615e-06, -1.2591e-06,  1.2144e-06,  ..., -1.2964e-06,
         -1.0282e-06, -1.0580e-06],
        [ 6.6344e-05,  6.9669e-05, -7.3272e-05,  ...,  4.5959e-05,
          9.3916e-05,  2.9611e-05],
        [-2.8312e-06, -2.1160e-06,  2.1458e-06,  ..., -2.1905e-06,
         -1.7360e-06, -1.9222e-06],
        [-4.1127e-06, -3.3975e-06,  3.1143e-06,  ..., -3.3379e-06,
         -2.7567e-06, -2.5034e-06]], device='cuda:0')
Loss: 0.9915770292282104


Running epoch 1, step 1177, batch 129
Sampled inputs[:2]: tensor([[   0, 5159,  292,  ...,  772,  271, 3728],
        [   0, 2555,  984,  ..., 5900, 1576,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.9223e-05,  1.0233e-06,  7.7040e-06,  ..., -1.3800e-04,
         -1.7543e-05, -1.8422e-04],
        [-3.3155e-06, -2.4512e-06,  2.4363e-06,  ..., -2.5779e-06,
         -2.0489e-06, -2.1681e-06],
        [ 6.2530e-05,  6.6823e-05, -7.0292e-05,  ...,  4.3009e-05,
          9.1547e-05,  2.7033e-05],
        [-5.5581e-06, -4.0531e-06,  4.2468e-06,  ..., -4.2766e-06,
         -3.4124e-06, -3.8445e-06],
        [-8.0764e-06, -6.5118e-06,  6.1542e-06,  ..., -6.5118e-06,
         -5.4091e-06, -4.9621e-06]], device='cuda:0')
Loss: 0.9979670643806458


Running epoch 1, step 1178, batch 130
Sampled inputs[:2]: tensor([[    0,   475,   668,  ..., 17680,   368,  1351],
        [    0,  2320,    63,  ...,   858,    13, 40170]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0666e-04, -1.1200e-04,  1.0084e-04,  ..., -1.4506e-04,
          1.7578e-04,  5.1563e-05],
        [-5.0664e-06, -3.6657e-06,  3.5390e-06,  ..., -3.9488e-06,
         -3.1814e-06, -3.4347e-06],
        [ 5.8298e-05,  6.3768e-05, -6.7431e-05,  ...,  3.9701e-05,
          8.8790e-05,  2.3948e-05],
        [-8.3596e-06, -5.9605e-06,  6.0722e-06,  ..., -6.4671e-06,
         -5.2452e-06, -5.9754e-06],
        [-1.2547e-05, -9.8944e-06,  9.1344e-06,  ..., -1.0118e-05,
         -8.5235e-06, -7.9870e-06]], device='cuda:0')
Loss: 1.0077052116394043


Running epoch 1, step 1179, batch 131
Sampled inputs[:2]: tensor([[    0,  3211,   328,  ...,  2098,  1231, 35325],
        [    0,   259,  6887,  ...,  1400,   292,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0438e-04, -2.0944e-04, -2.0086e-04,  ..., -1.5209e-04,
          1.4999e-04,  1.0393e-04],
        [-6.7055e-06, -4.8801e-06,  4.7535e-06,  ..., -5.2303e-06,
         -4.2021e-06, -4.5002e-06],
        [ 1.1053e-04,  1.7398e-04, -1.2727e-04,  ...,  8.5954e-05,
          1.3893e-04,  6.4677e-05],
        [-1.1176e-05, -8.0168e-06,  8.2329e-06,  ..., -8.6427e-06,
         -6.9812e-06, -7.8976e-06],
        [-1.6451e-05, -1.3053e-05,  1.2159e-05,  ..., -1.3277e-05,
         -1.1146e-05, -1.0341e-05]], device='cuda:0')
Loss: 0.9986520409584045


Running epoch 1, step 1180, batch 132
Sampled inputs[:2]: tensor([[   0,  992,  409,  ..., 5843,  344,  259],
        [   0,    9,  278,  ...,  278,  298,  452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7865e-04, -2.5772e-04, -3.0042e-04,  ..., -1.3089e-04,
          1.3897e-04,  2.1954e-04],
        [-8.4564e-06, -5.9307e-06,  5.8934e-06,  ..., -6.5565e-06,
         -5.2750e-06, -5.8189e-06],
        [ 1.0638e-04,  1.7138e-04, -1.2440e-04,  ...,  8.2854e-05,
          1.3643e-04,  6.1562e-05],
        [-1.4037e-05, -9.6932e-06,  1.0155e-05,  ..., -1.0818e-05,
         -8.7470e-06, -1.0163e-05],
        [-2.0772e-05, -1.5959e-05,  1.5125e-05,  ..., -1.6645e-05,
         -1.3962e-05, -1.3351e-05]], device='cuda:0')
Loss: 0.991310179233551


Running epoch 1, step 1181, batch 133
Sampled inputs[:2]: tensor([[    0,  1978, 20360,  ...,   898,   699, 10262],
        [    0,  3217, 16714,  ...,   462,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1203e-04, -3.0192e-04, -2.4614e-04,  ..., -1.3604e-04,
          4.6002e-05,  1.6853e-04],
        [-1.0118e-05, -7.1079e-06,  7.1153e-06,  ..., -7.8157e-06,
         -6.2510e-06, -6.9216e-06],
        [ 1.0239e-04,  1.6848e-04, -1.2136e-04,  ...,  7.9829e-05,
          1.3407e-04,  5.8880e-05],
        [-1.6853e-05, -1.1645e-05,  1.2286e-05,  ..., -1.2919e-05,
         -1.0379e-05, -1.2115e-05],
        [-2.4855e-05, -1.9103e-05,  1.8194e-05,  ..., -1.9848e-05,
         -1.6570e-05, -1.5914e-05]], device='cuda:0')
Loss: 1.014892816543579


Running epoch 1, step 1182, batch 134
Sampled inputs[:2]: tensor([[   0,  706, 1005,  ...,  278,  266, 5590],
        [   0,  221,  474,  ...,  266, 2025,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8917e-04, -2.9303e-04, -3.0764e-04,  ..., -6.6325e-05,
         -1.5145e-04, -3.8776e-07],
        [-1.1779e-05, -8.1360e-06,  8.2850e-06,  ..., -9.1344e-06,
         -7.2122e-06, -8.1062e-06],
        [ 9.8426e-05,  1.6595e-04, -1.1841e-04,  ...,  7.6745e-05,
          1.3181e-04,  5.6079e-05],
        [-1.9670e-05, -1.3337e-05,  1.4357e-05,  ..., -1.5140e-05,
         -1.1988e-05, -1.4216e-05],
        [-2.8849e-05, -2.1845e-05,  2.1145e-05,  ..., -2.3067e-05,
         -1.9059e-05, -1.8522e-05]], device='cuda:0')
Loss: 0.9750370979309082


Running epoch 1, step 1183, batch 135
Sampled inputs[:2]: tensor([[   0, 1086,   26,  ...,  298,  527,  298],
        [   0, 3529,  271,  ..., 1553,  365, 2714]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5909e-04, -3.0658e-04, -5.8311e-04,  ..., -1.6372e-04,
         -9.4358e-05,  1.2034e-04],
        [-1.3493e-05, -9.3430e-06,  9.3803e-06,  ..., -1.0528e-05,
         -8.3447e-06, -9.3654e-06],
        [ 9.4492e-05,  1.6306e-04, -1.1571e-04,  ...,  7.3601e-05,
          1.2924e-04,  5.3247e-05],
        [-2.2486e-05, -1.5274e-05,  1.6212e-05,  ..., -1.7419e-05,
         -1.3858e-05, -1.6376e-05],
        [-3.3021e-05, -2.5064e-05,  2.3976e-05,  ..., -2.6509e-05,
         -2.1964e-05, -2.1309e-05]], device='cuda:0')
Loss: 0.9684311747550964
Graident accumulation at epoch 1, step 1183, batch 135
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0032, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.5408e-06,  1.7400e-05, -1.4967e-04,  ..., -3.2832e-05,
         -3.9521e-06,  1.9023e-05],
        [-1.2214e-05, -6.7539e-06,  6.2357e-06,  ..., -8.9096e-06,
         -6.5622e-06, -7.7980e-06],
        [ 3.6419e-05,  5.4397e-05, -3.0743e-05,  ...,  3.6096e-05,
          4.4496e-05,  1.6171e-05],
        [-1.5549e-05, -2.7296e-06,  9.1236e-06,  ..., -9.6139e-06,
         -6.0690e-06, -1.0690e-05],
        [-2.8679e-05, -2.1447e-05,  1.9203e-05,  ..., -2.4068e-05,
         -1.8652e-05, -1.8752e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8367e-08, 5.1481e-08, 5.0064e-08,  ..., 1.7873e-08, 1.1826e-07,
         3.8188e-08],
        [8.0768e-11, 4.9932e-11, 1.9000e-11,  ..., 5.5988e-11, 1.8391e-11,
         2.3570e-11],
        [1.7143e-09, 1.1298e-09, 3.6675e-10,  ..., 1.3160e-09, 5.4286e-10,
         4.0820e-10],
        [4.3750e-10, 3.6189e-10, 1.4903e-10,  ..., 3.9904e-10, 1.6285e-10,
         1.6501e-10],
        [3.8780e-10, 2.1183e-10, 6.3736e-11,  ..., 2.7989e-10, 6.6546e-11,
         1.0889e-10]], device='cuda:0')
optimizer state dict: 148.0
lr: [8.391350875673234e-06, 8.391350875673234e-06]
scheduler_last_epoch: 148


Running epoch 1, step 1184, batch 136
Sampled inputs[:2]: tensor([[    0,   300,  6263,  ..., 18488,  1665,  1640],
        [    0,  2823,   287,  ...,  3504,     9, 13910]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3307e-05, -4.0994e-05, -2.3506e-05,  ...,  6.2542e-05,
          1.2721e-05, -1.0053e-04],
        [-1.6317e-06, -1.0729e-06,  1.1101e-06,  ..., -1.3262e-06,
         -1.0133e-06, -1.1548e-06],
        [-3.7104e-06, -2.5183e-06,  2.7120e-06,  ..., -2.9355e-06,
         -2.2352e-06, -2.5481e-06],
        [-2.7120e-06, -1.7434e-06,  1.9222e-06,  ..., -2.1905e-06,
         -1.6838e-06, -2.0117e-06],
        [-3.8743e-06, -2.7418e-06,  2.8014e-06,  ..., -3.1441e-06,
         -2.4885e-06, -2.4438e-06]], device='cuda:0')
Loss: 1.0061321258544922


Running epoch 1, step 1185, batch 137
Sampled inputs[:2]: tensor([[    0, 28107,    14,  ...,   864,   298,   413],
        [    0,   446, 28686,  ...,    35,  2706, 19712]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5623e-04,  1.7729e-04,  2.7716e-04,  ..., -5.3363e-05,
          9.6040e-05, -1.9956e-04],
        [-3.2112e-06, -2.3395e-06,  1.9670e-06,  ..., -2.6971e-06,
         -2.2501e-06, -2.3916e-06],
        [-7.4804e-06, -5.6922e-06,  4.9472e-06,  ..., -6.1989e-06,
         -5.1856e-06, -5.4687e-06],
        [-5.3644e-06, -3.8594e-06,  3.3900e-06,  ..., -4.5151e-06,
         -3.8296e-06, -4.2170e-06],
        [-7.9870e-06, -6.2734e-06,  5.2154e-06,  ..., -6.7651e-06,
         -5.8264e-06, -5.4091e-06]], device='cuda:0')
Loss: 0.9599205851554871


Running epoch 1, step 1186, batch 138
Sampled inputs[:2]: tensor([[    0, 16765,   367,  ..., 30192,  7038,  8135],
        [    0,  7264, 14450,  ...,   367,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0236e-04,  1.5574e-04,  8.5337e-05,  ...,  1.0442e-04,
          6.0013e-05, -2.5794e-04],
        [-4.8280e-06, -3.5018e-06,  3.1218e-06,  ..., -3.9637e-06,
         -3.1926e-06, -3.5167e-06],
        [-1.1414e-05, -8.5831e-06,  7.9274e-06,  ..., -9.2536e-06,
         -7.4655e-06, -8.2105e-06],
        [-8.1807e-06, -5.8562e-06,  5.4911e-06,  ..., -6.7055e-06,
         -5.4538e-06, -6.2734e-06],
        [-1.1891e-05, -9.3132e-06,  8.1509e-06,  ..., -9.9093e-06,
         -8.2850e-06, -7.9274e-06]], device='cuda:0')
Loss: 0.9835702180862427


Running epoch 1, step 1187, batch 139
Sampled inputs[:2]: tensor([[   0,  287,  271,  ..., 5090,  631, 3276],
        [   0,  342, 4014,  ...,  368,  408, 2105]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9847e-04,  4.4107e-05, -3.3958e-06,  ...,  1.5127e-04,
         -5.3050e-05, -3.5118e-04],
        [-6.4224e-06, -4.6641e-06,  4.3437e-06,  ..., -5.1707e-06,
         -4.0829e-06, -4.5449e-06],
        [-1.5348e-05, -1.1519e-05,  1.1086e-05,  ..., -1.2204e-05,
         -9.6112e-06, -1.0774e-05],
        [-1.0982e-05, -7.8678e-06,  7.7114e-06,  ..., -8.7917e-06,
         -6.9812e-06, -8.1807e-06],
        [-1.5736e-05, -1.2323e-05,  1.1176e-05,  ..., -1.2890e-05,
         -1.0565e-05, -1.0207e-05]], device='cuda:0')
Loss: 0.9731181859970093


Running epoch 1, step 1188, batch 140
Sampled inputs[:2]: tensor([[    0, 13466,    14,  ..., 11227,  1966,  4039],
        [    0,    33,    12,  ...,  1110,   467, 17467]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.8531e-05, -4.9703e-07,  1.1237e-04,  ...,  9.0266e-06,
          2.1754e-04, -2.3688e-04],
        [-8.1584e-06, -5.8860e-06,  5.4315e-06,  ..., -6.5565e-06,
         -5.1484e-06, -5.8562e-06],
        [-1.9431e-05, -1.4499e-05,  1.3798e-05,  ..., -1.5423e-05,
         -1.2115e-05, -1.3843e-05],
        [-1.3843e-05, -9.8646e-06,  9.5591e-06,  ..., -1.1072e-05,
         -8.7544e-06, -1.0416e-05],
        [-2.0176e-05, -1.5676e-05,  1.4111e-05,  ..., -1.6481e-05,
         -1.3471e-05, -1.3337e-05]], device='cuda:0')
Loss: 0.9688210487365723


Running epoch 1, step 1189, batch 141
Sampled inputs[:2]: tensor([[    0,   328, 16219,  ..., 14559,   351,   587],
        [    0,  2356,   292,  ...,    12,   287,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6729e-04,  1.3011e-04, -1.1357e-04,  ...,  1.8401e-05,
         -5.5217e-06, -3.7301e-04],
        [-9.7230e-06, -7.0184e-06,  6.5193e-06,  ..., -7.8827e-06,
         -6.2287e-06, -6.9439e-06],
        [-2.2963e-05, -1.7121e-05,  1.6436e-05,  ..., -1.8328e-05,
         -1.4499e-05, -1.6242e-05],
        [-1.6540e-05, -1.1742e-05,  1.1511e-05,  ..., -1.3337e-05,
         -1.0632e-05, -1.2428e-05],
        [-2.3991e-05, -1.8671e-05,  1.6943e-05,  ..., -1.9714e-05,
         -1.6212e-05, -1.5706e-05]], device='cuda:0')
Loss: 0.9613835215568542


Running epoch 1, step 1190, batch 142
Sampled inputs[:2]: tensor([[   0,   20,   13,  ...,  496,   14, 1032],
        [   0, 1932,  278,  ...,  609,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6110e-04,  2.9253e-04, -1.0110e-04,  ...,  1.1504e-05,
         -2.8318e-05, -2.6159e-04],
        [-1.1362e-05, -8.0615e-06,  7.5698e-06,  ..., -9.2685e-06,
         -7.3165e-06, -8.3819e-06],
        [-2.6762e-05, -1.9655e-05,  1.9059e-05,  ..., -2.1458e-05,
         -1.6943e-05, -1.9491e-05],
        [-1.9178e-05, -1.3366e-05,  1.3247e-05,  ..., -1.5557e-05,
         -1.2353e-05, -1.4812e-05],
        [-2.8193e-05, -2.1577e-05,  1.9789e-05,  ..., -2.3276e-05,
         -1.9088e-05, -1.9029e-05]], device='cuda:0')
Loss: 0.9817978143692017


Running epoch 1, step 1191, batch 143
Sampled inputs[:2]: tensor([[    0,   380,   560,  ...,   287,  6769,   806],
        [    0, 16064, 10937,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1839e-04,  1.5574e-04, -5.7009e-05,  ..., -5.5048e-05,
         -1.7701e-04, -4.3167e-04],
        [-1.2979e-05, -9.2089e-06,  8.7321e-06,  ..., -1.0528e-05,
         -8.2999e-06, -9.4324e-06],
        [-3.0637e-05, -2.2486e-05,  2.2024e-05,  ..., -2.4453e-05,
         -1.9267e-05, -2.2009e-05],
        [-2.1964e-05, -1.5289e-05,  1.5348e-05,  ..., -1.7703e-05,
         -1.4037e-05, -1.6734e-05],
        [-3.2127e-05, -2.4632e-05,  2.2754e-05,  ..., -2.6420e-05,
         -2.1651e-05, -2.1383e-05]], device='cuda:0')
Loss: 0.9818943738937378
Graident accumulation at epoch 1, step 1191, batch 143
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0026,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0286,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.2528e-06,  3.1235e-05, -1.4040e-04,  ..., -3.5053e-05,
         -2.1258e-05, -2.6046e-05],
        [-1.2290e-05, -6.9994e-06,  6.4854e-06,  ..., -9.0714e-06,
         -6.7359e-06, -7.9615e-06],
        [ 2.9714e-05,  4.6709e-05, -2.5466e-05,  ...,  3.0041e-05,
          3.8120e-05,  1.2353e-05],
        [-1.6191e-05, -3.9855e-06,  9.7461e-06,  ..., -1.0423e-05,
         -6.8658e-06, -1.1294e-05],
        [-2.9024e-05, -2.1766e-05,  1.9558e-05,  ..., -2.4303e-05,
         -1.8952e-05, -1.9015e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8323e-08, 5.1454e-08, 5.0017e-08,  ..., 1.7858e-08, 1.1817e-07,
         3.8336e-08],
        [8.0856e-11, 4.9967e-11, 1.9057e-11,  ..., 5.6043e-11, 1.8442e-11,
         2.3635e-11],
        [1.7136e-09, 1.1292e-09, 3.6687e-10,  ..., 1.3153e-09, 5.4269e-10,
         4.0827e-10],
        [4.3755e-10, 3.6177e-10, 1.4911e-10,  ..., 3.9895e-10, 1.6288e-10,
         1.6512e-10],
        [3.8845e-10, 2.1222e-10, 6.4190e-11,  ..., 2.8031e-10, 6.6948e-11,
         1.0924e-10]], device='cuda:0')
optimizer state dict: 149.0
lr: [8.269470205012111e-06, 8.269470205012111e-06]
scheduler_last_epoch: 149


Running epoch 1, step 1192, batch 144
Sampled inputs[:2]: tensor([[   0,  266, 2511,  ..., 3220, 4164, 1173],
        [   0,   12, 2418,  ...,  446,  381, 2204]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0412e-05,  2.4842e-05,  9.8134e-05,  ...,  4.5439e-06,
          1.8250e-05,  2.0724e-05],
        [-1.6838e-06, -1.1250e-06,  1.1250e-06,  ..., -1.2517e-06,
         -9.2015e-07, -1.1548e-06],
        [-4.0531e-06, -2.8461e-06,  2.8610e-06,  ..., -3.0100e-06,
         -2.2501e-06, -2.7567e-06],
        [-2.8163e-06, -1.8626e-06,  1.9521e-06,  ..., -2.0862e-06,
         -1.5572e-06, -2.0266e-06],
        [-4.0233e-06, -2.9802e-06,  2.8014e-06,  ..., -3.0994e-06,
         -2.4289e-06, -2.5183e-06]], device='cuda:0')
Loss: 0.9760563969612122


Running epoch 1, step 1193, batch 145
Sampled inputs[:2]: tensor([[    0,    12,   328,  ...,   578,    19,    40],
        [    0,   389, 18984,  ...,   287,   768,  1070]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0931e-04,  5.2064e-05,  1.0206e-04,  ...,  7.5855e-05,
          8.6860e-05,  1.3932e-04],
        [-3.4049e-06, -2.3246e-06,  2.2948e-06,  ..., -2.6301e-06,
         -2.0228e-06, -2.3097e-06],
        [-7.9572e-06, -5.6922e-06,  5.6624e-06,  ..., -6.1244e-06,
         -4.7684e-06, -5.3644e-06],
        [-5.7667e-06, -3.8743e-06,  4.0084e-06,  ..., -4.4405e-06,
         -3.4794e-06, -4.1127e-06],
        [-8.2254e-06, -6.1691e-06,  5.7966e-06,  ..., -6.5267e-06,
         -5.2899e-06, -5.1260e-06]], device='cuda:0')
Loss: 1.014262318611145


Running epoch 1, step 1194, batch 146
Sampled inputs[:2]: tensor([[   0,  271, 4728,  ...,  344,  259, 1774],
        [   0,  432,  984,  ...,  287,  496,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1597e-05,  3.0905e-05,  1.1395e-06,  ...,  7.3738e-05,
          1.9554e-05,  8.0669e-05],
        [-5.0291e-06, -3.4571e-06,  3.3826e-06,  ..., -4.0308e-06,
         -3.1404e-06, -3.4794e-06],
        [-1.1742e-05, -8.3894e-06,  8.3745e-06,  ..., -9.2983e-06,
         -7.2867e-06, -8.0466e-06],
        [-8.4937e-06, -5.7071e-06,  5.9009e-06,  ..., -6.7651e-06,
         -5.3421e-06, -6.1840e-06],
        [-1.2368e-05, -9.2536e-06,  8.7172e-06,  ..., -1.0118e-05,
         -8.2403e-06, -7.8529e-06]], device='cuda:0')
Loss: 0.9861869812011719


Running epoch 1, step 1195, batch 147
Sampled inputs[:2]: tensor([[    0, 21178,  1952,  ..., 14930,     9,   689],
        [    0,  1234,   278,  ...,  8635,   271,   546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5517e-05,  1.2108e-04, -1.7105e-04,  ..., -3.2329e-05,
         -6.0306e-05,  3.7716e-05],
        [-6.6459e-06, -4.5598e-06,  4.6194e-06,  ..., -5.2825e-06,
         -4.0568e-06, -4.5374e-06],
        [-1.5736e-05, -1.1161e-05,  1.1563e-05,  ..., -1.2338e-05,
         -9.5069e-06, -1.0684e-05],
        [-1.1325e-05, -7.5772e-06,  8.1211e-06,  ..., -8.9109e-06,
         -6.8992e-06, -8.1360e-06],
        [-1.6302e-05, -1.2130e-05,  1.1802e-05,  ..., -1.3232e-05,
         -1.0610e-05, -1.0237e-05]], device='cuda:0')
Loss: 0.9902255535125732


Running epoch 1, step 1196, batch 148
Sampled inputs[:2]: tensor([[    0,    12,   287,  ..., 12678,  2503,   401],
        [    0,  2261,     9,  ..., 15008,    14,   333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9999e-05,  1.6896e-04, -1.8343e-04,  ...,  3.3945e-05,
          3.3216e-05,  1.5327e-04],
        [-8.2701e-06, -5.7071e-06,  5.7518e-06,  ..., -6.5640e-06,
         -5.0999e-06, -5.6773e-06],
        [-1.9580e-05, -1.3992e-05,  1.4409e-05,  ..., -1.5363e-05,
         -1.1981e-05, -1.3366e-05],
        [-1.4141e-05, -9.5293e-06,  1.0163e-05,  ..., -1.1116e-05,
         -8.7097e-06, -1.0207e-05],
        [-2.0176e-05, -1.5140e-05,  1.4633e-05,  ..., -1.6391e-05,
         -1.3292e-05, -1.2755e-05]], device='cuda:0')
Loss: 0.9790847301483154


Running epoch 1, step 1197, batch 149
Sampled inputs[:2]: tensor([[    0,     9,   292,  ...,   944,   278,  1758],
        [    0,  7935,  6521,  ..., 41312,   365,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3496e-05,  1.2779e-04, -3.6513e-04,  ...,  6.2362e-05,
         -1.2601e-04,  2.3741e-05],
        [-9.9540e-06, -6.8545e-06,  6.9737e-06,  ..., -7.8455e-06,
         -6.0312e-06, -6.7949e-06],
        [-2.3574e-05, -1.6823e-05,  1.7464e-05,  ..., -1.8418e-05,
         -1.4231e-05, -1.6034e-05],
        [-1.7032e-05, -1.1466e-05,  1.2338e-05,  ..., -1.3307e-05,
         -1.0312e-05, -1.2204e-05],
        [-2.4110e-05, -1.8090e-05,  1.7613e-05,  ..., -1.9506e-05,
         -1.5691e-05, -1.5199e-05]], device='cuda:0')
Loss: 0.9934809803962708


Running epoch 1, step 1198, batch 150
Sampled inputs[:2]: tensor([[   0,  292,  685,  ...,  278, 3281,  298],
        [   0,  516,  689,  ...,  278,  516, 6137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5691e-05,  7.8591e-05, -6.3855e-04,  ..., -2.3030e-05,
         -2.2090e-04, -1.3243e-04],
        [-1.1504e-05, -7.8380e-06,  8.1286e-06,  ..., -9.0525e-06,
         -6.8992e-06, -7.8604e-06],
        [-2.7269e-05, -1.9237e-05,  2.0385e-05,  ..., -2.1234e-05,
         -1.6257e-05, -1.8522e-05],
        [-1.9819e-05, -1.3158e-05,  1.4499e-05,  ..., -1.5453e-05,
         -1.1854e-05, -1.4231e-05],
        [-2.7791e-05, -2.0638e-05,  2.0474e-05,  ..., -2.2382e-05,
         -1.7866e-05, -1.7434e-05]], device='cuda:0')
Loss: 0.9482191205024719


Running epoch 1, step 1199, batch 151
Sampled inputs[:2]: tensor([[    0,    12, 12774,  ...,  1231,   278,   266],
        [    0,  1231,   278,  ...,    12,  2606,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1582e-04,  2.6326e-04, -5.8440e-04,  ...,  4.3703e-05,
         -2.5524e-04, -1.5825e-04],
        [-1.3180e-05, -9.0525e-06,  9.2909e-06,  ..., -1.0356e-05,
         -7.9423e-06, -9.0078e-06],
        [-3.1233e-05, -2.2262e-05,  2.3305e-05,  ..., -2.4334e-05,
         -1.8761e-05, -2.1249e-05],
        [-2.2665e-05, -1.5214e-05,  1.6540e-05,  ..., -1.7673e-05,
         -1.3642e-05, -1.6272e-05],
        [-3.1874e-05, -2.3901e-05,  2.3395e-05,  ..., -2.5675e-05,
         -2.0638e-05, -2.0057e-05]], device='cuda:0')
Loss: 0.9710493087768555
Graident accumulation at epoch 1, step 1199, batch 151
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.6541e-06,  5.4437e-05, -1.8480e-04,  ..., -2.7178e-05,
         -4.4656e-05, -3.9266e-05],
        [-1.2379e-05, -7.2047e-06,  6.7659e-06,  ..., -9.1999e-06,
         -6.8566e-06, -8.0661e-06],
        [ 2.3619e-05,  3.9812e-05, -2.0589e-05,  ...,  2.4604e-05,
          3.2432e-05,  8.9932e-06],
        [-1.6838e-05, -5.1084e-06,  1.0426e-05,  ..., -1.1148e-05,
         -7.5434e-06, -1.1792e-05],
        [-2.9309e-05, -2.1979e-05,  1.9942e-05,  ..., -2.4440e-05,
         -1.9120e-05, -1.9119e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8278e-08, 5.1472e-08, 5.0309e-08,  ..., 1.7842e-08, 1.1812e-07,
         3.8323e-08],
        [8.0949e-11, 4.9999e-11, 1.9124e-11,  ..., 5.6094e-11, 1.8487e-11,
         2.3693e-11],
        [1.7128e-09, 1.1286e-09, 3.6705e-10,  ..., 1.3146e-09, 5.4250e-10,
         4.0831e-10],
        [4.3763e-10, 3.6164e-10, 1.4924e-10,  ..., 3.9887e-10, 1.6290e-10,
         1.6522e-10],
        [3.8908e-10, 2.1258e-10, 6.4673e-11,  ..., 2.8069e-10, 6.7307e-11,
         1.0953e-10]], device='cuda:0')
optimizer state dict: 150.0
lr: [8.147853974409676e-06, 8.147853974409676e-06]
scheduler_last_epoch: 150


Running epoch 1, step 1200, batch 152
Sampled inputs[:2]: tensor([[   0,  266, 1553,  ..., 8954,   21,  409],
        [   0,  259, 1143,  ...,  593,  360,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1826e-04, -1.8413e-04,  6.1452e-06,  ..., -1.0278e-04,
         -4.2575e-06, -1.7145e-04],
        [-1.5944e-06, -1.1399e-06,  1.1995e-06,  ..., -1.2815e-06,
         -9.6112e-07, -1.0505e-06],
        [-3.8445e-06, -2.8461e-06,  3.0547e-06,  ..., -3.0696e-06,
         -2.3097e-06, -2.5779e-06],
        [-2.7716e-06, -1.9521e-06,  2.1756e-06,  ..., -2.1905e-06,
         -1.6391e-06, -1.9372e-06],
        [-3.7998e-06, -2.9653e-06,  2.9802e-06,  ..., -3.1441e-06,
         -2.4587e-06, -2.3544e-06]], device='cuda:0')
Loss: 0.9877411723136902


Running epoch 1, step 1201, batch 153
Sampled inputs[:2]: tensor([[    0, 12440,   578,  ..., 25918,   287,   996],
        [    0,  7994,    12,  ..., 13800,   278,   795]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6088e-04, -2.1592e-04, -3.7839e-05,  ..., -2.0682e-04,
         -1.5418e-04, -2.7710e-04],
        [-3.1888e-06, -2.1085e-06,  2.4736e-06,  ..., -2.5257e-06,
         -1.8887e-06, -2.1011e-06],
        [-7.5400e-06, -5.2005e-06,  6.1840e-06,  ..., -5.9158e-06,
         -4.4405e-06, -4.9621e-06],
        [-5.6177e-06, -3.6210e-06,  4.5598e-06,  ..., -4.3809e-06,
         -3.2857e-06, -3.9041e-06],
        [-7.5102e-06, -5.4538e-06,  6.0797e-06,  ..., -6.0946e-06,
         -4.7535e-06, -4.5747e-06]], device='cuda:0')
Loss: 0.9711534976959229


Running epoch 1, step 1202, batch 154
Sampled inputs[:2]: tensor([[    0,  1590,  2140,  ...,   287,  5342,  1319],
        [    0,   292, 15087,  ...,  2675,  1663,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1695e-04,  8.1978e-06, -6.4761e-05,  ..., -5.1735e-05,
         -5.3629e-04, -7.5001e-04],
        [-4.7386e-06, -3.0920e-06,  3.7104e-06,  ..., -3.7774e-06,
         -2.7679e-06, -3.1218e-06],
        [-1.1235e-05, -7.6294e-06,  9.2983e-06,  ..., -8.8364e-06,
         -6.4969e-06, -7.3463e-06],
        [-8.4192e-06, -5.3421e-06,  6.8992e-06,  ..., -6.6161e-06,
         -4.8652e-06, -5.8711e-06],
        [-1.1116e-05, -7.9721e-06,  9.0748e-06,  ..., -9.0301e-06,
         -6.9290e-06, -6.6906e-06]], device='cuda:0')
Loss: 0.9484952688217163


Running epoch 1, step 1203, batch 155
Sampled inputs[:2]: tensor([[    0,   680,   271,  ..., 12942,   271,   266],
        [    0,   380, 26073,  ...,   709,   266,  2421]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7684e-04, -1.1320e-05, -1.2673e-04,  ..., -8.9324e-05,
         -5.6199e-04, -9.0388e-04],
        [-6.3777e-06, -4.2319e-06,  4.9695e-06,  ..., -5.0589e-06,
         -3.7141e-06, -4.0755e-06],
        [-1.5199e-05, -1.0446e-05,  1.2442e-05,  ..., -1.1906e-05,
         -8.7619e-06, -9.6858e-06],
        [-1.1340e-05, -7.3239e-06,  9.1940e-06,  ..., -8.8513e-06,
         -6.5044e-06, -7.6517e-06],
        [-1.5050e-05, -1.0923e-05,  1.2189e-05,  ..., -1.2189e-05,
         -9.3877e-06, -8.8513e-06]], device='cuda:0')
Loss: 1.0186846256256104


Running epoch 1, step 1204, batch 156
Sampled inputs[:2]: tensor([[    0,   824,   278,  ..., 10513,  6909,  4077],
        [    0,    15,    83,  ...,  6030,    14, 14080]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6788e-04, -6.4469e-05, -7.7622e-05,  ..., -1.6014e-04,
         -6.0302e-04, -9.1046e-04],
        [-8.0392e-06, -5.4091e-06,  6.2212e-06,  ..., -6.3330e-06,
         -4.6492e-06, -5.0291e-06],
        [-1.9282e-05, -1.3441e-05,  1.5631e-05,  ..., -1.5020e-05,
         -1.1072e-05, -1.2100e-05],
        [-1.4231e-05, -9.3654e-06,  1.1459e-05,  ..., -1.1042e-05,
         -8.1137e-06, -9.4175e-06],
        [-1.8954e-05, -1.3933e-05,  1.5214e-05,  ..., -1.5274e-05,
         -1.1772e-05, -1.0997e-05]], device='cuda:0')
Loss: 1.0224279165267944


Running epoch 1, step 1205, batch 157
Sampled inputs[:2]: tensor([[    0,   287, 16974,  ...,   300,  2283,  4013],
        [    0,   298,   894,  ...,   396,   298,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2983e-04,  3.4806e-05, -7.3649e-05,  ..., -1.8390e-04,
         -7.2364e-04, -9.3480e-04],
        [-9.7677e-06, -6.6012e-06,  7.3910e-06,  ..., -7.7263e-06,
         -5.6624e-06, -6.3181e-06],
        [-2.3484e-05, -1.6466e-05,  1.8612e-05,  ..., -1.8403e-05,
         -1.3560e-05, -1.5259e-05],
        [-1.7032e-05, -1.1303e-05,  1.3426e-05,  ..., -1.3307e-05,
         -9.7752e-06, -1.1563e-05],
        [-2.3395e-05, -1.7211e-05,  1.8299e-05,  ..., -1.8954e-05,
         -1.4558e-05, -1.4156e-05]], device='cuda:0')
Loss: 0.9637186527252197


Running epoch 1, step 1206, batch 158
Sampled inputs[:2]: tensor([[   0, 7111,  409,  ..., 1908, 1260,  883],
        [   0, 1967, 6851,  ..., 1151,  809,  360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1144e-04, -2.3143e-05, -5.2157e-05,  ..., -1.0454e-04,
         -8.8544e-04, -9.6480e-04],
        [-1.1407e-05, -7.7337e-06,  8.6203e-06,  ..., -8.9779e-06,
         -6.6124e-06, -7.3388e-06],
        [-2.7537e-05, -1.9327e-05,  2.1785e-05,  ..., -2.1473e-05,
         -1.5885e-05, -1.7807e-05],
        [-1.9982e-05, -1.3299e-05,  1.5721e-05,  ..., -1.5542e-05,
         -1.1466e-05, -1.3500e-05],
        [-2.7329e-05, -2.0146e-05,  2.1353e-05,  ..., -2.2039e-05,
         -1.7002e-05, -1.6451e-05]], device='cuda:0')
Loss: 0.9911944270133972


Running epoch 1, step 1207, batch 159
Sampled inputs[:2]: tensor([[    0,    13, 37178,  ...,  1692,  3287, 10652],
        [    0,  7230,    13,  ...,  1400,   367,  1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.5843e-04,  5.1627e-05, -8.5566e-05,  ..., -2.3043e-05,
         -1.0100e-03, -9.4154e-04],
        [-1.3009e-05, -8.9258e-06,  9.7230e-06,  ..., -1.0341e-05,
         -7.7225e-06, -8.3372e-06],
        [-3.1233e-05, -2.2173e-05,  2.4483e-05,  ..., -2.4602e-05,
         -1.8433e-05, -2.0102e-05],
        [-2.2799e-05, -1.5356e-05,  1.7717e-05,  ..., -1.7941e-05,
         -1.3433e-05, -1.5356e-05],
        [-3.1233e-05, -2.3320e-05,  2.4170e-05,  ..., -2.5451e-05,
         -1.9893e-05, -1.8716e-05]], device='cuda:0')
Loss: 1.0137497186660767
Graident accumulation at epoch 1, step 1207, batch 159
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0155, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.3632e-05,  5.4156e-05, -1.7488e-04,  ..., -2.6764e-05,
         -1.4119e-04, -1.2949e-04],
        [-1.2442e-05, -7.3768e-06,  7.0616e-06,  ..., -9.3141e-06,
         -6.9432e-06, -8.0932e-06],
        [ 1.8134e-05,  3.3613e-05, -1.6082e-05,  ...,  1.9683e-05,
          2.7345e-05,  6.0837e-06],
        [-1.7434e-05, -6.1331e-06,  1.1155e-05,  ..., -1.1827e-05,
         -8.1324e-06, -1.2148e-05],
        [-2.9502e-05, -2.2113e-05,  2.0365e-05,  ..., -2.4541e-05,
         -1.9197e-05, -1.9079e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8430e-08, 5.1423e-08, 5.0266e-08,  ..., 1.7825e-08, 1.1902e-07,
         3.9171e-08],
        [8.1037e-11, 5.0028e-11, 1.9200e-11,  ..., 5.6145e-11, 1.8528e-11,
         2.3738e-11],
        [1.7121e-09, 1.1279e-09, 3.6728e-10,  ..., 1.3139e-09, 5.4230e-10,
         4.0831e-10],
        [4.3771e-10, 3.6151e-10, 1.4940e-10,  ..., 3.9879e-10, 1.6292e-10,
         1.6529e-10],
        [3.8966e-10, 2.1291e-10, 6.5193e-11,  ..., 2.8105e-10, 6.7636e-11,
         1.0977e-10]], device='cuda:0')
optimizer state dict: 151.0
lr: [8.026520767887557e-06, 8.026520767887557e-06]
scheduler_last_epoch: 151
Epoch 1 | Batch 159/1048 | Training PPL: 3327.5705406176703 | time 11.50677490234375
Saving checkpoint at epoch 1, step 1207, batch 159
Epoch 1 | Validation PPL: 7.026386622762795 | Learning rate: 8.026520767887557e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1207, AFTER epoch 1, step 1207


Running epoch 1, step 1208, batch 160
Sampled inputs[:2]: tensor([[    0,  6088,  1172,  ...,   546,   401,   925],
        [    0,   199, 11296,  ...,   266, 10463,  8256]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1629e-06,  2.8349e-05, -2.2762e-05,  ..., -8.5462e-05,
          1.5053e-04, -5.7321e-05],
        [-1.6615e-06, -1.1995e-06,  1.1697e-06,  ..., -1.3337e-06,
         -1.0133e-06, -9.9838e-07],
        [-3.9935e-06, -3.0100e-06,  2.9951e-06,  ..., -3.2336e-06,
         -2.4736e-06, -2.4438e-06],
        [-2.9802e-06, -2.1309e-06,  2.1607e-06,  ..., -2.3842e-06,
         -1.8179e-06, -1.8775e-06],
        [-3.9041e-06, -3.0845e-06,  2.8759e-06,  ..., -3.2485e-06,
         -2.5928e-06, -2.2054e-06]], device='cuda:0')
Loss: 0.9922026991844177


Running epoch 1, step 1209, batch 161
Sampled inputs[:2]: tensor([[    0,  1526,   422,  ..., 22454,   409, 31482],
        [    0,  1412,    35,  ...,  6077,   298,  1826]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8577e-05,  2.0158e-05, -2.5606e-05,  ..., -1.4090e-04,
          1.7622e-04, -2.3769e-05],
        [-3.2932e-06, -2.3171e-06,  2.2426e-06,  ..., -2.7344e-06,
         -2.1234e-06, -2.1979e-06],
        [-7.8976e-06, -5.7667e-06,  5.7518e-06,  ..., -6.5267e-06,
         -5.0813e-06, -5.2750e-06],
        [-5.7518e-06, -3.9786e-06,  4.0233e-06,  ..., -4.7535e-06,
         -3.7029e-06, -3.9935e-06],
        [-7.9274e-06, -6.0201e-06,  5.6773e-06,  ..., -6.7204e-06,
         -5.4389e-06, -4.9174e-06]], device='cuda:0')
Loss: 0.9670197367668152


Running epoch 1, step 1210, batch 162
Sampled inputs[:2]: tensor([[   0, 2319,   30,  ...,  508, 6703,   12],
        [   0, 1236, 6446,  ...,  300,  706, 3698]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6843e-05,  1.2437e-04, -6.7464e-05,  ..., -1.5150e-04,
          1.1308e-04, -2.3769e-05],
        [-4.9695e-06, -3.5167e-06,  3.4720e-06,  ..., -4.0978e-06,
         -3.0994e-06, -3.2187e-06],
        [-1.1951e-05, -8.7619e-06,  8.8513e-06,  ..., -9.8199e-06,
         -7.4655e-06, -7.7486e-06],
        [-8.7023e-06, -6.0648e-06,  6.2436e-06,  ..., -7.1377e-06,
         -5.4240e-06, -5.8636e-06],
        [-1.1861e-05, -9.0599e-06,  8.6576e-06,  ..., -9.9987e-06,
         -7.9274e-06, -7.1377e-06]], device='cuda:0')
Loss: 1.0011831521987915


Running epoch 1, step 1211, batch 163
Sampled inputs[:2]: tensor([[    0,   292,    58,  ...,   319,   221,  1061],
        [    0,   278,   266,  ..., 10639,   292,  4723]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9490e-05,  1.5198e-04,  2.6983e-05,  ..., -1.3486e-04,
          1.4107e-04,  4.5748e-05],
        [-6.7130e-06, -4.5747e-06,  4.7013e-06,  ..., -5.4538e-06,
         -4.0382e-06, -4.4405e-06],
        [-1.6063e-05, -1.1399e-05,  1.1936e-05,  ..., -1.2994e-05,
         -9.7007e-06, -1.0580e-05],
        [-1.1638e-05, -7.8231e-06,  8.3894e-06,  ..., -9.4175e-06,
         -7.0259e-06, -7.9945e-06],
        [-1.5974e-05, -1.1846e-05,  1.1727e-05,  ..., -1.3262e-05,
         -1.0356e-05, -9.7603e-06]], device='cuda:0')
Loss: 0.935696542263031


Running epoch 1, step 1212, batch 164
Sampled inputs[:2]: tensor([[    0,   300,   369,  ...,    12,   970,    12],
        [    0,    13, 10036,  ...,   328,  2347, 12801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4750e-04,  1.2141e-04,  8.7084e-05,  ..., -1.9451e-04,
          1.0720e-04, -8.4693e-06],
        [-8.3148e-06, -5.6550e-06,  5.9679e-06,  ..., -6.7130e-06,
         -4.9621e-06, -5.3719e-06],
        [ 1.0640e-04,  1.2317e-04, -1.2383e-04,  ...,  1.1153e-04,
          8.9956e-05,  3.6162e-05],
        [-1.4558e-05, -9.7454e-06,  1.0774e-05,  ..., -1.1697e-05,
         -8.7097e-06, -9.7975e-06],
        [-1.9744e-05, -1.4648e-05,  1.4812e-05,  ..., -1.6332e-05,
         -1.2711e-05, -1.1832e-05]], device='cuda:0')
Loss: 1.0073051452636719


Running epoch 1, step 1213, batch 165
Sampled inputs[:2]: tensor([[    0, 13081,   278,  ...,   368,   266,  1717],
        [    0, 38717,  1679,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8643e-04,  2.7067e-04,  1.1348e-04,  ..., -2.1023e-04,
         -2.1818e-05, -1.0105e-04],
        [-9.8795e-06, -6.6683e-06,  7.2122e-06,  ..., -7.9423e-06,
         -5.8264e-06, -6.3628e-06],
        [ 1.0273e-04,  1.2074e-04, -1.2076e-04,  ...,  1.0868e-04,
          8.7974e-05,  3.3838e-05],
        [-1.7345e-05, -1.1489e-05,  1.3098e-05,  ..., -1.3858e-05,
         -1.0237e-05, -1.1683e-05],
        [-2.3335e-05, -1.7196e-05,  1.7777e-05,  ..., -1.9208e-05,
         -1.4827e-05, -1.3888e-05]], device='cuda:0')
Loss: 0.9518640041351318


Running epoch 1, step 1214, batch 166
Sampled inputs[:2]: tensor([[    0,    19,     9,  ..., 11504,   446,   381],
        [    0,   287,  2503,  ...,   496,    14, 37791]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7666e-04,  3.0551e-04,  1.7579e-04,  ..., -2.2454e-04,
          3.2117e-05, -5.5809e-05],
        [-1.1541e-05, -7.8380e-06,  8.3447e-06,  ..., -9.2909e-06,
         -6.7949e-06, -7.5176e-06],
        [ 9.8648e-05,  1.1776e-04, -1.1778e-04,  ...,  1.0540e-04,
          8.5590e-05,  3.1007e-05],
        [-2.0221e-05, -1.3486e-05,  1.5125e-05,  ..., -1.6198e-05,
         -1.1928e-05, -1.3769e-05],
        [-2.7388e-05, -2.0251e-05,  2.0683e-05,  ..., -2.2545e-05,
         -1.7330e-05, -1.6496e-05]], device='cuda:0')
Loss: 0.9592124819755554


Running epoch 1, step 1215, batch 167
Sampled inputs[:2]: tensor([[   0,  266, 9823,  ...,   14, 1062, 7676],
        [   0, 1552,  300,  ..., 1085,   12,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0345e-04,  2.0058e-05,  1.5138e-05,  ..., -1.1034e-04,
          3.2117e-05, -9.5813e-05],
        [-1.3173e-05, -8.7991e-06,  9.4622e-06,  ..., -1.0632e-05,
         -7.7263e-06, -8.7544e-06],
        [ 9.4834e-05,  1.1548e-04, -1.1498e-04,  ...,  1.0238e-04,
          8.3489e-05,  2.8235e-05],
        [-2.3022e-05, -1.5073e-05,  1.7107e-05,  ..., -1.8463e-05,
         -1.3500e-05, -1.5944e-05],
        [-3.1352e-05, -2.2694e-05,  2.3544e-05,  ..., -2.5779e-05,
         -1.9670e-05, -1.9178e-05]], device='cuda:0')
Loss: 0.9224623441696167
Graident accumulation at epoch 1, step 1215, batch 167
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.8614e-05,  5.0746e-05, -1.5588e-04,  ..., -3.5122e-05,
         -1.2386e-04, -1.2613e-04],
        [-1.2515e-05, -7.5190e-06,  7.3017e-06,  ..., -9.4459e-06,
         -7.0215e-06, -8.1593e-06],
        [ 2.5804e-05,  4.1800e-05, -2.5972e-05,  ...,  2.7952e-05,
          3.2960e-05,  8.2988e-06],
        [-1.7993e-05, -7.0270e-06,  1.1750e-05,  ..., -1.2491e-05,
         -8.6692e-06, -1.2528e-05],
        [-2.9687e-05, -2.2172e-05,  2.0682e-05,  ..., -2.4665e-05,
         -1.9245e-05, -1.9089e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8382e-08, 5.1372e-08, 5.0216e-08,  ..., 1.7819e-08, 1.1891e-07,
         3.9141e-08],
        [8.1130e-11, 5.0056e-11, 1.9270e-11,  ..., 5.6202e-11, 1.8569e-11,
         2.3791e-11],
        [1.7194e-09, 1.1402e-09, 3.8013e-10,  ..., 1.3231e-09, 5.4873e-10,
         4.0870e-10],
        [4.3780e-10, 3.6138e-10, 1.4954e-10,  ..., 3.9873e-10, 1.6294e-10,
         1.6538e-10],
        [3.9026e-10, 2.1322e-10, 6.5682e-11,  ..., 2.8144e-10, 6.7955e-11,
         1.1003e-10]], device='cuda:0')
optimizer state dict: 152.0
lr: [7.905489126218852e-06, 7.905489126218852e-06]
scheduler_last_epoch: 152


Running epoch 1, step 1216, batch 168
Sampled inputs[:2]: tensor([[    0,   278,  5492,  ...,   328,   995,    13],
        [    0,  1526,  3502,  ..., 11727,  3736,  1661]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6691e-05, -3.3553e-05, -5.5409e-05,  ...,  2.6314e-05,
          3.3279e-05, -1.0587e-04],
        [-1.6987e-06, -1.1101e-06,  1.2144e-06,  ..., -1.3784e-06,
         -1.0431e-06, -1.1027e-06],
        [-4.0829e-06, -2.8014e-06,  3.0845e-06,  ..., -3.3081e-06,
         -2.5183e-06, -2.6673e-06],
        [-2.9951e-06, -1.9372e-06,  2.2054e-06,  ..., -2.4289e-06,
         -1.8477e-06, -2.0415e-06],
        [-4.1723e-06, -2.9653e-06,  3.0994e-06,  ..., -3.4720e-06,
         -2.7567e-06, -2.5183e-06]], device='cuda:0')
Loss: 0.9984226226806641


Running epoch 1, step 1217, batch 169
Sampled inputs[:2]: tensor([[    0,  1356,    18,  ...,    31,   333,   199],
        [    0,   266,  2552,  ...,    13, 16179,   800]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3997e-05, -9.5752e-06,  9.3745e-05,  ..., -2.5372e-05,
          1.7136e-04, -1.5042e-04],
        [-3.3975e-06, -2.3097e-06,  2.4065e-06,  ..., -2.7269e-06,
         -2.0489e-06, -2.1458e-06],
        [-8.2552e-06, -5.8711e-06,  6.1542e-06,  ..., -6.6161e-06,
         -5.0068e-06, -5.2452e-06],
        [ 1.2632e-04,  1.8580e-04, -1.2837e-04,  ...,  1.3748e-04,
          1.3564e-04,  8.8175e-05],
        [-8.3745e-06, -6.1989e-06,  6.1840e-06,  ..., -6.9141e-06,
         -5.4538e-06, -4.9621e-06]], device='cuda:0')
Loss: 1.0087990760803223


Running epoch 1, step 1218, batch 170
Sampled inputs[:2]: tensor([[    0,    13, 23904,  ...,   560,  8840,    26],
        [    0,  1690, 16858,  ...,   199,   395,  3902]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5128e-05, -6.0964e-05,  5.8790e-05,  ..., -7.9657e-05,
          1.1114e-04, -2.9577e-04],
        [-4.9695e-06, -3.2932e-06,  3.5837e-06,  ..., -3.9712e-06,
         -2.9095e-06, -3.1590e-06],
        [-1.2025e-05, -8.2999e-06,  9.1493e-06,  ..., -9.5516e-06,
         -7.0333e-06, -7.6294e-06],
        [ 1.2357e-04,  1.8417e-04, -1.2624e-04,  ...,  1.3534e-04,
          1.3417e-04,  8.6320e-05],
        [-1.2159e-05, -8.7470e-06,  9.1493e-06,  ..., -9.9242e-06,
         -7.6443e-06, -7.1377e-06]], device='cuda:0')
Loss: 0.9480988383293152


Running epoch 1, step 1219, batch 171
Sampled inputs[:2]: tensor([[   0,  266, 3382,  ...,  759,  631,  369],
        [   0,  759, 1128,  ...,  221,  474,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0790e-04, -6.7572e-05,  6.2100e-05,  ..., -9.1486e-05,
         -3.0153e-05, -5.9129e-04],
        [-6.5714e-06, -4.3437e-06,  4.8950e-06,  ..., -5.1931e-06,
         -3.7737e-06, -4.1127e-06],
        [-1.5929e-05, -1.0908e-05,  1.2487e-05,  ..., -1.2472e-05,
         -9.0897e-06, -9.9540e-06],
        [ 1.2064e-04,  1.8231e-04, -1.2374e-04,  ...,  1.3313e-04,
          1.3262e-04,  8.4472e-05],
        [-1.5810e-05, -1.1325e-05,  1.2234e-05,  ..., -1.2755e-05,
         -9.7454e-06, -9.1195e-06]], device='cuda:0')
Loss: 0.9512932300567627


Running epoch 1, step 1220, batch 172
Sampled inputs[:2]: tensor([[    0,   199, 14973,  ...,   638,  1119,  1329],
        [    0, 15666,   609,  ...,   527,  4486,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0825e-06, -7.9938e-05, -5.1811e-05,  ..., -1.0839e-04,
          9.8123e-06, -8.2169e-04],
        [-8.1807e-06, -5.4091e-06,  6.1169e-06,  ..., -6.4522e-06,
         -4.6976e-06, -5.1335e-06],
        [-1.9744e-05, -1.3530e-05,  1.5602e-05,  ..., -1.5438e-05,
         -1.1265e-05, -1.2353e-05],
        [ 1.1776e-04,  1.8045e-04, -1.2145e-04,  ...,  1.3090e-04,
          1.3097e-04,  8.2550e-05],
        [-1.9565e-05, -1.4022e-05,  1.5244e-05,  ..., -1.5751e-05,
         -1.2040e-05, -1.1265e-05]], device='cuda:0')
Loss: 0.9581174850463867


Running epoch 1, step 1221, batch 173
Sampled inputs[:2]: tensor([[   0,   12, 4856,  ...,  342,  266, 1040],
        [   0,  995,   13,  ..., 3494,  367, 6768]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2561e-05,  4.0914e-05,  1.5460e-05,  ..., -1.1508e-04,
          2.0882e-05, -7.5372e-04],
        [-9.7901e-06, -6.5863e-06,  7.3537e-06,  ..., -7.7486e-06,
         -5.6811e-06, -6.1095e-06],
        [ 5.1564e-05,  4.8717e-05, -4.5256e-05,  ...,  2.6286e-05,
          6.6330e-05,  6.7491e-06],
        [ 1.1483e-04,  1.7834e-04, -1.1914e-04,  ...,  1.2856e-04,
          1.2918e-04,  8.0665e-05],
        [-2.3410e-05, -1.7107e-05,  1.8284e-05,  ..., -1.8969e-05,
         -1.4588e-05, -1.3471e-05]], device='cuda:0')
Loss: 1.0012521743774414


Running epoch 1, step 1222, batch 174
Sampled inputs[:2]: tensor([[    0,  1756,   271,  ...,   259, 48595, 19882],
        [    0,    12,   401,  ...,   504,   565,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1494e-05,  8.2725e-06, -3.7502e-05,  ..., -1.5320e-04,
         -1.4850e-04, -9.8663e-04],
        [-1.1437e-05, -7.6443e-06,  8.6129e-06,  ..., -9.0376e-06,
         -6.5789e-06, -7.0557e-06],
        [ 4.7690e-05,  4.6110e-05, -4.2112e-05,  ...,  2.3246e-05,
          6.4200e-05,  4.5288e-06],
        [ 1.1186e-04,  1.7646e-04, -1.1677e-04,  ...,  1.2625e-04,
          1.2757e-04,  7.8877e-05],
        [-2.7180e-05, -1.9789e-05,  2.1294e-05,  ..., -2.2009e-05,
         -1.6823e-05, -1.5438e-05]], device='cuda:0')
Loss: 0.9919213056564331


Running epoch 1, step 1223, batch 175
Sampled inputs[:2]: tensor([[   0,    9,  287,  ...,  369, 2968, 8347],
        [   0, 3103, 2134,  ..., 6627,  275, 1911]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6625e-05, -5.1377e-05, -2.9324e-05,  ..., -1.3957e-04,
         -6.3059e-05, -8.8071e-04],
        [-1.3180e-05, -8.7321e-06,  9.6783e-06,  ..., -1.0453e-05,
         -7.6666e-06, -8.3148e-06],
        [ 4.3488e-05,  4.3308e-05, -3.9370e-05,  ...,  1.9819e-05,
          6.1517e-05,  1.4890e-06],
        [ 1.0897e-04,  1.7467e-04, -1.1494e-04,  ...,  1.2389e-04,
          1.2573e-04,  7.6701e-05],
        [-3.1650e-05, -2.2858e-05,  2.4170e-05,  ..., -2.5749e-05,
         -1.9819e-05, -1.8477e-05]], device='cuda:0')
Loss: 0.9610650539398193
Graident accumulation at epoch 1, step 1223, batch 175
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0011],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.1090e-05,  4.0534e-05, -1.4322e-04,  ..., -4.5567e-05,
         -1.1778e-04, -2.0158e-04],
        [-1.2582e-05, -7.6403e-06,  7.5393e-06,  ..., -9.5466e-06,
         -7.0860e-06, -8.1749e-06],
        [ 2.7572e-05,  4.1951e-05, -2.7311e-05,  ...,  2.7139e-05,
          3.5815e-05,  7.6178e-06],
        [-5.2966e-06,  1.1143e-05, -9.1922e-07,  ...,  1.1476e-06,
          4.7704e-06, -3.6051e-06],
        [-2.9883e-05, -2.2240e-05,  2.1031e-05,  ..., -2.4774e-05,
         -1.9302e-05, -1.9028e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8324e-08, 5.1323e-08, 5.0167e-08,  ..., 1.7821e-08, 1.1879e-07,
         3.9877e-08],
        [8.1222e-11, 5.0082e-11, 1.9344e-11,  ..., 5.6255e-11, 1.8609e-11,
         2.3837e-11],
        [1.7195e-09, 1.1409e-09, 3.8130e-10,  ..., 1.3221e-09, 5.5196e-10,
         4.0829e-10],
        [4.4924e-10, 3.9152e-10, 1.6261e-10,  ..., 4.1368e-10, 1.7858e-10,
         1.7110e-10],
        [3.9087e-10, 2.1352e-10, 6.6200e-11,  ..., 2.8182e-10, 6.8280e-11,
         1.1026e-10]], device='cuda:0')
optimizer state dict: 153.0
lr: [7.784777544094901e-06, 7.784777544094901e-06]
scheduler_last_epoch: 153


Running epoch 1, step 1224, batch 176
Sampled inputs[:2]: tensor([[   0,  590,   16,  ...,   13,   35, 1151],
        [   0,  367, 3704,  ..., 1746,   14,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5239e-05,  1.2606e-04,  1.7301e-04,  ..., -1.4436e-05,
          1.0044e-04, -3.5728e-05],
        [-1.6242e-06, -1.1623e-06,  1.1995e-06,  ..., -1.3113e-06,
         -9.7603e-07, -1.0356e-06],
        [-3.9935e-06, -2.9653e-06,  3.0994e-06,  ..., -3.2037e-06,
         -2.4140e-06, -2.5630e-06],
        [-3.0249e-06, -2.1309e-06,  2.3097e-06,  ..., -2.4289e-06,
         -1.8403e-06, -2.0415e-06],
        [-3.8445e-06, -2.9951e-06,  2.9653e-06,  ..., -3.1739e-06,
         -2.4885e-06, -2.2799e-06]], device='cuda:0')
Loss: 1.0016978979110718


Running epoch 1, step 1225, batch 177
Sampled inputs[:2]: tensor([[    0, 13312,  9048,  ..., 33470,  8672,  3524],
        [    0,   266,  6071,  ...,  1061,  1107,   839]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5788e-05,  3.4839e-04,  4.3040e-04,  ..., -1.1701e-05,
          1.2652e-04, -5.5821e-05],
        [-3.2037e-06, -2.1458e-06,  2.2873e-06,  ..., -2.6301e-06,
         -1.9185e-06, -2.1234e-06],
        [-7.8082e-06, -5.4389e-06,  5.9307e-06,  ..., -6.3628e-06,
         -4.6790e-06, -5.1409e-06],
        [-5.9158e-06, -3.8818e-06,  4.3660e-06,  ..., -4.8429e-06,
         -3.5763e-06, -4.1276e-06],
        [-7.6592e-06, -5.6028e-06,  5.7518e-06,  ..., -6.4075e-06,
         -4.9323e-06, -4.6641e-06]], device='cuda:0')
Loss: 0.972170352935791


Running epoch 1, step 1226, batch 178
Sampled inputs[:2]: tensor([[    0, 14296,   292,  ...,    18,   271, 16158],
        [    0,  1760,     9,  ...,   278,  6607,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8131e-06,  4.3556e-04,  6.1822e-04,  ...,  5.5200e-05,
          1.2396e-04, -7.9101e-05],
        [-4.8578e-06, -3.3230e-06,  3.4496e-06,  ..., -3.9935e-06,
         -3.0212e-06, -3.2336e-06],
        [-1.1742e-05, -8.4043e-06,  8.8662e-06,  ..., -9.6112e-06,
         -7.3463e-06, -7.7933e-06],
        [-8.8811e-06, -5.9828e-06,  6.5118e-06,  ..., -7.3165e-06,
         -5.6177e-06, -6.2585e-06],
        [-1.1653e-05, -8.7619e-06,  8.7172e-06,  ..., -9.8348e-06,
         -7.8380e-06, -7.1526e-06]], device='cuda:0')
Loss: 0.9893805384635925


Running epoch 1, step 1227, batch 179
Sampled inputs[:2]: tensor([[    0,  2790,   266,  ...,   401,  1496,    14],
        [    0, 22340,   574,  ...,   494,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6307e-04,  3.9253e-04,  5.6098e-04,  ...,  5.0279e-05,
          1.3866e-04, -3.5555e-04],
        [-6.4075e-06, -4.3139e-06,  4.6045e-06,  ..., -5.2676e-06,
         -3.8855e-06, -4.2617e-06],
        [-1.5557e-05, -1.0952e-05,  1.1921e-05,  ..., -1.2726e-05,
         -9.4622e-06, -1.0282e-05],
        [-1.1712e-05, -7.7486e-06,  8.7172e-06,  ..., -9.6560e-06,
         -7.2047e-06, -8.2403e-06],
        [ 5.3987e-05,  5.4923e-05, -4.4828e-05,  ...,  7.3160e-05,
          6.1509e-05,  4.7418e-05]], device='cuda:0')
Loss: 0.9600518345832825


Running epoch 1, step 1228, batch 180
Sampled inputs[:2]: tensor([[    0,   677,  8708,  ..., 19891,   267,   287],
        [    0,  3047,  4878,  ...,   352, 10854, 34025]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4382e-04,  4.3179e-04,  6.0912e-04,  ..., -5.1327e-05,
          1.8770e-04, -4.1327e-04],
        [-7.9870e-06, -5.2154e-06,  5.7667e-06,  ..., -6.5491e-06,
         -4.7982e-06, -5.3495e-06],
        [-1.9401e-05, -1.3202e-05,  1.4916e-05,  ..., -1.5736e-05,
         -1.1563e-05, -1.2830e-05],
        [-1.4573e-05, -9.2983e-06,  1.0893e-05,  ..., -1.1951e-05,
         -8.8364e-06, -1.0297e-05],
        [ 5.0143e-05,  5.2524e-05, -4.1863e-05,  ...,  7.0061e-05,
          5.9230e-05,  4.5094e-05]], device='cuda:0')
Loss: 0.9612500071525574


Running epoch 1, step 1229, batch 181
Sampled inputs[:2]: tensor([[    0,   923,    13,  ...,   199,   677,  3826],
        [    0,  3908,   300,  ..., 10874,  2667,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6667e-04,  4.7050e-04,  6.9907e-04,  ..., -6.4404e-05,
          3.3322e-04, -3.3024e-04],
        [-9.6262e-06, -6.4075e-06,  6.8396e-06,  ..., -7.8753e-06,
         -5.8189e-06, -6.4597e-06],
        [-2.3425e-05, -1.6272e-05,  1.7762e-05,  ..., -1.8969e-05,
         -1.4082e-05, -1.5557e-05],
        [-1.7494e-05, -1.1429e-05,  1.2875e-05,  ..., -1.4335e-05,
         -1.0692e-05, -1.2383e-05],
        [ 4.6149e-05,  4.9395e-05, -3.9076e-05,  ...,  6.6753e-05,
          5.6577e-05,  4.2576e-05]], device='cuda:0')
Loss: 0.9766114354133606


Running epoch 1, step 1230, batch 182
Sampled inputs[:2]: tensor([[   0, 1128, 3231,  ..., 8375,  199, 2038],
        [   0,   14,  475,  ..., 6895, 5842, 2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0196e-04,  4.8422e-04,  6.5608e-04,  ...,  2.4349e-05,
          2.4363e-04, -4.6106e-04],
        [-1.1176e-05, -7.4580e-06,  8.1137e-06,  ..., -9.1642e-06,
         -6.7130e-06, -7.3500e-06],
        [-2.7165e-05, -1.8835e-05,  2.0981e-05,  ..., -2.2024e-05,
         -1.6183e-05, -1.7688e-05],
        [-2.0310e-05, -1.3269e-05,  1.5289e-05,  ..., -1.6645e-05,
         -1.2286e-05, -1.4089e-05],
        [ 4.2573e-05,  4.6802e-05, -3.6036e-05,  ...,  6.3758e-05,
          5.4402e-05,  4.0743e-05]], device='cuda:0')
Loss: 1.008377194404602


Running epoch 1, step 1231, batch 183
Sampled inputs[:2]: tensor([[    0,   607, 11059,  ...,  2081,  1194,   278],
        [    0,  1477,  5648,  ...,  4391,  1722,   369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0964e-04,  4.8687e-04,  6.8059e-04,  ...,  2.0636e-06,
          1.7099e-04, -5.9578e-04],
        [-1.2808e-05, -8.5607e-06,  9.4026e-06,  ..., -1.0438e-05,
         -7.6443e-06, -8.3037e-06],
        [-3.1188e-05, -2.1636e-05,  2.4304e-05,  ..., -2.5138e-05,
         -1.8463e-05, -2.0072e-05],
        [-2.3380e-05, -1.5296e-05,  1.7792e-05,  ..., -1.9014e-05,
         -1.4015e-05, -1.5981e-05],
        [ 3.8803e-05,  4.4015e-05, -3.2967e-05,  ...,  6.0718e-05,
          5.2077e-05,  3.8701e-05]], device='cuda:0')
Loss: 0.9810497164726257
Graident accumulation at epoch 1, step 1231, batch 183
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.6945e-05,  8.5167e-05, -6.0840e-05,  ..., -4.0804e-05,
         -8.8902e-05, -2.4100e-04],
        [-1.2604e-05, -7.7324e-06,  7.7257e-06,  ..., -9.6358e-06,
         -7.1418e-06, -8.1878e-06],
        [ 2.1696e-05,  3.5592e-05, -2.2150e-05,  ...,  2.1911e-05,
          3.0388e-05,  4.8489e-06],
        [-7.1049e-06,  8.4988e-06,  9.5190e-07,  ..., -8.6852e-07,
          2.8919e-06, -4.8427e-06],
        [-2.3014e-05, -1.5615e-05,  1.5631e-05,  ..., -1.6224e-05,
         -1.2164e-05, -1.3255e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8277e-08, 5.1509e-08, 5.0580e-08,  ..., 1.7803e-08, 1.1870e-07,
         4.0193e-08],
        [8.1305e-11, 5.0105e-11, 1.9413e-11,  ..., 5.6308e-11, 1.8649e-11,
         2.3882e-11],
        [1.7188e-09, 1.1402e-09, 3.8151e-10,  ..., 1.3214e-09, 5.5175e-10,
         4.0829e-10],
        [4.4933e-10, 3.9137e-10, 1.6276e-10,  ..., 4.1363e-10, 1.7860e-10,
         1.7118e-10],
        [3.9198e-10, 2.1525e-10, 6.7221e-11,  ..., 2.8522e-10, 7.0924e-11,
         1.1165e-10]], device='cuda:0')
optimizer state dict: 154.0
lr: [7.664404467299166e-06, 7.664404467299166e-06]
scheduler_last_epoch: 154


Running epoch 1, step 1232, batch 184
Sampled inputs[:2]: tensor([[   0,   14, 4746,  ...,  266, 1119, 1705],
        [   0, 9818,  347,  ...,  413, 7359,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0275e-05,  1.1939e-04,  2.7496e-04,  ..., -6.5970e-05,
          1.5730e-04,  1.4785e-04],
        [-1.6168e-06, -1.1772e-06,  9.9838e-07,  ..., -1.4529e-06,
         -1.1921e-06, -1.1697e-06],
        [-3.9935e-06, -3.0398e-06,  2.6822e-06,  ..., -3.5465e-06,
         -2.9355e-06, -2.8759e-06],
        [-2.8610e-06, -2.0266e-06,  1.8254e-06,  ..., -2.5779e-06,
         -2.1309e-06, -2.1607e-06],
        [-4.1723e-06, -3.2336e-06,  2.7567e-06,  ..., -3.7849e-06,
         -3.2037e-06, -2.7716e-06]], device='cuda:0')
Loss: 1.0063155889511108


Running epoch 1, step 1233, batch 185
Sampled inputs[:2]: tensor([[   0, 3351,  352,  ...,   17,  287,  357],
        [   0,  342, 4781,  ...,  630,  940,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0511e-04,  2.5279e-04,  2.1294e-04,  ..., -7.5679e-05,
          1.9593e-04,  1.5396e-04],
        [-3.1069e-06, -2.1234e-06,  2.1234e-06,  ..., -2.7493e-06,
         -2.2426e-06, -2.1532e-06],
        [-7.4506e-06, -5.2750e-06,  5.4836e-06,  ..., -6.4224e-06,
         -5.2750e-06, -5.0217e-06],
        [-5.6177e-06, -3.6806e-06,  3.9861e-06,  ..., -4.9472e-06,
         -4.0829e-06, -4.0829e-06],
        [-7.7188e-06, -5.6624e-06,  5.5879e-06,  ..., -6.8098e-06,
         -5.7518e-06, -4.7386e-06]], device='cuda:0')
Loss: 0.9308792352676392


Running epoch 1, step 1234, batch 186
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,   365,  2463,   391],
        [    0,   266,   554,  ..., 10679,  3790,   857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4161e-04,  2.1629e-04,  2.3584e-04,  ..., -1.3784e-04,
          5.3417e-05,  5.4726e-05],
        [-4.7162e-06, -3.1590e-06,  3.4422e-06,  ..., -4.0233e-06,
         -3.1926e-06, -3.1292e-06],
        [-1.1474e-05, -7.9572e-06,  8.9258e-06,  ..., -9.5814e-06,
         -7.6443e-06, -7.4953e-06],
        [-8.5831e-06, -5.5358e-06,  6.5193e-06,  ..., -7.2718e-06,
         -5.8189e-06, -5.9903e-06],
        [-1.1399e-05, -8.2850e-06,  8.7023e-06,  ..., -9.8050e-06,
         -8.1211e-06, -6.7949e-06]], device='cuda:0')
Loss: 0.9867205619812012


Running epoch 1, step 1235, batch 187
Sampled inputs[:2]: tensor([[    0,  2296,   446,  ...,  2937,   287,  2795],
        [    0,   381, 19527,  ...,   271,   298,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4161e-04,  2.5076e-05,  1.2899e-04,  ..., -1.9657e-04,
         -3.9724e-05,  7.8252e-05],
        [-6.3255e-06, -4.1872e-06,  4.7609e-06,  ..., -5.2899e-06,
         -4.1090e-06, -4.1425e-06],
        [-1.5527e-05, -1.0625e-05,  1.2383e-05,  ..., -1.2770e-05,
         -9.9540e-06, -1.0088e-05],
        [-1.1533e-05, -7.3835e-06,  9.0227e-06,  ..., -9.5814e-06,
         -7.4804e-06, -7.9423e-06],
        [-1.5303e-05, -1.0997e-05,  1.1981e-05,  ..., -1.2964e-05,
         -1.0520e-05, -9.1046e-06]], device='cuda:0')
Loss: 0.9951503872871399


Running epoch 1, step 1236, batch 188
Sampled inputs[:2]: tensor([[    0,   287, 21212,  ...,  3123,   944,   278],
        [    0, 23487,   273,  ...,   368,   259,   422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9406e-04, -5.0920e-05,  1.0325e-05,  ..., -2.5701e-04,
          1.3705e-04,  2.4168e-04],
        [-8.0392e-06, -5.3197e-06,  5.9977e-06,  ..., -6.6459e-06,
         -5.1670e-06, -5.2229e-06],
        [-1.9670e-05, -1.3456e-05,  1.5542e-05,  ..., -1.6019e-05,
         -1.2517e-05, -1.2726e-05],
        [-1.4603e-05, -9.3654e-06,  1.1303e-05,  ..., -1.1995e-05,
         -9.3728e-06, -9.9689e-06],
        [-1.9357e-05, -1.3918e-05,  1.5005e-05,  ..., -1.6257e-05,
         -1.3232e-05, -1.1504e-05]], device='cuda:0')
Loss: 1.0081380605697632


Running epoch 1, step 1237, batch 189
Sampled inputs[:2]: tensor([[   0,  474,  221,  ..., 2945,    9,  287],
        [   0,  221,  474,  ..., 1871,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9583e-05, -4.0167e-05, -6.1299e-05,  ..., -1.2208e-04,
         -7.8145e-05,  1.4979e-05],
        [-9.6411e-06, -6.1691e-06,  7.3239e-06,  ..., -7.9423e-06,
         -6.0201e-06, -6.2361e-06],
        [-2.3469e-05, -1.5557e-05,  1.8865e-05,  ..., -1.9014e-05,
         -1.4484e-05, -1.5050e-05],
        [-1.7524e-05, -1.0841e-05,  1.3821e-05,  ..., -1.4335e-05,
         -1.0900e-05, -1.1891e-05],
        [-2.2978e-05, -1.6063e-05,  1.8120e-05,  ..., -1.9193e-05,
         -1.5289e-05, -1.3486e-05]], device='cuda:0')
Loss: 0.9426683783531189


Running epoch 1, step 1238, batch 190
Sampled inputs[:2]: tensor([[  0, 894, 496,  ..., 266, 623, 587],
        [  0,  14, 747,  ..., 367, 300, 369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2568e-05, -1.2469e-04, -1.1322e-04,  ..., -9.0446e-05,
         -1.2776e-04, -1.8217e-04],
        [-1.1355e-05, -7.1898e-06,  8.4713e-06,  ..., -9.2685e-06,
         -6.9588e-06, -7.4506e-06],
        [-2.7791e-05, -1.8269e-05,  2.1964e-05,  ..., -2.2352e-05,
         -1.6868e-05, -1.8120e-05],
        [-2.0474e-05, -1.2584e-05,  1.5907e-05,  ..., -1.6615e-05,
         -1.2517e-05, -1.4037e-05],
        [-2.7359e-05, -1.8924e-05,  2.1175e-05,  ..., -2.2665e-05,
         -1.7911e-05, -1.6391e-05]], device='cuda:0')
Loss: 0.9516420364379883


Running epoch 1, step 1239, batch 191
Sampled inputs[:2]: tensor([[   0, 1486,  292,  ..., 7484,   15, 5357],
        [   0, 4852,  266,  ..., 2523, 2080, 2632]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9903e-05, -2.0325e-04, -1.1322e-04,  ..., -7.8352e-05,
          2.9464e-05, -7.2652e-05],
        [-1.2971e-05, -8.3596e-06,  9.5963e-06,  ..., -1.0565e-05,
         -7.9423e-06, -8.5011e-06],
        [-3.1963e-05, -2.1383e-05,  2.5049e-05,  ..., -2.5660e-05,
         -1.9386e-05, -2.0802e-05],
        [-2.3335e-05, -1.4625e-05,  1.7978e-05,  ..., -1.8910e-05,
         -1.4275e-05, -1.6004e-05],
        [-3.1322e-05, -2.2024e-05,  2.4080e-05,  ..., -2.5928e-05,
         -2.0504e-05, -1.8731e-05]], device='cuda:0')
Loss: 0.9546512961387634
Graident accumulation at epoch 1, step 1239, batch 191
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0081,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.1241e-05,  5.6325e-05, -6.6078e-05,  ..., -4.4559e-05,
         -7.7065e-05, -2.2417e-04],
        [-1.2641e-05, -7.7951e-06,  7.9127e-06,  ..., -9.7287e-06,
         -7.2219e-06, -8.2191e-06],
        [ 1.6330e-05,  2.9894e-05, -1.7430e-05,  ...,  1.7154e-05,
          2.5410e-05,  2.2838e-06],
        [-8.7280e-06,  6.1863e-06,  2.6545e-06,  ..., -2.6726e-06,
          1.1752e-06, -5.9588e-06],
        [-2.3845e-05, -1.6256e-05,  1.6476e-05,  ..., -1.7195e-05,
         -1.2998e-05, -1.3802e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8229e-08, 5.1499e-08, 5.0542e-08,  ..., 1.7791e-08, 1.1858e-07,
         4.0158e-08],
        [8.1392e-11, 5.0125e-11, 1.9486e-11,  ..., 5.6363e-11, 1.8693e-11,
         2.3930e-11],
        [1.7181e-09, 1.1395e-09, 3.8176e-10,  ..., 1.3208e-09, 5.5158e-10,
         4.0831e-10],
        [4.4943e-10, 3.9119e-10, 1.6292e-10,  ..., 4.1357e-10, 1.7863e-10,
         1.7127e-10],
        [3.9257e-10, 2.1552e-10, 6.7734e-11,  ..., 2.8561e-10, 7.1273e-11,
         1.1189e-10]], device='cuda:0')
optimizer state dict: 155.0
lr: [7.544388289888527e-06, 7.544388289888527e-06]
scheduler_last_epoch: 155


Running epoch 1, step 1240, batch 192
Sampled inputs[:2]: tensor([[    0, 10348,  2994,  ...,   266, 24089, 10607],
        [    0,  2805,   391,  ...,    12,   259,  1420]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0032e-04, -5.9720e-05,  7.2028e-05,  ..., -2.9611e-05,
         -1.6860e-05,  2.8530e-05],
        [-1.6689e-06, -1.0803e-06,  1.2890e-06,  ..., -1.2740e-06,
         -9.4995e-07, -9.7603e-07],
        [-4.1723e-06, -2.8312e-06,  3.4124e-06,  ..., -3.1888e-06,
         -2.3991e-06, -2.4587e-06],
        [-3.0398e-06, -1.9521e-06,  2.4438e-06,  ..., -2.3097e-06,
         -1.7360e-06, -1.8775e-06],
        [-3.8147e-06, -2.7418e-06,  3.0994e-06,  ..., -3.0249e-06,
         -2.3842e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9932059645652771


Running epoch 1, step 1241, batch 193
Sampled inputs[:2]: tensor([[   0,  806,  300,  ...,  360, 4918, 1106],
        [   0,   14,   23,  ...,  278,  266, 1462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6766e-05,  8.9025e-05,  1.5363e-04,  ...,  6.3816e-06,
         -2.3553e-05, -6.5419e-05],
        [-3.2410e-06, -2.2799e-06,  2.5257e-06,  ..., -2.6152e-06,
         -2.0452e-06, -1.9036e-06],
        [-7.9423e-06, -5.8115e-06,  6.5565e-06,  ..., -6.3777e-06,
         -5.0217e-06, -4.6492e-06],
        [-5.8413e-06, -4.0680e-06,  4.7535e-06,  ..., -4.7088e-06,
         -3.7327e-06, -3.6359e-06],
        [-7.4804e-06, -5.7817e-06,  6.1244e-06,  ..., -6.1989e-06,
         -5.0962e-06, -3.9786e-06]], device='cuda:0')
Loss: 0.9774065017700195


Running epoch 1, step 1242, batch 194
Sampled inputs[:2]: tensor([[   0,   12,  298,  ..., 5125, 6654, 4925],
        [   0,   14,   22,  ..., 1319,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8976e-05,  1.2127e-04,  2.3969e-04,  ..., -2.4587e-05,
          1.3556e-04,  2.1904e-04],
        [-4.9472e-06, -3.4869e-06,  3.6657e-06,  ..., -4.0233e-06,
         -3.2224e-06, -3.0734e-06],
        [-1.2353e-05, -9.1046e-06,  9.6709e-06,  ..., -9.9987e-06,
         -8.0764e-06, -7.6443e-06],
        [-8.7619e-06, -6.1095e-06,  6.7502e-06,  ..., -7.1228e-06,
         -5.7742e-06, -5.7369e-06],
        [-1.1861e-05, -9.1940e-06,  9.2089e-06,  ..., -9.9093e-06,
         -8.2999e-06, -6.7800e-06]], device='cuda:0')
Loss: 0.9748350977897644


Running epoch 1, step 1243, batch 195
Sampled inputs[:2]: tensor([[   0, 1415,  300,  ..., 1497, 5715, 4555],
        [   0, 5603, 6598,  ..., 1692, 1713,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7133e-04,  1.1758e-04,  5.8780e-05,  ...,  2.9275e-06,
          1.9063e-04, -1.6147e-06],
        [-6.5714e-06, -4.4927e-06,  4.9770e-06,  ..., -5.2750e-06,
         -4.1313e-06, -4.0941e-06],
        [-1.6466e-05, -1.1712e-05,  1.3173e-05,  ..., -1.3128e-05,
         -1.0327e-05, -1.0192e-05],
        [-1.1742e-05, -7.9200e-06,  9.2685e-06,  ..., -9.4175e-06,
         -7.4431e-06, -7.6890e-06],
        [-1.5557e-05, -1.1697e-05,  1.2323e-05,  ..., -1.2830e-05,
         -1.0535e-05, -8.8513e-06]], device='cuda:0')
Loss: 0.9621604681015015


Running epoch 1, step 1244, batch 196
Sampled inputs[:2]: tensor([[    0,  2733,   278,  ..., 10936,    14,  6593],
        [    0, 15152,  1106,  ...,   607,   266,  2529]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9265e-04, -4.9836e-05, -7.5408e-05,  ..., -2.1423e-05,
          1.6674e-04, -5.1710e-05],
        [-8.1509e-06, -5.4985e-06,  6.2659e-06,  ..., -6.5193e-06,
         -5.0217e-06, -5.0478e-06],
        [-2.0459e-05, -1.4335e-05,  1.6600e-05,  ..., -1.6257e-05,
         -1.2562e-05, -1.2606e-05],
        [-1.4737e-05, -9.7677e-06,  1.1787e-05,  ..., -1.1742e-05,
         -9.1046e-06, -9.5814e-06],
        [-1.9267e-05, -1.4275e-05,  1.5467e-05,  ..., -1.5825e-05,
         -1.2785e-05, -1.0878e-05]], device='cuda:0')
Loss: 0.9891761541366577


Running epoch 1, step 1245, batch 197
Sampled inputs[:2]: tensor([[    0,    17,  3737,  ...,   298,   396,   221],
        [    0,  2297,   287,  ..., 10826, 13886,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2238e-06, -4.9136e-05,  6.6345e-06,  ..., -6.7514e-05,
          2.5465e-04,  2.7681e-05],
        [-9.8050e-06, -6.5416e-06,  7.3910e-06,  ..., -7.8827e-06,
         -6.0648e-06, -6.1281e-06],
        [-2.4185e-05, -1.6838e-05,  1.9372e-05,  ..., -1.9282e-05,
         -1.4916e-05, -1.4991e-05],
        [-1.7673e-05, -1.1586e-05,  1.3873e-05,  ..., -1.4171e-05,
         -1.1005e-05, -1.1608e-05],
        [-2.3142e-05, -1.6958e-05,  1.8314e-05,  ..., -1.9059e-05,
         -1.5378e-05, -1.3173e-05]], device='cuda:0')
Loss: 0.9471473693847656


Running epoch 1, step 1246, batch 198
Sampled inputs[:2]: tensor([[   0,  565, 1360,  ...,  278, 2722, 1683],
        [   0,   14,  221,  ...,  298,  408, 1849]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6879e-05, -4.9608e-05,  8.6174e-06,  ...,  1.3438e-05,
          6.4136e-04,  1.0386e-04],
        [-1.1511e-05, -7.6219e-06,  8.5160e-06,  ..., -9.3505e-06,
         -7.2196e-06, -7.3202e-06],
        [-2.8297e-05, -1.9550e-05,  2.2218e-05,  ..., -2.2739e-05,
         -1.7658e-05, -1.7852e-05],
        [-2.0668e-05, -1.3448e-05,  1.5885e-05,  ..., -1.6734e-05,
         -1.3031e-05, -1.3754e-05],
        [-2.7403e-05, -1.9863e-05,  2.1204e-05,  ..., -2.2739e-05,
         -1.8388e-05, -1.5944e-05]], device='cuda:0')
Loss: 0.9849061965942383


Running epoch 1, step 1247, batch 199
Sampled inputs[:2]: tensor([[   0, 3544,  417,  ...,  380,  381, 3794],
        [   0,   12,  287,  ...,  266, 2105, 3925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5810e-06, -1.1189e-04, -1.6141e-04,  ...,  1.1957e-04,
          5.8899e-04,  1.0825e-04],
        [-1.3024e-05, -8.6203e-06,  9.7454e-06,  ..., -1.0595e-05,
         -8.1956e-06, -8.2143e-06],
        [-3.2037e-05, -2.2098e-05,  2.5436e-05,  ..., -2.5734e-05,
         -2.0012e-05, -2.0012e-05],
        [-2.3529e-05, -1.5259e-05,  1.8328e-05,  ..., -1.9044e-05,
         -1.4871e-05, -1.5542e-05],
        [-3.0965e-05, -2.2426e-05,  2.4214e-05,  ..., -2.5675e-05,
         -2.0787e-05, -1.7792e-05]], device='cuda:0')
Loss: 0.9475887417793274
Graident accumulation at epoch 1, step 1247, batch 199
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0191],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.5275e-05,  3.9503e-05, -7.5611e-05,  ..., -2.8146e-05,
         -1.0460e-05, -1.9093e-04],
        [-1.2679e-05, -7.8776e-06,  8.0960e-06,  ..., -9.8153e-06,
         -7.3193e-06, -8.2186e-06],
        [ 1.1493e-05,  2.4695e-05, -1.3143e-05,  ...,  1.2865e-05,
          2.0868e-05,  5.4173e-08],
        [-1.0208e-05,  4.0418e-06,  4.2219e-06,  ..., -4.3097e-06,
         -4.2944e-07, -6.9172e-06],
        [-2.4557e-05, -1.6873e-05,  1.7250e-05,  ..., -1.8043e-05,
         -1.3777e-05, -1.4201e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8171e-08, 5.1460e-08, 5.0517e-08,  ..., 1.7788e-08, 1.1881e-07,
         4.0129e-08],
        [8.1480e-11, 5.0149e-11, 1.9562e-11,  ..., 5.6419e-11, 1.8742e-11,
         2.3974e-11],
        [1.7174e-09, 1.1389e-09, 3.8202e-10,  ..., 1.3201e-09, 5.5142e-10,
         4.0830e-10],
        [4.4953e-10, 3.9103e-10, 1.6309e-10,  ..., 4.1352e-10, 1.7867e-10,
         1.7134e-10],
        [3.9314e-10, 2.1581e-10, 6.8252e-11,  ..., 2.8598e-10, 7.1634e-11,
         1.1209e-10]], device='cuda:0')
optimizer state dict: 156.0
lr: [7.424747351382533e-06, 7.424747351382533e-06]
scheduler_last_epoch: 156


Running epoch 1, step 1248, batch 200
Sampled inputs[:2]: tensor([[    0,   299,   292,  ...,   266,  2474,   360],
        [    0,   437,   266,  ...,   266, 16084,  1781]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5737e-05, -3.2386e-05,  5.6155e-05,  ..., -8.3103e-05,
          9.3608e-05, -1.1537e-05],
        [-1.6689e-06, -1.1921e-06,  1.3262e-06,  ..., -1.3486e-06,
         -1.0505e-06, -1.0431e-06],
        [-4.1723e-06, -3.0994e-06,  3.4869e-06,  ..., -3.3826e-06,
         -2.6524e-06, -2.6524e-06],
        [-3.0398e-06, -2.1607e-06,  2.5034e-06,  ..., -2.4438e-06,
         -1.9222e-06, -1.9968e-06],
        [-3.9339e-06, -3.0696e-06,  3.2336e-06,  ..., -3.2783e-06,
         -2.6673e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9999642372131348


Running epoch 1, step 1249, batch 201
Sampled inputs[:2]: tensor([[    0,  5379,  6922,  ...,  1115, 43884,  2843],
        [    0,   352,   266,  ...,  2416,   287,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2886e-04, -8.5255e-05,  1.1992e-05,  ..., -1.1501e-04,
          1.7643e-05,  4.1762e-05],
        [-3.2783e-06, -2.2501e-06,  2.6152e-06,  ..., -2.6226e-06,
         -2.0266e-06, -1.9856e-06],
        [-8.1360e-06, -5.7817e-06,  6.8396e-06,  ..., -6.4820e-06,
         -5.0217e-06, -4.9770e-06],
        [-6.0350e-06, -4.0829e-06,  4.9919e-06,  ..., -4.7833e-06,
         -3.7253e-06, -3.8445e-06],
        [-7.6294e-06, -5.7220e-06,  6.3032e-06,  ..., -6.2436e-06,
         -5.0515e-06, -4.2617e-06]], device='cuda:0')
Loss: 0.9893020987510681


Running epoch 1, step 1250, batch 202
Sampled inputs[:2]: tensor([[    0,    12,  3454,  ...,   717,  1765, 14906],
        [    0,  1624,  7437,  ...,    12, 16369,  5153]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0731e-04, -1.8921e-05, -6.7368e-05,  ..., -1.1044e-04,
         -9.0713e-05, -3.3130e-04],
        [-4.7982e-06, -3.2857e-06,  3.8520e-06,  ..., -3.8892e-06,
         -3.0175e-06, -2.9393e-06],
        [-1.1802e-05, -8.3745e-06,  1.0043e-05,  ..., -9.4920e-06,
         -7.4059e-06, -7.1973e-06],
        [-8.8215e-06, -5.9232e-06,  7.3761e-06,  ..., -7.0930e-06,
         -5.5656e-06, -5.6922e-06],
        [-1.1146e-05, -8.3447e-06,  9.3281e-06,  ..., -9.1940e-06,
         -7.4804e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9424043297767639


Running epoch 1, step 1251, batch 203
Sampled inputs[:2]: tensor([[   0,  772,  699,  ..., 1849,  287, 7134],
        [   0, 1875, 2117,  ..., 1422, 1059,  963]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4754e-04,  4.9502e-05, -1.6210e-04,  ..., -2.4302e-04,
         -2.9702e-04, -5.1365e-04],
        [-6.3404e-06, -4.2468e-06,  5.1185e-06,  ..., -5.1409e-06,
         -4.0084e-06, -3.9078e-06],
        [-1.5646e-05, -1.0893e-05,  1.3381e-05,  ..., -1.2547e-05,
         -9.8497e-06, -9.5367e-06],
        [-1.1757e-05, -7.6964e-06,  9.9093e-06,  ..., -9.4473e-06,
         -7.4506e-06, -7.6145e-06],
        [-1.4782e-05, -1.0848e-05,  1.2442e-05,  ..., -1.2159e-05,
         -9.9391e-06, -8.1807e-06]], device='cuda:0')
Loss: 0.9649984240531921


Running epoch 1, step 1252, batch 204
Sampled inputs[:2]: tensor([[    0, 16371,    12,  ...,  1296,   680,  1098],
        [    0,  1530,    17,  ...,   409,  1611,   895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1711e-04, -8.7653e-05, -2.4079e-04,  ..., -2.6918e-04,
         -2.2551e-04, -5.0611e-04],
        [-7.9721e-06, -5.3644e-06,  6.4075e-06,  ..., -6.4448e-06,
         -5.0813e-06, -4.9137e-06],
        [-1.9580e-05, -1.3679e-05,  1.6689e-05,  ..., -1.5661e-05,
         -1.2428e-05, -1.1936e-05],
        [-1.4722e-05, -9.7081e-06,  1.2353e-05,  ..., -1.1802e-05,
         -9.4175e-06, -9.5218e-06],
        [-1.8597e-05, -1.3679e-05,  1.5616e-05,  ..., -1.5274e-05,
         -1.2606e-05, -1.0312e-05]], device='cuda:0')
Loss: 0.9748885035514832


Running epoch 1, step 1253, batch 205
Sampled inputs[:2]: tensor([[   0,  221,  474,  ...,   12,  259, 1220],
        [   0, 2027,  365,  ...,  368, 1782,  394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0751e-05, -3.0284e-06, -1.0790e-04,  ..., -1.0439e-04,
         -3.9055e-04, -5.8330e-04],
        [-9.7603e-06, -6.4522e-06,  7.4655e-06,  ..., -7.8902e-06,
         -6.2212e-06, -6.2101e-06],
        [-2.4021e-05, -1.6585e-05,  1.9491e-05,  ..., -1.9282e-05,
         -1.5378e-05, -1.5154e-05],
        [-1.7792e-05, -1.1563e-05,  1.4260e-05,  ..., -1.4305e-05,
         -1.1414e-05, -1.1802e-05],
        [-2.3127e-05, -1.6749e-05,  1.8388e-05,  ..., -1.9059e-05,
         -1.5795e-05, -1.3411e-05]], device='cuda:0')
Loss: 0.9395010471343994


Running epoch 1, step 1254, batch 206
Sampled inputs[:2]: tensor([[    0, 26473,  2117,  ...,    13,  3292,   950],
        [    0,  5281,  4452,  ...,    14,  3391,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0986e-05, -3.8994e-05, -1.4268e-04,  ..., -1.4897e-04,
         -5.1656e-04, -5.8735e-04],
        [-1.1377e-05, -7.5251e-06,  8.6725e-06,  ..., -9.1791e-06,
         -7.2643e-06, -7.2680e-06],
        [-2.7984e-05, -1.9327e-05,  2.2694e-05,  ..., -2.2411e-05,
         -1.7926e-05, -1.7688e-05],
        [-2.0683e-05, -1.3448e-05,  1.6525e-05,  ..., -1.6615e-05,
         -1.3299e-05, -1.3784e-05],
        [-2.7001e-05, -1.9565e-05,  2.1473e-05,  ..., -2.2188e-05,
         -1.8448e-05, -1.5676e-05]], device='cuda:0')
Loss: 0.940261721611023


Running epoch 1, step 1255, batch 207
Sampled inputs[:2]: tensor([[    0,   221,   527,  ...,   417,   199, 30714],
        [    0,   221,   380,  ..., 10022,    12,   461]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6444e-05,  4.5851e-05, -2.8178e-04,  ..., -7.4827e-05,
         -3.8117e-04, -6.7080e-04],
        [-1.3053e-05, -8.5011e-06,  1.0014e-05,  ..., -1.0528e-05,
         -8.2478e-06, -8.3707e-06],
        [-3.2067e-05, -2.1800e-05,  2.6137e-05,  ..., -2.5660e-05,
         -2.0310e-05, -2.0325e-05],
        [-2.3767e-05, -1.5207e-05,  1.9073e-05,  ..., -1.9088e-05,
         -1.5110e-05, -1.5900e-05],
        [-3.0786e-05, -2.1994e-05,  2.4617e-05,  ..., -2.5287e-05,
         -2.0817e-05, -1.7911e-05]], device='cuda:0')
Loss: 0.9530808925628662
Graident accumulation at epoch 1, step 1255, batch 207
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.9392e-05,  4.0138e-05, -9.6228e-05,  ..., -3.2814e-05,
         -4.7531e-05, -2.3891e-04],
        [-1.2717e-05, -7.9400e-06,  8.2878e-06,  ..., -9.8865e-06,
         -7.4121e-06, -8.2338e-06],
        [ 7.1374e-06,  2.0046e-05, -9.2154e-06,  ...,  9.0129e-06,
          1.6750e-05, -1.9838e-06],
        [-1.1564e-05,  2.1170e-06,  5.7071e-06,  ..., -5.7876e-06,
         -1.8975e-06, -7.8154e-06],
        [-2.5180e-05, -1.7385e-05,  1.7987e-05,  ..., -1.8767e-05,
         -1.4481e-05, -1.4572e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8122e-08, 5.1410e-08, 5.0546e-08,  ..., 1.7775e-08, 1.1884e-07,
         4.0539e-08],
        [8.1569e-11, 5.0171e-11, 1.9642e-11,  ..., 5.6474e-11, 1.8791e-11,
         2.4020e-11],
        [1.7167e-09, 1.1382e-09, 3.8232e-10,  ..., 1.3194e-09, 5.5129e-10,
         4.0831e-10],
        [4.4965e-10, 3.9087e-10, 1.6329e-10,  ..., 4.1347e-10, 1.7872e-10,
         1.7142e-10],
        [3.9369e-10, 2.1607e-10, 6.8790e-11,  ..., 2.8634e-10, 7.1996e-11,
         1.1230e-10]], device='cuda:0')
optimizer state dict: 157.0
lr: [7.305499933960942e-06, 7.305499933960942e-06]
scheduler_last_epoch: 157


Running epoch 1, step 1256, batch 208
Sampled inputs[:2]: tensor([[    0,   292, 41192,  ..., 34298,  8741,   271],
        [    0,    13, 26011,  ...,   342,  3873,   720]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5521e-06, -5.5171e-05, -5.4571e-05,  ..., -1.8110e-05,
          1.7714e-04,  1.0267e-04],
        [-1.7211e-06, -1.1623e-06,  1.1474e-06,  ..., -1.4231e-06,
         -1.1846e-06, -1.1325e-06],
        [-4.2617e-06, -2.9951e-06,  3.0249e-06,  ..., -3.4869e-06,
         -2.9057e-06, -2.8014e-06],
        [-3.0249e-06, -1.9968e-06,  2.0713e-06,  ..., -2.4885e-06,
         -2.0862e-06, -2.0862e-06],
        [-4.2617e-06, -3.1292e-06,  2.9951e-06,  ..., -3.5912e-06,
         -3.0845e-06, -2.5928e-06]], device='cuda:0')
Loss: 0.9858416318893433


Running epoch 1, step 1257, batch 209
Sampled inputs[:2]: tensor([[    0,   935,   508,  ...,   287, 41582,    12],
        [    0,  1136,   944,  ...,   401, 13771,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7646e-04, -1.9057e-04, -6.5853e-05,  ..., -1.7291e-05,
          3.5354e-04,  2.9254e-04],
        [-3.2783e-06, -2.1607e-06,  2.2054e-06,  ..., -2.7269e-06,
         -2.2650e-06, -2.2501e-06],
        [-7.8827e-06, -5.3644e-06,  5.7518e-06,  ..., -6.4075e-06,
         -5.3495e-06, -5.1856e-06],
        [-5.7966e-06, -3.7104e-06,  4.0680e-06,  ..., -4.8131e-06,
         -4.0531e-06, -4.1425e-06],
        [-7.9274e-06, -5.6326e-06,  5.7369e-06,  ..., -6.6012e-06,
         -5.6773e-06, -4.7684e-06]], device='cuda:0')
Loss: 0.9267700910568237


Running epoch 1, step 1258, batch 210
Sampled inputs[:2]: tensor([[   0, 1978,  352,  ..., 2276,   12,  221],
        [   0,  292,  263,  ...,  342, 4575,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7915e-04, -1.2979e-04, -3.5647e-05,  ..., -8.6016e-05,
          5.0388e-04,  2.8286e-04],
        [-4.8578e-06, -3.1963e-06,  3.4720e-06,  ..., -4.0233e-06,
         -3.3379e-06, -3.1963e-06],
        [-1.1683e-05, -7.9870e-06,  9.0003e-06,  ..., -9.5069e-06,
         -7.9274e-06, -7.4059e-06],
        [-8.7321e-06, -5.5954e-06,  6.5416e-06,  ..., -7.2271e-06,
         -6.1095e-06, -6.0126e-06],
        [-1.1593e-05, -8.3148e-06,  8.8364e-06,  ..., -9.6560e-06,
         -8.2999e-06, -6.7055e-06]], device='cuda:0')
Loss: 0.9947016835212708


Running epoch 1, step 1259, batch 211
Sampled inputs[:2]: tensor([[   0, 3468,  278,  ..., 2442,  292,  380],
        [   0,   13, 1529,  ..., 8197, 2700, 9629]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9576e-04, -1.7333e-04, -2.0109e-04,  ..., -8.7245e-05,
          5.3967e-04,  4.2661e-04],
        [-6.5118e-06, -4.3362e-06,  4.7609e-06,  ..., -5.3495e-06,
         -4.3884e-06, -4.1388e-06],
        [-1.5736e-05, -1.0893e-05,  1.2308e-05,  ..., -1.2740e-05,
         -1.0490e-05, -9.7156e-06],
        [-1.1742e-05, -7.6517e-06,  8.9705e-06,  ..., -9.6411e-06,
         -8.0466e-06, -7.8306e-06],
        [-1.5378e-05, -1.1176e-05,  1.1906e-05,  ..., -1.2770e-05,
         -1.0863e-05, -8.7023e-06]], device='cuda:0')
Loss: 0.9949156641960144


Running epoch 1, step 1260, batch 212
Sampled inputs[:2]: tensor([[   0, 4137,  300,  ..., 2579,  278,  266],
        [   0,  300, 2607,  ..., 1279,  368,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7563e-04, -2.2067e-04, -1.5775e-04,  ..., -6.7852e-05,
          6.6570e-04,  4.3781e-04],
        [-8.1286e-06, -5.4240e-06,  5.9754e-06,  ..., -6.6906e-06,
         -5.4389e-06, -5.2042e-06],
        [-1.9729e-05, -1.3649e-05,  1.5512e-05,  ..., -1.6004e-05,
         -1.3068e-05, -1.2293e-05],
        [-1.4722e-05, -9.6187e-06,  1.1310e-05,  ..., -1.2100e-05,
         -9.9838e-06, -9.8720e-06],
        [-1.9059e-05, -1.3828e-05,  1.4827e-05,  ..., -1.5840e-05,
         -1.3381e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.9744639992713928


Running epoch 1, step 1261, batch 213
Sampled inputs[:2]: tensor([[   0, 2344,  271,  ..., 5415,   14, 1075],
        [   0,  462,  221,  ...,   29,  413, 1801]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5186e-04, -6.9589e-05, -1.9328e-04,  ...,  1.9434e-04,
          2.1993e-04,  2.7252e-04],
        [-9.7454e-06, -6.3591e-06,  7.2494e-06,  ..., -7.9721e-06,
         -6.4820e-06, -6.2250e-06],
        [-2.3633e-05, -1.6019e-05,  1.8790e-05,  ..., -1.9044e-05,
         -1.5572e-05, -1.4678e-05],
        [-1.7717e-05, -1.1303e-05,  1.3798e-05,  ..., -1.4469e-05,
         -1.1951e-05, -1.1854e-05],
        [-2.2784e-05, -1.6242e-05,  1.7926e-05,  ..., -1.8835e-05,
         -1.5929e-05, -1.2934e-05]], device='cuda:0')
Loss: 0.9406254887580872


Running epoch 1, step 1262, batch 214
Sampled inputs[:2]: tensor([[    0,  1295,   508,  ...,   829,   772,   278],
        [    0,   269,    12,  ..., 45645,    14,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7185e-04, -3.3404e-05, -1.9328e-04,  ...,  3.3813e-04,
          2.2031e-04,  4.6838e-04],
        [-1.1526e-05, -7.5214e-06,  8.4490e-06,  ..., -9.4399e-06,
         -7.6741e-06, -7.4320e-06],
        [-2.7955e-05, -1.8954e-05,  2.1935e-05,  ..., -2.2575e-05,
         -1.8477e-05, -1.7539e-05],
        [-2.0728e-05, -1.3255e-05,  1.5944e-05,  ..., -1.6958e-05,
         -1.3992e-05, -1.3925e-05],
        [-2.7105e-05, -1.9282e-05,  2.1011e-05,  ..., -2.2456e-05,
         -1.9014e-05, -1.5646e-05]], device='cuda:0')
Loss: 0.9672787189483643


Running epoch 1, step 1263, batch 215
Sampled inputs[:2]: tensor([[    0, 26074,   486,  ...,  2314,   266,  1090],
        [    0,   471,  6210,  ...,  4274,   344, 11451]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0780e-04, -2.0059e-04, -2.3568e-04,  ...,  2.2626e-04,
          1.7152e-04,  4.6660e-04],
        [-1.3255e-05, -8.5719e-06,  9.7528e-06,  ..., -1.0774e-05,
         -8.7619e-06, -8.5421e-06],
        [-3.2306e-05, -2.1726e-05,  2.5392e-05,  ..., -2.5928e-05,
         -2.1234e-05, -2.0340e-05],
        [-2.3887e-05, -1.5140e-05,  1.8418e-05,  ..., -1.9386e-05,
         -1.6019e-05, -1.6056e-05],
        [-3.1188e-05, -2.2024e-05,  2.4199e-05,  ..., -2.5705e-05,
         -2.1771e-05, -1.8060e-05]], device='cuda:0')
Loss: 0.9698516726493835
Graident accumulation at epoch 1, step 1263, batch 215
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0234, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0156,  0.0156, -0.0287,  ...,  0.0291, -0.0144, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.2673e-05,  1.6066e-05, -1.1017e-04,  ..., -6.9069e-06,
         -2.5625e-05, -1.6836e-04],
        [-1.2770e-05, -8.0032e-06,  8.4343e-06,  ..., -9.9752e-06,
         -7.5471e-06, -8.2646e-06],
        [ 3.1931e-06,  1.5868e-05, -5.7547e-06,  ...,  5.5188e-06,
          1.2952e-05, -3.8194e-06],
        [-1.2796e-05,  3.9133e-07,  6.9782e-06,  ..., -7.1475e-06,
         -3.3096e-06, -8.6395e-06],
        [-2.5781e-05, -1.7849e-05,  1.8608e-05,  ..., -1.9461e-05,
         -1.5210e-05, -1.4921e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8159e-08, 5.1399e-08, 5.0551e-08,  ..., 1.7809e-08, 1.1875e-07,
         4.0716e-08],
        [8.1663e-11, 5.0194e-11, 1.9718e-11,  ..., 5.6533e-11, 1.8849e-11,
         2.4069e-11],
        [1.7160e-09, 1.1376e-09, 3.8259e-10,  ..., 1.3188e-09, 5.5119e-10,
         4.0831e-10],
        [4.4977e-10, 3.9071e-10, 1.6347e-10,  ..., 4.1344e-10, 1.7880e-10,
         1.7151e-10],
        [3.9427e-10, 2.1634e-10, 6.9307e-11,  ..., 2.8671e-10, 7.2398e-11,
         1.1252e-10]], device='cuda:0')
optimizer state dict: 158.0
lr: [7.186664259670068e-06, 7.186664259670068e-06]
scheduler_last_epoch: 158


Running epoch 1, step 1264, batch 216
Sampled inputs[:2]: tensor([[   0,   27, 3961,  ...,  462,  221,  474],
        [   0,  298,  696,  ..., 3502,  287, 1047]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4232e-05, -9.4470e-05,  8.2293e-05,  ..., -3.2145e-05,
          5.8133e-05,  2.7513e-04],
        [-1.7658e-06, -1.0505e-06,  1.1399e-06,  ..., -1.4082e-06,
         -1.1325e-06, -1.2666e-06],
        [-4.3511e-06, -2.7120e-06,  2.9653e-06,  ..., -3.4422e-06,
         -2.7865e-06, -3.1292e-06],
        [-3.0845e-06, -1.8030e-06,  2.0564e-06,  ..., -2.4438e-06,
         -1.9819e-06, -2.2799e-06],
        [-4.2915e-06, -2.8014e-06,  2.8759e-06,  ..., -3.5018e-06,
         -2.8908e-06, -2.9057e-06]], device='cuda:0')
Loss: 0.9894334673881531


Running epoch 1, step 1265, batch 217
Sampled inputs[:2]: tensor([[    0,   266,  6079,  ...,   437,   266, 44526],
        [    0,   287,  2997,  ...,   437,   266,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2694e-05, -4.7102e-05,  5.6894e-05,  ..., -2.0001e-05,
          3.1397e-05,  3.5899e-04],
        [-3.4347e-06, -2.2799e-06,  2.3022e-06,  ..., -2.8461e-06,
         -2.3097e-06, -2.3693e-06],
        [-8.4639e-06, -5.8413e-06,  5.9903e-06,  ..., -6.9737e-06,
         -5.6922e-06, -5.8562e-06],
        [-6.0350e-06, -3.9339e-06,  4.1574e-06,  ..., -4.9770e-06,
         -4.0680e-06, -4.3213e-06],
        [-8.3148e-06, -5.9605e-06,  5.7966e-06,  ..., -7.0184e-06,
         -5.8562e-06, -5.3793e-06]], device='cuda:0')
Loss: 0.9987911581993103


Running epoch 1, step 1266, batch 218
Sampled inputs[:2]: tensor([[   0,   89, 2023,  ..., 3230,  328,  790],
        [   0,   12, 1631,  ..., 1143,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2150e-04,  1.2252e-05,  1.7135e-04,  ..., -4.6020e-05,
          1.5566e-05,  1.9145e-04],
        [-5.0962e-06, -3.3453e-06,  3.3304e-06,  ..., -4.2170e-06,
         -3.4347e-06, -3.5912e-06],
        [-1.2636e-05, -8.5533e-06,  8.7470e-06,  ..., -1.0327e-05,
         -8.4043e-06, -8.8364e-06],
        [-8.9705e-06, -5.7369e-06,  6.0350e-06,  ..., -7.3910e-06,
         -6.0350e-06, -6.5416e-06],
        [-1.2457e-05, -8.7619e-06,  8.4788e-06,  ..., -1.0431e-05,
         -8.7172e-06, -8.1211e-06]], device='cuda:0')
Loss: 0.9521169662475586


Running epoch 1, step 1267, batch 219
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ..., 27714,   278,   266],
        [    0,  2720,    14,  ...,   300, 15867,   368]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4674e-04, -1.3633e-05,  6.1713e-05,  ..., -7.1129e-05,
         -1.0238e-04, -3.0176e-05],
        [-6.6757e-06, -4.3660e-06,  4.5225e-06,  ..., -5.5134e-06,
         -4.4629e-06, -4.5598e-06],
        [-1.6570e-05, -1.1176e-05,  1.1921e-05,  ..., -1.3500e-05,
         -1.0937e-05, -1.1221e-05],
        [-1.1817e-05, -7.5251e-06,  8.2999e-06,  ..., -9.7156e-06,
         -7.8976e-06, -8.3894e-06],
        [-1.6183e-05, -1.1399e-05,  1.1444e-05,  ..., -1.3545e-05,
         -1.1295e-05, -1.0177e-05]], device='cuda:0')
Loss: 0.9525999426841736


Running epoch 1, step 1268, batch 220
Sampled inputs[:2]: tensor([[   0,  334,  287,  ..., 1348, 6139,  342],
        [   0, 4385,  342,  ..., 3644,  775,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2869e-04,  8.2122e-05, -2.7564e-05,  ..., -1.1800e-04,
         -1.3602e-04, -1.2515e-04],
        [-8.2999e-06, -5.4538e-06,  5.8264e-06,  ..., -6.8098e-06,
         -5.4911e-06, -5.4985e-06],
        [-2.0534e-05, -1.3903e-05,  1.5244e-05,  ..., -1.6645e-05,
         -1.3426e-05, -1.3515e-05],
        [-1.4842e-05, -9.4920e-06,  1.0818e-05,  ..., -1.2100e-05,
         -9.8050e-06, -1.0237e-05],
        [-1.9923e-05, -1.4111e-05,  1.4544e-05,  ..., -1.6600e-05,
         -1.3798e-05, -1.2174e-05]], device='cuda:0')
Loss: 0.9817505478858948


Running epoch 1, step 1269, batch 221
Sampled inputs[:2]: tensor([[    0,   298,   452,  ..., 41263,     9,   367],
        [    0,  8822,  1486,  ...,    12,   287,  6903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5991e-04,  1.3605e-04, -3.1953e-05,  ..., -1.0598e-04,
         -1.5192e-04, -1.1883e-04],
        [-1.0163e-05, -6.6981e-06,  6.8918e-06,  ..., -8.3372e-06,
         -6.7875e-06, -6.8024e-06],
        [-2.5332e-05, -1.7256e-05,  1.8150e-05,  ..., -2.0608e-05,
         -1.6838e-05, -1.6868e-05],
        [-1.7941e-05, -1.1548e-05,  1.2629e-05,  ..., -1.4648e-05,
         -1.2010e-05, -1.2457e-05],
        [-2.4751e-05, -1.7583e-05,  1.7419e-05,  ..., -2.0653e-05,
         -1.7405e-05, -1.5378e-05]], device='cuda:0')
Loss: 0.9624987840652466


Running epoch 1, step 1270, batch 222
Sampled inputs[:2]: tensor([[   0, 1760,    9,  ..., 5996,   71,   19],
        [   0,  257,  221,  ..., 1474, 2044,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8260e-04,  1.6331e-04, -1.8043e-04,  ..., -8.3002e-05,
         -1.7546e-04, -1.2233e-04],
        [-1.1943e-05, -7.6964e-06,  8.0466e-06,  ..., -9.8050e-06,
         -7.8827e-06, -8.0913e-06],
        [-2.9653e-05, -1.9819e-05,  2.1160e-05,  ..., -2.4095e-05,
         -1.9491e-05, -1.9923e-05],
        [-2.1085e-05, -1.3307e-05,  1.4760e-05,  ..., -1.7211e-05,
         -1.3962e-05, -1.4782e-05],
        [-2.9042e-05, -2.0191e-05,  2.0370e-05,  ..., -2.4199e-05,
         -2.0176e-05, -1.8269e-05]], device='cuda:0')
Loss: 0.9602718353271484


Running epoch 1, step 1271, batch 223
Sampled inputs[:2]: tensor([[   0, 4191,  368,  ...,  367, 4182,   14],
        [   0,  320,  472,  ..., 1345,   14, 1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0934e-04,  1.0280e-04, -2.3430e-04,  ..., -1.4188e-04,
          9.3436e-05, -1.5935e-04],
        [-1.3694e-05, -8.7097e-06,  9.2760e-06,  ..., -1.1176e-05,
         -8.9258e-06, -9.2164e-06],
        [-3.4034e-05, -2.2471e-05,  2.4378e-05,  ..., -2.7493e-05,
         -2.2098e-05, -2.2754e-05],
        [-2.4170e-05, -1.5050e-05,  1.6995e-05,  ..., -1.9610e-05,
         -1.5803e-05, -1.6853e-05],
        [-3.3185e-05, -2.2829e-05,  2.3380e-05,  ..., -2.7493e-05,
         -2.2784e-05, -2.0742e-05]], device='cuda:0')
Loss: 0.9625779390335083
Graident accumulation at epoch 1, step 1271, batch 223
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0291, -0.0143, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.1340e-05,  2.4739e-05, -1.2259e-04,  ..., -2.0405e-05,
         -1.3719e-05, -1.6746e-04],
        [-1.2863e-05, -8.0738e-06,  8.5184e-06,  ..., -1.0095e-05,
         -7.6850e-06, -8.3598e-06],
        [-5.2965e-07,  1.2034e-05, -2.7414e-06,  ...,  2.2176e-06,
          9.4467e-06, -5.7129e-06],
        [-1.3934e-05, -1.1528e-06,  7.9798e-06,  ..., -8.3937e-06,
         -4.5589e-06, -9.4608e-06],
        [-2.6521e-05, -1.8347e-05,  1.9085e-05,  ..., -2.0264e-05,
         -1.5967e-05, -1.5503e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8196e-08, 5.1358e-08, 5.0556e-08,  ..., 1.7811e-08, 1.1864e-07,
         4.0701e-08],
        [8.1769e-11, 5.0220e-11, 1.9784e-11,  ..., 5.6601e-11, 1.8910e-11,
         2.4130e-11],
        [1.7155e-09, 1.1369e-09, 3.8280e-10,  ..., 1.3182e-09, 5.5112e-10,
         4.0842e-10],
        [4.4990e-10, 3.9055e-10, 1.6360e-10,  ..., 4.1341e-10, 1.7887e-10,
         1.7162e-10],
        [3.9498e-10, 2.1665e-10, 6.9784e-11,  ..., 2.8718e-10, 7.2844e-11,
         1.1283e-10]], device='cuda:0')
optimizer state dict: 159.0
lr: [7.068258487638268e-06, 7.068258487638268e-06]
scheduler_last_epoch: 159


Running epoch 1, step 1272, batch 224
Sampled inputs[:2]: tensor([[    0,    27,   417,  ...,    18,   365,   806],
        [    0,   342,  8514,  ...,   266, 46850,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2084e-06, -9.7776e-05, -1.9862e-04,  ...,  5.1886e-05,
          1.2689e-04, -4.9901e-05],
        [-1.6540e-06, -1.2070e-06,  1.1325e-06,  ..., -1.3486e-06,
         -1.1623e-06, -1.0952e-06],
        [-4.2915e-06, -3.2485e-06,  3.0994e-06,  ..., -3.5018e-06,
         -3.0100e-06, -2.9057e-06],
        [-3.0398e-06, -2.1905e-06,  2.1458e-06,  ..., -2.4736e-06,
         -2.1458e-06, -2.1160e-06],
        [-4.1127e-06, -3.2634e-06,  2.9206e-06,  ..., -3.4422e-06,
         -3.0398e-06, -2.5779e-06]], device='cuda:0')
Loss: 0.9961499571800232


Running epoch 1, step 1273, batch 225
Sampled inputs[:2]: tensor([[    0,   688,  2353,  ..., 20538, 10393,    12],
        [    0,   278,  6318,  ...,   458,    17,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3006e-05, -1.6939e-04, -2.6597e-04,  ...,  4.6006e-05,
          2.7221e-04,  4.1240e-05],
        [-3.3453e-06, -2.3320e-06,  2.3544e-06,  ..., -2.6748e-06,
         -2.2575e-06, -2.1160e-06],
        [-8.3447e-06, -6.0946e-06,  6.1989e-06,  ..., -6.7055e-06,
         -5.6922e-06, -5.3793e-06],
        [-6.0499e-06, -4.1872e-06,  4.4107e-06,  ..., -4.8280e-06,
         -4.1425e-06, -4.0233e-06],
        [-8.0168e-06, -6.1691e-06,  5.8711e-06,  ..., -6.6310e-06,
         -5.8115e-06, -4.7982e-06]], device='cuda:0')
Loss: 0.9877234101295471


Running epoch 1, step 1274, batch 226
Sampled inputs[:2]: tensor([[   0,   13, 8982,  ...,  462,  221,  494],
        [   0, 1062,  648,  ...,  266, 4939,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7309e-05, -1.9992e-04, -3.6115e-04,  ...,  5.3656e-05,
          1.1429e-04, -6.4857e-05],
        [-4.8727e-06, -3.3304e-06,  3.5763e-06,  ..., -3.9265e-06,
         -3.2336e-06, -3.1218e-06],
        [-1.2070e-05, -8.5980e-06,  9.4026e-06,  ..., -9.7156e-06,
         -8.0764e-06, -7.7784e-06],
        [-8.8662e-06, -5.9828e-06,  6.7949e-06,  ..., -7.1228e-06,
         -5.9605e-06, -5.9605e-06],
        [-1.1444e-05, -8.6129e-06,  8.7768e-06,  ..., -9.4920e-06,
         -8.1658e-06, -6.8098e-06]], device='cuda:0')
Loss: 0.933344304561615


Running epoch 1, step 1275, batch 227
Sampled inputs[:2]: tensor([[   0,  894,   16,  ...,  892,  300,  722],
        [   0,  271,  266,  ..., 8122, 1387,  616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0959e-05, -2.3682e-04, -4.0469e-04,  ...,  8.5200e-05,
          3.0996e-04,  4.8943e-05],
        [-6.5118e-06, -4.3735e-06,  4.6864e-06,  ..., -5.2527e-06,
         -4.2915e-06, -4.2394e-06],
        [-1.6034e-05, -1.1221e-05,  1.2308e-05,  ..., -1.2904e-05,
         -1.0639e-05, -1.0446e-05],
        [-1.1876e-05, -7.8455e-06,  8.9258e-06,  ..., -9.5367e-06,
         -7.8976e-06, -8.0764e-06],
        [-1.5289e-05, -1.1235e-05,  1.1533e-05,  ..., -1.2651e-05,
         -1.0788e-05, -9.2089e-06]], device='cuda:0')
Loss: 0.9567758440971375


Running epoch 1, step 1276, batch 228
Sampled inputs[:2]: tensor([[    0,   491, 10524,  ...,  2218,  5627,  4199],
        [    0,  2241,  8274,  ...,   908,  1811,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9210e-05, -1.3766e-04, -3.3880e-04,  ...,  2.5684e-04,
          1.5069e-04,  1.5072e-04],
        [-8.2925e-06, -5.6252e-06,  5.9083e-06,  ..., -6.6385e-06,
         -5.5060e-06, -5.3346e-06],
        [-2.0444e-05, -1.4514e-05,  1.5527e-05,  ..., -1.6376e-05,
         -1.3709e-05, -1.3202e-05],
        [-1.5065e-05, -1.0096e-05,  1.1191e-05,  ..., -1.2025e-05,
         -1.0118e-05, -1.0133e-05],
        [-1.9401e-05, -1.4454e-05,  1.4499e-05,  ..., -1.6004e-05,
         -1.3828e-05, -1.1608e-05]], device='cuda:0')
Loss: 0.9910345077514648


Running epoch 1, step 1277, batch 229
Sampled inputs[:2]: tensor([[    0,    11,   360,  ...,  4524,  1553,   401],
        [    0,  1295,   898,  ...,   298, 38754,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.6378e-05, -3.7655e-05, -3.4826e-04,  ...,  3.8589e-04,
          1.8131e-05,  2.6578e-04],
        [-9.9838e-06, -6.7055e-06,  7.1004e-06,  ..., -8.0392e-06,
         -6.6012e-06, -6.4969e-06],
        [-2.4587e-05, -1.7211e-05,  1.8612e-05,  ..., -1.9714e-05,
         -1.6332e-05, -1.5974e-05],
        [-1.7971e-05, -1.1899e-05,  1.3292e-05,  ..., -1.4409e-05,
         -1.1988e-05, -1.2189e-05],
        [-2.3425e-05, -1.7151e-05,  1.7464e-05,  ..., -1.9327e-05,
         -1.6481e-05, -1.4096e-05]], device='cuda:0')
Loss: 0.9715607762336731


Running epoch 1, step 1278, batch 230
Sampled inputs[:2]: tensor([[    0,   292, 16983,  ...,   221,   474,  4800],
        [    0,   271,   957,  ...,  1597,  1276,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9628e-05,  4.0830e-05, -1.9429e-04,  ...,  3.8589e-04,
          1.8131e-05,  4.7781e-04],
        [-1.1586e-05, -7.7188e-06,  8.0280e-06,  ..., -9.4920e-06,
         -7.7710e-06, -7.7561e-06],
        [-2.8431e-05, -1.9744e-05,  2.1070e-05,  ..., -2.3112e-05,
         -1.9073e-05, -1.8895e-05],
        [-2.0802e-05, -1.3642e-05,  1.4983e-05,  ..., -1.6987e-05,
         -1.4104e-05, -1.4514e-05],
        [-2.7329e-05, -1.9819e-05,  1.9953e-05,  ..., -2.2829e-05,
         -1.9372e-05, -1.6838e-05]], device='cuda:0')
Loss: 0.9466329216957092


Running epoch 1, step 1279, batch 231
Sampled inputs[:2]: tensor([[    0,  5136,   446,  ...,  1173,   300,   266],
        [    0,  1171,   341,  ...,   278, 14713,    18]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.0019e-05,  5.3372e-05, -1.1907e-04,  ...,  3.5825e-04,
          6.3075e-05,  6.2681e-04],
        [-1.3299e-05, -8.8736e-06,  9.2275e-06,  ..., -1.0848e-05,
         -8.8736e-06, -8.8438e-06],
        [-3.2783e-05, -2.2799e-05,  2.4259e-05,  ..., -2.6569e-05,
         -2.1905e-05, -2.1711e-05],
        [-2.3887e-05, -1.5698e-05,  1.7218e-05,  ..., -1.9431e-05,
         -1.6116e-05, -1.6570e-05],
        [-3.1441e-05, -2.2843e-05,  2.2918e-05,  ..., -2.6211e-05,
         -2.2233e-05, -1.9342e-05]], device='cuda:0')
Loss: 0.9949859976768494
Graident accumulation at epoch 1, step 1279, batch 231
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0291, -0.0143, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.4208e-05,  2.7602e-05, -1.2223e-04,  ...,  1.7461e-05,
         -6.0396e-06, -8.8034e-05],
        [-1.2906e-05, -8.1538e-06,  8.5893e-06,  ..., -1.0171e-05,
         -7.8038e-06, -8.4082e-06],
        [-3.7549e-06,  8.5511e-06, -4.1332e-08,  ..., -6.6101e-07,
          6.3116e-06, -7.3127e-06],
        [-1.4929e-05, -2.6074e-06,  8.9037e-06,  ..., -9.4975e-06,
         -5.7146e-06, -1.0172e-05],
        [-2.7013e-05, -1.8796e-05,  1.9468e-05,  ..., -2.0859e-05,
         -1.6594e-05, -1.5887e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8144e-08, 5.1310e-08, 5.0519e-08,  ..., 1.7922e-08, 1.1852e-07,
         4.1053e-08],
        [8.1864e-11, 5.0249e-11, 1.9850e-11,  ..., 5.6663e-11, 1.8970e-11,
         2.4184e-11],
        [1.7148e-09, 1.1363e-09, 3.8300e-10,  ..., 1.3176e-09, 5.5105e-10,
         4.0849e-10],
        [4.5002e-10, 3.9040e-10, 1.6373e-10,  ..., 4.1337e-10, 1.7895e-10,
         1.7172e-10],
        [3.9557e-10, 2.1695e-10, 7.0240e-11,  ..., 2.8758e-10, 7.3266e-11,
         1.1310e-10]], device='cuda:0')
optimizer state dict: 160.0
lr: [6.950300711301095e-06, 6.950300711301095e-06]
scheduler_last_epoch: 160


Running epoch 1, step 1280, batch 232
Sampled inputs[:2]: tensor([[   0,  437, 1916,  ...,   13, 1303, 2708],
        [   0,  756,  401,  ..., 8385, 1004,  775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5085e-05, -9.5884e-06, -3.0227e-05,  ...,  2.9371e-05,
          1.4446e-04,  9.1740e-05],
        [-1.6913e-06, -1.0654e-06,  1.2442e-06,  ..., -1.2815e-06,
         -1.0431e-06, -1.0729e-06],
        [-4.3213e-06, -2.8163e-06,  3.3528e-06,  ..., -3.2485e-06,
         -2.6673e-06, -2.7865e-06],
        [-3.0845e-06, -1.8999e-06,  2.3544e-06,  ..., -2.3097e-06,
         -1.8924e-06, -2.0564e-06],
        [-3.9339e-06, -2.7120e-06,  2.9802e-06,  ..., -3.0696e-06,
         -2.5928e-06, -2.3395e-06]], device='cuda:0')
Loss: 0.9804637432098389


Running epoch 1, step 1281, batch 233
Sampled inputs[:2]: tensor([[    0, 21801, 13084,  ...,  1738,  2946,    12],
        [    0,   199,  5990,  ...,   278,   638,  5513]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4229e-04, -2.6639e-05,  1.0290e-06,  ..., -7.7863e-05,
          5.9008e-05,  1.7178e-04],
        [-3.4943e-06, -2.1085e-06,  2.1607e-06,  ..., -2.7791e-06,
         -2.2650e-06, -2.3767e-06],
        [-8.7023e-06, -5.5581e-06,  5.7518e-06,  ..., -6.9290e-06,
         -5.7220e-06, -6.0052e-06],
        [-6.3032e-06, -3.7402e-06,  4.0904e-06,  ..., -4.9919e-06,
         -4.1127e-06, -4.4405e-06],
        [-8.3447e-06, -5.5581e-06,  5.2303e-06,  ..., -6.8694e-06,
         -5.8264e-06, -5.4389e-06]], device='cuda:0')
Loss: 0.9523426294326782


Running epoch 1, step 1282, batch 234
Sampled inputs[:2]: tensor([[    0,  1706,  8554,  ...,  9742,   221, 14082],
        [    0,   292,    41,  ...,   271,  9536,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1959e-04,  2.5743e-04,  2.3639e-04,  ..., -5.8120e-05,
          2.0519e-04,  1.6395e-04],
        [-5.1335e-06, -3.2708e-06,  2.8983e-06,  ..., -4.2543e-06,
         -3.6657e-06, -3.6955e-06],
        [-1.2845e-05, -8.6129e-06,  7.8082e-06,  ..., -1.0639e-05,
         -9.2387e-06, -9.3281e-06],
        [-9.2238e-06, -5.7518e-06,  5.4315e-06,  ..., -7.6294e-06,
         -6.6310e-06, -6.8694e-06],
        [-1.2696e-05, -8.8066e-06,  7.3314e-06,  ..., -1.0803e-05,
         -9.6112e-06, -8.7321e-06]], device='cuda:0')
Loss: 0.9717563986778259


Running epoch 1, step 1283, batch 235
Sampled inputs[:2]: tensor([[    0,  1371, 10516,  ...,  2456,    13,  6469],
        [    0,   266,  1194,  ...,  2267,    15,  1224]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6653e-04,  3.4756e-04,  2.2404e-04,  ..., -7.6168e-05,
          2.1242e-04,  1.7144e-04],
        [-6.7130e-06, -4.3139e-06,  3.9116e-06,  ..., -5.5656e-06,
         -4.8354e-06, -4.8205e-06],
        [-1.6689e-05, -1.1191e-05,  1.0476e-05,  ..., -1.3769e-05,
         -1.2025e-05, -1.1981e-05],
        [-1.2055e-05, -7.5474e-06,  7.3314e-06,  ..., -9.9540e-06,
         -8.7321e-06, -8.9705e-06],
        [-1.6421e-05, -1.1444e-05,  9.8646e-06,  ..., -1.3933e-05,
         -1.2487e-05, -1.1101e-05]], device='cuda:0')
Loss: 0.9337498545646667


Running epoch 1, step 1284, batch 236
Sampled inputs[:2]: tensor([[    0,  4100,    12,  ...,    13,  4710,  1558],
        [    0,    14,  1062,  ..., 10417,    13, 30579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9094e-05,  4.0911e-04,  1.4712e-04,  ..., -1.5783e-04,
          3.4168e-04,  3.3159e-04],
        [-8.2180e-06, -5.3048e-06,  5.0738e-06,  ..., -6.7800e-06,
         -5.8413e-06, -5.7891e-06],
        [ 8.7713e-05,  4.7658e-05, -4.3611e-05,  ...,  5.4627e-05,
          4.5549e-05,  1.5914e-05],
        [-1.4812e-05, -9.2909e-06,  9.5665e-06,  ..., -1.2144e-05,
         -1.0565e-05, -1.0811e-05],
        [-1.9982e-05, -1.3962e-05,  1.2755e-05,  ..., -1.6779e-05,
         -1.4916e-05, -1.3068e-05]], device='cuda:0')
Loss: 0.9517323970794678


Running epoch 1, step 1285, batch 237
Sampled inputs[:2]: tensor([[   0,   35, 3815,  ...,  278, 7097, 4601],
        [   0,   24,   15,  ...,  221,  380,  417]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9950e-05,  6.2148e-04,  2.1412e-04,  ..., -1.2441e-04,
          2.6053e-04,  2.4962e-04],
        [-1.0073e-05, -6.3181e-06,  6.0797e-06,  ..., -8.2701e-06,
         -7.0781e-06, -7.1973e-06],
        [ 8.3243e-05,  4.4991e-05, -4.0974e-05,  ...,  5.1021e-05,
          4.2464e-05,  1.2517e-05],
        [-1.7822e-05, -1.0923e-05,  1.1243e-05,  ..., -1.4588e-05,
         -1.2651e-05, -1.3180e-05],
        [-2.4542e-05, -1.6734e-05,  1.5378e-05,  ..., -2.0534e-05,
         -1.8165e-05, -1.6347e-05]], device='cuda:0')
Loss: 0.9254269599914551


Running epoch 1, step 1286, batch 238
Sampled inputs[:2]: tensor([[    0,  4347,   638,  ...,  1345,   292, 15343],
        [    0,   266,  6737,  ...,  2409,    12,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2109e-04,  5.2321e-04,  2.6321e-04,  ..., -2.0693e-04,
          3.8404e-04,  2.7888e-04],
        [-1.1660e-05, -7.3835e-06,  7.2271e-06,  ..., -9.5889e-06,
         -8.1286e-06, -8.2329e-06],
        [ 7.9249e-05,  4.2279e-05, -3.7949e-05,  ...,  4.7772e-05,
          3.9886e-05,  9.9391e-06],
        [-2.0742e-05, -1.2800e-05,  1.3433e-05,  ..., -1.6972e-05,
         -1.4573e-05, -1.5177e-05],
        [-2.8387e-05, -1.9491e-05,  1.8254e-05,  ..., -2.3738e-05,
         -2.0802e-05, -1.8626e-05]], device='cuda:0')
Loss: 0.9867350459098816


Running epoch 1, step 1287, batch 239
Sampled inputs[:2]: tensor([[    0,    12,  2085,  ...,   287,   593,  4137],
        [    0,  4855, 15679,  ...,   278,   266,  1912]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1061e-04,  5.2321e-04,  3.2637e-04,  ..., -2.9106e-04,
          2.7283e-04,  3.8362e-04],
        [-1.3247e-05, -8.4415e-06,  8.4490e-06,  ..., -1.0870e-05,
         -9.1195e-06, -9.2685e-06],
        [ 7.5255e-05,  3.9507e-05, -3.4715e-05,  ...,  4.4554e-05,
          3.7383e-05,  7.3314e-06],
        [-2.3663e-05, -1.4707e-05,  1.5773e-05,  ..., -1.9327e-05,
         -1.6406e-05, -1.7174e-05],
        [-3.1903e-05, -2.2084e-05,  2.1055e-05,  ..., -2.6673e-05,
         -2.3156e-05, -2.0757e-05]], device='cuda:0')
Loss: 0.9769178628921509
Graident accumulation at epoch 1, step 1287, batch 239
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0021,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0291, -0.0143, -0.0173]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.9848e-05,  7.7163e-05, -7.7375e-05,  ..., -1.3392e-05,
          2.1847e-05, -4.0869e-05],
        [-1.2941e-05, -8.1826e-06,  8.5753e-06,  ..., -1.0241e-05,
         -7.9354e-06, -8.4942e-06],
        [ 4.1461e-06,  1.1647e-05, -3.5087e-06,  ...,  3.8604e-06,
          9.4187e-06, -5.8483e-06],
        [-1.5802e-05, -3.8174e-06,  9.5906e-06,  ..., -1.0480e-05,
         -6.7837e-06, -1.0872e-05],
        [-2.7502e-05, -1.9125e-05,  1.9627e-05,  ..., -2.1440e-05,
         -1.7250e-05, -1.6374e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8255e-08, 5.1532e-08, 5.0575e-08,  ..., 1.7989e-08, 1.1848e-07,
         4.1159e-08],
        [8.1958e-11, 5.0270e-11, 1.9901e-11,  ..., 5.6724e-11, 1.9034e-11,
         2.4245e-11],
        [1.7188e-09, 1.1367e-09, 3.8383e-10,  ..., 1.3183e-09, 5.5190e-10,
         4.0813e-10],
        [4.5013e-10, 3.9023e-10, 1.6381e-10,  ..., 4.1333e-10, 1.7904e-10,
         1.7185e-10],
        [3.9619e-10, 2.1722e-10, 7.0613e-11,  ..., 2.8800e-10, 7.3729e-11,
         1.1341e-10]], device='cuda:0')
optimizer state dict: 161.0
lr: [6.8328089556364305e-06, 6.8328089556364305e-06]
scheduler_last_epoch: 161


Running epoch 1, step 1288, batch 240
Sampled inputs[:2]: tensor([[    0,   367,  3675,  ...,    22,  3180,    14],
        [    0,    55,  2258,  ..., 32764,    75,   338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0536e-05, -7.3523e-05,  1.5519e-05,  ...,  1.2014e-04,
         -3.8911e-05,  1.4564e-04],
        [-1.6764e-06, -1.1250e-06,  1.2219e-06,  ..., -1.3113e-06,
         -1.0580e-06, -1.0952e-06],
        [-4.3809e-06, -3.0100e-06,  3.3677e-06,  ..., -3.4273e-06,
         -2.7716e-06, -2.8759e-06],
        [-3.0696e-06, -2.0415e-06,  2.3246e-06,  ..., -2.3991e-06,
         -1.9521e-06, -2.0862e-06],
        [-4.0531e-06, -2.8908e-06,  3.0547e-06,  ..., -3.2485e-06,
         -2.6971e-06, -2.4885e-06]], device='cuda:0')
Loss: 0.9960254430770874


Running epoch 1, step 1289, batch 241
Sampled inputs[:2]: tensor([[   0,  266, 2653,  ...,   29,   16,   14],
        [   0,   17,   14,  ...,  650, 1711,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9563e-05, -1.7301e-05,  1.7768e-04,  ...,  2.3425e-04,
         -1.0456e-05,  1.6706e-04],
        [-3.3006e-06, -2.1905e-06,  2.3097e-06,  ..., -2.6673e-06,
         -2.1011e-06, -2.2277e-06],
        [-8.5533e-06, -5.8115e-06,  6.3479e-06,  ..., -6.8545e-06,
         -5.4091e-06, -5.7518e-06],
        [-6.0499e-06, -3.9414e-06,  4.3809e-06,  ..., -4.8727e-06,
         -3.8594e-06, -4.2617e-06],
        [-7.9572e-06, -5.6028e-06,  5.7966e-06,  ..., -6.5118e-06,
         -5.2750e-06, -4.9621e-06]], device='cuda:0')
Loss: 0.9864646196365356


Running epoch 1, step 1290, batch 242
Sampled inputs[:2]: tensor([[    0, 10288,   300,  ...,  5365,    12,  3539],
        [    0,    14,   747,  ...,   259,  6027,  1889]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1176e-04,  1.3994e-04,  3.5988e-04,  ...,  4.4444e-04,
         -1.5521e-04,  1.3840e-04],
        [-4.9546e-06, -3.3230e-06,  3.5614e-06,  ..., -3.9414e-06,
         -3.0436e-06, -3.3081e-06],
        [-1.2904e-05, -8.8811e-06,  9.7603e-06,  ..., -1.0207e-05,
         -7.8976e-06, -8.6278e-06],
        [-9.1046e-06, -5.9977e-06,  6.7651e-06,  ..., -7.1973e-06,
         -5.5879e-06, -6.3479e-06],
        [-1.1712e-05, -8.4043e-06,  8.7172e-06,  ..., -9.5069e-06,
         -7.5996e-06, -7.2718e-06]], device='cuda:0')
Loss: 0.9663305282592773


Running epoch 1, step 1291, batch 243
Sampled inputs[:2]: tensor([[    0,   292, 29800,  ...,  4144,   278,  1243],
        [    0,   287, 49722,  ...,  7551,   278,  5711]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0000e-04,  2.2687e-04,  1.6518e-04,  ...,  5.1265e-04,
         -4.0239e-04, -9.7196e-05],
        [-6.6310e-06, -4.4033e-06,  4.7684e-06,  ..., -5.2601e-06,
         -4.1388e-06, -4.4182e-06],
        [-1.7017e-05, -1.1653e-05,  1.2949e-05,  ..., -1.3426e-05,
         -1.0595e-05, -1.1355e-05],
        [-1.2130e-05, -7.8976e-06,  9.0450e-06,  ..., -9.5665e-06,
         -7.5847e-06, -8.4490e-06],
        [-1.5646e-05, -1.1191e-05,  1.1697e-05,  ..., -1.2681e-05,
         -1.0341e-05, -9.6709e-06]], device='cuda:0')
Loss: 0.9664713740348816


Running epoch 1, step 1292, batch 244
Sampled inputs[:2]: tensor([[    0,  1241,  2098,  ...,  1862,   631,   369],
        [    0,    27,  5375,  ...,  5357, 14933, 10944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0249e-04,  1.3832e-04,  2.0538e-04,  ...,  5.1822e-04,
         -2.7879e-04,  1.8903e-05],
        [-8.2478e-06, -5.5879e-06,  5.9679e-06,  ..., -6.5863e-06,
         -5.1819e-06, -5.4836e-06],
        [-2.1249e-05, -1.4827e-05,  1.6227e-05,  ..., -1.6883e-05,
         -1.3307e-05, -1.4171e-05],
        [-1.5095e-05, -1.0014e-05,  1.1325e-05,  ..., -1.1966e-05,
         -9.4697e-06, -1.0490e-05],
        [-1.9580e-05, -1.4275e-05,  1.4693e-05,  ..., -1.6004e-05,
         -1.3039e-05, -1.2130e-05]], device='cuda:0')
Loss: 0.991547703742981


Running epoch 1, step 1293, batch 245
Sampled inputs[:2]: tensor([[    0,   381,  1795,  ...,    12,   344,   593],
        [    0, 32444,    41,  ...,    14,    18,    59]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2983e-04,  1.7190e-05,  3.6049e-04,  ...,  5.4941e-04,
         -2.0664e-04,  2.8693e-04],
        [-9.8869e-06, -6.8098e-06,  7.1079e-06,  ..., -7.9274e-06,
         -6.3144e-06, -6.5714e-06],
        [-2.5541e-05, -1.8165e-05,  1.9357e-05,  ..., -2.0415e-05,
         -1.6317e-05, -1.7047e-05],
        [-1.8120e-05, -1.2249e-05,  1.3500e-05,  ..., -1.4439e-05,
         -1.1601e-05, -1.2591e-05],
        [-2.3484e-05, -1.7434e-05,  1.7509e-05,  ..., -1.9297e-05,
         -1.5900e-05, -1.4573e-05]], device='cuda:0')
Loss: 1.0122578144073486


Running epoch 1, step 1294, batch 246
Sampled inputs[:2]: tensor([[    0,  6143,   642,  ...,   199, 14300,    41],
        [    0,   328,   957,  ...,   298,   275,  8570]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7337e-05,  3.3070e-05,  1.7672e-04,  ...,  5.2754e-04,
         -2.0663e-04,  4.8173e-05],
        [-1.1444e-05, -7.8678e-06,  8.2552e-06,  ..., -9.2089e-06,
         -7.3574e-06, -7.5996e-06],
        [-2.9445e-05, -2.0906e-05,  2.2426e-05,  ..., -2.3589e-05,
         -1.8910e-05, -1.9595e-05],
        [-2.1040e-05, -1.4171e-05,  1.5751e-05,  ..., -1.6823e-05,
         -1.3568e-05, -1.4603e-05],
        [-2.7165e-05, -2.0161e-05,  2.0355e-05,  ..., -2.2382e-05,
         -1.8507e-05, -1.6794e-05]], device='cuda:0')
Loss: 0.9963128566741943


Running epoch 1, step 1295, batch 247
Sampled inputs[:2]: tensor([[   0, 1145,   13,  ...,  721, 1119, 3495],
        [   0, 5353, 5234,  ..., 1458,   14, 7157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6728e-05,  1.5496e-04,  9.5702e-05,  ...,  4.1307e-04,
         -1.8381e-04,  4.9223e-05],
        [-1.3106e-05, -8.9258e-06,  9.4399e-06,  ..., -1.0505e-05,
         -8.3186e-06, -8.6874e-06],
        [-3.3736e-05, -2.3782e-05,  2.5675e-05,  ..., -2.6956e-05,
         -2.1458e-05, -2.2411e-05],
        [-2.4065e-05, -1.6056e-05,  1.7986e-05,  ..., -1.9163e-05,
         -1.5333e-05, -1.6659e-05],
        [-3.1009e-05, -2.2873e-05,  2.3216e-05,  ..., -2.5496e-05,
         -2.0936e-05, -1.9148e-05]], device='cuda:0')
Loss: 0.9777905941009521
Graident accumulation at epoch 1, step 1295, batch 247
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.8190e-05,  8.4942e-05, -6.0067e-05,  ...,  2.9254e-05,
          1.2818e-06, -3.1859e-05],
        [-1.2957e-05, -8.2569e-06,  8.6618e-06,  ..., -1.0267e-05,
         -7.9737e-06, -8.5136e-06],
        [ 3.5787e-07,  8.1038e-06, -5.9039e-07,  ...,  7.7878e-07,
          6.3311e-06, -7.5046e-06],
        [-1.6629e-05, -5.0412e-06,  1.0430e-05,  ..., -1.1349e-05,
         -7.6387e-06, -1.1451e-05],
        [-2.7853e-05, -1.9500e-05,  1.9986e-05,  ..., -2.1846e-05,
         -1.7619e-05, -1.6651e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8197e-08, 5.1505e-08, 5.0534e-08,  ..., 1.8141e-08, 1.1840e-07,
         4.1120e-08],
        [8.2048e-11, 5.0299e-11, 1.9970e-11,  ..., 5.6778e-11, 1.9084e-11,
         2.4297e-11],
        [1.7182e-09, 1.1362e-09, 3.8410e-10,  ..., 1.3177e-09, 5.5181e-10,
         4.0823e-10],
        [4.5026e-10, 3.9009e-10, 1.6397e-10,  ..., 4.1329e-10, 1.7910e-10,
         1.7195e-10],
        [3.9676e-10, 2.1753e-10, 7.1081e-11,  ..., 2.8837e-10, 7.4093e-11,
         1.1367e-10]], device='cuda:0')
optimizer state dict: 162.0
lr: [6.715801174410152e-06, 6.715801174410152e-06]
scheduler_last_epoch: 162


Running epoch 1, step 1296, batch 248
Sampled inputs[:2]: tensor([[    0,  1607,    12,  ...,   895,  1503,   369],
        [    0,   292, 23242,  ...,  6494,  3560,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7733e-04,  6.8246e-05,  6.6592e-05,  ..., -3.8158e-05,
          1.3065e-04,  1.7075e-04],
        [-1.6019e-06, -1.0431e-06,  1.1325e-06,  ..., -1.2442e-06,
         -9.8348e-07, -1.0356e-06],
        [-4.0531e-06, -2.8312e-06,  3.0845e-06,  ..., -3.1590e-06,
         -2.5183e-06, -2.6226e-06],
        [-2.9653e-06, -1.9073e-06,  2.2054e-06,  ..., -2.3097e-06,
         -1.8403e-06, -2.0266e-06],
        [-3.7104e-06, -2.7120e-06,  2.7418e-06,  ..., -2.9951e-06,
         -2.4736e-06, -2.2352e-06]], device='cuda:0')
Loss: 0.9735095500946045


Running epoch 1, step 1297, batch 249
Sampled inputs[:2]: tensor([[   0, 2328,  271,  ...,  706,   13, 8961],
        [   0,  266, 1790,  ...,  292,   78,  527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7436e-05, -1.5046e-04, -1.7939e-04,  ..., -8.5018e-05,
          3.8520e-04,  2.6795e-04],
        [-3.2410e-06, -2.1830e-06,  2.2948e-06,  ..., -2.5481e-06,
         -2.0564e-06, -2.1160e-06],
        [-8.3447e-06, -5.9307e-06,  6.3181e-06,  ..., -6.5565e-06,
         -5.3197e-06, -5.4836e-06],
        [-5.9456e-06, -3.9339e-06,  4.4107e-06,  ..., -4.6641e-06,
         -3.7923e-06, -4.1127e-06],
        [-7.6443e-06, -5.6922e-06,  5.6475e-06,  ..., -6.1989e-06,
         -5.2005e-06, -4.6641e-06]], device='cuda:0')
Loss: 1.0023818016052246


Running epoch 1, step 1298, batch 250
Sampled inputs[:2]: tensor([[    0,   259,  1513,  ...,   275, 19511,  2350],
        [    0,    12,  1197,  ...,   516,  1136,  9774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2885e-05, -1.5611e-04, -1.0490e-04,  ..., -1.0227e-04,
          4.6022e-04,  1.3297e-04],
        [-4.7535e-06, -3.1739e-06,  3.4198e-06,  ..., -3.7253e-06,
         -2.9393e-06, -3.0212e-06],
        [-1.2189e-05, -8.5682e-06,  9.3579e-06,  ..., -9.5367e-06,
         -7.5847e-06, -7.8231e-06],
        [-8.8811e-06, -5.8189e-06,  6.7055e-06,  ..., -6.9439e-06,
         -5.5209e-06, -5.9977e-06],
        [-1.1027e-05, -8.1807e-06,  8.2850e-06,  ..., -8.9258e-06,
         -7.3612e-06, -6.5342e-06]], device='cuda:0')
Loss: 0.9706107974052429


Running epoch 1, step 1299, batch 251
Sampled inputs[:2]: tensor([[    0, 24440,  1918,  ...,   769,  1254,   596],
        [    0,   320,  4886,  ...,    14,   333,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2230e-04, -2.1588e-04, -6.0510e-05,  ..., -2.0553e-04,
          4.5835e-04,  2.0512e-04],
        [-6.3330e-06, -4.2990e-06,  4.5523e-06,  ..., -4.9919e-06,
         -3.9302e-06, -4.0047e-06],
        [-1.6242e-05, -1.1578e-05,  1.2472e-05,  ..., -1.2785e-05,
         -1.0148e-05, -1.0401e-05],
        [-1.1832e-05, -7.8902e-06,  8.9109e-06,  ..., -9.2834e-06,
         -7.3686e-06, -7.9498e-06],
        [-1.4663e-05, -1.1012e-05,  1.1012e-05,  ..., -1.1936e-05,
         -9.8199e-06, -8.6501e-06]], device='cuda:0')
Loss: 0.9796653985977173


Running epoch 1, step 1300, batch 252
Sampled inputs[:2]: tensor([[    0,    14,  7870,  ...,   284,   830,   292],
        [    0,  7294, 23782,  ...,   471, 11528,  3437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6440e-05, -6.9087e-05,  1.0003e-04,  ..., -1.6493e-04,
          3.1863e-04,  3.0865e-04],
        [-7.8529e-06, -5.2899e-06,  5.6103e-06,  ..., -6.2138e-06,
         -4.9062e-06, -5.0776e-06],
        [-2.0295e-05, -1.4290e-05,  1.5467e-05,  ..., -1.5974e-05,
         -1.2696e-05, -1.3202e-05],
        [-1.4722e-05, -9.6932e-06,  1.1012e-05,  ..., -1.1593e-05,
         -9.2164e-06, -1.0096e-05],
        [-1.8358e-05, -1.3605e-05,  1.3709e-05,  ..., -1.4931e-05,
         -1.2279e-05, -1.0990e-05]], device='cuda:0')
Loss: 0.9530678391456604


Running epoch 1, step 1301, batch 253
Sampled inputs[:2]: tensor([[   0, 2663,  328,  ...,  292,   86,   16],
        [   0,   12,  287,  ..., 3359, 1751, 5048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9270e-06, -6.6902e-05, -1.0813e-05,  ..., -1.4725e-04,
          4.5832e-04,  3.7719e-04],
        [-9.4846e-06, -6.4299e-06,  6.8471e-06,  ..., -7.4729e-06,
         -5.9493e-06, -6.1207e-06],
        [-2.4527e-05, -1.7405e-05,  1.8850e-05,  ..., -1.9282e-05,
         -1.5482e-05, -1.5959e-05],
        [-1.7747e-05, -1.1809e-05,  1.3426e-05,  ..., -1.3933e-05,
         -1.1183e-05, -1.2137e-05],
        [-2.2173e-05, -1.6585e-05,  1.6704e-05,  ..., -1.8016e-05,
         -1.4961e-05, -1.3284e-05]], device='cuda:0')
Loss: 0.9748069047927856


Running epoch 1, step 1302, batch 254
Sampled inputs[:2]: tensor([[    0,  1497, 16170,  ...,  1888,  2350,   578],
        [    0,   446, 21112,  ..., 22092,    22,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8828e-05, -1.2429e-04,  1.9816e-05,  ..., -1.8636e-04,
          3.3407e-04,  2.9309e-04],
        [-1.1124e-05, -7.6294e-06,  8.0392e-06,  ..., -8.7172e-06,
         -6.9551e-06, -7.1041e-06],
        [-2.8908e-05, -2.0742e-05,  2.2188e-05,  ..., -2.2650e-05,
         -1.8239e-05, -1.8641e-05],
        [-2.0877e-05, -1.4089e-05,  1.5795e-05,  ..., -1.6302e-05,
         -1.3120e-05, -1.4104e-05],
        [-2.6077e-05, -1.9714e-05,  1.9625e-05,  ..., -2.1100e-05,
         -1.7583e-05, -1.5520e-05]], device='cuda:0')
Loss: 1.0233644247055054


Running epoch 1, step 1303, batch 255
Sampled inputs[:2]: tensor([[   0,   12,  616,  ...,  278,  266, 2907],
        [   0, 6541,  287,  ..., 1061, 4786,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6278e-05, -2.1547e-04, -1.1650e-04,  ..., -2.8985e-04,
          5.1150e-04,  4.9780e-04],
        [-1.2666e-05, -8.6129e-06,  9.1568e-06,  ..., -9.9093e-06,
         -7.9237e-06, -8.1174e-06],
        [-3.2991e-05, -2.3410e-05,  2.5332e-05,  ..., -2.5749e-05,
         -2.0742e-05, -2.1309e-05],
        [-2.3812e-05, -1.5900e-05,  1.8016e-05,  ..., -1.8552e-05,
         -1.4953e-05, -1.6131e-05],
        [-2.9653e-05, -2.2158e-05,  2.2322e-05,  ..., -2.3901e-05,
         -1.9938e-05, -1.7650e-05]], device='cuda:0')
Loss: 0.9380623698234558
Graident accumulation at epoch 1, step 1303, batch 255
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0291, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3999e-05,  5.4901e-05, -6.5710e-05,  ..., -2.6561e-06,
          5.2304e-05,  2.1106e-05],
        [-1.2928e-05, -8.2925e-06,  8.7113e-06,  ..., -1.0231e-05,
         -7.9687e-06, -8.4739e-06],
        [-2.9770e-06,  4.9525e-06,  2.0018e-06,  ..., -1.8740e-06,
          3.6237e-06, -8.8850e-06],
        [-1.7347e-05, -6.1271e-06,  1.1189e-05,  ..., -1.2069e-05,
         -8.3702e-06, -1.1919e-05],
        [-2.8033e-05, -1.9766e-05,  2.0220e-05,  ..., -2.2051e-05,
         -1.7851e-05, -1.6751e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8140e-08, 5.1500e-08, 5.0497e-08,  ..., 1.8207e-08, 1.1854e-07,
         4.1327e-08],
        [8.2126e-11, 5.0323e-11, 2.0034e-11,  ..., 5.6819e-11, 1.9128e-11,
         2.4338e-11],
        [1.7176e-09, 1.1356e-09, 3.8436e-10,  ..., 1.3170e-09, 5.5168e-10,
         4.0827e-10],
        [4.5038e-10, 3.8996e-10, 1.6413e-10,  ..., 4.1322e-10, 1.7914e-10,
         1.7204e-10],
        [3.9724e-10, 2.1780e-10, 7.1508e-11,  ..., 2.8865e-10, 7.4417e-11,
         1.1386e-10]], device='cuda:0')
optimizer state dict: 163.0
lr: [6.599295247432596e-06, 6.599295247432596e-06]
scheduler_last_epoch: 163


Running epoch 1, step 1304, batch 256
Sampled inputs[:2]: tensor([[    0, 28559,  1357,  ...,  7720,  1398, 41925],
        [    0,  2086, 10663,  ...,   271,   266,  6927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1365e-05,  3.5577e-05,  1.6501e-04,  ..., -9.8085e-05,
          1.4297e-04,  1.5614e-04],
        [-1.5572e-06, -1.0729e-06,  1.2144e-06,  ..., -1.1846e-06,
         -9.3132e-07, -9.7603e-07],
        [ 5.1590e-05,  8.4215e-05, -4.9745e-05,  ...,  5.6015e-05,
          9.8878e-05,  1.8277e-05],
        [-3.0547e-06, -2.0564e-06,  2.4885e-06,  ..., -2.3097e-06,
         -1.8254e-06, -2.0266e-06],
        [-3.4869e-06, -2.6673e-06,  2.8163e-06,  ..., -2.7567e-06,
         -2.2650e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9714046120643616


Running epoch 1, step 1305, batch 257
Sampled inputs[:2]: tensor([[    0,   221,   334,  ...,   271,   266,  7246],
        [    0,  7240,   365,  ...,   630,   491, 10524]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0017e-05,  3.4831e-05,  2.9184e-04,  ...,  1.0651e-05,
          1.2093e-04,  2.9248e-04],
        [-3.1292e-06, -2.2277e-06,  2.2352e-06,  ..., -2.5555e-06,
         -2.0564e-06, -2.0489e-06],
        [ 4.7418e-05,  8.1041e-05, -4.6869e-05,  ...,  5.2409e-05,
          9.5928e-05,  1.5446e-05],
        [-5.9158e-06, -4.1276e-06,  4.3958e-06,  ..., -4.7833e-06,
         -3.8669e-06, -4.0531e-06],
        [-7.4506e-06, -5.7369e-06,  5.4836e-06,  ..., -6.2287e-06,
         -5.1707e-06, -4.5449e-06]], device='cuda:0')
Loss: 1.0152980089187622


Running epoch 1, step 1306, batch 258
Sampled inputs[:2]: tensor([[    0,  3502,   527,  ..., 21301, 22248,  1773],
        [    0,   266,  7264,  ...,  3211,   328,   275]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8912e-05,  4.8970e-05,  3.0886e-04,  ...,  6.3484e-05,
          1.3151e-04,  3.9780e-04],
        [-4.7386e-06, -3.3677e-06,  3.2261e-06,  ..., -3.8296e-06,
         -3.0473e-06, -3.1441e-06],
        [ 4.3127e-05,  7.7823e-05, -4.4008e-05,  ...,  4.8997e-05,
          9.3260e-05,  1.2510e-05],
        [-8.9258e-06, -6.2436e-06,  6.3032e-06,  ..., -7.1675e-06,
         -5.7518e-06, -6.2138e-06],
        [-1.1325e-05, -8.7619e-06,  8.0317e-06,  ..., -9.4026e-06,
         -7.7188e-06, -7.0035e-06]], device='cuda:0')
Loss: 0.9644568562507629


Running epoch 1, step 1307, batch 259
Sampled inputs[:2]: tensor([[   0,  367, 2063,  ..., 3022,  221,  733],
        [   0,  199, 7513,  ...,  271,  259,  957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1585e-05,  4.8970e-05,  3.3912e-04,  ...,  1.1853e-04,
          1.8770e-04,  4.7360e-04],
        [-6.3106e-06, -4.4405e-06,  4.3362e-06,  ..., -5.0440e-06,
         -4.0084e-06, -4.2170e-06],
        [ 3.8835e-05,  7.4768e-05, -4.0789e-05,  ...,  4.5674e-05,
          9.0623e-05,  9.5596e-06],
        [-1.1906e-05, -8.2403e-06,  8.4788e-06,  ..., -9.4622e-06,
         -7.5921e-06, -8.3596e-06],
        [-1.4991e-05, -1.1489e-05,  1.0729e-05,  ..., -1.2323e-05,
         -1.0103e-05, -9.3281e-06]], device='cuda:0')
Loss: 0.9539756774902344


Running epoch 1, step 1308, batch 260
Sampled inputs[:2]: tensor([[    0,  2588, 25531,  ...,  1977,   300,   259],
        [    0, 11348,   292,  ...,  3904,  1110,  8079]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0122e-06,  3.5227e-04,  4.1759e-04,  ...,  2.0944e-04,
          1.3366e-04,  5.1195e-04],
        [-7.9051e-06, -5.5060e-06,  5.2974e-06,  ..., -6.3926e-06,
         -5.0887e-06, -5.4166e-06],
        [ 3.4573e-05,  7.1803e-05, -3.8003e-05,  ...,  4.2142e-05,
          8.7777e-05,  6.3856e-06],
        [-1.4856e-05, -1.0148e-05,  1.0297e-05,  ..., -1.1936e-05,
         -9.6038e-06, -1.0669e-05],
        [-1.8954e-05, -1.4320e-05,  1.3262e-05,  ..., -1.5706e-05,
         -1.2890e-05, -1.2070e-05]], device='cuda:0')
Loss: 0.9614002704620361


Running epoch 1, step 1309, batch 261
Sampled inputs[:2]: tensor([[    0,    52, 26766,  ...,  4411,  4226,   278],
        [    0,    14,   381,  ...,   278,   269, 10376]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7765e-05,  3.4948e-04,  4.1759e-04,  ...,  2.4249e-04,
          1.9867e-04,  5.2500e-04],
        [-9.4548e-06, -6.6906e-06,  6.4149e-06,  ..., -7.6517e-06,
         -6.0946e-06, -6.4746e-06],
        [ 3.0133e-05,  6.8331e-05, -3.4650e-05,  ...,  3.8521e-05,
          8.4886e-05,  3.3309e-06],
        [-1.7852e-05, -1.2383e-05,  1.2517e-05,  ..., -1.4365e-05,
         -1.1556e-05, -1.2800e-05],
        [-2.2799e-05, -1.7434e-05,  1.6123e-05,  ..., -1.8910e-05,
         -1.5512e-05, -1.4558e-05]], device='cuda:0')
Loss: 1.003925085067749


Running epoch 1, step 1310, batch 262
Sampled inputs[:2]: tensor([[    0,   635,    13,  ...,   292,    20,   445],
        [    0, 35449,   824,  ...,   278, 30449,  3659]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7254e-05,  4.2861e-04,  4.5700e-04,  ...,  2.5113e-04,
          2.2801e-04,  6.3870e-04],
        [-1.0937e-05, -7.7710e-06,  7.5623e-06,  ..., -8.8438e-06,
         -7.0147e-06, -7.4357e-06],
        [ 2.6199e-05,  6.5365e-05, -3.1417e-05,  ...,  3.5377e-05,
          8.2472e-05,  7.6787e-07],
        [-2.0698e-05, -1.4424e-05,  1.4812e-05,  ..., -1.6630e-05,
         -1.3322e-05, -1.4767e-05],
        [-2.6211e-05, -2.0117e-05,  1.8865e-05,  ..., -2.1711e-05,
         -1.7747e-05, -1.6585e-05]], device='cuda:0')
Loss: 0.9668947458267212


Running epoch 1, step 1311, batch 263
Sampled inputs[:2]: tensor([[    0,   546,   360,  ...,  9107,  2772,  4496],
        [    0, 29073,   916,  ...,    12,   287,   850]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2353e-06,  3.8865e-04,  4.6741e-04,  ...,  1.6061e-04,
          6.1299e-04,  9.6699e-04],
        [-1.2457e-05, -8.8662e-06,  8.7321e-06,  ..., -1.0051e-05,
         -7.9572e-06, -8.3894e-06],
        [ 2.2235e-05,  6.2489e-05, -2.8168e-05,  ...,  3.2277e-05,
          8.0043e-05, -1.7057e-06],
        [-2.3574e-05, -1.6436e-05,  1.7136e-05,  ..., -1.8880e-05,
         -1.5095e-05, -1.6659e-05],
        [-2.9668e-05, -2.2769e-05,  2.1666e-05,  ..., -2.4512e-05,
         -2.0027e-05, -1.8552e-05]], device='cuda:0')
Loss: 0.9832118153572083
Graident accumulation at epoch 1, step 1311, batch 263
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0060, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0342, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0287,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.6076e-05,  8.8276e-05, -1.2399e-05,  ...,  1.3670e-05,
          1.0837e-04,  1.1569e-04],
        [-1.2881e-05, -8.3499e-06,  8.7133e-06,  ..., -1.0213e-05,
         -7.9676e-06, -8.4655e-06],
        [-4.5581e-07,  1.0706e-05, -1.0151e-06,  ...,  1.5411e-06,
          1.1266e-05, -8.1671e-06],
        [-1.7970e-05, -7.1580e-06,  1.1783e-05,  ..., -1.2750e-05,
         -9.0426e-06, -1.2393e-05],
        [-2.8196e-05, -2.0066e-05,  2.0364e-05,  ..., -2.2297e-05,
         -1.8068e-05, -1.6931e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8082e-08, 5.1599e-08, 5.0665e-08,  ..., 1.8215e-08, 1.1880e-07,
         4.2221e-08],
        [8.2199e-11, 5.0351e-11, 2.0090e-11,  ..., 5.6863e-11, 1.9172e-11,
         2.4384e-11],
        [1.7164e-09, 1.1383e-09, 3.8477e-10,  ..., 1.3168e-09, 5.5754e-10,
         4.0787e-10],
        [4.5049e-10, 3.8984e-10, 1.6426e-10,  ..., 4.1316e-10, 1.7919e-10,
         1.7215e-10],
        [3.9773e-10, 2.1810e-10, 7.1906e-11,  ..., 2.8896e-10, 7.4743e-11,
         1.1410e-10]], device='cuda:0')
optimizer state dict: 164.0
lr: [6.4833089778264036e-06, 6.4833089778264036e-06]
scheduler_last_epoch: 164


Running epoch 1, step 1312, batch 264
Sampled inputs[:2]: tensor([[    0,   300,  5631,  ...,  2278,  2669,  3011],
        [    0,  2099,  1718,  ..., 11271,   287,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4475e-05, -3.5114e-05,  1.9771e-06,  ..., -2.1933e-06,
         -6.9647e-05,  4.3386e-05],
        [-1.5497e-06, -1.1325e-06,  1.1921e-06,  ..., -1.1995e-06,
         -9.0525e-07, -9.6112e-07],
        [-4.2319e-06, -3.2485e-06,  3.4124e-06,  ..., -3.3081e-06,
         -2.5332e-06, -2.6971e-06],
        [-3.0398e-06, -2.1905e-06,  2.4289e-06,  ..., -2.3246e-06,
         -1.7732e-06, -1.9968e-06],
        [-3.6359e-06, -2.9355e-06,  2.8908e-06,  ..., -2.9504e-06,
         -2.3395e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9874274134635925


Running epoch 1, step 1313, batch 265
Sampled inputs[:2]: tensor([[    0, 18981,    13,  ...,   365,  2714,   408],
        [    0,   694,  2326,  ...,   278,  1781,  9660]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.8451e-05,  1.0640e-04,  3.4569e-05,  ...,  8.0166e-05,
         -3.0562e-05,  6.3789e-05],
        [-3.0100e-06, -2.2352e-06,  2.3022e-06,  ..., -2.3618e-06,
         -1.8217e-06, -1.8515e-06],
        [-8.1360e-06, -6.2287e-06,  6.5565e-06,  ..., -6.3926e-06,
         -4.9174e-06, -5.0813e-06],
        [-5.8860e-06, -4.2766e-06,  4.6790e-06,  ..., -4.5598e-06,
         -3.5316e-06, -3.8370e-06],
        [-7.0333e-06, -5.6624e-06,  5.5581e-06,  ..., -5.7220e-06,
         -4.5747e-06, -4.0531e-06]], device='cuda:0')
Loss: 0.9787128567695618


Running epoch 1, step 1314, batch 266
Sampled inputs[:2]: tensor([[   0,  437,  266,  ...,  630,  586,  824],
        [   0, 2667,  365,  ..., 9281, 1631, 9123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9141e-05,  1.3379e-04,  2.7541e-05,  ...,  5.4063e-05,
          2.1245e-04,  1.7917e-04],
        [-4.5151e-06, -3.3602e-06,  3.2485e-06,  ..., -3.6359e-06,
         -2.8573e-06, -2.9020e-06],
        [-1.2189e-05, -9.3132e-06,  9.2983e-06,  ..., -9.7603e-06,
         -7.6145e-06, -7.8976e-06],
        [-8.7172e-06, -6.3330e-06,  6.5118e-06,  ..., -6.9588e-06,
         -5.4836e-06, -5.9232e-06],
        [-1.0699e-05, -8.5682e-06,  7.9870e-06,  ..., -8.8513e-06,
         -7.1526e-06, -6.3926e-06]], device='cuda:0')
Loss: 0.9955881237983704


Running epoch 1, step 1315, batch 267
Sampled inputs[:2]: tensor([[    0,   446, 23105,  ..., 11867,   824,   368],
        [    0,  1188,    12,  ...,   292, 23032,   689]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.8865e-05,  1.4268e-04,  1.6617e-05,  ...,  8.3334e-05,
          1.8574e-04,  3.1868e-04],
        [-6.0573e-06, -4.5225e-06,  4.3288e-06,  ..., -4.8727e-06,
         -3.8408e-06, -3.9153e-06],
        [-1.6361e-05, -1.2547e-05,  1.2383e-05,  ..., -1.3098e-05,
         -1.0252e-05, -1.0669e-05],
        [-1.1623e-05, -8.4788e-06,  8.6129e-06,  ..., -9.2685e-06,
         -7.3239e-06, -7.9498e-06],
        [-1.4424e-05, -1.1593e-05,  1.0684e-05,  ..., -1.1936e-05,
         -9.6858e-06, -8.6725e-06]], device='cuda:0')
Loss: 0.9843745231628418


Running epoch 1, step 1316, batch 268
Sampled inputs[:2]: tensor([[    0,  1176,    13,  ...,  1919,   221,   380],
        [    0, 47354,  5923,  ...,   266, 14679,  8137]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1342e-05,  3.0145e-04,  5.1997e-05,  ...,  1.6633e-04,
          4.7883e-05,  1.9984e-04],
        [-7.5996e-06, -5.5954e-06,  5.3644e-06,  ..., -6.1095e-06,
         -4.8168e-06, -5.0329e-06],
        [-2.0325e-05, -1.5453e-05,  1.5274e-05,  ..., -1.6257e-05,
         -1.2785e-05, -1.3500e-05],
        [-1.4484e-05, -1.0446e-05,  1.0610e-05,  ..., -1.1563e-05,
         -9.1791e-06, -1.0155e-05],
        [-1.8060e-05, -1.4380e-05,  1.3307e-05,  ..., -1.4916e-05,
         -1.2174e-05, -1.1057e-05]], device='cuda:0')
Loss: 0.9313803911209106


Running epoch 1, step 1317, batch 269
Sampled inputs[:2]: tensor([[    0,    20,     9,  ...,    12,  2212, 24950],
        [    0,  1410,   271,  ...,   259, 27726,  9533]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1721e-04,  3.3402e-04,  8.0423e-06,  ...,  2.2168e-04,
         -6.5579e-05,  1.2540e-05],
        [-9.1344e-06, -6.6385e-06,  6.3144e-06,  ..., -7.3910e-06,
         -5.7779e-06, -6.2101e-06],
        [-2.4557e-05, -1.8403e-05,  1.8105e-05,  ..., -1.9759e-05,
         -1.5467e-05, -1.6674e-05],
        [-1.7315e-05, -1.2331e-05,  1.2428e-05,  ..., -1.3947e-05,
         -1.0997e-05, -1.2420e-05],
        [-2.1935e-05, -1.7151e-05,  1.5885e-05,  ..., -1.8179e-05,
         -1.4737e-05, -1.3754e-05]], device='cuda:0')
Loss: 0.9457469582557678


Running epoch 1, step 1318, batch 270
Sampled inputs[:2]: tensor([[    0, 16028,   669,  ...,   292,  6502,  7050],
        [    0,   278, 19142,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3617e-04,  5.6025e-04, -1.5393e-04,  ...,  2.4567e-04,
         -2.4264e-04, -7.5426e-05],
        [-1.0811e-05, -7.6592e-06,  7.2494e-06,  ..., -8.7097e-06,
         -6.7465e-06, -7.4692e-06],
        [-2.9027e-05, -2.1368e-05,  2.0817e-05,  ..., -2.3305e-05,
         -1.8179e-05, -1.9997e-05],
        [-2.0295e-05, -1.4134e-05,  1.4149e-05,  ..., -1.6317e-05,
         -1.2808e-05, -1.4730e-05],
        [-2.6047e-05, -1.9968e-05,  1.8343e-05,  ..., -2.1547e-05,
         -1.7390e-05, -1.6645e-05]], device='cuda:0')
Loss: 0.8987312316894531


Running epoch 1, step 1319, batch 271
Sampled inputs[:2]: tensor([[    0,  1029,  6068,  ..., 18017,   300,   259],
        [    0,    17,  2736,  ...,   352,   422,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5348e-04,  5.9621e-04, -1.6882e-04,  ...,  2.3060e-04,
         -2.4796e-04, -3.1316e-06],
        [-1.2316e-05, -8.7917e-06,  8.2850e-06,  ..., -9.9391e-06,
         -7.7300e-06, -8.5272e-06],
        [-3.3021e-05, -2.4453e-05,  2.3752e-05,  ..., -2.6509e-05,
         -2.0698e-05, -2.2754e-05],
        [-2.3142e-05, -1.6205e-05,  1.6145e-05,  ..., -1.8626e-05,
         -1.4670e-05, -1.6846e-05],
        [-2.9728e-05, -2.2903e-05,  2.0996e-05,  ..., -2.4602e-05,
         -1.9863e-05, -1.8969e-05]], device='cuda:0')
Loss: 0.9720969796180725
Graident accumulation at epoch 1, step 1319, batch 271
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0061, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0342],
        [ 0.0343, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4121e-05,  1.3907e-04, -2.8040e-05,  ...,  3.5363e-05,
          7.2739e-05,  1.0381e-04],
        [-1.2824e-05, -8.3940e-06,  8.6705e-06,  ..., -1.0186e-05,
         -7.9438e-06, -8.4717e-06],
        [-3.7123e-06,  7.1903e-06,  1.4616e-06,  ..., -1.2639e-06,
          8.0693e-06, -9.6258e-06],
        [-1.8487e-05, -8.0627e-06,  1.2220e-05,  ..., -1.3338e-05,
         -9.6054e-06, -1.2838e-05],
        [-2.8350e-05, -2.0350e-05,  2.0427e-05,  ..., -2.2528e-05,
         -1.8248e-05, -1.7135e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8149e-08, 5.1903e-08, 5.0643e-08,  ..., 1.8250e-08, 1.1874e-07,
         4.2179e-08],
        [8.2269e-11, 5.0378e-11, 2.0139e-11,  ..., 5.6905e-11, 1.9213e-11,
         2.4433e-11],
        [1.7157e-09, 1.1378e-09, 3.8495e-10,  ..., 1.3162e-09, 5.5741e-10,
         4.0798e-10],
        [4.5057e-10, 3.8971e-10, 1.6436e-10,  ..., 4.1309e-10, 1.7922e-10,
         1.7226e-10],
        [3.9821e-10, 2.1841e-10, 7.2275e-11,  ..., 2.8928e-10, 7.5063e-11,
         1.1434e-10]], device='cuda:0')
optimizer state dict: 165.0
lr: [6.367860089306028e-06, 6.367860089306028e-06]
scheduler_last_epoch: 165


Running epoch 1, step 1320, batch 272
Sampled inputs[:2]: tensor([[   0, 1235,   14,  ..., 3301,  549,   14],
        [   0, 7219,  591,  ...,  278,  266, 5908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0447e-05,  5.1940e-05, -7.4997e-05,  ..., -1.3362e-04,
          3.2641e-05, -1.3740e-04],
        [-1.5125e-06, -1.1548e-06,  1.1101e-06,  ..., -1.2293e-06,
         -9.7603e-07, -9.7603e-07],
        [-4.0531e-06, -3.2187e-06,  3.2037e-06,  ..., -3.3081e-06,
         -2.6226e-06, -2.6375e-06],
        [-2.8759e-06, -2.1756e-06,  2.2203e-06,  ..., -2.3395e-06,
         -1.8775e-06, -1.9968e-06],
        [-3.5167e-06, -2.9206e-06,  2.7120e-06,  ..., -2.9504e-06,
         -2.4289e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.978681743144989


Running epoch 1, step 1321, batch 273
Sampled inputs[:2]: tensor([[    0,  1265,   328,  ...,  2282, 35414,    13],
        [    0,   518,  9048,  ...,  1354,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1952e-04, -1.4187e-04, -1.7483e-04,  ..., -5.3224e-05,
          3.3414e-04,  1.0838e-04],
        [-2.8312e-06, -2.1458e-06,  1.9856e-06,  ..., -2.4065e-06,
         -1.8962e-06, -2.0489e-06],
        [-7.5549e-06, -5.8413e-06,  5.7817e-06,  ..., -6.3032e-06,
         -4.9174e-06, -5.3197e-06],
        [-5.3942e-06, -3.9861e-06,  3.9712e-06,  ..., -4.5747e-06,
         -3.6508e-06, -4.1872e-06],
        [-6.7651e-06, -5.4091e-06,  5.0664e-06,  ..., -5.7518e-06,
         -4.6492e-06, -4.3064e-06]], device='cuda:0')
Loss: 0.9500319361686707


Running epoch 1, step 1322, batch 274
Sampled inputs[:2]: tensor([[    0,  6693,  1235,  ..., 10814,  1810,   367],
        [    0,   266,  9076,  ...,   490,   437, 41298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7052e-04, -9.8384e-05, -3.6454e-04,  ..., -8.1832e-05,
          1.7982e-04, -7.7310e-05],
        [-4.3064e-06, -3.1888e-06,  3.0361e-06,  ..., -3.5912e-06,
         -2.7567e-06, -3.0696e-06],
        [-1.1548e-05, -8.7768e-06,  8.8215e-06,  ..., -9.4920e-06,
         -7.2569e-06, -8.0764e-06],
        [-8.1956e-06, -5.9083e-06,  6.0573e-06,  ..., -6.8098e-06,
         -5.2899e-06, -6.2287e-06],
        [-1.0207e-05, -8.0615e-06,  7.6443e-06,  ..., -8.5831e-06,
         -6.8247e-06, -6.4820e-06]], device='cuda:0')
Loss: 0.9716224074363708


Running epoch 1, step 1323, batch 275
Sampled inputs[:2]: tensor([[    0,  3377,    12,  ...,   333,   199,   769],
        [    0,  6369,  3335,  ..., 23951,  8461,    66]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6219e-04, -7.9929e-05, -1.6469e-04,  ...,  9.8131e-06,
          1.6935e-04, -7.5893e-07],
        [-5.8785e-06, -4.2692e-06,  4.0419e-06,  ..., -4.8503e-06,
         -3.7253e-06, -4.1798e-06],
        [-1.5870e-05, -1.1966e-05,  1.1802e-05,  ..., -1.2979e-05,
         -1.0014e-05, -1.1131e-05],
        [-1.1161e-05, -7.9349e-06,  8.0392e-06,  ..., -9.2089e-06,
         -7.1675e-06, -8.4043e-06],
        [-1.4052e-05, -1.0967e-05,  1.0237e-05,  ..., -1.1772e-05,
         -9.4175e-06, -9.0152e-06]], device='cuda:0')
Loss: 0.9522900581359863


Running epoch 1, step 1324, batch 276
Sampled inputs[:2]: tensor([[    0, 24062, 11234,  ...,  4252,   300,   970],
        [    0,  1552,   271,  ...,    13,   287,   995]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0099e-04, -1.1405e-04, -1.2110e-04,  ..., -1.3612e-05,
          1.8725e-04, -2.3057e-06],
        [-7.3686e-06, -5.4762e-06,  5.1223e-06,  ..., -6.0350e-06,
         -4.6864e-06, -5.2229e-06],
        [-2.0072e-05, -1.5453e-05,  1.5005e-05,  ..., -1.6317e-05,
         -1.2726e-05, -1.4082e-05],
        [-1.4022e-05, -1.0215e-05,  1.0200e-05,  ..., -1.1459e-05,
         -9.0078e-06, -1.0505e-05],
        [-1.7688e-05, -1.4096e-05,  1.2964e-05,  ..., -1.4737e-05,
         -1.1906e-05, -1.1399e-05]], device='cuda:0')
Loss: 0.9916672110557556


Running epoch 1, step 1325, batch 277
Sampled inputs[:2]: tensor([[   0,  266, 1586,  ..., 1888, 2117,  328],
        [   0,   12, 9248,  ..., 2673, 4239,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4828e-04, -1.0143e-04, -3.0903e-04,  ..., -3.9422e-05,
          3.0318e-04,  3.0536e-05],
        [-8.8885e-06, -6.5491e-06,  6.1803e-06,  ..., -7.2420e-06,
         -5.6140e-06, -6.2436e-06],
        [ 3.8377e-04,  2.5503e-04, -3.3818e-04,  ...,  2.8906e-04,
          3.3556e-04,  2.5993e-04],
        [-1.6898e-05, -1.2182e-05,  1.2286e-05,  ..., -1.3709e-05,
         -1.0751e-05, -1.2562e-05],
        [-2.1353e-05, -1.6898e-05,  1.5631e-05,  ..., -1.7717e-05,
         -1.4260e-05, -1.3664e-05]], device='cuda:0')
Loss: 0.9836087822914124


Running epoch 1, step 1326, batch 278
Sampled inputs[:2]: tensor([[   0, 1075,  940,  ..., 3780,   13, 4467],
        [   0, 9029,  634,  ..., 1424, 6872,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4819e-04, -7.1965e-05, -4.3496e-04,  ..., -4.6368e-05,
          3.0064e-04,  2.2308e-05],
        [-1.0453e-05, -7.6219e-06,  7.2531e-06,  ..., -8.5011e-06,
         -6.5193e-06, -7.3686e-06],
        [ 3.7945e-04,  2.5199e-04, -3.3503e-04,  ...,  2.8563e-04,
          3.3307e-04,  2.5686e-04],
        [-1.9819e-05, -1.4134e-05,  1.4357e-05,  ..., -1.6034e-05,
         -1.2442e-05, -1.4737e-05],
        [-2.5168e-05, -1.9670e-05,  1.8358e-05,  ..., -2.0847e-05,
         -1.6615e-05, -1.6198e-05]], device='cuda:0')
Loss: 0.9853289127349854


Running epoch 1, step 1327, batch 279
Sampled inputs[:2]: tensor([[    0, 26700,  5475,  ...,  5707,    65,    13],
        [    0,   471,    14,  ..., 27104,     9,   631]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7875e-04, -1.1038e-04, -5.6791e-04,  ..., -5.2339e-05,
          1.8299e-04,  7.0996e-06],
        [-1.2070e-05, -8.7097e-06,  8.2515e-06,  ..., -9.8348e-06,
         -7.5102e-06, -8.4862e-06],
        [ 3.7513e-04,  2.4898e-04, -3.3213e-04,  ...,  2.8212e-04,
          3.3045e-04,  2.5392e-04],
        [-2.2799e-05, -1.6101e-05,  1.6280e-05,  ..., -1.8492e-05,
         -1.4283e-05, -1.6898e-05],
        [ 1.0161e-04,  1.3537e-04, -8.1165e-05,  ...,  9.0196e-05,
          5.4054e-05,  6.1238e-05]], device='cuda:0')
Loss: 0.9708167314529419
Graident accumulation at epoch 1, step 1327, batch 279
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0027, -0.0343],
        [ 0.0343, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.6166e-05,  1.1412e-04, -8.2028e-05,  ...,  2.6593e-05,
          8.3764e-05,  9.4141e-05],
        [-1.2749e-05, -8.4256e-06,  8.6286e-06,  ..., -1.0151e-05,
         -7.9004e-06, -8.4731e-06],
        [ 3.4172e-05,  3.1369e-05, -3.1897e-05,  ...,  2.7074e-05,
          4.0307e-05,  1.6729e-05],
        [-1.8918e-05, -8.8665e-06,  1.2626e-05,  ..., -1.3853e-05,
         -1.0073e-05, -1.3244e-05],
        [-1.5354e-05, -4.7782e-06,  1.0268e-05,  ..., -1.1255e-05,
         -1.1018e-05, -9.2979e-06]], device='cuda:0')
optimizer state dict: tensor([[5.8234e-08, 5.1863e-08, 5.0915e-08,  ..., 1.8234e-08, 1.1865e-07,
         4.2137e-08],
        [8.2332e-11, 5.0404e-11, 2.0187e-11,  ..., 5.6945e-11, 1.9250e-11,
         2.4480e-11],
        [1.8547e-09, 1.1986e-09, 4.9487e-10,  ..., 1.3944e-09, 6.6605e-10,
         4.7205e-10],
        [4.5064e-10, 3.8958e-10, 1.6446e-10,  ..., 4.1302e-10, 1.7925e-10,
         1.7237e-10],
        [4.0814e-10, 2.3652e-10, 7.8790e-11,  ..., 2.9712e-10, 7.7910e-11,
         1.1798e-10]], device='cuda:0')
optimizer state dict: 166.0
lr: [6.252966223469409e-06, 6.252966223469409e-06]
scheduler_last_epoch: 166


Running epoch 1, step 1328, batch 280
Sampled inputs[:2]: tensor([[   0,   19,  669,  ...,   14, 4053,  352],
        [   0, 1871,  518,  ...,  271,  259, 1110]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7557e-05, -5.5526e-06,  5.8292e-05,  ...,  2.9278e-05,
          2.0347e-04,  1.2014e-04],
        [-1.4678e-06, -1.1250e-06,  8.9034e-07,  ..., -1.2442e-06,
         -1.0431e-06, -1.1325e-06],
        [-3.9041e-06, -3.0696e-06,  2.5928e-06,  ..., -3.2783e-06,
         -2.7418e-06, -2.9504e-06],
        [-2.6822e-06, -1.9968e-06,  1.6838e-06,  ..., -2.2650e-06,
         -1.9372e-06, -2.1756e-06],
        [-3.7253e-06, -2.9802e-06,  2.4438e-06,  ..., -3.1739e-06,
         -2.7120e-06, -2.6077e-06]], device='cuda:0')
Loss: 0.9807335138320923


Running epoch 1, step 1329, batch 281
Sampled inputs[:2]: tensor([[   0, 1555,   12,  ...,  809,  287,  259],
        [   0,  266, 1211,  ..., 1336,  694,  516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5775e-04, -1.2272e-04, -3.7569e-05,  ..., -9.8273e-06,
          2.5941e-04,  1.4333e-04],
        [-2.9877e-06, -2.3544e-06,  1.9558e-06,  ..., -2.4736e-06,
         -2.0415e-06, -2.1607e-06],
        [-8.0764e-06, -6.6161e-06,  5.7071e-06,  ..., -6.6608e-06,
         -5.5283e-06, -5.7966e-06],
        [-5.6177e-06, -4.3511e-06,  3.8296e-06,  ..., -4.6343e-06,
         -3.9190e-06, -4.2915e-06],
        [-7.3314e-06, -6.1691e-06,  5.1111e-06,  ..., -6.1840e-06,
         -5.2601e-06, -4.8876e-06]], device='cuda:0')
Loss: 1.0134907960891724


Running epoch 1, step 1330, batch 282
Sampled inputs[:2]: tensor([[    0,   360,  2063,  ..., 49105,   221,  1868],
        [    0,   266,  1916,  ...,   292, 12946,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3578e-04, -1.8518e-04, -2.6605e-04,  ..., -1.7138e-04,
          4.1786e-04,  1.0994e-04],
        [-4.5598e-06, -3.4422e-06,  2.9542e-06,  ..., -3.7476e-06,
         -2.9169e-06, -3.2708e-06],
        [-1.2219e-05, -9.5516e-06,  8.5831e-06,  ..., -9.9838e-06,
         -7.8529e-06, -8.6725e-06],
        [-8.4043e-06, -6.2361e-06,  5.6997e-06,  ..., -6.8843e-06,
         -5.4836e-06, -6.3479e-06],
        [-1.1027e-05, -8.8811e-06,  7.6443e-06,  ..., -9.2238e-06,
         -7.4953e-06, -7.2569e-06]], device='cuda:0')
Loss: 0.9349279403686523


Running epoch 1, step 1331, batch 283
Sampled inputs[:2]: tensor([[   0, 2355, 2728,  ...,  554, 9025,  368],
        [   0, 4902,  518,  ..., 5493, 3227,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9369e-04, -2.7022e-04, -3.5389e-04,  ..., -2.2272e-04,
          5.6704e-04,  3.0506e-04],
        [-6.0648e-06, -4.6566e-06,  3.9451e-06,  ..., -5.0366e-06,
         -3.9600e-06, -4.2915e-06],
        [-1.6391e-05, -1.3068e-05,  1.1504e-05,  ..., -1.3560e-05,
         -1.0759e-05, -1.1533e-05],
        [-1.1280e-05, -8.5160e-06,  7.6517e-06,  ..., -9.3281e-06,
         -7.4804e-06, -8.4341e-06],
        [-1.4782e-05, -1.2159e-05,  1.0222e-05,  ..., -1.2517e-05,
         -1.0237e-05, -9.6411e-06]], device='cuda:0')
Loss: 1.002820372581482


Running epoch 1, step 1332, batch 284
Sampled inputs[:2]: tensor([[   0, 8588, 3937,  ...,  516, 1128, 2341],
        [   0,  471,   14,  ..., 1260, 2129,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1781e-04, -1.9645e-04, -2.2156e-04,  ..., -2.1153e-04,
          5.8818e-04,  2.8215e-04],
        [-7.5027e-06, -5.7742e-06,  4.8503e-06,  ..., -6.2734e-06,
         -4.9062e-06, -5.3719e-06],
        [-2.0355e-05, -1.6272e-05,  1.4246e-05,  ..., -1.6943e-05,
         -1.3366e-05, -1.4484e-05],
        [-1.3977e-05, -1.0572e-05,  9.4399e-06,  ..., -1.1653e-05,
         -9.2834e-06, -1.0595e-05],
        [-1.8358e-05, -1.5125e-05,  1.2621e-05,  ..., -1.5631e-05,
         -1.2726e-05, -1.2085e-05]], device='cuda:0')
Loss: 0.9354199171066284


Running epoch 1, step 1333, batch 285
Sampled inputs[:2]: tensor([[    0,   677, 20206,  ...,   292,   334,  1550],
        [    0,   417,   199,  ...,  2057,   342, 11927]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.4797e-04, -3.2818e-04, -3.6851e-04,  ..., -3.5480e-04,
          8.1037e-04,  9.7573e-05],
        [-8.8811e-06, -6.7502e-06,  5.8189e-06,  ..., -7.4953e-06,
         -5.7593e-06, -6.4000e-06],
        [-2.3991e-05, -1.8895e-05,  1.7062e-05,  ..., -2.0072e-05,
         -1.5557e-05, -1.7092e-05],
        [-1.6570e-05, -1.2361e-05,  1.1377e-05,  ..., -1.3933e-05,
         -1.0893e-05, -1.2651e-05],
        [-2.1622e-05, -1.7568e-05,  1.5095e-05,  ..., -1.8507e-05,
         -1.4812e-05, -1.4201e-05]], device='cuda:0')
Loss: 0.9657387137413025


Running epoch 1, step 1334, batch 286
Sampled inputs[:2]: tensor([[   0,   45,   17,  ...,  278, 4112,   14],
        [   0,   21,   66,  ..., 1377,  278, 1634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5630e-04, -3.8906e-04, -3.4609e-04,  ..., -3.2433e-04,
          5.7026e-04, -5.6508e-05],
        [-1.0386e-05, -7.8008e-06,  6.7130e-06,  ..., -8.8289e-06,
         -6.7651e-06, -7.5549e-06],
        [-2.8253e-05, -2.1890e-05,  1.9744e-05,  ..., -2.3767e-05,
         -1.8314e-05, -2.0295e-05],
        [-1.9267e-05, -1.4164e-05,  1.3039e-05,  ..., -1.6302e-05,
         -1.2681e-05, -1.4812e-05],
        [-2.5645e-05, -2.0504e-05,  1.7598e-05,  ..., -2.2069e-05,
         -1.7568e-05, -1.7017e-05]], device='cuda:0')
Loss: 0.971256673336029


Running epoch 1, step 1335, batch 287
Sampled inputs[:2]: tensor([[   0,  587,  292,  ...,   12,  287, 2261],
        [   0,  401,  266,  ...,  266, 2236, 1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.2453e-04, -3.9442e-04, -5.0254e-04,  ..., -3.4825e-04,
          7.5966e-04,  1.1830e-04],
        [-1.1988e-05, -8.9705e-06,  7.8082e-06,  ..., -1.0088e-05,
         -7.6704e-06, -8.5756e-06],
        [-3.2574e-05, -2.5213e-05,  2.2918e-05,  ..., -2.7180e-05,
         -2.0817e-05, -2.3052e-05],
        [-2.2188e-05, -1.6280e-05,  1.5125e-05,  ..., -1.8612e-05,
         -1.4372e-05, -1.6779e-05],
        [-2.9325e-05, -2.3440e-05,  2.0266e-05,  ..., -2.5049e-05,
         -1.9848e-05, -1.9178e-05]], device='cuda:0')
Loss: 0.9708705544471741
Graident accumulation at epoch 1, step 1335, batch 287
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0343, -0.0089,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.7003e-05,  6.3270e-05, -1.2408e-04,  ..., -1.0891e-05,
          1.5135e-04,  9.6556e-05],
        [-1.2673e-05, -8.4801e-06,  8.5466e-06,  ..., -1.0144e-05,
         -7.8774e-06, -8.4834e-06],
        [ 2.7497e-05,  2.5711e-05, -2.6416e-05,  ...,  2.1649e-05,
          3.4195e-05,  1.2751e-05],
        [-1.9245e-05, -9.6078e-06,  1.2876e-05,  ..., -1.4329e-05,
         -1.0503e-05, -1.3598e-05],
        [-1.6751e-05, -6.6443e-06,  1.1268e-05,  ..., -1.2635e-05,
         -1.1901e-05, -1.0286e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8701e-08, 5.1967e-08, 5.1116e-08,  ..., 1.8337e-08, 1.1911e-07,
         4.2108e-08],
        [8.2394e-11, 5.0434e-11, 2.0228e-11,  ..., 5.6990e-11, 1.9289e-11,
         2.4529e-11],
        [1.8539e-09, 1.1981e-09, 4.9490e-10,  ..., 1.3938e-09, 6.6582e-10,
         4.7211e-10],
        [4.5068e-10, 3.8946e-10, 1.6452e-10,  ..., 4.1296e-10, 1.7928e-10,
         1.7248e-10],
        [4.0859e-10, 2.3683e-10, 7.9122e-11,  ..., 2.9745e-10, 7.8226e-11,
         1.1823e-10]], device='cuda:0')
optimizer state dict: 167.0
lr: [6.138644937102163e-06, 6.138644937102163e-06]
scheduler_last_epoch: 167


Running epoch 1, step 1336, batch 288
Sampled inputs[:2]: tensor([[    0, 11435,  1226,  ...,    13,  1875,  6394],
        [    0,  1211, 11131,  ..., 31480,   565,   446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0672e-05,  1.2721e-05, -1.0699e-04,  ...,  8.9637e-06,
         -7.5556e-05, -3.1345e-05],
        [-1.5050e-06, -1.0729e-06,  9.9838e-07,  ..., -1.2890e-06,
         -9.4250e-07, -1.0505e-06],
        [-3.9339e-06, -2.8759e-06,  2.8163e-06,  ..., -3.3230e-06,
         -2.4587e-06, -2.6971e-06],
        [-2.7567e-06, -1.9073e-06,  1.8999e-06,  ..., -2.3544e-06,
         -1.7583e-06, -2.0415e-06],
        [-3.6359e-06, -2.7567e-06,  2.5481e-06,  ..., -3.1292e-06,
         -2.3991e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9450663328170776


Running epoch 1, step 1337, batch 289
Sampled inputs[:2]: tensor([[   0, 2736, 2523,  ..., 4086, 4798, 7701],
        [   0, 1531,   14,  ..., 6169,   17,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6327e-05,  8.8421e-05, -2.6905e-05,  ...,  8.3271e-05,
          6.7959e-05,  2.1741e-04],
        [-2.9802e-06, -2.3022e-06,  1.9819e-06,  ..., -2.5705e-06,
         -2.0377e-06, -2.0713e-06],
        [-7.8678e-06, -6.3032e-06,  5.6177e-06,  ..., -6.7502e-06,
         -5.4091e-06, -5.4389e-06],
        [ 1.9826e-04,  1.5547e-04, -1.6824e-04,  ...,  2.2815e-04,
          1.9750e-04,  1.4753e-04],
        [-7.3016e-06, -6.0797e-06,  5.1260e-06,  ..., -6.4075e-06,
         -5.3048e-06, -4.6641e-06]], device='cuda:0')
Loss: 0.9990510940551758


Running epoch 1, step 1338, batch 290
Sampled inputs[:2]: tensor([[   0,  221,  709,  ..., 3365, 3504,  278],
        [   0, 5150, 1030,  ...,   14,  475, 1763]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8039e-05,  1.4644e-04, -4.7963e-05,  ...,  1.6291e-04,
          3.4324e-05,  4.5662e-05],
        [-4.5002e-06, -3.3528e-06,  3.0771e-06,  ..., -3.8221e-06,
         -2.9579e-06, -3.1143e-06],
        [-1.1981e-05, -9.2387e-06,  8.7917e-06,  ..., -1.0118e-05,
         -7.8976e-06, -8.2403e-06],
        [ 1.9546e-04,  1.5358e-04, -1.6612e-04,  ...,  2.2586e-04,
          1.9579e-04,  1.4550e-04],
        [-1.0967e-05, -8.8066e-06,  7.8976e-06,  ..., -9.4771e-06,
         -7.6592e-06, -6.9737e-06]], device='cuda:0')
Loss: 0.9802443981170654


Running epoch 1, step 1339, batch 291
Sampled inputs[:2]: tensor([[    0, 15689,   278,  ..., 12016,   271,  4353],
        [    0,   360,  5323,  ..., 29974,    25,    27]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4828e-05,  4.5164e-05, -1.1564e-04,  ...,  1.2122e-04,
          1.9119e-04,  1.4991e-04],
        [-6.0126e-06, -4.5002e-06,  4.2245e-06,  ..., -5.0068e-06,
         -3.8594e-06, -4.1127e-06],
        [-1.6183e-05, -1.2502e-05,  1.2100e-05,  ..., -1.3411e-05,
         -1.0371e-05, -1.1057e-05],
        [ 1.9251e-04,  1.5140e-04, -1.6383e-04,  ...,  2.2358e-04,
          1.9407e-04,  1.4345e-04],
        [-1.4573e-05, -1.1757e-05,  1.0684e-05,  ..., -1.2398e-05,
         -9.9689e-06, -9.1940e-06]], device='cuda:0')
Loss: 1.0031754970550537


Running epoch 1, step 1340, batch 292
Sampled inputs[:2]: tensor([[    0,  6668,   565,  ...,   360,   259,  8166],
        [    0, 12182,  6294,  ...,  1042,  1070,  2228]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.3443e-05,  1.4889e-04, -1.4151e-04,  ...,  1.1479e-04,
          2.5402e-04,  2.8420e-05],
        [-7.4953e-06, -5.6624e-06,  5.2154e-06,  ..., -6.2436e-06,
         -4.7758e-06, -5.1409e-06],
        [-2.0355e-05, -1.5900e-05,  1.5065e-05,  ..., -1.6868e-05,
         -1.2919e-05, -1.3962e-05],
        [ 1.8974e-04,  1.4925e-04, -1.6191e-04,  ...,  2.2128e-04,
          1.9237e-04,  1.4141e-04],
        [-1.8239e-05, -1.4856e-05,  1.3247e-05,  ..., -1.5527e-05,
         -1.2368e-05, -1.1548e-05]], device='cuda:0')
Loss: 0.9863647222518921


Running epoch 1, step 1341, batch 293
Sampled inputs[:2]: tensor([[    0,   300,   266,  ...,   266,   912, 11457],
        [    0,   729,  3430,  ...,  9715,    13, 42383]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7998e-05,  1.5352e-04, -1.6758e-04,  ...,  1.0963e-04,
          4.0332e-04,  9.0669e-05],
        [-9.0152e-06, -6.7577e-06,  6.2436e-06,  ..., -7.4729e-06,
         -5.6587e-06, -6.2436e-06],
        [-2.4587e-05, -1.8999e-05,  1.8120e-05,  ..., -2.0266e-05,
         -1.5363e-05, -1.7002e-05],
        [ 2.4384e-04,  2.5312e-04, -1.7960e-04,  ...,  2.7385e-04,
          2.6726e-04,  1.7139e-04],
        [-2.1920e-05, -1.7673e-05,  1.5870e-05,  ..., -1.8552e-05,
         -1.4648e-05, -1.3977e-05]], device='cuda:0')
Loss: 0.9713538289070129


Running epoch 1, step 1342, batch 294
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,  6451,   292,    34],
        [    0,   935, 28368,  ...,   342,   259,  4600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3733e-05,  1.6844e-04, -1.0748e-04,  ...,  2.3335e-04,
          2.8733e-04,  1.7771e-06],
        [-1.0543e-05, -7.8604e-06,  7.3314e-06,  ..., -8.7395e-06,
         -6.6198e-06, -7.3016e-06],
        [-2.8700e-05, -2.2084e-05,  2.1249e-05,  ..., -2.3633e-05,
         -1.7911e-05, -1.9833e-05],
        [ 6.4758e-04,  7.8612e-04, -4.5403e-04,  ...,  6.6596e-04,
          6.4635e-04,  4.3369e-04],
        [-2.5585e-05, -2.0519e-05,  1.8597e-05,  ..., -2.1636e-05,
         -1.7062e-05, -1.6302e-05]], device='cuda:0')
Loss: 0.9514272212982178


Running epoch 1, step 1343, batch 295
Sampled inputs[:2]: tensor([[   0,   12,  344,  ...,   14, 2295,  516],
        [   0, 1270,  413,  ...,  413,  711,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3596e-05,  1.0612e-04, -9.8107e-05,  ...,  8.9829e-05,
          7.1988e-04,  4.0042e-04],
        [-1.2062e-05, -9.1270e-06,  8.3521e-06,  ..., -1.0036e-05,
         -7.6704e-06, -8.3074e-06],
        [-3.3021e-05, -2.5809e-05,  2.4319e-05,  ..., -2.7344e-05,
         -2.0921e-05, -2.2739e-05],
        [ 6.4473e-04,  7.8376e-04, -4.5203e-04,  ...,  6.6354e-04,
          6.4438e-04,  4.3169e-04],
        [-2.9340e-05, -2.3872e-05,  2.1204e-05,  ..., -2.4930e-05,
         -1.9833e-05, -1.8641e-05]], device='cuda:0')
Loss: 0.9920706152915955
Graident accumulation at epoch 1, step 1343, batch 295
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0343, -0.0090,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-7.2943e-05,  6.7555e-05, -1.2148e-04,  ..., -8.1942e-07,
          2.0821e-04,  1.2694e-04],
        [-1.2612e-05, -8.5448e-06,  8.5271e-06,  ..., -1.0134e-05,
         -7.8567e-06, -8.4658e-06],
        [ 2.1445e-05,  2.0559e-05, -2.1342e-05,  ...,  1.6750e-05,
          2.8683e-05,  9.2021e-06],
        [ 4.7153e-05,  6.9729e-05, -3.3615e-05,  ...,  5.3458e-05,
          5.4986e-05,  3.0931e-05],
        [-1.8010e-05, -8.3671e-06,  1.2262e-05,  ..., -1.3864e-05,
         -1.2694e-05, -1.1121e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8645e-08, 5.1926e-08, 5.1075e-08,  ..., 1.8327e-08, 1.1951e-07,
         4.2227e-08],
        [8.2457e-11, 5.0467e-11, 2.0277e-11,  ..., 5.7034e-11, 1.9329e-11,
         2.4574e-11],
        [1.8532e-09, 1.1976e-09, 4.9500e-10,  ..., 1.3931e-09, 6.6559e-10,
         4.7215e-10],
        [8.6591e-10, 1.0033e-09, 3.6869e-10,  ..., 8.5283e-10, 5.9433e-10,
         3.5867e-10],
        [4.0904e-10, 2.3716e-10, 7.9493e-11,  ..., 2.9778e-10, 7.8541e-11,
         1.1846e-10]], device='cuda:0')
optimizer state dict: 168.0
lr: [6.0249136994947676e-06, 6.0249136994947676e-06]
scheduler_last_epoch: 168


Running epoch 1, step 1344, batch 296
Sampled inputs[:2]: tensor([[    0,   380,   759,  ...,  1420,  1804,   490],
        [    0,   273,   298,  ..., 23554,    12,  1530]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9800e-05,  3.3207e-04,  2.9349e-05,  ...,  1.3463e-04,
         -2.0245e-04, -1.7250e-04],
        [-1.6019e-06, -1.2815e-06,  7.8231e-07,  ..., -1.4305e-06,
         -1.1399e-06, -1.2070e-06],
        [-4.4107e-06, -3.7104e-06,  2.3395e-06,  ..., -3.9339e-06,
         -3.1888e-06, -3.3230e-06],
        [-2.8610e-06, -2.2799e-06,  1.4082e-06,  ..., -2.5928e-06,
         -2.1011e-06, -2.2799e-06],
        [-4.3213e-06, -3.7402e-06,  2.2948e-06,  ..., -3.9339e-06,
         -3.2634e-06, -3.0547e-06]], device='cuda:0')
Loss: 0.9631288051605225


Running epoch 1, step 1345, batch 297
Sampled inputs[:2]: tensor([[    0,    83,    12,  ...,  3781,   292, 27247],
        [    0, 14165,    14,  ..., 34395, 31103,  6905]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2529e-05,  2.1613e-04, -2.9154e-04,  ...,  1.3041e-04,
         -8.7546e-06, -8.8577e-05],
        [-3.1665e-06, -2.4140e-06,  1.7174e-06,  ..., -2.7940e-06,
         -2.2054e-06, -2.3991e-06],
        [-8.7023e-06, -6.8694e-06,  5.0962e-06,  ..., -7.5847e-06,
         -6.0052e-06, -6.4671e-06],
        [-5.6475e-06, -4.2468e-06,  3.1143e-06,  ..., -5.0217e-06,
         -4.0010e-06, -4.4852e-06],
        [-8.2552e-06, -6.7055e-06,  4.7982e-06,  ..., -7.3314e-06,
         -5.9605e-06, -5.7369e-06]], device='cuda:0')
Loss: 0.9858208298683167


Running epoch 1, step 1346, batch 298
Sampled inputs[:2]: tensor([[    0,  1403,    12,  ...,  1062,  2283, 13614],
        [    0,   266, 28695,  ...,   278,   266,  6087]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3215e-05,  3.0912e-04, -3.7831e-04,  ...,  1.6782e-04,
         -2.2040e-04, -2.2902e-04],
        [-4.7907e-06, -3.6582e-06,  2.6934e-06,  ..., -4.1947e-06,
         -3.2485e-06, -3.5688e-06],
        [-1.3292e-05, -1.0505e-05,  7.9870e-06,  ..., -1.1548e-05,
         -8.9854e-06, -9.7901e-06],
        [-8.5682e-06, -6.4522e-06,  4.9323e-06,  ..., -7.5251e-06,
         -5.8711e-06, -6.6459e-06],
        [-1.2606e-05, -1.0252e-05,  7.4804e-06,  ..., -1.1176e-05,
         -8.9258e-06, -8.7470e-06]], device='cuda:0')
Loss: 0.9610595703125


Running epoch 1, step 1347, batch 299
Sampled inputs[:2]: tensor([[   0,  266, 1634,  ...,  310, 1372,  287],
        [   0, 1196, 2612,  ..., 2489,   14,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7977e-05,  1.9038e-04, -5.4168e-04,  ...,  7.4848e-05,
         -4.2820e-05, -2.6693e-04],
        [-6.1914e-06, -4.6864e-06,  3.7216e-06,  ..., -5.3868e-06,
         -4.1202e-06, -4.5449e-06],
        [-1.7047e-05, -1.3322e-05,  1.0982e-05,  ..., -1.4678e-05,
         -1.1280e-05, -1.2338e-05],
        [-1.1206e-05, -8.3297e-06,  7.0035e-06,  ..., -9.7454e-06,
         -7.5176e-06, -8.6129e-06],
        [-1.5929e-05, -1.2875e-05,  1.0058e-05,  ..., -1.4022e-05,
         -1.1116e-05, -1.0774e-05]], device='cuda:0')
Loss: 0.9244814515113831


Running epoch 1, step 1348, batch 300
Sampled inputs[:2]: tensor([[    0,  3561,   278,  ..., 37517,   278,  1090],
        [    0,    13,  1107,  ...,   287, 25185,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0234e-04,  8.0893e-05, -5.1848e-04,  ...,  1.7930e-04,
         -5.7583e-05, -2.4661e-04],
        [-7.7710e-06, -5.7146e-06,  4.7348e-06,  ..., -6.7428e-06,
         -5.0813e-06, -5.7295e-06],
        [-2.1368e-05, -1.6168e-05,  1.3903e-05,  ..., -1.8328e-05,
         -1.3843e-05, -1.5527e-05],
        [-1.4037e-05, -1.0125e-05,  8.8587e-06,  ..., -1.2159e-05,
         -9.2238e-06, -1.0803e-05],
        [-2.0042e-05, -1.5661e-05,  1.2785e-05,  ..., -1.7554e-05,
         -1.3679e-05, -1.3620e-05]], device='cuda:0')
Loss: 0.9595701694488525


Running epoch 1, step 1349, batch 301
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   199,   769, 18432],
        [    0,  1128,   292,  ...,  1485,   287, 11833]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4534e-04,  2.1054e-04, -5.5612e-04,  ...,  1.6460e-04,
         -9.1605e-05, -2.2411e-04],
        [-9.3058e-06, -6.8545e-06,  5.6438e-06,  ..., -8.0615e-06,
         -6.1765e-06, -6.8173e-06],
        [-2.5511e-05, -1.9372e-05,  1.6540e-05,  ..., -2.1890e-05,
         -1.6779e-05, -1.8477e-05],
        [-1.6928e-05, -1.2226e-05,  1.0625e-05,  ..., -1.4648e-05,
         -1.1310e-05, -1.2979e-05],
        [-2.3887e-05, -1.8731e-05,  1.5184e-05,  ..., -2.0936e-05,
         -1.6540e-05, -1.6168e-05]], device='cuda:0')
Loss: 0.9781808257102966


Running epoch 1, step 1350, batch 302
Sampled inputs[:2]: tensor([[    0,    14, 38591,  ...,   955,   892,  1635],
        [    0,  1943,   300,  ..., 43803,   368,  2400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4540e-04,  2.8566e-04, -5.4422e-04,  ...,  3.4074e-04,
         -1.3840e-04, -2.2090e-04],
        [-1.0788e-05, -7.9796e-06,  6.6422e-06,  ..., -9.3058e-06,
         -7.0892e-06, -7.8306e-06],
        [-2.9624e-05, -2.2590e-05,  1.9506e-05,  ..., -2.5317e-05,
         -1.9267e-05, -2.1279e-05],
        [-1.9729e-05, -1.4313e-05,  1.2591e-05,  ..., -1.6987e-05,
         -1.3046e-05, -1.5005e-05],
        [-2.7463e-05, -2.1666e-05,  1.7703e-05,  ..., -2.4006e-05,
         -1.8880e-05, -1.8418e-05]], device='cuda:0')
Loss: 0.9733297228813171


Running epoch 1, step 1351, batch 303
Sampled inputs[:2]: tensor([[    0,   446,   475,  ...,   300,   729, 11566],
        [    0,   709,   630,  ...,  6263,   409,   508]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8568e-04,  3.0399e-04, -5.6373e-04,  ...,  4.6461e-04,
         -3.5470e-04, -2.2797e-04],
        [-1.2375e-05, -9.0823e-06,  7.7896e-06,  ..., -1.0535e-05,
         -8.0168e-06, -8.8811e-06],
        [-3.3915e-05, -2.5749e-05,  2.2814e-05,  ..., -2.8685e-05,
         -2.1860e-05, -2.4170e-05],
        [-2.2709e-05, -1.6384e-05,  1.4856e-05,  ..., -1.9297e-05,
         -1.4827e-05, -1.7092e-05],
        [-3.1099e-05, -2.4483e-05,  2.0459e-05,  ..., -2.6956e-05,
         -2.1264e-05, -2.0713e-05]], device='cuda:0')
Loss: 0.9677761197090149
Graident accumulation at epoch 1, step 1351, batch 303
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0025,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0231,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.0422e-04,  9.1199e-05, -1.6571e-04,  ...,  4.5724e-05,
          1.5192e-04,  9.1452e-05],
        [-1.2588e-05, -8.5985e-06,  8.4534e-06,  ..., -1.0174e-05,
         -7.8727e-06, -8.5073e-06],
        [ 1.5909e-05,  1.5928e-05, -1.6927e-05,  ...,  1.2206e-05,
          2.3629e-05,  5.8649e-06],
        [ 4.0167e-05,  6.1118e-05, -2.8768e-05,  ...,  4.6183e-05,
          4.8004e-05,  2.6129e-05],
        [-1.9319e-05, -9.9786e-06,  1.3081e-05,  ..., -1.5173e-05,
         -1.3551e-05, -1.2081e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8735e-08, 5.1967e-08, 5.1341e-08,  ..., 1.8524e-08, 1.1952e-07,
         4.2236e-08],
        [8.2527e-11, 5.0499e-11, 2.0318e-11,  ..., 5.7088e-11, 1.9374e-11,
         2.4628e-11],
        [1.8525e-09, 1.1970e-09, 4.9502e-10,  ..., 1.3926e-09, 6.6540e-10,
         4.7226e-10],
        [8.6556e-10, 1.0026e-09, 3.6854e-10,  ..., 8.5235e-10, 5.9395e-10,
         3.5860e-10],
        [4.0960e-10, 2.3752e-10, 7.9832e-11,  ..., 2.9821e-10, 7.8915e-11,
         1.1877e-10]], device='cuda:0')
optimizer state dict: 169.0
lr: [5.9117898897731115e-06, 5.9117898897731115e-06]
scheduler_last_epoch: 169


Running epoch 1, step 1352, batch 304
Sampled inputs[:2]: tensor([[    0, 14652,    12,  ..., 17330,   996,  3294],
        [    0,   292,   474,  ...,  1085,   494,  2665]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9562e-05,  5.8492e-05, -9.0012e-05,  ...,  2.1472e-05,
         -1.0773e-05, -9.7290e-06],
        [-1.4529e-06, -1.0654e-06,  9.2015e-07,  ..., -1.2740e-06,
         -9.2760e-07, -1.0505e-06],
        [-3.8445e-06, -2.9057e-06,  2.6375e-06,  ..., -3.3230e-06,
         -2.4438e-06, -2.7269e-06],
        [-2.7269e-06, -1.9521e-06,  1.8105e-06,  ..., -2.3842e-06,
         -1.7509e-06, -2.0862e-06],
        [-3.5316e-06, -2.7865e-06,  2.3842e-06,  ..., -3.1143e-06,
         -2.3842e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9386765956878662


Running epoch 1, step 1353, batch 305
Sampled inputs[:2]: tensor([[    0,   452,    13,  ...,   358,    13, 12347],
        [    0,  2383,  9843,  ...,   401,  3959,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0882e-05,  1.7973e-04, -3.4131e-05,  ...,  1.1070e-05,
          2.7089e-05, -5.0771e-05],
        [-2.9728e-06, -2.1383e-06,  1.9409e-06,  ..., -2.5928e-06,
         -1.8701e-06, -2.1309e-06],
        [-7.8976e-06, -5.8413e-06,  5.5432e-06,  ..., -6.7949e-06,
         -4.9174e-06, -5.5879e-06],
        [-5.5730e-06, -3.9041e-06,  3.8072e-06,  ..., -4.8429e-06,
         -3.5018e-06, -4.2170e-06],
        [-7.2271e-06, -5.5581e-06,  4.9472e-06,  ..., -6.3479e-06,
         -4.7833e-06, -4.7088e-06]], device='cuda:0')
Loss: 0.982846200466156


Running epoch 1, step 1354, batch 306
Sampled inputs[:2]: tensor([[    0, 14576,  6617,  ...,    17,   367,  1608],
        [    0,  3086,   504,  ...,    14,   759,   935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.3784e-05,  4.1403e-04,  1.9748e-04,  ...,  7.3651e-05,
         -4.9597e-06, -2.2364e-04],
        [-4.4778e-06, -3.3379e-06,  2.9020e-06,  ..., -3.9041e-06,
         -2.9504e-06, -3.1814e-06],
        [-1.1861e-05, -9.1344e-06,  8.2552e-06,  ..., -1.0252e-05,
         -7.7784e-06, -8.4192e-06],
        [-8.4043e-06, -6.1393e-06,  5.6773e-06,  ..., -7.3165e-06,
         -5.5730e-06, -6.3330e-06],
        [-1.0878e-05, -8.6874e-06,  7.3910e-06,  ..., -9.5963e-06,
         -7.5847e-06, -7.1079e-06]], device='cuda:0')
Loss: 0.956156313419342


Running epoch 1, step 1355, batch 307
Sampled inputs[:2]: tensor([[    0,   380,  6119,  ..., 11823,   287,  6797],
        [    0,  9058,  4048,  ...,    14,   759,  1403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5227e-04,  4.6703e-04,  1.4450e-04,  ...,  4.1241e-05,
         -1.1907e-04, -3.2122e-04],
        [-5.9828e-06, -4.4182e-06,  4.0568e-06,  ..., -5.1782e-06,
         -3.8967e-06, -4.2170e-06],
        [-1.5825e-05, -1.2085e-05,  1.1474e-05,  ..., -1.3605e-05,
         -1.0267e-05, -1.1176e-05],
        [-1.1206e-05, -8.1360e-06,  7.9274e-06,  ..., -9.6858e-06,
         -7.3388e-06, -8.3745e-06],
        [-1.4395e-05, -1.1444e-05,  1.0177e-05,  ..., -1.2651e-05,
         -9.9540e-06, -9.3728e-06]], device='cuda:0')
Loss: 0.9807647466659546


Running epoch 1, step 1356, batch 308
Sampled inputs[:2]: tensor([[    0,   352,   357,  ...,   461,   654, 19725],
        [    0,  1581, 11884,  ...,  7031,   689,   527]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9127e-04,  5.3485e-04,  1.3038e-04,  ..., -6.2172e-05,
         -2.7066e-04, -5.7411e-04],
        [-7.5176e-06, -5.5209e-06,  5.1744e-06,  ..., -6.4522e-06,
         -4.8392e-06, -5.2825e-06],
        [ 7.3315e-05,  3.7903e-05, -4.7054e-05,  ...,  6.6075e-05,
          3.3645e-06,  1.1228e-05],
        [-1.4052e-05, -1.0133e-05,  1.0073e-05,  ..., -1.2040e-05,
         -9.0674e-06, -1.0461e-05],
        [-1.8090e-05, -1.4275e-05,  1.2949e-05,  ..., -1.5765e-05,
         -1.2353e-05, -1.1772e-05]], device='cuda:0')
Loss: 0.9570872783660889


Running epoch 1, step 1357, batch 309
Sampled inputs[:2]: tensor([[   0, 4868, 1027,  ...,  409, 3047, 2953],
        [   0, 2587,   27,  ...,  259, 2462, 1220]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6968e-04,  5.6719e-04,  9.7449e-05,  ...,  2.0029e-06,
         -2.9832e-04, -5.4781e-04],
        [-9.0897e-06, -6.7279e-06,  6.1281e-06,  ..., -7.8455e-06,
         -5.8822e-06, -6.5044e-06],
        [ 6.9112e-05,  3.4595e-05, -4.4313e-05,  ...,  6.2394e-05,
          5.9292e-07,  8.0094e-06],
        [-1.6868e-05, -1.2264e-05,  1.1832e-05,  ..., -1.4544e-05,
         -1.0952e-05, -1.2726e-05],
        [-2.2024e-05, -1.7405e-05,  1.5467e-05,  ..., -1.9252e-05,
         -1.5065e-05, -1.4573e-05]], device='cuda:0')
Loss: 0.9825087785720825


Running epoch 1, step 1358, batch 310
Sampled inputs[:2]: tensor([[   0,  650,   14,  ..., 6330,  221,  494],
        [   0, 1858,  499,  ...,   14, 1032,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9479e-05,  8.2692e-04,  1.4540e-04,  ...,  1.1976e-05,
         -4.8864e-04, -5.4781e-04],
        [-1.0647e-05, -7.8380e-06,  6.8471e-06,  ..., -9.2983e-06,
         -7.0743e-06, -7.8380e-06],
        [ 6.4970e-05,  3.1585e-05, -4.2182e-05,  ...,  5.8595e-05,
         -2.5363e-06,  4.5225e-06],
        [-1.9684e-05, -1.4231e-05,  1.3143e-05,  ..., -1.7196e-05,
         -1.3173e-05, -1.5244e-05],
        [-2.6286e-05, -2.0504e-05,  1.7643e-05,  ..., -2.3186e-05,
         -1.8328e-05, -1.7926e-05]], device='cuda:0')
Loss: 0.9404796957969666


Running epoch 1, step 1359, batch 311
Sampled inputs[:2]: tensor([[    0,   300,   259,  ...,   352, 12080,   634],
        [    0,   638,  2708,  ..., 28492,  1814,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2722e-04,  8.2671e-04,  1.5020e-04,  ...,  5.2808e-05,
         -5.5347e-04, -6.2066e-04],
        [-1.2212e-05, -8.9705e-06,  7.9498e-06,  ..., -1.0572e-05,
         -7.9721e-06, -8.9258e-06],
        [ 6.0798e-05,  2.8486e-05, -3.9053e-05,  ...,  5.5227e-05,
         -4.9354e-06,  1.6168e-06],
        [-2.2560e-05, -1.6287e-05,  1.5259e-05,  ..., -1.9521e-05,
         -1.4819e-05, -1.7345e-05],
        [-3.0041e-05, -2.3440e-05,  2.0415e-05,  ..., -2.6315e-05,
         -2.0668e-05, -2.0355e-05]], device='cuda:0')
Loss: 0.9692723155021667
Graident accumulation at epoch 1, step 1359, batch 311
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.1072e-05,  1.6475e-04, -1.3412e-04,  ...,  4.6432e-05,
          8.1377e-05,  2.0240e-05],
        [-1.2551e-05, -8.6357e-06,  8.4030e-06,  ..., -1.0214e-05,
         -7.8827e-06, -8.5491e-06],
        [ 2.0398e-05,  1.7184e-05, -1.9139e-05,  ...,  1.6508e-05,
          2.0773e-05,  5.4401e-06],
        [ 3.3894e-05,  5.3377e-05, -2.4365e-05,  ...,  3.9612e-05,
          4.1722e-05,  2.1782e-05],
        [-2.0391e-05, -1.1325e-05,  1.3815e-05,  ..., -1.6288e-05,
         -1.4263e-05, -1.2908e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8693e-08, 5.2598e-08, 5.1313e-08,  ..., 1.8509e-08, 1.1970e-07,
         4.2579e-08],
        [8.2594e-11, 5.0528e-11, 2.0360e-11,  ..., 5.7142e-11, 1.9418e-11,
         2.4683e-11],
        [1.8543e-09, 1.1966e-09, 4.9605e-10,  ..., 1.3942e-09, 6.6476e-10,
         4.7179e-10],
        [8.6521e-10, 1.0019e-09, 3.6841e-10,  ..., 8.5188e-10, 5.9358e-10,
         3.5854e-10],
        [4.1009e-10, 2.3784e-10, 8.0169e-11,  ..., 2.9860e-10, 7.9263e-11,
         1.1906e-10]], device='cuda:0')
optimizer state dict: 170.0
lr: [5.799290794242787e-06, 5.799290794242787e-06]
scheduler_last_epoch: 170


Running epoch 1, step 1360, batch 312
Sampled inputs[:2]: tensor([[   0, 3380, 1197,  ...,  631,  369, 3123],
        [   0, 1485,  271,  ..., 6359, 1799, 5442]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2522e-04,  3.3645e-05, -8.5725e-06,  ..., -3.3911e-05,
          0.0000e+00, -1.5125e-04],
        [-1.5348e-06, -1.1176e-06,  1.1101e-06,  ..., -1.2517e-06,
         -9.3877e-07, -1.0505e-06],
        [-4.0531e-06, -3.0845e-06,  3.0994e-06,  ..., -3.3081e-06,
         -2.5183e-06, -2.8163e-06],
        [-2.8163e-06, -2.0415e-06,  2.1309e-06,  ..., -2.2799e-06,
         -1.7360e-06, -2.0564e-06],
        [-3.6359e-06, -2.9057e-06,  2.7269e-06,  ..., -3.0696e-06,
         -2.4289e-06, -2.3544e-06]], device='cuda:0')
Loss: 0.9691933989524841


Running epoch 1, step 1361, batch 313
Sampled inputs[:2]: tensor([[    0,   266, 30368,  ...,   950,   266,  1868],
        [    0,   300,  5864,  ...,    12,  3667,   796]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7694e-04, -7.2214e-05,  7.1021e-05,  ..., -5.0171e-05,
         -1.0788e-04, -2.4672e-04],
        [-3.0994e-06, -2.2054e-06,  2.0713e-06,  ..., -2.6226e-06,
         -1.9819e-06, -2.2575e-06],
        [-8.3745e-06, -6.1989e-06,  5.9307e-06,  ..., -7.0184e-06,
         -5.3644e-06, -6.0946e-06],
        [-5.6028e-06, -3.9339e-06,  3.8818e-06,  ..., -4.7088e-06,
         -3.5763e-06, -4.2617e-06],
        [-7.5996e-06, -5.8264e-06,  5.2601e-06,  ..., -6.5267e-06,
         -5.1558e-06, -5.1558e-06]], device='cuda:0')
Loss: 0.963959276676178


Running epoch 1, step 1362, batch 314
Sampled inputs[:2]: tensor([[   0, 9611,  278,  ...,  278,  638,  600],
        [   0, 4258,  717,  ...,   34,  609, 1169]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0390e-04, -4.4247e-05,  1.5096e-04,  ..., -6.1792e-05,
          5.2272e-05, -8.9907e-06],
        [-4.6641e-06, -3.3230e-06,  3.0249e-06,  ..., -4.0159e-06,
         -3.0696e-06, -3.4794e-06],
        [-1.2517e-05, -9.2834e-06,  8.6576e-06,  ..., -1.0669e-05,
         -8.2403e-06, -9.2238e-06],
        [-8.3447e-06, -5.8562e-06,  5.6103e-06,  ..., -7.1526e-06,
         -5.5134e-06, -6.4820e-06],
        [-1.1504e-05, -8.8066e-06,  7.7933e-06,  ..., -1.0028e-05,
         -7.9870e-06, -7.9125e-06]], device='cuda:0')
Loss: 0.9719796776771545


Running epoch 1, step 1363, batch 315
Sampled inputs[:2]: tensor([[    0,  1985,   278,  ...,   677, 12292, 17956],
        [    0,   391,  4356,  ...,   287, 32873,  5362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4663e-04, -2.8274e-05,  2.3554e-05,  ..., -1.5339e-05,
         -1.8639e-04, -2.4754e-04],
        [-6.0946e-06, -4.3958e-06,  4.0010e-06,  ..., -5.2601e-06,
         -4.0829e-06, -4.5225e-06],
        [-1.6272e-05, -1.2174e-05,  1.1414e-05,  ..., -1.3888e-05,
         -1.0893e-05, -1.1906e-05],
        [-1.0997e-05, -7.7933e-06,  7.5176e-06,  ..., -9.4622e-06,
         -7.4208e-06, -8.5384e-06],
        [-1.5080e-05, -1.1638e-05,  1.0356e-05,  ..., -1.3143e-05,
         -1.0625e-05, -1.0267e-05]], device='cuda:0')
Loss: 0.958012044429779


Running epoch 1, step 1364, batch 316
Sampled inputs[:2]: tensor([[   0, 2923,  266,  ..., 7763,  360, 1255],
        [   0, 1850,  311,  ..., 3655, 3133, 9000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3420e-04, -8.5612e-05,  7.6088e-05,  ...,  6.0920e-06,
         -1.5578e-04, -2.7909e-04],
        [-7.5921e-06, -5.6103e-06,  4.9286e-06,  ..., -6.5714e-06,
         -5.1111e-06, -5.5805e-06],
        [ 2.6609e-04,  3.5027e-04, -2.0695e-04,  ...,  2.1279e-04,
          2.5945e-04,  6.0391e-05],
        [-1.3843e-05, -1.0103e-05,  9.3430e-06,  ..., -1.1981e-05,
         -9.4175e-06, -1.0684e-05],
        [-1.8731e-05, -1.4797e-05,  1.2755e-05,  ..., -1.6391e-05,
         -1.3247e-05, -1.2666e-05]], device='cuda:0')
Loss: 1.0096913576126099


Running epoch 1, step 1365, batch 317
Sampled inputs[:2]: tensor([[   0,  287, 4579,  ...,  909,   12,  344],
        [   0,  278, 7524,  ..., 1288,  669,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9321e-05, -1.7191e-04,  1.7830e-05,  ...,  1.3281e-05,
         -2.7481e-04, -3.6376e-04],
        [-9.0003e-06, -6.5789e-06,  5.8748e-06,  ..., -7.8902e-06,
         -6.0722e-06, -6.7279e-06],
        [ 2.6246e-04,  3.4782e-04, -2.0430e-04,  ...,  2.0956e-04,
          2.5712e-04,  5.7589e-05],
        [-1.6481e-05, -1.1832e-05,  1.1183e-05,  ..., -1.4424e-05,
         -1.1176e-05, -1.2919e-05],
        [-2.2218e-05, -1.7226e-05,  1.5259e-05,  ..., -1.9535e-05,
         -1.5602e-05, -1.5125e-05]], device='cuda:0')
Loss: 0.9727328419685364


Running epoch 1, step 1366, batch 318
Sampled inputs[:2]: tensor([[   0,  266, 1403,  ..., 5145,  266, 3470],
        [   0,  638, 1276,  ..., 1589, 2432,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4791e-05, -2.7571e-04, -8.8365e-05,  ...,  8.1317e-05,
         -5.4652e-04, -4.9708e-04],
        [-1.0490e-05, -7.6368e-06,  6.9104e-06,  ..., -9.1121e-06,
         -6.9663e-06, -7.7784e-06],
        [ 2.5835e-04,  3.4484e-04, -2.0129e-04,  ...,  2.0621e-04,
          2.5470e-04,  5.4684e-05],
        [-1.9282e-05, -1.3769e-05,  1.3210e-05,  ..., -1.6704e-05,
         -1.2830e-05, -1.5005e-05],
        [-2.5824e-05, -1.9938e-05,  1.7852e-05,  ..., -2.2545e-05,
         -1.7881e-05, -1.7479e-05]], device='cuda:0')
Loss: 0.9635537266731262


Running epoch 1, step 1367, batch 319
Sampled inputs[:2]: tensor([[    0,   369,   726,  ...,    83,   409,   729],
        [    0, 13706,  1862,  ...,   275,  1036, 42948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9554e-04, -3.2351e-04, -1.8208e-04,  ...,  3.5680e-05,
         -5.2260e-04, -7.2770e-04],
        [-1.1958e-05, -8.6576e-06,  7.9907e-06,  ..., -1.0327e-05,
         -7.8529e-06, -8.8438e-06],
        [ 2.5441e-04,  3.4203e-04, -1.9819e-04,  ...,  2.0300e-04,
          2.5236e-04,  5.1852e-05],
        [-2.2084e-05, -1.5676e-05,  1.5400e-05,  ..., -1.9014e-05,
         -1.4521e-05, -1.7181e-05],
        [-2.9266e-05, -2.2516e-05,  2.0489e-05,  ..., -2.5421e-05,
         -2.0072e-05, -1.9759e-05]], device='cuda:0')
Loss: 0.9694637060165405
Graident accumulation at epoch 1, step 1367, batch 319
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-5.3411e-05,  1.1592e-04, -1.3891e-04,  ...,  4.5357e-05,
          2.0980e-05, -5.4554e-05],
        [-1.2491e-05, -8.6379e-06,  8.3618e-06,  ..., -1.0225e-05,
         -7.8797e-06, -8.5786e-06],
        [ 4.3800e-05,  4.9668e-05, -3.7044e-05,  ...,  3.5158e-05,
          4.3931e-05,  1.0081e-05],
        [ 2.8296e-05,  4.6472e-05, -2.0389e-05,  ...,  3.3750e-05,
          3.6098e-05,  1.7885e-05],
        [-2.1278e-05, -1.2444e-05,  1.4482e-05,  ..., -1.7201e-05,
         -1.4844e-05, -1.3593e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8672e-08, 5.2651e-08, 5.1294e-08,  ..., 1.8491e-08, 1.1986e-07,
         4.3066e-08],
        [8.2654e-11, 5.0553e-11, 2.0404e-11,  ..., 5.7192e-11, 1.9460e-11,
         2.4737e-11],
        [1.9172e-09, 1.3124e-09, 5.3484e-10,  ..., 1.4340e-09, 7.2778e-10,
         4.7401e-10],
        [8.6483e-10, 1.0011e-09, 3.6828e-10,  ..., 8.5139e-10, 5.9320e-10,
         3.5848e-10],
        [4.1054e-10, 2.3810e-10, 8.0509e-11,  ..., 2.9895e-10, 7.9587e-11,
         1.1933e-10]], device='cuda:0')
optimizer state dict: 171.0
lr: [5.687433603747612e-06, 5.687433603747612e-06]
scheduler_last_epoch: 171
Epoch 1 | Batch 319/1048 | Training PPL: 2334.8656900614005 | time 29.876717567443848
Saving checkpoint at epoch 1, step 1367, batch 319
Epoch 1 | Validation PPL: 6.887492257383025 | Learning rate: 5.687433603747612e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1367, AFTER epoch 1, step 1367


Running epoch 1, step 1368, batch 320
Sampled inputs[:2]: tensor([[    0, 33119,   391,  ...,   292,  4462,  2721],
        [    0,   341,   298,  ...,   298,  1304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3171e-05,  2.0207e-04,  1.1801e-04,  ...,  4.9925e-05,
         -3.4487e-04, -1.6099e-05],
        [-1.5795e-06, -1.0580e-06,  8.9407e-07,  ..., -1.3486e-06,
         -1.0282e-06, -1.2517e-06],
        [-4.2915e-06, -3.0100e-06,  2.6077e-06,  ..., -3.6657e-06,
         -2.8312e-06, -3.4124e-06],
        [-2.8014e-06, -1.8626e-06,  1.6466e-06,  ..., -2.3991e-06,
         -1.8626e-06, -2.3097e-06],
        [-4.1127e-06, -2.9504e-06,  2.4140e-06,  ..., -3.5912e-06,
         -2.8610e-06, -3.0994e-06]], device='cuda:0')
Loss: 0.9355139136314392


Running epoch 1, step 1369, batch 321
Sampled inputs[:2]: tensor([[    0, 13576,   431,  ...,    14,   475,   298],
        [    0,  7638,   720,  ...,  3059, 10777,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4977e-04,  1.4843e-04,  7.3014e-05,  ...,  9.2423e-05,
         -1.8694e-04, -1.0467e-05],
        [-3.0696e-06, -2.1979e-06,  1.8254e-06,  ..., -2.6599e-06,
         -2.0936e-06, -2.3693e-06],
        [-8.2850e-06, -6.1095e-06,  5.2899e-06,  ..., -7.0930e-06,
         -5.5581e-06, -6.3181e-06],
        [-5.5730e-06, -3.9190e-06,  3.4124e-06,  ..., -4.8280e-06,
         -3.8594e-06, -4.5151e-06],
        [-7.8827e-06, -5.9754e-06,  4.9025e-06,  ..., -6.8843e-06,
         -5.5581e-06, -5.6475e-06]], device='cuda:0')
Loss: 1.0051957368850708


Running epoch 1, step 1370, batch 322
Sampled inputs[:2]: tensor([[   0,  368, 2418,  ..., 3275, 1116, 5189],
        [   0,   14,  747,  ..., 2039,  287, 8053]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0236e-04,  1.6871e-04,  4.0760e-05,  ...,  3.1898e-05,
         -1.0446e-04,  4.9365e-05],
        [-4.5896e-06, -3.4273e-06,  2.8461e-06,  ..., -3.9637e-06,
         -3.1590e-06, -3.4720e-06],
        [-1.2338e-05, -9.5367e-06,  8.1807e-06,  ..., -1.0610e-05,
         -8.4639e-06, -9.2983e-06],
        [-8.3745e-06, -6.1840e-06,  5.3644e-06,  ..., -7.2271e-06,
         -5.8264e-06, -6.6459e-06],
        [-1.1608e-05, -9.2387e-06,  7.5251e-06,  ..., -1.0192e-05,
         -8.3894e-06, -8.2254e-06]], device='cuda:0')
Loss: 1.0054234266281128


Running epoch 1, step 1371, batch 323
Sampled inputs[:2]: tensor([[   0,  278, 2097,  ..., 1754,  287,  631],
        [   0,  328, 6379,  ...,  287, 1342,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0398e-04,  1.9463e-04, -4.5255e-05,  ...,  9.1869e-05,
         -2.3452e-04,  5.8908e-05],
        [-6.1169e-06, -4.5598e-06,  3.8743e-06,  ..., -5.2452e-06,
         -4.2170e-06, -4.6045e-06],
        [-1.6481e-05, -1.2651e-05,  1.1116e-05,  ..., -1.4037e-05,
         -1.1265e-05, -1.2353e-05],
        [-1.1280e-05, -8.2701e-06,  7.3761e-06,  ..., -9.6411e-06,
         -7.8231e-06, -8.9258e-06],
        [-1.5453e-05, -1.2249e-05,  1.0207e-05,  ..., -1.3456e-05,
         -1.1146e-05, -1.0848e-05]], device='cuda:0')
Loss: 0.9665471315383911


Running epoch 1, step 1372, batch 324
Sampled inputs[:2]: tensor([[    0,  1064,   266,  ...,  2971,   292,   474],
        [    0,    12,  1808,  ...,   847,   300, 44349]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3152e-04,  2.3401e-04,  1.8181e-05,  ...,  7.2984e-05,
         -6.0042e-05,  1.7601e-04],
        [-7.6592e-06, -5.7593e-06,  4.7311e-06,  ..., -6.6087e-06,
         -5.3793e-06, -5.7369e-06],
        [-2.0385e-05, -1.5885e-05,  1.3486e-05,  ..., -1.7539e-05,
         -1.4290e-05, -1.5214e-05],
        [-1.4007e-05, -1.0386e-05,  8.9481e-06,  ..., -1.2100e-05,
         -9.9540e-06, -1.1042e-05],
        [-1.9386e-05, -1.5572e-05,  1.2547e-05,  ..., -1.7032e-05,
         -1.4305e-05, -1.3530e-05]], device='cuda:0')
Loss: 0.9770652055740356


Running epoch 1, step 1373, batch 325
Sampled inputs[:2]: tensor([[   0,  281,   82,  ..., 2485,  417,  199],
        [   0,   13, 1311,  ...,  271,  795,  957]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6446e-04,  2.2368e-04, -1.0361e-05,  ...,  6.2660e-05,
          5.1856e-05,  2.5188e-04],
        [-9.1195e-06, -6.8769e-06,  5.6438e-06,  ..., -7.9647e-06,
         -6.4969e-06, -6.8322e-06],
        [-2.4229e-05, -1.8969e-05,  1.6063e-05,  ..., -2.1100e-05,
         -1.7181e-05, -1.8075e-05],
        [-1.6719e-05, -1.2428e-05,  1.0692e-05,  ..., -1.4648e-05,
         -1.2055e-05, -1.3217e-05],
        [-2.3097e-05, -1.8656e-05,  1.5005e-05,  ..., -2.0519e-05,
         -1.7226e-05, -1.6093e-05]], device='cuda:0')
Loss: 0.9677742719650269


Running epoch 1, step 1374, batch 326
Sampled inputs[:2]: tensor([[    0,   266,  4616,  ...,  1906,  7256,   287],
        [    0, 25009,   407,  ..., 13076,    13,  5226]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0332e-04,  1.7379e-05, -1.7257e-05,  ...,  1.0245e-04,
          6.4439e-05,  3.8367e-04],
        [-1.0632e-05, -7.9051e-06,  6.6198e-06,  ..., -9.2834e-06,
         -7.5176e-06, -8.0094e-06],
        [-2.8223e-05, -2.1771e-05,  1.8835e-05,  ..., -2.4498e-05,
         -1.9833e-05, -2.1115e-05],
        [-1.9550e-05, -1.4305e-05,  1.2591e-05,  ..., -1.7092e-05,
         -1.3962e-05, -1.5542e-05],
        [-2.6897e-05, -2.1428e-05,  1.7598e-05,  ..., -2.3842e-05,
         -1.9908e-05, -1.8805e-05]], device='cuda:0')
Loss: 0.9704461097717285


Running epoch 1, step 1375, batch 327
Sampled inputs[:2]: tensor([[   0,  685,  344,  ...,  680,  401,  616],
        [   0, 3119,  278,  ...,  352,  674,  369]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6626e-05,  1.0185e-04,  6.9357e-05,  ...,  3.9661e-05,
          3.3702e-06,  4.8260e-04],
        [-1.2189e-05, -8.9481e-06,  7.7300e-06,  ..., -1.0572e-05,
         -8.4490e-06, -9.1046e-06],
        [-3.2276e-05, -2.4617e-05,  2.1875e-05,  ..., -2.7865e-05,
         -2.2262e-05, -2.3976e-05],
        [-2.2486e-05, -1.6227e-05,  1.4767e-05,  ..., -1.9506e-05,
         -1.5706e-05, -1.7703e-05],
        [-3.0592e-05, -2.4155e-05,  2.0310e-05,  ..., -2.7001e-05,
         -2.2292e-05, -2.1264e-05]], device='cuda:0')
Loss: 0.9754754304885864
Graident accumulation at epoch 1, step 1375, batch 327
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.0407e-05,  1.1452e-04, -1.1809e-04,  ...,  4.4787e-05,
          1.9219e-05, -8.3864e-07],
        [-1.2461e-05, -8.6689e-06,  8.2986e-06,  ..., -1.0260e-05,
         -7.9366e-06, -8.6312e-06],
        [ 3.6192e-05,  4.2240e-05, -3.1152e-05,  ...,  2.8856e-05,
          3.7312e-05,  6.6756e-06],
        [ 2.3218e-05,  4.0202e-05, -1.6873e-05,  ...,  2.8424e-05,
          3.0917e-05,  1.4327e-05],
        [-2.2210e-05, -1.3615e-05,  1.5065e-05,  ..., -1.8181e-05,
         -1.5588e-05, -1.4360e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8620e-08, 5.2608e-08, 5.1248e-08,  ..., 1.8475e-08, 1.1974e-07,
         4.3256e-08],
        [8.2720e-11, 5.0582e-11, 2.0443e-11,  ..., 5.7246e-11, 1.9512e-11,
         2.4795e-11],
        [1.9163e-09, 1.3117e-09, 5.3478e-10,  ..., 1.4334e-09, 7.2755e-10,
         4.7411e-10],
        [8.6447e-10, 1.0004e-09, 3.6813e-10,  ..., 8.5092e-10, 5.9285e-10,
         3.5843e-10],
        [4.1106e-10, 2.3845e-10, 8.0841e-11,  ..., 2.9938e-10, 8.0004e-11,
         1.1967e-10]], device='cuda:0')
optimizer state dict: 172.0
lr: [5.576235411042707e-06, 5.576235411042707e-06]
scheduler_last_epoch: 172


Running epoch 1, step 1376, batch 328
Sampled inputs[:2]: tensor([[    0,   287, 19777,  ...,   266,  5061,   278],
        [    0, 38232,   446,  ...,   287,  2456, 29919]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0845e-04,  8.5040e-05,  9.2634e-05,  ..., -8.5845e-05,
          9.5219e-05,  1.8722e-05],
        [-1.5125e-06, -1.1548e-06,  8.0466e-07,  ..., -1.3933e-06,
         -1.2666e-06, -1.2219e-06],
        [-4.0829e-06, -3.2634e-06,  2.3693e-06,  ..., -3.7402e-06,
         -3.4124e-06, -3.2336e-06],
        [-2.6971e-06, -2.0415e-06,  1.4529e-06,  ..., -2.5183e-06,
         -2.3544e-06, -2.2799e-06],
        [-4.1723e-06, -3.3826e-06,  2.3842e-06,  ..., -3.8445e-06,
         -3.5614e-06, -3.0994e-06]], device='cuda:0')
Loss: 0.9863370060920715


Running epoch 1, step 1377, batch 329
Sampled inputs[:2]: tensor([[    0,  1254,  2921,  ...,  1888, 33569,  3201],
        [    0,    14,    28,  ..., 16032,   694,  1441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0736e-05,  1.1430e-04,  1.3786e-04,  ..., -7.0347e-05,
          9.5219e-05,  2.6468e-04],
        [-2.9877e-06, -2.3097e-06,  1.8775e-06,  ..., -2.6524e-06,
         -2.3171e-06, -2.2724e-06],
        [-8.0466e-06, -6.4820e-06,  5.4091e-06,  ..., -7.1228e-06,
         -6.2287e-06, -6.0797e-06],
        [-5.5432e-06, -4.2468e-06,  3.6135e-06,  ..., -4.9472e-06,
         -4.3809e-06, -4.4405e-06],
        [-7.8082e-06, -6.4671e-06,  5.1260e-06,  ..., -7.0184e-06,
         -6.3032e-06, -5.5283e-06]], device='cuda:0')
Loss: 1.0024504661560059


Running epoch 1, step 1378, batch 330
Sampled inputs[:2]: tensor([[    0,   300, 11040,  ...,   266,  1736,  3487],
        [    0,   221,   381,  ...,   360,  8978,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7464e-05,  1.6989e-04,  1.8061e-04,  ..., -1.4549e-04,
          2.7457e-04,  4.4007e-04],
        [-4.4703e-06, -3.3155e-06,  2.9653e-06,  ..., -3.9190e-06,
         -3.2447e-06, -3.3602e-06],
        [-1.1981e-05, -9.2238e-06,  8.4937e-06,  ..., -1.0461e-05,
         -8.6874e-06, -8.9407e-06],
        [-8.2403e-06, -6.0424e-06,  5.6997e-06,  ..., -7.2569e-06,
         -6.0722e-06, -6.5267e-06],
        [-1.1384e-05, -9.0599e-06,  7.8678e-06,  ..., -1.0103e-05,
         -8.6874e-06, -7.9274e-06]], device='cuda:0')
Loss: 0.974484920501709


Running epoch 1, step 1379, batch 331
Sampled inputs[:2]: tensor([[   0,  285,  590,  ...,  199,  395, 3523],
        [   0, 1016,  271,  ...,  461,  616,  993]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7606e-04,  1.6673e-04,  8.4205e-05,  ..., -2.0455e-04,
          5.6909e-04,  4.4007e-04],
        [-6.1244e-06, -4.2990e-06,  3.7998e-06,  ..., -5.4091e-06,
         -4.4145e-06, -4.7237e-06],
        [-1.5974e-05, -1.1787e-05,  1.0744e-05,  ..., -1.3992e-05,
         -1.1563e-05, -1.2085e-05],
        [-1.1146e-05, -7.7635e-06,  7.1973e-06,  ..., -9.8944e-06,
         -8.2031e-06, -8.9556e-06],
        [-1.5736e-05, -1.1802e-05,  1.0297e-05,  ..., -1.3977e-05,
         -1.1876e-05, -1.1176e-05]], device='cuda:0')
Loss: 0.9451249241828918


Running epoch 1, step 1380, batch 332
Sampled inputs[:2]: tensor([[    0, 17471,  7279,  ...,   328,  6179,   287],
        [    0,   825,  1243,  ...,    15,    22,    42]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3382e-04,  2.4705e-04,  2.0476e-04,  ..., -2.7520e-04,
          4.9208e-04,  5.8091e-04],
        [-7.6517e-06, -5.4538e-06,  4.7982e-06,  ..., -6.7353e-06,
         -5.5395e-06, -5.8264e-06],
        [-1.9997e-05, -1.4931e-05,  1.3530e-05,  ..., -1.7494e-05,
         -1.4529e-05, -1.5065e-05],
        [-1.3977e-05, -9.8646e-06,  9.1344e-06,  ..., -1.2338e-05,
         -1.0289e-05, -1.1116e-05],
        [-1.9640e-05, -1.4976e-05,  1.2919e-05,  ..., -1.7464e-05,
         -1.4946e-05, -1.3888e-05]], device='cuda:0')
Loss: 0.9969958662986755


Running epoch 1, step 1381, batch 333
Sampled inputs[:2]: tensor([[   0, 4448,   12,  ..., 3183,  328, 9559],
        [   0,  944,  278,  ..., 5755,  292,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8598e-04,  2.2476e-04,  2.8640e-04,  ..., -3.4756e-04,
          4.2589e-04,  6.1369e-04],
        [-9.0972e-06, -6.5714e-06,  5.5321e-06,  ..., -8.1435e-06,
         -6.7167e-06, -7.0035e-06],
        [-2.3931e-05, -1.8150e-05,  1.5721e-05,  ..., -2.1309e-05,
         -1.7747e-05, -1.8284e-05],
        [-1.6734e-05, -1.1966e-05,  1.0550e-05,  ..., -1.5065e-05,
         -1.2614e-05, -1.3500e-05],
        [-2.3484e-05, -1.8179e-05,  1.5020e-05,  ..., -2.1234e-05,
         -1.8179e-05, -1.6853e-05]], device='cuda:0')
Loss: 0.9698488712310791


Running epoch 1, step 1382, batch 334
Sampled inputs[:2]: tensor([[   0, 1360,   14,  ...,  287, 2429, 2498],
        [   0, 1594,  586,  ...,   13,  701,  308]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5171e-04,  1.7795e-04,  4.1641e-04,  ..., -3.7493e-04,
          4.5435e-04,  7.3521e-04],
        [-1.0602e-05, -7.6070e-06,  6.5304e-06,  ..., -9.4622e-06,
         -7.7672e-06, -8.1509e-06],
        [-2.7806e-05, -2.0906e-05,  1.8522e-05,  ..., -2.4676e-05,
         -2.0459e-05, -2.1219e-05],
        [-1.9491e-05, -1.3836e-05,  1.2472e-05,  ..., -1.7479e-05,
         -1.4551e-05, -1.5706e-05],
        [-2.7269e-05, -2.0966e-05,  1.7703e-05,  ..., -2.4602e-05,
         -2.0981e-05, -1.9521e-05]], device='cuda:0')
Loss: 0.949138343334198


Running epoch 1, step 1383, batch 335
Sampled inputs[:2]: tensor([[    0, 10334,    17,  ...,   391,  1566, 24837],
        [    0,  1603,   694,  ...,    36,    18,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9063e-04,  1.3129e-04,  4.6402e-04,  ..., -4.1223e-04,
          2.9880e-04,  8.9123e-04],
        [-1.2137e-05, -8.7395e-06,  7.3873e-06,  ..., -1.0870e-05,
         -8.9519e-06, -9.4175e-06],
        [-3.1710e-05, -2.3872e-05,  2.0906e-05,  ..., -2.8178e-05,
         -2.3410e-05, -2.4334e-05],
        [-2.2203e-05, -1.5803e-05,  1.4029e-05,  ..., -1.9982e-05,
         -1.6667e-05, -1.8045e-05],
        [-3.1322e-05, -2.4080e-05,  2.0161e-05,  ..., -2.8282e-05,
         -2.4155e-05, -2.2545e-05]], device='cuda:0')
Loss: 0.9728240966796875
Graident accumulation at epoch 1, step 1383, batch 335
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.5429e-05,  1.1619e-04, -5.9875e-05,  ..., -9.1437e-07,
          4.7177e-05,  8.8368e-05],
        [-1.2429e-05, -8.6760e-06,  8.2075e-06,  ..., -1.0321e-05,
         -8.0381e-06, -8.7098e-06],
        [ 2.9402e-05,  3.5629e-05, -2.5946e-05,  ...,  2.3152e-05,
          3.1239e-05,  3.5747e-06],
        [ 1.8676e-05,  3.4602e-05, -1.3783e-05,  ...,  2.3584e-05,
          2.6159e-05,  1.1089e-05],
        [-2.3121e-05, -1.4661e-05,  1.5575e-05,  ..., -1.9191e-05,
         -1.6445e-05, -1.5179e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8645e-08, 5.2573e-08, 5.1412e-08,  ..., 1.8626e-08, 1.1971e-07,
         4.4007e-08],
        [8.2785e-11, 5.0608e-11, 2.0477e-11,  ..., 5.7307e-11, 1.9573e-11,
         2.4859e-11],
        [1.9154e-09, 1.3110e-09, 5.3468e-10,  ..., 1.4327e-09, 7.2737e-10,
         4.7423e-10],
        [8.6410e-10, 9.9963e-10, 3.6796e-10,  ..., 8.5047e-10, 5.9254e-10,
         3.5840e-10],
        [4.1163e-10, 2.3879e-10, 8.1166e-11,  ..., 2.9988e-10, 8.0507e-11,
         1.2005e-10]], device='cuda:0')
optimizer state dict: 173.0
lr: [5.465713208182582e-06, 5.465713208182582e-06]
scheduler_last_epoch: 173


Running epoch 1, step 1384, batch 336
Sampled inputs[:2]: tensor([[   0,   12, 1250,  ...,  381, 1524, 2204],
        [   0, 2314,  516,  ..., 1871,   13, 1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1700e-04, -1.0579e-05,  1.2255e-04,  ...,  4.9613e-06,
          1.3148e-04,  2.0111e-04],
        [-1.4305e-06, -9.9093e-07,  1.0729e-06,  ..., -1.2517e-06,
         -9.4250e-07, -1.0133e-06],
        [-3.8147e-06, -2.7418e-06,  3.0398e-06,  ..., -3.3081e-06,
         -2.5034e-06, -2.6971e-06],
        [-2.7567e-06, -1.8403e-06,  2.1607e-06,  ..., -2.3991e-06,
         -1.7807e-06, -2.0564e-06],
        [-3.4422e-06, -2.5928e-06,  2.6822e-06,  ..., -3.0547e-06,
         -2.4140e-06, -2.2501e-06]], device='cuda:0')
Loss: 0.9693861603736877


Running epoch 1, step 1385, batch 337
Sampled inputs[:2]: tensor([[   0,  591,  953,  ..., 4118, 5750,  292],
        [   0,  508,  586,  ..., 6157, 3146, 7647]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1442e-04,  2.4607e-06, -4.7299e-05,  ..., -7.3996e-05,
          1.9286e-04,  1.2574e-04],
        [-2.9132e-06, -1.9893e-06,  2.0340e-06,  ..., -2.5630e-06,
         -2.0228e-06, -2.1234e-06],
        [-7.5549e-06, -5.4240e-06,  5.6773e-06,  ..., -6.5714e-06,
         -5.2154e-06, -5.4687e-06],
        [-5.4538e-06, -3.6135e-06,  3.9786e-06,  ..., -4.7833e-06,
         -3.7774e-06, -4.2021e-06],
        [-7.1824e-06, -5.3793e-06,  5.2601e-06,  ..., -6.3777e-06,
         -5.2601e-06, -4.7982e-06]], device='cuda:0')
Loss: 0.960071861743927


Running epoch 1, step 1386, batch 338
Sampled inputs[:2]: tensor([[    0,  1197, 12404,  ...,   287,   271,  4893],
        [    0,  1487,   409,  ...,  6979,  1273,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4022e-04, -3.5330e-05, -2.1253e-05,  ..., -1.1014e-04,
          2.6202e-04,  2.6274e-05],
        [-4.3809e-06, -3.1739e-06,  2.9840e-06,  ..., -3.8669e-06,
         -3.1628e-06, -3.2037e-06],
        [-1.1459e-05, -8.6725e-06,  8.3745e-06,  ..., -1.0014e-05,
         -8.2105e-06, -8.3297e-06],
        [-8.1956e-06, -5.7891e-06,  5.8040e-06,  ..., -7.2122e-06,
         -5.9083e-06, -6.3330e-06],
        [-1.0997e-05, -8.6725e-06,  7.8529e-06,  ..., -9.8050e-06,
         -8.3297e-06, -7.4059e-06]], device='cuda:0')
Loss: 0.9991973042488098


Running epoch 1, step 1387, batch 339
Sampled inputs[:2]: tensor([[    0,  5775,   292,  ...,  8671,  1339,   642],
        [    0,   923,  2583,  ..., 11385,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6540e-05, -1.3466e-04, -6.9285e-05,  ..., -1.2144e-04,
          3.0134e-04,  9.1629e-05],
        [-5.8785e-06, -4.1425e-06,  4.0121e-06,  ..., -5.1185e-06,
         -4.1239e-06, -4.3586e-06],
        [-1.5423e-05, -1.1355e-05,  1.1280e-05,  ..., -1.3322e-05,
         -1.0729e-05, -1.1355e-05],
        [-1.1086e-05, -7.5921e-06,  7.8455e-06,  ..., -9.6411e-06,
         -7.7784e-06, -8.7023e-06],
        [-1.4707e-05, -1.1280e-05,  1.0520e-05,  ..., -1.2934e-05,
         -1.0803e-05, -1.0043e-05]], device='cuda:0')
Loss: 0.9723548293113708


Running epoch 1, step 1388, batch 340
Sampled inputs[:2]: tensor([[    0,   271,  8278,  ...,   271,  8278,  3560],
        [    0,   981,    12,  ...,   266, 12907,  6670]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9160e-05, -5.1795e-05, -2.9218e-05,  ..., -1.0700e-04,
          1.7964e-04,  9.1629e-05],
        [-7.3537e-06, -5.1856e-06,  5.0627e-06,  ..., -6.3404e-06,
         -5.0850e-06, -5.4166e-06],
        [-1.9327e-05, -1.4186e-05,  1.4246e-05,  ..., -1.6525e-05,
         -1.3232e-05, -1.4156e-05],
        [-1.3873e-05, -9.4995e-06,  9.9167e-06,  ..., -1.1936e-05,
         -9.5814e-06, -1.0833e-05],
        [-1.8239e-05, -1.3977e-05,  1.3158e-05,  ..., -1.5914e-05,
         -1.3247e-05, -1.2383e-05]], device='cuda:0')
Loss: 0.9540681838989258


Running epoch 1, step 1389, batch 341
Sampled inputs[:2]: tensor([[   0, 4441, 1821,  ...,  642, 2310,   14],
        [   0,  565,   27,  ...,   88, 4451,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6832e-05, -1.5345e-04,  1.5833e-05,  ..., -1.4738e-04,
          2.1449e-05, -2.5369e-05],
        [-8.8811e-06, -6.2063e-06,  6.0052e-06,  ..., -7.6964e-06,
         -6.1132e-06, -6.6906e-06],
        [-2.3529e-05, -1.7062e-05,  1.7017e-05,  ..., -2.0176e-05,
         -1.6019e-05, -1.7583e-05],
        [-1.6615e-05, -1.1280e-05,  1.1653e-05,  ..., -1.4380e-05,
         -1.1437e-05, -1.3217e-05],
        [-2.2322e-05, -1.6853e-05,  1.5810e-05,  ..., -1.9535e-05,
         -1.6108e-05, -1.5512e-05]], device='cuda:0')
Loss: 0.9667837619781494


Running epoch 1, step 1390, batch 342
Sampled inputs[:2]: tensor([[   0, 3761,   12,  ...,   14,   22,  287],
        [   0,  741,  266,  ...,  271, 5166,  596]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6228e-04, -3.5590e-04, -6.8142e-05,  ..., -5.1023e-05,
         -8.1759e-06,  9.2405e-05],
        [-1.0364e-05, -7.1898e-06,  7.0259e-06,  ..., -8.9258e-06,
         -7.0520e-06, -7.8157e-06],
        [-2.7493e-05, -1.9789e-05,  1.9938e-05,  ..., -2.3425e-05,
         -1.8477e-05, -2.0549e-05],
        [-1.9461e-05, -1.3120e-05,  1.3679e-05,  ..., -1.6749e-05,
         -1.3240e-05, -1.5497e-05],
        [-2.5943e-05, -1.9431e-05,  1.8418e-05,  ..., -2.2560e-05,
         -1.8492e-05, -1.8016e-05]], device='cuda:0')
Loss: 0.9590402245521545


Running epoch 1, step 1391, batch 343
Sampled inputs[:2]: tensor([[   0, 1597,  278,  ...,   20,   38,  446],
        [   0,  829,  874,  ...,  292,  380,  759]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9692e-04, -3.1373e-04, -2.3016e-05,  ..., -3.3601e-05,
         -8.1759e-06,  3.0475e-04],
        [-1.1802e-05, -8.2403e-06,  7.9647e-06,  ..., -1.0170e-05,
         -8.1398e-06, -8.9407e-06],
        [-3.1263e-05, -2.2724e-05,  2.2590e-05,  ..., -2.6718e-05,
         -2.1383e-05, -2.3529e-05],
        [-2.2143e-05, -1.5073e-05,  1.5490e-05,  ..., -1.9103e-05,
         -1.5326e-05, -1.7732e-05],
        [-2.9504e-05, -2.2307e-05,  2.0877e-05,  ..., -2.5749e-05,
         -2.1398e-05, -2.0638e-05]], device='cuda:0')
Loss: 0.9582093358039856
Graident accumulation at epoch 1, step 1391, batch 343
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-3.9194e-05,  7.3201e-05, -5.6190e-05,  ..., -4.1830e-06,
          4.1642e-05,  1.1001e-04],
        [-1.2366e-05, -8.6324e-06,  8.1832e-06,  ..., -1.0306e-05,
         -8.0483e-06, -8.7329e-06],
        [ 2.3335e-05,  2.9793e-05, -2.1093e-05,  ...,  1.8165e-05,
          2.5977e-05,  8.6432e-07],
        [ 1.4594e-05,  2.9634e-05, -1.0856e-05,  ...,  1.9315e-05,
          2.2010e-05,  8.2072e-06],
        [-2.3759e-05, -1.5426e-05,  1.6105e-05,  ..., -1.9847e-05,
         -1.6940e-05, -1.5725e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8626e-08, 5.2619e-08, 5.1361e-08,  ..., 1.8608e-08, 1.1959e-07,
         4.4056e-08],
        [8.2841e-11, 5.0626e-11, 2.0520e-11,  ..., 5.7353e-11, 1.9620e-11,
         2.4914e-11],
        [1.9145e-09, 1.3102e-09, 5.3466e-10,  ..., 1.4320e-09, 7.2710e-10,
         4.7431e-10],
        [8.6372e-10, 9.9886e-10, 3.6783e-10,  ..., 8.4998e-10, 5.9218e-10,
         3.5836e-10],
        [4.1209e-10, 2.3905e-10, 8.1521e-11,  ..., 3.0024e-10, 8.0885e-11,
         1.2036e-10]], device='cuda:0')
optimizer state dict: 174.0
lr: [5.355883883924591e-06, 5.355883883924591e-06]
scheduler_last_epoch: 174


Running epoch 1, step 1392, batch 344
Sampled inputs[:2]: tensor([[   0, 2270, 3279,  ...,  380,  475,  768],
        [   0,  287,  955,  ...,  462, 3363, 1340]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2603e-05,  6.0012e-05, -1.7229e-04,  ..., -1.7875e-04,
         -7.6620e-05, -1.7366e-04],
        [-1.3635e-06, -1.0431e-06,  9.1270e-07,  ..., -1.2368e-06,
         -1.0431e-06, -1.0133e-06],
        [-3.6210e-06, -2.8759e-06,  2.6822e-06,  ..., -3.2336e-06,
         -2.7120e-06, -2.6375e-06],
        [-2.5630e-06, -1.9372e-06,  1.8254e-06,  ..., -2.3246e-06,
         -1.9819e-06, -2.0266e-06],
        [-3.4571e-06, -2.8461e-06,  2.4885e-06,  ..., -3.1441e-06,
         -2.7269e-06, -2.2948e-06]], device='cuda:0')
Loss: 0.9631301760673523


Running epoch 1, step 1393, batch 345
Sampled inputs[:2]: tensor([[    0,  2280,   344,  ...,   287,   266,  3344],
        [    0,   273,    14,  ...,   271,   266, 25408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0740e-04,  1.2291e-04, -1.9301e-04,  ..., -2.5309e-04,
          5.2652e-05, -1.1330e-04],
        [-2.8387e-06, -2.0638e-06,  1.9409e-06,  ..., -2.4736e-06,
         -2.0117e-06, -2.1160e-06],
        [-7.4953e-06, -5.6922e-06,  5.5730e-06,  ..., -6.4820e-06,
         -5.3048e-06, -5.5134e-06],
        [-5.2899e-06, -3.7998e-06,  3.8370e-06,  ..., -4.6194e-06,
         -3.8072e-06, -4.1723e-06],
        [-7.0781e-06, -5.5730e-06,  5.1409e-06,  ..., -6.2436e-06,
         -5.3048e-06, -4.7982e-06]], device='cuda:0')
Loss: 0.9642521739006042


Running epoch 1, step 1394, batch 346
Sampled inputs[:2]: tensor([[   0,  266, 2374,  ..., 1551,  518,  638],
        [   0, 4175,  437,  ..., 1700,   14,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6962e-04,  5.1422e-05, -3.9393e-04,  ..., -5.1930e-04,
          2.3989e-04, -1.5198e-04],
        [-4.2617e-06, -3.1441e-06,  3.0212e-06,  ..., -3.6359e-06,
         -2.9653e-06, -3.1590e-06],
        [-1.1295e-05, -8.6427e-06,  8.6427e-06,  ..., -9.5516e-06,
         -7.8529e-06, -8.2999e-06],
        [-7.9572e-06, -5.7966e-06,  5.9679e-06,  ..., -6.7800e-06,
         -5.6103e-06, -6.2585e-06],
        [-1.0505e-05, -8.3596e-06,  7.8529e-06,  ..., -9.0897e-06,
         -7.7784e-06, -7.1079e-06]], device='cuda:0')
Loss: 0.9788819551467896


Running epoch 1, step 1395, batch 347
Sampled inputs[:2]: tensor([[    0,  3440,  5745,  ...,   360,  4998,   654],
        [    0,  3773, 23452,  ..., 14393,  1121,   304]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8874e-04, -1.2872e-04, -4.4790e-04,  ..., -6.0242e-04,
          3.1828e-04, -1.8410e-04],
        [-5.6699e-06, -4.2766e-06,  4.0643e-06,  ..., -4.8205e-06,
         -3.9786e-06, -4.2170e-06],
        [-1.5169e-05, -1.1846e-05,  1.1683e-05,  ..., -1.2800e-05,
         -1.0610e-05, -1.1221e-05],
        [-1.0654e-05, -7.9274e-06,  8.0392e-06,  ..., -9.0301e-06,
         -7.5474e-06, -8.3894e-06],
        [-1.4096e-05, -1.1429e-05,  1.0595e-05,  ..., -1.2174e-05,
         -1.0505e-05, -9.6262e-06]], device='cuda:0')
Loss: 0.9821416735649109


Running epoch 1, step 1396, batch 348
Sampled inputs[:2]: tensor([[    0, 45050,   342,  ...,  3729,   287, 27888],
        [    0,  7779,    12,  ...,  1380, 10199,  1086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3497e-04, -2.3214e-04, -5.2804e-04,  ..., -6.3807e-04,
          2.1988e-04, -1.2653e-04],
        [-7.0781e-06, -5.2266e-06,  5.1595e-06,  ..., -5.9754e-06,
         -4.8839e-06, -5.3123e-06],
        [-1.8850e-05, -1.4424e-05,  1.4737e-05,  ..., -1.5780e-05,
         -1.2934e-05, -1.4052e-05],
        [-1.3351e-05, -9.7007e-06,  1.0245e-05,  ..., -1.1250e-05,
         -9.2834e-06, -1.0625e-05],
        [-1.7419e-05, -1.3873e-05,  1.3292e-05,  ..., -1.4946e-05,
         -1.2770e-05, -1.1981e-05]], device='cuda:0')
Loss: 0.9694725275039673


Running epoch 1, step 1397, batch 349
Sampled inputs[:2]: tensor([[    0, 41921,  1955,  ...,    75,   221,   334],
        [    0,  1070, 17816,  ...,  5547,  9966,   518]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0365e-04, -1.3089e-04, -6.4637e-04,  ..., -6.9391e-04,
          1.5511e-04, -1.8913e-04],
        [-8.5309e-06, -6.3367e-06,  6.1058e-06,  ..., -7.2867e-06,
         -5.9940e-06, -6.4224e-06],
        [-3.4425e-05,  9.9145e-05, -5.6717e-05,  ...,  6.5854e-05,
          8.1308e-05,  4.7631e-06],
        [-1.6034e-05, -1.1712e-05,  1.2048e-05,  ..., -1.3679e-05,
         -1.1355e-05, -1.2785e-05],
        [-2.1189e-05, -1.6898e-05,  1.5900e-05,  ..., -1.8343e-05,
         -1.5706e-05, -1.4558e-05]], device='cuda:0')
Loss: 0.999291718006134


Running epoch 1, step 1398, batch 350
Sampled inputs[:2]: tensor([[   0, 1883, 1090,  ...,  365, 1943,  298],
        [   0, 2785, 1061,  ..., 1194,  692, 4339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9856e-04, -3.2395e-04, -1.0105e-03,  ..., -6.6661e-04,
          1.8520e-04, -3.7969e-04],
        [-1.0014e-05, -7.1861e-06,  7.1339e-06,  ..., -8.5384e-06,
         -6.8992e-06, -7.6294e-06],
        [-3.8359e-05,  9.6790e-05, -5.3811e-05,  ...,  6.2576e-05,
          7.8924e-05,  1.6041e-06],
        [-1.8880e-05, -1.3292e-05,  1.4104e-05,  ..., -1.6078e-05,
         -1.3098e-05, -1.5229e-05],
        [-2.4900e-05, -1.9222e-05,  1.8582e-05,  ..., -2.1517e-05,
         -1.8105e-05, -1.7345e-05]], device='cuda:0')
Loss: 0.9611179232597351


Running epoch 1, step 1399, batch 351
Sampled inputs[:2]: tensor([[   0,  298,  894,  ..., 7605, 3220,  259],
        [   0,  292,  380,  ..., 6156,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9612e-04, -3.3817e-04, -1.2408e-03,  ..., -8.0085e-04,
          8.8873e-05, -4.3383e-04],
        [-1.1459e-05, -8.2813e-06,  7.9125e-06,  ..., -9.9093e-06,
         -8.0913e-06, -8.8215e-06],
        [-4.2144e-05,  9.3795e-05, -5.1502e-05,  ...,  5.9044e-05,
          7.5869e-05, -1.4358e-06],
        [-2.1487e-05, -1.5259e-05,  1.5542e-05,  ..., -1.8597e-05,
         -1.5318e-05, -1.7479e-05],
        [-2.8864e-05, -2.2411e-05,  2.0996e-05,  ..., -2.5243e-05,
         -2.1413e-05, -2.0295e-05]], device='cuda:0')
Loss: 0.955260694026947
Graident accumulation at epoch 1, step 1399, batch 351
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4338e-05,  3.2064e-05, -1.7465e-04,  ..., -8.3850e-05,
          4.6365e-05,  5.5623e-05],
        [-1.2275e-05, -8.5973e-06,  8.1561e-06,  ..., -1.0266e-05,
         -8.0526e-06, -8.7418e-06],
        [ 1.6788e-05,  3.6193e-05, -2.4134e-05,  ...,  2.2253e-05,
          3.0966e-05,  6.3431e-07],
        [ 1.0986e-05,  2.5145e-05, -8.2159e-06,  ...,  1.5524e-05,
          1.8278e-05,  5.6386e-06],
        [-2.4270e-05, -1.6125e-05,  1.6594e-05,  ..., -2.0387e-05,
         -1.7388e-05, -1.6182e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8813e-08, 5.2680e-08, 5.2850e-08,  ..., 1.9231e-08, 1.1948e-07,
         4.4200e-08],
        [8.2890e-11, 5.0643e-11, 2.0562e-11,  ..., 5.7394e-11, 1.9665e-11,
         2.4967e-11],
        [1.9143e-09, 1.3177e-09, 5.3677e-10,  ..., 1.4341e-09, 7.3213e-10,
         4.7384e-10],
        [8.6332e-10, 9.9809e-10, 3.6770e-10,  ..., 8.4948e-10, 5.9182e-10,
         3.5830e-10],
        [4.1251e-10, 2.3931e-10, 8.1880e-11,  ..., 3.0058e-10, 8.1262e-11,
         1.2065e-10]], device='cuda:0')
optimizer state dict: 175.0
lr: [5.246764221148206e-06, 5.246764221148206e-06]
scheduler_last_epoch: 175


Running epoch 1, step 1400, batch 352
Sampled inputs[:2]: tensor([[    0, 10766,  8311,  ...,   328,   957,  1231],
        [    0,   902, 11331,  ...,  1795,   365,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8879e-05,  6.4347e-05,  1.1950e-05,  ..., -6.3788e-07,
         -9.3365e-05, -5.6802e-05],
        [-1.4454e-06, -1.0133e-06,  1.0207e-06,  ..., -1.2815e-06,
         -1.0356e-06, -1.0803e-06],
        [-3.8147e-06, -2.8312e-06,  2.8908e-06,  ..., -3.3975e-06,
         -2.7716e-06, -2.8610e-06],
        [-2.7418e-06, -1.8924e-06,  2.0266e-06,  ..., -2.4289e-06,
         -1.9819e-06, -2.1756e-06],
        [-3.6061e-06, -2.8163e-06,  2.6673e-06,  ..., -3.2783e-06,
         -2.7865e-06, -2.5332e-06]], device='cuda:0')
Loss: 0.9756625294685364


Running epoch 1, step 1401, batch 353
Sampled inputs[:2]: tensor([[    0,     9,  9925,  ...,   527, 23286,  6062],
        [    0,  3179,   221,  ...,   910,   706,  1102]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6271e-04, -1.9601e-05, -1.8670e-05,  ...,  2.6293e-04,
         -2.2027e-04,  5.3964e-05],
        [-2.9579e-06, -1.8887e-06,  1.9707e-06,  ..., -2.6152e-06,
         -2.0787e-06, -2.3991e-06],
        [-7.8082e-06, -5.2601e-06,  5.5879e-06,  ..., -6.8247e-06,
         -5.4687e-06, -6.2138e-06],
        [-5.5432e-06, -3.4869e-06,  3.8445e-06,  ..., -4.9174e-06,
         -3.9488e-06, -4.6939e-06],
        [-7.5996e-06, -5.3048e-06,  5.3197e-06,  ..., -6.7502e-06,
         -5.6177e-06, -5.6773e-06]], device='cuda:0')
Loss: 0.9427341818809509


Running epoch 1, step 1402, batch 354
Sampled inputs[:2]: tensor([[   0, 8754,   14,  ..., 6125,  394,  927],
        [   0, 8353, 1842,  ...,   38,  643,  472]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3677e-04, -2.5629e-05,  1.9621e-06,  ...,  1.9564e-04,
         -3.1879e-04,  2.2538e-05],
        [-4.3809e-06, -2.9840e-06,  3.0212e-06,  ..., -3.8147e-06,
         -3.0771e-06, -3.4273e-06],
        [-1.1683e-05, -8.3745e-06,  8.6129e-06,  ..., -1.0103e-05,
         -8.2254e-06, -9.0450e-06],
        [-8.2999e-06, -5.5730e-06,  5.9605e-06,  ..., -7.2271e-06,
         -5.8711e-06, -6.7949e-06],
        [-1.1191e-05, -8.3297e-06,  8.0764e-06,  ..., -9.8795e-06,
         -8.3596e-06, -8.1360e-06]], device='cuda:0')
Loss: 0.9697602391242981


Running epoch 1, step 1403, batch 355
Sampled inputs[:2]: tensor([[    0,  6275,    12,  ...,  2027,  2887,   287],
        [    0, 25778,  3804,  ...,  2354,    12,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3930e-04, -1.0283e-05,  6.7104e-05,  ...,  1.9072e-04,
         -2.6244e-04,  2.2966e-04],
        [-5.8487e-06, -4.0568e-06,  3.9823e-06,  ..., -5.0738e-06,
         -4.1425e-06, -4.5151e-06],
        [-1.5616e-05, -1.1384e-05,  1.1384e-05,  ..., -1.3471e-05,
         -1.1072e-05, -1.1995e-05],
        [-1.1042e-05, -7.5698e-06,  7.8380e-06,  ..., -9.5963e-06,
         -7.8827e-06, -8.9705e-06],
        [-1.4931e-05, -1.1310e-05,  1.0669e-05,  ..., -1.3173e-05,
         -1.1235e-05, -1.0759e-05]], device='cuda:0')
Loss: 0.9981867074966431


Running epoch 1, step 1404, batch 356
Sampled inputs[:2]: tensor([[    0, 36122,  1085,  ...,  6231,     9,  7794],
        [    0,   749,     9,  ...,  2756,    14,  1062]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3995e-04, -1.0436e-04,  9.7424e-05,  ...,  2.0552e-04,
         -1.8541e-04,  2.0580e-04],
        [-7.2718e-06, -5.0850e-06,  5.0627e-06,  ..., -6.3255e-06,
         -5.1633e-06, -5.5507e-06],
        [-1.9357e-05, -1.4186e-05,  1.4380e-05,  ..., -1.6734e-05,
         -1.3754e-05, -1.4693e-05],
        [-1.3739e-05, -9.4920e-06,  9.9689e-06,  ..., -1.1966e-05,
         -9.8199e-06, -1.1042e-05],
        [-1.8492e-05, -1.4067e-05,  1.3486e-05,  ..., -1.6332e-05,
         -1.3933e-05, -1.3143e-05]], device='cuda:0')
Loss: 1.0229424238204956


Running epoch 1, step 1405, batch 357
Sampled inputs[:2]: tensor([[   0, 6909,  352,  ..., 1075,  706, 6909],
        [   0,  278, 4452,  ...,   14,   18, 3046]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8503e-05, -2.2709e-04,  3.6756e-06,  ...,  2.8753e-04,
         -1.8725e-04,  4.0717e-04],
        [-8.7619e-06, -6.1132e-06,  6.0685e-06,  ..., -7.5847e-06,
         -6.1169e-06, -6.6385e-06],
        [-2.3291e-05, -1.7107e-05,  1.7226e-05,  ..., -2.0072e-05,
         -1.6361e-05, -1.7583e-05],
        [-1.6510e-05, -1.1414e-05,  1.1906e-05,  ..., -1.4335e-05,
         -1.1660e-05, -1.3202e-05],
        [-2.2128e-05, -1.6898e-05,  1.6093e-05,  ..., -1.9491e-05,
         -1.6525e-05, -1.5602e-05]], device='cuda:0')
Loss: 1.0022963285446167


Running epoch 1, step 1406, batch 358
Sampled inputs[:2]: tensor([[   0,  516, 1424,  ..., 3473,  278, 2442],
        [   0,  278,  266,  ...,   12,  850, 4952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.3217e-05, -3.6019e-04, -1.7050e-04,  ...,  3.0641e-04,
         -2.2324e-04,  2.4135e-04],
        [-1.0259e-05, -7.1488e-06,  7.1712e-06,  ..., -8.7842e-06,
         -7.0482e-06, -7.6890e-06],
        [-2.7284e-05, -1.9982e-05,  2.0355e-05,  ..., -2.3276e-05,
         -1.8880e-05, -2.0444e-05],
        [-1.9357e-05, -1.3351e-05,  1.4111e-05,  ..., -1.6600e-05,
         -1.3426e-05, -1.5318e-05],
        [-2.5764e-05, -1.9655e-05,  1.8880e-05,  ..., -2.2486e-05,
         -1.8999e-05, -1.8001e-05]], device='cuda:0')
Loss: 0.9524517059326172


Running epoch 1, step 1407, batch 359
Sampled inputs[:2]: tensor([[    0,    13,  3105,  ...,   496,    14,   879],
        [    0,  3412,  1707,  ..., 11114,    15,  1821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7299e-05, -4.0991e-04, -2.2872e-04,  ...,  2.7744e-04,
         -2.6144e-04,  2.4135e-04],
        [-1.1675e-05, -8.1621e-06,  8.1472e-06,  ..., -1.0014e-05,
         -8.0541e-06, -8.7395e-06],
        [-3.1009e-05, -2.2754e-05,  2.3127e-05,  ..., -2.6509e-05,
         -2.1517e-05, -2.3201e-05],
        [-2.2054e-05, -1.5251e-05,  1.6049e-05,  ..., -1.8939e-05,
         -1.5348e-05, -1.7419e-05],
        [-2.9340e-05, -2.2426e-05,  2.1487e-05,  ..., -2.5645e-05,
         -2.1696e-05, -2.0459e-05]], device='cuda:0')
Loss: 0.9713765382766724
Graident accumulation at epoch 1, step 1407, batch 359
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.1741e-06, -1.2134e-05, -1.8006e-04,  ..., -4.7721e-05,
          1.5585e-05,  7.4196e-05],
        [-1.2215e-05, -8.5538e-06,  8.1552e-06,  ..., -1.0241e-05,
         -8.0528e-06, -8.7416e-06],
        [ 1.2008e-05,  3.0299e-05, -1.9408e-05,  ...,  1.7377e-05,
          2.5718e-05, -1.7492e-06],
        [ 7.6819e-06,  2.1105e-05, -5.7895e-06,  ...,  1.2077e-05,
          1.4915e-05,  3.3328e-06],
        [-2.4777e-05, -1.6755e-05,  1.7083e-05,  ..., -2.0912e-05,
         -1.7818e-05, -1.6609e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8756e-08, 5.2796e-08, 5.2849e-08,  ..., 1.9289e-08, 1.1942e-07,
         4.4214e-08],
        [8.2943e-11, 5.0659e-11, 2.0608e-11,  ..., 5.7437e-11, 1.9711e-11,
         2.5018e-11],
        [1.9134e-09, 1.3169e-09, 5.3677e-10,  ..., 1.4333e-09, 7.3186e-10,
         4.7390e-10],
        [8.6294e-10, 9.9733e-10, 3.6759e-10,  ..., 8.4899e-10, 5.9146e-10,
         3.5825e-10],
        [4.1296e-10, 2.3958e-10, 8.2260e-11,  ..., 3.0094e-10, 8.1652e-11,
         1.2095e-10]], device='cuda:0')
optimizer state dict: 176.0
lr: [5.1383708942904056e-06, 5.1383708942904056e-06]
scheduler_last_epoch: 176


Running epoch 1, step 1408, batch 360
Sampled inputs[:2]: tensor([[    0,  1171,  2926,  ...,   259,  4288,   654],
        [    0, 18125, 16419,  ...,   278,   638, 11744]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5564e-05,  3.1386e-05, -9.0915e-05,  ..., -4.6810e-05,
         -3.0471e-05, -3.2686e-05],
        [-1.4082e-06, -1.0803e-06,  9.3877e-07,  ..., -1.2591e-06,
         -1.0878e-06, -1.0207e-06],
        [-3.7402e-06, -3.0100e-06,  2.6822e-06,  ..., -3.3379e-06,
         -2.8759e-06, -2.7269e-06],
        [-2.7120e-06, -2.0415e-06,  1.8626e-06,  ..., -2.4289e-06,
         -2.1011e-06, -2.1011e-06],
        [-3.6806e-06, -3.0398e-06,  2.5779e-06,  ..., -3.3379e-06,
         -2.9653e-06, -2.4885e-06]], device='cuda:0')
Loss: 1.0018001794815063


Running epoch 1, step 1409, batch 361
Sampled inputs[:2]: tensor([[    0,  3352,   259,  ...,  3565,    12,   409],
        [    0,   287,  4170,  ...,    27, 12612,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7067e-05,  1.6519e-05, -1.4221e-06,  ..., -1.2765e-04,
         -5.1445e-05, -8.8077e-06],
        [-2.8536e-06, -2.2203e-06,  1.9819e-06,  ..., -2.4736e-06,
         -2.1085e-06, -2.0340e-06],
        [-7.6741e-06, -6.2585e-06,  5.7071e-06,  ..., -6.6608e-06,
         -5.6773e-06, -5.5283e-06],
        [-5.4538e-06, -4.1872e-06,  3.9339e-06,  ..., -4.7386e-06,
         -4.0680e-06, -4.1425e-06],
        [-7.2569e-06, -6.0946e-06,  5.2750e-06,  ..., -6.4373e-06,
         -5.6773e-06, -4.8578e-06]], device='cuda:0')
Loss: 1.002744436264038


Running epoch 1, step 1410, batch 362
Sampled inputs[:2]: tensor([[    0,  6532,  6984,  ...,   271,  8212, 14409],
        [    0,  2700,  5221,  ...,   298,   259,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0818e-05,  1.9716e-05,  9.3241e-05,  ..., -1.3209e-04,
         -1.2859e-04,  7.1906e-05],
        [-4.2692e-06, -3.2261e-06,  2.8498e-06,  ..., -3.7476e-06,
         -3.1590e-06, -3.2261e-06],
        [-1.1384e-05, -9.0003e-06,  8.1807e-06,  ..., -9.9391e-06,
         -8.4192e-06, -8.5533e-06],
        [-8.0466e-06, -6.0052e-06,  5.5656e-06,  ..., -7.0930e-06,
         -6.0350e-06, -6.4224e-06],
        [-1.0893e-05, -8.8364e-06,  7.6741e-06,  ..., -9.6858e-06,
         -8.4788e-06, -7.6145e-06]], device='cuda:0')
Loss: 0.9877824783325195


Running epoch 1, step 1411, batch 363
Sampled inputs[:2]: tensor([[    0,   342,   970,  ...,   401,  2907,  1657],
        [    0, 14867,   278,  ...,   674,   369,  4127]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1413e-04,  2.7211e-04,  3.1812e-04,  ..., -2.1718e-04,
         -1.3069e-04,  7.0313e-05],
        [-5.7295e-06, -4.2990e-06,  3.7886e-06,  ..., -4.9844e-06,
         -4.2394e-06, -4.2766e-06],
        [-1.5438e-05, -1.2130e-05,  1.0967e-05,  ..., -1.3381e-05,
         -1.1504e-05, -1.1504e-05],
        [-1.0893e-05, -8.0764e-06,  7.4506e-06,  ..., -9.5069e-06,
         -8.1956e-06, -8.5682e-06],
        [-1.4618e-05, -1.1817e-05,  1.0177e-05,  ..., -1.2919e-05,
         -1.1459e-05, -1.0148e-05]], device='cuda:0')
Loss: 0.9739927649497986


Running epoch 1, step 1412, batch 364
Sampled inputs[:2]: tensor([[   0, 3306, 4057,  ...,  287,  266, 1692],
        [   0,  292,  380,  ..., 1725,  271,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4998e-04,  3.5022e-04,  4.3077e-04,  ..., -3.4374e-04,
         -2.5893e-04,  8.0315e-05],
        [-7.1749e-06, -5.4240e-06,  4.8243e-06,  ..., -6.2063e-06,
         -5.2378e-06, -5.2601e-06],
        [-1.9237e-05, -1.5259e-05,  1.3858e-05,  ..., -1.6615e-05,
         -1.4186e-05, -1.4171e-05],
        [-1.3649e-05, -1.0207e-05,  9.5069e-06,  ..., -1.1832e-05,
         -1.0103e-05, -1.0550e-05],
        [-1.8150e-05, -1.4871e-05,  1.2815e-05,  ..., -1.6034e-05,
         -1.4156e-05, -1.2472e-05]], device='cuda:0')
Loss: 0.966600239276886


Running epoch 1, step 1413, batch 365
Sampled inputs[:2]: tensor([[   0,   47,   12,  ..., 4367,  278,  471],
        [   0,  874,  590,  ...,  300,  867,  638]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2106e-04,  3.3780e-04,  4.2927e-04,  ..., -3.6121e-04,
         -2.8018e-04,  6.3223e-05],
        [-8.6054e-06, -6.3702e-06,  5.8524e-06,  ..., -7.4208e-06,
         -6.2138e-06, -6.2883e-06],
        [-2.2933e-05, -1.7822e-05,  1.6719e-05,  ..., -1.9729e-05,
         -1.6704e-05, -1.6809e-05],
        [-1.6347e-05, -1.1958e-05,  1.1533e-05,  ..., -1.4111e-05,
         -1.1958e-05, -1.2606e-05],
        [-2.1696e-05, -1.7449e-05,  1.5497e-05,  ..., -1.9073e-05,
         -1.6719e-05, -1.4812e-05]], device='cuda:0')
Loss: 0.9660887718200684


Running epoch 1, step 1414, batch 366
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   658,   221,   474],
        [    0, 17301,   300,  ...,   278,   546,  1576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6187e-04,  2.3608e-04,  3.4310e-04,  ..., -1.3571e-04,
         -3.2826e-04,  1.5440e-04],
        [-9.9987e-06, -7.3612e-06,  6.7465e-06,  ..., -8.6576e-06,
         -7.2494e-06, -7.3388e-06],
        [-2.6807e-05, -2.0713e-05,  1.9386e-05,  ..., -2.3156e-05,
         -1.9550e-05, -1.9699e-05],
        [-1.9014e-05, -1.3828e-05,  1.3299e-05,  ..., -1.6510e-05,
         -1.3970e-05, -1.4752e-05],
        [-2.5406e-05, -2.0310e-05,  1.8001e-05,  ..., -2.2411e-05,
         -1.9565e-05, -1.7390e-05]], device='cuda:0')
Loss: 0.9964339733123779


Running epoch 1, step 1415, batch 367
Sampled inputs[:2]: tensor([[   0,  278,  634,  ...,  598, 1722,  591],
        [   0,   12,  266,  ...,  674,  369,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.0755e-05,  2.6824e-04,  4.0722e-04,  ..., -4.4576e-05,
         -4.7968e-04,  6.9790e-05],
        [-1.1355e-05, -8.3596e-06,  7.7300e-06,  ..., -9.8273e-06,
         -8.1807e-06, -8.3745e-06],
        [-3.0607e-05, -2.3574e-05,  2.2337e-05,  ..., -2.6360e-05,
         -2.2084e-05, -2.2516e-05],
        [-2.1741e-05, -1.5780e-05,  1.5356e-05,  ..., -1.8850e-05,
         -1.5855e-05, -1.6958e-05],
        [-2.8819e-05, -2.2963e-05,  2.0593e-05,  ..., -2.5332e-05,
         -2.1964e-05, -1.9714e-05]], device='cuda:0')
Loss: 0.9494054317474365
Graident accumulation at epoch 1, step 1415, batch 367
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.1878e-07,  1.5903e-05, -1.2133e-04,  ..., -4.7407e-05,
         -3.3942e-05,  7.3755e-05],
        [-1.2129e-05, -8.5344e-06,  8.1127e-06,  ..., -1.0199e-05,
         -8.0656e-06, -8.7049e-06],
        [ 7.7464e-06,  2.4911e-05, -1.5233e-05,  ...,  1.3003e-05,
          2.0938e-05, -3.8259e-06],
        [ 4.7396e-06,  1.7417e-05, -3.6750e-06,  ...,  8.9847e-06,
          1.1838e-05,  1.3038e-06],
        [-2.5181e-05, -1.7375e-05,  1.7434e-05,  ..., -2.1354e-05,
         -1.8233e-05, -1.6920e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8705e-08, 5.2815e-08, 5.2962e-08,  ..., 1.9272e-08, 1.1953e-07,
         4.4175e-08],
        [8.2989e-11, 5.0679e-11, 2.0647e-11,  ..., 5.7476e-11, 1.9758e-11,
         2.5063e-11],
        [1.9124e-09, 1.3161e-09, 5.3673e-10,  ..., 1.4326e-09, 7.3161e-10,
         4.7393e-10],
        [8.6255e-10, 9.9658e-10, 3.6746e-10,  ..., 8.4850e-10, 5.9112e-10,
         3.5818e-10],
        [4.1338e-10, 2.3986e-10, 8.2602e-11,  ..., 3.0128e-10, 8.2053e-11,
         1.2122e-10]], device='cuda:0')
optimizer state dict: 177.0
lr: [5.030720466797722e-06, 5.030720466797722e-06]
scheduler_last_epoch: 177


Running epoch 1, step 1416, batch 368
Sampled inputs[:2]: tensor([[    0,  7527,    15,  ...,  2677,   292, 30654],
        [    0,  1561,    14,  ...,  4433,   352,  1561]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2627e-05,  1.5717e-04, -8.4430e-05,  ...,  5.3624e-05,
         -4.0976e-04, -2.3713e-04],
        [-1.3858e-06, -1.0207e-06,  1.0207e-06,  ..., -1.2144e-06,
         -9.5367e-07, -1.0952e-06],
        [-3.9041e-06, -2.9057e-06,  3.0696e-06,  ..., -3.3528e-06,
         -2.6226e-06, -3.0249e-06],
        [-2.6822e-06, -1.9222e-06,  2.0713e-06,  ..., -2.3395e-06,
         -1.8328e-06, -2.2203e-06],
        [-3.5763e-06, -2.7120e-06,  2.7418e-06,  ..., -3.1143e-06,
         -2.5332e-06, -2.5630e-06]], device='cuda:0')
Loss: 0.9305810332298279


Running epoch 1, step 1417, batch 369
Sampled inputs[:2]: tensor([[    0, 25409,   287,  ...,  1005,   344,  3493],
        [    0,    14,   417,  ...,    43,   503,    67]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9735e-05,  2.3481e-04, -5.0753e-05,  ...,  1.9091e-04,
         -6.2364e-04, -4.2805e-04],
        [-2.7642e-06, -2.0340e-06,  2.0042e-06,  ..., -2.4140e-06,
         -1.9372e-06, -2.0936e-06],
        [-7.5400e-06, -5.7220e-06,  5.8711e-06,  ..., -6.5267e-06,
         -5.2303e-06, -5.6475e-06],
        [-5.3644e-06, -3.8892e-06,  4.0680e-06,  ..., -4.7088e-06,
         -3.7998e-06, -4.2915e-06],
        [-6.9290e-06, -5.3942e-06,  5.2601e-06,  ..., -6.0797e-06,
         -5.0664e-06, -4.8131e-06]], device='cuda:0')
Loss: 0.9804379343986511


Running epoch 1, step 1418, batch 370
Sampled inputs[:2]: tensor([[    0,   767,  4478,  ...,   278,   266, 19201],
        [    0,  4672,   278,  ...,    13,   265, 49987]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.8052e-05,  1.3987e-04, -5.2538e-05,  ...,  1.5787e-04,
         -5.0766e-04, -3.5544e-04],
        [-4.1202e-06, -3.1069e-06,  2.9877e-06,  ..., -3.5688e-06,
         -2.9057e-06, -3.0920e-06],
        [-1.1310e-05, -8.8215e-06,  8.7768e-06,  ..., -9.7454e-06,
         -7.9572e-06, -8.4043e-06],
        [-8.0913e-06, -6.0350e-06,  6.1393e-06,  ..., -7.0333e-06,
         -5.7667e-06, -6.4075e-06],
        [-1.0386e-05, -8.3297e-06,  7.8827e-06,  ..., -9.0748e-06,
         -7.7039e-06, -7.1675e-06]], device='cuda:0')
Loss: 0.9877826571464539


Running epoch 1, step 1419, batch 371
Sampled inputs[:2]: tensor([[    0, 19350,   271,  ...,   445,  1841,   446],
        [    0,  2386,  4012,  ...,   300, 15480,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.2884e-06,  1.0752e-04, -1.4378e-04,  ...,  1.8814e-04,
         -4.8284e-04, -2.3299e-04],
        [-5.4017e-06, -4.0904e-06,  3.9227e-06,  ..., -4.6939e-06,
         -3.8892e-06, -4.0606e-06],
        [-1.4842e-05, -1.1623e-05,  1.1533e-05,  ..., -1.2800e-05,
         -1.0639e-05, -1.1012e-05],
        [-1.0639e-05, -7.9572e-06,  8.0764e-06,  ..., -9.2685e-06,
         -7.7486e-06, -8.4490e-06],
        [-1.3679e-05, -1.1042e-05,  1.0401e-05,  ..., -1.1981e-05,
         -1.0327e-05, -9.4026e-06]], device='cuda:0')
Loss: 0.9550467133522034


Running epoch 1, step 1420, batch 372
Sampled inputs[:2]: tensor([[   0, 4994, 8429,  ...,   12,  795,  596],
        [   0, 6477,   12,  ..., 2931,  221,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0559e-05, -8.2829e-06, -2.2942e-04,  ...,  2.7334e-04,
         -4.8679e-04, -2.2897e-04],
        [-6.7502e-06, -5.0738e-06,  4.9360e-06,  ..., -5.8562e-06,
         -4.8392e-06, -5.0291e-06],
        [-1.8448e-05, -1.4365e-05,  1.4439e-05,  ..., -1.5914e-05,
         -1.3173e-05, -1.3620e-05],
        [ 2.3070e-05,  6.2899e-05, -6.7178e-05,  ...,  3.9474e-05,
          4.3130e-05,  3.8217e-05],
        [-1.7047e-05, -1.3709e-05,  1.3039e-05,  ..., -1.4946e-05,
         -1.2860e-05, -1.1668e-05]], device='cuda:0')
Loss: 0.9737478494644165


Running epoch 1, step 1421, batch 373
Sampled inputs[:2]: tensor([[    0, 29258,   765,  ...,  4196,    19,    12],
        [    0, 13964,    13,  ...,    14,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4729e-05,  1.6004e-04, -2.2891e-04,  ...,  2.0797e-04,
         -5.5473e-04, -3.8681e-04],
        [-8.1807e-06, -6.1244e-06,  6.0163e-06,  ..., -7.0632e-06,
         -5.7481e-06, -6.0126e-06],
        [-2.2203e-05, -1.7256e-05,  1.7479e-05,  ..., -1.9088e-05,
         -1.5587e-05, -1.6242e-05],
        [ 2.0447e-05,  6.0992e-05, -6.5107e-05,  ...,  3.7269e-05,
          4.1454e-05,  3.6310e-05],
        [-2.0489e-05, -1.6481e-05,  1.5751e-05,  ..., -1.7911e-05,
         -1.5244e-05, -1.3888e-05]], device='cuda:0')
Loss: 0.9445391297340393


Running epoch 1, step 1422, batch 374
Sampled inputs[:2]: tensor([[   0,  360, 3285,  ...,  423, 3579,  468],
        [   0, 9829,  292,  ..., 2928, 1029,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1616e-04,  9.3876e-05, -1.2453e-04,  ...,  3.5645e-04,
         -3.0151e-04, -2.6408e-04],
        [-9.4995e-06, -7.1451e-06,  6.8992e-06,  ..., -8.2254e-06,
         -6.7465e-06, -7.0184e-06],
        [-2.5809e-05, -2.0146e-05,  2.0087e-05,  ..., -2.2218e-05,
         -1.8224e-05, -1.8954e-05],
        [ 1.7795e-05,  5.8995e-05, -6.3274e-05,  ...,  3.4944e-05,
          3.9472e-05,  3.4179e-05],
        [-2.3916e-05, -1.9312e-05,  1.8165e-05,  ..., -2.0936e-05,
         -1.7881e-05, -1.6272e-05]], device='cuda:0')
Loss: 0.9712153077125549


Running epoch 1, step 1423, batch 375
Sampled inputs[:2]: tensor([[    0,    12,   895,  ...,    13,  2900,    14],
        [    0,   298, 39056,  ...,   221,  1061,  2165]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5175e-04,  2.6880e-04,  4.7380e-05,  ...,  4.5491e-04,
         -6.2586e-04, -3.2168e-04],
        [-1.1005e-05, -8.1211e-06,  7.8417e-06,  ..., -9.5442e-06,
         -7.7598e-06, -8.2180e-06],
        [-3.0071e-05, -2.3007e-05,  2.2918e-05,  ..., -2.5898e-05,
         -2.1070e-05, -2.2307e-05],
        [ 1.4964e-05,  5.7214e-05, -6.1434e-05,  ...,  3.2470e-05,
          3.7580e-05,  3.1869e-05],
        [-2.8089e-05, -2.2188e-05,  2.0817e-05,  ..., -2.4617e-05,
         -2.0832e-05, -1.9416e-05]], device='cuda:0')
Loss: 0.9558194875717163
Graident accumulation at epoch 1, step 1423, batch 375
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.4438e-05,  4.1193e-05, -1.0446e-04,  ...,  2.8253e-06,
         -9.3133e-05,  3.4212e-05],
        [-1.2017e-05, -8.4930e-06,  8.0856e-06,  ..., -1.0134e-05,
         -8.0350e-06, -8.6562e-06],
        [ 3.9647e-06,  2.0120e-05, -1.1418e-05,  ...,  9.1130e-06,
          1.6737e-05, -5.6740e-06],
        [ 5.7620e-06,  2.1396e-05, -9.4509e-06,  ...,  1.1333e-05,
          1.4412e-05,  4.3603e-06],
        [-2.5472e-05, -1.7857e-05,  1.7773e-05,  ..., -2.1681e-05,
         -1.8493e-05, -1.7170e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8670e-08, 5.2834e-08, 5.2911e-08,  ..., 1.9459e-08, 1.1981e-07,
         4.4234e-08],
        [8.3027e-11, 5.0694e-11, 2.0688e-11,  ..., 5.7510e-11, 1.9798e-11,
         2.5106e-11],
        [1.9114e-09, 1.3153e-09, 5.3672e-10,  ..., 1.4318e-09, 7.3133e-10,
         4.7396e-10],
        [8.6192e-10, 9.9886e-10, 3.7087e-10,  ..., 8.4870e-10, 5.9194e-10,
         3.5883e-10],
        [4.1376e-10, 2.4012e-10, 8.2953e-11,  ..., 3.0158e-10, 8.2405e-11,
         1.2147e-10]], device='cuda:0')
optimizer state dict: 178.0
lr: [4.9238293885951606e-06, 4.9238293885951606e-06]
scheduler_last_epoch: 178


Running epoch 1, step 1424, batch 376
Sampled inputs[:2]: tensor([[    0,  2485,    12,  ...,   293,   259, 14600],
        [    0,  1217,     9,  ...,  1821,     5,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8536e-05,  1.1266e-04, -6.9013e-07,  ..., -6.1084e-05,
          3.4813e-05,  3.1203e-05],
        [-1.4231e-06, -9.6858e-07,  9.4995e-07,  ..., -1.2517e-06,
         -1.0058e-06, -1.0133e-06],
        [-3.6508e-06, -2.6226e-06,  2.6375e-06,  ..., -3.1739e-06,
         -2.5630e-06, -2.5630e-06],
        [-2.7120e-06, -1.8403e-06,  1.8775e-06,  ..., -2.3991e-06,
         -1.9521e-06, -2.0415e-06],
        [-3.5167e-06, -2.6226e-06,  2.5183e-06,  ..., -3.1292e-06,
         -2.6077e-06, -2.2799e-06]], device='cuda:0')
Loss: 0.979469895362854


Running epoch 1, step 1425, batch 377
Sampled inputs[:2]: tensor([[   0,  278, 4191,  ...,  381, 3020,  352],
        [   0, 7333,  342,  ...,   13, 1818, 6183]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4501e-04,  1.8664e-04, -7.1608e-06,  ..., -4.0408e-05,
         -7.0388e-05, -1.0391e-04],
        [-2.7493e-06, -1.8626e-06,  1.8626e-06,  ..., -2.4736e-06,
         -1.9372e-06, -2.0564e-06],
        [-7.3165e-06, -5.0962e-06,  5.3644e-06,  ..., -6.4224e-06,
         -5.0217e-06, -5.2601e-06],
        [-5.3048e-06, -3.5018e-06,  3.7253e-06,  ..., -4.7684e-06,
         -3.7476e-06, -4.1574e-06],
        [-6.9588e-06, -4.9919e-06,  5.0366e-06,  ..., -6.1989e-06,
         -5.0068e-06, -4.6045e-06]], device='cuda:0')
Loss: 0.9466241002082825


Running epoch 1, step 1426, batch 378
Sampled inputs[:2]: tensor([[    0,   342,   408,  ...,  5162, 25842,  4855],
        [    0,    76,   472,  ..., 21215,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.1862e-05,  2.6931e-04,  2.4400e-05,  ...,  2.7007e-04,
         -8.5438e-04, -1.9996e-04],
        [-4.1723e-06, -2.8834e-06,  2.8126e-06,  ..., -3.6731e-06,
         -2.9281e-06, -3.0175e-06],
        [-1.1042e-05, -7.9274e-06,  8.0466e-06,  ..., -9.5814e-06,
         -7.6741e-06, -7.7784e-06],
        [-8.0317e-06, -5.4240e-06,  5.6177e-06,  ..., -7.0781e-06,
         -5.6848e-06, -6.1244e-06],
        [-1.0476e-05, -7.7784e-06,  7.5251e-06,  ..., -9.2387e-06,
         -7.6592e-06, -6.8098e-06]], device='cuda:0')
Loss: 0.9812447428703308


Running epoch 1, step 1427, batch 379
Sampled inputs[:2]: tensor([[    0,  2911,   287,  ...,  2178, 22788,  8645],
        [    0,   346,   462,  ..., 35247,  2547,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1001e-04,  3.3492e-04,  2.0987e-05,  ...,  4.3260e-04,
         -8.5592e-04, -3.9047e-04],
        [-5.5432e-06, -3.8520e-06,  3.8557e-06,  ..., -4.8354e-06,
         -3.8892e-06, -3.9600e-06],
        [-1.4707e-05, -1.0610e-05,  1.1012e-05,  ..., -1.2651e-05,
         -1.0192e-05, -1.0252e-05],
        [-1.0699e-05, -7.2569e-06,  7.7337e-06,  ..., -9.3430e-06,
         -7.5549e-06, -8.0764e-06],
        [-1.3843e-05, -1.0341e-05,  1.0207e-05,  ..., -1.2115e-05,
         -1.0103e-05, -8.9109e-06]], device='cuda:0')
Loss: 0.9711496233940125


Running epoch 1, step 1428, batch 380
Sampled inputs[:2]: tensor([[    0,   634, 10095,  ...,   367, 24607,   287],
        [    0, 19641,   437,  ...,  2992,   518,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8649e-05,  4.3849e-04,  2.0466e-05,  ...,  4.2975e-04,
         -8.5033e-04, -3.8339e-04],
        [-6.9886e-06, -4.8131e-06,  4.8839e-06,  ..., -5.9605e-06,
         -4.7684e-06, -4.9882e-06],
        [-1.8731e-05, -1.3441e-05,  1.4067e-05,  ..., -1.5810e-05,
         -1.2696e-05, -1.3128e-05],
        [-1.3486e-05, -9.1046e-06,  9.8050e-06,  ..., -1.1519e-05,
         -9.2685e-06, -1.0133e-05],
        [-1.7449e-05, -1.2979e-05,  1.2890e-05,  ..., -1.5035e-05,
         -1.2517e-05, -1.1340e-05]], device='cuda:0')
Loss: 0.9658465385437012


Running epoch 1, step 1429, batch 381
Sampled inputs[:2]: tensor([[    0,   298, 49038,  ...,   288,  1690,  2736],
        [    0,  1549,  7052,  ...,  2529,  3958,    37]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4085e-04,  5.2920e-04,  1.4598e-04,  ...,  5.1128e-04,
         -7.2553e-04, -1.5561e-04],
        [-8.4043e-06, -5.9158e-06,  5.7817e-06,  ..., -7.2345e-06,
         -5.8860e-06, -6.0685e-06],
        [-2.2665e-05, -1.6645e-05,  1.6734e-05,  ..., -1.9327e-05,
         -1.5780e-05, -1.6108e-05],
        [-1.6198e-05, -1.1191e-05,  1.1571e-05,  ..., -1.3962e-05,
         -1.1444e-05, -1.2323e-05],
        [-2.1189e-05, -1.6153e-05,  1.5378e-05,  ..., -1.8448e-05,
         -1.5602e-05, -1.3977e-05]], device='cuda:0')
Loss: 1.0166637897491455


Running epoch 1, step 1430, batch 382
Sampled inputs[:2]: tensor([[    0,  3761,   527,  ..., 24518,   391,   638],
        [    0,  3084,   278,  ..., 10981,  3589,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.9032e-05,  6.4746e-04,  1.0235e-04,  ...,  4.5000e-04,
         -7.8735e-04, -2.3545e-04],
        [-9.8944e-06, -6.9365e-06,  6.6608e-06,  ..., -8.5011e-06,
         -6.8843e-06, -7.1563e-06],
        [-2.6658e-05, -1.9580e-05,  1.9297e-05,  ..., -2.2754e-05,
         -1.8522e-05, -1.9044e-05],
        [-1.9014e-05, -1.3128e-05,  1.3299e-05,  ..., -1.6406e-05,
         -1.3381e-05, -1.4469e-05],
        [-2.5153e-05, -1.9118e-05,  1.7852e-05,  ..., -2.1905e-05,
         -1.8463e-05, -1.6734e-05]], device='cuda:0')
Loss: 0.940741777420044


Running epoch 1, step 1431, batch 383
Sampled inputs[:2]: tensor([[   0,   14, 6436,  ...,  271, 1211, 8917],
        [   0, 1635,  266,  ...,  437, 3302,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2969e-04,  6.4746e-04,  1.4315e-04,  ...,  5.6046e-04,
         -6.4873e-04, -1.2763e-04],
        [-1.1303e-05, -8.0168e-06,  7.6294e-06,  ..., -9.6932e-06,
         -7.8529e-06, -8.1062e-06],
        [-3.0503e-05, -2.2635e-05,  2.2113e-05,  ..., -2.6003e-05,
         -2.1189e-05, -2.1666e-05],
        [-2.1726e-05, -1.5199e-05,  1.5251e-05,  ..., -1.8701e-05,
         -1.5251e-05, -1.6391e-05],
        [-2.8700e-05, -2.2054e-05,  2.0415e-05,  ..., -2.4989e-05,
         -2.1115e-05, -1.8999e-05]], device='cuda:0')
Loss: 0.9895445704460144
Graident accumulation at epoch 1, step 1431, batch 383
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.5963e-05,  1.0182e-04, -7.9701e-05,  ...,  5.8589e-05,
         -1.4869e-04,  1.8027e-05],
        [-1.1945e-05, -8.4454e-06,  8.0400e-06,  ..., -1.0090e-05,
         -8.0168e-06, -8.6012e-06],
        [ 5.1794e-07,  1.5844e-05, -8.0649e-06,  ...,  5.6015e-06,
          1.2944e-05, -7.2732e-06],
        [ 3.0132e-06,  1.7737e-05, -6.9806e-06,  ...,  8.3298e-06,
          1.1446e-05,  2.2852e-06],
        [-2.5795e-05, -1.8276e-05,  1.8037e-05,  ..., -2.2011e-05,
         -1.8755e-05, -1.7353e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8628e-08, 5.3201e-08, 5.2879e-08,  ..., 1.9754e-08, 1.2011e-07,
         4.4206e-08],
        [8.3072e-11, 5.0708e-11, 2.0726e-11,  ..., 5.7546e-11, 1.9840e-11,
         2.5146e-11],
        [1.9104e-09, 1.3145e-09, 5.3668e-10,  ..., 1.4311e-09, 7.3104e-10,
         4.7395e-10],
        [8.6153e-10, 9.9809e-10, 3.7073e-10,  ..., 8.4820e-10, 5.9159e-10,
         3.5874e-10],
        [4.1417e-10, 2.4036e-10, 8.3286e-11,  ..., 3.0190e-10, 8.2768e-11,
         1.2171e-10]], device='cuda:0')
optimizer state dict: 179.0
lr: [4.817713993572543e-06, 4.817713993572543e-06]
scheduler_last_epoch: 179


Running epoch 1, step 1432, batch 384
Sampled inputs[:2]: tensor([[   0,  257,   13,  ...,  328,  630, 1403],
        [   0,  266, 1422,  ...,  446, 1992,  586]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2615e-05, -1.2417e-04,  4.1055e-05,  ..., -1.1802e-04,
          2.1544e-06,  6.5070e-05],
        [-1.3635e-06, -1.1623e-06,  9.2015e-07,  ..., -1.2144e-06,
         -1.1176e-06, -9.3877e-07],
        [-3.6955e-06, -3.2932e-06,  2.6226e-06,  ..., -3.3081e-06,
         -3.0249e-06, -2.5481e-06],
        [-2.6226e-06, -2.2054e-06,  1.8105e-06,  ..., -2.3544e-06,
         -2.1756e-06, -1.8999e-06],
        [-3.5763e-06, -3.2634e-06,  2.4885e-06,  ..., -3.2485e-06,
         -3.0398e-06, -2.3097e-06]], device='cuda:0')
Loss: 1.0000510215759277


Running epoch 1, step 1433, batch 385
Sampled inputs[:2]: tensor([[   0,  271, 3403,  ..., 6168,  300, 2257],
        [   0, 4209,  278,  ...,  287, 9971,  717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8354e-05,  1.0300e-04,  3.8174e-05,  ..., -1.8481e-04,
          3.8702e-05, -5.6516e-05],
        [-2.9206e-06, -2.2501e-06,  1.9781e-06,  ..., -2.4810e-06,
         -2.1532e-06, -2.0862e-06],
        [-8.0764e-06, -6.5267e-06,  5.7518e-06,  ..., -6.9141e-06,
         -6.0201e-06, -5.8413e-06],
        [-5.5283e-06, -4.2319e-06,  3.8669e-06,  ..., -4.7237e-06,
         -4.1425e-06, -4.1202e-06],
        [-7.8380e-06, -6.4969e-06,  5.4240e-06,  ..., -6.8247e-06,
         -6.0946e-06, -5.3793e-06]], device='cuda:0')
Loss: 0.9618563055992126


Running epoch 1, step 1434, batch 386
Sampled inputs[:2]: tensor([[    0, 30229,    12,  ...,   518,   717,   271],
        [    0,   365,  1462,  ...,   518,  6104,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2599e-04,  7.2158e-05,  1.1362e-05,  ..., -1.7799e-04,
          5.4084e-05, -1.6513e-04],
        [-4.2245e-06, -3.1777e-06,  2.9244e-06,  ..., -3.5688e-06,
         -3.0249e-06, -3.0287e-06],
        [-1.1608e-05, -9.1046e-06,  8.5086e-06,  ..., -9.8050e-06,
         -8.3297e-06, -8.3297e-06],
        [-8.1211e-06, -6.0201e-06,  5.8338e-06,  ..., -6.8694e-06,
         -5.8636e-06, -6.0871e-06],
        [-1.0997e-05, -8.8960e-06,  7.8380e-06,  ..., -9.4771e-06,
         -8.3148e-06, -7.4059e-06]], device='cuda:0')
Loss: 0.9500448703765869


Running epoch 1, step 1435, batch 387
Sampled inputs[:2]: tensor([[    0,  2220,  1110,  ...,   382,    18,    13],
        [    0,   494,   360,  ...,   391, 24104, 35211]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1581e-04,  1.6003e-04, -1.1362e-04,  ..., -1.7729e-04,
         -1.6114e-04, -1.8261e-04],
        [-5.6773e-06, -4.0010e-06,  3.9078e-06,  ..., -4.7758e-06,
         -3.8743e-06, -4.1686e-06],
        [-1.5572e-05, -1.1504e-05,  1.1429e-05,  ..., -1.3083e-05,
         -1.0654e-05, -1.1414e-05],
        [-1.0893e-05, -7.5772e-06,  7.8306e-06,  ..., -9.1642e-06,
         -7.4804e-06, -8.3372e-06],
        [-1.4573e-05, -1.1131e-05,  1.0341e-05,  ..., -1.2502e-05,
         -1.0565e-05, -9.9987e-06]], device='cuda:0')
Loss: 0.9382269382476807


Running epoch 1, step 1436, batch 388
Sampled inputs[:2]: tensor([[    0,  2771,  2070,  ...,   221,   396,   298],
        [    0,   292, 21050,  ...,  4142, 23314,  1027]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9275e-04,  1.4696e-04, -1.6770e-04,  ..., -1.2997e-04,
         -2.9216e-04, -2.1725e-04],
        [-7.0781e-06, -5.0440e-06,  4.8690e-06,  ..., -5.9679e-06,
         -4.8354e-06, -5.1446e-06],
        [-1.9342e-05, -1.4484e-05,  1.4231e-05,  ..., -1.6317e-05,
         -1.3322e-05, -1.4067e-05],
        [ 6.3382e-05,  5.8305e-05, -3.4235e-05,  ...,  5.3062e-05,
          6.4358e-05,  4.4811e-05],
        [-1.8060e-05, -1.3977e-05,  1.2875e-05,  ..., -1.5542e-05,
         -1.3173e-05, -1.2279e-05]], device='cuda:0')
Loss: 0.9824745655059814


Running epoch 1, step 1437, batch 389
Sampled inputs[:2]: tensor([[   0,  759, 1184,  ...,  472,  346,   14],
        [   0,  824,   14,  ...,  278, 9328, 1049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.9264e-04,  2.1155e-04, -1.3615e-04,  ..., -7.6505e-05,
         -5.9595e-04, -3.4330e-04],
        [-8.4639e-06, -6.0350e-06,  5.9791e-06,  ..., -7.0557e-06,
         -5.6997e-06, -6.0610e-06],
        [-2.3097e-05, -1.7285e-05,  1.7405e-05,  ..., -1.9282e-05,
         -1.5721e-05, -1.6585e-05],
        [ 6.0581e-05,  5.6338e-05, -3.1895e-05,  ...,  5.0887e-05,
          6.2615e-05,  4.2859e-05],
        [-2.1383e-05, -1.6585e-05,  1.5631e-05,  ..., -1.8254e-05,
         -1.5467e-05, -1.4365e-05]], device='cuda:0')
Loss: 0.9623339176177979


Running epoch 1, step 1438, batch 390
Sampled inputs[:2]: tensor([[   0, 2849, 1173,  ..., 1481,   12,  287],
        [   0,  461, 4182,  ..., 7461,  292, 4895]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2282e-04,  3.0638e-04, -1.2138e-04,  ...,  2.2967e-05,
         -6.7574e-04, -2.4526e-04],
        [-9.8646e-06, -7.0408e-06,  6.9849e-06,  ..., -8.2478e-06,
         -6.6757e-06, -7.0445e-06],
        [-2.6852e-05, -2.0146e-05,  2.0310e-05,  ..., -2.2501e-05,
         -1.8388e-05, -1.9222e-05],
        [ 5.7854e-05,  5.4386e-05, -2.9839e-05,  ...,  4.8547e-05,
          6.0678e-05,  4.0817e-05],
        [-2.4974e-05, -1.9431e-05,  1.8328e-05,  ..., -2.1413e-05,
         -1.8179e-05, -1.6719e-05]], device='cuda:0')
Loss: 0.9728145003318787


Running epoch 1, step 1439, batch 391
Sampled inputs[:2]: tensor([[    0,   266, 12964,  ...,   300,  3979,  4706],
        [    0,   409,   699,  ...,    12,   546,   696]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7073e-04,  3.7158e-04, -1.1239e-04,  ..., -1.6569e-05,
         -6.9282e-04, -4.7419e-04],
        [-1.1228e-05, -8.0317e-06,  8.0131e-06,  ..., -9.3877e-06,
         -7.6517e-06, -7.9870e-06],
        [-3.0562e-05, -2.2948e-05,  2.3276e-05,  ..., -2.5570e-05,
         -2.1011e-05, -2.1771e-05],
        [ 5.5142e-05,  5.2449e-05, -2.7708e-05,  ...,  4.6297e-05,
          5.8740e-05,  3.8835e-05],
        [-2.8387e-05, -2.2143e-05,  2.0996e-05,  ..., -2.4319e-05,
         -2.0757e-05, -1.8895e-05]], device='cuda:0')
Loss: 0.9648621082305908
Graident accumulation at epoch 1, step 1439, batch 391
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.0439e-05,  1.2880e-04, -8.2970e-05,  ...,  5.1073e-05,
         -2.0311e-04, -3.1194e-05],
        [-1.1874e-05, -8.4041e-06,  8.0373e-06,  ..., -1.0020e-05,
         -7.9803e-06, -8.5398e-06],
        [-2.5901e-06,  1.1965e-05, -4.9308e-06,  ...,  2.4843e-06,
          9.5489e-06, -8.7230e-06],
        [ 8.2261e-06,  2.1208e-05, -9.0534e-06,  ...,  1.2127e-05,
          1.6175e-05,  5.9402e-06],
        [-2.6054e-05, -1.8663e-05,  1.8333e-05,  ..., -2.2242e-05,
         -1.8955e-05, -1.7507e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8791e-08, 5.3286e-08, 5.2839e-08,  ..., 1.9735e-08, 1.2047e-07,
         4.4387e-08],
        [8.3115e-11, 5.0721e-11, 2.0769e-11,  ..., 5.7577e-11, 1.9879e-11,
         2.5185e-11],
        [1.9094e-09, 1.3137e-09, 5.3668e-10,  ..., 1.4303e-09, 7.3075e-10,
         4.7395e-10],
        [8.6371e-10, 9.9984e-10, 3.7113e-10,  ..., 8.4950e-10, 5.9444e-10,
         3.5989e-10],
        [4.1456e-10, 2.4061e-10, 8.3644e-11,  ..., 3.0219e-10, 8.3116e-11,
         1.2195e-10]], device='cuda:0')
optimizer state dict: 180.0
lr: [4.712390497088522e-06, 4.712390497088522e-06]
scheduler_last_epoch: 180


Running epoch 1, step 1440, batch 392
Sampled inputs[:2]: tensor([[   0, 3804,  300,  ..., 5062, 9848, 3515],
        [   0, 4154,   12,  ...,   14,  560,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2890e-05, -1.9358e-04, -1.4545e-04,  ..., -1.7196e-04,
          2.6095e-04, -3.2638e-05],
        [-1.2890e-06, -9.9093e-07,  9.4622e-07,  ..., -1.1548e-06,
         -9.6112e-07, -9.2387e-07],
        [-3.4571e-06, -2.7418e-06,  2.7567e-06,  ..., -3.0398e-06,
         -2.5332e-06, -2.4438e-06],
        [-2.4885e-06, -1.8775e-06,  1.9222e-06,  ..., -2.2203e-06,
         -1.8552e-06, -1.8999e-06],
        [-3.1739e-06, -2.6226e-06,  2.4736e-06,  ..., -2.8461e-06,
         -2.4736e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9853367805480957


Running epoch 1, step 1441, batch 393
Sampled inputs[:2]: tensor([[    0,  4667,   446,  ...,  1868, 16028,   669],
        [    0,   298,   369,  ...,  5936,   968,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7779e-04, -1.9018e-04, -7.4696e-05,  ..., -1.8254e-04,
          2.7675e-04, -1.3010e-04],
        [-2.6822e-06, -1.9148e-06,  1.9222e-06,  ..., -2.3544e-06,
         -1.9222e-06, -1.9595e-06],
        [-7.2420e-06, -5.3495e-06,  5.6177e-06,  ..., -6.2585e-06,
         -5.1260e-06, -5.2154e-06],
        [-5.1558e-06, -3.6061e-06,  3.8743e-06,  ..., -4.5151e-06,
         -3.6955e-06, -3.9563e-06],
        [-6.7055e-06, -5.1260e-06,  5.0515e-06,  ..., -5.9009e-06,
         -5.0068e-06, -4.4256e-06]], device='cuda:0')
Loss: 0.9529209733009338


Running epoch 1, step 1442, batch 394
Sampled inputs[:2]: tensor([[    0,   287,  2269,  ..., 22413,   391,   266],
        [    0,  1503,  1785,  ...,   221,   380,  1869]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2962e-04, -1.4314e-04, -8.1495e-05,  ..., -9.9680e-05,
         -1.7267e-06, -3.0474e-04],
        [-3.9935e-06, -2.8610e-06,  2.9132e-06,  ..., -3.4422e-06,
         -2.7940e-06, -2.8871e-06],
        [-1.0863e-05, -8.0317e-06,  8.5533e-06,  ..., -9.2238e-06,
         -7.5400e-06, -7.6890e-06],
        [-7.8380e-06, -5.4836e-06,  6.0201e-06,  ..., -6.7204e-06,
         -5.5060e-06, -5.9381e-06],
        [-9.8497e-06, -7.5549e-06,  7.5549e-06,  ..., -8.5235e-06,
         -7.2271e-06, -6.4075e-06]], device='cuda:0')
Loss: 0.9446309208869934


Running epoch 1, step 1443, batch 395
Sampled inputs[:2]: tensor([[    0,  5896,   352,  ...,  1168,   767,  1390],
        [    0,  7120,   344,  ...,  6273,    52, 22639]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0288e-04, -1.5218e-04, -2.0892e-04,  ..., -2.0731e-04,
          8.0336e-05, -2.2227e-04],
        [-5.3346e-06, -3.8110e-06,  3.9339e-06,  ..., -4.5747e-06,
         -3.7551e-06, -3.8631e-06],
        [-1.4573e-05, -1.0699e-05,  1.1533e-05,  ..., -1.2308e-05,
         -1.0148e-05, -1.0312e-05],
        [-1.0446e-05, -7.2643e-06,  8.0764e-06,  ..., -8.8960e-06,
         -7.3388e-06, -7.9051e-06],
        [-1.3351e-05, -1.0148e-05,  1.0297e-05,  ..., -1.1474e-05,
         -9.8050e-06, -8.6874e-06]], device='cuda:0')
Loss: 1.0024867057800293


Running epoch 1, step 1444, batch 396
Sampled inputs[:2]: tensor([[    0,   257,   298,  ...,  3768,   271,   266],
        [    0,   292,   221,  ...,   796, 12886,   694]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5180e-05, -3.2712e-04, -1.1206e-04,  ..., -1.0853e-04,
          2.9349e-04, -2.0757e-04],
        [-6.7502e-06, -4.6827e-06,  4.5784e-06,  ..., -5.9083e-06,
         -4.8876e-06, -5.1372e-06],
        [-1.8269e-05, -1.3083e-05,  1.3530e-05,  ..., -1.5646e-05,
         -1.3009e-05, -1.3411e-05],
        [-1.3098e-05, -8.8513e-06,  9.3132e-06,  ..., -1.1429e-05,
         -9.5144e-06, -1.0364e-05],
        [-1.7166e-05, -1.2606e-05,  1.2353e-05,  ..., -1.4931e-05,
         -1.2815e-05, -1.1623e-05]], device='cuda:0')
Loss: 0.9181267619132996


Running epoch 1, step 1445, batch 397
Sampled inputs[:2]: tensor([[    0,  6976, 16084,  ...,    19,  9955,  3854],
        [    0,   720,  1122,  ...,   656,   287, 14258]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6583e-04, -1.9626e-04,  3.3586e-05,  ..., -1.3357e-04,
          1.6473e-05, -3.7271e-04],
        [-8.1360e-06, -5.6736e-06,  5.7109e-06,  ..., -7.0035e-06,
         -5.8487e-06, -6.0536e-06],
        [-2.2024e-05, -1.5959e-05,  1.6764e-05,  ..., -1.8656e-05,
         -1.5661e-05, -1.5929e-05],
        [-1.5914e-05, -1.0878e-05,  1.1727e-05,  ..., -1.3649e-05,
         -1.1496e-05, -1.2331e-05],
        [-2.0489e-05, -1.5259e-05,  1.5184e-05,  ..., -1.7673e-05,
         -1.5318e-05, -1.3724e-05]], device='cuda:0')
Loss: 0.9685195684432983


Running epoch 1, step 1446, batch 398
Sampled inputs[:2]: tensor([[    0,  1478,    14,  ...,   266,  9417,  9105],
        [    0, 21748,   792,  ...,   408,   266, 31879]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1529e-05, -8.2002e-05,  7.8439e-05,  ..., -1.4520e-04,
          1.4275e-04, -3.3433e-04],
        [-9.4697e-06, -6.6571e-06,  6.6198e-06,  ..., -8.1956e-06,
         -6.8843e-06, -7.0818e-06],
        [ 4.1693e-05,  6.7505e-05, -4.8676e-05,  ...,  4.6477e-05,
          6.3017e-05,  2.4707e-05],
        [-1.8612e-05, -1.2830e-05,  1.3635e-05,  ..., -1.6078e-05,
         -1.3627e-05, -1.4521e-05],
        [-2.4050e-05, -1.8045e-05,  1.7747e-05,  ..., -2.0877e-05,
         -1.8179e-05, -1.6257e-05]], device='cuda:0')
Loss: 0.9895451068878174


Running epoch 1, step 1447, batch 399
Sampled inputs[:2]: tensor([[    0, 11661,    12,  ...,  1707,   394,   264],
        [    0, 25845,  4034,  ...,   474,   221,   474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3764e-04, -1.0844e-05, -9.1144e-05,  ..., -1.2684e-04,
         -6.1533e-05, -3.1380e-04],
        [-1.0729e-05, -7.6257e-06,  7.6108e-06,  ..., -9.2685e-06,
         -7.7710e-06, -8.0056e-06],
        [ 3.8132e-05,  6.4748e-05, -4.5696e-05,  ...,  4.3512e-05,
          6.0573e-05,  2.2188e-05],
        [-2.1100e-05, -1.4693e-05,  1.5691e-05,  ..., -1.8194e-05,
         -1.5371e-05, -1.6443e-05],
        [-2.7150e-05, -2.0534e-05,  2.0295e-05,  ..., -2.3499e-05,
         -2.0444e-05, -1.8254e-05]], device='cuda:0')
Loss: 0.9280601143836975
Graident accumulation at epoch 1, step 1447, batch 399
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0190],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.7160e-05,  1.1483e-04, -8.3787e-05,  ...,  3.3282e-05,
         -1.8895e-04, -5.9455e-05],
        [-1.1759e-05, -8.3262e-06,  7.9946e-06,  ..., -9.9445e-06,
         -7.9593e-06, -8.4863e-06],
        [ 1.4821e-06,  1.7243e-05, -9.0073e-06,  ...,  6.5871e-06,
          1.4651e-05, -5.6318e-06],
        [ 5.2935e-06,  1.7618e-05, -6.5790e-06,  ...,  9.0945e-06,
          1.3021e-05,  3.7018e-06],
        [-2.6163e-05, -1.8850e-05,  1.8529e-05,  ..., -2.2368e-05,
         -1.9104e-05, -1.7581e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8751e-08, 5.3232e-08, 5.2794e-08,  ..., 1.9731e-08, 1.2035e-07,
         4.4441e-08],
        [8.3147e-11, 5.0729e-11, 2.0806e-11,  ..., 5.7605e-11, 1.9919e-11,
         2.5224e-11],
        [1.9090e-09, 1.3166e-09, 5.3823e-10,  ..., 1.4308e-09, 7.3369e-10,
         4.7397e-10],
        [8.6329e-10, 9.9906e-10, 3.7100e-10,  ..., 8.4898e-10, 5.9409e-10,
         3.5980e-10],
        [4.1488e-10, 2.4079e-10, 8.3972e-11,  ..., 3.0244e-10, 8.3451e-11,
         1.2216e-10]], device='cuda:0')
optimizer state dict: 181.0
lr: [4.6078749934927426e-06, 4.6078749934927426e-06]
scheduler_last_epoch: 181


Running epoch 1, step 1448, batch 400
Sampled inputs[:2]: tensor([[   0, 1420, 6319,  ...,  292, 4895, 4050],
        [   0,  259, 1329,  ...,  266,  706, 1663]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0771e-05,  9.8763e-05,  4.9719e-05,  ..., -5.1664e-05,
          1.5975e-04,  4.8412e-05],
        [-1.3933e-06, -1.1027e-06,  9.1642e-07,  ..., -1.2368e-06,
         -1.0580e-06, -9.9838e-07],
        [-3.7402e-06, -3.1143e-06,  2.6524e-06,  ..., -3.3230e-06,
         -2.8759e-06, -2.7120e-06],
        [-2.6971e-06, -2.1160e-06,  1.8328e-06,  ..., -2.3991e-06,
         -2.0862e-06, -2.0415e-06],
        [-3.5167e-06, -2.9951e-06,  2.4289e-06,  ..., -3.1739e-06,
         -2.8312e-06, -2.3842e-06]], device='cuda:0')
Loss: 0.9836987853050232


Running epoch 1, step 1449, batch 401
Sampled inputs[:2]: tensor([[    0,  2286,  1085,  ...,  1387,  1184,  1802],
        [    0,   266,   283,  ...,   271, 48829,   580]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8127e-04,  1.3676e-04,  2.0005e-05,  ..., -2.3891e-04,
          4.2065e-05, -8.1866e-05],
        [-2.6822e-06, -2.1830e-06,  1.8589e-06,  ..., -2.4214e-06,
         -2.0936e-06, -1.9297e-06],
        [-7.3612e-06, -6.2585e-06,  5.4687e-06,  ..., -6.6161e-06,
         -5.7667e-06, -5.3048e-06],
        [-5.2601e-06, -4.2319e-06,  3.7700e-06,  ..., -4.7535e-06,
         -4.1574e-06, -3.9786e-06],
        [-6.8396e-06, -5.9754e-06,  4.9770e-06,  ..., -6.2585e-06,
         -5.6475e-06, -4.6045e-06]], device='cuda:0')
Loss: 0.9973547458648682


Running epoch 1, step 1450, batch 402
Sampled inputs[:2]: tensor([[    0,  5260,   365,  ...,  7242,   471,   391],
        [    0, 43788,    12,  ...,    12,  6288,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8403e-05,  2.0695e-04,  3.0426e-05,  ..., -2.0939e-04,
          1.8616e-04,  3.8914e-05],
        [-4.0382e-06, -3.0883e-06,  2.6822e-06,  ..., -3.5912e-06,
         -3.1590e-06, -2.9802e-06],
        [-1.1072e-05, -8.8960e-06,  7.9572e-06,  ..., -9.8050e-06,
         -8.7172e-06, -8.1062e-06],
        [-7.9572e-06, -5.9977e-06,  5.4687e-06,  ..., -7.1079e-06,
         -6.3479e-06, -6.1691e-06],
        [-1.0327e-05, -8.5384e-06,  7.2718e-06,  ..., -9.3132e-06,
         -8.5384e-06, -7.0333e-06]], device='cuda:0')
Loss: 0.9668856859207153


Running epoch 1, step 1451, batch 403
Sampled inputs[:2]: tensor([[    0,  2229,   352,  ...,  4988,    33,    13],
        [    0, 23988, 26825,  ...,   373,   221,   334]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6269e-05,  3.5010e-04,  1.2016e-04,  ..., -1.7124e-04,
          3.1644e-04,  3.2496e-05],
        [-5.3719e-06, -4.0866e-06,  3.5800e-06,  ..., -4.7758e-06,
         -4.1798e-06, -3.9078e-06],
        [-1.4514e-05, -1.1623e-05,  1.0490e-05,  ..., -1.2815e-05,
         -1.1340e-05, -1.0401e-05],
        [-1.0520e-05, -7.9200e-06,  7.2643e-06,  ..., -9.4026e-06,
         -8.3596e-06, -8.0392e-06],
        [-1.3635e-05, -1.1221e-05,  9.6709e-06,  ..., -1.2234e-05,
         -1.1146e-05, -9.0748e-06]], device='cuda:0')
Loss: 0.9769482612609863


Running epoch 1, step 1452, batch 404
Sampled inputs[:2]: tensor([[   0, 1730, 2068,  ...,  445, 2704,  445],
        [   0,  270,  472,  ...,  292,   73,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6474e-04,  2.4059e-04,  2.1857e-04,  ...,  2.1917e-05,
          4.4079e-04,  3.7289e-04],
        [-6.6683e-06, -5.0366e-06,  4.3735e-06,  ..., -5.9605e-06,
         -5.2154e-06, -4.9211e-06],
        [-1.8194e-05, -1.4454e-05,  1.2934e-05,  ..., -1.6138e-05,
         -1.4260e-05, -1.3217e-05],
        [-1.3188e-05, -9.8422e-06,  8.9183e-06,  ..., -1.1891e-05,
         -1.0580e-05, -1.0245e-05],
        [-1.7032e-05, -1.3888e-05,  1.1906e-05,  ..., -1.5318e-05,
         -1.3918e-05, -1.1519e-05]], device='cuda:0')
Loss: 0.9634335041046143


Running epoch 1, step 1453, batch 405
Sampled inputs[:2]: tensor([[    0,  2088,  1745,  ...,   293, 16489,    12],
        [    0,  1428,   266,  ...,  3169,  3058,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2238e-04,  1.9319e-04,  3.2616e-04,  ..., -4.4427e-06,
          4.4185e-04,  2.8768e-04],
        [-8.0168e-06, -6.1691e-06,  5.2936e-06,  ..., -7.1675e-06,
         -6.3032e-06, -5.8301e-06],
        [-2.1964e-05, -1.7732e-05,  1.5676e-05,  ..., -1.9506e-05,
         -1.7256e-05, -1.5780e-05],
        [-1.5810e-05, -1.2018e-05,  1.0766e-05,  ..., -1.4231e-05,
         -1.2696e-05, -1.2130e-05],
        [-2.0519e-05, -1.7002e-05,  1.4365e-05,  ..., -1.8477e-05,
         -1.6809e-05, -1.3709e-05]], device='cuda:0')
Loss: 0.9896976351737976


Running epoch 1, step 1454, batch 406
Sampled inputs[:2]: tensor([[    0,  1456, 32380,  ...,    12,  1172, 12557],
        [    0,   515,   352,  ...,    40, 25575,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0050e-04,  2.4970e-04,  2.5167e-04,  ...,  7.4139e-05,
          2.8216e-04,  1.9445e-04],
        [-9.2685e-06, -7.0557e-06,  6.3144e-06,  ..., -8.2627e-06,
         -7.1563e-06, -6.7018e-06],
        [-2.5347e-05, -2.0221e-05,  1.8656e-05,  ..., -2.2426e-05,
         -1.9550e-05, -1.8135e-05],
        [-1.8314e-05, -1.3754e-05,  1.2927e-05,  ..., -1.6406e-05,
         -1.4387e-05, -1.3977e-05],
        [-2.3469e-05, -1.9312e-05,  1.6898e-05,  ..., -2.1100e-05,
         -1.8999e-05, -1.5579e-05]], device='cuda:0')
Loss: 0.9567397236824036


Running epoch 1, step 1455, batch 407
Sampled inputs[:2]: tensor([[    0,  2348,   565,  ...,    12,   709,   266],
        [    0,  2834,   266,  ..., 39474,    12, 15441]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4984e-04,  2.1880e-04,  1.1679e-04,  ...,  1.3833e-04,
          1.0085e-04, -3.1787e-04],
        [-1.0535e-05, -7.8753e-06,  7.2755e-06,  ..., -9.3654e-06,
         -7.9833e-06, -7.6555e-06],
        [-2.8893e-05, -2.2531e-05,  2.1532e-05,  ..., -2.5392e-05,
         -2.1756e-05, -2.0683e-05],
        [-2.0802e-05, -1.5289e-05,  1.4894e-05,  ..., -1.8552e-05,
         -1.5981e-05, -1.5929e-05],
        [-2.6613e-05, -2.1458e-05,  1.9401e-05,  ..., -2.3782e-05,
         -2.1100e-05, -1.7636e-05]], device='cuda:0')
Loss: 0.9300639033317566
Graident accumulation at epoch 1, step 1455, batch 407
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0141,  0.0024,  ..., -0.0020,  0.0235, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0143, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 9.4428e-05,  1.2523e-04, -6.3730e-05,  ...,  4.3787e-05,
         -1.5997e-04, -8.5296e-05],
        [-1.1637e-05, -8.2811e-06,  7.9227e-06,  ..., -9.8866e-06,
         -7.9617e-06, -8.4033e-06],
        [-1.5554e-06,  1.3266e-05, -5.9534e-06,  ...,  3.3892e-06,
          1.1011e-05, -7.1369e-06],
        [ 2.6840e-06,  1.4327e-05, -4.4317e-06,  ...,  6.3298e-06,
          1.0120e-05,  1.7387e-06],
        [-2.6208e-05, -1.9111e-05,  1.8616e-05,  ..., -2.2509e-05,
         -1.9304e-05, -1.7587e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8754e-08, 5.3227e-08, 5.2755e-08,  ..., 1.9730e-08, 1.2024e-07,
         4.4498e-08],
        [8.3175e-11, 5.0740e-11, 2.0839e-11,  ..., 5.7635e-11, 1.9963e-11,
         2.5257e-11],
        [1.9079e-09, 1.3158e-09, 5.3816e-10,  ..., 1.4300e-09, 7.3343e-10,
         4.7392e-10],
        [8.6286e-10, 9.9829e-10, 3.7085e-10,  ..., 8.4847e-10, 5.9375e-10,
         3.5970e-10],
        [4.1517e-10, 2.4101e-10, 8.4265e-11,  ..., 3.0271e-10, 8.3813e-11,
         1.2235e-10]], device='cuda:0')
optimizer state dict: 182.0
lr: [4.504183453666481e-06, 4.504183453666481e-06]
scheduler_last_epoch: 182


Running epoch 1, step 1456, batch 408
Sampled inputs[:2]: tensor([[    0,  2939,    14,  ...,  1702,  1481,   278],
        [    0,   521,   486,  ...,   278, 25182,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0757e-05,  7.3582e-05, -1.5053e-04,  ...,  6.6309e-05,
          7.3778e-05, -2.0256e-04],
        [-1.1399e-06, -8.1956e-07,  8.5682e-07,  ..., -1.0952e-06,
         -9.0897e-07, -9.9093e-07],
        [-3.2037e-06, -2.2650e-06,  2.6673e-06,  ..., -2.8908e-06,
         -2.3693e-06, -2.5481e-06],
        [-2.3991e-06, -1.6168e-06,  1.8999e-06,  ..., -2.2650e-06,
         -1.8701e-06, -2.1458e-06],
        [-2.9206e-06, -2.1011e-06,  2.3693e-06,  ..., -2.6375e-06,
         -2.2501e-06, -2.0862e-06]], device='cuda:0')
Loss: 0.9567080736160278


Running epoch 1, step 1457, batch 409
Sampled inputs[:2]: tensor([[    0,  6702, 18279,  ...,    14, 47571,    12],
        [    0,   278,  5717,  ...,  5342,  5147,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1377e-05,  3.4544e-04,  1.1025e-04,  ...,  1.5312e-04,
          6.0905e-05, -2.5156e-04],
        [-2.3544e-06, -1.7583e-06,  1.4938e-06,  ..., -2.2277e-06,
         -1.9446e-06, -1.9893e-06],
        [-6.4373e-06, -4.8578e-06,  4.6790e-06,  ..., -5.8264e-06,
         -5.0962e-06, -5.0962e-06],
        [-4.8727e-06, -3.4794e-06,  3.2336e-06,  ..., -4.5896e-06,
         -4.0457e-06, -4.2468e-06],
        [-6.0201e-06, -4.6045e-06,  4.2245e-06,  ..., -5.4836e-06,
         -4.9919e-06, -4.3213e-06]], device='cuda:0')
Loss: 0.9094269871711731


Running epoch 1, step 1458, batch 410
Sampled inputs[:2]: tensor([[    0,   984,    13,  ...,    13, 37385,   490],
        [    0,  3889,  4039,  ...,   616, 22910,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2431e-05,  2.8726e-04,  6.5292e-06,  ...,  9.2134e-05,
          1.0906e-04, -1.8252e-04],
        [-3.6955e-06, -2.7418e-06,  2.5593e-06,  ..., -3.3826e-06,
         -2.8908e-06, -2.8983e-06],
        [-1.0252e-05, -7.7933e-06,  7.8976e-06,  ..., -9.1344e-06,
         -7.8380e-06, -7.7486e-06],
        [-7.5549e-06, -5.4315e-06,  5.4687e-06,  ..., -6.8992e-06,
         -5.9381e-06, -6.1542e-06],
        [-9.3579e-06, -7.3016e-06,  6.9663e-06,  ..., -8.4639e-06,
         -7.5698e-06, -6.4820e-06]], device='cuda:0')
Loss: 0.9707580804824829


Running epoch 1, step 1459, batch 411
Sampled inputs[:2]: tensor([[    0,   221,  6872,  ...,   806,   518,   266],
        [    0,   733,   560,  ...,  1172, 22808,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4801e-05,  7.9688e-05, -2.4461e-05,  ...,  8.1631e-05,
          4.4816e-04,  3.9495e-04],
        [-5.0366e-06, -3.6173e-06,  3.5353e-06,  ..., -4.5821e-06,
         -3.8147e-06, -3.9116e-06],
        [-1.3858e-05, -1.0237e-05,  1.0729e-05,  ..., -1.2293e-05,
         -1.0312e-05, -1.0386e-05],
        [-1.0192e-05, -7.1079e-06,  7.4953e-06,  ..., -9.2387e-06,
         -7.7561e-06, -8.2254e-06],
        [-1.2666e-05, -9.6262e-06,  9.4995e-06,  ..., -1.1399e-05,
         -9.9689e-06, -8.7321e-06]], device='cuda:0')
Loss: 0.9463445544242859


Running epoch 1, step 1460, batch 412
Sampled inputs[:2]: tensor([[   0,   14,  560,  ..., 1248, 1398, 1268],
        [   0,  368, 2035,  ...,  266, 1122,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4571e-04,  1.3364e-04,  5.7154e-07,  ..., -4.4289e-05,
          4.7477e-04,  2.5514e-04],
        [-6.3926e-06, -4.5933e-06,  4.4368e-06,  ..., -5.7742e-06,
         -4.8131e-06, -4.9397e-06],
        [-1.7762e-05, -1.3128e-05,  1.3545e-05,  ..., -1.5646e-05,
         -1.3158e-05, -1.3262e-05],
        [-1.2890e-05, -9.0003e-06,  9.3803e-06,  ..., -1.1593e-05,
         -9.7379e-06, -1.0341e-05],
        [-1.6183e-05, -1.2293e-05,  1.1973e-05,  ..., -1.4499e-05,
         -1.2696e-05, -1.1146e-05]], device='cuda:0')
Loss: 0.9411389231681824


Running epoch 1, step 1461, batch 413
Sampled inputs[:2]: tensor([[    0,    20, 13016,  ...,    14,  2743,   516],
        [    0,   266,  1336,  ...,  1841,  9705,  1219]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0385e-05,  2.2840e-04,  6.0250e-05,  ...,  1.2244e-04,
          2.9900e-04,  1.0394e-04],
        [-7.6964e-06, -5.3905e-06,  5.3458e-06,  ..., -6.9216e-06,
         -5.7667e-06, -5.9530e-06],
        [-2.1249e-05, -1.5348e-05,  1.6227e-05,  ..., -1.8626e-05,
         -1.5631e-05, -1.5810e-05],
        [-1.5482e-05, -1.0543e-05,  1.1273e-05,  ..., -1.3888e-05,
         -1.1645e-05, -1.2428e-05],
        [-1.9476e-05, -1.4454e-05,  1.4447e-05,  ..., -1.7360e-05,
         -1.5169e-05, -1.3366e-05]], device='cuda:0')
Loss: 0.9508581161499023


Running epoch 1, step 1462, batch 414
Sampled inputs[:2]: tensor([[    0,   792,    83,  ..., 29085, 15914,   365],
        [    0, 16187,   565,  ...,   586,  3196,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0096e-04,  2.3736e-04,  1.7796e-04,  ...,  8.6234e-05,
          2.4625e-04,  1.3129e-04],
        [-9.0152e-06, -6.3516e-06,  6.3665e-06,  ..., -8.0317e-06,
         -6.7092e-06, -6.8583e-06],
        [-2.4870e-05, -1.8135e-05,  1.9222e-05,  ..., -2.1711e-05,
         -1.8254e-05, -1.8343e-05],
        [-1.8090e-05, -1.2435e-05,  1.3418e-05,  ..., -1.6078e-05,
         -1.3515e-05, -1.4335e-05],
        [-2.2665e-05, -1.7032e-05,  1.7025e-05,  ..., -2.0161e-05,
         -1.7658e-05, -1.5438e-05]], device='cuda:0')
Loss: 0.9544277787208557


Running epoch 1, step 1463, batch 415
Sampled inputs[:2]: tensor([[   0,  409,  394,  ...,  475, 5458,  328],
        [   0, 2771,   13,  ..., 1412,   35,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2185e-05,  2.1538e-04,  1.8952e-04,  ...,  2.3606e-04,
          4.4834e-04,  3.3690e-04],
        [-1.0282e-05, -7.2904e-06,  7.1563e-06,  ..., -9.2238e-06,
         -7.7300e-06, -7.8864e-06],
        [-2.8402e-05, -2.0817e-05,  2.1651e-05,  ..., -2.4930e-05,
         -2.1011e-05, -2.1070e-05],
        [-2.0638e-05, -1.4268e-05,  1.5043e-05,  ..., -1.8492e-05,
         -1.5602e-05, -1.6481e-05],
        [-2.5958e-05, -1.9565e-05,  1.9245e-05,  ..., -2.3186e-05,
         -2.0325e-05, -1.7807e-05]], device='cuda:0')
Loss: 0.9866458773612976
Graident accumulation at epoch 1, step 1463, batch 415
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.6766e-05,  1.3424e-04, -3.8405e-05,  ...,  6.3015e-05,
         -9.9137e-05, -4.3077e-05],
        [-1.1501e-05, -8.1820e-06,  7.8461e-06,  ..., -9.8203e-06,
         -7.9386e-06, -8.3516e-06],
        [-4.2400e-06,  9.8576e-06, -3.1929e-06,  ...,  5.5730e-07,
          7.8085e-06, -8.5303e-06],
        [ 3.5175e-07,  1.1468e-05, -2.4842e-06,  ...,  3.8476e-06,
          7.5483e-06, -8.3224e-08],
        [-2.6183e-05, -1.9156e-05,  1.8679e-05,  ..., -2.2577e-05,
         -1.9406e-05, -1.7609e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8702e-08, 5.3220e-08, 5.2738e-08,  ..., 1.9766e-08, 1.2032e-07,
         4.4567e-08],
        [8.3197e-11, 5.0742e-11, 2.0869e-11,  ..., 5.7663e-11, 2.0003e-11,
         2.5294e-11],
        [1.9068e-09, 1.3149e-09, 5.3809e-10,  ..., 1.4292e-09, 7.3314e-10,
         4.7389e-10],
        [8.6242e-10, 9.9750e-10, 3.7071e-10,  ..., 8.4797e-10, 5.9340e-10,
         3.5961e-10],
        [4.1543e-10, 2.4116e-10, 8.4551e-11,  ..., 3.0294e-10, 8.4142e-11,
         1.2254e-10]], device='cuda:0')
optimizer state dict: 183.0
lr: [4.401331722582158e-06, 4.401331722582158e-06]
scheduler_last_epoch: 183


Running epoch 1, step 1464, batch 416
Sampled inputs[:2]: tensor([[    0,  6660, 13165,  ...,   380,   333,   199],
        [    0,   199,   769,  ...,   380,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.8014e-06, -1.7842e-05, -1.4525e-04,  ..., -1.1660e-04,
         -1.6968e-05, -6.4917e-05],
        [-1.2890e-06, -9.1270e-07,  8.7172e-07,  ..., -1.1474e-06,
         -1.0133e-06, -9.8348e-07],
        [-3.6508e-06, -2.6971e-06,  2.6673e-06,  ..., -3.1888e-06,
         -2.8163e-06, -2.7567e-06],
        [-2.6375e-06, -1.8179e-06,  1.8552e-06,  ..., -2.3395e-06,
         -2.0862e-06, -2.1309e-06],
        [-3.4273e-06, -2.6375e-06,  2.4587e-06,  ..., -3.0696e-06,
         -2.8014e-06, -2.4140e-06]], device='cuda:0')
Loss: 0.9914169907569885


Running epoch 1, step 1465, batch 417
Sampled inputs[:2]: tensor([[    0,   301,   298,  ...,   806,   352, 22105],
        [    0,  1342,    14,  ...,  1236, 15667, 12931]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.3616e-05, -2.4908e-05, -1.5483e-04,  ..., -2.9026e-04,
         -2.5153e-04, -1.3248e-04],
        [-2.6152e-06, -1.9707e-06,  1.9744e-06,  ..., -2.3022e-06,
         -1.9409e-06, -1.9073e-06],
        [-7.5549e-06, -5.9307e-06,  6.0499e-06,  ..., -6.5714e-06,
         -5.5283e-06, -5.4836e-06],
        [-5.2750e-06, -3.8892e-06,  4.1202e-06,  ..., -4.6194e-06,
         -3.9041e-06, -4.0382e-06],
        [-6.7651e-06, -5.5134e-06,  5.2899e-06,  ..., -6.0350e-06,
         -5.3048e-06, -4.5896e-06]], device='cuda:0')
Loss: 0.9762375950813293


Running epoch 1, step 1466, batch 418
Sampled inputs[:2]: tensor([[   0,   12,  221,  ...,  593,  360,  726],
        [   0,  607,  259,  ...,  995,   13, 6507]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3728e-04,  1.2081e-05, -1.0825e-04,  ..., -2.7708e-04,
         -3.4896e-04,  5.9388e-05],
        [-3.9190e-06, -2.9057e-06,  2.9281e-06,  ..., -3.4049e-06,
         -2.9467e-06, -2.8238e-06],
        [-1.1116e-05, -8.6278e-06,  8.8513e-06,  ..., -9.5814e-06,
         -8.2999e-06, -7.9721e-06],
        [-7.9423e-06, -5.7966e-06,  6.1765e-06,  ..., -6.8843e-06,
         -5.9903e-06, -6.0350e-06],
        [-9.9987e-06, -8.1062e-06,  7.7933e-06,  ..., -8.8364e-06,
         -7.9870e-06, -6.7055e-06]], device='cuda:0')
Loss: 0.9562738537788391


Running epoch 1, step 1467, batch 419
Sampled inputs[:2]: tensor([[    0,   668,  1837,  ...,  4381,    14, 11451],
        [    0,  4014,    88,  ...,  1103,    14,  1771]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1360e-04, -3.4537e-05, -1.4496e-04,  ..., -2.0106e-04,
         -1.9102e-04,  2.2653e-04],
        [-5.2303e-06, -3.8818e-06,  3.8408e-06,  ..., -4.5598e-06,
         -3.9525e-06, -3.7737e-06],
        [ 6.2962e-05,  5.0591e-05, -3.0581e-05,  ...,  4.0642e-05,
          8.8090e-05,  2.4691e-05],
        [-1.0684e-05, -7.8082e-06,  8.1286e-06,  ..., -9.3132e-06,
         -8.1211e-06, -8.1211e-06],
        [-1.3247e-05, -1.0744e-05,  1.0177e-05,  ..., -1.1712e-05,
         -1.0580e-05, -8.8662e-06]], device='cuda:0')
Loss: 0.9796648025512695


Running epoch 1, step 1468, batch 420
Sampled inputs[:2]: tensor([[   0, 1253,  287,  ..., 2988,   14,  417],
        [   0,  221,  374,  ..., 2296,  365, 4579]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2612e-05,  2.8276e-05,  1.3070e-04,  ..., -4.0175e-04,
          2.0874e-04,  3.5516e-04],
        [-6.4820e-06, -4.9099e-06,  4.6790e-06,  ..., -5.7369e-06,
         -5.0850e-06, -4.7497e-06],
        [ 5.9505e-05,  4.7640e-05, -2.8018e-05,  ...,  3.7454e-05,
          8.4991e-05,  2.2039e-05],
        [-1.3173e-05, -9.8497e-06,  9.8869e-06,  ..., -1.1668e-05,
         -1.0416e-05, -1.0177e-05],
        [-1.6510e-05, -1.3590e-05,  1.2532e-05,  ..., -1.4752e-05,
         -1.3620e-05, -1.1176e-05]], device='cuda:0')
Loss: 0.9516969919204712


Running epoch 1, step 1469, batch 421
Sampled inputs[:2]: tensor([[    0,  8416,   669,  ...,   298,   894,   496],
        [    0,    12,   461,  ...,  2525,   278, 23762]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4528e-04,  9.1000e-05,  2.4684e-04,  ..., -4.0595e-04,
          3.4726e-04,  4.6538e-04],
        [-7.7710e-06, -5.8860e-06,  5.5917e-06,  ..., -6.8620e-06,
         -6.0275e-06, -5.6326e-06],
        [ 5.5974e-05,  4.4809e-05, -2.5335e-05,  ...,  3.4369e-05,
          8.2368e-05,  1.9595e-05],
        [-1.5765e-05, -1.1802e-05,  1.1794e-05,  ..., -1.3933e-05,
         -1.2316e-05, -1.2033e-05],
        [-1.9684e-05, -1.6198e-05,  1.4871e-05,  ..., -1.7568e-05,
         -1.6108e-05, -1.3217e-05]], device='cuda:0')
Loss: 0.9635691046714783


Running epoch 1, step 1470, batch 422
Sampled inputs[:2]: tensor([[    0,   300,  3808,  ...,   496,    14,  1364],
        [    0,   328,   266,  ...,   352, 13107,  4302]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9202e-04,  3.7541e-05,  1.5170e-04,  ..., -4.6848e-04,
          4.3638e-04,  5.8950e-04],
        [-9.0823e-06, -6.8992e-06,  6.5826e-06,  ..., -8.0019e-06,
         -6.9775e-06, -6.5453e-06],
        [ 5.2323e-05,  4.1859e-05, -2.2415e-05,  ...,  3.1195e-05,
          7.9701e-05,  1.7032e-05],
        [-1.8328e-05, -1.3769e-05,  1.3806e-05,  ..., -1.6168e-05,
         -1.4178e-05, -1.3925e-05],
        [-2.2933e-05, -1.8939e-05,  1.7419e-05,  ..., -2.0459e-05,
         -1.8641e-05, -1.5333e-05]], device='cuda:0')
Loss: 0.9859016537666321


Running epoch 1, step 1471, batch 423
Sampled inputs[:2]: tensor([[    0,   669,    14,  ...,   596,   292,   494],
        [    0,  1901, 11083,  ...,   360,  6055,  2374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1088e-05,  9.1237e-05,  2.0964e-04,  ..., -3.3812e-04,
          4.5669e-04,  5.7710e-04],
        [-1.0423e-05, -7.8417e-06,  7.5139e-06,  ..., -9.2164e-06,
         -7.9907e-06, -7.5884e-06],
        [ 4.8597e-05,  3.9117e-05, -1.9613e-05,  ...,  2.7857e-05,
          7.6900e-05,  1.4201e-05],
        [-2.0981e-05, -1.5594e-05,  1.5728e-05,  ..., -1.8582e-05,
         -1.6205e-05, -1.6086e-05],
        [-2.6375e-05, -2.1562e-05,  1.9953e-05,  ..., -2.3603e-05,
         -2.1368e-05, -1.7762e-05]], device='cuda:0')
Loss: 0.9651762843132019
Graident accumulation at epoch 1, step 1471, batch 423
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3199e-05,  1.2994e-04, -1.3601e-05,  ...,  2.2901e-05,
         -4.3555e-05,  1.8940e-05],
        [-1.1393e-05, -8.1480e-06,  7.8129e-06,  ..., -9.7599e-06,
         -7.9438e-06, -8.2753e-06],
        [ 1.0437e-06,  1.2784e-05, -4.8349e-06,  ...,  3.2873e-06,
          1.4718e-05, -6.2572e-06],
        [-1.7815e-06,  8.7617e-06, -6.6300e-07,  ...,  1.6047e-06,
          5.1730e-06, -1.6835e-06],
        [-2.6203e-05, -1.9397e-05,  1.8806e-05,  ..., -2.2680e-05,
         -1.9602e-05, -1.7624e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8645e-08, 5.3175e-08, 5.2729e-08,  ..., 1.9861e-08, 1.2041e-07,
         4.4855e-08],
        [8.3223e-11, 5.0753e-11, 2.0905e-11,  ..., 5.7690e-11, 2.0047e-11,
         2.5326e-11],
        [1.9073e-09, 1.3151e-09, 5.3793e-10,  ..., 1.4285e-09, 7.3832e-10,
         4.7362e-10],
        [8.6200e-10, 9.9674e-10, 3.7058e-10,  ..., 8.4746e-10, 5.9307e-10,
         3.5951e-10],
        [4.1571e-10, 2.4138e-10, 8.4864e-11,  ..., 3.0320e-10, 8.4515e-11,
         1.2274e-10]], device='cuda:0')
optimizer state dict: 184.0
lr: [4.299335516882092e-06, 4.299335516882092e-06]
scheduler_last_epoch: 184


Running epoch 1, step 1472, batch 424
Sampled inputs[:2]: tensor([[   0,  342,  721,  ..., 2429,   14,  475],
        [   0, 7303,   12,  ..., 1085,  413,  711]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.2287e-05,  6.2495e-05, -1.6188e-05,  ..., -4.5978e-05,
          4.6631e-05,  5.4724e-05],
        [-1.2517e-06, -8.4564e-07,  9.3877e-07,  ..., -1.1101e-06,
         -8.6799e-07, -8.9407e-07],
        [-3.3826e-06, -2.4438e-06,  2.7567e-06,  ..., -2.9355e-06,
         -2.2948e-06, -2.3544e-06],
        [-2.5332e-06, -1.6764e-06,  1.9968e-06,  ..., -2.2501e-06,
         -1.7583e-06, -1.9222e-06],
        [-3.0249e-06, -2.2799e-06,  2.3991e-06,  ..., -2.6822e-06,
         -2.2054e-06, -1.9073e-06]], device='cuda:0')
Loss: 0.9314369559288025


Running epoch 1, step 1473, batch 425
Sampled inputs[:2]: tensor([[    0, 38495, 36253,  ..., 11006,  5699,    19],
        [    0,    13,  4831,  ...,   333,   199,  2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6650e-05, -5.6434e-05, -1.4503e-04,  ..., -9.8026e-05,
         -4.0712e-06,  4.2369e-05],
        [-2.5555e-06, -1.7844e-06,  1.9372e-06,  ..., -2.2501e-06,
         -1.8813e-06, -1.8775e-06],
        [-6.9439e-06, -5.1707e-06,  5.7220e-06,  ..., -6.0350e-06,
         -5.1111e-06, -5.0366e-06],
        [-5.0515e-06, -3.4571e-06,  4.0382e-06,  ..., -4.4554e-06,
         -3.7551e-06, -3.9339e-06],
        [-6.1840e-06, -4.8131e-06,  4.9472e-06,  ..., -5.5134e-06,
         -4.8727e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9655248522758484


Running epoch 1, step 1474, batch 426
Sampled inputs[:2]: tensor([[    0, 48545,    26,  ...,  1471,   266,   319],
        [    0,   344,   259,  ...,  6787, 10045,  9799]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2921e-04, -8.1181e-05, -3.2049e-04,  ..., -2.1883e-04,
         -8.5533e-05, -1.4556e-04],
        [-3.7774e-06, -2.5295e-06,  2.8796e-06,  ..., -3.3751e-06,
         -2.8275e-06, -2.8610e-06],
        [-1.0222e-05, -7.2122e-06,  8.4490e-06,  ..., -8.9407e-06,
         -7.5549e-06, -7.5102e-06],
        [-7.5102e-06, -4.8801e-06,  6.0350e-06,  ..., -6.7204e-06,
         -5.6624e-06, -5.9903e-06],
        [-9.2387e-06, -6.8098e-06,  7.4357e-06,  ..., -8.2552e-06,
         -7.2718e-06, -6.2287e-06]], device='cuda:0')
Loss: 0.9294981956481934


Running epoch 1, step 1475, batch 427
Sampled inputs[:2]: tensor([[   0, 1103,  271,  ...,  957,  756,  368],
        [   0,   14, 3609,  ...,  298,  413,   29]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0086e-04, -2.1922e-06, -8.2198e-05,  ..., -3.0098e-04,
         -5.2861e-05, -1.1443e-04],
        [-5.0664e-06, -3.5875e-06,  3.8147e-06,  ..., -4.5449e-06,
         -3.8780e-06, -3.7737e-06],
        [-1.3828e-05, -1.0341e-05,  1.1250e-05,  ..., -1.2204e-05,
         -1.0535e-05, -1.0103e-05],
        [-1.0088e-05, -6.9812e-06,  7.9572e-06,  ..., -9.0599e-06,
         -7.7933e-06, -7.8976e-06],
        [-1.2487e-05, -9.7454e-06,  9.8944e-06,  ..., -1.1280e-05,
         -1.0133e-05, -8.4341e-06]], device='cuda:0')
Loss: 0.9666382670402527


Running epoch 1, step 1476, batch 428
Sampled inputs[:2]: tensor([[   0, 1144, 2680,  ...,  963,    9, 1184],
        [   0, 5583,  598,  ...,  199,  395, 6551]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2654e-04,  2.7012e-05, -3.9324e-05,  ..., -2.7655e-04,
         -8.9929e-05, -3.9194e-04],
        [-6.2808e-06, -4.5039e-06,  4.8205e-06,  ..., -5.6028e-06,
         -4.7535e-06, -4.6119e-06],
        [-1.7270e-05, -1.3024e-05,  1.4320e-05,  ..., -1.5125e-05,
         -1.2964e-05, -1.2413e-05],
        [-1.2547e-05, -8.7917e-06,  1.0118e-05,  ..., -1.1176e-05,
         -9.5516e-06, -9.6783e-06],
        [-1.5512e-05, -1.2204e-05,  1.2517e-05,  ..., -1.3903e-05,
         -1.2428e-05, -1.0289e-05]], device='cuda:0')
Loss: 0.9355036020278931


Running epoch 1, step 1477, batch 429
Sampled inputs[:2]: tensor([[    0,  4014,    88,  ...,    14, 11961,    13],
        [    0,   328,  5180,  ...,   344,  2356,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6179e-05, -1.9194e-05, -1.8061e-04,  ..., -3.5262e-04,
         -1.3678e-04, -3.9497e-04],
        [-7.6070e-06, -5.4799e-06,  5.8636e-06,  ..., -6.7353e-06,
         -5.7295e-06, -5.5470e-06],
        [-2.1040e-05, -1.5870e-05,  1.7449e-05,  ..., -1.8314e-05,
         -1.5721e-05, -1.5080e-05],
        [-1.5289e-05, -1.0759e-05,  1.2353e-05,  ..., -1.3515e-05,
         -1.1563e-05, -1.1705e-05],
        [-1.8716e-05, -1.4737e-05,  1.5125e-05,  ..., -1.6689e-05,
         -1.4931e-05, -1.2390e-05]], device='cuda:0')
Loss: 0.9925917983055115


Running epoch 1, step 1478, batch 430
Sampled inputs[:2]: tensor([[    0,   508, 22318,  ...,    13,  1107,  4093],
        [    0,  1197,  3025,  ...,    14,   747,  3739]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7254e-04, -1.4988e-04, -1.9581e-04,  ..., -3.8552e-04,
         -1.0378e-04, -5.2899e-04],
        [-8.8513e-06, -6.3665e-06,  6.9290e-06,  ..., -7.7784e-06,
         -6.5640e-06, -6.4000e-06],
        [-2.4572e-05, -1.8522e-05,  2.0653e-05,  ..., -2.1279e-05,
         -1.8105e-05, -1.7539e-05],
        [-1.7911e-05, -1.2599e-05,  1.4722e-05,  ..., -1.5691e-05,
         -1.3314e-05, -1.3627e-05],
        [-2.1681e-05, -1.7092e-05,  1.7762e-05,  ..., -1.9252e-05,
         -1.7107e-05, -1.4268e-05]], device='cuda:0')
Loss: 0.9862748384475708


Running epoch 1, step 1479, batch 431
Sampled inputs[:2]: tensor([[    0,  4601,   328,  ..., 10258,  2282,    12],
        [    0,   554,  1034,  ...,  3313,   365,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2206e-04, -1.7880e-04, -2.5882e-04,  ..., -3.4113e-04,
          1.1881e-05, -5.2920e-04],
        [-1.0148e-05, -7.3202e-06,  7.8678e-06,  ..., -8.9332e-06,
         -7.5325e-06, -7.3388e-06],
        [-2.8014e-05, -2.1219e-05,  2.3365e-05,  ..., -2.4348e-05,
         -2.0728e-05, -2.0027e-05],
        [-2.0444e-05, -1.4454e-05,  1.6645e-05,  ..., -1.7971e-05,
         -1.5236e-05, -1.5564e-05],
        [-2.4796e-05, -1.9640e-05,  2.0146e-05,  ..., -2.2084e-05,
         -1.9640e-05, -1.6354e-05]], device='cuda:0')
Loss: 0.9732652902603149
Graident accumulation at epoch 1, step 1479, batch 431
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.8085e-05,  9.9069e-05, -3.8122e-05,  ..., -1.3502e-05,
         -3.8011e-05, -3.5874e-05],
        [-1.1269e-05, -8.0652e-06,  7.8184e-06,  ..., -9.6773e-06,
         -7.9027e-06, -8.1816e-06],
        [-1.8621e-06,  9.3832e-06, -2.0149e-06,  ...,  5.2372e-07,
          1.1173e-05, -7.6342e-06],
        [-3.6478e-06,  6.4401e-06,  1.0678e-06,  ..., -3.5287e-07,
          3.1320e-06, -3.0716e-06],
        [-2.6062e-05, -1.9421e-05,  1.8940e-05,  ..., -2.2620e-05,
         -1.9606e-05, -1.7497e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8602e-08, 5.3154e-08, 5.2744e-08,  ..., 1.9957e-08, 1.2029e-07,
         4.5090e-08],
        [8.3243e-11, 5.0756e-11, 2.0946e-11,  ..., 5.7712e-11, 2.0084e-11,
         2.5355e-11],
        [1.9061e-09, 1.3143e-09, 5.3794e-10,  ..., 1.4277e-09, 7.3801e-10,
         4.7355e-10],
        [8.6155e-10, 9.9595e-10, 3.7049e-10,  ..., 8.4694e-10, 5.9271e-10,
         3.5939e-10],
        [4.1591e-10, 2.4152e-10, 8.5185e-11,  ..., 3.0338e-10, 8.4816e-11,
         1.2288e-10]], device='cuda:0')
optimizer state dict: 185.0
lr: [4.19821042247685e-06, 4.19821042247685e-06]
scheduler_last_epoch: 185


Running epoch 1, step 1480, batch 432
Sampled inputs[:2]: tensor([[    0,    14,    69,  ...,   287,   259,  5158],
        [    0,  3227,   278,  ...,  2950,    14, 15544]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6703e-05,  1.5317e-04, -1.9202e-04,  ...,  0.0000e+00,
          1.2790e-05,  1.2258e-04],
        [-1.3113e-06, -9.0897e-07,  8.9779e-07,  ..., -1.1474e-06,
         -1.0431e-06, -1.0282e-06],
        [-3.7551e-06, -2.7865e-06,  2.7716e-06,  ..., -3.2932e-06,
         -3.0100e-06, -2.9653e-06],
        [-2.6524e-06, -1.8105e-06,  1.8701e-06,  ..., -2.3246e-06,
         -2.1458e-06, -2.1905e-06],
        [-3.2932e-06, -2.5481e-06,  2.3544e-06,  ..., -2.9653e-06,
         -2.8014e-06, -2.4289e-06]], device='cuda:0')
Loss: 0.9452487826347351


Running epoch 1, step 1481, batch 433
Sampled inputs[:2]: tensor([[    0,  8450,   292,  ...,   352,   722, 37719],
        [    0,    21,    14,  ...,  1159,  1978, 33323]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3166e-04,  1.8652e-05, -4.7994e-04,  ...,  1.2940e-05,
         -1.2240e-04,  2.2638e-04],
        [-2.6599e-06, -1.7360e-06,  1.8179e-06,  ..., -2.3693e-06,
         -1.9930e-06, -2.2128e-06],
        [-7.5698e-06, -5.2303e-06,  5.6177e-06,  ..., -6.6608e-06,
         -5.6773e-06, -6.1989e-06],
        [-5.2750e-06, -3.3751e-06,  3.7327e-06,  ..., -4.6939e-06,
         -4.0159e-06, -4.5449e-06],
        [-6.9588e-06, -4.9025e-06,  4.9919e-06,  ..., -6.2287e-06,
         -5.4538e-06, -5.3197e-06]], device='cuda:0')
Loss: 0.9463490843772888


Running epoch 1, step 1482, batch 434
Sampled inputs[:2]: tensor([[    0,  1737,   278,  ...,  2604,   367,  2002],
        [    0,   298,   669,  ...,   287, 19731,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1820e-04,  9.8789e-05, -3.4209e-04,  ..., -7.8889e-05,
         -1.2785e-04,  2.6359e-04],
        [-3.9861e-06, -2.7046e-06,  2.7604e-06,  ..., -3.5614e-06,
         -3.0287e-06, -3.2336e-06],
        [-1.1340e-05, -8.0913e-06,  8.5086e-06,  ..., -9.9838e-06,
         -8.5533e-06, -9.0748e-06],
        [-7.8678e-06, -5.2229e-06,  5.6326e-06,  ..., -7.0035e-06,
         -6.0275e-06, -6.6310e-06],
        [-1.0327e-05, -7.5400e-06,  7.4953e-06,  ..., -9.2685e-06,
         -8.1807e-06, -7.7039e-06]], device='cuda:0')
Loss: 0.9511996507644653


Running epoch 1, step 1483, batch 435
Sampled inputs[:2]: tensor([[   0, 5902,  518,  ..., 3126,   12,  497],
        [   0,  287,  298,  ...,   14, 1147,  199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1781e-04,  2.2633e-04, -3.9858e-04,  ..., -9.6064e-05,
         -2.3759e-04,  2.8512e-04],
        [-5.3272e-06, -3.5651e-06,  3.6880e-06,  ..., -4.7758e-06,
         -4.0792e-06, -4.3139e-06],
        [-1.4961e-05, -1.0565e-05,  1.1221e-05,  ..., -1.3217e-05,
         -1.1384e-05, -1.1921e-05],
        [-1.0446e-05, -6.8471e-06,  7.4878e-06,  ..., -9.3430e-06,
         -8.0839e-06, -8.7768e-06],
        [-1.3739e-05, -9.9391e-06,  9.9838e-06,  ..., -1.2368e-05,
         -1.0982e-05, -1.0207e-05]], device='cuda:0')
Loss: 0.9150136709213257


Running epoch 1, step 1484, batch 436
Sampled inputs[:2]: tensor([[    0,   266, 27347,  ...,   368,  3367,    13],
        [    0,    12,   722,  ...,   674,   369,   897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8981e-04,  2.7146e-04, -5.7766e-04,  ..., -2.2419e-04,
         -3.2934e-04,  1.8935e-04],
        [ 6.2273e-05,  3.9112e-05, -5.6582e-05,  ...,  4.8079e-05,
          8.6123e-05,  3.7456e-05],
        [-1.8388e-05, -1.2919e-05,  1.4454e-05,  ..., -1.6108e-05,
         -1.3813e-05, -1.4246e-05],
        [-1.3024e-05, -8.4937e-06,  9.9018e-06,  ..., -1.1533e-05,
         -9.9167e-06, -1.0647e-05],
        [-1.6630e-05, -1.2040e-05,  1.2636e-05,  ..., -1.4871e-05,
         -1.3188e-05, -1.1988e-05]], device='cuda:0')
Loss: 0.9358435273170471


Running epoch 1, step 1485, batch 437
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   278,  4697,    14],
        [    0,   462,   221,  ...,   278, 48911,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4408e-04,  1.6735e-04, -6.4824e-04,  ..., -1.1514e-04,
         -1.4790e-04,  1.7645e-04],
        [ 6.1043e-05,  3.8221e-05, -5.5613e-05,  ...,  4.6991e-05,
          8.5210e-05,  3.6591e-05],
        [-2.1785e-05, -1.5542e-05,  1.7375e-05,  ..., -1.9103e-05,
         -1.6361e-05, -1.6600e-05],
        [-1.5497e-05, -1.0274e-05,  1.1988e-05,  ..., -1.3739e-05,
         -1.1794e-05, -1.2502e-05],
        [-1.9580e-05, -1.4424e-05,  1.5110e-05,  ..., -1.7524e-05,
         -1.5527e-05, -1.3858e-05]], device='cuda:0')
Loss: 0.968948245048523


Running epoch 1, step 1486, batch 438
Sampled inputs[:2]: tensor([[    0,   654,   300,  ..., 21762,  3597, 11117],
        [    0,  3115,  1640,  ...,   300,   266,  5453]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.9800e-05,  3.1227e-04, -4.0767e-04,  ..., -1.4754e-04,
         -2.5580e-04,  7.7930e-05],
        [ 5.9866e-05,  3.7313e-05, -5.4734e-05,  ...,  4.5896e-05,
          8.4256e-05,  3.5701e-05],
        [-2.5079e-05, -1.8135e-05,  2.0042e-05,  ..., -2.2069e-05,
         -1.8969e-05, -1.8999e-05],
        [-1.7852e-05, -1.2018e-05,  1.3843e-05,  ..., -1.5900e-05,
         -1.3694e-05, -1.4372e-05],
        [-2.2545e-05, -1.6853e-05,  1.7449e-05,  ..., -2.0236e-05,
         -1.8001e-05, -1.5825e-05]], device='cuda:0')
Loss: 0.9506490230560303


Running epoch 1, step 1487, batch 439
Sampled inputs[:2]: tensor([[    0,   275, 11628,  ...,   408,  1296,  3796],
        [    0,  1912,  3461,  ...,   446,  9337,  1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4317e-04,  3.5853e-04, -4.9218e-04,  ..., -1.8724e-04,
         -2.4768e-04, -1.6317e-05],
        [ 5.8600e-05,  3.6359e-05, -5.3698e-05,  ...,  4.4815e-05,
          8.3288e-05,  3.4770e-05],
        [ 3.3540e-04,  2.4557e-04, -3.3551e-04,  ...,  3.3387e-04,
          3.5398e-04,  2.2831e-04],
        [-2.0444e-05, -1.3955e-05,  1.6093e-05,  ..., -1.8105e-05,
         -1.5706e-05, -1.6384e-05],
        [-2.5630e-05, -1.9401e-05,  2.0072e-05,  ..., -2.2948e-05,
         -2.0549e-05, -1.7941e-05]], device='cuda:0')
Loss: 0.9823882579803467
Graident accumulation at epoch 1, step 1487, batch 439
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.4594e-05,  1.2501e-04, -8.3528e-05,  ..., -3.0876e-05,
         -5.8978e-05, -3.3918e-05],
        [-4.2820e-06, -3.6228e-06,  1.6667e-06,  ..., -4.2280e-06,
          1.2164e-06, -3.8865e-06],
        [ 3.1864e-05,  3.3001e-05, -3.5365e-05,  ...,  3.3858e-05,
          4.5453e-05,  1.5960e-05],
        [-5.3275e-06,  4.4006e-06,  2.5703e-06,  ..., -2.1281e-06,
          1.2482e-06, -4.4028e-06],
        [-2.6019e-05, -1.9419e-05,  1.9054e-05,  ..., -2.2653e-05,
         -1.9700e-05, -1.7542e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8564e-08, 5.3230e-08, 5.2933e-08,  ..., 1.9972e-08, 1.2023e-07,
         4.5045e-08],
        [8.6593e-11, 5.2027e-11, 2.3808e-11,  ..., 5.9663e-11, 2.7000e-11,
         2.6538e-11],
        [2.0167e-09, 1.3733e-09, 6.4997e-10,  ..., 1.5377e-09, 8.6257e-10,
         5.2520e-10],
        [8.6111e-10, 9.9515e-10, 3.7038e-10,  ..., 8.4642e-10, 5.9236e-10,
         3.5930e-10],
        [4.1615e-10, 2.4166e-10, 8.5503e-11,  ..., 3.0360e-10, 8.5153e-11,
         1.2308e-10]], device='cuda:0')
optimizer state dict: 186.0
lr: [4.097971892163585e-06, 4.097971892163585e-06]
scheduler_last_epoch: 186


Running epoch 1, step 1488, batch 440
Sampled inputs[:2]: tensor([[   0,  706, 6989,  ..., 6914,   15, 2537],
        [   0,    5, 7523,  ...,  199, 8871,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2749e-04,  5.9918e-05,  1.4345e-04,  ..., -2.9224e-05,
          1.2690e-04,  4.1942e-05],
        [-1.2293e-06, -1.0505e-06,  9.0897e-07,  ..., -1.1697e-06,
         -1.0729e-06, -8.7917e-07],
        [-3.5018e-06, -3.1441e-06,  2.7865e-06,  ..., -3.3379e-06,
         -3.0696e-06, -2.5332e-06],
        [-2.4885e-06, -2.1160e-06,  1.9372e-06,  ..., -2.3842e-06,
         -2.2054e-06, -1.9073e-06],
        [-3.1292e-06, -2.9355e-06,  2.4438e-06,  ..., -3.0547e-06,
         -2.8908e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9995089769363403


Running epoch 1, step 1489, batch 441
Sampled inputs[:2]: tensor([[    0,   391,  1351,  ...,    13,    40,     9],
        [    0, 26138,    17,  ...,   401,  1867,  4977]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0369e-04, -1.2381e-04,  3.0157e-04,  ..., -2.5722e-06,
          4.5669e-04,  2.0972e-04],
        [-2.5034e-06, -2.0340e-06,  1.8403e-06,  ..., -2.3097e-06,
         -2.1011e-06, -1.7807e-06],
        [-7.0781e-06, -6.0797e-06,  5.6028e-06,  ..., -6.5416e-06,
         -5.9754e-06, -5.0515e-06],
        [-5.0515e-06, -4.0978e-06,  3.9041e-06,  ..., -4.6939e-06,
         -4.3213e-06, -3.8296e-06],
        [-6.3330e-06, -5.6326e-06,  4.9174e-06,  ..., -5.9605e-06,
         -5.6028e-06, -4.2170e-06]], device='cuda:0')
Loss: 0.9930357933044434


Running epoch 1, step 1490, batch 442
Sampled inputs[:2]: tensor([[    0,   298, 11712,  ...,   221,   273,   298],
        [    0,   281,   221,  ...,  2236, 15064,  1458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0335e-04, -3.2795e-04,  9.7263e-05,  ..., -2.7436e-05,
          7.3056e-04,  3.0979e-04],
        [-4.0606e-06, -2.7120e-06,  2.6673e-06,  ..., -3.6731e-06,
         -3.1367e-06, -3.1814e-06],
        [-1.1191e-05, -8.0466e-06,  7.9721e-06,  ..., -1.0103e-05,
         -8.7321e-06, -8.6874e-06],
        [-7.8529e-06, -5.2527e-06,  5.4166e-06,  ..., -7.1973e-06,
         -6.2287e-06, -6.3926e-06],
        [-1.0714e-05, -7.7337e-06,  7.3463e-06,  ..., -9.8050e-06,
         -8.6129e-06, -7.8827e-06]], device='cuda:0')
Loss: 0.864304780960083


Running epoch 1, step 1491, batch 443
Sampled inputs[:2]: tensor([[    0,  2496, 10545,  ...,   287, 13978,   408],
        [    0,   834,    89,  ...,  4030,    12,  6528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8384e-06, -1.0791e-05,  3.5138e-05,  ...,  6.2564e-05,
          3.0754e-04,  8.9508e-05],
        [-5.2676e-06, -3.5800e-06,  3.6731e-06,  ..., -4.7237e-06,
         -4.0419e-06, -4.0606e-06],
        [-1.4603e-05, -1.0610e-05,  1.1027e-05,  ..., -1.3024e-05,
         -1.1280e-05, -1.1131e-05],
        [-1.0297e-05, -6.9663e-06,  7.5772e-06,  ..., -9.3132e-06,
         -8.0690e-06, -8.2701e-06],
        [-1.3649e-05, -1.0058e-05,  9.9093e-06,  ..., -1.2368e-05,
         -1.0952e-05, -9.8050e-06]], device='cuda:0')
Loss: 0.949407696723938


Running epoch 1, step 1492, batch 444
Sampled inputs[:2]: tensor([[    0,  1619,   938,  ...,   292, 10026, 14367],
        [    0, 10565,  2677,  ...,   298,   292, 11188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4246e-05, -6.4908e-06, -1.9817e-05,  ...,  1.4045e-04,
          3.3898e-04,  6.3659e-05],
        [-6.5565e-06, -4.5188e-06,  4.6194e-06,  ..., -5.9009e-06,
         -5.0627e-06, -5.0366e-06],
        [-1.8284e-05, -1.3426e-05,  1.3918e-05,  ..., -1.6361e-05,
         -1.4186e-05, -1.3888e-05],
        [-1.2800e-05, -8.7470e-06,  9.4846e-06,  ..., -1.1608e-05,
         -1.0051e-05, -1.0267e-05],
        [-1.6898e-05, -1.2636e-05,  1.2398e-05,  ..., -1.5363e-05,
         -1.3664e-05, -1.2070e-05]], device='cuda:0')
Loss: 0.9712238907814026


Running epoch 1, step 1493, batch 445
Sampled inputs[:2]: tensor([[    0,  1163,  5728,  ..., 24586,   756,    14],
        [    0,   635,    13,  ...,    13,  4710,  1416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2206e-05,  4.9028e-06,  1.4515e-04,  ...,  5.6790e-05,
          3.3133e-04,  2.2592e-04],
        [-7.8902e-06, -5.3719e-06,  5.5693e-06,  ..., -7.0706e-06,
         -6.0238e-06, -6.1095e-06],
        [-2.1949e-05, -1.5959e-05,  1.6749e-05,  ..., -1.9535e-05,
         -1.6853e-05, -1.6794e-05],
        [-1.5393e-05, -1.0394e-05,  1.1407e-05,  ..., -1.3888e-05,
         -1.1958e-05, -1.2428e-05],
        [-2.0251e-05, -1.4991e-05,  1.4916e-05,  ..., -1.8314e-05,
         -1.6227e-05, -1.4544e-05]], device='cuda:0')
Loss: 0.9394099712371826


Running epoch 1, step 1494, batch 446
Sampled inputs[:2]: tensor([[   0, 2341, 7956,  ..., 2355,  413,   72],
        [   0, 1500,  367,  ...,  344, 4250,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9516e-05,  3.1979e-05,  2.1578e-04,  ...,  6.1420e-05,
          3.2795e-04,  2.4846e-04],
        [-9.1717e-06, -6.3404e-06,  6.5193e-06,  ..., -8.1956e-06,
         -7.0967e-06, -6.9812e-06],
        [-2.5451e-05, -1.8775e-05,  1.9521e-05,  ..., -2.2620e-05,
         -1.9819e-05, -1.9193e-05],
        [ 7.0452e-05,  8.0899e-05, -3.2438e-05,  ...,  6.1040e-05,
          5.1645e-05,  2.7543e-05],
        [-2.3410e-05, -1.7658e-05,  1.7375e-05,  ..., -2.1160e-05,
         -1.9059e-05, -1.6555e-05]], device='cuda:0')
Loss: 0.9691282510757446


Running epoch 1, step 1495, batch 447
Sampled inputs[:2]: tensor([[    0,   342, 43937,  ...,   298,   413,    29],
        [    0,  6945,  2360,  ...,    30,   413,    16]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.2991e-05,  3.4495e-04,  3.9925e-04,  ..., -6.5154e-06,
          3.7091e-04,  2.9865e-04],
        [-1.0513e-05, -7.2941e-06,  7.2718e-06,  ..., -9.4771e-06,
         -8.2292e-06, -8.0839e-06],
        [-2.9162e-05, -2.1681e-05,  2.1815e-05,  ..., -2.6211e-05,
         -2.3082e-05, -2.2247e-05],
        [ 6.7785e-05,  7.8992e-05, -3.0933e-05,  ...,  5.8418e-05,
          4.9276e-05,  2.5263e-05],
        [-2.6911e-05, -2.0459e-05,  1.9491e-05,  ..., -2.4587e-05,
         -2.2247e-05, -1.9282e-05]], device='cuda:0')
Loss: 0.9312328696250916
Graident accumulation at epoch 1, step 1495, batch 447
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.9836e-05,  1.4701e-04, -3.5251e-05,  ..., -2.8440e-05,
         -1.5989e-05, -6.6145e-07],
        [-4.9051e-06, -3.9900e-06,  2.2272e-06,  ..., -4.7529e-06,
          2.7182e-07, -4.3062e-06],
        [ 2.5762e-05,  2.7533e-05, -2.9647e-05,  ...,  2.7851e-05,
          3.8600e-05,  1.2139e-05],
        [ 1.9837e-06,  1.1860e-05, -7.8004e-07,  ...,  3.9265e-06,
          6.0510e-06, -1.4362e-06],
        [-2.6108e-05, -1.9523e-05,  1.9097e-05,  ..., -2.2846e-05,
         -1.9955e-05, -1.7716e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8509e-08, 5.3295e-08, 5.3039e-08,  ..., 1.9953e-08, 1.2025e-07,
         4.5090e-08],
        [8.6617e-11, 5.2028e-11, 2.3837e-11,  ..., 5.9693e-11, 2.7041e-11,
         2.6577e-11],
        [2.0156e-09, 1.3723e-09, 6.4980e-10,  ..., 1.5369e-09, 8.6224e-10,
         5.2517e-10],
        [8.6484e-10, 1.0004e-09, 3.7096e-10,  ..., 8.4899e-10, 5.9420e-10,
         3.5958e-10],
        [4.1646e-10, 2.4184e-10, 8.5797e-11,  ..., 3.0390e-10, 8.5563e-11,
         1.2333e-10]], device='cuda:0')
optimizer state dict: 187.0
lr: [3.998635243264737e-06, 3.998635243264737e-06]
scheduler_last_epoch: 187


Running epoch 1, step 1496, batch 448
Sampled inputs[:2]: tensor([[    0,    13, 20054,  ...,    19,     9,   266],
        [    0, 13751,    12,  ...,  1264,  5676,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1606e-04, -1.6612e-05, -1.6473e-04,  ..., -2.1112e-05,
          5.2860e-05,  6.1918e-05],
        [-1.3635e-06, -8.1956e-07,  1.0803e-06,  ..., -1.1325e-06,
         -9.1270e-07, -1.0282e-06],
        [-3.7998e-06, -2.3693e-06,  3.2037e-06,  ..., -3.0994e-06,
         -2.5183e-06, -2.8014e-06],
        [-2.6077e-06, -1.5274e-06,  2.1607e-06,  ..., -2.1458e-06,
         -1.7285e-06, -2.0266e-06],
        [-3.4124e-06, -2.2203e-06,  2.7865e-06,  ..., -2.8461e-06,
         -2.4140e-06, -2.3395e-06]], device='cuda:0')
Loss: 0.9604316353797913


Running epoch 1, step 1497, batch 449
Sampled inputs[:2]: tensor([[    0,   292,    40,  ..., 26995,   278,   717],
        [    0,   474,   221,  ..., 32291,   360,  2458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1235e-04,  1.1473e-04, -3.5653e-04,  ...,  2.0802e-04,
          5.2860e-05,  2.6578e-04],
        [-2.7046e-06, -1.6727e-06,  2.1458e-06,  ..., -2.3469e-06,
         -1.8291e-06, -2.0415e-06],
        [-7.4804e-06, -4.7833e-06,  6.3628e-06,  ..., -6.3628e-06,
         -5.0068e-06, -5.5134e-06],
        [-5.1856e-06, -3.1367e-06,  4.3064e-06,  ..., -4.4703e-06,
         -3.4794e-06, -4.0382e-06],
        [-6.6757e-06, -4.4554e-06,  5.5283e-06,  ..., -5.8115e-06,
         -4.7982e-06, -4.5747e-06]], device='cuda:0')
Loss: 0.9583362340927124


Running epoch 1, step 1498, batch 450
Sampled inputs[:2]: tensor([[    0,    12,   221,  ...,   292, 27729,  9837],
        [    0,    13,  4363,  ...,   271,  2462,   709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5500e-04,  2.1600e-04, -2.6559e-04,  ...,  3.0092e-04,
          1.1145e-05,  3.8792e-04],
        [-3.9339e-06, -2.5555e-06,  3.0808e-06,  ..., -3.4794e-06,
         -2.8200e-06, -2.9914e-06],
        [-1.0759e-05, -7.2122e-06,  9.0897e-06,  ..., -9.3132e-06,
         -7.6145e-06, -7.9125e-06],
        [-7.5996e-06, -4.7833e-06,  6.2287e-06,  ..., -6.6906e-06,
         -5.4315e-06, -5.9605e-06],
        [-9.6858e-06, -6.7800e-06,  7.9721e-06,  ..., -8.5682e-06,
         -7.3165e-06, -6.6161e-06]], device='cuda:0')
Loss: 0.9422233700752258


Running epoch 1, step 1499, batch 451
Sampled inputs[:2]: tensor([[    0,    12,  1790,  ..., 11026,   292,  2116],
        [    0,   417,   199,  ...,  9472, 15004,   511]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9052e-04,  1.9562e-04, -2.9000e-04,  ...,  1.1668e-04,
          8.5850e-05,  5.4742e-04],
        [-5.2601e-06, -3.4980e-06,  4.0717e-06,  ..., -4.5747e-06,
         -3.7737e-06, -3.8408e-06],
        [-1.4335e-05, -9.9391e-06,  1.2010e-05,  ..., -1.2293e-05,
         -1.0282e-05, -1.0222e-05],
        [-1.0207e-05, -6.6385e-06,  8.2850e-06,  ..., -8.8364e-06,
         -7.3388e-06, -7.7412e-06],
        [-1.2845e-05, -9.2983e-06,  1.0490e-05,  ..., -1.1250e-05,
         -9.8199e-06, -8.4937e-06]], device='cuda:0')
Loss: 0.973566472530365


Running epoch 1, step 1500, batch 452
Sampled inputs[:2]: tensor([[    0, 15372, 10123,  ...,  1782,    12,   266],
        [    0,    81,  1619,  ...,  2442,    13,  1581]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2424e-04, -5.0814e-05, -2.4462e-04,  ..., -1.6768e-05,
          4.1746e-04,  7.2173e-04],
        [-6.5640e-06, -4.4741e-06,  5.0105e-06,  ..., -5.7667e-06,
         -4.8913e-06, -4.7907e-06],
        [-1.8060e-05, -1.2860e-05,  1.4856e-05,  ..., -1.5676e-05,
         -1.3500e-05, -1.2934e-05],
        [ 1.0956e-04,  2.0635e-04, -6.9474e-05,  ...,  1.1297e-04,
          1.6550e-04,  4.8226e-05],
        [-1.6272e-05, -1.2085e-05,  1.3083e-05,  ..., -1.4439e-05,
         -1.2934e-05, -1.0833e-05]], device='cuda:0')
Loss: 0.9885566234588623


Running epoch 1, step 1501, batch 453
Sampled inputs[:2]: tensor([[    0,  1854,   292,  ...,   328,  1360,    14],
        [    0, 21540,   527,  ...,   824,    14,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4552e-04, -2.4364e-04, -3.4035e-04,  ..., -1.4038e-04,
          7.3891e-04,  1.0127e-03],
        [-7.9051e-06, -5.2638e-06,  5.7630e-06,  ..., -7.0706e-06,
         -5.9046e-06, -6.0126e-06],
        [-2.1905e-05, -1.5289e-05,  1.7241e-05,  ..., -1.9327e-05,
         -1.6406e-05, -1.6317e-05],
        [ 1.0694e-04,  2.0484e-04, -6.7991e-05,  ...,  1.1039e-04,
          1.6348e-04,  4.5767e-05],
        [-1.9893e-05, -1.4395e-05,  1.5303e-05,  ..., -1.7896e-05,
         -1.5751e-05, -1.3813e-05]], device='cuda:0')
Loss: 0.9572040438652039


Running epoch 1, step 1502, batch 454
Sampled inputs[:2]: tensor([[   0, 1795,  650,  ...,  516, 2793, 1109],
        [   0, 1867,  300,  ...,  259, 3095, 1842]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7416e-04, -1.3543e-04, -2.3962e-04,  ..., -2.0677e-04,
          7.2262e-04,  1.0940e-03],
        [-9.1642e-06, -6.1207e-06,  6.8061e-06,  ..., -8.1584e-06,
         -6.7763e-06, -6.9365e-06],
        [-2.5615e-05, -1.7941e-05,  2.0504e-05,  ..., -2.2516e-05,
         -1.8954e-05, -1.9044e-05],
        [ 1.0439e-04,  2.0312e-04, -6.5786e-05,  ...,  1.0820e-04,
          1.6171e-04,  4.3800e-05],
        [-2.3022e-05, -1.6734e-05,  1.7971e-05,  ..., -2.0653e-05,
         -1.8045e-05, -1.5929e-05]], device='cuda:0')
Loss: 0.9541919827461243


Running epoch 1, step 1503, batch 455
Sampled inputs[:2]: tensor([[    0, 18837,   394,  ...,   271,  1398,  1871],
        [    0,    12,  3518,  ...,  1580,  2573,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2155e-05, -1.7765e-04, -4.8651e-04,  ..., -2.5670e-04,
          7.7996e-04,  1.2842e-03],
        [-1.0557e-05, -6.9290e-06,  7.7188e-06,  ..., -9.3654e-06,
         -7.6964e-06, -8.1137e-06],
        [-2.9609e-05, -2.0400e-05,  2.3350e-05,  ..., -2.5883e-05,
         -2.1577e-05, -2.2262e-05],
        [ 1.0165e-04,  2.0158e-04, -6.3923e-05,  ...,  1.0583e-04,
          1.5987e-04,  4.1431e-05],
        [-2.6569e-05, -1.8984e-05,  2.0459e-05,  ..., -2.3693e-05,
         -2.0489e-05, -1.8582e-05]], device='cuda:0')
Loss: 0.9350077509880066
Graident accumulation at epoch 1, step 1503, batch 455
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0148,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.8068e-05,  1.1454e-04, -8.0376e-05,  ..., -5.1266e-05,
          6.3606e-05,  1.2783e-04],
        [-5.4703e-06, -4.2839e-06,  2.7764e-06,  ..., -5.2142e-06,
         -5.2501e-07, -4.6870e-06],
        [ 2.0225e-05,  2.2740e-05, -2.4347e-05,  ...,  2.2478e-05,
          3.2582e-05,  8.6991e-06],
        [ 1.1950e-05,  3.0832e-05, -7.0943e-06,  ...,  1.4117e-05,
          2.1433e-05,  2.8505e-06],
        [-2.6154e-05, -1.9469e-05,  1.9233e-05,  ..., -2.2931e-05,
         -2.0008e-05, -1.7802e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8453e-08, 5.3274e-08, 5.3223e-08,  ..., 1.9998e-08, 1.2074e-07,
         4.6694e-08],
        [8.6642e-11, 5.2024e-11, 2.3873e-11,  ..., 5.9721e-11, 2.7073e-11,
         2.6617e-11],
        [2.0144e-09, 1.3714e-09, 6.4969e-10,  ..., 1.5360e-09, 8.6185e-10,
         5.2514e-10],
        [8.7431e-10, 1.0400e-09, 3.7468e-10,  ..., 8.5934e-10, 6.1916e-10,
         3.6094e-10],
        [4.1675e-10, 2.4195e-10, 8.6130e-11,  ..., 3.0416e-10, 8.5897e-11,
         1.2355e-10]], device='cuda:0')
optimizer state dict: 188.0
lr: [3.900215655287364e-06, 3.900215655287364e-06]
scheduler_last_epoch: 188


Running epoch 1, step 1504, batch 456
Sampled inputs[:2]: tensor([[   0,  259, 5918,  ...,  508, 3433, 1351],
        [   0,  266, 1624,  ...,   14,   19,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1766e-05, -6.6714e-05,  2.8164e-05,  ..., -1.3805e-04,
          1.9457e-05, -1.0241e-04],
        [-1.3784e-06, -7.7859e-07,  8.8289e-07,  ..., -1.2144e-06,
         -9.7603e-07, -1.1325e-06],
        [-3.8743e-06, -2.3246e-06,  2.6822e-06,  ..., -3.3677e-06,
         -2.7269e-06, -3.1143e-06],
        [-2.6375e-06, -1.4231e-06,  1.7062e-06,  ..., -2.3544e-06,
         -1.8850e-06, -2.2203e-06],
        [-3.5316e-06, -2.1607e-06,  2.3544e-06,  ..., -3.1143e-06,
         -2.6226e-06, -2.6524e-06]], device='cuda:0')
Loss: 0.9371465444564819


Running epoch 1, step 1505, batch 457
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   352, 10572,   345],
        [    0,   401,   953,  ..., 10914,   554,  2360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2983e-05, -2.2744e-04, -1.1483e-05,  ..., -1.5951e-04,
          1.4838e-04,  7.1330e-05],
        [-2.6748e-06, -1.6056e-06,  1.8105e-06,  ..., -2.4065e-06,
         -1.9446e-06, -2.1458e-06],
        [-7.4506e-06, -4.7237e-06,  5.4985e-06,  ..., -6.5565e-06,
         -5.3346e-06, -5.7667e-06],
        [-5.0813e-06, -2.9281e-06,  3.5465e-06,  ..., -4.6045e-06,
         -3.7327e-06, -4.2319e-06],
        [-6.7800e-06, -4.4256e-06,  4.8578e-06,  ..., -6.0499e-06,
         -5.1111e-06, -4.8429e-06]], device='cuda:0')
Loss: 0.9398108124732971


Running epoch 1, step 1506, batch 458
Sampled inputs[:2]: tensor([[    0,    13,    41,  ...,     5,   271,  2936],
        [    0,    14,   759,  ..., 15790,   278,   706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6270e-05, -1.7622e-04,  2.2170e-05,  ..., -8.3978e-05,
          9.0616e-05,  2.2282e-05],
        [-3.9786e-06, -2.5593e-06,  2.8908e-06,  ..., -3.5167e-06,
         -2.9057e-06, -3.0436e-06],
        [-1.1116e-05, -7.5698e-06,  8.7470e-06,  ..., -9.7156e-06,
         -8.0913e-06, -8.2999e-06],
        [-7.6592e-06, -4.7907e-06,  5.7817e-06,  ..., -6.8098e-06,
         -5.6550e-06, -6.1169e-06],
        [-9.9689e-06, -7.0333e-06,  7.6294e-06,  ..., -8.8662e-06,
         -7.6741e-06, -6.8843e-06]], device='cuda:0')
Loss: 0.9941564798355103


Running epoch 1, step 1507, batch 459
Sampled inputs[:2]: tensor([[    0,  2734,  2338,  ...,  3977,   970, 10537],
        [    0,   431, 19346,  ...,    14,  3237, 18548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1055e-04, -1.6299e-04,  1.6690e-04,  ..., -1.2347e-04,
          2.4225e-04,  1.8655e-04],
        [-5.3197e-06, -3.5875e-06,  3.8259e-06,  ..., -4.7535e-06,
         -3.9861e-06, -4.0047e-06],
        [-1.4633e-05, -1.0461e-05,  1.1474e-05,  ..., -1.2934e-05,
         -1.1027e-05, -1.0744e-05],
        [-1.0177e-05, -6.7577e-06,  7.6294e-06,  ..., -9.1642e-06,
         -7.7859e-06, -8.0392e-06],
        [-1.3188e-05, -9.7454e-06,  1.0073e-05,  ..., -1.1832e-05,
         -1.0446e-05, -8.9407e-06]], device='cuda:0')
Loss: 0.9882088303565979


Running epoch 1, step 1508, batch 460
Sampled inputs[:2]: tensor([[    0,   259,  2180,  ...,   638,  1615,   694],
        [    0,   278,  1253,  ...,   266,  1274, 22300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7377e-04, -1.5655e-04,  1.6956e-04,  ..., -1.7266e-04,
          2.7389e-04,  3.7541e-04],
        [-6.6087e-06, -4.5076e-06,  4.8988e-06,  ..., -5.8413e-06,
         -4.8541e-06, -4.8615e-06],
        [-1.8239e-05, -1.3173e-05,  1.4707e-05,  ..., -1.5959e-05,
         -1.3471e-05, -1.3143e-05],
        [-1.2755e-05, -8.5682e-06,  9.9242e-06,  ..., -1.1325e-05,
         -9.5144e-06, -9.8646e-06],
        [-1.6198e-05, -1.2115e-05,  1.2696e-05,  ..., -1.4424e-05,
         -1.2636e-05, -1.0766e-05]], device='cuda:0')
Loss: 0.9598649740219116


Running epoch 1, step 1509, batch 461
Sampled inputs[:2]: tensor([[    0,  3386, 43625,  ...,    19,  2125,   271],
        [    0,  3231,   271,  ...,  9279,  8231, 28871]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2170e-04, -1.8274e-04,  1.9524e-04,  ..., -2.6648e-04,
          2.9190e-04,  2.9633e-04],
        [-7.8082e-06, -5.4166e-06,  5.8264e-06,  ..., -6.9812e-06,
         -5.8673e-06, -5.7928e-06],
        [-2.1592e-05, -1.5751e-05,  1.7494e-05,  ..., -1.9044e-05,
         -1.6212e-05, -1.5602e-05],
        [-1.5065e-05, -1.0237e-05,  1.1787e-05,  ..., -1.3500e-05,
         -1.1452e-05, -1.1727e-05],
        [-1.9312e-05, -1.4573e-05,  1.5229e-05,  ..., -1.7300e-05,
         -1.5274e-05, -1.2867e-05]], device='cuda:0')
Loss: 0.9493606686592102


Running epoch 1, step 1510, batch 462
Sampled inputs[:2]: tensor([[   0, 2162,   73,  ...,  278,  266, 1059],
        [   0, 5489,   80,  ...,  221,  380,  333]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1719e-04, -1.7899e-04,  1.6358e-04,  ..., -2.3441e-04,
          2.1145e-04,  1.0290e-04],
        [-9.0674e-06, -6.2883e-06,  6.9067e-06,  ..., -8.0839e-06,
         -6.7614e-06, -6.6534e-06],
        [-2.5108e-05, -1.8239e-05,  2.0683e-05,  ..., -2.2054e-05,
         -1.8686e-05, -1.7941e-05],
        [-1.7539e-05, -1.1891e-05,  1.4037e-05,  ..., -1.5646e-05,
         -1.3210e-05, -1.3493e-05],
        [-2.2262e-05, -1.6779e-05,  1.7866e-05,  ..., -1.9893e-05,
         -1.7494e-05, -1.4685e-05]], device='cuda:0')
Loss: 0.957817018032074


Running epoch 1, step 1511, batch 463
Sampled inputs[:2]: tensor([[   0, 6411,  300,  ...,  287, 4152, 1952],
        [   0, 3484,  437,  ...,  298,  995, 4009]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1367e-05, -1.7094e-04,  3.5483e-04,  ..., -3.0050e-04,
          1.8153e-04,  1.4895e-04],
        [-1.0327e-05, -7.1935e-06,  7.9274e-06,  ..., -9.1717e-06,
         -7.6666e-06, -7.5474e-06],
        [-2.8685e-05, -2.0951e-05,  2.3797e-05,  ..., -2.5094e-05,
         -2.1234e-05, -2.0415e-05],
        [-2.0012e-05, -1.3635e-05,  1.6153e-05,  ..., -1.7777e-05,
         -1.4991e-05, -1.5333e-05],
        [-2.5257e-05, -1.9133e-05,  2.0415e-05,  ..., -2.2486e-05,
         -1.9759e-05, -1.6578e-05]], device='cuda:0')
Loss: 0.9121302366256714
Graident accumulation at epoch 1, step 1511, batch 463
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.6124e-05,  8.5994e-05, -3.6856e-05,  ..., -7.6190e-05,
          7.5399e-05,  1.2994e-04],
        [-5.9559e-06, -4.5748e-06,  3.2915e-06,  ..., -5.6099e-06,
         -1.2392e-06, -4.9730e-06],
        [ 1.5334e-05,  1.8371e-05, -1.9533e-05,  ...,  1.7721e-05,
          2.7201e-05,  5.7877e-06],
        [ 8.7542e-06,  2.6385e-05, -4.7696e-06,  ...,  1.0927e-05,
          1.7791e-05,  1.0322e-06],
        [-2.6064e-05, -1.9436e-05,  1.9352e-05,  ..., -2.2886e-05,
         -1.9983e-05, -1.7680e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8397e-08, 5.3250e-08, 5.3296e-08,  ..., 2.0069e-08, 1.2065e-07,
         4.6669e-08],
        [8.6662e-11, 5.2024e-11, 2.3912e-11,  ..., 5.9745e-11, 2.7105e-11,
         2.6647e-11],
        [2.0132e-09, 1.3705e-09, 6.4961e-10,  ..., 1.5351e-09, 8.6144e-10,
         5.2503e-10],
        [8.7384e-10, 1.0392e-09, 3.7457e-10,  ..., 8.5879e-10, 6.1877e-10,
         3.6081e-10],
        [4.1697e-10, 2.4208e-10, 8.6461e-11,  ..., 3.0436e-10, 8.6202e-11,
         1.2370e-10]], device='cuda:0')
optimizer state dict: 189.0
lr: [3.80272816760364e-06, 3.80272816760364e-06]
scheduler_last_epoch: 189


Running epoch 1, step 1512, batch 464
Sampled inputs[:2]: tensor([[    0,  3933,  6394,  ...,  1364,   950,   847],
        [    0,   790, 43134,  ...,   446,   381,  1034]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.6505e-05, -1.7898e-06,  3.7272e-05,  ..., -3.7018e-05,
          3.4696e-05,  2.5623e-05],
        [-1.3113e-06, -9.7603e-07,  1.0431e-06,  ..., -1.1548e-06,
         -1.0058e-06, -8.7917e-07],
        [ 1.4372e-04,  1.7078e-04, -1.0664e-04,  ...,  1.3486e-04,
          1.3442e-04,  5.6091e-05],
        [-2.6077e-06, -1.9372e-06,  2.1905e-06,  ..., -2.3097e-06,
         -2.0415e-06, -1.8477e-06],
        [-3.2187e-06, -2.6673e-06,  2.6971e-06,  ..., -2.9206e-06,
         -2.6524e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9966326951980591


Running epoch 1, step 1513, batch 465
Sampled inputs[:2]: tensor([[    0,   560, 23501,  ...,   292,   494,   221],
        [    0,  2992,   352,  ...,   259,  2063,  6088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5334e-05, -2.3647e-04,  2.6353e-05,  ..., -9.4838e-06,
          3.2248e-05,  1.8637e-04],
        [-2.5406e-06, -1.8701e-06,  2.0638e-06,  ..., -2.2426e-06,
         -1.8924e-06, -1.7658e-06],
        [ 1.4019e-04,  1.6814e-04, -1.0351e-04,  ...,  1.3182e-04,
          1.3192e-04,  5.3632e-05],
        [-5.0664e-06, -3.6582e-06,  4.3213e-06,  ..., -4.4554e-06,
         -3.7923e-06, -3.6880e-06],
        [-6.1989e-06, -4.9770e-06,  5.2750e-06,  ..., -5.5283e-06,
         -4.8876e-06, -3.9265e-06]], device='cuda:0')
Loss: 0.9500530958175659


Running epoch 1, step 1514, batch 466
Sampled inputs[:2]: tensor([[    0,   843, 17111,  ...,    12,   461,  6176],
        [    0,    43,   527,  ...,  4309,    14,  8050]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.1851e-05, -3.8984e-04,  9.0822e-05,  ..., -2.1849e-05,
          1.8139e-04,  2.6627e-04],
        [-3.8370e-06, -2.8461e-06,  2.9467e-06,  ..., -3.4496e-06,
         -2.9281e-06, -2.6673e-06],
        [ 1.3635e-04,  1.6507e-04, -1.0068e-04,  ...,  1.2826e-04,
          1.2883e-04,  5.0950e-05],
        [-7.7188e-06, -5.6401e-06,  6.1914e-06,  ..., -6.9290e-06,
         -5.9381e-06, -5.6401e-06],
        [-9.4324e-06, -7.6443e-06,  7.6145e-06,  ..., -8.5682e-06,
         -7.5847e-06, -6.0275e-06]], device='cuda:0')
Loss: 0.9739012718200684


Running epoch 1, step 1515, batch 467
Sampled inputs[:2]: tensor([[   0,  278, 2088,  ...,   69,   14,   71],
        [   0,  634, 1621,  ...,  688,  586, 8477]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6288e-05, -3.6742e-04,  1.1406e-04,  ...,  1.0722e-04,
          6.5066e-05,  3.4707e-04],
        [-5.1931e-06, -3.7886e-06,  4.0643e-06,  ..., -4.5523e-06,
         -3.8072e-06, -3.5390e-06],
        [ 1.3241e-04,  1.6215e-04, -9.7233e-05,  ...,  1.2503e-04,
          1.2621e-04,  4.8387e-05],
        [-1.0386e-05, -7.4878e-06,  8.4862e-06,  ..., -9.0897e-06,
         -7.6815e-06, -7.4431e-06],
        [-1.2621e-05, -1.0133e-05,  1.0341e-05,  ..., -1.1250e-05,
         -9.8646e-06, -7.9647e-06]], device='cuda:0')
Loss: 0.9631563425064087


Running epoch 1, step 1516, batch 468
Sampled inputs[:2]: tensor([[   0, 1611,  266,  ...,  266, 2673, 6277],
        [   0, 1580,  271,  ...,  656,  943, 1883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6288e-05, -3.1641e-04,  1.6741e-04,  ...,  8.6783e-05,
          1.3371e-04,  4.8543e-04],
        [-6.5118e-06, -4.7646e-06,  5.1595e-06,  ..., -5.6401e-06,
         -4.6901e-06, -4.4256e-06],
        [ 2.2304e-04,  2.3183e-04, -1.6895e-04,  ...,  1.9470e-04,
          1.7056e-04,  8.0810e-05],
        [-1.3068e-05, -9.4548e-06,  1.0811e-05,  ..., -1.1295e-05,
         -9.4771e-06, -9.3356e-06],
        [-1.5751e-05, -1.2651e-05,  1.3024e-05,  ..., -1.3888e-05,
         -1.2100e-05, -9.9316e-06]], device='cuda:0')
Loss: 0.9771389961242676


Running epoch 1, step 1517, batch 469
Sampled inputs[:2]: tensor([[    0,   472,   346,  ...,   266,   720,   342],
        [    0, 19191,   266,  ...,   287,   843,  1528]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4190e-04, -2.8172e-04, -6.8890e-06,  ...,  3.3395e-04,
         -1.1488e-04,  1.5737e-04],
        [-7.8753e-06, -5.6475e-06,  6.2473e-06,  ..., -6.7726e-06,
         -5.6177e-06, -5.3495e-06],
        [ 2.1914e-04,  2.2912e-04, -1.6563e-04,  ...,  1.9145e-04,
          1.6788e-04,  7.8173e-05],
        [-1.5810e-05, -1.1221e-05,  1.3091e-05,  ..., -1.3575e-05,
         -1.1355e-05, -1.1303e-05],
        [-1.8924e-05, -1.4991e-05,  1.5676e-05,  ..., -1.6600e-05,
         -1.4439e-05, -1.1928e-05]], device='cuda:0')
Loss: 0.967678427696228


Running epoch 1, step 1518, batch 470
Sampled inputs[:2]: tensor([[    0,   352,   644,  ...,  2928,   590,  3040],
        [    0,   287,  1790,  ..., 11367,  9476,  2545]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3753e-04, -6.3490e-05,  2.0662e-04,  ...,  3.6092e-04,
         -1.8624e-04,  3.1540e-04],
        [-9.1344e-06, -6.6161e-06,  7.3425e-06,  ..., -7.8827e-06,
         -6.5118e-06, -6.2063e-06],
        [ 2.1559e-04,  2.2626e-04, -1.6238e-04,  ...,  1.8832e-04,
          1.6534e-04,  7.5788e-05],
        [-1.8343e-05, -1.3113e-05,  1.5385e-05,  ..., -1.5795e-05,
         -1.3128e-05, -1.3128e-05],
        [-2.1964e-05, -1.7539e-05,  1.8403e-05,  ..., -1.9327e-05,
         -1.6734e-05, -1.3813e-05]], device='cuda:0')
Loss: 0.9599097371101379


Running epoch 1, step 1519, batch 471
Sampled inputs[:2]: tensor([[    0,   266,  5528,  ...,   685,   266,  1231],
        [    0, 22390,   292,  ...,  3552,   278,   317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3679e-05, -1.6849e-04,  1.3720e-04,  ...,  4.1722e-04,
          3.4849e-05,  3.9869e-04],
        [-1.0476e-05, -7.6294e-06,  8.4005e-06,  ..., -9.0748e-06,
         -7.5176e-06, -7.1116e-06],
        [ 2.1169e-04,  2.2319e-04, -1.5913e-04,  ...,  1.8488e-04,
          1.6244e-04,  7.3136e-05],
        [-2.1011e-05, -1.5080e-05,  1.7576e-05,  ..., -1.8135e-05,
         -1.5110e-05, -1.5028e-05],
        [-2.5272e-05, -2.0236e-05,  2.1085e-05,  ..., -2.2307e-05,
         -1.9327e-05, -1.5900e-05]], device='cuda:0')
Loss: 0.9698248505592346
Graident accumulation at epoch 1, step 1519, batch 471
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0156, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.4880e-05,  6.0545e-05, -1.9450e-05,  ..., -2.6849e-05,
          7.1344e-05,  1.5681e-04],
        [-6.4079e-06, -4.8803e-06,  3.8024e-06,  ..., -5.9564e-06,
         -1.8670e-06, -5.1869e-06],
        [ 3.4969e-05,  3.8853e-05, -3.3492e-05,  ...,  3.4437e-05,
          4.0724e-05,  1.2523e-05],
        [ 5.7777e-06,  2.2238e-05, -2.5351e-06,  ...,  8.0211e-06,
          1.4501e-05, -5.7385e-07],
        [-2.5985e-05, -1.9516e-05,  1.9525e-05,  ..., -2.2828e-05,
         -1.9918e-05, -1.7502e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8341e-08, 5.3225e-08, 5.3261e-08,  ..., 2.0223e-08, 1.2053e-07,
         4.6781e-08],
        [8.6685e-11, 5.2030e-11, 2.3959e-11,  ..., 5.9768e-11, 2.7134e-11,
         2.6671e-11],
        [2.0560e-09, 1.4189e-09, 6.7428e-10,  ..., 1.5677e-09, 8.8696e-10,
         5.2986e-10],
        [8.7341e-10, 1.0384e-09, 3.7450e-10,  ..., 8.5826e-10, 6.1838e-10,
         3.6068e-10],
        [4.1719e-10, 2.4225e-10, 8.6819e-11,  ..., 3.0456e-10, 8.6489e-11,
         1.2383e-10]], device='cuda:0')
optimizer state dict: 190.0
lr: [3.7061876771526483e-06, 3.7061876771526483e-06]
scheduler_last_epoch: 190


Running epoch 1, step 1520, batch 472
Sampled inputs[:2]: tensor([[   0, 5340,  287,  ...,  912, 2837, 5340],
        [   0, 4294,  278,  ...,   13, 2759, 5160]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7765e-04, -3.4147e-05, -9.9276e-05,  ..., -3.7919e-06,
          1.9512e-04,  1.1947e-04],
        [-1.3039e-06, -9.3877e-07,  9.6858e-07,  ..., -1.1772e-06,
         -1.0058e-06, -9.0525e-07],
        [-3.6657e-06, -2.7716e-06,  2.9206e-06,  ..., -3.2485e-06,
         -2.8163e-06, -2.4736e-06],
        [-2.5630e-06, -1.8105e-06,  1.9521e-06,  ..., -2.3395e-06,
         -2.0117e-06, -1.8701e-06],
        [-3.2037e-06, -2.5034e-06,  2.5034e-06,  ..., -2.8759e-06,
         -2.5630e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9904441833496094


Running epoch 1, step 1521, batch 473
Sampled inputs[:2]: tensor([[    0,    13,  6913,  ...,   278,  1317,  4470],
        [    0,   607, 27288,  ...,   445,  4712,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7106e-04,  6.5863e-05,  8.9465e-05,  ..., -5.5913e-05,
          2.0032e-04, -4.9288e-06],
        [-2.4736e-06, -1.8217e-06,  1.8775e-06,  ..., -2.3097e-06,
         -1.9893e-06, -1.7472e-06],
        [-7.0035e-06, -5.3793e-06,  5.7071e-06,  ..., -6.3777e-06,
         -5.4985e-06, -4.7833e-06],
        [-4.9174e-06, -3.5092e-06,  3.8445e-06,  ..., -4.6194e-06,
         -3.9935e-06, -3.6731e-06],
        [-6.1542e-06, -4.9025e-06,  4.8876e-06,  ..., -5.6773e-06,
         -5.0664e-06, -3.8594e-06]], device='cuda:0')
Loss: 0.9685840010643005


Running epoch 1, step 1522, batch 474
Sampled inputs[:2]: tensor([[   0,  271, 4787,  ...,  292,  494,  221],
        [   0,  342,  266,  ..., 4998, 4756, 5139]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1670e-04, -6.7670e-05, -6.6172e-05,  ..., -1.9606e-04,
          5.2382e-04, -1.3025e-04],
        [-3.7029e-06, -2.7530e-06,  2.8238e-06,  ..., -3.4049e-06,
         -2.8312e-06, -2.5630e-06],
        [-1.0610e-05, -8.2105e-06,  8.7619e-06,  ..., -9.5218e-06,
         -7.9274e-06, -7.1228e-06],
        [-7.3463e-06, -5.3197e-06,  5.8264e-06,  ..., -6.7651e-06,
         -5.6475e-06, -5.3719e-06],
        [-9.0897e-06, -7.3165e-06,  7.3016e-06,  ..., -8.2999e-06,
         -7.2122e-06, -5.5954e-06]], device='cuda:0')
Loss: 0.9288983941078186


Running epoch 1, step 1523, batch 475
Sampled inputs[:2]: tensor([[    0, 22387,   292,  ...,   352,  3097,   996],
        [    0,   221,   334,  ...,   706,  2680,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2434e-04,  3.7033e-05, -7.6055e-05,  ..., -1.2167e-04,
          4.1837e-04, -1.0059e-04],
        [-5.1707e-06, -3.5502e-06,  3.7774e-06,  ..., -4.6715e-06,
         -3.8221e-06, -3.6880e-06],
        [-1.4722e-05, -1.0639e-05,  1.1697e-05,  ..., -1.3053e-05,
         -1.0729e-05, -1.0207e-05],
        [-1.0163e-05, -6.8247e-06,  7.6964e-06,  ..., -9.2089e-06,
         -7.5996e-06, -7.5921e-06],
        [-1.2845e-05, -9.5963e-06,  9.9093e-06,  ..., -1.1563e-05,
         -9.8795e-06, -8.2329e-06]], device='cuda:0')
Loss: 0.9327709078788757


Running epoch 1, step 1524, batch 476
Sampled inputs[:2]: tensor([[    0,   287,  7763,  ...,   689,  2409,   699],
        [    0, 12456,    14,  ...,  1822,  1016,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.2381e-04,  2.7077e-06, -2.0337e-04,  ..., -5.9799e-05,
          5.7819e-04,  1.5599e-05],
        [-6.5640e-06, -4.4741e-06,  4.6715e-06,  ..., -5.9605e-06,
         -4.9025e-06, -4.7684e-06],
        [-1.8597e-05, -1.3366e-05,  1.4395e-05,  ..., -1.6600e-05,
         -1.3724e-05, -1.3128e-05],
        [-1.2800e-05, -8.5309e-06,  9.4324e-06,  ..., -1.1668e-05,
         -9.6709e-06, -9.7081e-06],
        [-1.6525e-05, -1.2234e-05,  1.2398e-05,  ..., -1.4961e-05,
         -1.2830e-05, -1.0811e-05]], device='cuda:0')
Loss: 0.9482722282409668


Running epoch 1, step 1525, batch 477
Sampled inputs[:2]: tensor([[    0, 31571,    13,  ...,   367,  2177,   271],
        [    0,   409, 22809,  ...,   342,   720,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1001e-04,  1.4438e-04, -1.9857e-04,  ..., -1.6594e-04,
          4.9761e-04, -3.2916e-05],
        [-7.7784e-06, -5.3830e-06,  5.7146e-06,  ..., -7.0706e-06,
         -5.7742e-06, -5.6028e-06],
        [-2.2054e-05, -1.6093e-05,  1.7568e-05,  ..., -1.9729e-05,
         -1.6198e-05, -1.5453e-05],
        [-1.5244e-05, -1.0304e-05,  1.1623e-05,  ..., -1.3903e-05,
         -1.1429e-05, -1.1496e-05],
        [-1.9476e-05, -1.4663e-05,  1.5050e-05,  ..., -1.7673e-05,
         -1.5080e-05, -1.2629e-05]], device='cuda:0')
Loss: 0.9803510904312134


Running epoch 1, step 1526, batch 478
Sampled inputs[:2]: tensor([[    0,    13, 20773,  ..., 22463,  2587,   292],
        [    0,    12,  1471,  ...,  1356,   600,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3266e-04,  1.9196e-04, -2.4152e-04,  ..., -1.6594e-04,
          3.0658e-04,  1.0160e-05],
        [-9.0599e-06, -6.2399e-06,  6.7204e-06,  ..., -8.1882e-06,
         -6.6757e-06, -6.4895e-06],
        [-2.5585e-05, -1.8582e-05,  2.0564e-05,  ..., -2.2754e-05,
         -1.8671e-05, -1.7792e-05],
        [-1.7732e-05, -1.1928e-05,  1.3664e-05,  ..., -1.6078e-05,
         -1.3195e-05, -1.3292e-05],
        [-2.2486e-05, -1.6883e-05,  1.7554e-05,  ..., -2.0295e-05,
         -1.7330e-05, -1.4454e-05]], device='cuda:0')
Loss: 0.914190411567688


Running epoch 1, step 1527, batch 479
Sampled inputs[:2]: tensor([[    0,  5862,    13,  ..., 12497,   287,  3570],
        [    0,   221,   334,  ...,  1422, 30163,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1187e-04,  1.2095e-04, -2.9863e-06,  ..., -1.0700e-04,
          2.8994e-04,  1.1416e-04],
        [-1.0334e-05, -7.1079e-06,  7.6964e-06,  ..., -9.3877e-06,
         -7.6294e-06, -7.4580e-06],
        [-2.9176e-05, -2.1100e-05,  2.3529e-05,  ..., -2.6047e-05,
         -2.1309e-05, -2.0415e-05],
        [-2.0280e-05, -1.3620e-05,  1.5691e-05,  ..., -1.8477e-05,
         -1.5102e-05, -1.5318e-05],
        [-2.5734e-05, -1.9222e-05,  2.0191e-05,  ..., -2.3305e-05,
         -1.9833e-05, -1.6645e-05]], device='cuda:0')
Loss: 0.984807550907135
Graident accumulation at epoch 1, step 1527, batch 479
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0288,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.8204e-05,  6.6586e-05, -1.7804e-05,  ..., -3.4864e-05,
          9.3203e-05,  1.5255e-04],
        [-6.8005e-06, -5.1030e-06,  4.1918e-06,  ..., -6.2995e-06,
         -2.4433e-06, -5.4140e-06],
        [ 2.8554e-05,  3.2858e-05, -2.7790e-05,  ...,  2.8388e-05,
          3.4521e-05,  9.2288e-06],
        [ 3.1719e-06,  1.8653e-05, -7.1246e-07,  ...,  5.3713e-06,
          1.1540e-05, -2.0483e-06],
        [-2.5960e-05, -1.9486e-05,  1.9592e-05,  ..., -2.2876e-05,
         -1.9909e-05, -1.7416e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8327e-08, 5.3186e-08, 5.3208e-08,  ..., 2.0214e-08, 1.2049e-07,
         4.6748e-08],
        [8.6705e-11, 5.2029e-11, 2.3994e-11,  ..., 5.9796e-11, 2.7165e-11,
         2.6700e-11],
        [2.0548e-09, 1.4179e-09, 6.7416e-10,  ..., 1.5669e-09, 8.8653e-10,
         5.2974e-10],
        [8.7294e-10, 1.0375e-09, 3.7437e-10,  ..., 8.5775e-10, 6.1798e-10,
         3.6055e-10],
        [4.1744e-10, 2.4237e-10, 8.7140e-11,  ..., 3.0480e-10, 8.6796e-11,
         1.2398e-10]], device='cuda:0')
optimizer state dict: 191.0
lr: [3.6106089361640563e-06, 3.6106089361640563e-06]
scheduler_last_epoch: 191
Epoch 1 | Batch 479/1048 | Training PPL: 2639.8052045350946 | time 48.22682595252991
Saving checkpoint at epoch 1, step 1527, batch 479
Epoch 1 | Validation PPL: 6.7984289020540976 | Learning rate: 3.6106089361640563e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1527, AFTER epoch 1, step 1527


Running epoch 1, step 1528, batch 480
Sampled inputs[:2]: tensor([[   0, 7428, 1566,  ...,  199, 1726, 5647],
        [   0, 2914,  352,  ...,  897,  328, 1679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4232e-04,  5.3604e-05, -7.6249e-05,  ...,  2.2225e-05,
         -2.1028e-04, -1.5490e-04],
        [-1.3337e-06, -9.0525e-07,  1.1250e-06,  ..., -1.1474e-06,
         -8.9407e-07, -8.9034e-07],
        [-3.9935e-06, -2.8014e-06,  3.5614e-06,  ..., -3.3826e-06,
         -2.6375e-06, -2.6226e-06],
        [-2.6822e-06, -1.7583e-06,  2.3395e-06,  ..., -2.2799e-06,
         -1.7658e-06, -1.8701e-06],
        [-3.2037e-06, -2.3395e-06,  2.8014e-06,  ..., -2.7716e-06,
         -2.2650e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9559827446937561


Running epoch 1, step 1529, batch 481
Sampled inputs[:2]: tensor([[    0, 44210,    89,  ...,    43,  1707,   266],
        [    0, 14979,   408,  ...,   369,  1716,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7069e-04, -1.1600e-05, -1.1954e-04,  ..., -4.5384e-06,
         -2.0743e-04, -2.3468e-04],
        [-2.6003e-06, -1.7881e-06,  2.1383e-06,  ..., -2.3097e-06,
         -1.8105e-06, -1.7732e-06],
        [ 7.5048e-05,  5.5393e-05, -5.1120e-05,  ...,  5.8129e-05,
          4.4748e-05,  2.1244e-05],
        [-5.2303e-06, -3.4869e-06,  4.4703e-06,  ..., -4.6194e-06,
         -3.6135e-06, -3.7402e-06],
        [-6.3330e-06, -4.7386e-06,  5.4240e-06,  ..., -5.6773e-06,
         -4.6790e-06, -3.9339e-06]], device='cuda:0')
Loss: 0.9787570238113403


Running epoch 1, step 1530, batch 482
Sampled inputs[:2]: tensor([[   0,  287, 5724,  ...,  298,  591, 2609],
        [   0, 9466,   36,  ..., 1795,  437,  874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0449e-05,  1.4579e-07,  1.3156e-05,  ...,  5.4617e-06,
         -6.7019e-05, -1.1152e-04],
        [-3.8445e-06, -2.7791e-06,  3.1590e-06,  ..., -3.4124e-06,
         -2.7455e-06, -2.6450e-06],
        [ 7.1338e-05,  5.2324e-05, -4.7872e-05,  ...,  5.4866e-05,
          4.1961e-05,  1.8666e-05],
        [-7.7337e-06, -5.4687e-06,  6.6012e-06,  ..., -6.8545e-06,
         -5.5209e-06, -5.5954e-06],
        [-9.4324e-06, -7.3761e-06,  8.0913e-06,  ..., -8.4490e-06,
         -7.1228e-06, -5.9307e-06]], device='cuda:0')
Loss: 0.9785236120223999


Running epoch 1, step 1531, batch 483
Sampled inputs[:2]: tensor([[    0,    14,  1032,  ...,   292,   494,  2065],
        [    0,   292, 44809,  ...,   642,   437,  9038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3239e-05,  2.2925e-04, -5.5186e-05,  ...,  5.7243e-05,
         -1.6926e-04, -4.1907e-04],
        [-5.1036e-06, -3.6880e-06,  4.1574e-06,  ..., -4.5821e-06,
         -3.6657e-06, -3.5390e-06],
        [ 6.7761e-05,  4.9641e-05, -4.4832e-05,  ...,  5.1618e-05,
          3.9383e-05,  1.6208e-05],
        [-1.0222e-05, -7.2047e-06,  8.6576e-06,  ..., -9.1642e-06,
         -7.3537e-06, -7.4431e-06],
        [-1.2547e-05, -9.7901e-06,  1.0684e-05,  ..., -1.1310e-05,
         -9.4920e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.9397559762001038


Running epoch 1, step 1532, batch 484
Sampled inputs[:2]: tensor([[    0,   292,    33,  ..., 32754,   300, 14476],
        [    0,  6112,   278,  ...,  4092,   490,  2774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.8500e-05,  1.9530e-04, -4.8212e-05,  ..., -3.9443e-05,
         -2.9489e-04, -5.6607e-04],
        [-6.3777e-06, -4.5784e-06,  5.2899e-06,  ..., -5.6773e-06,
         -4.4219e-06, -4.3102e-06],
        [ 1.2310e-04,  1.3546e-04, -8.8881e-05,  ...,  1.1505e-04,
          1.2532e-04,  3.1929e-05],
        [-1.2800e-05, -8.9705e-06,  1.1042e-05,  ..., -1.1370e-05,
         -8.8662e-06, -9.0972e-06],
        [-1.5453e-05, -1.2040e-05,  1.3351e-05,  ..., -1.3873e-05,
         -1.1384e-05, -9.5218e-06]], device='cuda:0')
Loss: 0.9695659279823303


Running epoch 1, step 1533, batch 485
Sampled inputs[:2]: tensor([[    0,    12,   297,  ...,  2980,  1145, 17207],
        [    0,    14, 49601,  ...,    12,   298,   374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3308e-05,  3.2816e-04, -1.0190e-05,  ...,  8.0747e-05,
         -2.5966e-04, -5.2867e-04],
        [-7.6815e-06, -5.5917e-06,  6.2585e-06,  ..., -6.8620e-06,
         -5.4277e-06, -5.1744e-06],
        [ 1.1947e-04,  1.3245e-04, -8.5990e-05,  ...,  1.1176e-04,
          1.2247e-04,  2.9530e-05],
        [-1.5363e-05, -1.0952e-05,  1.3024e-05,  ..., -1.3724e-05,
         -1.0893e-05, -1.0908e-05],
        [-1.8597e-05, -1.4737e-05,  1.5795e-05,  ..., -1.6779e-05,
         -1.3992e-05, -1.1444e-05]], device='cuda:0')
Loss: 0.9581455588340759


Running epoch 1, step 1534, batch 486
Sampled inputs[:2]: tensor([[    0,   278,  3358,  ...,    12,   287,  9612],
        [    0,   714,    14,  ...,  1501, 11397, 31940]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0042e-05,  5.4194e-04, -1.2024e-05,  ...,  1.3885e-04,
         -4.0525e-04, -6.6446e-04],
        [-8.9779e-06, -6.5379e-06,  7.2345e-06,  ..., -8.0168e-06,
         -6.4112e-06, -6.0238e-06],
        [ 1.1576e-04,  1.2959e-04, -8.2980e-05,  ...,  1.0846e-04,
          1.1963e-04,  2.7101e-05],
        [-1.7911e-05, -1.2785e-05,  1.5035e-05,  ..., -1.6004e-05,
         -1.2860e-05, -1.2673e-05],
        [-2.1756e-05, -1.7270e-05,  1.8299e-05,  ..., -1.9640e-05,
         -1.6555e-05, -1.3344e-05]], device='cuda:0')
Loss: 0.9598366618156433


Running epoch 1, step 1535, batch 487
Sampled inputs[:2]: tensor([[    0,  2278,   292,  ..., 12060,  1319,   292],
        [    0,    13, 26335,  ...,     5,  2570, 34403]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9672e-05,  6.1528e-04,  1.7862e-04,  ...,  4.2509e-05,
         -2.5351e-04, -5.0978e-04],
        [-1.0237e-05, -7.6033e-06,  8.2105e-06,  ..., -9.2015e-06,
         -7.4916e-06, -6.9030e-06],
        [ 1.1216e-04,  1.2642e-04, -8.0029e-05,  ...,  1.0509e-04,
          1.1654e-04,  2.4582e-05],
        [-2.0355e-05, -1.4827e-05,  1.7002e-05,  ..., -1.8314e-05,
         -1.4976e-05, -1.4491e-05],
        [-2.5019e-05, -2.0236e-05,  2.0921e-05,  ..., -2.2739e-05,
         -1.9491e-05, -1.5490e-05]], device='cuda:0')
Loss: 0.9905114769935608
Graident accumulation at epoch 1, step 1535, batch 487
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.6417e-05,  1.2145e-04,  1.8384e-06,  ..., -2.7127e-05,
          5.8531e-05,  8.6314e-05],
        [-7.1442e-06, -5.3531e-06,  4.5937e-06,  ..., -6.5897e-06,
         -2.9481e-06, -5.5629e-06],
        [ 3.6915e-05,  4.2214e-05, -3.3014e-05,  ...,  3.6059e-05,
          4.2723e-05,  1.0764e-05],
        [ 8.1917e-07,  1.5305e-05,  1.0590e-06,  ...,  3.0028e-06,
          8.8888e-06, -3.2926e-06],
        [-2.5866e-05, -1.9561e-05,  1.9725e-05,  ..., -2.2862e-05,
         -1.9867e-05, -1.7223e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8277e-08, 5.3511e-08, 5.3187e-08,  ..., 2.0196e-08, 1.2044e-07,
         4.6961e-08],
        [8.6723e-11, 5.2035e-11, 2.4037e-11,  ..., 5.9821e-11, 2.7194e-11,
         2.6721e-11],
        [2.0653e-09, 1.4325e-09, 6.7989e-10,  ..., 1.5763e-09, 8.9922e-10,
         5.2982e-10],
        [8.7248e-10, 1.0367e-09, 3.7429e-10,  ..., 8.5723e-10, 6.1759e-10,
         3.6040e-10],
        [4.1765e-10, 2.4254e-10, 8.7490e-11,  ..., 3.0501e-10, 8.7089e-11,
         1.2410e-10]], device='cuda:0')
optimizer state dict: 192.0
lr: [3.5160065499038043e-06, 3.5160065499038043e-06]
scheduler_last_epoch: 192


Running epoch 1, step 1536, batch 488
Sampled inputs[:2]: tensor([[   0,  278, 2354,  ..., 4974, 7757,  472],
        [   0,  586,  940,  ..., 1471, 2612,  591]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3656e-06,  2.0111e-05, -3.9073e-05,  ...,  2.8952e-05,
         -5.0752e-05,  5.7951e-05],
        [-1.2591e-06, -9.4622e-07,  1.1027e-06,  ..., -1.0803e-06,
         -8.1584e-07, -8.4564e-07],
        [-3.6806e-06, -2.8759e-06,  3.4124e-06,  ..., -3.1441e-06,
         -2.4140e-06, -2.4438e-06],
        [-2.4885e-06, -1.8328e-06,  2.2799e-06,  ..., -2.1309e-06,
         -1.6093e-06, -1.7583e-06],
        [-3.0249e-06, -2.4736e-06,  2.7567e-06,  ..., -2.6524e-06,
         -2.1309e-06, -1.8552e-06]], device='cuda:0')
Loss: 0.948292076587677


Running epoch 1, step 1537, batch 489
Sampled inputs[:2]: tensor([[    0,   607,  2697,  ...,   391, 14410, 14997],
        [    0,  9116,   278,  ...,  6997,  3244,  1192]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2482e-04,  5.2190e-05, -1.1503e-04,  ...,  7.2297e-05,
          9.2336e-08, -1.1490e-04],
        [ 6.4874e-05,  9.1860e-05, -5.1502e-05,  ...,  5.0656e-05,
          7.5643e-05,  5.9888e-05],
        [-7.1973e-06, -5.8413e-06,  6.3181e-06,  ..., -6.5118e-06,
         -5.2601e-06, -4.7386e-06],
        [-4.9472e-06, -3.7998e-06,  4.2617e-06,  ..., -4.5151e-06,
         -3.6210e-06, -3.4794e-06],
        [-6.1244e-06, -5.1707e-06,  5.2601e-06,  ..., -5.6773e-06,
         -4.7684e-06, -3.7253e-06]], device='cuda:0')
Loss: 0.9753333926200867


Running epoch 1, step 1538, batch 490
Sampled inputs[:2]: tensor([[    0,   271, 16217,  ...,  6352,  4546,  2558],
        [    0,   266,  2086,  ...,  4283,   720,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1537e-04,  6.3177e-05,  5.1730e-05,  ...,  3.5757e-05,
          1.7968e-04, -7.5634e-05],
        [ 6.3645e-05,  9.0816e-05, -5.0534e-05,  ...,  4.9479e-05,
          7.4585e-05,  5.9024e-05],
        [-1.0759e-05, -8.9854e-06,  9.3132e-06,  ..., -9.8944e-06,
         -8.2999e-06, -7.2420e-06],
        [-7.3910e-06, -5.8264e-06,  6.2585e-06,  ..., -6.8396e-06,
         -5.7220e-06, -5.2974e-06],
        [-9.2983e-06, -8.0615e-06,  7.8529e-06,  ..., -8.7321e-06,
         -7.6145e-06, -5.7966e-06]], device='cuda:0')
Loss: 1.0035532712936401


Running epoch 1, step 1539, batch 491
Sampled inputs[:2]: tensor([[   0, 2771,   13,  ..., 4169,  278,  266],
        [   0,  437,  266,  ..., 5512,  822,   89]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4761e-04,  2.6196e-04,  1.7940e-04,  ...,  2.3829e-05,
          2.5600e-04,  5.0286e-06],
        [ 6.2408e-05,  8.9826e-05, -4.9636e-05,  ...,  4.8339e-05,
          7.3549e-05,  5.8190e-05],
        [-1.4409e-05, -1.2040e-05,  1.2159e-05,  ..., -1.3232e-05,
         -1.1340e-05, -9.6858e-06],
        [-9.8646e-06, -7.7635e-06,  8.1062e-06,  ..., -9.1195e-06,
         -7.8231e-06, -7.0706e-06],
        [-1.2413e-05, -1.0744e-05,  1.0252e-05,  ..., -1.1638e-05,
         -1.0327e-05, -7.7188e-06]], device='cuda:0')
Loss: 0.9868056178092957


Running epoch 1, step 1540, batch 492
Sampled inputs[:2]: tensor([[    0,   266, 11692,  ...,   278, 14620, 12718],
        [    0,   278,   264,  ..., 21836,   344,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5252e-04,  2.7783e-04,  3.1622e-04,  ...,  2.3829e-05,
          2.1496e-04,  2.0980e-05],
        [ 6.1089e-05,  8.8842e-05, -4.8473e-05,  ...,  4.7222e-05,
          7.2715e-05,  5.7337e-05],
        [-1.8165e-05, -1.4991e-05,  1.5631e-05,  ..., -1.6466e-05,
         -1.3784e-05, -1.2174e-05],
        [-1.2502e-05, -9.6858e-06,  1.0520e-05,  ..., -1.1340e-05,
         -9.4771e-06, -8.8662e-06],
        [-1.5438e-05, -1.3247e-05,  1.2994e-05,  ..., -1.4305e-05,
         -1.2457e-05, -9.5814e-06]], device='cuda:0')
Loss: 0.9529644250869751


Running epoch 1, step 1541, batch 493
Sampled inputs[:2]: tensor([[    0, 10386,  6404,  ...,   292,   325, 12071],
        [    0,   298,  8761,  ...,   271,   266,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0167e-04,  3.7250e-04,  3.8768e-04,  ...,  5.6383e-05,
          1.7112e-04,  1.0954e-04],
        [ 5.9599e-05,  8.8019e-05, -4.7490e-05,  ...,  4.5918e-05,
          7.1824e-05,  5.6189e-05],
        [-2.2486e-05, -1.7539e-05,  1.8686e-05,  ..., -2.0221e-05,
         -1.6391e-05, -1.5467e-05],
        [-1.5259e-05, -1.1206e-05,  1.2405e-05,  ..., -1.3754e-05,
         -1.1139e-05, -1.1027e-05],
        [-1.9312e-05, -1.5512e-05,  1.5631e-05,  ..., -1.7703e-05,
         -1.4871e-05, -1.2383e-05]], device='cuda:0')
Loss: 0.9548808932304382


Running epoch 1, step 1542, batch 494
Sampled inputs[:2]: tensor([[   0,   12,  638,  ...,  374,  221,  527],
        [   0, 2612,  271,  ...,  369, 9862,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8615e-04,  4.3051e-04,  4.5199e-04,  ...,  1.1112e-04,
          1.6504e-04,  4.2539e-05],
        [ 5.8362e-05,  8.7147e-05, -4.6559e-05,  ...,  4.4763e-05,
          7.0897e-05,  5.5243e-05],
        [-2.5958e-05, -2.0072e-05,  2.1517e-05,  ..., -2.3395e-05,
         -1.8969e-05, -1.8016e-05],
        [-1.7688e-05, -1.2860e-05,  1.4313e-05,  ..., -1.6019e-05,
         -1.2964e-05, -1.2949e-05],
        [-2.2396e-05, -1.7837e-05,  1.8105e-05,  ..., -2.0564e-05,
         -1.7285e-05, -1.4484e-05]], device='cuda:0')
Loss: 0.9303637742996216


Running epoch 1, step 1543, batch 495
Sampled inputs[:2]: tensor([[   0, 2827, 5744,  ...,  365,  513,   13],
        [   0,  446, 1845,  ...,  422,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0106e-04,  4.8371e-04,  6.3804e-04,  ...,  1.4023e-04,
          1.7504e-04,  1.0280e-04],
        [ 5.6880e-05,  8.6246e-05, -4.5598e-05,  ...,  4.3496e-05,
          6.9943e-05,  5.4155e-05],
        [-3.0160e-05, -2.2858e-05,  2.4498e-05,  ..., -2.7001e-05,
         -2.1785e-05, -2.1085e-05],
        [-2.0549e-05, -1.4588e-05,  1.6235e-05,  ..., -1.8492e-05,
         -1.4864e-05, -1.5095e-05],
        [-2.6107e-05, -2.0325e-05,  2.0653e-05,  ..., -2.3812e-05,
         -1.9893e-05, -1.7047e-05]], device='cuda:0')
Loss: 0.9230958819389343
Graident accumulation at epoch 1, step 1543, batch 495
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0037,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5331e-05,  1.5768e-04,  6.5458e-05,  ..., -1.0391e-05,
          7.0182e-05,  8.7963e-05],
        [-7.4178e-07,  3.8068e-06, -4.2546e-07,  ..., -1.5811e-06,
          4.3410e-06,  4.0893e-07],
        [ 3.0208e-05,  3.5706e-05, -2.7263e-05,  ...,  2.9753e-05,
          3.6272e-05,  7.5793e-06],
        [-1.3176e-06,  1.2315e-05,  2.5766e-06,  ...,  8.5328e-07,
          6.5135e-06, -4.4728e-06],
        [-2.5890e-05, -1.9638e-05,  1.9817e-05,  ..., -2.2957e-05,
         -1.9870e-05, -1.7206e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8310e-08, 5.3692e-08, 5.3541e-08,  ..., 2.0195e-08, 1.2035e-07,
         4.6924e-08],
        [8.9872e-11, 5.9421e-11, 2.6092e-11,  ..., 6.1653e-11, 3.2059e-11,
         2.9627e-11],
        [2.0642e-09, 1.4316e-09, 6.7981e-10,  ..., 1.5755e-09, 8.9880e-10,
         5.2973e-10],
        [8.7203e-10, 1.0359e-09, 3.7418e-10,  ..., 8.5671e-10, 6.1719e-10,
         3.6027e-10],
        [4.1791e-10, 2.4271e-10, 8.7829e-11,  ..., 3.0527e-10, 8.7398e-11,
         1.2427e-10]], device='cuda:0')
optimizer state dict: 193.0
lr: [3.422394974442298e-06, 3.422394974442298e-06]
scheduler_last_epoch: 193


Running epoch 1, step 1544, batch 496
Sampled inputs[:2]: tensor([[    0,   271, 28279,  ...,   367,   806,   271],
        [    0,    14,   747,  ..., 12545,    12, 15209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9125e-05,  5.1657e-06, -7.3796e-06,  ...,  1.2313e-05,
          8.0394e-05, -2.2826e-05],
        [-1.3486e-06, -9.6112e-07,  1.0282e-06,  ..., -1.1921e-06,
         -9.4995e-07, -9.2387e-07],
        [-3.9339e-06, -2.9355e-06,  3.2336e-06,  ..., -3.4571e-06,
         -2.7716e-06, -2.6673e-06],
        [-2.6375e-06, -1.8626e-06,  2.1160e-06,  ..., -2.3395e-06,
         -1.8626e-06, -1.8775e-06],
        [-3.3528e-06, -2.6226e-06,  2.6971e-06,  ..., -3.0249e-06,
         -2.5481e-06, -2.1309e-06]], device='cuda:0')
Loss: 0.9416552186012268


Running epoch 1, step 1545, batch 497
Sampled inputs[:2]: tensor([[    0,  9582,  3645,  ...,  1027,    12,   461],
        [    0,  2906, 46441,  ..., 39156,   287, 11452]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0145e-04, -7.4252e-05, -6.7561e-05,  ...,  3.7788e-05,
          2.3388e-04,  6.4699e-05],
        [-2.6748e-06, -1.9819e-06,  2.0638e-06,  ..., -2.4065e-06,
         -1.9930e-06, -1.8068e-06],
        [-7.7039e-06, -5.9903e-06,  6.3628e-06,  ..., -6.8843e-06,
         -5.7817e-06, -5.1111e-06],
        [-5.2899e-06, -3.8892e-06,  4.2617e-06,  ..., -4.7535e-06,
         -3.9637e-06, -3.6955e-06],
        [-6.6906e-06, -5.4091e-06,  5.4389e-06,  ..., -6.0946e-06,
         -5.3197e-06, -4.1723e-06]], device='cuda:0')
Loss: 0.9854059219360352


Running epoch 1, step 1546, batch 498
Sampled inputs[:2]: tensor([[   0,  409, 3669,  ...,   12,  374,   20],
        [   0,  271,  259,  ..., 1345,  352,  365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5450e-04,  1.7484e-04, -9.2410e-05,  ..., -8.0306e-05,
         -4.2250e-05,  1.7408e-05],
        [-4.0829e-06, -2.9430e-06,  3.0920e-06,  ..., -3.6433e-06,
         -2.9020e-06, -2.7977e-06],
        [-1.1787e-05, -8.9556e-06,  9.5367e-06,  ..., -1.0461e-05,
         -8.5086e-06, -7.9721e-06],
        [-7.9721e-06, -5.6922e-06,  6.2883e-06,  ..., -7.1079e-06,
         -5.7146e-06, -5.6624e-06],
        [-1.0148e-05, -7.9870e-06,  8.0615e-06,  ..., -9.1642e-06,
         -7.7486e-06, -6.4373e-06]], device='cuda:0')
Loss: 0.9377102851867676


Running epoch 1, step 1547, batch 499
Sampled inputs[:2]: tensor([[    0,   591, 18622,  ...,   955,  6118,  9191],
        [    0,   824,   278,  ...,   266, 10997,   863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8735e-04,  2.4785e-04, -1.4555e-04,  ..., -4.1556e-05,
         -1.0344e-04, -9.7370e-05],
        [-5.3421e-06, -3.8110e-06,  4.0755e-06,  ..., -4.7460e-06,
         -3.7216e-06, -3.5949e-06],
        [-1.5274e-05, -1.1444e-05,  1.2472e-05,  ..., -1.3426e-05,
         -1.0744e-05, -1.0088e-05],
        [-1.0446e-05, -7.3239e-06,  8.3297e-06,  ..., -9.2387e-06,
         -7.2941e-06, -7.2941e-06],
        [-1.3083e-05, -1.0177e-05,  1.0490e-05,  ..., -1.1712e-05,
         -9.7752e-06, -8.0764e-06]], device='cuda:0')
Loss: 0.9221706986427307


Running epoch 1, step 1548, batch 500
Sampled inputs[:2]: tensor([[   0,  221,  380,  ...,  508, 1853,   14],
        [   0, 3699, 3058,  ...,  820, 5327, 8055]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6580e-04,  4.2676e-04,  1.4528e-04,  ...,  1.4293e-04,
         -3.1942e-04, -4.8938e-05],
        [-6.6236e-06, -4.7535e-06,  5.0142e-06,  ..., -5.9679e-06,
         -4.6976e-06, -4.4703e-06],
        [-1.8910e-05, -1.4290e-05,  1.5333e-05,  ..., -1.6883e-05,
         -1.3530e-05, -1.2562e-05],
        [-1.2964e-05, -9.1717e-06,  1.0237e-05,  ..., -1.1683e-05,
         -9.2760e-06, -9.1270e-06],
        [-1.6317e-05, -1.2815e-05,  1.2979e-05,  ..., -1.4827e-05,
         -1.2383e-05, -1.0118e-05]], device='cuda:0')
Loss: 0.9762446284294128


Running epoch 1, step 1549, batch 501
Sampled inputs[:2]: tensor([[   0,   21, 1304,  ..., 3577,   13, 2497],
        [   0,  287, 1477,  ...,  997,  292, 4471]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9460e-04,  8.3764e-04,  5.1898e-04,  ...,  7.2344e-05,
         -7.0605e-04, -3.3136e-04],
        [-7.9349e-06, -5.8040e-06,  5.8115e-06,  ..., -7.2122e-06,
         -5.9120e-06, -5.4091e-06],
        [-2.2724e-05, -1.7509e-05,  1.7777e-05,  ..., -2.0504e-05,
         -1.7077e-05, -1.5289e-05],
        [-1.5676e-05, -1.1288e-05,  1.1832e-05,  ..., -1.4305e-05,
         -1.1884e-05, -1.1168e-05],
        [-1.9759e-05, -1.5795e-05,  1.5110e-05,  ..., -1.8150e-05,
         -1.5721e-05, -1.2487e-05]], device='cuda:0')
Loss: 0.9518194794654846


Running epoch 1, step 1550, batch 502
Sampled inputs[:2]: tensor([[    0,   413,    20,  ...,  2089,    12, 21064],
        [    0,  1682,   271,  ...,   300,   266, 10935]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.3491e-04,  9.9173e-04,  6.1218e-04,  ..., -1.7603e-04,
         -8.4521e-04, -5.6030e-04],
        [-9.2238e-06, -6.8918e-06,  6.7130e-06,  ..., -8.4192e-06,
         -7.0520e-06, -6.3144e-06],
        [-2.6494e-05, -2.0817e-05,  2.0608e-05,  ..., -2.4006e-05,
         -2.0385e-05, -1.7926e-05],
        [-1.8224e-05, -1.3404e-05,  1.3672e-05,  ..., -1.6689e-05,
         -1.4149e-05, -1.3053e-05],
        [-2.3156e-05, -1.8865e-05,  1.7598e-05,  ..., -2.1368e-05,
         -1.8835e-05, -1.4722e-05]], device='cuda:0')
Loss: 0.9589856266975403


Running epoch 1, step 1551, batch 503
Sampled inputs[:2]: tensor([[    0,   278,   565,  ...,  1125,  5222,   287],
        [    0, 25939, 47777,  ...,    13,  3483,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5284e-04,  9.6971e-04,  6.3421e-04,  ..., -1.0272e-04,
         -9.0466e-04, -5.0526e-04],
        [-1.0587e-05, -7.8082e-06,  7.7337e-06,  ..., -9.6038e-06,
         -7.9758e-06, -7.2382e-06],
        [-3.0577e-05, -2.3723e-05,  2.3857e-05,  ..., -2.7537e-05,
         -2.3186e-05, -2.0668e-05],
        [-2.0862e-05, -1.5147e-05,  1.5713e-05,  ..., -1.8999e-05,
         -1.5952e-05, -1.4916e-05],
        [-2.6524e-05, -2.1368e-05,  2.0236e-05,  ..., -2.4334e-05,
         -2.1279e-05, -1.6838e-05]], device='cuda:0')
Loss: 0.9482784867286682
Graident accumulation at epoch 1, step 1551, batch 503
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-6.9082e-05,  2.3888e-04,  1.2233e-04,  ..., -1.9624e-05,
         -2.7302e-05,  2.8640e-05],
        [-1.7263e-06,  2.6453e-06,  3.9045e-07,  ..., -2.3834e-06,
          3.1094e-06, -3.5579e-07],
        [ 2.4129e-05,  2.9763e-05, -2.2151e-05,  ...,  2.4024e-05,
          3.0326e-05,  4.7545e-06],
        [-3.2720e-06,  9.5692e-06,  3.8903e-06,  ..., -1.1319e-06,
          4.2670e-06, -5.5172e-06],
        [-2.5953e-05, -1.9811e-05,  1.9859e-05,  ..., -2.3095e-05,
         -2.0011e-05, -1.7169e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8557e-08, 5.4579e-08, 5.3889e-08,  ..., 2.0185e-08, 1.2104e-07,
         4.7133e-08],
        [8.9894e-11, 5.9422e-11, 2.6126e-11,  ..., 6.1684e-11, 3.2091e-11,
         2.9650e-11],
        [2.0631e-09, 1.4307e-09, 6.7970e-10,  ..., 1.5747e-09, 8.9844e-10,
         5.2963e-10],
        [8.7160e-10, 1.0351e-09, 3.7405e-10,  ..., 8.5621e-10, 6.1683e-10,
         3.6013e-10],
        [4.1820e-10, 2.4292e-10, 8.8151e-11,  ..., 3.0556e-10, 8.7763e-11,
         1.2443e-10]], device='cuda:0')
optimizer state dict: 194.0
lr: [3.32978851444543e-06, 3.32978851444543e-06]
scheduler_last_epoch: 194


Running epoch 1, step 1552, batch 504
Sampled inputs[:2]: tensor([[   0,   12,  344,  ..., 2337, 1122,  408],
        [   0,  278, 1620,  ...,  360, 1758,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6284e-05,  1.4019e-04, -1.0200e-04,  ...,  2.4447e-04,
         -1.8798e-04,  1.4584e-05],
        [-1.3560e-06, -8.1211e-07,  9.9838e-07,  ..., -1.1623e-06,
         -8.7172e-07, -9.6858e-07],
        [-3.8445e-06, -2.4587e-06,  3.0696e-06,  ..., -3.2634e-06,
         -2.4885e-06, -2.6822e-06],
        [-2.6375e-06, -1.5199e-06,  2.0266e-06,  ..., -2.2799e-06,
         -1.7062e-06, -1.9521e-06],
        [-3.3379e-06, -2.2054e-06,  2.5630e-06,  ..., -2.8759e-06,
         -2.2799e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9240190386772156


Running epoch 1, step 1553, batch 505
Sampled inputs[:2]: tensor([[    0,   970,    13,  ..., 13798,    14,  1841],
        [    0, 17900,   554,  ...,   266,  7912,    26]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9327e-04,  2.2902e-04, -8.1551e-05,  ...,  3.8334e-04,
         -4.3215e-04, -2.1345e-04],
        [-2.6822e-06, -1.7881e-06,  2.1532e-06,  ..., -2.2575e-06,
         -1.7397e-06, -1.7844e-06],
        [-7.7486e-06, -5.4240e-06,  6.6608e-06,  ..., -6.4820e-06,
         -5.0515e-06, -5.0962e-06],
        [-5.2899e-06, -3.4422e-06,  4.4405e-06,  ..., -4.4554e-06,
         -3.4347e-06, -3.6806e-06],
        [-6.4969e-06, -4.7386e-06,  5.3942e-06,  ..., -5.5581e-06,
         -4.5002e-06, -3.9563e-06]], device='cuda:0')
Loss: 0.9740224480628967


Running epoch 1, step 1554, batch 506
Sampled inputs[:2]: tensor([[    0,   413,    16,  ...,   493,  2104,    14],
        [    0,   298, 21144,  ...,  7825, 19426,  3709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2884e-04,  5.3255e-04,  4.3677e-05,  ...,  4.8319e-04,
         -6.4490e-04, -2.1345e-04],
        [-4.0755e-06, -2.7791e-06,  2.9355e-06,  ..., -3.5688e-06,
         -2.8573e-06, -2.8424e-06],
        [-1.1802e-05, -8.4788e-06,  9.1642e-06,  ..., -1.0267e-05,
         -8.3447e-06, -8.1360e-06],
        [-7.9423e-06, -5.3048e-06,  5.9381e-06,  ..., -7.0035e-06,
         -5.6401e-06, -5.7667e-06],
        [-1.0282e-05, -7.6294e-06,  7.6890e-06,  ..., -9.1344e-06,
         -7.6890e-06, -6.6236e-06]], device='cuda:0')
Loss: 0.9573082327842712


Running epoch 1, step 1555, batch 507
Sampled inputs[:2]: tensor([[   0,  298,  894,  ...,  266, 2904, 1679],
        [   0,   17,   12,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8600e-04,  7.6322e-04,  1.8018e-04,  ...,  5.0288e-04,
         -4.4467e-04, -1.2587e-04],
        [-5.5060e-06, -3.7476e-06,  3.7998e-06,  ..., -4.9099e-06,
         -3.9376e-06, -3.9302e-06],
        [-1.5825e-05, -1.1399e-05,  1.1846e-05,  ..., -1.3992e-05,
         -1.1429e-05, -1.1161e-05],
        [-1.0625e-05, -7.1004e-06,  7.5996e-06,  ..., -9.5665e-06,
         -7.7412e-06, -7.8827e-06],
        [-1.4052e-05, -1.0401e-05,  1.0133e-05,  ..., -1.2696e-05,
         -1.0699e-05, -9.2909e-06]], device='cuda:0')
Loss: 0.9492963552474976


Running epoch 1, step 1556, batch 508
Sampled inputs[:2]: tensor([[    0,     9,   287,  ..., 16261,   417,   199],
        [    0,  3825,  1626,  ...,  5096,  3775,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1848e-05,  8.0927e-04,  1.6598e-04,  ...,  4.8972e-04,
         -4.4478e-04, -5.8351e-06],
        [-6.8247e-06, -4.7237e-06,  4.8354e-06,  ..., -6.0499e-06,
         -4.8429e-06, -4.8429e-06],
        [-1.9699e-05, -1.4380e-05,  1.5095e-05,  ..., -1.7300e-05,
         -1.4067e-05, -1.3813e-05],
        [-1.3232e-05, -8.9929e-06,  9.7305e-06,  ..., -1.1802e-05,
         -9.5069e-06, -9.7677e-06],
        [-1.7315e-05, -1.2994e-05,  1.2785e-05,  ..., -1.5527e-05,
         -1.3053e-05, -1.1347e-05]], device='cuda:0')
Loss: 0.9500954151153564


Running epoch 1, step 1557, batch 509
Sampled inputs[:2]: tensor([[    0,   474,   513,  ...,   221,  2951,  7773],
        [    0,    14,  3948,  ...,   571, 10097,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6656e-05,  7.3456e-04,  2.7363e-04,  ...,  6.2685e-04,
         -2.7627e-04, -1.0374e-04],
        [-8.0541e-06, -5.6401e-06,  5.7295e-06,  ..., -7.2196e-06,
         -5.7817e-06, -5.7183e-06],
        [-2.3216e-05, -1.7107e-05,  1.7896e-05,  ..., -2.0579e-05,
         -1.6719e-05, -1.6227e-05],
        [-1.5646e-05, -1.0744e-05,  1.1556e-05,  ..., -1.4096e-05,
         -1.1362e-05, -1.1563e-05],
        [-2.0370e-05, -1.5438e-05,  1.5154e-05,  ..., -1.8403e-05,
         -1.5497e-05, -1.3284e-05]], device='cuda:0')
Loss: 0.958964467048645


Running epoch 1, step 1558, batch 510
Sampled inputs[:2]: tensor([[   0, 6584,  278,  ..., 1039,  965, 1410],
        [   0,  278, 7914,  ..., 1194,  300, 4419]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8210e-04,  9.7500e-04,  2.2713e-04,  ...,  6.4530e-04,
         -3.4715e-04, -6.7830e-05],
        [-9.4473e-06, -6.4932e-06,  6.7279e-06,  ..., -8.4862e-06,
         -6.6906e-06, -6.7241e-06],
        [-2.6986e-05, -1.9565e-05,  2.0832e-05,  ..., -2.3946e-05,
         -1.9163e-05, -1.8850e-05],
        [-1.8299e-05, -1.2353e-05,  1.3553e-05,  ..., -1.6510e-05,
         -1.3106e-05, -1.3530e-05],
        [-2.3872e-05, -1.7762e-05,  1.7792e-05,  ..., -2.1577e-05,
         -1.7896e-05, -1.5564e-05]], device='cuda:0')
Loss: 0.9618266224861145


Running epoch 1, step 1559, batch 511
Sampled inputs[:2]: tensor([[   0,   15, 2537,  ...,   14, 3544,  417],
        [   0, 3036,  471,  ...,  287, 1906,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6086e-04,  8.8393e-04,  2.7171e-04,  ...,  6.1002e-04,
         -3.4379e-04, -6.5773e-05],
        [-1.0639e-05, -7.5586e-06,  7.7635e-06,  ..., -9.5740e-06,
         -7.6406e-06, -7.5363e-06],
        [-3.0607e-05, -2.2918e-05,  2.4125e-05,  ..., -2.7254e-05,
         -2.2054e-05, -2.1338e-05],
        [-2.0713e-05, -1.4484e-05,  1.5743e-05,  ..., -1.8716e-05,
         -1.5013e-05, -1.5266e-05],
        [-2.6882e-05, -2.0623e-05,  2.0474e-05,  ..., -2.4378e-05,
         -2.0415e-05, -1.7487e-05]], device='cuda:0')
Loss: 0.9665379524230957
Graident accumulation at epoch 1, step 1559, batch 511
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.8260e-05,  3.0339e-04,  1.3727e-04,  ...,  4.3341e-05,
         -5.8951e-05,  1.9199e-05],
        [-2.6176e-06,  1.6249e-06,  1.1278e-06,  ..., -3.1024e-06,
          2.0344e-06, -1.0738e-06],
        [ 1.8656e-05,  2.4495e-05, -1.7523e-05,  ...,  1.8896e-05,
          2.5088e-05,  2.1452e-06],
        [-5.0161e-06,  7.1638e-06,  5.0755e-06,  ..., -2.8903e-06,
          2.3390e-06, -6.4921e-06],
        [-2.6046e-05, -1.9892e-05,  1.9921e-05,  ..., -2.3223e-05,
         -2.0051e-05, -1.7201e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8629e-08, 5.5305e-08, 5.3909e-08,  ..., 2.0537e-08, 1.2104e-07,
         4.7090e-08],
        [8.9917e-11, 5.9420e-11, 2.6160e-11,  ..., 6.1714e-11, 3.2117e-11,
         2.9677e-11],
        [2.0619e-09, 1.4298e-09, 6.7961e-10,  ..., 1.5738e-09, 8.9803e-10,
         5.2956e-10],
        [8.7115e-10, 1.0342e-09, 3.7392e-10,  ..., 8.5571e-10, 6.1644e-10,
         3.6000e-10],
        [4.1850e-10, 2.4311e-10, 8.8482e-11,  ..., 3.0585e-10, 8.8092e-11,
         1.2461e-10]], device='cuda:0')
optimizer state dict: 195.0
lr: [3.2382013209886466e-06, 3.2382013209886466e-06]
scheduler_last_epoch: 195


Running epoch 1, step 1560, batch 512
Sampled inputs[:2]: tensor([[   0,  593,  300,  ...,  278, 4694,   12],
        [   0,  623,   12,  ..., 4792, 6572,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8088e-06,  1.4514e-05, -1.6418e-04,  ..., -1.3864e-05,
         -3.1855e-05, -4.3430e-05],
        [-1.2890e-06, -9.9093e-07,  1.0654e-06,  ..., -1.1474e-06,
         -9.0152e-07, -8.5309e-07],
        [-3.7402e-06, -3.0100e-06,  3.2485e-06,  ..., -3.3379e-06,
         -2.6226e-06, -2.5034e-06],
        [-2.5332e-06, -1.9073e-06,  2.1756e-06,  ..., -2.2352e-06,
         -1.7509e-06, -1.7881e-06],
        [-3.1143e-06, -2.6226e-06,  2.6524e-06,  ..., -2.8610e-06,
         -2.3693e-06, -1.9372e-06]], device='cuda:0')
Loss: 0.9727201461791992


Running epoch 1, step 1561, batch 513
Sampled inputs[:2]: tensor([[    0,   287, 17044,  ...,   496,    14,  1841],
        [    0,   344,  3693,  ...,  1782,  3679,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2700e-05, -2.9775e-05, -1.5367e-04,  ..., -4.2841e-05,
          6.1644e-05,  8.6698e-05],
        [-2.5928e-06, -2.0117e-06,  2.0787e-06,  ..., -2.3320e-06,
         -1.8366e-06, -1.7136e-06],
        [-7.5102e-06, -6.1393e-06,  6.3777e-06,  ..., -6.7353e-06,
         -5.3346e-06, -4.9621e-06],
        [-5.0664e-06, -3.8743e-06,  4.2319e-06,  ..., -4.5449e-06,
         -3.5912e-06, -3.5688e-06],
        [-6.2883e-06, -5.3793e-06,  5.2452e-06,  ..., -5.7966e-06,
         -4.8131e-06, -3.8445e-06]], device='cuda:0')
Loss: 1.0003947019577026


Running epoch 1, step 1562, batch 514
Sampled inputs[:2]: tensor([[    0, 42306,   278,  ...,  1110,  3427,  4224],
        [    0,    13,  1924,  ...,  2117,   300, 26473]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5712e-05, -1.9054e-05, -1.8605e-04,  ..., -6.5282e-06,
          5.4552e-05,  3.5667e-05],
        [-3.8445e-06, -3.0249e-06,  2.9802e-06,  ..., -3.5241e-06,
         -2.8275e-06, -2.5965e-06],
        [-1.1161e-05, -9.1344e-06,  9.2387e-06,  ..., -1.0103e-05,
         -8.1807e-06, -7.4208e-06],
        [-7.4506e-06, -5.7593e-06,  6.0275e-06,  ..., -6.7949e-06,
         -5.4911e-06, -5.3272e-06],
        [-9.4473e-06, -8.0615e-06,  7.6741e-06,  ..., -8.7619e-06,
         -7.4059e-06, -5.7966e-06]], device='cuda:0')
Loss: 0.9564926624298096


Running epoch 1, step 1563, batch 515
Sampled inputs[:2]: tensor([[   0, 2530,  634,  ...,   15, 8808,    9],
        [   0,  328, 1410,  ..., 7344,   12, 5067]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0719e-04, -2.3769e-04, -5.4977e-04,  ..., -7.9704e-06,
          3.1007e-04,  5.2373e-05],
        [-5.0887e-06, -4.0308e-06,  3.9786e-06,  ..., -4.7237e-06,
         -3.7439e-06, -3.4608e-06],
        [-1.4648e-05, -1.1966e-05,  1.2249e-05,  ..., -1.3366e-05,
         -1.0684e-05, -9.7454e-06],
        [-9.8199e-06, -7.6368e-06,  8.0243e-06,  ..., -9.0748e-06,
         -7.2271e-06, -7.0408e-06],
        [-1.2487e-05, -1.0610e-05,  1.0222e-05,  ..., -1.1638e-05,
         -9.7305e-06, -7.6368e-06]], device='cuda:0')
Loss: 0.9702053666114807


Running epoch 1, step 1564, batch 516
Sampled inputs[:2]: tensor([[   0,  259, 6022,  ..., 1871, 1209, 1241],
        [   0, 1615,  292,  ..., 4824,  292, 9936]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2361e-04, -2.9512e-04, -4.0046e-04,  ...,  1.3947e-05,
          2.5194e-04,  4.0712e-05],
        [-6.4075e-06, -5.1111e-06,  5.0515e-06,  ..., -5.9009e-06,
         -4.7125e-06, -4.3213e-06],
        [ 5.8099e-05,  5.2734e-05, -4.0539e-05,  ...,  1.0134e-04,
          5.4960e-05,  5.6618e-05],
        [-1.2383e-05, -9.7081e-06,  1.0215e-05,  ..., -1.1355e-05,
         -9.0972e-06, -8.8140e-06],
        [-1.5691e-05, -1.3500e-05,  1.2919e-05,  ..., -1.4588e-05,
         -1.2264e-05, -9.5740e-06]], device='cuda:0')
Loss: 0.9929344058036804


Running epoch 1, step 1565, batch 517
Sampled inputs[:2]: tensor([[    0,   721,  1717,  ...,   278, 26029,    12],
        [    0,    19,     9,  ...,  4971,   367,  1675]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2361e-04, -2.7431e-04, -4.7978e-04,  ..., -3.2122e-05,
          4.8202e-04,  1.8075e-04],
        [-7.7412e-06, -6.1616e-06,  5.9605e-06,  ..., -7.1600e-06,
         -5.7854e-06, -5.2154e-06],
        [ 5.4404e-05,  4.9605e-05, -3.7812e-05,  ...,  9.7857e-05,
          5.1965e-05,  5.4114e-05],
        [-1.4946e-05, -1.1735e-05,  1.2033e-05,  ..., -1.3784e-05,
         -1.1198e-05, -1.0639e-05],
        [-1.8984e-05, -1.6376e-05,  1.5303e-05,  ..., -1.7762e-05,
         -1.5095e-05, -1.1645e-05]], device='cuda:0')
Loss: 0.971697986125946


Running epoch 1, step 1566, batch 518
Sampled inputs[:2]: tensor([[   0,  437,  638,  ..., 4514,   14,  333],
        [   0, 6408,  391,  ...,  870,  278,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6866e-04, -3.4723e-04, -5.5588e-04,  ..., -6.7212e-05,
          5.8211e-04,  2.3988e-04],
        [-9.0525e-06, -7.2271e-06,  6.9886e-06,  ..., -8.3148e-06,
         -6.7614e-06, -6.0648e-06],
        [ 5.0679e-05,  4.6416e-05, -3.4668e-05,  ...,  9.4549e-05,
          4.9149e-05,  5.1700e-05],
        [-1.7494e-05, -1.3791e-05,  1.4119e-05,  ..., -1.6049e-05,
         -1.3120e-05, -1.2383e-05],
        [-2.2188e-05, -1.9237e-05,  1.7956e-05,  ..., -2.0668e-05,
         -1.7673e-05, -1.3568e-05]], device='cuda:0')
Loss: 0.9603198170661926


Running epoch 1, step 1567, batch 519
Sampled inputs[:2]: tensor([[    0,    15, 43895,  ...,   292,   380, 16795],
        [    0, 15003, 19278,  ...,   287,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5267e-04, -3.8829e-04, -6.1118e-04,  ..., -8.6070e-05,
          6.8375e-04,  2.9983e-04],
        [-1.0394e-05, -8.1956e-06,  8.0019e-06,  ..., -9.5516e-06,
         -7.7225e-06, -7.0184e-06],
        [ 4.6775e-05,  4.3480e-05, -3.1539e-05,  ...,  9.0988e-05,
          4.6362e-05,  4.8958e-05],
        [-2.0057e-05, -1.5602e-05,  1.6131e-05,  ..., -1.8418e-05,
         -1.4968e-05, -1.4320e-05],
        [-2.5541e-05, -2.1845e-05,  2.0593e-05,  ..., -2.3782e-05,
         -2.0206e-05, -1.5758e-05]], device='cuda:0')
Loss: 0.9589887857437134
Graident accumulation at epoch 1, step 1567, batch 519
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2370e-04,  2.3422e-04,  6.2427e-05,  ...,  3.0400e-05,
          1.5319e-05,  4.7262e-05],
        [-3.3952e-06,  6.4285e-07,  1.8152e-06,  ..., -3.7474e-06,
          1.0587e-06, -1.6683e-06],
        [ 2.1468e-05,  2.6394e-05, -1.8925e-05,  ...,  2.6105e-05,
          2.7216e-05,  6.8266e-06],
        [-6.5202e-06,  4.8873e-06,  6.1810e-06,  ..., -4.4431e-06,
          6.0829e-07, -7.2749e-06],
        [-2.5996e-05, -2.0087e-05,  1.9988e-05,  ..., -2.3279e-05,
         -2.0067e-05, -1.7056e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8694e-08, 5.5401e-08, 5.4229e-08,  ..., 2.0524e-08, 1.2139e-07,
         4.7133e-08],
        [8.9936e-11, 5.9428e-11, 2.6198e-11,  ..., 6.1743e-11, 3.2145e-11,
         2.9696e-11],
        [2.0621e-09, 1.4303e-09, 6.7992e-10,  ..., 1.5805e-09, 8.9928e-10,
         5.3142e-10],
        [8.7069e-10, 1.0334e-09, 3.7381e-10,  ..., 8.5519e-10, 6.1605e-10,
         3.5985e-10],
        [4.1873e-10, 2.4334e-10, 8.8818e-11,  ..., 3.0611e-10, 8.8412e-11,
         1.2473e-10]], device='cuda:0')
optimizer state dict: 196.0
lr: [3.1476473893945937e-06, 3.1476473893945937e-06]
scheduler_last_epoch: 196


Running epoch 1, step 1568, batch 520
Sampled inputs[:2]: tensor([[    0,  2416,   352,  ...,   278,  1036, 16832],
        [    0,   221,   422,  ...,  2693,   733,   381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4118e-05,  1.1681e-04,  1.8251e-04,  ...,  2.0966e-05,
         -9.9649e-05,  2.2419e-04],
        [-1.3486e-06, -8.7917e-07,  9.9093e-07,  ..., -1.1921e-06,
         -9.8348e-07, -9.9838e-07],
        [-3.8445e-06, -2.6673e-06,  2.9951e-06,  ..., -3.3975e-06,
         -2.8610e-06, -2.8312e-06],
        [-2.6226e-06, -1.6764e-06,  1.9968e-06,  ..., -2.3395e-06,
         -1.9372e-06, -2.0266e-06],
        [-3.3826e-06, -2.4289e-06,  2.5332e-06,  ..., -3.0696e-06,
         -2.6822e-06, -2.3544e-06]], device='cuda:0')
Loss: 0.9483304619789124


Running epoch 1, step 1569, batch 521
Sampled inputs[:2]: tensor([[    0,  7377, 30662,  ...,   287,   694, 13403],
        [    0,   775,   721,  ...,  5650,   518, 11548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6561e-04,  1.2303e-04,  1.9311e-04,  ...,  6.6626e-05,
          5.8234e-05,  4.0545e-04],
        [-2.7195e-06, -1.8999e-06,  1.9968e-06,  ..., -2.3991e-06,
         -1.8701e-06, -1.9297e-06],
        [-7.9274e-06, -5.8413e-06,  6.1989e-06,  ..., -6.9737e-06,
         -5.5134e-06, -5.6177e-06],
        [-5.2750e-06, -3.6284e-06,  4.0233e-06,  ..., -4.6641e-06,
         -3.6508e-06, -3.9041e-06],
        [-6.7800e-06, -5.1558e-06,  5.1260e-06,  ..., -6.1095e-06,
         -5.0664e-06, -4.5151e-06]], device='cuda:0')
Loss: 0.9601656794548035


Running epoch 1, step 1570, batch 522
Sampled inputs[:2]: tensor([[    0,   446,  1115,  ...,  1869,  4971,  1954],
        [    0,   292, 17181,  ...,   634,  5039,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0762e-04,  1.1820e-04,  8.9265e-05,  ...,  4.8214e-05,
         -3.7055e-06,  4.4577e-04],
        [-3.9786e-06, -2.8834e-06,  3.0249e-06,  ..., -3.5614e-06,
         -2.7604e-06, -2.7567e-06],
        [-1.1504e-05, -8.7619e-06,  9.3430e-06,  ..., -1.0267e-05,
         -8.0913e-06, -7.9423e-06],
        [-7.7188e-06, -5.4985e-06,  6.1393e-06,  ..., -6.9141e-06,
         -5.4017e-06, -5.5954e-06],
        [-9.7752e-06, -7.7188e-06,  7.7039e-06,  ..., -8.9258e-06,
         -7.3761e-06, -6.2957e-06]], device='cuda:0')
Loss: 0.9552995562553406


Running epoch 1, step 1571, batch 523
Sampled inputs[:2]: tensor([[   0,  287,  552,  ..., 7407, 2401,  287],
        [   0,   21,   13,  ...,   14,  747,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3235e-04,  8.9218e-05,  1.1648e-04,  ...,  1.5803e-05,
         -1.3610e-04,  4.8511e-04],
        [-5.3197e-06, -3.8594e-06,  4.1351e-06,  ..., -4.7237e-06,
         -3.6135e-06, -3.6173e-06],
        [-1.5408e-05, -1.1757e-05,  1.2770e-05,  ..., -1.3649e-05,
         -1.0580e-05, -1.0446e-05],
        [-1.0327e-05, -7.3612e-06,  8.3894e-06,  ..., -9.1493e-06,
         -7.0408e-06, -7.3537e-06],
        [-1.2934e-05, -1.0282e-05,  1.0416e-05,  ..., -1.1727e-05,
         -9.5665e-06, -8.1733e-06]], device='cuda:0')
Loss: 0.9833597540855408


Running epoch 1, step 1572, batch 524
Sampled inputs[:2]: tensor([[    0,   271,   259,  ...,  4511,    14,   333],
        [    0,    14,  5551,  ...,   668, 11988,  2538]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6433e-04,  1.2336e-04,  1.1213e-04,  ..., -7.1249e-05,
          1.2601e-04,  6.2450e-04],
        [-6.6236e-06, -4.8503e-06,  4.9956e-06,  ..., -5.9307e-06,
         -4.5896e-06, -4.5635e-06],
        [-1.9133e-05, -1.4693e-05,  1.5482e-05,  ..., -1.7047e-05,
         -1.3337e-05, -1.3053e-05],
        [-1.2875e-05, -9.2462e-06,  1.0140e-05,  ..., -1.1519e-05,
         -8.9779e-06, -9.2760e-06],
        [-1.6168e-05, -1.2904e-05,  1.2711e-05,  ..., -1.4707e-05,
         -1.2070e-05, -1.0259e-05]], device='cuda:0')
Loss: 0.9558907151222229


Running epoch 1, step 1573, batch 525
Sampled inputs[:2]: tensor([[    0,   494,   298,  ...,   408, 32859, 14550],
        [    0,   367,  6267,  ...,     9,   287, 17056]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4784e-04,  4.4698e-05,  1.3823e-04,  ...,  7.3009e-05,
          3.5924e-05,  5.0279e-04],
        [-8.0243e-06, -5.8115e-06,  5.9083e-06,  ..., -7.2569e-06,
         -5.6326e-06, -5.6215e-06],
        [-2.3186e-05, -1.7598e-05,  1.8388e-05,  ..., -2.0787e-05,
         -1.6317e-05, -1.5989e-05],
        [-1.5572e-05, -1.1027e-05,  1.1973e-05,  ..., -1.4052e-05,
         -1.0960e-05, -1.1347e-05],
        [-1.9819e-05, -1.5542e-05,  1.5229e-05,  ..., -1.8120e-05,
         -1.4871e-05, -1.2733e-05]], device='cuda:0')
Loss: 0.9722252488136292


Running epoch 1, step 1574, batch 526
Sampled inputs[:2]: tensor([[    0,   278,  6481,  ...,    13,  8970,    12],
        [    0,   957,  1357,  ..., 26179,   287,  6458]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8307e-04,  5.1721e-05, -6.4044e-05,  ...,  2.2026e-05,
          1.0981e-04,  6.0190e-04],
        [-9.3132e-06, -6.6869e-06,  6.9663e-06,  ..., -8.3447e-06,
         -6.4820e-06, -6.4857e-06],
        [-2.6897e-05, -2.0266e-05,  2.1651e-05,  ..., -2.3916e-05,
         -1.8790e-05, -1.8463e-05],
        [-1.8105e-05, -1.2696e-05,  1.4164e-05,  ..., -1.6183e-05,
         -1.2621e-05, -1.3143e-05],
        [-2.2918e-05, -1.7881e-05,  1.7896e-05,  ..., -2.0802e-05,
         -1.7092e-05, -1.4640e-05]], device='cuda:0')
Loss: 0.9555032253265381


Running epoch 1, step 1575, batch 527
Sampled inputs[:2]: tensor([[    0,  6847,   437,  ...,    17,    14,    16],
        [    0, 48214,   287,  ...,   494,  8524,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.4399e-04,  5.5054e-05,  8.0163e-05,  ..., -1.4083e-05,
          5.3820e-05,  6.8040e-04],
        [-1.0729e-05, -7.6853e-06,  7.8753e-06,  ..., -9.6112e-06,
         -7.5698e-06, -7.5437e-06],
        [-3.0920e-05, -2.3231e-05,  2.4483e-05,  ..., -2.7478e-05,
         -2.1875e-05, -2.1428e-05],
        [-2.0742e-05, -1.4514e-05,  1.5922e-05,  ..., -1.8552e-05,
         -1.4663e-05, -1.5169e-05],
        [-2.6599e-05, -2.0638e-05,  2.0429e-05,  ..., -2.4110e-05,
         -2.0042e-05, -1.7174e-05]], device='cuda:0')
Loss: 0.9665111899375916
Graident accumulation at epoch 1, step 1575, batch 527
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4573e-04,  2.1630e-04,  6.4200e-05,  ...,  2.5951e-05,
          1.9169e-05,  1.1058e-04],
        [-4.1286e-06, -1.8997e-07,  2.4212e-06,  ..., -4.3337e-06,
          1.9583e-07, -2.2558e-06],
        [ 1.6229e-05,  2.1431e-05, -1.4584e-05,  ...,  2.0747e-05,
          2.2307e-05,  4.0011e-06],
        [-7.9424e-06,  2.9472e-06,  7.1551e-06,  ..., -5.8540e-06,
         -9.1882e-07, -8.0643e-06],
        [-2.6056e-05, -2.0142e-05,  2.0032e-05,  ..., -2.3362e-05,
         -2.0064e-05, -1.7068e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8754e-08, 5.5348e-08, 5.4181e-08,  ..., 2.0504e-08, 1.2127e-07,
         4.7549e-08],
        [8.9961e-11, 5.9428e-11, 2.6234e-11,  ..., 6.1774e-11, 3.2170e-11,
         2.9724e-11],
        [2.0610e-09, 1.4294e-09, 6.7984e-10,  ..., 1.5797e-09, 8.9886e-10,
         5.3135e-10],
        [8.7025e-10, 1.0326e-09, 3.7369e-10,  ..., 8.5468e-10, 6.1565e-10,
         3.5972e-10],
        [4.1902e-10, 2.4352e-10, 8.9146e-11,  ..., 3.0638e-10, 8.8726e-11,
         1.2490e-10]], device='cuda:0')
optimizer state dict: 197.0
lr: [3.058140557094472e-06, 3.058140557094472e-06]
scheduler_last_epoch: 197


Running epoch 1, step 1576, batch 528
Sampled inputs[:2]: tensor([[    0,   275,  2101,  ...,  1145,   590,  1619],
        [    0, 26396,    83,  ...,   292,    18,   590]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.7681e-05, -1.7782e-04, -3.9988e-04,  ..., -3.3885e-05,
          1.4022e-04, -2.0402e-05],
        [-1.3113e-06, -9.4622e-07,  8.8289e-07,  ..., -1.2368e-06,
         -9.6858e-07, -9.0152e-07],
        [-3.3975e-06, -2.5630e-06,  2.5779e-06,  ..., -3.0994e-06,
         -2.4885e-06, -2.1607e-06],
        [-2.4587e-06, -1.7285e-06,  1.7509e-06,  ..., -2.3395e-06,
         -1.8924e-06, -1.7807e-06],
        [-3.1143e-06, -2.4438e-06,  2.3097e-06,  ..., -2.8461e-06,
         -2.3842e-06, -1.7881e-06]], device='cuda:0')
Loss: 0.9607300162315369


Running epoch 1, step 1577, batch 529
Sampled inputs[:2]: tensor([[    0,   616,  2002,  ..., 19763,   642,   342],
        [    0,   221,   451,  ...,   741, 25712,   950]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1128e-04, -2.7450e-04, -2.7819e-04,  ..., -7.2426e-05,
          2.9524e-05, -1.9652e-04],
        [-2.6524e-06, -1.6615e-06,  1.7546e-06,  ..., -2.5183e-06,
         -1.8403e-06, -1.9968e-06],
        [-7.0035e-06, -4.5747e-06,  5.2005e-06,  ..., -6.3479e-06,
         -4.7386e-06, -4.8727e-06],
        [-5.0366e-06, -3.0547e-06,  3.4943e-06,  ..., -4.7982e-06,
         -3.5688e-06, -3.9414e-06],
        [-6.3032e-06, -4.2617e-06,  4.5896e-06,  ..., -5.7369e-06,
         -4.4554e-06, -3.9935e-06]], device='cuda:0')
Loss: 0.9193148612976074


Running epoch 1, step 1578, batch 530
Sampled inputs[:2]: tensor([[    0,  1278,    69,  ...,    15,  7377, 20524],
        [    0, 14094,    83,  ...,  1431,   221,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4678e-04, -3.9085e-04, -3.7151e-04,  ...,  2.5402e-05,
          3.3003e-05, -1.0552e-04],
        [-3.9414e-06, -2.5891e-06,  2.7902e-06,  ..., -3.6657e-06,
         -2.6673e-06, -2.8834e-06],
        [-1.0610e-05, -7.3314e-06,  8.3596e-06,  ..., -9.5367e-06,
         -7.0781e-06, -7.3314e-06],
        [-7.4655e-06, -4.7907e-06,  5.5805e-06,  ..., -6.9737e-06,
         -5.1409e-06, -5.7071e-06],
        [-9.2685e-06, -6.6310e-06,  7.1079e-06,  ..., -8.4192e-06,
         -6.5267e-06, -5.8338e-06]], device='cuda:0')
Loss: 0.949928879737854


Running epoch 1, step 1579, batch 531
Sampled inputs[:2]: tensor([[    0,   278,  5210,  ...,  1968,  2002,   923],
        [    0,    13, 15578,  ...,   221,   494,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3092e-04, -5.0847e-04, -3.6142e-04,  ..., -5.4216e-05,
          3.3150e-04,  1.4677e-04],
        [-5.1707e-06, -3.5949e-06,  3.6247e-06,  ..., -4.8354e-06,
         -3.6508e-06, -3.7663e-06],
        [-1.4096e-05, -1.0252e-05,  1.0982e-05,  ..., -1.2726e-05,
         -9.7901e-06, -9.6858e-06],
        [-9.8348e-06, -6.6757e-06,  7.2569e-06,  ..., -9.2238e-06,
         -7.0632e-06, -7.4878e-06],
        [-1.2383e-05, -9.3281e-06,  9.3877e-06,  ..., -1.1310e-05,
         -9.0897e-06, -7.7412e-06]], device='cuda:0')
Loss: 0.9682256579399109


Running epoch 1, step 1580, batch 532
Sampled inputs[:2]: tensor([[   0,  278, 5798,  ...,  266,  729, 1798],
        [   0,  741, 4933,  ...,  932,  365,  838]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6361e-04, -5.0730e-04, -4.4171e-04,  ...,  3.4508e-05,
          2.8692e-04,  2.7405e-04],
        [-6.5193e-06, -4.5635e-06,  4.7497e-06,  ..., -5.9828e-06,
         -4.4778e-06, -4.6529e-06],
        [-1.7896e-05, -1.3128e-05,  1.4395e-05,  ..., -1.5959e-05,
         -1.2159e-05, -1.2159e-05],
        [-1.2398e-05, -8.5086e-06,  9.5218e-06,  ..., -1.1414e-05,
         -8.6501e-06, -9.2611e-06],
        [-1.5453e-05, -1.1787e-05,  1.2070e-05,  ..., -1.3992e-05,
         -1.1161e-05, -9.5740e-06]], device='cuda:0')
Loss: 0.9551295638084412


Running epoch 1, step 1581, batch 533
Sampled inputs[:2]: tensor([[    0,  1067,   408,  ...,  4657,  1016,   271],
        [    0,   685,  3482,  ..., 23113,    12,  6481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5284e-04, -4.5164e-04, -3.2365e-04,  ..., -8.4020e-05,
          1.8018e-04,  2.1580e-04],
        [-7.8753e-06, -5.4874e-06,  5.6550e-06,  ..., -7.1973e-06,
         -5.4464e-06, -5.5656e-06],
        [-2.1592e-05, -1.5840e-05,  1.7107e-05,  ..., -1.9252e-05,
         -1.4856e-05, -1.4648e-05],
        [-1.5005e-05, -1.0267e-05,  1.1317e-05,  ..., -1.3784e-05,
         -1.0587e-05, -1.1131e-05],
        [-1.8686e-05, -1.4275e-05,  1.4380e-05,  ..., -1.6943e-05,
         -1.3664e-05, -1.1586e-05]], device='cuda:0')
Loss: 0.9508172869682312


Running epoch 1, step 1582, batch 534
Sampled inputs[:2]: tensor([[    0,  2286,    29,  ...,   518,  1307, 16881],
        [    0,  5160,   278,  ...,   496,    14, 46919]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1276e-04, -2.4265e-04, -9.9548e-05,  ...,  4.6809e-05,
          1.2060e-04,  1.6300e-04],
        [-9.2238e-06, -6.6496e-06,  6.6757e-06,  ..., -8.4266e-06,
         -6.5044e-06, -6.4634e-06],
        [-2.5466e-05, -1.9327e-05,  2.0206e-05,  ..., -2.2829e-05,
         -1.7941e-05, -1.7256e-05],
        [-1.7658e-05, -1.2532e-05,  1.3404e-05,  ..., -1.6212e-05,
         -1.2673e-05, -1.2986e-05],
        [-2.2024e-05, -1.7375e-05,  1.6987e-05,  ..., -2.0057e-05,
         -1.6451e-05, -1.3687e-05]], device='cuda:0')
Loss: 0.999639093875885


Running epoch 1, step 1583, batch 535
Sampled inputs[:2]: tensor([[   0, 2192, 3182,  ..., 1445, 1531,  300],
        [   0,  266, 2967,  ...,  287, 4432,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7273e-04, -1.8501e-04, -1.0982e-04,  ..., -7.6649e-05,
          8.9097e-05,  4.2239e-05],
        [-1.0565e-05, -7.6555e-06,  7.8008e-06,  ..., -9.5889e-06,
         -7.3835e-06, -7.3239e-06],
        [ 1.6081e-05,  2.4765e-05, -5.9046e-05,  ..., -5.5183e-07,
          3.5014e-05,  1.2224e-05],
        [-2.0236e-05, -1.4439e-05,  1.5639e-05,  ..., -1.8418e-05,
         -1.4327e-05, -1.4715e-05],
        [-2.5168e-05, -1.9982e-05,  1.9699e-05,  ..., -2.2873e-05,
         -1.8716e-05, -1.5587e-05]], device='cuda:0')
Loss: 0.9634256958961487
Graident accumulation at epoch 1, step 1583, batch 535
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4843e-04,  1.7617e-04,  4.6798e-05,  ...,  1.5691e-05,
          2.6162e-05,  1.0374e-04],
        [-4.7722e-06, -9.3652e-07,  2.9591e-06,  ..., -4.8593e-06,
         -5.6211e-07, -2.7626e-06],
        [ 1.6214e-05,  2.1765e-05, -1.9030e-05,  ...,  1.8617e-05,
          2.3578e-05,  4.8234e-06],
        [-9.1717e-06,  1.2086e-06,  8.0035e-06,  ..., -7.1104e-06,
         -2.2597e-06, -8.7294e-06],
        [-2.5967e-05, -2.0126e-05,  1.9999e-05,  ..., -2.3313e-05,
         -1.9929e-05, -1.6920e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8725e-08, 5.5327e-08, 5.4139e-08,  ..., 2.0489e-08, 1.2116e-07,
         4.7503e-08],
        [8.9982e-11, 5.9427e-11, 2.6268e-11,  ..., 6.1804e-11, 3.2192e-11,
         2.9747e-11],
        [2.0592e-09, 1.4286e-09, 6.8265e-10,  ..., 1.5781e-09, 8.9918e-10,
         5.3097e-10],
        [8.6978e-10, 1.0318e-09, 3.7356e-10,  ..., 8.5417e-10, 6.1524e-10,
         3.5957e-10],
        [4.1924e-10, 2.4368e-10, 8.9445e-11,  ..., 3.0660e-10, 8.8987e-11,
         1.2502e-10]], device='cuda:0')
optimizer state dict: 198.0
lr: [2.969694501513574e-06, 2.969694501513574e-06]
scheduler_last_epoch: 198


Running epoch 1, step 1584, batch 536
Sampled inputs[:2]: tensor([[   0, 6978, 2285,  ..., 4477,  271,  221],
        [   0,  394,  292,  ..., 1711,  365,  897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4091e-05, -6.2503e-05, -1.2134e-04,  ..., -6.4193e-05,
          1.0851e-04,  1.0269e-04],
        [-1.3039e-06, -1.0133e-06,  1.0133e-06,  ..., -1.2070e-06,
         -9.3505e-07, -8.9407e-07],
        [-3.7104e-06, -3.0249e-06,  3.0398e-06,  ..., -3.4124e-06,
         -2.6375e-06, -2.5630e-06],
        [-2.5481e-06, -1.9372e-06,  2.0564e-06,  ..., -2.3246e-06,
         -1.7956e-06, -1.8477e-06],
        [-3.1292e-06, -2.6673e-06,  2.5183e-06,  ..., -2.9504e-06,
         -2.3842e-06, -1.9968e-06]], device='cuda:0')
Loss: 0.9831357002258301


Running epoch 1, step 1585, batch 537
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   696,   700,   328],
        [    0,   369, 19287,  ..., 12502,  6626,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4055e-05,  8.5879e-05,  2.4144e-05,  ...,  5.0677e-06,
          1.3200e-04,  3.1450e-04],
        [-2.6524e-06, -2.1756e-06,  2.1011e-06,  ..., -2.3842e-06,
         -1.9483e-06, -1.7881e-06],
        [-7.4953e-06, -6.5118e-06,  6.3479e-06,  ..., -6.7800e-06,
         -5.5730e-06, -5.1111e-06],
        [-5.1558e-06, -4.2021e-06,  4.2766e-06,  ..., -4.6194e-06,
         -3.7923e-06, -3.6880e-06],
        [-6.2436e-06, -5.6326e-06,  5.1707e-06,  ..., -5.7817e-06,
         -4.9621e-06, -3.9488e-06]], device='cuda:0')
Loss: 0.9698100686073303


Running epoch 1, step 1586, batch 538
Sampled inputs[:2]: tensor([[   0,  374, 5195,  ...,  266, 5555,   14],
        [   0, 2732,  413,  ...,  287,  266, 3668]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9781e-04, -1.7273e-05, -1.2704e-04,  ..., -2.2800e-05,
          3.4602e-04,  4.4555e-04],
        [-3.8892e-06, -3.1367e-06,  3.0026e-06,  ..., -3.5912e-06,
         -2.9691e-06, -2.7008e-06],
        [-1.0818e-05, -9.1940e-06,  9.0301e-06,  ..., -9.9242e-06,
         -8.2552e-06, -7.4655e-06],
        [-7.5102e-06, -5.9754e-06,  6.0871e-06,  ..., -6.9141e-06,
         -5.7593e-06, -5.5283e-06],
        [-9.2089e-06, -8.1211e-06,  7.4804e-06,  ..., -8.6278e-06,
         -7.4953e-06, -5.8338e-06]], device='cuda:0')
Loss: 0.9410995841026306


Running epoch 1, step 1587, batch 539
Sampled inputs[:2]: tensor([[   0,  642,  271,  ..., 5430, 2314, 6431],
        [   0,  452,  298,  ...,  287, 1575, 7856]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0156e-04, -1.9704e-04, -2.0654e-04,  ...,  9.1253e-05,
          3.2195e-04,  5.5670e-04],
        [-5.2303e-06, -4.1574e-06,  3.9712e-06,  ..., -4.7758e-06,
         -3.9376e-06, -3.6024e-06],
        [-1.4618e-05, -1.2189e-05,  1.2010e-05,  ..., -1.3217e-05,
         -1.0952e-05, -9.9838e-06],
        [-1.0133e-05, -7.9125e-06,  8.0839e-06,  ..., -9.2089e-06,
         -7.6294e-06, -7.3835e-06],
        [-1.2398e-05, -1.0744e-05,  9.8944e-06,  ..., -1.1474e-05,
         -9.9242e-06, -7.7710e-06]], device='cuda:0')
Loss: 0.9160252809524536


Running epoch 1, step 1588, batch 540
Sampled inputs[:2]: tensor([[    0,  5689,   271,  ...,   352,  9985,  3260],
        [    0, 24781,   287,  ...,   266,  3873,  1400]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6194e-04, -1.7011e-04, -2.4867e-04,  ...,  1.0823e-04,
          3.8718e-04,  4.9803e-04],
        [-6.5193e-06, -5.1707e-06,  5.0440e-06,  ..., -5.9009e-06,
         -4.7609e-06, -4.4703e-06],
        [-1.8373e-05, -1.5169e-05,  1.5333e-05,  ..., -1.6421e-05,
         -1.3322e-05, -1.2442e-05],
        [-1.2696e-05, -9.8646e-06,  1.0319e-05,  ..., -1.1414e-05,
         -9.2462e-06, -9.1940e-06],
        [-1.5408e-05, -1.3232e-05,  1.2502e-05,  ..., -1.4096e-05,
         -1.1966e-05, -9.5665e-06]], device='cuda:0')
Loss: 0.9349308013916016


Running epoch 1, step 1589, batch 541
Sampled inputs[:2]: tensor([[    0,   677,  9606,  ...,  9468,  9268,   328],
        [    0,  7712, 31756,  ...,   895,   360,   630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5953e-04, -2.2793e-04, -3.3422e-04,  ..., -1.7735e-05,
          5.4666e-04,  4.9829e-04],
        [-7.9647e-06, -6.0834e-06,  6.1169e-06,  ..., -7.0781e-06,
         -5.6103e-06, -5.3868e-06],
        [-2.2247e-05, -1.7852e-05,  1.8433e-05,  ..., -1.9610e-05,
         -1.5676e-05, -1.4901e-05],
        [-1.5408e-05, -1.1586e-05,  1.2450e-05,  ..., -1.3635e-05,
         -1.0863e-05, -1.1005e-05],
        [-1.8731e-05, -1.5661e-05,  1.5095e-05,  ..., -1.6928e-05,
         -1.4186e-05, -1.1519e-05]], device='cuda:0')
Loss: 0.9508584141731262


Running epoch 1, step 1590, batch 542
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,    17,   271,   266],
        [    0,   346,   462,  ..., 37683,    14,  1500]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5261e-04, -1.6999e-05, -2.7874e-04,  ...,  2.8174e-04,
          1.8554e-04,  7.8730e-04],
        [-9.3356e-06, -7.0296e-06,  7.1600e-06,  ..., -8.2627e-06,
         -6.5044e-06, -6.3293e-06],
        [-2.6152e-05, -2.0683e-05,  2.1636e-05,  ..., -2.2948e-05,
         -1.8224e-05, -1.7568e-05],
        [-1.8075e-05, -1.3381e-05,  1.4596e-05,  ..., -1.5914e-05,
         -1.2591e-05, -1.2927e-05],
        [-2.1979e-05, -1.8120e-05,  1.7688e-05,  ..., -1.9774e-05,
         -1.6466e-05, -1.3575e-05]], device='cuda:0')
Loss: 0.9203119277954102


Running epoch 1, step 1591, batch 543
Sampled inputs[:2]: tensor([[   0,   12,  689,  ..., 1110, 1712, 2228],
        [   0,  266,  824,  ..., 1799,  287, 6250]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5016e-04, -6.0448e-05, -3.2244e-04,  ...,  2.3244e-04,
          1.9761e-04,  8.5138e-04],
        [-1.0610e-05, -8.0504e-06,  8.2478e-06,  ..., -9.3803e-06,
         -7.3500e-06, -7.1488e-06],
        [-2.9713e-05, -2.3648e-05,  2.4840e-05,  ..., -2.6077e-05,
         -2.0608e-05, -1.9908e-05],
        [-2.0564e-05, -1.5333e-05,  1.6816e-05,  ..., -1.8075e-05,
         -1.4216e-05, -1.4618e-05],
        [-2.4870e-05, -2.0668e-05,  2.0236e-05,  ..., -2.2382e-05,
         -1.8582e-05, -1.5311e-05]], device='cuda:0')
Loss: 0.9896263480186462
Graident accumulation at epoch 1, step 1591, batch 543
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5860e-04,  1.5251e-04,  9.8745e-06,  ...,  3.7366e-05,
          4.3307e-05,  1.7851e-04],
        [-5.3560e-06, -1.6479e-06,  3.4880e-06,  ..., -5.3114e-06,
         -1.2409e-06, -3.2013e-06],
        [ 1.1621e-05,  1.7223e-05, -1.4643e-05,  ...,  1.4148e-05,
          1.9159e-05,  2.3503e-06],
        [-1.0311e-05, -4.4562e-07,  8.8847e-06,  ..., -8.2068e-06,
         -3.4553e-06, -9.3182e-06],
        [-2.5857e-05, -2.0181e-05,  2.0023e-05,  ..., -2.3220e-05,
         -1.9795e-05, -1.6759e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8729e-08, 5.5276e-08, 5.4189e-08,  ..., 2.0523e-08, 1.2107e-07,
         4.8180e-08],
        [9.0005e-11, 5.9432e-11, 2.6310e-11,  ..., 6.1830e-11, 3.2214e-11,
         2.9769e-11],
        [2.0580e-09, 1.4277e-09, 6.8258e-10,  ..., 1.5772e-09, 8.9871e-10,
         5.3083e-10],
        [8.6934e-10, 1.0310e-09, 3.7347e-10,  ..., 8.5364e-10, 6.1482e-10,
         3.5943e-10],
        [4.1944e-10, 2.4386e-10, 8.9765e-11,  ..., 3.0679e-10, 8.9243e-11,
         1.2513e-10]], device='cuda:0')
optimizer state dict: 199.0
lr: [2.882322737981248e-06, 2.882322737981248e-06]
scheduler_last_epoch: 199


Running epoch 1, step 1592, batch 544
Sampled inputs[:2]: tensor([[    0,  2950,    13,  ..., 16513,   300,  2205],
        [    0,  5129,  1245,  ...,   292, 24298,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2927e-04, -6.3053e-05,  2.6853e-05,  ..., -1.5524e-04,
         -7.8199e-05, -1.1297e-04],
        [-1.3411e-06, -1.0729e-06,  1.0133e-06,  ..., -1.2368e-06,
         -9.6112e-07, -8.9034e-07],
        [-3.7998e-06, -3.1590e-06,  3.0696e-06,  ..., -3.4720e-06,
         -2.7120e-06, -2.5183e-06],
        [-2.5630e-06, -2.0117e-06,  2.0266e-06,  ..., -2.3544e-06,
         -1.8254e-06, -1.7881e-06],
        [-3.2634e-06, -2.8163e-06,  2.5630e-06,  ..., -3.0398e-06,
         -2.4736e-06, -2.0117e-06]], device='cuda:0')
Loss: 0.9734547138214111


Running epoch 1, step 1593, batch 545
Sampled inputs[:2]: tensor([[    0,  4263,  4865,  ...,  1878,   278,  4450],
        [    0,  1894,   317,  ...,  9920,    13, 19888]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3002e-05, -1.7542e-04,  2.8922e-05,  ..., -1.3614e-04,
          4.5121e-05, -7.2480e-05],
        [-2.7344e-06, -2.1979e-06,  1.8850e-06,  ..., -2.5183e-06,
         -2.0191e-06, -1.8664e-06],
        [-7.5996e-06, -6.4224e-06,  5.6922e-06,  ..., -6.9886e-06,
         -5.6773e-06, -5.1707e-06],
        [-5.1409e-06, -4.1127e-06,  3.7178e-06,  ..., -4.7684e-06,
         -3.8520e-06, -3.7104e-06],
        [-6.7204e-06, -5.8711e-06,  4.9174e-06,  ..., -6.2734e-06,
         -5.2899e-06, -4.2468e-06]], device='cuda:0')
Loss: 0.9798992276191711


Running epoch 1, step 1594, batch 546
Sampled inputs[:2]: tensor([[   0, 6418,  446,  ...,  413,   29,  413],
        [   0,  328,  266,  ...,   14, 3352,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4401e-05,  6.7186e-05,  4.0272e-04,  ..., -2.0839e-04,
          1.5479e-05, -2.1453e-04],
        [-4.0382e-06, -3.2485e-06,  2.6748e-06,  ..., -3.7253e-06,
         -3.1069e-06, -2.7344e-06],
        [-1.1235e-05, -9.5665e-06,  8.1062e-06,  ..., -1.0371e-05,
         -8.7619e-06, -7.5847e-06],
        [-7.7635e-06, -6.2138e-06,  5.2974e-06,  ..., -7.2569e-06,
         -6.1616e-06, -5.5656e-06],
        [-9.8348e-06, -8.6129e-06,  6.8992e-06,  ..., -9.2089e-06,
         -8.0615e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9504598379135132


Running epoch 1, step 1595, batch 547
Sampled inputs[:2]: tensor([[    0,  1855,    14,  ...,    12,   287, 16479],
        [    0,    12,   298,  ...,   292,    36,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0500e-04,  1.3136e-05,  2.3961e-04,  ..., -1.7225e-04,
          1.5844e-04, -2.5670e-04],
        [-5.3942e-06, -4.2468e-06,  3.6508e-06,  ..., -4.9323e-06,
         -4.0233e-06, -3.6955e-06],
        [-1.4961e-05, -1.2442e-05,  1.1012e-05,  ..., -1.3620e-05,
         -1.1265e-05, -1.0148e-05],
        [-1.0327e-05, -8.0541e-06,  7.2196e-06,  ..., -9.5367e-06,
         -7.9125e-06, -7.4655e-06],
        [-1.3098e-05, -1.1221e-05,  9.3728e-06,  ..., -1.2100e-05,
         -1.0401e-05, -8.2254e-06]], device='cuda:0')
Loss: 0.93138188123703


Running epoch 1, step 1596, batch 548
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,    13,   635,    13],
        [    0,    14,  6707,  ..., 17771,   300,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1390e-05, -5.0669e-05,  8.7156e-05,  ..., -1.3080e-04,
          3.3726e-04, -2.3968e-04],
        [-6.7726e-06, -5.1744e-06,  4.5523e-06,  ..., -6.2510e-06,
         -5.0366e-06, -4.7311e-06],
        [-1.8731e-05, -1.5065e-05,  1.3724e-05,  ..., -1.7107e-05,
         -1.3947e-05, -1.2785e-05],
        [-1.2860e-05, -9.7081e-06,  8.9481e-06,  ..., -1.1966e-05,
         -9.7826e-06, -9.4473e-06],
        [-1.6510e-05, -1.3664e-05,  1.1802e-05,  ..., -1.5289e-05,
         -1.2934e-05, -1.0416e-05]], device='cuda:0')
Loss: 0.9573094248771667


Running epoch 1, step 1597, batch 549
Sampled inputs[:2]: tensor([[    0,   221,   474,  ..., 19245,   565,    14],
        [    0,   927,   259,  ...,   328,  9430,  2330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8149e-04, -1.3596e-04,  2.5654e-05,  ..., -1.0182e-04,
          3.3726e-04, -5.7513e-05],
        [-8.0094e-06, -6.0983e-06,  5.5209e-06,  ..., -7.4282e-06,
         -5.9046e-06, -5.5879e-06],
        [-2.2262e-05, -1.7747e-05,  1.6734e-05,  ..., -2.0355e-05,
         -1.6361e-05, -1.5125e-05],
        [-1.5363e-05, -1.1519e-05,  1.1034e-05,  ..., -1.4335e-05,
         -1.1533e-05, -1.1273e-05],
        [-1.9416e-05, -1.5974e-05,  1.4216e-05,  ..., -1.8001e-05,
         -1.5050e-05, -1.2152e-05]], device='cuda:0')
Loss: 0.9650409817695618


Running epoch 1, step 1598, batch 550
Sampled inputs[:2]: tensor([[   0, 3261, 1518,  ..., 5019,  287, 1906],
        [   0, 1716,  271,  ...,  292,   78, 1365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.5023e-05, -1.7373e-04, -6.5095e-06,  ..., -1.8357e-04,
          3.2462e-04, -9.5296e-05],
        [-9.3952e-06, -7.3127e-06,  6.5416e-06,  ..., -8.7023e-06,
         -7.0147e-06, -6.5006e-06],
        [-2.6286e-05, -2.1458e-05,  1.9908e-05,  ..., -2.4080e-05,
         -1.9610e-05, -1.7792e-05],
        [-1.8016e-05, -1.3858e-05,  1.3076e-05,  ..., -1.6779e-05,
         -1.3679e-05, -1.3128e-05],
        [-2.2709e-05, -1.9118e-05,  1.6764e-05,  ..., -2.1115e-05,
         -1.7852e-05, -1.4178e-05]], device='cuda:0')
Loss: 1.0083558559417725


Running epoch 1, step 1599, batch 551
Sampled inputs[:2]: tensor([[   0,  292,   46,  ..., 1217,   17,  292],
        [   0,  368,  275,  ..., 6389, 9102,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3839e-05, -2.6670e-04,  3.3858e-05,  ..., -2.2481e-04,
          4.2486e-04,  2.0182e-04],
        [-1.0647e-05, -8.4229e-06,  7.3276e-06,  ..., -9.9540e-06,
         -8.1025e-06, -7.4245e-06],
        [-2.9698e-05, -2.4572e-05,  2.2292e-05,  ..., -2.7448e-05,
         -2.2545e-05, -2.0280e-05],
        [-2.0429e-05, -1.5974e-05,  1.4663e-05,  ..., -1.9208e-05,
         -1.5810e-05, -1.5035e-05],
        [-2.5779e-05, -2.1994e-05,  1.8865e-05,  ..., -2.4170e-05,
         -2.0608e-05, -1.6235e-05]], device='cuda:0')
Loss: 0.9687213897705078
Graident accumulation at epoch 1, step 1599, batch 551
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0155,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.4613e-04,  1.1059e-04,  1.2273e-05,  ...,  1.1149e-05,
          8.1463e-05,  1.8084e-04],
        [-5.8851e-06, -2.3254e-06,  3.8720e-06,  ..., -5.7756e-06,
         -1.9271e-06, -3.6236e-06],
        [ 7.4895e-06,  1.3044e-05, -1.0950e-05,  ...,  9.9881e-06,
          1.4989e-05,  8.7186e-08],
        [-1.1323e-05, -1.9985e-06,  9.4625e-06,  ..., -9.3069e-06,
         -4.6908e-06, -9.8899e-06],
        [-2.5850e-05, -2.0362e-05,  1.9907e-05,  ..., -2.3315e-05,
         -1.9876e-05, -1.6707e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8671e-08, 5.5292e-08, 5.4136e-08,  ..., 2.0553e-08, 1.2113e-07,
         4.8173e-08],
        [9.0028e-11, 5.9444e-11, 2.6338e-11,  ..., 6.1868e-11, 3.2247e-11,
         2.9794e-11],
        [2.0568e-09, 1.4269e-09, 6.8240e-10,  ..., 1.5764e-09, 8.9832e-10,
         5.3071e-10],
        [8.6889e-10, 1.0302e-09, 3.7331e-10,  ..., 8.5315e-10, 6.1446e-10,
         3.5930e-10],
        [4.1968e-10, 2.4410e-10, 9.0031e-11,  ..., 3.0707e-10, 8.9579e-11,
         1.2527e-10]], device='cuda:0')
optimizer state dict: 200.0
lr: [2.796038617665642e-06, 2.796038617665642e-06]
scheduler_last_epoch: 200


Running epoch 1, step 1600, batch 552
Sampled inputs[:2]: tensor([[   0,  600,  287,  ..., 1933,  221,  494],
        [   0,  504,  409,  ..., 5863, 2621,  824]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.7253e-05,  2.1322e-04,  2.4316e-04,  ..., -2.8416e-04,
          1.8274e-04, -6.1904e-05],
        [-1.3635e-06, -1.0952e-06,  8.9034e-07,  ..., -1.3039e-06,
         -1.1250e-06, -9.6858e-07],
        [-3.7551e-06, -3.1441e-06,  2.6822e-06,  ..., -3.5465e-06,
         -3.0547e-06, -2.6673e-06],
        [-2.6077e-06, -2.0564e-06,  1.7360e-06,  ..., -2.5183e-06,
         -2.1756e-06, -1.9670e-06],
        [-3.2932e-06, -2.8610e-06,  2.2948e-06,  ..., -3.1590e-06,
         -2.8163e-06, -2.1607e-06]], device='cuda:0')
Loss: 0.9609989523887634


Running epoch 1, step 1601, batch 553
Sampled inputs[:2]: tensor([[    0,  7879,  5435,  ...,  1586, 12115,   271],
        [    0, 14409, 45007,  ...,  1197,   266,   944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7180e-05,  1.6184e-04,  2.4586e-04,  ..., -3.3158e-04,
          3.3771e-04, -1.6309e-04],
        [-2.8014e-06, -2.1085e-06,  1.7956e-06,  ..., -2.5928e-06,
         -2.1979e-06, -2.0117e-06],
        [-7.8082e-06, -6.1244e-06,  5.4687e-06,  ..., -7.1228e-06,
         -6.0350e-06, -5.6028e-06],
        [-5.3048e-06, -3.9041e-06,  3.5018e-06,  ..., -4.9174e-06,
         -4.1723e-06, -4.0084e-06],
        [-6.7651e-06, -5.5283e-06,  4.5896e-06,  ..., -6.3032e-06,
         -5.5581e-06, -4.4852e-06]], device='cuda:0')
Loss: 0.9581311941146851


Running epoch 1, step 1602, batch 554
Sampled inputs[:2]: tensor([[    0,   843,    14,  ...,   659,   271, 10511],
        [    0,    13,  2549,  ...,   221,   382,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4617e-04,  2.0260e-04,  5.3977e-04,  ..., -3.9043e-04,
          3.4597e-04, -2.0666e-04],
        [-4.1425e-06, -3.1367e-06,  2.7493e-06,  ..., -3.8743e-06,
         -3.2261e-06, -2.9467e-06],
        [-1.1548e-05, -9.1195e-06,  8.3447e-06,  ..., -1.0639e-05,
         -8.8662e-06, -8.1807e-06],
        [-7.8529e-06, -5.8115e-06,  5.3421e-06,  ..., -7.3761e-06,
         -6.1542e-06, -5.8785e-06],
        [-9.9391e-06, -8.1658e-06,  6.9886e-06,  ..., -9.3579e-06,
         -8.1211e-06, -6.5267e-06]], device='cuda:0')
Loss: 0.9623624682426453


Running epoch 1, step 1603, batch 555
Sampled inputs[:2]: tensor([[    0,   365,  2849,  ...,     9,  3365,  5027],
        [    0,  6904,  6069,  ..., 17196,   471,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0381e-04,  2.2860e-04,  5.4959e-04,  ..., -3.5945e-04,
          5.0181e-04, -2.5019e-04],
        [-5.5507e-06, -4.1872e-06,  3.6545e-06,  ..., -5.1707e-06,
         -4.3139e-06, -3.9153e-06],
        [-1.5363e-05, -1.2130e-05,  1.1057e-05,  ..., -1.4126e-05,
         -1.1846e-05, -1.0788e-05],
        [-1.0461e-05, -7.7635e-06,  7.1153e-06,  ..., -9.8050e-06,
         -8.2403e-06, -7.7859e-06],
        [-1.3292e-05, -1.0923e-05,  9.3281e-06,  ..., -1.2472e-05,
         -1.0878e-05, -8.6576e-06]], device='cuda:0')
Loss: 0.9938398003578186


Running epoch 1, step 1604, batch 556
Sampled inputs[:2]: tensor([[   0,  300, 1064,  ..., 6953,  944,  278],
        [   0,  266, 1441,  ..., 1817, 1589,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6614e-04,  2.8397e-04,  5.4841e-04,  ..., -2.7138e-04,
          3.4451e-04, -3.0866e-04],
        [-6.9216e-06, -5.1633e-06,  4.7423e-06,  ..., -6.3106e-06,
         -5.1223e-06, -4.7684e-06],
        [-1.9148e-05, -1.4961e-05,  1.4305e-05,  ..., -1.7270e-05,
         -1.4096e-05, -1.3143e-05],
        [-1.3098e-05, -9.6187e-06,  9.3356e-06,  ..., -1.1995e-05,
         -9.7901e-06, -9.5069e-06],
        [-1.6242e-05, -1.3247e-05,  1.1787e-05,  ..., -1.4991e-05,
         -1.2785e-05, -1.0327e-05]], device='cuda:0')
Loss: 0.933719277381897


Running epoch 1, step 1605, batch 557
Sampled inputs[:2]: tensor([[   0,  380, 1075,  ...,  298,  365, 4920],
        [   0,  591, 1545,  ...,   71,  462,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5182e-04,  3.1887e-04,  5.0214e-04,  ..., -3.3618e-04,
          1.4625e-04, -4.1421e-04],
        [-8.2925e-06, -6.1914e-06,  5.7481e-06,  ..., -7.5325e-06,
         -6.0126e-06, -5.6475e-06],
        [-2.2858e-05, -1.7807e-05,  1.7315e-05,  ..., -2.0504e-05,
         -1.6466e-05, -1.5438e-05],
        [-1.5676e-05, -1.1511e-05,  1.1347e-05,  ..., -1.4275e-05,
         -1.1452e-05, -1.1235e-05],
        [-1.9312e-05, -1.5706e-05,  1.4216e-05,  ..., -1.7717e-05,
         -1.4901e-05, -1.2048e-05]], device='cuda:0')
Loss: 0.916568398475647


Running epoch 1, step 1606, batch 558
Sampled inputs[:2]: tensor([[    0,    12,  1041,  ..., 22086,  3073,   554],
        [    0,   266,  4505,  ...,    12,   461,   806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1761e-04,  2.7325e-04,  3.2519e-04,  ..., -2.6325e-04,
          1.2185e-04, -2.7333e-04],
        [-9.6485e-06, -7.2420e-06,  6.6347e-06,  ..., -8.8140e-06,
         -7.0408e-06, -6.6161e-06],
        [-2.6703e-05, -2.0966e-05,  2.0102e-05,  ..., -2.4125e-05,
         -1.9446e-05, -1.8135e-05],
        [-1.8358e-05, -1.3582e-05,  1.3180e-05,  ..., -1.6838e-05,
         -1.3553e-05, -1.3247e-05],
        [-2.2605e-05, -1.8477e-05,  1.6555e-05,  ..., -2.0862e-05,
         -1.7583e-05, -1.4178e-05]], device='cuda:0')
Loss: 0.9571768641471863


Running epoch 1, step 1607, batch 559
Sampled inputs[:2]: tensor([[    0, 48705,   292,  ...,   266,  2548,  2697],
        [    0,     9,  1471,  ...,   741,   266,  5821]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4622e-04,  3.1624e-04,  2.2104e-04,  ..., -3.3820e-04,
          2.2454e-04, -1.5112e-04],
        [-1.1131e-05, -8.2701e-06,  7.5698e-06,  ..., -1.0170e-05,
         -8.0839e-06, -7.6666e-06],
        [-3.0845e-05, -2.4021e-05,  2.2948e-05,  ..., -2.7895e-05,
         -2.2426e-05, -2.1055e-05],
        [-2.1070e-05, -1.5460e-05,  1.4953e-05,  ..., -1.9342e-05,
         -1.5505e-05, -1.5244e-05],
        [-2.6152e-05, -2.1189e-05,  1.8939e-05,  ..., -2.4155e-05,
         -2.0266e-05, -1.6503e-05]], device='cuda:0')
Loss: 0.9407784938812256
Graident accumulation at epoch 1, step 1607, batch 559
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5614e-04,  1.3115e-04,  3.3149e-05,  ..., -2.3786e-05,
          9.5770e-05,  1.4764e-04],
        [-6.4097e-06, -2.9199e-06,  4.2418e-06,  ..., -6.2151e-06,
         -2.5427e-06, -4.0279e-06],
        [ 3.6560e-06,  9.3374e-06, -7.5600e-06,  ...,  6.1998e-06,
          1.1247e-05, -2.0271e-06],
        [-1.2298e-05, -3.3446e-06,  1.0012e-05,  ..., -1.0310e-05,
         -5.7722e-06, -1.0425e-05],
        [-2.5880e-05, -2.0445e-05,  1.9810e-05,  ..., -2.3399e-05,
         -1.9915e-05, -1.6686e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8673e-08, 5.5336e-08, 5.4131e-08,  ..., 2.0647e-08, 1.2106e-07,
         4.8147e-08],
        [9.0062e-11, 5.9453e-11, 2.6369e-11,  ..., 6.1909e-11, 3.2280e-11,
         2.9823e-11],
        [2.0557e-09, 1.4260e-09, 6.8224e-10,  ..., 1.5756e-09, 8.9792e-10,
         5.3063e-10],
        [8.6846e-10, 1.0294e-09, 3.7316e-10,  ..., 8.5267e-10, 6.1408e-10,
         3.5917e-10],
        [4.1995e-10, 2.4431e-10, 9.0300e-11,  ..., 3.0735e-10, 8.9900e-11,
         1.2541e-10]], device='cuda:0')
optimizer state dict: 201.0
lr: [2.7108553255335225e-06, 2.7108553255335225e-06]
scheduler_last_epoch: 201


Running epoch 1, step 1608, batch 560
Sampled inputs[:2]: tensor([[    0,   726,  8241,  ...,   266,  5994,     9],
        [    0,   266,  2623,  ...,     5, 10781,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4499e-04,  8.7195e-05, -2.7288e-05,  ..., -1.5756e-04,
         -9.1123e-06, -6.7967e-05],
        [-1.4454e-06, -9.9093e-07,  9.8348e-07,  ..., -1.2666e-06,
         -1.0133e-06, -9.4250e-07],
        [-3.9637e-06, -2.8759e-06,  2.8908e-06,  ..., -3.4273e-06,
         -2.7865e-06, -2.5183e-06],
        [-2.7418e-06, -1.8626e-06,  1.9372e-06,  ..., -2.4140e-06,
         -1.9521e-06, -1.8477e-06],
        [-3.3230e-06, -2.5183e-06,  2.3842e-06,  ..., -2.9206e-06,
         -2.4736e-06, -1.9521e-06]], device='cuda:0')
Loss: 0.9380955696105957


Running epoch 1, step 1609, batch 561
Sampled inputs[:2]: tensor([[   0,   15,   19,  ...,  266, 6391, 1777],
        [   0, 1480,  518,  ...,  445,   28,  445]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7253e-04,  1.8255e-04, -1.7636e-05,  ..., -2.0131e-04,
         -6.5081e-05,  8.7090e-05],
        [-2.7940e-06, -2.0862e-06,  1.9968e-06,  ..., -2.4587e-06,
         -1.9968e-06, -1.8254e-06],
        [ 8.9524e-05,  9.4703e-05, -2.8200e-05,  ...,  3.9956e-05,
          3.1013e-05,  4.1497e-05],
        [-5.3197e-06, -3.9637e-06,  3.9637e-06,  ..., -4.6939e-06,
         -3.8594e-06, -3.6284e-06],
        [-6.3032e-06, -5.1856e-06,  4.7684e-06,  ..., -5.6177e-06,
         -4.8578e-06, -3.7625e-06]], device='cuda:0')
Loss: 0.954191267490387


Running epoch 1, step 1610, batch 562
Sampled inputs[:2]: tensor([[   0,  560,  199,  ...,  266, 1371, 4811],
        [   0,   14,  417,  ..., 8821, 6845,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6281e-05,  1.1486e-04, -2.5154e-04,  ..., -2.4462e-04,
          1.2065e-05,  1.0269e-04],
        [-4.2990e-06, -3.2037e-06,  2.9393e-06,  ..., -3.8520e-06,
         -3.0845e-06, -2.9057e-06],
        [ 8.5441e-05,  9.1499e-05, -2.5383e-05,  ...,  3.6200e-05,
          2.8032e-05,  3.8576e-05],
        [-8.0019e-06, -5.9605e-06,  5.7071e-06,  ..., -7.1973e-06,
         -5.8562e-06, -5.6550e-06],
        [-1.0028e-05, -8.1807e-06,  7.2718e-06,  ..., -9.0897e-06,
         -7.7188e-06, -6.2361e-06]], device='cuda:0')
Loss: 0.957149863243103


Running epoch 1, step 1611, batch 563
Sampled inputs[:2]: tensor([[    0,   413,    28,  ...,   328, 37605,  6499],
        [    0,   422,    14,  ...,   271,  1360,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2420e-04, -1.0438e-04, -8.0153e-05,  ..., -4.9377e-04,
          7.4237e-04,  4.3108e-04],
        [-5.7966e-06, -4.3139e-06,  3.7774e-06,  ..., -5.3123e-06,
         -4.3362e-06, -3.9935e-06],
        [ 8.1418e-05,  8.8355e-05, -2.2880e-05,  ...,  3.2326e-05,
          2.4680e-05,  3.5715e-05],
        [-1.0669e-05, -7.9125e-06,  7.2569e-06,  ..., -9.8199e-06,
         -8.1211e-06, -7.6666e-06],
        [-1.3828e-05, -1.1191e-05,  9.5963e-06,  ..., -1.2800e-05,
         -1.0997e-05, -8.7544e-06]], device='cuda:0')
Loss: 0.980453610420227


Running epoch 1, step 1612, batch 564
Sampled inputs[:2]: tensor([[   0, 6762,  689,  ..., 7061,   14,  381],
        [   0, 5522, 5662,  ...,  638, 1231, 1098]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5431e-04, -2.4114e-04, -1.2078e-04,  ..., -6.0053e-04,
          8.8628e-04,  4.8222e-04],
        [-7.1749e-06, -5.2750e-06,  4.8131e-06,  ..., -6.5342e-06,
         -5.2899e-06, -4.9770e-06],
        [ 7.7633e-05,  8.5598e-05, -1.9855e-05,  ...,  2.9018e-05,
          2.2057e-05,  3.3078e-05],
        [-1.3322e-05, -9.7230e-06,  9.3281e-06,  ..., -1.2159e-05,
         -9.9689e-06, -9.6336e-06],
        [-1.7077e-05, -1.3649e-05,  1.2130e-05,  ..., -1.5706e-05,
         -1.3396e-05, -1.0870e-05]], device='cuda:0')
Loss: 0.9481762051582336


Running epoch 1, step 1613, batch 565
Sampled inputs[:2]: tensor([[    0,   266,  1658,  ...,   278,  1083,  5993],
        [    0,   328,   471,  ..., 11137,   679,  6585]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7104e-04, -1.9141e-04, -8.4062e-05,  ..., -5.8192e-04,
          8.8325e-04,  3.7603e-04],
        [-8.5533e-06, -6.3702e-06,  5.6326e-06,  ..., -7.8753e-06,
         -6.4075e-06, -6.0126e-06],
        [ 7.3863e-05,  8.2543e-05, -1.7382e-05,  ...,  2.5427e-05,
          1.9077e-05,  3.0306e-05],
        [-1.5944e-05, -1.1764e-05,  1.0952e-05,  ..., -1.4707e-05,
         -1.2085e-05, -1.1690e-05],
        [-2.0504e-05, -1.6510e-05,  1.4335e-05,  ..., -1.9029e-05,
         -1.6242e-05, -1.3195e-05]], device='cuda:0')
Loss: 0.9685319662094116


Running epoch 1, step 1614, batch 566
Sampled inputs[:2]: tensor([[    0,   927, 13407,  ...,   616,  3955,  2567],
        [    0,   271, 16084,  ...,   688,  1122,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7101e-04, -3.6008e-04,  3.6967e-05,  ..., -6.2124e-04,
          1.0138e-03,  3.4713e-04],
        [-9.9316e-06, -7.5251e-06,  6.7353e-06,  ..., -9.0823e-06,
         -7.3686e-06, -6.8545e-06],
        [ 7.0108e-05,  7.9250e-05, -1.4178e-05,  ...,  2.2119e-05,
          1.6410e-05,  2.7967e-05],
        [-1.8597e-05, -1.4000e-05,  1.3173e-05,  ..., -1.7032e-05,
         -1.3955e-05, -1.3411e-05],
        [-2.3648e-05, -1.9401e-05,  1.6943e-05,  ..., -2.1875e-05,
         -1.8641e-05, -1.5013e-05]], device='cuda:0')
Loss: 0.9875564575195312


Running epoch 1, step 1615, batch 567
Sampled inputs[:2]: tensor([[    0,    13,  2615,  ..., 31594, 15867,  3484],
        [    0,    12,   287,  ...,   365,  1943,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0461e-04, -3.1146e-04,  4.0123e-04,  ..., -6.2468e-04,
          1.1353e-03,  4.5733e-04],
        [-1.1288e-05, -8.6948e-06,  7.6145e-06,  ..., -1.0371e-05,
         -8.5086e-06, -7.8157e-06],
        [ 6.6338e-05,  7.5778e-05, -1.1481e-05,  ...,  1.8513e-05,
          1.3161e-05,  2.5255e-05],
        [-2.1175e-05, -1.6250e-05,  1.4924e-05,  ..., -1.9535e-05,
         -1.6205e-05, -1.5363e-05],
        [-2.6941e-05, -2.2531e-05,  1.9222e-05,  ..., -2.5079e-05,
         -2.1622e-05, -1.7203e-05]], device='cuda:0')
Loss: 0.9566875696182251
Graident accumulation at epoch 1, step 1615, batch 567
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.5098e-04,  8.6893e-05,  6.9958e-05,  ..., -8.3875e-05,
          1.9973e-04,  1.7861e-04],
        [-6.8975e-06, -3.4974e-06,  4.5790e-06,  ..., -6.6307e-06,
         -3.1393e-06, -4.4067e-06],
        [ 9.9242e-06,  1.5982e-05, -7.9521e-06,  ...,  7.4311e-06,
          1.1438e-05,  7.0110e-07],
        [-1.3185e-05, -4.6351e-06,  1.0503e-05,  ..., -1.1233e-05,
         -6.8154e-06, -1.0919e-05],
        [-2.5986e-05, -2.0653e-05,  1.9751e-05,  ..., -2.3567e-05,
         -2.0086e-05, -1.6738e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8625e-08, 5.5378e-08, 5.4237e-08,  ..., 2.1016e-08, 1.2223e-07,
         4.8308e-08],
        [9.0099e-11, 5.9469e-11, 2.6400e-11,  ..., 6.1955e-11, 3.2321e-11,
         2.9854e-11],
        [2.0580e-09, 1.4303e-09, 6.8169e-10,  ..., 1.5744e-09, 8.9720e-10,
         5.3073e-10],
        [8.6804e-10, 1.0287e-09, 3.7301e-10,  ..., 8.5220e-10, 6.1373e-10,
         3.5905e-10],
        [4.2025e-10, 2.4457e-10, 9.0579e-11,  ..., 3.0767e-10, 9.0278e-11,
         1.2558e-10]], device='cuda:0')
optimizer state dict: 202.0
lr: [2.626785878335505e-06, 2.626785878335505e-06]
scheduler_last_epoch: 202


Running epoch 1, step 1616, batch 568
Sampled inputs[:2]: tensor([[   0,  259, 2561,  ...,   77, 4830,  292],
        [   0, 1503,  369,  ..., 1336,  271, 8429]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0738e-04, -2.0892e-04, -2.8653e-04,  ..., -5.6302e-05,
         -3.3979e-05, -1.0232e-04],
        [-1.3262e-06, -1.0803e-06,  1.0356e-06,  ..., -1.1995e-06,
         -9.0525e-07, -8.8662e-07],
        [-3.7104e-06, -3.0398e-06,  3.0845e-06,  ..., -3.2932e-06,
         -2.4587e-06, -2.4587e-06],
        [-2.5928e-06, -2.0564e-06,  2.1011e-06,  ..., -2.3097e-06,
         -1.7360e-06, -1.8030e-06],
        [-3.0100e-06, -2.5779e-06,  2.4289e-06,  ..., -2.7418e-06,
         -2.1607e-06, -1.8254e-06]], device='cuda:0')
Loss: 0.9647838473320007


Running epoch 1, step 1617, batch 569
Sampled inputs[:2]: tensor([[   0, 4350,   14,  ...,  266, 9479,  944],
        [   0,  527,  496,  ...,   12,  795, 8296]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4158e-04, -1.8228e-04, -2.8752e-04,  ..., -5.0255e-05,
          4.0640e-05,  9.0329e-05],
        [-2.7418e-06, -2.2128e-06,  2.0340e-06,  ..., -2.5034e-06,
         -1.9483e-06, -1.8366e-06],
        [-7.8529e-06, -6.5118e-06,  6.1691e-06,  ..., -7.1079e-06,
         -5.5432e-06, -5.2601e-06],
        [-5.3048e-06, -4.2319e-06,  4.0978e-06,  ..., -4.8131e-06,
         -3.7327e-06, -3.7104e-06],
        [-6.5118e-06, -5.6177e-06,  4.9770e-06,  ..., -6.0499e-06,
         -4.9323e-06, -4.0457e-06]], device='cuda:0')
Loss: 0.9623037576675415


Running epoch 1, step 1618, batch 570
Sampled inputs[:2]: tensor([[    0,   680,   993,  ...,   699, 11426,    12],
        [    0,  4215,  1478,  ...,   644,   409,  3803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3195e-04, -4.1942e-04, -1.9117e-04,  ..., -6.0483e-05,
          7.9818e-05,  1.0498e-04],
        [-4.0829e-06, -3.3528e-06,  2.9691e-06,  ..., -3.7998e-06,
         -3.0063e-06, -2.8498e-06],
        [-1.1638e-05, -9.7901e-06,  9.0301e-06,  ..., -1.0699e-05,
         -8.4639e-06, -8.0913e-06],
        [-7.8976e-06, -6.3926e-06,  5.9977e-06,  ..., -7.3165e-06,
         -5.7742e-06, -5.7667e-06],
        [-9.7901e-06, -8.5086e-06,  7.4059e-06,  ..., -9.2089e-06,
         -7.5996e-06, -6.3106e-06]], device='cuda:0')
Loss: 0.9579747319221497


Running epoch 1, step 1619, batch 571
Sampled inputs[:2]: tensor([[    0,  1192, 11929,  ...,   266,  1551,  1860],
        [    0,   271,  8130,  ...,   609, 28676,   965]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1283e-04, -3.9212e-04, -1.3433e-04,  ..., -9.3970e-05,
          2.9987e-05,  1.8021e-04],
        [-5.4985e-06, -4.4927e-06,  3.9674e-06,  ..., -5.1111e-06,
         -4.0419e-06, -3.8408e-06],
        [-1.5691e-05, -1.3188e-05,  1.2085e-05,  ..., -1.4424e-05,
         -1.1444e-05, -1.0937e-05],
        [-1.0565e-05, -8.5384e-06,  7.9498e-06,  ..., -9.7901e-06,
         -7.7412e-06, -7.7188e-06],
        [-1.3188e-05, -1.1459e-05,  9.9093e-06,  ..., -1.2398e-05,
         -1.0252e-05, -8.5309e-06]], device='cuda:0')
Loss: 0.9621949195861816


Running epoch 1, step 1620, batch 572
Sampled inputs[:2]: tensor([[   0,  857,  344,  ..., 1529, 9106, 1447],
        [   0,  271,  266,  ..., 1034, 1928,   15]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9914e-04, -4.4509e-04, -1.6926e-04,  ..., -1.9373e-04,
         -6.1344e-07,  5.8576e-05],
        [-6.9141e-06, -5.5805e-06,  5.0776e-06,  ..., -6.3106e-06,
         -5.0031e-06, -4.7572e-06],
        [ 5.4670e-05,  6.5787e-05, -5.1422e-05,  ...,  7.8028e-05,
          4.9447e-05,  2.4662e-05],
        [-1.3247e-05, -1.0565e-05,  1.0140e-05,  ..., -1.2055e-05,
         -9.5591e-06, -9.5516e-06],
        [-1.6451e-05, -1.4231e-05,  1.2562e-05,  ..., -1.5244e-05,
         -1.2666e-05, -1.0513e-05]], device='cuda:0')
Loss: 0.9582930207252502


Running epoch 1, step 1621, batch 573
Sampled inputs[:2]: tensor([[    0, 11822,    12,  ...,   554,  3845,   271],
        [    0, 15402, 44149,  ...,   266,  1403,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3202e-04, -3.2494e-04, -1.6881e-04,  ..., -1.7209e-04,
          2.4283e-05,  1.1101e-04],
        [-8.3670e-06, -6.7502e-06,  6.1058e-06,  ..., -7.5772e-06,
         -6.0834e-06, -5.6997e-06],
        [ 5.0647e-05,  6.2390e-05, -4.8367e-05,  ...,  7.4496e-05,
          4.6422e-05,  2.2040e-05],
        [-1.6093e-05, -1.2830e-05,  1.2241e-05,  ..., -1.4529e-05,
         -1.1675e-05, -1.1489e-05],
        [-1.9804e-05, -1.7166e-05,  1.5050e-05,  ..., -1.8239e-05,
         -1.5333e-05, -1.2554e-05]], device='cuda:0')
Loss: 0.9687914848327637


Running epoch 1, step 1622, batch 574
Sampled inputs[:2]: tensor([[    0,   328, 27958,  ...,   417,   199,  2038],
        [    0,   266,   944,  ..., 14981,  1952,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6203e-04, -6.4083e-04, -4.1110e-04,  ..., -2.7199e-04,
          3.9159e-04,  3.3441e-04],
        [-9.8869e-06, -7.7710e-06,  6.9663e-06,  ..., -8.9705e-06,
         -7.2084e-06, -6.8024e-06],
        [ 4.6504e-05,  5.9410e-05, -4.5804e-05,  ...,  7.0726e-05,
          4.3367e-05,  1.9060e-05],
        [-1.8910e-05, -1.4700e-05,  1.3903e-05,  ..., -1.7107e-05,
         -1.3746e-05, -1.3605e-05],
        [-2.3603e-05, -1.9968e-05,  1.7315e-05,  ..., -2.1771e-05,
         -1.8299e-05, -1.5132e-05]], device='cuda:0')
Loss: 0.9638825058937073


Running epoch 1, step 1623, batch 575
Sampled inputs[:2]: tensor([[    0,  1477,    12,  ..., 31038,   408,   298],
        [    0,   689,  2149,  ...,  4263,    14,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6203e-04, -7.1017e-04, -4.8761e-04,  ..., -3.3125e-04,
          6.5504e-04,  3.7487e-04],
        [-1.1370e-05, -8.8885e-06,  7.8715e-06,  ..., -1.0274e-05,
         -8.2739e-06, -7.8157e-06],
        [ 4.2421e-05,  5.6161e-05, -4.3047e-05,  ...,  6.7150e-05,
          4.0387e-05,  1.6288e-05],
        [-2.1636e-05, -1.6771e-05,  1.5639e-05,  ..., -1.9535e-05,
         -1.5773e-05, -1.5557e-05],
        [-2.7150e-05, -2.2873e-05,  1.9640e-05,  ..., -2.4930e-05,
         -2.1026e-05, -1.7338e-05]], device='cuda:0')
Loss: 0.9427112936973572
Graident accumulation at epoch 1, step 1623, batch 575
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-9.9682e-05,  7.1863e-06,  1.4201e-05,  ..., -1.0861e-04,
          2.4526e-04,  1.9824e-04],
        [-7.3447e-06, -4.0365e-06,  4.9083e-06,  ..., -6.9950e-06,
         -3.6528e-06, -4.7476e-06],
        [ 1.3174e-05,  2.0000e-05, -1.1462e-05,  ...,  1.3403e-05,
          1.4333e-05,  2.2598e-06],
        [-1.4030e-05, -5.8487e-06,  1.1016e-05,  ..., -1.2063e-05,
         -7.7112e-06, -1.1383e-05],
        [-2.6102e-05, -2.0875e-05,  1.9740e-05,  ..., -2.3703e-05,
         -2.0180e-05, -1.6798e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8698e-08, 5.5827e-08, 5.4421e-08,  ..., 2.1105e-08, 1.2254e-07,
         4.8401e-08],
        [9.0139e-11, 5.9488e-11, 2.6436e-11,  ..., 6.1999e-11, 3.2357e-11,
         2.9886e-11],
        [2.0578e-09, 1.4321e-09, 6.8286e-10,  ..., 1.5773e-09, 8.9793e-10,
         5.3047e-10],
        [8.6764e-10, 1.0279e-09, 3.7288e-10,  ..., 8.5173e-10, 6.1337e-10,
         3.5893e-10],
        [4.2057e-10, 2.4485e-10, 9.0874e-11,  ..., 3.0798e-10, 9.0629e-11,
         1.2576e-10]], device='cuda:0')
optimizer state dict: 203.0
lr: [2.5438431226169712e-06, 2.5438431226169712e-06]
scheduler_last_epoch: 203


Running epoch 1, step 1624, batch 576
Sampled inputs[:2]: tensor([[    0,   300, 13523,  ..., 42438,   786,  1416],
        [    0,   342,   726,  ...,    12,   895,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7832e-05, -9.1188e-05, -4.5213e-05,  ...,  1.6214e-05,
         -1.4659e-04, -1.3436e-04],
        [-1.3337e-06, -1.0207e-06,  9.9838e-07,  ..., -1.1995e-06,
         -9.5367e-07, -8.7544e-07],
        [-3.6359e-06, -2.8461e-06,  2.9504e-06,  ..., -3.1888e-06,
         -2.5481e-06, -2.2948e-06],
        [-2.4885e-06, -1.8701e-06,  1.9670e-06,  ..., -2.2203e-06,
         -1.7807e-06, -1.7136e-06],
        [-3.0696e-06, -2.5481e-06,  2.4438e-06,  ..., -2.7567e-06,
         -2.3097e-06, -1.7732e-06]], device='cuda:0')
Loss: 0.9493126273155212


Running epoch 1, step 1625, batch 577
Sampled inputs[:2]: tensor([[   0,  259,  587,  ...,   14,   71,  462],
        [   0,  493,  221,  ...,  259,  726, 2786]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8977e-04, -3.6627e-06,  1.5058e-04,  ..., -1.4585e-04,
         -4.0300e-04, -2.6012e-04],
        [-2.7716e-06, -2.0340e-06,  2.1234e-06,  ..., -2.4810e-06,
         -1.8515e-06, -1.8664e-06],
        [-7.5996e-06, -5.7369e-06,  6.2287e-06,  ..., -6.6757e-06,
         -5.0068e-06, -4.9770e-06],
        [-5.2005e-06, -3.7402e-06,  4.1872e-06,  ..., -4.6045e-06,
         -3.4496e-06, -3.6508e-06],
        [-6.3032e-06, -5.0068e-06,  5.0515e-06,  ..., -5.6624e-06,
         -4.4554e-06, -3.8147e-06]], device='cuda:0')
Loss: 0.9465907216072083


Running epoch 1, step 1626, batch 578
Sampled inputs[:2]: tensor([[    0,   272,   352,  ...,   590,  4361,   446],
        [    0, 50208,   292,  ...,   408,   266,  3775]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3167e-04,  7.9064e-05,  2.0948e-04,  ..., -2.3669e-04,
         -3.1512e-04, -1.9852e-04],
        [-4.1500e-06, -3.0622e-06,  3.0622e-06,  ..., -3.7327e-06,
         -2.8126e-06, -2.7902e-06],
        [-1.1444e-05, -8.7768e-06,  9.0599e-06,  ..., -1.0177e-05,
         -7.7039e-06, -7.5698e-06],
        [-7.8380e-06, -5.7071e-06,  6.0648e-06,  ..., -7.0184e-06,
         -5.3048e-06, -5.5358e-06],
        [-9.5218e-06, -7.6443e-06,  7.3612e-06,  ..., -8.6427e-06,
         -6.8396e-06, -5.8264e-06]], device='cuda:0')
Loss: 0.9878668785095215


Running epoch 1, step 1627, batch 579
Sampled inputs[:2]: tensor([[   0,   12, 1197,  ...,  352, 2513,  266],
        [   0,  199, 2834,  ..., 1236,  768, 4316]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.0898e-04, -3.8920e-06,  7.1049e-05,  ..., -2.0450e-04,
         -4.7216e-04, -3.7537e-04],
        [-5.5507e-06, -4.0904e-06,  4.1500e-06,  ..., -4.9621e-06,
         -3.7737e-06, -3.6880e-06],
        [-1.5229e-05, -1.1638e-05,  1.2234e-05,  ..., -1.3456e-05,
         -1.0312e-05, -9.9242e-06],
        [-1.0535e-05, -7.6443e-06,  8.2850e-06,  ..., -9.3728e-06,
         -7.1600e-06, -7.3314e-06],
        [-1.2651e-05, -1.0118e-05,  9.9242e-06,  ..., -1.1384e-05,
         -9.1195e-06, -7.6219e-06]], device='cuda:0')
Loss: 0.9304164052009583


Running epoch 1, step 1628, batch 580
Sampled inputs[:2]: tensor([[    0, 12165,    12,  ...,  2860, 10718,   278],
        [    0,   221,   264,  ...,  3613,  3222, 14000]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8352e-04,  4.4902e-05,  8.8185e-05,  ..., -1.8168e-04,
         -4.1613e-04, -2.8239e-04],
        [-6.9961e-06, -5.1484e-06,  5.0925e-06,  ..., -6.2883e-06,
         -4.8839e-06, -4.7162e-06],
        [-1.9193e-05, -1.4648e-05,  1.4976e-05,  ..., -1.7047e-05,
         -1.3322e-05, -1.2696e-05],
        [-1.3307e-05, -9.6560e-06,  1.0140e-05,  ..., -1.1921e-05,
         -9.2909e-06, -9.3728e-06],
        [-1.6198e-05, -1.2860e-05,  1.2353e-05,  ..., -1.4618e-05,
         -1.1906e-05, -9.9465e-06]], device='cuda:0')
Loss: 0.9763545393943787


Running epoch 1, step 1629, batch 581
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,  2805,   607, 10848],
        [    0, 10446,    14,  ...,   266,  1164,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9979e-04,  7.3225e-05,  1.0680e-05,  ..., -2.3752e-04,
         -3.2295e-04, -2.2001e-04],
        [-8.4043e-06, -6.2585e-06,  6.0163e-06,  ..., -7.5772e-06,
         -5.8450e-06, -5.7220e-06],
        [-2.2963e-05, -1.7747e-05,  1.7673e-05,  ..., -2.0459e-05,
         -1.5900e-05, -1.5333e-05],
        [-1.5929e-05, -1.1712e-05,  1.1936e-05,  ..., -1.4335e-05,
         -1.1101e-05, -1.1340e-05],
        [-1.9491e-05, -1.5646e-05,  1.4663e-05,  ..., -1.7643e-05,
         -1.4290e-05, -1.2077e-05]], device='cuda:0')
Loss: 0.9598453044891357


Running epoch 1, step 1630, batch 582
Sampled inputs[:2]: tensor([[    0,  1412, 11275,  ...,   668, 14849,   367],
        [    0,    19,    14,  ...,    13,  6673,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.3263e-04,  1.4752e-05,  1.1685e-04,  ...,  4.2446e-05,
         -2.9046e-04, -4.3490e-05],
        [-9.8050e-06, -7.2569e-06,  6.8359e-06,  ..., -8.9779e-06,
         -6.9402e-06, -6.8545e-06],
        [-2.6807e-05, -2.0608e-05,  2.0176e-05,  ..., -2.4170e-05,
         -1.8865e-05, -1.8254e-05],
        [-1.8567e-05, -1.3560e-05,  1.3538e-05,  ..., -1.6972e-05,
         -1.3202e-05, -1.3500e-05],
        [-2.2992e-05, -1.8239e-05,  1.6898e-05,  ..., -2.1026e-05,
         -1.7047e-05, -1.4536e-05]], device='cuda:0')
Loss: 0.9583889245986938


Running epoch 1, step 1631, batch 583
Sampled inputs[:2]: tensor([[   0,  361, 1224,  ..., 4401, 4261, 1663],
        [   0, 1979,  352,  ...,  292, 1591,  446]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5144e-05,  5.6956e-05,  3.7956e-04,  ..., -5.0162e-05,
         -1.4023e-04, -5.0869e-05],
        [-1.1101e-05, -8.2701e-06,  7.7672e-06,  ..., -1.0185e-05,
         -7.9907e-06, -7.7374e-06],
        [-3.0428e-05, -2.3484e-05,  2.2963e-05,  ..., -2.7433e-05,
         -2.1711e-05, -2.0638e-05],
        [ 1.6783e-04,  2.5066e-04, -1.4940e-04,  ...,  2.1302e-04,
          2.6123e-04,  1.0596e-04],
        [-2.6107e-05, -2.0817e-05,  1.9237e-05,  ..., -2.3872e-05,
         -1.9625e-05, -1.6414e-05]], device='cuda:0')
Loss: 0.9494969248771667
Graident accumulation at epoch 1, step 1631, batch 583
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-8.7199e-05,  1.2163e-05,  5.0737e-05,  ..., -1.0277e-04,
          2.0671e-04,  1.7333e-04],
        [-7.7203e-06, -4.4599e-06,  5.1942e-06,  ..., -7.3140e-06,
         -4.0866e-06, -5.0466e-06],
        [ 8.8137e-06,  1.5651e-05, -8.0191e-06,  ...,  9.3194e-06,
          1.0729e-05, -3.0005e-08],
        [ 4.1560e-06,  1.9803e-05, -5.0255e-06,  ...,  1.0445e-05,
          1.9183e-05,  3.5183e-07],
        [-2.6103e-05, -2.0869e-05,  1.9690e-05,  ..., -2.3720e-05,
         -2.0124e-05, -1.6760e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8640e-08, 5.5774e-08, 5.4511e-08,  ..., 2.1086e-08, 1.2244e-07,
         4.8355e-08],
        [9.0172e-11, 5.9497e-11, 2.6470e-11,  ..., 6.2040e-11, 3.2388e-11,
         2.9916e-11],
        [2.0567e-09, 1.4312e-09, 6.8270e-10,  ..., 1.5765e-09, 8.9751e-10,
         5.3036e-10],
        [8.9494e-10, 1.0897e-09, 3.9483e-10,  ..., 8.9626e-10, 6.8099e-10,
         3.6980e-10],
        [4.2083e-10, 2.4504e-10, 9.1154e-11,  ..., 3.0824e-10, 9.0924e-11,
         1.2590e-10]], device='cuda:0')
optimizer state dict: 204.0
lr: [2.4620397327550194e-06, 2.4620397327550194e-06]
scheduler_last_epoch: 204


Running epoch 1, step 1632, batch 584
Sampled inputs[:2]: tensor([[    0,   259,  3022,  ...,   437,  5100,  1782],
        [    0,   368, 46614,  ...,  1070,   278,  1028]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7724e-05,  4.8563e-05, -1.2156e-04,  ...,  5.4837e-05,
         -8.5730e-05, -1.1410e-04],
        [-1.3858e-06, -1.1176e-06,  8.6427e-07,  ..., -1.3560e-06,
         -1.0282e-06, -1.0282e-06],
        [-3.8445e-06, -3.1292e-06,  2.6375e-06,  ..., -3.6359e-06,
         -2.7865e-06, -2.7269e-06],
        [-2.5779e-06, -2.0266e-06,  1.6615e-06,  ..., -2.5183e-06,
         -1.9222e-06, -1.9968e-06],
        [-3.3379e-06, -2.7865e-06,  2.2501e-06,  ..., -3.1888e-06,
         -2.5481e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9535592794418335


Running epoch 1, step 1633, batch 585
Sampled inputs[:2]: tensor([[    0,   494,   221,  ...,   298,  1062,  4923],
        [    0, 10893, 10997,  ...,   367,   616,  7903]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0176e-04,  4.8563e-05,  7.1783e-05,  ...,  2.9962e-04,
         -6.4213e-04, -2.8153e-04],
        [-2.7940e-06, -2.0340e-06,  1.8775e-06,  ..., -2.6822e-06,
         -2.0042e-06, -1.9968e-06],
        [-7.5996e-06, -5.6326e-06,  5.5134e-06,  ..., -7.0184e-06,
         -5.2899e-06, -5.1260e-06],
        [-5.2601e-06, -3.6955e-06,  3.6582e-06,  ..., -5.0217e-06,
         -3.7551e-06, -3.8743e-06],
        [-6.6310e-06, -5.0664e-06,  4.7237e-06,  ..., -6.1691e-06,
         -4.8429e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9182615280151367


Running epoch 1, step 1634, batch 586
Sampled inputs[:2]: tensor([[   0,  607,  443,  ...,  259, 2646, 1597],
        [   0,   89, 2023,  ...,  271,   13,  704]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8656e-04,  7.9532e-05,  8.2303e-05,  ...,  2.7933e-04,
         -9.2359e-04, -3.0870e-04],
        [-4.1127e-06, -3.0547e-06,  2.8461e-06,  ..., -3.8818e-06,
         -2.9728e-06, -2.8871e-06],
        [-1.1295e-05, -8.5384e-06,  8.4043e-06,  ..., -1.0327e-05,
         -7.9572e-06, -7.5996e-06],
        [-7.8529e-06, -5.6475e-06,  5.6550e-06,  ..., -7.3612e-06,
         -5.6401e-06, -5.7071e-06],
        [-9.7603e-06, -7.6443e-06,  7.1228e-06,  ..., -9.0301e-06,
         -7.2569e-06, -6.0499e-06]], device='cuda:0')
Loss: 0.9743207097053528


Running epoch 1, step 1635, batch 587
Sampled inputs[:2]: tensor([[    0,  1477,   591,  ...,  4111, 18012, 11991],
        [    0,   292, 15156,  ...,    35,  3815,  1422]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0399e-04,  3.2256e-04,  2.5866e-04,  ...,  2.6749e-04,
         -1.2316e-03, -4.3607e-04],
        [-5.5134e-06, -4.1649e-06,  3.8520e-06,  ..., -5.1782e-06,
         -4.0680e-06, -3.8184e-06],
        [-1.5140e-05, -1.1697e-05,  1.1370e-05,  ..., -1.3858e-05,
         -1.0967e-05, -1.0118e-05],
        [-1.0550e-05, -7.7635e-06,  7.6666e-06,  ..., -9.8497e-06,
         -7.7561e-06, -7.5623e-06],
        [-1.3068e-05, -1.0416e-05,  9.5963e-06,  ..., -1.2100e-05,
         -9.9540e-06, -8.0615e-06]], device='cuda:0')
Loss: 0.9616324305534363


Running epoch 1, step 1636, batch 588
Sampled inputs[:2]: tensor([[    0,   271,  8429,  ...,  9404,   963,   344],
        [    0, 12919,   292,  ...,   221,   273,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5854e-04,  4.3585e-04,  2.6119e-04,  ...,  3.3423e-04,
         -1.4331e-03, -5.0562e-04],
        [-6.8173e-06, -5.1707e-06,  4.8019e-06,  ..., -6.3926e-06,
         -5.0366e-06, -4.7386e-06],
        [-1.8716e-05, -1.4499e-05,  1.4201e-05,  ..., -1.7107e-05,
         -1.3545e-05, -1.2562e-05],
        [-1.3068e-05, -9.6411e-06,  9.5740e-06,  ..., -1.2174e-05,
         -9.6112e-06, -9.4250e-06],
        [-1.6078e-05, -1.2890e-05,  1.1921e-05,  ..., -1.4886e-05,
         -1.2279e-05, -9.9391e-06]], device='cuda:0')
Loss: 0.9526867270469666


Running epoch 1, step 1637, batch 589
Sampled inputs[:2]: tensor([[    0,   278,  8608,  ...,   293,  1608,   391],
        [    0,  1527, 21622,  ..., 14406,    13,  6182]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3465e-04,  3.0702e-04,  2.3955e-04,  ...,  3.6061e-04,
         -1.0656e-03, -2.5183e-04],
        [-8.2329e-06, -6.2585e-06,  5.7071e-06,  ..., -7.7039e-06,
         -6.1095e-06, -5.7518e-06],
        [-2.2739e-05, -1.7717e-05,  1.6987e-05,  ..., -2.0802e-05,
         -1.6555e-05, -1.5423e-05],
        [-1.5780e-05, -1.1712e-05,  1.1355e-05,  ..., -1.4693e-05,
         -1.1668e-05, -1.1466e-05],
        [-1.9401e-05, -1.5646e-05,  1.4186e-05,  ..., -1.7986e-05,
         -1.4916e-05, -1.2144e-05]], device='cuda:0')
Loss: 0.9690383672714233


Running epoch 1, step 1638, batch 590
Sampled inputs[:2]: tensor([[    0,  1172,   365,  ...,  1119, 15573,  3701],
        [    0,   445,     8,  ...,    13, 25386,    17]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.4599e-04,  3.2651e-04,  3.9652e-04,  ...,  3.6858e-04,
         -1.2439e-03, -2.8970e-04],
        [-9.6187e-06, -7.3612e-06,  6.6534e-06,  ..., -8.9854e-06,
         -7.1749e-06, -6.6869e-06],
        [-2.6643e-05, -2.0951e-05,  1.9863e-05,  ..., -2.4408e-05,
         -1.9565e-05, -1.8060e-05],
        [-1.8448e-05, -1.3828e-05,  1.3247e-05,  ..., -1.7181e-05,
         -1.3754e-05, -1.3374e-05],
        [-2.2650e-05, -1.8433e-05,  1.6525e-05,  ..., -2.1026e-05,
         -1.7524e-05, -1.4171e-05]], device='cuda:0')
Loss: 0.9646945595741272


Running epoch 1, step 1639, batch 591
Sampled inputs[:2]: tensor([[   0, 1431,  221,  ...,  756,  409,  275],
        [   0,  591, 2036,  ...,  266, 1027,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1378e-04,  3.0842e-04,  3.7799e-04,  ...,  3.4474e-04,
         -1.1223e-03, -1.8163e-04],
        [-1.1116e-05, -8.3670e-06,  7.7710e-06,  ..., -1.0282e-05,
         -8.1062e-06, -7.6927e-06],
        [-3.0816e-05, -2.3887e-05,  2.3171e-05,  ..., -2.8029e-05,
         -2.2203e-05, -2.0877e-05],
        [-2.1249e-05, -1.5713e-05,  1.5438e-05,  ..., -1.9610e-05,
         -1.5505e-05, -1.5341e-05],
        [-2.6032e-05, -2.0921e-05,  1.9148e-05,  ..., -2.4006e-05,
         -1.9789e-05, -1.6302e-05]], device='cuda:0')
Loss: 0.9517770409584045
Graident accumulation at epoch 1, step 1639, batch 591
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7101e-05,  4.1789e-05,  8.3462e-05,  ..., -5.8017e-05,
          7.3804e-05,  1.3783e-04],
        [-8.0599e-06, -4.8506e-06,  5.4519e-06,  ..., -7.6108e-06,
         -4.4885e-06, -5.3112e-06],
        [ 4.8508e-06,  1.1697e-05, -4.9001e-06,  ...,  5.5845e-06,
          7.4357e-06, -2.1147e-06],
        [ 1.6155e-06,  1.6251e-05, -2.9792e-06,  ...,  7.4394e-06,
          1.5714e-05, -1.2174e-06],
        [-2.6096e-05, -2.0875e-05,  1.9636e-05,  ..., -2.3749e-05,
         -2.0091e-05, -1.6714e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8845e-08, 5.5814e-08, 5.4599e-08,  ..., 2.1184e-08, 1.2357e-07,
         4.8340e-08],
        [9.0205e-11, 5.9508e-11, 2.6504e-11,  ..., 6.2084e-11, 3.2421e-11,
         2.9945e-11],
        [2.0555e-09, 1.4303e-09, 6.8256e-10,  ..., 1.5757e-09, 8.9710e-10,
         5.3027e-10],
        [8.9450e-10, 1.0889e-09, 3.9467e-10,  ..., 8.9575e-10, 6.8055e-10,
         3.6966e-10],
        [4.2109e-10, 2.4523e-10, 9.1429e-11,  ..., 3.0851e-10, 9.1225e-11,
         1.2604e-10]], device='cuda:0')
optimizer state dict: 205.0
lr: [2.381388209021682e-06, 2.381388209021682e-06]
scheduler_last_epoch: 205


Running epoch 1, step 1640, batch 592
Sampled inputs[:2]: tensor([[   0,    9,  870,  ..., 2671,  965, 3229],
        [   0, 3532,  300,  ...,   12,  461,  806]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3416e-04, -3.7417e-05, -7.1478e-05,  ...,  3.1486e-05,
          2.9565e-05,  3.0503e-06],
        [-1.4380e-06, -1.0356e-06,  9.2387e-07,  ..., -1.2666e-06,
         -9.9838e-07, -1.0505e-06],
        [-4.0531e-06, -3.0547e-06,  2.8312e-06,  ..., -3.5465e-06,
         -2.8014e-06, -2.9206e-06],
        [-2.8312e-06, -2.0117e-06,  1.8701e-06,  ..., -2.5034e-06,
         -1.9819e-06, -2.1607e-06],
        [-3.3826e-06, -2.6375e-06,  2.3097e-06,  ..., -3.0249e-06,
         -2.5034e-06, -2.2650e-06]], device='cuda:0')
Loss: 0.9434201717376709


Running epoch 1, step 1641, batch 593
Sampled inputs[:2]: tensor([[    0, 17508,    65,  ...,  8848, 13900,   796],
        [    0,  5116,  4330,  ...,   925,   699,  1351]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8120e-05,  5.3179e-05, -7.3522e-05,  ..., -9.6972e-06,
         -1.1985e-04, -6.4791e-05],
        [-2.9132e-06, -2.1085e-06,  1.8850e-06,  ..., -2.5779e-06,
         -2.0489e-06, -2.1309e-06],
        [-8.1360e-06, -6.1095e-06,  5.7071e-06,  ..., -7.1079e-06,
         -5.6475e-06, -5.8860e-06],
        [-5.5581e-06, -3.9786e-06,  3.7104e-06,  ..., -4.9323e-06,
         -3.9339e-06, -4.2617e-06],
        [-6.9290e-06, -5.3644e-06,  4.7386e-06,  ..., -6.1691e-06,
         -5.1111e-06, -4.6492e-06]], device='cuda:0')
Loss: 0.9543437957763672


Running epoch 1, step 1642, batch 594
Sampled inputs[:2]: tensor([[   0,  298, 2587,  ...,  298,  894,  496],
        [   0, 8840,   26,  ...,   28,   16,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5512e-04,  1.0841e-04,  7.0604e-05,  ...,  2.8679e-05,
         -6.2164e-05,  1.7965e-05],
        [-4.4256e-06, -3.1590e-06,  2.7791e-06,  ..., -3.9488e-06,
         -3.1367e-06, -3.3155e-06],
        [-1.2308e-05, -9.1493e-06,  8.4043e-06,  ..., -1.0863e-05,
         -8.6725e-06, -9.0748e-06],
        [-8.2552e-06, -5.8487e-06,  5.3570e-06,  ..., -7.4059e-06,
         -5.9307e-06, -6.4373e-06],
        [-1.0714e-05, -8.1956e-06,  7.1526e-06,  ..., -9.6112e-06,
         -7.9721e-06, -7.3612e-06]], device='cuda:0')
Loss: 0.9380626082420349


Running epoch 1, step 1643, batch 595
Sampled inputs[:2]: tensor([[   0,  417,  199,  ..., 8762, 4204,  391],
        [   0, 3388,  278,  ..., 7203,  271, 1746]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.5055e-04,  2.1432e-04, -1.5904e-04,  ..., -7.3482e-05,
         -1.8275e-04, -2.3902e-04],
        [-5.7295e-06, -4.1872e-06,  3.7998e-06,  ..., -5.1335e-06,
         -4.0531e-06, -4.1649e-06],
        [-1.5914e-05, -1.2025e-05,  1.1474e-05,  ..., -1.4052e-05,
         -1.1146e-05, -1.1310e-05],
        [-1.0788e-05, -7.7859e-06,  7.4580e-06,  ..., -9.6858e-06,
         -7.7039e-06, -8.1509e-06],
        [-1.3694e-05, -1.0669e-05,  9.6411e-06,  ..., -1.2293e-05,
         -1.0148e-05, -9.0376e-06]], device='cuda:0')
Loss: 0.9234718084335327


Running epoch 1, step 1644, batch 596
Sampled inputs[:2]: tensor([[    0,  1716,  1773,  ...,  5014,    12,   847],
        [    0,  4889,  3593,  ..., 19787,   287, 22475]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6929e-04,  1.8329e-04, -2.7382e-04,  ..., -1.4896e-04,
         -1.3553e-04, -3.8822e-04],
        [-7.1973e-06, -5.3421e-06,  4.8280e-06,  ..., -6.4448e-06,
         -5.1036e-06, -5.1148e-06],
        [-1.9848e-05, -1.5274e-05,  1.4454e-05,  ..., -1.7583e-05,
         -1.4022e-05, -1.3888e-05],
        [-1.3545e-05, -9.9465e-06,  9.4846e-06,  ..., -1.2174e-05,
         -9.7007e-06, -1.0028e-05],
        [-1.7032e-05, -1.3545e-05,  1.2100e-05,  ..., -1.5363e-05,
         -1.2755e-05, -1.1079e-05]], device='cuda:0')
Loss: 0.9745262265205383


Running epoch 1, step 1645, batch 597
Sampled inputs[:2]: tensor([[   0,  741, 2985,  ...,  199,  769,  278],
        [   0, 1125,  278,  ..., 6447,  609,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7499e-04,  2.0393e-04, -2.7382e-04,  ..., -8.4535e-05,
         -1.8367e-04, -2.0781e-04],
        [-8.6352e-06, -6.3777e-06,  5.8636e-06,  ..., -7.6741e-06,
         -6.1169e-06, -6.1430e-06],
        [-2.4050e-05, -1.8403e-05,  1.7628e-05,  ..., -2.1175e-05,
         -1.6987e-05, -1.6898e-05],
        [-1.6361e-05, -1.1943e-05,  1.1586e-05,  ..., -1.4573e-05,
         -1.1683e-05, -1.2130e-05],
        [-2.0429e-05, -1.6198e-05,  1.4633e-05,  ..., -1.8343e-05,
         -1.5303e-05, -1.3359e-05]], device='cuda:0')
Loss: 0.942450225353241


Running epoch 1, step 1646, batch 598
Sampled inputs[:2]: tensor([[   0,  346,  462,  ..., 2915,  275, 2565],
        [   0,  809,  367,  ...,  717,  287, 1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7574e-04,  3.6066e-04, -2.3022e-04,  ...,  1.4582e-04,
         -4.8894e-04, -2.5678e-04],
        [-1.0036e-05, -7.3202e-06,  6.7353e-06,  ..., -8.9630e-06,
         -7.1153e-06, -7.1786e-06],
        [-2.7746e-05, -2.1011e-05,  2.0206e-05,  ..., -2.4468e-05,
         -1.9625e-05, -1.9476e-05],
        [-1.8939e-05, -1.3664e-05,  1.3292e-05,  ..., -1.6958e-05,
         -1.3575e-05, -1.4096e-05],
        [-2.3693e-05, -1.8567e-05,  1.6853e-05,  ..., -2.1294e-05,
         -1.7747e-05, -1.5460e-05]], device='cuda:0')
Loss: 0.8922945261001587


Running epoch 1, step 1647, batch 599
Sampled inputs[:2]: tensor([[    0,   266, 10262,  ...,   271,  3437,  4392],
        [    0,   409, 35049,  ...,    12,   699,   394]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2498e-04,  5.6307e-04, -9.2690e-05,  ...,  8.7050e-05,
         -5.3570e-04, -2.3916e-04],
        [-1.1474e-05, -8.3111e-06,  7.4357e-06,  ..., -1.0341e-05,
         -8.3148e-06, -8.3931e-06],
        [-3.1710e-05, -2.3857e-05,  2.2411e-05,  ..., -2.8178e-05,
         -2.2873e-05, -2.2724e-05],
        [-2.1622e-05, -1.5445e-05,  1.4588e-05,  ..., -1.9535e-05,
         -1.5840e-05, -1.6406e-05],
        [-2.7388e-05, -2.1249e-05,  1.8850e-05,  ..., -2.4781e-05,
         -2.0877e-05, -1.8261e-05]], device='cuda:0')
Loss: 0.9219314455986023
Graident accumulation at epoch 1, step 1647, batch 599
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.1073e-06,  9.3917e-05,  6.5847e-05,  ..., -4.3510e-05,
          1.2854e-05,  1.0013e-04],
        [-8.4013e-06, -5.1966e-06,  5.6502e-06,  ..., -7.8839e-06,
         -4.8712e-06, -5.6194e-06],
        [ 1.1947e-06,  8.1419e-06, -2.1689e-06,  ...,  2.2083e-06,
          4.4048e-06, -4.1756e-06],
        [-7.0821e-07,  1.3081e-05, -1.2225e-06,  ...,  4.7419e-06,
          1.2559e-05, -2.7363e-06],
        [-2.6225e-05, -2.0912e-05,  1.9557e-05,  ..., -2.3852e-05,
         -2.0169e-05, -1.6869e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8892e-08, 5.6075e-08, 5.4553e-08,  ..., 2.1171e-08, 1.2374e-07,
         4.8348e-08],
        [9.0247e-11, 5.9517e-11, 2.6532e-11,  ..., 6.2129e-11, 3.2458e-11,
         2.9985e-11],
        [2.0545e-09, 1.4295e-09, 6.8238e-10,  ..., 1.5749e-09, 8.9673e-10,
         5.3026e-10],
        [8.9407e-10, 1.0880e-09, 3.9449e-10,  ..., 8.9523e-10, 6.8012e-10,
         3.6956e-10],
        [4.2142e-10, 2.4544e-10, 9.1693e-11,  ..., 3.0882e-10, 9.1569e-11,
         1.2625e-10]], device='cuda:0')
optimizer state dict: 206.0
lr: [2.3019008756738137e-06, 2.3019008756738137e-06]
scheduler_last_epoch: 206


Running epoch 1, step 1648, batch 600
Sampled inputs[:2]: tensor([[    0,     9,   300,  ...,  6838,   328, 18619],
        [    0, 39200,  1828,  ...,   300,  3067,  4443]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0637e-04, -2.3475e-04, -1.5134e-04,  ...,  1.1428e-04,
          3.2482e-04, -1.0412e-04],
        [-1.3560e-06, -9.9838e-07,  9.9838e-07,  ..., -1.2442e-06,
         -9.9838e-07, -9.4250e-07],
        [-3.6955e-06, -2.8014e-06,  2.9504e-06,  ..., -3.3230e-06,
         -2.6971e-06, -2.4885e-06],
        [-2.5928e-06, -1.8775e-06,  2.0117e-06,  ..., -2.3693e-06,
         -1.9521e-06, -1.8924e-06],
        [-3.0547e-06, -2.4140e-06,  2.3693e-06,  ..., -2.7865e-06,
         -2.3544e-06, -1.8775e-06]], device='cuda:0')
Loss: 0.9483883380889893


Running epoch 1, step 1649, batch 601
Sampled inputs[:2]: tensor([[    0,   756,   401,  ...,   271,  7272,  1663],
        [    0, 28011,    12,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8464e-04, -1.8339e-04, -2.1344e-04,  ...,  2.0749e-05,
         -1.2304e-04, -3.2310e-04],
        [-2.6524e-06, -1.9297e-06,  2.0489e-06,  ..., -2.3618e-06,
         -1.9260e-06, -1.8217e-06],
        [-7.3165e-06, -5.4389e-06,  6.0797e-06,  ..., -6.3628e-06,
         -5.2452e-06, -4.8280e-06],
        [-5.1558e-06, -3.6508e-06,  4.2021e-06,  ..., -4.5449e-06,
         -3.7625e-06, -3.6880e-06],
        [-6.1244e-06, -4.7535e-06,  4.9770e-06,  ..., -5.4091e-06,
         -4.6492e-06, -3.6955e-06]], device='cuda:0')
Loss: 0.9275630116462708


Running epoch 1, step 1650, batch 602
Sampled inputs[:2]: tensor([[    0,   397,  1267,  ...,  1276,   292,   221],
        [    0, 10676,   328,  ...,     9,   360,  2583]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3708e-04, -2.0661e-04, -1.2700e-04,  ...,  9.8997e-06,
         -8.4563e-05, -1.8612e-04],
        [-4.0606e-06, -3.1292e-06,  3.0696e-06,  ..., -3.6508e-06,
         -2.9989e-06, -2.7977e-06],
        [-1.1280e-05, -8.9407e-06,  9.1493e-06,  ..., -9.9689e-06,
         -8.2552e-06, -7.5549e-06],
        [-7.8380e-06, -5.9307e-06,  6.2287e-06,  ..., -6.9886e-06,
         -5.8040e-06, -5.6103e-06],
        [-9.5367e-06, -7.8231e-06,  7.5549e-06,  ..., -8.5533e-06,
         -7.3612e-06, -5.9009e-06]], device='cuda:0')
Loss: 0.9669355154037476


Running epoch 1, step 1651, batch 603
Sampled inputs[:2]: tensor([[    0, 17471,  4778,  ...,  2177,   271,   266],
        [    0,    12, 32425,  ...,   389,   221,   494]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0863e-04, -5.5174e-05, -1.4503e-04,  ...,  5.2445e-05,
         -2.7914e-04, -3.4578e-04],
        [-5.5283e-06, -4.3064e-06,  4.0606e-06,  ..., -4.9695e-06,
         -4.1090e-06, -3.8482e-06],
        [-1.5214e-05, -1.2249e-05,  1.2025e-05,  ..., -1.3500e-05,
         -1.1310e-05, -1.0356e-05],
        [-1.0550e-05, -8.1211e-06,  8.1360e-06,  ..., -9.4324e-06,
         -7.9051e-06, -7.6368e-06],
        [-1.2979e-05, -1.0803e-05,  1.0028e-05,  ..., -1.1697e-05,
         -1.0177e-05, -8.1956e-06]], device='cuda:0')
Loss: 0.9571051001548767


Running epoch 1, step 1652, batch 604
Sampled inputs[:2]: tensor([[    0,    14,  1147,  ...,    19,    14, 42301],
        [    0,  1098,   259,  ...,  6572,  1477,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0863e-04, -8.9758e-05, -2.1525e-04,  ...,  1.1640e-04,
         -3.8217e-04, -4.9760e-04],
        [-6.9514e-06, -5.4464e-06,  5.1335e-06,  ..., -6.2063e-06,
         -5.0999e-06, -4.8243e-06],
        [-1.9208e-05, -1.5602e-05,  1.5244e-05,  ..., -1.6987e-05,
         -1.4141e-05, -1.3083e-05],
        [-1.3262e-05, -1.0297e-05,  1.0282e-05,  ..., -1.1802e-05,
         -9.8124e-06, -9.5740e-06],
        [-1.6272e-05, -1.3679e-05,  1.2636e-05,  ..., -1.4648e-05,
         -1.2666e-05, -1.0312e-05]], device='cuda:0')
Loss: 0.9510496258735657


Running epoch 1, step 1653, batch 605
Sampled inputs[:2]: tensor([[   0, 1823,   12,  ..., 1874,  271,  266],
        [   0,   12, 5820,  ...,  221,  380,  560]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2775e-04, -4.0455e-05, -1.9593e-04,  ...,  1.4655e-04,
         -5.4567e-05, -1.9879e-04],
        [-8.3745e-06, -6.5714e-06,  6.0573e-06,  ..., -7.5474e-06,
         -6.2548e-06, -5.8226e-06],
        [-2.2978e-05, -1.8671e-05,  1.7926e-05,  ..., -2.0474e-05,
         -1.7121e-05, -1.5661e-05],
        [-1.5900e-05, -1.2338e-05,  1.2055e-05,  ..., -1.4275e-05,
         -1.1943e-05, -1.1496e-05],
        [-1.9699e-05, -1.6555e-05,  1.5005e-05,  ..., -1.7866e-05,
         -1.5512e-05, -1.2487e-05]], device='cuda:0')
Loss: 0.9805013537406921


Running epoch 1, step 1654, batch 606
Sampled inputs[:2]: tensor([[    0,  5332,   391,  ...,   221,   334,  1530],
        [    0, 28926,   266,  ...,  1061,  2615,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0336e-04, -6.5869e-05, -2.4412e-04,  ..., -9.9917e-06,
         -9.8023e-05, -3.9025e-04],
        [-9.7156e-06, -7.6517e-06,  6.9402e-06,  ..., -8.8289e-06,
         -7.2978e-06, -6.7689e-06],
        [-2.6569e-05, -2.1681e-05,  2.0534e-05,  ..., -2.3872e-05,
         -1.9923e-05, -1.8150e-05],
        [-1.8433e-05, -1.4365e-05,  1.3806e-05,  ..., -1.6719e-05,
         -1.3955e-05, -1.3381e-05],
        [-2.2814e-05, -1.9252e-05,  1.7211e-05,  ..., -2.0862e-05,
         -1.8075e-05, -1.4484e-05]], device='cuda:0')
Loss: 0.9626753926277161


Running epoch 1, step 1655, batch 607
Sampled inputs[:2]: tensor([[    0,   287,  2026,  ..., 16374,   266,  2236],
        [    0,   328,  6875,  ...,   369,   654,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0199e-04, -4.8757e-05, -2.7680e-04,  ..., -3.5708e-05,
         -1.9599e-04, -4.3662e-04],
        [-1.1146e-05, -8.8140e-06,  8.0131e-06,  ..., -1.0073e-05,
         -8.3484e-06, -7.7002e-06],
        [-3.0562e-05, -2.5034e-05,  2.3767e-05,  ..., -2.7359e-05,
         -2.2903e-05, -2.0742e-05],
        [-2.1175e-05, -1.6585e-05,  1.5967e-05,  ..., -1.9103e-05,
         -1.5981e-05, -1.5244e-05],
        [-2.6122e-05, -2.2143e-05,  1.9833e-05,  ..., -2.3797e-05,
         -2.0698e-05, -1.6466e-05]], device='cuda:0')
Loss: 0.9695873260498047
Graident accumulation at epoch 1, step 1655, batch 607
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.2902e-05,  7.9649e-05,  3.1582e-05,  ..., -4.2730e-05,
         -8.0299e-06,  4.6457e-05],
        [-8.6758e-06, -5.5584e-06,  5.8865e-06,  ..., -8.1028e-06,
         -5.2189e-06, -5.8274e-06],
        [-1.9810e-06,  4.8244e-06,  4.2468e-07,  ..., -7.4840e-07,
          1.6740e-06, -5.8323e-06],
        [-2.7548e-06,  1.0115e-05,  4.9643e-07,  ...,  2.3574e-06,
          9.7046e-06, -3.9871e-06],
        [-2.6215e-05, -2.1035e-05,  1.9585e-05,  ..., -2.3846e-05,
         -2.0222e-05, -1.6828e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8874e-08, 5.6021e-08, 5.4575e-08,  ..., 2.1151e-08, 1.2365e-07,
         4.8491e-08],
        [9.0281e-11, 5.9535e-11, 2.6570e-11,  ..., 6.2168e-11, 3.2495e-11,
         3.0015e-11],
        [2.0534e-09, 1.4287e-09, 6.8226e-10,  ..., 1.5741e-09, 8.9636e-10,
         5.3016e-10],
        [8.9363e-10, 1.0872e-09, 3.9435e-10,  ..., 8.9470e-10, 6.7970e-10,
         3.6943e-10],
        [4.2168e-10, 2.4568e-10, 9.1995e-11,  ..., 3.0907e-10, 9.1906e-11,
         1.2640e-10]], device='cuda:0')
optimizer state dict: 207.0
lr: [2.223589879069793e-06, 2.223589879069793e-06]
scheduler_last_epoch: 207


Running epoch 1, step 1656, batch 608
Sampled inputs[:2]: tensor([[    0,  6010,   829,  ...,   668,  1784,   587],
        [    0,   266, 15324,  ...,   943,  1613,  7178]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2916e-04,  2.7469e-05, -6.7927e-05,  ..., -2.9681e-05,
          5.1811e-05, -3.4945e-05],
        [-1.5274e-06, -1.0729e-06,  1.0058e-06,  ..., -1.3262e-06,
         -1.0803e-06, -1.1027e-06],
        [-4.1723e-06, -3.0398e-06,  2.9653e-06,  ..., -3.5912e-06,
         -2.9504e-06, -2.9802e-06],
        [-2.9057e-06, -2.0117e-06,  1.9819e-06,  ..., -2.5332e-06,
         -2.0564e-06, -2.1756e-06],
        [-3.6657e-06, -2.7567e-06,  2.5481e-06,  ..., -3.2187e-06,
         -2.7418e-06, -2.4587e-06]], device='cuda:0')
Loss: 0.986515462398529


Running epoch 1, step 1657, batch 609
Sampled inputs[:2]: tensor([[    0,   367,  3399,  ..., 13481,   408,  6944],
        [    0,  1487,  2511,  ..., 27735,   760,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0679e-04,  1.3508e-07, -8.0861e-05,  ...,  1.0549e-04,
         -1.9200e-04, -5.6918e-05],
        [-2.9355e-06, -2.1532e-06,  1.9409e-06,  ..., -2.6599e-06,
         -2.1234e-06, -2.1532e-06],
        [-8.4043e-06, -6.4224e-06,  5.9605e-06,  ..., -7.5847e-06,
         -6.0797e-06, -6.1244e-06],
        [-5.7817e-06, -4.2021e-06,  3.9488e-06,  ..., -5.2750e-06,
         -4.2021e-06, -4.4256e-06],
        [-7.1228e-06, -5.6028e-06,  4.9323e-06,  ..., -6.5267e-06,
         -5.4389e-06, -4.8727e-06]], device='cuda:0')
Loss: 0.9744974374771118


Running epoch 1, step 1658, batch 610
Sampled inputs[:2]: tensor([[    0,  1932,    15,  ...,   344,   984,   344],
        [    0,   677, 35427,  ..., 30465,  2783,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2564e-04,  2.5865e-05, -1.3910e-04,  ...,  1.1939e-04,
         -2.1215e-04, -1.4926e-04],
        [-4.2692e-06, -3.2112e-06,  2.9393e-06,  ..., -3.8967e-06,
         -3.1739e-06, -3.1069e-06],
        [-1.2025e-05, -9.3728e-06,  8.8811e-06,  ..., -1.0878e-05,
         -8.8513e-06, -8.6427e-06],
        [-8.3894e-06, -6.2287e-06,  5.9903e-06,  ..., -7.6890e-06,
         -6.2436e-06, -6.3926e-06],
        [-1.0282e-05, -8.2552e-06,  7.4059e-06,  ..., -9.4324e-06,
         -7.9572e-06, -6.8694e-06]], device='cuda:0')
Loss: 0.9572491645812988


Running epoch 1, step 1659, batch 611
Sampled inputs[:2]: tensor([[  0, 380, 341,  ..., 955, 644, 271],
        [  0,   9, 342,  ...,  12, 709, 857]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9121e-05,  7.4765e-05, -1.4771e-04,  ...,  2.2107e-04,
         -3.4234e-04, -1.2271e-04],
        [-5.6103e-06, -4.3139e-06,  3.9525e-06,  ..., -5.0962e-06,
         -4.1723e-06, -4.0531e-06],
        [ 4.4470e-05,  2.6251e-05, -1.2092e-05,  ...,  8.2097e-05,
          6.8379e-06,  3.2446e-05],
        [-1.1012e-05, -8.3447e-06,  8.0615e-06,  ..., -1.0028e-05,
         -8.1956e-06, -8.3297e-06],
        [-1.3337e-05, -1.0952e-05,  9.8199e-06,  ..., -1.2189e-05,
         -1.0356e-05, -8.8215e-06]], device='cuda:0')
Loss: 0.9344222545623779


Running epoch 1, step 1660, batch 612
Sampled inputs[:2]: tensor([[    0,  2352,  4275,  ..., 10518,   342,   266],
        [    0,   199,  1139,  ...,    13,  1303, 26330]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4968e-04,  1.2087e-04, -2.2195e-04,  ...,  1.5530e-04,
         -5.9385e-04, -9.0067e-05],
        [-6.9737e-06, -5.3346e-06,  4.9584e-06,  ..., -6.3255e-06,
         -5.1558e-06, -4.9993e-06],
        [ 4.0790e-05,  2.3420e-05, -9.1562e-06,  ...,  7.8834e-05,
          4.2302e-06,  2.9973e-05],
        [-1.3605e-05, -1.0267e-05,  1.0073e-05,  ..., -1.2383e-05,
         -1.0081e-05, -1.0222e-05],
        [-1.6436e-05, -1.3426e-05,  1.2234e-05,  ..., -1.4961e-05,
         -1.2651e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9492218494415283


Running epoch 1, step 1661, batch 613
Sampled inputs[:2]: tensor([[    0,   352,   266,  ...,   490, 10112,  3804],
        [    0,  3806,    13,  ..., 11786,  2254,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.9679e-04, -1.0725e-05, -2.1792e-04,  ...,  2.0854e-04,
         -6.8262e-04, -1.2255e-04],
        [-8.3745e-06, -6.4075e-06,  6.0238e-06,  ..., -7.5772e-06,
         -6.0946e-06, -5.9605e-06],
        [ 3.6915e-05,  2.0350e-05, -5.9823e-06,  ...,  7.5407e-05,
          1.6225e-06,  2.7335e-05],
        [-1.6302e-05, -1.2293e-05,  1.2219e-05,  ..., -1.4767e-05,
         -1.1876e-05, -1.2174e-05],
        [-1.9729e-05, -1.6138e-05,  1.4871e-05,  ..., -1.7926e-05,
         -1.5005e-05, -1.2755e-05]], device='cuda:0')
Loss: 0.98763507604599


Running epoch 1, step 1662, batch 614
Sampled inputs[:2]: tensor([[    0,  4323,  8213,  ...,  1153,   278,  4258],
        [    0,   508,  2322,  ...,   968,   266, 15123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4497e-04,  4.4921e-05, -3.8215e-04,  ...,  2.1325e-04,
         -7.5692e-04, -8.5915e-05],
        [-9.8571e-06, -7.4580e-06,  6.9998e-06,  ..., -8.8811e-06,
         -7.1302e-06, -7.0035e-06],
        [ 3.2862e-05,  1.7370e-05, -3.1064e-06,  ...,  7.1875e-05,
         -1.1938e-06,  2.4489e-05],
        [-1.9118e-05, -1.4246e-05,  1.4141e-05,  ..., -1.7226e-05,
         -1.3828e-05, -1.4246e-05],
        [-2.3201e-05, -1.8790e-05,  1.7270e-05,  ..., -2.1011e-05,
         -1.7583e-05, -1.5005e-05]], device='cuda:0')
Loss: 0.946530818939209


Running epoch 1, step 1663, batch 615
Sampled inputs[:2]: tensor([[   0, 3634, 3444,  ...,  642, 2156,  266],
        [   0,   73,   30,  ..., 4112,   12, 9416]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1379e-04, -4.7633e-05, -3.4850e-04,  ...,  2.1101e-04,
         -8.0404e-04, -4.7574e-05],
        [-1.1250e-05, -8.5682e-06,  8.1398e-06,  ..., -1.0096e-05,
         -8.0988e-06, -7.9386e-06],
        [ 8.6475e-05,  6.4174e-05, -2.3372e-05,  ...,  1.3882e-04,
          4.9926e-05,  6.0671e-05],
        [-2.1905e-05, -1.6451e-05,  1.6510e-05,  ..., -1.9640e-05,
         -1.5751e-05, -1.6212e-05],
        [-2.6360e-05, -2.1547e-05,  1.9923e-05,  ..., -2.3827e-05,
         -1.9953e-05, -1.6972e-05]], device='cuda:0')
Loss: 0.9850708842277527
Graident accumulation at epoch 1, step 1663, batch 615
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9767e-05,  6.6921e-05, -6.4256e-06,  ..., -1.7356e-05,
         -8.7630e-05,  3.7054e-05],
        [-8.9333e-06, -5.8593e-06,  6.1118e-06,  ..., -8.3021e-06,
         -5.5069e-06, -6.0386e-06],
        [ 6.8646e-06,  1.0759e-05, -1.9550e-06,  ...,  1.3208e-05,
          6.4992e-06,  8.1803e-07],
        [-4.6698e-06,  7.4582e-06,  2.0978e-06,  ...,  1.5769e-07,
          7.1591e-06, -5.2096e-06],
        [-2.6229e-05, -2.1086e-05,  1.9619e-05,  ..., -2.3844e-05,
         -2.0195e-05, -1.6843e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8913e-08, 5.5967e-08, 5.4642e-08,  ..., 2.1174e-08, 1.2417e-07,
         4.8444e-08],
        [9.0317e-11, 5.9549e-11, 2.6610e-11,  ..., 6.2208e-11, 3.2529e-11,
         3.0048e-11],
        [2.0588e-09, 1.4313e-09, 6.8213e-10,  ..., 1.5918e-09, 8.9795e-10,
         5.3331e-10],
        [8.9321e-10, 1.0864e-09, 3.9423e-10,  ..., 8.9419e-10, 6.7927e-10,
         3.6932e-10],
        [4.2195e-10, 2.4590e-10, 9.2299e-11,  ..., 3.0933e-10, 9.2212e-11,
         1.2656e-10]], device='cuda:0')
optimizer state dict: 208.0
lr: [2.1464671858134968e-06, 2.1464671858134968e-06]
scheduler_last_epoch: 208


Running epoch 1, step 1664, batch 616
Sampled inputs[:2]: tensor([[    0,  2377,   271,  ...,   395,   394,    14],
        [    0,   266, 20604,  ...,   409, 13764,  6048]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.4788e-05, -4.7367e-05, -2.9356e-04,  ...,  2.2175e-05,
         -9.7800e-05,  3.9960e-05],
        [-1.4976e-06, -1.1027e-06,  1.2144e-06,  ..., -1.2070e-06,
         -9.4622e-07, -9.0152e-07],
        [-4.0531e-06, -3.1292e-06,  3.4720e-06,  ..., -3.3081e-06,
         -2.6375e-06, -2.4885e-06],
        [-2.8908e-06, -2.1309e-06,  2.4438e-06,  ..., -2.3246e-06,
         -1.8328e-06, -1.8328e-06],
        [-3.1590e-06, -2.6077e-06,  2.6524e-06,  ..., -2.6524e-06,
         -2.2501e-06, -1.8030e-06]], device='cuda:0')
Loss: 0.9737432599067688


Running epoch 1, step 1665, batch 617
Sampled inputs[:2]: tensor([[    0,  2422,   300,  ...,   630,   729,  3400],
        [    0, 31550,    14,  ...,   278,   266,  4901]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6189e-05, -5.1272e-05, -3.5255e-04,  ...,  6.7050e-05,
         -2.8611e-04,  1.3944e-05],
        [-2.8983e-06, -2.1309e-06,  2.2352e-06,  ..., -2.4885e-06,
         -1.9744e-06, -1.9222e-06],
        [-7.8678e-06, -6.0201e-06,  6.4969e-06,  ..., -6.7502e-06,
         -5.4240e-06, -5.1856e-06],
        [-5.5879e-06, -4.0978e-06,  4.5002e-06,  ..., -4.7982e-06,
         -3.8296e-06, -3.8743e-06],
        [-6.3777e-06, -5.1260e-06,  5.1409e-06,  ..., -5.5879e-06,
         -4.7386e-06, -3.8892e-06]], device='cuda:0')
Loss: 0.9407268166542053


Running epoch 1, step 1666, batch 618
Sampled inputs[:2]: tensor([[    0,     5,  4413,  ...,  9205, 16744,   775],
        [    0, 23230,    12,  ...,  5092,   741,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1815e-04, -2.4600e-04, -3.4382e-04,  ...,  5.9524e-05,
          1.0622e-04,  4.5512e-04],
        [-4.2617e-06, -3.1441e-06,  3.1032e-06,  ..., -3.7625e-06,
         -3.0547e-06, -2.9877e-06],
        [-1.1623e-05, -8.9556e-06,  9.1642e-06,  ..., -1.0177e-05,
         -8.3745e-06, -8.0317e-06],
        [-8.2701e-06, -6.0946e-06,  6.2734e-06,  ..., -7.3314e-06,
         -6.0052e-06, -6.0648e-06],
        [-9.6411e-06, -7.7486e-06,  7.4059e-06,  ..., -8.6129e-06,
         -7.4208e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9480488300323486


Running epoch 1, step 1667, batch 619
Sampled inputs[:2]: tensor([[    0, 23842,   342,  ...,   365,  4011, 10151],
        [    0, 40624,   266,  ..., 12236,   292,    41]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7908e-04, -3.8437e-04, -3.5506e-04,  ...,  7.2766e-05,
          4.0809e-04,  5.6910e-04],
        [-5.6103e-06, -4.2990e-06,  3.9749e-06,  ..., -5.0589e-06,
         -4.2096e-06, -3.9376e-06],
        [-1.5363e-05, -1.2249e-05,  1.1832e-05,  ..., -1.3709e-05,
         -1.1519e-05, -1.0625e-05],
        [-1.0833e-05, -8.2999e-06,  8.0094e-06,  ..., -9.8199e-06,
         -8.2403e-06, -7.9721e-06],
        [-1.2919e-05, -1.0699e-05,  9.7007e-06,  ..., -1.1727e-05,
         -1.0267e-05, -8.2552e-06]], device='cuda:0')
Loss: 0.9779621958732605


Running epoch 1, step 1668, batch 620
Sampled inputs[:2]: tensor([[   0,  806, 1255,  ...,  474,  221,  380],
        [   0,  266, 6449,  ...,  474,  221,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0916e-04, -5.2528e-04, -3.0878e-04,  ...,  5.6912e-05,
          5.8292e-04,  5.7320e-04],
        [-7.0408e-06, -5.3272e-06,  5.0701e-06,  ..., -6.2883e-06,
         -5.1856e-06, -4.8392e-06],
        [-1.9297e-05, -1.5229e-05,  1.5020e-05,  ..., -1.7107e-05,
         -1.4216e-05, -1.3113e-05],
        [-1.3590e-05, -1.0282e-05,  1.0200e-05,  ..., -1.2189e-05,
         -1.0133e-05, -9.7975e-06],
        [-1.6123e-05, -1.3232e-05,  1.2249e-05,  ..., -1.4544e-05,
         -1.2591e-05, -1.0133e-05]], device='cuda:0')
Loss: 0.9589584469795227


Running epoch 1, step 1669, batch 621
Sampled inputs[:2]: tensor([[    0,  2270,   278,  ..., 36325,  5892,  3558],
        [    0,   199,  3289,  ...,  2269,  6476,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5112e-05, -4.7623e-04, -2.8813e-04,  ..., -1.2703e-04,
          6.2507e-04,  6.5813e-04],
        [-8.6129e-06, -6.1989e-06,  6.1356e-06,  ..., -7.5921e-06,
         -6.1281e-06, -6.0014e-06],
        [-2.3648e-05, -1.7837e-05,  1.8209e-05,  ..., -2.0698e-05,
         -1.6868e-05, -1.6317e-05],
        [-1.6496e-05, -1.1906e-05,  1.2241e-05,  ..., -1.4618e-05,
         -1.1899e-05, -1.1973e-05],
        [-1.9819e-05, -1.5467e-05,  1.4842e-05,  ..., -1.7673e-05,
         -1.4976e-05, -1.2726e-05]], device='cuda:0')
Loss: 0.9362601041793823


Running epoch 1, step 1670, batch 622
Sampled inputs[:2]: tensor([[    0,    14, 21687,  ...,   943,  2153,  4089],
        [    0,   369,   726,  ...,   292,   221,   358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0551e-04, -6.0409e-04, -3.3501e-04,  ..., -4.0075e-06,
          6.0329e-04,  7.1424e-04],
        [-1.0021e-05, -7.2643e-06,  7.2233e-06,  ..., -8.8438e-06,
         -7.1563e-06, -6.9924e-06],
        [-2.7522e-05, -2.0847e-05,  2.1368e-05,  ..., -2.4080e-05,
         -1.9655e-05, -1.8999e-05],
        [-1.9237e-05, -1.3947e-05,  1.4432e-05,  ..., -1.7047e-05,
         -1.3895e-05, -1.4000e-05],
        [-2.3052e-05, -1.8075e-05,  1.7434e-05,  ..., -2.0549e-05,
         -1.7419e-05, -1.4782e-05]], device='cuda:0')
Loss: 0.9995194673538208


Running epoch 1, step 1671, batch 623
Sampled inputs[:2]: tensor([[   0,  199, 2834,  ..., 3988, 1049,  935],
        [   0,  380,  333,  ..., 8127,  504,  679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6805e-05, -4.2401e-04, -2.9178e-04,  ...,  7.6702e-05,
          3.0731e-04,  6.4134e-04],
        [-1.1437e-05, -8.2999e-06,  8.2068e-06,  ..., -1.0066e-05,
         -8.1770e-06, -7.9609e-06],
        [-3.1248e-05, -2.3797e-05,  2.4199e-05,  ..., -2.7329e-05,
         -2.2426e-05, -2.1547e-05],
        [-2.1905e-05, -1.5914e-05,  1.6384e-05,  ..., -1.9372e-05,
         -1.5877e-05, -1.5922e-05],
        [-2.6226e-05, -2.0668e-05,  1.9789e-05,  ..., -2.3380e-05,
         -1.9908e-05, -1.6779e-05]], device='cuda:0')
Loss: 0.931993842124939
Graident accumulation at epoch 1, step 1671, batch 623
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9471e-05,  1.7829e-05, -3.4961e-05,  ..., -7.9501e-06,
         -4.8136e-05,  9.7483e-05],
        [-9.1836e-06, -6.1034e-06,  6.3213e-06,  ..., -8.4784e-06,
         -5.7739e-06, -6.2308e-06],
        [ 3.0534e-06,  7.3036e-06,  6.6047e-07,  ...,  9.1546e-06,
          3.6066e-06, -1.4185e-06],
        [-6.3933e-06,  5.1209e-06,  3.5264e-06,  ..., -1.7952e-06,
          4.8555e-06, -6.2808e-06],
        [-2.6229e-05, -2.1044e-05,  1.9636e-05,  ..., -2.3798e-05,
         -2.0166e-05, -1.6836e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8855e-08, 5.6091e-08, 5.4672e-08,  ..., 2.1159e-08, 1.2414e-07,
         4.8807e-08],
        [9.0357e-11, 5.9559e-11, 2.6650e-11,  ..., 6.2247e-11, 3.2563e-11,
         3.0081e-11],
        [2.0577e-09, 1.4305e-09, 6.8203e-10,  ..., 1.5910e-09, 8.9756e-10,
         5.3324e-10],
        [8.9280e-10, 1.0856e-09, 3.9410e-10,  ..., 8.9367e-10, 6.7884e-10,
         3.6920e-10],
        [4.2222e-10, 2.4608e-10, 9.2599e-11,  ..., 3.0957e-10, 9.2516e-11,
         1.2671e-10]], device='cuda:0')
optimizer state dict: 209.0
lr: [2.070544580925664e-06, 2.070544580925664e-06]
scheduler_last_epoch: 209


Running epoch 1, step 1672, batch 624
Sampled inputs[:2]: tensor([[    0,  7066,  2737,  ...,  2269,   271,   927],
        [    0,    13, 32291,  ...,  3740,  3616,  1274]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.0917e-05, -2.8132e-05, -1.3910e-04,  ...,  3.0352e-05,
         -1.3930e-04, -2.2597e-05],
        [-1.3858e-06, -1.0058e-06,  1.1548e-06,  ..., -1.1772e-06,
         -9.4622e-07, -9.0897e-07],
        [-3.7700e-06, -2.8312e-06,  3.3677e-06,  ..., -3.1739e-06,
         -2.5481e-06, -2.4885e-06],
        [-2.7120e-06, -1.9222e-06,  2.3842e-06,  ..., -2.2799e-06,
         -1.8179e-06, -1.8775e-06],
        [-3.1441e-06, -2.4885e-06,  2.7269e-06,  ..., -2.7120e-06,
         -2.2948e-06, -1.8850e-06]], device='cuda:0')
Loss: 0.9780000448226929


Running epoch 1, step 1673, batch 625
Sampled inputs[:2]: tensor([[   0,  342,  516,  ...,   12,  729, 3701],
        [   0, 3756,   13,  ..., 1704,  278, 5851]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4261e-04, -1.4022e-04, -1.9904e-04,  ..., -5.4841e-05,
         -6.1028e-05, -1.5246e-04],
        [-2.7940e-06, -2.1234e-06,  2.3469e-06,  ..., -2.3469e-06,
         -1.9595e-06, -1.8403e-06],
        [-7.7039e-06, -6.0499e-06,  6.9141e-06,  ..., -6.4373e-06,
         -5.3942e-06, -5.1260e-06],
        [-5.4240e-06, -4.0531e-06,  4.7982e-06,  ..., -4.5300e-06,
         -3.7700e-06, -3.7700e-06],
        [-6.3330e-06, -5.2303e-06,  5.5283e-06,  ..., -5.4389e-06,
         -4.7833e-06, -3.8669e-06]], device='cuda:0')
Loss: 0.9877558350563049


Running epoch 1, step 1674, batch 626
Sampled inputs[:2]: tensor([[    0,  3169, 12186,  ...,   940,   271, 13929],
        [    0,   300, 16683,  ...,  8709,    40,  9817]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.7209e-05, -1.9835e-04, -3.7075e-04,  ..., -1.2994e-04,
         -1.0132e-04, -2.1299e-04],
        [-4.2319e-06, -3.1069e-06,  3.4720e-06,  ..., -3.5241e-06,
         -2.9132e-06, -2.7753e-06],
        [-1.1697e-05, -8.8662e-06,  1.0207e-05,  ..., -9.6709e-06,
         -8.0317e-06, -7.7188e-06],
        [-8.2552e-06, -5.9530e-06,  7.0930e-06,  ..., -6.8396e-06,
         -5.6401e-06, -5.7071e-06],
        [-9.5814e-06, -7.6592e-06,  8.1509e-06,  ..., -8.1509e-06,
         -7.0781e-06, -5.8189e-06]], device='cuda:0')
Loss: 0.9537846446037292


Running epoch 1, step 1675, batch 627
Sampled inputs[:2]: tensor([[   0, 4538,  271,  ..., 1603,  591,  688],
        [   0, 6132,  300,  ...,   37,  271,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7701e-05, -1.3401e-04, -4.5557e-04,  ..., -1.0564e-04,
         -1.9907e-04, -2.0892e-04],
        [-5.6773e-06, -4.2096e-06,  4.5523e-06,  ..., -4.7535e-06,
         -3.9935e-06, -3.6806e-06],
        [-1.5661e-05, -1.2055e-05,  1.3381e-05,  ..., -1.3024e-05,
         -1.1027e-05, -1.0192e-05],
        [-1.0982e-05, -8.0541e-06,  9.2238e-06,  ..., -9.1642e-06,
         -7.7114e-06, -7.5176e-06],
        [-1.2890e-05, -1.0461e-05,  1.0759e-05,  ..., -1.1027e-05,
         -9.7454e-06, -7.7263e-06]], device='cuda:0')
Loss: 0.9475585222244263


Running epoch 1, step 1676, batch 628
Sampled inputs[:2]: tensor([[    0,  3941,   257,  ...,    50,   699, 13374],
        [    0,   278,   795,  ...,  1774, 14474,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.3180e-04, -1.5126e-04, -6.0812e-04,  ..., -8.9452e-05,
         -3.6822e-05, -1.9154e-04],
        [-7.0408e-06, -5.1074e-06,  5.5432e-06,  ..., -6.0201e-06,
         -4.9025e-06, -4.7609e-06],
        [-1.9446e-05, -1.4588e-05,  1.6347e-05,  ..., -1.6436e-05,
         -1.3456e-05, -1.3053e-05],
        [-1.3635e-05, -9.7454e-06,  1.1221e-05,  ..., -1.1608e-05,
         -9.4548e-06, -9.6634e-06],
        [-1.6034e-05, -1.2636e-05,  1.3173e-05,  ..., -1.3888e-05,
         -1.1891e-05, -9.9018e-06]], device='cuda:0')
Loss: 0.9304923415184021


Running epoch 1, step 1677, batch 629
Sampled inputs[:2]: tensor([[    0,  7314,    19,  ...,  8350,   365, 13801],
        [    0,  3665,  1419,  ...,   600,   847,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3675e-05, -1.4335e-04, -6.3868e-04,  ..., -1.0358e-04,
         -8.2968e-05, -2.1123e-04],
        [-8.4266e-06, -6.1132e-06,  6.5938e-06,  ..., -7.2569e-06,
         -5.9232e-06, -5.7369e-06],
        [-2.3142e-05, -1.7405e-05,  1.9401e-05,  ..., -1.9699e-05,
         -1.6168e-05, -1.5616e-05],
        [-1.6242e-05, -1.1623e-05,  1.3307e-05,  ..., -1.3933e-05,
         -1.1392e-05, -1.1586e-05],
        [-1.9178e-05, -1.5140e-05,  1.5691e-05,  ..., -1.6719e-05,
         -1.4350e-05, -1.1899e-05]], device='cuda:0')
Loss: 0.9620970487594604


Running epoch 1, step 1678, batch 630
Sampled inputs[:2]: tensor([[    0,    69, 27768,  ...,  1869,  1566,   367],
        [    0,   510,    13,  ...,  3454,   513,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5965e-06,  1.1683e-04, -3.7622e-04,  ..., -7.9492e-05,
         -1.4133e-04, -2.6016e-04],
        [-9.7975e-06, -7.1339e-06,  7.4245e-06,  ..., -8.5086e-06,
         -7.0110e-06, -6.7651e-06],
        [-2.6956e-05, -2.0415e-05,  2.1994e-05,  ..., -2.3127e-05,
         -1.9148e-05, -1.8433e-05],
        [-1.8895e-05, -1.3575e-05,  1.4976e-05,  ..., -1.6361e-05,
         -1.3523e-05, -1.3687e-05],
        [-2.2501e-05, -1.7837e-05,  1.7852e-05,  ..., -1.9774e-05,
         -1.7107e-05, -1.4149e-05]], device='cuda:0')
Loss: 0.9126735925674438


Running epoch 1, step 1679, batch 631
Sampled inputs[:2]: tensor([[    0,    20,  2637,  ..., 14044,     9,   292],
        [    0,   850,    13,  ..., 11823,    13, 30706]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8806e-05,  1.6218e-04, -2.8789e-04,  ...,  2.8324e-05,
         -1.3513e-05, -3.3302e-05],
        [-1.1183e-05, -8.2441e-06,  8.4080e-06,  ..., -9.7677e-06,
         -8.0764e-06, -7.7561e-06],
        [-3.0801e-05, -2.3648e-05,  2.4930e-05,  ..., -2.6599e-05,
         -2.2128e-05, -2.1175e-05],
        [-2.1622e-05, -1.5751e-05,  1.6987e-05,  ..., -1.8850e-05,
         -1.5669e-05, -1.5728e-05],
        [-2.5764e-05, -2.0698e-05,  2.0310e-05,  ..., -2.2784e-05,
         -1.9774e-05, -1.6324e-05]], device='cuda:0')
Loss: 0.9688712358474731
Graident accumulation at epoch 1, step 1679, batch 631
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9405e-05,  3.2264e-05, -6.0254e-05,  ..., -4.3227e-06,
         -4.4674e-05,  8.4404e-05],
        [-9.3836e-06, -6.3175e-06,  6.5300e-06,  ..., -8.6074e-06,
         -6.0041e-06, -6.3833e-06],
        [-3.3205e-07,  4.2085e-06,  3.0874e-06,  ...,  5.5793e-06,
          1.0332e-06, -3.3941e-06],
        [-7.9161e-06,  3.0338e-06,  4.8725e-06,  ..., -3.5007e-06,
          2.8031e-06, -7.2256e-06],
        [-2.6182e-05, -2.1010e-05,  1.9703e-05,  ..., -2.3697e-05,
         -2.0127e-05, -1.6785e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8796e-08, 5.6061e-08, 5.4701e-08,  ..., 2.1138e-08, 1.2402e-07,
         4.8760e-08],
        [9.0392e-11, 5.9567e-11, 2.6695e-11,  ..., 6.2280e-11, 3.2596e-11,
         3.0111e-11],
        [2.0566e-09, 1.4296e-09, 6.8197e-10,  ..., 1.5901e-09, 8.9715e-10,
         5.3315e-10],
        [8.9237e-10, 1.0847e-09, 3.9400e-10,  ..., 8.9314e-10, 6.7841e-10,
         3.6908e-10],
        [4.2246e-10, 2.4626e-10, 9.2919e-11,  ..., 3.0978e-10, 9.2815e-11,
         1.2685e-10]], device='cuda:0')
optimizer state dict: 210.0
lr: [1.995833666043061e-06, 1.995833666043061e-06]
scheduler_last_epoch: 210


Running epoch 1, step 1680, batch 632
Sampled inputs[:2]: tensor([[   0, 3860,  694,  ..., 1027,  292,  221],
        [   0, 2380, 2667,  ...,   14,  381, 5621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5138e-05, -5.4071e-05, -1.0076e-04,  ...,  1.0708e-04,
          6.6908e-05, -8.2413e-05],
        [-1.4007e-06, -9.1642e-07,  1.0729e-06,  ..., -1.3039e-06,
         -1.0133e-06, -1.0505e-06],
        [-3.6955e-06, -2.5034e-06,  3.0100e-06,  ..., -3.3379e-06,
         -2.6226e-06, -2.6524e-06],
        [-2.7120e-06, -1.7211e-06,  2.1458e-06,  ..., -2.5183e-06,
         -1.9670e-06, -2.1160e-06],
        [-3.2932e-06, -2.2948e-06,  2.6226e-06,  ..., -2.9951e-06,
         -2.4289e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9914727210998535


Running epoch 1, step 1681, batch 633
Sampled inputs[:2]: tensor([[   0,   14,  381,  ..., 2195,  278,  266],
        [   0,  271,  266,  ..., 3795,  908,  587]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2107e-04, -2.0334e-04, -2.8660e-04,  ...,  4.1415e-04,
          3.1354e-04, -2.4753e-04],
        [-2.8089e-06, -1.7732e-06,  1.8775e-06,  ..., -2.6226e-06,
         -2.0266e-06, -2.2054e-06],
        [-7.5400e-06, -4.9472e-06,  5.4091e-06,  ..., -6.8545e-06,
         -5.3644e-06, -5.6922e-06],
        [-5.4240e-06, -3.3006e-06,  3.6955e-06,  ..., -5.0962e-06,
         -3.9786e-06, -4.4107e-06],
        [-6.7204e-06, -4.5300e-06,  4.7237e-06,  ..., -6.1542e-06,
         -4.9621e-06, -4.6790e-06]], device='cuda:0')
Loss: 0.9036197662353516


Running epoch 1, step 1682, batch 634
Sampled inputs[:2]: tensor([[   0,   15, 4291,  ..., 1685,  278, 2101],
        [   0, 1890,  278,  ..., 1400,  367, 1874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4677e-04, -2.2821e-04, -3.8901e-04,  ...,  3.3848e-04,
          3.4493e-04, -4.1370e-04],
        [-4.1351e-06, -2.7418e-06,  3.0100e-06,  ..., -3.7625e-06,
         -2.9393e-06, -3.1292e-06],
        [-1.1265e-05, -7.7188e-06,  8.7917e-06,  ..., -9.9838e-06,
         -7.8529e-06, -8.2552e-06],
        [-8.0466e-06, -5.1633e-06,  6.0499e-06,  ..., -7.3314e-06,
         -5.7667e-06, -6.3181e-06],
        [-9.7454e-06, -6.9141e-06,  7.3910e-06,  ..., -8.7619e-06,
         -7.1377e-06, -6.5714e-06]], device='cuda:0')
Loss: 0.954497754573822


Running epoch 1, step 1683, batch 635
Sampled inputs[:2]: tensor([[    0,  1874,   300,  ...,    14,  5372,    12],
        [    0,  3256,   221,  ..., 18116,   292, 47989]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7019e-04, -1.6298e-04, -3.9400e-04,  ...,  2.9771e-04,
          4.4588e-04, -2.8095e-04],
        [-5.6848e-06, -3.7774e-06,  3.9712e-06,  ..., -5.1111e-06,
         -4.0345e-06, -4.2021e-06],
        [-1.5259e-05, -1.0639e-05,  1.1548e-05,  ..., -1.3471e-05,
         -1.0788e-05, -1.0982e-05],
        [-1.0833e-05, -7.0557e-06,  7.8604e-06,  ..., -9.8050e-06,
         -7.8380e-06, -8.3297e-06],
        [-1.3337e-05, -9.6262e-06,  9.8199e-06,  ..., -1.1966e-05,
         -9.9093e-06, -8.8662e-06]], device='cuda:0')
Loss: 0.9310546517372131


Running epoch 1, step 1684, batch 636
Sampled inputs[:2]: tensor([[   0,  995,   13,  ..., 2192, 2534,  287],
        [   0,  277,  279,  ...,   12,  287,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0381e-04, -3.2468e-04, -4.3038e-04,  ...,  2.9355e-04,
          3.5254e-04, -3.9929e-04],
        [-7.1302e-06, -4.7758e-06,  5.0589e-06,  ..., -6.3404e-06,
         -5.0478e-06, -5.1409e-06],
        [-1.9073e-05, -1.3426e-05,  1.4693e-05,  ..., -1.6689e-05,
         -1.3471e-05, -1.3456e-05],
        [-1.3515e-05, -8.8960e-06,  1.0006e-05,  ..., -1.2085e-05,
         -9.7230e-06, -1.0170e-05],
        [-1.6570e-05, -1.2144e-05,  1.2413e-05,  ..., -1.4782e-05,
         -1.2383e-05, -1.0788e-05]], device='cuda:0')
Loss: 0.94168621301651


Running epoch 1, step 1685, batch 637
Sampled inputs[:2]: tensor([[    0,    13, 36961,  ...,  6671, 13711,  4568],
        [    0,  6067,  1188,  ...,  5282,   756,   342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.6472e-04, -2.8379e-04, -3.3401e-04,  ...,  1.9447e-04,
          3.5072e-04, -4.1508e-04],
        [-8.4862e-06, -5.8040e-06,  6.0648e-06,  ..., -7.6070e-06,
         -6.1654e-06, -6.0871e-06],
        [-2.2724e-05, -1.6332e-05,  1.7583e-05,  ..., -2.0102e-05,
         -1.6510e-05, -1.6004e-05],
        [-1.6063e-05, -1.0833e-05,  1.1973e-05,  ..., -1.4499e-05,
         -1.1884e-05, -1.2040e-05],
        [-1.9759e-05, -1.4842e-05,  1.4886e-05,  ..., -1.7852e-05,
         -1.5229e-05, -1.2875e-05]], device='cuda:0')
Loss: 0.9778249859809875


Running epoch 1, step 1686, batch 638
Sampled inputs[:2]: tensor([[   0, 1340, 1049,  ..., 1441, 1211, 4165],
        [   0, 2025,  287,  ...,  381, 1487, 3506]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8201e-04, -3.9731e-04, -1.3910e-04,  ...,  1.6786e-04,
          5.0361e-04, -3.8030e-04],
        [-9.8497e-06, -7.0035e-06,  7.1600e-06,  ..., -8.8364e-06,
         -7.2978e-06, -6.9737e-06],
        [-2.6539e-05, -1.9863e-05,  2.0802e-05,  ..., -2.3559e-05,
         -1.9729e-05, -1.8522e-05],
        [-1.8731e-05, -1.3158e-05,  1.4178e-05,  ..., -1.6883e-05,
         -1.4089e-05, -1.3858e-05],
        [-2.2978e-05, -1.7926e-05,  1.7554e-05,  ..., -2.0832e-05,
         -1.8075e-05, -1.4842e-05]], device='cuda:0')
Loss: 1.017342209815979


Running epoch 1, step 1687, batch 639
Sampled inputs[:2]: tensor([[    0,   221,   380,  ...,   630,  3765, 19107],
        [    0,   334,   344,  ...,   266,  4141,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4987e-04, -4.1059e-04, -1.2267e-04,  ...,  2.8228e-04,
          5.0823e-04, -3.9553e-04],
        [-1.1265e-05, -8.0392e-06,  8.2105e-06,  ..., -1.0051e-05,
         -8.2739e-06, -7.8976e-06],
        [-3.0503e-05, -2.2858e-05,  2.3946e-05,  ..., -2.6911e-05,
         -2.2426e-05, -2.1100e-05],
        [-2.1532e-05, -1.5154e-05,  1.6339e-05,  ..., -1.9282e-05,
         -1.5996e-05, -1.5780e-05],
        [-2.6241e-05, -2.0534e-05,  2.0087e-05,  ..., -2.3663e-05,
         -2.0459e-05, -1.6809e-05]], device='cuda:0')
Loss: 0.9679155349731445
Graident accumulation at epoch 1, step 1687, batch 639
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0401,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4772e-06, -1.2021e-05, -6.6495e-05,  ...,  2.4337e-05,
          1.0616e-05,  3.6411e-05],
        [-9.5717e-06, -6.4896e-06,  6.6981e-06,  ..., -8.7517e-06,
         -6.2311e-06, -6.5348e-06],
        [-3.3491e-06,  1.5018e-06,  5.1733e-06,  ...,  2.3302e-06,
         -1.3128e-06, -5.1647e-06],
        [-9.2777e-06,  1.2149e-06,  6.0192e-06,  ..., -5.0788e-06,
          9.2312e-07, -8.0810e-06],
        [-2.6188e-05, -2.0962e-05,  1.9741e-05,  ..., -2.3693e-05,
         -2.0160e-05, -1.6787e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8760e-08, 5.6174e-08, 5.4661e-08,  ..., 2.1197e-08, 1.2415e-07,
         4.8867e-08],
        [9.0429e-11, 5.9572e-11, 2.6735e-11,  ..., 6.2319e-11, 3.2631e-11,
         3.0143e-11],
        [2.0555e-09, 1.4287e-09, 6.8186e-10,  ..., 1.5892e-09, 8.9675e-10,
         5.3306e-10],
        [8.9194e-10, 1.0839e-09, 3.9387e-10,  ..., 8.9261e-10, 6.7799e-10,
         3.6896e-10],
        [4.2272e-10, 2.4644e-10, 9.3229e-11,  ..., 3.1003e-10, 9.3141e-11,
         1.2701e-10]], device='cuda:0')
optimizer state dict: 211.0
lr: [1.922345857645641e-06, 1.922345857645641e-06]
scheduler_last_epoch: 211
Epoch 1 | Batch 639/1048 | Training PPL: 2306.125862507577 | time 66.59449291229248
Saving checkpoint at epoch 1, step 1687, batch 639
Epoch 1 | Validation PPL: 6.747768610488709 | Learning rate: 1.922345857645641e-06
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1687, AFTER epoch 1, step 1687


Running epoch 1, step 1688, batch 640
Sampled inputs[:2]: tensor([[   0, 4110,  271,  ...,  944,  278, 3230],
        [   0,  266, 1254,  ...,  369, 2870,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5047e-05, -1.0796e-04, -2.1545e-04,  ...,  4.6318e-05,
          5.8670e-05, -2.9732e-04],
        [-1.3933e-06, -9.7603e-07,  1.1101e-06,  ..., -1.1697e-06,
         -9.4250e-07, -9.4995e-07],
        [ 6.3280e-05,  8.3925e-05, -3.9101e-05,  ...,  5.0664e-05,
          5.7508e-05,  3.9663e-05],
        [-2.7865e-06, -1.9372e-06,  2.3544e-06,  ..., -2.3246e-06,
         -1.8924e-06, -1.9819e-06],
        [-3.2037e-06, -2.4885e-06,  2.6971e-06,  ..., -2.7418e-06,
         -2.3842e-06, -1.9670e-06]], device='cuda:0')
Loss: 0.9921216368675232


Running epoch 1, step 1689, batch 641
Sampled inputs[:2]: tensor([[    0,   767,  1615,  ...,  2952,  1760,     9],
        [    0,  1235,   368,  ..., 12152,  8498,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5973e-04, -1.1581e-04, -3.0146e-04,  ..., -5.8517e-05,
          2.6033e-05, -1.8945e-04],
        [-2.8312e-06, -2.0415e-06,  2.3320e-06,  ..., -2.3693e-06,
         -1.8887e-06, -1.8626e-06],
        [ 5.9287e-05,  8.0885e-05, -3.5539e-05,  ...,  4.7326e-05,
          5.4871e-05,  3.7100e-05],
        [-5.6475e-06, -4.0084e-06,  4.8876e-06,  ..., -4.6939e-06,
         -3.7551e-06, -3.8818e-06],
        [-6.4969e-06, -5.1409e-06,  5.5730e-06,  ..., -5.5581e-06,
         -4.7386e-06, -3.9190e-06]], device='cuda:0')
Loss: 0.999732494354248


Running epoch 1, step 1690, batch 642
Sampled inputs[:2]: tensor([[   0,    9,  391,  ...,  300, 2646, 1717],
        [   0,   16,   14,  ..., 5148,  259, 1951]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8072e-04, -1.9982e-04, -4.0017e-04,  ..., -1.0288e-06,
         -6.0224e-05, -1.7725e-04],
        [-4.2021e-06, -3.0026e-06,  3.3453e-06,  ..., -3.5837e-06,
         -2.8349e-06, -2.7865e-06],
        [ 5.5591e-05,  7.8143e-05, -3.2559e-05,  ...,  4.4063e-05,
          5.2278e-05,  3.4657e-05],
        [-8.2850e-06, -5.8636e-06,  6.9439e-06,  ..., -7.0632e-06,
         -5.6177e-06, -5.7593e-06],
        [-9.5963e-06, -7.5400e-06,  8.0168e-06,  ..., -8.3297e-06,
         -7.0482e-06, -5.7966e-06]], device='cuda:0')
Loss: 0.9613182544708252


Running epoch 1, step 1691, batch 643
Sampled inputs[:2]: tensor([[   0,  278, 4575,  ..., 1220,  278, 4575],
        [   0,  292, 3030,  ..., 1231, 2156,  266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8413e-04, -1.5603e-04, -3.6507e-04,  ..., -4.7578e-05,
         -1.3427e-05, -2.1181e-04],
        [-5.5358e-06, -4.0606e-06,  4.4405e-06,  ..., -4.7460e-06,
         -3.8482e-06, -3.7141e-06],
        [ 5.2015e-05,  7.5223e-05, -2.9430e-05,  ...,  4.0964e-05,
          4.9566e-05,  3.2213e-05],
        [-1.0893e-05, -7.8902e-06,  9.1791e-06,  ..., -9.3281e-06,
         -7.5847e-06, -7.6666e-06],
        [-1.2711e-05, -1.0207e-05,  1.0669e-05,  ..., -1.1086e-05,
         -9.5516e-06, -7.7486e-06]], device='cuda:0')
Loss: 0.9819587469100952


Running epoch 1, step 1692, batch 644
Sampled inputs[:2]: tensor([[    0,   292,    33,  ...,   352,   266,  9129],
        [    0,  1387,   369,  ..., 15722,    14,  8157]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.6584e-05, -1.4072e-04, -2.4108e-04,  ..., -3.7276e-05,
         -1.6982e-04, -1.5949e-04],
        [-6.9141e-06, -5.0664e-06,  5.4538e-06,  ..., -6.0126e-06,
         -4.8913e-06, -4.7497e-06],
        [ 4.8305e-05,  7.2391e-05, -2.6479e-05,  ...,  3.7611e-05,
          4.6750e-05,  2.9501e-05],
        [-1.3471e-05, -9.7528e-06,  1.1161e-05,  ..., -1.1697e-05,
         -9.5665e-06, -9.6634e-06],
        [-1.6108e-05, -1.2845e-05,  1.3217e-05,  ..., -1.4201e-05,
         -1.2219e-05, -1.0058e-05]], device='cuda:0')
Loss: 0.9360232353210449


Running epoch 1, step 1693, batch 645
Sampled inputs[:2]: tensor([[    0,  2663,    12,  ..., 24113,   497,    14],
        [    0,  2140,    12,  ...,   696,   688,  1998]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.1822e-05, -4.9540e-05, -2.9879e-04,  ..., -1.2743e-04,
         -1.2358e-04, -1.5113e-04],
        [-8.3447e-06, -6.0648e-06,  6.6459e-06,  ..., -7.1824e-06,
         -5.7966e-06, -5.6550e-06],
        [ 4.4371e-05,  6.9486e-05, -2.2963e-05,  ...,  3.4377e-05,
          4.4187e-05,  2.6997e-05],
        [-1.6287e-05, -1.1720e-05,  1.3635e-05,  ..., -1.4007e-05,
         -1.1377e-05, -1.1533e-05],
        [-1.9237e-05, -1.5303e-05,  1.5959e-05,  ..., -1.6853e-05,
         -1.4439e-05, -1.1891e-05]], device='cuda:0')
Loss: 0.9648296236991882


Running epoch 1, step 1694, batch 646
Sampled inputs[:2]: tensor([[    0,  5685,   565,  ..., 23968,    14,   381],
        [    0,  2544,   394,  ...,    14,  1062,   516]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5414e-04, -2.4298e-04, -1.2611e-04,  ..., -2.1278e-05,
          2.6173e-04,  3.8645e-05],
        [-9.7007e-06, -7.0706e-06,  7.6592e-06,  ..., -8.4415e-06,
         -6.8769e-06, -6.6534e-06],
        [ 4.0601e-05,  6.6565e-05, -1.9982e-05,  ...,  3.0950e-05,
          4.1221e-05,  2.4270e-05],
        [-1.8999e-05, -1.3717e-05,  1.5721e-05,  ..., -1.6525e-05,
         -1.3582e-05, -1.3649e-05],
        [-2.2486e-05, -1.7911e-05,  1.8492e-05,  ..., -1.9863e-05,
         -1.7121e-05, -1.4052e-05]], device='cuda:0')
Loss: 0.979718029499054


Running epoch 1, step 1695, batch 647
Sampled inputs[:2]: tensor([[    0,  2738,   278,  ...,   292,    35,  2147],
        [    0, 17262,   342,  ...,   472,   346,   462]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.0071e-05, -2.6619e-04, -2.7605e-04,  ..., -5.5042e-05,
          2.8993e-04, -3.2539e-05],
        [-1.1086e-05, -8.1211e-06,  8.7693e-06,  ..., -9.6485e-06,
         -7.8529e-06, -7.6368e-06],
        [ 1.3580e-04,  2.3324e-04, -1.0480e-04,  ...,  8.4456e-05,
          1.8934e-04,  6.6655e-05],
        [-2.1607e-05, -1.5654e-05,  1.7911e-05,  ..., -1.8775e-05,
         -1.5423e-05, -1.5572e-05],
        [-2.5675e-05, -2.0489e-05,  2.1145e-05,  ..., -2.2650e-05,
         -1.9506e-05, -1.6049e-05]], device='cuda:0')
Loss: 0.956486701965332
Graident accumulation at epoch 1, step 1695, batch 647
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-4.7776e-06, -3.7437e-05, -8.7450e-05,  ...,  1.6399e-05,
          3.8548e-05,  2.9516e-05],
        [-9.7232e-06, -6.6528e-06,  6.9052e-06,  ..., -8.8414e-06,
         -6.3933e-06, -6.6450e-06],
        [ 1.0566e-05,  2.4676e-05, -5.8241e-06,  ...,  1.0543e-05,
          1.7752e-05,  2.0173e-06],
        [-1.0511e-05, -4.7192e-07,  7.2084e-06,  ..., -6.4485e-06,
         -7.1146e-07, -8.8301e-06],
        [-2.6137e-05, -2.0915e-05,  1.9882e-05,  ..., -2.3589e-05,
         -2.0095e-05, -1.6714e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8706e-08, 5.6189e-08, 5.4682e-08,  ..., 2.1179e-08, 1.2411e-07,
         4.8819e-08],
        [9.0461e-11, 5.9578e-11, 2.6785e-11,  ..., 6.2350e-11, 3.2660e-11,
         3.0172e-11],
        [2.0719e-09, 1.4817e-09, 6.9216e-10,  ..., 1.5947e-09, 9.3171e-10,
         5.3697e-10],
        [8.9152e-10, 1.0830e-09, 3.9380e-10,  ..., 8.9207e-10, 6.7755e-10,
         3.6883e-10],
        [4.2296e-10, 2.4661e-10, 9.3583e-11,  ..., 3.1023e-10, 9.3428e-11,
         1.2714e-10]], device='cuda:0')
optimizer state dict: 212.0
lr: [1.8500923853120123e-06, 1.8500923853120123e-06]
scheduler_last_epoch: 212


Running epoch 1, step 1696, batch 648
Sampled inputs[:2]: tensor([[    0, 44175,   744,  ..., 16394, 26528,    12],
        [    0,   996,  2226,  ...,   516,  3470,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1829e-04,  1.3742e-04, -4.5225e-05,  ..., -2.1899e-05,
         -1.0616e-05, -1.1008e-04],
        [-1.3188e-06, -8.6799e-07,  1.1250e-06,  ..., -1.1176e-06,
         -8.9407e-07, -9.4250e-07],
        [-3.4273e-06, -2.3842e-06,  3.1888e-06,  ..., -2.8312e-06,
         -2.3395e-06, -2.3246e-06],
        [-2.5481e-06, -1.6391e-06,  2.3395e-06,  ..., -2.1458e-06,
         -1.7583e-06, -1.8850e-06],
        [-2.8759e-06, -2.0862e-06,  2.5779e-06,  ..., -2.4140e-06,
         -2.1011e-06, -1.7658e-06]], device='cuda:0')
Loss: 0.9070693254470825


Running epoch 1, step 1697, batch 649
Sampled inputs[:2]: tensor([[   0,  923,   13,  ...,  300, 8262,   12],
        [   0,  879,   27,  ...,   13, 2764, 3860]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7330e-07,  2.2753e-06, -2.1696e-05,  ...,  7.9037e-05,
          2.4213e-04,  2.6166e-04],
        [-2.7120e-06, -1.8887e-06,  2.1607e-06,  ..., -2.3693e-06,
         -1.9222e-06, -1.9334e-06],
        [-7.2271e-06, -5.3942e-06,  6.2436e-06,  ..., -6.2585e-06,
         -5.2005e-06, -5.0068e-06],
        [-5.3495e-06, -3.6955e-06,  4.5151e-06,  ..., -4.6641e-06,
         -3.8594e-06, -3.9861e-06],
        [-6.1840e-06, -4.8131e-06,  5.1409e-06,  ..., -5.4687e-06,
         -4.7535e-06, -3.9414e-06]], device='cuda:0')
Loss: 0.9858054518699646


Running epoch 1, step 1698, batch 650
Sampled inputs[:2]: tensor([[    0,   266,  1527,  ...,  2525,    14, 11570],
        [    0,  2577,   995,  ...,  6104,    14,  2032]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.4740e-05,  1.1870e-05, -6.8459e-05,  ...,  2.6719e-05,
          8.6891e-05,  2.8933e-04],
        [-4.1649e-06, -2.8946e-06,  3.4049e-06,  ..., -3.5986e-06,
         -2.8573e-06, -2.9467e-06],
        [-1.1340e-05, -8.3745e-06,  9.9689e-06,  ..., -9.7305e-06,
         -7.8827e-06, -7.9125e-06],
        [-8.1658e-06, -5.6326e-06,  7.0333e-06,  ..., -7.0333e-06,
         -5.6699e-06, -6.0424e-06],
        [-9.5069e-06, -7.3761e-06,  8.0764e-06,  ..., -8.3745e-06,
         -7.1228e-06, -6.1169e-06]], device='cuda:0')
Loss: 0.9566689729690552


Running epoch 1, step 1699, batch 651
Sampled inputs[:2]: tensor([[    0, 28684,   472,  ...,   317,     9,  1926],
        [    0, 27366,   504,  ...,  1358,   365,  6883]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6620e-05,  4.1114e-05, -3.3370e-04,  ...,  6.8583e-05,
          1.5414e-04,  1.9476e-04],
        [-5.4836e-06, -3.8445e-06,  4.5002e-06,  ..., -4.7684e-06,
         -3.8184e-06, -3.8482e-06],
        [-1.4886e-05, -1.1027e-05,  1.3158e-05,  ..., -1.2830e-05,
         -1.0461e-05, -1.0237e-05],
        [-1.0759e-05, -7.4729e-06,  9.3132e-06,  ..., -9.3281e-06,
         -7.5772e-06, -7.8902e-06],
        [-1.2487e-05, -9.7603e-06,  1.0699e-05,  ..., -1.1042e-05,
         -9.4324e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.967700719833374


Running epoch 1, step 1700, batch 652
Sampled inputs[:2]: tensor([[   0,   25,    5,  ..., 3935,   14,   16],
        [   0,  792,   83,  ...,  300,  768,  932]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2654e-04,  2.2521e-05, -3.9149e-04,  ...,  3.1457e-05,
         -3.7518e-05,  1.3393e-04],
        [-6.8471e-06, -4.7982e-06,  5.6103e-06,  ..., -5.9232e-06,
         -4.7944e-06, -4.7497e-06],
        [-1.8522e-05, -1.3724e-05,  1.6376e-05,  ..., -1.5900e-05,
         -1.3113e-05, -1.2621e-05],
        [-1.3441e-05, -9.3356e-06,  1.1638e-05,  ..., -1.1593e-05,
         -9.5442e-06, -9.7677e-06],
        [-1.5542e-05, -1.2174e-05,  1.3337e-05,  ..., -1.3709e-05,
         -1.1846e-05, -9.7305e-06]], device='cuda:0')
Loss: 0.9564216732978821


Running epoch 1, step 1701, batch 653
Sampled inputs[:2]: tensor([[   0, 2388, 6604,  ..., 5005, 1196,  717],
        [   0,  765,  292,  ...,  623,   12, 7117]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5364e-04,  2.0819e-04, -6.3353e-04,  ...,  9.2000e-05,
         -1.7521e-04,  1.3805e-04],
        [-8.1733e-06, -5.5693e-06,  6.6757e-06,  ..., -7.0930e-06,
         -5.6401e-06, -5.7109e-06],
        [-2.2233e-05, -1.5989e-05,  1.9595e-05,  ..., -1.9088e-05,
         -1.5438e-05, -1.5214e-05],
        [-1.6123e-05, -1.0848e-05,  1.3888e-05,  ..., -1.3962e-05,
         -1.1273e-05, -1.1809e-05],
        [-1.8492e-05, -1.4126e-05,  1.5825e-05,  ..., -1.6302e-05,
         -1.3828e-05, -1.1615e-05]], device='cuda:0')
Loss: 0.9367856979370117


Running epoch 1, step 1702, batch 654
Sampled inputs[:2]: tensor([[   0,  287, 1070,  ...,  292,  221,  374],
        [   0,  328, 2097,  ...,  365, 1941,  607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2769e-04,  2.5960e-04, -5.5227e-04,  ...,  7.7907e-05,
          6.9656e-05,  2.1744e-04],
        [-9.5144e-06, -6.4857e-06,  7.5772e-06,  ..., -8.3894e-06,
         -6.7353e-06, -6.7316e-06],
        [-2.5809e-05, -1.8612e-05,  2.2233e-05,  ..., -2.2486e-05,
         -1.8343e-05, -1.7866e-05],
        [-1.8731e-05, -1.2599e-05,  1.5706e-05,  ..., -1.6510e-05,
         -1.3448e-05, -1.3895e-05],
        [-2.1815e-05, -1.6630e-05,  1.8209e-05,  ..., -1.9461e-05,
         -1.6600e-05, -1.3880e-05]], device='cuda:0')
Loss: 0.9600739479064941


Running epoch 1, step 1703, batch 655
Sampled inputs[:2]: tensor([[    0,  3577,    12,  ...,  4222,  2137, 31332],
        [    0,  2652,   271,  ...,   634,  1921,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0775e-04,  4.0074e-04, -5.4173e-04,  ...,  8.1249e-05,
          3.0260e-05,  2.5512e-04],
        [-1.0878e-05, -7.4841e-06,  8.6352e-06,  ..., -9.5665e-06,
         -7.7710e-06, -7.7076e-06],
        [-2.9579e-05, -2.1532e-05,  2.5406e-05,  ..., -2.5734e-05,
         -2.1219e-05, -2.0549e-05],
        [-2.1428e-05, -1.4566e-05,  1.7896e-05,  ..., -1.8865e-05,
         -1.5534e-05, -1.5907e-05],
        [-2.4959e-05, -1.9178e-05,  2.0787e-05,  ..., -2.2247e-05,
         -1.9163e-05, -1.5952e-05]], device='cuda:0')
Loss: 0.9549299478530884
Graident accumulation at epoch 1, step 1703, batch 655
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0292, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.4755e-06,  6.3804e-06, -1.3288e-04,  ...,  2.2884e-05,
          3.7719e-05,  5.2076e-05],
        [-9.8387e-06, -6.7359e-06,  7.0782e-06,  ..., -8.9139e-06,
         -6.5311e-06, -6.7512e-06],
        [ 6.5516e-06,  2.0055e-05, -2.7011e-06,  ...,  6.9151e-06,
          1.3855e-05, -2.3934e-07],
        [-1.1602e-05, -1.8813e-06,  8.2772e-06,  ..., -7.6901e-06,
         -2.1938e-06, -9.5378e-06],
        [-2.6019e-05, -2.0741e-05,  1.9972e-05,  ..., -2.3455e-05,
         -2.0002e-05, -1.6637e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8659e-08, 5.6293e-08, 5.4921e-08,  ..., 2.1164e-08, 1.2399e-07,
         4.8836e-08],
        [9.0489e-11, 5.9575e-11, 2.6833e-11,  ..., 6.2379e-11, 3.2688e-11,
         3.0201e-11],
        [2.0707e-09, 1.4807e-09, 6.9211e-10,  ..., 1.5938e-09, 9.3122e-10,
         5.3686e-10],
        [8.9109e-10, 1.0822e-09, 3.9373e-10,  ..., 8.9154e-10, 6.7711e-10,
         3.6872e-10],
        [4.2316e-10, 2.4673e-10, 9.3922e-11,  ..., 3.1042e-10, 9.3702e-11,
         1.2727e-10]], device='cuda:0')
optimizer state dict: 213.0
lr: [1.7790842900034565e-06, 1.7790842900034565e-06]
scheduler_last_epoch: 213


Running epoch 1, step 1704, batch 656
Sampled inputs[:2]: tensor([[   0,   18,   14,  ...,  446,  747, 1193],
        [   0,   19,   14,  ...,  278, 2588,  944]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.3839e-05,  1.9287e-04,  4.3778e-05,  ..., -5.4751e-05,
         -2.5387e-05,  7.4529e-06],
        [-1.4007e-06, -1.0580e-06,  1.0431e-06,  ..., -1.2740e-06,
         -1.0580e-06, -9.7603e-07],
        [-3.8743e-06, -3.0994e-06,  3.0845e-06,  ..., -3.5465e-06,
         -2.9951e-06, -2.7120e-06],
        [-2.7120e-06, -2.0415e-06,  2.1011e-06,  ..., -2.4885e-06,
         -2.1011e-06, -1.9819e-06],
        [-3.3081e-06, -2.7716e-06,  2.6077e-06,  ..., -3.0845e-06,
         -2.6971e-06, -2.1458e-06]], device='cuda:0')
Loss: 0.958133339881897


Running epoch 1, step 1705, batch 657
Sampled inputs[:2]: tensor([[    0,    15,    72,  ...,   380, 22463,  2587],
        [    0,  6416,   367,  ...,   496,    14,    20]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3850e-05,  1.7161e-04, -8.8063e-05,  ..., -2.9022e-06,
          1.0237e-04,  4.3046e-05],
        [-2.7195e-06, -2.0862e-06,  1.8924e-06,  ..., -2.5108e-06,
         -2.1607e-06, -1.9521e-06],
        [-7.3612e-06, -5.9307e-06,  5.5879e-06,  ..., -6.7353e-06,
         -5.8413e-06, -5.1707e-06],
        [-5.2601e-06, -4.0233e-06,  3.7923e-06,  ..., -4.8876e-06,
         -4.2766e-06, -3.9488e-06],
        [-6.4522e-06, -5.3942e-06,  4.8280e-06,  ..., -6.0052e-06,
         -5.3942e-06, -4.1872e-06]], device='cuda:0')
Loss: 0.9675843119621277


Running epoch 1, step 1706, batch 658
Sampled inputs[:2]: tensor([[   0, 1726, 3775,  ...,  300,  266, 1686],
        [   0, 5221, 7166,  ..., 4309,  342,  996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5903e-06,  9.1165e-07, -1.3925e-04,  ..., -2.9128e-05,
          1.3820e-04,  5.1438e-05],
        [-4.1872e-06, -3.1143e-06,  2.9728e-06,  ..., -3.7700e-06,
         -3.1516e-06, -2.9281e-06],
        [-1.1355e-05, -8.8513e-06,  8.7172e-06,  ..., -1.0103e-05,
         -8.5384e-06, -7.7784e-06],
        [-8.0466e-06, -5.9456e-06,  5.9083e-06,  ..., -7.2718e-06,
         -6.1616e-06, -5.8711e-06],
        [-9.9093e-06, -8.0317e-06,  7.4804e-06,  ..., -8.9854e-06,
         -7.8678e-06, -6.2883e-06]], device='cuda:0')
Loss: 0.9806702733039856


Running epoch 1, step 1707, batch 659
Sampled inputs[:2]: tensor([[    0,  8023,  1309,  ...,  3370,   266, 14988],
        [    0,    12,  3570,  ...,   273,   298,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5645e-04, -4.3702e-05, -2.4841e-04,  ..., -1.3557e-04,
          2.2942e-04,  1.2487e-04],
        [-5.6550e-06, -4.1425e-06,  4.0457e-06,  ..., -5.0291e-06,
         -4.1574e-06, -3.9339e-06],
        [-1.5318e-05, -1.1817e-05,  1.1861e-05,  ..., -1.3500e-05,
         -1.1325e-05, -1.0490e-05],
        [-1.0788e-05, -7.8529e-06,  8.0243e-06,  ..., -9.6262e-06,
         -8.0690e-06, -7.8380e-06],
        [-1.3441e-05, -1.0788e-05,  1.0192e-05,  ..., -1.2085e-05,
         -1.0520e-05, -8.5235e-06]], device='cuda:0')
Loss: 0.9618711471557617


Running epoch 1, step 1708, batch 660
Sampled inputs[:2]: tensor([[    0,  3703,   278,  ...,  9807,    14, 10365],
        [    0,  4120,   278,  ...,   298,   273,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1912e-04, -2.5211e-05, -2.6235e-04,  ..., -1.6144e-04,
          3.3593e-04,  1.0441e-04],
        [-6.9737e-06, -5.1782e-06,  5.1409e-06,  ..., -6.1691e-06,
         -5.1074e-06, -4.8690e-06],
        [-1.8969e-05, -1.4782e-05,  1.5095e-05,  ..., -1.6600e-05,
         -1.3903e-05, -1.3053e-05],
        [-1.3366e-05, -9.8497e-06,  1.0259e-05,  ..., -1.1832e-05,
         -9.9167e-06, -9.7752e-06],
        [-1.6481e-05, -1.3351e-05,  1.2785e-05,  ..., -1.4722e-05,
         -1.2800e-05, -1.0476e-05]], device='cuda:0')
Loss: 0.9785762429237366


Running epoch 1, step 1709, batch 661
Sampled inputs[:2]: tensor([[    0,   365,  1941,  ..., 38029,  1790, 44066],
        [    0,  3070,  9719,  ...,   600,  4207,  4293]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7552e-04, -8.2846e-05, -5.4916e-04,  ..., -1.5297e-04,
          4.8548e-04,  1.5121e-04],
        [-8.3894e-06, -6.1691e-06,  6.1989e-06,  ..., -7.4133e-06,
         -6.1579e-06, -5.8450e-06],
        [-2.2903e-05, -1.7688e-05,  1.8284e-05,  ..., -2.0027e-05,
         -1.6794e-05, -1.5765e-05],
        [-1.6093e-05, -1.1750e-05,  1.2405e-05,  ..., -1.4231e-05,
         -1.1958e-05, -1.1757e-05],
        [-1.9833e-05, -1.5944e-05,  1.5438e-05,  ..., -1.7717e-05,
         -1.5438e-05, -1.2591e-05]], device='cuda:0')
Loss: 0.9711663722991943


Running epoch 1, step 1710, batch 662
Sampled inputs[:2]: tensor([[    0,   266,  1144,  ..., 21458,    12, 15890],
        [    0,  3152,  1385,  ...,  1403,   518,  2088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7627e-04, -9.9055e-05, -5.7694e-04,  ..., -2.6965e-04,
          3.7846e-04,  2.3485e-05],
        [-9.8124e-06, -7.1041e-06,  7.3537e-06,  ..., -8.6054e-06,
         -7.0818e-06, -6.8583e-06],
        [-2.6807e-05, -2.0385e-05,  2.1666e-05,  ..., -2.3291e-05,
         -1.9342e-05, -1.8522e-05],
        [-1.8910e-05, -1.3568e-05,  1.4789e-05,  ..., -1.6600e-05,
         -1.3784e-05, -1.3843e-05],
        [-2.3097e-05, -1.8314e-05,  1.8194e-05,  ..., -2.0504e-05,
         -1.7732e-05, -1.4707e-05]], device='cuda:0')
Loss: 0.9540131688117981


Running epoch 1, step 1711, batch 663
Sampled inputs[:2]: tensor([[    0,  6491,  3667,  ...,  5042,    14,  2152],
        [    0,  3473,   278,  ..., 11743,   472,   346]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9708e-04, -2.5136e-05, -5.9538e-04,  ..., -2.6889e-04,
          6.8296e-04,  4.5596e-06],
        [-1.1154e-05, -8.1323e-06,  8.4117e-06,  ..., -9.7826e-06,
         -8.1696e-06, -7.7784e-06],
        [-3.0592e-05, -2.3410e-05,  2.4870e-05,  ..., -2.6613e-05,
         -2.2441e-05, -2.1130e-05],
        [-2.1651e-05, -1.5624e-05,  1.7054e-05,  ..., -1.8999e-05,
         -1.6034e-05, -1.5825e-05],
        [-2.6271e-05, -2.0981e-05,  2.0802e-05,  ..., -2.3365e-05,
         -2.0489e-05, -1.6719e-05]], device='cuda:0')
Loss: 0.9997043013572693
Graident accumulation at epoch 1, step 1711, batch 663
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-1.3880e-05,  3.2288e-06, -1.7913e-04,  ..., -6.2935e-06,
          1.0224e-04,  4.7325e-05],
        [-9.9702e-06, -6.8756e-06,  7.2115e-06,  ..., -9.0008e-06,
         -6.6949e-06, -6.8539e-06],
        [ 2.8372e-06,  1.5709e-05,  5.6057e-08,  ...,  3.5622e-06,
          1.0225e-05, -2.3284e-06],
        [-1.2607e-05, -3.2556e-06,  9.1549e-06,  ..., -8.8210e-06,
         -3.5777e-06, -1.0167e-05],
        [-2.6044e-05, -2.0765e-05,  2.0055e-05,  ..., -2.3446e-05,
         -2.0050e-05, -1.6646e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8639e-08, 5.6237e-08, 5.5221e-08,  ..., 2.1215e-08, 1.2433e-07,
         4.8787e-08],
        [9.0523e-11, 5.9581e-11, 2.6877e-11,  ..., 6.2412e-11, 3.2722e-11,
         3.0231e-11],
        [2.0695e-09, 1.4797e-09, 6.9204e-10,  ..., 1.5929e-09, 9.3080e-10,
         5.3677e-10],
        [8.9066e-10, 1.0813e-09, 3.9362e-10,  ..., 8.9101e-10, 6.7669e-10,
         3.6860e-10],
        [4.2343e-10, 2.4693e-10, 9.4260e-11,  ..., 3.1065e-10, 9.4028e-11,
         1.2742e-10]], device='cuda:0')
optimizer state dict: 214.0
lr: [1.7093324223767748e-06, 1.7093324223767748e-06]
scheduler_last_epoch: 214


Running epoch 1, step 1712, batch 664
Sampled inputs[:2]: tensor([[    0,  1250,  1797,  ...,   266,  1417,   367],
        [    0,   278, 14971,  ...,  2341,   266,   717]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0955e-05,  7.9829e-05, -7.0031e-05,  ..., -2.0151e-05,
         -4.1242e-06, -1.6147e-04],
        [-1.4007e-06, -8.5682e-07,  9.2387e-07,  ..., -1.2591e-06,
         -1.0803e-06, -1.0282e-06],
        [-3.9041e-06, -2.5332e-06,  2.8461e-06,  ..., -3.4273e-06,
         -2.9653e-06, -2.7716e-06],
        [-2.7567e-06, -1.6540e-06,  1.8999e-06,  ..., -2.4885e-06,
         -2.1458e-06, -2.0862e-06],
        [-3.3528e-06, -2.2352e-06,  2.3991e-06,  ..., -3.0100e-06,
         -2.7120e-06, -2.1905e-06]], device='cuda:0')
Loss: 0.9188515543937683


Running epoch 1, step 1713, batch 665
Sampled inputs[:2]: tensor([[    0,  1941,   437,  ..., 16539,  4129,  4156],
        [    0,   634,   631,  ...,  3431,   287, 27947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2244e-04,  1.9987e-04,  8.1175e-05,  ...,  2.7080e-05,
          2.9482e-05, -2.7659e-04],
        [-2.8461e-06, -1.8775e-06,  2.0638e-06,  ..., -2.5257e-06,
         -2.0787e-06, -2.0862e-06],
        [-7.9572e-06, -5.5283e-06,  6.2436e-06,  ..., -6.9588e-06,
         -5.7817e-06, -5.6922e-06],
        [-5.5283e-06, -3.5763e-06,  4.1649e-06,  ..., -4.9025e-06,
         -4.0680e-06, -4.1723e-06],
        [-6.8545e-06, -4.9323e-06,  5.2601e-06,  ..., -6.1244e-06,
         -5.2899e-06, -4.5300e-06]], device='cuda:0')
Loss: 0.9643337726593018


Running epoch 1, step 1714, batch 666
Sampled inputs[:2]: tensor([[    0,    14,  1845,  ...,   806,   352,   408],
        [    0, 24414,  4865,  ...,  8720,   344,  1566]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1630e-04, -1.6321e-05,  1.7244e-05,  ...,  8.9741e-05,
          3.7703e-04, -1.3177e-04],
        [-4.2021e-06, -2.8536e-06,  3.1590e-06,  ..., -3.7178e-06,
         -3.0473e-06, -3.0920e-06],
        [-1.1757e-05, -8.3745e-06,  9.5367e-06,  ..., -1.0252e-05,
         -8.4639e-06, -8.4639e-06],
        [-8.2254e-06, -5.4762e-06,  6.4597e-06,  ..., -7.2569e-06,
         -5.9903e-06, -6.2436e-06],
        [-1.0073e-05, -7.4655e-06,  7.9721e-06,  ..., -8.9854e-06,
         -7.7486e-06, -6.7055e-06]], device='cuda:0')
Loss: 0.9242402911186218


Running epoch 1, step 1715, batch 667
Sampled inputs[:2]: tensor([[   0, 6673,  298,  ..., 4391,  292,  221],
        [   0,  287,  221,  ..., 1871, 1482,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3570e-05,  7.4139e-05, -3.1329e-05,  ..., -3.2496e-05,
          2.0407e-04, -1.6630e-04],
        [-5.6252e-06, -3.7923e-06,  4.3213e-06,  ..., -4.9323e-06,
         -3.9972e-06, -4.0755e-06],
        [-1.5602e-05, -1.1012e-05,  1.2904e-05,  ..., -1.3486e-05,
         -1.1027e-05, -1.1057e-05],
        [-1.1027e-05, -7.2867e-06,  8.8736e-06,  ..., -9.6262e-06,
         -7.8604e-06, -8.2701e-06],
        [-1.3337e-05, -9.8199e-06,  1.0759e-05,  ..., -1.1787e-05,
         -1.0088e-05, -8.7470e-06]], device='cuda:0')
Loss: 0.9278489351272583


Running epoch 1, step 1716, batch 668
Sampled inputs[:2]: tensor([[   0, 5319,   14,  ..., 2372, 2356, 4093],
        [   0, 4073, 1548,  ...,  292,  221,  301]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4970e-05,  1.3771e-04, -1.5956e-04,  ...,  8.9639e-05,
          9.4667e-05, -3.8982e-04],
        [-7.0184e-06, -4.7684e-06,  5.5358e-06,  ..., -6.1244e-06,
         -4.9099e-06, -5.0440e-06],
        [-1.9506e-05, -1.3858e-05,  1.6496e-05,  ..., -1.6794e-05,
         -1.3575e-05, -1.3709e-05],
        [-1.3739e-05, -9.1717e-06,  1.1362e-05,  ..., -1.1936e-05,
         -9.6411e-06, -1.0252e-05],
        [-1.6496e-05, -1.2234e-05,  1.3590e-05,  ..., -1.4529e-05,
         -1.2293e-05, -1.0714e-05]], device='cuda:0')
Loss: 0.9486271739006042


Running epoch 1, step 1717, batch 669
Sampled inputs[:2]: tensor([[    0,    12,   287,  ...,   381,  3513,  1501],
        [    0,   892,   271,  ...,   278,   266, 10237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.9812e-05,  1.9642e-04, -2.3097e-04,  ...,  4.7744e-05,
          1.6281e-04, -4.1965e-04],
        [-8.4192e-06, -5.7667e-06,  6.7204e-06,  ..., -7.3016e-06,
         -5.8636e-06, -6.0350e-06],
        [ 7.2561e-04,  5.1173e-04, -6.6181e-04,  ...,  4.8206e-04,
          4.5387e-04,  4.3593e-04],
        [-1.6466e-05, -1.1079e-05,  1.3791e-05,  ..., -1.4201e-05,
         -1.1496e-05, -1.2264e-05],
        [-1.9744e-05, -1.4797e-05,  1.6451e-05,  ..., -1.7315e-05,
         -1.4707e-05, -1.2800e-05]], device='cuda:0')
Loss: 0.9414790868759155


Running epoch 1, step 1718, batch 670
Sampled inputs[:2]: tensor([[    0,  9509, 21000,  ...,  1953,    14,   333],
        [    0, 14349,   278,  ...,   365,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.4142e-06,  1.4837e-04, -3.8948e-04,  ...,  4.6672e-05,
          4.4471e-05, -5.3735e-04],
        [-9.7603e-06, -6.7353e-06,  7.8902e-06,  ..., -8.4192e-06,
         -6.7167e-06, -6.9849e-06],
        [ 7.2187e-04,  5.0893e-04, -6.5836e-04,  ...,  4.7897e-04,
          4.5154e-04,  4.3331e-04],
        [-1.9163e-05, -1.3001e-05,  1.6265e-05,  ..., -1.6436e-05,
         -1.3188e-05, -1.4275e-05],
        [-2.2739e-05, -1.7151e-05,  1.9133e-05,  ..., -1.9848e-05,
         -1.6719e-05, -1.4722e-05]], device='cuda:0')
Loss: 0.9311835169792175


Running epoch 1, step 1719, batch 671
Sampled inputs[:2]: tensor([[    0,   391,  9095,  ...,   417,   199,  2038],
        [    0,    13, 41550,  ...,    12,   546,  1996]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.6986e-05,  5.1934e-05, -4.9585e-04,  ..., -5.0966e-05,
          5.1244e-05, -4.8428e-04],
        [-1.1191e-05, -7.7561e-06,  9.0450e-06,  ..., -9.6411e-06,
         -7.7151e-06, -7.9311e-06],
        [ 7.1800e-04,  5.0599e-04, -6.5506e-04,  ...,  4.7563e-04,
          4.4876e-04,  4.3073e-04],
        [ 6.3181e-05,  8.2471e-05, -3.2741e-05,  ...,  6.5184e-05,
          7.9986e-05,  2.5842e-05],
        [-2.6047e-05, -1.9789e-05,  2.1920e-05,  ..., -2.2769e-05,
         -1.9267e-05, -1.6779e-05]], device='cuda:0')
Loss: 0.9850783944129944
Graident accumulation at epoch 1, step 1719, batch 671
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[-2.7932e-06,  8.0993e-06, -2.1080e-04,  ..., -1.0761e-05,
          9.7143e-05, -5.8358e-06],
        [-1.0092e-05, -6.9636e-06,  7.3949e-06,  ..., -9.0648e-06,
         -6.7969e-06, -6.9617e-06],
        [ 7.4354e-05,  6.4737e-05, -6.5455e-05,  ...,  5.0769e-05,
          5.4078e-05,  4.0978e-05],
        [-5.0284e-06,  5.3170e-06,  4.9653e-06,  ..., -1.4205e-06,
          4.7786e-06, -6.5657e-06],
        [-2.6045e-05, -2.0668e-05,  2.0242e-05,  ..., -2.3378e-05,
         -1.9972e-05, -1.6659e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8590e-08, 5.6184e-08, 5.5411e-08,  ..., 2.1197e-08, 1.2421e-07,
         4.8973e-08],
        [9.0558e-11, 5.9582e-11, 2.6932e-11,  ..., 6.2443e-11, 3.2749e-11,
         3.0264e-11],
        [2.5830e-09, 1.7343e-09, 1.1204e-09,  ..., 1.8176e-09, 1.1312e-09,
         7.2176e-10],
        [8.9377e-10, 1.0871e-09, 3.9430e-10,  ..., 8.9437e-10, 6.8241e-10,
         3.6890e-10],
        [4.2368e-10, 2.4707e-10, 9.4647e-11,  ..., 3.1086e-10, 9.4305e-11,
         1.2757e-10]], device='cuda:0')
optimizer state dict: 215.0
lr: [1.6408474411262143e-06, 1.6408474411262143e-06]
scheduler_last_epoch: 215


Running epoch 1, step 1720, batch 672
Sampled inputs[:2]: tensor([[    0, 11752,   280,  ..., 14814,  1128,   360],
        [    0,  2228,  1416,  ...,  3766,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2978e-04, -4.3288e-05, -5.6191e-05,  ...,  3.2972e-05,
         -1.2447e-04, -1.8062e-04],
        [-1.3858e-06, -7.9721e-07,  1.0878e-06,  ..., -1.1921e-06,
         -8.8289e-07, -1.0282e-06],
        [-3.7104e-06, -2.2203e-06,  3.1441e-06,  ..., -3.1292e-06,
         -2.3246e-06, -2.6524e-06],
        [-2.6822e-06, -1.4901e-06,  2.2054e-06,  ..., -2.2948e-06,
         -1.6987e-06, -2.0564e-06],
        [-3.2187e-06, -2.0117e-06,  2.6375e-06,  ..., -2.7567e-06,
         -2.1309e-06, -2.1011e-06]], device='cuda:0')
Loss: 0.9465258121490479


Running epoch 1, step 1721, batch 673
Sampled inputs[:2]: tensor([[    0,  4645,  7688,  ..., 26535,   471,   287],
        [    0,  2173,   292,  ...,   344,  8106,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9859e-05, -4.5838e-05, -9.3235e-05,  ..., -2.7583e-05,
         -2.1228e-04, -3.0982e-04],
        [-2.8312e-06, -1.9073e-06,  2.1681e-06,  ..., -2.4065e-06,
         -1.9185e-06, -1.9707e-06],
        [-7.7039e-06, -5.4389e-06,  6.3330e-06,  ..., -6.4969e-06,
         -5.2154e-06, -5.2601e-06],
        [-5.4836e-06, -3.6359e-06,  4.3958e-06,  ..., -4.6641e-06,
         -3.7402e-06, -3.9935e-06],
        [-6.5565e-06, -4.8429e-06,  5.2601e-06,  ..., -5.6624e-06,
         -4.7237e-06, -4.1127e-06]], device='cuda:0')
Loss: 0.9831556081771851


Running epoch 1, step 1722, batch 674
Sampled inputs[:2]: tensor([[   0, 1159,  278,  ...,    9,  271,  266],
        [   0,  221,  380,  ...,  631, 2820,  344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0018e-04,  1.1097e-04, -9.1120e-05,  ..., -5.7683e-05,
         -3.4250e-04, -2.9130e-04],
        [-4.2692e-06, -2.9504e-06,  3.3975e-06,  ..., -3.5986e-06,
         -2.8722e-06, -2.8871e-06],
        [-1.1519e-05, -8.4043e-06,  9.8050e-06,  ..., -9.6858e-06,
         -7.8380e-06, -7.7486e-06],
        [-8.2850e-06, -5.6624e-06,  6.9141e-06,  ..., -6.9737e-06,
         -5.6103e-06, -5.8785e-06],
        [-9.7156e-06, -7.4506e-06,  8.0764e-06,  ..., -8.3894e-06,
         -7.0781e-06, -6.0052e-06]], device='cuda:0')
Loss: 0.97771817445755


Running epoch 1, step 1723, batch 675
Sampled inputs[:2]: tensor([[    0,   360,   259,  ...,    14,   381,  1371],
        [    0, 31318,    14,  ...,  1682,  1501,  1548]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6351e-04,  2.3048e-04,  1.0632e-04,  ..., -1.4755e-04,
         -5.0553e-04, -3.7570e-04],
        [-5.6550e-06, -3.9935e-06,  4.5449e-06,  ..., -4.7609e-06,
         -3.8259e-06, -3.7812e-06],
        [-1.5303e-05, -1.1384e-05,  1.3113e-05,  ..., -1.2845e-05,
         -1.0416e-05, -1.0192e-05],
        [-1.0997e-05, -7.6890e-06,  9.2685e-06,  ..., -9.2238e-06,
         -7.4729e-06, -7.7412e-06],
        [-1.2904e-05, -1.0103e-05,  1.0803e-05,  ..., -1.1131e-05,
         -9.4324e-06, -7.9125e-06]], device='cuda:0')
Loss: 0.988344669342041


Running epoch 1, step 1724, batch 676
Sampled inputs[:2]: tensor([[   0,  591,  688,  ...,  271, 3390,   12],
        [   0, 6640,   13,  ...,  292,  221,  273]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0384e-04,  4.4143e-04,  6.7089e-05,  ..., -1.0200e-04,
         -6.4405e-04, -5.1077e-04],
        [-7.0333e-06, -4.8913e-06,  5.6252e-06,  ..., -5.9903e-06,
         -4.7572e-06, -4.8392e-06],
        [-1.8969e-05, -1.3933e-05,  1.6227e-05,  ..., -1.6063e-05,
         -1.2890e-05, -1.2949e-05],
        [-1.3545e-05, -9.3281e-06,  1.1340e-05,  ..., -1.1504e-05,
         -9.2164e-06, -9.7677e-06],
        [-1.6138e-05, -1.2457e-05,  1.3441e-05,  ..., -1.4052e-05,
         -1.1772e-05, -1.0133e-05]], device='cuda:0')
Loss: 0.9205397367477417


Running epoch 1, step 1725, batch 677
Sampled inputs[:2]: tensor([[    0,   266,  3574,  ...,  7052,  3829,   292],
        [    0,    34,     9,  ...,    19,    14, 45576]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2059e-04,  4.4406e-04, -2.6208e-05,  ..., -1.4451e-04,
         -6.5302e-04, -6.6627e-04],
        [-8.4862e-06, -5.8152e-06,  6.7651e-06,  ..., -7.2271e-06,
         -5.6997e-06, -5.9493e-06],
        [-2.3082e-05, -1.6600e-05,  1.9625e-05,  ..., -1.9491e-05,
         -1.5497e-05, -1.6063e-05],
        [-1.6227e-05, -1.1005e-05,  1.3515e-05,  ..., -1.3784e-05,
         -1.0937e-05, -1.1884e-05],
        [-1.9729e-05, -1.4856e-05,  1.6317e-05,  ..., -1.7107e-05,
         -1.4201e-05, -1.2666e-05]], device='cuda:0')
Loss: 0.9517965316772461


Running epoch 1, step 1726, batch 678
Sampled inputs[:2]: tensor([[   0,  271,  266,  ...,  275, 2576, 3588],
        [   0, 3141,  311,  ...,  328, 7818,  408]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2049e-04,  3.8403e-04,  1.2002e-04,  ..., -2.2429e-04,
         -5.6087e-04, -5.0326e-04],
        [-9.8273e-06, -6.8061e-06,  7.7561e-06,  ..., -8.4490e-06,
         -6.7353e-06, -6.8806e-06],
        [-2.6688e-05, -1.9431e-05,  2.2516e-05,  ..., -2.2739e-05,
         -1.8269e-05, -1.8552e-05],
        [-1.8835e-05, -1.2927e-05,  1.5527e-05,  ..., -1.6168e-05,
         -1.2994e-05, -1.3821e-05],
        [-2.2948e-05, -1.7494e-05,  1.8820e-05,  ..., -2.0057e-05,
         -1.6809e-05, -1.4707e-05]], device='cuda:0')
Loss: 0.9785526394844055


Running epoch 1, step 1727, batch 679
Sampled inputs[:2]: tensor([[   0, 4882,   12,  ...,   12, 9575,  287],
        [   0,   12,  344,  ...,  824,   12,  968]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.4706e-04,  4.0869e-04,  1.3553e-04,  ..., -2.7443e-04,
         -4.4338e-04, -4.3168e-04],
        [-1.1198e-05, -7.7672e-06,  8.8289e-06,  ..., -9.6485e-06,
         -7.8008e-06, -7.8306e-06],
        [-3.0294e-05, -2.2098e-05,  2.5526e-05,  ..., -2.5898e-05,
         -2.1085e-05, -2.1055e-05],
        [-2.1502e-05, -1.4782e-05,  1.7703e-05,  ..., -1.8522e-05,
         -1.5095e-05, -1.5773e-05],
        [-2.6196e-05, -2.0027e-05,  2.1473e-05,  ..., -2.2978e-05,
         -1.9535e-05, -1.6779e-05]], device='cuda:0')
Loss: 0.9953504800796509
Graident accumulation at epoch 1, step 1727, batch 679
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 4.2192e-05,  4.8159e-05, -1.7617e-04,  ..., -3.7128e-05,
          4.3090e-05, -4.8420e-05],
        [-1.0203e-05, -7.0440e-06,  7.5383e-06,  ..., -9.1232e-06,
         -6.8973e-06, -7.0486e-06],
        [ 6.3889e-05,  5.6053e-05, -5.6357e-05,  ...,  4.3103e-05,
          4.6562e-05,  3.4774e-05],
        [-6.6758e-06,  3.3071e-06,  6.2390e-06,  ..., -3.1307e-06,
          2.7912e-06, -7.4864e-06],
        [-2.6060e-05, -2.0603e-05,  2.0365e-05,  ..., -2.3338e-05,
         -1.9928e-05, -1.6671e-05]], device='cuda:0')
optimizer state dict: tensor([[5.8731e-08, 5.6295e-08, 5.5374e-08,  ..., 2.1251e-08, 1.2428e-07,
         4.9110e-08],
        [9.0592e-11, 5.9583e-11, 2.6983e-11,  ..., 6.2473e-11, 3.2777e-11,
         3.0295e-11],
        [2.5813e-09, 1.7330e-09, 1.1200e-09,  ..., 1.8164e-09, 1.1306e-09,
         7.2148e-10],
        [8.9333e-10, 1.0862e-09, 3.9422e-10,  ..., 8.9381e-10, 6.8196e-10,
         3.6878e-10],
        [4.2394e-10, 2.4723e-10, 9.5013e-11,  ..., 3.1108e-10, 9.4592e-11,
         1.2773e-10]], device='cuda:0')
optimizer state dict: 216.0
lr: [1.5736398113547236e-06, 1.5736398113547236e-06]
scheduler_last_epoch: 216


Running epoch 1, step 1728, batch 680
Sampled inputs[:2]: tensor([[   0, 1832,  292,  ..., 2176, 1345,   14],
        [   0,   12,  328,  ...,  908, 1086,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1886e-05, -9.1222e-05, -1.1439e-04,  ...,  3.3108e-06,
         -8.7923e-05,  1.2689e-05],
        [-1.3858e-06, -9.0525e-07,  1.1548e-06,  ..., -1.1772e-06,
         -8.9779e-07, -9.2015e-07],
        [-3.6210e-06, -2.4736e-06,  3.2336e-06,  ..., -3.0398e-06,
         -2.3246e-06, -2.3395e-06],
        [-2.6971e-06, -1.7211e-06,  2.3544e-06,  ..., -2.2948e-06,
         -1.7509e-06, -1.8626e-06],
        [-3.1143e-06, -2.2352e-06,  2.6971e-06,  ..., -2.6822e-06,
         -2.1607e-06, -1.8403e-06]], device='cuda:0')
Loss: 0.9590024948120117


Running epoch 1, step 1729, batch 681
Sampled inputs[:2]: tensor([[    0,   908,    14,  ...,    19,    27,   287],
        [    0,  1266,  2257,  ..., 27146,  1141,  1196]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4266e-04, -1.1316e-04, -1.5539e-04,  ...,  1.6025e-04,
         -9.7921e-05, -1.4517e-04],
        [-2.8163e-06, -1.9930e-06,  2.3693e-06,  ..., -2.3320e-06,
         -1.8664e-06, -1.8217e-06],
        [-7.5549e-06, -5.6177e-06,  6.7353e-06,  ..., -6.2287e-06,
         -5.0515e-06, -4.8429e-06],
        [-5.4687e-06, -3.8072e-06,  4.8131e-06,  ..., -4.5300e-06,
         -3.6359e-06, -3.7029e-06],
        [-6.3032e-06, -4.9323e-06,  5.4985e-06,  ..., -5.3644e-06,
         -4.5449e-06, -3.7253e-06]], device='cuda:0')
Loss: 0.9902479648590088


Running epoch 1, step 1730, batch 682
Sampled inputs[:2]: tensor([[    0,   221,   467,  ..., 21991,   630,  3990],
        [    0,  7203,   271,  ...,    12,   275,  3338]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9593e-04, -1.3067e-04, -2.9558e-05,  ...,  1.3965e-04,
         -1.0012e-04, -1.4786e-04],
        [-4.2170e-06, -2.8871e-06,  3.5539e-06,  ..., -3.5688e-06,
         -2.7418e-06, -2.7977e-06],
        [-1.1429e-05, -8.1807e-06,  1.0222e-05,  ..., -9.5963e-06,
         -7.4655e-06, -7.4804e-06],
        [-8.2105e-06, -5.5060e-06,  7.2420e-06,  ..., -6.9439e-06,
         -5.3495e-06, -5.6848e-06],
        [-9.5218e-06, -7.1824e-06,  8.3297e-06,  ..., -8.2254e-06,
         -6.6906e-06, -5.7369e-06]], device='cuda:0')
Loss: 0.9785847067832947


Running epoch 1, step 1731, batch 683
Sampled inputs[:2]: tensor([[   0, 1184, 1451,  ...,  934,  352,  266],
        [   0, 1416,  367,  ...,  555,  764,  367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.6505e-04, -7.6681e-05, -7.7879e-05,  ...,  6.8006e-05,
         -1.0457e-04, -7.5323e-05],
        [-5.6326e-06, -3.9302e-06,  4.7535e-06,  ..., -4.7237e-06,
         -3.7253e-06, -3.7886e-06],
        [-1.5512e-05, -1.1325e-05,  1.3813e-05,  ..., -1.2949e-05,
         -1.0327e-05, -1.0386e-05],
        [-1.1012e-05, -7.5474e-06,  9.7007e-06,  ..., -9.2238e-06,
         -7.2867e-06, -7.7561e-06],
        [-1.2830e-05, -9.8646e-06,  1.1191e-05,  ..., -1.1027e-05,
         -9.1791e-06, -7.9274e-06]], device='cuda:0')
Loss: 0.9701966643333435


Running epoch 1, step 1732, batch 684
Sampled inputs[:2]: tensor([[   0,  199,  769,  ...,  685, 1423,   13],
        [   0,  278,  266,  ...,  292,  474,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.5403e-04, -1.7511e-04, -2.1779e-04,  ..., -8.5525e-06,
         -2.8116e-04, -5.6809e-05],
        [-6.9812e-06, -4.7982e-06,  5.9456e-06,  ..., -5.7891e-06,
         -4.5076e-06, -4.7125e-06],
        [-1.9297e-05, -1.3828e-05,  1.7345e-05,  ..., -1.5900e-05,
         -1.2532e-05, -1.2890e-05],
        [-1.3739e-05, -9.2536e-06,  1.2234e-05,  ..., -1.1355e-05,
         -8.8736e-06, -9.6932e-06],
        [-1.5840e-05, -1.2010e-05,  1.3933e-05,  ..., -1.3441e-05,
         -1.1101e-05, -9.7379e-06]], device='cuda:0')
Loss: 0.9323952198028564


Running epoch 1, step 1733, batch 685
Sampled inputs[:2]: tensor([[    0,  4653, 21419,  ...,  7845,   300,   565],
        [    0,   301,   298,  ..., 10030,   300,  3780]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.2684e-04, -5.8789e-05, -1.6445e-04,  ..., -1.0283e-04,
         -1.1165e-04,  1.0069e-04],
        [-8.4937e-06, -5.8711e-06,  6.9141e-06,  ..., -7.1451e-06,
         -5.7518e-06, -5.7705e-06],
        [-2.3291e-05, -1.6868e-05,  2.0131e-05,  ..., -1.9476e-05,
         -1.5855e-05, -1.5676e-05],
        [ 2.6546e-04,  3.5277e-04, -1.7015e-04,  ...,  2.3699e-04,
          3.0403e-04,  1.9878e-04],
        [-1.9535e-05, -1.4931e-05,  1.6451e-05,  ..., -1.6838e-05,
         -1.4350e-05, -1.2152e-05]], device='cuda:0')
Loss: 0.9694005846977234


Running epoch 1, step 1734, batch 686
Sampled inputs[:2]: tensor([[    0,  2834, 25800,  ...,    12,   367,  2870],
        [    0,   472,   346,  ...,   298,   527,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5962e-04, -6.7086e-05,  8.2984e-05,  ...,  1.1017e-04,
         -3.6767e-04, -7.6434e-05],
        [-9.8869e-06, -6.8061e-06,  8.0615e-06,  ..., -8.3447e-06,
         -6.6869e-06, -6.7241e-06],
        [-2.7061e-05, -1.9506e-05,  2.3454e-05,  ..., -2.2650e-05,
         -1.8373e-05, -1.8194e-05],
        [ 2.6273e-04,  3.5098e-04, -1.6778e-04,  ...,  2.3466e-04,
          3.0220e-04,  1.9682e-04],
        [-2.2769e-05, -1.7315e-05,  1.9222e-05,  ..., -1.9610e-05,
         -1.6645e-05, -1.4119e-05]], device='cuda:0')
Loss: 0.9600963592529297


Running epoch 1, step 1735, batch 687
Sampled inputs[:2]: tensor([[    0,   377,   472,  ...,  9256,  3807,  5499],
        [    0,   328,  1690,  ...,  2670,   287, 11287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.7536e-04,  1.6210e-04,  1.1371e-04,  ...,  3.3851e-04,
         -6.9660e-04, -2.2832e-04],
        [-1.1198e-05, -7.7821e-06,  9.1270e-06,  ..., -9.4995e-06,
         -7.6555e-06, -7.6480e-06],
        [-3.0532e-05, -2.2113e-05,  2.6479e-05,  ..., -2.5600e-05,
         -2.0847e-05, -2.0504e-05],
        [ 2.6015e-04,  3.4913e-04, -1.6557e-04,  ...,  2.3241e-04,
          3.0031e-04,  1.9492e-04],
        [-2.5839e-05, -1.9729e-05,  2.1830e-05,  ..., -2.2262e-05,
         -1.8969e-05, -1.5974e-05]], device='cuda:0')
Loss: 0.9216862320899963
Graident accumulation at epoch 1, step 1735, batch 687
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1551e-04,  5.9552e-05, -1.4718e-04,  ...,  4.3604e-07,
         -3.0878e-05, -6.6410e-05],
        [-1.0302e-05, -7.1178e-06,  7.6972e-06,  ..., -9.1608e-06,
         -6.9731e-06, -7.1085e-06],
        [ 5.4447e-05,  4.8237e-05, -4.8073e-05,  ...,  3.6232e-05,
          3.9821e-05,  2.9246e-05],
        [ 2.0007e-05,  3.7889e-05, -1.0942e-05,  ...,  2.0424e-05,
          3.2543e-05,  1.2755e-05],
        [-2.6038e-05, -2.0516e-05,  2.0511e-05,  ..., -2.3230e-05,
         -1.9833e-05, -1.6601e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9274e-08, 5.6265e-08, 5.5332e-08,  ..., 2.1344e-08, 1.2464e-07,
         4.9113e-08],
        [9.0627e-11, 5.9584e-11, 2.7039e-11,  ..., 6.2501e-11, 3.2803e-11,
         3.0323e-11],
        [2.5797e-09, 1.7318e-09, 1.1196e-09,  ..., 1.8153e-09, 1.1299e-09,
         7.2118e-10],
        [9.6012e-10, 1.2070e-09, 4.2124e-10,  ..., 9.4694e-10, 7.7146e-10,
         4.0641e-10],
        [4.2419e-10, 2.4737e-10, 9.5395e-11,  ..., 3.1126e-10, 9.4858e-11,
         1.2785e-10]], device='cuda:0')
optimizer state dict: 217.0
lr: [1.5077198029747941e-06, 1.5077198029747941e-06]
scheduler_last_epoch: 217


Running epoch 1, step 1736, batch 688
Sampled inputs[:2]: tensor([[    0,  2698,   221,  ...,  8352,  5680,   782],
        [    0,   278, 11554,  ...,  4713,  1039, 17088]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4365e-04, -3.3869e-05,  8.4655e-06,  ...,  1.5929e-04,
          6.7496e-05,  1.3350e-04],
        [-1.4678e-06, -1.0207e-06,  1.1623e-06,  ..., -1.2368e-06,
         -1.0431e-06, -1.0207e-06],
        [-4.0233e-06, -2.9653e-06,  3.4124e-06,  ..., -3.3826e-06,
         -2.8908e-06, -2.7865e-06],
        [-2.8312e-06, -1.9670e-06,  2.3395e-06,  ..., -2.3991e-06,
         -2.0266e-06, -2.0564e-06],
        [-3.3975e-06, -2.6226e-06,  2.8163e-06,  ..., -2.9355e-06,
         -2.5928e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9657065868377686


Running epoch 1, step 1737, batch 689
Sampled inputs[:2]: tensor([[    0,   659,   278,  ...,   593,  2177,   266],
        [    0,    12,  6426,  ...,  2629, 13422,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3002e-04,  4.4379e-05, -2.5897e-04,  ...,  6.0291e-05,
          1.0353e-04,  9.9091e-05],
        [-2.7493e-06, -1.9297e-06,  2.2724e-06,  ..., -2.3097e-06,
         -1.9409e-06, -1.9036e-06],
        [-7.5847e-06, -5.5879e-06,  6.6906e-06,  ..., -6.3181e-06,
         -5.3793e-06, -5.1707e-06],
        [-5.4687e-06, -3.7923e-06,  4.7386e-06,  ..., -4.5896e-06,
         -3.8743e-06, -3.9563e-06],
        [-6.3032e-06, -4.8578e-06,  5.4389e-06,  ..., -5.3942e-06,
         -4.7535e-06, -3.9414e-06]], device='cuda:0')
Loss: 0.9287118911743164


Running epoch 1, step 1738, batch 690
Sampled inputs[:2]: tensor([[    0,    12,   266,  ...,   278,   266, 10995],
        [    0,  2546,   300,  ...,    14,  1075,   756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1133e-04,  3.5934e-05, -1.9145e-04,  ...,  1.6567e-04,
          2.2579e-05,  1.8477e-04],
        [-4.1947e-06, -2.8387e-06,  3.4496e-06,  ..., -3.4869e-06,
         -2.8014e-06, -2.9095e-06],
        [-1.1578e-05, -8.1807e-06,  1.0118e-05,  ..., -9.5367e-06,
         -7.7188e-06, -7.8976e-06],
        [-8.2999e-06, -5.5209e-06,  7.1228e-06,  ..., -6.8843e-06,
         -5.5432e-06, -5.9977e-06],
        [-9.5516e-06, -7.0781e-06,  8.1509e-06,  ..., -8.0764e-06,
         -6.8247e-06, -5.9828e-06]], device='cuda:0')
Loss: 0.9336038827896118


Running epoch 1, step 1739, batch 691
Sampled inputs[:2]: tensor([[    0,   221, 18844,  ...,   199, 10174,   259],
        [    0,  1371,   287,  ...,   689,   278, 12774]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3637e-04, -9.9629e-05, -7.2906e-05,  ...,  6.6263e-05,
         -1.7126e-04, -6.0652e-06],
        [-5.6252e-06, -3.7476e-06,  4.4778e-06,  ..., -4.7088e-06,
         -3.7849e-06, -3.9898e-06],
        [-1.5482e-05, -1.0803e-05,  1.3158e-05,  ..., -1.2830e-05,
         -1.0386e-05, -1.0818e-05],
        [-1.1131e-05, -7.2792e-06,  9.2387e-06,  ..., -9.2983e-06,
         -7.4953e-06, -8.2180e-06],
        [-1.3068e-05, -9.5218e-06,  1.0818e-05,  ..., -1.1086e-05,
         -9.3728e-06, -8.4117e-06]], device='cuda:0')
Loss: 0.9597045183181763


Running epoch 1, step 1740, batch 692
Sampled inputs[:2]: tensor([[    0,   396,   221,  ...,  1279,   720,   292],
        [    0, 11853,  1611,  ...,  4413,  4240,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8541e-04, -1.1153e-04, -7.2502e-05,  ...,  2.0622e-05,
         -1.1681e-04, -1.1035e-07],
        [-7.1153e-06, -4.6268e-06,  5.5581e-06,  ..., -5.9679e-06,
         -4.7013e-06, -5.1223e-06],
        [-1.9565e-05, -1.3381e-05,  1.6347e-05,  ..., -1.6272e-05,
         -1.2934e-05, -1.3873e-05],
        [-1.4022e-05, -8.9481e-06,  1.1399e-05,  ..., -1.1772e-05,
         -9.2983e-06, -1.0483e-05],
        [-1.6615e-05, -1.1817e-05,  1.3500e-05,  ..., -1.4126e-05,
         -1.1712e-05, -1.0900e-05]], device='cuda:0')
Loss: 0.9110139012336731


Running epoch 1, step 1741, batch 693
Sampled inputs[:2]: tensor([[    0,   365,   984,  ..., 18562,  4237, 31813],
        [    0,  1732,   292,  ...,  3440,  4010,  1487]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.9095e-05, -1.5963e-04, -1.0441e-04,  ..., -3.4700e-05,
          4.3171e-05,  5.2086e-05],
        [-8.4341e-06, -5.6699e-06,  6.6981e-06,  ..., -7.1302e-06,
         -5.6475e-06, -6.0201e-06],
        [ 1.3708e-04,  1.9887e-04, -2.3245e-04,  ...,  2.3315e-04,
          1.8897e-04,  4.8015e-05],
        [-1.6674e-05, -1.1049e-05,  1.3813e-05,  ..., -1.4126e-05,
         -1.1235e-05, -1.2405e-05],
        [-1.9670e-05, -1.4514e-05,  1.6227e-05,  ..., -1.6913e-05,
         -1.4096e-05, -1.2837e-05]], device='cuda:0')
Loss: 0.9947566986083984


Running epoch 1, step 1742, batch 694
Sampled inputs[:2]: tensor([[    0, 14026,  4137,  ..., 12292,  1553,   278],
        [    0,  1781,   659,  ...,    12,  1478,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5554e-04, -1.5751e-04,  1.7251e-05,  ...,  1.0114e-04,
         -2.5737e-05,  7.5957e-05],
        [-9.8795e-06, -6.6198e-06,  7.6964e-06,  ..., -8.4788e-06,
         -6.6906e-06, -7.1228e-06],
        [ 1.3312e-04,  1.9617e-04, -2.2948e-04,  ...,  2.2956e-04,
          1.8616e-04,  4.5110e-05],
        [-1.9416e-05, -1.2785e-05,  1.5751e-05,  ..., -1.6674e-05,
         -1.3217e-05, -1.4536e-05],
        [-2.3291e-05, -1.7062e-05,  1.8880e-05,  ..., -2.0251e-05,
         -1.6794e-05, -1.5311e-05]], device='cuda:0')
Loss: 0.9352072477340698


Running epoch 1, step 1743, batch 695
Sampled inputs[:2]: tensor([[    0, 16286,  5356,  ...,   590,  2161,     5],
        [    0,   381, 13565,  ...,     9,   847,   300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.0007e-04, -1.0329e-04,  1.5263e-05,  ...,  2.7290e-04,
          1.3467e-04,  1.4104e-04],
        [-1.1191e-05, -7.6182e-06,  8.6278e-06,  ..., -9.6634e-06,
         -7.6890e-06, -8.0578e-06],
        [ 1.2957e-04,  1.9335e-04, -2.2671e-04,  ...,  2.2640e-04,
          1.8347e-04,  4.2636e-05],
        [-2.2024e-05, -1.4752e-05,  1.7658e-05,  ..., -1.9044e-05,
         -1.5259e-05, -1.6488e-05],
        [-2.6390e-05, -1.9610e-05,  2.1234e-05,  ..., -2.3052e-05,
         -1.9267e-05, -1.7293e-05]], device='cuda:0')
Loss: 0.95610111951828
Graident accumulation at epoch 1, step 1743, batch 695
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.3951e-05,  4.3269e-05, -1.3094e-04,  ...,  2.7682e-05,
         -1.4323e-05, -4.5665e-05],
        [-1.0391e-05, -7.1678e-06,  7.7902e-06,  ..., -9.2111e-06,
         -7.0447e-06, -7.2034e-06],
        [ 6.1959e-05,  6.2748e-05, -6.5937e-05,  ...,  5.5249e-05,
          5.4187e-05,  3.0585e-05],
        [ 1.5804e-05,  3.2625e-05, -8.0820e-06,  ...,  1.6477e-05,
          2.7763e-05,  9.8304e-06],
        [-2.6073e-05, -2.0425e-05,  2.0584e-05,  ..., -2.3213e-05,
         -1.9776e-05, -1.6670e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9305e-08, 5.6219e-08, 5.5277e-08,  ..., 2.1397e-08, 1.2454e-07,
         4.9084e-08],
        [9.0662e-11, 5.9582e-11, 2.7087e-11,  ..., 6.2532e-11, 3.2829e-11,
         3.0358e-11],
        [2.5939e-09, 1.7674e-09, 1.1698e-09,  ..., 1.8647e-09, 1.1624e-09,
         7.2228e-10],
        [9.5965e-10, 1.2060e-09, 4.2113e-10,  ..., 9.4635e-10, 7.7092e-10,
         4.0627e-10],
        [4.2446e-10, 2.4751e-10, 9.5750e-11,  ..., 3.1148e-10, 9.5134e-11,
         1.2802e-10]], device='cuda:0')
optimizer state dict: 218.0
lr: [1.4430974891391325e-06, 1.4430974891391325e-06]
scheduler_last_epoch: 218


Running epoch 1, step 1744, batch 696
Sampled inputs[:2]: tensor([[    0,  1075, 14981,  ...,   221,   380,  1075],
        [    0,   278,   266,  ...,   274, 30228,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7374e-05, -3.5761e-05, -1.0286e-04,  ..., -5.0850e-05,
          8.2744e-07,  1.2637e-04],
        [-1.3858e-06, -1.0058e-06,  9.9838e-07,  ..., -1.1623e-06,
         -9.5367e-07, -9.5367e-07],
        [-3.9041e-06, -2.9057e-06,  2.9951e-06,  ..., -3.2037e-06,
         -2.6077e-06, -2.6226e-06],
        [-2.7865e-06, -1.9670e-06,  2.0862e-06,  ..., -2.3246e-06,
         -1.9073e-06, -2.0117e-06],
        [-3.3081e-06, -2.5779e-06,  2.5034e-06,  ..., -2.7865e-06,
         -2.3544e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9871659874916077


Running epoch 1, step 1745, batch 697
Sampled inputs[:2]: tensor([[    0,   287, 14752,  ...,   910, 26097,  1477],
        [    0,   344,  8133,  ...,   278,  1603,   674]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.7649e-05, -1.4351e-05, -4.0335e-05,  ...,  4.0474e-05,
          2.4393e-04,  1.6346e-04],
        [-2.8238e-06, -2.0266e-06,  1.9819e-06,  ..., -2.4065e-06,
         -1.9819e-06, -1.9073e-06],
        [ 2.7888e-05,  8.4660e-05, -2.4218e-05,  ...,  1.0397e-04,
          7.8455e-05,  9.0272e-05],
        [-5.6475e-06, -4.0233e-06,  4.1127e-06,  ..., -4.8131e-06,
         -4.0382e-06, -4.0233e-06],
        [-6.7353e-06, -5.2750e-06,  4.9770e-06,  ..., -5.8115e-06,
         -5.0217e-06, -4.1723e-06]], device='cuda:0')
Loss: 0.9717281460762024


Running epoch 1, step 1746, batch 698
Sampled inputs[:2]: tensor([[    0, 10026,   992,  ...,   273,  2831,  8716],
        [    0,    12,   266,  ...,  5308,   266, 14679]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2419e-04, -6.5090e-05, -3.8729e-04,  ...,  4.9785e-05,
          3.7793e-04,  2.0263e-04],
        [-4.2990e-06, -2.9095e-06,  3.1292e-06,  ..., -3.6433e-06,
         -2.8983e-06, -2.9355e-06],
        [ 2.4014e-05,  8.2216e-05, -2.0985e-05,  ...,  1.0077e-04,
          7.6041e-05,  8.7634e-05],
        [-8.4341e-06, -5.6624e-06,  6.3628e-06,  ..., -7.1377e-06,
         -5.7667e-06, -6.0201e-06],
        [-1.0073e-05, -7.4655e-06,  7.6741e-06,  ..., -8.6278e-06,
         -7.2420e-06, -6.2436e-06]], device='cuda:0')
Loss: 0.9489336609840393


Running epoch 1, step 1747, batch 699
Sampled inputs[:2]: tensor([[    0,  1110, 26330,  ...,  1558,   674,  2351],
        [    0,  3445,   328,  ...,   278, 12323,   554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8928e-04, -1.1792e-04,  3.4202e-05,  ..., -5.0701e-05,
          2.6034e-04,  2.3108e-04],
        [-5.6550e-06, -3.8221e-06,  4.1276e-06,  ..., -4.8503e-06,
         -3.8818e-06, -3.8743e-06],
        [ 2.0259e-05,  7.9549e-05, -1.8034e-05,  ...,  9.7491e-05,
          7.3358e-05,  8.5071e-05],
        [-1.1131e-05, -7.4506e-06,  8.4192e-06,  ..., -9.5218e-06,
         -7.7188e-06, -7.9721e-06],
        [-1.3322e-05, -9.8944e-06,  1.0177e-05,  ..., -1.1533e-05,
         -9.7156e-06, -8.2701e-06]], device='cuda:0')
Loss: 0.9620795845985413


Running epoch 1, step 1748, batch 700
Sampled inputs[:2]: tensor([[    0,    45,  6556,  ...,  1477,   352,  1611],
        [    0,    14,   475,  ..., 44038,    12,   894]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5646e-04, -9.2347e-05,  3.4867e-05,  ..., -1.7816e-04,
          5.0059e-04,  3.0248e-04],
        [-7.1228e-06, -4.9099e-06,  5.1260e-06,  ..., -6.1914e-06,
         -4.9323e-06, -4.9397e-06],
        [ 1.6146e-05,  7.6315e-05, -1.5054e-05,  ...,  9.3751e-05,
          7.0363e-05,  8.2106e-05],
        [-1.3977e-05, -9.5665e-06,  1.0416e-05,  ..., -1.2144e-05,
         -9.7901e-06, -1.0118e-05],
        [-1.6913e-05, -1.2785e-05,  1.2726e-05,  ..., -1.4856e-05,
         -1.2457e-05, -1.0684e-05]], device='cuda:0')
Loss: 0.9772394895553589


Running epoch 1, step 1749, batch 701
Sampled inputs[:2]: tensor([[   0,  689, 3953,  ...,  461,  943,  352],
        [   0, 1644, 1742,  ...,  287, 1704, 2044]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.1841e-04, -1.5186e-04, -3.2097e-05,  ..., -2.2613e-04,
          5.8912e-04,  2.5340e-04],
        [-8.4788e-06, -5.9083e-06,  6.2361e-06,  ..., -7.3239e-06,
         -5.8338e-06, -5.8375e-06],
        [ 1.2301e-05,  7.3350e-05, -1.1716e-05,  ...,  9.0547e-05,
          6.7785e-05,  7.9558e-05],
        [-1.6689e-05, -1.1548e-05,  1.2740e-05,  ..., -1.4409e-05,
         -1.1608e-05, -1.2010e-05],
        [-1.9982e-05, -1.5303e-05,  1.5348e-05,  ..., -1.7494e-05,
         -1.4678e-05, -1.2554e-05]], device='cuda:0')
Loss: 0.9670152068138123


Running epoch 1, step 1750, batch 702
Sampled inputs[:2]: tensor([[    0,    12, 17906,  ...,  2086,   287,  4419],
        [    0,   287,  9430,  ...,  3121,   352,   360]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1984e-04, -7.4716e-05, -1.2730e-04,  ..., -2.5486e-04,
          5.3639e-04,  2.8916e-04],
        [-9.8571e-06, -6.8992e-06,  7.4208e-06,  ..., -8.4713e-06,
         -6.7540e-06, -6.7428e-06],
        [ 7.7018e-05,  1.4874e-04, -4.2848e-05,  ...,  1.7818e-04,
          1.2648e-04,  1.0169e-04],
        [-1.9461e-05, -1.3500e-05,  1.5229e-05,  ..., -1.6704e-05,
         -1.3456e-05, -1.3918e-05],
        [-2.3171e-05, -1.7822e-05,  1.8165e-05,  ..., -2.0191e-05,
         -1.6958e-05, -1.4476e-05]], device='cuda:0')
Loss: 0.9546157121658325


Running epoch 1, step 1751, batch 703
Sampled inputs[:2]: tensor([[    0,   266,  2604,  ...,   278,  4035,  4165],
        [    0, 24674,   513,  ...,  6099,    12,  4863]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7976e-04, -1.0857e-04, -2.7797e-05,  ..., -3.7605e-04,
          3.8572e-04,  2.6544e-04],
        [-1.1243e-05, -7.9274e-06,  8.6576e-06,  ..., -9.6112e-06,
         -7.6517e-06, -7.6592e-06],
        [ 7.3174e-05,  1.4577e-04, -3.9257e-05,  ...,  1.7499e-04,
          1.2395e-04,  9.9099e-05],
        [-2.2203e-05, -1.5512e-05,  1.7777e-05,  ..., -1.8954e-05,
         -1.5229e-05, -1.5825e-05],
        [-2.6301e-05, -2.0370e-05,  2.1011e-05,  ..., -2.2843e-05,
         -1.9193e-05, -1.6429e-05]], device='cuda:0')
Loss: 0.9742927551269531
Graident accumulation at epoch 1, step 1751, batch 703
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1453e-04,  2.8085e-05, -1.2062e-04,  ..., -1.2691e-05,
          2.5681e-05, -1.4555e-05],
        [-1.0476e-05, -7.2438e-06,  7.8770e-06,  ..., -9.2511e-06,
         -7.1054e-06, -7.2490e-06],
        [ 6.3081e-05,  7.1051e-05, -6.3269e-05,  ...,  6.7223e-05,
          6.1162e-05,  3.7437e-05],
        [ 1.2003e-05,  2.7812e-05, -5.4961e-06,  ...,  1.2934e-05,
          2.3464e-05,  7.2649e-06],
        [-2.6096e-05, -2.0420e-05,  2.0626e-05,  ..., -2.3176e-05,
         -1.9718e-05, -1.6646e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9475e-08, 5.6175e-08, 5.5222e-08,  ..., 2.1517e-08, 1.2456e-07,
         4.9105e-08],
        [9.0698e-11, 5.9585e-11, 2.7135e-11,  ..., 6.2562e-11, 3.2855e-11,
         3.0386e-11],
        [2.5966e-09, 1.7869e-09, 1.1702e-09,  ..., 1.8935e-09, 1.1766e-09,
         7.3138e-10],
        [9.5918e-10, 1.2050e-09, 4.2103e-10,  ..., 9.4576e-10, 7.7038e-10,
         4.0612e-10],
        [4.2473e-10, 2.4767e-10, 9.6096e-11,  ..., 3.1169e-10, 9.5407e-11,
         1.2817e-10]], device='cuda:0')
optimizer state dict: 219.0
lr: [1.3797827447013867e-06, 1.3797827447013867e-06]
scheduler_last_epoch: 219


Running epoch 1, step 1752, batch 704
Sampled inputs[:2]: tensor([[    0,    14,  3080,  ...,   910,   266,  5275],
        [    0,  1626,     5,  ..., 10536,  1763,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1651e-05,  1.3868e-04,  9.3995e-05,  ..., -7.0453e-05,
         -1.4352e-04,  2.2327e-05],
        [-1.5572e-06, -9.6858e-07,  1.0505e-06,  ..., -1.2815e-06,
         -1.0058e-06, -1.1474e-06],
        [-4.3511e-06, -2.8908e-06,  3.1590e-06,  ..., -3.5614e-06,
         -2.8610e-06, -3.2187e-06],
        [-2.9504e-06, -1.8105e-06,  2.0713e-06,  ..., -2.4438e-06,
         -1.9222e-06, -2.2352e-06],
        [-3.8445e-06, -2.6226e-06,  2.7120e-06,  ..., -3.2187e-06,
         -2.6822e-06, -2.6524e-06]], device='cuda:0')
Loss: 0.9202435612678528


Running epoch 1, step 1753, batch 705
Sampled inputs[:2]: tensor([[    0, 41638,  4573,  ...,   259,   790,  1416],
        [    0,  1167,  2667,  ...,  4769,    13,  5019]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.2823e-05,  1.0465e-04,  2.3341e-04,  ..., -6.2866e-05,
         -1.2239e-04,  7.4278e-05],
        [-2.9206e-06, -1.9521e-06,  2.0638e-06,  ..., -2.5183e-06,
         -2.0042e-06, -2.1532e-06],
        [-8.0019e-06, -5.6773e-06,  6.0946e-06,  ..., -6.7949e-06,
         -5.5432e-06, -5.8413e-06],
        [-5.6028e-06, -3.7029e-06,  4.1276e-06,  ..., -4.8280e-06,
         -3.8743e-06, -4.2617e-06],
        [-7.0930e-06, -5.1409e-06,  5.2601e-06,  ..., -6.1542e-06,
         -5.1856e-06, -4.8280e-06]], device='cuda:0')
Loss: 0.9547694325447083


Running epoch 1, step 1754, batch 706
Sampled inputs[:2]: tensor([[    0,   344,  2183,  ...,    14,   759,   596],
        [    0,     8,    39,  ...,  7406,    13, 10896]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3482e-05, -5.2055e-05,  2.4915e-04,  ...,  4.1727e-06,
         -1.4446e-04,  1.2940e-04],
        [-4.4033e-06, -2.9057e-06,  3.0696e-06,  ..., -3.8967e-06,
         -3.0696e-06, -3.2261e-06],
        [-1.1846e-05, -8.3297e-06,  8.8960e-06,  ..., -1.0327e-05,
         -8.3447e-06, -8.5384e-06],
        [-8.2999e-06, -5.4464e-06,  6.0126e-06,  ..., -7.3463e-06,
         -5.8711e-06, -6.2734e-06],
        [-1.0684e-05, -7.6890e-06,  7.8082e-06,  ..., -9.4920e-06,
         -7.8976e-06, -7.1973e-06]], device='cuda:0')
Loss: 0.9750519394874573


Running epoch 1, step 1755, batch 707
Sampled inputs[:2]: tensor([[    0,    15,    19,  ...,    12,   287,  7897],
        [    0,   259,  1380,  ...,   287, 10221,   280]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0118e-05, -1.2348e-04,  2.4469e-04,  ..., -1.8733e-04,
          7.4830e-05,  4.7462e-06],
        [-5.7220e-06, -3.9488e-06,  4.0457e-06,  ..., -5.0813e-06,
         -4.0755e-06, -4.1611e-06],
        [-1.5527e-05, -1.1340e-05,  1.1846e-05,  ..., -1.3575e-05,
         -1.1161e-05, -1.1101e-05],
        [-1.0923e-05, -7.4878e-06,  8.0541e-06,  ..., -9.7007e-06,
         -7.9125e-06, -8.2254e-06],
        [-1.3784e-05, -1.0327e-05,  1.0252e-05,  ..., -1.2279e-05,
         -1.0401e-05, -9.1791e-06]], device='cuda:0')
Loss: 0.944230318069458


Running epoch 1, step 1756, batch 708
Sampled inputs[:2]: tensor([[    0, 41855,     9,  ..., 33073,   401,  4528],
        [    0,   266,  2555,  ...,   587,    14, 14947]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.4715e-05, -1.6789e-04,  2.4632e-04,  ..., -2.2289e-04,
          1.3771e-04, -1.4896e-04],
        [-7.1302e-06, -4.9174e-06,  5.1111e-06,  ..., -6.3255e-06,
         -5.0440e-06, -5.1521e-06],
        [-1.9372e-05, -1.4067e-05,  1.4976e-05,  ..., -1.6883e-05,
         -1.3754e-05, -1.3739e-05],
        [-1.3605e-05, -9.2834e-06,  1.0185e-05,  ..., -1.2055e-05,
         -9.7454e-06, -1.0177e-05],
        [-1.7151e-05, -1.2815e-05,  1.2904e-05,  ..., -1.5259e-05,
         -1.2845e-05, -1.1295e-05]], device='cuda:0')
Loss: 0.9337757229804993


Running epoch 1, step 1757, batch 709
Sampled inputs[:2]: tensor([[    0,  9419,   221,  ...,    15, 22168,     9],
        [    0,  1276,   292,  ...,    83,  1837,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3253e-04, -1.6225e-04,  1.2822e-04,  ..., -2.4711e-04,
          3.4587e-04, -4.1117e-05],
        [-8.5533e-06, -5.8226e-06,  6.2510e-06,  ..., -7.5474e-06,
         -5.8934e-06, -6.1020e-06],
        [-2.3305e-05, -1.6719e-05,  1.8299e-05,  ..., -2.0251e-05,
         -1.6138e-05, -1.6347e-05],
        [-1.6406e-05, -1.1034e-05,  1.2524e-05,  ..., -1.4454e-05,
         -1.1414e-05, -1.2144e-05],
        [-2.0295e-05, -1.5080e-05,  1.5497e-05,  ..., -1.8030e-05,
         -1.4916e-05, -1.3217e-05]], device='cuda:0')
Loss: 0.964836597442627


Running epoch 1, step 1758, batch 710
Sampled inputs[:2]: tensor([[    0,   313,    66,  ...,   894,  2973, 25074],
        [    0,  1765,  5370,  ...,  1711,   292,   380]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.5570e-05, -1.2412e-04,  1.8646e-04,  ..., -1.4932e-04,
          2.9942e-04, -3.9411e-05],
        [-9.9540e-06, -6.9030e-06,  7.4580e-06,  ..., -8.7246e-06,
         -6.8098e-06, -6.9961e-06],
        [-2.7239e-05, -1.9878e-05,  2.1860e-05,  ..., -2.3559e-05,
         -1.8731e-05, -1.8865e-05],
        [-1.9237e-05, -1.3195e-05,  1.5058e-05,  ..., -1.6823e-05,
         -1.3262e-05, -1.4037e-05],
        [-2.3514e-05, -1.7792e-05,  1.8358e-05,  ..., -2.0802e-05,
         -1.7196e-05, -1.5125e-05]], device='cuda:0')
Loss: 1.0031468868255615


Running epoch 1, step 1759, batch 711
Sampled inputs[:2]: tensor([[   0, 1070, 5746,  ...,  278,  689,   14],
        [   0,   17,  590,  ..., 1412,   35, 5015]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.4243e-05,  4.3756e-05,  4.2757e-04,  ..., -2.1619e-04,
          1.9471e-04, -8.9229e-06],
        [-1.1452e-05, -7.9162e-06,  8.4639e-06,  ..., -1.0006e-05,
         -7.8306e-06, -8.0392e-06],
        [-3.1322e-05, -2.2873e-05,  2.4825e-05,  ..., -2.7061e-05,
         -2.1607e-05, -2.1711e-05],
        [-2.2084e-05, -1.5132e-05,  1.7039e-05,  ..., -1.9282e-05,
         -1.5259e-05, -1.6093e-05],
        [-2.7061e-05, -2.0474e-05,  2.0862e-05,  ..., -2.3916e-05,
         -1.9833e-05, -1.7434e-05]], device='cuda:0')
Loss: 0.921716570854187
Graident accumulation at epoch 1, step 1759, batch 711
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0020,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0650e-04,  2.9652e-05, -6.5803e-05,  ..., -3.3042e-05,
          4.2584e-05, -1.3992e-05],
        [-1.0574e-05, -7.3110e-06,  7.9356e-06,  ..., -9.3266e-06,
         -7.1779e-06, -7.3280e-06],
        [ 5.3640e-05,  6.1658e-05, -5.4460e-05,  ...,  5.7795e-05,
          5.2885e-05,  3.1522e-05],
        [ 8.5947e-06,  2.3517e-05, -3.2425e-06,  ...,  9.7122e-06,
          1.9591e-05,  4.9291e-06],
        [-2.6192e-05, -2.0425e-05,  2.0650e-05,  ..., -2.3250e-05,
         -1.9729e-05, -1.6725e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9417e-08, 5.6120e-08, 5.5350e-08,  ..., 2.1543e-08, 1.2448e-07,
         4.9056e-08],
        [9.0738e-11, 5.9589e-11, 2.7179e-11,  ..., 6.2599e-11, 3.2883e-11,
         3.0420e-11],
        [2.5950e-09, 1.7857e-09, 1.1697e-09,  ..., 1.8923e-09, 1.1759e-09,
         7.3112e-10],
        [9.5871e-10, 1.2041e-09, 4.2089e-10,  ..., 9.4519e-10, 7.6984e-10,
         4.0597e-10],
        [4.2503e-10, 2.4784e-10, 9.6435e-11,  ..., 3.1195e-10, 9.5705e-11,
         1.2834e-10]], device='cuda:0')
optimizer state dict: 220.0
lr: [1.3177852447071903e-06, 1.3177852447071903e-06]
scheduler_last_epoch: 220


Running epoch 1, step 1760, batch 712
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,  5503,   259,  1036],
        [    0, 10205,   342,  ...,  6354, 12230,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6715e-05, -2.3766e-05, -8.4137e-05,  ...,  9.1525e-05,
         -1.3960e-04,  2.8199e-05],
        [-1.5646e-06, -8.8662e-07,  1.1697e-06,  ..., -1.2815e-06,
         -8.7917e-07, -1.1399e-06],
        [-4.3213e-06, -2.6226e-06,  3.4571e-06,  ..., -3.5465e-06,
         -2.4587e-06, -3.1441e-06],
        [-2.9206e-06, -1.6615e-06,  2.2948e-06,  ..., -2.3842e-06,
         -1.6317e-06, -2.1905e-06],
        [-3.7402e-06, -2.3246e-06,  2.8759e-06,  ..., -3.1143e-06,
         -2.2799e-06, -2.5481e-06]], device='cuda:0')
Loss: 0.9308556914329529


Running epoch 1, step 1761, batch 713
Sampled inputs[:2]: tensor([[    0,  3398,  6361,  ..., 12942,   518,  4066],
        [    0,  1086,  5564,  ..., 29319, 32982,   344]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7279e-04,  3.0518e-05, -9.5682e-05,  ...,  5.8007e-05,
         -1.9313e-04,  4.2810e-05],
        [-3.0398e-06, -1.9372e-06,  2.2426e-06,  ..., -2.5555e-06,
         -1.8403e-06, -2.1532e-06],
        [-8.4341e-06, -5.6922e-06,  6.6608e-06,  ..., -7.0781e-06,
         -5.1707e-06, -5.9456e-06],
        [-5.7667e-06, -3.7029e-06,  4.4554e-06,  ..., -4.8429e-06,
         -3.4943e-06, -4.2170e-06],
        [-7.2718e-06, -5.0664e-06,  5.5730e-06,  ..., -6.2138e-06,
         -4.7684e-06, -4.7684e-06]], device='cuda:0')
Loss: 0.9631965756416321


Running epoch 1, step 1762, batch 714
Sampled inputs[:2]: tensor([[    0,    14, 13078,  ...,  1994,    12,   287],
        [    0,    14,  2729,  ...,   266,  1659, 14362]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5011e-05, -8.9873e-05, -2.9743e-04,  ...,  1.0341e-04,
          1.1823e-04,  1.4122e-04],
        [-4.5002e-06, -2.8610e-06,  3.2634e-06,  ..., -3.8743e-06,
         -2.8312e-06, -3.2410e-06],
        [-1.2457e-05, -8.3596e-06,  9.7305e-06,  ..., -1.0639e-05,
         -7.8976e-06, -8.8811e-06],
        [-8.5533e-06, -5.4389e-06,  6.4969e-06,  ..., -7.3463e-06,
         -5.3942e-06, -6.3628e-06],
        [-1.0788e-05, -7.4804e-06,  8.1807e-06,  ..., -9.3877e-06,
         -7.3016e-06, -7.1228e-06]], device='cuda:0')
Loss: 0.9585486054420471


Running epoch 1, step 1763, batch 715
Sampled inputs[:2]: tensor([[    0, 22568,   287,  ...,    12,   471,    12],
        [    0, 18901,     5,  ...,  2253,   278, 17423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7878e-04, -1.7058e-05, -2.2977e-04,  ...,  1.9943e-05,
          2.4820e-04,  1.7786e-04],
        [-5.8562e-06, -3.9339e-06,  4.1649e-06,  ..., -5.1707e-06,
         -3.9116e-06, -4.2096e-06],
        [-1.6093e-05, -1.1340e-05,  1.2338e-05,  ..., -1.4052e-05,
         -1.0759e-05, -1.1414e-05],
        [-1.1235e-05, -7.5251e-06,  8.3372e-06,  ..., -9.9242e-06,
         -7.5549e-06, -8.3596e-06],
        [-1.4096e-05, -1.0267e-05,  1.0520e-05,  ..., -1.2532e-05,
         -1.0014e-05, -9.2536e-06]], device='cuda:0')
Loss: 0.9809379577636719


Running epoch 1, step 1764, batch 716
Sampled inputs[:2]: tensor([[    0,   396,   298,  ...,    52,  5065,    13],
        [    0, 43587,  1390,  ...,    12,   768,  1952]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5707e-04,  4.5896e-05, -1.2535e-05,  ...,  8.1591e-05,
          3.3037e-04,  2.0910e-04],
        [-7.4282e-06, -4.9099e-06,  5.2676e-06,  ..., -6.4895e-06,
         -4.8876e-06, -5.3495e-06],
        [-2.0355e-05, -1.4126e-05,  1.5572e-05,  ..., -1.7583e-05,
         -1.3426e-05, -1.4454e-05],
        [-1.4171e-05, -9.3207e-06,  1.0468e-05,  ..., -1.2383e-05,
         -9.3803e-06, -1.0535e-05],
        [-1.7881e-05, -1.2815e-05,  1.3322e-05,  ..., -1.5736e-05,
         -1.2547e-05, -1.1772e-05]], device='cuda:0')
Loss: 0.9310383796691895


Running epoch 1, step 1765, batch 717
Sampled inputs[:2]: tensor([[    0,  7117,   278,  ...,   287,   266,   944],
        [    0,   344,  8260,  ..., 16020, 18216, 11348]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9111e-04,  1.4142e-04,  1.5880e-04,  ...,  1.6267e-05,
          4.4754e-04,  2.8064e-04],
        [-8.8066e-06, -5.8636e-06,  6.1765e-06,  ..., -7.6964e-06,
         -5.9232e-06, -6.2957e-06],
        [-2.4095e-05, -1.6928e-05,  1.8314e-05,  ..., -2.0862e-05,
         -1.6332e-05, -1.6987e-05],
        [-1.6898e-05, -1.1191e-05,  1.2316e-05,  ..., -1.4812e-05,
         -1.1511e-05, -1.2472e-05],
        [ 4.4584e-05,  6.3537e-05, -5.1455e-05,  ...,  4.7408e-05,
          4.2254e-05, -8.1497e-06]], device='cuda:0')
Loss: 0.9631089568138123


Running epoch 1, step 1766, batch 718
Sampled inputs[:2]: tensor([[   0,  287,  516,  ..., 2386, 3492, 1663],
        [   0,  408, 1782,  ...,  271,  729, 1692]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6410e-04,  1.7268e-04,  3.5414e-04,  ...,  8.3765e-05,
          2.3417e-04,  1.7358e-04],
        [-1.0222e-05, -6.8620e-06,  7.3537e-06,  ..., -8.8662e-06,
         -6.8694e-06, -7.1712e-06],
        [-2.8029e-05, -1.9833e-05,  2.1785e-05,  ..., -2.4125e-05,
         -1.8999e-05, -1.9446e-05],
        [-1.9670e-05, -1.3113e-05,  1.4730e-05,  ..., -1.7092e-05,
         -1.3344e-05, -1.4275e-05],
        [ 4.1336e-05,  6.1003e-05, -4.8684e-05,  ...,  4.4651e-05,
          3.9900e-05, -1.0005e-05]], device='cuda:0')
Loss: 0.9475868940353394


Running epoch 1, step 1767, batch 719
Sampled inputs[:2]: tensor([[   0, 1197,  729,  ...,  674,  369, 8222],
        [   0,  957, 1231,  ...,  800,  342, 1398]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3848e-04,  2.2747e-04,  3.4162e-04,  ..., -3.3006e-05,
          2.2348e-04,  2.1187e-04],
        [-1.1638e-05, -7.7523e-06,  8.3745e-06,  ..., -1.0118e-05,
         -7.8455e-06, -8.2441e-06],
        [ 5.1008e-04,  4.5181e-04, -4.0890e-04,  ...,  3.7399e-04,
          3.6151e-04,  9.7563e-05],
        [-2.2396e-05, -1.4804e-05,  1.6786e-05,  ..., -1.9491e-05,
         -1.5229e-05, -1.6406e-05],
        [ 3.7864e-05,  5.8664e-05, -4.6061e-05,  ...,  4.1611e-05,
          3.7411e-05, -1.2359e-05]], device='cuda:0')
Loss: 0.9845516681671143
Graident accumulation at epoch 1, step 1767, batch 719
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 6.2005e-05,  4.9434e-05, -2.5061e-05,  ..., -3.3038e-05,
          6.0674e-05,  8.5945e-06],
        [-1.0680e-05, -7.3552e-06,  7.9795e-06,  ..., -9.4057e-06,
         -7.2447e-06, -7.4196e-06],
        [ 9.9285e-05,  1.0067e-04, -8.9903e-05,  ...,  8.9414e-05,
          8.3748e-05,  3.8126e-05],
        [ 5.4955e-06,  1.9685e-05, -1.2397e-06,  ...,  6.7919e-06,
          1.6109e-05,  2.7955e-06],
        [-1.9787e-05, -1.2516e-05,  1.3979e-05,  ..., -1.6764e-05,
         -1.4015e-05, -1.6288e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9472e-08, 5.6116e-08, 5.5411e-08,  ..., 2.1522e-08, 1.2440e-07,
         4.9052e-08],
        [9.0783e-11, 5.9589e-11, 2.7222e-11,  ..., 6.2639e-11, 3.2912e-11,
         3.0458e-11],
        [2.8526e-09, 1.9880e-09, 1.3357e-09,  ..., 2.0303e-09, 1.3054e-09,
         7.3990e-10],
        [9.5825e-10, 1.2031e-09, 4.2076e-10,  ..., 9.4463e-10, 7.6931e-10,
         4.0583e-10],
        [4.2604e-10, 2.5104e-10, 9.8460e-11,  ..., 3.1337e-10, 9.7009e-11,
         1.2837e-10]], device='cuda:0')
optimizer state dict: 221.0
lr: [1.2571144629157273e-06, 1.2571144629157273e-06]
scheduler_last_epoch: 221


Running epoch 1, step 1768, batch 720
Sampled inputs[:2]: tensor([[    0,  1862,   674,  ...,   391,   266,  7688],
        [    0,   508,  1548,  ...,   494, 10792,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.2314e-04, -1.5104e-04, -9.7969e-05,  ..., -3.9935e-05,
          1.7990e-04, -1.3403e-04],
        [-1.2517e-06, -8.5682e-07,  9.1270e-07,  ..., -1.1846e-06,
         -8.6427e-07, -9.9838e-07],
        [-3.5465e-06, -2.4140e-06,  2.8610e-06,  ..., -3.1292e-06,
         -2.2650e-06, -2.5630e-06],
        [-2.5183e-06, -1.6615e-06,  1.9222e-06,  ..., -2.3544e-06,
         -1.6987e-06, -2.0415e-06],
        [-3.0100e-06, -2.0862e-06,  2.3544e-06,  ..., -2.6822e-06,
         -2.0415e-06, -1.9521e-06]], device='cuda:0')
Loss: 0.9383567571640015


Running epoch 1, step 1769, batch 721
Sampled inputs[:2]: tensor([[    0,  3951,    77,  ...,  7062,   278,   600],
        [    0, 13245,  1503,  ...,    14,  5605,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.4284e-04, -1.9548e-04, -3.4787e-05,  ..., -6.2206e-05,
          1.8428e-04, -1.6728e-04],
        [-2.6003e-06, -1.9446e-06,  2.0303e-06,  ..., -2.3395e-06,
         -1.8068e-06, -1.8477e-06],
        [-7.3463e-06, -5.5879e-06,  6.1840e-06,  ..., -6.3777e-06,
         -4.9174e-06, -4.9919e-06],
        [-5.2303e-06, -3.8221e-06,  4.2766e-06,  ..., -4.6492e-06,
         -3.5763e-06, -3.8370e-06],
        [-6.1095e-06, -4.8131e-06,  5.0068e-06,  ..., -5.4091e-06,
         -4.3809e-06, -3.7700e-06]], device='cuda:0')
Loss: 0.9761164784431458


Running epoch 1, step 1770, batch 722
Sampled inputs[:2]: tensor([[    0,   531,    20,  ...,    12,  1644,   680],
        [    0,  1607, 26394,  ...,    19,   471,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5150e-04, -7.9918e-05,  1.3535e-05,  ..., -9.7608e-05,
          8.8971e-05, -2.3876e-04],
        [-3.9786e-06, -2.9653e-06,  3.1330e-06,  ..., -3.5092e-06,
         -2.7604e-06, -2.7418e-06],
        [-1.1221e-05, -8.5384e-06,  9.4622e-06,  ..., -9.6112e-06,
         -7.5847e-06, -7.4506e-06],
        [-7.9572e-06, -5.7891e-06,  6.5416e-06,  ..., -6.9439e-06,
         -5.4538e-06, -5.6848e-06],
        [-9.2536e-06, -7.3165e-06,  7.6145e-06,  ..., -8.0913e-06,
         -6.6906e-06, -5.6028e-06]], device='cuda:0')
Loss: 0.9547377824783325


Running epoch 1, step 1771, batch 723
Sampled inputs[:2]: tensor([[   0,  894,   73,  ..., 2323,  909, 4103],
        [   0,  365,  925,  ...,  909,  598,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4410e-04, -2.4164e-05,  1.8423e-05,  ..., -1.1464e-04,
          5.8487e-05, -2.3389e-04],
        [-5.5730e-06, -4.0531e-06,  4.2580e-06,  ..., -4.8280e-06,
         -3.7290e-06, -3.7923e-06],
        [ 5.8497e-05,  1.0218e-04, -5.5133e-06,  ...,  3.6762e-05,
          8.0585e-05,  3.7429e-05],
        [-1.0967e-05, -7.8455e-06,  8.7619e-06,  ..., -9.4324e-06,
         -7.3165e-06, -7.7561e-06],
        [-1.2964e-05, -1.0118e-05,  1.0341e-05,  ..., -1.1235e-05,
         -9.1791e-06, -7.8976e-06]], device='cuda:0')
Loss: 0.949640691280365


Running epoch 1, step 1772, batch 724
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,    13,  2853,   445],
        [    0,   266, 10726,  ..., 13973, 22191, 15913]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.6824e-05, -2.0357e-04,  2.9333e-05,  ...,  1.3309e-05,
          3.3602e-05, -1.3231e-04],
        [-6.8769e-06, -4.9621e-06,  5.4054e-06,  ..., -5.9307e-06,
         -4.5411e-06, -4.6901e-06],
        [ 5.4623e-05,  9.9391e-05, -1.9519e-06,  ...,  3.3484e-05,
          7.8186e-05,  3.4747e-05],
        [-1.3694e-05, -9.7007e-06,  1.1250e-05,  ..., -1.1742e-05,
         -8.9854e-06, -9.7379e-06],
        [-1.5944e-05, -1.2398e-05,  1.3024e-05,  ..., -1.3813e-05,
         -1.1191e-05, -9.7975e-06]], device='cuda:0')
Loss: 0.9698224067687988


Running epoch 1, step 1773, batch 725
Sampled inputs[:2]: tensor([[    0,  1253,  3197,  ...,   271,   266, 27896],
        [    0,   271,   266,  ...,    70,    27,  5311]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6437e-05, -2.1159e-04, -7.0006e-05,  ..., -9.6574e-05,
          1.0069e-04, -1.6282e-04],
        [-8.2701e-06, -5.9381e-06,  6.6049e-06,  ..., -7.0930e-06,
         -5.3830e-06, -5.5917e-06],
        [ 5.0659e-05,  9.6486e-05,  1.6393e-06,  ...,  3.0190e-05,
          7.5787e-05,  3.2169e-05],
        [-1.6466e-05, -1.1623e-05,  1.3739e-05,  ..., -1.4037e-05,
         -1.0639e-05, -1.1623e-05],
        [-1.9044e-05, -1.4812e-05,  1.5751e-05,  ..., -1.6451e-05,
         -1.3232e-05, -1.1638e-05]], device='cuda:0')
Loss: 0.9678625464439392


Running epoch 1, step 1774, batch 726
Sampled inputs[:2]: tensor([[    0,  6481,   298,  ...,  6145, 16858,   824],
        [    0,  6795,  1728,  ...,   578,    19,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6585e-05, -2.4057e-04, -1.2623e-05,  ..., -5.4688e-05,
          2.1735e-04, -7.7431e-05],
        [-9.6634e-06, -6.8136e-06,  7.6182e-06,  ..., -8.2925e-06,
         -6.2697e-06, -6.5900e-06],
        [ 4.6815e-05,  9.3967e-05,  4.6195e-06,  ...,  2.6942e-05,
          7.3358e-05,  2.9472e-05],
        [-1.9222e-05, -1.3322e-05,  1.5795e-05,  ..., -1.6406e-05,
         -1.2405e-05, -1.3664e-05],
        [-2.2426e-05, -1.7136e-05,  1.8284e-05,  ..., -1.9372e-05,
         -1.5512e-05, -1.3828e-05]], device='cuda:0')
Loss: 0.9546495079994202


Running epoch 1, step 1775, batch 727
Sampled inputs[:2]: tensor([[    0,   560,   199,  ...,   292, 12605,  2096],
        [    0,    14,   747,  ...,  8271,   365,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1990e-04, -1.8868e-04,  5.5549e-05,  ...,  1.5014e-04,
         -4.9080e-05, -7.1690e-04],
        [-1.1027e-05, -7.6890e-06,  8.5495e-06,  ..., -9.5367e-06,
         -7.2308e-06, -7.7002e-06],
        [ 4.3119e-05,  9.1464e-05,  7.4209e-06,  ...,  2.3649e-05,
          7.0795e-05,  2.6566e-05],
        [-2.1815e-05, -1.4938e-05,  1.7650e-05,  ..., -1.8790e-05,
         -1.4246e-05, -1.5825e-05],
        [-2.5690e-05, -1.9372e-05,  2.0638e-05,  ..., -2.2322e-05,
         -1.7896e-05, -1.6183e-05]], device='cuda:0')
Loss: 0.8914909362792969
Graident accumulation at epoch 1, step 1775, batch 727
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 8.7795e-05,  2.5622e-05, -1.7000e-05,  ..., -1.4720e-05,
          4.9698e-05, -6.3954e-05],
        [-1.0715e-05, -7.3885e-06,  8.0365e-06,  ..., -9.4188e-06,
         -7.2433e-06, -7.4477e-06],
        [ 9.3668e-05,  9.9753e-05, -8.0171e-05,  ...,  8.2838e-05,
          8.2453e-05,  3.6970e-05],
        [ 2.7645e-06,  1.6223e-05,  6.4934e-07,  ...,  4.2337e-06,
          1.3074e-05,  9.3348e-07],
        [-2.0377e-05, -1.3202e-05,  1.4645e-05,  ..., -1.7319e-05,
         -1.4403e-05, -1.6278e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9515e-08, 5.6096e-08, 5.5359e-08,  ..., 2.1523e-08, 1.2428e-07,
         4.9517e-08],
        [9.0813e-11, 5.9589e-11, 2.7268e-11,  ..., 6.2668e-11, 3.2931e-11,
         3.0487e-11],
        [2.8516e-09, 1.9944e-09, 1.3344e-09,  ..., 2.0288e-09, 1.3091e-09,
         7.3987e-10],
        [9.5777e-10, 1.2021e-09, 4.2065e-10,  ..., 9.4403e-10, 7.6874e-10,
         4.0568e-10],
        [4.2628e-10, 2.5116e-10, 9.8788e-11,  ..., 3.1356e-10, 9.7232e-11,
         1.2850e-10]], device='cuda:0')
optimizer state dict: 222.0
lr: [1.1977796703520529e-06, 1.1977796703520529e-06]
scheduler_last_epoch: 222


Running epoch 1, step 1776, batch 728
Sampled inputs[:2]: tensor([[    0,   278,   266,  ...,   380,  4053,   352],
        [    0,  2548,   720,  ...,  1795,  1109, 32948]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0919e-04, -9.0842e-05,  3.5314e-05,  ..., -1.0786e-04,
          5.3187e-05,  7.4165e-05],
        [-1.3858e-06, -9.9093e-07,  1.1325e-06,  ..., -1.1176e-06,
         -8.5682e-07, -8.9779e-07],
        [-4.0531e-06, -2.9653e-06,  3.4869e-06,  ..., -3.2485e-06,
         -2.5034e-06, -2.6226e-06],
        [-2.8759e-06, -1.9968e-06,  2.4438e-06,  ..., -2.2948e-06,
         -1.7583e-06, -1.9372e-06],
        [-3.1888e-06, -2.4438e-06,  2.6673e-06,  ..., -2.6226e-06,
         -2.1160e-06, -1.9222e-06]], device='cuda:0')
Loss: 0.9486876130104065


Running epoch 1, step 1777, batch 729
Sampled inputs[:2]: tensor([[    0,  4834,   278,  ...,    13,  8382,   669],
        [    0,   300, 12579,  ...,  1722,   369,  5049]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1610e-06, -9.0842e-05, -1.2839e-05,  ..., -7.2902e-05,
          2.0275e-04,  1.6260e-04],
        [-2.7791e-06, -2.0564e-06,  2.1383e-06,  ..., -2.3395e-06,
         -1.8924e-06, -1.8738e-06],
        [-7.9870e-06, -6.0499e-06,  6.5267e-06,  ..., -6.6608e-06,
         -5.4091e-06, -5.3346e-06],
        [-5.5879e-06, -4.0233e-06,  4.4703e-06,  ..., -4.6641e-06,
         -3.7551e-06, -3.8892e-06],
        [-6.5565e-06, -5.1856e-06,  5.2005e-06,  ..., -5.6028e-06,
         -4.7535e-06, -4.0829e-06]], device='cuda:0')
Loss: 0.9647359848022461


Running epoch 1, step 1778, batch 730
Sampled inputs[:2]: tensor([[    0,  4929,  4214,  ...,  1172,   591,  4422],
        [    0,   342,   266,  ...,    14,  1364, 19388]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0612e-04, -1.4725e-04, -1.1272e-04,  ..., -3.6172e-05,
          2.1304e-04,  1.1786e-04],
        [-4.2021e-06, -3.0622e-06,  3.1516e-06,  ..., -3.5465e-06,
         -2.8908e-06, -2.8238e-06],
        [-1.2040e-05, -9.0450e-06,  9.5963e-06,  ..., -1.0058e-05,
         -8.2254e-06, -8.0168e-06],
        [-8.3596e-06, -5.9456e-06,  6.5267e-06,  ..., -7.0184e-06,
         -5.6922e-06, -5.8264e-06],
        [-9.9391e-06, -7.8231e-06,  7.7188e-06,  ..., -8.5235e-06,
         -7.2718e-06, -6.1691e-06]], device='cuda:0')
Loss: 0.9783965945243835


Running epoch 1, step 1779, batch 731
Sampled inputs[:2]: tensor([[   0,  677, 6499,  ..., 2738,   12,  287],
        [   0, 1529, 5227,  ..., 1480,  367,  925]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.5376e-05, -1.2388e-04, -7.6859e-05,  ..., -5.8930e-05,
          1.6410e-04, -1.3959e-05],
        [-5.6028e-06, -4.1351e-06,  4.2543e-06,  ..., -4.7609e-06,
         -3.8967e-06, -3.7700e-06],
        [-1.6004e-05, -1.2204e-05,  1.2875e-05,  ..., -1.3471e-05,
         -1.1086e-05, -1.0699e-05],
        [-1.1161e-05, -8.0615e-06,  8.8066e-06,  ..., -9.4324e-06,
         -7.7039e-06, -7.8082e-06],
        [-1.3262e-05, -1.0595e-05,  1.0416e-05,  ..., -1.1459e-05,
         -9.8348e-06, -8.2701e-06]], device='cuda:0')
Loss: 0.9771378040313721


Running epoch 1, step 1780, batch 732
Sampled inputs[:2]: tensor([[    0,   461,  1169,  ..., 14135,  2771,    13],
        [    0, 40995,  5863,  ...,    13,  9819,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.1420e-05, -1.6801e-04,  1.7725e-05,  ..., -6.2349e-05,
          1.1907e-04,  4.4399e-05],
        [-6.9588e-06, -5.1111e-06,  5.3868e-06,  ..., -5.8785e-06,
         -4.7833e-06, -4.7013e-06],
        [-1.9878e-05, -1.5095e-05,  1.6317e-05,  ..., -1.6659e-05,
         -1.3649e-05, -1.3366e-05],
        [-1.3918e-05, -1.0014e-05,  1.1221e-05,  ..., -1.1697e-05,
         -9.5069e-06, -9.8050e-06],
        [-1.6376e-05, -1.3053e-05,  1.3113e-05,  ..., -1.4096e-05,
         -1.2070e-05, -1.0252e-05]], device='cuda:0')
Loss: 0.9547422528266907


Running epoch 1, step 1781, batch 733
Sampled inputs[:2]: tensor([[    0, 39004,   266,  ...,   287, 21972,   278],
        [    0, 21930,    12,  ...,  2849,   863,   578]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0628e-04, -2.4225e-04,  6.7537e-05,  ..., -3.6685e-05,
         -1.0540e-04, -3.4501e-05],
        [-8.2701e-06, -6.1169e-06,  6.4522e-06,  ..., -7.0035e-06,
         -5.7518e-06, -5.6028e-06],
        [-2.3633e-05, -1.8120e-05,  1.9565e-05,  ..., -1.9893e-05,
         -1.6451e-05, -1.5974e-05],
        [-1.6600e-05, -1.2070e-05,  1.3515e-05,  ..., -1.4007e-05,
         -1.1504e-05, -1.1772e-05],
        [-1.9446e-05, -1.5676e-05,  1.5706e-05,  ..., -1.6823e-05,
         -1.4544e-05, -1.2219e-05]], device='cuda:0')
Loss: 0.9849042296409607


Running epoch 1, step 1782, batch 734
Sampled inputs[:2]: tensor([[    0, 18322,   287,  ...,   953,   271,   221],
        [    0,   631,  4013,  ...,   368, 20301,   874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1549e-04, -2.8188e-04,  3.4176e-05,  ...,  1.5226e-05,
         -1.5484e-04, -6.7297e-06],
        [-9.6709e-06, -7.0632e-06,  7.5102e-06,  ..., -8.1882e-06,
         -6.6794e-06, -6.5565e-06],
        [-2.7716e-05, -2.0966e-05,  2.2858e-05,  ..., -2.3305e-05,
         -1.9118e-05, -1.8761e-05],
        [-1.9476e-05, -1.3977e-05,  1.5795e-05,  ..., -1.6436e-05,
         -1.3389e-05, -1.3828e-05],
        [-2.2635e-05, -1.8016e-05,  1.8194e-05,  ..., -1.9565e-05,
         -1.6779e-05, -1.4231e-05]], device='cuda:0')
Loss: 0.9720047116279602


Running epoch 1, step 1783, batch 735
Sampled inputs[:2]: tensor([[    0,   271,   768,  ..., 15555,   278,   266],
        [    0,  1871,   401,  ...,    14,  4797,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7746e-04, -1.1870e-04,  2.4798e-05,  ..., -1.6754e-04,
         -3.4563e-04, -2.3988e-04],
        [-1.1049e-05, -8.0392e-06,  8.5756e-06,  ..., -9.3952e-06,
         -7.5810e-06, -7.5176e-06],
        [-3.1680e-05, -2.3827e-05,  2.6092e-05,  ..., -2.6718e-05,
         -2.1681e-05, -2.1473e-05],
        [-2.2292e-05, -1.5914e-05,  1.8045e-05,  ..., -1.8880e-05,
         -1.5222e-05, -1.5885e-05],
        [-2.5868e-05, -2.0474e-05,  2.0787e-05,  ..., -2.2411e-05,
         -1.9029e-05, -1.6257e-05]], device='cuda:0')
Loss: 0.9287465810775757
Graident accumulation at epoch 1, step 1783, batch 735
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1676e-04,  1.1190e-05, -1.2820e-05,  ..., -3.0002e-05,
          1.0165e-05, -8.1547e-05],
        [-1.0748e-05, -7.4536e-06,  8.0904e-06,  ..., -9.4165e-06,
         -7.2771e-06, -7.4547e-06],
        [ 8.1133e-05,  8.7395e-05, -6.9544e-05,  ...,  7.1882e-05,
          7.2039e-05,  3.1126e-05],
        [ 2.5880e-07,  1.3009e-05,  2.3889e-06,  ...,  1.9224e-06,
          1.0244e-05, -7.4833e-07],
        [-2.0926e-05, -1.3929e-05,  1.5259e-05,  ..., -1.7829e-05,
         -1.4866e-05, -1.6276e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9598e-08, 5.6054e-08, 5.5304e-08,  ..., 2.1530e-08, 1.2427e-07,
         4.9525e-08],
        [9.0845e-11, 5.9594e-11, 2.7314e-11,  ..., 6.2693e-11, 3.2956e-11,
         3.0513e-11],
        [2.8498e-09, 1.9930e-09, 1.3337e-09,  ..., 2.0275e-09, 1.3083e-09,
         7.3959e-10],
        [9.5731e-10, 1.2011e-09, 4.2055e-10,  ..., 9.4345e-10, 7.6820e-10,
         4.0552e-10],
        [4.2652e-10, 2.5133e-10, 9.9121e-11,  ..., 3.1375e-10, 9.7497e-11,
         1.2864e-10]], device='cuda:0')
optimizer state dict: 223.0
lr: [1.1397899338904206e-06, 1.1397899338904206e-06]
scheduler_last_epoch: 223


Running epoch 1, step 1784, batch 736
Sampled inputs[:2]: tensor([[    0,  4868,  3106,  ...,  2637,   278,   521],
        [    0, 39224,    34,  ...,   401,  1716,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2459e-05,  1.7874e-05, -2.0934e-05,  ...,  8.2951e-05,
          1.3450e-05, -7.7644e-05],
        [-1.4082e-06, -8.6799e-07,  9.4250e-07,  ..., -1.2219e-06,
         -9.0152e-07, -1.0431e-06],
        [-3.8743e-06, -2.5034e-06,  2.8312e-06,  ..., -3.3230e-06,
         -2.4736e-06, -2.8312e-06],
        [-2.7269e-06, -1.6168e-06,  1.8850e-06,  ..., -2.3693e-06,
         -1.7434e-06, -2.0713e-06],
        [-3.3528e-06, -2.2501e-06,  2.3097e-06,  ..., -2.9504e-06,
         -2.2799e-06, -2.2650e-06]], device='cuda:0')
Loss: 0.9361206889152527


Running epoch 1, step 1785, batch 737
Sampled inputs[:2]: tensor([[   0, 8125, 5241,  ...,  328, 3227,  278],
        [   0, 1039,  259,  ...,  221,  685,  546]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6362e-04,  8.9222e-05, -4.1197e-05,  ...,  1.7759e-04,
         -2.6015e-04, -3.3152e-05],
        [-2.8685e-06, -1.7770e-06,  1.9334e-06,  ..., -2.5257e-06,
         -1.8254e-06, -2.1309e-06],
        [-7.8976e-06, -5.1409e-06,  5.7817e-06,  ..., -6.8545e-06,
         -5.0515e-06, -5.7220e-06],
        [-5.4836e-06, -3.3006e-06,  3.8221e-06,  ..., -4.8429e-06,
         -3.5316e-06, -4.1872e-06],
        [-6.7800e-06, -4.6194e-06,  4.7982e-06,  ..., -6.0052e-06,
         -4.6194e-06, -4.5449e-06]], device='cuda:0')
Loss: 0.9598941802978516


Running epoch 1, step 1786, batch 738
Sampled inputs[:2]: tensor([[    0,    13,   786,  ...,   275,  2623,    13],
        [    0,   266, 11080,  ...,   413,  7308,   413]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2487e-06, -2.4808e-05, -9.8131e-06,  ..., -1.4957e-05,
         -8.6680e-05,  1.0975e-04],
        [-4.2990e-06, -2.9169e-06,  3.0883e-06,  ..., -3.7774e-06,
         -2.8536e-06, -3.0436e-06],
        [ 5.7719e-05,  8.1167e-05, -6.6870e-05,  ...,  8.3343e-05,
          6.4406e-05,  7.5698e-06],
        [-8.2403e-06, -5.4762e-06,  6.1318e-06,  ..., -7.2569e-06,
         -5.5134e-06, -6.0424e-06],
        [-1.0118e-05, -7.4953e-06,  7.5400e-06,  ..., -8.9854e-06,
         -7.1973e-06, -6.5416e-06]], device='cuda:0')
Loss: 1.0343470573425293


Running epoch 1, step 1787, batch 739
Sampled inputs[:2]: tensor([[    0,  2851,  5442,  ..., 38820,    14,   417],
        [    0,  4998,  1921,  ...,   968,   266,  1136]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.1452e-05, -1.1197e-04, -1.4993e-04,  ..., -7.4538e-05,
         -7.5031e-05,  1.6127e-04],
        [-5.5507e-06, -3.9153e-06,  4.0717e-06,  ..., -4.9546e-06,
         -3.8035e-06, -4.0345e-06],
        [ 5.4128e-05,  7.8350e-05, -6.3815e-05,  ...,  8.0124e-05,
          6.1769e-05,  4.9323e-06],
        [-1.0639e-05, -7.3463e-06,  8.1137e-06,  ..., -9.5069e-06,
         -7.3463e-06, -7.9796e-06],
        [-1.3113e-05, -9.9093e-06,  1.0043e-05,  ..., -1.1697e-05,
         -9.5069e-06, -8.5235e-06]], device='cuda:0')
Loss: 0.9543191194534302


Running epoch 1, step 1788, batch 740
Sampled inputs[:2]: tensor([[    0,   669,  1528,  ..., 21826,   259,  5024],
        [    0,   475,   266,  ...,   843,   287,  1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7157e-04, -1.0477e-04, -2.6709e-04,  ..., -1.5046e-05,
         -1.6541e-04,  1.9880e-04],
        [-6.8992e-06, -4.9211e-06,  5.1446e-06,  ..., -6.1393e-06,
         -4.7535e-06, -4.9882e-06],
        [ 5.0328e-05,  7.5370e-05, -6.0567e-05,  ...,  7.6772e-05,
          5.9057e-05,  2.2501e-06],
        [-1.3337e-05, -9.3579e-06,  1.0364e-05,  ..., -1.1891e-05,
         -9.2685e-06, -9.9912e-06],
        [-1.6227e-05, -1.2472e-05,  1.2621e-05,  ..., -1.4499e-05,
         -1.1891e-05, -1.0550e-05]], device='cuda:0')
Loss: 0.9768863320350647


Running epoch 1, step 1789, batch 741
Sampled inputs[:2]: tensor([[   0,   34, 3881,  ..., 1027,  271,  266],
        [   0, 3261, 5866,  ...,  593,  360, 2502]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6323e-04, -1.4835e-04, -2.5688e-05,  ..., -9.6793e-05,
         -2.2658e-04,  1.9568e-05],
        [-8.1882e-06, -5.9046e-06,  6.2250e-06,  ..., -7.3016e-06,
         -5.6140e-06, -5.8934e-06],
        [ 4.6513e-05,  7.2435e-05, -5.7154e-05,  ...,  7.3389e-05,
          5.6523e-05, -4.0233e-07],
        [-1.5974e-05, -1.1295e-05,  1.2688e-05,  ..., -1.4231e-05,
         -1.0997e-05, -1.1928e-05],
        [-1.9252e-05, -1.4931e-05,  1.5244e-05,  ..., -1.7241e-05,
         -1.4052e-05, -1.2472e-05]], device='cuda:0')
Loss: 0.9583461880683899


Running epoch 1, step 1790, batch 742
Sampled inputs[:2]: tensor([[    0, 15912,    14,  ..., 25535,    18,  3947],
        [    0,   792,    83,  ...,   957, 13285,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9254e-04, -6.8859e-05,  1.3605e-04,  ..., -1.8086e-04,
         -2.3265e-04, -1.9350e-05],
        [-9.5293e-06, -6.9253e-06,  7.2829e-06,  ..., -8.5086e-06,
         -6.5565e-06, -6.8210e-06],
        [ 4.2818e-05,  6.9514e-05, -5.4040e-05,  ...,  7.0126e-05,
          5.3931e-05, -2.9355e-06],
        [-1.8522e-05, -1.3202e-05,  1.4789e-05,  ..., -1.6510e-05,
         -1.2785e-05, -1.3769e-05],
        [-2.2382e-05, -1.7539e-05,  1.7822e-05,  ..., -2.0087e-05,
         -1.6406e-05, -1.4439e-05]], device='cuda:0')
Loss: 0.9447394609451294


Running epoch 1, step 1791, batch 743
Sampled inputs[:2]: tensor([[    0, 10296,   809,  ..., 27683,    12,   287],
        [    0,   756,   943,  ...,  4016,    12,   627]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.0436e-04,  5.6165e-06, -3.6485e-05,  ..., -1.0715e-04,
         -3.0758e-04, -9.8909e-05],
        [-1.0759e-05, -7.7672e-06,  8.2254e-06,  ..., -9.6038e-06,
         -7.4282e-06, -7.7710e-06],
        [ 3.9480e-05,  6.7204e-05, -5.1224e-05,  ...,  6.7280e-05,
          5.1621e-05, -5.3495e-06],
        [-2.0951e-05, -1.4797e-05,  1.6771e-05,  ..., -1.8671e-05,
         -1.4514e-05, -1.5706e-05],
        [-2.5257e-05, -1.9595e-05,  2.0161e-05,  ..., -2.2575e-05,
         -1.8507e-05, -1.6317e-05]], device='cuda:0')
Loss: 0.9188053011894226
Graident accumulation at epoch 1, step 1791, batch 743
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.2552e-04,  1.0633e-05, -1.5186e-05,  ..., -3.7717e-05,
         -2.1610e-05, -8.3284e-05],
        [-1.0749e-05, -7.4850e-06,  8.1039e-06,  ..., -9.4352e-06,
         -7.2922e-06, -7.4863e-06],
        [ 7.6968e-05,  8.5376e-05, -6.7712e-05,  ...,  7.1422e-05,
          6.9997e-05,  2.7478e-05],
        [-1.8622e-06,  1.0228e-05,  3.8272e-06,  ..., -1.3700e-07,
          7.7686e-06, -2.2441e-06],
        [-2.1359e-05, -1.4496e-05,  1.5749e-05,  ..., -1.8303e-05,
         -1.5230e-05, -1.6280e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9580e-08, 5.5997e-08, 5.5250e-08,  ..., 2.1520e-08, 1.2424e-07,
         4.9485e-08],
        [9.0870e-11, 5.9594e-11, 2.7355e-11,  ..., 6.2723e-11, 3.2978e-11,
         3.0542e-11],
        [2.8485e-09, 1.9955e-09, 1.3350e-09,  ..., 2.0300e-09, 1.3096e-09,
         7.3888e-10],
        [9.5679e-10, 1.2002e-09, 4.2041e-10,  ..., 9.4285e-10, 7.6765e-10,
         4.0536e-10],
        [4.2673e-10, 2.5146e-10, 9.9428e-11,  ..., 3.1394e-10, 9.7742e-11,
         1.2877e-10]], device='cuda:0')
optimizer state dict: 224.0
lr: [1.083154114868752e-06, 1.083154114868752e-06]
scheduler_last_epoch: 224


Running epoch 1, step 1792, batch 744
Sampled inputs[:2]: tensor([[    0,   259,  2122,  ...,   554,   392, 10814],
        [    0,   401,  3704,  ...,    14,  1062,  1804]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2032e-04,  2.4126e-05, -8.3925e-05,  ..., -1.9941e-04,
          1.0751e-04, -2.4140e-04],
        [-1.3560e-06, -9.1642e-07,  1.0952e-06,  ..., -1.1474e-06,
         -8.2329e-07, -8.9407e-07],
        [-3.6955e-06, -2.5630e-06,  3.1739e-06,  ..., -3.0994e-06,
         -2.2203e-06, -2.4140e-06],
        [-2.6673e-06, -1.7434e-06,  2.2650e-06,  ..., -2.2501e-06,
         -1.5944e-06, -1.8552e-06],
        [-3.0547e-06, -2.2650e-06,  2.5630e-06,  ..., -2.6226e-06,
         -2.0117e-06, -1.8254e-06]], device='cuda:0')
Loss: 0.9433491230010986


Running epoch 1, step 1793, batch 745
Sampled inputs[:2]: tensor([[    0,   494,   221,  ...,   437,   266,  2143],
        [    0, 13595,  3803,  ...,  1992,  4770,   818]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4189e-04,  1.6069e-04, -4.7390e-05,  ..., -2.1105e-04,
          8.8426e-05, -3.3039e-04],
        [-2.7269e-06, -2.0042e-06,  2.2426e-06,  ..., -2.3544e-06,
         -1.8515e-06, -1.8217e-06],
        [-7.4506e-06, -5.6624e-06,  6.4969e-06,  ..., -6.3926e-06,
         -5.0515e-06, -4.9621e-06],
        [-5.3346e-06, -3.8445e-06,  4.5747e-06,  ..., -4.5896e-06,
         -3.5912e-06, -3.7476e-06],
        [-6.2883e-06, -5.0664e-06,  5.3495e-06,  ..., -5.5134e-06,
         -4.5896e-06, -3.8669e-06]], device='cuda:0')
Loss: 0.986098051071167


Running epoch 1, step 1794, batch 746
Sampled inputs[:2]: tensor([[   0,  607,  259,  ...,  271,  669,   12],
        [   0, 5841,  328,  ..., 2051,  266,  756]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.7747e-04,  1.7525e-04,  1.9463e-05,  ..., -1.9642e-04,
         -1.3127e-04, -4.9351e-04],
        [-4.0233e-06, -2.9728e-06,  3.3081e-06,  ..., -3.5092e-06,
         -2.7269e-06, -2.7455e-06],
        [-1.1146e-05, -8.4788e-06,  9.6858e-06,  ..., -9.6411e-06,
         -7.5251e-06, -7.5847e-06],
        [-7.9721e-06, -5.7667e-06,  6.8396e-06,  ..., -6.9290e-06,
         -5.3570e-06, -5.7295e-06],
        [-9.3430e-06, -7.5251e-06,  7.9274e-06,  ..., -8.2701e-06,
         -6.7949e-06, -5.8636e-06]], device='cuda:0')
Loss: 0.9638601541519165


Running epoch 1, step 1795, batch 747
Sampled inputs[:2]: tensor([[    0, 37312,    12,  ...,   278,   795, 40854],
        [    0,   870,   278,  ...,   478,   401,   897]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1575e-04,  1.1368e-04, -1.1004e-04,  ..., -2.7817e-04,
          2.6110e-05, -5.9189e-04],
        [-5.2825e-06, -3.9786e-06,  4.3213e-06,  ..., -4.6715e-06,
         -3.7104e-06, -3.6806e-06],
        [-1.4663e-05, -1.1280e-05,  1.2696e-05,  ..., -1.2785e-05,
         -1.0222e-05, -1.0088e-05],
        [-1.0461e-05, -7.6741e-06,  8.9407e-06,  ..., -9.1940e-06,
         -7.2941e-06, -7.6517e-06],
        [-1.2264e-05, -9.9540e-06,  1.0341e-05,  ..., -1.0923e-05,
         -9.1642e-06, -7.7412e-06]], device='cuda:0')
Loss: 0.9672321081161499


Running epoch 1, step 1796, batch 748
Sampled inputs[:2]: tensor([[    0,   221,   259,  ...,   199, 13800,  9254],
        [    0,   344,   259,  ..., 47553,   287, 28978]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.2383e-04,  1.5935e-04,  7.4825e-06,  ..., -2.0465e-04,
         -6.8146e-05, -7.4578e-04],
        [-6.6608e-06, -4.7944e-06,  5.3570e-06,  ..., -5.9307e-06,
         -4.5784e-06, -4.7758e-06],
        [-1.8373e-05, -1.3515e-05,  1.5676e-05,  ..., -1.6049e-05,
         -1.2487e-05, -1.2875e-05],
        [-1.3113e-05, -9.1791e-06,  1.1027e-05,  ..., -1.1608e-05,
         -8.9481e-06, -9.8273e-06],
        [-1.5587e-05, -1.2025e-05,  1.2934e-05,  ..., -1.3888e-05,
         -1.1310e-05, -1.0051e-05]], device='cuda:0')
Loss: 0.9137705564498901


Running epoch 1, step 1797, batch 749
Sampled inputs[:2]: tensor([[   0,  275, 1620,  ..., 3020,  278,  259],
        [   0,  360,  259,  ...,   12,  358,   19]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6602e-04,  2.3420e-04,  6.0828e-06,  ..., -1.1650e-04,
         -1.8938e-04, -9.2575e-04],
        [-8.1137e-06, -5.8897e-06,  6.4746e-06,  ..., -7.1898e-06,
         -5.4836e-06, -5.6587e-06],
        [-2.2277e-05, -1.6630e-05,  1.8910e-05,  ..., -1.9446e-05,
         -1.5065e-05, -1.5229e-05],
        [-1.5900e-05, -1.1265e-05,  1.3292e-05,  ..., -1.4022e-05,
         -1.0721e-05, -1.1601e-05],
        [-1.8835e-05, -1.4767e-05,  1.5572e-05,  ..., -1.6779e-05,
         -1.3635e-05, -1.1861e-05]], device='cuda:0')
Loss: 0.9772456288337708


Running epoch 1, step 1798, batch 750
Sampled inputs[:2]: tensor([[   0,  278,  638,  ...,  278,  266, 9387],
        [   0,  546,  360,  ...,   12,  461, 8753]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.0716e-04,  2.8998e-04, -9.0306e-05,  ...,  1.2725e-05,
         -3.9607e-04, -1.0433e-03],
        [-9.4846e-06, -6.7651e-06,  7.5623e-06,  ..., -8.3297e-06,
         -6.2622e-06, -6.6124e-06],
        [-2.6122e-05, -1.9163e-05,  2.2158e-05,  ..., -2.2575e-05,
         -1.7256e-05, -1.7837e-05],
        [-1.8656e-05, -1.2971e-05,  1.5587e-05,  ..., -1.6287e-05,
         -1.2271e-05, -1.3597e-05],
        [-2.1920e-05, -1.6898e-05,  1.8120e-05,  ..., -1.9327e-05,
         -1.5527e-05, -1.3784e-05]], device='cuda:0')
Loss: 0.9446895122528076


Running epoch 1, step 1799, batch 751
Sampled inputs[:2]: tensor([[   0,  266,  298,  ...,  266,  818,  278],
        [   0,  795, 1445,  ..., 6292,  287, 9782]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6446e-04,  3.3921e-04,  1.8904e-05,  ..., -8.5948e-05,
         -4.6479e-04, -1.1141e-03],
        [-1.1064e-05, -7.6629e-06,  8.5905e-06,  ..., -9.6634e-06,
         -7.1935e-06, -7.8492e-06],
        [-3.0473e-05, -2.1800e-05,  2.5257e-05,  ..., -2.6226e-05,
         -1.9863e-05, -2.1219e-05],
        [-2.1458e-05, -1.4603e-05,  1.7539e-05,  ..., -1.8686e-05,
         -1.3970e-05, -1.5832e-05],
        [-2.5883e-05, -1.9312e-05,  2.0787e-05,  ..., -2.2739e-05,
         -1.8030e-05, -1.6704e-05]], device='cuda:0')
Loss: 0.94306880235672
Graident accumulation at epoch 1, step 1799, batch 751
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7941e-04,  4.3491e-05, -1.1777e-05,  ..., -4.2540e-05,
         -6.5928e-05, -1.8637e-04],
        [-1.0781e-05, -7.5028e-06,  8.1526e-06,  ..., -9.4580e-06,
         -7.2823e-06, -7.5226e-06],
        [ 6.6224e-05,  7.4658e-05, -5.8415e-05,  ...,  6.1657e-05,
          6.1011e-05,  2.2609e-05],
        [-3.8217e-06,  7.7452e-06,  5.1983e-06,  ..., -1.9919e-06,
          5.5947e-06, -3.6029e-06],
        [-2.1812e-05, -1.4977e-05,  1.6253e-05,  ..., -1.8747e-05,
         -1.5510e-05, -1.6322e-05]], device='cuda:0')
optimizer state dict: tensor([[5.9962e-08, 5.6057e-08, 5.5195e-08,  ..., 2.1505e-08, 1.2434e-07,
         5.0677e-08],
        [9.0901e-11, 5.9593e-11, 2.7401e-11,  ..., 6.2753e-11, 3.2997e-11,
         3.0573e-11],
        [2.8466e-09, 1.9940e-09, 1.3343e-09,  ..., 2.0286e-09, 1.3087e-09,
         7.3859e-10],
        [9.5629e-10, 1.1992e-09, 4.2030e-10,  ..., 9.4226e-10, 7.6707e-10,
         4.0521e-10],
        [4.2697e-10, 2.5158e-10, 9.9761e-11,  ..., 3.1415e-10, 9.7969e-11,
         1.2892e-10]], device='cuda:0')
optimizer state dict: 225.0
lr: [1.027880867734583e-06, 1.027880867734583e-06]
scheduler_last_epoch: 225


Running epoch 1, step 1800, batch 752
Sampled inputs[:2]: tensor([[    0,   792,   342,  ..., 12152,  9904,  1239],
        [    0,   546, 28676,  ...,   271,  1267,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.3840e-05, -3.2494e-05,  7.0177e-06,  ...,  6.5090e-07,
         -3.0370e-05,  4.1186e-05],
        [-1.3784e-06, -1.1250e-06,  1.1027e-06,  ..., -1.1995e-06,
         -1.0356e-06, -9.9838e-07],
        [-3.9935e-06, -3.3528e-06,  3.4124e-06,  ..., -3.4720e-06,
         -3.0100e-06, -2.8908e-06],
        [-2.7865e-06, -2.2501e-06,  2.3246e-06,  ..., -2.4140e-06,
         -2.0862e-06, -2.1011e-06],
        [-3.3528e-06, -2.9206e-06,  2.7865e-06,  ..., -2.9802e-06,
         -2.6822e-06, -2.2501e-06]], device='cuda:0')
Loss: 1.002564787864685


Running epoch 1, step 1801, batch 753
Sampled inputs[:2]: tensor([[    0,   421,  6007,  ...,   408,  2105,   843],
        [    0,   380, 26765,  ...,     9,   367,  6930]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2682e-05, -1.8911e-04, -1.0137e-05,  ...,  1.0094e-04,
          1.4831e-04,  1.7684e-04],
        [-2.7046e-06, -2.1383e-06,  2.1085e-06,  ..., -2.4587e-06,
         -1.9968e-06, -1.9893e-06],
        [-7.9274e-06, -6.3628e-06,  6.5565e-06,  ..., -7.0930e-06,
         -5.7369e-06, -5.7369e-06],
        [-5.4985e-06, -4.2617e-06,  4.4554e-06,  ..., -4.9621e-06,
         -4.0233e-06, -4.2021e-06],
        [-6.5863e-06, -5.4538e-06,  5.3197e-06,  ..., -5.9754e-06,
         -5.0366e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9775656461715698


Running epoch 1, step 1802, batch 754
Sampled inputs[:2]: tensor([[    0, 20241,  1244,  ...,  6232,  1004,   300],
        [    0,  3592,   417,  ...,  4893,   328,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.7026e-04, -1.8202e-04, -1.7112e-04,  ...,  1.0668e-04,
         -4.1807e-05,  1.3487e-04],
        [-3.9786e-06, -2.9467e-06,  3.1516e-06,  ..., -3.5390e-06,
         -2.7902e-06, -2.9020e-06],
        [-1.1653e-05, -8.7917e-06,  9.8497e-06,  ..., -1.0148e-05,
         -8.0019e-06, -8.3297e-06],
        [-8.1360e-06, -5.8636e-06,  6.7353e-06,  ..., -7.1675e-06,
         -5.6550e-06, -6.1840e-06],
        [-9.4622e-06, -7.4655e-06,  7.7784e-06,  ..., -8.4043e-06,
         -6.9365e-06, -6.2212e-06]], device='cuda:0')
Loss: 0.9500527381896973


Running epoch 1, step 1803, batch 755
Sampled inputs[:2]: tensor([[   0, 3058,  292,  ..., 1387, 1236,  369],
        [   0, 3253, 1573,  ...,  298,  358,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9456e-04, -6.2411e-05, -9.9460e-05,  ...,  2.4569e-05,
         -1.4770e-04,  2.7241e-05],
        [-5.4315e-06, -3.9227e-06,  4.0941e-06,  ..., -4.8876e-06,
         -3.8557e-06, -4.0345e-06],
        [-1.5706e-05, -1.1683e-05,  1.2711e-05,  ..., -1.3888e-05,
         -1.0997e-05, -1.1474e-05],
        [-1.0803e-05, -7.6368e-06,  8.5235e-06,  ..., -9.6560e-06,
         -7.6368e-06, -8.3297e-06],
        [ 1.1026e-04,  9.9306e-05, -1.4285e-04,  ...,  1.3669e-04,
          1.4616e-04,  1.1559e-04]], device='cuda:0')
Loss: 0.9593026041984558


Running epoch 1, step 1804, batch 756
Sampled inputs[:2]: tensor([[    0, 11541,  4784,  ...,  2837, 38541,    12],
        [    0,    14, 38914,  ...,   266,  5690,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1056e-04,  1.3909e-04,  1.5261e-04,  ...,  2.1625e-05,
         -8.2190e-05,  1.1551e-04],
        [-6.8471e-06, -4.9733e-06,  4.9546e-06,  ..., -6.2063e-06,
         -4.9807e-06, -5.0701e-06],
        [-1.9729e-05, -1.4797e-05,  1.5333e-05,  ..., -1.7613e-05,
         -1.4216e-05, -1.4380e-05],
        [-1.3560e-05, -9.6485e-06,  1.0230e-05,  ..., -1.2249e-05,
         -9.8869e-06, -1.0416e-05],
        [ 1.0662e-04,  9.6430e-05, -1.4052e-04,  ...,  1.3328e-04,
          1.4315e-04,  1.1313e-04]], device='cuda:0')
Loss: 0.9661273956298828


Running epoch 1, step 1805, batch 757
Sampled inputs[:2]: tensor([[    0,   471,   590,  ...,  5007,    13,  2920],
        [    0,   445,    29,  ..., 20247,   272,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4066e-04,  2.9573e-04,  2.2330e-04,  ...,  1.4237e-04,
         -2.4543e-04,  3.4063e-04],
        [-8.3223e-06, -5.8338e-06,  5.6103e-06,  ..., -7.6294e-06,
         -6.0461e-06, -6.3367e-06],
        [-2.3782e-05, -1.7434e-05,  1.7405e-05,  ..., -2.1517e-05,
         -1.7270e-05, -1.7807e-05],
        [-1.6391e-05, -1.1288e-05,  1.1466e-05,  ..., -1.5095e-05,
         -1.2077e-05, -1.2904e-05],
        [ 1.0290e-04,  9.3972e-05, -1.3865e-04,  ...,  1.2969e-04,
          1.4026e-04,  1.1021e-04]], device='cuda:0')
Loss: 0.8616816997528076


Running epoch 1, step 1806, batch 758
Sampled inputs[:2]: tensor([[    0,    12,  4567,  ...,  4154,  1799, 11883],
        [    0,  2561,  4994,  ..., 10407,   287,  1339]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6925e-04,  1.2252e-04,  1.3083e-04,  ...,  9.2732e-05,
         -3.7937e-04,  3.7806e-04],
        [-9.7081e-06, -6.7800e-06,  6.7949e-06,  ..., -8.7768e-06,
         -6.9104e-06, -7.2531e-06],
        [-2.7686e-05, -2.0206e-05,  2.0921e-05,  ..., -2.4736e-05,
         -1.9684e-05, -2.0400e-05],
        [-1.9193e-05, -1.3150e-05,  1.3970e-05,  ..., -1.7390e-05,
         -1.3798e-05, -1.4856e-05],
        [ 9.9796e-05,  9.1632e-05, -1.3594e-04,  ...,  1.2705e-04,
          1.3817e-04,  1.0833e-04]], device='cuda:0')
Loss: 0.9560494422912598


Running epoch 1, step 1807, batch 759
Sampled inputs[:2]: tensor([[    0,   376,   283,  ..., 29188,   292,  7627],
        [    0, 10064,   768,  ...,   266,  2816,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0971e-04,  9.9252e-05, -1.4810e-05,  ...,  1.2558e-04,
         -3.3914e-04,  2.4745e-04],
        [-1.1042e-05, -7.7784e-06,  7.7486e-06,  ..., -1.0014e-05,
         -7.9162e-06, -8.2441e-06],
        [-3.1486e-05, -2.3097e-05,  2.3842e-05,  ..., -2.8148e-05,
         -2.2486e-05, -2.3097e-05],
        [-2.1785e-05, -1.5050e-05,  1.5892e-05,  ..., -1.9789e-05,
         -1.5765e-05, -1.6868e-05],
        [ 9.6533e-05,  8.9069e-05, -1.3348e-04,  ...,  1.2407e-04,
          1.3564e-04,  1.0620e-04]], device='cuda:0')
Loss: 0.9933291673660278
Graident accumulation at epoch 1, step 1807, batch 759
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.9244e-04,  4.9067e-05, -1.2081e-05,  ..., -2.5728e-05,
         -9.3249e-05, -1.4299e-04],
        [-1.0807e-05, -7.5303e-06,  8.1122e-06,  ..., -9.5136e-06,
         -7.3457e-06, -7.5947e-06],
        [ 5.6453e-05,  6.4883e-05, -5.0190e-05,  ...,  5.2676e-05,
          5.2662e-05,  1.8038e-05],
        [-5.6181e-06,  5.4657e-06,  6.2677e-06,  ..., -3.7716e-06,
          3.4587e-06, -4.9294e-06],
        [-9.9771e-06, -4.5727e-06,  1.2793e-06,  ..., -4.4653e-06,
         -3.9533e-07, -4.0701e-06]], device='cuda:0')
optimizer state dict: tensor([[5.9998e-08, 5.6010e-08, 5.5140e-08,  ..., 2.1500e-08, 1.2433e-07,
         5.0688e-08],
        [9.0932e-11, 5.9594e-11, 2.7434e-11,  ..., 6.2791e-11, 3.3027e-11,
         3.0611e-11],
        [2.8447e-09, 1.9925e-09, 1.3336e-09,  ..., 2.0274e-09, 1.3079e-09,
         7.3839e-10],
        [9.5581e-10, 1.1982e-09, 4.2013e-10,  ..., 9.4171e-10, 7.6655e-10,
         4.0509e-10],
        [4.3587e-10, 2.5927e-10, 1.1748e-10,  ..., 3.2922e-10, 1.1627e-10,
         1.4007e-10]], device='cuda:0')
optimizer state dict: 226.0
lr: [9.739786387225548e-07, 9.739786387225548e-07]
scheduler_last_epoch: 226


Running epoch 1, step 1808, batch 760
Sampled inputs[:2]: tensor([[    0,  5625,  2558,  ...,   680,   292,   494],
        [    0,  7011,   650,  ..., 28839, 11610,  3222]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.0997e-05, -7.1271e-05, -1.0068e-04,  ..., -1.3944e-05,
          7.9645e-05,  7.0003e-05],
        [-1.3560e-06, -1.0580e-06,  1.0207e-06,  ..., -1.1474e-06,
         -9.6112e-07, -9.4622e-07],
        [-3.9637e-06, -3.2783e-06,  3.2037e-06,  ..., -3.3975e-06,
         -2.8610e-06, -2.8014e-06],
        [-2.7269e-06, -2.1309e-06,  2.1458e-06,  ..., -2.3246e-06,
         -1.9521e-06, -2.0117e-06],
        [-3.1739e-06, -2.7418e-06,  2.5034e-06,  ..., -2.7716e-06,
         -2.4438e-06, -2.0564e-06]], device='cuda:0')
Loss: 0.9648867249488831


Running epoch 1, step 1809, batch 761
Sampled inputs[:2]: tensor([[   0,  278, 1478,  ...,  266, 1607, 1220],
        [   0, 1458,  365,  ..., 5399, 1110,  870]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4811e-05, -4.4399e-05, -2.2083e-04,  ...,  4.9504e-05,
          1.7512e-05,  3.8835e-05],
        [-2.7418e-06, -2.0191e-06,  2.1681e-06,  ..., -2.2724e-06,
         -1.8328e-06, -1.9073e-06],
        [-7.8678e-06, -6.1244e-06,  6.6608e-06,  ..., -6.5416e-06,
         -5.3495e-06, -5.5134e-06],
        [-5.4985e-06, -4.0382e-06,  4.5747e-06,  ..., -4.5598e-06,
         -3.6955e-06, -4.0233e-06],
        [-6.3479e-06, -5.1856e-06,  5.2303e-06,  ..., -5.4091e-06,
         -4.6492e-06, -4.0978e-06]], device='cuda:0')
Loss: 0.9456468820571899


Running epoch 1, step 1810, batch 762
Sampled inputs[:2]: tensor([[    0,    13,  2497,  ...,   943,   259,  2646],
        [    0, 25938,   359,  ...,    36, 15859,   504]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8604e-04,  5.3889e-05, -1.6942e-04,  ...,  1.9419e-05,
         -9.9869e-05, -6.3450e-05],
        [-4.0457e-06, -3.0175e-06,  3.1516e-06,  ..., -3.4422e-06,
         -2.8312e-06, -2.8238e-06],
        [-1.1504e-05, -9.0152e-06,  9.6112e-06,  ..., -9.7603e-06,
         -8.1062e-06, -8.0466e-06],
        [-8.0466e-06, -5.9605e-06,  6.5863e-06,  ..., -6.8545e-06,
         -5.6624e-06, -5.9232e-06],
        [-9.4324e-06, -7.7486e-06,  7.6741e-06,  ..., -8.1956e-06,
         -7.1228e-06, -6.0648e-06]], device='cuda:0')
Loss: 0.9446198344230652


Running epoch 1, step 1811, batch 763
Sampled inputs[:2]: tensor([[   0,  266, 2057,  ...,   88, 1801,   66],
        [   0, 1067,  271,  ...,  266,  940,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4194e-04,  1.0701e-04, -1.1900e-04,  ...,  9.1387e-05,
         -4.9914e-05, -6.6789e-05],
        [-5.4613e-06, -4.0606e-06,  4.1500e-06,  ..., -4.6641e-06,
         -3.8594e-06, -3.8296e-06],
        [-1.5557e-05, -1.2085e-05,  1.2636e-05,  ..., -1.3188e-05,
         -1.1042e-05, -1.0908e-05],
        [-1.0982e-05, -8.0615e-06,  8.7172e-06,  ..., -9.3579e-06,
         -7.7784e-06, -8.0839e-06],
        [-1.2815e-05, -1.0401e-05,  1.0163e-05,  ..., -1.1116e-05,
         -9.7007e-06, -8.2999e-06]], device='cuda:0')
Loss: 0.9856206178665161


Running epoch 1, step 1812, batch 764
Sampled inputs[:2]: tensor([[    0,  3908,  4274,  ...,   298,  7998, 11109],
        [    0,   266,  2109,  ...,  6730, 11558,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5450e-04,  1.1506e-04, -2.0227e-04,  ...,  1.6559e-05,
         -1.8360e-04,  2.3463e-05],
        [-6.9290e-06, -4.9397e-06,  5.2452e-06,  ..., -5.8338e-06,
         -4.7274e-06, -4.9025e-06],
        [-1.9819e-05, -1.4752e-05,  1.6004e-05,  ..., -1.6540e-05,
         -1.3575e-05, -1.3992e-05],
        [-1.3828e-05, -9.7305e-06,  1.0937e-05,  ..., -1.1623e-05,
         -9.4622e-06, -1.0245e-05],
        [-1.6347e-05, -1.2726e-05,  1.2875e-05,  ..., -1.3977e-05,
         -1.1951e-05, -1.0699e-05]], device='cuda:0')
Loss: 0.9682665467262268


Running epoch 1, step 1813, batch 765
Sampled inputs[:2]: tensor([[    0,   271,   266,  ...,    14,   333,   199],
        [    0, 21891,     9,  ...,  5216,   717,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8889e-04,  1.8176e-04, -3.0295e-04,  ...,  1.8571e-06,
         -3.4696e-04, -9.1289e-05],
        [-8.2925e-06, -5.8599e-06,  6.3926e-06,  ..., -7.0110e-06,
         -5.5805e-06, -5.8301e-06],
        [ 4.6939e-05,  5.2721e-05, -3.0364e-05,  ...,  2.1611e-05,
          8.7999e-05,  6.0535e-07],
        [-1.6510e-05, -1.1496e-05,  1.3277e-05,  ..., -1.3918e-05,
         -1.1124e-05, -1.2137e-05],
        [-1.9491e-05, -1.5065e-05,  1.5602e-05,  ..., -1.6734e-05,
         -1.4067e-05, -1.2651e-05]], device='cuda:0')
Loss: 0.9647827744483948


Running epoch 1, step 1814, batch 766
Sampled inputs[:2]: tensor([[    0,    12, 47869,  ...,   259,  5698,    13],
        [    0,  5597, 11929,  ...,   271,   275,   955]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8559e-04,  3.5050e-04, -1.3867e-04,  ...,  4.3788e-05,
         -3.7191e-04, -1.8437e-04],
        [-9.6485e-06, -6.7353e-06,  7.4655e-06,  ..., -8.1435e-06,
         -6.4485e-06, -6.7502e-06],
        [ 4.3064e-05,  5.0054e-05, -2.7071e-05,  ...,  1.8362e-05,
          8.5466e-05, -2.0173e-06],
        [-1.9342e-05, -1.3314e-05,  1.5631e-05,  ..., -1.6302e-05,
         -1.2986e-05, -1.4164e-05],
        [-2.2575e-05, -1.7315e-05,  1.8150e-05,  ..., -1.9401e-05,
         -1.6242e-05, -1.4588e-05]], device='cuda:0')
Loss: 0.9443029761314392


Running epoch 1, step 1815, batch 767
Sampled inputs[:2]: tensor([[    0,   298, 22296,  ...,   287,  6494,   644],
        [    0,   271,   266,  ..., 46357, 11101, 10621]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6458e-04,  3.1614e-04, -1.2785e-04,  ..., -5.0759e-06,
         -3.6196e-04, -2.1676e-04],
        [-1.1154e-05, -7.6704e-06,  8.4639e-06,  ..., -9.4846e-06,
         -7.3537e-06, -7.8753e-06],
        [ 3.8862e-05,  4.7297e-05, -2.4046e-05,  ...,  1.4667e-05,
          8.2917e-05, -5.0869e-06],
        [-2.2098e-05, -1.5013e-05,  1.7539e-05,  ..., -1.8775e-05,
         -1.4663e-05, -1.6294e-05],
        [-2.6256e-05, -1.9819e-05,  2.0713e-05,  ..., -2.2709e-05,
         -1.8612e-05, -1.7121e-05]], device='cuda:0')
Loss: 0.9351221323013306
Graident accumulation at epoch 1, step 1815, batch 767
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4966e-04,  7.5774e-05, -2.3658e-05,  ..., -2.3663e-05,
         -1.2012e-04, -1.5036e-04],
        [-1.0842e-05, -7.5443e-06,  8.1474e-06,  ..., -9.5107e-06,
         -7.3465e-06, -7.6228e-06],
        [ 5.4694e-05,  6.3124e-05, -4.7575e-05,  ...,  4.8875e-05,
          5.5687e-05,  1.5725e-05],
        [-7.2661e-06,  3.4178e-06,  7.3948e-06,  ..., -5.2720e-06,
          1.6466e-06, -6.0659e-06],
        [-1.1605e-05, -6.0973e-06,  3.2226e-06,  ..., -6.2897e-06,
         -2.2170e-06, -5.3753e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0523e-08, 5.6054e-08, 5.5102e-08,  ..., 2.1478e-08, 1.2433e-07,
         5.0684e-08],
        [9.0966e-11, 5.9594e-11, 2.7478e-11,  ..., 6.2818e-11, 3.3048e-11,
         3.0642e-11],
        [2.8434e-09, 1.9927e-09, 1.3328e-09,  ..., 2.0256e-09, 1.3135e-09,
         7.3767e-10],
        [9.5534e-10, 1.1972e-09, 4.2002e-10,  ..., 9.4112e-10, 7.6600e-10,
         4.0495e-10],
        [4.3612e-10, 2.5940e-10, 1.1779e-10,  ..., 3.2941e-10, 1.1650e-10,
         1.4023e-10]], device='cuda:0')
optimizer state dict: 227.0
lr: [9.214556645637851e-07, 9.214556645637851e-07]
scheduler_last_epoch: 227


Running epoch 1, step 1816, batch 768
Sampled inputs[:2]: tensor([[   0, 6508, 4305,  ...,  806, 3888, 4431],
        [   0, 5998,  591,  ..., 3126,   12,  358]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3434e-05,  1.8402e-05,  8.4374e-05,  ..., -1.1947e-04,
         -2.0873e-05, -1.1545e-04],
        [-1.5125e-06, -8.1211e-07,  7.1153e-07,  ..., -1.3560e-06,
         -1.0729e-06, -1.2442e-06],
        [-4.1127e-06, -2.3246e-06,  2.1607e-06,  ..., -3.6061e-06,
         -2.9206e-06, -3.2336e-06],
        [-2.7269e-06, -1.4305e-06,  1.3486e-06,  ..., -2.4736e-06,
         -1.9819e-06, -2.2799e-06],
        [-4.0829e-06, -2.2948e-06,  1.9372e-06,  ..., -3.6210e-06,
         -3.0249e-06, -3.0845e-06]], device='cuda:0')
Loss: 0.8902508616447449


Running epoch 1, step 1817, batch 769
Sampled inputs[:2]: tensor([[    0,  2440,   709,  ...,  4505,  1549,  4111],
        [    0, 21413,  1735,  ..., 10789, 12523,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2313e-04, -7.4529e-05,  3.3537e-05,  ..., -2.2768e-04,
         -8.0589e-05, -1.2104e-04],
        [-2.9206e-06, -1.7881e-06,  1.7993e-06,  ..., -2.5332e-06,
         -1.9781e-06, -2.2128e-06],
        [-7.9572e-06, -5.0962e-06,  5.3495e-06,  ..., -6.7949e-06,
         -5.4240e-06, -5.8264e-06],
        [-5.4687e-06, -3.2857e-06,  3.5539e-06,  ..., -4.7535e-06,
         -3.7402e-06, -4.2319e-06],
        [-7.3612e-06, -4.7833e-06,  4.5896e-06,  ..., -6.4075e-06,
         -5.3197e-06, -5.1111e-06]], device='cuda:0')
Loss: 0.9394980072975159


Running epoch 1, step 1818, batch 770
Sampled inputs[:2]: tensor([[   0, 1690, 2558,  ..., 2025,   12,  266],
        [   0, 1119,  943,  ...,  759,  920, 8874]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.2993e-05, -7.4529e-05,  1.3165e-05,  ..., -2.6872e-04,
          1.1933e-04, -4.4026e-05],
        [-4.2766e-06, -2.8834e-06,  2.7977e-06,  ..., -3.7402e-06,
         -3.0212e-06, -3.1590e-06],
        [-1.1861e-05, -8.4043e-06,  8.4192e-06,  ..., -1.0297e-05,
         -8.5235e-06, -8.5980e-06],
        [-8.1658e-06, -5.4613e-06,  5.6401e-06,  ..., -7.1675e-06,
         -5.8413e-06, -6.2138e-06],
        [-1.0610e-05, -7.6592e-06,  7.0930e-06,  ..., -9.3877e-06,
         -8.0615e-06, -7.2569e-06]], device='cuda:0')
Loss: 0.9575069546699524


Running epoch 1, step 1819, batch 771
Sampled inputs[:2]: tensor([[    0,  4988, 36842,  ...,  7630, 18362,    13],
        [    0,   382,  9279,  ...,   445, 37790,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2342e-04, -1.6893e-04,  3.1231e-05,  ..., -2.5437e-04,
          1.9982e-04, -7.0372e-05],
        [-5.6177e-06, -3.9637e-06,  3.8408e-06,  ..., -4.9919e-06,
         -4.0866e-06, -4.0904e-06],
        [-1.5765e-05, -1.1683e-05,  1.1623e-05,  ..., -1.3918e-05,
         -1.1638e-05, -1.1310e-05],
        [-1.0848e-05, -7.6070e-06,  7.8157e-06,  ..., -9.6858e-06,
         -8.0019e-06, -8.1807e-06],
        [-1.3918e-05, -1.0535e-05,  9.7454e-06,  ..., -1.2517e-05,
         -1.0833e-05, -9.4026e-06]], device='cuda:0')
Loss: 1.0421199798583984


Running epoch 1, step 1820, batch 772
Sampled inputs[:2]: tensor([[    0,  2255, 21868,  ...,   591,  5902,   259],
        [    0,   413,    29,  ...,   818,   278,   970]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.9273e-04,  1.1653e-05, -4.5379e-05,  ..., -2.8105e-04,
          1.4791e-04, -2.3962e-04],
        [-7.0855e-06, -4.9844e-06,  4.7348e-06,  ..., -6.2957e-06,
         -5.1968e-06, -5.1409e-06],
        [-1.9699e-05, -1.4648e-05,  1.4305e-05,  ..., -1.7419e-05,
         -1.4707e-05, -1.4126e-05],
        [-1.3679e-05, -9.5889e-06,  9.6187e-06,  ..., -1.2219e-05,
         -1.0222e-05, -1.0282e-05],
        [-1.7360e-05, -1.3247e-05,  1.1981e-05,  ..., -1.5706e-05,
         -1.3739e-05, -1.1712e-05]], device='cuda:0')
Loss: 0.9335817098617554


Running epoch 1, step 1821, batch 773
Sampled inputs[:2]: tensor([[    0,   275,  1184,  ...,   328, 46278,  2117],
        [    0,   935,  2613,  ...,   623,  4289,  6803]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.4234e-04,  2.8225e-05, -1.1856e-04,  ..., -4.2709e-04,
          1.6343e-04, -3.2287e-04],
        [-8.4490e-06, -5.9530e-06,  5.8152e-06,  ..., -7.5027e-06,
         -6.1207e-06, -6.0573e-06],
        [-2.3350e-05, -1.7375e-05,  1.7390e-05,  ..., -2.0623e-05,
         -1.7181e-05, -1.6540e-05],
        [-1.6287e-05, -1.1414e-05,  1.1779e-05,  ..., -1.4514e-05,
         -1.1995e-05, -1.2122e-05],
        [-2.0444e-05, -1.5691e-05,  1.4514e-05,  ..., -1.8463e-05,
         -1.5974e-05, -1.3590e-05]], device='cuda:0')
Loss: 0.9435636401176453


Running epoch 1, step 1822, batch 774
Sampled inputs[:2]: tensor([[   0,  328,  843,  ...,  298,  292,   37],
        [   0, 2088, 5370,  ..., 1110, 3380,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6669e-04, -4.1300e-06, -1.0748e-04,  ..., -4.4459e-04,
          2.4657e-04, -4.2073e-04],
        [-9.8050e-06, -6.9588e-06,  6.9328e-06,  ..., -8.7023e-06,
         -7.1041e-06, -7.0259e-06],
        [-2.7224e-05, -2.0355e-05,  2.0787e-05,  ..., -2.4021e-05,
         -1.9968e-05, -1.9327e-05],
        [-1.9014e-05, -1.3381e-05,  1.4089e-05,  ..., -1.6913e-05,
         -1.3962e-05, -1.4164e-05],
        [-2.3678e-05, -1.8269e-05,  1.7256e-05,  ..., -2.1353e-05,
         -1.8448e-05, -1.5721e-05]], device='cuda:0')
Loss: 0.9760348200798035


Running epoch 1, step 1823, batch 775
Sampled inputs[:2]: tensor([[    0,  1268,   278,  ...,   461,   925,   630],
        [    0,   437, 38603,  ..., 37253, 10432,   278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3377e-04,  2.3449e-05, -2.2463e-04,  ..., -5.7497e-04,
          1.1896e-04, -5.5994e-04],
        [-1.1183e-05, -7.9274e-06,  8.0355e-06,  ..., -9.9018e-06,
         -8.0131e-06, -7.9349e-06],
        [-3.1158e-05, -2.3231e-05,  2.4095e-05,  ..., -2.7448e-05,
         -2.2605e-05, -2.1920e-05],
        [-2.1741e-05, -1.5266e-05,  1.6369e-05,  ..., -1.9282e-05,
         -1.5758e-05, -1.6056e-05],
        [-2.6837e-05, -2.0742e-05,  1.9878e-05,  ..., -2.4185e-05,
         -2.0757e-05, -1.7658e-05]], device='cuda:0')
Loss: 0.9687384366989136
Graident accumulation at epoch 1, step 1823, batch 775
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0109, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4807e-04,  7.0542e-05, -4.3755e-05,  ..., -7.8793e-05,
         -9.6213e-05, -1.9132e-04],
        [-1.0876e-05, -7.5826e-06,  8.1362e-06,  ..., -9.5498e-06,
         -7.4132e-06, -7.6540e-06],
        [ 4.6109e-05,  5.4489e-05, -4.0408e-05,  ...,  4.1243e-05,
          4.7858e-05,  1.1961e-05],
        [-8.7136e-06,  1.5494e-06,  8.2922e-06,  ..., -6.6730e-06,
         -9.3876e-08, -7.0649e-06],
        [-1.3128e-05, -7.5618e-06,  4.8882e-06,  ..., -8.0792e-06,
         -4.0710e-06, -6.6035e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0517e-08, 5.5999e-08, 5.5097e-08,  ..., 2.1787e-08, 1.2422e-07,
         5.0947e-08],
        [9.1000e-11, 5.9597e-11, 2.7515e-11,  ..., 6.2853e-11, 3.3079e-11,
         3.0675e-11],
        [2.8415e-09, 1.9913e-09, 1.3321e-09,  ..., 2.0243e-09, 1.3127e-09,
         7.3742e-10],
        [9.5486e-10, 1.1963e-09, 4.1987e-10,  ..., 9.4055e-10, 7.6549e-10,
         4.0480e-10],
        [4.3640e-10, 2.5957e-10, 1.1807e-10,  ..., 3.2967e-10, 1.1681e-10,
         1.4040e-10]], device='cuda:0')
optimizer state dict: 228.0
lr: [8.703199712272026e-07, 8.703199712272026e-07]
scheduler_last_epoch: 228


Running epoch 1, step 1824, batch 776
Sampled inputs[:2]: tensor([[    0,  7070,    86,  ...,   298,  4930,   518],
        [    0,    13, 11273,  ...,   292,  1057,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.7395e-05,  1.9768e-04,  1.5855e-04,  ..., -1.4967e-04,
         -6.9104e-05, -1.6598e-04],
        [-1.3188e-06, -9.6112e-07,  9.6112e-07,  ..., -1.1846e-06,
         -1.0058e-06, -9.9838e-07],
        [-3.6210e-06, -2.7418e-06,  2.8461e-06,  ..., -3.2187e-06,
         -2.7269e-06, -2.7269e-06],
        [-2.5332e-06, -1.8030e-06,  1.9222e-06,  ..., -2.2948e-06,
         -1.9372e-06, -1.9968e-06],
        [-3.2037e-06, -2.5332e-06,  2.4289e-06,  ..., -2.9355e-06,
         -2.6077e-06, -2.2799e-06]], device='cuda:0')
Loss: 0.9369843006134033


Running epoch 1, step 1825, batch 777
Sampled inputs[:2]: tensor([[    0,   221,   474,  ...,    14, 10961,    12],
        [    0,   278,  1041,  ...,  2098,  1837,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.5077e-04,  2.8040e-04,  2.4941e-04,  ..., -1.4323e-04,
         -1.4497e-04, -2.4117e-04],
        [-2.6450e-06, -2.0266e-06,  1.9372e-06,  ..., -2.3916e-06,
         -2.0117e-06, -1.9446e-06],
        [-7.5251e-06, -6.0201e-06,  5.8860e-06,  ..., -6.7353e-06,
         -5.6773e-06, -5.5134e-06],
        [-5.1409e-06, -3.8743e-06,  3.9190e-06,  ..., -4.6641e-06,
         -3.9190e-06, -3.9488e-06],
        [-6.4075e-06, -5.3346e-06,  4.8727e-06,  ..., -5.8860e-06,
         -5.1856e-06, -4.4107e-06]], device='cuda:0')
Loss: 0.9451848864555359


Running epoch 1, step 1826, batch 778
Sampled inputs[:2]: tensor([[   0, 2518,  437,  ...,   12, 1041,  283],
        [   0, 4485,  741,  ...,  292,  221,  341]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2163e-04,  1.9936e-04,  2.5826e-04,  ..., -1.6838e-04,
         -5.0221e-05, -2.1314e-04],
        [-4.0755e-06, -2.9653e-06,  2.9951e-06,  ..., -3.6433e-06,
         -2.9579e-06, -2.9579e-06],
        [-1.1608e-05, -8.8513e-06,  9.1195e-06,  ..., -1.0312e-05,
         -8.4192e-06, -8.3894e-06],
        [-7.9274e-06, -5.6773e-06,  6.0648e-06,  ..., -7.1079e-06,
         -5.7593e-06, -6.0052e-06],
        [-9.7901e-06, -7.7933e-06,  7.4804e-06,  ..., -8.9109e-06,
         -7.6145e-06, -6.6310e-06]], device='cuda:0')
Loss: 0.9606152772903442


Running epoch 1, step 1827, batch 779
Sampled inputs[:2]: tensor([[    0,  4113,   709,  ..., 22407,  3231,  1130],
        [    0, 38460,     9,  ...,   829,   870,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.0933e-05,  2.2034e-04,  2.2979e-04,  ..., -2.0788e-04,
         -9.6339e-05, -2.4625e-04],
        [-5.4464e-06, -4.0010e-06,  4.0829e-06,  ..., -4.8652e-06,
         -3.9116e-06, -3.8594e-06],
        [-1.5453e-05, -1.1891e-05,  1.2353e-05,  ..., -1.3739e-05,
         -1.1116e-05, -1.0967e-05],
        [-1.0669e-05, -7.7337e-06,  8.3297e-06,  ..., -9.5516e-06,
         -7.6666e-06, -7.9125e-06],
        [-1.2964e-05, -1.0446e-05,  1.0058e-05,  ..., -1.1832e-05,
         -1.0028e-05, -8.5980e-06]], device='cuda:0')
Loss: 0.9801604151725769


Running epoch 1, step 1828, batch 780
Sampled inputs[:2]: tensor([[    0,  1074,  1593,  ...,   992,  1810,   300],
        [    0,  2670, 31283,  ...,    18,  9106,  1389]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5644e-04,  1.6816e-04,  2.3346e-04,  ..., -2.6242e-04,
         -5.9345e-05, -1.7122e-04],
        [-6.7130e-06, -5.0142e-06,  4.9993e-06,  ..., -6.1169e-06,
         -5.0068e-06, -4.8578e-06],
        [-1.9118e-05, -1.4901e-05,  1.5199e-05,  ..., -1.7285e-05,
         -1.4246e-05, -1.3739e-05],
        [-1.3292e-05, -9.8050e-06,  1.0297e-05,  ..., -1.2144e-05,
         -9.9763e-06, -1.0073e-05],
        [-1.6183e-05, -1.3173e-05,  1.2502e-05,  ..., -1.4946e-05,
         -1.2860e-05, -1.0863e-05]], device='cuda:0')
Loss: 1.0071784257888794


Running epoch 1, step 1829, batch 781
Sampled inputs[:2]: tensor([[    0,   278, 38717,  ...,  9945,   367,  5430],
        [    0,    12,  3367,  ..., 16917, 12221, 12138]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1387e-04,  2.1993e-04,  2.1111e-04,  ..., -1.6860e-04,
         -7.7187e-05, -5.4540e-05],
        [-8.1211e-06, -6.0424e-06,  6.1542e-06,  ..., -7.2271e-06,
         -5.9381e-06, -5.8077e-06],
        [ 4.7820e-05,  6.2185e-05, -4.3526e-05,  ...,  2.1914e-05,
          5.7186e-05, -8.1363e-06],
        [-1.6049e-05, -1.1817e-05,  1.2666e-05,  ..., -1.4320e-05,
         -1.1802e-05, -1.2025e-05],
        [-1.9416e-05, -1.5795e-05,  1.5229e-05,  ..., -1.7583e-05,
         -1.5199e-05, -1.2904e-05]], device='cuda:0')
Loss: 0.9901663064956665


Running epoch 1, step 1830, batch 782
Sampled inputs[:2]: tensor([[    0,   949, 11135,  ...,   278,   772,    13],
        [    0,   266,  1784,  ...,  1119,  1276,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2757e-04,  4.4381e-05,  1.9032e-04,  ..., -2.3690e-04,
          2.0953e-04,  9.5998e-05],
        [-9.4995e-06, -6.9961e-06,  7.2569e-06,  ..., -8.4192e-06,
         -6.8657e-06, -6.8210e-06],
        [ 4.3826e-05,  5.9294e-05, -4.0128e-05,  ...,  1.8472e-05,
          5.4474e-05, -1.1072e-05],
        [-1.8850e-05, -1.3739e-05,  1.5005e-05,  ..., -1.6734e-05,
         -1.3687e-05, -1.4171e-05],
        [-2.2694e-05, -1.8284e-05,  1.7911e-05,  ..., -2.0459e-05,
         -1.7554e-05, -1.5125e-05]], device='cuda:0')
Loss: 0.9669448137283325


Running epoch 1, step 1831, batch 783
Sampled inputs[:2]: tensor([[    0,  1236, 14637,  ...,  6601,  3058,    12],
        [    0,    12,  2212,  ..., 12415,  2131,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9381e-04,  1.0957e-04,  2.9387e-04,  ..., -2.5104e-04,
          3.0107e-04,  7.3042e-05],
        [-1.0863e-05, -8.0764e-06,  8.1882e-06,  ..., -9.6336e-06,
         -7.9162e-06, -7.7337e-06],
        [ 4.0101e-05,  5.6180e-05, -3.7357e-05,  ...,  1.5164e-05,
          5.1539e-05, -1.3560e-05],
        [-2.1547e-05, -1.5900e-05,  1.6943e-05,  ..., -1.9163e-05,
         -1.5832e-05, -1.6078e-05],
        [-2.5839e-05, -2.1026e-05,  2.0191e-05,  ..., -2.3320e-05,
         -2.0206e-05, -1.7062e-05]], device='cuda:0')
Loss: 0.9785673022270203
Graident accumulation at epoch 1, step 1831, batch 783
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4264e-04,  7.4445e-05, -9.9917e-06,  ..., -9.6018e-05,
         -5.6485e-05, -1.6489e-04],
        [-1.0874e-05, -7.6320e-06,  8.1414e-06,  ..., -9.5582e-06,
         -7.4635e-06, -7.6620e-06],
        [ 4.5508e-05,  5.4658e-05, -4.0103e-05,  ...,  3.8635e-05,
          4.8226e-05,  9.4089e-06],
        [-9.9970e-06, -1.9548e-07,  9.1572e-06,  ..., -7.9220e-06,
         -1.6677e-06, -7.9663e-06],
        [-1.4399e-05, -8.9082e-06,  6.4185e-06,  ..., -9.6033e-06,
         -5.6845e-06, -7.6493e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0494e-08, 5.5955e-08, 5.5128e-08,  ..., 2.1829e-08, 1.2419e-07,
         5.0901e-08],
        [9.1027e-11, 5.9603e-11, 2.7555e-11,  ..., 6.2883e-11, 3.3108e-11,
         3.0704e-11],
        [2.8403e-09, 1.9925e-09, 1.3321e-09,  ..., 2.0225e-09, 1.3140e-09,
         7.3686e-10],
        [9.5437e-10, 1.1953e-09, 4.1973e-10,  ..., 9.3998e-10, 7.6497e-10,
         4.0466e-10],
        [4.3664e-10, 2.5975e-10, 1.1836e-10,  ..., 3.2988e-10, 1.1710e-10,
         1.4055e-10]], device='cuda:0')
optimizer state dict: 229.0
lr: [8.205793726931199e-07, 8.205793726931199e-07]
scheduler_last_epoch: 229


Running epoch 1, step 1832, batch 784
Sampled inputs[:2]: tensor([[   0,  287, 3609,  ..., 3661, 5944,  838],
        [   0,  287,  768,  ...,  221,  474,  221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2115e-05,  6.4496e-05,  1.1422e-04,  ...,  2.9847e-06,
         -1.2654e-05,  3.7442e-05],
        [-1.4082e-06, -1.0133e-06,  1.0580e-06,  ..., -1.2293e-06,
         -9.9093e-07, -1.0282e-06],
        [-4.0233e-06, -3.0398e-06,  3.2485e-06,  ..., -3.5018e-06,
         -2.8908e-06, -2.9802e-06],
        [-2.8163e-06, -1.9968e-06,  2.1905e-06,  ..., -2.4438e-06,
         -1.9968e-06, -2.1309e-06],
        [-3.3528e-06, -2.6226e-06,  2.6077e-06,  ..., -2.9802e-06,
         -2.5779e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.962022066116333


Running epoch 1, step 1833, batch 785
Sampled inputs[:2]: tensor([[   0,  300, 5201,  ..., 1997, 7423,  417],
        [   0,   13, 4596,  ...,  408,  689,  298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.0112e-05,  6.4496e-05,  1.8177e-05,  ..., -7.7500e-05,
         -7.1999e-06,  1.2656e-05],
        [-2.6971e-06, -1.8589e-06,  2.0415e-06,  ..., -2.3916e-06,
         -1.8515e-06, -1.9893e-06],
        [-7.6145e-06, -5.4687e-06,  6.2585e-06,  ..., -6.6459e-06,
         -5.2601e-06, -5.5283e-06],
        [-5.3495e-06, -3.6210e-06,  4.2170e-06,  ..., -4.7386e-06,
         -3.7029e-06, -4.0829e-06],
        [-6.3032e-06, -4.7237e-06,  5.0068e-06,  ..., -5.6177e-06,
         -4.6641e-06, -4.2170e-06]], device='cuda:0')
Loss: 0.9440675377845764


Running epoch 1, step 1834, batch 786
Sampled inputs[:2]: tensor([[   0,  271, 3421,  ...,  306,  472,  346],
        [   0, 2974,  278,  ...,  365, 8758,  271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.3167e-05,  4.4438e-05, -1.2675e-05,  ..., -5.1537e-05,
          2.7989e-04,  6.5878e-05],
        [-4.0829e-06, -2.8089e-06,  3.0473e-06,  ..., -3.5912e-06,
         -2.7493e-06, -2.9579e-06],
        [-1.1578e-05, -8.2999e-06,  9.3579e-06,  ..., -1.0028e-05,
         -7.8082e-06, -8.2254e-06],
        [-8.1062e-06, -5.4538e-06,  6.3032e-06,  ..., -7.1377e-06,
         -5.4985e-06, -6.1095e-06],
        [-9.5665e-06, -7.1824e-06,  7.4953e-06,  ..., -8.4788e-06,
         -6.9290e-06, -6.2436e-06]], device='cuda:0')
Loss: 0.9509652853012085


Running epoch 1, step 1835, batch 787
Sampled inputs[:2]: tensor([[    0,  8878,  6716,  ...,  8878,   328, 31139],
        [    0,   527,  2811,  ...,   287,  1288,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.9294e-05,  7.1010e-05,  1.5775e-04,  ..., -1.0707e-04,
          4.6736e-04,  2.1804e-04],
        [-5.4240e-06, -3.8967e-06,  4.0382e-06,  ..., -4.7982e-06,
         -3.8445e-06, -3.9116e-06],
        [-1.5393e-05, -1.1548e-05,  1.2353e-05,  ..., -1.3456e-05,
         -1.0967e-05, -1.0952e-05],
        [-1.0848e-05, -7.6592e-06,  8.4043e-06,  ..., -9.5963e-06,
         -7.7635e-06, -8.1509e-06],
        [-1.2830e-05, -1.0058e-05,  9.9987e-06,  ..., -1.1474e-05,
         -9.7752e-06, -8.4341e-06]], device='cuda:0')
Loss: 0.9892072081565857


Running epoch 1, step 1836, batch 788
Sampled inputs[:2]: tensor([[   0, 1862,   14,  ..., 2310, 2915, 4016],
        [   0,  515,  266,  ...,   18, 3770, 1345]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4020e-04, -3.7484e-05,  1.3309e-04,  ..., -1.2779e-04,
          4.6561e-04,  2.9100e-04],
        [-6.7800e-06, -4.8950e-06,  5.0962e-06,  ..., -5.9828e-06,
         -4.7684e-06, -4.8615e-06],
        [-1.9386e-05, -1.4588e-05,  1.5676e-05,  ..., -1.6898e-05,
         -1.3664e-05, -1.3724e-05],
        [-1.3545e-05, -9.6262e-06,  1.0625e-05,  ..., -1.1951e-05,
         -9.5963e-06, -1.0148e-05],
        [-1.6093e-05, -1.2636e-05,  1.2651e-05,  ..., -1.4350e-05,
         -1.2130e-05, -1.0520e-05]], device='cuda:0')
Loss: 0.9593359231948853


Running epoch 1, step 1837, batch 789
Sampled inputs[:2]: tensor([[    0,   271, 10474,  ...,   298,  2286,    29],
        [    0,  2923,   391,  ...,    14,  5424,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8826e-04, -1.8678e-05,  7.2409e-05,  ..., -1.6366e-04,
          4.6765e-04,  2.5422e-04],
        [-8.2105e-06, -5.9307e-06,  6.1691e-06,  ..., -7.2196e-06,
         -5.7444e-06, -5.7928e-06],
        [-2.3320e-05, -1.7583e-05,  1.8835e-05,  ..., -2.0295e-05,
         -1.6391e-05, -1.6317e-05],
        [-1.6287e-05, -1.1608e-05,  1.2770e-05,  ..., -1.4320e-05,
         -1.1481e-05, -1.2025e-05],
        [-1.9357e-05, -1.5274e-05,  1.5214e-05,  ..., -1.7256e-05,
         -1.4558e-05, -1.2517e-05]], device='cuda:0')
Loss: 0.9628862142562866


Running epoch 1, step 1838, batch 790
Sampled inputs[:2]: tensor([[    0,  9342,   600,  ...,   199, 12095,   291],
        [    0,    12, 30621,  ...,   578,  3126,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.6035e-04,  2.1122e-05,  1.4849e-04,  ..., -2.3572e-04,
          4.6595e-04,  4.8156e-04],
        [-9.5144e-06, -6.8918e-06,  7.2569e-06,  ..., -8.3297e-06,
         -6.6049e-06, -6.7465e-06],
        [ 4.8214e-05,  6.1504e-05, -3.6131e-05,  ...,  5.9246e-05,
          4.3002e-05,  5.8295e-06],
        [-1.8939e-05, -1.3545e-05,  1.5080e-05,  ..., -1.6570e-05,
         -1.3247e-05, -1.4067e-05],
        [-2.2292e-05, -1.7658e-05,  1.7747e-05,  ..., -1.9789e-05,
         -1.6659e-05, -1.4454e-05]], device='cuda:0')
Loss: 0.9526121616363525


Running epoch 1, step 1839, batch 791
Sampled inputs[:2]: tensor([[    0,  9017,   600,  ...,  6133,  1098,   352],
        [    0,   335,   446,  ...,  5795,    12, 12433]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3943e-04, -1.1474e-05,  1.1657e-04,  ..., -2.0674e-04,
          3.1788e-04,  4.2815e-04],
        [-1.0811e-05, -7.9423e-06,  8.3447e-06,  ..., -9.4846e-06,
         -7.5251e-06, -7.6555e-06],
        [ 4.4340e-05,  5.8256e-05, -3.2704e-05,  ...,  5.5804e-05,
          4.0290e-05,  3.1026e-06],
        [-2.1622e-05, -1.5676e-05,  1.7434e-05,  ..., -1.8939e-05,
         -1.5117e-05, -1.6049e-05],
        [-2.5377e-05, -2.0355e-05,  2.0400e-05,  ..., -2.2605e-05,
         -1.8984e-05, -1.6451e-05]], device='cuda:0')
Loss: 0.9646923542022705
Graident accumulation at epoch 1, step 1839, batch 791
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4232e-04,  6.5853e-05,  2.6641e-06,  ..., -1.0709e-04,
         -1.9049e-05, -1.0558e-04],
        [-1.0868e-05, -7.6631e-06,  8.1617e-06,  ..., -9.5508e-06,
         -7.4696e-06, -7.6613e-06],
        [ 4.5391e-05,  5.5017e-05, -3.9363e-05,  ...,  4.0352e-05,
          4.7432e-05,  8.7782e-06],
        [-1.1159e-05, -1.7435e-06,  9.9850e-06,  ..., -9.0237e-06,
         -3.0127e-06, -8.7745e-06],
        [-1.5497e-05, -1.0053e-05,  7.8166e-06,  ..., -1.0903e-05,
         -7.0144e-06, -8.5295e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0491e-08, 5.5899e-08, 5.5087e-08,  ..., 2.1850e-08, 1.2417e-07,
         5.1034e-08],
        [9.1053e-11, 5.9606e-11, 2.7597e-11,  ..., 6.2910e-11, 3.3132e-11,
         3.0732e-11],
        [2.8394e-09, 1.9939e-09, 1.3319e-09,  ..., 2.0236e-09, 1.3143e-09,
         7.3614e-10],
        [9.5388e-10, 1.1944e-09, 4.1962e-10,  ..., 9.3939e-10, 7.6443e-10,
         4.0451e-10],
        [4.3684e-10, 2.5991e-10, 1.1866e-10,  ..., 3.3006e-10, 1.1735e-10,
         1.4068e-10]], device='cuda:0')
optimizer state dict: 230.0
lr: [7.722414697591851e-07, 7.722414697591851e-07]
scheduler_last_epoch: 230


Running epoch 1, step 1840, batch 792
Sampled inputs[:2]: tensor([[   0, 1853, 3373,  ..., 3020, 6695,  300],
        [   0,  266, 3727,  ..., 1143,  271, 5213]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.1134e-05,  6.2100e-06, -5.3222e-05,  ..., -6.7140e-05,
          1.0334e-04, -1.4071e-05],
        [-1.4380e-06, -9.5367e-07,  1.0133e-06,  ..., -1.2517e-06,
         -8.8289e-07, -1.0431e-06],
        [-4.2021e-06, -2.9206e-06,  3.1739e-06,  ..., -3.6359e-06,
         -2.6077e-06, -3.0398e-06],
        [ 1.1039e-04,  8.5047e-05, -4.7317e-05,  ...,  7.3161e-05,
          8.1742e-05,  3.6444e-05],
        [-3.5167e-06, -2.5034e-06,  2.5332e-06,  ..., -3.1143e-06,
         -2.3246e-06, -2.3991e-06]], device='cuda:0')
Loss: 0.9651790261268616


Running epoch 1, step 1841, batch 793
Sampled inputs[:2]: tensor([[    0,  1420,  2337,  ...,   722, 28860,   287],
        [    0,   898,  1427,  ...,   508,  1860,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1270e-04,  2.3629e-04,  1.2645e-04,  ..., -1.5048e-04,
          3.2994e-05,  2.9878e-06],
        [-2.7642e-06, -1.9521e-06,  2.0266e-06,  ..., -2.4140e-06,
         -1.9632e-06, -2.0117e-06],
        [-8.0019e-06, -5.9605e-06,  6.3032e-06,  ..., -6.9588e-06,
         -5.7220e-06, -5.8115e-06],
        [ 1.0771e-04,  8.3020e-05, -4.5171e-05,  ...,  7.0807e-05,
          7.9507e-05,  3.4372e-05],
        [-6.6757e-06, -5.1409e-06,  5.0664e-06,  ..., -5.9307e-06,
         -5.0515e-06, -4.5300e-06]], device='cuda:0')
Loss: 0.9764245748519897


Running epoch 1, step 1842, batch 794
Sampled inputs[:2]: tensor([[    0,   694,   266,  ...,  3007,   300,  5726],
        [    0,   374,   298,  ..., 11183,    12,   654]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6872e-04,  3.8497e-04,  7.4108e-05,  ..., -1.3218e-04,
         -8.3930e-05, -1.5377e-04],
        [-4.2319e-06, -2.9057e-06,  3.0473e-06,  ..., -3.7253e-06,
         -2.9020e-06, -3.1069e-06],
        [-1.1936e-05, -8.6725e-06,  9.2983e-06,  ..., -1.0416e-05,
         -8.3148e-06, -8.6725e-06],
        [ 1.0500e-04,  8.1255e-05, -4.3189e-05,  ...,  6.8378e-05,
          7.7741e-05,  3.2331e-05],
        [-1.0252e-05, -7.6443e-06,  7.6592e-06,  ..., -9.1493e-06,
         -7.5549e-06, -6.9886e-06]], device='cuda:0')
Loss: 0.9401813745498657


Running epoch 1, step 1843, batch 795
Sampled inputs[:2]: tensor([[    0,  5007,  7551,  ...,     9,  2095,   300],
        [    0,   287,  4599,  ..., 11812,   266,  1036]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7208e-04,  5.5078e-04,  1.2483e-04,  ..., -1.6598e-04,
         -2.5116e-04, -2.5245e-04],
        [-5.6401e-06, -3.9190e-06,  4.1053e-06,  ..., -4.9844e-06,
         -3.9004e-06, -4.1202e-06],
        [-1.5870e-05, -1.1608e-05,  1.2457e-05,  ..., -1.3888e-05,
         -1.1101e-05, -1.1429e-05],
        [ 1.0230e-04,  7.9362e-05, -4.1088e-05,  ...,  6.5964e-05,
          7.5819e-05,  3.0319e-05],
        [-1.3560e-05, -1.0222e-05,  1.0252e-05,  ..., -1.2130e-05,
         -1.0028e-05, -9.1344e-06]], device='cuda:0')
Loss: 0.9716850519180298


Running epoch 1, step 1844, batch 796
Sampled inputs[:2]: tensor([[   0, 7555, 3908,  ...,  259, 8477,  278],
        [   0,  600,   14,  ...,  221, 8187, 1802]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.2692e-04,  5.8543e-04,  3.0749e-05,  ..., -1.8157e-04,
         -3.3020e-04, -2.7541e-04],
        [-7.0110e-06, -4.8615e-06,  5.2154e-06,  ..., -6.1393e-06,
         -4.7311e-06, -5.0589e-06],
        [-2.0042e-05, -1.4529e-05,  1.5974e-05,  ..., -1.7345e-05,
         -1.3590e-05, -1.4260e-05],
        [ 9.9408e-05,  7.7440e-05, -3.8674e-05,  ...,  6.3565e-05,
          7.4105e-05,  2.8263e-05],
        [-1.6749e-05, -1.2577e-05,  1.2890e-05,  ..., -1.4856e-05,
         -1.2100e-05, -1.1146e-05]], device='cuda:0')
Loss: 0.954275369644165


Running epoch 1, step 1845, batch 797
Sampled inputs[:2]: tensor([[   0, 1751,  287,  ..., 6079, 1059,  287],
        [   0, 2013,   13,  ...,  271,  266,  908]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.0702e-04,  6.2753e-04,  2.6463e-06,  ..., -2.6642e-04,
         -2.9672e-04, -2.6417e-04],
        [-8.3447e-06, -5.9642e-06,  6.3106e-06,  ..., -7.3314e-06,
         -5.7369e-06, -5.9865e-06],
        [-2.3857e-05, -1.7792e-05,  1.9267e-05,  ..., -2.0757e-05,
         -1.6496e-05, -1.6972e-05],
        [ 9.6815e-05,  7.5324e-05, -3.6454e-05,  ...,  6.1255e-05,
          7.2153e-05,  2.6356e-05],
        [-1.9938e-05, -1.5438e-05,  1.5587e-05,  ..., -1.7777e-05,
         -1.4707e-05, -1.3247e-05]], device='cuda:0')
Loss: 0.9683536887168884


Running epoch 1, step 1846, batch 798
Sampled inputs[:2]: tensor([[    0, 33315,   266,  ...,    12,  1126,    14],
        [    0,   600,   518,  ...,  3134,   278, 37342]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.6658e-04,  5.7311e-04, -5.0249e-05,  ..., -3.4664e-04,
         -2.8079e-04, -3.2772e-04],
        [-9.6560e-06, -6.9551e-06,  7.3984e-06,  ..., -8.4937e-06,
         -6.6608e-06, -6.8992e-06],
        [-2.7582e-05, -2.0757e-05,  2.2560e-05,  ..., -2.4065e-05,
         -1.9148e-05, -1.9580e-05],
        [ 9.4222e-05,  7.3372e-05, -3.4189e-05,  ...,  5.8945e-05,
          7.0320e-05,  2.4448e-05],
        [-2.2948e-05, -1.7956e-05,  1.8179e-05,  ..., -2.0519e-05,
         -1.7002e-05, -1.5184e-05]], device='cuda:0')
Loss: 0.9594557285308838


Running epoch 1, step 1847, batch 799
Sampled inputs[:2]: tensor([[    0,   292,   474,  ...,   446, 14932,   365],
        [    0,  3978,  2697,  ...,   461,  5955,  3792]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.1072e-04,  6.1088e-04, -1.3524e-04,  ..., -2.5064e-04,
         -2.3384e-04, -4.9612e-04],
        [-1.1086e-05, -7.8529e-06,  8.4043e-06,  ..., -9.8124e-06,
         -7.6443e-06, -8.0168e-06],
        [-3.1546e-05, -2.3320e-05,  2.5541e-05,  ..., -2.7612e-05,
         -2.1785e-05, -2.2545e-05],
        [ 9.1525e-05,  7.1710e-05, -3.2222e-05,  ...,  5.6472e-05,
          6.8480e-05,  2.2303e-05],
        [-2.6584e-05, -2.0325e-05,  2.0787e-05,  ..., -2.3797e-05,
         -1.9535e-05, -1.7732e-05]], device='cuda:0')
Loss: 0.9610378742218018
Graident accumulation at epoch 1, step 1847, batch 799
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0142, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.6916e-04,  1.2036e-04, -1.1126e-05,  ..., -1.2145e-04,
         -4.0527e-05, -1.4464e-04],
        [-1.0890e-05, -7.6820e-06,  8.1860e-06,  ..., -9.5770e-06,
         -7.4871e-06, -7.6969e-06],
        [ 3.7697e-05,  4.7184e-05, -3.2873e-05,  ...,  3.3556e-05,
          4.0511e-05,  5.6459e-06],
        [-8.9095e-07,  5.6018e-06,  5.7643e-06,  ..., -2.4742e-06,
          4.1366e-06, -5.6668e-06],
        [-1.6606e-05, -1.1080e-05,  9.1136e-06,  ..., -1.2193e-05,
         -8.2665e-06, -9.4498e-06]], device='cuda:0')
optimizer state dict: tensor([[6.0691e-08, 5.6216e-08, 5.5050e-08,  ..., 2.1890e-08, 1.2410e-07,
         5.1229e-08],
        [9.1085e-11, 5.9608e-11, 2.7640e-11,  ..., 6.2944e-11, 3.3157e-11,
         3.0765e-11],
        [2.8376e-09, 1.9924e-09, 1.3312e-09,  ..., 2.0224e-09, 1.3135e-09,
         7.3591e-10],
        [9.6131e-10, 1.1983e-09, 4.2024e-10,  ..., 9.4164e-10, 7.6836e-10,
         4.0460e-10],
        [4.3711e-10, 2.6006e-10, 1.1897e-10,  ..., 3.3030e-10, 1.1761e-10,
         1.4085e-10]], device='cuda:0')
optimizer state dict: 231.0
lr: [7.253136488789125e-07, 7.253136488789125e-07]
scheduler_last_epoch: 231
Epoch 1 | Batch 799/1048 | Training PPL: 2182.667417398266 | time 84.96034049987793
Saving checkpoint at epoch 1, step 1847, batch 799
Epoch 1 | Validation PPL: 6.723030605251268 | Learning rate: 7.253136488789125e-07
Checkpoint saved to exp/gpt2_scratch_neuro_tokenizer_bayes_fwd_mini/checkpoint.1_1847, AFTER epoch 1, step 1847


Running epoch 1, step 1848, batch 800
Sampled inputs[:2]: tensor([[   0,  382,   17,  ..., 8733,   13, 9306],
        [   0, 2958,  298,  ...,   12,  709,  616]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1844e-04, -7.2317e-05, -6.8403e-05,  ...,  5.5866e-05,
         -2.6006e-04,  0.0000e+00],
        [-1.3337e-06, -7.9349e-07,  1.0431e-06,  ..., -1.2368e-06,
         -8.7172e-07, -1.1027e-06],
        [-3.7700e-06, -2.3395e-06,  3.1292e-06,  ..., -3.4124e-06,
         -2.4289e-06, -2.9802e-06],
        [-2.6375e-06, -1.5050e-06,  2.1309e-06,  ..., -2.4289e-06,
         -1.7136e-06, -2.2352e-06],
        [-3.1590e-06, -2.0862e-06,  2.5630e-06,  ..., -2.9057e-06,
         -2.1607e-06, -2.3097e-06]], device='cuda:0')
Loss: 0.9270754456520081


Running epoch 1, step 1849, batch 801
Sampled inputs[:2]: tensor([[   0,  953,  328,  ..., 2245,   12, 1253],
        [   0, 4213, 1921,  ..., 1340, 1049,  292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.8234e-04, -6.3089e-05, -1.2111e-04,  ...,  3.1297e-05,
         -3.4564e-04, -1.0143e-04],
        [-2.7344e-06, -1.7844e-06,  2.1756e-06,  ..., -2.4289e-06,
         -1.7919e-06, -2.0713e-06],
        [-7.9423e-06, -5.3495e-06,  6.6310e-06,  ..., -6.9439e-06,
         -5.1260e-06, -5.8711e-06],
        [-5.4687e-06, -3.4571e-06,  4.4852e-06,  ..., -4.8131e-06,
         -3.5390e-06, -4.2766e-06],
        [-6.4671e-06, -4.6194e-06,  5.2750e-06,  ..., -5.7966e-06,
         -4.4703e-06, -4.4405e-06]], device='cuda:0')
Loss: 0.9945716261863708


Running epoch 1, step 1850, batch 802
Sampled inputs[:2]: tensor([[    0,   944,   278,  ..., 17330,  1683,   360],
        [    0,  5041,    14,  ...,  1027,  1722,  6554]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 4.3666e-04, -6.7034e-05, -1.2995e-04,  ..., -1.6834e-05,
         -3.9193e-04, -1.0114e-04],
        [-4.0531e-06, -2.7306e-06,  3.2037e-06,  ..., -3.6284e-06,
         -2.7195e-06, -3.0696e-06],
        [-1.1668e-05, -8.1360e-06,  9.7454e-06,  ..., -1.0267e-05,
         -7.7337e-06, -8.6576e-06],
        [-8.0913e-06, -5.3048e-06,  6.6012e-06,  ..., -7.1973e-06,
         -5.4091e-06, -6.3628e-06],
        [-9.5516e-06, -7.0482e-06,  7.7933e-06,  ..., -8.6129e-06,
         -6.7800e-06, -6.5714e-06]], device='cuda:0')
Loss: 0.9472113251686096


Running epoch 1, step 1851, batch 803
Sampled inputs[:2]: tensor([[    0,   266,  5232,  ...,  2719,    13, 25385],
        [    0,  1176, 33084,  ...,   266,  2269,  1209]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.9722e-04, -7.1376e-06, -2.4208e-04,  ..., -5.2167e-05,
         -3.7797e-04, -1.3668e-04],
        [-5.3793e-06, -3.7365e-06,  4.2021e-06,  ..., -4.8503e-06,
         -3.7104e-06, -4.0308e-06],
        [-1.5512e-05, -1.1161e-05,  1.2860e-05,  ..., -1.3769e-05,
         -1.0595e-05, -1.1399e-05],
        [-1.0744e-05, -7.3016e-06,  8.6725e-06,  ..., -9.6560e-06,
         -7.4208e-06, -8.3745e-06],
        [-1.2696e-05, -9.6262e-06,  1.0282e-05,  ..., -1.1533e-05,
         -9.2685e-06, -8.6278e-06]], device='cuda:0')
Loss: 0.965935468673706


Running epoch 1, step 1852, batch 804
Sampled inputs[:2]: tensor([[    0,   380,  1075,  ..., 16948,   266,  1751],
        [    0,  8290,   391,  ...,   298,  1253,     7]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6540e-04, -8.2257e-05, -2.7249e-04,  ..., -1.4496e-05,
         -2.3361e-04, -1.8607e-04],
        [-6.7800e-06, -4.6566e-06,  5.2676e-06,  ..., -6.0946e-06,
         -4.5560e-06, -5.0440e-06],
        [-1.9476e-05, -1.3843e-05,  1.6093e-05,  ..., -1.7196e-05,
         -1.2934e-05, -1.4216e-05],
        [-1.3381e-05, -9.0003e-06,  1.0759e-05,  ..., -1.1981e-05,
         -8.9929e-06, -1.0341e-05],
        [-1.5929e-05, -1.1906e-05,  1.2830e-05,  ..., -1.4395e-05,
         -1.1355e-05, -1.0729e-05]], device='cuda:0')
Loss: 0.9565526247024536


Running epoch 1, step 1853, batch 805
Sampled inputs[:2]: tensor([[   0,  925,  271,  ...,  391,  721, 1576],
        [   0, 1742,   14,  ..., 1684,   13, 1107]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.9414e-04, -3.3143e-05, -2.6660e-04,  ..., -1.4406e-04,
         -3.8188e-04, -3.2305e-04],
        [-8.2552e-06, -5.6401e-06,  6.2957e-06,  ..., -7.4059e-06,
         -5.5470e-06, -6.1244e-06],
        [-2.3469e-05, -1.6704e-05,  1.9103e-05,  ..., -2.0772e-05,
         -1.5676e-05, -1.7151e-05],
        [-1.6093e-05, -1.0803e-05,  1.2726e-05,  ..., -1.4409e-05,
         -1.0848e-05, -1.2413e-05],
        [-1.9580e-05, -1.4603e-05,  1.5482e-05,  ..., -1.7732e-05,
         -1.4037e-05, -1.3262e-05]], device='cuda:0')
Loss: 0.9392732977867126


Running epoch 1, step 1854, batch 806
Sampled inputs[:2]: tensor([[    0, 33792,   352,  ...,   278,   546, 30495],
        [    0,  2202,   292,  ...,  2431,  2267,  3423]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6599e-04, -6.9986e-05, -2.3261e-04,  ..., -1.8401e-04,
         -2.5220e-04, -2.4060e-04],
        [-9.5963e-06, -6.6757e-06,  7.3761e-06,  ..., -8.5905e-06,
         -6.4969e-06, -7.0259e-06],
        [-2.7314e-05, -1.9789e-05,  2.2382e-05,  ..., -2.4185e-05,
         -1.8433e-05, -1.9789e-05],
        [-1.8761e-05, -1.2845e-05,  1.4961e-05,  ..., -1.6764e-05,
         -1.2748e-05, -1.4298e-05],
        [-2.2754e-05, -1.7300e-05,  1.8120e-05,  ..., -2.0623e-05,
         -1.6510e-05, -1.5303e-05]], device='cuda:0')
Loss: 0.9930600523948669


Running epoch 1, step 1855, batch 807
Sampled inputs[:2]: tensor([[    0,   367,   925,  ..., 25491,   847,   328],
        [    0,  1085,  4878,  ...,   298,   894,   496]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.6599e-04,  5.8989e-06, -2.5507e-04,  ..., -1.7825e-04,
         -3.8975e-04, -2.2348e-04],
        [-1.0982e-05, -7.6815e-06,  8.4639e-06,  ..., -9.7901e-06,
         -7.4171e-06, -7.9796e-06],
        [-3.1397e-05, -2.2843e-05,  2.5764e-05,  ..., -2.7701e-05,
         -2.1130e-05, -2.2620e-05],
        [-2.1487e-05, -1.4797e-05,  1.7166e-05,  ..., -1.9133e-05,
         -1.4551e-05, -1.6265e-05],
        [-2.5988e-05, -1.9863e-05,  2.0742e-05,  ..., -2.3484e-05,
         -1.8820e-05, -1.7375e-05]], device='cuda:0')
Loss: 0.9680315256118774
Graident accumulation at epoch 1, step 1855, batch 807
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0024,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.0884e-04,  1.0891e-04, -3.5520e-05,  ..., -1.2713e-04,
         -7.5450e-05, -1.5252e-04],
        [-1.0899e-05, -7.6820e-06,  8.2137e-06,  ..., -9.5983e-06,
         -7.4801e-06, -7.7251e-06],
        [ 3.0788e-05,  4.0181e-05, -2.7009e-05,  ...,  2.7430e-05,
          3.4347e-05,  2.8193e-06],
        [-2.9506e-06,  3.5620e-06,  6.9045e-06,  ..., -4.1401e-06,
          2.2678e-06, -6.7266e-06],
        [-1.7544e-05, -1.1958e-05,  1.0277e-05,  ..., -1.3322e-05,
         -9.3219e-06, -1.0242e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1074e-08, 5.6160e-08, 5.5060e-08,  ..., 2.1900e-08, 1.2413e-07,
         5.1227e-08],
        [9.1114e-11, 5.9607e-11, 2.7684e-11,  ..., 6.2977e-11, 3.3179e-11,
         3.0798e-11],
        [2.8357e-09, 1.9909e-09, 1.3305e-09,  ..., 2.0211e-09, 1.3126e-09,
         7.3568e-10],
        [9.6081e-10, 1.1973e-09, 4.2011e-10,  ..., 9.4107e-10, 7.6780e-10,
         4.0446e-10],
        [4.3735e-10, 2.6020e-10, 1.1928e-10,  ..., 3.3052e-10, 1.1785e-10,
         1.4101e-10]], device='cuda:0')
optimizer state dict: 232.0
lr: [6.798030810329725e-07, 6.798030810329725e-07]
scheduler_last_epoch: 232


Running epoch 1, step 1856, batch 808
Sampled inputs[:2]: tensor([[    0,   328,  9424,  ...,    13, 24635,   368],
        [    0,    12,   696,  ..., 14275,  2661,  6129]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9343e-07, -1.4359e-05,  1.5220e-04,  ..., -2.1086e-06,
          9.7102e-05, -2.3392e-05],
        [-1.3486e-06, -1.0207e-06,  1.0356e-06,  ..., -1.2293e-06,
         -9.4995e-07, -9.7603e-07],
        [-3.9637e-06, -3.0845e-06,  3.2037e-06,  ..., -3.5465e-06,
         -2.7269e-06, -2.8312e-06],
        [-2.6822e-06, -1.9819e-06,  2.1309e-06,  ..., -2.4289e-06,
         -1.8775e-06, -2.0415e-06],
        [-3.3379e-06, -2.6971e-06,  2.6226e-06,  ..., -3.0398e-06,
         -2.4438e-06, -2.2054e-06]], device='cuda:0')
Loss: 0.9830897450447083


Running epoch 1, step 1857, batch 809
Sampled inputs[:2]: tensor([[    0,  5440,    13,  ...,  1878,   342,  2060],
        [    0,   287,   298,  ..., 14121,  3121,   409]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.4229e-04, -5.3659e-05,  4.5412e-06,  ...,  1.5834e-04,
          2.1812e-04,  1.7077e-04],
        [-2.8610e-06, -2.0042e-06,  2.1011e-06,  ..., -2.5108e-06,
         -1.9409e-06, -2.0936e-06],
        [-8.2552e-06, -5.9605e-06,  6.4075e-06,  ..., -7.1526e-06,
         -5.5432e-06, -5.9903e-06],
        [-5.4389e-06, -3.7402e-06,  4.1425e-06,  ..., -4.7684e-06,
         -3.6955e-06, -4.1425e-06],
        [-7.1079e-06, -5.3495e-06,  5.3644e-06,  ..., -6.2883e-06,
         -5.0813e-06, -4.7982e-06]], device='cuda:0')
Loss: 0.9544647932052612


Running epoch 1, step 1858, batch 810
Sampled inputs[:2]: tensor([[    0,  1336, 10446,  ...,   409,   275, 12528],
        [    0,   367,  2870,  ...,  1456, 17304,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9936e-05, -1.1189e-04, -2.0287e-05,  ...,  1.1869e-04,
          1.5923e-04,  1.2007e-04],
        [-4.1872e-06, -2.9542e-06,  3.2187e-06,  ..., -3.6657e-06,
         -2.8275e-06, -3.0063e-06],
        [-1.2070e-05, -8.7619e-06,  9.8050e-06,  ..., -1.0431e-05,
         -8.0615e-06, -8.5831e-06],
        [-8.1211e-06, -5.6177e-06,  6.4969e-06,  ..., -7.0930e-06,
         -5.4538e-06, -6.0648e-06],
        [-1.0177e-05, -7.7188e-06,  8.0317e-06,  ..., -8.9854e-06,
         -7.2420e-06, -6.7204e-06]], device='cuda:0')
Loss: 0.983022928237915


Running epoch 1, step 1859, batch 811
Sampled inputs[:2]: tensor([[    0,    14,  3228,  ..., 13747,   287, 20295],
        [    0,   578,   221,  ...,   287,  1254,  4318]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.2419e-04, -1.7006e-04,  1.9859e-04,  ...,  4.1057e-04,
         -1.7823e-04,  4.1225e-04],
        [-5.6177e-06, -3.8408e-06,  4.0345e-06,  ..., -5.0068e-06,
         -3.8333e-06, -4.1835e-06],
        [ 1.9660e-04,  2.7810e-04, -1.5488e-04,  ...,  2.0272e-04,
          2.2435e-04,  1.1471e-04],
        [-1.0982e-05, -7.3612e-06,  8.1435e-06,  ..., -9.8199e-06,
         -7.5102e-06, -8.4788e-06],
        [-1.3798e-05, -1.0133e-05,  1.0207e-05,  ..., -1.2353e-05,
         -9.8497e-06, -9.4622e-06]], device='cuda:0')
Loss: 0.9210466742515564


Running epoch 1, step 1860, batch 812
Sampled inputs[:2]: tensor([[    0,  1167,   278,  ...,   278,  1853,  1424],
        [    0, 49141,    14,  ...,   342,   259,  1943]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.6670e-04, -2.4778e-04,  2.8661e-04,  ...,  3.9489e-04,
         -2.3622e-04,  3.4359e-04],
        [-6.9141e-06, -4.8019e-06,  5.1595e-06,  ..., -6.1244e-06,
         -4.6864e-06, -5.0813e-06],
        [ 1.9283e-04,  2.7527e-04, -1.5142e-04,  ...,  1.9952e-04,
          2.2189e-04,  1.1213e-04],
        [-1.3605e-05, -9.2387e-06,  1.0513e-05,  ..., -1.2055e-05,
         -9.2164e-06, -1.0379e-05],
        [-1.6779e-05, -1.2517e-05,  1.2860e-05,  ..., -1.4946e-05,
         -1.1951e-05, -1.1317e-05]], device='cuda:0')
Loss: 0.9923936128616333


Running epoch 1, step 1861, batch 813
Sampled inputs[:2]: tensor([[    0,   508, 12163,  ...,  4920,   344, 11003],
        [    0,  1640,  1103,  ...,   685,  1478,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.7876e-04, -2.3441e-04,  3.4616e-04,  ...,  4.6922e-04,
         -1.3885e-04,  2.3243e-04],
        [-8.3148e-06, -5.6960e-06,  6.2250e-06,  ..., -7.3761e-06,
         -5.5581e-06, -6.1840e-06],
        [ 1.8899e-04,  2.7261e-04, -1.4825e-04,  ...,  1.9611e-04,
          2.1942e-04,  1.0917e-04],
        [-1.6272e-05, -1.0937e-05,  1.2644e-05,  ..., -1.4469e-05,
         -1.0930e-05, -1.2539e-05],
        [-2.0012e-05, -1.4782e-05,  1.5438e-05,  ..., -1.7866e-05,
         -1.4156e-05, -1.3657e-05]], device='cuda:0')
Loss: 0.9212344884872437


Running epoch 1, step 1862, batch 814
Sampled inputs[:2]: tensor([[   0,  360,  508,  ...,  259,  554, 1319],
        [   0,   14,  475,  ..., 4103,  278, 4190]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.5029e-04, -1.9122e-04,  2.5376e-04,  ...,  5.7074e-04,
         -2.3331e-04,  2.7849e-04],
        [-9.7528e-06, -6.6645e-06,  7.3425e-06,  ..., -8.5905e-06,
         -6.4112e-06, -7.1339e-06],
        [ 1.8487e-04,  2.6966e-04, -1.4483e-04,  ...,  1.9263e-04,
          2.1691e-04,  1.0643e-04],
        [-1.9103e-05, -1.2845e-05,  1.4938e-05,  ..., -1.6868e-05,
         -1.2629e-05, -1.4491e-05],
        [-2.3246e-05, -1.7241e-05,  1.8060e-05,  ..., -2.0683e-05,
         -1.6302e-05, -1.5654e-05]], device='cuda:0')
Loss: 0.9403842091560364


Running epoch 1, step 1863, batch 815
Sampled inputs[:2]: tensor([[   0,  292,  960,  ...,  271, 1356,   14],
        [   0, 3406,  300,  ..., 1726, 3521, 4481]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.8013e-04,  3.3602e-05,  2.7271e-04,  ...,  4.6674e-04,
         -2.1468e-04,  2.1192e-04],
        [-1.1057e-05, -7.5512e-06,  8.3931e-06,  ..., -9.7752e-06,
         -7.2643e-06, -8.0802e-06],
        [ 1.8116e-04,  2.6706e-04, -1.4164e-04,  ...,  1.8934e-04,
          2.1453e-04,  1.0382e-04],
        [-2.1666e-05, -1.4536e-05,  1.7084e-05,  ..., -1.9178e-05,
         -1.4283e-05, -1.6414e-05],
        [-2.6256e-05, -1.9491e-05,  2.0579e-05,  ..., -2.3425e-05,
         -1.8403e-05, -1.7591e-05]], device='cuda:0')
Loss: 0.9278910756111145
Graident accumulation at epoch 1, step 1863, batch 815
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.6597e-04,  1.0138e-04, -4.6973e-06,  ..., -6.7739e-05,
         -8.9373e-05, -1.1608e-04],
        [-1.0915e-05, -7.6689e-06,  8.2317e-06,  ..., -9.6160e-06,
         -7.4585e-06, -7.7606e-06],
        [ 4.5825e-05,  6.2869e-05, -3.8473e-05,  ...,  4.3621e-05,
          5.2365e-05,  1.2919e-05],
        [-4.8222e-06,  1.7522e-06,  7.9225e-06,  ..., -5.6438e-06,
          6.1276e-07, -7.6953e-06],
        [-1.8415e-05, -1.2712e-05,  1.1307e-05,  ..., -1.4332e-05,
         -1.0230e-05, -1.0977e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1787e-08, 5.6105e-08, 5.5079e-08,  ..., 2.2096e-08, 1.2405e-07,
         5.1221e-08],
        [9.1145e-11, 5.9605e-11, 2.7726e-11,  ..., 6.3009e-11, 3.3199e-11,
         3.0833e-11],
        [2.8657e-09, 2.0603e-09, 1.3493e-09,  ..., 2.0549e-09, 1.3573e-09,
         7.4573e-10],
        [9.6032e-10, 1.1964e-09, 4.1998e-10,  ..., 9.4049e-10, 7.6724e-10,
         4.0433e-10],
        [4.3760e-10, 2.6032e-10, 1.1958e-10,  ..., 3.3074e-10, 1.1807e-10,
         1.4118e-10]], device='cuda:0')
optimizer state dict: 233.0
lr: [6.357167206333992e-07, 6.357167206333992e-07]
scheduler_last_epoch: 233


Running epoch 1, step 1864, batch 816
Sampled inputs[:2]: tensor([[    0,   870,   278,  ...,  1274, 10112,  3269],
        [    0,   275,  2351,  ...,    14,  4520,    12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.4253e-05,  8.0752e-05, -1.0272e-04,  ...,  7.1389e-05,
         -1.0963e-04, -3.5350e-05],
        [-1.3933e-06, -9.6858e-07,  1.0803e-06,  ..., -1.2517e-06,
         -9.2387e-07, -1.0431e-06],
        [-3.9339e-06, -2.8163e-06,  3.2485e-06,  ..., -3.4720e-06,
         -2.5928e-06, -2.8759e-06],
        [-2.6673e-06, -1.8105e-06,  2.1458e-06,  ..., -2.3991e-06,
         -1.7807e-06, -2.0713e-06],
        [-3.3081e-06, -2.4736e-06,  2.6524e-06,  ..., -2.9802e-06,
         -2.3395e-06, -2.2203e-06]], device='cuda:0')
Loss: 0.9606526494026184


Running epoch 1, step 1865, batch 817
Sampled inputs[:2]: tensor([[   0, 1234,  278,  ..., 1237, 1008,  417],
        [   0, 3227,  300,  ..., 1817, 5709,  300]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.9633e-05,  1.9011e-04, -1.0216e-04,  ...,  7.3576e-05,
         -1.6144e-04, -9.9340e-05],
        [-2.7120e-06, -1.8850e-06,  2.1011e-06,  ..., -2.4140e-06,
         -1.8254e-06, -1.9595e-06],
        [-7.6592e-06, -5.5283e-06,  6.3479e-06,  ..., -6.7055e-06,
         -5.1111e-06, -5.4240e-06],
        [-5.2303e-06, -3.5688e-06,  4.2170e-06,  ..., -4.6641e-06,
         -3.5465e-06, -3.9563e-06],
        [-6.3181e-06, -4.8131e-06,  5.0813e-06,  ..., -5.6624e-06,
         -4.5449e-06, -4.1053e-06]], device='cuda:0')
Loss: 0.9591058492660522


Running epoch 1, step 1866, batch 818
Sampled inputs[:2]: tensor([[    0,   360,  2374,  ...,   221,   474,   357],
        [    0, 24063,   717,  ...,  2228,  1416,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5631e-04,  9.9523e-05, -2.3368e-04,  ...,  2.6883e-05,
         -1.0354e-04, -1.3921e-04],
        [-4.0010e-06, -2.7418e-06,  3.1665e-06,  ..., -3.5837e-06,
         -2.7232e-06, -2.9728e-06],
        [-1.1414e-05, -8.0466e-06,  9.6411e-06,  ..., -9.9838e-06,
         -7.6443e-06, -8.2254e-06],
        [-7.7635e-06, -5.2080e-06,  6.4224e-06,  ..., -6.9290e-06,
         -5.2899e-06, -5.9828e-06],
        [-9.3728e-06, -6.9737e-06,  7.6890e-06,  ..., -8.3894e-06,
         -6.7502e-06, -6.1914e-06]], device='cuda:0')
Loss: 0.9511611461639404


Running epoch 1, step 1867, batch 819
Sampled inputs[:2]: tensor([[   0,  462, 9202,  ...,   15, 3256,  271],
        [   0,  221,  380,  ..., 1590,  997, 2239]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.9272e-05,  3.4048e-04, -1.6142e-04,  ...,  8.4029e-05,
         -2.1957e-04, -1.6374e-04],
        [-5.3495e-06, -3.6657e-06,  4.2990e-06,  ..., -4.7088e-06,
         -3.5651e-06, -3.8669e-06],
        [-1.5289e-05, -1.0803e-05,  1.3068e-05,  ..., -1.3202e-05,
         -1.0058e-05, -1.0759e-05],
        [-1.0461e-05, -7.0184e-06,  8.7917e-06,  ..., -9.1791e-06,
         -6.9588e-06, -7.8604e-06],
        [-1.2398e-05, -9.2387e-06,  1.0297e-05,  ..., -1.0952e-05,
         -8.7768e-06, -8.0094e-06]], device='cuda:0')
Loss: 0.9390902519226074


Running epoch 1, step 1868, batch 820
Sampled inputs[:2]: tensor([[   0,  792,  287,  ...,  706, 9751,  278],
        [   0, 3087,  401,  ..., 1875, 4122,  278]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0282e-04,  2.8930e-04, -1.1079e-04,  ...,  3.9377e-05,
         -8.2996e-05, -1.1822e-04],
        [-6.6906e-06, -4.7162e-06,  5.4240e-06,  ..., -5.8413e-06,
         -4.4741e-06, -4.7013e-06],
        [-1.9103e-05, -1.3888e-05,  1.6496e-05,  ..., -1.6421e-05,
         -1.2666e-05, -1.3158e-05],
        [-1.3158e-05, -9.0897e-06,  1.1176e-05,  ..., -1.1429e-05,
         -8.7619e-06, -9.6262e-06],
        [-1.5453e-05, -1.1832e-05,  1.2949e-05,  ..., -1.3590e-05,
         -1.1012e-05, -9.7677e-06]], device='cuda:0')
Loss: 0.9837228655815125


Running epoch 1, step 1869, batch 821
Sampled inputs[:2]: tensor([[    0,   401,  3740,  ...,  5980,   271,   266],
        [    0,    12,   287,  ...,    15, 35654,     9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.7799e-05,  3.2205e-04, -7.8512e-05,  ...,  2.8459e-05,
         -1.1613e-04, -1.9417e-04],
        [-8.0317e-06, -5.7146e-06,  6.5044e-06,  ..., -7.0110e-06,
         -5.3905e-06, -5.6773e-06],
        [-2.2978e-05, -1.6898e-05,  1.9833e-05,  ..., -1.9804e-05,
         -1.5348e-05, -1.5974e-05],
        [-1.5885e-05, -1.1101e-05,  1.3486e-05,  ..., -1.3813e-05,
         -1.0639e-05, -1.1712e-05],
        [-1.8582e-05, -1.4380e-05,  1.5557e-05,  ..., -1.6376e-05,
         -1.3322e-05, -1.1869e-05]], device='cuda:0')
Loss: 0.9482458829879761


Running epoch 1, step 1870, batch 822
Sampled inputs[:2]: tensor([[   0,  767, 1345,  ...,  276,  327,  328],
        [   0, 1099, 2851,  ...,  518,  496,  287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.1214e-04,  3.8857e-04,  9.2930e-05,  ..., -3.7433e-05,
         -1.2538e-04, -9.7011e-05],
        [-9.4101e-06, -6.7428e-06,  7.3873e-06,  ..., -8.2627e-06,
         -6.5006e-06, -6.7279e-06],
        [-2.6971e-05, -2.0042e-05,  2.2635e-05,  ..., -2.3425e-05,
         -1.8641e-05, -1.8984e-05],
        [-1.8552e-05, -1.3113e-05,  1.5251e-05,  ..., -1.6272e-05,
         -1.2890e-05, -1.3843e-05],
        [-2.1935e-05, -1.7136e-05,  1.7866e-05,  ..., -1.9476e-05,
         -1.6242e-05, -1.4208e-05]], device='cuda:0')
Loss: 0.9532942175865173


Running epoch 1, step 1871, batch 823
Sampled inputs[:2]: tensor([[    0,    14, 45192,  ..., 24171,   292,  3620],
        [    0,  5301,   792,  ..., 27135, 34090,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.5795e-06,  4.5288e-04,  2.6223e-04,  ..., -1.3449e-05,
         -1.1137e-05, -1.7486e-04],
        [-1.0811e-05, -7.8529e-06,  8.3707e-06,  ..., -9.5591e-06,
         -7.5586e-06, -7.7337e-06],
        [-3.0607e-05, -2.3216e-05,  2.5421e-05,  ..., -2.6867e-05,
         -2.1607e-05, -2.1577e-05],
        [-2.1085e-05, -1.5199e-05,  1.7121e-05,  ..., -1.8671e-05,
         -1.4916e-05, -1.5765e-05],
        [-2.5213e-05, -2.0087e-05,  2.0340e-05,  ..., -2.2620e-05,
         -1.9044e-05, -1.6399e-05]], device='cuda:0')
Loss: 0.9971820116043091
Graident accumulation at epoch 1, step 1871, batch 823
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 3.3023e-04,  1.3653e-04,  2.1996e-05,  ..., -6.2310e-05,
         -8.1549e-05, -1.2195e-04],
        [-1.0905e-05, -7.6873e-06,  8.2456e-06,  ..., -9.6103e-06,
         -7.4685e-06, -7.7579e-06],
        [ 3.8182e-05,  5.4260e-05, -3.2083e-05,  ...,  3.6572e-05,
          4.4968e-05,  9.4696e-06],
        [-6.4485e-06,  5.7037e-08,  8.8423e-06,  ..., -6.9466e-06,
         -9.4012e-07, -8.5023e-06],
        [-1.9095e-05, -1.3449e-05,  1.2210e-05,  ..., -1.5161e-05,
         -1.1111e-05, -1.1519e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1726e-08, 5.6254e-08, 5.5093e-08,  ..., 2.2074e-08, 1.2392e-07,
         5.1200e-08],
        [9.1171e-11, 5.9607e-11, 2.7769e-11,  ..., 6.3038e-11, 3.3223e-11,
         3.0862e-11],
        [2.8638e-09, 2.0587e-09, 1.3485e-09,  ..., 2.0536e-09, 1.3564e-09,
         7.4545e-10],
        [9.5980e-10, 1.1954e-09, 4.1986e-10,  ..., 9.3990e-10, 7.6669e-10,
         4.0417e-10],
        [4.3780e-10, 2.6046e-10, 1.1988e-10,  ..., 3.3092e-10, 1.1831e-10,
         1.4131e-10]], device='cuda:0')
optimizer state dict: 234.0
lr: [5.930613044608946e-07, 5.930613044608946e-07]
scheduler_last_epoch: 234


Running epoch 1, step 1872, batch 824
Sampled inputs[:2]: tensor([[    0,   380,  3584,  ..., 24402,  2057,     9],
        [    0,    13,   711,  ...,   591,   953,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.8651e-05,  9.7084e-05, -1.7986e-05,  ..., -9.7380e-06,
         -1.2368e-04, -1.4454e-04],
        [-1.2890e-06, -1.0356e-06,  1.0431e-06,  ..., -1.1399e-06,
         -9.9838e-07, -9.5367e-07],
        [-3.6955e-06, -3.0696e-06,  3.2187e-06,  ..., -3.2336e-06,
         -2.8610e-06, -2.6822e-06],
        [-2.5034e-06, -1.9968e-06,  2.1309e-06,  ..., -2.2203e-06,
         -1.9670e-06, -1.9372e-06],
        [-3.0398e-06, -2.6673e-06,  2.5928e-06,  ..., -2.7418e-06,
         -2.5332e-06, -2.0415e-06]], device='cuda:0')
Loss: 0.9422320127487183


Running epoch 1, step 1873, batch 825
Sampled inputs[:2]: tensor([[    0,    12,  4957,  ...,   944,   278,   609],
        [    0,   342, 22510,  ..., 49108,   278, 25904]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.3271e-05,  2.2670e-04, -1.7667e-04,  ..., -3.4193e-06,
         -2.9128e-04, -3.0473e-04],
        [-2.6226e-06, -1.8813e-06,  2.0638e-06,  ..., -2.3246e-06,
         -1.8887e-06, -1.9297e-06],
        [-7.4506e-06, -5.5432e-06,  6.2883e-06,  ..., -6.4820e-06,
         -5.3346e-06, -5.2899e-06],
        [-5.0813e-06, -3.5837e-06,  4.1723e-06,  ..., -4.5300e-06,
         -3.7104e-06, -3.8892e-06],
        [-6.1989e-06, -4.8429e-06,  5.1111e-06,  ..., -5.5283e-06,
         -4.7535e-06, -4.0531e-06]], device='cuda:0')
Loss: 0.9278546571731567


Running epoch 1, step 1874, batch 826
Sampled inputs[:2]: tensor([[   0, 6809,  344,  ...,   14, 1266,  795],
        [   0, 1231,  352,  ..., 8524,   14,  381]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.9236e-04,  3.7501e-04, -1.3156e-04,  ..., -8.1534e-05,
         -5.1864e-04, -2.1182e-04],
        [-4.0159e-06, -2.7865e-06,  3.1292e-06,  ..., -3.5390e-06,
         -2.7567e-06, -2.9653e-06],
        [-1.1414e-05, -8.2105e-06,  9.5814e-06,  ..., -9.8944e-06,
         -7.8231e-06, -8.1956e-06],
        [-7.7784e-06, -5.2825e-06,  6.3628e-06,  ..., -6.8694e-06,
         -5.3942e-06, -5.9754e-06],
        [-9.3877e-06, -7.0930e-06,  7.6890e-06,  ..., -8.3297e-06,
         -6.8992e-06, -6.1989e-06]], device='cuda:0')
Loss: 0.9174069166183472


Running epoch 1, step 1875, batch 827
Sampled inputs[:2]: tensor([[   0, 1615,  328,  ...,  266, 3133,  963],
        [   0,  266, 3634,  ...,  694,  266, 1784]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.8218e-05,  2.8041e-04, -3.7976e-05,  ..., -1.9806e-04,
         -4.2780e-04, -3.0794e-04],
        [-5.3570e-06, -3.7476e-06,  4.1723e-06,  ..., -4.7162e-06,
         -3.6284e-06, -3.9153e-06],
        [-1.5259e-05, -1.1057e-05,  1.2785e-05,  ..., -1.3262e-05,
         -1.0327e-05, -1.0937e-05],
        [-1.0461e-05, -7.1600e-06,  8.5384e-06,  ..., -9.2238e-06,
         -7.1079e-06, -7.9721e-06],
        [-1.2517e-05, -9.5069e-06,  1.0207e-05,  ..., -1.1131e-05,
         -9.0748e-06, -8.2552e-06]], device='cuda:0')
Loss: 0.9687795042991638


Running epoch 1, step 1876, batch 828
Sampled inputs[:2]: tensor([[   0,  965,  300,  ...,  546,  857, 4350],
        [   0, 1920,   19,  ..., 5232,  796, 1303]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7905e-05,  2.6832e-04, -9.0928e-05,  ..., -1.5104e-04,
         -4.4743e-04, -2.2710e-04],
        [-6.7353e-06, -4.7609e-06,  5.2452e-06,  ..., -5.9530e-06,
         -4.5747e-06, -4.9360e-06],
        [-1.9372e-05, -1.4171e-05,  1.6168e-05,  ..., -1.6913e-05,
         -1.3128e-05, -1.3947e-05],
        [-1.3173e-05, -9.1270e-06,  1.0714e-05,  ..., -1.1653e-05,
         -8.9556e-06, -1.0058e-05],
        [-1.5855e-05, -1.2115e-05,  1.2875e-05,  ..., -1.4141e-05,
         -1.1474e-05, -1.0505e-05]], device='cuda:0')
Loss: 0.9799109101295471


Running epoch 1, step 1877, batch 829
Sampled inputs[:2]: tensor([[    0, 41010,  6737,  ...,   963,   409,   382],
        [    0,   344, 14017,  ...,    65,   298,   634]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0545e-04,  2.8660e-04, -8.9780e-05,  ..., -3.7887e-04,
         -5.6994e-04, -2.3070e-04],
        [-8.0764e-06, -5.7220e-06,  6.4373e-06,  ..., -7.0781e-06,
         -5.3905e-06, -5.8487e-06],
        [-2.3112e-05, -1.6958e-05,  1.9699e-05,  ..., -2.0027e-05,
         -1.5408e-05, -1.6496e-05],
        [-1.5765e-05, -1.0960e-05,  1.3128e-05,  ..., -1.3813e-05,
         -1.0520e-05, -1.1921e-05],
        [-1.8895e-05, -1.4514e-05,  1.5661e-05,  ..., -1.6764e-05,
         -1.3515e-05, -1.2390e-05]], device='cuda:0')
Loss: 0.9701254367828369


Running epoch 1, step 1878, batch 830
Sampled inputs[:2]: tensor([[    0,   278, 10875,  ...,   445,   267,    14],
        [    0,    12,  3067,  ...,  1381,   278,  5011]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8789e-05,  1.6190e-04, -1.2517e-04,  ..., -4.0056e-04,
         -3.7725e-04, -1.4254e-04],
        [-9.3430e-06, -6.6683e-06,  7.4208e-06,  ..., -8.2254e-06,
         -6.3218e-06, -6.7912e-06],
        [-2.6703e-05, -1.9714e-05,  2.2709e-05,  ..., -2.3216e-05,
         -1.8016e-05, -1.9088e-05],
        [-1.8314e-05, -1.2822e-05,  1.5199e-05,  ..., -1.6138e-05,
         -1.2413e-05, -1.3918e-05],
        [-2.1890e-05, -1.6943e-05,  1.8105e-05,  ..., -1.9461e-05,
         -1.5825e-05, -1.4357e-05]], device='cuda:0')
Loss: 0.9693798422813416


Running epoch 1, step 1879, batch 831
Sampled inputs[:2]: tensor([[    0,   565,  5539,  ...,    12,   516, 14426],
        [    0, 49018,   292,  ...,  8774,   642,   365]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.5897e-05,  2.3492e-04, -5.8327e-05,  ..., -3.8926e-04,
         -4.2321e-04, -6.1839e-05],
        [-1.0639e-05, -7.7561e-06,  8.3894e-06,  ..., -9.4473e-06,
         -7.3873e-06, -7.7672e-06],
        [-3.0398e-05, -2.2918e-05,  2.5645e-05,  ..., -2.6673e-05,
         -2.1026e-05, -2.1905e-05],
        [-2.0832e-05, -1.4924e-05,  1.7151e-05,  ..., -1.8507e-05,
         -1.4469e-05, -1.5914e-05],
        [-2.5079e-05, -1.9819e-05,  2.0564e-05,  ..., -2.2501e-05,
         -1.8582e-05, -1.6592e-05]], device='cuda:0')
Loss: 0.9755234718322754
Graident accumulation at epoch 1, step 1879, batch 831
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.9362e-04,  1.4637e-04,  1.3963e-05,  ..., -9.5005e-05,
         -1.1572e-04, -1.1594e-04],
        [-1.0878e-05, -7.6942e-06,  8.2600e-06,  ..., -9.5940e-06,
         -7.4604e-06, -7.7589e-06],
        [ 3.1324e-05,  4.6542e-05, -2.6310e-05,  ...,  3.0248e-05,
          3.8368e-05,  6.3321e-06],
        [-7.8868e-06, -1.4410e-06,  9.6732e-06,  ..., -8.1026e-06,
         -2.2930e-06, -9.2435e-06],
        [-1.9693e-05, -1.4086e-05,  1.3045e-05,  ..., -1.5895e-05,
         -1.1858e-05, -1.2027e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1665e-08, 5.6253e-08, 5.5041e-08,  ..., 2.2204e-08, 1.2398e-07,
         5.1153e-08],
        [9.1193e-11, 5.9607e-11, 2.7811e-11,  ..., 6.3064e-11, 3.3244e-11,
         3.0891e-11],
        [2.8618e-09, 2.0572e-09, 1.3479e-09,  ..., 2.0523e-09, 1.3555e-09,
         7.4518e-10],
        [9.5927e-10, 1.1944e-09, 4.1973e-10,  ..., 9.3931e-10, 7.6614e-10,
         4.0402e-10],
        [4.3799e-10, 2.6059e-10, 1.2018e-10,  ..., 3.3109e-10, 1.1854e-10,
         1.4144e-10]], device='cuda:0')
optimizer state dict: 235.0
lr: [5.518433506354004e-07, 5.518433506354004e-07]
scheduler_last_epoch: 235


Running epoch 1, step 1880, batch 832
Sampled inputs[:2]: tensor([[    0,    14,   496,  ...,   368,   259,   490],
        [    0,  1197, 10640,  ...,  2405,   437,  5880]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4309e-04,  1.3343e-04,  8.9892e-05,  ..., -1.2156e-04,
         -1.0009e-04, -1.4909e-04],
        [-1.3188e-06, -1.0058e-06,  1.0282e-06,  ..., -1.2144e-06,
         -9.6112e-07, -9.7603e-07],
        [-3.8743e-06, -3.0845e-06,  3.2187e-06,  ..., -3.5167e-06,
         -2.8312e-06, -2.8312e-06],
        [-2.6673e-06, -2.0117e-06,  2.1607e-06,  ..., -2.4587e-06,
         -1.9670e-06, -2.0564e-06],
        [-3.2485e-06, -2.6822e-06,  2.6077e-06,  ..., -3.0249e-06,
         -2.5183e-06, -2.2054e-06]], device='cuda:0')
Loss: 0.9614201784133911


Running epoch 1, step 1881, batch 833
Sampled inputs[:2]: tensor([[   0,  292, 2860,  ...,  266, 7000, 7806],
        [   0, 3630, 2199,  ..., 4157,   27, 4765]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7420e-04,  5.6479e-05,  1.5509e-04,  ..., -3.7563e-05,
          3.1891e-05, -1.8512e-04],
        [-2.7344e-06, -2.0415e-06,  2.1383e-06,  ..., -2.4736e-06,
         -1.9595e-06, -1.9595e-06],
        [-7.8678e-06, -6.0946e-06,  6.5118e-06,  ..., -7.0482e-06,
         -5.6177e-06, -5.6326e-06],
        [-5.3942e-06, -3.9637e-06,  4.3660e-06,  ..., -4.8578e-06,
         -3.8445e-06, -4.0382e-06],
        [-6.6459e-06, -5.3793e-06,  5.3346e-06,  ..., -6.1095e-06,
         -5.0813e-06, -4.4405e-06]], device='cuda:0')
Loss: 0.9580485224723816


Running epoch 1, step 1882, batch 834
Sampled inputs[:2]: tensor([[    0,  7018,    14,  ...,  8288,    12,  1250],
        [    0,  3611, 10765,  ...,   271,  4317,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 8.2850e-05,  6.0524e-05,  1.5769e-04,  ..., -7.8081e-05,
          8.0817e-05, -5.6407e-05],
        [-4.0606e-06, -3.0622e-06,  3.2261e-06,  ..., -3.6955e-06,
         -2.9430e-06, -2.9728e-06],
        [-1.1638e-05, -9.0748e-06,  9.7901e-06,  ..., -1.0490e-05,
         -8.4192e-06, -8.4490e-06],
        [-7.9721e-06, -5.9307e-06,  6.5714e-06,  ..., -7.2271e-06,
         -5.7667e-06, -6.0797e-06],
        [-9.8050e-06, -8.0019e-06,  8.0466e-06,  ..., -9.0599e-06,
         -7.5996e-06, -6.6459e-06]], device='cuda:0')
Loss: 0.9486968517303467


Running epoch 1, step 1883, batch 835
Sampled inputs[:2]: tensor([[    0, 14161,  1241,  ..., 15255,   768,  4239],
        [    0,   496,    14,  ...,   266,   596,    13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9102e-05,  3.3083e-04,  2.1342e-04,  ..., -1.3099e-04,
          1.0077e-04, -4.0031e-05],
        [-5.6997e-06, -4.0531e-06,  4.2841e-06,  ..., -5.0738e-06,
         -3.9265e-06, -4.2468e-06],
        [-1.6227e-05, -1.2025e-05,  1.2949e-05,  ..., -1.4335e-05,
         -1.1250e-05, -1.1995e-05],
        [-1.0863e-05, -7.6666e-06,  8.4713e-06,  ..., -9.6709e-06,
         -7.5325e-06, -8.3745e-06],
        [-1.3977e-05, -1.0759e-05,  1.0878e-05,  ..., -1.2636e-05,
         -1.0312e-05, -9.7007e-06]], device='cuda:0')
Loss: 0.9191351532936096


Running epoch 1, step 1884, batch 836
Sampled inputs[:2]: tensor([[   0,  461,  654,  ..., 4145, 7600, 4142],
        [   0,  266, 4411,  ...,  368, 6388, 3484]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.7197e-04,  1.9108e-04,  2.0958e-04,  ..., -1.9898e-04,
         -2.1730e-06, -2.7352e-04],
        [-7.0408e-06, -4.9993e-06,  5.4389e-06,  ..., -6.2287e-06,
         -4.7684e-06, -5.2080e-06],
        [ 8.7377e-05,  1.3295e-04, -9.0553e-05,  ...,  9.8601e-05,
          1.3928e-04,  6.0449e-05],
        [-1.3471e-05, -9.4995e-06,  1.0841e-05,  ..., -1.1906e-05,
         -9.1717e-06, -1.0312e-05],
        [-1.7077e-05, -1.3173e-05,  1.3635e-05,  ..., -1.5363e-05,
         -1.2442e-05, -1.1727e-05]], device='cuda:0')
Loss: 0.9814004302024841


Running epoch 1, step 1885, batch 837
Sampled inputs[:2]: tensor([[    0,    80, 10802,  ...,   287, 28533, 25359],
        [    0,   287,  3284,  ...,   221,   493,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4377e-04,  3.4101e-07,  3.7083e-05,  ..., -1.6268e-04,
          4.3978e-05, -4.2780e-04],
        [-8.3447e-06, -5.9754e-06,  6.4000e-06,  ..., -7.4357e-06,
         -5.7295e-06, -6.1914e-06],
        [ 8.3413e-05,  1.2997e-04, -8.7409e-05,  ...,  9.5009e-05,
          1.3640e-04,  5.7513e-05],
        [-1.6123e-05, -1.1452e-05,  1.2867e-05,  ..., -1.4350e-05,
         -1.1124e-05, -1.2398e-05],
        [-2.0355e-05, -1.5706e-05,  1.6168e-05,  ..., -1.8373e-05,
         -1.4946e-05, -1.3947e-05]], device='cuda:0')
Loss: 0.9938701391220093


Running epoch 1, step 1886, batch 838
Sampled inputs[:2]: tensor([[   0,   12,  271,  ...,   12,  298,  273],
        [   0,  369, 4492,  ..., 9415, 4365,  352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.5840e-05,  1.8652e-05,  7.3607e-05,  ..., -9.2847e-05,
          1.3047e-04, -3.4717e-04],
        [-9.8422e-06, -6.9067e-06,  7.4357e-06,  ..., -8.6948e-06,
         -6.6571e-06, -7.3090e-06],
        [ 7.9211e-05,  1.2718e-04, -8.4235e-05,  ...,  9.1508e-05,
          1.3375e-04,  5.4429e-05],
        [-1.8954e-05, -1.3210e-05,  1.4924e-05,  ..., -1.6734e-05,
         -1.2904e-05, -1.4558e-05],
        [-2.3901e-05, -1.8135e-05,  1.8761e-05,  ..., -2.1398e-05,
         -1.7345e-05, -1.6376e-05]], device='cuda:0')
Loss: 0.9166224002838135


Running epoch 1, step 1887, batch 839
Sampled inputs[:2]: tensor([[    0,  4154, 14296,  ...,   516,  1796, 18233],
        [    0,    12,   638,  ...,   380,   560,   199]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 3.1850e-05,  1.5031e-04,  1.8267e-04,  ..., -1.1380e-04,
          2.1981e-04, -4.2242e-04],
        [-1.1176e-05, -7.8753e-06,  8.4192e-06,  ..., -9.8944e-06,
         -7.6704e-06, -8.3074e-06],
        [ 7.5426e-05,  1.2435e-04, -8.1180e-05,  ...,  8.8170e-05,
          1.3086e-04,  5.1672e-05],
        [-2.1562e-05, -1.5065e-05,  1.6935e-05,  ..., -1.9088e-05,
         -1.4931e-05, -1.6570e-05],
        [-2.7105e-05, -2.0623e-05,  2.1264e-05,  ..., -2.4274e-05,
         -1.9908e-05, -1.8537e-05]], device='cuda:0')
Loss: 0.9413558840751648
Graident accumulation at epoch 1, step 1887, batch 839
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0287, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.6744e-04,  1.4676e-04,  3.0833e-05,  ..., -9.6885e-05,
         -8.2163e-05, -1.4659e-04],
        [-1.0908e-05, -7.7123e-06,  8.2759e-06,  ..., -9.6240e-06,
         -7.4814e-06, -7.8137e-06],
        [ 3.5734e-05,  5.4323e-05, -3.1797e-05,  ...,  3.6040e-05,
          4.7617e-05,  1.0866e-05],
        [-9.2543e-06, -2.8034e-06,  1.0399e-05,  ..., -9.2012e-06,
         -3.5568e-06, -9.9762e-06],
        [-2.0434e-05, -1.4740e-05,  1.3867e-05,  ..., -1.6733e-05,
         -1.2663e-05, -1.2678e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1605e-08, 5.6219e-08, 5.5020e-08,  ..., 2.2195e-08, 1.2390e-07,
         5.1280e-08],
        [9.1227e-11, 5.9610e-11, 2.7854e-11,  ..., 6.3099e-11, 3.3270e-11,
         3.0929e-11],
        [2.8646e-09, 2.0706e-09, 1.3531e-09,  ..., 2.0580e-09, 1.3713e-09,
         7.4711e-10],
        [9.5878e-10, 1.1935e-09, 4.1960e-10,  ..., 9.3873e-10, 7.6559e-10,
         4.0389e-10],
        [4.3829e-10, 2.6076e-10, 1.2051e-10,  ..., 3.3135e-10, 1.1882e-10,
         1.4165e-10]], device='cuda:0')
optimizer state dict: 236.0
lr: [5.120691576200498e-07, 5.120691576200498e-07]
scheduler_last_epoch: 236


Running epoch 1, step 1888, batch 840
Sampled inputs[:2]: tensor([[    0,    22,  2577,  ...,  4970,     9,  3868],
        [    0,  3761,    12,  ...,  3476, 20966,   391]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7172e-04,  2.1659e-05,  2.1028e-04,  ...,  1.7519e-04,
          1.3314e-04,  1.8857e-04],
        [-1.2517e-06, -1.0431e-06,  8.6427e-07,  ..., -1.2219e-06,
         -1.0282e-06, -1.0133e-06],
        [-3.6806e-06, -3.1590e-06,  2.7418e-06,  ..., -3.5167e-06,
         -2.9653e-06, -2.8908e-06],
        [-2.5034e-06, -2.0564e-06,  1.7807e-06,  ..., -2.4587e-06,
         -2.1011e-06, -2.1458e-06],
        [-3.0398e-06, -2.6822e-06,  2.2203e-06,  ..., -2.9206e-06,
         -2.5332e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9773997068405151


Running epoch 1, step 1889, batch 841
Sampled inputs[:2]: tensor([[   0, 1682,  271,  ...,  367, 3210,  271],
        [   0, 4323, 2377,  ..., 3878, 4044,   14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.2612e-04,  4.9457e-05,  4.9631e-05,  ...,  1.7659e-04,
          1.4223e-04,  2.6474e-04],
        [-2.5705e-06, -2.0191e-06,  1.8924e-06,  ..., -2.4363e-06,
         -1.9670e-06, -1.9893e-06],
        [-7.5847e-06, -6.1840e-06,  5.9903e-06,  ..., -7.0781e-06,
         -5.7817e-06, -5.7518e-06],
        [-5.0664e-06, -3.9339e-06,  3.8669e-06,  ..., -4.8280e-06,
         -3.9414e-06, -4.1276e-06],
        [-6.1691e-06, -5.2303e-06,  4.7684e-06,  ..., -5.8413e-06,
         -4.9472e-06, -4.3064e-06]], device='cuda:0')
Loss: 0.958104133605957


Running epoch 1, step 1890, batch 842
Sampled inputs[:2]: tensor([[   0,   17,  292,  ..., 2269, 3887,  278],
        [   0, 1596, 2700,  ...,  943,  266, 4086]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5265e-04,  2.5618e-04, -1.1691e-05,  ...,  9.0947e-05,
         -1.5242e-04,  1.9820e-04],
        [-4.0308e-06, -2.9802e-06,  2.9355e-06,  ..., -3.7625e-06,
         -2.9132e-06, -3.0845e-06],
        [-1.1697e-05, -8.9854e-06,  9.1344e-06,  ..., -1.0729e-05,
         -8.4192e-06, -8.7619e-06],
        [-7.7933e-06, -5.6773e-06,  5.8785e-06,  ..., -7.3016e-06,
         -5.6997e-06, -6.2287e-06],
        [-9.8050e-06, -7.8082e-06,  7.4804e-06,  ..., -9.1493e-06,
         -7.4357e-06, -6.7949e-06]], device='cuda:0')
Loss: 0.9579547047615051


Running epoch 1, step 1891, batch 843
Sampled inputs[:2]: tensor([[   0,  266, 1513,  ...,  367, 1941,  344],
        [   0,  328,  490,  ..., 6280, 4283, 4582]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.1788e-04,  6.5589e-05, -2.3433e-04,  ...,  4.9616e-05,
          7.4378e-06,  2.5859e-04],
        [-5.4315e-06, -4.0457e-06,  3.9786e-06,  ..., -4.9844e-06,
         -3.9265e-06, -4.0531e-06],
        [-1.5721e-05, -1.2219e-05,  1.2323e-05,  ..., -1.4246e-05,
         -1.1384e-05, -1.1593e-05],
        [-1.0550e-05, -7.7635e-06,  8.0243e-06,  ..., -9.6858e-06,
         -7.6964e-06, -8.2552e-06],
        [-1.3143e-05, -1.0654e-05,  1.0073e-05,  ..., -1.2174e-05,
         -1.0103e-05, -8.9854e-06]], device='cuda:0')
Loss: 0.9783108234405518


Running epoch 1, step 1892, batch 844
Sampled inputs[:2]: tensor([[    0,    17,  3978,  ...,  3988,   598,    12],
        [    0,    14, 12285,  ...,   616,   515,   352]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0772e-04,  1.4567e-04, -2.9125e-04,  ...,  1.1942e-04,
         -1.7038e-04,  3.3218e-04],
        [-6.8471e-06, -4.9844e-06,  5.0366e-06,  ..., -6.2212e-06,
         -4.8950e-06, -5.1260e-06],
        [-1.9893e-05, -1.5140e-05,  1.5646e-05,  ..., -1.7866e-05,
         -1.4231e-05, -1.4767e-05],
        [-1.3351e-05, -9.6038e-06,  1.0185e-05,  ..., -1.2144e-05,
         -9.6336e-06, -1.0476e-05],
        [-1.6570e-05, -1.3173e-05,  1.2726e-05,  ..., -1.5229e-05,
         -1.2621e-05, -1.1429e-05]], device='cuda:0')
Loss: 0.9448469281196594


Running epoch 1, step 1893, batch 845
Sampled inputs[:2]: tensor([[    0,  2165,  9311,  ..., 10570,   437,   266],
        [    0,   578, 26976,  ...,  1389,    14,  1742]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.2255e-05,  3.4326e-04, -1.4426e-04,  ...,  1.7496e-04,
         -2.3212e-04,  3.0619e-04],
        [-8.2254e-06, -5.9679e-06,  5.9232e-06,  ..., -7.4878e-06,
         -5.9158e-06, -6.2063e-06],
        [-2.3797e-05, -1.8075e-05,  1.8403e-05,  ..., -2.1428e-05,
         -1.7196e-05, -1.7732e-05],
        [-1.6019e-05, -1.1474e-05,  1.1973e-05,  ..., -1.4633e-05,
         -1.1675e-05, -1.2606e-05],
        [-1.9997e-05, -1.5825e-05,  1.5065e-05,  ..., -1.8418e-05,
         -1.5378e-05, -1.3888e-05]], device='cuda:0')
Loss: 0.9130898714065552


Running epoch 1, step 1894, batch 846
Sampled inputs[:2]: tensor([[    0,  7382,  2252,  ..., 26084,   266,  5047],
        [    0,  1603,    27,  ..., 19959, 22776,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0523e-04,  3.8405e-04, -2.8236e-04,  ...,  1.6690e-04,
         -2.4457e-04,  5.2921e-04],
        [-9.6187e-06, -6.9886e-06,  6.9886e-06,  ..., -8.6799e-06,
         -6.9141e-06, -7.1600e-06],
        [ 1.1402e-04,  2.1311e-04, -6.4691e-05,  ...,  8.3902e-05,
          1.4081e-04,  2.6644e-05],
        [-1.8716e-05, -1.3441e-05,  1.4134e-05,  ..., -1.6943e-05,
         -1.3612e-05, -1.4558e-05],
        [-2.3246e-05, -1.8448e-05,  1.7658e-05,  ..., -2.1264e-05,
         -1.7896e-05, -1.5974e-05]], device='cuda:0')
Loss: 0.9580232501029968


Running epoch 1, step 1895, batch 847
Sampled inputs[:2]: tensor([[    0,   380,   981,  ...,   567,  5407,   472],
        [    0, 38136,    12,  ...,   367, 12851,  1040]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6346e-05,  3.5348e-04, -3.5224e-04,  ...,  1.3840e-04,
         -2.1520e-04,  6.5292e-04],
        [-1.1034e-05, -7.9796e-06,  8.1062e-06,  ..., -9.8199e-06,
         -7.7076e-06, -8.0951e-06],
        [ 1.0997e-04,  2.1016e-04, -6.1278e-05,  ...,  8.0639e-05,
          1.3850e-04,  2.3962e-05],
        [-2.1547e-05, -1.5393e-05,  1.6473e-05,  ..., -1.9222e-05,
         -1.5199e-05, -1.6510e-05],
        [-2.6375e-05, -2.0862e-05,  2.0236e-05,  ..., -2.3857e-05,
         -1.9848e-05, -1.7896e-05]], device='cuda:0')
Loss: 0.943038284778595
Graident accumulation at epoch 1, step 1895, batch 847
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 2.4233e-04,  1.6743e-04, -7.4740e-06,  ..., -7.3356e-05,
         -9.5467e-05, -6.6640e-05],
        [-1.0920e-05, -7.7390e-06,  8.2589e-06,  ..., -9.6436e-06,
         -7.5040e-06, -7.8419e-06],
        [ 4.3157e-05,  6.9907e-05, -3.4745e-05,  ...,  4.0500e-05,
          5.6706e-05,  1.2176e-05],
        [-1.0484e-05, -4.0624e-06,  1.1007e-05,  ..., -1.0203e-05,
         -4.7210e-06, -1.0630e-05],
        [-2.1028e-05, -1.5352e-05,  1.4504e-05,  ..., -1.7445e-05,
         -1.3382e-05, -1.3200e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1543e-08, 5.6288e-08, 5.5089e-08,  ..., 2.2192e-08, 1.2383e-07,
         5.1655e-08],
        [9.1257e-11, 5.9614e-11, 2.7892e-11,  ..., 6.3132e-11, 3.3296e-11,
         3.0964e-11],
        [2.8739e-09, 2.1127e-09, 1.3555e-09,  ..., 2.0624e-09, 1.3891e-09,
         7.4693e-10],
        [9.5829e-10, 1.1925e-09, 4.1945e-10,  ..., 9.3816e-10, 7.6506e-10,
         4.0376e-10],
        [4.3855e-10, 2.6093e-10, 1.2080e-10,  ..., 3.3159e-10, 1.1909e-10,
         1.4182e-10]], device='cuda:0')
optimizer state dict: 237.0
lr: [4.737448032587366e-07, 4.737448032587366e-07]
scheduler_last_epoch: 237


Running epoch 1, step 1896, batch 848
Sampled inputs[:2]: tensor([[    0, 18774,  4916,  ..., 35093,    19,    50],
        [    0,   292, 23950,  ...,  9305,   287,  4401]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-8.2740e-05,  8.2314e-05,  1.6701e-05,  ..., -2.4134e-06,
          1.0710e-04,  4.3664e-05],
        [-1.3635e-06, -1.1101e-06,  1.0356e-06,  ..., -1.2144e-06,
         -1.0058e-06, -9.8348e-07],
        [-3.8445e-06, -3.2932e-06,  3.1143e-06,  ..., -3.4422e-06,
         -2.8759e-06, -2.7716e-06],
        [-2.7269e-06, -2.2203e-06,  2.1607e-06,  ..., -2.4438e-06,
         -2.0415e-06, -2.0713e-06],
        [-3.2336e-06, -2.9057e-06,  2.5779e-06,  ..., -2.9653e-06,
         -2.5928e-06, -2.1756e-06]], device='cuda:0')
Loss: 0.9815125465393066


Running epoch 1, step 1897, batch 849
Sampled inputs[:2]: tensor([[    0,    14, 22157,  ...,  2341,   508, 22960],
        [    0, 49831,    12,  ...,   912,   221,   609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6064e-04, -2.1899e-04, -2.9744e-04,  ...,  1.5784e-04,
          3.6139e-04, -1.0445e-05],
        [-2.7493e-06, -1.9297e-06,  2.2128e-06,  ..., -2.4438e-06,
         -1.9372e-06, -2.0489e-06],
        [-7.7486e-06, -5.6922e-06,  6.6310e-06,  ..., -6.7800e-06,
         -5.4240e-06, -5.6475e-06],
        [-5.4091e-06, -3.7774e-06,  4.5449e-06,  ..., -4.7982e-06,
         -3.8296e-06, -4.1872e-06],
        [-6.5118e-06, -5.0068e-06,  5.4240e-06,  ..., -5.8264e-06,
         -4.8876e-06, -4.3958e-06]], device='cuda:0')
Loss: 0.9726924896240234


Running epoch 1, step 1898, batch 850
Sampled inputs[:2]: tensor([[    0,  1445,  3597,  ...,   281,    78,     9],
        [    0,   287,   266,  ..., 10238,    12, 39004]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.8188e-04, -1.1319e-04, -1.3678e-04,  ...,  1.5033e-04,
          2.1066e-04, -3.8327e-05],
        [-4.1202e-06, -2.8908e-06,  3.3379e-06,  ..., -3.6135e-06,
         -2.7977e-06, -3.0324e-06],
        [-1.1772e-05, -8.6278e-06,  1.0133e-05,  ..., -1.0177e-05,
         -7.9423e-06, -8.5086e-06],
        [-8.1658e-06, -5.6773e-06,  6.8992e-06,  ..., -7.1377e-06,
         -5.5432e-06, -6.2585e-06],
        [-9.6411e-06, -7.4059e-06,  8.0615e-06,  ..., -8.5384e-06,
         -7.0035e-06, -6.4373e-06]], device='cuda:0')
Loss: 0.941058874130249


Running epoch 1, step 1899, batch 851
Sampled inputs[:2]: tensor([[    0,  5143,  3877,  ...,   292, 44003,    12],
        [    0,   271,  4219,  ...,   644,    14,  3607]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.5182e-04, -1.8024e-04, -1.3678e-04,  ...,  3.5992e-05,
          3.1168e-04,  4.6834e-05],
        [-5.4464e-06, -3.8110e-06,  4.3809e-06,  ..., -4.7833e-06,
         -3.7327e-06, -4.0010e-06],
        [-1.5512e-05, -1.1295e-05,  1.3307e-05,  ..., -1.3396e-05,
         -1.0565e-05, -1.1131e-05],
        [-1.0714e-05, -7.3910e-06,  9.0301e-06,  ..., -9.3430e-06,
         -7.3314e-06, -8.1658e-06],
        [-1.2785e-05, -9.7603e-06,  1.0654e-05,  ..., -1.1295e-05,
         -9.3430e-06, -8.4490e-06]], device='cuda:0')
Loss: 0.9378188848495483


Running epoch 1, step 1900, batch 852
Sampled inputs[:2]: tensor([[   0, 3767, 2337,  ...,  950,  847,  300],
        [   0,   14, 8047,  ..., 3813,    9, 8237]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.9046e-04, -7.7003e-05,  1.8348e-05,  ..., -2.6553e-06,
          5.3579e-04,  1.6988e-04],
        [-6.8322e-06, -4.8466e-06,  5.2527e-06,  ..., -6.0499e-06,
         -4.8280e-06, -5.0664e-06],
        [-1.9535e-05, -1.4439e-05,  1.6049e-05,  ..., -1.7047e-05,
         -1.3739e-05, -1.4246e-05],
        [-1.3426e-05, -9.3877e-06,  1.0803e-05,  ..., -1.1846e-05,
         -9.5069e-06, -1.0341e-05],
        [-1.6302e-05, -1.2606e-05,  1.2964e-05,  ..., -1.4558e-05,
         -1.2279e-05, -1.0997e-05]], device='cuda:0')
Loss: 0.9593746066093445


Running epoch 1, step 1901, batch 853
Sampled inputs[:2]: tensor([[   0, 2615,   13,  ...,  940, 3661, 6837],
        [   0,  300,  266,  ...,   13, 2920,  609]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-6.0845e-04, -1.1178e-04, -7.0230e-05,  ..., -8.1095e-05,
          4.0188e-04,  1.1222e-04],
        [-8.1286e-06, -5.6699e-06,  6.4299e-06,  ..., -7.1749e-06,
         -5.5656e-06, -6.0052e-06],
        [-2.3335e-05, -1.6913e-05,  1.9655e-05,  ..., -2.0295e-05,
         -1.5870e-05, -1.6928e-05],
        [-1.6063e-05, -1.1012e-05,  1.3292e-05,  ..., -1.4111e-05,
         -1.0967e-05, -1.2308e-05],
        [-1.9148e-05, -1.4573e-05,  1.5602e-05,  ..., -1.7047e-05,
         -1.4015e-05, -1.2845e-05]], device='cuda:0')
Loss: 0.9466126561164856


Running epoch 1, step 1902, batch 854
Sampled inputs[:2]: tensor([[    0,   365,  1410,  ...,    12,  1478, 16062],
        [    0,     9,   298,  ...,    12, 24079,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-5.4110e-04, -1.8972e-04, -2.5049e-04,  ..., -2.2116e-05,
          4.8288e-04,  4.3605e-04],
        [-9.5293e-06, -6.5453e-06,  7.3723e-06,  ..., -8.4564e-06,
         -6.5155e-06, -7.1377e-06],
        [-2.7329e-05, -1.9535e-05,  2.2575e-05,  ..., -2.3857e-05,
         -1.8537e-05, -2.0057e-05],
        [-1.8790e-05, -1.2688e-05,  1.5192e-05,  ..., -1.6630e-05,
         -1.2837e-05, -1.4603e-05],
        [-2.2635e-05, -1.6958e-05,  1.8075e-05,  ..., -2.0221e-05,
         -1.6503e-05, -1.5393e-05]], device='cuda:0')
Loss: 0.9316977858543396


Running epoch 1, step 1903, batch 855
Sampled inputs[:2]: tensor([[    0,    12,   546,  ..., 24994, 31107,   266],
        [    0,  2715, 10929,  ...,  4978,   287,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4804e-04, -8.5031e-05, -1.8174e-04,  ...,  2.1260e-05,
          3.8607e-04,  4.2465e-04],
        [-1.0841e-05, -7.5065e-06,  8.3558e-06,  ..., -9.5963e-06,
         -7.5437e-06, -8.1584e-06],
        [-3.0965e-05, -2.2292e-05,  2.5555e-05,  ..., -2.6986e-05,
         -2.1353e-05, -2.2784e-05],
        [-2.1383e-05, -1.4536e-05,  1.7233e-05,  ..., -1.8880e-05,
         -1.4894e-05, -1.6704e-05],
        [-2.5660e-05, -1.9386e-05,  2.0444e-05,  ..., -2.2888e-05,
         -1.9051e-05, -1.7494e-05]], device='cuda:0')
Loss: 0.9282026886940002
Graident accumulation at epoch 1, step 1903, batch 855
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.7330e-04,  1.4219e-04, -2.4901e-05,  ..., -6.3895e-05,
         -4.7313e-05, -1.7511e-05],
        [-1.0912e-05, -7.7158e-06,  8.2686e-06,  ..., -9.6389e-06,
         -7.5080e-06, -7.8735e-06],
        [ 3.5745e-05,  6.0687e-05, -2.8715e-05,  ...,  3.3751e-05,
          4.8900e-05,  8.6797e-06],
        [-1.1574e-05, -5.1097e-06,  1.1629e-05,  ..., -1.1071e-05,
         -5.7383e-06, -1.1237e-05],
        [-2.1492e-05, -1.5755e-05,  1.5098e-05,  ..., -1.7990e-05,
         -1.3949e-05, -1.3629e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1683e-08, 5.6239e-08, 5.5067e-08,  ..., 2.2170e-08, 1.2385e-07,
         5.1784e-08],
        [9.1283e-11, 5.9611e-11, 2.7934e-11,  ..., 6.3161e-11, 3.3319e-11,
         3.0999e-11],
        [2.8720e-09, 2.1111e-09, 1.3548e-09,  ..., 2.0611e-09, 1.3882e-09,
         7.4671e-10],
        [9.5778e-10, 1.1915e-09, 4.1933e-10,  ..., 9.3758e-10, 7.6452e-10,
         4.0363e-10],
        [4.3877e-10, 2.6104e-10, 1.2110e-10,  ..., 3.3178e-10, 1.1934e-10,
         1.4199e-10]], device='cuda:0')
optimizer state dict: 238.0
lr: [4.36876143847339e-07, 4.36876143847339e-07]
scheduler_last_epoch: 238


Running epoch 1, step 1904, batch 856
Sampled inputs[:2]: tensor([[    0,  8538,    13,  ...,  3825, 33705,  2442],
        [    0,    12,   287,  ...,  2381, 12046,  2231]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-7.6375e-05,  9.4741e-05,  6.5453e-05,  ..., -1.1362e-04,
         -3.6268e-05,  1.2850e-04],
        [-1.3709e-06, -9.6112e-07,  9.6858e-07,  ..., -1.2442e-06,
         -1.0058e-06, -1.1027e-06],
        [-3.9041e-06, -2.9355e-06,  2.9355e-06,  ..., -3.5316e-06,
         -2.9057e-06, -3.1292e-06],
        [-2.6375e-06, -1.8477e-06,  1.9222e-06,  ..., -2.4438e-06,
         -1.9819e-06, -2.2054e-06],
        [-3.3528e-06, -2.6226e-06,  2.4587e-06,  ..., -3.1143e-06,
         -2.6673e-06, -2.5332e-06]], device='cuda:0')
Loss: 0.9340648651123047


Running epoch 1, step 1905, batch 857
Sampled inputs[:2]: tensor([[    0, 23070,   367,  ...,   287,   790,  3252],
        [    0,  4602,  2387,  ..., 11616,    14, 18434]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.8082e-04,  7.9369e-05, -2.7032e-05,  ...,  1.1298e-06,
         -9.0166e-05,  1.4870e-04],
        [-2.6748e-06, -1.8142e-06,  2.0489e-06,  ..., -2.3991e-06,
         -1.8068e-06, -2.1383e-06],
        [-7.7039e-06, -5.3942e-06,  6.2734e-06,  ..., -6.7651e-06,
         -5.2005e-06, -5.9903e-06],
        [-5.2303e-06, -3.4794e-06,  4.1574e-06,  ..., -4.7088e-06,
         -3.5614e-06, -4.3064e-06],
        [-6.3628e-06, -4.6790e-06,  5.0217e-06,  ..., -5.7369e-06,
         -4.6194e-06, -4.6045e-06]], device='cuda:0')
Loss: 0.961908221244812


Running epoch 1, step 1906, batch 858
Sampled inputs[:2]: tensor([[   0,  897,  328,  ...,  908,  696,  688],
        [   0,  287, 1410,  ..., 1255, 1699,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9763e-04,  1.1842e-04, -8.5157e-05,  ...,  3.4038e-05,
         -1.8966e-04,  6.1894e-06],
        [-4.0829e-06, -2.7977e-06,  3.1441e-06,  ..., -3.6061e-06,
         -2.7902e-06, -3.1516e-06],
        [-1.1697e-05, -8.2850e-06,  9.5665e-06,  ..., -1.0148e-05,
         -7.9572e-06, -8.8215e-06],
        [-8.0764e-06, -5.4017e-06,  6.4522e-06,  ..., -7.1228e-06,
         -5.5283e-06, -6.4373e-06],
        [-9.7305e-06, -7.2420e-06,  7.7188e-06,  ..., -8.6576e-06,
         -7.0930e-06, -6.8098e-06]], device='cuda:0')
Loss: 0.9376608729362488


Running epoch 1, step 1907, batch 859
Sampled inputs[:2]: tensor([[    0,   474,   706,  ...,    83, 38084,   475],
        [    0,  6328,    12,  ...,   417,   199,  1726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.7369e-04,  1.9138e-04, -1.3376e-04,  ..., -3.4364e-05,
          5.0088e-05,  1.1993e-04],
        [-5.4836e-06, -3.6433e-06,  4.0568e-06,  ..., -4.9099e-06,
         -3.7514e-06, -4.3139e-06],
        [-1.5393e-05, -1.0684e-05,  1.2279e-05,  ..., -1.3500e-05,
         -1.0505e-05, -1.1772e-05],
        [-1.0684e-05, -6.9812e-06,  8.2627e-06,  ..., -9.5516e-06,
         -7.3537e-06, -8.6427e-06],
        [-1.3173e-05, -9.4771e-06,  1.0103e-05,  ..., -1.1817e-05,
         -9.5963e-06, -9.3579e-06]], device='cuda:0')
Loss: 0.91989666223526


Running epoch 1, step 1908, batch 860
Sampled inputs[:2]: tensor([[    0,   594,    84,  ..., 24411, 14140, 12720],
        [    0,   756,    12,  ..., 29374,    12,  2726]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.5794e-04,  1.1893e-04, -3.7081e-04,  ..., -9.5093e-05,
          1.6287e-04,  1.8404e-04],
        [-6.8098e-06, -4.6492e-06,  4.9509e-06,  ..., -6.1095e-06,
         -4.7944e-06, -5.2974e-06],
        [-1.9237e-05, -1.3709e-05,  1.5095e-05,  ..., -1.6943e-05,
         -1.3500e-05, -1.4603e-05],
        [-1.3262e-05, -8.9183e-06,  1.0081e-05,  ..., -1.1891e-05,
         -9.3952e-06, -1.0654e-05],
        [-1.6436e-05, -1.2159e-05,  1.2442e-05,  ..., -1.4812e-05,
         -1.2308e-05, -1.1578e-05]], device='cuda:0')
Loss: 0.9558380246162415


Running epoch 1, step 1909, batch 861
Sampled inputs[:2]: tensor([[    0,  1196,  3570,  ...,   722, 15816,   287],
        [    0, 24086,   266,  ..., 18814,    19,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.4776e-04,  1.3094e-05, -2.6997e-04,  ..., -5.6012e-05,
          2.9524e-04,  1.3422e-04],
        [-8.1658e-06, -5.6550e-06,  6.0312e-06,  ..., -7.3016e-06,
         -5.7779e-06, -6.3181e-06],
        [-2.3350e-05, -1.6883e-05,  1.8507e-05,  ..., -2.0549e-05,
         -1.6496e-05, -1.7732e-05],
        [-1.6004e-05, -1.0915e-05,  1.2316e-05,  ..., -1.4305e-05,
         -1.1392e-05, -1.2815e-05],
        [-1.9759e-05, -1.4856e-05,  1.5154e-05,  ..., -1.7807e-05,
         -1.4901e-05, -1.3947e-05]], device='cuda:0')
Loss: 0.9784576296806335


Running epoch 1, step 1910, batch 862
Sampled inputs[:2]: tensor([[    0,  1340,   800,  ...,   259, 13583,   422],
        [    0,    12,   496,  ..., 11354,  4856,  1109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.0004e-04,  2.6578e-05, -1.6878e-04,  ..., -1.9718e-05,
          1.6197e-04, -6.4191e-05],
        [-9.4548e-06, -6.7130e-06,  7.1414e-06,  ..., -8.4490e-06,
         -6.7614e-06, -7.3090e-06],
        [ 1.4701e-04,  2.0800e-04, -1.1663e-04,  ...,  1.9529e-04,
          1.1328e-04,  1.8981e-04],
        [-1.8641e-05, -1.3061e-05,  1.4685e-05,  ..., -1.6674e-05,
         -1.3404e-05, -1.4946e-05],
        [-2.2903e-05, -1.7613e-05,  1.7941e-05,  ..., -2.0653e-05,
         -1.7419e-05, -1.6183e-05]], device='cuda:0')
Loss: 0.9716377854347229


Running epoch 1, step 1911, batch 863
Sampled inputs[:2]: tensor([[    0, 20596,  2943,  ...,  5560,  2512,   266],
        [    0,   504,   546,  ...,   634,   328,   630]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1702e-04,  2.6384e-04, -1.3219e-04,  ..., -8.0355e-05,
         -6.5914e-05, -1.2470e-04],
        [-1.0774e-05, -7.7039e-06,  8.2664e-06,  ..., -9.6187e-06,
         -7.6890e-06, -8.2552e-06],
        [ 1.4324e-04,  2.0506e-04, -1.1321e-04,  ...,  1.9197e-04,
          1.1064e-04,  1.8709e-04],
        [-2.1279e-05, -1.4998e-05,  1.7039e-05,  ..., -1.8984e-05,
         -1.5244e-05, -1.6943e-05],
        [-2.5973e-05, -2.0146e-05,  2.0623e-05,  ..., -2.3425e-05,
         -1.9729e-05, -1.8224e-05]], device='cuda:0')
Loss: 0.9681476354598999
Graident accumulation at epoch 1, step 1911, batch 863
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1426e-04,  1.5435e-04, -3.5629e-05,  ..., -6.5541e-05,
         -4.9174e-05, -2.8230e-05],
        [-1.0899e-05, -7.7146e-06,  8.2684e-06,  ..., -9.6369e-06,
         -7.5261e-06, -7.9117e-06],
        [ 4.6494e-05,  7.5124e-05, -3.7165e-05,  ...,  4.9573e-05,
          5.5074e-05,  2.6520e-05],
        [-1.2544e-05, -6.0986e-06,  1.2170e-05,  ..., -1.1862e-05,
         -6.6889e-06, -1.1808e-05],
        [-2.1940e-05, -1.6195e-05,  1.5651e-05,  ..., -1.8533e-05,
         -1.4527e-05, -1.4088e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1795e-08, 5.6252e-08, 5.5029e-08,  ..., 2.2154e-08, 1.2373e-07,
         5.1748e-08],
        [9.1308e-11, 5.9610e-11, 2.7975e-11,  ..., 6.3190e-11, 3.3345e-11,
         3.1036e-11],
        [2.8896e-09, 2.1510e-09, 1.3663e-09,  ..., 2.0959e-09, 1.3990e-09,
         7.8096e-10],
        [9.5728e-10, 1.1905e-09, 4.1920e-10,  ..., 9.3700e-10, 7.6398e-10,
         4.0352e-10],
        [4.3900e-10, 2.6119e-10, 1.2140e-10,  ..., 3.3200e-10, 1.1961e-10,
         1.4218e-10]], device='cuda:0')
optimizer state dict: 239.0
lr: [4.014688132388511e-07, 4.014688132388511e-07]
scheduler_last_epoch: 239


Running epoch 1, step 1912, batch 864
Sampled inputs[:2]: tensor([[    0,   437,  1690,  ...,  1274, 10695, 10762],
        [    0,   560,   199,  ...,  6408,   278,  1119]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.1051e-04,  1.5602e-04, -6.5081e-05,  ...,  1.8035e-04,
         -4.6102e-04, -2.6673e-04],
        [-1.3784e-06, -9.9093e-07,  1.1474e-06,  ..., -1.1995e-06,
         -9.7603e-07, -9.6858e-07],
        [-4.0829e-06, -3.0398e-06,  3.5614e-06,  ..., -3.5316e-06,
         -2.8908e-06, -2.8610e-06],
        [-2.7120e-06, -1.8999e-06,  2.3246e-06,  ..., -2.3395e-06,
         -1.8999e-06, -1.9819e-06],
        [-3.3379e-06, -2.6077e-06,  2.8312e-06,  ..., -2.9504e-06,
         -2.5183e-06, -2.1905e-06]], device='cuda:0')
Loss: 0.9941458702087402


Running epoch 1, step 1913, batch 865
Sampled inputs[:2]: tensor([[    0,  1295,  1178,  ...,  4808,   287,   996],
        [    0,   401,  3408,  ...,   287, 19892,   328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8220e-04,  1.2730e-04, -3.6667e-04,  ...,  1.2053e-04,
         -4.8040e-04, -4.0296e-04],
        [-2.7195e-06, -1.9893e-06,  2.2948e-06,  ..., -2.3469e-06,
         -1.8813e-06, -1.8701e-06],
        [-7.9870e-06, -6.0052e-06,  7.0930e-06,  ..., -6.8545e-06,
         -5.5283e-06, -5.5134e-06],
        [-5.3942e-06, -3.8669e-06,  4.7237e-06,  ..., -4.6343e-06,
         -3.6955e-06, -3.8743e-06],
        [-6.4224e-06, -5.0962e-06,  5.5581e-06,  ..., -5.6475e-06,
         -4.7833e-06, -4.1276e-06]], device='cuda:0')
Loss: 0.9757798910140991


Running epoch 1, step 1914, batch 866
Sampled inputs[:2]: tensor([[   0, 1304, 1040,  ...,  287, 1665,  741],
        [   0, 6803, 6298,  ...,  490, 1781,   12]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.9452e-05,  2.2570e-04, -2.4561e-04,  ...,  1.1777e-04,
         -4.6187e-04, -3.6593e-04],
        [-4.0382e-06, -3.1367e-06,  3.3453e-06,  ..., -3.5912e-06,
         -2.9393e-06, -2.8461e-06],
        [-1.1668e-05, -9.3877e-06,  1.0237e-05,  ..., -1.0356e-05,
         -8.5682e-06, -8.2701e-06],
        [-7.8976e-06, -6.0424e-06,  6.8247e-06,  ..., -7.0035e-06,
         -5.7369e-06, -5.8264e-06],
        [-9.5516e-06, -8.0913e-06,  8.1658e-06,  ..., -8.6874e-06,
         -7.5251e-06, -6.2883e-06]], device='cuda:0')
Loss: 0.9667368531227112


Running epoch 1, step 1915, batch 867
Sampled inputs[:2]: tensor([[    0,   391,  1866,  ...,  3711, 21119, 29613],
        [    0,  3398,   271,  ...,    13,  1581, 13600]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.7264e-05,  1.2976e-04, -5.4630e-04,  ..., -1.5481e-05,
         -1.9467e-04, -1.1692e-04],
        [-5.4315e-06, -4.0457e-06,  4.3288e-06,  ..., -4.8429e-06,
         -3.9376e-06, -3.9637e-06],
        [-1.5542e-05, -1.2070e-05,  1.3202e-05,  ..., -1.3769e-05,
         -1.1295e-05, -1.1250e-05],
        [-1.0639e-05, -7.7859e-06,  8.8215e-06,  ..., -9.4473e-06,
         -7.7039e-06, -8.0913e-06],
        [-1.2964e-05, -1.0505e-05,  1.0699e-05,  ..., -1.1727e-05,
         -1.0043e-05, -8.7172e-06]], device='cuda:0')
Loss: 0.9639659523963928


Running epoch 1, step 1916, batch 868
Sampled inputs[:2]: tensor([[    0,  1549,   824,  ...,  3609,   720,   417],
        [    0,  5635,   328,  ...,   287, 27260,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.6272e-05,  1.2719e-04, -6.6625e-04,  ..., -7.7771e-05,
         -3.4779e-04, -1.0475e-04],
        [-6.7726e-06, -5.0515e-06,  5.3793e-06,  ..., -5.9977e-06,
         -4.9360e-06, -4.9770e-06],
        [-1.9446e-05, -1.5140e-05,  1.6496e-05,  ..., -1.7136e-05,
         -1.4231e-05, -1.4231e-05],
        [-1.3366e-05, -9.8124e-06,  1.1057e-05,  ..., -1.1802e-05,
         -9.7603e-06, -1.0267e-05],
        [-1.6123e-05, -1.3113e-05,  1.3292e-05,  ..., -1.4514e-05,
         -1.2577e-05, -1.0937e-05]], device='cuda:0')
Loss: 0.9661210775375366


Running epoch 1, step 1917, batch 869
Sampled inputs[:2]: tensor([[   0,   12,  358,  ...,  352,  266,  319],
        [   0, 1304,  292,  ..., 2101,  292,  474]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2092e-04,  2.0966e-04, -6.7712e-04,  ..., -6.1308e-05,
         -3.6021e-04,  1.4974e-05],
        [-8.1062e-06, -6.0648e-06,  6.3553e-06,  ..., -7.2569e-06,
         -6.0014e-06, -6.0275e-06],
        [-2.3350e-05, -1.8269e-05,  1.9595e-05,  ..., -2.0772e-05,
         -1.7345e-05, -1.7226e-05],
        [-1.6019e-05, -1.1809e-05,  1.3068e-05,  ..., -1.4320e-05,
         -1.1921e-05, -1.2442e-05],
        [-1.9327e-05, -1.5780e-05,  1.5765e-05,  ..., -1.7554e-05,
         -1.5289e-05, -1.3217e-05]], device='cuda:0')
Loss: 0.9491229057312012


Running epoch 1, step 1918, batch 870
Sampled inputs[:2]: tensor([[    0,   508,   927,  ...,  1390,   674,   369],
        [    0,  7030,   631,  ..., 34748,    12,   298]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.2092e-04,  2.4101e-04, -5.3750e-04,  ..., -4.7942e-05,
         -4.2274e-04, -3.2446e-05],
        [-9.3952e-06, -6.9737e-06,  7.4953e-06,  ..., -8.3223e-06,
         -6.8173e-06, -6.9588e-06],
        [-2.6971e-05, -2.0877e-05,  2.3037e-05,  ..., -2.3708e-05,
         -1.9640e-05, -1.9744e-05],
        [-1.8582e-05, -1.3560e-05,  1.5467e-05,  ..., -1.6421e-05,
         -1.3545e-05, -1.4350e-05],
        [-2.2233e-05, -1.8001e-05,  1.8433e-05,  ..., -1.9968e-05,
         -1.7285e-05, -1.5058e-05]], device='cuda:0')
Loss: 0.9195346832275391


Running epoch 1, step 1919, batch 871
Sampled inputs[:2]: tensor([[    0,    26,  4044,  ...,  9531,   365,   993],
        [    0,   287,  2926,  ...,   266, 40854,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6634e-04,  3.2452e-04, -4.5192e-04,  ..., -1.0318e-04,
         -3.9789e-04, -3.2446e-05],
        [-1.0736e-05, -8.0168e-06,  8.5533e-06,  ..., -9.5442e-06,
         -7.8529e-06, -7.9423e-06],
        [-3.0786e-05, -2.3931e-05,  2.6241e-05,  ..., -2.7150e-05,
         -2.2516e-05, -2.2516e-05],
        [-2.1309e-05, -1.5616e-05,  1.7703e-05,  ..., -1.8895e-05,
         -1.5631e-05, -1.6451e-05],
        [-2.5481e-05, -2.0728e-05,  2.1055e-05,  ..., -2.2978e-05,
         -1.9923e-05, -1.7263e-05]], device='cuda:0')
Loss: 0.9765226244926453
Graident accumulation at epoch 1, step 1919, batch 871
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.1947e-04,  1.7137e-04, -7.7258e-05,  ..., -6.9305e-05,
         -8.4045e-05, -2.8652e-05],
        [-1.0882e-05, -7.7448e-06,  8.2969e-06,  ..., -9.6276e-06,
         -7.5588e-06, -7.9148e-06],
        [ 3.8766e-05,  6.5219e-05, -3.0825e-05,  ...,  4.1900e-05,
          4.7315e-05,  2.1617e-05],
        [-1.3421e-05, -7.0504e-06,  1.2724e-05,  ..., -1.2566e-05,
         -7.5831e-06, -1.2272e-05],
        [-2.2294e-05, -1.6648e-05,  1.6191e-05,  ..., -1.8978e-05,
         -1.5066e-05, -1.4406e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1761e-08, 5.6301e-08, 5.5178e-08,  ..., 2.2143e-08, 1.2377e-07,
         5.1697e-08],
        [9.1332e-11, 5.9615e-11, 2.8020e-11,  ..., 6.3218e-11, 3.3374e-11,
         3.1069e-11],
        [2.8877e-09, 2.1495e-09, 1.3656e-09,  ..., 2.0945e-09, 1.3981e-09,
         7.8069e-10],
        [9.5678e-10, 1.1896e-09, 4.1909e-10,  ..., 9.3642e-10, 7.6346e-10,
         4.0339e-10],
        [4.3921e-10, 2.6136e-10, 1.2173e-10,  ..., 3.3219e-10, 1.1989e-10,
         1.4233e-10]], device='cuda:0')
optimizer state dict: 240.0
lr: [3.6752822198246384e-07, 3.6752822198246384e-07]
scheduler_last_epoch: 240


Running epoch 1, step 1920, batch 872
Sampled inputs[:2]: tensor([[    0,   266,   858,  ..., 11265,   607,  7455],
        [    0,  6203,   352,  ...,   266,  3437,   287]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.3732e-05,  2.0480e-05,  1.5815e-07,  ..., -5.9741e-05,
          6.2613e-05,  4.6032e-05],
        [-1.3039e-06, -9.9838e-07,  1.0207e-06,  ..., -1.1697e-06,
         -9.4250e-07, -9.6112e-07],
        [-3.7551e-06, -2.9355e-06,  3.1292e-06,  ..., -3.2932e-06,
         -2.6673e-06, -2.6822e-06],
        [-2.5779e-06, -1.9222e-06,  2.1011e-06,  ..., -2.2948e-06,
         -1.8552e-06, -1.9968e-06],
        [-3.0845e-06, -2.5183e-06,  2.5183e-06,  ..., -2.7567e-06,
         -2.3246e-06, -2.0266e-06]], device='cuda:0')
Loss: 0.9488142132759094


Running epoch 1, step 1921, batch 873
Sampled inputs[:2]: tensor([[   0,  598,  278,  ...,  437,  266, 2388],
        [   0,  300,  266,  ...,  271, 4111, 1188]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0019e-06,  9.5281e-05, -7.8853e-05,  ..., -4.2550e-05,
         -2.8349e-05,  1.4469e-04],
        [-2.6673e-06, -1.9372e-06,  2.1011e-06,  ..., -2.3916e-06,
         -1.8589e-06, -1.9819e-06],
        [-7.8678e-06, -5.8711e-06,  6.5416e-06,  ..., -6.9588e-06,
         -5.4240e-06, -5.7369e-06],
        [-5.3197e-06, -3.7774e-06,  4.3362e-06,  ..., -4.7535e-06,
         -3.6880e-06, -4.1276e-06],
        [-6.2883e-06, -4.9472e-06,  5.1111e-06,  ..., -5.6922e-06,
         -4.6343e-06, -4.2468e-06]], device='cuda:0')
Loss: 0.9550657272338867


Running epoch 1, step 1922, batch 874
Sampled inputs[:2]: tensor([[    0,    28,  2973,  ...,  8762,  2134,    27],
        [    0,   259, 19567,  ...,   266,  3899,  2123]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 9.7317e-05,  1.3905e-04, -4.2193e-08,  ..., -6.3341e-05,
         -1.4046e-04,  1.6635e-04],
        [-4.0829e-06, -3.0249e-06,  3.3155e-06,  ..., -3.6061e-06,
         -2.8424e-06, -2.9318e-06],
        [-1.1981e-05, -9.2089e-06,  1.0267e-05,  ..., -1.0550e-05,
         -8.3745e-06, -8.5384e-06],
        [-8.0913e-06, -5.9232e-06,  6.8247e-06,  ..., -7.1526e-06,
         -5.6401e-06, -6.0797e-06],
        [-9.5516e-06, -7.7337e-06,  7.9870e-06,  ..., -8.5980e-06,
         -7.1377e-06, -6.3181e-06]], device='cuda:0')
Loss: 1.0193064212799072


Running epoch 1, step 1923, batch 875
Sampled inputs[:2]: tensor([[    0, 43071,   278,  ...,   266, 21576,  5936],
        [    0,   342,  3001,  ...,   369, 11195,   367]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.9009e-04, -7.2842e-06, -2.3912e-04,  ...,  1.3443e-05,
          8.6756e-06,  2.8822e-04],
        [-5.5507e-06, -3.8035e-06,  4.3884e-06,  ..., -4.9248e-06,
         -3.6731e-06, -4.1611e-06],
        [-1.6212e-05, -1.1533e-05,  1.3575e-05,  ..., -1.4231e-05,
         -1.0744e-05, -1.1936e-05],
        [-1.0863e-05, -7.3537e-06,  8.9258e-06,  ..., -9.6112e-06,
         -7.1898e-06, -8.4341e-06],
        [-1.3128e-05, -9.7454e-06,  1.0684e-05,  ..., -1.1772e-05,
         -9.2685e-06, -9.0301e-06]], device='cuda:0')
Loss: 0.9371116757392883


Running epoch 1, step 1924, batch 876
Sampled inputs[:2]: tensor([[    0,   879,    27,  ...,  3958,  2875,    14],
        [    0,  1624,   391,  ...,   391, 36249,   259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6552e-04,  4.1358e-05, -1.3624e-04,  ...,  3.3610e-05,
          1.8196e-04,  5.1872e-04],
        [-6.8992e-06, -4.9062e-06,  5.3383e-06,  ..., -6.1616e-06,
         -4.7460e-06, -5.1744e-06],
        [-2.0206e-05, -1.4931e-05,  1.6570e-05,  ..., -1.7866e-05,
         -1.3947e-05, -1.4946e-05],
        [-1.3500e-05, -9.4995e-06,  1.0848e-05,  ..., -1.2055e-05,
         -9.3505e-06, -1.0535e-05],
        [-1.6361e-05, -1.2606e-05,  1.3068e-05,  ..., -1.4782e-05,
         -1.2010e-05, -1.1295e-05]], device='cuda:0')
Loss: 0.9691481590270996


Running epoch 1, step 1925, batch 877
Sampled inputs[:2]: tensor([[    0, 16803,   965,  ..., 36064,    12, 13769],
        [    0,   275,  1911,  ...,  1371,  5151,  2813]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.6088e-04,  1.6551e-04, -1.5684e-04,  ...,  7.4831e-05,
          1.7917e-04,  4.8647e-04],
        [-8.3372e-06, -5.9567e-06,  6.3963e-06,  ..., -7.3537e-06,
         -5.6364e-06, -6.1728e-06],
        [-2.4378e-05, -1.8239e-05,  1.9923e-05,  ..., -2.1413e-05,
         -1.6689e-05, -1.7911e-05],
        [-1.6421e-05, -1.1660e-05,  1.3113e-05,  ..., -1.4499e-05,
         -1.1228e-05, -1.2666e-05],
        [-1.9655e-05, -1.5333e-05,  1.5631e-05,  ..., -1.7658e-05,
         -1.4350e-05, -1.3486e-05]], device='cuda:0')
Loss: 0.9574570059776306


Running epoch 1, step 1926, batch 878
Sampled inputs[:2]: tensor([[   0,   29,  413,  ..., 1527, 1503,  369],
        [   0,  445,   16,  ..., 7747, 5308, 6216]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.0737e-04,  4.3333e-04, -2.1974e-04,  ...,  2.4477e-05,
         -3.4288e-04,  3.2290e-04],
        [-9.7752e-06, -6.9775e-06,  7.3574e-06,  ..., -8.6203e-06,
         -6.6869e-06, -7.2308e-06],
        [-2.8461e-05, -2.1324e-05,  2.2948e-05,  ..., -2.5004e-05,
         -1.9729e-05, -2.0891e-05],
        [-1.9208e-05, -1.3642e-05,  1.5065e-05,  ..., -1.6958e-05,
         -1.3299e-05, -1.4812e-05],
        [-2.3082e-05, -1.8016e-05,  1.8060e-05,  ..., -2.0728e-05,
         -1.7092e-05, -1.5810e-05]], device='cuda:0')
Loss: 0.9144024848937988


Running epoch 1, step 1927, batch 879
Sampled inputs[:2]: tensor([[   0,  271, 2862,  ...,  287, 5699,   18],
        [   0, 2159,  271,  ..., 1268,  344,  259]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.5774e-05,  3.7444e-04, -3.2447e-04,  ..., -1.2313e-04,
         -1.4331e-04,  3.2290e-04],
        [-1.1310e-05, -8.0355e-06,  8.4005e-06,  ..., -9.9987e-06,
         -7.8045e-06, -8.3484e-06],
        [-3.2812e-05, -2.4498e-05,  2.6107e-05,  ..., -2.8908e-05,
         -2.2963e-05, -2.4080e-05],
        [-2.2069e-05, -1.5639e-05,  1.7077e-05,  ..., -1.9550e-05,
         -1.5445e-05, -1.6987e-05],
        [-2.6926e-05, -2.0906e-05,  2.0802e-05,  ..., -2.4244e-05,
         -2.0102e-05, -1.8477e-05]], device='cuda:0')
Loss: 0.9582485556602478
Graident accumulation at epoch 1, step 1927, batch 879
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 1.0295e-04,  1.9168e-04, -1.0198e-04,  ..., -7.4687e-05,
         -8.9972e-05,  6.5032e-06],
        [-1.0925e-05, -7.7739e-06,  8.3072e-06,  ..., -9.6647e-06,
         -7.5833e-06, -7.9581e-06],
        [ 3.1608e-05,  5.6247e-05, -2.5131e-05,  ...,  3.4820e-05,
          4.0287e-05,  1.7047e-05],
        [-1.4285e-05, -7.9092e-06,  1.3159e-05,  ..., -1.3264e-05,
         -8.3693e-06, -1.2743e-05],
        [-2.2757e-05, -1.7074e-05,  1.6652e-05,  ..., -1.9504e-05,
         -1.5570e-05, -1.4813e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1701e-08, 5.6385e-08, 5.5228e-08,  ..., 2.2136e-08, 1.2366e-07,
         5.1750e-08],
        [9.1369e-11, 5.9620e-11, 2.8062e-11,  ..., 6.3255e-11, 3.3401e-11,
         3.1107e-11],
        [2.8858e-09, 2.1479e-09, 1.3649e-09,  ..., 2.0933e-09, 1.3973e-09,
         7.8049e-10],
        [9.5631e-10, 1.1887e-09, 4.1896e-10,  ..., 9.3587e-10, 7.6294e-10,
         4.0327e-10],
        [4.3950e-10, 2.6153e-10, 1.2204e-10,  ..., 3.3245e-10, 1.2017e-10,
         1.4253e-10]], device='cuda:0')
optimizer state dict: 241.0
lr: [3.3505955649678844e-07, 3.3505955649678844e-07]
scheduler_last_epoch: 241


Running epoch 1, step 1928, batch 880
Sampled inputs[:2]: tensor([[    0,  2793,   271,  ...,   374,   298,   527],
        [    0,   631,   516,  ..., 13374,   898,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.0324e-04, -1.3602e-04, -5.4417e-05,  ...,  1.7487e-05,
          2.4204e-04,  1.1273e-05],
        [-1.3784e-06, -9.4250e-07,  1.1101e-06,  ..., -1.1772e-06,
         -9.2760e-07, -1.0207e-06],
        [ 6.7477e-05,  6.5005e-05, -4.9792e-05,  ...,  3.8827e-05,
          7.0095e-05,  2.0082e-05],
        [-2.6822e-06, -1.7956e-06,  2.2501e-06,  ..., -2.2799e-06,
         -1.7881e-06, -2.0564e-06],
        [-3.2485e-06, -2.3395e-06,  2.7120e-06,  ..., -2.7418e-06,
         -2.2650e-06, -2.1160e-06]], device='cuda:0')
Loss: 0.9625560641288757


Running epoch 1, step 1929, batch 881
Sampled inputs[:2]: tensor([[   0, 7849,  278,  ...,  346,  462,  221],
        [   0,  328,  266,  ...,  382,   17,   13]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.9898e-04, -1.9795e-04, -6.2336e-05,  ..., -4.8092e-05,
          4.3928e-04, -2.8598e-05],
        [-2.6599e-06, -1.8105e-06,  2.0042e-06,  ..., -2.4289e-06,
         -1.9111e-06, -2.0638e-06],
        [ 6.4065e-05,  6.2621e-05, -4.7184e-05,  ...,  3.5638e-05,
          6.7562e-05,  1.7504e-05],
        [-5.1111e-06, -3.3975e-06,  4.0308e-06,  ..., -4.6641e-06,
         -3.6731e-06, -4.0978e-06],
        [-6.4373e-06, -4.6343e-06,  5.0813e-06,  ..., -5.7518e-06,
         -4.7386e-06, -4.3213e-06]], device='cuda:0')
Loss: 0.8993643522262573


Running epoch 1, step 1930, batch 882
Sampled inputs[:2]: tensor([[    0,    18,    14,  ...,   300,   275,  1184],
        [    0,   278,  6653,  ...,  7524,   271, 28279]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.6772e-04, -2.0247e-04,  7.2260e-06,  ..., -3.4569e-05,
          6.8406e-04,  1.1418e-04],
        [-4.0382e-06, -2.9281e-06,  3.0920e-06,  ..., -3.6731e-06,
         -2.9840e-06, -3.0994e-06],
        [ 6.0101e-05,  5.9253e-05, -4.3861e-05,  ...,  3.2062e-05,
          6.4433e-05,  1.4524e-05],
        [-7.8827e-06, -5.6475e-06,  6.3106e-06,  ..., -7.1675e-06,
         -5.8487e-06, -6.2883e-06],
        [-9.7305e-06, -7.5251e-06,  7.7784e-06,  ..., -8.7768e-06,
         -7.4804e-06, -6.6310e-06]], device='cuda:0')
Loss: 0.981360137462616


Running epoch 1, step 1931, batch 883
Sampled inputs[:2]: tensor([[    0,  1732,   699,  ...,   417,   199,  1726],
        [    0,   287,   266,  ...,   998,   342, 17709]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.8464e-04, -1.2258e-04, -6.7761e-05,  ..., -4.8751e-05,
          7.3822e-04, -2.7575e-05],
        [-5.3570e-06, -3.9190e-06,  4.1574e-06,  ..., -4.8354e-06,
         -3.8520e-06, -4.0308e-06],
        [ 5.6197e-05,  5.6228e-05, -4.0538e-05,  ...,  2.8649e-05,
          6.1885e-05,  1.1827e-05],
        [-1.0625e-05, -7.6592e-06,  8.6203e-06,  ..., -9.5665e-06,
         -7.6443e-06, -8.3148e-06],
        [-1.2875e-05, -1.0103e-05,  1.0401e-05,  ..., -1.1578e-05,
         -9.6858e-06, -8.6427e-06]], device='cuda:0')
Loss: 0.9449753761291504


Running epoch 1, step 1932, batch 884
Sampled inputs[:2]: tensor([[    0, 16763,  1538,  ...,   631,  3299,   437],
        [    0,   292,    17,  ...,   265,  6943,    14]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.3615e-04,  6.7231e-05,  3.4461e-05,  ...,  1.4863e-04,
          5.2935e-04, -1.5163e-04],
        [-6.7055e-06, -4.8727e-06,  5.2452e-06,  ..., -6.0424e-06,
         -4.7907e-06, -5.0440e-06],
        [ 5.2427e-05,  5.3427e-05, -3.7260e-05,  ...,  2.5282e-05,
          5.9217e-05,  9.0107e-06],
        [-1.3262e-05, -9.4995e-06,  1.0870e-05,  ..., -1.1921e-05,
         -9.4920e-06, -1.0386e-05],
        [-1.6019e-05, -1.2562e-05,  1.3039e-05,  ..., -1.4439e-05,
         -1.2070e-05, -1.0818e-05]], device='cuda:0')
Loss: 0.9646066427230835


Running epoch 1, step 1933, batch 885
Sampled inputs[:2]: tensor([[   0, 2310,  292,  ...,  462,  508,  586],
        [   0,  380,  333,  ...,  333,  199, 2038]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-4.1242e-04,  9.8439e-05,  2.0622e-04,  ...,  1.8009e-04,
          5.9561e-04,  3.8053e-06],
        [-7.9572e-06, -5.8711e-06,  6.1244e-06,  ..., -7.2718e-06,
         -5.8562e-06, -6.0201e-06],
        [ 4.8970e-05,  5.0521e-05, -3.4623e-05,  ...,  2.1914e-05,
          5.6267e-05,  6.3732e-06],
        [-1.5691e-05, -1.1422e-05,  1.2629e-05,  ..., -1.4350e-05,
         -1.1623e-05, -1.2368e-05],
        [-1.9059e-05, -1.5214e-05,  1.5318e-05,  ..., -1.7434e-05,
         -1.4782e-05, -1.2979e-05]], device='cuda:0')
Loss: 0.9492169618606567


Running epoch 1, step 1934, batch 886
Sampled inputs[:2]: tensor([[    0,   409,  4146,  ...,     9,   360,   259],
        [    0,    14,  8058,  ..., 10316,   352,   266]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-2.7239e-04,  1.2755e-04,  2.7910e-05,  ...,  2.0130e-04,
          4.5320e-04, -9.2493e-05],
        [-9.2238e-06, -6.7465e-06,  7.2718e-06,  ..., -8.3596e-06,
         -6.7167e-06, -6.9477e-06],
        [ 4.5393e-05,  4.7943e-05, -3.1136e-05,  ...,  1.8859e-05,
          5.3793e-05,  3.7655e-06],
        [-1.8254e-05, -1.3165e-05,  1.5102e-05,  ..., -1.6540e-05,
         -1.3381e-05, -1.4335e-05],
        [-2.1920e-05, -1.7419e-05,  1.8045e-05,  ..., -1.9953e-05,
         -1.6943e-05, -1.4886e-05]], device='cuda:0')
Loss: 0.9499109983444214


Running epoch 1, step 1935, batch 887
Sampled inputs[:2]: tensor([[    0,    13, 20793,  ...,    17,   287,  1356],
        [    0,    83,   292,  ...,   445,    11, 16109]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.8179e-04, -2.2040e-05, -1.8946e-04,  ...,  3.0938e-04,
          4.8202e-04,  3.0236e-04],
        [-1.0535e-05, -7.6853e-06,  8.1770e-06,  ..., -9.5963e-06,
         -7.6927e-06, -7.9460e-06],
        [ 4.1653e-05,  4.5142e-05, -2.8334e-05,  ...,  1.5417e-05,
          5.1051e-05,  9.7902e-07],
        [-2.0847e-05, -1.4983e-05,  1.6950e-05,  ..., -1.8984e-05,
         -1.5348e-05, -1.6406e-05],
        [-2.5034e-05, -1.9848e-05,  2.0325e-05,  ..., -2.2843e-05,
         -1.9342e-05, -1.7017e-05]], device='cuda:0')
Loss: 0.9630602598190308
Graident accumulation at epoch 1, step 1935, batch 887
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 5.4473e-05,  1.7031e-04, -1.1073e-04,  ..., -3.6280e-05,
         -3.2773e-05,  3.6089e-05],
        [-1.0886e-05, -7.7650e-06,  8.2942e-06,  ..., -9.6579e-06,
         -7.5943e-06, -7.9569e-06],
        [ 3.2613e-05,  5.5137e-05, -2.5452e-05,  ...,  3.2879e-05,
          4.1364e-05,  1.5440e-05],
        [-1.4941e-05, -8.6166e-06,  1.3538e-05,  ..., -1.3836e-05,
         -9.0672e-06, -1.3110e-05],
        [-2.2985e-05, -1.7351e-05,  1.7020e-05,  ..., -1.9838e-05,
         -1.5947e-05, -1.5033e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1785e-08, 5.6329e-08, 5.5209e-08,  ..., 2.2209e-08, 1.2377e-07,
         5.1789e-08],
        [9.1388e-11, 5.9619e-11, 2.8101e-11,  ..., 6.3284e-11, 3.3427e-11,
         3.1139e-11],
        [2.8847e-09, 2.1478e-09, 1.3643e-09,  ..., 2.0914e-09, 1.3985e-09,
         7.7971e-10],
        [9.5578e-10, 1.1877e-09, 4.1883e-10,  ..., 9.3529e-10, 7.6241e-10,
         4.0314e-10],
        [4.3969e-10, 2.6167e-10, 1.2233e-10,  ..., 3.3264e-10, 1.2042e-10,
         1.4268e-10]], device='cuda:0')
optimizer state dict: 242.0
lr: [3.040677782773405e-07, 3.040677782773405e-07]
scheduler_last_epoch: 242


Running epoch 1, step 1936, batch 888
Sampled inputs[:2]: tensor([[   0, 7180,  266,  ..., 1805,   12,  221],
        [   0, 1746,   14,  ..., 3134, 5968,    9]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-1.0758e-04,  2.2478e-05, -1.6694e-04,  ..., -1.0504e-05,
          7.0059e-05, -1.0001e-04],
        [-1.3784e-06, -8.0839e-07,  1.0207e-06,  ..., -1.2517e-06,
         -8.9034e-07, -1.1325e-06],
        [-3.8743e-06, -2.3842e-06,  3.1441e-06,  ..., -3.3975e-06,
         -2.4736e-06, -3.0398e-06],
        [-2.5928e-06, -1.4901e-06,  2.0266e-06,  ..., -2.3693e-06,
         -1.7062e-06, -2.1905e-06],
        [-3.2485e-06, -2.0564e-06,  2.5481e-06,  ..., -2.8908e-06,
         -2.1905e-06, -2.3246e-06]], device='cuda:0')
Loss: 0.9290686249732971


Running epoch 1, step 1937, batch 889
Sampled inputs[:2]: tensor([[   0,  271,  266,  ..., 5933,   35, 5621],
        [   0,  344, 2574,  ..., 2558, 2663,  328]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-9.7139e-05, -2.4965e-05, -3.4788e-04,  ..., -8.8326e-05,
          1.1377e-05, -2.7530e-04],
        [-2.7344e-06, -1.8142e-06,  2.2054e-06,  ..., -2.3618e-06,
         -1.7732e-06, -2.0377e-06],
        [-7.8380e-06, -5.3942e-06,  6.8098e-06,  ..., -6.6310e-06,
         -5.0366e-06, -5.7071e-06],
        [-5.2452e-06, -3.4273e-06,  4.4554e-06,  ..., -4.5449e-06,
         -3.4124e-06, -4.0382e-06],
        [-6.4224e-06, -4.6194e-06,  5.3942e-06,  ..., -5.5730e-06,
         -4.4256e-06, -4.2766e-06]], device='cuda:0')
Loss: 0.9952605962753296


Running epoch 1, step 1938, batch 890
Sampled inputs[:2]: tensor([[    0,  5775,    12,  ...,    12,  1034,  9257],
        [    0,  9041,  8375,  ...,   221,   474, 43112]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 7.6670e-05, -2.2989e-05, -4.1871e-04,  ..., -2.2891e-06,
         -1.3696e-04, -3.8648e-04],
        [-4.0829e-06, -2.7493e-06,  3.3155e-06,  ..., -3.4794e-06,
         -2.6040e-06, -2.9616e-06],
        [-1.1653e-05, -8.1360e-06,  1.0192e-05,  ..., -9.7603e-06,
         -7.4059e-06, -8.2850e-06],
        [-7.9572e-06, -5.2601e-06,  6.7949e-06,  ..., -6.7651e-06,
         -5.0664e-06, -5.9754e-06],
        [-9.4920e-06, -6.9290e-06,  8.0317e-06,  ..., -8.1360e-06,
         -6.4820e-06, -6.1765e-06]], device='cuda:0')
Loss: 0.9136384129524231


Running epoch 1, step 1939, batch 891
Sampled inputs[:2]: tensor([[    0,  1142,    87,  ...,  2273,   287,   829],
        [    0,   474,   221,  ...,   287, 20640,   292]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[-3.1909e-06,  5.8240e-06, -6.1516e-04,  ...,  2.3094e-04,
         -3.0017e-04, -4.2846e-04],
        [-5.3868e-06, -3.6359e-06,  4.4033e-06,  ..., -4.6641e-06,
         -3.4608e-06, -3.8780e-06],
        [-1.5274e-05, -1.0625e-05,  1.3426e-05,  ..., -1.2949e-05,
         -9.7305e-06, -1.0729e-05],
        [-1.0505e-05, -6.9290e-06,  9.0003e-06,  ..., -9.0748e-06,
         -6.7279e-06, -7.8380e-06],
        [-1.2472e-05, -9.1046e-06,  1.0639e-05,  ..., -1.0803e-05,
         -8.5384e-06, -7.9870e-06]], device='cuda:0')
Loss: 0.9625369310379028


Running epoch 1, step 1940, batch 892
Sampled inputs[:2]: tensor([[    0,   275,   467,  ...,   298,   365,  2714],
        [    0, 10215,   408,  ...,  6071,   360,  1317]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 5.0402e-05, -6.0890e-05, -7.8994e-04,  ...,  2.7237e-04,
         -2.2583e-04, -3.9569e-04],
        [-6.8322e-06, -4.5672e-06,  5.4613e-06,  ..., -5.8785e-06,
         -4.3549e-06, -4.9584e-06],
        [ 1.5296e-04,  1.2101e-04, -7.1250e-05,  ...,  1.1265e-04,
          8.8293e-05,  6.3098e-05],
        [-1.3426e-05, -8.7470e-06,  1.1206e-05,  ..., -1.1504e-05,
         -8.5086e-06, -1.0073e-05],
        [-1.5974e-05, -1.1504e-05,  1.3277e-05,  ..., -1.3769e-05,
         -1.0818e-05, -1.0416e-05]], device='cuda:0')
Loss: 0.9800828695297241


Running epoch 1, step 1941, batch 893
Sampled inputs[:2]: tensor([[    0,   650,    14,  ...,  3687,   278, 26952],
        [    0,    14,   475,  ...,  2117,  2792, 12848]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6518e-04,  3.8596e-05, -7.6236e-04,  ...,  3.8112e-04,
         -2.0927e-04, -4.2749e-04],
        [-8.2925e-06, -5.5507e-06,  6.3777e-06,  ..., -7.1898e-06,
         -5.3756e-06, -6.1058e-06],
        [ 1.4879e-04,  1.1803e-04, -6.8389e-05,  ...,  1.0894e-04,
          8.5342e-05,  5.9879e-05],
        [-1.6153e-05, -1.0565e-05,  1.2979e-05,  ..., -1.3977e-05,
         -1.0461e-05, -1.2279e-05],
        [-1.9521e-05, -1.4096e-05,  1.5631e-05,  ..., -1.6987e-05,
         -1.3471e-05, -1.2934e-05]], device='cuda:0')
Loss: 0.9206372499465942


Running epoch 1, step 1942, batch 894
Sampled inputs[:2]: tensor([[    0,  1057,    14,  ...,    14,  4735,    13],
        [    0,   689,    13,  ...,   756,   271, 31773]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4430e-04,  1.2323e-04, -6.8402e-04,  ...,  3.6744e-04,
         -3.3552e-04, -5.5804e-04],
        [-9.6709e-06, -6.5118e-06,  7.3835e-06,  ..., -8.4266e-06,
         -6.3740e-06, -7.1265e-06],
        [ 1.4479e-04,  1.1516e-04, -6.5275e-05,  ...,  1.0541e-04,
          8.2496e-05,  5.6958e-05],
        [-1.8820e-05, -1.2405e-05,  1.5020e-05,  ..., -1.6391e-05,
         -1.2398e-05, -1.4335e-05],
        [-2.2948e-05, -1.6674e-05,  1.8224e-05,  ..., -2.0087e-05,
         -1.6078e-05, -1.5289e-05]], device='cuda:0')
Loss: 0.9766430854797363


Running epoch 1, step 1943, batch 895
Sampled inputs[:2]: tensor([[    0,   259,  2697,  ...,  1722, 12673, 15053],
        [    0, 38816,   292,  ...,   346,   462,   221]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 2.8357e-04,  2.1667e-04, -6.8570e-04,  ...,  3.4745e-04,
         -3.7807e-04, -7.9500e-04],
        [-1.0818e-05, -7.3612e-06,  8.3968e-06,  ..., -9.5069e-06,
         -7.2308e-06, -8.0206e-06],
        [ 1.4152e-04,  1.1274e-04, -6.2191e-05,  ...,  1.0245e-04,
          8.0142e-05,  5.4589e-05],
        [-2.1160e-05, -1.4059e-05,  1.7211e-05,  ..., -1.8567e-05,
         -1.4141e-05, -1.6227e-05],
        [-2.5690e-05, -1.8790e-05,  2.0713e-05,  ..., -2.2590e-05,
         -1.8150e-05, -1.7077e-05]], device='cuda:0')
Loss: 0.897275984287262
Graident accumulation at epoch 1, step 1943, batch 895
Updated and will save.
LLM.module.transformer.wte.weight[:5]: tensor([[ 0.0540, -0.0033, -0.0265,  ..., -0.0108, -0.0149,  0.0164],
        [ 0.0061, -0.0140,  0.0023,  ..., -0.0019,  0.0236, -0.0189],
        [ 0.0286, -0.0082,  0.0038,  ..., -0.0099, -0.0028, -0.0343],
        [ 0.0342, -0.0090,  0.0400,  ...,  0.0230,  0.0069, -0.0010],
        [-0.0154,  0.0157, -0.0289,  ...,  0.0293, -0.0141, -0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
optimizer state dict: tensor([[ 7.7383e-05,  1.7494e-04, -1.6823e-04,  ...,  2.0936e-06,
         -6.7302e-05, -4.7020e-05],
        [-1.0879e-05, -7.7246e-06,  8.3045e-06,  ..., -9.6428e-06,
         -7.5579e-06, -7.9633e-06],
        [ 4.3503e-05,  6.0897e-05, -2.9126e-05,  ...,  3.9836e-05,
          4.5241e-05,  1.9355e-05],
        [-1.5563e-05, -9.1609e-06,  1.3905e-05,  ..., -1.4309e-05,
         -9.5746e-06, -1.3422e-05],
        [-2.3255e-05, -1.7495e-05,  1.7389e-05,  ..., -2.0113e-05,
         -1.6167e-05, -1.5238e-05]], device='cuda:0')
optimizer state dict: tensor([[6.1804e-08, 5.6320e-08, 5.5624e-08,  ..., 2.2308e-08, 1.2379e-07,
         5.2370e-08],
        [9.1414e-11, 5.9614e-11, 2.8144e-11,  ..., 6.3311e-11, 3.3446e-11,
         3.1172e-11],
        [2.9018e-09, 2.1584e-09, 1.3668e-09,  ..., 2.0998e-09, 1.4035e-09,
         7.8191e-10],
        [9.5528e-10, 1.1867e-09, 4.1871e-10,  ..., 9.3470e-10, 7.6185e-10,
         4.0300e-10],
        [4.3991e-10, 2.6176e-10, 1.2263e-10,  ..., 3.3282e-10, 1.2063e-10,
         1.4283e-10]], device='cuda:0')
optimizer state dict: 243.0
lr: [2.745576231383573e-07, 2.745576231383573e-07]
scheduler_last_epoch: 243


Running epoch 1, step 1944, batch 896
Sampled inputs[:2]: tensor([[    0,  5750,   642,  ...,   221, 15441,   644],
        [    0,  1238,    14,  ...,   368,   940,   437]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 6.5511e-05,  2.0792e-04,  2.7277e-04,  ...,  1.8472e-04,
          1.9472e-05,  2.2482e-05],
        [-1.2219e-06, -9.9838e-07,  9.0152e-07,  ..., -1.1846e-06,
         -9.6112e-07, -9.5367e-07],
        [-3.6806e-06, -3.0696e-06,  2.9057e-06,  ..., -3.5018e-06,
         -2.8163e-06, -2.8461e-06],
        [-2.4587e-06, -1.9670e-06,  1.8701e-06,  ..., -2.3991e-06,
         -1.9372e-06, -2.0266e-06],
        [-2.9951e-06, -2.5779e-06,  2.3097e-06,  ..., -2.8908e-06,
         -2.4289e-06, -2.1309e-06]], device='cuda:0')
Loss: 0.9722515940666199


Running epoch 1, step 1945, batch 897
Sampled inputs[:2]: tensor([[   0,   18,  998,  ..., 5322,  504,  287],
        [   0,   16,   52,  ...,   12,  298,  374]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.8463e-04,  1.5352e-04,  3.6193e-04,  ...,  2.0653e-04,
          8.6572e-05, -1.3348e-04],
        [-2.5034e-06, -2.0340e-06,  1.8440e-06,  ..., -2.4289e-06,
         -1.9893e-06, -1.9222e-06],
        [-7.3910e-06, -6.2287e-06,  5.8264e-06,  ..., -7.0781e-06,
         -5.7667e-06, -5.6326e-06],
        [-4.9323e-06, -3.9637e-06,  3.7476e-06,  ..., -4.8280e-06,
         -3.9637e-06, -4.0233e-06],
        [-6.1244e-06, -5.3346e-06,  4.7088e-06,  ..., -5.9456e-06,
         -5.0515e-06, -4.2915e-06]], device='cuda:0')
Loss: 0.9846660494804382


Running epoch 1, step 1946, batch 898
Sampled inputs[:2]: tensor([[    0, 45589,    13,  ...,    23,  6873,     9],
        [    0,   342,   266,  ...,   586,  1944,   271]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.6443e-04,  2.0372e-04,  3.9167e-04,  ...,  2.5186e-04,
         -7.1348e-05, -1.2764e-04],
        [-3.8370e-06, -3.0473e-06,  2.8722e-06,  ..., -3.6284e-06,
         -2.9504e-06, -2.8834e-06],
        [-1.1265e-05, -9.2685e-06,  9.0301e-06,  ..., -1.0535e-05,
         -8.5384e-06, -8.4192e-06],
        [-7.5698e-06, -5.9307e-06,  5.8785e-06,  ..., -7.1973e-06,
         -5.8636e-06, -6.0201e-06],
        [-9.2983e-06, -7.9572e-06,  7.2569e-06,  ..., -8.8513e-06,
         -7.4804e-06, -6.3926e-06]], device='cuda:0')
Loss: 0.9505757093429565


Running epoch 1, step 1947, batch 899
Sampled inputs[:2]: tensor([[   0, 2165, 1323,  ...,  199,  677, 8376],
        [   0, 3543,  391,  ..., 3370, 2926, 8090]], device='cuda:0')
LLM.module.transformer.wte.weight[:5] grads: tensor([[ 1.4011e-04,  2.5430e-04,  1.8864e-04,  ...,  3.5765e-04,
         -3.5844e-04, -1.8181e-04],
        [-5.2750e-06, -4.0308e-06,  4.0121e-06,  ..., -4.8503e-06,
         -3.9041e-06, -3.9414e-06],
        [-1.5467e-05, -1.2264e-05,  1.2532e-05,  ..., -1.4111e-05,
         -1.1340e-05, -1.1548e-05],
        [-1.0297e-05, -7.7784e-06,  8.1286e-06,  ..., -9.5069e-06,
         -7.6592e-06, -8.0913e-06],
        [-1.2815e-05, -1.0580e-05,  1.0118e-05,  ..., -1.1921e-05,
         -9.9987e-06, -8.8513e-06]], device='cuda:0')
Loss: 0.962664008140564
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 1.339 MB uploaded (0.000 MB deduped)wandb: \ 1.339 MB of 1.339 MB uploaded (0.000 MB deduped)wandb: | 1.339 MB of 1.339 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:          Batch ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá
wandb:   End of epoch ‚ñÅ
wandb:          Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  Learning Rate ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Training PPL ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: Validation PPL ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:          Batch 799
wandb:   End of epoch 0
wandb:          Epoch 1
wandb:  Learning Rate 0.0
wandb:   Training PPL 2182.66742
wandb: Validation PPL 6.72303
wandb: 
wandb: üöÄ View run upbeat-breeze-312 at: https://wandb.ai/kenotron/brainlessgpt/runs/y1iuvq6b
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_140117-y1iuvq6b/logs
